<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">8 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/><a id="idTextAnchor006"/><a id="idTextAnchor007"/>Text generation with recurrent neural networks<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="idIndexMarker002"/></h1>
<p class="co-summary-head"><a id="marker-169"/>This chapter covers</p>
<ul class="calibre5">
<li class="co-summary-bullet">The idea behind RNNs and why they can handle sequential data</li>
<li class="co-summary-bullet">Character tokenization, word tokenization, and subword tokenization</li>
<li class="co-summary-bullet">How word embedding works<a class="calibre" id="idIndexMarker003"/></li>
<li class="co-summary-bullet">Building and training an RNN to generate text</li>
<li class="co-summary-bullet">Using temperature and top-K sampling to control the creativeness of text generation<a class="calibre" id="idIndexMarker004"/></li>
</ul>
<p class="body">So far in this book, we have discussed how to generate shapes, numbers, and images. Starting from this chapter, we’ll focus mainly on text generation. Generating text is often considered the holy grail of generative AI for several compelling reasons. Human language is incredibly complex and nuanced. It involves understanding not only grammar and vocabulary but also context, tone, and cultural references. Successfully generating coherent and contextually appropriate text is a significant challenge that requires deep understanding and processing of language.</p>
<p class="body">As humans, we primarily communicate through language. AI that can generate human-like text can interact more naturally with users, making technology more accessible and user-friendly. Text generation has many applications, from automating customer service responses to creating entire articles, scripting for games and movies, aiding in creative writing, and even building personal assistants. The potential effect across industries is enormous.</p>
<p class="body">In this chapter, we’ll make our first attempt at building and training models to generate text. You’ll learn to tackle three main challenges in modeling text generation. First, text is sequential data, consisting of data points organized in a specific sequence, where each point is successively ordered to reflect the inherent order and interdependencies within the data. Predicting outcomes for sequences is challenging due to their sensitive ordering. Altering the sequence of elements changes their meaning. Second, text exhibits long-range dependencies: the meaning of a certain part of the text depends on elements that appeared much earlier in the text (e.g., 100 words ago). Understanding and modeling these long-range dependencies is essential for generating coherent text. Lastly, human language is ambiguous and context dependent. Training a model to understand nuances, sarcasm, idioms, and cultural references to generate contextually accurate text is challenging.</p>
<p class="body">You’ll explore a specific neural network designed for handling sequential data, such as text or time series: the recurrent neural network (RNN). Traditional neural networks, such as feedforward neural networks or fully connected networks, treat each input independently. This means that the network processes each input separately, without considering any relationship or order between different inputs. In contrast, RNNs are specifically designed to handle sequential data. In an RNN, the output at a given time step depends not only on the current input but also on previous inputs. This allows RNNs to maintain a form of memory, capturing information from previous time steps to influence the processing of the current input.</p>
<p class="body"><a id="marker-170"/>This sequential processing makes RNNs suitable for tasks where the order of the inputs matters, such as language modeling, where the goal is to predict the next word in a sentence based on previous words. We’ll focus on one variant of RNN, long short-term memory (LSTM) networks, which can recognize both short-term and long-term data patterns in sequential data like text. LSTM models use a hidden state to capture information in previous time steps. Therefore, a trained LSTM model can produce coherent text based on the context. <a id="idIndexMarker005"/><a id="idIndexMarker006"/><a id="idIndexMarker007"/></p>
<p class="body">The style of the generated text depends on the training data. Additionally, as we plan to train a model from scratch for text generation, the length of the training text is a crucial factor. It needs to be sufficiently extensive for the model to effectively learn and mimic a particular writing style yet concise enough to avoid excessive computational demands during training. As a result, we’ll use the text from the novel <i class="fm-italics">Anna Karenina</i>, which appears to be of the right length for our purposes, to train an LSTM model. Since neural networks like an LSTM cannot accept text as input directly, you’ll learn to break down text into tokens (individual words in this chapter but can be parts of words, as you’ll see in later chapters), a process known as <i class="fm-italics">tokenization</i>. You’ll then create a dictionary to map each unique token into an integer (i.e., an index). Based on this dictionary, you’ll convert the text into a long sequence of integers, ready to be fed into a neural network.</p>
<p class="body">You’ll use sequences of indexes of a certain length as the input to train the LSTM model. You shift the sequence of inputs by one token to the right and use it as the output: you are effectively training the model to predict the next token in a sentence. This is the so-called <i class="fm-italics">sequence-to-sequence</i> prediction problem in natural language processing (NLP), and you’ll see it again in later chapters.</p>
<p class="body">Once the LSTM is trained, you’ll use it to generate text one token at a time based on previous tokens in the sequence as follows: you feed a prompt (part of a sentence such as “Anna and the”) to the trained model. The model then predicts the most likely next token and appends the selected token to your prompt. The updated prompt serves again as the input, and the model is used once more to predict the next token. The iterative process continues until the prompt reaches a certain length. This approach is similar to the mechanism employed by more advanced generative models like ChatGPT (though ChatGPT is not an LSTM). You’ll witness the trained LSTM model generating grammatically correct and coherent text, with a style matching that of the original novel.</p>
<p class="body">Finally, you also learn how to control the creativeness of the generated text using temperature and top-K sampling. Temperature controls the randomness of the predictions of the trained model. A high temperature makes the generated text more creative while a low temperature makes the text more confident and predictable. Top-K sampling is a method where you select the next token from the top K most probable tokens, rather than selecting from the entire vocabulary. A small value of K leads to the selection of highly likely tokens in each step, and this, in turn, makes the generated text less creative and more coherent.</p>
<p class="body">The primary goal of this chapter is not necessarily to generate the most coherent text possible, which, as mentioned earlier, presents substantial challenges. Instead, our objective is to demonstrate the limitations of RNNs, thereby setting the stage for the introduction of Transformers in subsequent chapters. More importantly, this chapter establishes the basic principles of text generation, including tokenization, word embedding, sequence prediction, temperature settings, and top-K sampling. Consequently, in later chapters, you will have a solid understanding of the fundamentals of NLP. This foundation will allow us to concentrate on other, more advanced aspects of NLP, such as how the attention mechanism functions and the architecture of Transformers.</p>
<h2 class="fm-head" id="heading_id_3">8.1 Introduction to RNNs</h2>
<p class="body"><a id="marker-171"/>At the beginning of this chapter, we touched upon the complexities involved in generating text, particularly when aiming for coherence and contextual relevance. This section dives deeper into these challenges and explores the architecture of RNNs. We’ll explain why RNNs are suitable for the task and their limitations (which are the reasons they have been overtaken by Transformers). <a id="idIndexMarker008"/><a id="idIndexMarker009"/></p>
<p class="body">RNNs are specifically designed to handle sequential data, making them capable of text generation, a task inherently sequential in nature. They utilize a form of memory, known as hidden states, to capture and retain information from earlier parts of the sequence. This capability is crucial for maintaining context and understanding dependencies as the sequence progresses.</p>
<p class="body">In this chapter, we will specifically utilize LSTM networks, advanced versions of RNNs, for text generation, using their advanced capabilities to tackle the challenges in this task.</p>
<h3 class="fm-head1" id="heading_id_4">8.1.1 Challenges in generating text</h3>
<p class="body">Text represents a quintessential example of <i class="fm-italics">sequential data</i>, which is defined as any dataset where the order of elements is critical. This structuring implies that the positioning of individual elements relative to each other holds significant meaning, often conveying essential information for understanding the data. Examples of sequential data include time series (like stock prices), textual content (such as sentences), and musical compositions (a succession of notes).<a id="marker-172"/><a id="idIndexMarker010"/></p>
<p class="body">This book primarily zeroes in on text generation, although it also ventures into music generation in chapters 13 and 14. The process of generating text is fraught with complexities. A primary challenge lies in modeling the sequence of words within sentences, where altering the order can drastically change the meaning. For instance, in the sentence “Kentucky defeated Vanderbilt in last night’s football game,” swapping ‘Kentucky’ and ‘Vanderbilt’ entirely reverses the sentence’s implication, despite using the same words. Furthermore, as mentioned in the introduction, text generation encounters challenges in handling long-range dependencies and dealing with the problem of ambiguity.</p>
<p class="body">In this chapter, we will explore one approach to tackle these<a id="idTextAnchor008"/> challenges—namely, by using RNNs. While this method isn’t flawless, it lays the groundwork for more advanced techniques you’ll encounter in later chapters. This approach will provide insight into managing word order, addressing long-range dependencies, and navigating the inherent ambiguity in text, equipping you with fundamental skills in text generation. The journey through this chapter serves as a stepping stone to more sophisticated methods and deeper understanding in the subsequent parts of the book. Along the way, you’ll acquire many valuable skills in NLP, such as text tokenization, word embedding, and sequence-to-sequence predictions.</p>
<h3 class="fm-head1" id="heading_id_5">8.1.2 How do RNNs work?</h3>
<p class="body">RNNs are a specialized form of artificial neural network designed to recognize patterns in sequences of data, such as text, music, or stock prices. Unlike traditional neural networks, which process inputs independently, RNNs have loops in them, allowing information to persist. <a id="idIndexMarker011"/></p>
<p class="body">One of the challenges in generating text is how to predict the next word based on all previous words so that the prediction captures both the long-range dependencies and contextual meaning. RNNs take input not just as a standalone item but as a sequence (like words in a sentence, for example). At each time step, the prediction is based on not only the current input but also all previous inputs in the form of a summary through a hidden state. Let’s consider the phrase “a frog has four legs” as an example. In the first time step, we use the word “a” to predict the second word “frog.” In the second time step, we predict the next word using both “a” and “frog.” By the time we predict the last word, we need to use all four previous words “a frog has four.”</p>
<p class="body">A key feature of RNNs is the so-called hidden state, which captures information in all previous elements in a sequence. This feature is crucial for the network’s ability to process and generate sequential data effectively. The functioning of RNNs and this sequential processing is depicted in figure 8.1, which illustrates how a layer of recurrent neurons unfolds over time.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="215" src="../../OEBPS/Images/CH08_F01_Liu.png" width="578"/></p>
<p class="figurecaption">Figure 8.1 How a layer of recurrent neurons unfolds through time. When a recurrent neural network makes a prediction on sequential data, it takes the hidden state from the previous time<a id="idTextAnchor009"/> step, h(t – 1), along with the input at the current time step, x(t), and generates the output, y(t), and the updated hidden state, h(t). The hidden state at time step t captures the information in all previous time steps, x(0), x(1), …, x(t).</p>
</div>
<p class="body"><a id="marker-173"/>The hidden state in RNNs plays a pivotal role in capturing information across all time steps. This allows RNNs to make predictions that are informed not just by the current input, x(t), but also by the accumulated knowledge from all previous inputs, <span class="times">x(0), x(1), …, x(t – 1)</span>. This attribute makes RNNs capable of understanding temporal dependencies. They can grasp the context from an input sequence, which is indispensable for tasks like language modeling, where the preceding words in a sentence set the stage for predicting the next word.</p>
<p class="body">However, RNNs are not without their drawbacks. Though standard RNNs are capable of handling short-term dependencies, they struggle with longer-range dependencies within text. This difficulty stems from the vanishing gradient problem, which occurs in long sequences where the gradients (essential for training the network) diminish, hindering the model’s ability to learn relationships over longer distances. To mitigate this, advanced versions of RNNs, such as LSTM networks, have been developed.</p>
<p class="body">LSTM networks were introduced by Hochreiter and Schmidhuber in 1997.<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">1</a></sup> An LSTM network is composed of LSTM units (or cells), each of which has a more complex structure than a standard RNN neuron. The cell state is the key innovation of LSTMs: it acts as a kind of conveyor belt, running straight down the entire chain of LSTM units. It has the ability to carry relevant information through the network. The ability to add or remove information to the cell state allows LSTMs to capture long-term dependencies and remember information for long periods. This makes them more effective for tasks like language modeling and text generation. In this chapter, we will harness the LSTM model to undertake a project on text generation, aiming to mimic the style of the novel <i class="fm-italics">Anna Karenina</i>.</p>
<p class="body">However, it’s noteworthy that even advanced RNN variants like LSTMs encounter hurdles in capturing extremely long-range dependencies in sequence data. We will discuss these challenges and provide solutions in the next chapter, continuing our exploration of sophisticated models for effective sequence data processing and generation.<a id="idIndexMarker012"/></p>
<h3 class="fm-head1" id="heading_id_6">8.1.3 Steps in training a LSTM model</h3>
<p class="body">Next, we’ll discuss the steps involved in training an LSTM model to generate text. This overview aims to provide a foundational understanding of the training process before embarking on the project. <a id="idIndexMarker013"/><a id="idIndexMarker014"/><a id="marker-174"/></p>
<p class="body">The choice of text for training depends on the desired output. A lengthy novel serves as a good starting point. Its extensive content enables the model to learn and replicate a specific writing style effectively. An ample amount of text data enhances the model’s proficiency in this style. At the same time, novels are generally not excessively long, which helps in managing the training time. For our LSTM model training, we’ll utilize the text from <i class="fm-italics">Anna Karenina</i>, aligning with our previously outlined training data criteria.</p>
<p class="body">Similar to other deep neural networks, LSTM models cannot process raw text directly. Instead, we’ll convert the text into numerical form. This begins by breaking down the text into smaller pieces, a process known as tokenization, where each piece is a token. Tokens can be entire words, punctuation marks (like an exclamation mark or a comma), or special characters (such as &amp; or %). For this chapter, each of these elements will be treated as separate tokens. Although this method of tokenization may not be the most efficient, it is easy to implement since all we need is to map words to tokens. We will use subword tokenization in subsequent chapters where some infrequent words are broken into smaller pieces such as syllables. Following tokenization, we assign a unique integer to each token, creating a numerical representation of the text as a sequence of integers.</p>
<p class="body">To prepare the training data, we divide this long sequence into shorter sequences of equal length. For our project, we’ll use sequences comprising 100 integers each. These sequences form the features (the <i class="timesitalic">x</i> variable) of our model. We then generate the output <i class="timesitalic">y</i> by shifting the input sequence one token to the right. This setup enables the LSTM model to predict the next token in a sequence. The pairs of input and output serve as the training data. Our model includes LSTM layers to understand long-term patterns in the text and an embedding layer to grasp semantic meanings.</p>
<p class="body">Let’s revisit the example of predicting the sentence “a frog has four legs” that we mentioned earlier. Figure 8.2 is a diagram of how the training of the LSTM model works.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="233" src="../../OEBPS/Images/CH08_F02_Liu.png" width="616"/></p>
<p class="figurecaption">Figure 8.2 An example of how an LSTM model is trained. We first break down the training text into tokens and assign a unique integer to each token, creating a numerical representation of the text as a sequence of indexes. We then divide this long sequence into shorter sequences of equal length. These sequences form the features (the x variable) of our model. We then generate the output y by shifting the input sequence one token to the right. This setup enables the LSTM model to predict the next token based on previous tokens in the sequence.</p>
</div>
<p class="body">In the first time step, the model uses the word “a” to predict the word “frog.” Since there’s no preceding word for “a,” we initialize the hidden state with zeros. The LSTM model receives both the index for “a” and this initial hidden state as input and outputs the predicted next word along with an updated hidden state, h0. In the subsequent time step, the word “frog” and the updated state h0 are used to predict “has” and generate a new hidden state, h1. This sequence of predicting the next word and updating the hidden state continues until the model forecasts the final word in the sentence, “legs.”</p>
<p class="body">The predictions are then compared to the actual next word in the sentence. Since the model is effectively predicting the next token out of all possible tokens in the vocabulary, there is a multicategory classification problem. We tweak the model parameters in each iteration to minimize the cross-entropy loss so that in the next iteration, the model predictions move closer to actual outputs in the training data.</p>
<p class="body">Once the model is trained, generating text begins with a seed sequence input into the model. The model predicts the next token, which is then appended to your sequence. This iterative process of prediction and sequence updating is repeated to generate text for as long as desired.<a id="idIndexMarker015"/><a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="idIndexMarker018"/><a id="marker-175"/></p>
<h2 class="fm-head" id="heading_id_7">8.2 Fundamentals of NLP</h2>
<p class="body">Deep learning models, including the LSTM models we discussed earlier and Transformers, which you’ll learn in later chapters, cannot process raw text directly because they are designed to work with numerical data, typically in the form of vectors or matrices. The processing and learning capabilities of neural networks are based on mathematical operations like addition, multiplication, and activation functions, which require numerical input. Consequently, it’s essential first to break down text into smaller, more manageable elements known as tokens. These tokens can range from individual characters and words to subword units. <a id="idIndexMarker019"/><a id="idIndexMarker020"/><a id="idIndexMarker021"/></p>
<p class="body">The next crucial step in NLP tasks is transforming these tokens into numerical representations. This conversion is necessary for feeding them into deep neural networks, which is a fundamental part of training our models.</p>
<p class="body">In this section, we’ll discuss different tokenization methods, along with their advantages and drawbacks. Additionally, you’ll gain insights into the process of converting tokens into dense vector representations—a method known as word embedding. This technique is crucial for capturing the meaning of language in a format that deep learning models can effectively utilize.</p>
<h3 class="fm-head1" id="heading_id_8">8.2.1 Different tokenization methods</h3>
<p class="body">Tokenization involves dividing text into smaller parts, known as tokens, which can be in the form of words, characters, symbols, or other significant units. The primary goal of tokenization is to streamline the process of text data analysis and processing.<a id="idIndexMarker022"/><a id="idIndexMarker023"/><a id="marker-176"/></p>
<p class="body">Broadly speaking, there are three approaches to tokenization. The first is character tokenization, where the text is divided into its constituent characters. This method is used in languages with complex morphological structures, such as Turkish or Finnish, in which the meaning of words can change significantly with slight variations in characters. Take the English phrase “It is unbelievably good!” as an example; it’s broken down into individual characters as follows: <code class="fm-code-in-text">['I', 't', ' ', 'i', 's', ' ', 'u', 'n', 'b', 'e', 'l', 'i', 'e', 'v', 'a', 'b', 'l', 'y', ' ', 'g', 'o', 'o', 'd', '!']</code>. A key advantage of character tokenization is the limited number of unique tokens. This limitation significantly reduces the parameters in deep learning models, leading to faster and more efficient training. However, the major drawback is that individual characters often lack significant meaning, making it challenging for machine learning models to derive meaningful insights from a sequence of characters.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 8.1</p>
<p class="fm-sidebar-text">Use character tokenization to divide the phrase “Hi, there!” into individual tokens.</p>
</div>
<p class="body">The second approach is word tokenization, where the text is split into individual words and punctuation marks. It is used often in situations where the number of unique words is not too large. For instance, the same phrase “It is unbelievably good!” becomes five tokens: <code class="fm-code-in-text">['It', 'is', 'unbelievably', 'good', '!']</code>. The main advantage of this method is that each word inherently carries semantic meaning, making it more straightforward for models to interpret the text. The downside, however, lies in the substantial increase in unique tokens, which increases the number of parameters in deep learning models. This increase can lead to slower and less efficient training processes.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 8.2</p>
<p class="fm-sidebar-text">Use word tokenization to break down the phrase “Hi, how are you?” into individual tokens.</p>
</div>
<p class="body">The third approach is subword tokenization. This method, a key concept in NLP, breaks text into smaller, meaningful components called subwords. For instance, the phrase “It is unbelievably good!” would be divided into tokens like <code class="fm-code-in-text">['It', 'is', 'un', 'believ', 'ably', 'good', '!']</code>. Most advanced language models, including ChatGPT, use subword tokenization, and you’ll use this method in the next few chapters. Subword tokenization strikes a balance between the more traditional tokenization techniques that typically split text into either individual words or characters. Word-based tokenization, while capturing more meaning, leads to a vast vocabulary. Conversely, character-based tokenization results in a smaller vocabulary, but each token carries less semantic value.</p>
<p class="body">Subword tokenization effectively mitigates these problems by keeping frequently used words whole in the vocabulary while dividing less common or more complex words into subcomponents. This technique is particularly advantageous for languages with large vocabularies or those exhibiting a high degree of word form variation. By adopting subword tokenization, the overall vocabulary size is substantially reduced. This reduction enhances the efficiency and effectiveness of language processing tasks, especially when dealing with a wide range of linguistic structures.</p>
<p class="body">In this chapter, we will focus on word tokenization, as it offers a straightforward foundation for beginners. As we progress to later chapters, our attention will shift to subword tokenization, utilizing models that have already been trained with this technique. This approach allows us to concentrate on more advanced topics, such as understanding the Transformer architecture and exploring the inner workings of the attention mechanism.</p>
<h3 class="fm-head1" id="heading_id_9">8.2.2 Word embedding</h3>
<p class="body"><a id="marker-177"/>Word embedding is a method that transforms tokens into compact vector representations, capturing their semantic information and interrelationships. This technique is vital in NLP, especially since deep neural networks, including models like LSTM and Transformers, require numerical input.<a id="idIndexMarker024"/><a id="idIndexMarker025"/></p>
<p class="body">Traditionally, tokens are converted into numbers using one-hot encoding before being fed into NLP models. In one-hot encoding, each token is represented by a vector where only one element is ‘1’, and the rest are ‘0’s. For example, in this chapter, there are 12,778 unique word-based tokens in the text for the novel <i class="fm-italics">Anna Karenina</i>. Each token is represented by a vector of 12,778 dimensions. Consequently, a phrase like “happy families are all alike” is represented as a 5 <span class="cambria">×</span> 12,778 matrix, where 5 represents the number of tokens. This representation, however, is highly inefficient due to its large dimensionality, leading to an increased number of parameters, which can hinder training speed and efficiency.</p>
<p class="body">LSTMs, Transformers, and other advanced NLP models address this inefficiency through word embedding. Instead of bulky one-hot vectors, word embedding uses continuous, lower-dimensional vectors (e.g., 128-value vectors we use in this chapter). As a result, the phrase “happy families are all alike” is represented by a more compact 5 <span class="cambria">×</span> 128 matrix after word embedding. This streamlined representation drastically reduces the model’s complexity and enhances training efficiency.</p>
<p class="body">Word embedding not only reduces word complexity by condensing it into a lower-dimensional space but also effectively captures the context and the nuanced semantic relationships between words, a feature that simpler representations like one-hot encoding lack, for the following reasons. In one-hot encoding, all tokens have the same distance from each other in vector space. However, in word embeddings, tokens with similar meanings are represented by vectors close to each other in the embedding space. Word embeddings are learned from the text in the training data; the resulting vectors capture contextual information. Tokens that appear in similar contexts will have similar embeddings, even if they are not explicitly related.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Word embedding in NLP</p>
<p class="fm-sidebar-text">Word embeddings are a powerful method for representing tokens in NLP that offer significant advantages over traditional one-hot encoding in capturing context and semantic relationships between words.</p>
<p class="fm-sidebar-text">One-hot encoding represents tokens as sparse vectors with a dimension equal to the size of the vocabulary, where each token is represented by a vector with all zeros except for a single one at the index corresponding to the token. In contrast, word embeddings represent tokens as dense vectors with much lower dimensions (e.g., 128 dimensions in this chapter and 256 dimensions in chapter 12). This dense representation is more efficient and can capture more information.</p>
<p class="fm-sidebar-text">Specifically, in one-hot encoding, all tokens have the same distance from each other in the vector space, meaning there is no notion of similarity between tokens. However, in word embeddings, similar tokens are represented by vectors that are close to each other in the embedding space. For example, the words “king” and “queen” would have similar embeddings, reflecting their semantic relationship.</p>
<p class="fm-sidebar-text">Word embeddings are learned from the text in the training data. The embedding process uses the context in which tokens appear to learn their embeddings, meaning that the resulting vectors capture contextual information. Tokens that appear in similar contexts will have similar embeddings, even if they are not explicitly related.</p>
<p class="fm-sidebar-text">Overall, word embeddings provide a more nuanced and efficient representation of words that captures semantic relationships and contextual information, making them more suitable for NLP tasks compared to one-hot encoding.<a id="marker-178"/></p>
</div>
<p class="body">In practical terms, particularly in frameworks like PyTorch, word embedding is implemented by passing indexes through a linear layer, which compresses them into a lower-dimensional space. That is, when you pass an index to the <code class="fm-code-in-text">nn.Embedding()</code> layer, it looks up the corresponding row in the embedding matrix and returns the embedding vector for that index, avoiding the need to create potentially very large one-hot vectors. The weights of this embedding layer are not predefined but are learned during the training process. This learning aspect enables the model to refine its understanding of word semantics based on the training data, leading to a more nuanced and context-aware representation of language in the neural network. This approach significantly enhances the model’s ability to process and interpret language data efficiently and meaningfully.<a id="idIndexMarker026"/><a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="idIndexMarker029"/><a id="idIndexMarker030"/></p>
<h2 class="fm-head" id="heading_id_10">8.3 Preparing data to train the LSTM model</h2>
<p class="body">In this section, we’ll process text data and get it ready for training. We’ll first break text down into individual tokens. Our next step involves creating a dictionary that assigns each token an index, essentially mapping them to integers. After this setup, we will organize these tokens into batches of training data, which will be crucial for training an LSTM model in the subsequent section.<a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="marker-179"/></p>
<p class="body">We’ll walk through the tokenization process in a detailed, step-by-step manner, ensuring you gain a thorough understanding of how tokenization functions. We’ll use word tokenization, owing to its simplicity in dividing text into words, as opposed to the more complex subword tokenization that demands a nuanced grasp of linguistic structure. In later chapters, we’ll employ pretrained tokenizers for subword tokenization using more sophisticated methods. This will allow us to focus on advanced topics, such as the attention mechanism and the Transformer architecture, without getting bogged down in the initial stages of text processing.</p>
<h3 class="fm-head1" id="heading_id_11">8.3.1 Downloading and cleaning up the text</h3>
<p class="body">We’ll use the text from the novel <i class="fm-italics">Anna Karenina</i> to train our model. Go to <a class="url" href="https://mng.bz/znmX">https://mng.bz/znmX</a> to download the text file and save it as anna.txt in the folder /files/ on your computer. After that, open the file and delete everything after line 39888, which says, <code class="fm-code-in-text">"END OF THIS PROJECT GUTENBERG EBOOK ANNA KARENINA</code>.<code class="fm-code-in-text">"</code> Or you can simply download the file anna.txt from the book’s GitHub repository: <a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>. <a id="idIndexMarker034"/><a id="idIndexMarker035"/><a id="idIndexMarker036"/></p>
<p class="body">First, we load up the data and print out some passages to get a feeling about the dataset:</p>
<pre class="programlisting">with open("files/anna.txt","r") as f:
    text=f.read()    
words=text.split(" ")    
print(words[:20]) </pre>
<p class="body">The output is</p>
<pre class="programlisting">['Chapter', '1\n\n\nHappy', 'families', 'are', 'all', 'alike;', 'every',
 'unhappy', 'family', 'is', 'unhappy', 'in', 'its',
'own\nway.\n\nEverything', 'was', 'in', 'confusion', 'in', 'the',
"Oblonskys'"]</pre>
<p class="body">As you can see, line breaks (represented by \n) are considered part of the text. Therefore, we should replace these line breaks with spaces so they are not in the vocabulary. Additionally, converting all words to lowercase is helpful in our setting, as it ensures words like “The” and “the” are recognized as the same token. This step is vital for reducing the variety of unique tokens, thereby making the training process more efficient. Furthermore, punctuation marks should be spaced apart from the words they follow. Without this separation, combinations like “way.” and “way” would be erroneously treated as different tokens. To address these problems, we’ll clean up the text:</p>
<pre class="programlisting">clean_text=text.lower().replace("\n", " ")               <span class="fm-combinumeral">①</span>
clean_text=clean_text.replace("-", " ")                  <span class="fm-combinumeral">②</span>
for x in ",.:;?!$()/_&amp;%*@'`":
    clean_text=clean_text.replace(f"{x}", f" {x} ")
clean_text=clean_text.replace('"', ' " ')                <span class="fm-combinumeral">③</span>
text=clean_text.split()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Replaces line break with a space</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Replaces a hyphen with a space</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Adds a space around punctuation marks and special characters</p>
<p class="body">Next, we obtain unique tokens:</p>
<pre class="programlisting">from collections import Counter   
word_counts = Counter(text)    
words=sorted(word_counts, key=word_counts.get,
                      reverse=True)
print(words[:10])</pre>
<p class="body">The list <code class="fm-code-in-text">words</code> contains all the unique tokens in the text, with the most frequent one appearing first, and the least frequent one last. The output from the preceding code block is</p>
<pre class="programlisting">[',', '.', 'the', '"', 'and', 'to', 'of', 'he', "'", 'a']</pre>
<p class="body"><a id="marker-180"/>The preceding output shows the most frequent 10 tokens. The comma (<code class="fm-code-in-text">,</code>) and the period (<code class="fm-code-in-text">.</code>) are the most and the second most frequent tokens, respectively. The word “the” is the third most frequent token, and so on.</p>
<p class="body">We now create two dictionaries: one mapping tokens to indexes and the other mapping indexes to tokens.</p>
<p class="fm-code-listing-caption">Listing 8.1 Dictionaries to map tokens to indexes and indexes to tokens</p>
<pre class="programlisting">text_length=len(text)                                      <span class="fm-combinumeral">①</span>
num_unique_words=len(words)                                <span class="fm-combinumeral">②</span>
print(f"the text contains {text_length} words")
print(f"there are {num_unique_words} unique tokens")
word_to_int={v:k for k,v in enumerate(words)}              <span class="fm-combinumeral">③</span>
int_to_word={k:v for k,v in enumerate(words)}              <span class="fm-combinumeral">④</span>
print({k:v for k,v in word_to_int.items() if k in words[:10]})
print({k:v for k,v in int_to_word.items() if v in words[:10]})</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The length of text (how many tokens in the text)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The length of unique tokens</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Maps tokens to indexes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Maps indexes to tokens</p>
<p class="body">The output from the preceding code block is</p>
<pre class="programlisting">the text contains 437098 words
there are 12778 unique tokens
{',': 0, '.': 1, 'the': 2, '"': 3, 'and': 4, 'to': 5, 'of': 6, 'he': 7,
"'": 8, 'a': 9}
{0: ',', 1: '.', 2: 'the', 3: '"', 4: 'and', 5: 'to', 6: 'of', 7: 'he',
 8: "'", 9: 'a'}</pre>
<p class="body">The text for the novel <i class="fm-italics">Anna Karenina</i> has a total of 437,098 tokens. There are 12,778 unique tokens. The dictionary <code class="fm-code-in-text">word_to_int</code> assigns an index to each unique token. For example, the comma (<code class="fm-code-in-text">,</code>) is assigned an index of 0, and the period (<code class="fm-code-in-text">.</code>) is assigned an index of 1. The dictionary <code class="fm-code-in-text">int_to_word</code> translates an index back to a token. For example, index 2 is translated back to the token “<code class="fm-code-in-text">the</code>”. Index 4 is translated back to the token “<code class="fm-code-in-text">and</code>”, and so on. <a id="idIndexMarker037"/><a id="idIndexMarker038"/></p>
<p class="body">Finally, we convert the whole text to indexes:</p>
<pre class="programlisting">print(text[0:20])
wordidx=[word_to_int[w] for w in text]  
print([word_to_int[w] for w in text[0:20]])  </pre>
<p class="body">The output is</p>
<pre class="programlisting">['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every',
 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.',
 'everything', 'was']
[208, 670, 283, 3024, 82, 31, 2461, 35, 202, 690, 365, 38, 690, 10, 234,
 147, 166, 1, 149, 12]</pre>
<p class="body"><a id="marker-181"/>We convert all tokens in the text into the corresponding indexes and save them in a list <code class="fm-code-in-text">wordidx</code>. The preceding output shows the first 20 tokens in the text, as well as the corresponding indexes. For example, the first token in the text is <code class="fm-code-in-text">chapter</code>, with an index value of 208. <a id="idIndexMarker039"/><a id="idIndexMarker040"/><a id="idIndexMarker041"/><a id="idIndexMarker042"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 8.3</p>
<p class="fm-sidebar-text">Find out the index value of the token <code class="fm-code-in-text1">anna</code> in the dictionary <code class="fm-code-in-text1">word_to_int</code>.<a id="idIndexMarker043"/><a id="idIndexMarker044"/></p>
</div>
<h3 class="fm-head1" id="heading_id_12">8.3.2 Creating batches of training data</h3>
<p class="body">Next, we create pairs of <span class="times">(x, y)</span> for training purposes. Each x is a sequence with 100 indexes. There is nothing magical about the number 100, and you can easily change it to 90 or 110 and have similar results. Setting the number too large may slow down training, while setting the number too small may lead to the model’s failure to capture long-range dependencies. We then slide the window right by one token and use it as the target y. Shifting the sequence by one token to the right and using it as the output during sequence generation is a common technique in training language models, including Transformers. The code block in the following listing creates the training data. <a id="idIndexMarker045"/><a id="idIndexMarker046"/><a id="idIndexMarker047"/></p>
<p class="fm-code-listing-caption">Listing 8.2 Creating training data</p>
<pre class="programlisting">import torch
seq_len=100                                             <span class="fm-combinumeral">①</span>
xys=[]
for n in range(0, len(wordidx)-seq_len-1):              <span class="fm-combinumeral">②</span>
    x = wordidx[n:n+seq_len]                            <span class="fm-combinumeral">③</span>
    y = wordidx[n+1:n+seq_len+1]                        <span class="fm-combinumeral">④</span>
    xys.append((torch.tensor(x),(torch.tensor(y))))</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Each input contains 100 indexes.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Starting from the first token in text, slides to the right one at a time</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Defines the input x</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Shifts the input x to the right by one token and uses it as the output y</p>
<p class="body">By shifting the sequence one token to the right and using it as output, the model is trained to predict the next token given the previous tokens. For instance, if the input sequence is <code class="fm-code-in-text">"how are you"</code>, then the shifted sequence would be <code class="fm-code-in-text">"are you today"</code>. During training, the model learns to predict <code class="fm-code-in-text">'are'</code> after seeing <code class="fm-code-in-text">'how'</code>, <code class="fm-code-in-text">'you'</code> after seeing <code class="fm-code-in-text">'are'</code>, and so on. This helps the model learn the probability distribution of the next token in a sequence. You’ll see this practice again and again later in this book.</p>
<p class="body">We’ll create batches of data for training, with 32 pairs of <span class="times">(x, y)</span> in each batch:</p>
<pre class="programlisting">from torch.utils.data import DataLoader
  
torch.manual_seed(42)
batch_size=32
loader = DataLoader(xys, batch_size=batch_size, shuffle=True)</pre>
<p class="body">We now have the training dataset. Next, we’ll create an LSTM model and train it using the data we just processed. <a id="idIndexMarker048"/><a id="idIndexMarker049"/><a id="marker-182"/><a id="idIndexMarker050"/></p>
<h2 class="fm-head" id="heading_id_13">8.4 Building and training the LSTM model</h2>
<p class="body">In this section, you’ll begin by constructing an LSTM model using PyTorch’s built-in LSTM layer. This model will start with a word embedding layer, which transforms each index into a dense vector of 128 dimensions. Your training data will pass through this embedding layer before being fed into the LSTM layer. This LSTM layer is designed to process elements of a sequence in a sequential manner. Following the LSTM layer, the data will proceed to a linear layer, which has an output size matching the size of your vocabulary. The outputs generated by the LSTM model are essentially logits, serving as inputs for the softmax function to compute probabilities.<a id="idIndexMarker051"/><a id="idIndexMarker052"/></p>
<p class="body">Once you have built the LSTM model, the next step will involve using your training data to train this model. This training phase is crucial to refine the model’s ability to understand and generate patterns consistent with the data it has been fed.</p>
<h3 class="fm-head1" id="heading_id_14">8.4.1 Building an LSTM model</h3>
<p class="body">In listing 8.3, we define a <code class="fm-code-in-text">WordLSTM()</code> class to serve as our LSTM model to be trained to generate text in the style of <i class="fm-italics">Anna Karenina</i>. The class is defined as shown in the following listing.<a id="idIndexMarker053"/><a id="idIndexMarker054"/><a id="idIndexMarker055"/></p>
<p class="fm-code-listing-caption">Listing 8.3 Defining the <code class="fm-code-in-text">WordLSTM()</code> class</p>
<pre class="programlisting">from torch import nn
device="cuda" if torch.cuda.is_available() else "cpu"
class WordLSTM(nn.Module):
    def __init__(self, input_size=128, n_embed=128,
             n_layers=3, drop_prob=0.2):
        super().__init__()
        self.input_size=input_size
        self.drop_prob = drop_prob
        self.n_layers = n_layers
        self.n_embed = n_embed
        vocab_size=len(word_to_int)
        self.embedding=nn.Embedding(vocab_size,n_embed)    <span class="fm-combinumeral">①</span>
        self.lstm = nn.LSTM(input_size=self.input_size,
            hidden_size=self.n_embed,
            num_layers=self.n_layers,
            dropout=self.drop_prob,batch_first=True)       <span class="fm-combinumeral">②</span>
        self.fc = nn.Linear(input_size, vocab_size)    
  
    def forward(self, x, hc):
        embed=self.embedding(x)
        x, hc = self.lstm(embed, hc)                       <span class="fm-combinumeral">③</span>
        x = self.fc(x)
        return x, hc      
        
    def init_hidden(self, n_seqs):                         <span class="fm-combinumeral">④</span>
        weight = next(self.parameters()).data
        return (weight.new(self.n_layers,
                           n_seqs, self.n_embed).zero_(),
                weight.new(self.n_layers,
                           n_seqs, self.n_embed).zero_())  </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Training data first goes through an embedding layer.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates an LSTM layer with the PyTorch LSTM() class</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> In each time step, the LSTM layer uses the previous token and the hidden state to predict the next token and the next hidden state.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Initiates the hidden state for the first token in the input sequence</p>
<p class="body"><a id="marker-183"/>The <code class="fm-code-in-text">WordLSTM()</code> class defined previously has three layers: the word embedding layer, the LSTM layer, and a final linear layer. We set the value of the argument <code class="fm-code-in-text">n_layers</code> to 3, which means the LSTM layer stacks three LSTMs together to form a stacked LSTM, with the last two LSTMs taking the output from the previous LSTM as input. The <code class="fm-code-in-text">init_hidden()</code> method fills the hidden state with zeros when the model uses the first element in the sequence to make predictions. In each time step, the input is the current token and the previous hidden state while the output is the next token and the next hidden state. <a id="idIndexMarker056"/><a id="idIndexMarker057"/><a id="idIndexMarker058"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">How the <code class="fm-code-in-text2">torch.nn.Embedding()</code> class works</p>
<p class="fm-sidebar-text">The <code class="fm-code-in-text1">torch.nn.Embedding()</code> class in PyTorch is used to create an embedding layer in a neural network. An embedding layer is a trainable lookup table that maps integer indexes to dense, continuous vector representations (embeddings). <a id="idIndexMarker059"/></p>
<p class="fm-sidebar-text">When you create an instance of <code class="fm-code-in-text1">torch.nn.Embedding()</code>, you need to specify two main parameters: num_embeddings, the size of the vocabulary (total number of unique tokens), and embedding_dim, the size of each embedding vector (the dimensionality of the output embeddings).</p>
<p class="fm-sidebar-text">Internally, the class creates a matrix (or lookup table) of shape (num_embeddings, embedding_dim) where each row corresponds to the embedding vector for a particular index. Initially, these embeddings are randomly initialized but are learned and updated during training through backpropagation.</p>
<p class="fm-sidebar-text">When you pass a tensor of indexes to the embedding layer (during the forward pass of the network), it looks up the corresponding embedding vectors in the lookup table and returns them. More information about the class is provided by PyTorch at <a class="url" href="https://mng.bz/n0Zd">https://mng.bz/n0Zd</a>.</p>
</div>
<p class="body">We create an instance of the <code class="fm-code-in-text">WordLSTM()</code> class and use it as our LSTM model, as follows:<a id="idIndexMarker060"/></p>
<pre class="programlisting">model=WordLSTM().to(device)</pre>
<p class="body">When the LSTM model is created, the weights are randomly initialized. When we use pairs of <span class="times">(x, y)</span> to train the model, LSTM learns to predict the next token based on all previous tokens in the sequence by adjusting the model parameters. As we have illustrated in figure 8.2, LSTM learns to predict the next token and the next hidden state based on the current token and the current hidden state, which is a summary of the information in all previous tokens.</p>
<p class="body">We use the Adam optimizer with a learning rate of 0.0001. The loss function is the cross-entropy loss since this is essentially a multicategory classification problem: the model is trying to predict the next token from a dictionary with 12,778 choices:</p>
<pre class="programlisting">lr=0.0001
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
loss_func = nn.CrossEntropyLoss()</pre>
<p class="body">Now that the LSTM model is built, we’ll train the model with the batches of training data we prepared before. <a id="idIndexMarker061"/><a id="idIndexMarker062"/><a id="marker-184"/></p>
<h3 class="fm-head1" id="heading_id_15">8.4.2 Training the LSTM model</h3>
<p class="body">During each training epoch, we go through all data batches of data <span class="times">(x, y)</span> in the training set. The LSTM model receives the input sequence, x, and generates a predicted output sequence, <i class="timestalic">ŷ</i>. This prediction is compared with the actual output sequence, y, to compute the cross-entropy loss since we essentially conduct a multicategory classification here. We then tweak the model’s parameters to reduce this loss, as we did in chapter 2 when classifying clothing items.<a id="idIndexMarker063"/><a id="idIndexMarker064"/></p>
<p class="body">Though we could divide our data into training and validation sets, training the model until no further improvements are seen on the validation set (as we have done in chapter 2), our primary aim here is to grasp how LSTM models function, not necessarily to achieve the best parameter tuning. Therefore, we’ll train the model for 50 epochs.</p>
<p class="fm-code-listing-caption">Listing 8.4 Training the LSTM model to generate text</p>
<pre class="programlisting">model.train()
  
for epoch in range(50):
    tloss=0
    sh,sc = model.init_hidden(batch_size)
    for i, (x,y) in enumerate(loader):                      <span class="fm-combinumeral">①</span>
        if x.shape[0]==batch_size:
            inputs, targets = x.to(device), y.to(device)
            optimizer.zero_grad()
            output, (sh,sc) = model(inputs, (sh,sc))        <span class="fm-combinumeral">②</span>
            loss = loss_func(output.transpose(1,2),targets) <span class="fm-combinumeral">③</span> 
            sh,sc=sh.detach(),sc.detach()
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 5)
            optimizer.step()                                <span class="fm-combinumeral">④</span>
            tloss+=loss.item()
        if (i+1)%1000==0:
            print(f"at epoch {epoch} iteration {i+1}\
            average loss = {tloss/(i+1)}")</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Iterates through all batches of (x,y) in the training data</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Uses the model to predict the output sequence</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Compares the predictions with the actual output and calculates the loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Tweaks model parameters to minimize loss</p>
<p class="body">In the preceding code listing, <code class="fm-code-in-text">sh</code> and <code class="fm-code-in-text">sc</code> together form the hidden state. In particular, the cell state <code class="fm-code-in-text">sc</code> acts as a conveyor belt, carrying information over many time steps, with information added or removed in each time step. The component <code class="fm-code-in-text">sh</code> is the output of the LSTM cell at a given time step. It contains information about the current input and is used to pass information to the next LSTM cell in the sequence. <a id="idIndexMarker065"/><a id="idIndexMarker066"/></p>
<p class="body">If you have a CUDA-enabled GPU, this training takes about 6 hours. If you use CPU only, it may take a day or two, depending on your hardware. Or you can download the pretrained weights from my website: <a class="url" href="https://mng.bz/vJZa">https://mng.bz/vJZa</a>.</p>
<p class="body">Next, we save the trained model weights in the local folder:</p>
<pre class="programlisting">import pickle
  
torch.save(model.state_dict(),"files/wordLSTM.pth")
with open("files/word_to_int.p","wb") as fb:
    pickle.dump(word_to_int, fb)</pre>
<p class="body">The dictionary <code class="fm-code-in-text">word_to_int</code> is also saved on your computer, which is a practical step ensuring that you can generate text using the trained model without needing to repeat the tokenization process.<a id="idIndexMarker067"/><a id="idIndexMarker068"/><a id="idIndexMarker069"/><a id="marker-185"/></p>
<h2 class="fm-head" id="heading_id_16">8.5 Generating text with the trained LSTM model</h2>
<p class="body">Now that you have a trained LSTM model, you’ll learn how to use it to generate text in this section. The goal is to see if the trained model can generate grammatically correct and coherent text by iteratively predicting the next token based on previous tokens. You’ll also learn to use temperature and top-K sampling to control the creativeness of the generated text.<a id="idIndexMarker070"/><a id="idIndexMarker071"/><a id="idIndexMarker072"/></p>
<p class="body">When generating text with the trained LSTM model, we start with a prompt as the initial input to the model. We use the trained model to predict the most likely next token. After appending the next token to the prompt, we feed the new sequence to the trained model to predict the next token again. We repeat this process until the sequence reaches a certain length.</p>
<h3 class="fm-head1" id="heading_id_17">8.5.1 Generating text by predicting the next token</h3>
<p class="body">First, we load the trained model weights and the dictionary <code class="fm-code-in-text">word_to_int</code> from the local folder:<a id="idIndexMarker073"/><a id="idIndexMarker074"/><a id="marker-186"/></p>
<pre class="programlisting">model.load_state_dict(torch.load("files/wordLSTM.pth",
                                    map_location=device))
with open("files/word_to_int.p","rb") as fb:
    word_to_int = pickle.load(fb)
int_to_word={v:k for k,v in word_to_int.items()}</pre>
<p class="body">The file <code class="fm-code-in-text">word_to_int.p</code> is also available in the book’s GitHub repository. We switch the positions of keys and values in the dictionary <code class="fm-code-in-text">word_to_int</code> to create the dictionary <code class="fm-code-in-text">int_to_word</code>. <a id="idIndexMarker075"/><a id="idIndexMarker076"/></p>
<p class="body">To generate text with the trained LSTM model, we need a prompt as the starting point of the generated text. We’ll set the default prompt to “Anna and the.” An easy way to determine when to stop is to limit the generated text to a certain length, say 200 tokens: once the desired length is reached, we ask the model to stop generating.</p>
<p class="body">The following listing defines a <code class="fm-code-in-text">sample()</code> function to generate text based on a prompt.<a id="idIndexMarker077"/></p>
<p class="fm-code-listing-caption">Listing 8.5 A <code class="fm-code-in-text">sample()</code> function to generate text</p>
<pre class="programlisting">import numpy as np
def sample(model, prompt, length=200):
    model.eval()
    text = prompt.lower().split(' ')
    hc = model.init_hidden(1)
    length = length - len(text)                              <span class="fm-combinumeral">①</span>
    for i in range(0, length):
        if len(text)&lt;= seq_len:
            x = torch.tensor([[word_to_int[w] for w in text]])
        else:
            x = torch.tensor([[word_to_int[w] for w \
in text[-seq_len:]]])                                        <span class="fm-combinumeral">②</span>
        inputs = x.to(device)
        output, hc = model(inputs, hc)                       <span class="fm-combinumeral">③</span>
        logits = output[0][-1]
        p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()
        idx = np.random.choice(len(logits), p=p)             <span class="fm-combinumeral">④</span>
        text.append(int_to_word[idx])                        <span class="fm-combinumeral">⑤</span>
    text=" ".join(text)
    for m in ",.:;?!$()/_&amp;%*@'`":
        text=text.replace(f" {m}", f"{m} ")
    text=text.replace('"  ', '"')
    text=text.replace("'  ", "'")
    text=text.replace('" ', '"')
    text=text.replace("' ", "'")
    return text  </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Determines how many tokens need to be generated</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The input is the current sequence; trims it if it’s longer than 100 tokens</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Makes a prediction using the trained model</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Selects the next token based on predicted probabilities</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Appends the predicted next token to the sequence and repeats</p>
<p class="body">The function <code class="fm-code-in-text">sample()</code> takes three arguments. The first is the trained LSTM model you will be using. The second is the starting prompt for text generation, which can be a phrase of any length, in quotes. The third parameter specifies the length of the text to be generated, measured in tokens, with a default value of 200 tokens.<a id="idIndexMarker078"/></p>
<p class="body">Within the function, we first deduct the number of tokens in the prompt from the total desired length to determine the number of tokens that need to be generated. When generating the next token, we consider the current sequence’s length. If it’s under 100 tokens, we input the entire sequence into the model; if it’s over 100 tokens, only the last 100 tokens of the sequence are used as input. This input is then fed into the trained LSTM model to predict the subsequent token, which we then add to the current sequence. We continue this process until the sequence reaches the desired length.</p>
<p class="body"><a id="marker-187"/>When generating the next token, the model employs the random.choice(len(logits), p = p) method from NumPy. Here, the method’s first parameter indicates the range of choices, which in this case is len(logits) = 12778. This signifies that the model will randomly select an integer from 0 to 12,777, with each integer corresponding to a different token in the vocabulary. The second parameter, p, is an array containing 12,778 elements where each element denotes the probability of selecting a corresponding token from the vocabulary. Tokens with a higher probability in this array are more likely to be chosen.</p>
<p class="body">Let’s generate a passage with the model using “Anna and the prince” as the prompt (make sure you put a space before punctuation marks when you use your own prompt):</p>
<pre class="programlisting">torch.manual_seed(42)
np.random.seed(42)
print(sample(model, prompt='Anna and the prince'))  </pre>
<p class="body">Here, I fixed the random seed number to 42 in both PyTorch and NumPy in case you want to reproduce results. The generated passage is</p>
<pre class="programlisting">anna and the prince did not forget what he had not spoken. when the softening barrier was not so long as he had talked to his brother,  all the hopelessness of the impression. "official tail,  a man who had tried him,  though he had been able to get across his charge and locked close,  and the light round the snow was in the light of the altar villa. the article in law levin was first more precious than it was to him so that if it was most easy as it would be as the same. this was now perfectly interested. when he had got up close out into the sledge,  but it was locked in the light window with their one grass,  and in the band of the leaves of his projects,  and all the same stupid woman,  and really,  and i swung his arms round that thinking of bed. a little box with the two boys were with the point of a gleam of filling the boy,  noiselessly signed the bottom of his mouth,  and answering them took the red</pre>
<p class="body">You may have noticed that the text generated is entirely in lowercase. This is because, during the text processing stage, we converted all uppercase letters to lowercase to minimize the number of unique tokens.</p>
<p class="body">The text generated from 6 hours of training is quite impressive! Most of the sentences adhere to grammatical norms. While it may not match the level of sophistication seen in text generated by advanced systems like ChatGPT, it’s a significant achievement. With skills acquired in this exercise, you are ready to train more advanced text generation models in later chapters. <a id="idIndexMarker079"/><a id="idIndexMarker080"/></p>
<h3 class="fm-head1" id="heading_id_18">8.5.2 Temperature and top-K sampling in text generation</h3>
<p class="body">The creativity of the generated text can be controlled by using techniques like temperature and top-K sampling.<a id="idIndexMarker081"/><a id="idIndexMarker082"/><a id="idIndexMarker083"/><a id="idIndexMarker084"/></p>
<p class="body">Temperature adjusts the distribution of probabilities assigned to each potential token before selecting the next one. It effectively scales the logits, which are the inputs to the softmax function calculating these probabilities, by the value of the temperature. Logits are the outputs of the LSTM model prior to the application of the softmax function.</p>
<p class="body">In the <code class="fm-code-in-text">sample()</code> function we just defined, we didn’t adjust the logits, implying a default temperature of 1. A lower temperature (below 1; e.g., 0.8) results in fewer variations, making the model more deterministic and conservative, favoring more likely choices. Conversely, a higher temperature (above 1; e.g., 1.5) makes it more likely to choose improbable words in text generation, leading to more varied and inventive outputs. However, this could also make the text less coherent or relevant, as the model might opt for less probable words.<a id="marker-188"/><a id="idIndexMarker085"/></p>
<p class="body">Top-K sampling is another method to influence the output. This approach involves selecting the next word from the top K most probable options as predicted by the model. The probability distribution is truncated to include only the top K words. With a small K value, such as 5, the model’s choices are limited to a few highly probable words, resulting in more predictable and coherent but potentially less diverse and interesting outputs. In the <code class="fm-code-in-text">sample()</code> function we defined earlier, we did not apply top-K sampling, so the value of K was effectively the size of the vocabulary (12,778 in our case).<a id="idIndexMarker086"/></p>
<p class="body">Next, we introduce a new function, <code class="fm-code-in-text">generate()</code>, for text generation. This function is similar to the <code class="fm-code-in-text">sample()</code> function but includes two additional parameters: <code class="fm-code-in-text">temperature</code> and <code class="fm-code-in-text">top_k</code>, allowing for more control over the creativity and randomness of the generated text. The function <code class="fm-code-in-text">generate()</code> is defined in the following listing.<a id="idIndexMarker087"/><a id="idIndexMarker088"/><a id="idIndexMarker089"/></p>
<p class="fm-code-listing-caption">Listing 8.6 Generating text with temperature and top-K sampling</p>
<pre class="programlisting">def generate(model, prompt , top_k=None, 
             length=200, temperature=1):
    model.eval()
    text = prompt.lower().split(' ')
    hc = model.init_hidden(1)
    length = length - len(text)    
    for i in range(0, length):
        if len(text)&lt;= seq_len:
            x = torch.tensor([[word_to_int[w] for w in text]])
        else:
            x = torch.tensor([[word_to_int[w] for w in text[-seq_len:]]])
        inputs = x.to(device)
        output, hc = model(inputs, hc)
        logits = output[0][-1]
        logits = logits/temperature                             <span class="fm-combinumeral">①</span>
        p = nn.functional.softmax(logits, dim=0).detach().cpu()    
        if top_k is None:
            idx = np.random.choice(len(logits), p=p.numpy())
        else:
            ps, tops = p.topk(top_k)                            <span class="fm-combinumeral">②</span>
            ps=ps/ps.sum()
            idx = np.random.choice(tops, p=ps.numpy())          <span class="fm-combinumeral">③</span>
        text.append(int_to_word[idx])
     
    text=" ".join(text)
    for m in ",.:;?!$()/_&amp;%*@'`":
        text=text.replace(f" {m}", f"{m} ")
    text=text.replace('"  ', '"')   
    text=text.replace("'  ", "'")  
    text=text.replace('" ', '"')   
    text=text.replace("' ", "'")     
    return text  </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Scales the logits with temperature</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Keeps only the K most probable candidates</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Selects the next token from the top K candidates</p>
<p class="body">Compared to the <code class="fm-code-in-text">sample()</code> function, the new function <code class="fm-code-in-text">generate()</code> has two more optional arguments: <code class="fm-code-in-text">top_k</code> and <code class="fm-code-in-text">temperature</code>. By default, <code class="fm-code-in-text">top_k</code> is set to <code class="fm-code-in-text">None,</code> and <code class="fm-code-in-text">temperature</code> is set to 1. Therefore, if you call the <code class="fm-code-in-text">generate()</code> function without specifying these two arguments, the output will be the same as what you would get from the function <code class="fm-code-in-text">sample()</code>.<a id="idIndexMarker090"/><a id="idIndexMarker091"/><a id="marker-189"/><a id="idIndexMarker092"/></p>
<p class="body">Let’s illustrate the variations in generated text by focusing on the creation of a single token. For this purpose, we’ll use “I ’ m not going to see” as the prompt (note the space before the apostrophe, as we previously have done in the chapter). We call the <code class="fm-code-in-text">generate()</code> function 10 times, setting its length argument to be one more than the prompt’s length. This approach ensures that the function appends only one extra token to the prompt:<a id="idIndexMarker093"/></p>
<pre class="programlisting">prompt="I ' m not going to see"
torch.manual_seed(42)
np.random.seed(42)
for _ in range(10):
    print(generate(model, prompt, top_k=None, 
         length=len(prompt.split(" "))+1, temperature=1))</pre>
<p class="body">The output is</p>
<pre class="programlisting">i'm not going to see you
i'm not going to see those
i'm not going to see me
i'm not going to see you
i'm not going to see her
i'm not going to see her
i'm not going to see the
i'm not going to see my
i'm not going to see you
i'm not going to see me</pre>
<p class="body">With the default setting of top_k = None and temperature = 1, there is some degree of repetition in the output. For example, the word “you” was repeated three times. There are a total of six unique tokens.</p>
<p class="body">However, the functionality of <code class="fm-code-in-text">generate()</code> expands when you adjust these two arguments. For instance, setting a low temperature, like 0.5, and a small <code class="fm-code-in-text">top_k</code> value, such as 3, results in generated text that is more predictable and less creative.<a id="marker-190"/><a id="idIndexMarker094"/></p>
<p class="body">Let’s repeat the single token example. This time, we set the temperature to 0.5 and <code class="fm-code-in-text">top_k</code> value to 3:</p>
<pre class="programlisting">prompt="I ' m not going to see"
torch.manual_seed(42)
np.random.seed(42)
for _ in range(10):
    print(generate(model, prompt, top_k=3, 
         length=len(prompt.split(" "))+1, temperature=0.5))</pre>
<p class="body">The output is</p>
<pre class="programlisting">i'm not going to see you
i'm not going to see the
i'm not going to see her
i'm not going to see you
i'm not going to see you
i'm not going to see you
i'm not going to see you
i'm not going to see her
i'm not going to see you
i'm not going to see her</pre>
<p class="body">The output has fewer variations: there are only 3 unique tokens from 10 attempts, “you,” “the,” and “her.”</p>
<p class="body">Let’s see this in action by using “Anna and the prince” as our starting prompt when we set the temperature to 0.5 and <code class="fm-code-in-text">top_k</code> value to 3:</p>
<pre class="programlisting">torch.manual_seed(42)
np.random.seed(42)
print(generate(model, prompt='Anna and the prince',
               top_k=3,
               temperature=0.5))</pre>
<p class="body">The output is</p>
<pre class="programlisting">anna and the prince had no milk. but,  "answered levin,  and he stopped. "i've been skating to look at you all the harrows,  and i'm glad. . .  ""no,  i'm going to the country. ""no,  it's not a nice fellow. ""yes,  sir. ""well,  what do you think about it? ""why,  what's the matter? ""yes,  yes,  "answered levin,  smiling,  and he went into the hall. "yes,  i'll come for him and go away,  "he said,  looking at the crumpled front of his shirt. "i have not come to see him,  "she said,  and she went out. "i'm very glad,  "she said,  with a slight bow to the ambassador's hand. "i'll go to the door. "she looked at her watch,  and she did not know what to say </pre>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 8.4</p>
<p class="fm-sidebar-text">Generate text by setting temperature to 0.6 and <code class="fm-code-in-text1">top_k</code> to 10 and using “Anna and the nurse” as the starting prompt. Set the random seed number to 0 in both PyTorch and NumPy.</p>
</div>
<p class="body"><a id="marker-191"/>Conversely, opting for a higher <code class="fm-code-in-text">temperature</code> value, like 1.5, coupled with a higher <code class="fm-code-in-text">top_k</code> value, for instance, <code class="fm-code-in-text">None</code> (enabling selection from the entire pool of 12,778 tokens), leads to outputs that are more creative and less predictable. This is demonstrated next, in the single token example. This time, we set the temperature to 2 and <code class="fm-code-in-text">top_k</code> value to <code class="fm-code-in-text">None</code>:</p>
<pre class="programlisting">prompt="I ' m not going to see"
torch.manual_seed(42)
np.random.seed(42)
for _ in range(10):
    print(generate(model, prompt, top_k=None, 
         length=len(prompt.split(" "))+1, temperature=2))</pre>
<p class="body">The output is</p>
<pre class="programlisting">i'm not going to see them
i'm not going to see scarlatina
i'm not going to see behind
i'm not going to see us
i'm not going to see it
i'm not going to see it
i'm not going to see a
i'm not going to see misery
i'm not going to see another
i'm not going to see seryozha</pre>
<p class="body">The output has almost no repetition: there are 9 unique tokens from 10 attempts; only the word “it” was repeated.</p>
<p class="body">Let’s again use “Anna and the prince” as the initial prompt but set the temperature to 2 and top_k value to None and see what happens:</p>
<pre class="programlisting">torch.manual_seed(42)
np.random.seed(42)
print(generate(model, prompt='Anna and the prince',
               top_k=None,
               temperature=2))</pre>
<p class="body">The generated text is</p>
<pre class="programlisting">anna and the prince took sheaves covered suddenly people. "pyotr marya borissovna,  propped mihail though her son will seen how much evening her husband;  if tomorrow she liked great time too. "adopted heavens details for it women from this terrible,  admitting this touching all everything ill with flirtation shame consolation altogether:  ivan only all the circle with her honorable carriage in its house dress,  beethoven ashamed had the conversations raised mihailov stay of close i taste work? "on new farming show ivan nothing. hat yesterday if interested understand every hundred of two with six thousand roubles according to women living over a thousand:  snetkov possibly try disagreeable schools with stake old glory mysterious one have people some moral conclusion,  got down and then their wreath. darya alexandrovna thought inwardly peaceful with varenka out of the listen from and understand presented she was impossible anguish. simply satisfied with staying after presence came where he pushed up his hand as marya her pretty hands into their quarters. waltz was about the rider gathered;  sviazhsky further alone have an hand paused riding towards an exquisite</pre>
<p class="body">The output generated is not repetitive, although it lacks coherence in many places.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 8.5</p>
<p class="fm-sidebar-text">Generate text by setting temperature to 2 and <code class="fm-code-in-text1">top_k</code> to 10000 and using “Anna and the nurse” as the starting prompt. Set the random seed number to 0 in both PyTorch and NumPy.</p>
</div>
<p class="body"><a id="marker-192"/>In this chapter, you have acquired foundational skills in NLP, including word-level tokenization, word embedding, and sequence prediction. Through these exercises, you’ve learned to construct a language model based on word-level tokenization and have trained it using LSTM for text generation. Moving forward, the next few chapters will introduce you to training Transformers, the type of models used in systems like ChatGPT. This will provide you with a more in-depth understanding of advanced text generation techniques.<a id="idIndexMarker095"/><a id="idIndexMarker096"/><a id="idIndexMarker097"/><a id="idIndexMarker098"/><a id="idIndexMarker099"/><a id="idIndexMarker100"/><a id="idIndexMarker101"/></p>
<h2 class="fm-head" id="heading_id_19">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">RNNs are a specialized form of artificial neural network designed to recognize patterns in sequences of data, such as text, music, or stock prices. Unlike traditional neural networks, which process inputs independently, RNNs have loops in them, allowing information to persist. LSTM networks are improved versions of RNNs.</p>
</li>
<li class="fm-list-bullet">
<p class="list">There are three approaches to tokenization. The first is character tokenization, where the text is divided into its constituent characters. The second approach is word tokenization, where the text is split into individual words. The third approach is subword tokenization, which breaks words into smaller, meaningful components called subwords.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Word embedding is a method that transforms words into compact vector representations, capturing their semantic information and interrelationships. This technique is vital in NLP, especially since deep neural networks, including models like LSTM and Transformers, require numerical input.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Temperature is a parameter that influences the behavior of text generation models. It controls the randomness of the predictions by scaling the logits (the inputs to the softmax function for probability calculation) before applying softmax. Low temperature makes the model more conservative in its predictions but also more repetitive. At higher temperatures, the model becomes less repetitive and more innovative, increasing the diversity of the generated text.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Top-K sampling is another way to influence the behavior of text generation models. It involves selecting the next word from the K most likely candidates, as determined by the model. The probability distribution is truncated to keep only the top K words. Small values of K make the output more predictable and coherent but potentially less diverse and interesting.<a id="marker-193"/></p>
</li>
</ul>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">1</a></sup>  Sepp Hochreiter and Jurgen Schmidhuber, 1997, “Long Short-Term Memory,” <i class="fm-italics">Neural Computation</i> 9(8): 1735-1780.</p>
</div></body></html>