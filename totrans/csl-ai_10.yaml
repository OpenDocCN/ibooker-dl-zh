- en: 8 Counterfactuals and parallel worlds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Motivating examples for counterfactual reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turning counterfactual questions into symbolic form
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building parallel world graphs for counterfactual reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the counterfactual inference algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building counterfactual deep generative models of images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marjani, a good friend of mine, once had to choose between two dating prospects
    at the same time. She had something of a mental score card for an ideal long-term
    match. She had good chemistry with one guy, but he didn’t rank well on the score
    card. In contrast, the second guy checked all the boxes, so she chose him. But
    after some time, despite him meeting all her criteria, she couldn’t muster any
    feelings for him. It was like a failed ritual summoning; the stars were perfectly
    aligned, but the summoned spirit never showed up. And so, as any of us would in
    that situation, she posed the *counterfactual question*:'
  prefs: []
  type: TYPE_NORMAL
- en: I chose a partner based on my criteria and it’s not working out. Would it have
    worked out if I chose based on chemistry?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Counterfactual queries like this describe hypothetical events that did not occur
    but could have occurred if something had been different. Counterfactuals are fundamental
    to how we define causality; if the answer to Marjani’s question is yes, it implies
    that choosing based on her score card *caused* her love life to be unsuccessful.
  prefs: []
  type: TYPE_NORMAL
- en: Counterfactuals are core to the bread-and-butter question of causal effect inference,
    where we compare observed outcomes to “potential outcomes” that didn’t happen,
    like the outcome of Marjani’s love life if she chose a partner based on chemistry.
    More broadly, answering counterfactual questions is useful in learning policies
    for better decision-making. When some action leads to some outcome, and you ask
    how a different action might have led to a different outcome, a good answer can
    help you select better actions in the future. For example, after this experience,
    Marjani revised her score card to factor in chemistry when considering later romantic
    prospects.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at practical examples in this chapter, but I led with this love and
    romance example because it is universally relatable. It illustrates how fundamental
    counterfactual reasoning is to human cognition—our judgments about the world are
    fueled by our imagination of what could have been.
  prefs: []
  type: TYPE_NORMAL
- en: Note that Marjani’s counterfactual reasoning involves a type of prediction.
    Like a Marvel superhero film, she is imagining a *parallel world* where she chose
    based on chemistry, and she’s *predicting* the outcome of her love life in that
    world. But statistical machine learning algorithms are better at making predictions
    than humans. That insight leads us to the prospect of building AI that automates
    human-like counterfactual reasoning with statistical machine learning tools.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll pursue that goal by learning to formalize counterfactual
    questions with probability. In the next chapter, we’ll implement a probabilistic
    counterfactual inference algorithm that can answer these questions. Let’s start
    by exploring some practical case studies that motivate algorithmic counterfactual
    reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Motivating counterfactual reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, I’ll introduce some case studies demonstrating the business value of answering
    counterfactual questions. I’ll then argue how they are useful for enhancing decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Online gaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall the online gaming example from chapter 7, where the amount of in-game
    purchases a player made was driven by their level of engagement in side-quests
    and whether they were in a guild. Suppose we observed an individual player who
    was highly engaged in side-quests and had many in-game purchases. A counterfactual
    question of interest might be, “What would their amount of in-game purchases be
    if their engagement was low?”
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 The streaming wars
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The intense competition amongst subscription streaming companies for a finite
    market of subscribers has been dubbed “the streaming wars.” Netflix is a dominant
    player with long experience in the space. It has learned to attract new subscribers
    by building blockbuster franchises from scratch, such as *House of Cards*, *Stranger
    Things,* and *Squid Game*.
  prefs: []
  type: TYPE_NORMAL
- en: However, Netflix competes with Amazon, Apple, and Disney—companies with extremely
    deep pockets. They can compete with Netflix’s ability to build franchises from
    scratch by simply buying existing successful franchises (e.g., *Star Wars*) and
    making novel content within that franchise (e.g., *The Mandalorian*).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that Disney is in talks to buy *James Bond*, the most valuable spy thriller
    franchise ever, and Netflix believes that a successful Bond deal may cause it
    to lose subscribers to Disney. Netflix hopes to prevent this by striking a deal
    with a famous showrunner to create a new spy-thriller franchise called *Dead Drop*.
    This new franchise would combine tried and true spy thriller tropes (e.g., gadgetry,
    exotic backdrops, car chases, over-the-top action sequences) with the complex
    characters, diverse representation, and emotionally compelling storylines characteristic
    of Netflix-produced shows. There is uncertainty about whether Netflix executives
    can close a deal with the candidate showrunner, as both parties would have to
    agree on creative control, budget, royalties, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the Bond deal succeeded, and Disney Plus now runs new series and films
    set in the “Bond-verse.” However, the *Dead Drop* deal fell through. Netflix then
    acquires data that identifies some subscribers who subsequently left Netflix,
    subscribed to Disney Plus, and went on to watch the new Bond content.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Netflix executive would be inclined to ask the following counterfactual:
    “Would those lost subscribers have stayed, had the *Dead Drop* deal succeeded?”
    Suppose the answer is “no,” because the Bond content was so strong an attraction
    that the *Dead Drop* deal outcome didn’t matter. In this case, the employees who
    failed to close the deal should not be blamed for losing subscribers.'
  prefs: []
  type: TYPE_NORMAL
- en: Or, suppose the *Dead Drop* deal succeeded, and Netflix subscribers can now
    watch the new *Dead Drop* franchise. “Would those subscribers who watch *Dead
    Drop* have left for the new Bond series on Disney, had the deal failed?” Again,
    if the answer is “no,” the employees who successfully closed the deal shouldn’t
    get credit for keeping all those subscribers.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, answering these questions would help inform future deal-making
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.3 Counterfactuals analysis of machine learning models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we are focusing on using a causal model to reason counterfactually
    about some data generating process. In machine learning, often the goal is a counterfactual
    analysis of a machine learning model itself; i.e., given some input features and
    some output predictions, how would the predictions have differed if the inputs
    were different? This counterfactual analysis supports explainable AI (XAI), AI
    fairness, and other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Counterfactual analysis in classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider the task of classification—a trained algorithm takes as input some
    set of features for a given example and produces a predicted class for that example.
    For example, given the details of a loan application, an algorithm classifies
    the application as “reject” or “approve.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a rejected application, a counterfactual question naturally arises: “Would
    the application have been approved if *some elements* of the application were
    different?” Often, the goal of the counterfactual analysis is to find a minimal
    change to the feature vector that corresponds to a change in the classification.
    Figure 8.1 illustrates this idea.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F01_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 What is the minimal change to the input feature that would have led
    to approval? In this case, the loan would have been approved if income were $20,000
    higher.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Finding the minimal change that would have led to approval requires defining
    a distance metric in the feature space and then finding the feature value on the
    other side of the class boundary. In this example, the hypothetical condition
    “if the applicant had $20,000 more a year in income . . .” corresponds to the
    smallest change to the feature vector (in terms of distance on the decision surface)
    that would have led to approval. This type of analysis is useful for XAI; i.e.,
    for understanding how features drive classification on a case-by-case basis.
  prefs: []
  type: TYPE_NORMAL
- en: Counterfactual algorithmic recourse
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Increasing salary by $20,000 is unrealistic for most loan applicants. That’s
    where counterfactual-based algorithmic recourse can be useful. *Algorithmic recourse*
    looks for the nearest hypothetical condition that would have led to a different
    classification. It operates under the constraint that the hypothetical condition
    was *achievable* or *actionable* by the applicant. Figure 8.2 shows how this works.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the assumption is that increasing income by $5,000 *and* improving
    one’s credit score was achievable, according to some criteria (while increasing
    income by $20,000 was not).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F02_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 In algorithmic recourse, we’re often interested in the nearest *actionable*
    feature vector on the other side of the decision boundary.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Algorithmic recourse aims to give individuals subjected to machine learning–based
    decisions information that they can work with. If one fails an exam and asks why,
    an explanation of “because you are not a genius” is less useful than “because
    you didn’t review the practice exam,” even though both may be true.
  prefs: []
  type: TYPE_NORMAL
- en: Counterfactual fairness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Counterfactual fairness analysis is a similar analysis that applies in cases
    where some of the input features correspond to attributes of a person. The idea
    is that certain attributes of an individual person should not, on ethical grounds,
    impact the classification. For example, it is unethical to use one’s ethnicity
    or gender in the decision to offer a loan. Even if such “protected attributes”
    are not explicitly coded into the input features, the classification algorithm
    may have learned proxies for protected attributes, such as the neighborhood where
    one lives, one’s social network, shopping habits, etc. It may make sense to have
    such features in the model, and it may not be obvious when those features behave
    as proxies for protected attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 uses the loan algorithm example to illustrate how a counterfactual
    fairness analysis would ask counterfactual questions. In this case, the counterfactual
    question is “Would this person have been approved if they were of a different
    ethnicity?” The analyst would find features that are proxies for ethnicity and
    then see if a change to those proxies corresponding to a change in ethnicity would
    result in a classification of “approve.” Some techniques attempt to use this analysis
    during training to produce fairer algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F03_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 For counterfactual fairness, suppose we want to test whether the
    algorithm has a bias against certain ethnicities. For a given feature vector,
    an ethnicity element, and a corresponding “reject” outcome, we test if the outcome
    would change if the ethnicity element changed.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While counterfactual fairness analysis is not enough to solve the broad problem
    of AI fairness, it is an essential element in the AI fairness toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.4 Regret and why do we care about what “would have happened?”
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditional machine learning is usually forward-looking. Given data, you make
    a prediction, that prediction drives some decision to be made in the present,
    and that decision brings about some future cost or benefit. We want good predictions
    so we can get more future benefits. Imagine, for example, a machine learning algorithm
    that could accurately forecast the performance of a stock portfolio—that would
    obviously be quite valuable.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine a different algorithm that could accurately tell you how your portfolio
    would perform today if you had bought different stocks; that would certainly be
    less valuable than predicting the future. This contrast highlights a common criticism
    that modeling counterfactuals is backward-looking. For example, the counterfactual
    questions in our motivating case studies focus on decisions and outcomes that
    happened in the past. What’s done is done; getting the answers to such questions
    won’t change the past.
  prefs: []
  type: TYPE_NORMAL
- en: But, first, not all counterfactuals are retrospective. In section 8.3 we’ll
    model questions like “What are the chances the subscriber would churn if you don’t
    send them a promotion and would not churn if you did send a promotion?” (“Churn”
    means to stop using a product or service within a certain time period.) That question
    has no past tense, does have business value, and is something we can model.
  prefs: []
  type: TYPE_NORMAL
- en: Second, *retrospective* *counterfactuals help you understand how to make better
    decisions in the future*. Indeed, analyzing how your portfolio would have performed
    given different allocations—what investors call “backtesting”—is ideal for comparing
    various investment strategies. Similarly, the counterfactual insights from a failed
    *Dead Drop* deal might help Netflix executives make a deal with another famous
    showrunner.
  prefs: []
  type: TYPE_NORMAL
- en: When we consider retrospective reasoning about things that *would have* or *could
    have* been, we arrive at the notion of *regret*. Regret is about retrospective
    counterfactual contrasts; given a choice, regret is a comparison between an outcome
    of the option you chose and an imagined counterfactual outcome of an option you
    rejected. In colloquial terms, regret is the bad feeling you get when the counterfactual
    outcome of an option you rejected is better than the option you chose. But cognitive
    science calls this *negative regret*; there is also *positive regret*, which is
    the good feeling you get when, upon comparing to imagined counterfactual outcomes,
    you realize you chose the better option (as in, “whew, I really dodged a bullet”).
  prefs: []
  type: TYPE_NORMAL
- en: Regret can be useful for learning to make better decisions. Suppose you make
    a choice, you pay a cost (time, effort, resources, etc.), and it leads to an outcome.
    That gives you a baseline single point of data for learning. Now, suppose that,
    with the benefit of hindsight, you could imagine with 100% accuracy the outcome
    that would have occurred had you made a different choice. Now you have two comparable
    points of data for learning, and you only had to pay a cost for one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Usually your ability to imagine the counterfactual outcome of the rejected option
    is not 100% accurate. Even with the benefit of hindsight, there is still some
    uncertainty about the counterfactual outcome. But that’s no problem—we can model
    that uncertainty with probability. As long as hindsight knowledge provides you
    with some information about counterfactual outcomes, you can do better than the
    baseline of learning from a single point of data.
  prefs: []
  type: TYPE_NORMAL
- en: In reinforcement learning and other automated decision-making, we often call
    our decision-making criteria “policies.” We can incorporate counterfactual analysis
    and regret in *evaluating* and *updating* *policies*.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.5 Reinforcement learning and automated decision-making
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In automated decision-making, a “policy” is a function that takes in information
    about a decision problem and automatically selects some course of action. Reinforcement
    learning algorithms aim to find policies that optimize good outcomes over time.
  prefs: []
  type: TYPE_NORMAL
- en: Automated counterfactual reasoning can credit good outcomes to the appropriate
    actions. In the investing example, we can imagine an algorithm that periodically
    backtests different portfolio allocation policies as more recent prices enter
    the data. Similarly, imagine we were writing a reinforcement learning (RL) algorithm
    to learn to play a game. We could have the algorithm use saved game instances
    to simulate how that game instance would have turned out differently if it had
    used a different policy. The algorithm can quantify the concept of regret by comparing
    those simulated outcomes to actual outcomes and using the results to learn a better
    policy. This would reduce the number of games the AI needed to learn a good policy,
    as well as enable it to learn from simulated conditions that don’t occur normally
    in the game. We’ll focus more on automated decision-making, bandits, and reinforcement
    learning in chapter 12.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.6 Steps to answering a counterfactual query
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Across each of these applications, we can answer these counterfactual inference
    questions with the following workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pose the counterfactual question*—Clearly articulate the counterfactual question(s)
    we want to pose in the simplest terms.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Convert to a mathematical query*—Convert the query to mathematical symbols
    so it is formal enough to apply mathematical or algorithmic analysis.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Do inference*—Run an inference algorithm that will generate an answer to the
    question.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following sections, we’ll focus on steps 1 and 2\. In chapter 9, we’ll
    handle step 3 with an SCM-based algorithm for inferring the query we create in
    step 2\. In chapter 10, we’ll see ways to do step 3 without an SCM but only data
    and a DAG.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Symbolic representation of counterfactuals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 7, we saw the “counterfactual notation,” which uses subscripts to
    represent interventions. Now we are going to use this notation for counterfactual
    expressions. The trick is remembering, as we’ll see, that counterfactual queries
    are just a special type of *interventional* queries. We’ll see how interventional
    queries flow into counterfactual queries by revisiting our online gaming example.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Hypothetical statements and questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider our online gaming case study. When considering how much a player makes,
    we might say something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: The level of in-game purchases for a typical player would be more than $50.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We’ll call this a *hypothetical statement.* In grammatical terms, I am using
    a *modal verb* (e.g., “would”, “could”, “should” in “would be more”) to intentionally
    mark *hypothetical language* rather than using *declarative language* (e.g., “is
    more” or “will be more”), which we use to make statements about objective facts.
  prefs: []
  type: TYPE_NORMAL
- en: We want to formalize this statement in probability notation. For this statement,
    we’ll write *P*(*I*>50)—recall that we used the random variable *I* to represent
    *In-Game Purchases*, *E* to represent *Side-Quest Engagement*, and *G* to represent
    *Guild Membership*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use hypothetical language in our open questions as well, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: What would be the amount of in-game purchases for a typical player?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I am inquiring about the range of values the variable *I* could take, and I
    represent that with *P*(*I*).
  prefs: []
  type: TYPE_NORMAL
- en: Declarative vs. hypothetical language and probability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Declarative language express certainty, as in “amount of in-game purchases is
    more than $50.” In contrast, hypothetical language is used for statements that
    convey conjecture, imagination, and supposition, as in “amount of in-game purchases
    *would be* more than $50.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of us learn to associate probability notation with declarative language,
    because of probability theory’s connection to propositional logic: *P*(*I*>50)
    quantifies the probability that the declarative statement “amount of in-game purchases
    is more than $50” is true. But we are going to lean into the hypothetical language.'
  prefs: []
  type: TYPE_NORMAL
- en: Hypothetical language has an implicit lack of certainty—we are talking of things
    that *could be*, rather than things that *are*. Lack of certainty is equivalent
    to uncertainty, and the Bayesian philosophy we adopt in this book nudges us toward
    using probability to model uncertainty, so using hypothetical language will make
    it easier for us to formalize the question in probability notation. We’ll find
    this will help us formalize causal statements and questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the tense of the question or statement doesn’t matter when we map
    it to probabilistic notation. For example, we could have used this phrasing:'
  prefs: []
  type: TYPE_NORMAL
- en: What would *have been* the amount of in-game purchases for a typical player?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Regardless of tense, we use the notation *P*(*I*) to represent our uncertainty
    about a variable of interest in our question.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Facts filter hypotheticals to a subpopulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose my statement was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The level of in-game purchases for a player with high side-quest engagement
    would be more than $50\. *P*( *I*>50| *E*=“high”)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Here, I am making a statement about a subset of players (those with high side-quest
    engagement) rather than all players. I’m doing the same when I ask this question:'
  prefs: []
  type: TYPE_NORMAL
- en: What would be the level of in-game purchases for players with high side-quest
    engagement? *P*( *I*| *E*=“high”)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The fact that *Side-Quest Engagement* is high serves to filter the population
    of players down to those for whom that fact is true. As discussed in chapter 2,
    we use conditional probability to zoom in on a subpopulation. In this example,
    we use *P*(*I*>50|*E*=“high”) for the statement, and *P*(*I*|*E*=“high”) for the
    question.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll use “factual conditions” to refer to facts, events, and evidence like *E*=“high”
    that narrow down the target population. These factual conditions appear on the
    right side of “|” in the conditional probability notation *P*(.|.). We might normally
    call them “conditions,” but I want to avoid confusion with “conditional hypothetical,”
    which I’ll introduce next.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3 Conditional hypotheticals, interventions, and simulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, suppose I made the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: If a player’s side-quest engagement was high, they would spend more than $50
    on in-game purchases. *P* ( *I* [*E*] [=“high”]>50)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We’ll call this a *conditional hypothetical* statement. We’ll call the “If side-quest
    engagement was high” part the *hypothetical condition*, and “they would spend
    more than $50 on in-game purchases” is the *hypothetical outcome*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hypothetical conditions in conditional hypothetical questions often follow
    a similar “what if” style of phrasing:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What* would be the amount of in-game purchases for a player *if* their side-quest
    engagement was high? *P* ( *I* [*E*] [=“high”])'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will use the intervention notation (i.e., the subscripts in counterfactual
    notation) to represent these conditions. For the statement, we will use *P*(*I*[*E*][=“high”]>50),
    and for the question, *P*(*I*[*E*][=“high”]).
  prefs: []
  type: TYPE_NORMAL
- en: Imagination, conditions, and interventions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using the ideal intervention to model hypothetical conditions in conditional
    hypothetical statements is a philosophical keystone of our causal modeling approach.
    The idea is that when we pose hypothetical conditionals, *we only attend to the
    causal consequences of the hypothetical conditional*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refresher: Ideal intervention'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'An ideal intervention is a change to the data generating process that does
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Targets a fixed variable (e.g., X)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sets that variable to a specific value (e.g., x)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In so doing, severs the causal influence of that variable’s parents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This definition generalizes to a *set* of variables.
  prefs: []
  type: TYPE_NORMAL
- en: We sometimes write interventions with *do-notation*, as in do(*X*=*x*). In counterfactual
    notation, for a variable *Y*, we write *Y*[*X*][=][*x*] to indicate that the variable
    *Y* is under the influence of an intervention on *X*. In a DAG, we represent ideal
    intervention with graph surgery, meaning we cut the incoming edges to the target
    variable. In an SCM, we represent an ideal intervention by replacing the target
    variable’s assignment function with the intervention value. Causal libraries often
    implement these operations for us, often with a function or method called “do”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me illustrate by counterexample. Suppose we ask this:'
  prefs: []
  type: TYPE_NORMAL
- en: What would be the amount of in-game purchases for a player if their side-quest
    engagement were high?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Suppose we then modeled this with *P*(*I*|*E*=“high”). Then inference on this
    query would use not just the causal impact that high engagement has on *In-Game
    Purchases* but also the non-causal association through the path *E* ← *G* → *I*;
    you can infer whether a player is in a guild from their level of *Side-Quest Engagement*,
    and *Guild Membership* also drives *In-Game Purchases*. But this question is not
    about *Guild Membership*; we’re just interested in how *Side-Quest Engagement*
    drives *In-Game Purchases*.
  prefs: []
  type: TYPE_NORMAL
- en: '“What if” hypotheticals use the ideal intervention because they attend only
    to the causal consequences of the condition. To illustrate, let’s rephrase the
    previous question to make that implied ideal intervention explicit:'
  prefs: []
  type: TYPE_NORMAL
- en: What would be the amount of *In-Game Purchases* for a player if their *side-quest
    engagement* *were* *set to* high? *P* ( *I* [*E*] [=“high”])
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The verb “set” connotes the action of intervening. Modeling hypothetical conditions
    with ideal interventions argues that the original phrasing and this phrasing mean
    the same thing (going forward, I’ll use the original phrasing).
  prefs: []
  type: TYPE_NORMAL
- en: As humans, we answer “what if” questions like the preceding *P*(*I*[*E*][=“high”])
    question (either the original or rephrased version) by imagining a world where
    the hypothetical condition is true and then imagining how the hypothetical scenario
    plays out as a consequence. The variables in our hypothetical condition may have
    their own causal drivers in the data-generating process (e.g., *Guild Membership*
    is a cause of *Side-Quest Engagement*), but we ignore those drivers because we
    are only interested in the consequences of the hypothetical condition. We isolate
    the variables in a hypothetical condition in our imaginations just as we would
    in an experiment. The ideal intervention is the right tool for setting a variable
    independently of its causes.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding confusion between factual and hypothetical conditions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is particularly easy to confuse “factual conditions” with “hypothetical conditions.”
    To reiterate in general terms, in the question “What would *Y* be if *X* were
    *x*”, *X* = *x* is a hypothetical condition and we use the notation *P*(*Y*[*X*][=][*x*]).
    In contrast, factual conditions serve to narrow down the population we are asking
    about. For example, in the question “What would *Y* be for cases where *X* is
    *x*?” *X* = *x* is an actual condition used to filter down to cases where *X*
    = *x*. Here, we use notation *P*(*Y*|*X*=*x*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind that we can combine factual and hypothetical conditions, as in
    the following question:'
  prefs: []
  type: TYPE_NORMAL
- en: What would be the amount of in-game purchases for a player *in a guild* if their
    side-quest engagement was high? *P* ( *I* [*E*] [=“high”]| *G*= *g*)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Here, we are asking a conditional hypothetical on a subset of players who are
    guild members. This query is different from the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What would be the amount of in-game purchases for a player if their side-quest
    engagement was high *and* they were in a guild? *P* ( *I* [*E*] [=“high”], [*G*]
    [=“member”])
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That said, with the ambiguity of natural language, someone might ask the second
    question when what they really want is the answer to the first question. It is
    up to the modeler to dispel confusion, clarify meaning, and write down the correct
    notation.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.4 Counterfactual statements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In natural language, a *counterfactual statement* is a conditional hypothetical
    statement where there is some conflict between factual conditions and hypothetical
    conditions or outcomes. In other words, it is a conditional hypothetical statement
    that is “counter to the facts.”
  prefs: []
  type: TYPE_NORMAL
- en: 'In everyday language, those conflicting factual conditions could be stated
    before the statement or implied by context. For our purposes, we’ll require counterfactual
    statements to state the conflicting factual conditions explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '*For a player with low side-quest engagement and an amount of in-game purchases
    less than $50,* if the player’s side-quest engagement were high, they would spend
    more than $50 on in-game purchases. *P* ( *I* [*E*] [=“high”]>50| *E*=“low”, *I* 
    £50)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'As a question, we might ask:'
  prefs: []
  type: TYPE_NORMAL
- en: What would be the amount of in-game purchases for a player *with low side-quest
    engagement and in-game purchases less than $50* if their side-quest engagement
    was high? *P* ( *I* [*E*] [=“high”] | *E*=“low”, *I*  £50)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In both the statement and the question, the factual condition of low engagement
    conflicts with the hypothetical condition of high engagement. In the statement,
    the hypothetical outcome where in-game purchases is more than 50 conflicts with
    the factual condition where it is less than or equal to 50\. Similarly, the question
    considers all possible hypothetical outcomes for in-game purchases, most of which
    conflict with the factual condition of being less than or equal to 50\. We use
    counterfactual notation to write these queries just as we would other conditional
    hypotheticals..
  prefs: []
  type: TYPE_NORMAL
- en: Overview of terminology in formalizing counterfactuals
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Hypothetical language*—Used to express hypotheses, conjecture, supposition,
    and imagined possibilities. In English, it often involves “would” or “could” and
    contrasts with the declarative language. It is arguably easier to formalize causal
    statements and questions phrased in hypothetical language.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hypothetical statement*—A statement about the world phrased in hypothetical
    language, such as “*Y* would be *y,*” which we’d write in math as *P*(*Y*=*y*).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Factual conditions*—Refer to facts, events, and evidence that narrow down
    the scope of what’s being talked about (the target population). Used as the conditions
    in conditional probability. For example, we’d write “Where *Z* is *z*, *Y* would
    be *y*” as *P*(*Y*=*y*|*Z*=*z*).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hypothetical conditions*—Conditions that frame a hypothetical scenario, as
    in “what if *X* were *x*?” or “If *X* were *x* …” We model hypothetical conditions
    with the ideal intervention and subscript [*X*][=][*x*] in counterfactual notation.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Conditional hypothetical statement*—A hypothetical statement with hypothetical
    conditions, such as “If *X* were *x*, *Y* would be *y*,” which becomes *P*(*Y*[*X*][=][*x*]=*y*).
    We can add factual conditions like “Where *Z* is *z*, if *X* were *x*, *Y* would
    be *y*” becomes *P*(*Y*[*X*][=][*x*]=*y*|*Z*=*z*).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Counterfactual statement*—A counterfactual statement is a conditional hypothetical
    statement where the variables in the factual conditions overlap with those in
    the hypothetical conditions or hypothetical outcomes. For example, in “Where *X*
    is *x*, if *X* were *x''*, *Y* would be *y*” (*P*(*Y*[*X*][=][*x*]*['']*=*y*|*X*=*x*)),
    the factual condition “Where *X* is *x*” overlaps with the hypothetical condition
    “if *X* were *x''*”. In “Where *Y* is *y*, if *X* were *x''*, *Y* would be *y''*”
    (*P*(*Y*[*X*][=][*x*]*['']*=*y''*|*Y*=*y*)), the factual condition “Where *Y*
    is *y*” overlaps with the hypothetical outcome “*Y* would be *y.*”'
  prefs: []
  type: TYPE_NORMAL
- en: '*Consistency rule*—You can drop a hypothetical condition in the subscript if
    a factual condition and a hypothetical condition overlap but don’t conflict. For
    example, *P*(*Y*[*X*][=][*x*]|*X*=*x*) = *P*(*Y*|*X*=*x*).'
  prefs: []
  type: TYPE_NORMAL
- en: Note that many texts will use the word “counterfactual” to describe formal causal
    queries that don’t necessarily condition on factual conditions, such as *Y*[*X*][=][*x*]
    or *P*(*Y*[*X*][=][*x*]=*y*) or *P*(*Y*[*X*][=][*x*]=1, *Y*[*X*][=][*x*]*[']*=0).
    I’m using “counterfactual statement” and other phrases above to describe common
    hypothetical and counterfactual natural language and to aid in the task of converting
    to formal counterfactual notation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can combine conflicting factual conditions with other non-conflicting
    factual conditions, such as being a member of the guild in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: What would be the amount of in-game purchases for a player *in a guild* with
    low side-quest engagement and in-game purchases less than $50 if their side-quest
    engagement was high? *P*( *I* [*E*] [=“high”] | *E*=“low”, *I* £50, *G*=“member”)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Figure 8.4 diagrams the elements of a formalized counterfactual query.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F04_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 Elements of a conditional counterfactual hypothetical formalized
    in counterfactual notation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Formalizing counterfactuals with large language models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Formalizing a counterfactual question into counterfactual notation is an excellent
    task for a large language model (LLM). State-of-the-art LLMs perform quite well
    at benchmarks where a natural language query is converted to a symbolic query,
    such as an SQL statement, and formalizing a counterfactual question is an example
    of this task. We’ll look more at LLMs and causality in chapter 13, but for now
    you can experiment with prompting your favorite LLM to convert questions to counterfactual
    notation.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.5 The consistency rule
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider the distribution *P*(*I*[*E*][=“high”]|*E*=“low”). Suppose that instead
    of the subscript [*E*][=“high”] we had [*E*][=“low”], so the distribution is *P*(*I*[*E*][=“low”]
    |*E*=“low”). The *consistency rule* states that this distribution is equivalent
    to the simpler *P*(*I*|*E*=“low”). More generally, *P*(*Y*[*X*][=][*x*]|*X*=*x*,
    *Z*=*z*) = *P*(*Y* |*X*=*x*, *Z*=*z*) for any *z*.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, *P*(*I*[*E*][=“low”]|*E*=“low”) corresponds to the rather odd question,
    “What would be the amount of in-game purchases for a player with low side-quest
    engagementif their side-quest engagement was low?” In this question, the factual
    condition and the hypothetical condition overlap but don’t conflict. The *consistency
    rule* says that, in this case, we drop the hypothetical condition, saying that
    this is equivalent to asking “What would be the amount of in-game purchases for
    a player with low side-quest engagement?”
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider a version of this counterfactual where we observe an actual outcome
    for in-game purchases. Specifically, consider *P*(*I*[*E*][=“high”]|*E*=“low”,
    *I*=75). This is the corresponding counterfactual question:'
  prefs: []
  type: TYPE_NORMAL
- en: What would be the amount of in-game purchases for a player *with low side-quest
    engagement and in-game purchases equal to $75* if their side-quest engagement
    was high?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now, instead, suppose we changed it to *P*(*I*[*E*][=“low”]|*E*=“low”, *I*=75).
    By the consistency rule, this collapses to *P*(*I*|*E*=“low”, *I*=75):'
  prefs: []
  type: TYPE_NORMAL
- en: What would be the amount of in-game purchases for a player *with low side-quest
    engagement and in-game purchases equal to $75*?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The answer, of course, is $75\. If we ask about the distribution of *I* conditional
    on *I*=75, then we have a distribution with all the probability value concentrated
    on 75.
  prefs: []
  type: TYPE_NORMAL
- en: In counterfactual reasoning, we often want to know about hypothetical outcomes
    for the same variables we observe in the factual conditions. The consistency rule
    states that if the hypothetical conditions are the same as what actually happened,
    the hypothetical outcome must be the same as what actually happened.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that we use an intervention to model the hypothetical condition. The
    rule assures us that if a player had low *Side-Quest Engagement* and a certain
    amount of *In-Game Purchases*, they would have the exact same amount of *In-Game
    Purchases* if they were selected for an experiment that randomly selected them
    for the low *Side-Quest Engagement* group. That’s important if we expect our causal
    inferences to predict the outcomes of experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.6 More examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 8.1 presents several additional examples of mapping counterfactual questions
    to counterfactual notation.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 Examples of counterfactual notation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Question | Type | Distribution in counterfactual notation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| What would be the amount of in-game purchases for a typical player?  | Hypothetical  |
    *P*( *I*)  |'
  prefs: []
  type: TYPE_TB
- en: '| What would be the amount of in-game purchases for a player with high side-quest
    engagement?  | Hypothetical focused on highly engaged players  | *P*( *I*&#124;
    *E*=“high”)  |'
  prefs: []
  type: TYPE_TB
- en: '| What would be the amount of in-game purchases for a player if they had high
    side-quest engagement?  | Conditional hypothetical  | *P*( *I* [*E*] [=“high”])  |'
  prefs: []
  type: TYPE_TB
- en: '| What would be the level of engagement *and* amount of in-game purchases if
    the player were a guild member?  | Conditional hypothetical on two outcomes of
    interest  | *P*( *E* [*G*] [=“member”], *I* [*G*] [=“member”])  |'
  prefs: []
  type: TYPE_TB
- en: '| What would be the level of in-game purchases for a player if they had high
    side-quest engagement and they were not a guild member?  | Conditional hypothetical
    with two hypothetical conditions  | *P*( *I* [*E*] [=“high”,] [*G*] [=“nonmember”])  |'
  prefs: []
  type: TYPE_TB
- en: '| What would be the level of in-game purchases for a player in a guild if they
    had high side-quest engagement?  | Conditional hypothetical focused on guild members  |
    *P*( *I* [*E*] [=“high”]&#124; *G*=“member”)  |'
  prefs: []
  type: TYPE_TB
- en: '| For a player with low engagement, what would their level of in-game purchases
    be if their level of engagement was high?  | Counterfactual. Factual condition
    conflicts with hypothetical condition.  | *P*( *I* [*E*] [=“high”]&#124; *E*=“low”)  |'
  prefs: []
  type: TYPE_TB
- en: '| For a player who had at most $50 of in-game purchases, what would their level
    of in-game purchases be if their level of engagement was high?  | Counterfactual.
    Factual condition (in-game purchases of £$50) conflicts with possible hypothetical
    outcomes (in-game purchases possibly >$50).  | *P*( *I* [*E*] [=“high”]&#124;
    *I* £50)  |'
  prefs: []
  type: TYPE_TB
- en: '| For a player who had low engagement and at most $50 of in-game purchases,
    what would their level of in-game purchases be if their level of engagement was
    high?  | Counterfactual. Factual conditions conflict with a hypothetical condition
    and possible hypothetical outcomes.  | *P*( *I* [*E*] [=“high”]&#124; *E*=“low”,
    *I* £50)  |'
  prefs: []
  type: TYPE_TB
- en: '| For a player in a guild who had low engagement, what would their level of
    in-game purchases be if their engagement were high and they weren’t a guild member?  |
    Counterfactual. Factual conditions conflict with hypothetical conditions.  | *P*(
    *I* [*E*] [=“high”,] [*G*] [=“nonmember”]&#124; *E*=“low”, *G*=“member”)  |'
  prefs: []
  type: TYPE_TB
- en: '| What would be the level of engagement if the player were a guild member?
    Moreover, what would be their level of in-game purchases if they were *not* a
    guild member?  | Counterfactual. Involves two conflicting hypothetical conditions
    on two different outcomes.  | *P*( *E* [*G*] [=“member”], *I* [*G*] [=“nonmember”])  |'
  prefs: []
  type: TYPE_TB
- en: The last case in table 8.1 is a special case, more common in theory than practice,
    that does not involve a factual condition but has conflicting hypothetical conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at a particular class of counterfactuals that involve binary
    causes and outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Binary counterfactuals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An important subclass of counterfactual query is one we’ll call *binary counterfactuals*.
    These are counterfactuals involving binary hypothetical conditions and outcome
    variables. Binary variables, especially binary causes, arise when we think in
    terms of observational and experimental studies, where we have “exposed” and “unexposed”
    groups, or “treatments” and “control” groups. But binary variables are also useful
    in reasoning about the occurrence of events; an event either happens or does not.
  prefs: []
  type: TYPE_NORMAL
- en: Binary counterfactual queries deserve special mention because they are often
    simpler to think about, have simplifying mathematical properties that queries
    on nonbinary variables lack, and have several practical applications that we’ll
    cover in this section. Further, you can often word the question you want to answer
    in binary terms, such that you can convert nonbinary variables to binary variables
    when formalizing your query. To illustrate, in our online gaming example, suppose
    a player made $152.34 in online purchases, and we ask “Why did this player pay
    so much?” We are not interested in why they paid exactly that specific amount
    but why they paid such a high amount, where “such a high amount” is defined as,
    for example, more than $120\. So our binary indicator variable is *X* = {1 if
    *I* ≥ 120 else 0}.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Probabilities of causation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The probabilities of causation are an especially useful class of binary counterfactuals.
    Their utility lies in helping us answer “why” questions. They are foundational
    concepts in practical applications, including attribution in marketing, credit
    assignment in reinforcement learning, root cause analysis in engineering, and
    personalized medicine.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s demonstrate the usefulness of the probabilities of causation in the context
    of a churn attribution problem. In a subscription business model, churn is the
    rate at which your service loses subscribers, and it has a major impact on the
    value of a business or business unit. Typically, a company deploys a predictive
    algorithm that rates subscribers as having some degree of churn risk. The company
    wants to discourage subscribers with a high risk of churn from actually doing
    so. In our example, the company will send a promotion that will entice the subscriber
    to stay (not churn). The probabilities of causation can help us understand why
    a user would churn or stay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a binary (true/false) cause *X* and outcome *Y*, we’ll define the following
    probabilities of causation: probability of necessity, of sufficiency, of necessity
    *and* sufficiency, of enablement, and of disablement.'
  prefs: []
  type: TYPE_NORMAL
- en: Probability of necessity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For a binary cause *X* and binary outcome *Y*, the *probability of necessity*
    (PN) is the query *P*(*Y*[*X*][=][0]=0|*X*=1, *Y*=1). In plain language, the question
    underlying PN is “For cases where *X* happened, and *Y* happened, if *X* had not
    happened, would *Y* not have happened?” In other words, did *X* *need* to happen
    for *Y* to happen?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider our churn problem. Let *X* represent whether we sent a promotion
    and *Y* represent whether the user stayed (didn’t churn). In this example, *P*(*Y*[*X*][=][0]=0|*X*=1,
    *Y*=1) represents the query “For a subscriber who received the promotion and stayed,
    what are the chances they would have churned if they had not received the promotion?”
    In other words, was the promotional offer necessary to maintain the subscriber?
  prefs: []
  type: TYPE_NORMAL
- en: Probability of sufficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *probability of sufficiency* (PS) is *P*(*Y*[*X*][=][1]=1|*X*=0, *Y*=0).
    A common plain language articulation of PS is “For cases where neither *X* nor
    *Y* happened, if *X* had happened, would *Y* have happened?” In other words, is
    *X* happening sufficient to cause *Y* to happen? For example, “for users who did
    not receive a promotion and didn’t stay (churned), would they have stayed had
    they received the promotion?” In other words, would a promotion have been enough
    (sufficient) to keep them?
  prefs: []
  type: TYPE_NORMAL
- en: The plain language interpretation of sufficiency can be confusing. The factual
    conditions of the counterfactual query zoom in on cases where *X*=0 and *Y*=0
    (cases where neither *X* nor *Y* happened). However, we’re often interested in
    looking at cases where *X*=1 and *Y*=1 and asking if *X* was sufficient by itself
    to cause *Y*=1\. In other words, given that *X* happened and *Y* happened, would
    *Y* still have happened even if the various other events that influenced *Y* had
    turned out different? But *P*(*Y*[*X*][=1]=1|*X*=0, *Y*=0) entails this interpretation
    without requiring us to enumerate all the “various other events that influenced
    *Y*” in the query. See the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for pointers to deeper research discussions on sufficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilities of causation and the law
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The probabilities of causation are closely related to legal concepts in the
    law. It is helpful to know this relationship, since practical applications often
    intersect with the law, and many stakeholders we work with in practical settings
    have legal training.
  prefs: []
  type: TYPE_NORMAL
- en: '*But-for causation and the probability of necessity*—The but-for test is one
    test for determining causation in tort and criminal law. The way we phrase the
    probability of necessity is the probabilistic equivalent to the but-for test,
    rephrasing “if *X* had not happened, would *Y* not have happened?” as “but for
    *X* happening, would *Y* have happened?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Proximal causality and the probability of sufficiency*—In law, proximate cause
    refers to the primacy that a cause *X* had in the chain of events that directly
    brings about an outcome (e.g., injury or damage). There is indeed a connection
    with sufficiency, though not an equivalency. Proximal causality indeed considers
    whether a causal event was sufficient to cause the outcome, but legal theories
    of proximal cause often go beyond sufficiency to invoke moral judgments as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability of necessity and sufficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *probability of necessity and sufficiency* (PNS) is *P*(*Y*[*X*][=1]=1,
    *Y*[*X*][=0]=0). In plain language, *P*(*Y*[*X*][=1]=1, *Y*[*X*][=0]=0) reads,
    “*Y* would be 0 if *X* were 0 *and* *Y* would be 1 if *X* were 1.” For example,
    “What are the chances that a given user would churn if they didn’t receive a promotion
    and would stay if they did receive a promotion?” PNS decomposes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch8-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: Probability of disablement and enablement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *probabilities of disablement* (PD) and *enablement* (PE)are similar to
    PN and PS, except they do not condition on the cause *X*.
  prefs: []
  type: TYPE_NORMAL
- en: PD is the query *P*(*Y*[*X*][=][0]=0|*Y*=1), meaning “For cases where *Y* happened,
    if *X* had not happened would *Y* not have happened?” For the churn problem, PD
    asks the question “What is the overall chance of churn if we don’t send promotions?
    exclusively in reference to the subpopulation of users who didn’t churn (regardless
    of whether they received a promotion).
  prefs: []
  type: TYPE_NORMAL
- en: PE is the query *P*(*Y*[*X*][=1]=1|*Y*=0), or “For cases where *Y* didn’t happen,
    if *X* had happened, would *Y* have happened?” In our churn problem, PE asks,
    “What is the overall chance of staying if we send promotions?” exclusively in
    reference to the subpopulation of users who churned (regardless of whether they
    received a promotion).
  prefs: []
  type: TYPE_NORMAL
- en: The probabilities of causation can work as basic counterfactual primitives in
    advanced applications of counterfactual analysis. Next, I’ll give an example in
    the context of attribution.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 Probabilities of causation and attribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The probabilities of causation are the core ingredients for methods that quantify
    why a given outcome happens. For example, suppose that a company’s network has
    a faulty server, such that accessing the server can cause the network to crash.
    Suppose the network crashes, and you’re tasked with analyzing the logs to find
    the root cause. You find that your colleague Lazlo has accessed the faulty server.
    Is Lazlo to blame?
  prefs: []
  type: TYPE_NORMAL
- en: To answer that, you might quantify the chances that Lazlo was a sufficient cause
    of the crash; i.e., the chance that Lazlo accessing the server was enough to tip
    the domino that ultimately led to the network to crash. Second, what are the chances
    that Lazlo was a necessary cause? For example, perhaps Lazlo wasn’t a necessary
    cause because if he hadn’t accessed the server, someone else would have eventually.
  prefs: []
  type: TYPE_NORMAL
- en: 'The probabilities of causation need to be combined with other elements to provide
    a complete view of attribution. One example is the concept of abnormality. The
    abnormality of a causal event describes whether that event, in some sense, violated
    expectations. For example, Lazlo might get more blame for crashing the network
    if it was highly unusual for employees to access that server. We can quantify
    the abnormality of a causal event with probability; if event *X*=1 was abnormal,
    then it was unlikely to have occurred, so we assign a low value to *P*(*X*=1).
    One attribution measure, called actual causal strength (ACS), combines abnormality
    with probabilities of causation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch8-eqs-3x.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, this approach views attribution as a trade-off between being
    an *abnormal* necessary cause and a *normal* sufficient cause.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a growing body of methods that combine attribution methods from
    the field of explainable AI (e.g., Shapley and SHAP values) with concepts of abnormality
    and causal concepts, such as the probabilities of causation. See the book notes
    at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for a list of references, including actual causal strength and explainable AI
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.3 Binary counterfactuals and uplift modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Statistical analysis of campaigns to influence human behavior is common in business,
    politics, and research. For instance, in our churn example, the goal of offering
    a promotion is to convince people not to churn. Similarly, businesses advertise
    to convince people to buy their products, and politicians reach out to voters
    to get them to vote or donate to a campaign.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the challenges of campaigns to influence behavior is identifying who
    is likely to respond favorably to your attempt to influence so you only spend
    your limited resources influencing those people. John Wanamaker, a pioneer of
    the field of marketing, put it best:'
  prefs: []
  type: TYPE_NORMAL
- en: Half the money I spend on advertising is wasted; the trouble is I don’t know
    which half.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Uplift modeling* refers to a class of statistical techniques that seek to
    answer this question with data. However, a data scientist approaching this problem
    space for the first time will find various statistical approaches, varying in
    terminology, presumptive data types, modeling assumptions, and modeling approaches,
    leading to confusion. Binary counterfactuals are quite useful in understanding
    the problem at a high level and how various solutions succeed or fail at addressing
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting users into persuadables, sure things, lost causes, and sleeping dogs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In our churn example, we can assume there are two kinds of subscribers. For
    some subscribers, a promotion will influence their decision to churn. Others are
    non-responders, meaning people for whom the promotion will have no influence.
    We can break up the latter non-responders into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Lost causes*—People who will churn regardless of whether they receive a promotion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sure things*—People who will stay regardless of whether they receive a promotion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of the people who do respond to the promotion, we have two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Persuadables*—Subscribers who could be persuaded by a promotion not to leave
    the service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sleeping dogs*—Subscribers who would not churn if you didn’t send a promotion,
    and people who would churn if you did'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sleeping dogs are named for the expression “let sleeping dogs lie” (last they
    wake up and bite you). These people will do what you want if you leave them alone,
    but they’ll behave against your wishes if you don’t. Have you ever received a
    marketing email from a subscription service and thought, “These people send me
    too much spam! I’m going to cancel.” You were a “sleeping dog”—the company’s email
    was the kick that woke you up, and you bit them for it. Figure 8.5 shows how our
    subscribers break down into these four segments.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F05_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 In attempts to influence behavior, we break down the target population
    into these four segments. Given limited resources, we want to target our influence
    efforts on the persuadables and avoid the others, especially the sleeping dogs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Promotions have a cost in terms of the additional value you give to the subscriber.
    You want to avoid spending that cost on subscribers who weren’t going to churn
    (sure things) and subscribers who were always going to churn (lost causes). And
    you definitely want to avoid spending that cost only to cause someone to churn
    (sleeping dogs). So, of these four groups, you want to send your promotions *only*
    to the persuadables. The task of statistical analysis is to segment our users
    into these four groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where counterfactuals can help us; we can define each segment in probabilistic
    counterfactual terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Lost causes*—People who probably would churn if we send a promotion and still
    churn if we did not send a promotion; i.e., *P*(*Y*[*X*][=1]=0, *Y*[*X*][=0]=0)
    is high.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sure things*—People who probably would stay if we send a promotion and stay
    if we did not send a promotion; i.e., *P*(*Y*[*X*][=1]=1, *Y*[*X*][=0]=1) is high.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Persuadables*—People who probably would stay if we send a promotion and churn
    if we did not send a promotion; i.e., *P*(*Y*[*X*][=1]=1, *Y*[*X*][=0]=0) is high.
    In other words, PNS is high.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sleeping dogs*—People who probably would churn if we send a promotion and
    would stay if we did not send a promotion; i.e., *P*(*Y*[*X*][=1]=0, *Y*[*X*][=0]=1)
    is high.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see, in figure 8.6, how the population can be segmented.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F06_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 We can segment the population in counterfactual terms.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using counterfactuals for segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each subscriber has some set of attributes (demographics, usage habits, content
    preferences, etc.). Our goal is to convert these attributes to predict whether
    a subscriber is a persuadable, sleeping dog, lost cause, or sure thing.
  prefs: []
  type: TYPE_NORMAL
- en: Let *C* represent a set of subscriber attributes. Given a subscriber with attributes
    *C*=*c*, our causal query of interest is *P*(*Y*[*X*][=1], *Y*[*X*][=0]|*C*=*c*).
    Various statistical segmentation methods seek to define *C* such that users fall
    into groups that have high probability for one of the four outcomes of *P*(*Y*[*X*][=1],
    *Y*[*X*][=0]|*C*=*c*), but before we apply the stats, our first task will be to
    ensure we can estimate this query using sufficient assumptions and data. We’ll
    cover how to estimate counterfactuals with SCMs in chapter 9 and how to use identification
    with broader estimation techniques in chapter 10.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned to pose our causal query and formalize it into math,
    let’s revisit the steps of making the counterfactual inference, in figure 8.7\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F07_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 The counterfactual inference workflow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the next section, we’ll study the idea of possible worlds and parallel world
    graphs. These ideas are important to both identification (determining whether
    we can answer the question) and the inference algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Possible worlds and parallel world graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, I’ll introduce the notion of possible worlds and parallel world
    graphs, an extension of a causal DAG for an SCM that supports counterfactual reasoning
    across possible worlds.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 Potential outcomes in possible worlds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Counterfactual reasoning involves reasoning over *possible worlds*. A possible
    world is a way the world is or could be. The *actual world* is the possible world
    with the event outcomes we observed. All other possible worlds are *hypothetical
    worlds*.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the data generating process (DGP), the actual world is how the DGP
    unrolled to produce our data. Other possible worlds are defined by all the ways
    the DGP could have produced different data.
  prefs: []
  type: TYPE_NORMAL
- en: '*Potential outcomes* are a fundamental concept in causal effect inference.
    “Potential outcomes” refers to outcomes of the same variable across differing
    possible worlds. If you have a headache and take an aspirin, you might say there
    are two potential outcomes in two possible worlds: one where your headache gets
    better and one where it doesn’t.'
  prefs: []
  type: TYPE_NORMAL
- en: Review of possible world terminology
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Possible world*—A way the world is or could be'
  prefs: []
  type: TYPE_NORMAL
- en: '*Actual world*—A possible world with observed event outcomes'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hypothetical world*—A possible world with no observed event outcomes'
  prefs: []
  type: TYPE_NORMAL
- en: '*Potential outcomes*—Outcomes of the same variable across differing possible
    worlds'
  prefs: []
  type: TYPE_NORMAL
- en: '*Parallel worlds*—A set of possible worlds being reasoned over, sharing both
    common and differing attributes'
  prefs: []
  type: TYPE_NORMAL
- en: '*Parallel world graph*—A graphical representation of parallel worlds used both
    for identifying counterfactual queries and in counterfactual inference algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 The parallel world graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A parallel world graph is a simple extension of a causal DAG that captures causality
    across possible worlds. Continuing with the online gaming example, suppose we
    are interested in the question, “For a player who had low engagement and less
    than $50 of in-game purchases, what would their level of in-game purchases be
    if their level of engagement was high?” I.e., *P*(*I*[*E*][=“high”]|*E*=“low”,
    *I*<50). For this counterfactual query, we can visualize both the actual and the
    hypothetical worlds in figure 8.8
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F08_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 To answer the counterfactual query for the online gaming example,
    we start by duplicating the causal DAG across possible worlds.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We duplicate the causal DAG for the online gaming example across both possible
    worlds. Having one DAG for each world reflects that the causal structure of the
    DGP is the same in each world. But we’ll need to connect these DAGs in some way
    to reason *across* worlds.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll connect the two worlds using an SCM defined on the causal DAG. We’ll suppose
    that the original nodes of the DAG are the endogenous variables of the SCM and
    expand the DAG visualization by adding the exogenous variables. Further, the two
    causal DAGs will use the same exogenous nodes. We call the resulting graph a *parallel
    world graph* (or, for this typical case of two possible worlds, a “twin-world
    graph”). Figure 8.9 visualizes the parallel world graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F09_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 In the parallel world graph, we use the exogenous variables in an
    SCM to unite the duplicate causal DAGs across worlds. The result is a single SCM
    with duplicate endogenous variables.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Refresher: The structural causal model (SCM)'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'An SCM is a causal model with the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Endogenous variables*—Endogenous variables are the variables we specifically
    want to model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exogenous variables*—A set of exogenous variables. Exogenous variables are
    proxies for all the causes of our endogenous variables we don’t wish to model
    explicitly. In our formulation, we pair each endogenous variable *X* with a single
    exogenous variable *N**[X]* (there are more general formulations).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exogenous distributions*—To use the SCM as a generative model, we need a set
    of marginal probability distributions for each exogenous variable, such as *P*(*N**[X]*),
    which represents the modeler’s uncertainty about the values *N**[X]*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Functional assignments*—Each endogenous variable has a functional assignment
    that sets its value deterministically, given its parents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, written as a generative model, an SCM for our online game model
    would look as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![sidebar figure](../Images/ness-ch8-eqs-2x.png)'
  prefs: []
  type: TYPE_IMG
- en: The assignment functions induce the causal DAG; each variable is a node, the
    exogenous variables are root nodes, and the inputs of a variable’s assignment
    function correspond to its parents in the DAG. The SCM is a particular case of
    a causal graphical model where endogenous variables are set by deterministic functions
    rather than sampled from causal Markov kernels.
  prefs: []
  type: TYPE_NORMAL
- en: The result is a single SCM with one shared set of exogenous variables and duplicate
    sets of endogenous variables—one set for each possible world. Note that in an
    SCM, the endogenous variables are set deterministically, given the exogenous variables.
    So upon observing that *E*=“low” and *I*<50 in the actual world, we know that
    the hypothetical outcomes of *E* and *I* must be the same. Indeed, even though
    *Guild Membership* (*G*) is a latent variable in the actual world, we know that
    whatever value *G* takes in the actual world must be the same as in the hypothetical
    world. In other words, our SCM upholds the consistency rule, as illustrated in
    figure 8.10\. In figure 8.10, the *E* and *I* in the actual world are observed
    variables because we condition on them in the query *P*(*I*[*E*][=“high”]|*E*=“low”,
    *I* < 50).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F10_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 In an SCM, the endogenous variables are set deterministically, given
    the exogenous variables. In this model, the endogenous variables are duplicated
    across worlds. Therefore, upon observing low engagement and less than $50 of in-game
    purchases in the actual world, we know that those values must be the same in the
    hypothetical world unless we change something in the hypothetical world.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 8.4.3 Applying the hypothetical condition via graph surgery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The hypothetical world will, typically, differ from the actual world by the
    hypothetical condition. For example, in *P*(*I*[*E*][=“high”]|*E*=“low”, *I* <
    50), “if engagement were high” ([*E*][=“high”]) differs from the factual condition
    “engagement was low” (*E*=“low”). As we’ve discussed, we model the hypothetical
    condition with the ideal intervention—we intervene on *E*, setting it to “high”
    in the hypothetical world. We model the ideal intervention on the graph with graph
    surgery—we’ll remove incoming edges to the *E* variable in the hypothetical world
    as in figure 8.11.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F11_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 The ideal intervention and graph surgery represents the hypothetical
    condition in the hypothetical world. In this setting, the outcome for *I* in the
    hypothetical world can now take a different outcome than it has in the actual
    world because its parent *E* has a different outcome than it has in the actual
    world.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now the outcome for *In-Game Purchases* (*I*) in the hypothetical world can
    take a different outcome than the actual world’s outcome of *I*=50 because its
    causal parent *E* has different outcomes in each world.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.4 Reasoning across more than two possible worlds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The counterfactual notation and the parallel worlds graph formalism support
    counterfactual reasoning that extends across more than two possible worlds. To
    illustrate, let’s refer back to the Netflix example at the beginning of the chapter.
    Summarizing the story, the key variables in that narrative are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Disney is trying to close a deal to buy the Bond franchise. Let *B* = “success”
    if the deal closes. Otherwise, *B* = “fail”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Netflix is trying to close a deal to start a new spy franchise called *Dead
    Drop*. *D* = “success” if the *Dead Drop* deal closes and “fail” otherwise. If
    the Bond deal closes, it will affect the terms of this deal. Therefore, *B* causes
    *D*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the *Dead Drop* deal closes, it will affect engagement in spy-thriller-related
    content on Netflix. Let *E* = “high” if a subscriber’s engagement in Netflix’s
    spy-thriller content is high and “low” otherwise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outcome of the Bond deal and the *Dead Drop* deal will both affect the attrition
    of spy-thriller fans to Disney. Let *A* be the rate of attrition to Disney.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this case study, the following multi-world counterfactual is plausible.
    Suppose the Bond deal was successful (*B* = “success”), but Netflix’s *Dead Drop*
    deal failed, and as a result, engagement was low (*E* = “low”) and Netflix attrition
    to Disney is 10 percent. Figure 8.12 illustrates this actual world outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a Netflix executive, you start wondering about attribution. You assume that
    engagement would have been high if the Dead Drop deal had been successful. You
    ask the following counterfactual question:'
  prefs: []
  type: TYPE_NORMAL
- en: Disney’s Bond deal succeeded, the Dead Drop deal failed, and as a result, Netflix’s
    spy thriller engagement was low, and attrition to Disney was 10%. I assume that
    had the Dead Drop deal been successful, engagement would have been high. In that
    case, how much attribution would there have been?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can implement this assumption with world 2 in the parallel world graph in
    figure 8.13\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F12_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 A causal DAG representing the Netflix case study. The light gray
    nodes are observed outcomes in the actual world. The dark nodes are latent variables.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F13_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 The second possible world represents the assumption that if the
    *Dead Drop* deal was successful (via intervention *D=*“success”) engagement would
    have been high (*E**[D]*[=“success”]=“high”).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Finally, you wonder what the level of Netflix attrition would be *if the Bond
    deal had failed*. But you wonder this based on your second-world assumption that
    engagement would be high if the *Dead Drop* deal had been successful. Since the
    Bond deal failing is a hypothetical condition that conflicts with the Bond deal
    success condition in the second world, you need a third world, as illustrated
    in figure 8.14.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F14_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 Given the actual outcomes in world 1, the hypothetical conditions
    and outcomes in world 2, you pose conditions in world 3 and reason about attrition
    in world 3.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In summary, this is the counterfactual question:'
  prefs: []
  type: TYPE_NORMAL
- en: Disney’s Bond deal succeeded, the *Dead Drop* deal failed, and as a result,
    Netflix’s spy thriller engagement was low, and attrition to Disney was 10%. I
    assume that had the *Dead Drop* deal been successful, engagement would have been
    high. In that case, how much attribution would there have been if the Bond deal
    had failed?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Note that the preceding reasoning is different from the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Disney’s Bond deal succeeded, the *Dead Drop* deal failed, and as a result,
    Netflix’s spy thriller engagement was low, and attrition to Disney was 10%. I
    assume that had the *Dead Drop* deal been successful *and the Bond deal failed*,
    engagement would have been high. In that case, how much attribution would there
    have been?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Figure 8.15 illustrates the latter question.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F15_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 In the case of assuming *E**[B]*[=“fail”,]*[D]*[=“success”], only
    two worlds are needed.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The latter question assumes engagement would be high if the Bond deal failed
    *and* the *Dead Drop* deal was successful (*E*[*B*][=“fail”,] [*D*][=“success”][=][“][high”]).
    In contrast, the former “three world” question assumes engagement would be high
    if both deals were successful. Then, in the third world, It allows for different
    possible levels of engagement in the hypothetical scenario where the Bond deal
    failed. For example, perhaps engagement would be high since Netflix would have
    its spy-thriller franchise and Disney wouldn't. Or perhaps, without a Bond reboot
    there would be less overall interest in spy-thrillers, resulting in low engagement
    in *Dead Drop*.
  prefs: []
  type: TYPE_NORMAL
- en: '8.4.5 Rule of thumb: Hypothetical worlds should be simpler'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider again the endogenous nodes in our online gaming example in figure 8.16\.
    Notice that, in this example, the two worlds have the same sets of endogenous
    nodes, and the edges in the hypothetical world are a subset of the edges of those
    in the actual world. In other words, the possible world where we do intervene
    is simpler than the possible world where we condition on evidence.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F16_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 The graph representing the possible world with the hypothetical
    conditional is simpler than the graph representing the actual world.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similarly, in the three-world graph for the Netflix case study, world 3 is a
    subgraph of world 2, which is a subgraph of world 1\. As an algorithmic rule of
    thumb, it is useful to have this descending ordering on possible worlds. This
    rule of thumb reduces the risk of algorithmic instability.
  prefs: []
  type: TYPE_NORMAL
- en: That said, there are use cases for having more complicated hypothetical worlds.
    For example, a modeler could introduce new nodes as conditions in the hypothetical
    world. Or they could use stochastic interventions that randomly introduce new
    edges in the hypothetical world. Indeed, human counterfactual reasoning can be
    quite imaginative. Exploring such approaches could lead to interesting new algorithms
    for causal AI.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll dive into using parallel world graphs in an algorithm
    for general counterfactual inference.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Counterfactual statements describe hypothetical events that potentially conflict
    with actual events. They are fundamental to defining causality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counterfactual reasoning supports learning policies for better decision-making.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counterfactual reasoning involves reasoning over possible worlds. A *possible
    world* is a way the world is or could be. The *actual world* is a possible world
    with event outcomes we observed. Other possible worlds are hypothetical worlds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In machine learning, often the goal is counterfactual analysis of a machine
    learning model itself. Here, we reason about how a prediction would have been
    different if elements of the input feature vector were different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counterfactual analysis in classification can help find the minimal change in
    features that would have led to a different classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counterfactual analysis supports explainable AI by helping identify changes
    to features that would have changed the prediction outcome on a case-by-case basis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counterfactual analysis supports algorithmic recourse by identifying *actionable*
    changes to features that would change the prediction outcome.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counterfactual analysis supports AI fairness by identifying features corresponding
    to protected attributes where changes to said features would change the prediction
    outcome.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Potential outcomes” is a commonly used term that refers to outcomes for a given
    variable from across possible worlds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use the ideal intervention and parallel world graphs to model hypothetical
    conditions in natural language counterfactual statements and questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counterfactual notation helps represent hypothetical statements and questions
    in the language of probability. Probability can be used to quantify uncertainty
    about the truth of hypothetical statements and questions, including counterfactuals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using hypothetical language rather than declarative language helps with formalizing
    a counterfactual statement or question into counterfactual notation. Using hypothetical
    language implies imagined possibility, and thus uncertainty, which invites us
    to think about the probability of a hypothetical statement being true.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary counterfactual queries refer to queries on variables (hypothetical conditions
    and outcomes) that are binary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *probabilities of causation,* such as the *probability of necessity* (PN),
    *probability of sufficiency* (PS), and *probability of necessity and sufficiency*
    (PNS), are binary counterfactual queries that are useful as primitives in causal
    attribution methods and other types of advanced causal queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary counterfactual queries are also useful for distinguishing between “persuadables,”
    “sure things,” “lost causes,” and “sleeping dogs” in uplift modeling problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A parallel world graph is a simple extension of a causal DAG that captures causality
    across possible worlds. It represents an SCM over possible worlds that share a
    common set of exogenous variables and duplicate sets of endogenous variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
