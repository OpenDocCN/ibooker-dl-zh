- en: Chapter 4\. Reusable Model Elements
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。可重用的模型元素
- en: 'Developing an ML model can be a daunting task. Besides the data engineering
    aspect of the task, you also need to understand how to build the model. In the
    early days of ML, tree-based models (such as random forests) were king for applying
    straight-up classification or regression tasks to tabular datasets, and model
    architecture was determined by parameters related to model initialization. These
    parameters, known as hyperparameters, include the number of decision trees in
    a forest and the number of features considered by each tree when splitting a node.
    However, it is not straightforward to convert some types of data, such as images
    or text, into tabular form: images may have different dimensions, and texts vary
    in length. That’s why deep learning has become the de facto standard model architecture
    for image and text classification.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 开发一个ML模型可能是一项艰巨的任务。除了任务的数据工程方面，您还需要了解如何构建模型。在ML的早期阶段，基于树的模型（如随机森林）是将直接应用于表格数据集的分类或回归任务的王者，模型架构是由与模型初始化相关的参数确定的。这些参数，称为超参数，包括森林中的决策树数量以及在拆分节点时每棵树考虑的特征数量。然而，将某些类型的数据，如图像或文本，转换为表格形式并不是直截了当的：图像可能具有不同的尺寸，文本长度也不同。这就是为什么深度学习已经成为图像和文本分类的事实标准模型架构的原因。
- en: As deep-learning architecture gains popularity, a community has grown around
    it. Creators have built and tested different model structures for academic and
    Kaggle challenges. Many have made their models open source so that they are available
    for transfer learning—anyone can use them for their own purposes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习架构的流行，围绕它形成了一个社区。创建者为学术和Kaggle挑战构建和测试了不同的模型结构。许多人已经将他们的模型开源，以便进行迁移学习-任何人都可以将它们用于自己的目的。
- en: For example, ResNet is an image classification model trained on the ImageNet
    dataset, which is about 150GB in size and contains more than a million images.
    The labels in this data include plants, geological formations, natural objects,
    sports, persons, and animals. So how can you reuse the ResNet model to classify
    your own set of images, even with different categories or labels?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，ResNet是在ImageNet数据集上训练的图像分类模型，该数据集约为150GB，包含超过一百万张图像。这些数据中的标签包括植物、地质形态、自然物体、体育、人物和动物。那么您如何重用ResNet模型来对您自己的图像集进行分类，即使具有不同的类别或标签？
- en: Open source models such as ResNet have very complicated structures. While the
    source code is available for anyone to access on sites like GitHub, downloading
    the source code is not the most user-friendly way to reproduce or reuse these
    models. There are almost always other dependencies that you have to overcome to
    compile or run the source code. So how can we make such models available and usable
    to nonexperts?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 像ResNet这样的开源模型具有非常复杂的结构。虽然源代码可以在GitHub等网站上供任何人访问，但下载源代码并不是复制或重用这些模型的最用户友好的方式。通常还有其他依赖项需要克服才能编译或运行源代码。那么我们如何使这些模型对非专家可用和可用？
- en: TensorFlow Hub (TFH) is designed to solve this problem. It enables transfer
    learning by making a variety of ML models freely available as libraries or web
    API calls. Anyone can write just a single line of code to load the model. All
    models can be invoked via a simple web call, and then the entire model is downloaded
    to your source code’s runtime. You don’t need to build the model yourself.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Hub（TFH）旨在解决这个问题。它通过将各种ML模型作为库或Web API调用免费提供，从而实现迁移学习。任何人都可以写一行代码来加载模型。所有模型都可以通过简单的Web调用调用，然后整个模型将下载到您的源代码运行时。您不需要自己构建模型。
- en: This definitely saves development and training time and increases accessibility.
    It also allows users to try out different models and build their own applications
    more quickly. Another benefit of transfer learning is that since you are not retraining
    the whole model from scratch, you may not need a high-powered GPU or TPU to get
    started.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝对节省了开发和训练时间，并增加了可访问性。它还允许用户尝试不同的模型并更快地构建自己的应用程序。迁移学习的另一个好处是，由于您不是从头开始重新训练整个模型，因此您可能不需要高性能的GPU或TPU即可开始。
- en: In this chapter, we are going to take a look at just how easy it is to leverage
    TensorFlow Hub. So let’s start with how TFH is organized. Then you’ll download
    one of the TFH pretrained image classification models and see how to use it for
    your own images.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看一看如何轻松利用TensorFlow Hub。所以让我们从TFH的组织方式开始。然后您将下载TFH预训练的图像分类模型之一，并看看如何将其用于您自己的图像。
- en: The Basic TensorFlow Hub Workflow
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的TensorFlow Hub工作流程
- en: '[TensorFlow Hub](https://oreil.ly/dQxxy) ([Figure 4-1](#tensorflow_hub_home_page))
    is a repository of pretrained models curated by Google. Users may download any
    model into their own runtime and perform fine-tuning and training with their own
    data.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[TensorFlow Hub](https://oreil.ly/dQxxy)（[图4-1](#tensorflow_hub_home_page)）是由Google策划的预训练模型的存储库。用户可以将任何模型下载到自己的运行时，并使用自己的数据进行微调和训练。'
- en: '![TensorFlow Hub home page](Images/t2pr_0401.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![TensorFlow Hub主页](Images/t2pr_0401.png)'
- en: Figure 4-1\. TensorFlow Hub home page
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1\. TensorFlow Hub主页
- en: 'To use TFH, you must install it via the familiar Pythonic `pip install` command
    in your Python cell or terminal:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用TFH，您必须通过熟悉的Pythonic `pip install`命令在您的Python单元格或终端中安装它：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then you can start using it in your source code by importing it:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您可以通过导入它在您的源代码中开始使用它：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'First, invoke the model:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，调用模型：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is a pretrained text embedding model. *Text embedding* is the process
    of mapping a string of text to a multidimensional vector of numeric representation.
    You can give this model four text strings:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个预训练的文本嵌入模型。*文本嵌入*是将文本字符串映射到数字表示的多维向量的过程。您可以给这个模型四个文本字符串：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Before you look at the results, inspect the shape of the model output:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看结果之前，请检查模型输出的形状：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It should be:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 应该是：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'There are four outputs, each 128 units long. [Figure 4-2](#text_embedding_output)
    shows one of the outputs:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个输出，每个输出长度为128个单位。[图4-2](#text_embedding_output)显示其中一个输出：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Text embedding output](Images/t2pr_0402.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![文本嵌入输出](Images/t2pr_0402.png)'
- en: Figure 4-2\. Text embedding output
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2. 文本嵌入输出
- en: As indicated in this simple example, you did not train this model. You only
    loaded it and used it to get a result with your own data. This pretrained model
    simply converts each text string into a vector representation of 128 dimensions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在这个简单示例中所示，您没有训练这个模型。您只是加载它并用它来处理您自己的数据。这个预训练模型简单地将每个文本字符串转换为一个128维的向量表示。
- en: 'On the TensorFlow Hub home page, click the Models tab. As you can see, TensorFlow
    Hub categorizes its pretrained models into four problem domains: image, text,
    video, and audio.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow Hub首页，点击“Models”选项卡。如您所见，TensorFlow Hub将其预训练模型分类为四个问题领域：图像、文本、视频和音频。
- en: '[Figure 4-3](#general_pattern_for_transfer_learning) shows the general pattern
    for a transfer learning model.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-3](#general_pattern_for_transfer_learning)展示了迁移学习模型的一般模式。'
- en: From [Figure 4-3](#general_pattern_for_transfer_learning), you can see that
    the pretrained model (from TensorFlow Hub) is sandwiched between an input layer
    and an output layer, and there can be some optional layers prior to the output
    layer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图4-3](#general_pattern_for_transfer_learning)中，您可以看到预训练模型（来自TensorFlow Hub）被夹在输入层和输出层之间，输出层之前可能还有一些可选层。
- en: '![General pattern for transfer learning](Images/t2pr_0403.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![迁移学习的一般模式](Images/t2pr_0403.png)'
- en: Figure 4-3\. General pattern for transfer learning
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3. 迁移学习的一般模式
- en: 'To use any of the models, you’ll need to address a few important considerations,
    such as input and output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用任何模型，您需要解决一些重要的考虑因素，例如输入和输出：
- en: Input layer
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层
- en: 'Input data must be properly formatted (or “shaped”), so pay special attention
    to each model’s input requirements (found in the Usage section on the web page
    that describes the individual model). Take the [ResNet feature vector](https://oreil.ly/6xGeP),
    for example: the Usage section states the required size and color values for the
    input images and that the output is a batch of feature vectors. If your data does
    not meet the requirements, you’ll need to apply some of the data transformation
    techniques you learned in [“Preparing Image Data for Processing”](ch03.xhtml#preparing_image_data_for_processing).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据必须被正确格式化（或“塑造”），因此请特别注意每个模型的输入要求（在描述各个模型的网页上的“Usage”部分中找到）。以[ResNet特征向量](https://oreil.ly/6xGeP)为例：Usage部分说明了输入图像所需的大小和颜色值，以及输出是一批特征向量。如果您的数据不符合要求，您需要应用一些数据转换技术，这些技术可以在[“为处理准备图像数据”](ch03.xhtml#preparing_image_data_for_processing)中学到。
- en: Output layer
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层
- en: Another important and necessary element is the output layer. This is a must
    if you wish to retrain the model with your own data. In the simple embedding example
    shown earlier, we didn’t retrain the model; we merely fed it a few text strings
    to see the model output. An output layer serves the purpose of mapping the model
    output to the most likely labels if the problem is a classification problem. If
    it is a regression problem, then it serves to map the model output to a numeric
    value. A typical output layer is called “dense,” with either one node (for regression
    or binary classification) or multiple nodes (such as for multiclass classification).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要且必要的元素是输出层。如果您希望使用自己的数据重新训练模型，这是必须的。在之前展示的简单嵌入示例中，我们没有重新训练模型；我们只是输入了一些文本字符串来查看模型的输出。输出层的作用是将模型的输出映射到最可能的标签，如果问题是分类问题的话。如果是回归问题，那么它的作用是将模型的输出映射到一个数值。典型的输出层称为“密集层”，可以是一个节点（用于回归或二元分类）或多个节点（例如用于多类分类）。
- en: Optional layers
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可选层
- en: Optionally, you can add one or more layers before the output layer to improve
    model performance. These layers may help you extract more features to improve
    model accuracy, such as a convolution layer (Conv1D, Conv2D). They can also help
    prevent or reduce model overfitting. For example, dropout reduces overfitting
    by randomly setting an output to zero. If a node outputs an array such as [0.5,
    0.1, 2.1, 0.9] and you set a dropout ratio of 0.25, then during training, by random
    chance, one of the four values in the array will be set to zero; for example,
    [0.5, 0, 2.1, 0.9]. Again, this is considered optional. Your training does not
    require it, but it may help improve your model’s accuracy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，您可以在输出层之前添加一个或多个层以提高模型性能。这些层可以帮助您提取更多特征以提高模型准确性，例如卷积层（Conv1D、Conv2D）。它们还可以帮助防止或减少模型过拟合。例如，通过随机将输出设置为零，dropout可以减少过拟合。如果一个节点输出一个数组，例如[0.5,
    0.1, 2.1, 0.9]，并且您设置了0.25的dropout比率，那么在训练过程中，根据随机机会，数组中的四个值中的一个将被设置为零；例如，[0.5,
    0, 2.1, 0.9]。再次强调，这是可选的。您的训练不需要它，但它可能有助于提高模型的准确性。
- en: Image Classification by Transfer Learning
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过迁移学习进行图像分类
- en: 'We are going to walk through an image classification example with transfer
    learning. In this example, your image data consists of five classes of flowers.
    You will use the ResNet feature vector as your pretrained model. We will address
    these common tasks:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个使用迁移学习的图像分类示例来进行讲解。在这个示例中，您的图像数据包括五类花。您将使用ResNet特征向量作为预训练模型。我们将解决以下常见任务：
- en: Model requirements
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型要求
- en: Data transformation and input processing
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换和输入处理
- en: Model implementation with TFH
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TFH模型实现
- en: Output definition
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出定义
- en: Mapping output to plain-text format
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输出映射到纯文本格式
- en: Model Requirements
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型要求
- en: Let’s look at the [ResNet v1_101 feature vector](https://oreil.ly/70grM) model.
    This web page contains an overview, a download URL, instructions, and, most importantly,
    the code you’ll need to use the model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看[ResNet v1_101特征向量](https://oreil.ly/70grM)模型。这个网页包含了一个概述、一个下载URL、说明以及最重要的是您需要使用该模型的代码。
- en: In the Usage section, you can see that to load the model, all you need to do
    is pass the URL to `hub.KerasLayer`. The Usage section also includes the model
    requirements. By default, it expects the input image, which is written as an array
    of shape [height, width, depth], to be [224, 224, 3]. The pixel value is expected
    to be within the range [0, 1]. As the output, it provides the `Dense` layer with
    the number of nodes, which reflects the number of classes in the training images.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用部分中，您可以看到要加载模型，您只需要将URL传递给`hub.KerasLayer`。使用部分还包括模型要求。默认情况下，它期望输入图像，写为形状数组[高度，宽度，深度]，为[224,
    224, 3]。像素值应在范围[0, 1]内。作为输出，它提供了具有节点数的`Dense`层，反映了训练图像中类别的数量。
- en: Data Transformation and Input Processing
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换和输入处理
- en: 'It is your job to transform your images into the required shape and normalize
    the pixel scale to within the required range. As we’ve seen, images usually come
    in different size and pixel values. A typical color JPEG image pixel value for
    each RGB channel might be anywhere from 0 to 225\. So, we need operations to standardize
    image size to [224, 224, 3], and to normalize pixel value to a [0, 1] range. If
    we use `ImageDataGenerator` in TensorFlow, these operations are provided as input
    flags. Here’s how to load the images and create a generator:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您的任务是将图像转换为所需的形状，并将像素比例标准化到所需范围内。正如我们所见，图像通常具有不同的大小和像素值。每个RGB通道的典型彩色JPEG图像像素值可能在0到225之间。因此，我们需要操作来将图像大小标准化为[224,
    224, 3]，并将像素值标准化为[0, 1]范围。如果我们在TensorFlow中使用`ImageDataGenerator`，这些操作将作为输入标志提供。以下是如何加载图像并创建生成器：
- en: 'Start by loading the libraries:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先加载库：
- en: '[PRE7]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Load the data you need. For this example, let’s use the flower images provided
    by TensorFlow:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的数据。在这个例子中，让我们使用TensorFlow提供的花卉图像：
- en: '[PRE8]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Open `data_dir` and find the images. You can see the file structure in the
    file path:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`data_dir`并找到图像。您可以在文件路径中看到文件结构：
- en: '[PRE9]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is what will display:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是将显示的内容：
- en: '[PRE10]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There are five classes of flowers. Each class corresponds to a directory.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有五类花卉。每个类对应一个目录。
- en: 'Define some global variables to store pixel values and *batch size* (the number
    of samples in a batch of training images). You don’t yet need the third dimension
    of the image, just the height and width for now:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一些全局变量来存储像素值和*批量大小*（训练图像批次中的样本数）。目前您只需要图像的高度和宽度，不需要图像的第三个维度：
- en: '[PRE11]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Specify image normalization and a fraction of data for cross validation. It
    is a good idea to hold out a fraction of training data for cross validation, which
    is a means of evaluating the model training process through each epoch. At the
    end of each training epoch, the model contains a set of trained weights and biases.
    At this point, the data held out for cross validation, which the model has never
    seen, can be used as a test for model accuracy:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定图像标准化和用于交叉验证的数据分数。将一部分训练数据保留用于交叉验证是一个好主意，这是通过每个时代评估模型训练过程的一种方法。在每个训练时代结束时，模型包含一组经过训练的权重和偏差。此时，用于交叉验证的数据，模型从未见过，可以用作模型准确性的测试：
- en: '[PRE12]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `ImageDataGenerator` definition and generator instance both accept our arguments
    in a dictionary format. The rescaling factor and validation fraction go to the
    generator definition, while the standardized image size and batch size go to the
    generator instance.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`ImageDataGenerator`定义和生成器实例都以字典格式接受我们的参数。重新缩放因子和验证分数进入生成器定义，而标准化图像大小和批量大小进入生成器实例。'
- en: The `interpolation` argument indicates that the generator needs to resample
    the image data to `target_size`, which is 224 × 224 pixels.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`插值`参数表示生成器需要将图像数据重新采样到`target_size`，即224×224像素。'
- en: 'Now, do the same for the training data generator:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，对训练数据生成器执行相同操作：
- en: '[PRE13]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Identify mapping of class index to class name. Since the flower classes are
    encoded in the index, you need a map to recover the flower class names:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别类索引到类名的映射。由于花卉类别被编码在索引中，您需要一个映射来恢复花卉类别名称：
- en: '[PRE14]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can display the `idx_labels` to see how these classes are mapped:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以显示`idx_labels`以查看这些类是如何映射的：
- en: '[PRE15]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You’ve now normalized and standardized your image data. The image generators
    are defined and instantiated for training and validation data. You also have the
    label lookup to decode model prediction, and you’re ready to implement the model
    with TFH.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经对图像数据进行了标准化和标准化。图像生成器已被定义并实例化用于训练和验证数据。您还具有标签查找来解码模型预测，并且已准备好使用TFH实现模型。
- en: Model Implementation with TensorFlow Hub
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow Hub实现模型
- en: 'As you saw back in [Figure 4-3](#general_pattern_for_transfer_learning), the
    pretrained model is sandwiched between an input and an output layer. You can define
    this model structure accordingly:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[图4-3](#general_pattern_for_transfer_learning)中看到的，预训练模型被夹在输入层和输出层之间。您可以相应地定义这个模型结构：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Notice a few things here:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里有几点：
- en: There is an input layer that defines the input shape of images as [224, 224,
    3].
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个输入层，定义图像的输入形状为[224, 224, 3]。
- en: When `InputLayer` is invoked, `trainable` should be set to False. This indicates
    that you want to reuse the current values from the pretrained model.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当调用`InputLayer`时，`trainable`应设置为False。这表示您希望重用预训练模型的当前值。
- en: There is an output layer called `Dense` that provides the model output (this
    is described in the Usage section of the summary page).
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个名为`Dense`的输出层提供模型输出（这在摘要页面的使用部分中有描述）。
- en: 'After the model is built, you’re ready to start training. First, specify the
    loss function and pick an optimizer:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型后，您可以开始训练。首先，指定损失函数并选择优化器：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then specify the number of batches for training data and cross-validation data:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后指定用于训练数据和交叉验证数据的批次数：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then start the training process:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后开始训练过程：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After the training process runs through all the epochs specified, the model
    is trained.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过指定的所有时代运行训练过程后，模型已经训练完成。
- en: Defining the Output
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义输出
- en: 'According to the Usage guideline, the output layer `Dense` consists of a number
    of nodes, which reflects how many classes are in the expected images. This means
    each node outputs a probability for that class. It is your job to find which one
    of these probabilities is the highest and map that node to the flower class using
    `idx_labels.` Recall that the `idx_labels` dictionary looks like this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用指南，输出层`Dense`由一定数量的节点组成，反映了预期图像中有多少类别。这意味着每个节点为该类别输出一个概率。您的任务是找到这些概率中哪一个最高，并使用`idx_labels`将该节点映射到花卉类别。回想一下，`idx_labels`字典如下所示：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `Dense` layer’s output consists of five nodes in the exact same order. You’ll
    need to write a few lines of code to map the position with the highest probability
    to the corresponding flower class.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dense`层的输出由五个节点以完全相同的顺序组成。您需要编写几行代码将具有最高概率的位置映射到相应的花卉类别。'
- en: Mapping Output to Plain-Text Format
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将输出映射到纯文本格式
- en: 'Let’s use the validation images to understand a bit more about how to map the
    model prediction output to the actual class for each image. You’ll use the `predict`
    function to score these validation images. Retrieve the NumPy array for the first
    batch:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用验证图像来更好地了解如何将模型预测输出映射到每个图像的实际类别。您将使用`predict`函数对这些验证图像进行评分。检索第一批次的NumPy数组：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'There are 731 images and 5 corresponding classes in the cross-validation data.
    Therefore, the output shape is [731, 5]:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证数据中有731张图像和5个对应的类别。因此，输出形状为[731, 5]：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Each row represents the probability distribution for the image class. For the
    first image, the highest probability, 1.0701914e-08 (highlighted in the preceding
    code), is in the last position, which corresponds to index 4 of that row (remember,
    the numbering of an index starts with 0).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 每行代表了图像类别的概率分布。对于第一张图像，最高概率为1.0701914e-08（在上述代码中突出显示），位于该行的最后位置，对应于该行的索引4（请记住，索引的编号从0开始）。
- en: 'Now you need to find the position where the highest probability occurs for
    each row, using this code:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您需要使用以下代码找到每行中最高概率出现的位置：
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And if you display the results with the `print` command, you’ll see this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用`print`命令显示结果，您将看到以下内容：
- en: '[PRE24]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, apply the lookup with `idx_labels` to each element in this array. For
    each element, use a function:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对该数组中的每个元素应用`idx_labels`的查找。对于每个元素，使用一个函数：
- en: '[PRE25]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To apply a function to each element of a NumPy array, you’ll need to vectorize
    the function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要将函数应用于NumPy数组的每个元素，您需要对函数进行矢量化：
- en: '[PRE26]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then apply this vectorized function to each element in the array:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将此矢量化函数应用于数组中的每个元素：
- en: '[PRE27]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, output the result side by side with the image folder and filename
    so that you can save it for reporting or further investigation. You can do this
    with Python pandas DataFrame manipulation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将结果与图像文件夹和文件名并排输出，以便保存以供报告或进一步调查。您可以使用Python pandas DataFrame操作来实现这一点：
- en: '[PRE28]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Let’s take a look at the `results` dataframe, which is 731 rows × 2 columns.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`results`数据框，它是731行×2列。
- en: '|   | File | Prediction |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|   | 文件 | 预测 |'
- en: '| --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | daisy/100080576_f52e8ee070_n.jpg | daisy |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 0 | daisy/100080576_f52e8ee070_n.jpg | daisy | 雏菊 |'
- en: '| 1 | daisy/10140303196_b88d3d6cec.jpg | sunflowers |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 1 | daisy/10140303196_b88d3d6cec.jpg | sunflowers | 向日葵 |'
- en: '| 2 | daisy/10172379554_b296050f82_n.jpg | daisy |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 2 | daisy/10172379554_b296050f82_n.jpg | daisy | 雏菊 |'
- en: '| 3 | daisy/10172567486_2748826a8b.jpg | dandelion |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 3 | daisy/10172567486_2748826a8b.jpg | dandelion | 玛丽金花 |'
- en: '| 4 | daisy/10172636503_21bededa75_n.jpg | daisy |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 4 | daisy/10172636503_21bededa75_n.jpg | daisy | 雏菊 |'
- en: '| ... | ... | ... |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... |'
- en: '| 726 | tulips/14068200854_5c13668df9_m.jpg | sunflowers |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 726 | tulips/14068200854_5c13668df9_m.jpg | sunflowers | 向日葵 |'
- en: '| 727 | tulips/14068295074_cd8b85bffa.jpg | roses |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 727 | tulips/14068295074_cd8b85bffa.jpg | roses | 玫瑰 |'
- en: '| 728 | tulips/14068348874_7b36c99f6a.jpg | dandelion |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 728 | tulips/14068348874_7b36c99f6a.jpg | dandelion | 郁金香 |'
- en: '| 729 | tulips/14068378204_7b26baa30d_n.jpg | tulips |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 729 | tulips/14068378204_7b26baa30d_n.jpg | tulips | 郁金香 |'
- en: '| 730 | tulips/14071516088_b526946e17_n.jpg | dandelion |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 730 | tulips/14071516088_b526946e17_n.jpg | dandelion | 玛丽金花 |'
- en: 'Evaluation: Creating a Confusion Matrix'
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估：创建混淆矩阵
- en: A *confusion matrix*, which evaluates the classification results by comparing
    model output with ground truth, is the easiest way to get an initial feel for
    how well the model performs. Let’s look at how to create a confusion matrix.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵通过比较模型输出和实际情况来评估分类结果，是了解模型表现的最简单方法。让我们看看如何创建混淆矩阵。
- en: 'You’ll use pandas Series as the data structure for building your confusion
    matrix:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用pandas Series作为构建混淆矩阵的数据结构：
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then you’ll utilize pandas again to produce the matrix:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将再次利用pandas生成矩阵：
- en: '[PRE30]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[Figure 4-4](#confusion_matrix_for_flower_image_classi) shows the confusion
    matrix. Each row represents the distribution of actual flower labels by predictions.
    For example, looking at the first row, you will notice that there are a total
    of 126 samples that are actually class 0, which is daisy. The model correctly
    predicted 118 of these images as class 0; four are misclassified as class 1, which
    is dandelion; one is misclassified as class 2, which is roses; three are misclassified
    as class 3, which is sunflowers; and none has been misclassified as class 4, which
    is tulips.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-4](#confusion_matrix_for_flower_image_classi)显示了混淆矩阵。每行代表了实际花标签的分布情况。例如，看第一行，您会注意到总共有126个样本实际上是类别0，即雏菊。模型正确地将这些图像中的118个预测为类别0；四个被错误分类为类别1，即蒲公英；一个被错误分类为类别2，即玫瑰；三个被错误分类为类别3，即向日葵；没有被错误分类为类别4，即郁金香。'
- en: '![Confusion matrix for flower image classification](Images/t2pr_0404.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![花卉图像分类的混淆矩阵](Images/t2pr_0404.png)'
- en: Figure 4-4\. Confusion matrix for flower image classification
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4\. 花卉图像分类的混淆矩阵
- en: 'Next, use the `sklearn` library to provide a statistical report for each class
    of images:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用`sklearn`库为每个图像类别提供统计报告：
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This result shows that the model has the best performance when classifying daisies
    (class 0), with an f1-score of 0.92\. Its performance is worst in classifying
    roses (class 2), with an f1-score of 0.85\. The “support” column indicates the
    sample size in each class.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果表明，当对雏菊（类别 0）进行分类时，该模型的性能最佳，f1 分数为 0.92。在对玫瑰（类别 2）进行分类时，其性能最差，f1 分数为 0.85。“支持”列显示了每个类别中的样本量。
- en: Summary
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: You have just completed an example project using a pretrained model from TensorFlow
    Hub. You appended the necessary input layer, performed data normalization and
    standardization, trained the model, and scored a batch of images.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您刚刚完成了一个使用来自 TensorFlow Hub 的预训练模型的示例项目。您添加了必要的输入层，执行了数据归一化和标准化，训练了模型，并对一批图像进行了评分。
- en: This experience shows the importance of meeting the model’s input and output
    requirements. Just as importantly, pay close attention to the output format of
    the pretrained model. (This information is all available in the model documentation
    page on the TensorFlow Hub website.) Finally, you also need to create a function
    that maps the model output to plain text to make it meaningful and interpretable.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个经验表明了满足模型的输入和输出要求的重要性。同样重要的是，要密切关注预训练模型的输出格式。（这些信息都可以在 TensorFlow Hub 网站上的模型文档页面找到。）最后，您还需要创建一个函数，将模型的输出映射到纯文本，以使其具有意义并可解释。
- en: Using the tf.keras.applications Module for Pretrained Models
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 tf.keras.applications 模块进行预训练模型
- en: Another place to find a pretrained model for your own use is the `tf.keras.applications`
    module (see the [list of available models](https://oreil.ly/HQJBl)). When the
    Keras API became available in TensorFlow, this module became a part of the TensorFlow
    ecosystem.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个为您自己使用找到预训练模型的地方是 `tf.keras.applications` 模块（请参阅[可用模型列表](https://oreil.ly/HQJBl)）。当
    Keras API 在 TensorFlow 中可用时，该模块成为 TensorFlow 生态系统的一部分。
- en: Each model comes with pretrained weights, and using them is just as easy as
    using TensorFlow Hub. Keras provides the flexibility needed to conveniently fine-tune
    your models. By making each layer in a model accessible, `tf.keras.applications`
    lets you specify which layers to retrain and which layers to leave untouched.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都带有预训练的权重，使用它们和使用 TensorFlow Hub 一样简单。Keras 提供了方便地微调模型所需的灵活性。通过使模型中的每一层可访问，`tf.keras.applications`
    让您可以指定哪些层要重新训练，哪些层保持不变。
- en: Model Implementation with tf.keras.applications
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 tf.keras.applications 实现模型
- en: 'As with TensorFlow Hub, you need only one line of code to load a pretrained
    model from the Keras module:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 与 TensorFlow Hub 一样，您只需要一行代码从 Keras 模块加载一个预训练模型：
- en: '[PRE32]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Notice the `include_top` input argument. Remember that you need to add an output
    layer for your own data. By setting `include_top` to False, you can add your own
    `Dense` layer for the classification output. You’ll also initialize the model
    weights from `imagenet`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `include_top` 输入参数。请记住，您需要为自己的数据添加一个输出层。通过将 `include_top` 设置为 False，您可以为分类输出添加自己的
    `Dense` 层。您还将从 `imagenet` 初始化模型权重。
- en: 'Then place `base_model` inside a sequential architecture, as you did in the
    TensorFlow Hub example:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将 `base_model` 放入一个顺序架构中，就像您在 TensorFlow Hub 示例中所做的那样：
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Add `GlobalAveragePooling2D`, which averages the output array into one numeric
    value, to do an aggregation before sending it to the final `Dense` layer for prediction.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 添加 `GlobalAveragePooling2D`，将输出数组平均为一个数值，然后将其发送到最终的 `Dense` 层进行预测。
- en: 'Now compile the model and launch the training process as usual:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在编译模型并像往常一样启动训练过程：
- en: '[PRE34]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: To score image data, follow the same steps as you did in [“Mapping Output to
    Plain-Text Format”](#mapping_output_to_plain_text_format).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要对图像数据进行评分，请按照您在 [“将输出映射到纯文本格式”](#mapping_output_to_plain_text_format) 中所做的步骤进行。
- en: Fine-Tuning Models from tf.keras.applications
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 tf.keras.applications 微调模型
- en: 'If you wish to experiment with your training routine by releasing some layers
    of the base model for training, you can do so easily. To start, you need to find
    out exactly how many layers are in your base model and designate the base model
    as trainable:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望通过释放一些基础模型的层进行训练来尝试您的训练例程，您可以轻松地这样做。首先，您需要找出基础模型中有多少层，并将基础模型指定为可训练的：
- en: '[PRE35]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As indicated, in this version of the ResNet model, there are 377 layers. Usually
    we start the retraining process with layers close to the end of the model. In
    this case, designate layer 370 as the starting layer for fine-tuning, while holding
    the weights in layers before 300 untouched:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，在这个版本的 ResNet 模型中，有 377 层。通常我们从模型末尾附近的层开始重新训练过程。在这种情况下，将第 370 层指定为微调的起始层，同时保持在第
    300 层之前的权重不变：
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then put together the model with the `Sequential` class:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用 `Sequential` 类将模型组合起来：
- en: '[PRE37]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Tip
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can try `tf.keras.layers.Flatten()` instead of `tf.keras.layers.GlobalAveragePooling2D()`,
    and see which one gives you a better model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试使用 `tf.keras.layers.Flatten()` 而不是 `tf.keras.layers.GlobalAveragePooling2D()`，看看哪一个给您一个更好的模型。
- en: 'Compile the model, designating the optimizer and loss function as you did with
    TensorFlow Hub:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 编译模型，指定优化器和损失函数，就像您在 TensorFlow Hub 中所做的那样：
- en: '[PRE38]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Launch the training process:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 启动训练过程：
- en: '[PRE39]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This training may take considerably longer, since you’ve freed up more layers
    from the base model for retraining. Once the training is done, score the test
    data and compare the results as described in [“Mapping Output to Plain-Text Format”](#mapping_output_to_plain_text_format)
    and [“Evaluation: Creating a Confusion Matrix”](#evaluationcolon_creating_a_confusion_mat).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您已经释放了更多基础模型的层进行重新训练，这个训练可能需要更长时间。训练完成后，对测试数据进行评分，并按照 [“将输出映射到纯文本格式”](#mapping_output_to_plain_text_format)
    和 [“评估：创建混淆矩阵”](#evaluationcolon_creating_a_confusion_mat) 中描述的方式比较结果。
- en: Wrapping Up
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束
- en: 'In this chapter, you learned how to conduct transfer learning using pretrained,
    deep-learning models. There are two convenient ways to access pretrained models:
    TensorFlow Hub and the `tf.keras.applications` module. Both are simple to use
    and have elegant APIs and styles for quick model development. However, users are
    responsible for shaping their input data correctly and for providing a final `Dense`
    layer to handle model output.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，您学习了如何使用预训练的深度学习模型进行迁移学习。有两种方便的方式可以访问预训练模型：TensorFlow Hub 和 `tf.keras.applications`
    模块。两者都简单易用，具有优雅的 API 和风格，可以快速开发模型。然而，用户需要正确地塑造他们的输入数据，并提供一个最终的 `Dense` 层来处理模型输出。
- en: There are plenty of freely accessible pretrained models with abundant inventories
    that you can use to work with your own data. Taking advantage of them using transfer
    learning lets you spend less time building, training, and debugging models.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量免费可访问的预训练模型，具有丰富的库存，您可以使用它们来处理自己的数据。利用迁移学习来利用它们，让您花费更少的时间来构建、训练和调试模型。
