- en: Chapter 4\. Reusable Model Elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Developing an ML model can be a daunting task. Besides the data engineering
    aspect of the task, you also need to understand how to build the model. In the
    early days of ML, tree-based models (such as random forests) were king for applying
    straight-up classification or regression tasks to tabular datasets, and model
    architecture was determined by parameters related to model initialization. These
    parameters, known as hyperparameters, include the number of decision trees in
    a forest and the number of features considered by each tree when splitting a node.
    However, it is not straightforward to convert some types of data, such as images
    or text, into tabular form: images may have different dimensions, and texts vary
    in length. That’s why deep learning has become the de facto standard model architecture
    for image and text classification.'
  prefs: []
  type: TYPE_NORMAL
- en: As deep-learning architecture gains popularity, a community has grown around
    it. Creators have built and tested different model structures for academic and
    Kaggle challenges. Many have made their models open source so that they are available
    for transfer learning—anyone can use them for their own purposes.
  prefs: []
  type: TYPE_NORMAL
- en: For example, ResNet is an image classification model trained on the ImageNet
    dataset, which is about 150GB in size and contains more than a million images.
    The labels in this data include plants, geological formations, natural objects,
    sports, persons, and animals. So how can you reuse the ResNet model to classify
    your own set of images, even with different categories or labels?
  prefs: []
  type: TYPE_NORMAL
- en: Open source models such as ResNet have very complicated structures. While the
    source code is available for anyone to access on sites like GitHub, downloading
    the source code is not the most user-friendly way to reproduce or reuse these
    models. There are almost always other dependencies that you have to overcome to
    compile or run the source code. So how can we make such models available and usable
    to nonexperts?
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Hub (TFH) is designed to solve this problem. It enables transfer
    learning by making a variety of ML models freely available as libraries or web
    API calls. Anyone can write just a single line of code to load the model. All
    models can be invoked via a simple web call, and then the entire model is downloaded
    to your source code’s runtime. You don’t need to build the model yourself.
  prefs: []
  type: TYPE_NORMAL
- en: This definitely saves development and training time and increases accessibility.
    It also allows users to try out different models and build their own applications
    more quickly. Another benefit of transfer learning is that since you are not retraining
    the whole model from scratch, you may not need a high-powered GPU or TPU to get
    started.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to take a look at just how easy it is to leverage
    TensorFlow Hub. So let’s start with how TFH is organized. Then you’ll download
    one of the TFH pretrained image classification models and see how to use it for
    your own images.
  prefs: []
  type: TYPE_NORMAL
- en: The Basic TensorFlow Hub Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[TensorFlow Hub](https://oreil.ly/dQxxy) ([Figure 4-1](#tensorflow_hub_home_page))
    is a repository of pretrained models curated by Google. Users may download any
    model into their own runtime and perform fine-tuning and training with their own
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![TensorFlow Hub home page](Images/t2pr_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. TensorFlow Hub home page
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To use TFH, you must install it via the familiar Pythonic `pip install` command
    in your Python cell or terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can start using it in your source code by importing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'First, invoke the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a pretrained text embedding model. *Text embedding* is the process
    of mapping a string of text to a multidimensional vector of numeric representation.
    You can give this model four text strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Before you look at the results, inspect the shape of the model output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It should be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'There are four outputs, each 128 units long. [Figure 4-2](#text_embedding_output)
    shows one of the outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Text embedding output](Images/t2pr_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. Text embedding output
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As indicated in this simple example, you did not train this model. You only
    loaded it and used it to get a result with your own data. This pretrained model
    simply converts each text string into a vector representation of 128 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the TensorFlow Hub home page, click the Models tab. As you can see, TensorFlow
    Hub categorizes its pretrained models into four problem domains: image, text,
    video, and audio.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-3](#general_pattern_for_transfer_learning) shows the general pattern
    for a transfer learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: From [Figure 4-3](#general_pattern_for_transfer_learning), you can see that
    the pretrained model (from TensorFlow Hub) is sandwiched between an input layer
    and an output layer, and there can be some optional layers prior to the output
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![General pattern for transfer learning](Images/t2pr_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. General pattern for transfer learning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To use any of the models, you’ll need to address a few important considerations,
    such as input and output:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs: []
  type: TYPE_NORMAL
- en: 'Input data must be properly formatted (or “shaped”), so pay special attention
    to each model’s input requirements (found in the Usage section on the web page
    that describes the individual model). Take the [ResNet feature vector](https://oreil.ly/6xGeP),
    for example: the Usage section states the required size and color values for the
    input images and that the output is a batch of feature vectors. If your data does
    not meet the requirements, you’ll need to apply some of the data transformation
    techniques you learned in [“Preparing Image Data for Processing”](ch03.xhtml#preparing_image_data_for_processing).'
  prefs: []
  type: TYPE_NORMAL
- en: Output layer
  prefs: []
  type: TYPE_NORMAL
- en: Another important and necessary element is the output layer. This is a must
    if you wish to retrain the model with your own data. In the simple embedding example
    shown earlier, we didn’t retrain the model; we merely fed it a few text strings
    to see the model output. An output layer serves the purpose of mapping the model
    output to the most likely labels if the problem is a classification problem. If
    it is a regression problem, then it serves to map the model output to a numeric
    value. A typical output layer is called “dense,” with either one node (for regression
    or binary classification) or multiple nodes (such as for multiclass classification).
  prefs: []
  type: TYPE_NORMAL
- en: Optional layers
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, you can add one or more layers before the output layer to improve
    model performance. These layers may help you extract more features to improve
    model accuracy, such as a convolution layer (Conv1D, Conv2D). They can also help
    prevent or reduce model overfitting. For example, dropout reduces overfitting
    by randomly setting an output to zero. If a node outputs an array such as [0.5,
    0.1, 2.1, 0.9] and you set a dropout ratio of 0.25, then during training, by random
    chance, one of the four values in the array will be set to zero; for example,
    [0.5, 0, 2.1, 0.9]. Again, this is considered optional. Your training does not
    require it, but it may help improve your model’s accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Image Classification by Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to walk through an image classification example with transfer
    learning. In this example, your image data consists of five classes of flowers.
    You will use the ResNet feature vector as your pretrained model. We will address
    these common tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Model requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation and input processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model implementation with TFH
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output definition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapping output to plain-text format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at the [ResNet v1_101 feature vector](https://oreil.ly/70grM) model.
    This web page contains an overview, a download URL, instructions, and, most importantly,
    the code you’ll need to use the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the Usage section, you can see that to load the model, all you need to do
    is pass the URL to `hub.KerasLayer`. The Usage section also includes the model
    requirements. By default, it expects the input image, which is written as an array
    of shape [height, width, depth], to be [224, 224, 3]. The pixel value is expected
    to be within the range [0, 1]. As the output, it provides the `Dense` layer with
    the number of nodes, which reflects the number of classes in the training images.
  prefs: []
  type: TYPE_NORMAL
- en: Data Transformation and Input Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is your job to transform your images into the required shape and normalize
    the pixel scale to within the required range. As we’ve seen, images usually come
    in different size and pixel values. A typical color JPEG image pixel value for
    each RGB channel might be anywhere from 0 to 225\. So, we need operations to standardize
    image size to [224, 224, 3], and to normalize pixel value to a [0, 1] range. If
    we use `ImageDataGenerator` in TensorFlow, these operations are provided as input
    flags. Here’s how to load the images and create a generator:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by loading the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data you need. For this example, let’s use the flower images provided
    by TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open `data_dir` and find the images. You can see the file structure in the
    file path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is what will display:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are five classes of flowers. Each class corresponds to a directory.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define some global variables to store pixel values and *batch size* (the number
    of samples in a batch of training images). You don’t yet need the third dimension
    of the image, just the height and width for now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify image normalization and a fraction of data for cross validation. It
    is a good idea to hold out a fraction of training data for cross validation, which
    is a means of evaluating the model training process through each epoch. At the
    end of each training epoch, the model contains a set of trained weights and biases.
    At this point, the data held out for cross validation, which the model has never
    seen, can be used as a test for model accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `ImageDataGenerator` definition and generator instance both accept our arguments
    in a dictionary format. The rescaling factor and validation fraction go to the
    generator definition, while the standardized image size and batch size go to the
    generator instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `interpolation` argument indicates that the generator needs to resample
    the image data to `target_size`, which is 224 × 224 pixels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, do the same for the training data generator:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Identify mapping of class index to class name. Since the flower classes are
    encoded in the index, you need a map to recover the flower class names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can display the `idx_labels` to see how these classes are mapped:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You’ve now normalized and standardized your image data. The image generators
    are defined and instantiated for training and validation data. You also have the
    label lookup to decode model prediction, and you’re ready to implement the model
    with TFH.
  prefs: []
  type: TYPE_NORMAL
- en: Model Implementation with TensorFlow Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you saw back in [Figure 4-3](#general_pattern_for_transfer_learning), the
    pretrained model is sandwiched between an input and an output layer. You can define
    this model structure accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice a few things here:'
  prefs: []
  type: TYPE_NORMAL
- en: There is an input layer that defines the input shape of images as [224, 224,
    3].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When `InputLayer` is invoked, `trainable` should be set to False. This indicates
    that you want to reuse the current values from the pretrained model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is an output layer called `Dense` that provides the model output (this
    is described in the Usage section of the summary page).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After the model is built, you’re ready to start training. First, specify the
    loss function and pick an optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then specify the number of batches for training data and cross-validation data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then start the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: After the training process runs through all the epochs specified, the model
    is trained.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'According to the Usage guideline, the output layer `Dense` consists of a number
    of nodes, which reflects how many classes are in the expected images. This means
    each node outputs a probability for that class. It is your job to find which one
    of these probabilities is the highest and map that node to the flower class using
    `idx_labels.` Recall that the `idx_labels` dictionary looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `Dense` layer’s output consists of five nodes in the exact same order. You’ll
    need to write a few lines of code to map the position with the highest probability
    to the corresponding flower class.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping Output to Plain-Text Format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s use the validation images to understand a bit more about how to map the
    model prediction output to the actual class for each image. You’ll use the `predict`
    function to score these validation images. Retrieve the NumPy array for the first
    batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 731 images and 5 corresponding classes in the cross-validation data.
    Therefore, the output shape is [731, 5]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Each row represents the probability distribution for the image class. For the
    first image, the highest probability, 1.0701914e-08 (highlighted in the preceding
    code), is in the last position, which corresponds to index 4 of that row (remember,
    the numbering of an index starts with 0).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you need to find the position where the highest probability occurs for
    each row, using this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And if you display the results with the `print` command, you’ll see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, apply the lookup with `idx_labels` to each element in this array. For
    each element, use a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply a function to each element of a NumPy array, you’ll need to vectorize
    the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then apply this vectorized function to each element in the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, output the result side by side with the image folder and filename
    so that you can save it for reporting or further investigation. You can do this
    with Python pandas DataFrame manipulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a look at the `results` dataframe, which is 731 rows × 2 columns.
  prefs: []
  type: TYPE_NORMAL
- en: '|   | File | Prediction |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | daisy/100080576_f52e8ee070_n.jpg | daisy |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | daisy/10140303196_b88d3d6cec.jpg | sunflowers |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | daisy/10172379554_b296050f82_n.jpg | daisy |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | daisy/10172567486_2748826a8b.jpg | dandelion |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | daisy/10172636503_21bededa75_n.jpg | daisy |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 726 | tulips/14068200854_5c13668df9_m.jpg | sunflowers |'
  prefs: []
  type: TYPE_TB
- en: '| 727 | tulips/14068295074_cd8b85bffa.jpg | roses |'
  prefs: []
  type: TYPE_TB
- en: '| 728 | tulips/14068348874_7b36c99f6a.jpg | dandelion |'
  prefs: []
  type: TYPE_TB
- en: '| 729 | tulips/14068378204_7b26baa30d_n.jpg | tulips |'
  prefs: []
  type: TYPE_TB
- en: '| 730 | tulips/14071516088_b526946e17_n.jpg | dandelion |'
  prefs: []
  type: TYPE_TB
- en: 'Evaluation: Creating a Confusion Matrix'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *confusion matrix*, which evaluates the classification results by comparing
    model output with ground truth, is the easiest way to get an initial feel for
    how well the model performs. Let’s look at how to create a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll use pandas Series as the data structure for building your confusion
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you’ll utilize pandas again to produce the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-4](#confusion_matrix_for_flower_image_classi) shows the confusion
    matrix. Each row represents the distribution of actual flower labels by predictions.
    For example, looking at the first row, you will notice that there are a total
    of 126 samples that are actually class 0, which is daisy. The model correctly
    predicted 118 of these images as class 0; four are misclassified as class 1, which
    is dandelion; one is misclassified as class 2, which is roses; three are misclassified
    as class 3, which is sunflowers; and none has been misclassified as class 4, which
    is tulips.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Confusion matrix for flower image classification](Images/t2pr_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. Confusion matrix for flower image classification
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, use the `sklearn` library to provide a statistical report for each class
    of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This result shows that the model has the best performance when classifying daisies
    (class 0), with an f1-score of 0.92\. Its performance is worst in classifying
    roses (class 2), with an f1-score of 0.85\. The “support” column indicates the
    sample size in each class.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have just completed an example project using a pretrained model from TensorFlow
    Hub. You appended the necessary input layer, performed data normalization and
    standardization, trained the model, and scored a batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: This experience shows the importance of meeting the model’s input and output
    requirements. Just as importantly, pay close attention to the output format of
    the pretrained model. (This information is all available in the model documentation
    page on the TensorFlow Hub website.) Finally, you also need to create a function
    that maps the model output to plain text to make it meaningful and interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: Using the tf.keras.applications Module for Pretrained Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another place to find a pretrained model for your own use is the `tf.keras.applications`
    module (see the [list of available models](https://oreil.ly/HQJBl)). When the
    Keras API became available in TensorFlow, this module became a part of the TensorFlow
    ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Each model comes with pretrained weights, and using them is just as easy as
    using TensorFlow Hub. Keras provides the flexibility needed to conveniently fine-tune
    your models. By making each layer in a model accessible, `tf.keras.applications`
    lets you specify which layers to retrain and which layers to leave untouched.
  prefs: []
  type: TYPE_NORMAL
- en: Model Implementation with tf.keras.applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As with TensorFlow Hub, you need only one line of code to load a pretrained
    model from the Keras module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Notice the `include_top` input argument. Remember that you need to add an output
    layer for your own data. By setting `include_top` to False, you can add your own
    `Dense` layer for the classification output. You’ll also initialize the model
    weights from `imagenet`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then place `base_model` inside a sequential architecture, as you did in the
    TensorFlow Hub example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Add `GlobalAveragePooling2D`, which averages the output array into one numeric
    value, to do an aggregation before sending it to the final `Dense` layer for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now compile the model and launch the training process as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: To score image data, follow the same steps as you did in [“Mapping Output to
    Plain-Text Format”](#mapping_output_to_plain_text_format).
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning Models from tf.keras.applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you wish to experiment with your training routine by releasing some layers
    of the base model for training, you can do so easily. To start, you need to find
    out exactly how many layers are in your base model and designate the base model
    as trainable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As indicated, in this version of the ResNet model, there are 377 layers. Usually
    we start the retraining process with layers close to the end of the model. In
    this case, designate layer 370 as the starting layer for fine-tuning, while holding
    the weights in layers before 300 untouched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then put together the model with the `Sequential` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can try `tf.keras.layers.Flatten()` instead of `tf.keras.layers.GlobalAveragePooling2D()`,
    and see which one gives you a better model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile the model, designating the optimizer and loss function as you did with
    TensorFlow Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This training may take considerably longer, since you’ve freed up more layers
    from the base model for retraining. Once the training is done, score the test
    data and compare the results as described in [“Mapping Output to Plain-Text Format”](#mapping_output_to_plain_text_format)
    and [“Evaluation: Creating a Confusion Matrix”](#evaluationcolon_creating_a_confusion_mat).'
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned how to conduct transfer learning using pretrained,
    deep-learning models. There are two convenient ways to access pretrained models:
    TensorFlow Hub and the `tf.keras.applications` module. Both are simple to use
    and have elegant APIs and styles for quick model development. However, users are
    responsible for shaping their input data correctly and for providing a final `Dense`
    layer to handle model output.'
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of freely accessible pretrained models with abundant inventories
    that you can use to work with your own data. Taking advantage of them using transfer
    learning lets you spend less time building, training, and debugging models.
  prefs: []
  type: TYPE_NORMAL
