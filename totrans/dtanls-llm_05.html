<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="ch__text"> <span class="chapter-title-numbering"><span class="num-string">4</span></span> <span class="title-text"> Analyzing text data</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li id="p2">Classifying text</li> 
    <li id="p3">Extracting information</li> 
    <li id="p4">Clustering documents</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>Text data is ubiquitous and contains valuable information. For instance, think of newspaper articles, emails, reviews, or perhaps this book you are reading! However, analyzing text via computational means was difficult until only a few years ago. After all, unlike formal languages such as Python, natural language was not designed to be easy for computers to parse. The latest generation of language models enables text analysis at almost human levels for many popular tasks. In some cases, the performance of language models for text analysis and generation has even been shown, on average, to surpass the capabilities of humans [1].</p> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>In this chapter, we will see how to use large language models to analyze text. In certain ways, analyzing text data is a very “natural” application of language models. They have been trained on large amounts of text and can be applied directly for text analysis (i.e., without referring to external tools for the actual data analysis). This chapter covers several popular flavors of text analysis: classifying text documents, extracting tabular data from text, and clustering text documents into groups of semantically similar documents. For each of these use cases, we will see example code and discuss variants and extensions.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Classification, information extraction, and clustering are three important types of text analysis but by no means the only ones you may need in practice. However, working through the examples in this chapter will enable you to create custom data-processing pipelines for text data based on language models.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <h2 class=" readable-text-h2" id="preliminaries"><span class="num-string browsable-reference-id">4.1</span> Preliminaries</h2> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>Let’s make sure your system is set up properly for the example projects. The following examples use OpenAI’s GPT model series, accessed via OpenAI’s Python library. This library was discussed in detail in chapter 3. Make sure to follow the instructions in chapter 3 to be able to execute the example code.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p10"> 
   <p> <span class="print-book-callout-head">Warning</span> OpenAI’s Python library is changing quickly. The code in this chapter has been tested with version 1.29 of the OpenAI Python library but may not work with different versions. </p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>Besides the OpenAI library, we will use the popular <code>pandas</code> library. <code>pandas</code> is a popular library for handling tabular data (which we will use as input and output format). We will only use basic functionality from that library and explain the corresponding commands as they occur in the code. Make sure <code>pandas</code> is installed (e.g., try <code>import pandas</code> in the Python interpreter); if it isn’t, install it by entering the following command in the terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p12"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install pandas==2.2</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>Finally, for the last section in this chapter, you will need the clustering algorithms from the <code>scikit-learn</code> library. Run the following command in the terminal to install the appropriate version:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p14"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install scikit-learn==1.3</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p15"> 
   <p>The following sections contain code for three mini-projects that use language models for text analysis. No need to type in the code—you can find all the code on the book’s companion website in the resource section for this chapter. Although you can execute the code on your own data, this book comes with a couple of sample data sets we use in the examples (also on the companion website). And now it’s time to use language models for text classification!</p> 
  </div> 
  <div class="readable-text" id="p16"> 
   <h2 class=" readable-text-h2" id="classification"><span class="num-string browsable-reference-id">4.2</span> Classification</h2> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>So here you are, planning your Saturday evening and deliberating whether to go and see the newest installation of your favorite movie franchise. But is it worth it? Your social media feeds keep filling up with comments from your friends (and your friend’s friends), expanding on their movie experiences. You could browse through them manually, reading each one to get a better sense of whether the majority opinion about the movie is positive or negative. But who has time to do that? Can’t language models help us to automate this task?</p> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>Indeed they can. What we have here is an instance of one of the most classic text-processing problems: we have a text and want to classify it, mapping it to one of a fixed set of categories. In this case, the text to classify is a movie review. We want to classify it as positive (i.e., the writer thinks it was a great movie, and you should go see it!) or negative (save your money!). That means we have two categories. Table <a href="#tab__moviereviews">4.1</a> shows extracts from a few example reviews with the associated class labels. A review praising a movie as “well realized” is clearly positive, whereas one describing the movie as “obviously weak, cheap” is negative. You can find these and a few other reviews in a corresponding file on the book’s companion website.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p19"> 
   <h5 class=" browsable-container-h5" id="tab__moviereviews"><span class="num-string">Table <span class="browsable-reference-id">4.1</span></span> Extracts from movie reviews and associated class labels</h5> 
   <table> 
    <tbody> 
     <tr class="odd"> 
      <td style="text-align: center;"><strong>Review</strong></td> 
      <td style="text-align: center;"><strong>Class</strong></td> 
     </tr> 
     <tr class="even"> 
      <td style="text-align: center;">First of all this movie is a piece of reality very well realized artistically. ...</td> 
      <td style="text-align: center;">Positive</td> 
     </tr> 
     <tr class="odd"> 
      <td style="text-align: center;">Re-titled “Gangs, Inc.”, this is an obviously weak, cheap mobster melodrama. ...</td> 
      <td style="text-align: center;">Negative</td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>Classifying movie reviews is only one of many use cases for text classification. As another example, imagine trying to sort through your email inbox. Wouldn’t it be nice to automatically classify emails based on their content (e.g., using custom categories such as Work, Hobby, Childcare, etc.)? That’s yet another instance of text classification, this time with more than two categories. As a final example, imagine that you’re creating a website that enables users to leave free-text comments. Of course, you don’t want to show potentially offensive comments and would like to filter them out automatically. Again, that means you’re classifying text comments into one of two categories (Offensive and Inoffensive). We will now see how language models can easily be used for each scenario.</p> 
  </div> 
  <div class="readable-text" id="p21"> 
   <h3 class=" readable-text-h3" id="overview"><span class="num-string browsable-reference-id">4.2.1</span> Overview</h3> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>We’ll focus on classifying movie reviews (or, really, any type of review) into Positive (great movie!) and Negative (stay home!) reviews. For that, we’ll use OpenAI’s language models. We’ll assume that we have collected reviews to classify in a file on disk. The code we develop will iterate over all reviews, classify each using the language model, and return the classification result for each review.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>But how can we classify reviews? We will use OpenAI’s Python library, presented in chapter 3. For each review to classify, we will first generate a prompt. The prompt describes a task to a language model. In our case, that task assigns a review to one of our two categories (Positive or Negative). For instance, consider the following prompt as an example:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p24"> 
   <div class="code-area-container"> 
    <pre class="code-area">This movie is a piece of reality very well realized ...  #1
Is the sentiment positive or negative?              #2
Answer ("Positive"/"Negative"):   #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 Review
     <br/>#2 Question
     <br/>#3 Output format
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>This prompt contains the review to classify (<strong class="cueball">1</strong>), a question describing the classification task (<strong class="cueball">2</strong>), and a final statement describing the desired output format (<strong class="cueball">3</strong>). We will construct prompts of this type for each review, send the prompt to the language model, and (hopefully) get back one of the two possible answers (Positive or Negative). Figure <a href="#fig__classificationOverview">4.1</a> illustrates the high-level classification process for each review.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p26">  
   <img alt="figure" src="../Images/CH04_F01_Trummer.png" width="600" height="599"/> 
   <h5 class=" figure-container-h5" id="fig__classificationOverview"><span class="num-string">Figure <span class="browsable-reference-id">4.1</span></span> For each review, we generate a prompt that contains the review, together with instructions describing the classification task. Given the prompt as input, the language model outputs a class label for the review.</h5>
  </div> 
  <div class="readable-text" id="p27"> 
   <h3 class=" readable-text-h3" id="creating-prompts"><span class="num-string browsable-reference-id">4.2.2</span> Creating prompts</h3> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>Given a review, we generate a prompt instructing the language model to classify it. All the prompts we generate for classification follow the same prompt template.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p29"> 
    <h5 class=" callout-container-h5 readable-text-h5">Reminder: What is a prompt template?</h5> 
   </div> 
   <div class="readable-text" id="p30"> 
    <p> We briefly mentioned prompt templates in chapter 1. A prompt template is a text that contains placeholders. By substituting actual text for these placeholders, we obtain a prompt that we can send to the language model. We also say that a prompt <em>instantiates</em> a prompt template if the prompt can be obtained by substituting the template’s placeholders.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>The example prompt from the previous section instantiates the following prompt template:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p32"> 
   <div class="code-area-container"> 
    <pre class="code-area">[Review]                           #1
Is the sentiment positive or negative?  #2
Answer ("Positive"/"Negative"):   #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 Review (placeholder)
     <br/>#2 Question
     <br/>#3 Output format
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>Our template contains only a single placeholder: the text of the review to classify (<strong class="cueball">1</strong>). For each review, we will replace this placeholder with the actual review text. We also instruct the language model on what to do with the review text (<strong class="cueball">2</strong>) (check whether the underlying sentiment is positive or negative) and define the output format (<strong class="cueball">3</strong>). The latter step is important because there may be many ways to express the underlying sentiment: for example, “P” for positive and “N” for negative, or a longer answer such as “The review is positive.” If we don’t explicitly tell the language model to use a specific output format, it may choose any of these possibilities! In our scenario, we ultimately want to aggregate the classification results to learn the majority opinion (do most people like the movie or not?), and aggregating the results from each review becomes much simpler if all the classifications follow the same output format.</p> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>The following function follows the template to generate a prompt for a given review (specified as the input parameter <code>text</code>):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p35"> 
   <div class="code-area-container"> 
    <pre class="code-area">def create_prompt(text):
    task = 'Is the sentiment positive or negative?'
    answer_format = 'Answer ("Positive"/"Negative")'
    return f'{text}\n{task}\n{answer_format}:'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>The result of the function is the prompt, instantiating the template for the input review.</p> 
  </div> 
  <div class="readable-text" id="p37"> 
   <h3 class=" readable-text-h3" id="calling-the-model"><span class="num-string browsable-reference-id">4.2.3</span> Calling the model</h3> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>Next, we send generated prompts to a language model to obtain a solution. More precisely, we are using OpenAI’s GPT-4o model, OpenAI’s latest model at the time of writing. As this is one of OpenAI’s chat models, optimized for multistep interactions with users, we use the chat completions endpoint to communicate with the model. As discussed in more detail in chapter 3, this endpoint expects as input a history of prior messages (in addition to the specific model name). Here, we have only one prior “message”: the prompt. We classify it as a <code>user</code> message, encouraging the model to solve whatever task is described in the message. For instance, we can send prompts to the language model and collect the answers using the following piece of code (assuming that <code>prompt</code> contains the previously generated prompt text):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p39"> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai
client = openai.OpenAI()

response = client.chat.completions.create(
    model='gpt-4o',
    messages=[
        {'role':'user', 'content':prompt}
        ]
    )</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>However, using this code directly is problematic. OpenAI’s GPT models are hosted online and accessed remotely. This creates opportunities for failed attempts to reach the corresponding endpoint: for example, due to a temporary connection loss. Because of that, it is good practice to allow for a couple of retries when calling the model. In particular, when processing large data sets requiring many consecutive calls to OpenAI’s models, the chances of at least one unsuccessful call increase. Instead of interrupting computation with an exception, it is better to wait a few seconds before starting another try. Here is a completed version of the previous code—a function that calls the language model with automated retries:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p41"> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai
client = openai.OpenAI()

def call_llm(prompt):
    for nr_retries in range(1, 4):
        try:
            response = client.chat.completions.create(
                model='gpt-4o',
                messages=[
                    {'role':'user', 'content':prompt}
                    ]
                )
            return response.choices[0].message.content
        except:
            time.sleep(nr_retries * 2)
    raise Exception('Cannot query OpenAI model!')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>The <code>call_lm</code> function allows up to three retries with an increasing delay between them. This delay is realized by a call to the <code>time.sleep</code> function (using Python’s <code>time</code> library) whenever an exception (indicating, for instance, a temporary connection loss) is encountered. After three retries, the function fails with an exception (assuming, pessimistically, that whatever problem prevents us from contacting OpenAI will not be resolved any time soon). Whenever the call succeeds, the function returns the corresponding result.</p> 
  </div> 
  <div class="readable-text" id="p43"> 
   <h3 class=" readable-text-h3" id="end-to-end-classification-code"><span class="num-string browsable-reference-id">4.2.4</span> End-to-end classification code</h3> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>It’s time to put it all together! The next listing shows the code that matches the classification process we’ve discussed. It also contains the function for generating prompts (<strong class="cueball">2</strong>) and the one for calling the language model (<strong class="cueball">3</strong>).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p45"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__classification"><span class="num-string">Listing <span class="browsable-reference-id">4.1</span></span> Classifying input text by sentiment (positive, negative)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse  #1
import openai
import pandas as pd
import time

client = openai.OpenAI()

def create_prompt(text):                      #2
    """ Generates prompt for sentiment classification.
    
    Args:
        text: classify this text.
    
    Returns:
        input for LLM.
    """
    task = 'Is the sentiment positive or negative?'
    answer_format = 'Answer ("Positive"/"Negative")'
    return f'{text}\n{task}\n{answer_format}:'

def call_llm(prompt):                           #3
    """ Query large language model and return answer.
    
    Args:
        prompt: input prompt for language model.
    
    Returns:
        Answer by language model.
    """
    for nr_retries in range(1, 4):
        try:
            response = client.chat.completions.create(
                model='gpt-4o',
                messages=[
                    {'role':'user', 'content':prompt}
                    ]
                )
            return response.choices[0].message.content
        except:
            time.sleep(nr_retries * 2)
    raise Exception('Cannot query OpenAI model!')

def classify(text):       #4
    """ Classify input text.
    
    Args:
        text: assign this text to a class label.
    
    Returns:
        name of class.
    """
    prompt = create_prompt(text)
    label = call_llm(prompt)
    return label

if __name__ == '__main__':  #5

    parser = argparse.ArgumentParser()       #6
    parser.add_argument('file_path', type=str, help='Path to input file')
    args = parser.parse_args()
    
    df = pd.read_csv(args.file_path)     #7
    df['class'] = df['text'].apply(classify)  #8
    statistics = df['class'].value_counts()   #9
    print(statistics)
    df.to_csv('result.csv')</pre> 
    <div class="code-annotations-overlay-container">
     #1 Imports libraries
     <br/>#2 Generates classification prompts
     <br/>#3 Calls the large language model
     <br/>#4 Classifies one text document
     <br/>#5 Reads text, classifies, and writes result
     <br/>#6 Defines command-line arguments
     <br/>#7 Reads input
     <br/>#8 Classifies text
     <br/>#9 Generates output
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>First, let’s discuss the libraries used in listing <a href="#code__classification">4.1</a> (<strong class="cueball">1</strong>). We will reuse those libraries for the following projects, so it makes sense to have a closer look at them (and why we need them here). We want to start our code from the command line, specifying relevant parameters (e.g., the path of the input data) as arguments. The <code>argparse</code> library features useful functions to specify and read out such command-line arguments. Next, we need the <code>openai</code> library, discussed in chapter 3, to call OpenAI’s language model from Python. The <code>pandas</code> library supports standard operations on tabular data. Of course, tabular data is not our focus in this chapter. However, we will store text documents and related metadata as rows in tables, so the <code>pandas</code> library comes in handy. Finally, as discussed previously, we use the <code>time</code> library to implement delayed retries when calling the language model.</p> 
  </div> 
  <div class="readable-text" id="p47"> 
   <h3 class=" readable-text-h3" id="classifying-documents"><span class="num-string browsable-reference-id">4.2.5</span> Classifying documents</h3> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>The classification of a single text document (<strong class="cueball">4</strong>) combines the two functions discussed previously. Given an input text to classify, the code first creates a corresponding prompt (call to <code>create_prompt</code>) and then generates a suitable reply via a call to the language model (call to <code>call_llm</code>). The result is assumed to be the class label and is returned to the user.</p> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>Now we put it together (<strong class="cueball">5</strong>). This part of the code is executed when invoking the Python module from the command line and uses the functions we’ve introduced. The initial <code>if</code> condition (<strong class="cueball">5</strong>) ensures that the following code is only executed when invoking the module directly (instead of importing it from a different module).</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>First (<strong class="cueball">6</strong>), we define command-line arguments. We need only one argument here: a path to a .csv file containing the data to classify. We assume that each row contains one text document and that the text to classify is contained in the <code>text</code> column. We parse command-line arguments and make their values available in the <code>args</code> variable.</p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>Next, we load our input data from disk (<strong class="cueball">7</strong>). We assume that data is stored as a .csv file (comma-separated value): that is, a header line with column names, followed by lines containing data (fields are separated by commas, as the name suggests). Here, the <code>pandas</code> library comes in handy and enables us to load such data with a single command. The <code>df</code> variable then contains a <code>pandas</code> DataFrame containing data from the input file. We retrieve the DataFrame <code>text</code> column (<strong class="cueball">8</strong>) and apply the previously defined <code>classify</code> function to each row (using <code>pandas</code>’ <code>apply</code> method). Finally (<strong class="cueball">9</strong>), we generate and print out aggregate statistics (the number of occurrences for each answer generated by the model) and write the resulting classifications into a file (result.csv).</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <h3 class=" readable-text-h3" id="running-the-code"><span class="num-string browsable-reference-id">4.2.6</span> Running the code</h3> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>On the book’s companion website, download the file reviews.csv. This file contains a small number of movie reviews that we can use for classification. The file contains two columns: the review text and the associated sentiment (<code>neg</code> for negative sentiment and <code>pos</code> for positive sentiment). Of course, our goal is to detect such sentiments automatically. However, having the ground truth also enables us to assess the quality of the classifications.</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>You can test the code for classification as described next (the following commands have been tested on a Linux operating system). Using a terminal, change to the directory containing a Python module (listing1.py) with the code in listing <a href="#code__classification">4.1</a>. Then, run the following command (replacing <code>python</code> with the name of your Python interpreter, such as <code>python3</code>, if needed):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p55"> 
   <div class="code-area-container"> 
    <pre class="code-area">python listing1.py reviews.csv</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>Here, we assume that the input file (reviews.csv) is stored in the same repository as the code (otherwise, you have to substitute the corresponding path for the filename). Typically, the code should not take more than a few seconds to execute (slightly more if your connection is unstable, requiring retries). If execution succeeds, the only output you will see summarizes the number of labels assigned for each of the two possible classes.</p> 
  </div> 
  <div class="readable-text" id="p57"> 
   <p>After executing the code, you will find a result.csv file in the same repository. In addition to the columns of the input file, the result file contains a new <code>class</code> column. This column contains the classification results (positive and negative). Compare the label assigned by our classifier to the ground-truth sentiment. You will find that the classification is consistent in a majority of cases. Not bad for a few lines of Python code, right?</p> 
  </div> 
  <div class="readable-text" id="p58"> 
   <h3 class=" readable-text-h3" id="trying-out-variants"><span class="num-string browsable-reference-id">4.2.7</span> Trying out variants</h3> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>At this point, it is a good idea to play a bit more with the code and the data to get a better sense of how it works. For instance, try writing a few movie reviews yourself! For which reviews is the classification reliable, and where is it challenging? Also try a few variants of the prompt. Which instructions lead to better accuracy, and which degrade performance? To take just one example variation, try removing the part of the prompt that defines the output format precisely (the line <code>Answer ("Positive"/"Negative")</code>). Now try running the program with the changed prompt. What happens? In all likelihood, you will see more than two labels in your classification result (in the output of the program), including, for instance, abbreviations (e.g., “P” and “N”) as well as overly detailed answers (e.g., during testing, GPT-4o generated replies such as “The sentiment of this review is positive.”). In chapter 9, we evaluate the effect of different prompts on the model’s output quality.</p> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>You may also want to vary the model used for extraction. How about using one of the smaller model versions, such as GPT-3.5 (which is significantly cheaper per token processed)? And how about the model configuration? Listing <a href="#code__classification">4.1</a> only uses two parameters (the model name and the message history), both of which are required. However, in chapter 3, we saw various configuration parameters that can be applied here. For instance, try changing the <code>temperature</code> parameter (e.g., setting <code>temperature</code> to 0 will give you more deterministic results), or limit the length of the desired output! In rare cases, GPT models may generate output text that is longer than the desired classification result (which consists of a single token). You can avoid that by limiting the output length using the <code>max_tokens</code> parameter. At the same time, instead of restricting the output format only via instructions in the prompt, you may increase the likelihood of the two possible results (positive and negative) using the <code>logit_bias</code> parameter. We discuss model tuning further in chapter 9.</p> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>As yet another variant, try changing the classification task! For instance, it is relatively easy to classify using a different set of categories. All it takes is changing the instructions in the prompt (outlining all answer options as before). By changing a few lines of code, you can even obtain a versatile classification tool that enables users to specify the classification task and corresponding classes as additional command-line arguments. For example, beyond movie reviews, you can use this tool to categorize newspaper articles into one of several topic categories or to classify emails as either Urgent or Nonurgent. By now, you are hopefully convinced that language models enable text classification with relatively high quality and moderate implementation overheads. Time to broaden our scope to different tasks!</p> 
  </div> 
  <div class="readable-text" id="p62"> 
   <h2 class=" readable-text-h2" id="text-extraction"><span class="num-string browsable-reference-id">4.3</span> Text extraction</h2> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>Imagine that, given your expertise in data analysis with language models, you recently landed a highly sought-after job at Banana (a popular company producing various consumer electronics). The moment you sit down at the desk of your new office, emails from enthusiastic students inquiring about summer internships start rolling in. Having a summer intern would be nice, but how do you choose the best match? Ideally, you would like to compile a table comparing all applicants in terms of their GPA, their degree, the name of the company at which they did their most recent internship (if any), and so on. But combing through emails to compile that table manually seems tedious. Can’t you automate that?</p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>Of course you can. Let’s use language models to analyze emails to extract all the relevant factors to choose our lucky summer intern. What we have here is, again, a standard problem in text analysis: information extraction! In information extraction, we generally extract structured information (e.g., a data table) from text. Here, we consider emails (from applicants) as text documents. For each email, we want to extract a range of attributes: for example, name, GPA, and (current or most recent) degree. For instance, consider the following extract from an email from one of the hopeful applicants:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p65"> 
   <div class="code-area-container"> 
    <pre class="code-area">Hi!
My name is Martin, I would love to do a summer internship at Banana! 
A bit about myself: I am currently working on a Bachelor of Computer Science
at Stanford University, my current GPA is 4.0.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>Considering the three previously mentioned attributes, we can extract the name of the applicant (“Martin”), his GPA (“4.0”), and his degree (“Bachelor of Computer Science”). If analyzing emails from multiple applicants, we can represent the result as a data table, as shown in table <a href="#tab__summerinterns">4.2</a>. In the next section, we discuss how we can accomplish information extraction using language models.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p67"> 
   <h5 class=" browsable-container-h5" id="tab__summerinterns"><span class="num-string">Table <span class="browsable-reference-id">4.2</span></span> Extracted information about applicants for summer internships</h5> 
   <table> 
    <tbody> 
     <tr class="odd"> 
      <td style="text-align: center;"><strong>Name</strong></td> 
      <td style="text-align: center;"><strong>GPA</strong></td> 
      <td style="text-align: center;"><strong>Degree</strong></td> 
     </tr> 
     <tr class="even"> 
      <td style="text-align: center;">Martin</td> 
      <td style="text-align: center;">4.0</td> 
      <td style="text-align: center;">Bachelor of Computer Science</td> 
     </tr> 
     <tr class="odd"> 
      <td style="text-align: center;">Alice</td> 
      <td style="text-align: center;">4.0</td> 
      <td style="text-align: center;">Master of Software Engineering</td> 
     </tr> 
     <tr class="even"> 
      <td style="text-align: center;">Bob</td> 
      <td style="text-align: center;">3.7</td> 
      <td style="text-align: center;">Bachelor of Design</td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p68"> 
   <h3 class=" readable-text-h3" id="overview-1"><span class="num-string browsable-reference-id">4.3.1</span> Overview</h3> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>Again, we’ll assume that our emails are stored on disk (in a tabular data file where each row contains one email). We’ll iterate over emails and use the language model to extract all relevant attributes. Instead of hard-coding relevant attributes, we will allow users to specify those attributes on the command line (that way, you can easily reuse the code if your criteria for summer internships should change). As we use language models for text analysis (which are good at interpreting natural language), there is no need to specify attributes in any kind of formal language. Simply specify the attribute names (or, optionally, a short description in natural language), and the language model should be able to figure out what to extract. The output of our code will be a tabular data file (in .csv format) that contains content similar to table <a href="#tab__summerinterns">4.2</a>: the output table has one column for each extracted attribute and one row for each analyzed email.</p> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>So how can we extract attributes from a given email? Again, we want to generate a prompt that describes the extraction task to the language model. For instance, the following prompt should help us extract all relevant attributes from the previous email:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p71"> 
   <div class="code-area-container"> 
    <pre class="code-area"> #1
Extract the following properties into a table:
name,GPA,Degree
 #2
Text source: My name is Martin, I would love to do a summer 
internship at Banana! A bit about myself: I am currently 
working on a Bachelor of Computer Science at Stanford 
University, my current GPA is 4.0.
 #3
Mark the beginning of the table with &lt;BeginTable&gt; and the end with &lt;EndTable&gt;. Separate rows by newline symbols and separate fields by pipe
symbols (|). Omit the table header and insert values in the attribute 
order from above. Use the placeholder &lt;NA&gt; if the value for an attribute 
is not available.</pre> 
    <div class="code-annotations-overlay-container">
     #1 Task description
     <br/>#2 Text to analyze
     <br/>#3 Output format
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>The prompt consists of three parts: a task description, including a specification of the attributes to extract (<strong class="cueball">1</strong>); the source text for extraction (<strong class="cueball">2</strong>); and the desired output format, including values to use if the source text does not contain any information on specific attributes (<strong class="cueball">3</strong>). Sending this prompt to the language model should yield text that contains the desired extraction results.</p> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>The output from the language model is, first, a text string. Ultimately, we want to output a structured data table. That means we still need some postprocessing to extract values for all relevant attributes (name, GPA, and degree) from the output text. Figure <a href="#fig__textExtractionOverview">4.2</a> illustrates the steps of the extraction process (for a single text document).</p> 
  </div> 
  <div class="browsable-container figure-container" id="p74">  
   <img alt="figure" src="../Images/CH04_F02_Trummer.png" width="821" height="845"/> 
   <h5 class=" figure-container-h5" id="fig__textExtractionOverview"><span class="num-string">Figure <span class="browsable-reference-id">4.2</span></span> For each email, we generate a prompt that contains the email and a description of the extraction task. This description references the attributes to extract specified by the user. Given the prompt as input, the language model generates an answer text containing extracted attribute values. Via postprocessing, we extract those values from the raw answer text.</h5>
  </div> 
  <div class="readable-text" id="p75"> 
   <h3 class=" readable-text-h3" id="generating-prompts"><span class="num-string browsable-reference-id">4.3.2</span> Generating prompts</h3> 
  </div> 
  <div class="readable-text" id="p76"> 
   <p>We want to generate prompts that instantiate the following prompt template:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p77"> 
   <div class="code-area-container"> 
    <pre class="code-area"> #1
Extract the following properties into a table:
[List of attributes] 
 #2
Text source: [Email] 
 #3
Mark the beginning of the table with &lt;BeginTable&gt; and the end with &lt;EndTable&gt;.
Separate rows by newline symbols and separate fields by pipe symbols (|).
Omit the table header and insert values in the attribute order from above.
Use the placeholder &lt;NA&gt; if the value for an attribute is not available.</pre> 
    <div class="code-annotations-overlay-container">
     #1 Task description
     <br/>#2 Text to analyze
     <br/>#3 Output format
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>The prompt template contains a task description (<strong class="cueball">1</strong>), the source text for extraction (<strong class="cueball">2</strong>), and a specification of the output format (<strong class="cueball">3</strong>). Note that this prompt now contains two placeholders (the template we used in the previous section contained only a single placeholder): the list of attributes to extract and the source text for extraction.</p> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>We will generate prompts using the following code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p80"> 
   <div class="code-area-container"> 
    <pre class="code-area">def create_prompt(text, attributes):
    parts = []
     #1
    parts += ['Extract the following properties into a table:']
    parts += [','.join(attributes)]
 
    parts += [f'Text source: {text}']  #2
         #3
    parts += [
        ('Mark the beginning of the table with &lt;BeginTable&gt; '
        'and the end with &lt;EndTable&gt;.')]
    parts += [
        ('Separate rows by newline symbols and separate '
        'fields by pipe symbols (|).')]
    parts += [
        ('Omit the table header and insert values in '
        'the attribute order from above.')]
    parts += [
        ('Use the placeholder &lt;NA&gt; if the value '
        'for an attribute is not available.')]
    return '\n'.join(parts)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Generates a task description
     <br/>#2 Adds source text
     <br/>#3 Adds a description of the output format
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>This function takes as input the text to analyze (which we certainly want to include in the prompt) along with a list of attributes we want to extract. After generating the task description (<strong class="cueball">1</strong>), including the list of attributes to extract, the function adds the source text (<strong class="cueball">2</strong>), as well as a specification of the desired output format (<strong class="cueball">3</strong>). The prompt concatenates these parts.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <h3 class=" readable-text-h3" id="postprocessing"><span class="num-string browsable-reference-id">4.3.3</span> Postprocessing</h3> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>Compared to the previous project (text classification), our prompt has changed to adapt to the new task (text extraction). Even with a different prompt, we can still reuse the same function as in the last section to obtain an answer from the language model. On the other hand, we need to do a little more work than before to process the raw answer using the language model. For classification, we directly used the reply from the language model as the final result. In our current scenario (text extraction), we generally will want to extract values for multiple attributes for a single input text. As the output text from the language model contains values for all extracted attributes, we need to extract values for specific attributes from the raw answer text.</p> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>For instance, we might receive the following raw answer text from the language model:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p85"> 
   <div class="code-area-container"> 
    <pre class="code-area">| Martin | 4.0 | Bachelor of Computer Science |</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>To extract values for each attribute, we can split the raw text using pipe symbols as field delimiters (while removing the first and last pipe symbols in the answer). Ideally, we want to expand our scope beyond the specific use case we are currently considering (extracting information on applicants from emails). In some scenarios, we may extract multiple rows from the same text (imagine a scenario where multiple applicants together submit a group email—but that’s admittedly a less likely case). To support such use cases, we may also have to split the raw answer into text associated with different rows. To do that, we can use the newline symbol as row delimiters (as rows are split by newline symbols).</p> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>We can do all these things with the following function:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p88"> 
   <div class="code-area-container"> 
    <pre class="code-area">import re

def post_process(raw_answer):
    table_text = re.findall(      #1
        '&lt;BeginTable&gt;(.*)&lt;EndTable&gt;', 
        raw_answer, re.DOTALL)[0]
    
    results = []
    for raw_row in table_text.split('\n'):  #2
        if raw_row:                    #3
            row = raw_row.split('|')
            row = [field.strip() for field in row]
            row = [field for field in row if field]
            results.append(row)
    return results</pre> 
    <div class="code-annotations-overlay-container">
     #1 Extracts table data
     <br/>#2 Splits by row
     <br/>#3 Splits by field
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>The input to this function is raw text produced by the language model for a single text document. The output is a list of rows (where each result row is, again, represented as a list). To get from input to output, we first need to extract the part of the raw answer that contains the actual table data (<strong class="cueball">1</strong>). Answers generated by GPT-4o may contain a preamble or additional explanations beyond the extracted table (e.g., “Sure, here is the table you wanted: ...”). We need to separate the data we are interested in. Fortunately, that’s easy as long as GPT-4o is following our instructions (which, typically, it does): the data we’re interested in should be contained between two markers (<code>&lt;BeginTable&gt;</code> and <code>&lt;EndTable&gt;</code>). Hence, the regular expression <code>'&lt;BeginTable&gt;(.*)&lt;EndTable&gt;'</code> exactly matches the part of the output we’re interested in. We retrieve it using Python’s <code>re.findall</code> function, which, given a string and regular expression as input, returns a list of matching substrings. We use the <code>re.DOTALL</code> flag to ensure that the dot within the regular expression matches all characters and newlines (because the table may contain multiple lines). From the resulting matches, we take the first one. Note that this implicitly assumes at least one table in GPT’s output. Although that is typically the case, think about how to make the function more robust toward answers from the language model that do not comply with our instructions in the prompt.</p> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>Having extracted the table data in a text representation, we first split it into data associated with specific rows (<strong class="cueball">2</strong>) and data associated with specific cells (<strong class="cueball">3</strong>). After some cleanup (the Python function <code>strip</code> removes whitespace), we add the resulting cell values to our result list. This list of rows (where each row is, again, represented as a list) is returned.</p> 
  </div> 
  <div class="readable-text" id="p91"> 
   <h3 class=" readable-text-h3" id="end-to-end-extraction-code"><span class="num-string browsable-reference-id">4.3.4</span> End-to-end extraction code</h3> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>Listing <a href="#code__extraction">4.2</a> shows the completed Python code. The code structure is similar to listing <a href="#code__classification">4.1</a>, and some of the functions are shared among the two listings (rather than omitting repeated functions, this book aims to provide you with self-contained code so you don’t have to piece together code from multiple pages). In particular, the code uses the same libraries as before (<strong class="cueball">1</strong>) and invokes the language model via the same function (<strong class="cueball">3</strong>). You will recognize the function for creating prompts (<strong class="cueball">2</strong>) and the one for postprocessing raw output from the language model (<strong class="cueball">4</strong>), introduced earlier.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p93"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__extraction"><span class="num-string">Listing <span class="browsable-reference-id">4.2</span></span> Extracting user-defined attributes from text</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse  #1
import openai
import pandas as pd
import re
import time

client = openai.OpenAI()

def create_prompt(text, attributes):              #2
    """ Generates prompt for information extraction.
    
    Args:
        text: extract information from this text.
        attributes: list of attributes.
    
    Returns:
        input for LLM.
    """
    parts = []
    parts += ['Extract the following properties into a table:']
    parts += [','.join(attributes)]
    parts += [f'Text source: {text}']
    parts += [
        ('Mark the beginning of the table with &lt;BeginTable&gt; '
        'and the end with &lt;EndTable&gt;.')]
    parts += [
        ('Separate rows by newline symbols and separate '
        'fields by pipe symbols (|).')]
    parts += [
        ('Omit the table header and insert values in '
        'the attribute order from above.')]
    parts += [
        ('Use the placeholder &lt;NA&gt; if the value '
        'for an attribute is not available.')]
    return '\n'.join(parts)

def call_llm(prompt):                             #3
    """ Query large language model and return answer.
    
    Args:
        prompt: input prompt for language model.
    
    Returns:
        Answer by language model.
    """
    for nr_retries in range(1, 4):
        try:
            response = client.chat.completions.create(
                model='gpt-4o',
                messages=[
                    {'role':'user', 'content':prompt}
                    ]
                )
            return response.choices[0].message.content
        except:
            time.sleep(nr_retries * 2)
    raise Exception('Cannot query OpenAI model!')

def post_process(raw_answer):             #4
    """ Extract fields from raw text answer.
    
    Args:
        raw_answer: raw text generated by LLM.
    
    Returns:
        list of result rows.
    """
    table_text = re.findall(
        '&lt;BeginTable&gt;(.*)&lt;EndTable&gt;', 
        raw_answer, re.DOTALL)[0]
    
    results = []
    for raw_row in table_text.split('\n'):
        if raw_row:
            row = raw_row.split('|')
            row = [field.strip() for field in row]
            row = [field for field in row if field]
            results.append(row)
    return results

def extract_rows(text, attributes):           #5
    """ Extract values for attributes from text.
    
    Args:
        text: extract information from this text.
        attributes: list of attributes to extract.
    
    Returns:
        list of rows with attribute values.
    """
    prompt = create_prompt(text, attributes)
    result_text = call_llm(prompt)
    result_rows = post_process(result_text)
    return result_rows

if __name__ == '__main__':         #6

    parser = argparse.ArgumentParser()
    parser.add_argument('file_path', type=str, help='Path to input file')
    parser.add_argument('attributes', type=str, help='Attribute list')
    args = parser.parse_args()
    
    input_df = pd.read_csv(args.file_path)
    attributes = args.attributes.split('|')
    
    extractions = []
    for text in input_df['text'].values:          #7
        extractions += extract_rows(text, attributes)
    
    result_df = pd.DataFrame(extractions)
    result_df.columns = attributes
    result_df.to_csv('result.csv')</pre> 
    <div class="code-annotations-overlay-container">
     #1 Imports relevant libraries
     <br/>#2 Generates prompts
     <br/>#3 Invokes the language model
     <br/>#4 Postprocesses model output
     <br/>#5 Extracts data tables from text
     <br/>#6 Extracts information and writes the result
     <br/>#7 Iterates over text
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>The main function (<strong class="cueball">6</strong>) reads two input parameters from the command line:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p95">A path to a .csv file containing the text to analyze</li> 
   <li class="readable-text" id="p96">A list of attributes to extract, separated by pipe symbols</li> 
  </ul> 
  <div class="readable-text" id="p97"> 
   <p>After opening the input file (using the <code>pandas</code> library), we iterate over all input text documents (<strong class="cueball">7</strong>). Note that we expect the input text in the <code>text</code> column in the input file. To perform the actual extraction, we use the <code>extract_rows</code> function (<strong class="cueball">5</strong>). Given input text and a list of attributes to extract, this function generates a suitable prompt, obtains a raw answer from the language model, and postprocesses the raw answer to get structured output (which it returns). After iterating over the input text (<strong class="cueball">7</strong>), we store the final result in a file named result.csv (this file will be overwritten if it already exists).</p> 
  </div> 
  <div class="readable-text" id="p98"> 
   <h3 class=" readable-text-h3" id="trying-it-out"><span class="num-string browsable-reference-id">4.3.5</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p99"> 
   <p>You can find the code from listing <a href="#code__extraction">4.2</a> as listing2.py on the companion website. You can also download the file biographies.csv there, giving you a small data set to experiment on with your extractor (this is a bit different from our motivating scenario, but publicly available data on email applications is sparse). This file contains biographies of five famous people, as well as the associated names, with one person per row. Change into the directory containing listing2.py (as well as the data), and run</p> 
  </div> 
  <div class="browsable-container listing-container" id="p100"> 
   <div class="code-area-container"> 
    <pre class="code-area">python listing2.py biographies.csv 
  "name|city of birth|date of birth"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>The first parameter is the data set (if it is not in the same directory, adapt the path accordingly). The second parameter is the list of attributes to extract. We use the pipe symbol again to separate attributes. Note that we only identify attributes via their names; no need to refer to predefined categories. The language model can understand attribute semantics based on the name alone.</p> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>After executing the code (which should not take more than a minute), you will find the results stored in a file named result.csv. For example, executing the code on the sample data could yield the following table:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p103"> 
   <div class="code-area-container"> 
    <pre class="code-area">    name    city of birth    date of birth
0    Sergey Mikhailovich Brin    Moscow    August 21, 1973
1    Martin Luther King Jr.    Atlanta, Georgia    January 15, 1929
2    Anne E. Wojcicki    &lt;NA&gt;    July 28, 1973
3    Maria Salomea Skłodowska-Curie    Warsaw    7 November 1867
4    Alan Mathison Turing    Maida Vale, London    23 June 1912</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>Even if you execute the same code on the same data, you may see slight variations (due to randomization when generating model output). Each row in that file (besides the header row) represents an extraction. We are extracting name, birth city, and birth date. Hence, we expect one extracted row per biography (and that is what happens in our sample run). Note that there are missing values: for Ann E. Wojcicki, the biography snippet does not contain the city of birth. The language model reacts appropriately and inserts a corresponding placeholder (“&lt;N/A&gt;”), instead of a concrete value.</p> 
  </div> 
  <div class="readable-text" id="p105"> 
   <h2 class=" readable-text-h2" id="clustering"><span class="num-string browsable-reference-id">4.4</span> Clustering</h2> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>You’re a few weeks in at your new job at Banana. The job is great, but there is one problem: your inbox keeps overflowing with emails! It’s not only applications from hopeful summer interns (we took care of that in the last section). Those emails cover a variety of different topics, and making sure to read all relevant emails takes a lot of your time. Looking closer, you notice that many of the emails are redundant. For example, you observe that many emails try to draw attention to the same company events. For a moment, you ponder using your code for text classification (discussed in section 4.2) to categorize emails into several categories (e.g., associated with specific company events). After that, you can read only a few emails from each category to have a full overview of what’s happening at Banana. Alas, there is one problem: it is hard to come up with and maintain an exhaustive list of topics because those topics will keep changing over the course of your employment. Instead, it would be nice to automatically group different emails that are somewhat similar because, for instance, they discuss the same event. That way, you wouldn’t have to come up with a list of topics in advance.</p> 
  </div> 
  <div class="readable-text" id="p107"> 
   <p>What we want is to group similar emails into clusters. That’s yet another classical text-processing problem: text clustering. If you want to bring related text documents together without knowing the set of categories beforehand, clustering methods are probably the way to go! In this section, we will see how to use language models for text clustering.</p> 
  </div> 
  <div class="readable-text" id="p108"> 
   <h3 class=" readable-text-h3" id="overview-2"><span class="num-string browsable-reference-id">4.4.1</span> Overview</h3> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>Clustering is a classical approach in computer science. Clustering methods predate language models and advanced text analysis by quite a bit. However, traditionally, clustering focuses on elements that are expressed as vectors. We want to bring together (in the same cluster) vectors that have a small distance from each other (and, of course, there are various distance metrics that we can apply for vectors). However, that’s not really the case here: in our scenario, we want to assign similar emails (or, in general, similar text documents) to the same cluster. So how do we get from documents to vectors?</p> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>The answer is <em>embeddings</em>. An embedding represents a text document as a (typically high-dimensional) vector. That’s exactly what we’re looking for! Of course, this approach only makes sense if we map text documents to vectors that have something meaningful to say about the content of the documents. Ideally, we want documents with similar vectors (i.e., vectors with a small distance according to our preferred distance metric) to also have similar content. This means we cannot use naive methods to map text documents to vectors. Instead, we need an approach that considers the semantics of the text and takes them into account when generating a vector representation.</p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <p>Fortunately, language models can help! Providers like OpenAI offer language models that take text as input and produce embedding vectors as output. So, having a collection of text documents to cluster, we can calculate embedding vectors for all of them and apply any classical clustering algorithm to the resulting vectors. Figure <a href="#fig__clusteringOverview">4.3</a> illustrates this process. Next, we discuss how to implement it.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p112">  
   <img alt="figure" src="../Images/CH04_F03_Trummer.png" width="1100" height="525"/> 
   <h5 class=" figure-container-h5" id="fig__clusteringOverview"><span class="num-string">Figure <span class="browsable-reference-id">4.3</span></span> Clustering emails. We first calculate embedding vectors for all emails. Then we cluster those vectors to assign emails with similar content to the same cluster.</h5>
  </div> 
  <div class="readable-text" id="p113"> 
   <h3 class=" readable-text-h3" id="calculating-embeddings"><span class="num-string browsable-reference-id">4.4.2</span> Calculating embeddings</h3> 
  </div> 
  <div class="readable-text" id="p114"> 
   <p>For the examples discussed so far, we have used OpenAI’s chat completions endpoint. For clustering, we will use OpenAI’s embedding endpoint instead. The goal of embedding is to create a vector that compresses the semantics of a text. Different models can be used to calculate embeddings. The dimension of the vector depends on the model used. For the following code, we will use the <code>text-embedding-ada-002</code> model. You can try substituting other models for this one (you can find a list of OpenAI models for calculating embeddings at <a href="https://platform.openai.com/docs/guides/embeddings">https://platform.openai.com/docs/guides/embeddings</a>) to compare the output quality.</p> 
  </div> 
  <div class="readable-text" id="p115"> 
   <p>For instance, we can generate embeddings for text documents as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p116"> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai
client = openai.OpenAI()

response = client.embeddings.create(
    model='text-embedding-ada-002',
    input=text)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p117"> 
   <p>Here you see an extract from the corresponding response:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p118"> 
   <div class="code-area-container"> 
    <pre class="code-area">CreateEmbeddingResponse(
    data=[
        Embedding(embedding=[                            #1
            -0.005983137525618076, -0.000303583248751238, ...], 
            index=0, object='embedding')], 
    model='text-embedding-ada-002',    #2
    object='list', 
    usage=Usage(prompt_tokens=517, total_tokens=517))  #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 Embedding vector
     <br/>#2 Model that generated embeddings
     <br/>#3 Usage statistics
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p119"> 
   <p>The extract only shows values for the first few vector dimensions (<strong class="cueball">1</strong>) (whereas the full vector has over 1,000 dimensions). Besides the embedding vector, the response contains the model name (<strong class="cueball">2</strong>) and usage statistics (<strong class="cueball">3</strong>). Unlike earlier, usage statistics only refer to the number of tokens in the prompt (which is also the total number of tokens processed). Unlike text completion, the language model only reads tokens but does not generate them.</p> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>The most relevant part for us is, of course, the embedding vector itself. You can access that embedding vector via the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p121"> 
   <div class="code-area-container"> 
    <pre class="code-area">response.data[0].embedding</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>Most of the time, invoking the language model once should provide you with the embedding you are searching for. Of course, when calculating embedding vectors for a large number of emails, we may run into problems (i.e., failed connection attempts) every once in a while. This is why the final version of our embedding function again contains a retry mechanism:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p123"> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai
client = openai.OpenAI()

def get_embedding(text):
    for nr_retries in range(1, 4):
        try:
            response = client.embeddings.create(
                model='text-embedding-ada-002',
                input=text)
            return response.data[0].embedding
        except:
            time.sleep(nr_retries * 2)
    raise Exception('Cannot query OpenAI model!')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p124"> 
   <p>Given a text as input, we try up to three times to get a corresponding embedding vector (increasing the delay between retries after each failed attempt). This is the function we will use.</p> 
  </div> 
  <div class="readable-text" id="p125"> 
   <h3 class=" readable-text-h3" id="clustering-vectors"><span class="num-string browsable-reference-id">4.4.3</span> Clustering vectors</h3> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>To cluster vectors (representing documents), we will use the k-means clustering algorithm. K-means is a very popular clustering algorithm that works by iteratively refining the mapping from vectors to clusters. Unlike other clustering algorithms, the algorithm requires you to specify the number of clusters in advance. In our example scenario, that means choosing how fine-grained the partitioning of emails by their content should be.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p127"> 
    <h5 class=" callout-container-h5 readable-text-h5">How does the k-means algorithm work?</h5> 
   </div> 
   <div class="readable-text" id="p128"> 
    <p> The k-means algorithm takes as input a set of elements to cluster and a target number of clusters. It works by iteratively refining the mapping from elements to clusters until a termination criterion (e.g., a maximum number of iterations or minimal changes in cluster assignments between consecutive iterations) is met. The k-means algorithm associates each cluster with a vector (representing the center of that cluster). In each iteration, it assigns each vector to the cluster with the nearest center. Then, it recalculates the vectors associated with clusters (by averaging over the vectors of all elements currently assigned to the cluster).</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p129"> 
   <p>We will be using the k-means implementation in the <code>scikit-learn</code> library. Follow the instructions in the first section of this chapter to ensure that this library is installed (import clustering methods via <code>from sklearn.cluster import KMeans</code>). After importing the library, we can invoke the k-means implementation with the following (concise) piece of code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p130"> 
   <div class="code-area-container"> 
    <pre class="code-area">def get_kmeans(embeddings, k):
    kmeans = KMeans(n_clusters=k, init='k-means++')
    kmeans.fit(embeddings)
    return kmeans.labels_</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>The function takes a list of embedding vectors and the number of target clusters as input and then clusters those vectors using the k-means implementation. The result of clustering is labels associated with each embedding vector. Those labels indicate the ID of the associated cluster.</p> 
  </div> 
  <div class="readable-text" id="p132"> 
   <h3 class=" readable-text-h3" id="end-to-end-code-for-text-clustering"><span class="num-string browsable-reference-id">4.4.4</span> End-to-end code for text clustering</h3> 
  </div> 
  <div class="readable-text" id="p133"> 
   <p>The following listing shows the complete code for clustering text documents via embedding vectors. You will recognize the functions for calculating embedding vectors (<strong class="cueball">1</strong>) and clustering them (<strong class="cueball">2</strong>).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p134"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__clustering"><span class="num-string">Listing <span class="browsable-reference-id">4.3</span></span> Clustering text documents using language models</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import openai
import pandas as pd
import time

from sklearn.cluster import KMeans

client = openai.OpenAI()

def get_embedding(text):                        #1
    """ Calculate embedding vector for input text.
    
    Args:
        text: calculate embedding for this text.
    
    Returns:
        Vector representation of input text.
    """
    for nr_retries in range(1, 4):
        try:
            response = client.embeddings.create(
                model='text-embedding-ada-002',
                input=text)
            return response.data[0].embedding
        except:
            time.sleep(nr_retries * 2)
    raise Exception('Cannot query OpenAI model!')

def get_kmeans(embeddings, k):                #2
    """ Cluster embedding vectors using K-means.
    
    Args:
        embeddings: embedding vectors.
        k: number of result clusters.
    
    Returns:
        cluster IDs in embedding order.
    """
    kmeans = KMeans(n_clusters=k, init='k-means++')
    kmeans.fit(embeddings)
    return kmeans.labels_

if __name__ == '__main__':         #3

    parser = argparse.ArgumentParser()
    parser.add_argument('file_path', type=str, help='Path to input file')
    parser.add_argument('nr_clusters', type=int, help='Number of clusters')
    args = parser.parse_args()
    
    df = pd.read_csv(args.file_path)
    
    embeddings = df['text'].apply(get_embedding)
    df['clusterid'] = get_kmeans(list(embeddings), args.nr_clusters)
    
    df.to_csv('result.csv')</pre> 
    <div class="code-annotations-overlay-container">
     #1 Calculates embedding vectors
     <br/>#2 Clusters embeddings
     <br/>#3 Reads text and writes out clusters
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>The main function of listing <a href="#code__clustering">4.3</a> (<strong class="cueball">3</strong>) reads data from a file on disk. Again, we assume that data is contained in a .csv file and focus on the <code>text</code> column. First, we iterate over text documents and generate corresponding embeddings (by invoking the <code>get_embedding</code> function, discussed previously). Then, we cluster embedding vectors via the <code>get_kmeans</code> function. The cluster IDs become an additional column in the result table written to disk.</p> 
  </div> 
  <div class="readable-text" id="p136"> 
   <h3 class=" readable-text-h3" id="trying-it-out-1"><span class="num-string browsable-reference-id">4.4.5</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>Time to try clustering via embedding vectors! You can find the code from listing <a href="#code__clustering">4.3</a> on the book’s companion website (listing3.py), as well as a suitable data set (textmix.csv). This data set contains a mix of text snippets from two sources: a collection of poems and a repository of emails. We’ll try to separate the two via clustering: we expect emails and poems to be assigned to different clusters.</p> 
  </div> 
  <div class="readable-text" id="p138"> 
   <p>Change into the directory containing the code and data, and run the following command in the terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p139"> 
   <div class="code-area-container"> 
    <pre class="code-area">python listing3.py textmix.csv 2</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p140"> 
   <p>Here, textmix.csv is the name of the input file, and 2 is the number of target clusters (in this specific case, two seems like a reasonable choice, whereas determining the right number of clusters can be more difficult in other scenarios). The result will be stored in the file result.csv. It contains all the columns from the input file, as well as an additional column with the cluster ID (because we only use two clusters, this ID is either 0 or 1). Running the command, you will likely see a result that places emails in one cluster while putting poems in the other.</p> 
  </div> 
  <div class="readable-text" id="p141"> 
   <p>You may want to try different models to see differences in run time and result quality. You can also try different input text and vary the number of clusters. Besides that, you may want to implement some of the other use cases for embedding vectors, which are mentioned at the beginning of this section. For instance, how about implementing a retrieval interface that maps a natural language statement to the most closely related document (by comparing the embedding vectors of questions and documents)?</p> 
  </div> 
  <div class="readable-text" id="p142"> 
   <h3 class=" readable-text-h3" id="other-use-cases-for-embedding-vectors"><span class="num-string browsable-reference-id">4.4.6</span> Other use cases for embedding vectors</h3> 
  </div> 
  <div class="readable-text" id="p143"> 
   <p>So far, we have used vectors to identify similar documents via clustering. But this is not the only use case for embedding vectors! To name just a few examples, embedding vectors are often used to facilitate the retrieval of text documents related to a natural language question. Here, we compare an embedding vector associated with the question to embedding vectors associated with documents. Documents with similar vectors are more likely to be useful in answering the question.</p> 
  </div> 
  <div class="readable-text" id="p144"> 
   <p>For instance, we hope that the embedding vectors for the question “What is a Transformer model?” and the text “The Transformer is a neural network architecture, often used for language models” are similar due to related topics. If so, we can identify the document most relevant to the question by comparing embedding vectors. More precisely, we calculate embedding vectors once for each document that may be useful to answer questions. Then, whenever a new question is received, we calculate the associated embedding vector and retrieve documents with similar embedding vectors. We can then generate an answer based on those documents.</p> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>Another use case for embedding vectors is outlier detection. To identify text documents from a set that are strikingly different from other documents in the same set, we can compare their embedding vectors. Again, we only need to calculate embedding vectors once for each document. In doing so, we avoid having to use language models to compare documents. Instead, we simply compare embedding vectors (which is very fast).</p> 
  </div> 
  <div class="readable-text" id="p146"> 
   <p>In summary, although we have focused on clustering, there are many use cases for embedding vectors. This makes it worthwhile to learn how to generate and use them!</p> 
  </div> 
  <div class="readable-text" id="p147"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p148">You can apply language models directly to analyze text data.</li> 
   <li class="readable-text" id="p149">Prompts typically contain text to analyze, along with instructions. Instructions describe the task to solve as well as the output format.</li> 
   <li class="readable-text" id="p150">You can use chat completion for classification, extraction, and question answering.</li> 
   <li class="readable-text" id="p151">Raw model output may need postprocessing to change the format.</li> 
   <li class="readable-text" id="p152">Language models can transform a text into embedding vectors. You can create embedding vectors via the embedding endpoint. Comparing embedding vectors is relatively efficient.</li> 
   <li class="readable-text" id="p153">You can use embeddings for clustering, retrieval, and outlier detection.</li> 
  </ul> 
  <div class="readable-text" id="p154"> 
   <h2 class=" readable-text-h2" id="references"><span class="num-string browsable-reference-id">4.6</span> References</h2> 
  </div> 
  <ol> 
   <li class="readable-text" id="p155">Katz, D. M., Bommarito, M. J., Gao, S., et al. (2024). GPT-4 Passes the Bar Exam. <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 382</em>(2270), 1–17.</li> 
  </ol>
 </div></div></body></html>