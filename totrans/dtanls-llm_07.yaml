- en: 6 Analyzing images and videos
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 分析图像和视频
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Analyzing images
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析图像
- en: Comparing images
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较图像
- en: Analyzing videos
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析视频
- en: In the previous chapters, we have seen how to analyze text and structured data.
    Does that cover everything? Not even close! By far, the largest portion of data
    out there comes in the form of images and videos. For instance, videos alone account
    for an impressive two-thirds of the total data volume exchanged over the internet!
    In this chapter, we will see how language models can also help us extract useful
    insights from such data types.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了如何分析文本和结构化数据。这涵盖了所有内容吗？远远没有！到目前为止，最大的数据部分是以图像和视频的形式存在的。例如，仅视频就占互联网交换的总数据量的令人印象深刻的三分之二！在本章中，我们将看到语言模型如何帮助我们从这些数据类型中提取有用的见解。
- en: The following sections introduce a couple of small projects that process images
    and video data. GPT-4o is a natively multimodal model; we can use it for all these
    tasks. First, we will see how to use GPT-4o to answer free-form questions (in
    natural language) about images. Second, we will use GPT-4o to build an automated
    picture-tagging application, automatically tagging our holiday pictures with the
    people who appear in them.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几节介绍了一些处理图像和视频数据的小型项目。GPT-4o 是一个本地的多模态模型；我们可以用它来完成所有这些任务。首先，我们将看到如何使用 GPT-4o
    来回答关于图像的自由形式问题（用自然语言）。其次，我们将使用 GPT-4o 来构建一个自动图片标记应用程序，自动为我们的假日照片标记出现的人物。
- en: Finally, we will use GPT-4o to automatically generate titles for video files.
    The goal of these mini-projects is to illustrate features for visual data processing
    offered by the latest generation of large language models. After working through
    those projects, you should be able to build your own applications for image and
    video data processing in various scenarios.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用 GPT-4o 来自动为视频文件生成标题。这些小型项目的目标是展示最新一代大型语言模型提供的视觉数据处理功能。完成这些项目后，你应该能够构建自己的图像和视频数据处理应用程序，用于各种场景。
- en: 6.1 Setup
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 设置
- en: 'You will need to install one more Python package to run the example code. Specifically,
    you need OpenCV, a library for image processing. In the terminal, run the following
    command to install OpenCV:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要安装一个额外的 Python 包来运行示例代码。具体来说，你需要 OpenCV，这是一个用于图像处理的库。在终端中，运行以下命令来安装 OpenCV：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will use this library to, for example, read images from disk and split videos
    into frames.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个库，例如，从磁盘读取图像并将视频分割成帧。
- en: 'Next, you need to install one more library, enabling you to send requests directly
    to OpenAI’s web services (which you will use to send pictures stored locally to
    OpenAI):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要安装一个额外的库，这将使你能够直接向 OpenAI 的网络服务发送请求（你将使用它来发送存储在本地的图片到 OpenAI）：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Well done! If you didn’t encounter any error messages running these commands,
    your system is now configured for image and video data analysis using GPT-4o.
    Let’s start with our first project in the next section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 做得很好！如果你在运行这些命令时没有遇到任何错误信息，那么你的系统现在已经配置好了，可以使用 GPT-4o 进行图像和视频数据分析。让我们从下一节开始我们的第一个项目。
- en: 6.2 Answering questions about images
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 回答关于图像的问题
- en: Neural networks for detecting objects (such as cars) in images have been around
    for many years. So what’s the big deal about processing images with GPT-4o?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像中检测物体（如汽车）的神经网络已经存在很多年了。那么使用 GPT-4o 处理图像有什么大不了的？
- en: The primary limitation of classical models for image processing is that they
    need to be trained for specific analysis tasks. For example, let’s say you have
    a neural network that is really good at detecting pictures of cats. You can use
    that to filter out cat pictures from your personal collection. However, maybe
    you’re not into cats in general but are specifically interested in golden Persian
    cats. Unless your model is trained to detect that specific type of cat, you’re
    out of luck and need to label enough example pictures yourself. Doing that is
    tedious, and you may end up not using the model and instead going through the
    pictures by hand. The big deal about image processing with GPT-4o (and similar
    models) is that it solves a wide range of tasks with images based on just a description
    of the task (in natural language).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: We will use that to build a generic question-answering system for images. As
    a user, you will formulate arbitrary questions in natural language and point to
    a picture, and the system will generate a text answer. For example, asking the
    system to detect “golden Persian cats” in a picture should work out of the box
    without needing task-specific training data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Specifying multimodal input
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will create a system that takes two inputs:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: A URL leading to an image on the web
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A natural language question about the image
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output is an answer to the question (as text). Internally, the system uses
    GPT-4o to process the question on the input image. It generates multimodal prompts,
    combining multiple types of data (here, text and images). Figure [6.1](#fig__vqaprompt)
    shows an example prompt: it contains one image (of an apple) and a question about
    the image (whether the image shows a banana). The correct answer is “No” in this
    case.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F01_Trummer.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 Multimodal prompt containing an image and text. The prompt instructs
    the language model to decide whether the picture shows a banana. In this case,
    the expected output is “No” (otherwise “Yes”).
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: How can we create such prompts for GPT-4o? We can reuse the chat completions
    endpoint for that. As a reminder, this endpoint takes as input a list of prior
    messages exchanged between the user and (potentially) the system. For our visual
    question-answering system, we only need a single message (that originates from
    the user).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the prior code, messages can now contain multimodal content. In this
    specific case, this content consists of one text snippet (the question asked by
    the user) and one image (specified as a URL for the moment). This is the message
    we will use in the following code (`question` is a variable containing the question
    text, and `image_url` is the URL to the image):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Question text'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Image URL'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: First, note the `role` attribute identifying the message as generated by the
    user. Second, the message content is specified as a list of Python dictionaries.
    Each of those dictionaries describes one element of the message. As we are now
    considering multimodal data—that is, images and text—we need to clarify the type
    (or *modality*) of each input element. This is accomplished by setting the `type`
    attribute to either `text` or `image_url`. The actual content is specified using
    either the `text` attribute (**1**) or (in the case of an image) the `image_url`
    attribute (**2**). GPT-4o is flexible enough to understand that the question refers
    to the image and to process both appropriately.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip Whereas the input contains a single picture, the content of a message may
    contain multiple elements of the same type: for example, multiple images. We will
    exploit that capability for the project in the next section.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Code discussion
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following listing shows the complete code for our visual question-answering
    system. Taking the image URL and a question as input (**3**), the actual magic
    (i.e., visual question-answering) happens in the `analyze_image` function (**1**).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Answering questions about images via language models
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Answers question about an image'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Multimodal content'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Input parameters'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: As you see, all it takes is a few lines of Python code to answer questions about
    images! The function `analyze_image` (**1**) contains but a single call to GPT-4o,
    using the message described in the previous subsection (**2**). The fact that
    we now provide multimodal input does not change the format of the answer. Again,
    we get an object containing a message generated by the language model. Although
    the input may now be multimodal, the output is text. As we instructed the language
    model to generate an answer to the input question (**3**) (i.e., exactly what
    the user is looking for), the output is directly printed out for the user.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Trying it out
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time to test our visual question-answering system! You’re back at Banana (a
    producer of various consumer electronics, including laptops and smartphones, introduced
    in chapter 2) and looking for a new company logo. You want to base your logo on
    a picture of a banana. Searching the web, you find large repositories with images
    of fruit. But which of them are bananas? Instead of going through images by hand,
    you would much rather delegate that task to a language model. Luckily, you can
    directly use the code from the previous section by specifying the URL of each
    fruit picture, together with the question “Is this a banana (“Yes”,“No”)?” This
    means you’re using the visual question-answering system essentially as a classification
    method (which is only one of many possible use cases). You can then write a simple
    script, iterating over all relevant URLs and retaining the ones where the answer
    is “Yes.”
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'On the book’s companion website, you will find the code from listing [6.1](#code__visualqa)
    as well as pictures of fruit (look for the links labeled Fruit 1 to Fruit 5).
    Download the code, change to the containing repository in the terminal, and run
    the following code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this command, replace `[URL]` with the URL of the image (you can obtain a
    suitable URL by, for example, copying the Fruit 1 link on the book’s website).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Object classification is relatively easy, particularly for objects as common
    as bananas. So you should see accurate results for most examples. Try a few different
    fruits and possibly other images of your choice. The range of questions you can
    ask is virtually unlimited (putting aside the rather generous input length limit
    of 128,000 tokens, about 300 pages of text).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'A word of caution may be in order when it comes to processing costs. Processing
    images via GPT-4o can be expensive! The precise cost depends on the image size
    and the degree of detail used for image processing. You can control the degree
    of precision using the `detail` parameter. For instance, choose a low degree of
    precision using the following specification of image URLs (in the model input):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Set the `detail` attribute to `low` to pay a cost equivalent to 85 tokens per
    image (i.e., the cost equivalent of processing a text with 85 tokens using GPT-4o).
    If you set the degree of detail to `high` (the default), the cost consists of
    a fixed amount of 85 tokens and a variable amount that depends on the image size.
    To calculate the variable cost component, we first scale the image to a size (in
    pixels) of 2,048 × 2,048 (while maintaining the aspect ratio). This scaling step
    only applies to pictures with a size beyond 2,048 × 2,048 pixels. The second scaling
    step is performed in any case. It scales the shorter side of the image to a size
    of 768 pixels. Now consider the minimum number of 512 × 512 pixel squares needed
    to cover the image after the second scaling step. The variable cost component
    is proportional to the number of squares multiplied by a factor of 170 tokens
    (the cost per square, set by OpenAI).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let’s say we want to process an image of size 1,024 × 1,024 pixels
    with high precision. In that case, we can skip the first scaling step, as the
    image still fits within a 2,048 × 2,048 pixel square. The second scaling step,
    however, is performed in any case. It scales the image to a size of 768 × 768
    pixels. To cover a square with a length of 768 pixels on both sides, we require
    four squares of size 512 × 512 pixels. That means that to process our image, we
    pay the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 85 tokens (the fixed cost component)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4 × 170 tokens = 680 tokens (the variable cost component)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In total, we therefore pay 85 + 4 × 170 = 765 tokens. Given current prices,
    this corresponds to $0.003825 (i.e., less than one cent). Although that may seem
    acceptable, always keep costs in mind when processing large repositories of images
    via language models.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip To find the price for processing images with a specific resolution, you
    can also use the OpenAI price calculator: [https://openai.com/pricing](https://openai.com/pricing).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Tagging people in images
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine the following situation: you just came back from a (well-deserved)
    holiday with friends, and, of course, you have taken a large number of pictures.
    You want to send your friends the pictures in which they appear. But how to do
    that efficiently? You could go through the pictures by hand and tag each friend
    individually. But having just come back from vacation, your email inbox is overflowing,
    and you don’t have time to go through holiday pictures. What can you do?'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Overview
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will create a small application that automatically tags
    people in images. Users provide three inputs:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The path of a directory containing the pictures to tag
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The path of a directory containing pictures of the people to look for
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The path to an output directory into which tagged pictures are written
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To keep things simple, we will use filenames to represent tags. We assume that
    pictures showing people to look for are named after the person they show. For
    example, let’s say we have images named Joe.png and Jane.png in our directory
    containing the people to look for. Given a picture to tag, we will simply change
    the filename by prefixing it with the names of people that appear in it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: For instance, assume we have an image called beach.png in which both Joe and
    Jane appear. Then, in the output directory, we will create two files called Joebeach.png
    and Janebeach.png, showing that both appear in the beach picture. If we want to
    send out all pictures showing the same person, such as Joe, we can search for
    all files whose name satisfies the regular expression `Joe*.png` (with `*` representing
    arbitrary strings).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Internally, as a first step, we need to load pictures representing people to
    look for, as well as the pictures to tag. We will consider each pair of a person
    to look for and a picture to tag. For example, if we are looking for five people
    and have 10 pictures to tag, that makes 50 pairs to consider. For each of these
    pairs, we use GPT-4o to decide whether the corresponding person appears in the
    picture to tag.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: To make that happen, we will need multimodal prompts containing text and two
    pictures. The first picture shows the person to look for, and the second picture
    shows the picture to tag. Via text, we can instruct the language model to compare
    the pictures to decide whether the same person appears. Whenever we find a match—that
    is, a combination of a person and a picture in which that person appears—we will
    copy the corresponding picture to the output folder, prefixing its name with the
    name of the person.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Figure [6.2](#fig__tagprompt) shows an example prompt. On the left, we have
    a picture of Jane, one of the people we are looking for. On the right side, we
    have a picture to tag. The text instructions ask the language model to compare
    the two pictures, producing the answer “Yes” if they show the same person (and
    “No” otherwise). In this case, the pictures do not show the same person, and the
    correct answer should be “No.”
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F02_Trummer.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2 Multimodal prompt containing two images and text: the prompt instructs
    the language model to check whether the two pictures show the same person (expected
    answer: “Yes”) or not (expected answer: “No”).'
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.3.2 Encoding locally stored images
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we used GPT-4o to analyze images on the web. Now we
    are talking about our private holiday pictures. We may not want to make all of
    them publicly accessible on the web. So how can we share them with GPT-4o alone?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: We may have to convert images into a format suitable for GPT-4o. GPT-4o supports
    a wide range of image formats, including PNG, JPEG, WEBP, and GIF. For any format,
    the image file size is currently limited to 20 MB. To upload pictures of the supported
    types to GPT-4o, we first need to encode them using a base64 encoding.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: What is base64 encoding?
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The base64 encoding is a way to encode binary data as a printable string. As
    the name base64 suggests, the alphabet we use for the string is based on 64 characters.
    This means we can represent each character using six bits (because six bits allow
    representing 2⁶ = 64 possible characters). As computers store data at the granularity
    of bytes (i.e., 8 bits), it is convenient to encode groups of three bytes (i.e.,
    24 bits) together. Using base64 encoding, three bytes can be used to represent
    four characters (as 24/6 = 4).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, we can use the `base64` library to encode binary data in the base64
    format. The following code opens an image file stored at `image_path` and encodes
    it using base64 format:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We have transformed the binary image data into a string in base64 format. Before
    sending such images to GPT-4o, we still need to make one final transformation:
    we must represent the string using the UTF-8 encoding.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: What is UTF-8 encoding?
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'UTF-8 is a way to represent string data. It is extremely popular and used by
    about 98% of sites on the web. UTF-8 can represent over a million characters,
    covering a variety of languages. We can represent those characters using a fixed
    number of bytes: four bytes to represent each character. However, this is inefficient
    because it does not exploit the fact that certain characters are much more common
    than others. If we encode common characters with fewer bytes while reserving many-byte
    representations for the less common ones, we can represent the same text with
    fewer bytes. This is what UTF-8 does, and because different characters may need
    a different number of bytes for representation, it is also called a *variable-length
    standard*. At the same time, UTF-8 is designed to be backward-compatible with
    the older ASCII standard, using the same encoding as ASCII for the first 128 characters.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'To transform our base64 string encoding of the image into UTF-8, we can use
    Python’s `decode` function. Assuming that the image is still encoded in the `encoded`
    string variable, we can do so using the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting image, encoded as UTF-8 text string, is suitable as input for
    GPT-4o. Next, we will see how we can upload images in this format to the OpenAI
    platform. After uploading them, we can include references to those pictures in
    our prompts. Images are generally specified as components of the prompt:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, `image_url` represents the URL that leads to the image to analyze. Previously,
    we used publicly accessible URLs for that. Now we are analyzing private images
    that we will send to OpenAI to be used only to process specific requests. Assuming
    that `image` still represents the image encoded as a string, we can set the image
    URL as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This code assumes that the image is of type PNG (if not, replace the string
    `png` with the appropriate format identifier such as `jpeg`). The URL combines
    metadata about the image (such as the image type and encoding) with a string suffix
    representing the picture itself.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3 Sending locally stored images to OpenAI
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use this project as an opportunity to demonstrate an alternative way
    to interact with GPT models. Doing so will give us insights into how OpenAI’s
    Python library works internally. So far, we have been using Python wrappers that
    send requests to OpenAI’s platform in the background. To send our local images
    to GPT-4o, we will create those requests ourselves.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: We use Python’s `requests` library to create HTTP requests, sending our prompts
    (with text and images) to GPT-4o and collecting the answer. More precisely, we
    will be sending HTTP Post requests. This is the type of request accepted by the
    OpenAI platform. Such requests can be sent via the `requests.post` method.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Our requests will contain all relevant information needed by GPT-4o to solve
    the task we are interested in (in this case, verifying whether two images show
    the same person). First, we need to include headers in the request. We will use
    the following headers:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You see that we’re specifying headers as a Python dictionary. For our use case,
    we only need to store two properties: the type of our payload (we plan to send
    JSON content) and our access credentials (the three dots represent our OpenAI
    access key).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to specify the payload—that is, the content that we primarily
    want to send via the request:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Model specification'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '#2 First message'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Output length'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'You may notice that the payload contains exactly the fields we would typically
    specify in our invocations to the `completions.create` method. That is not a coincidence,
    as the latter method creates requests with a similar payload internally. First,
    the payload specifies the model (**1**): `gpt-4o` (to be able to process multimodal
    input prompts). We specify a list of messages with a single entry (**2**). This
    message is marked as originating from the user (`role:user`), and its content,
    abbreviated by three dots, will contain text instructions and images. Finally,
    we limit the answer length to a single token (`max_tokens:1`) (**3**). That makes
    sense because we are searching for a binary result: either the same person appears
    in multiple input images (expected answer: “Yes”) or not (expected answer: “No”).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'Having generated headers and a payload, we can invoke GPT-4o using the following
    code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As the first parameter, the invocation of `requests.post` specifies the URL
    to send the request to. In this case, `https://api.openai.com/v1/chat/completions`
    indicates that we want to perform a task of type `Completion`, using one of OpenAI’s
    chat models (which applies to GPT-4o). We use the headers and payload created
    previously.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'The response contains the GPT-4o result object. We can access the answer (indicating
    whether two images show the same person) via the following code snippet:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 6.3.4 The end-to-end implementation
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are now ready to discuss the end-to-end implementation! Listing [6.2](#code__taggingpictures)
    contains code for tagging people in pictures. Have a look at the main function
    (**4**) first. As discussed previously, users specify three directories as command-line
    parameters (**5**): a directory containing pictures to tag, a directory containing
    people to use for tagging, and an output directory.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, we load all images to tag as well as all images of the people
    to search for. We use the `load_images` function (**1**) for that. This function
    retrieves a list of all files in the input directory and then considers those
    ending with the suffix .png (i.e., we consider all PNG images). As discussed previously,
    we need to encode images as strings (via base64 encoding) that are ultimately
    represented via the UTF-8 encoding. The result of `load_images` is a Python dictionary
    mapping filenames to the associated, encoded images. This dictionary is returned
    as the result of the function.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 Tagging people in pictures stored locally
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Loads images from disk'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates a multimodal prompt'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Generates an answer for the prompt'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Tags images with people'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Command-line parameters'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Over people'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Over images'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Copies image in case of a match'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: After applying the `load_images` function to each of the two input directories,
    we end up with two Python dictionaries. One maps filenames of images showing people
    (which, by convention, are the names of those people) to the corresponding encoded
    image. The other maps the filenames of images to be tagged to the encoded images.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to match each picture to be tagged to all people that appear in
    it. As we use prompts comparing only two pictures at once, we need to look at
    each combination of a person and of an image to tag. That is why we use a double-nested
    `for` loop: one iterates over people (**6**) and the other over images to tag
    (**7**).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: For each combination of an image to tag and a person, we create a multimodal
    prompt using `create_prompt`. This function (**2**) assembles both encoded pictures,
    together with text instructions, into a prompt. The text instructions (“Do the
    images show the same person (“Yes”/“No”)?”) define the task as well as the expected
    output format (“Yes” or “No”). Each prompt is sent to GPT-4o via `call_llm`. As
    discussed previously, this function (**3**) uses the requests API to send locally
    stored images, together with text instructions, to GPT-4o. If GPT-4o answers “Yes,”
    the currently considered person appears in the currently considered image to tag.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: If the person appears in the image (**8**), we tag the image as follows. We
    use the name of the person (the name of the associated picture file without the
    .png suffix) and prepend it to the name of the file to tag. Next, we copy the
    file to tag to the output directory using the new filename (which indicates the
    tagging result).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.5 Trying it out
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s try it! If you have real vacation pictures to tag, you can use them.
    Otherwise, you will find suitable test data on the book’s companion website. Look
    for the Tagging link to access a zipped file; download this file and unzip its
    contents. After decompression, you should see three subdirectories in the resulting
    folder:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '*people*—A folder containing pictures of people (in this case, actors from
    the *Avengers* series). Filenames contain the names of the corresponding actors.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*pics*—Another set of pictures (in this case, more pictures of the same actors
    as in the people folder) to tag with the names of actors.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*processed*—An empty folder that can be used as the output directory.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ll assume that the decompressed folder is stored under the path /tagging
    (e.g., the path /tagging/people then leads to the subfolder with pictures of people
    to search for). Execute the code by running the following command from the terminal:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Tip If you are invoking the code on a Windows platform, you will have to adapt
    these paths. In particular, you will have to replace / with \.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: During processing, the implementation prints updates on whether specific people
    appear in specific pictures. The sample data contains two people to look for and
    four images to tag. This means processing should not take more than a few minutes
    (typically less than two).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: After processing finishes, look in the output folder. You should see pictures
    to tag, prefixed with the names of people appearing in them. Not bad for a few
    lines of Python code!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Generating titles for videos
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides many pictures (which we can now automatically tag, thanks to the code
    outlined in the previous section!), you also took quite a few videos on vacation.
    Automatically assigned filenames are not very informative. Which video is the
    one showing you swimming in the ocean? It would be great to assign meaningful
    captions to those videos and help you find the ones you’re looking for faster.
    But who has time to manually label videos? Again, we can use language models to
    do that task automatically.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1 Overview
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will develop a system that automatically assigns suitable titles to videos.
    This system uses GPT-4o in the background. To assign titles to videos, we will
    submit multimodal prompts containing video frames (i.e., images) together with
    text instructing the language model to come up with a title. Figure [6.3](#fig__videoprompt)
    shows an example prompt.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F03_Trummer.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3 Multimodal prompt for video processing: based on a selection of
    video frames, the language model is instructed to generate a suitable title.'
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It consists of multiple video frames (we only see the first and the last frame
    in figure [6.3](#fig__videoprompt); the three dots represent the ones in between)
    and the text instructions “Generate a concise title for the video.” Note that
    we have to pay for each video frame we’re submitting to GPT-4o. That means video
    data processing via GPT-4o quickly becomes expensive!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: As an answer, GPT-4o should send back a reasonable title. In the example shown
    in figure [6.3](#fig__videoprompt), this can be a reference to cars and, potentially,
    even a reference to the location (shown as white text in the frames).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2 Encoding video frames
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First we need to discuss video formats. In the last section, we saw how to encode
    images stored locally. Now we will expand that to videos. Ultimately, our goal
    is to extract a sequence of frames. However, videos are typically not stored as
    a sequence of frames but using more efficient encodings. For us, that means we
    first have to extract images from a video.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: We will use the OpenCV library for that. OpenCV is the Open Source Computer
    Vision Library. It provides various functionalities for computer vision as well
    as for image and video processing in general. Of course, we will use GPT-4o to
    do the computer vision part. Nevertheless, OpenCV will be useful for extracting
    frames from videos. If you haven’t done so already, now would be a good time to
    set up OpenCV by following the instructions in section [6.1](#sec__videosetup).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that the installation has worked and you can access OpenCV from
    Python. The corresponding Python library is called `cv2` (a name you will often
    see as a prefix in the following code snippets).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with a video stored locally, we first need to open the corresponding
    file. Run this code to open a video stored under the path `video_path`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Using the variable `video`, we can now read the video’s content via the `read`
    method:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result consists of tuples with two components: a `success` flag and a video
    frame. The `success` flag indicates whether we were able to read another frame.
    That’s no longer the case once we reach the end of the video. In that case, we
    do not obtain a valid frame, and the `success` flag is set to `False`.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume that we are able to read another frame. In that case, we will
    turn the frame into an image we can send to GPT-4o. OpenCV has us covered and
    provides the corresponding functionality:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `imencode` function turns a video frame into an image of the corresponding
    type. Here, we transform the frame into a JPEG picture. From the resulting tuple,
    the second component (`buffer`) is interesting for our purposes. It contains a
    binary representation of the corresponding picture.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s a situation we know from the previous section: we have a binary representation
    of an image and want to turn it into a suitable format for GPT-4o. Again, we first
    encode the image as a string via base64 encoding and then represent that string
    via UTF-8:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The resulting `frame` is encoded properly to be included as part of a GPT-4o
    prompt. Once you’re done processing the video, close the corresponding video capture
    object using the following code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we will put everything together to generate video titles for arbitrary
    videos.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.3 The end-to-end implementation
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Listing [6.3](#code__videocaption) generates titles for videos stored locally.
    The only input parameter is the path to the video. Given that, the implementation
    extracts some of the video frames (**4**) and then generates a prompt instructing
    GPT-4o to generate a video title based on a sample of frames. After sending this
    prompt to the language model, the answer contains a proposed video title.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 Generating a video title via language models
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#1 Extracts video frames'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates multimodal prompt'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Queries the language model'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Titles videos'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: The code extracts video frames using `extract_frames` (**1**). As discussed
    previously, this function uses the OpenCV library to open the video for frame
    extraction and proceeds to read each frame consecutively. We will only use up
    to 10 frames to generate a video title. That’s why extraction ends after at most
    10 frames (or fewer if the video is very short). Each extracted frame is encoded
    according to GPT-4o’s requirements (i.e., JPEG images encoded as strings). The
    result of the function is a list of encoded frames.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: During prompt generation (**2**), we combine relevant text instructions (“Generate
    a concise title for the video.”) with the first 10 frames from the video. To send
    those images, along with instructions, to GPT-4o, we use a Python wrapper again
    (**3**). Alternatively, we can create requests ourselves (as in the previous project).
    The response of the language model should contain a suitable title for our video.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we are only sending the first few frames of the video. If the video
    content changes drastically after those few frames, the title may not be optimal.
    The reason we only send 10 frames is computation fees. Keep in mind that you’re
    paying for each picture submitted in the prompt! Sending all frames of larger
    videos is typically prohibitively expensive. That’s why we content ourselves with
    sending only a small subset of video frames.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.4 Trying it out
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s try our video title generator! On the book’s companion website, this chapter’s
    section includes a Cars link that will guide you to a short video from a traffic
    camera showing traffic on a busy road. Download the video to your local machine.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Open the terminal, and change to the directory containing the code for this
    chapter. We’ll assume that the video was downloaded into the same directory (if
    not, replace the name of the video, cars.mp4, with the full path leading to it).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After a few seconds of computation time, you should see a proposal for a video
    title: for example, “Traffic Conditions on I-5 at SR 516 and 188th Street” (the
    precise title may vary across different runs due to randomization).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the title integrates information—the name of the location—that is
    only available in the form of text in the video. Using GPT-4o to extract text
    from images may be useful in various scenarios: for example, to extract data from
    forms.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-4o processes images as well as text.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompts can integrate text snippets and images.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-4o supports multiple image formats.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Images can be specified via a public image URL.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locally stored images can be uploaded to OpenAI.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-4o processes images in string encoding.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing images is costly compared to processing text. The cost of image processing
    may depend on the image size. Processing images with a low degree of detail reduces
    costs.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `base64` library can encode images as strings.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decompose videos into their frames to send them to GPT-4o.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCV library can be used to extract frames from videos.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
