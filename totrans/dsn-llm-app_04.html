<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Vocabulary and Tokenization"><div class="chapter" id="chapter-LLM-tokenization">
<h1><span class="label">Chapter 3. </span>Vocabulary and Tokenization</h1>


<p>In <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>, we dug deep into the datasets that are used to train the language models of today, including the process of creating them. Hopefully this foray has underscored how influential pre-training data is to the resulting model. In this chapter, we will discuss another fundamental ingredient of a language model: its vocabulary.</p>






<section data-type="sect1" data-pdf-bookmark="Vocabulary"><div class="sect1" id="id37">
<h1>Vocabulary</h1>

<p>What do you do first when you start learning a new language? You start acquiring its vocabulary<a data-type="indexterm" data-primary="vocabulary and tokenization" id="xi_vocabularyandtokenization3896"/><a data-type="indexterm" data-primary="tokens and tokenization" id="xi_tokensandtokenization3896"/>, expanding it as you gain more proficiency in the language. Let’s define vocabulary here as:</p>
<blockquote>
<p>All the words in a language that are understood by a specific person.</p></blockquote>

<p>The average native English speaker has a vocabulary of <a href="https://oreil.ly/bkc2C">20,000–35,000 words</a>. Similarly, every language model has its own vocabulary, with most vocabulary<a data-type="indexterm" data-primary="tokens as unit of measure for LLMs" id="id657"/> sizes ranging anywhere between 5,000 and 500,000 <em>tokens</em>.</p>

<p>As an example, let us explore the vocabulary of the GPT-NeoX-20B model<a data-type="indexterm" data-primary="GPT-NeoX 20B model" id="id658"/><a data-type="indexterm" data-primary="Eleuther AI" data-secondary="GPT-NeoX 20B model" id="id659"/>.
Open the file <a href="https://oreil.ly/Kages"><em>tokenizer.json</em></a> and Ctrl+F for “vocab,” a dictionary containing the vocabulary of the model. You can see that the words comprising the language model vocabulary don’t entirely look like English language words that appear in a dictionary. These word-like units are called “types,” and the instantiation of a type (when it appears in a sequence of text) is called a token.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Recently, and especially in industry, I seldom hear anyone use the term “type” except in older NLP textbooks. The term “token” is broadly used to refer to both the vocabulary units and when they appear in a text sequence. We will henceforth use the word “token” to describe both concepts, even though I personally am not the biggest fan of this usage.</p>
</div>

<p>In the vocabulary file, we see that next to each token is a number, which is called<a data-type="indexterm" data-primary="input ID (token)" id="id660"/><a data-type="indexterm" data-primary="token index" id="id661"/> the <em>input ID</em> or the <em>token index</em>. The vocabulary size of GPT-NeoX is just above 50,000.</p>

<p>Looking at the vocabulary file in detail, we notice that the first few hundred tokens are all single-character tokens, such as special characters, digits, capital letters, small letters, and accented characters. Longer words appear later on in the vocabulary. A lot of tokens correspond to just a part of a word, called<a data-type="indexterm" data-primary="subwords" id="id662"/> a <em>subword</em>, like “impl,” “inated,” and so on.</p>

<p>Let’s Ctrl+F for “office.” We get nine results:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s2">"Ġoffice"</code><code class="p">:</code> <code class="mi">3906</code>
<code class="s2">"Ġofficer"</code><code class="p">:</code> <code class="mi">5908</code>
<code class="s2">"Ġofficers"</code><code class="p">:</code> <code class="mi">6251</code>
<code class="s2">"ĠOffice"</code><code class="p">:</code> <code class="mi">7454</code>
<code class="s2">"ĠOfficer"</code><code class="p">:</code> <code class="mi">12743</code>
<code class="s2">"Ġoffices"</code><code class="p">:</code> <code class="mi">14145</code>
<code class="s2">"office"</code><code class="p">:</code> <code class="mi">30496</code>
<code class="s2">"Office"</code><code class="p">:</code> <code class="mi">33577</code>
<code class="s2">"ĠOfficers"</code><code class="p">:</code> <code class="mi">37209</code></pre>

<p>The Ġ character refers to a space before the word. For instance, in the sentence, “He stopped going to the office,” the space before the letter “o” in the word “office” is considered part of the token.
You can see that the tokens are case-sensitive: there are separate tokens for “office” and “Office.” Most models these days have case-sensitive vocabularies. Back in the day, the BERT model was released with both a cased and an uncased version.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Language models learn vector representations called embeddings for each of these tokens that reflect their syntactic and semantic meaning. We will go through the learning process in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>, and dive deeper into embeddings in <a data-type="xref" href="ch11.html#chapter_llm_interfaces">Chapter 11</a>.</p>
</div>

<p>Cased vocabularies<a data-type="indexterm" data-primary="cased versus uncased vocabularies" id="id663"/><a data-type="indexterm" data-primary="uncased versus cased vocabularies" id="id664"/> are almost always better, especially when you are training on such a huge body of text such that most tokens are seen by the model enough times to learn distinctive representations for them. For instance, there is a definite semantic difference between “web” and “Web,” and it is good to have separate tokens for them.</p>

<p>Let’s search for some numbers. Ctrl+F for “93.” There are only three results:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s2">"93"</code><code class="p">:</code> <code class="mi">4590</code>
<code class="s2">"937"</code><code class="p">:</code> <code class="mi">47508</code>
<code class="s2">"930"</code><code class="p">:</code> <code class="mi">48180</code></pre>

<p>It seems like not all numbers get their own tokens! Where is the token for 934? It is impractical to give every number its own token, especially if you want to limit your vocabulary size to say, just 50,000. Later in this chapter, we will discuss how vocabulary sizes are determined.
Popular names and places get their own token. There is a token representing Boston, Toronto, and Amsterdam, but none representing Mesa or Chennai. There is a token representing Ahmed and Donald, but none for Suhas or Maryam.</p>

<p>You might have noticed that tokens like:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s2">"]);"</code><code class="p">:</code> <code class="mi">9259</code></pre>

<p>exist, indicating that GPT-NeoX is also primed to process programming languages.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id665">
<h1>Exercise</h1>
<p>Go through the <a href="https://oreil.ly/FxPcz"><em>tokenizer.json</em> file</a> and explore the vocabulary in detail. Specifically:</p>

<ul>
<li>
<p>What are some unexpected tokens you see?</p>
</li>
<li>
<p>What are the top ten longest tokens?</p>
</li>
<li>
<p>Are there tokens representing words from other languages?</p>
</li>
</ul>
</div></aside>

<p>How are vocabularies determined? Surely, there was no executive committee holding emergency meetings burning midnight oil, with members making impassioned pleas to include the number 937 in the vocabulary at the expense of 934.</p>

<p>Let us revisit the definition of a vocabulary:</p>
<blockquote>
<p>All the words in a language that are understood by a specific person.</p></blockquote>

<p>Since we want our language model to be an expert at English, we can just include all words in the English dictionary as part of its vocabulary. Problem solved?</p>

<p>Not nearly. What do you do when you communicate with the language model using a word that it has never seen? This happens a lot more often than you think. New words get invented all the time, words have multiple forms (“understand,” “understanding,” “understandable”), multiple words can be combined into a single word, and so on. Moreover, there are millions of domain-specific words (biomedical, chemistry, and so on).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id666">
<h1>The Definition of a Word</h1>
<p>What exactly is a word<a data-type="indexterm" data-primary="word" id="id667"/>, anyway? It is surprisingly very hard to answer this. Conceptually, you could say that a word is the smallest unit of text that has a self-contained meaning. This is not exactly true. For example, the word “snowball” has components that have self-contained meanings of their own.  Algorithmically, you can say that a word is just a sequence of characters separated by white space. This isn’t always true either. For example, the word “Hong Kong” is generally regarded as a single word, even if it is separated by white space. Meanwhile the word “can’t” could potentially be regarded as two or three words, even if there is no white space separating them.</p>
</div></aside>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The account <a href="https://oreil.ly/FzfI9">@NYT_first_said</a> on the social media platform X posts words except proper nouns when they appear in the <em>New York Times</em> for the first time. Each day, an average of five new words appear in the US paper of record for the first time ever. On the day I wrote this section, the words were “unflippant,” “dumbeyed,” “dewdrenched,” “faceflat,” “saporous,” and “dronescape.” Many of these words might never get added to a dictionary.</p>
</div>

<p>A token that doesn’t exist in the vocabulary is called an out-of-vocabulary (OOV) token<a data-type="indexterm" data-primary="out-of-vocabulary (OOV) token" id="id668"/><a data-type="indexterm" data-primary="OOV (out-of-vocabulary) token" id="id669"/>. Traditionally, OOV tokens were represented using a special &lt;UNK&gt; token<a data-type="indexterm" data-primary="&lt;UNK&gt; token" data-primary-sortas="UNK token" id="id670"/>. The &lt;UNK&gt; token is a placeholder for all tokens that don’t exist in the vocabulary. All OOV tokens share the same embedding (and encode the same meaning), which is undesirable. Moreover, the &lt;UNK&gt; token cannot be used in generative models. You don’t want your model to output something like:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s1">'As a language model, I am trained to &lt;UNK&gt; sequences, and output &lt;UNK&gt; text'</code><code class="o">.</code></pre>

<p>To solve the OOV problem, one possible solution could be to represent tokens in terms of characters instead of words. Each character has its own embedding, and as long as all valid characters<a data-type="indexterm" data-primary="character-based tokens" id="id671"/> are included in the vocabulary, there will never be a chance of encountering an OOV token. However, there are many downsides to this. The number of tokens needed to represent the average sentence becomes much larger. For example, the sentence, “The number of tokens needed to represent the average sentence becomes much larger,” contains 13 tokens when you treat each word as a token, but 81 tokens when you treat each character as a token. This reduces the amount of content you can represent within a fixed sequence length, which makes both model training and inference slower, as we will show further in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>. Models support a limited sequence length, so this also reduces the amount of content you can fit in a single prompt. Later in this chapter, we will discuss models like CANINE, ByT5, and Charformer that attempt to use character-based tokens.</p>

<p>So, the middle ground and the best of both worlds (or the worst of both worlds—the field hasn’t come to a consensus yet) is using subwords<a data-type="indexterm" data-primary="subwords" id="id672"/>. Subwords are the predominant mode of representing vocabulary units in the language model space today. The GPT-NeoX vocabulary we explored earlier uses subword tokens. <a data-type="xref" href="#subword-tokens">Figure 3-1</a> shows the OpenAI tokenizer playground that demonstrates how words are split into their constituent subwords by OpenAI models.</p>

<figure><div id="subword-tokens" class="figure">
<img src="assets/dllm_0301.png" alt="Subword tokens" width="600" height="352"/>
<h6><span class="label">Figure 3-1. </span>Subword tokens</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id673">
<h1>Optimal Vocabulary Sizes</h1>
<p>Models have a wide range of vocabulary sizes<a data-type="indexterm" data-primary="vocabulary and tokenization" data-secondary="optimal vocabulary sizes" id="id674"/>. For example, for similarly sized models, Llama 3 utilizes a vocabulary size of 128,000, while Gemma 2 has a vocabulary size of 256,000. Multilingual models typically employ larger vocabularies.</p>

<p>What is the optimal vocabulary size? The larger the vocabulary size, the fewer the number of tokens required to represent a given text, thus increasing the compression efficiency.  Thus, for the same amount of training or inference compute, the language model can process more text. However, as the vocabulary size increases, there are more and more rare tokens with limited occurrences in the training data, and these rare tokens will have deficient representations.</p>

<p><a href="https://oreil.ly/gGq8D">Tao et al.</a> devised scaling laws<a data-type="indexterm" data-primary="scaling laws" id="id675"/> for vocabulary sizes. They note that the optimal vocabulary sizes increase as model sizes and compute increase. They observe that as of their article’s writing, most current models have suboptimal vocabulary sizes and could potentially benefit from increasing them.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Tokenizers"><div class="sect1" id="id38">
<h1>Tokenizers</h1>

<p>Next, let’s dive into tokenizers<a data-type="indexterm" data-primary="vocabulary and tokenization" data-secondary="tokenizer" id="xi_vocabularyandtokenizationtokenizer313233"/><a data-type="indexterm" data-primary="tokens and tokenization" data-secondary="tokenizer" id="xi_tokensandtokenizationtokenizer313233"/>, the software that serves as a text-processing interface between humans and models.</p>

<p>A tokenizer has two responsibilities:</p>
<ol>
<li>
<p>In the tokenizer pre-training stage, the tokenizer is run over a body of text to generate a vocabulary.</p>
</li>
<li>
<p>While processing input during both model training and inference, free-form raw text is run through the tokenizer algorithm to break the text into sequences of valid tokens. <a data-type="xref" href="#tokenizer-workflow">Figure 3-2</a> depicts the roles played by a tokenizer.</p>
</li>

</ol>

<figure><div id="tokenizer-workflow" class="figure">
<img src="assets/dllm_0302.png" alt="Tokenizer Workflows" width="600" height="389"/>
<h6><span class="label">Figure 3-2. </span>Tokenizer workflow</h6>
</div></figure>

<p>When we feed raw text to the tokenizer, it breaks the text into tokens that are part of the vocabulary and maps the tokens to their token indices. The sequence of token indices (input IDs)<a data-type="indexterm" data-primary="input ID (token)" id="id676"/> are then fed to the language model, where they are mapped to their corresponding embeddings. Let us explore this process in detail.</p>

<p>This time, let’s experiment with the FLAN-T5 model<a data-type="indexterm" data-primary="FLAN-T5 model" id="id677"/>. You need a Google Colab Pro or equivalent system to be able to run it:</p>

<pre data-type="programlisting" data-code-language="python"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">transformers</code> <code class="n">accelerate</code> <code class="n">sentencepiece</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">T5Tokenizer</code><code class="p">,</code> <code class="n">T5ForConditionalGeneration</code>


<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">T5Tokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"google/flan-t5-largel"</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">T5ForConditionalGeneration</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"google/flan-t5-large"</code><code class="p">,</code>
    <code class="n">device_map</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">)</code>


<code class="n">input_text</code> <code class="o">=</code> <code class="s2">"what is 937 + 934?"</code>
<code class="n">encoded_text</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">input_text</code><code class="p">)</code>
<code class="n">tokens</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">convert_ids_to_tokens</code><code class="p">(</code><code class="n">encoded_text</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">encoded_text</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">tokens</code><code class="p">)</code></pre>

<p>The output is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="mi">125</code><code class="p">,</code> <code class="mi">19</code><code class="p">,</code> <code class="mi">668</code><code class="p">,</code> <code class="mi">4118</code><code class="p">,</code> <code class="mi">1768</code><code class="p">,</code> <code class="mi">668</code><code class="p">,</code> <code class="mi">3710</code><code class="p">,</code> <code class="mi">58</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>
<code class="p">[</code><code class="s1">'▁what'</code><code class="p">,</code> <code class="s1">'▁is'</code><code class="p">,</code> <code class="s1">'▁9'</code><code class="p">,</code> <code class="s1">'37'</code><code class="p">,</code> <code class="s1">'▁+'</code><code class="p">,</code> <code class="s1">'▁9'</code><code class="p">,</code> <code class="s1">'34'</code><code class="p">,</code> <code class="s1">'?'</code><code class="p">,</code> <code class="s1">'&lt;/s&gt;'</code><code class="p">]</code></pre>

<p>The <code>encode()</code> function tokenizes the input text and returns the corresponding token indices. The token indices are mapped to the tokens they represent using the 
<span class="keep-together"><code>convert_ids_to_tokens()</code></span> function.</p>

<p>As you can see, the FLAN-T5 tokenizer doesn’t have dedicated tokens for the numbers 937 or 934. Therefore, it splits the numbers into “9” and “37.” The <code>&lt;/s&gt;</code> token is a special token indicating the end of the string. The <code>_</code> means that the token is preceded by a space.</p>

<p>Let’s try another example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">input_text</code> <code class="o">=</code> <code class="s2">"Insuffienct adoption of corduroy pants is the reason this</code><code class="w"/>

<code class="n">economy</code> <code class="ow">is</code> <code class="ow">in</code> <code class="n">the</code> <code class="n">dumps</code><code class="err">!!!</code><code class="s2">"</code><code class="w"/>
<code class="n">encoded_text</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">input_text</code><code class="p">)</code>
<code class="n">tokens</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">convert_ids_to_tokens</code><code class="p">(</code><code class="n">encoded_text</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">tokens</code><code class="p">)</code></pre>

<p>The output is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="s1">'▁In'</code><code class="p">,</code> <code class="s1">'s'</code><code class="p">,</code> <code class="s1">'uff'</code><code class="p">,</code> <code class="s1">'i'</code><code class="p">,</code> <code class="s1">'en'</code><code class="p">,</code> <code class="s1">'c'</code><code class="p">,</code> <code class="s1">'t'</code><code class="p">,</code> <code class="s1">'▁adoption'</code><code class="p">,</code> <code class="s1">'▁of'</code><code class="p">,</code> <code class="s1">'▁cord'</code><code class="p">,</code> <code class="s1">'u'</code><code class="p">,</code>
<code class="s1">'roy'</code><code class="p">,</code> <code class="s1">'▁pants'</code><code class="p">,</code> <code class="s1">'▁is'</code><code class="p">,</code> <code class="s1">'▁the'</code><code class="p">,</code> <code class="s1">'▁reason'</code><code class="p">,</code> <code class="s1">'▁this'</code><code class="p">,</code> <code class="s1">'▁economy'</code><code class="p">,</code> <code class="s1">'▁is'</code><code class="p">,</code> <code class="s1">'▁in'</code><code class="p">,</code>
<code class="s1">'▁the'</code><code class="p">,</code> <code class="s1">'▁dump'</code><code class="p">,</code> <code class="s1">'s'</code><code class="p">,</code> <code class="s1">'!!!'</code><code class="p">,</code> <code class="s1">'&lt;/s&gt;'</code><code class="p">]</code></pre>

<p>I made a deliberate typo with the word “Insufficient.” Note that subword tokenization<a data-type="indexterm" data-primary="typos and subword tokenization" id="id678"/> is rather brittle with typos. But at least the OOV problem has been dealt with by breaking the words into subwords. The vocabulary also doesn’t seem to have an entry for the word “corduroy,” thus confirming its poor sense of fashion. Meanwhile, note that there is a distinct token for three contiguous exclamation points, which is different from the token that represents a single exclamation point. Semantically, they do convey slightly different meanings.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Very large models trained on a massive body of text are more robust to misspellings<a data-type="indexterm" data-primary="misspellings" id="id679"/>.  A lot of misspellings already occur in the training set. For example, even the rare misspelling “Insuffienct” occurs 14 times in the C4 pre-training dataset. The more common misspelling “insufficent” occurs over 1,100 times. Larger models can also infer the misspelled word from its context. Smaller models like BERT are quite sensitive to misspellings.</p>
</div>

<p>If you are using models from OpenAI, you can explore their tokenization scheme using the <a href="https://oreil.ly/2QByi">tiktoken library</a> (no<a data-type="indexterm" data-primary="tiktoken library" id="id680"/><a data-type="indexterm" data-primary="OpenAI" data-secondary="tiktoken library" id="id681"/> relation to the social media 
<span class="keep-together">website</span>).</p>

<p>Using tiktoken, let’s see the different vocabularies available in the OpenAI ecosystem:</p>

<pre data-type="programlisting" data-code-language="python"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">tiktoken</code>

<code class="kn">import</code> <code class="nn">tiktoken</code>
<code class="n">tiktoken</code><code class="o">.</code><code class="n">list_encoding_names</code><code class="p">()</code></pre>

<p>The output is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="s1">'gpt2'</code><code class="p">,</code> <code class="s1">'r50k_base'</code><code class="p">,</code> <code class="s1">'p50k_base'</code><code class="p">,</code> <code class="s1">'p50k_edit'</code><code class="p">,</code> <code class="s1">'cl100k_base'</code><code class="p">,</code> <code class="s1">'o200k_base'</code><code class="p">]</code></pre>

<p>The numbers like 50K/100K are presumed to be the vocabulary size. OpenAI hasn’t revealed much information about these vocabularies. Their documentation does state that o200k_base is used by GPT-4o, while cl100k_base is used by GPT-4:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">encoding</code> <code class="o">=</code> <code class="n">tiktoken</code><code class="o">.</code><code class="n">encoding_for_model</code><code class="p">(</code><code class="s2">"gpt-4"</code><code class="p">)</code>
<code class="n">input_ids</code> <code class="o">=</code> <code class="n">encoding</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s2">"Insuffienct adoption of corduroy pants is the</code><code class="w"/>

<code class="n">reason</code> <code class="n">this</code> <code class="n">economy</code> <code class="ow">is</code> <code class="ow">in</code> <code class="n">the</code> <code class="n">dumps</code><code class="err">!!!</code><code class="s2">")</code><code class="w"/>
<code class="n">tokens</code> <code class="o">=</code> <code class="p">[</code><code class="n">encoding</code><code class="o">.</code><code class="n">decode_single_token_bytes</code><code class="p">(</code><code class="n">token</code><code class="p">)</code> <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">input_ids</code><code class="p">]</code></pre>

<p>The output is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="sa">b</code><code class="s1">'Ins'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">'uff'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">'ien'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">'ct'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">' adoption'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">' of'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">' cord'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">'uro'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">'y'</code><code class="p">,</code>
<code class="sa">b</code><code class="s1">' pants'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">' is'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">' the'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">' reason'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">' this'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">' economy'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">' is'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">' in'</code><code class="p">,</code>
<code class="sa">b</code><code class="s1">' the'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">' dumps'</code><code class="p">,</code> <code class="sa">b</code><code class="s1">'!!!'</code><code class="p">]</code></pre>

<p>As you can see there is not much difference between the tokenization used by GPT-4 and FLAN-T5.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id682">
<h1>Exercise</h1>
<p>This <a href="https://oreil.ly/TQoLz">repo</a> contains the vocabularies of o200k_base and cl100k_base. Find the differences between these vocabularies. What kinds of tokens are present in one but not the other?</p>
</div></aside>
<div data-type="tip"><h6>Tip</h6>
<p>For a given task, if you observe strange behavior from LLMs on only a subset of your inputs, it is worthwhile to check how they have been tokenized<a data-type="indexterm" data-primary="debugging" id="id683"/>. While you cannot definitively diagnose your problem just by analyzing the tokenization, it is often helpful in analysis. In my experience, a non-negligible number of LLM failures can be attributed to the way the text was tokenized. This is especially true if your target domain is different from the pre-training domain<a data-type="indexterm" data-startref="xi_vocabularyandtokenizationtokenizer313233" id="id684"/><a data-type="indexterm" data-startref="xi_tokensandtokenizationtokenizer313233" id="id685"/>.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id686">
<h1>Tokenization-Free Models</h1>
<p>As discussed in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>, the <em>consolidation effect</em>  is resulting<a data-type="indexterm" data-primary="consolidation effect" id="id687"/> in end-to-end architectures<a data-type="indexterm" data-primary="tokenization-free models" id="id688"/> that attempt to accept human input, perform all required processing, and generate human consumable output within a single model. However, one last holdout is the tokenization step. You have seen in the code shown previously that the tokenization is used as a preprocessing step to prepare the input to be fed into the model. The input to the model is the sequence of token indices and not raw text. But what if we make the model truly end-to-end by removing the tokenization step? Is it possible to directly feed raw text to the model and have it output results?</p>

<p>There have been forays into the world of tokenization-free language modeling, with models like CANINE, ByT5, and Charformer.</p>

<ul>
<li>
<p><a href="https://oreil.ly/ucLIk">CANINE</a> accepts Unicode codepoints<a data-type="indexterm" data-primary="CANINE" id="id689"/> as input. But there are 1,114,112 possible code points, rendering the vocabulary and resulting embedding layer size infeasible. To resolve this, CANINE uses hashed embeddings so that the effective vocabulary space is much smaller.</p>
</li>
<li>
<p><a href="https://oreil.ly/x38Vs">ByT5</a> accepts input in terms of bytes<a data-type="indexterm" data-primary="ByT5" id="id690"/>, so there are only 259 tokens in the vocabulary (including a few special tokens), thus reducing the embedding layer size drastically.</p>
</li>
<li>
<p><a href="https://oreil.ly/WJY1k">Charformer</a> also accepts input in terms<a data-type="indexterm" data-primary="Charformer" id="id691"/> of bytes and passes it to a gradient-based subword tokenizer module that constructs latent subwords.</p>
</li>
</ul>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Tokenization Pipeline"><div class="sect1" id="id239">
<h1>Tokenization Pipeline</h1>

<p><a data-type="xref" href="#huggingface-tokenizers-pipeline">Figure 3-3</a> depicts the sequence of steps performed by a tokenizer<a data-type="indexterm" data-primary="vocabulary and tokenization" data-secondary="tokenization pipeline" id="id692"/>.</p>

<figure><div id="huggingface-tokenizers-pipeline" class="figure">
<img src="assets/dllm_0303.png" alt="Hugging Face Tokenizers pipeline" width="600" height="66"/>
<h6><span class="label">Figure 3-3. </span>Hugging Face tokenizers pipeline</h6>
</div></figure>

<p>If you are using the <code>tokenizers</code> library from Hugging Face<a data-type="indexterm" data-primary="Hugging Face" data-secondary="tokenization pipeline" id="id693"/>, your input text is run through a <a href="https://oreil.ly/CcOKV">multistage tokenization pipeline</a>. This pipeline is composed of four 
<span class="keep-together">components</span>:</p>

<ul>
<li>
<p>Normalization</p>
</li>
<li>
<p>Pre-tokenization</p>
</li>
<li>
<p>Tokenization</p>
</li>
<li>
<p>Postprocessing</p>
</li>
</ul>

<p>Note that different models will execute different steps within these four components.</p>








<section data-type="sect2" data-pdf-bookmark="Normalization"><div class="sect2" id="id39">
<h2>Normalization</h2>

<p>Different types of normalization<a data-type="indexterm" data-primary="normalization step, tokenization pipeline" id="id694"/> applied include:</p>

<ul>
<li>
<p>Converting text to lowercase (if you are using an uncased model)</p>
</li>
<li>
<p>Stripping off accents from characters, like from the word Peña</p>
</li>
<li>
<p>Unicode normalization</p>
</li>
</ul>

<p>Let’s see what kind of normalization is applied on the uncased version of BERT:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"bert-base-uncased"</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">backend_tokenizer</code><code class="o">.</code><code class="n">normalizer</code><code class="o">.</code><code class="n">normalize_str</code><code class="p">(</code>
    <code class="s1">'Pédrò pôntificated at üs:-)'</code><code class="p">)</code></pre>

<p>The output is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">pedro</code> <code class="n">pontificated</code> <code class="n">at</code> <code class="n">us</code><code class="p">:</code><code class="o">-</code><code class="p">)</code></pre>

<p>As we can see, the accents have been removed and the text has been converted to 
<span class="keep-together">lowercase</span>.</p>

<p>There isn’t much normalization done in tokenizers for more recent models.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Pre-Tokenization"><div class="sect2" id="id40">
<h2>Pre-Tokenization</h2>

<p>Before we run the tokenizer<a data-type="indexterm" data-primary="pre-tokenization step, pipeline" id="id695"/> on the text, we can optionally perform a pre-tokenization step. As mentioned earlier, most tokenizers today employ subword tokenization. A common step is to first perform word tokenization and then feed the output of it to the subword tokenization algorithm. This step is called pre-tokenization.</p>

<p>Pre-tokenization is a relatively easy step in English compared to many other languages, since you can start with a very strong baseline just by splitting text on whitespace. There are outlier decisions to be made, such as how to deal with punctuation, multiple spaces, numbers, etc. In Hugging Face the regular expression:</p>

<pre data-type="programlisting" data-code-language="python" class="less_space pagebreak-before">\<code class="n">w</code><code class="o">+|</code><code class="p">[</code><code class="o">^</code>\<code class="n">w</code>\<code class="n">s</code><code class="p">]</code><code class="o">+</code></pre>

<p>is used to split on whitespace.</p>

<p>Let’s run the pre-tokenization step of the T5 tokenizer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"google/flan-t5-xl"</code><code class="p">)</code>
<code class="n">tokenizer</code><code class="o">.</code><code class="n">backend_tokenizer</code><code class="o">.</code><code class="n">pre_tokenizer</code><code class="o">.</code><code class="n">pre_tokenize_str</code><code class="p">(</code><code class="s2">"I'm starting to</code><code class="w"/>

<code class="n">suspect</code> <code class="o">-</code> <code class="n">I</code> <code class="n">am</code> <code class="mi">55</code> <code class="n">years</code> <code class="n">old</code><code class="err">!</code>   <code class="n">Time</code> <code class="n">to</code> <code class="n">vist</code> <code class="n">New</code> <code class="n">York</code><code class="err">?</code><code class="s2">")</code><code class="w"/></pre>

<p>The output is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[(</code><code class="s2">"▁I'm"</code><code class="p">,</code> <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">3</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁starting'</code><code class="p">,</code> <code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">12</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁to'</code><code class="p">,</code> <code class="p">(</code><code class="mi">12</code><code class="p">,</code> <code class="mi">15</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁suspect'</code><code class="p">,</code> <code class="p">(</code><code class="mi">15</code><code class="p">,</code> <code class="mi">23</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁-'</code><code class="p">,</code> <code class="p">(</code><code class="mi">23</code><code class="p">,</code> <code class="mi">25</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁I'</code><code class="p">,</code> <code class="p">(</code><code class="mi">25</code><code class="p">,</code> <code class="mi">27</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁am'</code><code class="p">,</code> <code class="p">(</code><code class="mi">27</code><code class="p">,</code> <code class="mi">30</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁55'</code><code class="p">,</code> <code class="p">(</code><code class="mi">30</code><code class="p">,</code> <code class="mi">33</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁years'</code><code class="p">,</code> <code class="p">(</code><code class="mi">33</code><code class="p">,</code> <code class="mi">39</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁old!'</code><code class="p">,</code> <code class="p">(</code><code class="mi">39</code><code class="p">,</code> <code class="mi">44</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁'</code><code class="p">,</code> <code class="p">(</code><code class="mi">44</code><code class="p">,</code> <code class="mi">45</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁'</code><code class="p">,</code> <code class="p">(</code><code class="mi">45</code><code class="p">,</code> <code class="mi">46</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁Time'</code><code class="p">,</code> <code class="p">(</code><code class="mi">46</code><code class="p">,</code> <code class="mi">51</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁to'</code><code class="p">,</code> <code class="p">(</code><code class="mi">51</code><code class="p">,</code> <code class="mi">54</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁vist'</code><code class="p">,</code> <code class="p">(</code><code class="mi">54</code><code class="p">,</code> <code class="mi">59</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁New'</code><code class="p">,</code> <code class="p">(</code><code class="mi">59</code><code class="p">,</code> <code class="mi">63</code><code class="p">)),</code>
 <code class="p">(</code><code class="s1">'▁York?'</code><code class="p">,</code> <code class="p">(</code><code class="mi">63</code><code class="p">,</code> <code class="mi">69</code><code class="p">))]</code></pre>

<p>Along with the pre-tokens (or word tokens), the character offsets are returned.</p>

<p>The T5 pre-tokenizer splits only on whitespace, doesn’t collapse multiple spaces into one, and doesn’t split on punctuation or numbers. The behavior can be vastly different for other tokenizers.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Tokenization"><div class="sect2" id="id240">
<h2>Tokenization</h2>

<p>After the optional pre-tokenization step, the actual tokenization step<a data-type="indexterm" data-primary="tokens and tokenization" data-secondary="tokenization pipeline step" id="id696"/> is performed. Some of the important algorithms in this space are byte pair encoding (BPE), byte-level BPE, WordPiece, and Unigram LM. The tokenizer comprises a set of rules that is learned during a pre-training phase over a pre-training dataset. Now let’s go through these algorithms in detail.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Byte Pair Encoding"><div class="sect2" id="id41">
<h2>Byte Pair Encoding</h2>

<p>This algorithm<a data-type="indexterm" data-primary="byte pair encoding (BPE) algorithm" id="id697"/><a data-type="indexterm" data-primary="BPE (byte pair encoding) algorithm" id="id698"/> is the simplest and most widely used tokenization algorithm.</p>










<section data-type="sect3" data-pdf-bookmark="Training stage"><div class="sect3" id="id359">
<h3>Training stage</h3>

<p>We take a training dataset, run it through the normalization and pre-tokenization steps discussed earlier, and record the unique tokens in the resulting output and their frequencies. We then construct an initial vocabulary consisting of the unique characters that make up these tokens. Starting from this initial vocabulary, we continue adding new tokens using <em>merge</em> rules. The merge rule is simple; we create a new token using the most frequent consecutive pairs of tokens. The merges continue until we reach the desired vocabulary size.</p>

<p>Let’s explore this with an example. Imagine our training dataset is composed of six words, each appearing just once:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s1">'bat'</code><code class="p">,</code> <code class="s1">'cat'</code><code class="p">,</code> <code class="s1">'cap'</code><code class="p">,</code> <code class="s1">'sap'</code><code class="p">,</code> <code class="s1">'map'</code><code class="p">,</code> <code class="s1">'fan'</code></pre>

<p>The initial vocabulary is then made up of:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s1">'b'</code><code class="p">,</code> <code class="s1">'a'</code><code class="p">,</code> <code class="s1">'t'</code><code class="p">,</code> <code class="s1">'c'</code><code class="p">,</code> <code class="s1">'p'</code><code class="p">,</code> <code class="s1">'s'</code><code class="p">,</code> <code class="s1">'m'</code><code class="p">,</code> <code class="s1">'f'</code><code class="p">,</code> <code class="s1">'n'</code></pre>

<p>The frequencies of contiguous token pairs are:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s1">'ba'</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'at'</code> <code class="o">-</code> <code class="mi">2</code><code class="p">,</code> <code class="s1">'ca'</code> <code class="o">-</code> <code class="mi">2</code><code class="p">,</code> <code class="s1">'ap'</code> <code class="o">-</code> <code class="mi">3</code><code class="p">,</code> <code class="s1">'sa'</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'ma'</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'fa'</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'an'</code> <code class="o">-</code> <code class="mi">1</code></pre>

<p>The most frequent pair is “ap,” so the first merge rule is to merge “a” and “p.” The vocabulary now is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s1">'b'</code><code class="p">,</code> <code class="s1">'a'</code><code class="p">,</code> <code class="s1">'t'</code><code class="p">,</code> <code class="s1">'c'</code><code class="p">,</code> <code class="s1">'p'</code><code class="p">,</code> <code class="s1">'s'</code><code class="p">,</code> <code class="s1">'m'</code><code class="p">,</code> <code class="s1">'f'</code><code class="p">,</code> <code class="s1">'n'</code><code class="p">,</code> <code class="s1">'ap'</code></pre>

<p>The new frequencies are:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s1">'ba'</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'at'</code> <code class="o">-</code> <code class="mi">2</code><code class="p">,</code> <code class="s1">'cap'</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'sap'</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'map'</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'fa'</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'an'</code> <code class="o">-</code> <code class="mi">1</code></pre>

<p>Now, the most frequent pair is “at,” so the next merge rule is to merge “a” and “t.” This process continues until we reach the vocabulary size.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Inference stage"><div class="sect3" id="id360">
<h3>Inference stage</h3>

<p>After the tokenizer has been trained, it can be used to divide the text into appropriate subword tokens and feed the text into the model. This happens in a similar fashion as the training step. After normalization and pre-tokenization of the input text, the resulting tokens are broken into individual characters, and all the merge rules are applied in order. The tokens standing after all merge rules have been applied are the final tokens, which are then fed to the model.</p>

<p>You can open the <a href="https://oreil.ly/7JAyY">vocabulary file</a> for GPT-NeoX again, and Ctrl+F “merges” to see the merge rules. As expected, the initial merge rules join single characters with each other. At the end of the merge list, you can see larger subwords like “out” and “comes” being merged into a single token.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id699">
<h1>Exercise</h1>
<p>Implement the BPE algorithm by yourself, using a subset of the Wikipedia dataset that can be downloaded from <a href="https://oreil.ly/xwGqK">here</a>. For a vocabulary size of 10,000, what tokens do you end up with and how do they differ from the vocabulary of the popular language models? Use the resulting tokenizer to tokenize a domain-specific dataset like the machine learning papers dataset from <a href="https://oreil.ly/MqEY9">here</a>. Do all the technical concepts get their own tokens? This gives you a clue on how effective general-purpose LMs will be for your use case.</p>
</div></aside>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Since all unique individual characters in the tokenizer training set will get their own token, it is guaranteed that there will be no OOV tokens as long as all tokens seen during inference in the future are made up of characters that were present in the training set. But Unicode consists of over a million code points and around 150,000 valid characters, which would not fit in a vocabulary of size 30,000. This means that if your input text contained a character that wasn’t in the training set, that character would be assigned an  &lt;UNK&gt; token. To resolve this, a variant of BPE called byte-level BPE is used. Byte-level BPE starts with 256 tokens, representing all the characters that can be represented by a byte. This ensures that every Unicode character can be encoded just by the concatenation of the constituent byte tokens. Hence, it also ensures that we will never encounter an &lt;UNK&gt; token. The GPT family of models use this tokenizer.</p>
</div>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="WordPiece"><div class="sect2" id="id42">
<h2>WordPiece</h2>

<p>WordPiece<a data-type="indexterm" data-primary="WordPiece tokenization" id="id700"/> is similar to BPE, so we will highlight only the differences.</p>

<p>Instead of the frequency approach used by BPE, WordPiece uses the maximum likelihood approach. The frequency of the token pairs in the dataset is normalized by the product of the frequency of the individual tokens. The pairs with the resulting highest score are then merged:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">score</code> <code class="o">=</code> <code class="n">freq</code><code class="p">(</code><code class="n">a</code><code class="p">,</code><code class="n">b</code><code class="p">)</code><code class="o">/</code><code class="p">(</code><code class="n">freq</code><code class="p">(</code><code class="n">a</code><code class="p">)</code> <code class="o">*</code> <code class="n">freq</code><code class="p">(</code><code class="n">b</code><code class="p">))</code></pre>

<p>This means that if a token pair is made up of tokens that individually have low frequency, they will be merged first.</p>

<p><a data-type="xref" href="#WordPiece">Figure 3-4</a> shows the merge priority and how the normalization by individual frequencies affects the order of merging.</p>

<figure><div id="WordPiece" class="figure">
<img src="assets/dllm_0304.png" alt="WordPiece tokenization" width="600" height="319"/>
<h6><span class="label">Figure 3-4. </span>WordPiece tokenization</h6>
</div></figure>

<p>During inference, merge rules are not used. Instead, for each pre-tokenized token in the input text, the tokenizer finds the longest subword from the vocabulary in the token and splits on it. For example, if the token is “understanding” and the longest subword in the dictionary within this token is “understand,” then it will be split into “understand” and “ing.”</p>










<section data-type="sect3" data-pdf-bookmark="Postprocessing"><div class="sect3" id="id43">
<h3>Postprocessing</h3>

<p>Now that we have looked at a couple of tokenizer algorithms, let’s move on to the next stage of  the pipeline, the postprocessing stage<a data-type="indexterm" data-primary="postprocessing stage, tokenization" id="id701"/>. This is where model-specific special tokens are added. Common tokens include [CLS], the classification token used in many language models, and [SEP], a separator token used to separate parts of the input.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id702">
<h1>The Curious Case of SolidMagiGoldkarp</h1>
<p>There are weird tokens that end up being part of a language model’s vocabulary due to the way the tokenization algorithms work. One such token<a data-type="indexterm" data-primary="SolidMagiGoldkarp token" id="id703"/> is “SolidMagiGoldkarp,” representing a now-deleted Reddit user who was one of the site’s most active posters because of his quest to count to infinity. This was a token in the GPT-2 tokenizer vocabulary. The same tokenizer was used in GPT-3 models, but the pre-training dataset of the model had changed, so it didn’t include many or any references to SolidMagiGoldkarp.  So now a token existed for SolidMagiGoldkarp but there was no signal in the pre-training dataset to learn from. This leads to some anomalous and hilarious behavior in GPT-3. These tokens are called <a href="https://oreil.ly/wnB-z">glitch tokens</a> or undertrained tokens<a data-type="indexterm" data-primary="glitch tokens" id="id704"/><a data-type="indexterm" data-primary="undertrained tokens" id="id705"/>.</p>

<p>Token etymology is a new hobby for many LLM enthusiasts. This involves finding rare tokens in the vocabulary of language models and unearthing their origins. This is not just fun and games though, as knowing the origin of rare tokens can give you an insight into the characteristics of the pre-training dataset. Using <a href="https://oreil.ly/z19c2">tiktoken</a>, find some rare vocabulary terms in GPT-4’s or GPT-4o’s vocabulary. Can you figure out their origins?</p>
</div></aside>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Special Tokens"><div class="sect2" id="id44">
<h2>Special Tokens</h2>

<p>Depending on the model<a data-type="indexterm" data-primary="vocabulary and tokenization" data-secondary="special tokens" id="xi_vocabularyandtokenizationspecialtokens348223"/><a data-type="indexterm" data-primary="tokens and tokenization" data-secondary="special tokens" id="xi_tokensandtokenizationspecialtokens348223"/>, a few special tokens are added to the vocabulary to facilitate processing. These tokens can include:</p>
<dl>
<dt>&lt;PAD&gt;</dt>
<dd>
<p>To indicate padding, in case the size of the input is less than the maximum sequence length.</p>
</dd>
<dt>&lt;EOS&gt;</dt>
<dd>
<p>To indicate the end of the sequence. Generative models stop generating after outputting this token.</p>
</dd>
<dt>&lt;UNK&gt;</dt>
<dd>
<p>To indicate an OOV term.</p>
</dd>
<dt>&lt;TOOL_CALL&gt;, &lt;/TOOL_CALL&gt;</dt>
<dd>
<p>Content between these tokens is used as input to an external tool, like an API call or a query to a database.</p>
</dd>
<dt>&lt;TOOL_RESULT&gt;, &lt;/TOOL_RESULT&gt;</dt>
<dd>
<p>Content between these tokens is used to represent the results from calling the aforementioned tools.</p>
</dd>
</dl>

<p>As we have seen, if our data is domain-specific like healthcare, scientific literature, etc., tokenization from a general-purpose tokenizer will be unsatisfactory. GALACTICA by Meta introduced several domain-specific tokens in their model and special tokenization rules:</p>

<ul>
<li>
<p>[START_REF] and [END_REF] for wrapping citations.</p>
</li>
<li>
<p>&lt;WORK&gt; to wrap tokens that make up an internal working memory, used for reasoning and code generation.</p>
</li>
<li>
<p>Numbers are handled by assigning each digit in the number its own token.</p>
</li>
<li>
<p>[START_SMILES], [START_DNA], [START_AMINO], [END_SMILES], [END_DNA], [END_AMINO] for protein sequences, DNA sequences, and amino acid sequences, respectively.</p>
</li>
</ul>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id706">
<h1>Evaluating Tokenizers</h1>
<p>Two popular metrics for evaluating tokenizers<a data-type="indexterm" data-primary="tokens and tokenization" data-secondary="evaluating tokenizers" id="id707"/> are fertility and parity.</p>

<p><em>Fertility</em> is a measure<a data-type="indexterm" data-primary="fertility, tokenizer" id="id708"/> of the average number of tokens needed to represent a dataset. It is calculated by dividing the number of tokens in a dataset by the number of words in a dataset. The higher the fertility, the lower the compression power of the tokenizer. <a href="https://oreil.ly/ZEG_x">Goldman et al.</a> show that higher compression leads to better downstream performance, although this is disputed in experiments by <a href="https://oreil.ly/GH_iK">Schmidt et al.</a>. For a tokenizer to achieve higher compression levels, it needs to be trained on larger datasets during the vocabulary generation phase.</p>

<p><em>Parity</em> is a measure<a data-type="indexterm" data-primary="parity, tokenizer" id="id709"/> of how fairly a tokenizer treats two languages. It is calculated by the ratio of tokens needed to represent the same data in one language versus the other.</p>

<p>Many language models today have multilingual support<a data-type="indexterm" data-primary="multilingual support" id="id710"/>. However, due to the tokenizer being trained on an English-centric corpus, the tokenization for other languages tends to be suboptimal. Thus, a sentence in a non-English language may need several times more tokens to represent it compared to the same sentence in English, as shown by <a href="https://oreil.ly/ZATOQ">Petrov et al.</a></p>
</div></aside>

<p>If you are using a model on domain-specific data like healthcare, finance, law, biomedical, etc.,  with a tokenizer that was trained on general-purpose data, the compression ratio will be relatively lower because domain-specific words do not have their own tokens and will be split into multiple tokens. One way to adapt models to specialized domains is for models to learn good vector representations for domain-specific terms.</p>

<p>To this end, we can add new tokens to existing tokenizers and continue pre-training the model on domain-specific data so that those new domain-specific tokens learn effective representations. We will learn more about continued pre-training in 
<span class="keep-together"><a data-type="xref" href="ch07.html#ch07">Chapter 7</a></span>.</p>

<p>For now, let’s see how we can add new tokens to a vocabulary using Hugging Face.</p>

<p>Consider the sentence, “The addition of CAR-T cells and antisense oligonucleotides drove down incidence rates.” The FLAN-T5 tokenizer splits this text as follows:</p>
<ul class="simplelist">
<li>['▁The', '▁addition', '▁of', '▁C', ' AR', '-', ' T', '▁cells', '▁and', '▁anti',
' s', ' ense', '▁', ' oli', ' gon', ' u', ' cle', ' o', ' t', ' ides', '▁drove',
 '▁down', '▁incidence', '▁rates', ' .', '&lt;/s&gt;']</li>
 </ul>

<p class="pagebreak-before">Let’s add the domain-specific terms to the vocabulary:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">T5Tokenizer</code><code class="p">,</code> <code class="n">T5ForConditionalGeneration</code>


<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">T5Tokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"google/flan-t5-large"</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">T5ForConditionalGeneration</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"google/flan-t5-large"</code><code class="p">,</code>
    <code class="n">device_map</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">)</code>

<code class="n">tokenizer</code><code class="o">.</code><code class="n">add_tokens</code><code class="p">([</code><code class="s2">"CAR-T"</code><code class="p">,</code> <code class="s2">"antisense"</code><code class="p">,</code> <code class="s2">"oligonucleotides"</code><code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">resize_token_embeddings</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">tokenizer</code><code class="p">))</code></pre>

<p>Now, tokenizing the string again gives the following tokens, with the domain-specific tokens being added:</p>
<ul class="simplelist">
<li>['▁The', '▁addition', '▁of', ' CAR-T', '▁cells', '▁and',
' antisense', ' oligonucleotides', '▁drove', '▁down',
'▁incidence', '▁rates', ' .', '&lt;/s&gt;']</li>
 </ul>

<p>We are only halfway done here. The embedding vectors corresponding to these new tokens do not contain any information about these tokens<a data-type="indexterm" data-startref="xi_vocabularyandtokenization3896" id="id711"/><a data-type="indexterm" data-startref="xi_vocabularyandtokenizationspecialtokens348223" id="id712"/><a data-type="indexterm" data-startref="xi_tokensandtokenization3896" id="id713"/><a data-type="indexterm" data-startref="xi_tokensandtokenizationspecialtokens348223" id="id714"/>. We will need to learn the right representations for these tokens, which we can do using fine-tuning or continued pre-training, which we will discuss in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id361">
<h1>Summary</h1>

<p>In this chapter, we focused on a key ingredient of language models: their vocabulary. We discussed how vocabularies are defined and constructed in the realm of language models. We introduced the concept of tokenization and presented tokenization algorithms like BPE and WordPiece that are used to construct vocabularies and break down raw input text into a sequence of tokens that can be consumed by the language model. We also explored the vocabularies of popular language models and noted how tokens can differ from human conceptions of a word.</p>

<p>In the next chapter, we will continue exploring the remaining ingredients of a language model, including its architecture and the learning objectives on which models are trained.</p>
</div></section>
</div></section></div>
</div>
</body></html>