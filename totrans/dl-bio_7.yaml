- en: Chapter 7\. Tips and Tricks for Deep Learning in Biology
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章. 生物学的深度学习技巧与窍门
- en: This final chapter brings together common themes from earlier chapters and distills
    practical strategies for applying deep learning techniques to biological problems.
    In machine learning, it’s rare for things to work perfectly on the first try—or
    even the tenth. Debugging is an expected part of the process, not a sign of failure.
    Don’t get discouraged.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一章汇集了前面章节的常见主题，并提炼了将深度学习技术应用于生物问题的实用策略。在机器学习中，事情第一次尝试就能完美工作的情况很少——甚至第十次尝试也很少。调试是过程的一个预期部分，而不是失败的标志。不要气馁。
- en: Here, we share a collection of tips that have helped us (and others) navigate
    the challenges of deep learning in biology. Some were learned the hard way, and
    others emerged from writing this book. This list isn’t exhaustive, but we hope
    it shortens your path to developing working models—and sharpens your instincts
    for when things go wrong.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们分享了一系列帮助我们（以及其他人）在生物学的深度学习挑战中导航的技巧。有些是通过艰难的方式学到的，有些则是从撰写这本书中产生的。这份清单并不全面，但我们希望它能缩短你开发工作模型的道路——并使你在事情出错时更加敏锐。
- en: Warning
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Don’t expect steady, incremental improvements in your project. Progress in deep
    learning—especially with biological data—is often highly nonlinear. You might
    spend weeks debugging with no clear gains, only to make one small change that
    suddenly unlocks everything. This is normal—and not a cause for concern.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 不要期望你的项目会有稳定的、渐进的改进。深度学习的进展——尤其是与生物数据相关——通常是高度非线性的。你可能会花费几周的时间调试，却看不到明显的进展，只有当你做出一个小小的改变，突然一切都解开时。这是正常的——并不是一个值得关注的问题。
- en: Simplify
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简化
- en: When things stop making sense, simplify. Strip your problem back to the bare
    minimum—a smaller dataset, a shallower model, or a simpler loss function. It’s
    easy to get lost in complex pipelines, but debugging is much easier when you can
    isolate one thing at a time.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当事情不再有意义时，简化。将你的问题简化到最基本的形式——更小的数据集、更浅的模型或更简单的损失函数。在复杂的管道中容易迷失方向，但当你一次隔离一个事物时，调试会容易得多。
- en: Once you’ve got something working again, you can reintroduce complexity gradually.
    Think of this as turning the knobs one by one instead of all at once. It’s not
    glamorous, but it’s one of the most reliable strategies for making progress.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你让系统再次工作，你可以逐步引入复杂性。把这想象成逐个转动旋钮，而不是一次性全部转动。这并不华丽，但这是实现进步最可靠的策略之一。
- en: Simplify Your Model
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简化你的模型
- en: When your model has too many bells and whistles, it becomes difficult to pinpoint
    where things are going wrong. Often, the most effective debugging strategy is
    to strip it down to the bare essentials.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的模型有太多花哨的功能时，确定出错的地方变得困难。通常，最有效的调试策略是将模型简化到最基本的形式——更小的数据集、更浅的模型或更简单的损失函数。在复杂的管道中容易迷失方向，但当你一次隔离一个事物时，调试会容易得多。
- en: Simplify your architecture
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 简化你的架构
- en: 'Complex architectures can make it difficult to reason about what’s going wrong.
    To help with this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的架构可能会使得推理出错的原因变得困难。为了帮助解决这个问题：
- en: Eliminate unnecessary layers
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 消除不必要的层
- en: If a layer doesn’t contribute directly to the input–output mapping—such as layers
    that simply add model capacity without altering dimensions—it’s best to remove
    it during debugging.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某一层不直接贡献于输入-输出映射——例如那些仅仅增加模型容量而不改变维度的层——在调试时最好将其移除。
- en: Call basic layers directly
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 直接调用基本层
- en: Use layers like `nn.Conv` and `nn.Dense` instead of custom blocks, which can
    obscure bugs and internal behavior.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像`nn.Conv`和`nn.Dense`这样的层，而不是自定义块，因为自定义块可能会掩盖错误和内部行为。
- en: Reduce depth and width
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 减少深度和宽度
- en: If your model has many layers or units per layer, consider reducing both. A
    shallower model is easier to debug and understand, especially in the early stages
    of development.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型有很多层或每层的单元很多，考虑同时减少这两者。较浅的模型更容易调试和理解，尤其是在开发的早期阶段。
- en: Remove residual connections
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 移除残差连接
- en: These can complicate debugging by introducing dependencies between layers and
    by masking issues in the layers they connect (like poor initialization or gradient
    problems).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些连接可能会通过引入层之间的依赖关系以及掩盖它们连接的层中的问题（如初始化不良或梯度问题）来使调试复杂化。
- en: Tip
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The ultimate simplification is to reduce your model to a direct mapping from
    inputs to outputs. For example, pass your input through a single `nn.Dense` layer
    and see if it can overfit a tiny batch of data. If that fails, the issue is likely
    not with the architecture, but with your data pipeline, loss function, or optimizer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的简化方法是减少模型到从输入到输出的直接映射。例如，将输入通过单个 `nn.Dense` 层，看看它是否能过拟合一小批数据。如果这失败了，问题可能不是架构问题，而是你的数据管道、损失函数或优化器的问题。
- en: Turn off extras, like normalization and dropout
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关闭额外的功能，如归一化和 dropout
- en: These add complexity that’s often unnecessary during early debugging. In Flax,
    you must explicitly manage both model state (e.g., batch norm statistics) and
    random number generators (RNGs), which can easily lead to subtle bugs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些增加了在早期调试中通常不必要的复杂性。在 Flax 中，你必须显式管理模型状态（例如，批归一化统计）和随机数生成器（RNGs），这很容易导致微妙的问题。
- en: Don’t use batch norm
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 不要使用批归一化
- en: 'The complexity of batch norm is threefold. First, it behaves differently during
    training and inference. Second, it introduces additional state (running mean and
    variance) that must be updated outside standard gradient updates. Third, it breaks
    a key assumption: most layers operate independently on each batch element, but
    batch norm computes statistics across the batch. This makes it incompatible with
    tools like `vmap` or sharded training (`pmap`, `pjit`) unless you take special
    care to synchronize statistics across devices.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化的复杂性有三重。首先，它在训练和推理期间表现不同。其次，它引入了必须在外部标准梯度更新之外更新的额外状态（运行均值和方差）。第三，它破坏了一个关键假设：大多数层独立地对每个批处理元素进行操作，但批归一化计算整个批次的统计信息。这使得它在与
    `vmap` 或分片训练（`pmap`、`pjit`）等工具不兼容，除非你特别小心地同步设备间的统计信息。
- en: Skip dropout
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 跳过 dropout
- en: Dropout introduces stochasticity, making it harder to determine whether poor
    performance is due to randomness or a deeper issue.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 引入了随机性，使得难以确定性能不佳是由于随机性还是更深层次的问题。
- en: Simplify your optimizer
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 简化你的优化器
- en: Don’t worry about experimenting with different optimizers or learning rate schedules
    until the basics are working. Pick a sensible default—like good old Adam with
    a learning rate of `1e-3`, and focus on solving more fundamental issues first.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本功能正常工作之前，不要担心尝试不同的优化器或学习率计划。选择一个合理的默认值——比如老式的 Adam，学习率为 `1e-3`，并首先专注于解决更基本的问题。
- en: Avoid mixed precision
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 避免混合精度
- en: While lower-precision data types like `bfloat16`, `float16`, or `TensorFloat32`
    can improve performance and memory usage (out of scope for this book), they can
    lead to subtle numerical instability that’s extremely hard to debug. Note that
    even if you pass in `float32` inputs, JAX may default to lower-precision matmuls.
    To force full `float32` precision globally (especially during debugging), add
    `jax.config.update("jax_default_matmul_precision", "float32")`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像 `bfloat16`、`float16` 或 `TensorFloat32` 这样的低精度数据类型可以提高性能和内存使用（本书范围之外），但它们可能导致难以调试的微妙数值不稳定性。请注意，即使你传递了
    `float32` 输入，JAX 也可能默认使用低精度矩阵乘法。为了全局强制使用完整的 `float32` 精度（尤其是在调试期间），请添加 `jax.config.update("jax_default_matmul_precision",
    "float32")`。
- en: Note
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although turning off these more advanced features may lead to worse performance
    (due to underfitting or overfitting) and feel like you’re going backward, once
    the basic setup is functioning correctly, you can systematically reenable features
    to assess their impact on model performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关闭这些更高级的功能可能会导致性能下降（由于欠拟合或过拟合）并感觉像是退步，但一旦基本设置运行正确，你可以系统地重新启用功能以评估它们对模型性能的影响。
- en: Simplify and Control Your Environment
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简化和控制你的环境
- en: 'Your model might be fine, but your technical environment could be introducing
    unexpected issues. Many bugs that look like deep learning failures are really
    just environmental gremlins—this means that cleaning up your setup is often the
    fastest way forward. Here are some tips to keep in mind:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型可能没有问题，但你的技术环境可能引入了意外的问题。许多看似深度学习失败的问题实际上只是环境中的小问题——这意味着清理你的设置通常是前进的最快方式。以下是一些需要记住的提示：
- en: Sort out determinism and reproducibility
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 整理确定性和可重复性
- en: 'It’s easier to isolate issues if your experiments are reproducible. In addition
    to turning off stochastic components like dropout, consider:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的实验是可重复的，那么隔离问题会更容易。除了关闭像 dropout 这样的随机组件外，还应考虑：
- en: Setting explicit random seeds
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 设置明确的随机种子
- en: For reproducibility in JAX, you need to set the seed for `jax.random.PRNGKey(...)`—this
    controls all randomness in model initialization, dropout, and other JAX-based
    operations. Note that you don’t need to set Python’s `random.seed(...)` or NumPy’s
    `np.random.seed(...)` unless you’re also using them separately (e.g., in your
    data preprocessing or non-JAX components).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于JAX的可重复性，你需要设置`jax.random.PRNGKey(...)`的种子——这控制了模型初始化、dropout和其他基于JAX的操作中的所有随机性。请注意，除非你也在单独使用它们（例如，在数据预处理或非JAX组件中），否则你不需要设置Python的`random.seed(...)`或NumPy的`np.random.seed(...)`。
- en: Turning off dataset shuffling
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 关闭数据集洗牌
- en: Don’t shuffle your training data, or shuffle with a fixed random seed, to maintain
    a consistent order of examples across runs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 不要对你的训练数据进行洗牌，或者使用固定的随机种子进行洗牌，以保持运行之间示例的一致顺序。
- en: Keeping the environment constant
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 保持环境恒定
- en: Avoid inconsistencies caused by external factors (e.g., use the same hardware,
    library versions, and configurations).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 避免由外部因素引起的不一致性（例如，使用相同的硬件、库版本和配置）。
- en: Strip down your training loop
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 简化你的训练循环
- en: Especially in JAX and Flax, training loops require manual control over RNGs,
    state, and updates—making them powerful but easy to get wrong.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在JAX和Flax中，训练循环需要手动控制RNGs、状态和更新——这使得它们功能强大但容易出错。
- en: Train on a single batch for just a few steps
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 仅对单个批次进行几步训练
- en: This is often enough to catch major issues.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常足以捕捉到主要问题。
- en: Disable extras like logging, metrics, or learning rate schedules
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用额外的功能，如日志记录、指标或学习率计划
- en: These can obscure what’s actually happening during training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可能会掩盖训练过程中实际发生的事情。
- en: Use fixed inputs and random seeds
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用固定的输入和随机种子
- en: Run your training step on the same batch every time. This eliminates variability
    due to changing data and makes bugs easier to isolate.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每次都在相同的批次上运行你的训练步骤。这消除了数据变化引起的可变性，使得错误更容易隔离。
- en: Avoid high-level abstractions temporarily
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时避免高级抽象
- en: If you’re using tools like `TrainState`, try replacing them with raw variable
    updates until the core logic works.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用`TrainState`等工具，尝试在核心逻辑工作之前用原始变量更新替换它们。
- en: Make your code self-contained
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使你的代码自包含
- en: Especially when working in interactive environments like Colab or Jupyter notebooks,
    it’s easy for your environment to get cluttered with old variables or states without
    you realizing it. Restarting the kernel and organizing your code into self-contained
    functions can really help with debugging.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在Colab或Jupyter笔记本等交互式环境中工作时，你的环境很容易在不经意间变得杂乱，充斥着旧的变量或状态。重启内核并将你的代码组织成自包含的函数，这真的有助于调试。
- en: Turn off JIT compilation (with caution)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 谨慎关闭JIT编译
- en: 'Disabling `@jax.jit` can help identify issues in your training step more easily
    by making stack traces clearer and behavior more explicit. However, be aware:
    turning off JIT can cause huge memory usage spikes and drastically slower execution,
    making this approach impractical for larger runs.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用`@jax.jit`可以使训练步骤中的问题更容易识别，因为这使得堆栈跟踪更清晰，行为更明确。然而，请注意：关闭JIT可能会导致内存使用量激增和执行速度大幅下降，这使得这种方法对于较大的运行来说不切实际。
- en: Train on a single GPU instead of multiple GPUs
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 用单个GPU而不是多个GPU进行训练
- en: 'If you are using GPUs, start with just one. Multi-GPU training via sharding
    (e.g., with `jax.pjit`) introduces a lot of additional complexity. (Note: `pmap`
    is being deprecated in favor of `pjit`, so it’s best not to rely on it.)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用GPU，从只使用一个开始。通过分片（例如，使用`jax.pjit`）的多GPU训练引入了许多额外的复杂性。（注意：`pmap`正在被弃用，以`pjit`取而代之，因此最好不要依赖它。）
- en: Use CPU for simpler debugging setups
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CPU进行简单的调试设置
- en: If your model is small and you’re trying to isolate a bug, using CPU can remove
    the complexity of device placement and driver issues. It’s slower, but often easier
    to reason about. Just be aware that some bugs may appear only on accelerators.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型很小，你正在尝试隔离一个错误，使用CPU可以去除设备放置和驱动程序问题的复杂性。它速度较慢，但通常更容易推理。只是要注意，一些错误可能仅在加速器上出现。
- en: Go back to NumPy
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 回到NumPy
- en: While `jax.numpy` mimics NumPy closely, the original NumPy library has simpler
    internals and often better error messages. You can’t train models or use autodiff,
    but for testing data transformations or verifying calculations, it can be useful
    to isolate and debug numpy-only code outside of JAX.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`jax.numpy`与NumPy非常相似，但原始的NumPy库具有更简单的内部结构，并且通常有更好的错误消息。你不能训练模型或使用自动微分，但为了测试数据转换或验证计算，将numpy-only代码隔离和调试在JAX之外可能很有用。
- en: These steps may feel tedious, but a clean and controlled environment is often
    the difference between spinning your wheels for days and finding a bug in 10 minutes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤可能感觉繁琐，但一个干净且受控的环境通常是几天徒劳和10分钟找到错误之间的区别。
- en: Warning
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'A quick note: different hardware backends behave slightly differently when
    running JAX code, especially when it comes to reproducibility.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 简要说明：不同的硬件后端在运行JAX代码时表现略有不同，尤其是在可重现性方面。
- en: '*TPUs* are fully deterministic in JAX. If you use the same random seed, you’ll
    get the same results every time.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TPU* 在JAX中是完全一致的。如果你使用相同的随机种子，你将每次都得到相同的结果。'
- en: '*GPUs* are mostly deterministic, but some operations (like convolutions or
    matrix multiplications) may vary slightly across runs unless you disable certain
    performance optimizations.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*GPU* 主要是一致的，但某些操作（如卷积或矩阵乘法）在运行中可能略有差异，除非你禁用了某些性能优化。'
- en: '*CPUs* are generally deterministic, but subtle sources of non-determinism (like
    thread scheduling) can still appear.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CPU* 通常是一致的，但细微的非确定性来源（如线程调度）仍然可能存在。'
- en: For early debugging in JAX, running on CPU can simplify things—but just be aware
    that bugs might show up differently when you move to GPU or TPU.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在JAX的早期调试中，在CPU上运行可以简化事情——但请注意，当你迁移到GPU或TPU时，错误可能会以不同的方式出现。
- en: Simplify the Data and Problem
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简化数据和问题
- en: 'Make your dataset and prediction task easier to simplify debugging. Here are
    some ways to make the data more manageable during early experimentation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使你的数据集和预测任务更容易简化调试。以下是一些在早期实验中使数据更易于管理的几种方法：
- en: Visualize individual examples
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化单个示例
- en: Actually plot or print raw inputs and labels—not just summaries. You’ll often
    catch issues like incorrect encodings, off-by-one errors, or mismatched image-label
    pairs this way. It’s surprisingly tempting to skip this step—don’t. Simply looking
    at the raw data can reveal issues that save you hours later.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 实际绘制或打印原始输入和标签——而不仅仅是摘要。你通常会通过这种方式捕捉到诸如编码错误、偏移量错误或图像-标签对不匹配等问题。跳过这一步骤似乎很有吸引力——不要这样做。简单地查看原始数据可以揭示在之后节省数小时的问题。
- en: Check class balance
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 检查类别平衡
- en: An imbalanced dataset can make the model appear broken when it’s just doing
    the naive thing (predicting the majority class). Consider subsampling or rebalancing
    during debugging.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型只是进行朴素预测（预测多数类）时，不平衡的数据集可能会让模型看起来像是有问题。在调试过程中考虑子采样或重新平衡。
- en: Remove data augmentation
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 移除数据增强
- en: Augmentations like cropping, flipping, or adding noise can hide underlying issues
    or make the task unnecessarily hard. Turn them off until you’re confident the
    core pipeline works.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 增强如裁剪、翻转或添加噪声可能会隐藏潜在问题或使任务变得不必要地困难。在确信核心管道工作正常之前，请关闭它们。
- en: Reduce the number of classes
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 减少类别的数量
- en: Instead of predicting many categories, reframe your task as binary classification
    to focus on the clearest signal first.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 不要预测许多类别，将你的任务重新定义为二元分类，首先关注最清晰的信号。
- en: Simplify the output space
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 简化输出空间
- en: On a similar note, if your target is complex (e.g., a regression label or structured
    output), try reducing it to something simpler. For instance, predict a binary
    class or thresholded version of the label instead. This can help verify the pipeline
    before tackling the full problem.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似的情况下，如果你的目标是复杂的（例如，回归标签或结构化输出），尝试将其简化为更简单的东西。例如，预测二进制类别或标签的阈值版本。这有助于在解决完整问题之前验证管道。
- en: Make your dataset smaller
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使你的数据集更小
- en: Large datasets slow everything down. Use a small, representative subset that
    captures the key structure.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据集会减慢一切。使用一个小的、具有代表性的子集来捕捉关键结构。
- en: Limit the scope of your data
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 限制你的数据范围
- en: Use a natural slice of your dataset. For example, restrict to a single species,
    tissue type, year, or patient group. This cuts variability and helps isolate bugs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你数据集的一个自然切片。例如，限制为单一物种、组织类型、年份或患者群体。这减少了变异性并有助于隔离错误。
- en: Check for label leakage
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 检查标签泄漏
- en: Especially in biological datasets, label leakage can creep in through metadata
    like patient ID, batch number, or experiment date. This can cause your model to
    perform suspiciously well by learning shortcuts. Double-check that no features
    or splits accidentally leak target-related information.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在生物数据集中，标签泄漏可能通过患者ID、批次号或实验日期等元数据悄悄进入。这可能导致你的模型通过学习捷径而表现出可疑的良好性能。务必检查没有特征或分割意外泄漏目标相关的信息。
- en: Work with synthetic or simulated data
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用合成或模拟数据
- en: If feasible, start with synthetic data that mimics key characteristics of your
    real dataset but is easier to understand and trace. You can also add controlled
    noise (e.g., `np.random.normal(0, 1)`) to test the model’s robustness.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可行，从模拟数据开始，这些数据模仿了你的真实数据集的关键特征，但更容易理解和追踪。你还可以添加受控的噪声（例如，`np.random.normal(0,
    1)`）来测试模型的鲁棒性。
- en: Simplifying the data isn’t about giving up—it’s about creating a controlled
    testbed where you can debug quickly, eliminate uncertainty, and build confidence
    before scaling back up.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 简化数据并不是放弃——而是在创建一个受控的测试平台，你可以快速调试、消除不确定性，并在扩展之前建立信心。
- en: Overfit to a Single Batch of Data
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对单个数据批次进行过拟合
- en: 'We’ve mentioned this briefly before, but it’s worth stating explicitly: if
    your model doesn’t seem to be learning much at all, a classic debugging strategy
    is to try to overfit to a single batch—meaning, rerun the training loop on the
    same batch repeatedly and see if the model can memorize it. This test helps confirm
    that the training loop, loss function, and optimizer are wired up correctly.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前简要提到过这一点，但值得明确指出：如果你的模型似乎根本学不到什么，一个经典的调试策略是尝试对单个批次进行过拟合——这意味着，在同一个批次上反复运行训练循环，看看模型是否能够记住它。这个测试有助于确认训练循环、损失函数和优化器是否正确连接。
- en: 'Instead of the usual training loop:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是通常的训练循环：
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'you can try this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试这样做：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If your setup is working correctly, the model should rapidly memorize the batch,
    and the loss should decrease significantly after a few steps. If not, consider
    checking for:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的设置工作正常，模型应该会迅速记住批次，损失在几步之后应该会显著下降。如果不这样，考虑检查以下内容：
- en: 'Learning rate issues:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率问题：
- en: The learning rate might be too high (causing divergence) or too low (causing
    no learning).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率可能太高（导致发散）或太低（导致无法学习）。
- en: Frozen parameters or bad gradients
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结的参数或坏的梯度
- en: Sometimes parameters are not being updated at all—for example, if they were
    accidentally excluded from the `params` dict due to naming mismatches or scoping
    issues. Also inspect gradients—if they’re all zero or NaN, that’s a clue.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有时参数根本不会被更新——例如，如果它们由于命名不匹配或作用域问题意外地被排除在`params`字典之外。还要检查梯度——如果它们都是零或NaN，那是一个线索。
- en: Loss function bugs
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数错误
- en: Make sure you’re using the correct loss function for your task (e.g., cross-entropy
    for classification) and that it’s behaving numerically as expected. Prefer standard
    implementations over custom home-made ones, at least during debugging.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你正在使用适合你任务的正确损失函数（例如，对于分类使用交叉熵），并且它在数值上表现如预期。至少在调试期间，优先使用标准实现而不是自定义的。
- en: Model initialization problems
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 模型初始化问题
- en: Poor or inconsistent weight initialization can prevent learning, especially
    in deeper networks. If you’re using custom modules, double-check their initializations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 糟糕或不一致的权重初始化可能会阻碍学习，尤其是在深度网络中。如果你使用自定义模块，请务必检查它们的初始化。
- en: Batch size too small
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 批次大小太小
- en: Very small batches (e.g., 1–2 examples) can lead to noisy gradients and unstable
    updates. For debugging, use a small but reasonable batch size like 8–32.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 非常小的批次（例如，1-2个示例）可能会导致梯度噪声和不稳定的更新。对于调试，使用8-32这样的小但合理的批次大小。
- en: Silent shape mismatches or broadcasting errors
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 静默的形状不匹配或广播错误
- en: These won’t always crash your code, but they can silently mess up your loss
    or gradients. Print tensor shapes and inspect intermediate outputs to confirm
    everything lines up as expected.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这些错误并不总是会崩溃你的代码，但它们可能会无声地破坏你的损失或梯度。打印张量形状并检查中间输出以确认一切如预期的那样对齐。
- en: This is one of the fastest and most informative debugging steps—if your model
    can’t learn a tiny batch, don’t bother scaling up yet.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最快且最有信息量的调试步骤之一——如果你的模型无法学习一个小批次，那么现在就别忙着扩展了。
- en: Go Back to Basics
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归基础知识
- en: 'If none of these debugging tips works and you are rapidly losing your mind,
    one of the most effective strategies is to revert to a simple, well-understood
    example that you know works:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些调试技巧都不起作用，你快要失去理智了，最有效的策略之一是回到一个简单、易于理解的例子，你知道它是有效的：
- en: Start with an established example
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个已建立的例子开始
- en: Train a simple model on a well-known dataset—like a basic CNN on MNIST for image
    problems. These examples are widely used and well-documented, making them a reliable
    way to get something working end-to-end.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个知名数据集上训练一个简单的模型——例如，在MNIST上使用基本CNN进行图像问题。这些例子被广泛使用且文档齐全，使它们成为实现端到端工作的可靠方式。
- en: Reproduce known results
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 重新生成已知结果
- en: Make sure your setup can successfully train the model and reach the expected
    performance (e.g., ~99% accuracy on MNIST). This confirms your training loop,
    model, and loss function are functioning correctly.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的设置可以成功训练模型并达到预期的性能（例如，MNIST上的~99%准确率）。这证实了你的训练循环、模型和损失函数是正常工作的。
- en: Swap in your dataset
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 更换你的数据集
- en: Once the baseline works, begin replacing the dataset with your own. Proceed
    gradually and check that everything still functions as expected.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦基线工作正常，开始用你自己的数据集替换它。逐步进行，并检查一切是否仍然按预期工作。
- en: Iteratively add complexity
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步增加复杂性
- en: With your data integrated, incrementally introduce more complex components—like
    deeper architectures or new training strategies. Watch for breakage after each
    change.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集成后，逐步引入更复杂的组件——如更深的架构或新的训练策略。注意每次更改后的故障情况。
- en: There’s absolutely nothing wrong with going back to a tutorial and training
    a simple linear model—sometimes that’s the fastest way to confirm your setup and
    get your bearings.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 回到教程并训练一个简单的线性模型绝对没有问题——有时那是最快确认你的设置并获得方向的方法。
- en: Log Everything
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记录一切
- en: Good logging is the difference between productive debugging and blindly guessing.
    When something goes wrong, clear logs can help you trace back exactly what happened—and
    when things go right, they help you understand why.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的日志记录是有效调试和盲目猜测之间的区别。当出现问题的时候，清晰的日志可以帮助你追溯确切发生了什么——而当事情顺利时，它们帮助你理解为什么会这样。
- en: Log training loss and key metrics over time
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 随时间记录训练损失和关键指标
- en: At a minimum, track loss, accuracy, and any relevant task-specific metrics (like
    auROC or auPRC). This makes it easier to spot overfitting, instability, or underperformance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，跟踪损失、准确率以及任何相关的任务特定指标（如auROC或auPRC）。这有助于更容易地发现过拟合、不稳定或性能不足。
- en: Log validation performance at regular intervals
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在固定的时间间隔记录验证性能
- en: Seeing how your model generalizes during training helps detect overfitting early
    and can catch bugs where validation performance diverges for no obvious reason.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 观察你的模型在训练过程中的泛化能力有助于早期检测过拟合，并且可以捕捉到验证性能出现偏差但原因不明显的错误。
- en: Log inputs, predictions, and errors
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 记录输入、预测和错误
- en: Save a few input samples, predicted outputs, and errors at each step (or epoch).
    This is especially useful for spotting systematic failures (e.g., always misclassifying
    a certain class).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤（或epoch）保存一些输入样本、预测输出和错误。这对于发现系统性的失败（例如，总是错误地分类某个类别）特别有用。
- en: Record configuration and hyperparameters
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 记录配置和超参数
- en: Save the learning rate, batch size, optimizer type, and model architecture along
    with each run. You will forget. Everyone forgets.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 将每次运行的学习率、批量大小、优化器类型和模型架构保存下来。你会忘记的。每个人都会忘记。
- en: Use a structured logger or tracking tool
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结构化日志记录器或跟踪工具
- en: Tools like TensorBoard, Weights & Biases, or even just structured JSON logs
    can make it easier to compare runs and understand what changed.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorBoard、Weights & Biases或仅仅是结构化的JSON日志等工具可以更容易地比较运行情况并理解发生了什么变化。
- en: Tip
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Logging might feel like overhead in the moment, but it’s one of the best time
    investments you can make. Even minimal logs can help you debug faster and avoid
    retraining unnecessarily.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 记录日志在当下可能感觉像是额外的负担，但这是你可以做的最好的时间投资之一。即使是最少的日志也能帮助你更快地调试并避免不必要的重新训练。
- en: 'There’s a rule of thumb: every new log reveals a bug you didn’t know you had.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个经验法则：每个新的日志都会揭示你不知道的bug。
- en: Ask for Help
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻求帮助
- en: If you’re still stuck, don’t be afraid to reach out. Community forums like Stack
    Overflow or GitHub Discussions are valuable resources. Talking to a colleague
    or friend can also help—sometimes just explaining the problem out loud (rubber
    ducking) leads to breakthroughs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然遇到困难，不要害怕寻求帮助。社区论坛如Stack Overflow或GitHub Discussions是宝贵的资源。与同事或朋友交谈也可能有所帮助——有时只是大声解释问题（橡皮鸭法）就能带来突破。
- en: You can also use LLMs like ChatGPT, Gemini, or Claude to help troubleshoot or
    explore ideas. Just remember that while these models can be very helpful, their
    suggestions aren’t always correct and can introduce new bugs—so double-check any
    code they generate.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用像ChatGPT、Gemini或Claude这样的LLM来帮助排查问题或探索想法。但请记住，虽然这些模型非常有帮助，但它们的建议并不总是正确的，可能会引入新的错误——所以请务必检查它们生成的任何代码。
- en: Common Data Issues
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见数据问题
- en: Often, it’s not your model that has a bug—it’s the data. Subtle issues in your
    dataset can quietly break your learning pipeline, and you may find yourself spending
    hours debugging the model setup when the problem is actually upstream. This section
    covers common data pitfalls that are worth checking before tearing your architecture
    apart.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，问题不在于你的模型有错误——而是数据。数据集中的微妙问题可能会悄无声息地破坏你的学习流程，你可能会发现自己花费数小时调试模型设置，而问题实际上在上游。
- en: Data Leakage
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据泄漏
- en: As physicist Richard Feynman famously said, “The first principle is that you
    must not fool yourself—and you are the easiest person to fool.” It’s all too easy
    to believe your model is doing well when it’s actually cheating. Data leakage
    happens when information that should be hidden during training is inadvertently
    accessible to the model, leading to overly optimistic performance metrics.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如著名物理学家理查德·费曼所说：“第一条原则是，你必须不要欺骗自己——而你是最容易欺骗的人。”当你的模型实际上在作弊时，相信你的模型表现良好是非常容易的。数据泄漏发生在训练期间应该隐藏的信息意外地被模型访问，导致过度乐观的性能指标。
- en: Obvious cases
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明显的情况
- en: Evaluating the model on the training set itself. This sounds slightly silly,
    but it’s surprisingly common—especially in informal settings like Kaggle notebooks.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练集本身上评估模型。这听起来有点愚蠢，但出奇地常见——尤其是在Kaggle笔记本这样的非正式环境中。
- en: Evaluating on a validation or test set where some examples overlap with the
    training set.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在验证集或测试集上评估，其中一些示例与训练集重叠。
- en: Subtle cases
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微妙的情况
- en: Leakage through preprocessing, for example, normalizing the full dataset before
    splitting into train/valid/test sets.
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过预处理泄漏，例如，在将数据集拆分为训练/验证/测试集之前对整个数据集进行归一化。
- en: Features that leak future information—correlated with the target only because
    they wouldn’t be available at prediction time.
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泄露未来信息的特征——仅因为它们在预测时不可用，才与目标相关。
- en: 'Here are some real-world examples of subtle leakage:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些现实世界的微妙泄漏的例子：
- en: You want to classify skin lesions as malignant or benign. Patients with cancer
    are photographed in one clinic and healthy patients in a different clinic. If
    one clinic uses brighter lighting, your model may learn to use brightness as a
    proxy for cancer.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望将皮肤病变分类为恶性或良性。癌症患者在一家诊所拍照，健康患者在另一家诊所拍照。如果一家诊所使用更亮的照明，你的模型可能会学会将亮度作为癌症的代理。
- en: You’re predicting protein binding to a gene, and you include gene expression
    level as an input to the model. Since a gene’s expression may be affected by binding,
    your model might learn to rely on this proxy instead of the DNA sequence features
    you intended it to learn.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你正在预测蛋白质与基因的结合，并将基因表达水平作为模型的输入。由于基因的表达可能会受到结合的影响，你的模型可能会学会依赖这个代理，而不是你打算让它学习的DNA序列特征。
- en: Warning
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If your model performs well on a test set but fails to generalize to new datasets
    or real-world settings, data leakage is one of the first things to investigate.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型在测试集上表现良好，但无法推广到新的数据集或现实世界场景，数据泄漏是首先要调查的问题之一。
- en: 'To avoid data leakage:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免数据泄漏：
- en: Always ensure that test data is fully isolated and untouched by the training
    pipeline.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是确保测试数据完全隔离，并且未被训练流程所触及。
- en: When adding new training data partway through a project, check whether it already
    appears in your validation or test sets.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在项目中途添加新的训练数据时，检查这些数据是否已经出现在你的验证集或测试集中。
- en: 'Ask yourself: *Would this feature be available at inference time?* If not,
    don’t use it.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问问自己：*这个特征在推理时是否可用？* 如果不可用，不要使用它。
- en: Use model interpretation tools to see what aspects of your data your model is
    relying on when making its predictions. Does what you see match your expectations,
    or is the model picking up on artifacts?
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型解释工具来查看你的模型在做出预测时依赖于哪些数据方面。你所看到的是否符合你的预期，或者模型是否在捕捉到伪影？
- en: Incorrect Data Labels
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误的数据标签
- en: 'It seems obvious, but incorrectly labeled data is one of the most common and
    frustrating causes of model underperformance. Some high-risk scenarios include:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然很明显，但错误标记的数据是模型性能不佳的最常见和最令人沮丧的原因之一。一些高风险场景包括：
- en: Labels stored separately from inputs
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将标签存储在输入之外
- en: If labels are in a separate file (e.g., a CSV with filenames and classes), they
    can easily be mismatched or misjoined during preprocessing. We certainly have
    several shameful anecdotes along these lines.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标签存储在单独的文件中（例如，包含文件名和类别的CSV文件），它们在预处理过程中很容易出现不匹配或错误连接。我们在这方面确实有一些令人尴尬的故事。
- en: Shuffling inputs and labels independently
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 独立地洗牌输入和标签
- en: If you shuffle data and labels separately, they’ll fall out of sync—silently.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你分别对数据和标签进行洗牌，它们将失去同步——默默地。
- en: 'Shape mismatches in TensorFlow datasets:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow数据集中的形状不匹配：
- en: '`tf.data.Dataset` won’t necessarily complain if your labels and inputs have
    mismatched shapes like data of shape `(100, 10)` but labels of shape `(43,)`.
    This can result in silent failures that only manifest much later.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data.Dataset`不会因为你的标签和输入具有不匹配的形状（例如，数据形状为`(100, 10)`但标签形状为`(43,)`）而必然抱怨。这可能导致沉默的失败，只有在很久以后才会显现。'
- en: Merging tabular datasets incorrectly
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 错误合并表格数据集
- en: Joining datasets without verifying alignment (e.g., via `merge` in pandas) can
    mislabel data rows without throwing errors.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有验证对齐的情况下（例如，通过pandas中的`merge`）合并数据集可能会错误地标记数据行而不会抛出错误。
- en: Data augmentation pipelines modifying labels incorrectly
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强管道错误修改标签
- en: Augmentation effectively increases your dataset—so it’s also a high-risk area
    for introducing label corruption.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 增强实际上增加了你的数据集——因此，它也是引入标签损坏的高风险区域。
- en: Note
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'A common warning sign of label issues: the training loss does goes down slightly
    during training but plateaus early at a high value, and accuracy (or other metrics)
    stays near random chance level.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 标签问题的常见警告信号：训练损失在训练过程中确实略有下降，但早期就达到一个高值并趋于平稳，准确率（或其他指标）保持在随机机会水平附近。
- en: For example, if the labels are scrambled, your accuracy would hover around random
    baseline values like 50% in balanced binary classification.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果标签被打乱，你的准确率将徘徊在平衡二元分类的随机基线值（如50%）附近。
- en: The best antidote to label issues is simply spending time inspecting your input-label
    pairs, both at the beginning as raw data, and at different points in your data
    pipeline. Check a few batches by hand. Plot examples and verify the label. It
    may feel tedious—but it’s one of the fastest ways to catch silent bugs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 解决标签问题的最佳方法就是花时间检查你的输入-标签对，无论是开始时的原始数据，还是在数据管道的不同阶段。手动检查几个批次。绘制示例并验证标签。这可能感觉有些繁琐——但它是最快捕捉沉默错误的方法之一。
- en: Imbalanced Classes
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不平衡的类别
- en: In biology, it’s common for one class to vastly outnumber others—like detecting
    rare mutations or identifying diseased cells. This class imbalance is not an issue
    in itself, but a model trained on imbalanced data might just learn to always predict
    the majority class, achieving deceptively high accuracy.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物学中，一个类别远远多于其他类别的情况很常见——比如检测罕见的突变或识别患病细胞。这种类别不平衡本身并不是问题，但训练在不平衡数据上的模型可能会学会总是预测多数类，从而实现欺骗性的高准确率。
- en: 'Warning signs:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 警告信号：
- en: Accuracy is high, but precision or recall on the minority class is poor.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率很高，但少数类的精确率或召回率很差。
- en: Confusion matrix shows the model rarely predicts the minority class.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵显示模型很少预测少数类。
- en: 'To address this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题：
- en: Use class weighting or focal loss to penalize the model more for mistakes on
    the minority class. Focal loss down-weights easy examples and focuses learning
    on hard, misclassified ones—especially useful when the rare class is easily overwhelmed.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用类别权重或焦点损失来惩罚模型对少数类的错误更多。焦点损失降低简单示例的权重，并专注于学习困难、被错误分类的示例——特别是当稀有类别容易被压倒时非常有用。
- en: Resample the data—either oversample the minority class or undersample the majority
    class—to reduce imbalance. Oversampling is often safer when data is scarce but
    can lead to overfitting if not done carefully.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新采样数据——要么对少数类进行过采样，要么对多数类进行欠采样，以减少不平衡。当数据稀缺时，过采样通常更安全，但如果不小心进行，可能会导致过拟合。
- en: Use stratified sampling to ensure class balance is preserved across your train,
    validation, and test splits. This means splitting the data so each subset maintains
    the original class proportions, avoiding skewed performance estimates.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分层抽样来确保在训练、验证和测试分割中保持类别平衡。这意味着分割数据，以便每个子集保持原始的类别比例，避免性能估计偏差。
- en: Distribution Shifts
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布偏移
- en: Training and test data can often come from different sources—different labs,
    species, sequencing protocols, or imaging setups. These shifts can cause models
    to learn dataset-specific artifacts instead of generalizable biology.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据和测试数据通常来自不同的来源——不同的实验室、物种、测序协议或成像设置。这些偏移可能导致模型学习特定于数据集的伪象，而不是可推广的生物学。
- en: 'Warning signs:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 警告信号：
- en: Strong validation performance doesn’t transfer to real-world or external datasets.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强大的验证性能并不能转移到现实世界或外部数据集。
- en: A model trained to predict dataset labels (e.g., lab or batch ID) performs surprisingly
    well.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练来预测数据集标签（例如，实验室或批次ID）的模型表现惊人。
- en: 'To catch and correct for this:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉和纠正这个问题：
- en: Visualize embeddings (e.g., via PCA or UMAP) colored by data source to spot
    clustering.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过数据源着色可视化嵌入（例如，通过PCA或UMAP）以发现聚类。
- en: Use batch effect correction or domain adaptation methods if needed.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如有必要，使用批量效应校正或领域自适应方法。
- en: Be cautious when mixing data from different sources—explicitly test generalization
    to new settings.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在混合来自不同来源的数据时要谨慎——明确测试对新环境的泛化能力。
- en: Biology-Specific Gotchas
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物学特有的陷阱
- en: 'Biology is a vast field, filled with complex systems and ever-evolving datasets.
    We can’t cover every pitfall here, but the following are some common sources of
    bugs and errors that we’ve encountered repeatedly—and that are worth keeping in
    mind when building models on biological data:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 生物学是一个广阔的领域，充满了复杂的系统和不断发展的数据集。我们无法在此涵盖每一个陷阱，但以下是我们反复遇到的一些常见错误和故障源，这些在基于生物数据构建模型时值得牢记：
- en: Versioning issues
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制问题
- en: Many biological datasets are tied to reference versions (e.g., genome builds,
    gene IDs, transcript annotations). It’s dangerously easy to mix up genome versions
    (e.g., GRCh37 vs GRCh38), gene ID versions, or even organism accessions. Make
    sure all parts of your pipeline are using consistent versions—or explicitly map
    between them.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 许多生物数据集都与参考版本（例如，基因组构建、基因ID、转录注释）相关联。混淆基因组版本（例如，GRCh37与GRCh38）、基因ID版本，甚至生物体存取编号非常危险。确保您的管道所有部分都使用一致的版本——或者明确地在它们之间进行映射。
- en: Data integration challenges
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集成挑战
- en: 'Combining datasets from different sources is common in biology but can lead
    to subtle inconsistencies: mismatched identifiers, differing file formats, or
    incompatible measurement units (e.g., read counts versus TPM versus RPKM). Carefully
    check alignment before merging datasets.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从不同来源组合数据集在生物学中很常见，但可能导致微妙的矛盾：不匹配的标识符、不同的文件格式或不相容的测量单位（例如，读取计数与TPM与RPKM）。在合并数据集之前仔细检查对齐。
- en: Biological heterogeneity
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 生物异质性
- en: Biological systems vary widely—across individuals, cell types, populations,
    and species. A model trained on European ancestry samples may not generalize to
    other ancestries. Likewise, models trained on data from immortalized cancer cell
    lines can fail when applied to normal primary cells. Always consider the scope
    and limitations of your training data.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 生物系统差异很大——跨越个体、细胞类型、种群和物种。在欧洲血统样本上训练的模型可能无法推广到其他血统。同样，在永生化癌细胞系上训练的模型在应用于正常原代细胞时可能会失败。始终考虑您训练数据的范围和限制。
- en: Ambiguous or soft labels
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊或软标签
- en: 'Biological categories are often not cleanly defined: cell types can be graded
    or transitional, and protein binding is often a continuous score, not a binary
    yes/no. Hard labels, where present, may oversimplify what is actually a spectrum.
    In these cases, performance ceilings may reflect label ambiguity, not model failure.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 生物类别通常没有干净的定义：细胞类型可以是分级或过渡性的，蛋白质结合通常是一个连续的分数，而不是二元的肯定/否定。如果存在硬标签，可能会过分简化实际上是一个连续谱。在这些情况下，性能上限可能反映了标签的模糊性，而不是模型失败。
- en: Experimental noise
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 实验噪声
- en: Just adding more data isn’t always better—low-quality experimental data can
    introduce noise that overwhelms signal. Look for ways to filter or denoise.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 仅增加更多数据并不总是更好的——低质量的实验数据可能会引入噪声，从而掩盖信号。寻找过滤或去噪的方法。
- en: Use quality metrics
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用质量指标
- en: Many experiments include built-in quality scores. Filtering on these can help.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 许多实验包括内置的质量分数。基于这些分数进行过滤可能会有所帮助。
- en: Leverage replicates
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 利用重复
- en: Use experimental replicates (same exact setup, multiple times) or biological
    replicates (same protocol, but different samples) to reduce variance. You can
    average replicate signals or use them to quantify uncertainty.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 使用实验重复（相同的精确设置，多次进行）或生物重复（相同的方案，但不同的样本）以减少变异性。您可以平均重复信号或使用它们来量化不确定性。
- en: Batch effects
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 批量效应
- en: Differences in lab conditions, reagent lots, sequencing machines, or protocols
    can introduce strong confounding signals. These technical artifacts often dominate
    the true biological signal if not accounted for. Visualize your data (e.g., with
    PCA or UMAP colored by batch) to assess how much batches cluster apart. You can
    also train a model to predict batch labels—if it performs better on this than
    your actual task, your model is probably learning batch-specific noise. If needed,
    apply normalization techniques like quantile normalization to mitigate these effects.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 实验室条件、试剂批次、测序机器或协议的差异可能会引入强烈的混杂信号。如果不加以考虑，这些技术伪影往往会掩盖真实的生物信号。通过可视化数据（例如，使用按批次着色的PCA或UMAP）来评估批次之间的聚类程度。您还可以训练一个模型来预测批次标签——如果它在实际任务上的表现优于您的模型，那么您的模型可能是在学习特定于批次的噪声。如果需要，可以应用如分位数归一化等归一化技术来减轻这些影响。
- en: Common Model Issues
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见模型问题
- en: Not all training failures come from bad data—sometimes, the model itself is
    the problem. In this section, we highlight common issues that arise during model
    training, from overfitting to gradient instability.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有训练失败都源于数据质量差——有时，问题出在模型本身。在本节中，我们将突出模型训练过程中出现的常见问题，从过拟合到梯度不稳定。
- en: There are more detailed deep learning debugging guides out there—for example,
    the [Deep Learning Tuning Playbook](https://oreil.ly/-t96r) by Google. But here
    we’ll recap some of the most common and practical failure modes, with a focus
    on how to identify and fix them efficiently.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在网上有更多详细的深度学习调试指南——例如，谷歌的[深度学习调优剧本](https://oreil.ly/-t96r)。但在这里，我们将回顾一些最常见且实用的故障模式，重点关注如何高效地识别和修复它们。
- en: Overfitting and Poor Generalization
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合和泛化能力差
- en: Overfitting is one of the most common issues in deep learning. Deep neural networks
    typically have high capacity and are only told to minimize training loss—without
    any built-in notion of generalization. As a result, they often perform well on
    training data but poorly on unseen data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是深度学习中常见的问题之一。深度神经网络通常具有很高的容量，并且只被要求最小化训练损失——没有任何内置的泛化概念。因此，它们通常在训练数据上表现良好，但在未见过的数据上表现较差。
- en: 'Fortunately, there’s a well-established set of regularization techniques to
    help reduce overfitting—many of which we’ve already discussed throughout this
    book:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一套成熟的正则化技术可以帮助减少过拟合——其中许多我们在本书中已经讨论过：
- en: Dropout
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout
- en: Randomly disables units in the network during training to prevent over-reliance
    on any one path.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中随机禁用网络中的单元，以防止过度依赖任何一条路径。
- en: Weight decay
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Weight decay
- en: Penalizes large weights (L1 or L2 regularization) to encourage simpler models
    that generalize better.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过惩罚大权重（L1或L2正则化）来鼓励更简单且泛化能力更强的模型。
- en: Early stopping
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止
- en: Monitors validation performance and stop training when performance starts to
    degrade, even if training loss continues to drop.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 监控验证性能，并在性能开始下降时停止训练，即使训练损失仍在下降。
- en: Data augmentation
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强
- en: Expands your dataset by applying small, meaningful transformations (e.g., rotations,
    flips in images; jittering or cropping in sequences).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用小的、有意义的变换（例如，图像的旋转、翻转；序列中的抖动或裁剪）来扩展您的数据集。
- en: Ensembling
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Ensembling
- en: Combines predictions from multiple models trained with different seeds or splits
    as a form of error correction. Even ensembling the same architecture can significantly
    improve robustness.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用不同种子或分割训练的多个模型的预测结果结合起来，作为一种错误校正的形式。即使结合相同的架构也可以显著提高鲁棒性。
- en: Also, check whether your validation or test data differs fundamentally from
    your training data (see the previous section). If the distribution has genuinely
    shifted, then the model might not be overfitting so much as encountering data
    it was never trained to handle.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，检查您的验证数据或测试数据是否与训练数据有根本性的不同（参见上一节）。如果分布确实发生了变化，那么模型可能不是过拟合，而是遇到了它从未训练过的数据。
- en: Note
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A model that performs well on training data but poorly on validation is likely
    overfitting. A model that performs poorly on both might be underfitting or struggling
    with a broken setup.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一个在训练数据上表现良好但在验证数据上表现差的模型很可能是过拟合。一个在两者上都表现不佳的模型可能是欠拟合或设置有误。
- en: Vanishing or Exploding Gradients
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vanishing or Exploding Gradients
- en: '*Vanishing gradients*, where gradient values approach zero, and exploding gradients,
    where they become excessively large, can severely disrupt training. Fortunately,
    these issues are relatively easy to detect.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*消失的梯度*，即梯度值接近零，以及*爆炸的梯度*，即梯度值变得过大，可能会严重干扰训练。幸运的是，这些问题相对容易检测。'
- en: A simple way to monitor gradients is to compute their L2 norm (also called the
    Euclidean norm), which summarizes the overall magnitude of the gradient as a single
    scalar. You can log this value alongside the loss during training.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 监控梯度的简单方法是通过计算它们的L2范数（也称为欧几里得范数），这会将梯度的整体幅度总结为一个单一的标量。你可以在训练期间记录这个值，并与损失值一起记录。
- en: 'Here’s how to compute the L2 norm of gradients in Flax:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在Flax中计算梯度L2范数的方法如下：
- en: '[PRE2]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can log this `grad_norm` over time and visualize it alongside the loss
    to examine gradient behavior:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在训练过程中记录这个`grad_norm`值，并将其与损失值一起可视化，以检查梯度行为：
- en: If `grad_norm` is close to 0, gradients are likely vanishing.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`grad_norm`接近0，梯度很可能是消失的。
- en: If it grows rapidly or spikes erratically, you may be seeing exploding gradients.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果它迅速增长或异常波动，你可能遇到了梯度爆炸。
- en: 'Common fixes to try out include:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 可以尝试的常见修复方法包括：
- en: Lower the learning rate or use a learning rate schedule.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低学习率或使用学习率调度。
- en: 'Use better weight initializers: try Xavier (Glorot) or He initialization, depending
    on your activation function.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更好的权重初始化器：根据你的激活函数尝试Xavier（Glorot）或He初始化。
- en: 'Normalize activations: Batch normalization or layer normalization helps stabilize
    the flow of gradients.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化激活：批归一化或层归一化有助于稳定梯度流。
- en: 'Add residual connections: These help gradients propagate through deep networks
    without degradation.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加残差连接：这些有助于梯度在深度网络中传播而不会退化。
- en: 'Clip gradients: This is a blunt but effective tool to cap extreme values and
    prevent instability.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 裁剪梯度：这是一个简单但有效的工具，可以限制极端值并防止不稳定。
- en: 'The fixes to this issue tend to be ones we’ve already mentioned, like reducing
    the learning rate or using a learning schedule, using different weight initializers,
    and adding either batch normalization or layer normalization. Adding residual
    connections between blocks can also be helpful. Finally, explicitly clipping gradients
    to a fixed threshold to avoid excessive values might sound like a really crude
    approach but is common and very effective. Here is an example implementation:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法通常是我们之前提到的，比如降低学习率或使用学习率调度，使用不同的权重初始化器，添加批归一化或层归一化。在块之间添加残差连接也可能有所帮助。最后，明确地将梯度裁剪到固定阈值，以避免过大的值，这可能听起来是一个非常粗略的方法，但它是常见且非常有效的。以下是一个示例实现：
- en: '[PRE3]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Or, if you are using `optax`, you can also clip gradients with:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你正在使用`optax`，你还可以使用以下方式裁剪梯度：
- en: '[PRE4]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Training Instability
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练不稳定
- en: A related issue to gradient issues is *training instability*, which can manifest
    in several ways, including erratic training losses, sudden spikes in validation
    loss, or even full-blown divergence. By *divergence*, we mean that the model fails
    to *converge* toward a stable solution; instead, the loss may oscillate wildly
    or become `NaN`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 与梯度问题相关的一个问题是*训练不稳定*，它可能以多种方式表现出来，包括训练损失的异常波动、验证损失的突然上升，甚至完全发散。通过*发散*，我们指的是模型未能*收敛*到一个稳定的解；相反，损失可能会剧烈波动或变为`NaN`。
- en: 'Training instability typically arises from a few common causes:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 训练不稳定通常由几个常见原因引起：
- en: Learning rate is too high
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率过高
- en: A high learning rate can cause the optimizer to overshoot minima, leading to
    instability. Try lowering the learning rate or using a warmup schedule that starts
    small and ramps up gradually.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 高学习率可能导致优化器超过最小值，导致不稳定。尝试降低学习率或使用从小开始逐渐增加的学习率预热计划。
- en: Using a nonadaptive optimizer
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非自适应优化器
- en: Adaptive optimizers like Adam, RMSProp, or Adagrad adjust learning rates per
    parameter and tend to be more robust out-of-the-box. While vanilla stochastic
    gradient descent (SGD) can be effective, it typically requires more careful tuning,
    especially with larger models or noisy data.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应优化器如Adam、RMSProp或Adagrad会根据每个参数调整学习率，并且通常开箱即用就更加稳健。虽然传统的随机梯度下降（SGD）可能有效，但它通常需要更仔细的调整，尤其是在较大的模型或噪声数据的情况下。
- en: Exploding gradients
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度爆炸
- en: In deep networks, gradients can grow too large and destabilize updates. As discussed
    earlier, apply *gradient clipping* or use normalization layers (like batch norm
    or layer norm) to control this.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度网络中，梯度可能变得过大并导致更新不稳定。如前所述，应用*梯度裁剪*或使用归一化层（如批归一化或层归一化）来控制这一点。
- en: Inappropriate batch size
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 不适当的批量大小
- en: Very small batches can lead to noisy gradient estimates that make training unstable.
    Larger batches offer more stable gradients—generally, try using the largest size
    your hardware allows, especially during early debugging.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 非常小的批量可能导致梯度估计噪声，使训练不稳定。较大的批量提供更稳定的梯度——通常，尽量使用硬件允许的最大尺寸，尤其是在早期调试期间。
- en: Poor weight initialization
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 糟糕的权重初始化
- en: Improper initialization can cause gradients to vanish or explode. Flax uses
    LeCun normal as the default initializer for `nn.Dense` and `nn.Conv`, which works
    well with ReLU activations. But for very deep networks or specific architectures,
    Xavier or He initialization may perform better.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 不当的初始化可能导致梯度消失或爆炸。Flax使用LeCun正态分布作为`nn.Dense`和`nn.Conv`的默认初始化器，这对于ReLU激活函数来说效果很好。但对于非常深的网络或特定的架构，Xavier或He初始化可能表现更好。
- en: Activation blowup
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 激活爆炸
- en: As networks deepen, intermediate activations can grow excessively large, especially
    with ReLUs or unnormalized inputs. To prevent this, keep activations centered
    and bounded, most commonly by applying batch normalization.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网络的加深，中间激活值可能会变得过大，尤其是在ReLU或未归一化的输入下。为了防止这种情况，保持激活值居中并限制在合理范围内，最常见的方法是应用批量归一化。
- en: Poor Model Performance
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能不佳
- en: The model trains and the dataset looks good. You’ve squashed overfitting. Everything
    generally looks sane. The only problem is that the model is just not that good.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练并且数据集看起来不错。你已经解决了过拟合问题。一切看起来都很正常。唯一的问题是模型本身并不那么好。
- en: How Well Should You Do?
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你应该做得有多好？
- en: 'We touched on this point in the introduction, but it’s worth restating: to
    judge performance, you need context. Here are some ways to anchor your expectations:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在引言中提到了这一点，但值得重申：要判断性能，你需要有上下文。以下是一些锚定你期望的方法：
- en: Random chance performance
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 随机机会性能
- en: What would random guessing achieve? For regression, how well would you do by
    always predicting the mean or median of the training set?
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 随机猜测会达到什么效果？对于回归，你总是预测训练集的均值或中位数会做得有多好？
- en: Baseline models
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 基准模型
- en: Try a simple linear regression or logistic regression. Sometimes these models
    perform surprisingly well—and if your deep model doesn’t beat them, something’s
    wrong.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试简单的线性回归或逻辑回归。有时这些模型表现出人意料的出色——如果你的深度模型没有打败它们，那么可能有问题。
- en: Other published models
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 其他已发表的模型
- en: If others have worked on this task, check what performance they report. You
    can often get architectural or preprocessing ideas from their work. But beware—published
    metrics aren’t always trustworthy, and they may not be directly comparable to
    your setup.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如果其他人已经在这个任务上工作过，检查他们报告的性能。你可以从他们的工作中获得架构或预处理想法。但要注意——已发表的指标并不总是可信的，它们可能无法直接与你的设置进行比较。
- en: Human performance
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 人类表现
- en: Can a human do this task? How well would an expert do it? This can help you
    calibrate expectations.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 人类能完成这个任务吗？专家会做得有多好？这可以帮助你校准期望。
- en: Experimental replicates
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 实验重复
- en: Biological measurements often contain noise due to sampling variability, measurement
    error, or biological variability itself. One way to estimate the ceiling for your
    model’s performance is to check how consistent the raw signal is across replicate
    experiments. If two replicates have a correlation of 0.85, your model is unlikely
    to exceed that. Don’t expect your model to be more consistent than biology itself.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 生物测量通常由于采样变异性、测量误差或生物本身的变异性而包含噪声。估计你模型性能上限的一种方法是通过检查原始信号在重复实验中的一致性。如果两个重复实验的相关性为0.85，你的模型不太可能超过这个值。不要期望你的模型比生物学本身更一致。
- en: Addressing Poor Model Performance
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决模型性能不佳问题
- en: 'While there’s no universal fix for a model that just isn’t performing well,
    the following strategies can help identify what’s wrong and suggest paths forward:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于表现不佳的模型没有万能的解决方案，但以下策略可以帮助识别问题并提出前进的道路：
- en: Check data quality
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 检查数据质量
- en: Many model issues are actually data issues. Dive into specific examples—especially
    ones the model gets wrong—and look for inconsistencies, noise, or labeling errors.
    If the data is highly domain specific and you’re not an expert, ask someone who
    is.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型问题实际上是数据问题。深入研究具体例子——特别是模型出错的地方——寻找不一致性、噪声或标签错误。如果数据高度专业且你不是专家，请咨询专家。
- en: Run error analysis
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 运行错误分析
- en: Where is the model doing well? Where does it consistently fail? Are there patterns
    to its mistakes—specific classes, edge cases, or confounding conditions? Systematic
    errors often point to missing features or broken assumptions.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在哪里表现良好？在哪里持续失败？是否存在错误模式——特定的类别、边缘情况或混淆条件？系统错误通常指向缺失的特征或错误的假设。
- en: Add more data
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 添加更多数据
- en: More data can help if the model is underfitting or struggling with rare cases.
    You can also try synthetic augmentation, bootstrapping from known examples, or
    generating simulations. Watch how performance scales with dataset size—plateaus
    may indicate other bottlenecks.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型欠拟合或难以处理罕见情况，更多的数据可能会有所帮助。你也可以尝试合成增强、从已知示例中进行引导或生成模拟。观察性能如何随着数据集大小而扩展——平台期可能表明其他瓶颈。
- en: Tune hyperparameters
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 调整超参数
- en: Some hyperparameters matter more than others—start with learning rate, batch
    size, model depth, and regularization strength. Use grid or random search over
    a small range to find better-performing settings.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 一些超参数比其他参数更重要——从学习率、批量大小、模型深度和正则化强度开始学习。在小范围内使用网格搜索或随机搜索来找到更好的性能设置。
- en: Try transfer learning
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试迁移学习
- en: If similar datasets or tasks exist, use pretrained models as a starting point.
    You can either fine-tune the whole model or freeze its feature extractor and train
    a smaller model on top. Alternatively, use learned embeddings from a related model
    as input features.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在类似的数据库或任务，可以使用预训练模型作为起点。你可以微调整个模型，或者冻结其特征提取器，在顶部训练一个更小的模型。或者，使用相关模型学习到的嵌入作为输入特征。
- en: Tip
  id: totrans-291
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'As always, if you’re stuck, revisit the basics: simplify the model, overfit
    a single batch, sanity-check your labels, and compare against baseline performance.
    Many of the strategies that help fix broken models can also clarify why a working
    model isn’t yet a good one.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，如果你遇到困难，请回顾基础知识：简化模型，过度拟合单个批次，检查你的标签是否合理，并与基线性能进行比较。许多帮助修复损坏模型的策略也可以阐明为什么一个正在工作的模型还不是一个好的模型。
- en: Final Thoughts
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的想法
- en: Deep learning in biology is hard. The data is messy, the goals are often open-ended,
    and training useful models can be finicky. But that’s exactly what makes it exciting.
    With every experiment, you’re not just solving a technical challenge—you’re helping
    push the boundaries of how we understand life itself.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 生物学中的深度学习很困难。数据很混乱，目标通常是开放式的，训练有用的模型可能很棘手。但正是这一点使得它令人兴奋。每一次实验，你不仅是在解决技术挑战，你还在帮助推动我们对生命本身理解的边界。
- en: The journey won’t always be smooth—models will fail in surprising ways, you’ll
    write catastrophic bugs, and the data will contain monumental errors. But if you
    stay curious, keep things modular, simplify when in doubt, and stay patient, you’ll
    find your way through.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 旅程不会总是顺利——模型可能会以令人惊讶的方式失败，你可能会编写灾难性的错误，数据可能包含重大的错误。但如果你保持好奇心，保持模块化，在不确定时简化，保持耐心，你将找到自己的路。
- en: Whether you’re building models to decode genomes, predict protein structures,
    or interpret microscopy images, we hope this book has helped you approach the
    work more confidently—and maybe even enjoy the process a little more.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 不论你是构建模型来解码基因组、预测蛋白质结构，还是解释显微镜图像，我们都希望这本书能帮助你更有信心地接近这项工作——也许还能让你在过程中享受更多乐趣。
- en: Good luck, and keep going!
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你好运，继续前进！
