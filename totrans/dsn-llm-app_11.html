<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. Inference Optimization"><div class="chapter" id="ch09">
<h1><span class="label">Chapter 9. </span>Inference Optimization</h1>


<p>In the past few chapters, we learned several techniques<a data-type="indexterm" data-primary="inference optimization" id="xi_inferenceoptimization9456"/><a data-type="indexterm" data-primary="optimization and optimizers" data-secondary="inference optimization" id="xi_optimizationandoptimizersinferenceoptimization9456"/> for adapting and  utilizing LLMs to solve specific tasks. In this chapter, we will learn how to efficiently perform inference on them for real-world usage. LLMs’ large size make deployment and inference particularly challenging, as they exert significant pressure on compute, memory, and energy requirements. This proves to be especially challenging on edge devices like mobile phones.</p>

<p>For the rest of the chapter, we will focus on the field of inference optimization, discussing the factors influencing LLM inference time. We will then showcase a variety of optimization techniques including caching, knowledge distillation, early exiting, quantization, parallel and speculative decoding, and more.</p>






<section data-type="sect1" data-pdf-bookmark="LLM Inference Challenges"><div class="sect1" id="id369">
<h1>LLM Inference Challenges</h1>

<p>What are the bottlenecks affecting LLM inference? As we all know, their gargantuan sizes necessitate vast computing and memory resources. Apart from that, two additional factors exacerbate the situation:</p>

<ul>
<li>
<p>As seen in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>, contemporary LLMs are based largely on decoder-only models that operate autoregressively. This means that each token is generated one after the other, thus imposing a sequential limitation. Later in this chapter, we will discuss techniques for parallel and speculative decoding that aim to speed up the decoding process.</p>
</li>
<li>
<p>As the input sequence length increases, the amount of compute needed increases quadratically. Later this chapter, we will discuss techniques like K-V caching that aim to alleviate this bottleneck.</p>
</li>
</ul>

<p>Let’s dive into the techniques used to optimize inference.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Inference Optimization Techniques"><div class="sect1" id="id370">
<h1>Inference Optimization Techniques</h1>

<p>Since this is a problem that severely impacts the deployment of LLMs in real-world use cases, considerable attention has been given to inference optimization research in major industry and academic labs. Dozens of optimization techniques have been developed in recent years, without which the present ubiquity of LLMs would not have been achieved. For a near-comprehensive survey of the various types of techniques used for optimizing inference, check out <a href="https://oreil.ly/MtzNn">Zhou et al.’s survey paper</a>.</p>

<p>We will now focus on some of the most promising and effective inference optimization techniques used in LLM deployments. While most of you may not be implementing these techniques by yourself but instead rely on third-party tools, understanding the optimization techniques and the tradeoffs involved provide valuable insights that can help you choose among various solutions.</p>

<p>Techniques for efficient inference aim to achieve the following three goals:</p>
<dl>
<dt>Reduce compute</dt>
<dd>
<p>Techniques like caching, knowledge distillation, and early exit, each of them employing distinct strategies to reduce computation.</p>
</dd>
<dt>Speed up decoding</dt>
<dd>
<p>Techniques for parallel and speculative decoding aim to improve the throughput of the model: the number of tokens generated per second.</p>
</dd>
<dt>Reduce storage needs</dt>
<dd>
<p>Quantization techniques aim to reduce the amount of storage needed for weights and activations of the model, by reducing space required to store numbers from 32 bits to 16, 8, or even 4 bits.</p>
</dd>
</dl>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Techniques for Reducing Compute"><div class="sect1" id="id146">
<h1>Techniques for Reducing Compute</h1>

<p>We can reduce compute<a data-type="indexterm" data-primary="inference optimization" data-secondary="reducing compute" id="xi_inferenceoptimizationreducingcompute93422"/><a data-type="indexterm" data-primary="reducing compute" id="xi_reducingcompute93422"/><a data-type="indexterm" data-primary="compute, reducing" id="xi_computereducing93422"/> required during inference by:</p>

<ul>
<li>
<p>Trading compute for extra storage, using methods like caching.</p>
</li>
<li>
<p>Foregoing certain operations during inference, using methods like <em>early exit</em>.</p>
</li>
<li>
<p>Deriving a smaller model from a larger model while preserving as many characteristics and capabilities from the larger model as possible, using techniques like knowledge distillation.</p>
</li>
</ul>

<p>The next sections will explore each of these methods in detail.</p>








<section data-type="sect2" data-pdf-bookmark="K-V Caching"><div class="sect2" id="id147">
<h2>K-V Caching</h2>

<p>As seen in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>, LLMs do not have session memory<a data-type="indexterm" data-primary="reducing compute" data-secondary="K-V caching" id="xi_reducingcomputeKVcaching94673"/><a data-type="indexterm" data-primary="K-V caching, reducing compute" id="xi_KVcachingreducingcompute94673"/><a data-type="indexterm" data-primary="caching to reduce compute" id="xi_cachingtoreducecompute94673"/><a data-type="indexterm" data-primary="compute, reducing" data-secondary="K-V caching" id="xi_computereducingKVcaching94673"/>; at every turn in an LLM conversation, the previous conversation history is added to the input. This means that every request to an LLM could potentially contain a lot of repetitive content in the prompt. For the repetitive parts of the prompt, the same computation is performed during the inference step again and again. Moreover, in autoregressive decoding, each token is generated as a function of the entire input and the previously generated tokens. Thus, there is a lot of duplicative computation.</p>

<p>One way to alleviate this duplicative computation is to cache the data and reuse them when required. More specifically, we cache the keys (K) and values (V) of the self-attention blocks of the Transformer architecture, referred to as the K-V cache. Recall our discussion in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a> about keys and values in the self-attention block of the Transformer.</p>

<p>Let’s look at some examples. Consider the task of analyzing sentiment of movie reviews. You might have a lengthy prompt providing detailed instructions on the nuances involved in analyzing sentiment. These instructions are included in the prompt for every input review being fed to the LLM.</p>

<p>Instead of incurring unnecessary overhead by repetitively processing the instruction tokens, the cache is consulted to fetch the K-V values for these tokens.</p>

<p>Similarly, consider the example of a question-answering assistant that provides customer support by answering questions from a product manual. In this case, the K-V values representing the product manual tokens can be cached and then reused for any requests where the product manual needs to be part of the prompt.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Caching can also enable adding a lot of few-shot examples in the prompt<a data-type="indexterm" data-primary="few-shot learning" data-secondary="and caching to reduce compute" data-secondary-sortas="caching to reduce compute" id="id1271"/><a data-type="indexterm" data-primary="prompting" data-secondary="few-shot" id="id1272"/>. This can sometimes be a lightweight alternative to 
<span class="keep-together">fine-tuning</span>.</p>
</div>

<p>Major LLM providers like Google’s Gemini<a data-type="indexterm" data-primary="Google Gemini" id="id1273"/><a data-type="indexterm" data-primary="Claude (Anthropic)" id="id1274"/><a data-type="indexterm" data-primary="Anthropic" data-secondary="Claude" id="id1275"/><a data-type="indexterm" data-primary="Gemini" id="id1276"/> and Anthropic’s Claude provide caching support for their models through their APIs, calling it context caching<a data-type="indexterm" data-primary="context caching" id="id1277"/>. This also vastly reduces the cost for end users, as cached tokens are billed only once.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Note that in the caching strategy, we are trading compute for additional storage. K-V caches can get unfeasibly large, especially at longer sequence lengths.</p>
</div>

<p>To keep costs under control, LLM providers typically limit the age of the cache to a short period or price users by caching duration.</p>

<p>As an example, let’s look at a request to Anthropic’s Claude suite of models that utilizes context caching:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">{</code>
    <code class="s2">"model"</code><code class="p">:</code> <code class="s2">"claude-3-5-sonnet"</code><code class="p">,</code>
    <code class="s2">"max_tokens"</code><code class="p">:</code> <code class="mi">1024</code><code class="p">,</code>
    <code class="s2">"system"</code><code class="p">:</code> <code class="p">[</code>
      <code class="p">{</code>
        <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"text"</code><code class="p">,</code>
        <code class="s2">"text"</code><code class="p">:</code> <code class="s2">"&lt;System Prompt&gt;"</code>
      <code class="p">},</code>
      <code class="p">{</code>
        <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"text"</code><code class="p">,</code>
        <code class="s2">"text"</code><code class="p">:</code> <code class="s2">"&lt;Product Manual&gt;"</code><code class="p">,</code>
        <code class="s2">"cache_control"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"type"</code><code class="p">:</code> <code class="s2">"ephemeral"</code><code class="p">}</code>
      <code class="p">}</code>
    <code class="p">],</code>
    <code class="s2">"messages"</code><code class="p">:</code> <code class="p">[</code>
      <code class="p">{</code>
        <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code>
        <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"Which battery should I use for the G-8 Ultra?"</code>
      <code class="p">}</code>
    <code class="p">]</code>
  <code class="p">}</code><code class="s1">'</code><code class="w"/></pre>

<p>The <code>cache_control</code> parameter is used to specify that the system prompt and the product manual is to be cached. As of the book’s writing, Claude’s cache is live by default for five minutes.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Organize your prompt to place the cacheable components at the beginning of the prompt, i.e., the prompt prefix.</p>
</div>

<p>Ultimately, caching can be very valuable in reducing inference time, especially in settings where instructions are repeated for a large number of calls, or the context window contains data like API documentation or RAG output that needs to persist across multiple calls.</p>

<p>Next, we’ll explore the early exit method for reducing inference-time compute<a data-type="indexterm" data-startref="xi_reducingcomputeKVcaching94673" id="id1278"/><a data-type="indexterm" data-startref="xi_KVcachingreducingcompute94673" id="id1279"/><a data-type="indexterm" data-startref="xi_cachingtoreducecompute94673" id="id1280"/><a data-type="indexterm" data-startref="xi_computereducingKVcaching94673" id="id1281"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Early Exit"><div class="sect2" id="id148">
<h2>Early Exit</h2>

<p>As shown in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>, the Transformer architecture<a data-type="indexterm" data-primary="reducing compute" data-secondary="early exit" id="xi_reducingcomputeearlyexit911079"/><a data-type="indexterm" data-primary="early exit, reducing compute" id="xi_earlyexitreducingcompute911079"/><a data-type="indexterm" data-primary="compute, reducing" data-secondary="early exit" id="xi_computereducingearlyexit911079"/> is made up of repeating blocks called layers. The output of each layer is an intermediate representation that gets fed as input to the layer above it. One simple way of reducing compute during inference is to exit inference at an intermediate layer and interpret it as the final output. This technique is called early exit. <a data-type="xref" href="#early-exit">Figure 9-1</a> shows early exit in practice.</p>

<figure><div id="early-exit" class="figure">
<img src="assets/dllm_0901.png" alt="early-exit" width="426" height="800"/>
<h6><span class="label">Figure 9-1. </span>Early exit in practice</h6>
</div></figure>

<p>Early exit can happen both at the sequence level and at the token level.</p>










<section data-type="sect3" data-pdf-bookmark="Sequence-level early exit"><div class="sect3" id="id149">
<h3>Sequence-level early exit</h3>

<p>In this scenario<a data-type="indexterm" data-primary="sequence-level early exit, reducing compute" id="id1282"/>, the forward pass in the Transformer is stopped at a particular layer for the entire input sequence, and the intermediate representations from that layer are taken as the final output. The layer at which to exit can be determined in advance or can be dynamically decided based on the input sequence.</p>

<p>To dynamically decide the layer to exit, you can train adapters on top of each layer, as shown in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>. These modules can then be used to predict whether the exit can happen at the current layer. For example<a data-type="indexterm" data-primary="FastBERT" id="id1283"/>, <a href="https://oreil.ly/GCfpt">FastBERT</a> implements modules at each layer that learn to solve a binary classification problem (to exit or not exit).</p>

<p>Not all methods depend on adding trainable modules. For example, the <a href="https://oreil.ly/_JqqH">hash-based early exiting approach (HashEE)</a> by Sun et al. uses an annotated set of sequences<a data-type="indexterm" data-primary="hash-based early exiting approach (HashEE)" id="id1284"/> along with their exit layers as the basis for determining the exit layers for new input sequences. This method is based on the hypothesis that similar sequences should exit at the same layers.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1285">
<h1>Exercise</h1>
<p>For a 7B open source model of your choice, apply early exit at the 75th percentile of layers in the model. Evaluate this technique on one of the datasets provided in the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a>.</p>

<p>What effect does static early exit have on the model’s performance? What latency gains were you able to achieve using early exit?</p>
</div></aside>

<p>The second early exit option is token-level early exit.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Token-level early exit"><div class="sect3" id="id150">
<h3>Token-level early exit</h3>

<p>In this approach, different tokens<a data-type="indexterm" data-primary="token-level early exit, reducing compute" id="id1286"/> from the same sequence can exit at different layers. This is more complex to implement than sequence-level early exit.</p>

<p>Similar to sequence-level early exit techniques, you can implement binary classifiers to decide whether to exit at a particular layer, but this happens at each token at each layer, instead of the entire sequence. For more details on token-level early exit, refer to <a href="https://oreil.ly/hfdCd">Schuster et al.</a>, who introduced the technique Confident Adaptive Language Modeling (CALM)<a data-type="indexterm" data-primary="Confident Adaptive Language Modeling (CALM)" id="id1287"/><a data-type="indexterm" data-primary="CALM (Confident Adaptive Language Modeling)" id="id1288"/>, that implements token-level early exit.</p>

<p>Recall that in the self-attention subblock of the Transformer, the representation of a token is calculated using the representations of all other tokens in the sequence in the same layer. But if we are using token-level early exit, it is possible that some tokens in a sequence might already have exited before that layer. The easiest way to resolve this issue is to copy the representations of the exited token to every layer above it.</p>

<p>While token-level early exit can be more fine-grained and effective than sequence-level early exit, it is slower than sequence-level early exit.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1289">
<h1>Exercise</h1>
<p>Apply Google’s <a href="https://oreil.ly/l5Xko">implementation</a> of the CALM method for token-level early exit over any of the datasets provided in the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a>.</p>

<p>Which tokens exit at early layers and which tokens exit at later layers? Do you observe any linguistic patterns? What can we infer from this about how language models learn?</p>
</div></aside>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In early exit, the reduction in compute comes at the cost of performance. However, this can be minimized by learning to exit at the optimal layer.</p>
</div>

<p>Dynamic early exit<a data-type="indexterm" data-primary="dynamic early exit" id="id1290"/><a data-type="indexterm" data-primary="dynamic inference" id="id1291"/> belongs to a class of techniques called <em>dynamic inference</em>, where the inference compute is determined dynamically, based on the characteristics of the input. One important example is the mixture of experts (MoE)<a data-type="indexterm" data-primary="mixture of experts (MoE) models" id="id1292"/><a data-type="indexterm" data-primary="MoE (mixture of experts) models" id="id1293"/> class of models, introduced in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>. In MoE models, a routing function chooses a small subset of expert modules to run inference on, thus reducing the amount of compute required.</p>

<p>Next, let’s explore how we can reduce inference time by creating a smaller derivative model from a larger model while limiting performance degradation, using a technique called knowledge distillation<a data-type="indexterm" data-startref="xi_reducingcomputeearlyexit911079" id="id1294"/><a data-type="indexterm" data-startref="xi_earlyexitreducingcompute911079" id="id1295"/><a data-type="indexterm" data-startref="xi_computereducingearlyexit911079" id="id1296"/>.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Knowledge Distillation"><div class="sect2" id="id151">
<h2>Knowledge Distillation</h2>

<p>In <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>, we briefly introduced distilled<a data-type="indexterm" data-primary="reducing compute" data-secondary="knowledge distillation" id="xi_reducingcomputeknowledgedistillation916563"/><a data-type="indexterm" data-primary="knowledge distillation, reducing compute" id="xi_knowledgedistillationreducingcompute916563"/><a data-type="indexterm" data-primary="distillation" data-secondary="reducing compute" id="xi_distillationreducingcompute916563"/><a data-type="indexterm" data-primary="compute, reducing" data-secondary="knowledge distillation" id="xi_computereducingknowledgedistillation916563"/> versions of models, like <a href="https://oreil.ly/rgiHZ">DistilBERT</a>.
These are smaller models that approximate the capabilities of the larger models they are distilled from, thus enabling speedier inference.</p>

<p>Over the years, several techniques have been developed for knowledge distillation. For a survey of research advances in this field, refer to <a href="https://oreil.ly/JZQf3">Xu et al.’s</a> survey paper.</p>

<p>The process of knowledge distillation can be divided into two steps: distillation data preparation and training<a data-type="indexterm" data-primary="distillation" data-secondary="data preparation" id="xi_distillationdatapreparation9170112"/>. The base model is referred to as the teacher model and the distilled model is referred to as the student model.</p>

<p><a data-type="xref" href="#knowledge-distillation">Figure 9-2</a> depicts the process of distilling a model.</p>

<figure><div id="knowledge-distillation" class="figure">
<img src="assets/dllm_0902.png" alt="knowledge-distillation" width="600" height="69"/>
<h6><span class="label">Figure 9-2. </span>Knowledge distillation</h6>
</div></figure>

<p>Here’s how the distillation data preparation and training steps work.</p>










<section data-type="sect3" data-pdf-bookmark="Distillation data preparation"><div class="sect3" id="id152">
<h3>Distillation data preparation</h3>

<p>Data for distillation is typically prepared by appropriately querying the teacher model and using the teacher’s outputs as the <em>knowledge</em> to be distilled. Ways to elicit relevant outputs from the teacher include:</p>
<dl>
<dt>Unsupervised generation</dt>
<dd>
<p>In this technique<a data-type="indexterm" data-primary="unsupervised generation, distillation" id="id1297"/>, the teacher is prompted with instructions and/or examples for solving a task. The teacher’s responses comprise the distillation dataset. This technique is commonly used to teach capabilities like CoT or instruction-following to smaller models. To accomplish that, teacher models are asked to respond to queries with the thought process leading up to the answer.</p>
</dd>
<dt>Data augmentation</dt>
<dd>
<p>In this technique<a data-type="indexterm" data-primary="data augmentation, distillation presentation" id="id1298"/>, the teacher is shown a set of seed input-output examples. Based on the seed examples, the teacher generates similar input-output examples, constituting the distillation dataset. Note that both the input and output are generated by the teacher model in this setting. The limitation of this technique is that the teacher is unable to generate sufficiently diverse examples.</p>
</dd>
<dt>Intermediate representations</dt>
<dd>
<p>This class of techniques<a data-type="indexterm" data-primary="intermediate representations, distillation" id="id1299"/><a data-type="indexterm" data-primary="white-box distillation" id="id1300"/> is known as white-box distillation. Here the distillation dataset consists of intermediate representations from a model, which can include activations or output logits. This data can be used to align the student model with the teacher model. The alignment is learned using methods like KL-divergence, discussed in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>.</p>
</dd>
<dt>Teacher feedback</dt>
<dd>
<p>In this class of techniques<a data-type="indexterm" data-primary="teacher feedback, distillation" id="id1301"/>, the outputs from a student model are assessed by the teacher model to generate feedback. The teacher model can be used to generate preference data, i.e., the quality ranking of outputs from the student. Feedback can also be given in the form of detailed instructions on how to improve on a given task. A popular technique using teacher feedback is RLAIF, which we introduced in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>.</p>
</dd>
<dt>Self-teaching</dt>
<dd>
<p>In this class of techniques<a data-type="indexterm" data-primary="self-teaching, distillation" id="id1302"/>, the teacher and student model are one and the same. The student model progressively refines its own outputs and uses them as the distillation set. One way of self-teaching is to generate multiple outputs for each task, along with reasoning steps, and choosing the best one to be part of the distillation set.</p>
</dd>
</dl>

<p>How many distillation examples do you need? Perhaps surprisingly, not a whole lot. <a href="https://oreil.ly/MuOOj">Zhou et al.</a> show that even one thousand very high-quality examples are enough to create a strong distillation set.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Just like fine-tuning and continued pre-training, knowledge distillation is susceptible to the catastrophic forgetting problem<a data-type="indexterm" data-primary="catastrophic forgetting" id="id1303"/> (introduced in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>).</p>
</div>

<p>Now that we have seen the various ways to create distillation datasets, let’s turn to the actual distillation process<a data-type="indexterm" data-startref="xi_distillationdatapreparation9170112" id="id1304"/>.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Distillation"><div class="sect3" id="id153">
<h3>Distillation</h3>

<p>Here are some techniques used to perform the distillation task<a data-type="indexterm" data-primary="distillation" id="xi_distillation920563"/>. For a more detailed survey of techniques, refer to <a href="https://oreil.ly/9mbiN">Xu et al.</a>:</p>
<dl>
<dt>Supervised fine-tuning</dt>
<dd>
<p>This is the simplest way to accomplish knowledge distillation<a data-type="indexterm" data-primary="supervised fine-tuning (SFT)" data-secondary="distillation process" id="id1305"/><a data-type="indexterm" data-primary="fine-tuning models" data-secondary="supervised fine-tuning" id="id1306"/>. The student model is fine-tuned using the distillation set with the objective of aligning its predictions with that of the teacher model. This method is typically used in black-box knowledge distillation settings, where the distillation set does not comprise any internal representations.</p>
</dd>
<dt>K-L divergence of output probabilities</dt>
<dd>
<p>In this method<a data-type="indexterm" data-primary="Kullback-Liebler (K-L) divergence" id="id1307"/>, our objective function is to minimize the K-L divergence between the output probability distribution of the teacher model and the student model.</p>
</dd>
<dt>Internal representation similarity</dt>
<dd>
<p>Conversely, instead of minimizing divergence<a data-type="indexterm" data-primary="internal representation similarity, distillation" id="id1308"/>, you can maximize similarity between aspects of the teacher and student model. This can be leveraged to perform layerwise distillation, where the internal representations of the teacher and the student are aligned at each layer. Refer to <a href="https://oreil.ly/g-C4L">Liang et al.</a> for an effective technique for layerwise distillation.</p>
</dd>
<dt>Reinforcement learning</dt>
<dd>
<p>This involves training a reward model<a data-type="indexterm" data-primary="reinforcement learning (RL)" id="id1309"/><a data-type="indexterm" data-primary="reward model, distillation process" id="id1310"/><a data-type="indexterm" data-primary="RL (reinforcement learning)" id="id1311"/> using the distillation data. The student model is then trained to maximize the reward as per the reward model. Recall our discussion on reinforcement learning in <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>.</p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1312">
<h1>Weak-to-Strong Generalization</h1>
<p>Burns et al. from OpenAI uncovered a phenomenon called  <a href="https://oreil.ly/xhrJL"><em>weak-to-strong generalization</em></a>. In this setting<a data-type="indexterm" data-primary="weak-to-strong generalization" id="id1313"/>, the teacher model is smaller/weaker than the student model.</p>

<p>The small teacher model is fine-tuned using distillation data. The held-out portion of the distillation data is queried to the teacher model to generate outputs. These are called <em>weak labels.</em> The weak labels are then used to fine-tune a much larger and more powerful student model.</p>

<p>Burns et al. note that the stronger student is able to learn from the labels generated by the weaker teacher. This is because the student is able to rely on its strong pre-trained representations, and fine-tuning on the weak labels only help it elicit what it already knows. Using smaller models for generating training data simplifies the overall training process.</p>
</div></aside>

<p>Ultimately, the technique you choose to distill your models depends on whether you have access to the teacher weights. If you do not have access to the teacher weights, then you can perform only supervised fine-tuning. White-box distillation, where you are trying to align intermediate representations and not just the output tokens, can be challenging to achieve. Note that all knowledge distillation techniques carry the risk of capability degradation or catastrophic forgetting, so you will have to evaluate the student model very carefully to quantify the difference in capabilities from the teacher model<a data-type="indexterm" data-startref="xi_distillation920563" id="id1314"/>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1315">
<h1>Exercise</h1>
<p>Take the Gemma 2B open source model and distill it into a smaller model that can still perform CoT generation. Which of the techniques presented in this chapter is more suitable for this exercise?</p>
</div></aside>

<p>In this section, we discussed three distinct techniques for reducing compute during inference: caching, early exit, and knowledge distillation. Next, let’s discuss techniques that can accelerate the decoding process<a data-type="indexterm" data-startref="xi_inferenceoptimizationreducingcompute93422" id="id1316"/><a data-type="indexterm" data-startref="xi_reducingcompute93422" id="id1317"/><a data-type="indexterm" data-startref="xi_computereducing93422" id="id1318"/>.</p>
</div></section>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Techniques for Accelerating Decoding"><div class="sect1" id="id154">
<h1>Techniques for Accelerating Decoding</h1>

<p>As we know, autoregressive models<a data-type="indexterm" data-primary="inference optimization" data-secondary="accelerating decoding" id="xi_inferenceoptimizationacceleratingdecoding923734"/><a data-type="indexterm" data-primary="accelerating decoding" id="xi_acceleratingdecoding923734"/><a data-type="indexterm" data-primary="decoding strategies" data-secondary="accelerating decoding" id="xi_decodingstrategiesacceleratingdecoding923734"/> output one token at a time, where the next token being generated is a function of the input tokens and all the previously generated tokens. This imposes a sequential limitation as you have to wait for the current token to be generated before generating the next one. Can we bypass this limitation? Several recent techniques like <em>speculative decoding</em> and <em>parallel decoding</em> have been developed. Let’s examine them in detail.</p>








<section data-type="sect2" data-pdf-bookmark="Speculative Decoding"><div class="sect2" id="id155">
<h2>Speculative Decoding</h2>

<p>The concept behind speculative decoding<a data-type="indexterm" data-primary="speculative decoding" id="id1319"/> is simple. A smaller model, called a draft model, is used to generate several subsequent candidate output tokens. Then, the main larger model is used to compute the conditional probabilities of the candidate output tokens at once, using them to decide which tokens to accept and which ones to reject. The more draft tokens accepted, the better the draft model.</p>

<p><a data-type="xref" href="#speculative-decoding">Figure 9-3</a> depicts the speculative decoding process.</p>

<figure><div id="speculative-decoding" class="figure">
<img src="assets/dllm_0903.png" alt="speculative-decoding" width="600" height="163"/>
<h6><span class="label">Figure 9-3. </span>Speculative decoding in action</h6>
</div></figure>

<p>Two important metrics in speculative decoding are:</p>
<dl>
<dt>Token acceptance rate</dt>
<dd>
<p>This is the percentage of tokens<a data-type="indexterm" data-primary="tokens and tokenization" data-secondary="acceptance rate for tokens" id="id1320"/> generated by the draft model that are accepted. Typically, this does not reach 1, because if it did, there is no need to use the main larger model.</p>
</dd>
<dt>Decoding speedup</dt>
<dd>
<p>This refers to the reduction<a data-type="indexterm" data-primary="decoding speedup" id="id1321"/> in latency between a model purely using autoregressive decoding versus one using speculative decoding.</p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1322">
<h1>Constructing Draft Models</h1>
<p>How do we ensure that the draft model<a data-type="indexterm" data-primary="draft models, constructing" id="id1323"/> has a high token acceptance rate? One way is to distill the draft model from the main model. This technique was introduced by<a data-type="indexterm" data-primary="DistillSpec" id="id1324"/> <a href="https://oreil.ly/scxWU">Zhou et al. and is called DistillSpec</a>.</p>

<p><a href="https://oreil.ly/KVZRB">Zhou et al.</a> introduced self-speculative decoding, where the draft model is a subset of the layers of the main model.</p>

<p>In many use cases, output text generated by the LLM includes commonly used phrases, prefixes, and boilerplate. The LLM could also be quoting existing bodies of text. All this can be directly fetched from external data repositories using a retrieval model instead of using a language model for generation. This technique, called retrieval-based speculative decoding (REST)<a data-type="indexterm" data-primary="retrieval-based speculative decoding (REST)" id="id1325"/><a data-type="indexterm" data-primary="REST (retrieval-based speculative decoding)" id="id1326"/> was introduced by <a href="https://oreil.ly/DFufh">He at al.</a></p>
</div></aside>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Parallel Decoding"><div class="sect2" id="id156">
<h2>Parallel Decoding</h2>

<p>Can we generate more than one token at the same time<a data-type="indexterm" data-primary="parallel decoding" id="id1327"/>? This can be done either using the same model (multi-token decoding) or multiple instances of the same model.</p>

<p>For the latter, we can control parallel generation through the prompt. For example, say you are writing an article about a tourist site, containing sections like Food, Stay, Safety Tips, etc. You can prompt the LLM to list the sections, marked with special tokens. These sections can then be generated in parallel, assuming the sections are fully independent of each other.</p>

<p><a data-type="xref" href="#parallel-generation">Figure 9-4</a> depicts the workflow of a system that generates parts of the output in a parallel fashion.</p>

<p>Let’s now explore how the same model can generate multiple tokens at a time, called multi-token decoding<a data-type="indexterm" data-primary="multi-token decoding" id="id1328"/>. Several techniques have been proposed recently for multi-token decoding, one of the most promising being Medusa<a data-type="indexterm" data-primary="Medusa" id="id1329"/> by <a href="https://oreil.ly/qT94i">Cai et al.</a></p>

<p>In Medusa, additional decoding heads are added to the model. These decoding heads represent subsequent tokens to be generated. For example, the standard decoding head is predicting the next (n + 1st) token in the sequence, and the additional decoding heads are predicting the n + 2nd, n + 3rd, and so on, tokens, respectively. Refer to the Medusa paper for more details on how this is implemented.</p>

<p>So far, we have learned techniques to accelerate the decoding process and to reduce compute<a data-type="indexterm" data-startref="xi_inferenceoptimizationacceleratingdecoding923734" id="id1330"/><a data-type="indexterm" data-startref="xi_acceleratingdecoding923734" id="id1331"/><a data-type="indexterm" data-startref="xi_decodingstrategiesacceleratingdecoding923734" id="id1332"/>. Next, let’s dive into quantization, a class of techniques to reduce the storage required by the model.</p>

<figure><div id="parallel-generation" class="figure">
<img src="assets/dllm_0904.png" alt="Parallel Generation" width="600" height="709"/>
<h6><span class="label">Figure 9-4. </span>Parallel decoding workflow</h6>
</div></figure>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Techniques for Reducing Storage Needs"><div class="sect1" id="id157">
<h1>Techniques for Reducing Storage Needs</h1>

<p>In Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.html#chapter_utilizing_llms">5</a> and
<a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.html#llm-fine-tuning">6</a>, we briefly touched upon quantization<a data-type="indexterm" data-primary="quantization" id="xi_quantization928492"/><a data-type="indexterm" data-primary="inference optimization" data-secondary="reducing storage needs" id="xi_inferenceoptimizationreducingstorageneeds928492"/><a data-type="indexterm" data-primary="storage needs, reducing" id="xi_storageneedsreducing928492"/> but promised to go into detail later. Let’s dive in!</p>

<p>The forward pass of a language model involves numbers representing inputs, weights, and activations. How are these numbers represented in memory?</p>

<p>Several types of numerical representation formats are available, like integer, floating point, etc. Typically, numbers in language models are represented in floating-point32 (FP32), also called single-precision floating point, which refers to a floating point number composed of 32 bits, or 4 bytes.</p>

<p>A number represented in FP32 is composed of three parts:</p>

<ul>
<li>
<p>A sign bit</p>
</li>
<li>
<p>The exponent (8 bits)</p>
</li>
<li>
<p>The mantissa/significand (23 bits)</p>
</li>
</ul>

<p>For more details on how FP32 works, see <a href="https://oreil.ly/uCYYl">“Demystifying Floating Point Precision”</a>.</p>

<p>The maximum and minimum value that can be represented using FP32 is 3.4028237 × 10<sup>38</sup> and 1.175494 × 10<sup>38</sup>, respectively. This is referred to as the range of values that can be represented by this format. Similarly, a number represented in float16 (FP16), also referred to as half-precision floating point, is composed of these three parts:</p>

<ul>
<li>
<p>A sign bit</p>
</li>
<li>
<p>The exponent (5 bits)</p>
</li>
<li>
<p>The mantissa/significand (10 bits)</p>
</li>
</ul>

<p>What happens when you take a number that is represented using FP32 and represent it in FP16? This amounts to a lossy conversion. In this case, both the range and the precision are impacted, because in FP16, 65,504 is the largest number you can represent, compared to 3.4 × 10<sup>38</sup> for FP32. The precision is impacted too, as the 32-bit version offers ~7 digits of precision, but the 16-bit version only offers ~3 digits of precision.</p>

<p>To prevent the massive loss in precision with FP16, bfloat16 (BF16)<a data-type="indexterm" data-primary="bfloat16 (BF16)" id="id1333"/><a data-type="indexterm" data-primary="brain floating point" id="id1334"/><a data-type="indexterm" data-primary="Google Brain" id="id1335"/>, also called the brain floating point, was invented by Google Brain.
In BF16, there are 8 digits for the exponent, and 7 bits for the mantissa.
This keeps the range of numbers represented the same as that of float32 at the cost of reduced precision.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Older GPUs like the NVIDIA T4 do not support BF16.</p>
</div>

<p>The process of converting representation of a number from a higher-precision format to a lower-precision format is called quantization. We can quantize 32-bit values to 8-bit integer formats as well. This reduces memory requirements by a factor of 4, at the cost of even more precision. With 8-bit quantization, we can represent numbers between –127 and 127, without any decimal point.</p>

<p>Integer quantization can be performed either symmetrically or asymmetrically.</p>








<section data-type="sect2" data-pdf-bookmark="Symmetric Quantization"><div class="sect2" id="id158">
<h2>Symmetric Quantization</h2>

<p>In this setting, the <em>0</em> value in the original format<a data-type="indexterm" data-primary="symmetric quantization" id="id1336"/> is mapped to the
<em>0</em> value in the integer representation. This means that when you quantize 0 represented in fp32 to int8, the value remains 0.</p>

<p>The remaining values can be mapped using various techniques, the most common being absmax quantization. In this method, if we know or can estimate the range of numbers that need to be represented, we can take the absolute maximum of the range and map it to the largest number in int8 (127), while the negative of the absolute maximum is mapped to the smallest number in int8 (–127). The remaining numbers are mapped according to scale.</p>

<p><a data-type="xref" href="#symmetric-quantization">Figure 9-5</a> depicts absmax quantization at work, quantizing a number represented in FP32 to int8.</p>

<figure><div id="symmetric-quantization" class="figure">
<img src="assets/dllm_0905.png" alt="Absmax quantization" width="600" height="233"/>
<h6><span class="label">Figure 9-5. </span>Absmax quantization</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Asymmetric Quantization"><div class="sect2" id="id159">
<h2>Asymmetric Quantization</h2>

<p>In this setting, the <em>0</em> value in the original format<a data-type="indexterm" data-primary="asymmetric quantization" id="id1337"/> is not guaranteed to be mapped to the <em>0</em> value in the integer representation.</p>

<p>A common technique is to take the minimum and maximum value that we need represented and map it to the minimum (–127) and maximum (127) values that can be represented in int8, respectively. For example, if the range of numbers we want represented is –23 to 87, then –23 is mapped to –127 and 87 is mapped to 127.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If the range of numbers you want represented include outliers, they can play spoilsport. You can take care of outliers by clipping them, so that all outliers will be represented by the same maximum/minimum value.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1338">
<h1>Exercise</h1>
<p>Let’s explore the effect that quantization has on precision and range. Take a few numbers (2.3888888, 0, 34.444, 12.3486*10^4, –1223.4566) and perform arithmetic operations using them in float32. Repeat the same operations using float16, bf16, and int8. How much precision do you lose at each quantization level?</p>
</div></aside>

<p>How is quantization used in practice? Typically, quantization is applied after training. Both the model’s weights and activations<a data-type="indexterm" data-primary="weights" id="id1339"/> can be quantized.</p>

<p>Quantizing weights is much easier than quantizing activations. Since we know the weights beforehand, we can calculate the range, outliers, scaling factors, etc. that are needed for the quantization algorithm.</p>

<p>For activations, depending on how much latency we can tolerate, we can either perform dynamic or static scaling. In dynamic scaling, statistics like range, outliers, etc. are calculated dynamically during inference at each layer. In static scaling, we take a reference calibration dataset to estimate the statistics. While this approach speeds up inference, it can result in more quantization errors<a data-type="indexterm" data-startref="xi_quantization928492" id="id1340"/><a data-type="indexterm" data-startref="xi_inferenceoptimizationreducingstorageneeds928492" id="id1341"/><a data-type="indexterm" data-startref="xi_storageneedsreducing928492" id="id1342"/>.</p>

<p>For more details on implementing quantization, see <a href="https://oreil.ly/bpi3b">“A Visual Guide to Quantization”</a> by Maarten Grootendorst.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1343">
<h1>Exercise</h1>
<p>On any of the datasets provided with the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a>, run Llama 3.1 in float32, float16, bf16, and int8 mode.</p>

<p>Calculate the following:</p>

<ul>
<li>
<p>Impact on model inference time</p>
</li>
<li>
<p>Impact on performance</p>
</li>
<li>
<p>Impact on storage requirements</p>
</li>
</ul>

<p>For the dataset you chose, is quantization worth it?</p>
</div></aside>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id160">
<h1>Summary</h1>

<p>In this chapter, we discussed the causes of bottlenecks in LLM inference<a data-type="indexterm" data-startref="xi_inferenceoptimization9456" id="id1344"/><a data-type="indexterm" data-startref="xi_optimizationandoptimizersinferenceoptimization9456" id="id1345"/>. We discussed a wide variety of techniques to make LLM inference more efficient, including techniques to reduce compute requirements, reduce storage requirements, and accelerate the decoding process. We explored techniques like caching, early exit, knowledge distillation, speculative and parallel decoding techniques, and quantization. In the next and final part of the book, we will explore LLM application paradigms and discuss the nuances involved in building full-fledged applications.</p>
</div></section>
</div></section></div>
</div>
</body></html>