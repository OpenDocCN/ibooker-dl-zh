- en: 11 Contextualizing prompts with retrieval-augmented generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: How RAG works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using tooling to create a basic RAG setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating vector databases into a RAG setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we learned in the previous chapter, one of the challenges of working with
    large language models (LLMs) is that they lack visibility of our context. In the
    second part of this book, we saw different ways in which we can arrange our prompts
    to help provide small insights into our context. However, these types of prompts
    are only useful before the lack of extra context leads to less valuable responses.
    Therefore, to increase the value of an LLM’s response, we need to place more contextual
    detail into our prompt. In this chapter, we’ll explore how to do this through
    retrieval-augmented generation, or RAG. We’ll learn how RAG works, why it’s beneficial,
    and how it’s not a big jump from prompt engineering to building our own RAG framework
    examples to establish our understanding of how they can help us in a testing context.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Extending prompts with RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To recap, RAG is an approach to improving the quality of an LLM’s response by
    combining existing corpus of data with a prompt. Although this broadly explains
    how RAG works, we need to dig a little deeper to better grasp how this combination
    of data is achieved. The process of a RAG system is relatively straightforward
    and can be summarized as shown in figure 11.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH11_F01_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 A visualization of how a basic RAG system works
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with a user input, which would be some sort of query. For example,
    we might send to our RAG system a query such as “I want to test ideas for deleting
    bookings.” This query is then sent to a library or tool that will examine our
    corpus of data for items within the data relevant to our query. In our example,
    this might be a collection of user stories that define each feature in the system.
    The library or tool would determine which user stories are most relevant and then
    return them to be added to a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a bot that makes recommendations for testing ideas. You answer with
    suggested risks to test for, based on the provided user story.This is the user
    story: `{relevant_document}`The user input is: `{user_input}`Compile a list of
    suggested risks to test for, based on the user story and the user input. |'
  prefs: []
  type: TYPE_TB
- en: The LLM will then consume both the user query and the relevant documents to
    return a response that is more accurate than if we sent the query “I want test
    ideas for deleting bookings” directly to an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: By providing data at the `{relevant_document}` point within a prompt relevant
    to the initial query at the `{user_input}` point, we get a response that has increased
    accuracy and value. But it does raise a question of why it would go about finding
    relevant data in the first place. Can’t we just send the data we have in each
    prompt and remove the need to do a relevancy check? Targeting what documents we
    add to a prompt is important for a few reasons. First, consider the size of the
    prompt we can create. The size of a prompt we can send to an LLM depends on its
    maximum sequence length or context window. A context window defines how many words,
    or tokens, can be processed by an LLM. If we add more tokens than the context
    window allows, then the LLM will either cut off the excess tokens at the end of
    the prompt (resulting in a partially completed prompt) or return an error. To
    put this in real terms, Llama-2, Meta’s open-source LLM, has a default context
    window of 4096 tokens, which is about the average equivalent of 10 pages in a
    book. This might feel like a lot initially, but it’s not unusual for our testing
    and development artifacts (for example, user stories, test scripts, code) to be
    much larger.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens and enterprise AI costs
  prefs: []
  type: TYPE_NORMAL
- en: How many tokens we send in a prompt is an important consideration if we’re working
    with models that charge based on the number of tokens that are sent. For example,
    at the time of writing, the gpt-4 turbo model, which has a context window of 128k,
    charges $10 per 1 million tokens. So, if we were attempting to max out the context
    window for each prompt, we would be paying approximately $1.28 per prompt, which
    would drain our budget fast. Therefore, efficient RAG prompting can be as much
    about keeping the bills down as it is about getting the most accurate response
    back from an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: New LLMs appear with much larger context windows that could potentially solve
    the problem of a prompt size. However, this leads us to our next reason for using
    relevancy searches—accuracy. If we were to use a larger context window, such as
    the 128k context window of gpt-4, we might be tempted to add more contextual data.
    But it runs the risk of diluting the quality of the response from an LLM. The
    more data we provide, the more potential noise we add to the prompt for the LLM
    to parse, which may lead to more generalized or unwanted responses. It can also
    make debugging of prompts and responses harder. As we’ve explored multiple times
    in previous chapters, we want to create the right type of prompts that maximize
    the chances of a desirable response. Therefore, targeting specific information
    to provide in a prompt can increase that chance, which means striking a balance
    between not too much context as to water down a response and not too little context
    as to miss out on important details.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, by storing the corpus of data separately from our prompt generation
    and LLM, we have better control over said data, which allows us to update the
    stored data as required. Although vector databases (something we’ll explore in
    detail later in the chapter) have become a tool that is synonymous with RAG platforms,
    we can use any source of data we like. As long as we can find the relevant data
    to be added to our prompt, RAG offers a lot of freedom in accessing data for additional
    context.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Building a RAG setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we learned how RAG frameworks work, to better appreciate the parts,
    let’s see how to build a basic RAG setup. We’ll do this by creating a framework
    that will execute the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Ingest a collection of text documents containing user stories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query the collection of user stories and find the most relevant document based
    on a user query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the relevant document and user query to a prompt and send it to gpt-3.5-turbo
    via the OpenAI platform.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the response and output the details of what the LLM returned.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Activity 11.1
  prefs: []
  type: TYPE_NORMAL
- en: In this part of the chapter, we’ll go through the steps required to build a
    basic example of a RAG system. If you would like to follow along with the example
    and build your own, download the initial code required for this framework from
    [https://mng.bz/gAlR](https://mng.bz/gAlR). All the necessary supporting code
    can be found in the repository, as well as a completed version of the RAG framework
    stored in `CompletedRAGDemo` for reference.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.1 Building our RAG framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll begin with a partially completed project that can also be found in the
    example framework code on GitHub. The project contains the following info to help
    us get started:'
  prefs: []
  type: TYPE_NORMAL
- en: A corpus of data that can be found in `resources/data`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The necessary dependencies required to build and run our RAG framework in `pom.xml`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ActivityRAGDemo`, which contains an empty `main` method where we’ll add our
    framework'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we begin building our RAG framework, let’s review the dependencies stored
    in our `pom.xml` file we’ll be using. These libraries will help us parse our documents
    and send our prompt to the OpenAI platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Adds all the user story text files into a string collection
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Provides the functionality to do a similarity check on string collection
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Sends our prompt to the OpenAI platform
  prefs: []
  type: TYPE_NORMAL
- en: 'With our dependencies in place, we now need to import the collection of user
    stories stored in each text file. Each user story focuses on a specific API endpoint
    for the sandbox application restful-booker-platform ([https://mng.bz/5Oy1](https://mng.bz/5Oy1)).
    Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: As a guest in order to cancel my booking I want to be able to send a DELETE
    request with the booking ID.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Acceptance Criteria:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The endpoint should accept a booking ID as a parameter in the path.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If a valid booking ID is provided, the server should cancel the booking and
    respond with a status of “OK” (200).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the booking ID is invalid or missing, the server should respond with a “Bad
    Request” error (400).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, a token can be provided in the cookie for authentication.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These user stories have been synthetically generated for this project’s purpose,
    but we could imagine that this data could have been extracted from a project management
    platform, a test management tool, or any type of structured data that we feel
    is relevant, from monitoring metrics to wiki entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'To pull in our user stories, we first need to add the following method to our
    `ActivityRAGDemo` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Takes the folder location as a parameter
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Locates the folder within resources
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Iterates through each file in the folder and its contents to a list
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Returns the list of file contents for further use
  prefs: []
  type: TYPE_NORMAL
- en: 'The method `loadFilesFromResources` gives us the ability to load all the user
    story files into a list of strings that we can later query. To test whether this
    has worked, we create a `main` method that we can execute to run our RAG setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Loads files from the data folder inside of resources
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Prints out the first file in the collection
  prefs: []
  type: TYPE_NORMAL
- en: 'After running this code within our IDE, we’ll see the following result output
    to confirm that our user stories are indeed being added to a list for future querying:'
  prefs: []
  type: TYPE_NORMAL
- en: As a guest in order to update branding information I want to be able to send
    a PUT request to /branding/ with necessary parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I should be able to send a PUT request to /branding/ with the necessary parameters
    including the branding information in the request body and an optional token in
    the cookie.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the request is successful, the response status should be 200 OK.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the request is unsuccessful due to bad parameters or missing data, the response
    status should be 400 Bad Request.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The request body should contain valid JSON data conforming to the schema defined
    in the Swagger JSON.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we want to consider the prompt that we’ll be sending to gpt3.5-turbo.
    We’ll utilize some of the tactics that should now feel comfortable to us in the
    following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are an expert software tester that makes recommendations for testing
    ideas and risks. You answer with suggested risks to test for, based on the provided
    user story delimited by three hashes and user input that is delimited by three
    backticks.Compile a list of suggested risks to test for, based on the user story
    and the user input.###{relevant_document}###[PRE3] |'
  prefs: []
  type: TYPE_TB
- en: 'Notice how we’ve parameterized the relevant document and user input sections.
    Eventually, our code will be replacing these two sections with the relevant documentation
    and our initial query for processing. We’ll come to that shortly, but first, we
    need to add the prompt to our code base:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]                                                                 ❷'
  prefs: []
  type: TYPE_NORMAL
- en: '{user_input}                                                        ❷'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: ❶ Loads files from the resources folder
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Defines the prompt to send to OpenAI
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next step is to work out which of our user stories is most relevant to
    the query that we will eventually be inputting. For this, we’ll be using the Apache
    `commons-text` library, which offers a range of different relevancy tools such
    as Levenshtein distance, Jaccard similarity, and the one we’ll be using—Cosine
    distance. How these different similarity tools work is beyond the scope of this
    book, but it’s worth noting that this area of RAG can influence what data is returned.
    Various similarity algorithms work in different ways and can become quite complex
    in production-ready RAG systems. Still, it’s worthwhile experimenting with basic
    approaches to gain a sense of how this part of a RAG system works, so we’ll create
    our similarity-matching method and add it into our class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Takes the list of user stories and the user query as params
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Uses cosineDistance to generate a similarity score
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Checks whether the current score is lower than the currently most relevant
    score
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Returns the entry in the list that is the closest match
  prefs: []
  type: TYPE_NORMAL
- en: The method loops through each document and uses `cosineDistance` to work out
    a similarity score. The lower the score, the more similar the document is to the
    query. The lowest-scoring document is eventually the one that is returned to us
    for use in our prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Working with different types of relevancy algorithms
  prefs: []
  type: TYPE_NORMAL
- en: '`cosineDistance` is just one of many different tools we can use to determine
    relevancy, and each has its own pros and cons. We’ll look at further tools later
    in this chapter to improve the relevancy search, but for now, `cosineDistance`
    will help us build a working prototype that we can iterate on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can create the necessary code to complete our prompt generation. To
    do this, we expand our `main` method to first allow a user to enter their query
    and then do a similarity check before adding it all into a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]                                                               ❷'
  prefs: []
  type: TYPE_NORMAL
- en: '{user_input}                                                      ❷'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: ❶ Waits for a user to input their query via the command line
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Loads files from the resources folder
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Finds the closest match to the user input in the loaded files
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Replaces the placeholder parameters in the prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now run this method, and when asked to add in a query, we can test out
    the generation of our prompt by submitting a query such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| I want test ideas for the GET room endpoint |'
  prefs: []
  type: TYPE_TB
- en: 'Sending this results in the following prompt being built:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are an expert software tester that makes recommendations for testing
    ideas and risks. You answer with suggested risks to test for, based on the provided
    user story delimited by three hashes and user input that is delimited by three
    backticks.Compile a list of suggested risks to test for, based on the user story
    and the user input.###As a guestIn order to browse available roomsI want to be
    able to retrieve a list of all available roomsAcceptance Criteria:   *   I should
    receive a response containing a list of available rooms   *   If there are no
    available rooms, I should receive an empty list   *   If there’s an error retrieving
    the room list, I should receive a 400 Bad Request errorHTTP Payload Contract'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10] |'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the user query has been added to the bottom of the prompt, and
    the user story that has been injected is the one that our `findClosestMatch` method
    has deemed the most relevant. It’s at this point that we’ll start to see limitations
    with our implementation. Trying out different queries will likely result in the
    selection of a less relevant user story. For example, using this query:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| I want a list of risks to test for the delete booking endpoint |'
  prefs: []
  type: TYPE_TB
- en: 'results in the following user story being selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| As a guestIn order to retrieve information about a bookingI want to be able
    to send a GET request with the booking ID |'
  prefs: []
  type: TYPE_TB
- en: This is because the `cosineDistance` method is limited in how well it can determine
    relevancy. We’ll explore how this can be handled later in the chapter, but it
    does highlight a limitation or risk of working with RAG frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, let’s complete our RAG framework so that it can send the prompt
    to OpenAI’s GPT model to get a response. For this, we’ll be using LangChain again
    to send our prompt to OpenAI and output a response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]                                                                  ❸'
  prefs: []
  type: TYPE_NORMAL
- en: '{user_input}                                                         ❸'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: ❶ Receives the user’s query for RAG
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Loads files from the resources folder
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Defines the prompt to be sent to OpenAI
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Finds the closest match to the user query in the loaded files
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Replaces placeholders in the prompt with the user query and file
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Instantiates a new GPT client using an Open AI key
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Sends the prompt to gpt3.5-turbo and prints the response
  prefs: []
  type: TYPE_NORMAL
- en: Providing an OPEN_AI_KEY
  prefs: []
  type: TYPE_NORMAL
- en: To send a request to OpenAI, a Project API key must be provided, which can be
    generated at [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys).
    You will need to create either a new account with the OpenAI platform or, depending
    on whether your credits have expired, add credit to your account, which can be
    done via [https://mng.bz/6YMD](https://mng.bz/6YMD). Once it’s set up, you will
    need to either add your Project API key directly in the code, replacing `System.getenv("OPEN_AI_KEY"),`
    or store your key as an environmental variable under the title `OPEN_AI_KEY`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our GPT implementation in place, we should now have a class to run that
    looks similar to this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '{user_input}'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'To recap, we’ve created the necessary code to:'
  prefs: []
  type: TYPE_NORMAL
- en: Load in a corpus of documents (in this instance, user stories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take in a user query via the command line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the query to find the most relevant document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add both the query and the document to our prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Send the prompt to OpenAI and return a response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With everything in place, we’re ready to start up our RAG framework and test
    it out.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.2 Testing our RAG framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once our RAG framework is up and running, we should be asked what query we
    would like to input. To test out our RAG framework, we can try inputting the following
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| What would you like help with? |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| I want test ideas for the payload portion of the PUT branding endpoint |'
  prefs: []
  type: TYPE_TB
- en: 'Our query retrieves the following user story from our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| As a guestIn order to update branding informationI want to be able to send
    a PUT request to /branding/ with necessary parametersAcceptance criteria   *  
    I should be able to send a PUT request to /branding/ with the necessary parameters
    including the branding information in the request body and an optional token in
    the cookie.   *   If the request is successful, the response status should be
    200 OK.   *   If the request is unsuccessful due to bad parameters or missing
    data, the response status should be 400 Bad Request.   *   The request body should
    contain valid JSON data conforming to the schema defined in the Swagger JSON.HTTP
    Payload contract'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting response from gpt3.5-turbo will look something like the response
    I received:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Based on the user input, here are some suggested risks to test for:   1. 
    Missing required parameters in the request body, such as name, map (latitude and
    longitude), logoUrl, description, contact information (name, address, phone, email)
    - Risk based on the user story mentioning necessary parameters in the request
    body.   2.  Sending invalid data types for parameters in the request body, such
    as sending a number instead of a string for the name field - Risk based on the
    HTTP Payload contract defined in the user story.   3.  Sending invalid JSON data
    that does not conform to the defined schema in the Swagger JSON - Risk based on
    the acceptance criteria specifying valid JSON data conforming to the schema.  
    4.  Not including the optional token in the cookie when sending the PUT request
    - Risk based on the acceptance criteria mentioning an optional token in the cookie.  
    5.  Sending a PUT request to the wrong endpoint or with incorrect formatting -
    Risk based on the user story requiring the PUT request to be sent to /branding/
    with necessary parameters. |'
  prefs: []
  type: TYPE_TB
- en: As expected, the response contains the list of risks we asked for, but what
    is notable is how it has returned context-specific details. For example, for the
    first risk that is focused on missing required parameters, we can see that it
    calls out specific parameters that exist in the HTTP payload example within the
    user story. We can also see that other risks refer to content written in the acceptance
    criteria, such as the use of an optional token stored within cookies (mentioned
    in risk 4).
  prefs: []
  type: TYPE_NORMAL
- en: 'The context-specific information provided in the response exists because of
    the additional details we provided within the prompt in the form of our user story.
    To demonstrate how important this additional information is to the quality of
    an LLM response, here is an example of a response I received using the same prompt
    without the user story added:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    1.  Data validation: Test that the payload accepts only the allowed data
    types and formats as specified in the API documentation. This could help ensure
    that invalid data does not cause unexpected behavior in the system. (Based on
    the need to test the payload portion of the PUT branding endpoint)   2.  Boundary
    testing: Test the system’s behavior when the payload includes data at the upper
    and lower limits of allowed values. This could help uncover any edge cases where
    the system may not handle extreme values correctly. (Based on the need to test
    the payload portion of the PUT branding endpoint)   3.  Performance testing: Test
    the system’s response time and resource usage when handling large payloads. This
    could reveal any performance bottlenecks or issues with scalability. (Based on
    the need to test the payload portion of the PUT branding endpoint)   4.  Security
    testing: Test for any vulnerabilities related to the payload input, such as SQL
    injection or cross-site scripting attacks. This could help ensure that the system
    is secure against malicious input. (Based on the need to test the payload portion
    of the PUT branding endpoint) |'
  prefs: []
  type: TYPE_TB
- en: As we can see, the items returned are more generic and shallow because our prompt
    lacks that additional context information, thus demonstrating the value of a RAG
    system in prompting LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, RAG frameworks do have limitations, and our basic framework is no
    different. For example, if we were to send this query:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| What risks should I consider for the Auth API? |'
  prefs: []
  type: TYPE_TB
- en: 'we might get a response like the following, which is returning information
    that is of no use to us:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    1.  Risk: Unable to successfully delete a room by providing a valid room
    ID   •   This risk is based on the acceptance criteria of being able to successfully
    delete a room by providing a valid room ID.   2.  Risk: Incorrect error handling
    for invalid room ID   •   This risk is based on the acceptance criteria of receiving
    a 400 Bad Request error when providing an invalid room ID.   3.  Risk: Confirmation
    message not displayed upon successful deletion   •   This risk is based on the
    acceptance criteria of receiving a confirmation message upon successful deletion.
    |'
  prefs: []
  type: TYPE_TB
- en: Although this might feel like our LLM is hallucinating, the problem lies in
    our relevancy checking. Our query is focused on the Auth API, which includes login,
    logout, and validate endpoints, but the mention of `room ID` in the response implies
    that a room-based user story was deemed the most relevant. Once again, we see
    the challenge of finding the most relevant data to provide in a RAG framework
    when we start to consider pulling relevant data from multiple data sources at
    once. Therefore, to progress, we need to consider more advanced tooling to help
    us improve the performance of our relevancy searches to optimize the LLM’s responses.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 11.2
  prefs: []
  type: TYPE_NORMAL
- en: Try out different types of queries based on the user stories found in the project.
    See which queries returned desired results and which didn’t. Consider what tweaks
    we could make to improve the erroneous queries.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Enhancing data storage for RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a deeper understanding of how RAG works, we can begin to explore
    the types of tools in the market that allow us to quickly implement a framework
    with our data. The process of finding the right type of data to bolster our prompts
    can be tricky, but there are some tools and platforms on the market that make
    setting up RAG frameworks easier through the use of SaaS platforms and vector
    databases. So, let’s conclude our exploration into RAG by discussing briefly what
    vector databases are, how they help, and how we can use one for our needs.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.1 Working with Vector databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike a SQL database, in which data is stored as different data types within
    rows inside tables, *vector* databases store data in the form of mathematical
    representations. Specifically, they are stored as *vectors,* which is a collection
    of numbers that represent an entity’s location across multiple dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: To give an example of how vectors work and why they are useful, let’s consider
    another area of software development that uses vectors—game development. Let’s
    say we have a character and two other entities in a 2D world and we want to know
    which of the entities is nearest to our character. We would use a vector that
    contains an X and Y position to determine the location of both. For example, if
    our character was in a central position on a map, their vector would be `(0,0)`.
    Now let’s say our entities were in X/Y positions (our vector dimensions) of `(5,5)`
    and `(10,10)`, as shown in figure 11.2.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the later entity with the position of `(10,10)` is farther away.
    But we can also calculate the distance of vectors mathematically by comparing
    them. So `(0,0)` to `(5,5)` generates a distance score of `7.071068`, and `(0,0)`
    to `(10,10)` has a distance score of `14.14214` (calculated using [https://mng.bz/o0lr](https://mng.bz/o0lr)).
    This, of course, is a basic example, but with vector databases, an entity may
    have vectors that contain many different dimensions, which makes the distance
    calculation much more complex.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH11_F02_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 Graph showing vectors of a character and entities
  prefs: []
  type: TYPE_NORMAL
- en: How these vectors and the related dimensions for our documents are calculated
    is beyond the scope of this book, but it is important to recognize that the purpose
    of using a vector database is to allow us to programmatically work out how close
    an item of data we’re interested in is in relation to our query. In other words,
    we use vector databases to work out relevancy just like we did in our basic RAG
    framework. However, instead of doing it across one dimension, we can compare it
    against many different dimensions at once—meaning in the context of the work we’ve
    done so far, increasing the accuracy of which user stories are deemed relevant
    to our query. Because it also allows support for multiple relevancy, we can extract
    more than one entity or document to add to our prompt if it is within a range
    of relevancy.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.2 Setting up a vector-database-backed RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There has been massive growth in the vector-database-backed RAG market, with
    tools such as LlamaIndex ([https://www.llamaindex.ai/](https://www.llamaindex.ai/))
    and Weviate ([https://weaviate.io/](https://weaviate.io/)). However, to get set
    up quickly with minimal setup and coding, we’ll be looking at a tool called Canopy,
    which is built by the company Pinecone ([https://www.pinecone.io/](https://www.pinecone.io/)).
    Pinecone offers the ability to create vector databases in the cloud, which are
    known as indexes on their platform. They have also created Canopy, a RAG framework
    that integrates with their cloud setup. Canopy is a great choice for a trial RAG
    framework because, unlike our earlier RAG framework, most of the work is taken
    care of by the framework. This means we can get started with a vector database-backed
    RAG framework much faster than if we were to build our own. This of course sacrifices
    control for convenience, but it will give us what we need to try out a vector-database-backed
    RAG. You can learn more about the different parts of Canopy in their README ([https://github.com/pinecone-io/canopy](https://github.com/pinecone-io/canopy)).
  prefs: []
  type: TYPE_NORMAL
- en: Canopy prerequisites
  prefs: []
  type: TYPE_NORMAL
- en: To run Canopy, you will need Python 3.11 installed on your machine. This is
    only required to install Canopy. Once installed, we’ll exclusively use the Canopy
    SLI to set up our framework.
  prefs: []
  type: TYPE_NORMAL
- en: To get us started, we’ll first need to install Canopy on our machine, which
    we do by running the `pip3 install canopy-sdk` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it’s installed, we then require a few API keys to get ourselves set up.
    First, we will need our OpenAI key, which can be found at [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys).
    Next, we’ll need to set up an account on Pinecone and extract the API key from
    it for Canopy to use to create our vector database. To do this, we need to sign
    up to Pinecone, which can be done here: [https://app.pinecone.io/?sessionType=login](https://app.pinecone.io/?sessionType=login).
    During the setup, you will be asked to provide a card for billing details to upgrade
    the free account to a standard one. We need to upgrade to a standard account to
    allow Canopy to create the necessary vector database. Failure to do so will cause
    Canopy to error when we begin to build our index for our RAG framework. At the
    time of writing the standard account is free, but it is unfortunately necessary
    to provide our account details to get access to the features we require.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have created our Pinecone account and upgraded it to a standard one,
    we can start working with Canopy to create our RAG framework. To do this, we need
    to set some environmental variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Or alternatively, if you are using windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The API keys for Pinecone and OpenAI are straightforward and can be found in
    the respective admin sections for each platform. The third variable, though, will
    set the name of the index that will be created on Pinecone’s platform, so we need
    to pick a name for our index, such as `test-index`. Once we have these variables
    in place, we can start Canopy by running the `canopy new` command.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming our API keys are all correct and our Pinecone account is correctly
    upgraded, Canopy will set up a new index in Pinecone (see figure 11.3) that we
    can use to upload our documents when we’re ready.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH11_F03_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 Pinecone indexes after a Canopy run
  prefs: []
  type: TYPE_NORMAL
- en: 'With our index ready, we can begin uploading our user story documents (which
    can be found in the supporting repository at [https://mng.bz/n0dg](https://mng.bz/n0dg)).
    We do this by running Canopy’s `upsert` command and providing the root folder
    for our user stories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This will kick off a process in which our user stories are uploaded into the
    index, and once the upload is completed, we can head back into Pinecone and confirm
    that they are present in our index, as shown in figure 11.4.
  prefs: []
  type: TYPE_NORMAL
- en: We now have everything loaded into our index and are ready to start up our RAG
    framework by running the `canopy start` command.
  prefs: []
  type: TYPE_NORMAL
- en: This starts the Canopy server, which we can now access to send our queries to.
    To access it, we open a new terminal/command prompt and run the `canopy` `chat`
    `--no-rag` command. (Be sure to export your `OPENAI_API_KEY` again if you start
    a new terminal window.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how we provide a flag of `--no-rag`. This enables Canopy to show, for
    comparison, both an output that uses RAG and an output that doesn’t. Upon starting
    up the chat feature, we’ll receive the following message to confirm that we’re
    ready to begin testing out our RAG framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Let’s Chat!User message: ([Esc] followed by [Enter] to accept input) |'
  prefs: []
  type: TYPE_TB
- en: '![](../../OEBPS/Images/CH11_F04_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 An index uploaded with user stories
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.3 Testing a Vector-database-backed RAG framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With Canopy now ready to receive our queries, we can begin testing it out with
    queries and comparing the performance of our RAG framework. For example, sending
    the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| List different types of tests for each JSON parameter in the PUT branding
    endpoint |'
  prefs: []
  type: TYPE_TB
- en: 'returned the following response (edited in size to make it easier to read):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| The different types of tests for each JSON parameter in the PUT branding
    endpoint include:   1.  **Name**:   •   Test for providing a valid string value
    for the name parameter.   2.  **Map (Latitude and Longitude)**:   •   Test for
    providing valid integer values for latitude and longitude parameters.   3.  **LogoUrl**:  
    •   Test for providing a valid string URL for the logoUrl parameter.   4.  **Description**:  
    •   Test for providing a valid string value for the description parameter.   5. 
    **Contact (Name, Address, Phone, Email)**:   •   Test for providing valid string
    values for name, address, phone, and email parameters in the contact object.These
    tests ensure that the JSON data provided in the request body of the PUT request
    to the branding endpoint conforms to the specified schema defined in the Swagger
    JSON.(Source: src/main/resources/data/put-branding.txt) |'
  prefs: []
  type: TYPE_TB
- en: 'The response has a few notable points. First, the response has been structured
    around the different JSON parameters that have been defined within the user story.
    Second, we can see that Canopy has provided a reference at the bottom of the output
    (Source: src/main/resources/data/put-branding.txt) to indicate which sources have
    been deemed most relevant from the vector database. This means that, similar to
    the basic RAG framework we built earlier, this RAG framework is working as expected,
    which is even more striking when compared to how our query performed without RAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    1.  Name:   −   Test for presence of name parameter   −   Test for invalid
    values (e.g. empty string, special characters)   2.  Logo:   −   Test for presence
    of logo parameter   −   Test for valid file format (e.g. PNG, JPEG)   −   Test
    for size limit (e.g. file size less than 5MB)   3.  Color:   −   Test for presence
    of color parameter   −   Test for valid color code format (e.g. hex code)   −  
    Test for invalid color values (e.g. ‘purple’ instead of ‘#123456’)   4.  Font:  
    −   Test for presence of font parameter   −   Test for valid font name or family  
    −   Test for invalid font values (e.g. empty string, special characters)   5. 
    Description:   −   Test for presence of description parameter   −   Test for valid
    length limit (e.g. less than 100 characters)   −   Test for invalid values (e.g.
    special characters, long strings)   6.  Website:   −   Test for presence of website
    parameter   −   Test for valid URL format   −   Test for invalid values (e.g.
    invalid domain, special characters) |'
  prefs: []
  type: TYPE_TB
- en: 'In this response, we can see that there is little connection to the context
    of our branding JSON payload. Again, this demonstrates the challenge we face when
    using LLMs and ensuring they have access to our context. But what about our previous
    problem with the basic RAG framework we created and its inability to bring in
    multiple user stories into a query? With Canopy backed with a vector database,
    we can test out how it performs with a query that has the potential to draw on
    multiple user stories in one single prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| List different types of tests for each Booking endpoint |'
  prefs: []
  type: TYPE_TB
- en: 'Sending this to Canopy returned the following response (again, edited in size
    for readability):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    1.  **GET Booking Endpoint:**   •   Functional Tests:   −   Verify that
    the endpoint returns a list of all bookings.   −   Test the functionality of filtering
    bookings by providing a room ID as a query parameter.   2.  **POST Booking Endpoint:**  
    •   Functional Tests:   −   Ensure the endpoint accepts booking details in JSON
    format and creates a new booking successfully.   −   Test the response and status
    code when the booking request is invalid.   3.  **GET Booking ID Endpoint:**  
    •   Functional Tests:   −   Confirm the endpoint retrieves the correct details
    for a provided booking ID.   −   Test the response when an invalid or missing
    booking ID is provided.   4.  **PUT Booking ID Endpoint:**   •   Functional Tests:  
    −   Ensure the endpoint updates the booking information correctly for a given
    booking ID.   −   Test the response and status code when the booking ID is missing
    or invalid.   5.  **GET Booking Summary Endpoint:**   •   Functional Tests:  
    −   Verify that the server responds with a summary of bookings for a specified
    room ID.   −   Test the response when the room ID is missing or invalid.(Source:
    src/main/resources/data) |'
  prefs: []
  type: TYPE_TB
- en: 'Reviewing this response, we can see that the source at the bottom of the prompt
    is referencing the root folder of our data (Source: src/main/resources/data),
    indicating that multiple files have been pulled into our query. This is confirmed
    by how each endpoint has been mentioned in the response as well. We can correlate
    each entry in the response to a booking-based user story stored within our data
    set.'
  prefs: []
  type: TYPE_NORMAL
- en: Activity 11.3
  prefs: []
  type: TYPE_NORMAL
- en: Using Canopy and Pinecone, prepare and upload your own custom set of data to
    an index. Try out different queries with your custom data to see how a RAG framework
    would perform in your context.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.4 Going forward with RAG frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By building our RAG framework before trying out existing platforms for RAG,
    we’ve now developed a deeper understanding of how RAG works and how it can be
    of benefit. However, what we’ve learned simply serves as an introduction to an
    area of LLM use that has lots of practical applications. What type of data we
    can store in RAG frameworks, whether we use vector databases or not, offers a
    lot of scope. Combined with what we’ve learned about writing effective prompts,
    RAG frameworks can be used to provide live analytics, production data, stored
    user content, and much more to help us create prompts that are more attuned to
    our context, which ultimately will help increase LLM use in our testing and elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retrieval-augmented generation (RAG) is an approach that combines contextual
    data and user queries in a prompt to an LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The selection of contextual data is based on its relevancy to the initial user
    query that has been provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing selected data improves accuracy and ensures that errors around context
    windows are avoided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having contextual data separate from an LLM makes the process of selecting and
    updating data sources easier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For RAG systems to work, we need the ability to upload and store data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG systems use similarity algorithms and tools to determine which data is most
    relevant to a query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building prompts that lack contextual data results in responses that are more
    generic and shallow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An LLM can return incorrect responses if the similarity algorithm returns inaccurate
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding relevant data becomes complex when queries are broad or multiple data
    sources need to be added to a prompt at once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector databases store vectors based on multiple dimensions, which are used
    to determine relevancy to a query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some frameworks and tools offer the ability to quickly set up a RAG framework
    using vector databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing vector databases and related tools and platforms makes it easier for
    us to query them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector databases allow us to pull multiple relevant files into a query at once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG frameworks can provide a wide range of types of data that has multiple applications
    for testing and software development as a whole.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
