- en: Chapter 4\. Understanding Generative AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In late 2015, a group of Silicon Valley entrepreneurs—including Elon Musk and
    Sam Altman—cofounded OpenAI with a mission to ensure that artificial general intelligence
    (AGI) benefits all of humanity. After initially focusing on reinforcement learning,
    the company shifted to generative AI, launching the GPT-2 model in 2019\. A year
    later, it released GPT-3, a model with 175 billion parameters trained on 570 GB
    of text, representing a massive leap from its predecessor.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: The turning point came on November 30, 2022, with the launch of ChatGPT. The
    application’s impact was immediate and transformative, attracting over one million
    users in its first week and 100 million in two months, making it the fastest-growing
    software application in history at the time. The success of ChatGPT triggered
    a surge of investment in generative AI, making the technology a priority for businesses
    worldwide. This led to the rapid development of new models from competitors, including
    Google’s Gemini and xAI’s Grok, each pushing the boundaries of the field.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explores how generative AI works, its core technologies, and its
    primary use cases.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks and Deep Learning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The foundational concepts behind modern generative AI began with early neural
    networks in the 1950s, which attempted to mirror the human brain. These simple
    networks had three components:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Receives the initial data
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layer
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Contains nodes with random weights that process the input to find patterns
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Output layer
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Applies an activation function to the processed data to determine the final
    output
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: While these early systems were limited by the available computing power, they
    established the core principles for modern innovations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '*Deep learning* is an evolution of this structure that uses a neural network
    with many hidden layers, sometimes numbering in the hundreds. This complexity
    allows deep learning models to process vast amounts of data and detect intricate,
    nonlinear patterns that humans might not be able to identify. The basic workflow
    of a deep learning model involves data passing through these layers, with the
    model refining its predictions through a process called *backpropagation*, where
    it learns from its mistakes by adjusting the weights in the network.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI Models
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike traditional AI models that classify or predict based on inputs, generative
    AI produces original outputs that resemble the data it was trained on. These models
    have been used to generate everything from realistic portraits to humanlike conversations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: There are different flavors of generative AI models. Most of them rely on deep
    learning systems. But there are often many tweaks and customizations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'For the AIF-C01 exam, the types of generative AI models to understand include
    the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial network (GAN)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variational autoencoder (VAE)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer model
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusion model
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among these, the transformer model is the most important for the exam.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at each of these in the following sections.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Network
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When Ian Goodfellow earned his Bachelor of Science and Master of Science degrees
    in computer science from Stanford University, he studied under one of the leading
    authorities on AI, Andrew Ng. This inspired him to pursue a career in this field,
    and he would go on to get a PhD in machine learning from Université de Montréal.
    It was here that he studied under Yoshua Bengio, another towering figure in AI.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: His first job out of school was as an intern at Google. He created a neural
    network that could translate addresses from images, which improved Google Maps.
    But it was in 2014 that Goodfellow had his major breakthrough—the generative adversarial
    network. This actually came about from a discussion he had with colleagues at
    a [microbrewery in Montreal, Canada](https://oreil.ly/9DJb4). The topic was about
    how to improve the training for a generative model. His friends talked about an
    approach that would use large amounts of resources. But Goodfellow thought a better
    method was to have two neural networks compete against each other. This would
    allow for the system to create better content.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: When Goodfellow went home later in the night, he could not stop thinking about
    this concept. He spent a few hours creating the GAN model, which generated compelling
    images.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The two neural networks in the GAN were the generator and the discriminator,
    as shown in [Figure 4-1](#figure_four_onedot_the_gan_model).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0401.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. The GAN model
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The generator creates synthetic data from random noise inputs. The discriminator,
    on the other hand, will evaluate the synthetic data and try to assess if it is
    real or not. This “adversarial” process will iterate until the generator is creating
    data that appears real.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: On its face, it is a simple concept. But of course, the GAN was based on complex
    math and algorithms. Not long after developing the GAN, Goodfellow published a
    paper on it, which immediately stirred significant interest from the AI community.
    Meta’s chief AI scientist, Yann LeCun, called it “the coolest idea in deep learning
    in the last 20 years.”
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: There also emerged tools to create images using GANs. Many were posted on social
    media sites like Twitter. In fact, one image was auctioned on Christie’s auction,
    fetching a hefty [$432,000](https://oreil.ly/wKwnu).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: But GANs also proved useful for diverse areas like scientific research, such
    as to improve the accuracy of detecting behavior of subatomic particles in the
    Large Hadron Collider at CERN in Switzerland.^([1](ch04.html#ch01fn2))
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Variational Autoencoder
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a 2013 paper,^([2](ch04.html#ch01fn3)) Diederik P. Kingma and Max Welling
    introduced the variational autoencoder, as diagrammed in [Figure 4-2](#figure_four_twodot_the_process_of_a_vae).
    It was a combination of a complex neural network and advanced probability theory.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0402.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. The process of a VAE
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A VAE has two main components that work together: an encoder and a decoder.
    The encoder acts like a smart summarizer that takes your original data and converts
    it into a compact representation, but unlike a simple summary, it creates what’s
    called a *probability distribution*. This means instead of just creating one fixed
    summary, it learns to capture the range of possible variations and uncertainties
    in the data.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The decoder works in reverse, taking these compressed representations and reconstructing
    them back into data that resembles the original input. What makes VAEs particularly
    powerful is that they don’t just learn to copy data perfectly. Instead, they learn
    the underlying patterns and relationships. This means you can sample from the
    probability distribution in the middle—that is, the latent space—to generate entirely
    new data that shares the same characteristics as your original dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'A common use case for a VAE is to create images. But it can also be used for:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: A VAE is effective in finding outliers, which can be critical for fraud detection
    and network security.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Drug discovery
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: A VAE can create molecular structures. This can help identify potential drug
    candidates quicker and more efficiently.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Sound
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: You can create sound effects and even new music.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Transformer Model
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The launch of the transformer model—which is at the heart of generative AI—came
    in August 2017\. It was published in an academic paper^([3](ch04.html#ch01fn4))
    by authors who were part of the Google Research team. The inspiration for the
    model actually came about from a lunch they had. The researchers debated the question:
    How can computers generate content that is humanlike?'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: What they came up with turned out to be one of the biggest innovations in AI—ever.
    The academic paper would ultimately be cited more than 80,000 times.^([4](ch04.html#ch01fn5))
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The irony is that Google did not initially pay much attention to the transformer.
    In the meantime, various startups, including OpenAI, saw this technology as the
    best approach for AI. Some of the Google researchers would go on to start their
    own AI ventures.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the introduction of the transformer model, the main approach with NLP
    was the use of recurrent neural networks (RNNs). This processes data, like text,
    speech, and time series, sequentially. But there’s a problem with this: it can
    fail to capture the context.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: To deal with this, there were some innovations like long short-term memory (LSTM)
    networks, which also proved to be limited.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: But with the transformer model, the approach was turned on its head. Instead
    of processing data step-by-step like RNNs, transformers used attention mechanisms
    to consider all parts of the input at once—allowing for a deeper and more flexible
    understanding of context in natural language.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'To accomplish this, the transformer architecture relies on four main components
    (see [Figure 4-3](#figure_four_threedot_the_process_of_a_t)):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Input embedding
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Converts words into numerical vectors that can be processed by the model
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Adds information about the position of each word in a sentence, since the model
    does not process input sequentially
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Encoder stack
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Analyzes the entire input sequence and builds a contextual representation of
    it
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Decoder stack
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Generates the output sequence, using the encoded input and previously generated
    outputs to produce fluent, coherent text
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0403.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. The process of a transformer model
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s take a look at each of the four main components.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Input embedding
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Input embedding converts tokens into a vector representation, which is a string
    of numbers. This allows a model to analyze the data and find the patterns.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose a model processes the following: “She ate the pizza.” The input embedding
    might look like the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: “She” → [0.25, –0.13, 0.40, ...]
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “ate” → [0.10, 0.22, –0.35, ...]
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “the” → [–0.05, 0.15, 0.20, ...]
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “pizza” → [0.30, –0.25, 0.50, ...]
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each number in a vector is called a *component*, and together they exist within
    a vector space—a mathematical structure made up of multiple dimensions. Each dimension
    represents a different direction or feature, allowing patterns in the data to
    be represented and analyzed. Creating these vectors involves complex calculations
    based on linear algebra.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem with input embedding is that it will jumble the order of the words.
    No doubt, this can mean that some of the context will be lost or confused.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where positional encoding comes in. This assigns a unique numerical
    vector to each position in the sequence of the words. Here’s an example based
    on our sentence:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Position 1 (for “She”) → [0.01, 0.02, 0.03]
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Position 2 (for “ate”) → [0.02, 0.03, 0.04]
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Position 3 (for “the”) → [0.03, 0.04, 0.05]
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Position 4 (for “pizza”) → [0.04, 0.05, 0.06]
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are then added to the input embeddings.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Encoder stack
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The encoder stack is where the transformer model attempts to understand the
    meaning of the text. This involves different layers of processing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The first one uses a self-attention mechanism, which shows how each word relates
    to every other word. In our example, the model will evaluate the relationship
    between “she” and “ate.” It will understand that “she” is the subject and is performing
    the action of “ate.”
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: After this, there will be processing with more layers of self-attention. The
    goal is to get a better understanding of the meaning of the text. At the end of
    this process, the model should have a solid understanding of “She ate the pizza.”
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Decoder stack
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decoder stack is responsible for generating an output sequence. Common tasks
    include translation, content generation, and summarization.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we want to translate the sentence “She ate the pizza” into Spanish.
    The decoder follows a step-by-step process for each word:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Masked self-attention
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: This allows the decoder to focus only on the words it has already generated,
    preventing it from “seeing” future words. For the first token, it predicts the
    most likely starting word.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder attention
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: This step connects the decoder to the information from the input sentence. For
    example, the model recognizes that “she” is the subject of the sentence.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Output generation
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Based on the previous steps, the decoder predicts the first word in Spanish—“Ella.”
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: This process continues word by word until the entire translation is complete.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: A transformer is a prediction engine
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The transformer model is certainly complex. But when you boil things down, it’s
    really about how it is a prediction engine. As we saw in the encoder and decoder
    stacks, the model is predicting the next word in a sequence.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The predictions are based on the complex relationships among all the words in
    the dataset. This is where the power of the system shines. By leveraging attention
    mechanisms with massive datasets—which are often most of the content on the internet—the
    transformer model can understand and create content in humanlike ways. There is
    also no need for labeling data. The reason is that the transformer is analyzing
    the relationships among the tokens.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: However, there are issues with the transformer model. After all, predictions
    are estimates and can sometimes be wrong. We’ll discuss some of these issues with
    generative AI later in this chapter.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Something else to keep in mind: the transformer model does not have inherent
    knowledge. There is no hardcoded logic, database access, and so on. Again, it’s
    all about making predictions.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion Model
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2015, researchers Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    and Surya Ganguli introduced the diffusion model. They set out the core principles
    in a paper^([5](ch04.html#ch01fn6)) that described an innovative way to create
    new data, such as for images and audio.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in [Figure 4-4](#figure_four_fourdot_the_diffusion_model), the diffusion
    model has two primary phases:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Forward diffusion
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The diffusion model gradually adds noise to the original dataset (such as images
    or audio). Through this process, it learns to understand the data distribution
    and how it transitions from structured data to pure random noise. This phase maps
    the pathway from meaningful data to complete noise.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Reverse diffusion
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The diffusion model reverses the forward process by starting with random noise
    and systematically removing it through many steps. This generates new data that
    is different from the original dataset but retains similar characteristics and
    features. The reverse phase essentially learns to reconstruct structured data
    from noise, enabling the creation of novel samples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0404.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. The diffusion model
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Examples of popular diffusion models include OpenAI’s DALL-E, Stability AI’s
    Stable Diffusion, and Midjourney. They use the text-to-image process.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how this works in DALL-E (not part of AWS). In the input box for
    ChatGPT, click “…” and then select Image. Enter the following prompt:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: A floating island with cascading waterfalls that fall into a swirling vortex
    of stars, under an aurora-lit sky.
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Figure 4-5](#figure_four_fivedot_an_image_created_by) shows the generated
    image.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![An image created by OpenAI’s DALL-E](assets/awsc_0405.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. An image created by OpenAI’s DALL-E
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Foundation Models
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the transformer or diffusion model, you can create foundation models (FMs).
    These are what you can use for your business or personal use, such as ChatGPT
    or Claude.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: There are two main types of FMs. One is the large language model (LLM), which
    is built on the transformer model. An LLM can handle many NLP tasks—answer questions
    about history, write a poem, write code, and so on. There seems to be no end to
    the capabilities. By comparison, traditional AI is mostly focused on a single
    task, such as making a forecast about sales or churn.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Next, there are multimodal models. These models can understand and generate
    different types of content like text, audio, images, and video. A multimodal system
    uses both the transformer and diffusion models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'The lifecycle for training FMs and developing applications for them is different
    from traditional machine learning workflows. For the purposes of the exam, the
    steps are:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Data selection
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pretraining
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss each of these steps in the following sections.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Data Selection
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The datasets for FMs are enormous. For example, OpenAI’s GPT-4 model includes
    nearly 500 billion parameters—the internal values the model adjusts during training
    to make accurate predictions—and processes around 45 terabytes of data. Sources
    for this training data include WebTest (a filtered snapshot of web pages), English
    Wikipedia, and large collections of public domain books.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: What about the more recent models, such as GPT-4 or GPT-o1? There are no details
    on the dataset size. The main reason is to protect competitive advantages. The
    world of model development is certainly high-stakes, especially since it costs
    substantial amounts to build FMs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: For the data selection process, there is no need to wrangle or clean the datasets.
    The model will work seamlessly with unlabeled data. As we saw earlier in this
    chapter, the transformer model will detect the patterns, such as with attention
    mechanisms.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, the larger the dataset, the better. This is known as the
    *scaling laws*. Research has shown that there is a positive relationship between
    the number of parameters in the model and the performance.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The data must be high quality and diverse. This helps to reduce the issues with
    bias and toxic content. Because of this, there is usually extensive curation and
    filtering of the datasets, which is where data science expertise becomes essential.
    There is also the use of various data selection methods, like the Data Selection
    with Importance Resampling (DSIR) framework. This helps to focus on data that
    is most relevant for a particular application.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pretraining stage is where the model learns to understand and generate humanlike
    text. This is done by using a technique called *semisupervised learning*. This
    takes the unlabeled dataset and creates synthetic labels, which are based on the
    data itself. For example, the model can use the transformer model to predict missing
    words or other gaps in a sentence. Given the massive sizes of the datasets, this
    automated approach is absolutely critical. It would be impossible to handle this
    in a manual way.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: But there is more to the process. There is also continuous pretraining, which
    is when the model is exposed to more data to refine and improve the learning.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: The pretraining and continuous pretraining require significant amounts of computing
    resources. A key part of this is the use of graphics processing units (GPUs) or
    tensor processing units (TPUs). GPUs are designed for parallel processing and
    are widely used in deep learning and generative AI models, while TPUs are specialized
    hardware developed by Google specifically to accelerate machine learning tasks.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An FM is quite powerful. But it will not be trained on proprietary data. This
    can certainly be limiting for businesses, which need more specialized FMs, say
    for handling customer support, legal, marketing, sales, and so on. What can you
    do? You can optimize an FM, which involves two main approaches:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval-augmented generation
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose you work in your company’s legal department. While FMs are useful for
    applications like summarization, they do not perform well when it comes to complex
    legal queries.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: If you want to create a system that can effectively extract contract clauses
    and entities, you can use fine-tuning of an existing FM. This is also known as
    *transfer learning*, which is where a model developed for one purpose can be used
    for another.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the main steps for fine-tuning:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Gather relevant documents that are specific to your domain or task. In our example,
    this would include contracts, agreements, and legal correspondence.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and security
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning often uses proprietary or highly sensitive data. This is why there
    needs to be strong privacy, security policies, and guardrails in place. The data
    should also be evaluated to mitigate issues with bias.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Data labeling
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning is a supervised learning process. This means you will label the
    dataset, such as marking specific clauses and entities.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Training
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'You will apply an algorithm to the dataset to adjust the weights and biases
    of the model. For this, there are two main approaches—instruction fine-tuning
    and reinforcement learning from human feedback (RLHF):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Instruction fine-tuning
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: This processes examples of how a model should respond based on certain prompts
    and the output to help the system learn better. It can be quite effective for
    applications like chatbots and virtual assistants.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: RLHF
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The first step in this process is to train the model using supervised learning,
    where it learns based on predicting humanlike responses. The next step is to refine
    the responses by using reinforcement learning, which is based on human feedback.
    It will reward or punish the responses based on this. The goal is to create a
    model that aligns more with human values.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Iterate and evaluate
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: You will iterate this on the dataset until the model learns to recognize and
    extract the clauses and entities.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Besides customizing the FM, fine-tuning can also improve the overall accuracy
    of the responses and help to reduce the bias. There is also the benefit of efficiency.
    You can leverage an existing model and make much smaller modifications to get
    better results.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: But there are drawbacks to fine-tuning. Like with a traditional ML model, there
    is the risk of overfitting. The reason is that the datasets can be too narrow.
    Another issue is that the fine-tuning may go too far—that is, the model may lose
    its advantages for being general-purpose. Finally, fine-tuning can still take
    considerable resources, often needing sophisticated GPUs and AI platforms.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'To help with the problems, there are more advanced fine-tuning methods:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Low-rank adaptation (LoRA)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Instead of adjusting all the model’s parameters, this technique takes a more
    targeted approach, using much fewer resources.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Representation fine-tuning (ReFT)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: This is an even more efficient approach to fine-tuning, which modifies less
    than 1% of the internal weights of the model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: RAG
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2024, *Time* magazine named Patrick Lewis as one of the 100 most influential
    people in AI.^([6](ch04.html#ch01fn7)) The primary reason for this is a paper^([7](ch04.html#ch01fn8))
    he cowrote in 2020 with other researchers from Meta, which set forth a framework
    to connect data to LLMs by searching external databases.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The authors called it *retrieval-augmented generation*. It not only allowed
    for customizing LLMs but also reducing hallucinations. RAG was also generally
    easier to use than fine-tuning, as there were no changes to the weights of the
    model. The process is outlined in [Figure 4-6](#figure_four_sixdot_the_rag_process).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0406.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. The RAG process
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here’s a look at the main steps:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Data collection and indexing
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: The RAG process begins by collecting relevant data from various sources such
    as PDFs, reports, articles, web pages, logs, and customer feedback. This information
    is then prepared for processing and storage.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Chunking
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Since LLMs have limits on how much data they can process at once, the collected
    dataset is divided into smaller, manageable chunks.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Embedding creation
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Each chunk of information is converted into a vector embedding using a specialized
    machine learning model. These embeddings capture the semantic meaning of the data
    and represent it in a high-dimensional mathematical format.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Vector database storage
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The generated embeddings are stored in a vector database, which is a specialized
    system designed to manage and efficiently search through high-dimensional vectors.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: User input
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: When a user submits a query or prompt, this marks the beginning of the retrieval
    phase of the RAG process.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Processing user input
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: The user’s prompt is converted into an embedding using the same ML model that
    was used for the data chunks. This ensures consistency between the user query
    representation and the stored data representations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: The system performs similarity search using techniques such as k-nearest neighbors
    (k-NN), which finds the most similar data points, or cosine similarity, which
    measures how closely the direction of two vectors aligns. This process locates
    the chunks in the vector database that best match the user’s prompt.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: The retrieved relevant information is combined with the original user prompt
    to create an augmented prompt that contains both the user’s question and the contextual
    information needed to answer it.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Response
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The augmented prompt is submitted to the LLM for processing. The LLM generates
    a response that reflects both the user’s original prompt and the relevant chunks
    of information retrieved from the vector database. This should result in a more
    informed and accurate answer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that AWS offers numerous options for vector database capabilities.
    Examples include Amazon OpenSearch Service, Amazon OpenSearch Serverless, and
    Amazon Kendra.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: There is also vector databases that use pgvector (this is for Amazon RDS and
    Amazon Aurora PostgreSQL-Compatible Edition). This is an extension for PostgreSQL,
    which is a popular open source database. Pgvector allows for storing, indexing,
    and querying of high-dimensional vector data. There are also enterprise features,
    such as atomicity, consistency, isolation, durability (ACID) compliance (which
    helps to provide for reliable transactions), point-in-time recovery, and support
    for complex queries.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG has seen significant adoption. According to a survey from [451 Research](https://oreil.ly/Ld7Wg),
    about 87% of the respondents said that they consider this method to be an effective
    approach for customization. Yet RAG has some disadvantages:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Data
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: An organization may not use enough relevant information. Or they may choose
    the wrong sources. Creating a useful RAG system usually requires data science
    expertise.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Search limitations
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search may have a good match, but it can sometimes miss the overall
    context of the information.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Chunking problems
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The chunking process can be delicate. It’s common to make inadequate divisions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With an FM—whether it is fine-tuned or uses RAG—you will need to evaluate it.
    Is it performing properly? Are the responses accurate? Are there hallucinations?
    Is there harmful or toxic content generated?
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation process is complex and time-consuming. But it is critical, in
    terms of building trust with users and effectively solving business problems.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main ways to evaluate an FM:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Human evaluation
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmark datasets
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard evaluation metrics
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human evaluation
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Human evaluation is essential because it helps assess aspects of a model that
    automated metrics may miss, such as user experience, creativity, and ethical behavior.
    These areas are often subjective and require nuanced human judgment. Human evaluation
    typically focuses on several key areas:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: User experience
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'How natural, intuitive, or satisfying is the interaction? One common metric
    is the Net Promoter Score (NPS), which asks, how likely are you to recommend this
    product or service to others?—rated on a scale from 0 to 10\. Participants may
    also be asked: Was the response easy to understand? Did it feel conversational?'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Contextual appropriateness
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Does the model stay on topic and provide responses that make sense in the given
    context? Reviewers might consider questions like, did the model understand the
    question? Did it refer to previous parts of the conversation accurately?
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Creativity and flexibility
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Are the responses varied and interesting—or repetitive and dull? Evaluators
    can assess whether the model provides diverse outputs when asked to generate content
    or brainstorm ideas.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Ethical considerations
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Are there signs of bias, harmful outputs, or inappropriate content? Human reviewers
    play a critical role in spotting these issues that may slip past automated filters.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Emotional intelligence
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Can the FM detect and respond appropriately to emotional tones, such as frustration,
    excitement, or sadness? Questions may include: Did the model acknowledge the user’s
    emotions? Did it respond in a sensitive manner?'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Human evaluations are usually conducted through panels or focus groups, which
    may range from a handful of participants to over 100\. Evaluators are often selected
    based on criteria such as:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Target audience (e.g., specific industry professionals)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diverse backgrounds (e.g., race, gender, culture)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experience level (from novice users to experts)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Participants typically spend time prompting the FM, reviewing its outputs, and
    providing detailed feedback. This is often followed by surveys or structured interviews.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: There’s also a form of passive human evaluation that’s built directly into many
    FMs. For example, ChatGPT allows users to give a thumbs up or down after a response.
    Some platforms include embedded feedback forms or prompt users to rate helpfulness.
    This type of real-time, user-driven feedback is valuable for tracking long-term
    trends and improving future versions of the model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark datasets
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A benchmark dataset allows for making a quantitative evaluation of an FM. They
    help gauge:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Does the FM perform tasks based on certain agreed-upon standards?
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Speed and efficiency
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: How fast does the FM take to generate a response? How many resources does it
    use? For example, some FMs will need to work in real time, such as with self-driving
    cars.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: If the FM is serving heavy volumes, is the performance consistent?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: This evaluates the FM for factors like bias and fairness.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: How does the FM perform when there are unusual or adversarial prompts?
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Generalization
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: This measures an FM’s ability to handle unseen data or tasks.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Creating a benchmark dataset can be challenging, as it usually takes the skills
    of an experienced data scientist. But there is also often the need to have one
    or more subject matter experts (SMEs) involved in the process. They will have
    the necessary experience for the domain that is being tested. For example, if
    a dataset benchmark is for drug discovery, then there will need to be SMEs who
    have a background in the pharmaceutical industry.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-7](#figure_four_sevendot_process_for_develo) shows the steps to putting
    together a dataset benchmark.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0407.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Process for developing dataset benchmarks
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s look at these steps in more detail:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: SMEs create relevant questions
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: SMEs develop a comprehensive set of questions that are not only relevant to
    the domain but also challenging enough to truly evaluate the FM’s capabilities.
    These questions are designed to test the depth and breadth of the FM’s knowledge
    and reasoning abilities.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: SMEs create answers
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The same SMEs conduct extensive research to provide high-quality reference answers
    to each question.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: FM processing
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The created questions are submitted to the FM being evaluated. The FM generates
    its own answers to each question, which will later be compared against the SME-created
    reference answers.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Judge model
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: An AI model serves as the judge, comparing the FM’s generated answers against
    the SME reference answers. The judge model evaluates the FM’s responses across
    multiple criteria, including accuracy, relevance, and comprehensiveness.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Performance score
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Based on the judge model’s comparative analysis, a final benchmark score is
    generated. This score provides a quantitative measure of the FM’s performance.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Standard evaluation metrics
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A standard evaluation metric is a widely used measurement to assess the performance
    of an FM. There are many available. But for the purposes of the AIF-C01 exam,
    these are the ones that you need to know:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Recall-Oriented Understudy for Gisting Evaluation (ROUGE)
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bilingual evaluation understudy (BLEU)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectional encoder representations from transformers score (BERTScore)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROUGE
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ROUGE is a collection of metrics to evaluate automatic summarization of text
    and machine translation, such as for foreign languages. It compares the overlaps
    of the content generated with an LLM to reference summaries, which are usually
    created by humans.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of ROUGE metrics, and each one measures similarity
    between machine-generated text and reference text in a specific way. One key category
    is ROUGE-N, which focuses on n-gram overlap. An *n*-gram is a sequence of *n*
    consecutive words.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, ROUGE-1 evaluates unigram (single word) overlap:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence: “The car stopped suddenly”'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unigrams: “The,” “car,” “stopped,” “suddenly”'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then ROUGE-2 evaluates bigram (two-word) overlap:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Bigrams: “The car,” “car stopped,” “stopped suddenly”'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can continue with ROUGE-3, ROUGE-4, etc., to evaluate longer phrase matches.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'Why use different *n* values? Because they give you different perspectives:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: ROUGE-1 captures basic word usage.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROUGE-2 and higher capture more structure, phrasing, and fluency.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These distinctions help break down what kind of similarity the model is capturing—are
    the right words there? Are they in the right order? This makes evaluation more
    granular and insightful.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'For interpreting ROUGE-N scores, the range is 0 to 1:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Good score
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a score above 0.5 (50%) is considered strong, especially for ROUGE-1\.
    However, what’s “good” depends on the task.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Bad score
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: A score near 0 means very little overlap, suggesting the model didn’t capture
    important content.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: In short, ROUGE-N helps you assess how well a model captures the key words and
    phrases a human would expect—step-by-step, from simple terms to full phrasing.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Next, there is ROUGE-L (longest common subsequence), which is a metric used
    to evaluate the similarity between a machine-generated text and a human-written
    reference by measuring the longest sequence of words that appears in both texts
    in the same order, though not necessarily consecutively. It is especially useful
    for evaluating long-form content like summaries or narratives, where exact word
    matches might be less important than preserving the structure and meaning of the
    original.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an example. Suppose we have this reference that is human written:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: The quick brown fox jumps over the lazy dog.
  id: totrans-280
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This is what the model generated:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The brown fox quickly jumps over a lazy dog.
  id: totrans-282
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s find the longest common subsequence (LCS)—words that appear in both texts,
    in the same order:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Common words: “The,” “brown,” “fox,” “jumps,” “over,” “lazy,” “dog”'
  id: totrans-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Then this is the LCS:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The brown fox jumps over lazy dog.
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a strong structural match even though some words differ slightly (*quickly*
    versus *quick*, or missing *the*).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the score:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Scores above 0.6 are generally good, especially for complex narratives.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scores in the 0.3–0.5 range may indicate decent content, but poorer structure
    or coherence.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A key benefit of ROUGE is that it is fairly simple, straightforward, and based
    on human judgment. But it is also an effective metric in measuring similarity.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: BLEU
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: BLEU is a metric used to evaluate the quality of machine-generated text by comparing
    it to human-written reference text. The closer the match, the better the quality—especially
    in translation tasks. BLEU scores range from 0 to 1, with 1 indicating a perfect
    match.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: While BLEU is similar to ROUGE in that both rely on *n*-gram analysis, there
    are key differences. BLEU focuses on precision, measuring how many of the generated
    *n*-grams appear in the reference and averaging them. It also penalizes shorter
    outputs through a “brevity penalty,” ensuring translations aren’t overly concise
    just to match key terms.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Introduced in the early 2000s, BLEU was one of the first automatic evaluation
    metrics for machine translation and remains widely used for its effectiveness
    and simplicity.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: BERTScore
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2018, researchers published a paper^([8](ch04.html#ch01fn9)) in which they
    created a new model called BERT, representing a major breakthrough for NLP. Google
    would eventually open source it. The result was that BERT became quite popular
    in the AI community, spawning variations on the model like RoBERTa and DistilBERT.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT also laid the foundation for a new evaluation metric: BERTScore. Unlike
    BLEU and ROUGE, which rely on exact or partial *n*-gram matches, BERTScore evaluates
    the semantic similarity between generated text and reference text. That is, it
    doesn’t just look at whether the same words appear—it checks whether the meaning
    is preserved. This is made possible through semantic search, a technique that
    uses vector embeddings to compare the meanings of words or sentences rather than
    their surface forms. For example, if a reference sentence says, “The cat sat on
    the mat,” and the generated version says, “A feline rested on a rug,” traditional
    metrics might score this poorly due to word mismatch. But BERTScore could recognize
    that the meaning is nearly identical.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: BERTScore offers a more nuanced view of text quality, which is especially useful
    when evaluating LLM outputs that may use different wording but convey the same
    idea. That said, it’s not meant to replace BLEU or ROUGE. In practice, data scientists
    often use multiple metrics together to get a fuller picture of model performance.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Resource for benchmark metrics: Hugging Face'
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2016, Clément Delangue, Julien Chaumond, and Thomas Wolf launched Hugging
    Face (*huggingface.co*). They first developed a chatbot that allowed teens to
    interact with an AI pal. The startup failed to get traction. But the founders
    did not give up. When building their chatbot, they saw that there was a need to
    have a hub for open source AI models and applications.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Today, Hugging Face is the place that many AI people go. There are over 1.4
    million AI models that you can download, and there are over 318,000 datasets.
    For all the AI models, there are detailed profiles, which include documentation,
    code samples, use cases, license information, and limitations. There are also
    benchmark metrics, which will often have comparisons to other models.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Hugging Face是许多AI人士会去的地方。这里有超过140万个可下载的AI模型，以及超过31.8万个数据集。对于所有AI模型，都有详细的简介，包括文档、代码示例、用例、许可信息和限制。还有基准指标，通常会有与其他模型的比较。
- en: Issues with benchmark metrics
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准指标的问题
- en: 'The use of metrics can be a controversial topic. There has been growing concern
    that they are not particularly effective or may be misleading. Part of the reason
    is that LLMs are highly complex and their inner workings may not be disclosed.
    Next, the LLM developers themselves are often the ones who compute the metrics,
    such as by publishing a blog or a white paper. But there are other issues:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 指标的使用可能是一个有争议的话题。人们越来越担心它们并不特别有效，或者可能是误导性的。部分原因是LLM非常复杂，其内部工作原理可能不会公开。其次，LLM的开发者本身往往是计算指标的人，例如通过发布博客或白皮书。但还有其他问题：
- en: Prompts
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Even a small change in the wording can have a major impact on the results for
    a metric.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是措辞上的微小变化也可能对指标的成果产生重大影响。
- en: Copying
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 复制
- en: The dataset for the metrics may actually be part of the training for an LLM.
    In a way, this is like when a student cheats on an exam.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 用于指标的数据库实际上可能是LLM训练的一部分。从某种意义上说，这就像学生考试作弊一样。
- en: Real-world application
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界应用
- en: Tests generally are focused on more theoretical aspects of an LLM. It may not
    pick up on real-world situations.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 测试通常更关注LLM的理论方面。它可能不会注意到现实世界的情况。
- en: Narrowness
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 狭隘性
- en: An evaluation metric will usually focus on one task or category.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标通常会关注一个任务或类别。
- en: Edge cases
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘情况
- en: It’s common for evaluation metrics to focus mostly on common use cases.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标通常主要关注常见用例。
- en: To help address these problems, there have emerged platforms that rank LLMs,
    such as Chatbot Arena. UC Berkeley roommates—Anastasios Angelopoulos and Wei-Lin
    Chiang—launched it in 2023\. What started as a school project has turned into
    one of the most popular destinations for data scientists and AI companies.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助解决这些问题，已经出现了对LLM进行排名的平台，例如Chatbot Arena。加州大学伯克利分校的室友——Anastasios Angelopoulos和Wei-Lin
    Chiang——在2023年启动了它。最初只是一个学校项目，现在已经变成了数据科学家和AI公司最受欢迎的目的地之一。
- en: The system uses a simple form where users ask a question and there are responses
    from two anonymous LLMs. They will rate which is better. Currently, there are
    over [170 models](https://oreil.ly/GojrE) on the platform, and they have logged
    more than two million votes.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统使用一个简单的形式，用户提出问题，有两个匿名的LLM提供回答。他们将评估哪个更好。目前，该平台上已有超过[170个模型](https://oreil.ly/GojrE)，并且已经记录了超过两百万票。
- en: Deployment
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署
- en: After an LLM meets the necessary performance criteria, it’s time to put it into
    production. This can mean that the model will be placed into an application or
    made available as an API. If the model is open source, it can be posted on a platform
    like Hugging Face or Grok.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 当LLM达到必要的性能标准后，就是将其投入生产的时候了。这可能意味着模型将被放入应用程序或作为API提供。如果模型是开源的，它可以在Hugging Face或Grok等平台上发布。
- en: As with any AI model, there should be ongoing monitoring and tracking, such
    as for accuracy, latency, and performance. There should also be evaluation of
    bias, energy usage, security, and potential toxic content. All this data is collected
    for the next model. For example, it’s typical for there to be minor updates every
    couple months. As for major upgrades, these may happen every six months to a year.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何AI模型一样，应该持续监控和跟踪，例如准确性、延迟和性能。还应该评估偏差、能源消耗、安全性和潜在的有害内容。所有这些数据都是为了下一个模型收集的。例如，通常每两个月会有小更新。至于重大升级，这些可能每六个月到一年发生一次。
- en: Capabilities of Generative AI
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式AI的能力
- en: Generative AI is a powerful technology. But there are certain areas where it
    performs exceptionally well. It’s important to know about them when implementing
    this technology, which will allow for better results.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI是一项强大的技术。但它在某些领域表现出色。在实施这项技术时了解这些领域非常重要，这将有助于获得更好的结果。
- en: Perhaps one of the biggest capabilities of generative AI is that it can automate
    tedious activities. True, a person can summarize a long document. But an FM can
    do this in a few seconds—and often with higher accuracy. What this means is that
    the AI can free up time for people to work on more important tasks.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'For the exam, here are some of the other capabilities to understand about generative
    AI:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Adaptability
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: An FM can span many domains. This is one of the most important advantages of
    generative AI. For businesses, this means that—instead of relying on multiple
    applications—there can be just one.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Responsiveness
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI usually can generate responses in near real time, which is critical
    for applications like chatbots.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: With a prompt or two, you can generate humanlike content, say a blog, memo,
    or email.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Data efficiency
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: You can help an FM learn using a few pieces of data.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Personalization
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: The responses can be tailored to your preferences. This can be automated based
    on prior interactions with the AI application.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can process large amounts of data. Depending on the model, it
    can be as large as books.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: In fact, an FM can even exhibit creativity, allowing for sparking ideas. A key
    reason for this is that the model leverages huge amounts of data and is based
    on probabilistic relationships.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a case study from a Wharton School MBA innovation course. Professors
    asked students to come up with a dozen ideas for new products or services. ChatGPT
    generated its own ideas, which included a dorm room chef kit and a collapsible
    laundry hamper. The professors then had an independent online panel evaluate the
    ideas and the results were startling. For those that were judged to be good ideas,
    40% were from the students and 47% came from ChatGPT (the rest were neutral).^([9](ch04.html#ch01fn10))
    Moreover, of the 40 best ideas, only five came from the students. According to
    the professors:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: We predict such a human-machine collaboration will deliver better products and
    services to the market, and improved solutions for whatever society needs in the
    future.
  id: totrans-338
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Drawbacks of Generative AI
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the launch of ChatGPT, it would not take long for issues to arise. By
    March 2023, technology leaders and researchers wrote an open letter expressing
    fears that generative AI could lead to major disruptions, like job displacement,
    disinformation, and loss of control of critical systems.^([10](ch04.html#ch01fn11))
    The letter called for the pause—for six months—of the development of FMs that
    were more powerful than OpenAI’s GPT-4 (at the time, this was the company’s top
    model). Some of the notable signatories were Steve Wozniak, Yuval Noah Harari,
    and Elon Musk.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the AI industry did not pause development. Rather, the pace increased
    significantly. There were no major incidents or mishaps, but this does not imply
    that generative AI is not without considerable issues. Some of the main ones include:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nondeterminism
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretability
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data security and privacy
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social and branding risks
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limited context windows
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recency
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Costs
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data challenge
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallucinations
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A hallucination is where an FM creates a false or misleading response. Some
    of the reasons for this are datasets that do not have enough relevant information
    and the probabilistic nature of the generative AI. According to research from
    Vectara—which publishes a hallucination leaderboard on GitHub—the largest FMs
    from OpenAI, Google, and Anthropic show hallucination rates of anywhere from 2.5%
    to 8.5%.^([11](ch04.html#ch01fn12)) In some cases, it can be more than 15%.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: A way to deal with this is to customize FMs, such as with fine-tuning and RAG.
    Another approach is to be more thoughtful with creating prompts, which we’ll learn
    about in the next chapter.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Some of the LLM providers have been integrating systems to help check the accuracy
    of the systems. This can involve doing a search of the internet to verify facts.
    Take ChatGPT. When you write a prompt, you can specify “Search.” This will access
    the internet for the response. It will also provide links to the references. ChatGPT
    also has a feature called *deep research*. This conducts a multistep research
    of a topic, such as by using more than 10 resources when generating a response.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Nondeterminism
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nondeterminism is when an FM generates a different response even though the
    prompt is the same. This may not necessarily mean there are hallucinations. But
    the response may have a different sentence structure, emphasis on certain points,
    and not even mention certain topics.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: A key reason for nondeterminism is the temperature of an FM, which is a setting
    for the randomness or “creativity” for the responses. For FMs like ChatGPT and
    Claude, you cannot set the temperature. Rather, it is an unknown value. But there
    are other FMs that allow you to do so. This is usually available to developers
    who use the APIs for these systems.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Nondeterminism has its advantages, though, as we saw with our example with the
    Wharton School MBA innovation case study. But then again, it could add too much
    uncertainty for certain applications, especially where there are higher stakes.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interpretability describes the level of understanding, explainability, and trust
    with an FM. This can be fairly low because of the sheer complexity of these systems.
    And as we’ve mentioned earlier, most of the details of the model may not be disclosed.
    This is known as a *black box*.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: This is even the case with open source models. It’s not uncommon for the FM
    developer to not disclose the weights and biases. This may also include the underlying
    datasets. The lack of interpretability can be difficult for industries that are
    highly regulated. In fact, they may even be prohibited from using a model that
    is not explainable.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Data Security and Privacy
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interacting with an FM can pose data security and privacy threats. If you enter
    sensitive information into a system—such as PII—this can be exposed. For some
    FMs, such as ChatGPT or Claude, the data is sent to the cloud. You will need to
    rely on their own security systems.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s true that such large organizations have extensive guardrails. In the case
    of OpenAI, they include:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Data encryption
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: This uses AES256 for data at rest—or in storage—and TLS 1.2 or higher for data
    in transit.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Internal security
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: There are rigorous access controls for authorized personnel. There are also
    advanced cybersecurity systems like firewalls and intrusion protection.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Audits
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Open AI has been vetted by third-party evaluations, such as SOC 2 Type 2 audits.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Bounty program
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI pays rewards for those who find vulnerabilities.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Safety and Security Committee
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: This is an independent group that evaluates the AI models and infrastructure.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Despite all these measures, security is never foolproof. This is why you should
    be careful with what information you enter into an FM. Of course, the same goes
    for when you use datasets when applying customization techniques like RAG and
    fine-tuning. This may include approaches like anonymization of the data. There
    should also be strong cybersecurity protection systems and policies. This can
    help mitigate the risk of data poisoning, which is when a hacker breaches a dataset
    and injects malicious information into it. It can mean that a FM generates toxic
    content or could allow for a backdoor into the model.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Social and Branding Risks
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In late 2023, a Chevrolet dealership rolled out a chatbot for its website. Unfortunately,
    some of the responses proved embarrassing.^([12](ch04.html#ch01fn13)) In some
    cases, the chatbot recommended cars for rivals. It even offered a Chevrolet Tahoe
    for just $1.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: This example was not a one-off. Generative AI does have social risks, which
    could damage a company’s brand and reputation. Customization techniques of an
    FM can certainly help. But in some cases, it may be best to not have a chatbot
    handle certain topics, which can be done by including filters in the chatbot.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Limited Context Windows
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A context window is the amount of text—expressed in tokens—a model can process
    at one time. For example, let’s say you are using a chatbot with a context window
    of 5,000 tokens. In your chat, the total number of tokens for the prompts and
    responses is 10,000\. This means that when generating a response, the chatbot
    will use the last 5,000 tokens. The rest will be ignored.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, the context windows for FMs have expanded greatly. You can find
    some examples in [Table 4-1](#table_four_onedot_context_windows).
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. Context windows
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Size |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 Turbo | 128,000 tokens |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| Meta Llama 3 | 128,000 tokens |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Opus | 200,000 tokens |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| Gemini 1.5 | 1,000,000 tokens |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: To put this into perspective, Gemini 1.5’s context window would handle about
    750,000 words or 2,500 pages. This would certainly be sufficient for many tasks.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: But the context window may still not adequately capture the meaning of the text.
    The reason is the “lost-in-the-middle effect.” This describes how an FM can actually
    get lost when processing the information in the midsection of the tokens.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Recency
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'FMs are pretrained, which means they are trained on datasets as of a certain
    cutoff date. But this presents a problem: recency. It means that more current
    information is not reflected in the FM, which can result in responses that are
    outdated. For example, if there has been a recent election, it may not know who
    the current president is.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: But some of the leading FMs have a workaround that allows for internet queries,
    which we learned about earlier in this chapter.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Costs
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The costs for building FMs can be huge. These are the main categories:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Training
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: You will need to hire a team of highly qualified data scientists, data engineers,
    and software engineers. Such skilled persons can fetch hefty salaries. In some
    cases, companies like OpenAI are offering seven-figure compensation packages for
    data scientists.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Data
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: There are the costs for collecting and licensing of the data. For example, in
    2024 OpenAI announced a [five-year partnership for $250+ million](https://oreil.ly/wU-us)
    with News Corp for licensing of content from publications like the Wall Street
    Journal, Barron’s, the New York Post, and MarketWatch.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: It can take thousands of GPUs to train an FM. To run an AI model at scale, some
    systems require over 100,000 GPUs. These chips are not cheap either, fetching
    $25,000 to $30,000 each. There may even be shortages of GPUs, requiring buying
    systems from a third party—at higher prices—or being put on a waitlist.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Getting an estimate for the costs of an FM can be difficult, as the developers
    mostly keep this information private. But an interview with the CEO of Anthropic,
    Dario Amodei, does shed some light on this.^([13](ch04.html#ch01fn14)) He said
    that the training costs could be anywhere from $5 billion to $10 billion for the
    years 2025 to 2026\. Over the long term—which he did not specify—a model could
    cost a staggering $100 billion.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Then there is the ambitious Stargate project,^([14](ch04.html#ch01fn15)) which
    is a joint venture between OpenAI, Oracle, and SoftBank. The goal is to raise
    $500 billion for building the infrastructure for next-generation AI models. The
    Stargate supercomputer system is expected to have 2 million GPUs and use one gigawatt
    of power each year.^([15](ch04.html#ch01fn16))
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a sense of the sheer scale of state-of-the-art AI data centers, look
    at Meta. The company’s CEO and cofounder, Mark Zuckerberg, had this to say about
    its [latest project](https://oreil.ly/HW7OE):'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: I announced last week that we expect to bring online almost a gigawatt of capacity
    this year. And we’re building a two-gigawatt and potentially bigger AI data center
    that is so big that it will cover a significant part of Manhattan if we were placed
    there.
  id: totrans-405
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我上周宣布，我们预计今年将上线近10吉瓦的产能。我们正在建设一个20吉瓦甚至更大的AI数据中心，如果将其放置在那里，它将覆盖曼哈顿的一个显著部分。
- en: 'With the escalating costs, there is a nagging question: Can AI be profitable?
    Many of the world’s top technology companies and venture capitalists think the
    answer is a clear yes.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 随着成本的上升，有一个令人烦恼的问题：AI能否盈利？世界上许多顶级科技公司和风险投资家认为答案是明确的肯定。
- en: 'In 2025, Google, Meta, Microsoft, and Amazon plan to spend at least $215 billion
    on the infrastructure for AI.^([16](ch04.html#ch01fn17)) This is what Amazon CEO,
    Andy Jassy, said about it: “We think virtually every application that we know
    of today is going to be reinvented with AI inside of it.”'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 到2025年，谷歌、Meta、微软和亚马逊计划至少在AI基础设施上投入至少2150亿美元。[16](ch04.html#ch01fn17)这是亚马逊CEO安迪·贾西对此的看法：“我们认为我们今天所知道的大多数应用程序都将通过AI进行重新发明。”
- en: But it’s still early days with AI and the pace of innovation has been dramatic.
    There will also likely be innovations and breakthroughs for more efficient and
    optimal approaches for creating and operating this technology.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 但AI仍处于早期阶段，创新的速度非常快。也可能会出现更多创新和突破，以更高效和更优化的方法来创建和运营这项技术。
- en: A notable example of this was from China. A 40-year-old Chinese billionaire
    investor launched his own AI startup, called DeepSeek. He believed there was an
    opportunity to disrupt the FM market by leveraging much better approaches to model
    development. With a fraction of the resources of companies like OpenAI—at least
    in terms of GPUs—his team was able to create a highly sophisticated model, called
    R1\. It proved quite capable based on a variety of benchmarks. It also apparently
    cost less than $6 million to develop.^([17](ch04.html#ch01fn18))
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显著的例子来自中国。一位40岁的中国亿万富翁投资者推出了自己的AI初创公司，名为DeepSeek。他相信通过利用更好的模型开发方法，有机会颠覆FM市场。与OpenAI等公司相比，他的团队在GPU资源方面仅占一小部分，却能创建一个高度复杂的模型，称为R1。基于各种基准测试，它证明相当有能力。它显然开发成本不到600万美元。[17](ch04.html#ch01fn18)
- en: Data Challenge
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据挑战
- en: Could we be running out of data for FMs? Well, some academic research predicts
    that this could be the case—at least in terms of quality data. If this turns out
    to be true, this would certainly represent a major issue for AI progress. If anything,
    there are already signs of problems. When new models are announced, they usually
    have minor improvements.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否正在耗尽FM的数据？好吧，一些学术研究预测，这可能是情况——至少在高质量数据方面。如果这被证明是正确的，这无疑将代表AI进步的一个重大问题。如果有什么的话，已经有问题的迹象。当宣布新的模型时，它们通常只有轻微的改进。
- en: But FM developers are looking at ways to deal with this problem, such as with
    synthetic data. This is data that is generally created by using AI models, which
    has proven helpful with use cases like self-driving cars. But the field of synthetic
    data is still in the early phases and there is much that needs to be done.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 但是FM开发者正在寻找解决这个问题的方法，例如使用合成数据。这种数据通常是通过使用AI模型创建的，这在自动驾驶汽车等用例中已被证明是有帮助的。但合成数据领域仍处于早期阶段，还有很多工作要做。
- en: 'Besides the potential of data scarcity, there is another looming issue: the
    proliferation of AI-generated content. A study from AWS estimates that it’s about
    57% of internet text.^([18](ch04.html#ch01fn19)) Some researchers think this could
    mean an adverse feedback loop for FMs, leading to model degradation or even collapse.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据稀缺的潜力外，还有一个迫在眉睫的问题：AI生成内容的激增。一项来自AWS的研究估计，这大约是互联网文本的57%。一些研究人员认为，这可能意味着FM的负面反馈循环，导致模型退化甚至崩溃。
- en: This is not to say that AI is doomed. Let’s face it, there have been many such
    predictions over the years—and yet the technology has continued to remain robust.
    But it does mean that the industry needs to be vigilant and continue to invest
    in ways for improvement.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是说AI注定要失败。让我们面对现实，多年来已经有过许多这样的预测——然而这项技术却继续保持着稳健。但这确实意味着该行业需要保持警惕，并继续投资于改进的方法。
- en: Evolution of FMs
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FM的演变
- en: Daniel Kahneman, famous psychologist, won the Nobel Prize in Economics in 2002\.
    This was for his pioneering research about human judgment under uncertainty. In
    fact, his ideas provide an interesting perspective on generative AI models. In
    his book *Thinking, Fast and Slow*, he set forth his ideas about the human thinking
    process. One is System 1 thinking, which is where a person thinks quickly and
    automatically. In a way, this is how GPT models operate. For example, with ChatGPT,
    it’s a quick interactive experience—with a prompt and response.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Next, there is System 2 thinking, which is when a person takes more time thinking
    about a problem, such as with sophisticated reasoning and planning. System 2 thinking
    applies to next-generation generative AI models. These are often referred to as
    reasoning models or agentic AI.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Besides taking a multistep approach to solving problems, these models also can
    act autonomously—or near autonomously—and also use tools to carry out tasks. There
    will often be multiple generative AI agents that will work collaboratively.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'Take an example for customer support. For this, we have the following agents:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: Customer interaction agent
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: A chatbot that communicates with the customer can handle common questions, such
    as by invoking a knowledge base agent. But for more difficult matters, the agent
    will delegate these to other agents.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: Issue categorization agent
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: This will evaluate the issue and determine what it is about, such as billing,
    technical issues, and so on.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis agent
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: If there are indications of frustration, then this can escalate the situation.
    This may mean handing off the customer to a human agent. This can be done with
    a task routing agent, which makes a determination based on factors like skill
    sets, experience, and workload.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Resolution monitoring agent
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: This will track the progress of all the support tickets. If there are gaps or
    problems, then the agent will escalate the tasks.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: This agentic technology is still in the early phases—but it is showing much
    progress. It’s a major priority for many of the largest technology firms like
    Amazon, Microsoft, and Salesforce.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Consider that Gartner said that agentic AI is poised to be the biggest technology
    trend for 2025.^([19](ch04.html#ch01fn20)) This is backed up with other research,
    such as from Deloitte. The firm predicts that 25% of companies using generative
    AI will [launch agentic AI projects or proof of concepts](https://oreil.ly/LOVKT)
    in 2025\. The adoption rate is expected to reach 50% by 2027.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: AGI
  id: totrans-430
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this chapter, we mentioned the vision of OpenAI, which is about AGI.
    The company defined it as a system that is “smarter than humans.”
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: But the topic of AGI is complicated. After all, the concept of “intelligence”
    is elusive, as there is no generally accepted standard. For example, a person
    does not have to have a genius-level IQ to be a genius.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: Consider Richard Feynman. In World War II, he worked on the Manhattan Project,
    helping to develop the atomic bomb by solving complex neutron equations. After
    this, he would become a professor at the California Institute of Technology (Caltech).
    There, he would make pioneering contributions to quantum electrodynamics and win
    the Nobel Prize in Physics in 1965.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: So what was his IQ? It was [125](https://oreil.ly/o9HHc), which is average intelligence.
    By comparison, a genius level is 180+.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'Then there are cases where people are geniuses and have fairly low IQs, such
    as Leslie Lemke. Even with an IQ of 58 and blind,^([20](ch04.html#ch01fn21)) he
    was able to play complex piano pieces—after hearing them only once. So what does
    intelligence mean for AGI? There are other characteristics that are important
    for superhuman capabilities:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in this chapter, it takes huge amounts of resources and energy
    for AI. But this is not practical for the proliferation of AGI. After all, the
    human brain is only about three pounds and consumes about 20 watts of power, or
    the amount for a dim light bulb.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Interact with the environment
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: An AGI system should be able to have physical capabilities like sight, smell,
    and feel. This will make it much more useful. It will also allow for more learning.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: An AGI system must be able to make effective decisions on its own.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: Creativity
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: While generative AI has shown some capacity for this, it is far from the levels
    that we have seen from humans, say with Steve Jobs, Einstein, or Shakespeare.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: There are various estimates on when AGI will be reached. Ray Kurzweil, who is
    a futurist, says this will happen by 2029.^([21](ch04.html#ch01fn22)) Sir Demis
    Hassabis, who is the CEO of Google DeepMind, is not as optimistic. He predicts
    AGI will be reached by 2034 or so.^([22](ch04.html#ch01fn23))
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: It’s likely that there will need to be more major innovations to achieve AGI.
    It seems that the transformer model will not be enough. If anything, next-generation
    models will need to go beyond being prediction machines. There will also need
    to be new forms of chips and computer systems, like quantum computing.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative AI is a key part of the AIF-C01 exam. This is why we did a deep
    dive—in this chapter—of this important technology. We looked at the core building
    blocks, like neural networks and deep learning. We also saw how generative AI
    has different forms: GANs, VAEs, diffusion models, and the transformer.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Among these, the transformer is the one that is most prominent. So, we looked
    at the inner workings, as well as the lifecycle, how it is a part of FMs, and
    the pros and cons.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how to evaluate generative AI solutions
    as well as the various use cases.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Quiz
  id: totrans-450
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To check your answers, please refer to the [“Chapter 4 Answer Key”](app02.html#answers_ch_4).
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: Which generative AI model involves two neural networks that compete against
    each other?
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Variational autoencoder (VAE)
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformer model
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Generative adversarial network (GAN)
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Diffusion model
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of positional encoding for a transformer model?
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To enhance the performance of backpropagation
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To process words in the original order of the text
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To improve the reliability of GPUs
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To make AI models more cost-effective
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a key advantage of retrieval-augmented generation (RAG) compared to
    the process of fine-tuning?
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RAG requires less energy.
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It significantly increases the model’s speed.
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: RAG mitigates bias in AI responses.
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It does not require the modification of a model’s internal weights.
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a key advantage of a transformer model over a recurrent neural network
    (RNN)?
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers require no labeled data.
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers can process entire datasets in parallel.
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: RNNs cannot process text.
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers do not require training.
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does an AI model create hallucinations?
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They rely on probabilistic predictions.
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They use GPUs, which can be unpredictable.
  id: totrans-474
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They are based on supervised learning.
  id: totrans-475
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They are only a problem with small models.
  id: totrans-476
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which part of a neural network allows for detecting patterns in a dataset?
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Input layer
  id: totrans-478
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Output layer
  id: totrans-479
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Hidden layer
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Activation layer
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([1](ch04.html#ch01fn2-marker)) CERN, [“Large Hadron Collider Begins Third
    Run”](https://oreil.ly/eo_Ke), HPCwire (website), July 6, 2022.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch04.html#ch01fn3-marker)) Diederik P. Kingma and Max Welling, [“Auto-Encoding
    Variational Bayes”](https://oreil.ly/Glgrm), arXiv, revised December 10, 2022.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch04.html#ch01fn4-marker)) Ashish Vaswani et al., [“Attention Is All You
    Need”](https://oreil.ly/AHoJR), arXiv, revised August 2, 2023.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch04.html#ch01fn5-marker)) Parmy Olson, [“Meet the $4 Billion AI Superstars
    That Google Lost”](https://oreil.ly/cWBBa), Bloomberg (website), July 13, 2023.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch04.html#ch01fn6-marker)) Jascha Sohl-Dickstein et al., [“Deep Unsupervised
    Learning Using Nonequilibrium Thermodynamics”](https://oreil.ly/IWAVQ), Proceedings
    of the 32nd International Conference on Machine Learning, JMLR: W&CP 37 (2015).'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch04.html#ch01fn7-marker)) Harry Booth, [“Patrick Lewis”](https://oreil.ly/8E_nA),
    *Time*, September 5, 2024.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch04.html#ch01fn8-marker)) Aleksandra Piktus et al., [“Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks”](https://oreil.ly/MoQye), Meta,
    Conference on Neural Information Processing Systems (NeurIPS), December 6, 2020.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch04.html#ch01fn9-marker)) Jacob Devlin et al., [“BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding”](https://oreil.ly/nfnkQ),
    preprint, arXiv, October 11, 2018.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch04.html#ch01fn10-marker)) Christian Terwiesch and Karl Ulrich, [“M.B.A.
    Students vs. AI: Who Comes Up with More Innovative Ideas?”](https://oreil.ly/WlZg8),
    *Wall Street Journal*, September 9, 2023.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch04.html#ch01fn11-marker)) Future of Life Institute, [“Pause Giant
    AI Experiments: An Open Letter”](https://oreil.ly/FXRBX), March 22, 2023.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch04.html#ch01fn12-marker)) Karen Emslie, [“LLM Hallucinations: A Bug
    or a Feature?”](https://oreil.ly/fA_7_), *Communications of the ACM*, May 23,
    2024.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch04.html#ch01fn13-marker)) Ben Sherry, [“A Chevrolet Dealership Used
    ChatGPT for Customer Service and Learned That AI Isn’t Always on Your Side”](https://oreil.ly/ZvcJo),
    *Inc.*, December 18, 2023.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch04.html#ch01fn14-marker)) Erin Snodgrass, [“CEO of Anthropic…Says It
    Could Cost $10 billion to Train AI in 2 Years”](https://oreil.ly/Dc6cC) *Business
    Insider*, April 30, 2024.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '^([14](ch04.html#ch01fn15-marker)) Joanna Stern, [“OpenAI Hails $500 Billion
    Stargate Plan: ‘More Computer Leads to Better Models’”](https://oreil.ly/NxHco),
    *Wall Street Journal*, updated January 22, 2025.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch04.html#ch01fn16-marker)) Jeremy Kahn, [“The $19.6 Billion Pivot”](https://oreil.ly/1vWk2)
    *Fortune*, February 25, 2025.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch04.html#ch01fn17-marker)) Nate Rattner, [“Tech Giants Double Down on
    Their Massive AI Spending”](https://oreil.ly/HCq32) *Wall Street Journal*, February
    6, 2025.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch04.html#ch01fn18-marker)) Kif Leswing, [“Nvidia Calls China’s DeepSeek
    R1 Model ‘an Excellent AI Advancement’”](https://oreil.ly/t76q_), CNBC, January
    27, 2025.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch04.html#ch01fn19-marker)) Tor Constantino, [“Is AI Quietly Killing
    Itself—and the Internet?”](https://oreil.ly/p1p-x), *Forbes* Australia, September
    2, 2024.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: ^([19](ch04.html#ch01fn20-marker)) David Ramel, [“Agentic AI Named Top Tech
    Trend for 2025”](https://oreil.ly/uFxHh), *Campus Technology*, October 23, 2024.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch04.html#ch01fn21-marker)) Darold Treffert, [“Whatever Happened to Leslie
    Lemke”](https://oreil.ly/5Mr6I), *Scientific American* (blog), June 17, 2014.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch04.html#ch01fn22-marker)) Pranav Dixit, [“At TIME100 Impact Dinner,
    AI Leaders Discuss the Technology’s Transformative Potential”](https://oreil.ly/by-I6)
    *Time*, September 17, 2024.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch04.html#ch01fn23-marker)) James Hurley, [“AI Has the Potential to ‘Cure
    All Diseases,’ Says DeepMind Chief”](https://oreil.ly/USHZW), *Sunday Times*,
    October 2, 2024.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
