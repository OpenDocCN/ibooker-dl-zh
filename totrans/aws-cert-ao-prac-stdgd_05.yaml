- en: Chapter 4\. Understanding Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In late 2015, a group of Silicon Valley entrepreneurs—including Elon Musk and
    Sam Altman—cofounded OpenAI with a mission to ensure that artificial general intelligence
    (AGI) benefits all of humanity. After initially focusing on reinforcement learning,
    the company shifted to generative AI, launching the GPT-2 model in 2019\. A year
    later, it released GPT-3, a model with 175 billion parameters trained on 570 GB
    of text, representing a massive leap from its predecessor.
  prefs: []
  type: TYPE_NORMAL
- en: The turning point came on November 30, 2022, with the launch of ChatGPT. The
    application’s impact was immediate and transformative, attracting over one million
    users in its first week and 100 million in two months, making it the fastest-growing
    software application in history at the time. The success of ChatGPT triggered
    a surge of investment in generative AI, making the technology a priority for businesses
    worldwide. This led to the rapid development of new models from competitors, including
    Google’s Gemini and xAI’s Grok, each pushing the boundaries of the field.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explores how generative AI works, its core technologies, and its
    primary use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The foundational concepts behind modern generative AI began with early neural
    networks in the 1950s, which attempted to mirror the human brain. These simple
    networks had three components:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs: []
  type: TYPE_NORMAL
- en: Receives the initial data
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layer
  prefs: []
  type: TYPE_NORMAL
- en: Contains nodes with random weights that process the input to find patterns
  prefs: []
  type: TYPE_NORMAL
- en: Output layer
  prefs: []
  type: TYPE_NORMAL
- en: Applies an activation function to the processed data to determine the final
    output
  prefs: []
  type: TYPE_NORMAL
- en: While these early systems were limited by the available computing power, they
    established the core principles for modern innovations.
  prefs: []
  type: TYPE_NORMAL
- en: '*Deep learning* is an evolution of this structure that uses a neural network
    with many hidden layers, sometimes numbering in the hundreds. This complexity
    allows deep learning models to process vast amounts of data and detect intricate,
    nonlinear patterns that humans might not be able to identify. The basic workflow
    of a deep learning model involves data passing through these layers, with the
    model refining its predictions through a process called *backpropagation*, where
    it learns from its mistakes by adjusting the weights in the network.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike traditional AI models that classify or predict based on inputs, generative
    AI produces original outputs that resemble the data it was trained on. These models
    have been used to generate everything from realistic portraits to humanlike conversations.
  prefs: []
  type: TYPE_NORMAL
- en: There are different flavors of generative AI models. Most of them rely on deep
    learning systems. But there are often many tweaks and customizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the AIF-C01 exam, the types of generative AI models to understand include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial network (GAN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variational autoencoder (VAE)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusion model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among these, the transformer model is the most important for the exam.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at each of these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When Ian Goodfellow earned his Bachelor of Science and Master of Science degrees
    in computer science from Stanford University, he studied under one of the leading
    authorities on AI, Andrew Ng. This inspired him to pursue a career in this field,
    and he would go on to get a PhD in machine learning from Université de Montréal.
    It was here that he studied under Yoshua Bengio, another towering figure in AI.
  prefs: []
  type: TYPE_NORMAL
- en: His first job out of school was as an intern at Google. He created a neural
    network that could translate addresses from images, which improved Google Maps.
    But it was in 2014 that Goodfellow had his major breakthrough—the generative adversarial
    network. This actually came about from a discussion he had with colleagues at
    a [microbrewery in Montreal, Canada](https://oreil.ly/9DJb4). The topic was about
    how to improve the training for a generative model. His friends talked about an
    approach that would use large amounts of resources. But Goodfellow thought a better
    method was to have two neural networks compete against each other. This would
    allow for the system to create better content.
  prefs: []
  type: TYPE_NORMAL
- en: When Goodfellow went home later in the night, he could not stop thinking about
    this concept. He spent a few hours creating the GAN model, which generated compelling
    images.
  prefs: []
  type: TYPE_NORMAL
- en: The two neural networks in the GAN were the generator and the discriminator,
    as shown in [Figure 4-1](#figure_four_onedot_the_gan_model).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. The GAN model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The generator creates synthetic data from random noise inputs. The discriminator,
    on the other hand, will evaluate the synthetic data and try to assess if it is
    real or not. This “adversarial” process will iterate until the generator is creating
    data that appears real.
  prefs: []
  type: TYPE_NORMAL
- en: On its face, it is a simple concept. But of course, the GAN was based on complex
    math and algorithms. Not long after developing the GAN, Goodfellow published a
    paper on it, which immediately stirred significant interest from the AI community.
    Meta’s chief AI scientist, Yann LeCun, called it “the coolest idea in deep learning
    in the last 20 years.”
  prefs: []
  type: TYPE_NORMAL
- en: There also emerged tools to create images using GANs. Many were posted on social
    media sites like Twitter. In fact, one image was auctioned on Christie’s auction,
    fetching a hefty [$432,000](https://oreil.ly/wKwnu).
  prefs: []
  type: TYPE_NORMAL
- en: But GANs also proved useful for diverse areas like scientific research, such
    as to improve the accuracy of detecting behavior of subatomic particles in the
    Large Hadron Collider at CERN in Switzerland.^([1](ch04.html#ch01fn2))
  prefs: []
  type: TYPE_NORMAL
- en: Variational Autoencoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a 2013 paper,^([2](ch04.html#ch01fn3)) Diederik P. Kingma and Max Welling
    introduced the variational autoencoder, as diagrammed in [Figure 4-2](#figure_four_twodot_the_process_of_a_vae).
    It was a combination of a complex neural network and advanced probability theory.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. The process of a VAE
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A VAE has two main components that work together: an encoder and a decoder.
    The encoder acts like a smart summarizer that takes your original data and converts
    it into a compact representation, but unlike a simple summary, it creates what’s
    called a *probability distribution*. This means instead of just creating one fixed
    summary, it learns to capture the range of possible variations and uncertainties
    in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: The decoder works in reverse, taking these compressed representations and reconstructing
    them back into data that resembles the original input. What makes VAEs particularly
    powerful is that they don’t just learn to copy data perfectly. Instead, they learn
    the underlying patterns and relationships. This means you can sample from the
    probability distribution in the middle—that is, the latent space—to generate entirely
    new data that shares the same characteristics as your original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common use case for a VAE is to create images. But it can also be used for:'
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs: []
  type: TYPE_NORMAL
- en: A VAE is effective in finding outliers, which can be critical for fraud detection
    and network security.
  prefs: []
  type: TYPE_NORMAL
- en: Drug discovery
  prefs: []
  type: TYPE_NORMAL
- en: A VAE can create molecular structures. This can help identify potential drug
    candidates quicker and more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Sound
  prefs: []
  type: TYPE_NORMAL
- en: You can create sound effects and even new music.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The launch of the transformer model—which is at the heart of generative AI—came
    in August 2017\. It was published in an academic paper^([3](ch04.html#ch01fn4))
    by authors who were part of the Google Research team. The inspiration for the
    model actually came about from a lunch they had. The researchers debated the question:
    How can computers generate content that is humanlike?'
  prefs: []
  type: TYPE_NORMAL
- en: What they came up with turned out to be one of the biggest innovations in AI—ever.
    The academic paper would ultimately be cited more than 80,000 times.^([4](ch04.html#ch01fn5))
  prefs: []
  type: TYPE_NORMAL
- en: The irony is that Google did not initially pay much attention to the transformer.
    In the meantime, various startups, including OpenAI, saw this technology as the
    best approach for AI. Some of the Google researchers would go on to start their
    own AI ventures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the introduction of the transformer model, the main approach with NLP
    was the use of recurrent neural networks (RNNs). This processes data, like text,
    speech, and time series, sequentially. But there’s a problem with this: it can
    fail to capture the context.'
  prefs: []
  type: TYPE_NORMAL
- en: To deal with this, there were some innovations like long short-term memory (LSTM)
    networks, which also proved to be limited.
  prefs: []
  type: TYPE_NORMAL
- en: But with the transformer model, the approach was turned on its head. Instead
    of processing data step-by-step like RNNs, transformers used attention mechanisms
    to consider all parts of the input at once—allowing for a deeper and more flexible
    understanding of context in natural language.
  prefs: []
  type: TYPE_NORMAL
- en: 'To accomplish this, the transformer architecture relies on four main components
    (see [Figure 4-3](#figure_four_threedot_the_process_of_a_t)):'
  prefs: []
  type: TYPE_NORMAL
- en: Input embedding
  prefs: []
  type: TYPE_NORMAL
- en: Converts words into numerical vectors that can be processed by the model
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding
  prefs: []
  type: TYPE_NORMAL
- en: Adds information about the position of each word in a sentence, since the model
    does not process input sequentially
  prefs: []
  type: TYPE_NORMAL
- en: Encoder stack
  prefs: []
  type: TYPE_NORMAL
- en: Analyzes the entire input sequence and builds a contextual representation of
    it
  prefs: []
  type: TYPE_NORMAL
- en: Decoder stack
  prefs: []
  type: TYPE_NORMAL
- en: Generates the output sequence, using the encoded input and previously generated
    outputs to produce fluent, coherent text
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. The process of a transformer model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s take a look at each of the four main components.
  prefs: []
  type: TYPE_NORMAL
- en: Input embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Input embedding converts tokens into a vector representation, which is a string
    of numbers. This allows a model to analyze the data and find the patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose a model processes the following: “She ate the pizza.” The input embedding
    might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: “She” → [0.25, –0.13, 0.40, ...]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “ate” → [0.10, 0.22, –0.35, ...]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “the” → [–0.05, 0.15, 0.20, ...]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “pizza” → [0.30, –0.25, 0.50, ...]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each number in a vector is called a *component*, and together they exist within
    a vector space—a mathematical structure made up of multiple dimensions. Each dimension
    represents a different direction or feature, allowing patterns in the data to
    be represented and analyzed. Creating these vectors involves complex calculations
    based on linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem with input embedding is that it will jumble the order of the words.
    No doubt, this can mean that some of the context will be lost or confused.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where positional encoding comes in. This assigns a unique numerical
    vector to each position in the sequence of the words. Here’s an example based
    on our sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: Position 1 (for “She”) → [0.01, 0.02, 0.03]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Position 2 (for “ate”) → [0.02, 0.03, 0.04]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Position 3 (for “the”) → [0.03, 0.04, 0.05]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Position 4 (for “pizza”) → [0.04, 0.05, 0.06]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are then added to the input embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder stack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The encoder stack is where the transformer model attempts to understand the
    meaning of the text. This involves different layers of processing.
  prefs: []
  type: TYPE_NORMAL
- en: The first one uses a self-attention mechanism, which shows how each word relates
    to every other word. In our example, the model will evaluate the relationship
    between “she” and “ate.” It will understand that “she” is the subject and is performing
    the action of “ate.”
  prefs: []
  type: TYPE_NORMAL
- en: After this, there will be processing with more layers of self-attention. The
    goal is to get a better understanding of the meaning of the text. At the end of
    this process, the model should have a solid understanding of “She ate the pizza.”
  prefs: []
  type: TYPE_NORMAL
- en: Decoder stack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decoder stack is responsible for generating an output sequence. Common tasks
    include translation, content generation, and summarization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we want to translate the sentence “She ate the pizza” into Spanish.
    The decoder follows a step-by-step process for each word:'
  prefs: []
  type: TYPE_NORMAL
- en: Masked self-attention
  prefs: []
  type: TYPE_NORMAL
- en: This allows the decoder to focus only on the words it has already generated,
    preventing it from “seeing” future words. For the first token, it predicts the
    most likely starting word.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder attention
  prefs: []
  type: TYPE_NORMAL
- en: This step connects the decoder to the information from the input sentence. For
    example, the model recognizes that “she” is the subject of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Output generation
  prefs: []
  type: TYPE_NORMAL
- en: Based on the previous steps, the decoder predicts the first word in Spanish—“Ella.”
  prefs: []
  type: TYPE_NORMAL
- en: This process continues word by word until the entire translation is complete.
  prefs: []
  type: TYPE_NORMAL
- en: A transformer is a prediction engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The transformer model is certainly complex. But when you boil things down, it’s
    really about how it is a prediction engine. As we saw in the encoder and decoder
    stacks, the model is predicting the next word in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: The predictions are based on the complex relationships among all the words in
    the dataset. This is where the power of the system shines. By leveraging attention
    mechanisms with massive datasets—which are often most of the content on the internet—the
    transformer model can understand and create content in humanlike ways. There is
    also no need for labeling data. The reason is that the transformer is analyzing
    the relationships among the tokens.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are issues with the transformer model. After all, predictions
    are estimates and can sometimes be wrong. We’ll discuss some of these issues with
    generative AI later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Something else to keep in mind: the transformer model does not have inherent
    knowledge. There is no hardcoded logic, database access, and so on. Again, it’s
    all about making predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2015, researchers Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    and Surya Ganguli introduced the diffusion model. They set out the core principles
    in a paper^([5](ch04.html#ch01fn6)) that described an innovative way to create
    new data, such as for images and audio.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in [Figure 4-4](#figure_four_fourdot_the_diffusion_model), the diffusion
    model has two primary phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Forward diffusion
  prefs: []
  type: TYPE_NORMAL
- en: The diffusion model gradually adds noise to the original dataset (such as images
    or audio). Through this process, it learns to understand the data distribution
    and how it transitions from structured data to pure random noise. This phase maps
    the pathway from meaningful data to complete noise.
  prefs: []
  type: TYPE_NORMAL
- en: Reverse diffusion
  prefs: []
  type: TYPE_NORMAL
- en: The diffusion model reverses the forward process by starting with random noise
    and systematically removing it through many steps. This generates new data that
    is different from the original dataset but retains similar characteristics and
    features. The reverse phase essentially learns to reconstruct structured data
    from noise, enabling the creation of novel samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. The diffusion model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Examples of popular diffusion models include OpenAI’s DALL-E, Stability AI’s
    Stable Diffusion, and Midjourney. They use the text-to-image process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how this works in DALL-E (not part of AWS). In the input box for
    ChatGPT, click “…” and then select Image. Enter the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: A floating island with cascading waterfalls that fall into a swirling vortex
    of stars, under an aurora-lit sky.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Figure 4-5](#figure_four_fivedot_an_image_created_by) shows the generated
    image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![An image created by OpenAI’s DALL-E](assets/awsc_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. An image created by OpenAI’s DALL-E
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Foundation Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the transformer or diffusion model, you can create foundation models (FMs).
    These are what you can use for your business or personal use, such as ChatGPT
    or Claude.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main types of FMs. One is the large language model (LLM), which
    is built on the transformer model. An LLM can handle many NLP tasks—answer questions
    about history, write a poem, write code, and so on. There seems to be no end to
    the capabilities. By comparison, traditional AI is mostly focused on a single
    task, such as making a forecast about sales or churn.
  prefs: []
  type: TYPE_NORMAL
- en: Next, there are multimodal models. These models can understand and generate
    different types of content like text, audio, images, and video. A multimodal system
    uses both the transformer and diffusion models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lifecycle for training FMs and developing applications for them is different
    from traditional machine learning workflows. For the purposes of the exam, the
    steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Data selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pretraining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss each of these steps in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Data Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The datasets for FMs are enormous. For example, OpenAI’s GPT-4 model includes
    nearly 500 billion parameters—the internal values the model adjusts during training
    to make accurate predictions—and processes around 45 terabytes of data. Sources
    for this training data include WebTest (a filtered snapshot of web pages), English
    Wikipedia, and large collections of public domain books.
  prefs: []
  type: TYPE_NORMAL
- en: What about the more recent models, such as GPT-4 or GPT-o1? There are no details
    on the dataset size. The main reason is to protect competitive advantages. The
    world of model development is certainly high-stakes, especially since it costs
    substantial amounts to build FMs.
  prefs: []
  type: TYPE_NORMAL
- en: For the data selection process, there is no need to wrangle or clean the datasets.
    The model will work seamlessly with unlabeled data. As we saw earlier in this
    chapter, the transformer model will detect the patterns, such as with attention
    mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, the larger the dataset, the better. This is known as the
    *scaling laws*. Research has shown that there is a positive relationship between
    the number of parameters in the model and the performance.
  prefs: []
  type: TYPE_NORMAL
- en: The data must be high quality and diverse. This helps to reduce the issues with
    bias and toxic content. Because of this, there is usually extensive curation and
    filtering of the datasets, which is where data science expertise becomes essential.
    There is also the use of various data selection methods, like the Data Selection
    with Importance Resampling (DSIR) framework. This helps to focus on data that
    is most relevant for a particular application.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pretraining stage is where the model learns to understand and generate humanlike
    text. This is done by using a technique called *semisupervised learning*. This
    takes the unlabeled dataset and creates synthetic labels, which are based on the
    data itself. For example, the model can use the transformer model to predict missing
    words or other gaps in a sentence. Given the massive sizes of the datasets, this
    automated approach is absolutely critical. It would be impossible to handle this
    in a manual way.
  prefs: []
  type: TYPE_NORMAL
- en: But there is more to the process. There is also continuous pretraining, which
    is when the model is exposed to more data to refine and improve the learning.
  prefs: []
  type: TYPE_NORMAL
- en: The pretraining and continuous pretraining require significant amounts of computing
    resources. A key part of this is the use of graphics processing units (GPUs) or
    tensor processing units (TPUs). GPUs are designed for parallel processing and
    are widely used in deep learning and generative AI models, while TPUs are specialized
    hardware developed by Google specifically to accelerate machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An FM is quite powerful. But it will not be trained on proprietary data. This
    can certainly be limiting for businesses, which need more specialized FMs, say
    for handling customer support, legal, marketing, sales, and so on. What can you
    do? You can optimize an FM, which involves two main approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval-augmented generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose you work in your company’s legal department. While FMs are useful for
    applications like summarization, they do not perform well when it comes to complex
    legal queries.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to create a system that can effectively extract contract clauses
    and entities, you can use fine-tuning of an existing FM. This is also known as
    *transfer learning*, which is where a model developed for one purpose can be used
    for another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the main steps for fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs: []
  type: TYPE_NORMAL
- en: Gather relevant documents that are specific to your domain or task. In our example,
    this would include contracts, agreements, and legal correspondence.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and security
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning often uses proprietary or highly sensitive data. This is why there
    needs to be strong privacy, security policies, and guardrails in place. The data
    should also be evaluated to mitigate issues with bias.
  prefs: []
  type: TYPE_NORMAL
- en: Data labeling
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning is a supervised learning process. This means you will label the
    dataset, such as marking specific clauses and entities.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs: []
  type: TYPE_NORMAL
- en: 'You will apply an algorithm to the dataset to adjust the weights and biases
    of the model. For this, there are two main approaches—instruction fine-tuning
    and reinforcement learning from human feedback (RLHF):'
  prefs: []
  type: TYPE_NORMAL
- en: Instruction fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: This processes examples of how a model should respond based on certain prompts
    and the output to help the system learn better. It can be quite effective for
    applications like chatbots and virtual assistants.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF
  prefs: []
  type: TYPE_NORMAL
- en: The first step in this process is to train the model using supervised learning,
    where it learns based on predicting humanlike responses. The next step is to refine
    the responses by using reinforcement learning, which is based on human feedback.
    It will reward or punish the responses based on this. The goal is to create a
    model that aligns more with human values.
  prefs: []
  type: TYPE_NORMAL
- en: Iterate and evaluate
  prefs: []
  type: TYPE_NORMAL
- en: You will iterate this on the dataset until the model learns to recognize and
    extract the clauses and entities.
  prefs: []
  type: TYPE_NORMAL
- en: Besides customizing the FM, fine-tuning can also improve the overall accuracy
    of the responses and help to reduce the bias. There is also the benefit of efficiency.
    You can leverage an existing model and make much smaller modifications to get
    better results.
  prefs: []
  type: TYPE_NORMAL
- en: But there are drawbacks to fine-tuning. Like with a traditional ML model, there
    is the risk of overfitting. The reason is that the datasets can be too narrow.
    Another issue is that the fine-tuning may go too far—that is, the model may lose
    its advantages for being general-purpose. Finally, fine-tuning can still take
    considerable resources, often needing sophisticated GPUs and AI platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help with the problems, there are more advanced fine-tuning methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Low-rank adaptation (LoRA)
  prefs: []
  type: TYPE_NORMAL
- en: Instead of adjusting all the model’s parameters, this technique takes a more
    targeted approach, using much fewer resources.
  prefs: []
  type: TYPE_NORMAL
- en: Representation fine-tuning (ReFT)
  prefs: []
  type: TYPE_NORMAL
- en: This is an even more efficient approach to fine-tuning, which modifies less
    than 1% of the internal weights of the model.
  prefs: []
  type: TYPE_NORMAL
- en: RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2024, *Time* magazine named Patrick Lewis as one of the 100 most influential
    people in AI.^([6](ch04.html#ch01fn7)) The primary reason for this is a paper^([7](ch04.html#ch01fn8))
    he cowrote in 2020 with other researchers from Meta, which set forth a framework
    to connect data to LLMs by searching external databases.
  prefs: []
  type: TYPE_NORMAL
- en: The authors called it *retrieval-augmented generation*. It not only allowed
    for customizing LLMs but also reducing hallucinations. RAG was also generally
    easier to use than fine-tuning, as there were no changes to the weights of the
    model. The process is outlined in [Figure 4-6](#figure_four_sixdot_the_rag_process).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. The RAG process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here’s a look at the main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data collection and indexing
  prefs: []
  type: TYPE_NORMAL
- en: The RAG process begins by collecting relevant data from various sources such
    as PDFs, reports, articles, web pages, logs, and customer feedback. This information
    is then prepared for processing and storage.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking
  prefs: []
  type: TYPE_NORMAL
- en: Since LLMs have limits on how much data they can process at once, the collected
    dataset is divided into smaller, manageable chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding creation
  prefs: []
  type: TYPE_NORMAL
- en: Each chunk of information is converted into a vector embedding using a specialized
    machine learning model. These embeddings capture the semantic meaning of the data
    and represent it in a high-dimensional mathematical format.
  prefs: []
  type: TYPE_NORMAL
- en: Vector database storage
  prefs: []
  type: TYPE_NORMAL
- en: The generated embeddings are stored in a vector database, which is a specialized
    system designed to manage and efficiently search through high-dimensional vectors.
  prefs: []
  type: TYPE_NORMAL
- en: User input
  prefs: []
  type: TYPE_NORMAL
- en: When a user submits a query or prompt, this marks the beginning of the retrieval
    phase of the RAG process.
  prefs: []
  type: TYPE_NORMAL
- en: Processing user input
  prefs: []
  type: TYPE_NORMAL
- en: The user’s prompt is converted into an embedding using the same ML model that
    was used for the data chunks. This ensures consistency between the user query
    representation and the stored data representations.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs: []
  type: TYPE_NORMAL
- en: The system performs similarity search using techniques such as k-nearest neighbors
    (k-NN), which finds the most similar data points, or cosine similarity, which
    measures how closely the direction of two vectors aligns. This process locates
    the chunks in the vector database that best match the user’s prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation
  prefs: []
  type: TYPE_NORMAL
- en: The retrieved relevant information is combined with the original user prompt
    to create an augmented prompt that contains both the user’s question and the contextual
    information needed to answer it.
  prefs: []
  type: TYPE_NORMAL
- en: Response
  prefs: []
  type: TYPE_NORMAL
- en: The augmented prompt is submitted to the LLM for processing. The LLM generates
    a response that reflects both the user’s original prompt and the relevant chunks
    of information retrieved from the vector database. This should result in a more
    informed and accurate answer.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that AWS offers numerous options for vector database capabilities.
    Examples include Amazon OpenSearch Service, Amazon OpenSearch Serverless, and
    Amazon Kendra.
  prefs: []
  type: TYPE_NORMAL
- en: There is also vector databases that use pgvector (this is for Amazon RDS and
    Amazon Aurora PostgreSQL-Compatible Edition). This is an extension for PostgreSQL,
    which is a popular open source database. Pgvector allows for storing, indexing,
    and querying of high-dimensional vector data. There are also enterprise features,
    such as atomicity, consistency, isolation, durability (ACID) compliance (which
    helps to provide for reliable transactions), point-in-time recovery, and support
    for complex queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG has seen significant adoption. According to a survey from [451 Research](https://oreil.ly/Ld7Wg),
    about 87% of the respondents said that they consider this method to be an effective
    approach for customization. Yet RAG has some disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs: []
  type: TYPE_NORMAL
- en: An organization may not use enough relevant information. Or they may choose
    the wrong sources. Creating a useful RAG system usually requires data science
    expertise.
  prefs: []
  type: TYPE_NORMAL
- en: Search limitations
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search may have a good match, but it can sometimes miss the overall
    context of the information.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking problems
  prefs: []
  type: TYPE_NORMAL
- en: The chunking process can be delicate. It’s common to make inadequate divisions.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With an FM—whether it is fine-tuned or uses RAG—you will need to evaluate it.
    Is it performing properly? Are the responses accurate? Are there hallucinations?
    Is there harmful or toxic content generated?
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation process is complex and time-consuming. But it is critical, in
    terms of building trust with users and effectively solving business problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main ways to evaluate an FM:'
  prefs: []
  type: TYPE_NORMAL
- en: Human evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmark datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard evaluation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Human evaluation is essential because it helps assess aspects of a model that
    automated metrics may miss, such as user experience, creativity, and ethical behavior.
    These areas are often subjective and require nuanced human judgment. Human evaluation
    typically focuses on several key areas:'
  prefs: []
  type: TYPE_NORMAL
- en: User experience
  prefs: []
  type: TYPE_NORMAL
- en: 'How natural, intuitive, or satisfying is the interaction? One common metric
    is the Net Promoter Score (NPS), which asks, how likely are you to recommend this
    product or service to others?—rated on a scale from 0 to 10\. Participants may
    also be asked: Was the response easy to understand? Did it feel conversational?'
  prefs: []
  type: TYPE_NORMAL
- en: Contextual appropriateness
  prefs: []
  type: TYPE_NORMAL
- en: Does the model stay on topic and provide responses that make sense in the given
    context? Reviewers might consider questions like, did the model understand the
    question? Did it refer to previous parts of the conversation accurately?
  prefs: []
  type: TYPE_NORMAL
- en: Creativity and flexibility
  prefs: []
  type: TYPE_NORMAL
- en: Are the responses varied and interesting—or repetitive and dull? Evaluators
    can assess whether the model provides diverse outputs when asked to generate content
    or brainstorm ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical considerations
  prefs: []
  type: TYPE_NORMAL
- en: Are there signs of bias, harmful outputs, or inappropriate content? Human reviewers
    play a critical role in spotting these issues that may slip past automated filters.
  prefs: []
  type: TYPE_NORMAL
- en: Emotional intelligence
  prefs: []
  type: TYPE_NORMAL
- en: 'Can the FM detect and respond appropriately to emotional tones, such as frustration,
    excitement, or sadness? Questions may include: Did the model acknowledge the user’s
    emotions? Did it respond in a sensitive manner?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Human evaluations are usually conducted through panels or focus groups, which
    may range from a handful of participants to over 100\. Evaluators are often selected
    based on criteria such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Target audience (e.g., specific industry professionals)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diverse backgrounds (e.g., race, gender, culture)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experience level (from novice users to experts)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Participants typically spend time prompting the FM, reviewing its outputs, and
    providing detailed feedback. This is often followed by surveys or structured interviews.
  prefs: []
  type: TYPE_NORMAL
- en: There’s also a form of passive human evaluation that’s built directly into many
    FMs. For example, ChatGPT allows users to give a thumbs up or down after a response.
    Some platforms include embedded feedback forms or prompt users to rate helpfulness.
    This type of real-time, user-driven feedback is valuable for tracking long-term
    trends and improving future versions of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A benchmark dataset allows for making a quantitative evaluation of an FM. They
    help gauge:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs: []
  type: TYPE_NORMAL
- en: Does the FM perform tasks based on certain agreed-upon standards?
  prefs: []
  type: TYPE_NORMAL
- en: Speed and efficiency
  prefs: []
  type: TYPE_NORMAL
- en: How fast does the FM take to generate a response? How many resources does it
    use? For example, some FMs will need to work in real time, such as with self-driving
    cars.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs: []
  type: TYPE_NORMAL
- en: If the FM is serving heavy volumes, is the performance consistent?
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI
  prefs: []
  type: TYPE_NORMAL
- en: This evaluates the FM for factors like bias and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  prefs: []
  type: TYPE_NORMAL
- en: How does the FM perform when there are unusual or adversarial prompts?
  prefs: []
  type: TYPE_NORMAL
- en: Generalization
  prefs: []
  type: TYPE_NORMAL
- en: This measures an FM’s ability to handle unseen data or tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a benchmark dataset can be challenging, as it usually takes the skills
    of an experienced data scientist. But there is also often the need to have one
    or more subject matter experts (SMEs) involved in the process. They will have
    the necessary experience for the domain that is being tested. For example, if
    a dataset benchmark is for drug discovery, then there will need to be SMEs who
    have a background in the pharmaceutical industry.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-7](#figure_four_sevendot_process_for_develo) shows the steps to putting
    together a dataset benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Process for developing dataset benchmarks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s look at these steps in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: SMEs create relevant questions
  prefs: []
  type: TYPE_NORMAL
- en: SMEs develop a comprehensive set of questions that are not only relevant to
    the domain but also challenging enough to truly evaluate the FM’s capabilities.
    These questions are designed to test the depth and breadth of the FM’s knowledge
    and reasoning abilities.
  prefs: []
  type: TYPE_NORMAL
- en: SMEs create answers
  prefs: []
  type: TYPE_NORMAL
- en: The same SMEs conduct extensive research to provide high-quality reference answers
    to each question.
  prefs: []
  type: TYPE_NORMAL
- en: FM processing
  prefs: []
  type: TYPE_NORMAL
- en: The created questions are submitted to the FM being evaluated. The FM generates
    its own answers to each question, which will later be compared against the SME-created
    reference answers.
  prefs: []
  type: TYPE_NORMAL
- en: Judge model
  prefs: []
  type: TYPE_NORMAL
- en: An AI model serves as the judge, comparing the FM’s generated answers against
    the SME reference answers. The judge model evaluates the FM’s responses across
    multiple criteria, including accuracy, relevance, and comprehensiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Performance score
  prefs: []
  type: TYPE_NORMAL
- en: Based on the judge model’s comparative analysis, a final benchmark score is
    generated. This score provides a quantitative measure of the FM’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Standard evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A standard evaluation metric is a widely used measurement to assess the performance
    of an FM. There are many available. But for the purposes of the AIF-C01 exam,
    these are the ones that you need to know:'
  prefs: []
  type: TYPE_NORMAL
- en: Recall-Oriented Understudy for Gisting Evaluation (ROUGE)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bilingual evaluation understudy (BLEU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectional encoder representations from transformers score (BERTScore)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROUGE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ROUGE is a collection of metrics to evaluate automatic summarization of text
    and machine translation, such as for foreign languages. It compares the overlaps
    of the content generated with an LLM to reference summaries, which are usually
    created by humans.
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of ROUGE metrics, and each one measures similarity
    between machine-generated text and reference text in a specific way. One key category
    is ROUGE-N, which focuses on n-gram overlap. An *n*-gram is a sequence of *n*
    consecutive words.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, ROUGE-1 evaluates unigram (single word) overlap:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence: “The car stopped suddenly”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unigrams: “The,” “car,” “stopped,” “suddenly”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then ROUGE-2 evaluates bigram (two-word) overlap:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bigrams: “The car,” “car stopped,” “stopped suddenly”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can continue with ROUGE-3, ROUGE-4, etc., to evaluate longer phrase matches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why use different *n* values? Because they give you different perspectives:'
  prefs: []
  type: TYPE_NORMAL
- en: ROUGE-1 captures basic word usage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROUGE-2 and higher capture more structure, phrasing, and fluency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These distinctions help break down what kind of similarity the model is capturing—are
    the right words there? Are they in the right order? This makes evaluation more
    granular and insightful.
  prefs: []
  type: TYPE_NORMAL
- en: 'For interpreting ROUGE-N scores, the range is 0 to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: Good score
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a score above 0.5 (50%) is considered strong, especially for ROUGE-1\.
    However, what’s “good” depends on the task.
  prefs: []
  type: TYPE_NORMAL
- en: Bad score
  prefs: []
  type: TYPE_NORMAL
- en: A score near 0 means very little overlap, suggesting the model didn’t capture
    important content.
  prefs: []
  type: TYPE_NORMAL
- en: In short, ROUGE-N helps you assess how well a model captures the key words and
    phrases a human would expect—step-by-step, from simple terms to full phrasing.
  prefs: []
  type: TYPE_NORMAL
- en: Next, there is ROUGE-L (longest common subsequence), which is a metric used
    to evaluate the similarity between a machine-generated text and a human-written
    reference by measuring the longest sequence of words that appears in both texts
    in the same order, though not necessarily consecutively. It is especially useful
    for evaluating long-form content like summaries or narratives, where exact word
    matches might be less important than preserving the structure and meaning of the
    original.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an example. Suppose we have this reference that is human written:'
  prefs: []
  type: TYPE_NORMAL
- en: The quick brown fox jumps over the lazy dog.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This is what the model generated:'
  prefs: []
  type: TYPE_NORMAL
- en: The brown fox quickly jumps over a lazy dog.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s find the longest common subsequence (LCS)—words that appear in both texts,
    in the same order:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Common words: “The,” “brown,” “fox,” “jumps,” “over,” “lazy,” “dog”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Then this is the LCS:'
  prefs: []
  type: TYPE_NORMAL
- en: The brown fox jumps over lazy dog.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a strong structural match even though some words differ slightly (*quickly*
    versus *quick*, or missing *the*).
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the score:'
  prefs: []
  type: TYPE_NORMAL
- en: Scores above 0.6 are generally good, especially for complex narratives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scores in the 0.3–0.5 range may indicate decent content, but poorer structure
    or coherence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A key benefit of ROUGE is that it is fairly simple, straightforward, and based
    on human judgment. But it is also an effective metric in measuring similarity.
  prefs: []
  type: TYPE_NORMAL
- en: BLEU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: BLEU is a metric used to evaluate the quality of machine-generated text by comparing
    it to human-written reference text. The closer the match, the better the quality—especially
    in translation tasks. BLEU scores range from 0 to 1, with 1 indicating a perfect
    match.
  prefs: []
  type: TYPE_NORMAL
- en: While BLEU is similar to ROUGE in that both rely on *n*-gram analysis, there
    are key differences. BLEU focuses on precision, measuring how many of the generated
    *n*-grams appear in the reference and averaging them. It also penalizes shorter
    outputs through a “brevity penalty,” ensuring translations aren’t overly concise
    just to match key terms.
  prefs: []
  type: TYPE_NORMAL
- en: Introduced in the early 2000s, BLEU was one of the first automatic evaluation
    metrics for machine translation and remains widely used for its effectiveness
    and simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: BERTScore
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2018, researchers published a paper^([8](ch04.html#ch01fn9)) in which they
    created a new model called BERT, representing a major breakthrough for NLP. Google
    would eventually open source it. The result was that BERT became quite popular
    in the AI community, spawning variations on the model like RoBERTa and DistilBERT.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT also laid the foundation for a new evaluation metric: BERTScore. Unlike
    BLEU and ROUGE, which rely on exact or partial *n*-gram matches, BERTScore evaluates
    the semantic similarity between generated text and reference text. That is, it
    doesn’t just look at whether the same words appear—it checks whether the meaning
    is preserved. This is made possible through semantic search, a technique that
    uses vector embeddings to compare the meanings of words or sentences rather than
    their surface forms. For example, if a reference sentence says, “The cat sat on
    the mat,” and the generated version says, “A feline rested on a rug,” traditional
    metrics might score this poorly due to word mismatch. But BERTScore could recognize
    that the meaning is nearly identical.'
  prefs: []
  type: TYPE_NORMAL
- en: BERTScore offers a more nuanced view of text quality, which is especially useful
    when evaluating LLM outputs that may use different wording but convey the same
    idea. That said, it’s not meant to replace BLEU or ROUGE. In practice, data scientists
    often use multiple metrics together to get a fuller picture of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Resource for benchmark metrics: Hugging Face'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2016, Clément Delangue, Julien Chaumond, and Thomas Wolf launched Hugging
    Face (*huggingface.co*). They first developed a chatbot that allowed teens to
    interact with an AI pal. The startup failed to get traction. But the founders
    did not give up. When building their chatbot, they saw that there was a need to
    have a hub for open source AI models and applications.
  prefs: []
  type: TYPE_NORMAL
- en: Today, Hugging Face is the place that many AI people go. There are over 1.4
    million AI models that you can download, and there are over 318,000 datasets.
    For all the AI models, there are detailed profiles, which include documentation,
    code samples, use cases, license information, and limitations. There are also
    benchmark metrics, which will often have comparisons to other models.
  prefs: []
  type: TYPE_NORMAL
- en: Issues with benchmark metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The use of metrics can be a controversial topic. There has been growing concern
    that they are not particularly effective or may be misleading. Part of the reason
    is that LLMs are highly complex and their inner workings may not be disclosed.
    Next, the LLM developers themselves are often the ones who compute the metrics,
    such as by publishing a blog or a white paper. But there are other issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompts
  prefs: []
  type: TYPE_NORMAL
- en: Even a small change in the wording can have a major impact on the results for
    a metric.
  prefs: []
  type: TYPE_NORMAL
- en: Copying
  prefs: []
  type: TYPE_NORMAL
- en: The dataset for the metrics may actually be part of the training for an LLM.
    In a way, this is like when a student cheats on an exam.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world application
  prefs: []
  type: TYPE_NORMAL
- en: Tests generally are focused on more theoretical aspects of an LLM. It may not
    pick up on real-world situations.
  prefs: []
  type: TYPE_NORMAL
- en: Narrowness
  prefs: []
  type: TYPE_NORMAL
- en: An evaluation metric will usually focus on one task or category.
  prefs: []
  type: TYPE_NORMAL
- en: Edge cases
  prefs: []
  type: TYPE_NORMAL
- en: It’s common for evaluation metrics to focus mostly on common use cases.
  prefs: []
  type: TYPE_NORMAL
- en: To help address these problems, there have emerged platforms that rank LLMs,
    such as Chatbot Arena. UC Berkeley roommates—Anastasios Angelopoulos and Wei-Lin
    Chiang—launched it in 2023\. What started as a school project has turned into
    one of the most popular destinations for data scientists and AI companies.
  prefs: []
  type: TYPE_NORMAL
- en: The system uses a simple form where users ask a question and there are responses
    from two anonymous LLMs. They will rate which is better. Currently, there are
    over [170 models](https://oreil.ly/GojrE) on the platform, and they have logged
    more than two million votes.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After an LLM meets the necessary performance criteria, it’s time to put it into
    production. This can mean that the model will be placed into an application or
    made available as an API. If the model is open source, it can be posted on a platform
    like Hugging Face or Grok.
  prefs: []
  type: TYPE_NORMAL
- en: As with any AI model, there should be ongoing monitoring and tracking, such
    as for accuracy, latency, and performance. There should also be evaluation of
    bias, energy usage, security, and potential toxic content. All this data is collected
    for the next model. For example, it’s typical for there to be minor updates every
    couple months. As for major upgrades, these may happen every six months to a year.
  prefs: []
  type: TYPE_NORMAL
- en: Capabilities of Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI is a powerful technology. But there are certain areas where it
    performs exceptionally well. It’s important to know about them when implementing
    this technology, which will allow for better results.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps one of the biggest capabilities of generative AI is that it can automate
    tedious activities. True, a person can summarize a long document. But an FM can
    do this in a few seconds—and often with higher accuracy. What this means is that
    the AI can free up time for people to work on more important tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the exam, here are some of the other capabilities to understand about generative
    AI:'
  prefs: []
  type: TYPE_NORMAL
- en: Adaptability
  prefs: []
  type: TYPE_NORMAL
- en: An FM can span many domains. This is one of the most important advantages of
    generative AI. For businesses, this means that—instead of relying on multiple
    applications—there can be just one.
  prefs: []
  type: TYPE_NORMAL
- en: Responsiveness
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI usually can generate responses in near real time, which is critical
    for applications like chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity
  prefs: []
  type: TYPE_NORMAL
- en: With a prompt or two, you can generate humanlike content, say a blog, memo,
    or email.
  prefs: []
  type: TYPE_NORMAL
- en: Data efficiency
  prefs: []
  type: TYPE_NORMAL
- en: You can help an FM learn using a few pieces of data.
  prefs: []
  type: TYPE_NORMAL
- en: Personalization
  prefs: []
  type: TYPE_NORMAL
- en: The responses can be tailored to your preferences. This can be automated based
    on prior interactions with the AI application.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can process large amounts of data. Depending on the model, it
    can be as large as books.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, an FM can even exhibit creativity, allowing for sparking ideas. A key
    reason for this is that the model leverages huge amounts of data and is based
    on probabilistic relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a case study from a Wharton School MBA innovation course. Professors
    asked students to come up with a dozen ideas for new products or services. ChatGPT
    generated its own ideas, which included a dorm room chef kit and a collapsible
    laundry hamper. The professors then had an independent online panel evaluate the
    ideas and the results were startling. For those that were judged to be good ideas,
    40% were from the students and 47% came from ChatGPT (the rest were neutral).^([9](ch04.html#ch01fn10))
    Moreover, of the 40 best ideas, only five came from the students. According to
    the professors:'
  prefs: []
  type: TYPE_NORMAL
- en: We predict such a human-machine collaboration will deliver better products and
    services to the market, and improved solutions for whatever society needs in the
    future.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Drawbacks of Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the launch of ChatGPT, it would not take long for issues to arise. By
    March 2023, technology leaders and researchers wrote an open letter expressing
    fears that generative AI could lead to major disruptions, like job displacement,
    disinformation, and loss of control of critical systems.^([10](ch04.html#ch01fn11))
    The letter called for the pause—for six months—of the development of FMs that
    were more powerful than OpenAI’s GPT-4 (at the time, this was the company’s top
    model). Some of the notable signatories were Steve Wozniak, Yuval Noah Harari,
    and Elon Musk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the AI industry did not pause development. Rather, the pace increased
    significantly. There were no major incidents or mishaps, but this does not imply
    that generative AI is not without considerable issues. Some of the main ones include:'
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nondeterminism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data security and privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social and branding risks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limited context windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Costs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data challenge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallucinations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A hallucination is where an FM creates a false or misleading response. Some
    of the reasons for this are datasets that do not have enough relevant information
    and the probabilistic nature of the generative AI. According to research from
    Vectara—which publishes a hallucination leaderboard on GitHub—the largest FMs
    from OpenAI, Google, and Anthropic show hallucination rates of anywhere from 2.5%
    to 8.5%.^([11](ch04.html#ch01fn12)) In some cases, it can be more than 15%.
  prefs: []
  type: TYPE_NORMAL
- en: A way to deal with this is to customize FMs, such as with fine-tuning and RAG.
    Another approach is to be more thoughtful with creating prompts, which we’ll learn
    about in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the LLM providers have been integrating systems to help check the accuracy
    of the systems. This can involve doing a search of the internet to verify facts.
    Take ChatGPT. When you write a prompt, you can specify “Search.” This will access
    the internet for the response. It will also provide links to the references. ChatGPT
    also has a feature called *deep research*. This conducts a multistep research
    of a topic, such as by using more than 10 resources when generating a response.
  prefs: []
  type: TYPE_NORMAL
- en: Nondeterminism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nondeterminism is when an FM generates a different response even though the
    prompt is the same. This may not necessarily mean there are hallucinations. But
    the response may have a different sentence structure, emphasis on certain points,
    and not even mention certain topics.
  prefs: []
  type: TYPE_NORMAL
- en: A key reason for nondeterminism is the temperature of an FM, which is a setting
    for the randomness or “creativity” for the responses. For FMs like ChatGPT and
    Claude, you cannot set the temperature. Rather, it is an unknown value. But there
    are other FMs that allow you to do so. This is usually available to developers
    who use the APIs for these systems.
  prefs: []
  type: TYPE_NORMAL
- en: Nondeterminism has its advantages, though, as we saw with our example with the
    Wharton School MBA innovation case study. But then again, it could add too much
    uncertainty for certain applications, especially where there are higher stakes.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interpretability describes the level of understanding, explainability, and trust
    with an FM. This can be fairly low because of the sheer complexity of these systems.
    And as we’ve mentioned earlier, most of the details of the model may not be disclosed.
    This is known as a *black box*.
  prefs: []
  type: TYPE_NORMAL
- en: This is even the case with open source models. It’s not uncommon for the FM
    developer to not disclose the weights and biases. This may also include the underlying
    datasets. The lack of interpretability can be difficult for industries that are
    highly regulated. In fact, they may even be prohibited from using a model that
    is not explainable.
  prefs: []
  type: TYPE_NORMAL
- en: Data Security and Privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interacting with an FM can pose data security and privacy threats. If you enter
    sensitive information into a system—such as PII—this can be exposed. For some
    FMs, such as ChatGPT or Claude, the data is sent to the cloud. You will need to
    rely on their own security systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s true that such large organizations have extensive guardrails. In the case
    of OpenAI, they include:'
  prefs: []
  type: TYPE_NORMAL
- en: Data encryption
  prefs: []
  type: TYPE_NORMAL
- en: This uses AES256 for data at rest—or in storage—and TLS 1.2 or higher for data
    in transit.
  prefs: []
  type: TYPE_NORMAL
- en: Internal security
  prefs: []
  type: TYPE_NORMAL
- en: There are rigorous access controls for authorized personnel. There are also
    advanced cybersecurity systems like firewalls and intrusion protection.
  prefs: []
  type: TYPE_NORMAL
- en: Audits
  prefs: []
  type: TYPE_NORMAL
- en: Open AI has been vetted by third-party evaluations, such as SOC 2 Type 2 audits.
  prefs: []
  type: TYPE_NORMAL
- en: Bounty program
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI pays rewards for those who find vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Safety and Security Committee
  prefs: []
  type: TYPE_NORMAL
- en: This is an independent group that evaluates the AI models and infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Despite all these measures, security is never foolproof. This is why you should
    be careful with what information you enter into an FM. Of course, the same goes
    for when you use datasets when applying customization techniques like RAG and
    fine-tuning. This may include approaches like anonymization of the data. There
    should also be strong cybersecurity protection systems and policies. This can
    help mitigate the risk of data poisoning, which is when a hacker breaches a dataset
    and injects malicious information into it. It can mean that a FM generates toxic
    content or could allow for a backdoor into the model.
  prefs: []
  type: TYPE_NORMAL
- en: Social and Branding Risks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In late 2023, a Chevrolet dealership rolled out a chatbot for its website. Unfortunately,
    some of the responses proved embarrassing.^([12](ch04.html#ch01fn13)) In some
    cases, the chatbot recommended cars for rivals. It even offered a Chevrolet Tahoe
    for just $1.
  prefs: []
  type: TYPE_NORMAL
- en: This example was not a one-off. Generative AI does have social risks, which
    could damage a company’s brand and reputation. Customization techniques of an
    FM can certainly help. But in some cases, it may be best to not have a chatbot
    handle certain topics, which can be done by including filters in the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Limited Context Windows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A context window is the amount of text—expressed in tokens—a model can process
    at one time. For example, let’s say you are using a chatbot with a context window
    of 5,000 tokens. In your chat, the total number of tokens for the prompts and
    responses is 10,000\. This means that when generating a response, the chatbot
    will use the last 5,000 tokens. The rest will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, the context windows for FMs have expanded greatly. You can find
    some examples in [Table 4-1](#table_four_onedot_context_windows).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. Context windows
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Size |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 Turbo | 128,000 tokens |'
  prefs: []
  type: TYPE_TB
- en: '| Meta Llama 3 | 128,000 tokens |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Opus | 200,000 tokens |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini 1.5 | 1,000,000 tokens |'
  prefs: []
  type: TYPE_TB
- en: To put this into perspective, Gemini 1.5’s context window would handle about
    750,000 words or 2,500 pages. This would certainly be sufficient for many tasks.
  prefs: []
  type: TYPE_NORMAL
- en: But the context window may still not adequately capture the meaning of the text.
    The reason is the “lost-in-the-middle effect.” This describes how an FM can actually
    get lost when processing the information in the midsection of the tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Recency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'FMs are pretrained, which means they are trained on datasets as of a certain
    cutoff date. But this presents a problem: recency. It means that more current
    information is not reflected in the FM, which can result in responses that are
    outdated. For example, if there has been a recent election, it may not know who
    the current president is.'
  prefs: []
  type: TYPE_NORMAL
- en: But some of the leading FMs have a workaround that allows for internet queries,
    which we learned about earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The costs for building FMs can be huge. These are the main categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs: []
  type: TYPE_NORMAL
- en: You will need to hire a team of highly qualified data scientists, data engineers,
    and software engineers. Such skilled persons can fetch hefty salaries. In some
    cases, companies like OpenAI are offering seven-figure compensation packages for
    data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs: []
  type: TYPE_NORMAL
- en: There are the costs for collecting and licensing of the data. For example, in
    2024 OpenAI announced a [five-year partnership for $250+ million](https://oreil.ly/wU-us)
    with News Corp for licensing of content from publications like the Wall Street
    Journal, Barron’s, the New York Post, and MarketWatch.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: It can take thousands of GPUs to train an FM. To run an AI model at scale, some
    systems require over 100,000 GPUs. These chips are not cheap either, fetching
    $25,000 to $30,000 each. There may even be shortages of GPUs, requiring buying
    systems from a third party—at higher prices—or being put on a waitlist.
  prefs: []
  type: TYPE_NORMAL
- en: Getting an estimate for the costs of an FM can be difficult, as the developers
    mostly keep this information private. But an interview with the CEO of Anthropic,
    Dario Amodei, does shed some light on this.^([13](ch04.html#ch01fn14)) He said
    that the training costs could be anywhere from $5 billion to $10 billion for the
    years 2025 to 2026\. Over the long term—which he did not specify—a model could
    cost a staggering $100 billion.
  prefs: []
  type: TYPE_NORMAL
- en: Then there is the ambitious Stargate project,^([14](ch04.html#ch01fn15)) which
    is a joint venture between OpenAI, Oracle, and SoftBank. The goal is to raise
    $500 billion for building the infrastructure for next-generation AI models. The
    Stargate supercomputer system is expected to have 2 million GPUs and use one gigawatt
    of power each year.^([15](ch04.html#ch01fn16))
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a sense of the sheer scale of state-of-the-art AI data centers, look
    at Meta. The company’s CEO and cofounder, Mark Zuckerberg, had this to say about
    its [latest project](https://oreil.ly/HW7OE):'
  prefs: []
  type: TYPE_NORMAL
- en: I announced last week that we expect to bring online almost a gigawatt of capacity
    this year. And we’re building a two-gigawatt and potentially bigger AI data center
    that is so big that it will cover a significant part of Manhattan if we were placed
    there.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'With the escalating costs, there is a nagging question: Can AI be profitable?
    Many of the world’s top technology companies and venture capitalists think the
    answer is a clear yes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2025, Google, Meta, Microsoft, and Amazon plan to spend at least $215 billion
    on the infrastructure for AI.^([16](ch04.html#ch01fn17)) This is what Amazon CEO,
    Andy Jassy, said about it: “We think virtually every application that we know
    of today is going to be reinvented with AI inside of it.”'
  prefs: []
  type: TYPE_NORMAL
- en: But it’s still early days with AI and the pace of innovation has been dramatic.
    There will also likely be innovations and breakthroughs for more efficient and
    optimal approaches for creating and operating this technology.
  prefs: []
  type: TYPE_NORMAL
- en: A notable example of this was from China. A 40-year-old Chinese billionaire
    investor launched his own AI startup, called DeepSeek. He believed there was an
    opportunity to disrupt the FM market by leveraging much better approaches to model
    development. With a fraction of the resources of companies like OpenAI—at least
    in terms of GPUs—his team was able to create a highly sophisticated model, called
    R1\. It proved quite capable based on a variety of benchmarks. It also apparently
    cost less than $6 million to develop.^([17](ch04.html#ch01fn18))
  prefs: []
  type: TYPE_NORMAL
- en: Data Challenge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Could we be running out of data for FMs? Well, some academic research predicts
    that this could be the case—at least in terms of quality data. If this turns out
    to be true, this would certainly represent a major issue for AI progress. If anything,
    there are already signs of problems. When new models are announced, they usually
    have minor improvements.
  prefs: []
  type: TYPE_NORMAL
- en: But FM developers are looking at ways to deal with this problem, such as with
    synthetic data. This is data that is generally created by using AI models, which
    has proven helpful with use cases like self-driving cars. But the field of synthetic
    data is still in the early phases and there is much that needs to be done.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the potential of data scarcity, there is another looming issue: the
    proliferation of AI-generated content. A study from AWS estimates that it’s about
    57% of internet text.^([18](ch04.html#ch01fn19)) Some researchers think this could
    mean an adverse feedback loop for FMs, leading to model degradation or even collapse.'
  prefs: []
  type: TYPE_NORMAL
- en: This is not to say that AI is doomed. Let’s face it, there have been many such
    predictions over the years—and yet the technology has continued to remain robust.
    But it does mean that the industry needs to be vigilant and continue to invest
    in ways for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of FMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Daniel Kahneman, famous psychologist, won the Nobel Prize in Economics in 2002\.
    This was for his pioneering research about human judgment under uncertainty. In
    fact, his ideas provide an interesting perspective on generative AI models. In
    his book *Thinking, Fast and Slow*, he set forth his ideas about the human thinking
    process. One is System 1 thinking, which is where a person thinks quickly and
    automatically. In a way, this is how GPT models operate. For example, with ChatGPT,
    it’s a quick interactive experience—with a prompt and response.
  prefs: []
  type: TYPE_NORMAL
- en: Next, there is System 2 thinking, which is when a person takes more time thinking
    about a problem, such as with sophisticated reasoning and planning. System 2 thinking
    applies to next-generation generative AI models. These are often referred to as
    reasoning models or agentic AI.
  prefs: []
  type: TYPE_NORMAL
- en: Besides taking a multistep approach to solving problems, these models also can
    act autonomously—or near autonomously—and also use tools to carry out tasks. There
    will often be multiple generative AI agents that will work collaboratively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take an example for customer support. For this, we have the following agents:'
  prefs: []
  type: TYPE_NORMAL
- en: Customer interaction agent
  prefs: []
  type: TYPE_NORMAL
- en: A chatbot that communicates with the customer can handle common questions, such
    as by invoking a knowledge base agent. But for more difficult matters, the agent
    will delegate these to other agents.
  prefs: []
  type: TYPE_NORMAL
- en: Issue categorization agent
  prefs: []
  type: TYPE_NORMAL
- en: This will evaluate the issue and determine what it is about, such as billing,
    technical issues, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis agent
  prefs: []
  type: TYPE_NORMAL
- en: If there are indications of frustration, then this can escalate the situation.
    This may mean handing off the customer to a human agent. This can be done with
    a task routing agent, which makes a determination based on factors like skill
    sets, experience, and workload.
  prefs: []
  type: TYPE_NORMAL
- en: Resolution monitoring agent
  prefs: []
  type: TYPE_NORMAL
- en: This will track the progress of all the support tickets. If there are gaps or
    problems, then the agent will escalate the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This agentic technology is still in the early phases—but it is showing much
    progress. It’s a major priority for many of the largest technology firms like
    Amazon, Microsoft, and Salesforce.
  prefs: []
  type: TYPE_NORMAL
- en: Consider that Gartner said that agentic AI is poised to be the biggest technology
    trend for 2025.^([19](ch04.html#ch01fn20)) This is backed up with other research,
    such as from Deloitte. The firm predicts that 25% of companies using generative
    AI will [launch agentic AI projects or proof of concepts](https://oreil.ly/LOVKT)
    in 2025\. The adoption rate is expected to reach 50% by 2027.
  prefs: []
  type: TYPE_NORMAL
- en: AGI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this chapter, we mentioned the vision of OpenAI, which is about AGI.
    The company defined it as a system that is “smarter than humans.”
  prefs: []
  type: TYPE_NORMAL
- en: But the topic of AGI is complicated. After all, the concept of “intelligence”
    is elusive, as there is no generally accepted standard. For example, a person
    does not have to have a genius-level IQ to be a genius.
  prefs: []
  type: TYPE_NORMAL
- en: Consider Richard Feynman. In World War II, he worked on the Manhattan Project,
    helping to develop the atomic bomb by solving complex neutron equations. After
    this, he would become a professor at the California Institute of Technology (Caltech).
    There, he would make pioneering contributions to quantum electrodynamics and win
    the Nobel Prize in Physics in 1965.
  prefs: []
  type: TYPE_NORMAL
- en: So what was his IQ? It was [125](https://oreil.ly/o9HHc), which is average intelligence.
    By comparison, a genius level is 180+.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then there are cases where people are geniuses and have fairly low IQs, such
    as Leslie Lemke. Even with an IQ of 58 and blind,^([20](ch04.html#ch01fn21)) he
    was able to play complex piano pieces—after hearing them only once. So what does
    intelligence mean for AGI? There are other characteristics that are important
    for superhuman capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in this chapter, it takes huge amounts of resources and energy
    for AI. But this is not practical for the proliferation of AGI. After all, the
    human brain is only about three pounds and consumes about 20 watts of power, or
    the amount for a dim light bulb.
  prefs: []
  type: TYPE_NORMAL
- en: Interact with the environment
  prefs: []
  type: TYPE_NORMAL
- en: An AGI system should be able to have physical capabilities like sight, smell,
    and feel. This will make it much more useful. It will also allow for more learning.
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous
  prefs: []
  type: TYPE_NORMAL
- en: An AGI system must be able to make effective decisions on its own.
  prefs: []
  type: TYPE_NORMAL
- en: Creativity
  prefs: []
  type: TYPE_NORMAL
- en: While generative AI has shown some capacity for this, it is far from the levels
    that we have seen from humans, say with Steve Jobs, Einstein, or Shakespeare.
  prefs: []
  type: TYPE_NORMAL
- en: There are various estimates on when AGI will be reached. Ray Kurzweil, who is
    a futurist, says this will happen by 2029.^([21](ch04.html#ch01fn22)) Sir Demis
    Hassabis, who is the CEO of Google DeepMind, is not as optimistic. He predicts
    AGI will be reached by 2034 or so.^([22](ch04.html#ch01fn23))
  prefs: []
  type: TYPE_NORMAL
- en: It’s likely that there will need to be more major innovations to achieve AGI.
    It seems that the transformer model will not be enough. If anything, next-generation
    models will need to go beyond being prediction machines. There will also need
    to be new forms of chips and computer systems, like quantum computing.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative AI is a key part of the AIF-C01 exam. This is why we did a deep
    dive—in this chapter—of this important technology. We looked at the core building
    blocks, like neural networks and deep learning. We also saw how generative AI
    has different forms: GANs, VAEs, diffusion models, and the transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: Among these, the transformer is the one that is most prominent. So, we looked
    at the inner workings, as well as the lifecycle, how it is a part of FMs, and
    the pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how to evaluate generative AI solutions
    as well as the various use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Quiz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To check your answers, please refer to the [“Chapter 4 Answer Key”](app02.html#answers_ch_4).
  prefs: []
  type: TYPE_NORMAL
- en: Which generative AI model involves two neural networks that compete against
    each other?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Variational autoencoder (VAE)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformer model
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Generative adversarial network (GAN)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Diffusion model
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of positional encoding for a transformer model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To enhance the performance of backpropagation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To process words in the original order of the text
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To improve the reliability of GPUs
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To make AI models more cost-effective
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a key advantage of retrieval-augmented generation (RAG) compared to
    the process of fine-tuning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RAG requires less energy.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It significantly increases the model’s speed.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: RAG mitigates bias in AI responses.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It does not require the modification of a model’s internal weights.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a key advantage of a transformer model over a recurrent neural network
    (RNN)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers require no labeled data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers can process entire datasets in parallel.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: RNNs cannot process text.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers do not require training.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does an AI model create hallucinations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They rely on probabilistic predictions.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They use GPUs, which can be unpredictable.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They are based on supervised learning.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They are only a problem with small models.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which part of a neural network allows for detecting patterns in a dataset?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Output layer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Hidden layer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Activation layer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([1](ch04.html#ch01fn2-marker)) CERN, [“Large Hadron Collider Begins Third
    Run”](https://oreil.ly/eo_Ke), HPCwire (website), July 6, 2022.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch04.html#ch01fn3-marker)) Diederik P. Kingma and Max Welling, [“Auto-Encoding
    Variational Bayes”](https://oreil.ly/Glgrm), arXiv, revised December 10, 2022.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch04.html#ch01fn4-marker)) Ashish Vaswani et al., [“Attention Is All You
    Need”](https://oreil.ly/AHoJR), arXiv, revised August 2, 2023.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch04.html#ch01fn5-marker)) Parmy Olson, [“Meet the $4 Billion AI Superstars
    That Google Lost”](https://oreil.ly/cWBBa), Bloomberg (website), July 13, 2023.
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch04.html#ch01fn6-marker)) Jascha Sohl-Dickstein et al., [“Deep Unsupervised
    Learning Using Nonequilibrium Thermodynamics”](https://oreil.ly/IWAVQ), Proceedings
    of the 32nd International Conference on Machine Learning, JMLR: W&CP 37 (2015).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch04.html#ch01fn7-marker)) Harry Booth, [“Patrick Lewis”](https://oreil.ly/8E_nA),
    *Time*, September 5, 2024.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch04.html#ch01fn8-marker)) Aleksandra Piktus et al., [“Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks”](https://oreil.ly/MoQye), Meta,
    Conference on Neural Information Processing Systems (NeurIPS), December 6, 2020.
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch04.html#ch01fn9-marker)) Jacob Devlin et al., [“BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding”](https://oreil.ly/nfnkQ),
    preprint, arXiv, October 11, 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch04.html#ch01fn10-marker)) Christian Terwiesch and Karl Ulrich, [“M.B.A.
    Students vs. AI: Who Comes Up with More Innovative Ideas?”](https://oreil.ly/WlZg8),
    *Wall Street Journal*, September 9, 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch04.html#ch01fn11-marker)) Future of Life Institute, [“Pause Giant
    AI Experiments: An Open Letter”](https://oreil.ly/FXRBX), March 22, 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch04.html#ch01fn12-marker)) Karen Emslie, [“LLM Hallucinations: A Bug
    or a Feature?”](https://oreil.ly/fA_7_), *Communications of the ACM*, May 23,
    2024.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch04.html#ch01fn13-marker)) Ben Sherry, [“A Chevrolet Dealership Used
    ChatGPT for Customer Service and Learned That AI Isn’t Always on Your Side”](https://oreil.ly/ZvcJo),
    *Inc.*, December 18, 2023.
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch04.html#ch01fn14-marker)) Erin Snodgrass, [“CEO of Anthropic…Says It
    Could Cost $10 billion to Train AI in 2 Years”](https://oreil.ly/Dc6cC) *Business
    Insider*, April 30, 2024.
  prefs: []
  type: TYPE_NORMAL
- en: '^([14](ch04.html#ch01fn15-marker)) Joanna Stern, [“OpenAI Hails $500 Billion
    Stargate Plan: ‘More Computer Leads to Better Models’”](https://oreil.ly/NxHco),
    *Wall Street Journal*, updated January 22, 2025.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch04.html#ch01fn16-marker)) Jeremy Kahn, [“The $19.6 Billion Pivot”](https://oreil.ly/1vWk2)
    *Fortune*, February 25, 2025.
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch04.html#ch01fn17-marker)) Nate Rattner, [“Tech Giants Double Down on
    Their Massive AI Spending”](https://oreil.ly/HCq32) *Wall Street Journal*, February
    6, 2025.
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch04.html#ch01fn18-marker)) Kif Leswing, [“Nvidia Calls China’s DeepSeek
    R1 Model ‘an Excellent AI Advancement’”](https://oreil.ly/t76q_), CNBC, January
    27, 2025.
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch04.html#ch01fn19-marker)) Tor Constantino, [“Is AI Quietly Killing
    Itself—and the Internet?”](https://oreil.ly/p1p-x), *Forbes* Australia, September
    2, 2024.
  prefs: []
  type: TYPE_NORMAL
- en: ^([19](ch04.html#ch01fn20-marker)) David Ramel, [“Agentic AI Named Top Tech
    Trend for 2025”](https://oreil.ly/uFxHh), *Campus Technology*, October 23, 2024.
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch04.html#ch01fn21-marker)) Darold Treffert, [“Whatever Happened to Leslie
    Lemke”](https://oreil.ly/5Mr6I), *Scientific American* (blog), June 17, 2014.
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch04.html#ch01fn22-marker)) Pranav Dixit, [“At TIME100 Impact Dinner,
    AI Leaders Discuss the Technology’s Transformative Potential”](https://oreil.ly/by-I6)
    *Time*, September 17, 2024.
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch04.html#ch01fn23-marker)) James Hurley, [“AI Has the Potential to ‘Cure
    All Diseases,’ Says DeepMind Chief”](https://oreil.ly/USHZW), *Sunday Times*,
    October 2, 2024.
  prefs: []
  type: TYPE_NORMAL
