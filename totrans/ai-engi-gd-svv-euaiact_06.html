<html><head></head><body><section data-pdf-bookmark="Chapter 6. AI Engineering for Limited-Risk AI Systems" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_6_ai_engineering_for_limited_risk_ai_systems_1748539923606988">
<h1><span class="label">Chapter 6. </span>AI Engineering for Limited-Risk AI Systems</h1>

<p>The previous chapter translated Articles 9 to 15 of the EU AI Act into actionable specifications for high-risk AI systems within the CRISP-ML(Q) lifecycle, providing practical checklists to support responsible development and management. It also introduced the concept of AI engineering in the context of EU AI Act compliance.</p>

<p>This chapter shifts focus to the Act’s requirements for limited-risk AI systems<a contenteditable="false" data-primary="limited-risk AI systems" data-secondary="AI engineering for" data-type="indexterm" id="l-r-ai-eng-for-1"/>, with particular attention to transparency obligations. It explores how organizations can address these requirements proactively through MLOps. See <a data-type="xref" href="#chapter_6_figure_1_1748539923579859">Figure 6-1</a> for a visual of the steps to take to move toward compliance with the EU AI Act.</p>

<figure><div class="figure" id="chapter_6_figure_1_1748539923579859"><img src="assets/taie_0601.png"/>
<h6><span class="label">Figure 6-1. </span>This chapter focuses on the requirements for limited-risk AI systems and the operationalization of compliance for such systems. See <a data-type="xref" href="ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819">Chapter 1</a> for an explanation of the end-to-end process steps toward EU AI Act compliance.</h6>
</div></figure>

<p>The two guiding questions for this chapter are:</p>

<ul>
	<li>
	<p>To comply with the EU AI Act, what requirements must limited-risk AI systems fulfill?</p>
	</li>
	<li>
	<p>What processes, structures, and AI engineering practices need to be established to comply with the Act with regard to transparency obligations?</p>
	</li>
</ul>

<p>As you may have noticed, I mention both compliance and transparency obligations here. Let’s examine the difference between these two concepts.</p>

<section data-pdf-bookmark="Compliance Assessment Versus Transparency Obligation" data-type="sect1"><div class="sect1" id="chapter_6_compliance_assessment_versus_transparency_obligati_1748539923607324">
<h1>Compliance Assessment Versus Transparency Obligation</h1>

<p>A compliance or conformity assessment<a contenteditable="false" data-primary="conformity assessment" data-type="indexterm" id="conf-assess-1"/> is a mandatory process for high-risk AI systems to ensure they meet the EU AI Act’s requirements, often involving third-party evaluation. Transparency obligations<a contenteditable="false" data-primary="transparency obligations" data-type="indexterm" id="trans-obl-1"/> are less strict and are a broader requirement for all AI systems interacting directly with humans. Let’s quickly take a closer look at each before diving deeper into the transparency obligations laid out in the Act.</p>

<p>A conformity assessment is a formal process by which a provider demonstrates that an AI system meets the requirements outlined in Chapter III, Section 2 of the EU AI Act. This process ensures compliance with legal obligations and may involve a third-party evaluation by a notified body (an independent organization designated by Member States to assess high-risk AI systems). The primary objective is to verify that the system conforms to EU regulatory standards before it is placed on the market.</p>

<p><a href="https://oreil.ly/cc73j">Article 50 of the EU AI Act</a> introduces transparency obligations<a contenteditable="false" data-primary="Article 50" data-seealso="transparency obligations" data-type="indexterm" id="id583"/> that apply to all AI systems intended to interact directly with humans, regardless of their risk classification. These obligations do not require a formal assessment but instead aim to prevent deception or harm by imposing specific duties on providers and deployers to inform individuals that they are interacting with an AI system.</p>

<p><a data-type="xref" href="#chapter_6_table_1_1748539923588699">Table 6-1</a> compares conformity assessments<a contenteditable="false" data-primary="transparency obligations" data-secondary="versus conformity assessment" data-secondary-sortas="conformity assessment" data-type="indexterm" id="trans-obl-v-conf-1"/> and transparency obligations under the EU AI Act.</p>

<table class="striped" id="chapter_6_table_1_1748539923588699">
	<caption><span class="label">Table 6-1. </span>Key differences between conformity assessments and transparency obligations</caption>
	<thead>
		<tr>
			<th>Feature</th>
			<th>Conformity assessment</th>
			<th>Transparency obligation</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Applicability scope</p>
			</td>
			<td>
			<p>High-risk AI systems only</p>
			</td>
			<td>
			<p>AI systems interacting with humans, broadly applicable</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Main goal</p>
			</td>
			<td>
			<p>Ensure compliance with technical and legal requirements</p>
			</td>
			<td>
			<p>Promote user awareness and prevent deception</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Nature</p>
			</td>
			<td>
			<p>Formal process, which requires third-party assessment</p>
			</td>
			<td>
			<p>Informative and clarity-focused</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Process</p>
			</td>
			<td>
			<p>Technical evaluations and compliance checks</p>
			</td>
			<td>
			<p>Labeling, information disclosure</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Responsibility</p>
			</td>
			<td>
			<p>Providers</p>
			</td>
			<td>
			<p>Providers and deployers</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Outcome</p>
			</td>
			<td>
			<p>Certificate and EU declaration of conformity</p>
			</td>
			<td>
			<p>Awareness and understanding for users</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Relation to other laws</p>
			</td>
			<td>
			<p>Part of EU harmonization law, when applicable</p>
			</td>
			<td>
			<p>In addition to other national or EU transparency laws</p>
			</td>
		</tr>
	</tbody>
</table>

<p>In sum, a conformity assessment<a contenteditable="false" data-primary="conformity assessment" data-startref="conf-assess-1" data-type="indexterm" id="id584"/> is a rigorous, technical process aimed at ensuring that high-risk AI systems are safe and fully compliant with the EU AI Act, while transparency obligations are designed to ensure that users are aware they are interacting with an AI system, regardless of its risk classification. In this chapter, we’ll explore what those obligations entail and how they can be addressed in practice<a contenteditable="false" data-primary="transparency obligations" data-secondary="versus conformity assessment" data-secondary-sortas="conformity assessment" data-startref="trans-obl-v-conf-1" data-type="indexterm" id="id585"/>.</p>
</div></section>

<section data-pdf-bookmark="Understanding Transparency Obligations" data-type="sect1"><div class="sect1" id="chapter_6_understanding_transparency_obligations_1748539923607396">
<h1>Understanding Transparency Obligations</h1>

<p>As I explained in <a data-type="xref" href="ch04.html#chapter_4_ai_system_assessment_and_tailoring_ai_engineering_1748539919034657">Chapter 4</a>, the term “limited risk” is not explicitly defined in the EU AI Act. Instead, the Act outlines three main categories: prohibited AI practices under the unacceptable risk category (Article 5), high-risk AI systems (Article 6), and AI systems with specific transparency obligations (Article 50).</p>

<p>“Limited-risk” AI systems<a contenteditable="false" data-primary="transparency obligations" data-secondary="informing users of AI interaction" data-type="indexterm" id="id586"/> <a contenteditable="false" data-primary="transparency obligations" data-secondary="marking synthetic content" data-type="indexterm" id="id587"/><a contenteditable="false" data-primary="transparency obligations" data-secondary="disclosing deepfakes" data-type="indexterm" id="id588"/>are generally understood to be those that are not explicitly prohibited or high risk but are still subject to transparency requirements when they interact directly with humans. Article 50 sets out the transparency obligations for both providers and deployers of AI systems. These obligations apply to <em>all AI systems intended to interact directly with natural persons</em>, regardless of their risk level. <a data-type="xref" href="#chapter_6_table_2_1748539923588741">Table 6-2</a> summarizes the key provisions.</p>

<table class="striped" id="chapter_6_table_2_1748539923588741">
	<caption><span class="label">Table 6-2. </span>Overview of transparency obligations for providers and deployers of AI systems according to Article 50 of the EU AI Act</caption>
	<thead>
		<tr>
			<th>Transparency obligation</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Informing users of AI interaction</p>
			</td>
			<td>
			<p>Providers must inform users when they are interacting with an AI system, unless it’s reasonably obvious or the system is used for law enforcement purposes (e.g., detecting, preventing, investigating, or prosecuting crimes). This applies to chatbots and content-generating tools.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Marking synthetic content</p>
			</td>
			<td>
			<p>Providers of AI systems that generate synthetic content (audio, images, video, text) must clearly mark these outputs as artificially generated or manipulated. The labels should be machine-readable and easily detectable, signaling the content’s non-authentic nature. This requirement does not apply to assistive editing tools or systems that do not significantly alter the original input.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Disclosing deepfakes</p>
			</td>
			<td>
			<p>Providers of AI systems that generate deepfakes (described by Article 3(60) as “AI-generated or manipulated image, audio or video content that resembles existing persons, objects, places, entities or events and would falsely appear to a person to be authentic or truthful”) must clearly label these outputs as artificial. The labels should be machine-readable and easily detectable. This obligation does not apply to systems that provide standard editing functions, do not significantly alter the original data, or are used for law enforcement purposes.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Additional transparency measures for emotion recognition and biometric categorization</p>
			</td>
			<td>
			<p>Deployers of emotion recognition and biometric systems must clearly inform users about how these technologies work and how their data is used, with particular attention to accessibility for individuals with disabilities. Note that meeting the Act’s transparency requirements does not ensure compliance with other legal standards, such as the GDPR.</p>
			</td>
		</tr>
	</tbody>
</table>

<p>Article 50 requires that AI systems intended to interact directly with natural persons be properly designed to support that interaction, as well as mandating that individuals be clearly informed that they are engaging with an AI system. This information should be provided at the start of the interaction. For instance, an AI system with a chat interface triggers this transparency obligation, and users should be informed of its artificial nature the first time they engage with it.</p>

<p>Here are some additional examples of systems that require disclosure:</p>

<ul>
	<li>
	<p>AI systems like Jasper AI or Voice.ai that create or modify content (audio, images, videos, or text) must clearly label their outputs as artificially created or altered.</p>
	</li>
	<li>
	<p>AI systems that generate deepfakes must clearly state that the content is artificially created or manipulated. This is in addition to the general requirement to mark all synthetic content.</p>
	</li>
	<li>
	<p>Emotion recognition and biometric categorization systems like Affectiva (by Smart Eye), which analyze facial expressions or biometric data to infer emotions or classify individuals, require deployers to inform users about the system’s <span class="keep-together">operation</span>.</p>
	</li>
	<li>
	<p>AI systems that generate or manipulate text, such as social media content bots, AI-powered news writing systems like Narrative Science, and large language models (e.g., OpenAI’s GPT models) that produce news articles, financial updates, weather reports, and other public-interest content must disclose that the content was artificially generated or manipulated.</p>
	</li>
</ul>

<p>According to Article 50, all disclosures of this type must be clear, easily noticeable, and accessible to all users. General site-wide disclaimers are insufficient. The disclosure must be made before the actual exposure to the content in question.</p>

<p>Now that you have an idea of what the key transparency obligations look like, let’s turn our attention to how AI engineering practices can support effective <span class="keep-together">implementation</span><a contenteditable="false" data-primary="transparency obligations" data-startref="trans-obl-1" data-type="indexterm" id="id589"/>.</p>
</div></section>

<section data-pdf-bookmark="Aligning AI Engineering with SMACTR and CRISP-ML(Q) for Transparency" data-type="sect1"><div class="sect1" id="chapter_6_aligning_ai_engineering_with_smactr_and_crisp_ml_q_1748539923607527">
<h1>Aligning AI Engineering with SMACTR and CRISP-ML(Q) <span class="keep-together">for Transparency</span></h1>

<p>As discussed earlier, a conformity assessment is a formal process that requires a third-party audit. However, this audit is typically conducted after deployment, when the system might already have negatively impacted users. Inioluwa Deborah Raji et al. address this in their widely cited paper <a href="https://oreil.ly/9RVsg">“Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing”</a>, in which they propose an internal audit framework designed to guide the practical implementation of ethical AI development: SMACTR (Scoping, Mapping, Artifact Collection, Testing, and Reflection)<a contenteditable="false" data-primary="SMACTR framework" data-type="indexterm" id="smactr-1"/>. SMACTR provides a structured approach for organizations to ensure that their AI systems align with their stated ethical principles and values. The framework is intended to be used throughout the entire AI system development lifecycle, not just at a single point in time, and it aids organizations in preparing effectively for third-party conformity audits.</p>

<section data-pdf-bookmark="The Five Stages of the SMACTR Framework" data-type="sect2"><div class="sect2" id="chapter_6_the_five_stages_of_the_smactr_framework_1748539923607656">
<h2>The Five Stages of the SMACTR Framework</h2>

<p>Let’s start by examining the five stages of the SMACTR<a contenteditable="false" data-primary="SMACTR framework" data-secondary="five stages of" data-tertiary="Scoping" data-type="indexterm" id="id590"/><a contenteditable="false" data-primary="SMACTR framework" data-secondary="five stages of" data-tertiary="Mapping" data-type="indexterm" id="id591"/><a contenteditable="false" data-primary="SMACTR framework" data-secondary="five stages of" data-tertiary="Artifact Collection" data-type="indexterm" id="id592"/><a contenteditable="false" data-primary="SMACTR framework" data-secondary="five stages of" data-tertiary="Testing" data-type="indexterm" id="id593"/><a contenteditable="false" data-primary="SMACTR framework" data-secondary="five stages of" data-tertiary="Reflection" data-type="indexterm" id="id594"/> framework<a contenteditable="false" data-primary="SMACTR framework" data-secondary="five stages of" data-type="indexterm" id="smactr-5-1"/>:</p>

<dl>
	<dt>Scoping</dt>
	<dd>
	<p>Clarifies the objectives of the audit by reviewing the motivations and intended impact of the system and confirming the principles meant to guide system development. This stage also involves mapping out use cases and identifying analogous deployments to anticipate potential harm.</p>
	</dd>
	<dt>Mapping</dt>
	<dd>
	<p>Connects the documents produced during the Scoping phase to the stakeholders that will be involved in testing and reviewing. FMEA should begin in this stage (for more on FMEA, see <a data-type="xref" href="ch05.html#chapter_5_failure_mode_and_effects_analysis_1748539922577786">“Failure Mode and Effects Analysis”</a>).</p>
	</dd>
	<dt>Artifact Collection</dt>
	<dd>
	<p>Gathers key documents and evidence needed to support ethical and technical evaluation, including an ethical review of the system’s use case and a social impact assessment. Prerequisite documents from system and engineering teams typically include a declaration of ethical objectives and a product requirements document (PRD).</p>
	</dd>
	<dt>Testing</dt>
	<dd>
	<p>Involves a thorough audit of the model’s performance, considering both technical and ethical aspects.</p>
	</dd>
	<dt>Reflection</dt>
	<dd>
	<p>Focuses on reviewing the entire development process and identifying opportunities for improvement in the future.</p>
	</dd>
</dl>

<p>As visualized in <a data-type="xref" href="#chapter_6_figure_2_1748539923579896">Figure 6-2</a>, each stage has its own set of documentation requirements, which help ensure the framework supports accountability and transparency.</p>

<figure><div class="figure" id="chapter_6_figure_2_1748539923579896"><img src="assets/taie_0602.png"/>
<h6><span class="label">Figure 6-2. </span>Overview of the SMACTR framework (source: <a href="https://oreil.ly/9RVsg"><em>https://oreil.ly/9RVsg</em></a>)</h6>
</div></figure>

<p><a data-type="xref" href="#chapter_6_table_3_1748539923588765">Table 6-3</a> summarizes the purpose, key activities, and expected outcomes of each stage, as outlined in the paper introducing the framework<a contenteditable="false" data-primary="SMACTR framework" data-secondary="five stages of" data-tertiary="purpose, activities, and outcomes of" data-type="indexterm" id="id595"/>.</p>

<table class="striped" id="chapter_6_table_3_1748539923588765">
	<caption><span class="label">Table 6-3. </span>The stages of the SMACTR framework</caption>
	<thead>
		<tr>
			<th>SMACTR stage</th>
			<th>Purpose</th>
			<th>Activities</th>
			<th>Outcomes</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>Scoping</td>
			<td>
			<p>Clarify the audit’s objective by reviewing the AI system’s motivations and intended impact. Define the project’s scope and the ethical principles that should guide its development.</p>
			</td>
			<td>
			<ul>
				<li>Review the product or request document specifying the AI system’s requirements and expectations.</li>
				<li>Identify the intended use cases and potential areas of harm or social impact.</li>
				<li>Map out analogous deployments either within the organization or from competitors to anticipate potential issues.</li>
				<li>Confirm the ethical principles and values that are meant to guide product development.</li>
				<li>Define a risk analysis centered on the failure to achieve the ethical principles.</li>
			</ul>
			</td>
			<td>
			<p>A clear understanding of the audit’s objectives, the intended impact of the AI system, and the ethical guidelines that should be followed. Key documentation to create includes AI system (project) scope documents, initial risk assessments, and ethical review reports.</p>
			</td>
		</tr>
		<tr>
			<td>Mapping</td>
			<td>
			<p>Identify all relevant stakeholders to ensure that the audit considers the perspectives and interests of all parties involved, including users, developers, and other affected groups.</p>
			</td>
			<td>
			<ul>
				<li>Identify all relevant stakeholders who may be affected by the AI system.</li>
				<li>Determine the potential impact of the system on each stakeholder group.</li>
			</ul>
			</td>
			<td>
			<p>A comprehensive stakeholder map and a clear understanding of the potential impacts of the AI system on different groups.</p>
			</td>
		</tr>
		<tr>
			<td>Artifact Collection</td>
			<td>
			<p>Gather all relevant documentation and data related to the development and deployment of the AI system. This information is essential for the auditors to thoroughly evaluate the system and identify potential risks and ethical concerns.</p>
			</td>
			<td>
			<ul>
				<li>Collect project-related documents, technical specifications, and relevant data.</li>
				<li>Gather information about data sources, data preparation steps, model training details, and evaluation results.</li>
				<li>Gather documentation related to the system’s development.</li>
			</ul>
			</td>
			<td>
			<p>A complete set of documents, data, and other artifacts related to the AI system, which facilitates analysis. This documentation may include data quality assessments, model cards, training logs, and performance reports.</p>
			</td>
		</tr>
		<tr>
			<td>Testing</td>
			<td>
			<p>Evaluate the AI system’s performance and identify any potential issues or risks that may arise during its operation. This goes beyond standard technical testing to include ethical implications and biases.</p>
			</td>
			<td>
			<ul>
				<li>Conduct technical tests to assess the model’s accuracy, robustness, and reliability.</li>
				<li>Evaluate the model’s fairness and identify any potential biases that may lead to discriminatory outcomes.</li>
				<li>Assess the model’s performance under various conditions, including edge cases and potential adversarial scenarios.</li>
				<li>Use both technical metrics and ethical considerations for evaluation.</li>
			</ul>
			</td>
			<td>
			<p>A thorough understanding of the system’s performance, limitations, and potential risks, focusing on both technical and ethical aspects. This includes performance reports and bias detection analysis.</p>
			</td>
		</tr>
		<tr>
			<td>Reflection</td>
			<td>
			<p>Analyze the results from the previous phase and evaluate them in light of the original scoping goals. This step focuses on summarizing findings, assessing how well ethical risks were addressed, and proposing improvements for the future.</p>
			</td>
			<td>
			<ul>
				<li>Evaluate the outcomes of the Testing stage with regard to the project’s original scoping goals.</li>
				<li>Identify any ethical principles that may be jeopardized when the AI system is deployed.</li>
				<li>Based on the test results, formulate recommendations for further mitigating identified risks and ensuring responsible use of the system.</li>
				<li>Develop a risk mitigation plan in collaboration with engineering teams.</li>
			</ul>
			</td>
			<td>
			<p>An algorithmic use-related risk analysis, including a risk mitigation plan that addresses the identified failures and potential risks. This will include a reflection on the ethical implications of deploying the AI system.</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="How the SMACTR Framework Aligns with the EU AI Act" data-type="sect2"><div class="sect2" id="chapter_6_how_the_smactr_framework_aligns_with_the_eu_ai_act_1748539923607723">
<h2>How the SMACTR Framework Aligns with the EU AI Act</h2>

<p>The SMACTR<a contenteditable="false" data-primary="SMACTR framework" data-secondary="five stages of" data-startref="smactr-5-1" data-type="indexterm" id="id596"/> framework is<a contenteditable="false" data-primary="SMACTR framework" data-secondary="alignment with the EU AI Act" data-type="indexterm" id="smactr-align-eua-1"/> intended for pre-deployment auditing and can be used to support compliance with the EU AI Act. Its emphasis on embedding ethical considerations throughout the AI development lifecycle directly aligns with the Act’s objectives and requirements. Let’s look at some of the key areas of alignment:</p>

<dl>
	<dt>Proactive and preventative auditing</dt>
	<dd>
	<p>The SMACTR framework promotes a proactive approach to ethical AI development by integrating audit processes into the design and development phases. This is in contrast with traditional external audits, which often occur after deployment, when risks may have already caused harm. This preventive orientation aligns well with the EU AI Act’s requirements for risk assessment and mitigation: by using SMACTR, AI engineers can anticipate and address potential risks before they become problems. While the framework wasn’t developed specifically for the EU AI Act, its focus on “urgent governance,” which emphasizes auditing not only for system reliability but also for societal harm, makes it a good fit for the Act’s goal of protecting fundamental rights.</p>
	</dd>
	<dt>Emphasis on transparency and documentation</dt>
	<dd>
	<p>SMACTR produces structured documentation at each stage, forming a comprehensive audit trail. This aligns with the EU AI Act’s requirements for comprehensive technical documentation and traceability, particularly for high-risk AI systems. The documentation that is generated, including artifacts such as model cards and datasheets, supports information sharing along the AI value chain and can serve as evidence of due diligence in addressing ethical concerns and meeting regulatory requirements.</p>
	</dd>
	<dt class="pagebreak-before">Alignment with ethical principles and values</dt>
	<dd>
	<p>The EU AI Act is grounded in the concept of trustworthy AI and builds on the ethical guidelines developed by the High-Level Expert Group on AI. The SMACTR framework is designed to help organizations align their AI systems with their stated ethical principles and values and enables them to evaluate how well the systems uphold those principles in practice.</p>
	</dd>
	<dt>Risk assessment integration</dt>
	<dd>
	<p>The Scoping stage of the framework involves a structured risk analysis that goes beyond technical reliability, mapping use cases and identifying potential sources of harm and social impact. Tools such as FMEA and anticipatory “What if…?” questions support this process. This broader risk-based approach, which incorporates ethical and societal considerations, aligns well with the EU AI Act’s emphasis on comprehensive risk management.</p>
	</dd>
	<dt>Accountability and responsibility</dt>
	<dd>
	<p>SMACTR explicitly promotes accountability by defining roles and responsibilities throughout the AI development lifecycle. This aligns with the EU AI Act’s emphasis on traceability, transparency, and explainability and defined obligations for providers, deployers, and other stakeholders. The Mapping stage of the framework, in particular, can help establish an internal record of individual accountability.</p>
	</dd>
</dl>

<p>As you can see, the SMACTR framework is not only a tool for ethical AI development but also a practical mechanism for supporting compliance with the EU AI Act. Its focus on proactive auditing, detailed documentation, alignment with ethical principles, integrated risk assessment, and accountability makes it especially suitable for guiding AI engineering teams in meeting the Act’s regulatory requirements. The framework can be implemented in full or in a streamlined form, depending on the desired level of rigor. It can also be adapted to different risk levels and specific system requirements, making it applicable across a wide range of use cases.</p>

<p>Crucially, SMACTR can be integrated with AI engineering practices to ensure that ethical and transparency considerations are addressed throughout the AI system lifecycle. By integrating SMACTR with the CRISP-ML(Q) methodology<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="integration with SMACTR framework" data-type="indexterm" id="crisp-smactr-1"/>, AI engineers can embed transparency and compliance measures in all stages of development.</p>

<p>The following sections outline how to integrate SMACTR<a contenteditable="false" data-primary="SMACTR framework" data-secondary="integration with CRISP-ML(Q)" data-type="indexterm" id="smactr-crisp-integ-1"/> into the CRISP-ML(Q) lifecycle and provide a step-by-step engineering guide to help organizations meet transparency obligations under the EU AI Act. For each phase of the lifecycle, I’ll start by examining the relevant transparency requirements, then describe the corresponding CRISP-ML(Q) activities, show how SMACTR<a contenteditable="false" data-primary="SMACTR framework" data-startref="smactr-1" data-type="indexterm" id="id597"/> can be applied, and identify the key artifacts that support compliance and traceability<a contenteditable="false" data-primary="SMACTR framework" data-secondary="alignment with the EU AI Act" data-startref="smactr-align-eua-1" data-type="indexterm" id="id598"/>.</p>
</div></section>

<section data-pdf-bookmark="Business and Data Understanding Phase" data-type="sect2"><div class="sect2" id="chapter_6_business_and_data_understanding_phase_1748539923607787">
<h2>Business and Data Understanding Phase</h2>

<p>In the business and data understanding phase, several transparency requirements need to be addressed to ensure that users are aware when they are interacting with an AI system and that synthetic content is clearly identified. Key requirements in this phase include:</p>

<dl>
	<dt>Document the intended purpose of the AI system</dt>
	<dd>
	<p>Clearly document the intended purpose of the AI system, specifying what it is designed to do and the problem it is meant to solve.</p>
	</dd>
	<dt>Define interaction points with humans</dt>
	<dd>
	<p>Identify all points where the AI system will interact directly with humans. This includes user interfaces, chatbots, or any other mechanism where a natural person will engage with the system.</p>
	</dd>
	<dt>Identify synthetic content generation capabilities</dt>
	<dd>
	<p>Determine whether the AI system can generate or manipulate audio, image, video, or text content. If so, document this capability, as any synthetic output must be clearly marked.</p>
	</dd>
	<dt>Plan notification mechanisms</dt>
	<dd>
	<p>Develop a plan for notifying users that they are interacting with an AI system. This may involve implementing visible notifications, clear labeling, and other accessible methods.</p>
	</dd>
</dl>

<section data-pdf-bookmark="CRISP-ML(Q) activities" data-type="sect3"><div class="sect3" id="chapter_6_crisp_ml_q_activities_1748539923607851">
<h3>CRISP-ML(Q) activities</h3>

<p>In the business and data understanding phase, the following CRISP-ML(Q) activities are crucial:</p>

<dl>
	<dt>Define scope and success criteria</dt>
	<dd>
	<p>Clearly define the project’s goals and the criteria for success. This includes identifying measurable outcomes and establishing KPIs for the AI system.</p>
	</dd>
	<dt>Assess data availability and quality</dt>
	<dd>
	<p>Evaluate the data available for training and validation, considering its quantity, relevance, accuracy, and completeness.</p>
	</dd>
	<dt>Evaluate feasibility</dt>
	<dd>
	<p>Assess the technical and operational feasibility of the project to determine whether its goals are achievable with the available resources and technologies.</p>
	</dd>
	<dt>Design data collection strategy</dt>
	<dd>
	<p>If there’s not enough existing data, design a strategy for collecting additional data. Specify how new data will be sourced, stored, and managed.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_6_smactr_integration_1748539923607912">
<h3>SMACTR integration</h3>

<p>The SMACTR framework enhances the<a contenteditable="false" data-primary="SMACTR framework" data-secondary="integration with CRISP-ML(Q)" data-startref="smactr-crisp-integ-1" data-type="indexterm" id="id599"/> CRISP-ML(Q) process<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="integration with SMACTR framework" data-startref="crisp-smactr-1" data-type="indexterm" id="id600"/> by integrating ethical considerations and auditability from the beginning. The Scoping, Mapping, and Artifact Collection stages are relevant to the business and data understanding phase. Here are the key tasks:</p>

<dl>
	<dt>Scoping (ethical review of system use case)</dt>
	<dd>
	<p>Conduct an ethical review of the AI system’s intended use. This involves considering who might be affected by the system and what the potential social impacts might be.</p>
	</dd>
	<dt>Mapping (stakeholder identification)</dt>
	<dd>
	<p>Identify all internal and external stakeholders who will be involved in or affected by the AI system. This could include developers, product managers, end users, and individuals whose rights may be impacted.</p>
	</dd>
	<dt>Artifact Collection (initial documentation gathering)</dt>
	<dd>
	<p>Begin gathering initial documentation for the AI system. This includes project proposals, requirements documents, and statements of ethical objectives.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Key artifacts" data-type="sect3"><div class="sect3" id="chapter_6_key_artifacts_1748539923607969">
<h3>Key artifacts</h3>

<p>By the end of the business<a contenteditable="false" data-primary="transparency obligations" data-secondary="key artifacts" data-type="indexterm" id="trans-obl-key-art-1"/> and data understanding phase, the following key artifacts should be produced to meet the requirements of the EU AI Act:</p>

<dl>
	<dt>Project scope document</dt>
	<dd>
	<p>A document that clearly defines the project’s objectives, scope, and success criteria. This should include details about what the AI system aims to achieve and the boundaries of the project.</p>
	</dd>
	<dt>Ethical review report</dt>
	<dd>
	<p>A report that outlines the ethical implications of the AI system, including potential risks and harms, and documents whether the system aligns with a set of ethical values or principles.</p>
	</dd>
	<dt>Stakeholder map</dt>
	<dd>
	<p>A visual representation of all stakeholders, showing their roles and relationships to the AI system. This map should clarify participant dynamics and provide context for interpreting the final audit report.</p>
	</dd>
	<dt>Data quality assessment</dt>
	<dd>
	<p>A report assessing the availability, quality, and suitability of the data for the project. This document should identify any data gaps or potential biases that need to be addressed in the next phase.</p>
	</dd>
	<dt>Initial transparency requirements document</dt>
	<dd>
	<p>A document that outlines the preliminary transparency requirements based on Article 50. This includes interaction points with humans, any synthetic content generation, and planned notification mechanisms.</p>
	</dd>
</dl>

<p><a data-type="xref" href="#chapter_6_table_4_1748539923588788">Table 6-4</a> summarizes the content of these artifacts.</p>

<table class="striped" id="chapter_6_table_4_1748539923588788">
	<caption><span class="label">Table 6-4. </span>Summary of key artifacts produced during the business and <span class="keep-together">data understanding phase</span></caption>
	<thead>
		<tr>
			<th>Artifact</th>
			<th>Content</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Project scope document</p>
			</td>
			<td>
			<ul>
				<li>Project objectives</li>
				<li>Success criteria</li>
				<li>Resource requirements</li>
				<li>Timeline</li>
				<li>Constraints</li>
				<li>Dependencies</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Ethical review report</p>
			</td>
			<td>
			<ul>
				<li>Impact assessment</li>
				<li>Risk analysis</li>
				<li>Mitigation strategies</li>
				<li>Compliance evaluation</li>
				<li>Ethics board recommendations</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Stakeholder map</p>
			</td>
			<td>
			<ul>
				<li>Involved parties</li>
				<li>Roles and responsibilities</li>
				<li>Communication paths</li>
				<li>Decision authority levels</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data quality assessment (assessment criteria)</p>
			</td>
			<td>
			<ul>
				<li>Completeness</li>
				<li>Accuracy</li>
				<li>Consistency</li>
				<li>Timeliness</li>
				<li>Privacy compliance</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Initial transparency requirements document</p>
			</td>
			<td>
			<ul>
				<li>System purpose declaration</li>
				<li>Human interaction points</li>
				<li>Synthetic content capabilities</li>
				<li>Notification mechanisms</li>
				<li>Documentation requirements</li>
				<li>Compliance checklist</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>

<section data-pdf-bookmark="Data Preparation Phase" data-type="sect2"><div class="sect2" id="chapter_6_data_preparation_phase_1748539923608033">
<h2>Data Preparation Phase</h2>

<p>In<a contenteditable="false" data-primary="transparency obligations" data-secondary="key artifacts" data-startref="trans-obl-key-art-1" data-type="indexterm" id="id601"/> the data preparation phase, the focus is on guaranteeing that the data used to train the AI system is well documented and auditable. This is crucial for transparency, accountability, and enabling deployers and users to understand how the AI system makes decisions. Here’s how the transparency requirements of Article 50 translate into practical steps in this phase:</p>

<dl>
	<dt>Implement data lineage tracking</dt>
	<dd>
	<p>Establish a system for tracking the origin and data flows. This includes recording where the data comes from, any transformations it undergoes, and how it is used. This is important to ensure that the data used by the system can be traced back to its original source, which can help to identify and correct potential issues. Technically, this requires components such as source system tracking, transformation history, and version control integration.</p>
	</dd>
	<dt>Document data transformations</dt>
	<dd>
	<p>All transformations applied to the data, such as cleaning, normalization, or aggregation, must be clearly documented. The documentation should describe what transformations were applied, why they were applied, and their impact on the data.</p>
	</dd>
	<dt>Establish audit trail</dt>
	<dd>
	<p>Create a robust audit trail by logging all data preparation activities, including who made the changes, when they were made, and what specific changes were implemented. An audit trail ensures that there is a record of all data preparation steps, which is essential for maintaining accountability and demonstrating compliance with transparency requirements.</p>
	</dd>
	<dt>Create metadata management system</dt>
	<dd>
	<p>Develop a system for managing metadata about the data, such as its format, type, source, and any associated quality metrics. A well-structured metadata management system is key for understanding the data and its characteristics and enables better data governance. It also supports the traceability and reusability of ML assets, such as data, features, and models.</p>
	</dd>
</dl>

<section data-pdf-bookmark="CRISP-ML(Q) activities" data-type="sect3"><div class="sect3" id="chapter_6_crisp_ml_q_activities_1748539923608093">
<h3>CRISP-ML(Q) activities</h3>

<p>The key activities<a contenteditable="false" data-primary="CRISP-ML(Q)" data-secondary="data preparation phase" data-type="indexterm" id="id602"/> for a structured approach to data preparation within machine learning projects during this phase include:</p>

<dl>
	<dt>Data cleaning and validation</dt>
	<dd>
	<p>Identify and correct any errors or inconsistencies in the data. This includes handling missing values, correcting typos, and ensuring data conforms to the defined standards.</p>
	</dd>
	<dt>Feature engineering</dt>
	<dd>
	<p>Create new features from the existing data that are relevant to the modeling task. This might involve transforming existing variables, creating interaction terms, or extracting other meaningful information to enhance model performance.</p>
	</dd>
	<dt>Data transformation</dt>
	<dd>
	<p>Convert the data from its raw format into a suitable format for machine learning algorithms. This may involve normalizing numerical data or encoding categorical data, for example.</p>
	</dd>
	<dt>Data documentation</dt>
	<dd>
	<p>Record the characteristics of the data including its sources, data types, and limitations, and document any issues or biases present in the data.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_6_smactr_integration_1748539923608144">
<h3>SMACTR integration</h3>

<p>The Artifact Collection, Testing, and Reflection stages of the SMACTR framework can be integrated into all the remaining CRISP-ML(Q) phases. Here are the key activities for the data preparation phase:</p>

<dl>
	<dt>Artifact Collection (data documentation)</dt>
	<dd>
	<p>Create detailed documentation about the data and all processing steps. This should go beyond basic recordkeeping.</p>
	</dd>
	<dt>Testing (initial data quality verification)</dt>
	<dd>
	<p>Perform rigorous validation of data quality and transformation effectiveness to ensure that the data preparation process meets both technical requirements and compliance standards and that it is aligned with transparency and ethical goals.</p>
	</dd>
	<dt>Reflection (data preparation impact analysis)</dt>
	<dd>
	<p>Consider the broader implications of the data preparation process. Analyze how each transformation might affect different groups or communities, and reflect on potential ethical concerns such as bias or unfair outcomes.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Key artifacts" data-type="sect3"><div class="sect3" id="chapter_6_key_artifacts_1748539923608199">
<h3>Key artifacts</h3>

<p>By the end of the data preparation phase, several key artifacts should be produced to meet the requirements of the EU AI Act. These include:</p>

<dl>
	<dt>Data quality reports</dt>
	<dd>
	<p>Detailed reports on the quality of the data, highlighting any issues or limitations. These reports should specify what metrics were used, what problems were found, and how they were resolved.</p>
	</dd>
	<dt>Feature documentation</dt>
	<dd>
	<p>Clear descriptions of all features used in the model, including how they were engineered and what they represent. This documentation should also explain the rationale behind the feature engineering choices.</p>
	</dd>
	<dt>Data lineage documentation</dt>
	<dd>
	<p>Records that trace the origin of the data and any transformations applied. This documentation should show the entire path the data takes, from its source to its final use in the system.</p>
	</dd>
	<dt>Transformation logs</dt>
	<dd>
	<p>Detailed logs of all data transformations, including the date, time, user, and specifics of each change. These logs serve as a verifiable record of the data transformation process.</p>
	</dd>
	<dt>Metadata schema</dt>
	<dd>
	<p>A defined structure and format for metadata, ensuring consistency in how data is documented. The metadata schema provides a clear framework for classifying and tracking data.</p>
	</dd>
</dl>

<p><a data-type="xref" href="#chapter_6_table_5_1748539923588811">Table 6-5</a> provides an overview of the content of each of these artifacts.</p>

<table class="striped" id="chapter_6_table_5_1748539923588811">
	<caption><span class="label">Table 6-5. </span>Summary of key artifacts produced during the data preparation phase</caption>
	<thead>
		<tr>
			<th>Artifact</th>
			<th>Content</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Data quality reports</p>
			</td>
			<td>
			<p>Validation checks:</p>

			<ul>
				<li>Schema validation</li>
				<li>Business rule compliance</li>
				<li>Constraint verification</li>
				<li>Format consistency</li>
			</ul>

			<p>Data quality assessments, e.g.:</p>

			<ul>
				<li>Completeness: (metric: null percentage; threshold: &lt;0.05; validation result: pass/fail)</li>
				<li>Accuracy: (metric: error rate; threshold: &lt;0.01; validation result: pass/fail)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Feature documentation</p>
			</td>
			<td>
			<ul>
				<li>Feature name</li>
				<li>Description (business meaning)</li>
				<li>Source (original data source)</li>
				<li>Transformations (list of applied transformations)</li>
				<li>Dependencies (parent features)</li>
				<li>Validation rules (business constraints)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data lineage documentation</p>
			</td>
			<td>
			<ul>
				<li>Operation ID</li>
				<li>Timestamp</li>
				<li>Data transformation type</li>
				<li>Input data: source (input location), version</li>
				<li>Output data: destination (output location), version</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Transformation logs</p>
			</td>
			<td>
			<ul>
				<li>Transform ID</li>
				<li>Method: transformation function, input parameters, validation results</li>
				<li>Metadata: timestamp, responsible user, code version</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Metadata schema</p>
			</td>
			<td>
			<ul>
				<li>Technical metadata: data types, schema version, storage format</li>
				<li>Business metadata: owners, update frequency, sensitivity level</li>
				<li>Operational metadata: processing history, quality metrics, dependency map</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>

<section data-pdf-bookmark="Modeling Phase" data-type="sect2"><div class="sect2" id="chapter_6_modeling_phase_1748539923608262">
<h2>Modeling Phase</h2>

<p>The core focus of the CRISP-ML(Q) modeling phase is to select and train models, optimize hyperparameters, and evaluate model performance. The following actions support compliance with Article 50’s transparency requirements in this phase, ensuring that an AI system’s decision-making process is interpretable, traceable, and <span class="keep-together">reproducible:</span></p>

<dl>
	<dt>Implement version control for models</dt>
	<dd>
	<p>Use a version control system (such as Git) to track all changes made to the model’s code, parameters, and configurations. This not only enables reproducibility but also provides the ability to revert to previous versions when necessary.</p>
	</dd>
	<dt>Document model behavior and intent</dt>
	<dd>
	<p>Develop comprehensive documentation that clearly explains how the model works, its intended applications, and its limitations. This should include details about the model’s architecture, the training process, the datasets used, and evaluation methodology. In the previous chapter, I gave a detailed overview of the types of documentation that can be created for AI systems. For example, model cards can be used to provide a concise summary of the model’s key characteristics (intended use, performance metrics, limitations, ethical considerations, and an overview of the evaluation data, scope, and associated risks).</p>
	</dd>
	<dt>Establish explainability mechanisms</dt>
	<dd>
	<p>Incorporate explainability tools or techniques to enhance understanding of the model’s decision-making process. For example, you might use feature importance analysis or surrogate models to provide insights into how the model arrives at its predictions.</p>
	</dd>
</dl>

<section data-pdf-bookmark="CRISP-ML(Q) activities" data-type="sect3"><div class="sect3" id="chapter_6_crisp_ml_q_activities_1748539923608321">
<h3>CRISP-ML(Q) activities</h3>

<p>The key activities during the modeling phase include:</p>

<dl>
	<dt>Model selection</dt>
	<dd>
	<p>Select the most appropriate model architecture based on the problem definition and requirements.</p>
	</dd>
	<dt>Training and validation</dt>
	<dd>
	<p>Train the selected model using the prepared data, and use a hold-out validation set to fine-tune hyperparameters and evaluate its performance. Document the entire training process.</p>
	</dd>
	<dt>Performance evaluation</dt>
	<dd>
	<p>Assess the model’s performance using appropriate metrics, focusing on both technical accuracy and fairness across different subgroups.</p>
	</dd>
	<dt>Model optimization</dt>
	<dd>
	<p>Fine-tune the model to improve performance, considering factors such as accuracy, fairness, and robustness.</p>
	</dd>
</dl>

<p>Make sure to thoroughly document all of these processes, along with the rationale behind all of your decisions.</p>
</div></section>

<section data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_6_smactr_integration_1748539923608375">
<h3>SMACTR integration</h3>

<p>The following activities show how the SMACTR stages of Artifact Collection, Testing, and Reflection can be integrated into the modeling phase:</p>

<dl>
	<dt>Artifact Collection</dt>
	<dd>
	<p>Document all relevant information about the model, including its architecture, training details, and performance metrics.</p>
	</dd>
	<dt>Testing</dt>
	<dd>
	<p>Thoroughly evaluate the model’s performance, considering both technical and ethical aspects. Include tests for fairness, accuracy, and robustness across different subgroups.</p>
	</dd>
	<dt>Reflection</dt>
	<dd>
	<p>Continuously assess the broader ethical implications of the model, including potential impacts on fundamental rights and societal outcomes. Document the model selection rationale and the ethical considerations behind modeling choices.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Key artifacts" data-type="sect3"><div class="sect3" id="chapter_6_key_artifacts_1748539923608429">
<h3>Key artifacts</h3>

<p><a data-type="xref" href="#chapter_6_table_6_1748539923588831">Table 6-6</a> lists the artifacts that should be generated and maintained during the modeling phase.</p>

<table class="striped" id="chapter_6_table_6_1748539923588831">
	<caption><span class="label">Table 6-6. </span>Summary of key artifacts produced during the modeling phase</caption>
	<thead>
		<tr>
			<th>Artifact</th>
			<th>Content</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Model cards</p>
			</td>
			<td>
			<p>Concise documents summarizing the model’s intended use, performance metrics, limitations, and ethical considerations. (See the <a href="https://oreil.ly/5fcMl">Model Card Toolkit</a>.)</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Training logs</p>
			</td>
			<td>
			<p>Comprehensive records of the training process, including hyperparameters, data used, and performance metrics at each stage.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Performance reports</p>
			</td>
			<td>
			<p>Detailed analyses of the model’s performance, including accuracy, precision, recall, and other relevant metrics. Also include results from fairness testing and bias detection and mitigation strategies.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Explainability documentation</p>
			</td>
			<td>
			<p>Structured summaries of the methods used for model interpretation and the insights gained from them.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Version control records</p>
			</td>
			<td>
			<p>Logs from the version control system showing all changes made to the model’s code and configurations. This ensures full traceability and enables reproducibility.</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="Technical implementation guide" data-type="sect3"><div class="sect3" id="chapter_6_technical_implementation_guide_1748539923608485">
<h3>Technical implementation guide</h3>

<p>The following infrastructure components enable teams to implement the practices outlined in the previous sections and support the transparency requirements of <span class="keep-together">Article 50</span>:</p>

<dl>
	<dt>Version control</dt>
	<dd>
	<p>Use a version control system such as Git to track changes to data, models, and code, to ensure transparency and facilitate rollbacks. You may want to adopt a dual tracking approach, using DVC for data versioning and MLflow for model versioning.</p>
	</dd>
	<dt>Metadata tracking system</dt>
	<dd>
	<p>Implement a robust metadata tracking system to record model versions, training data, evaluation results, and explainability information. The components of this system will include:</p>

	<ul>
		<li>
		<p>A scalable database (e.g., PostgreSQL) for structured metadata</p>
		</li>
		<li>
		<p>A document store (e.g., MongoDB) for unstructured data and logs</p>
		</li>
		<li>
		<p>A searchable index (e.g., Elasticsearch) for quick retrieval</p>
		</li>
	</ul>
	</dd>
	<dt>MLOps platform</dt>
	<dd>
	<p>Use an MLOps platform that supports versioning, experiment tracking, and pipeline automation. Options include Managed MLflow and Unity Catalog (Databricks), Weights &amp; Biases, and Metaflow, to name a few.</p>
	</dd>
</dl>
</div></section>
</div></section>

<section data-pdf-bookmark="Evaluation Phase" data-type="sect2"><div class="sect2" id="chapter_6_evaluation_phase_1748539923608549">
<h2>Evaluation Phase</h2>

<p>The CRISP-ML(Q) evaluation phase focuses on validating the AI model’s performance on unseen data and refining the model as needed prior to deployment. To meet the transparency obligations<a contenteditable="false" data-primary="transparency obligations" data-type="indexterm" id="id603"/> outlined in Article 50 of the EU AI Act, the following practical steps should be implemented:</p>

<dl>
	<dt>Record and track model predictions and confidence levels</dt>
	<dd>
	<p>Establish a system that logs all model predictions along with their associated confidence scores. This system should support easy retrieval and analysis to help users understand how the AI system generates its outputs. In addition to the predictions themselves, log any intermediate steps or features that contribute to these outcomes.</p>
	</dd>
	<dt>Verify notification mechanisms</dt>
	<dd>
	<p>Evaluate whether the system clearly informs users that they are interacting with an AI. The notification should describe the system’s purpose, data sources, and known limitations. Test the effectiveness and visibility of the disclosure in the user interface, and make sure it is accessible to all users.</p>
	</dd>
	<dt>Test content marking processes</dt>
	<dd>
	<p>If the AI system generates synthetic content (audio, images, videos, or text), it must have the capability to mark the content as artificial in a machine-readable format. Verify that these markers are detectable and compliant with deepfake disclosure requirements, if applicable, to enhance transparency and promote accountability.</p>
	</dd>
	<dt>Validate documentation completeness</dt>
	<dd>
	<p>Ensure all documentation is complete and up-to-date, including information on the intended use of the AI system, data sources, model cards, training logs, and performance reports. The documentation should be crafted in a way that is easily understandable for both deployers and end users, facilitating smooth communication and comprehension of the system’s functionality and purpose.</p>
	</dd>
</dl>

<section data-pdf-bookmark="CRISP-ML(Q) activities" data-type="sect3"><div class="sect3" id="chapter_6_crisp_ml_q_activities_1748539923608613">
<h3>CRISP-ML(Q) activities</h3>

<p>The main activities of the evaluation phase are:</p>

<dl>
	<dt>Model validation</dt>
	<dd>
	<p>Assess the model’s accuracy and generalization ability by using use case–specific metrics on unseen data. Employing techniques such as cross-validation can improve the robustness of the model, enabling a more reliable evaluation of its capabilities.</p>
	</dd>
	<dt>Performance assessment</dt>
	<dd>
	<p>When evaluating a model’s performance, it’s essential to consider not just accuracy but also fairness, robustness, and explainability. It’s also important to track the following:</p>

	<ul>
		<li>
		<p>Predictive performance across different continuous-training executions</p>
		</li>
		<li>
		<p>Metadata and artifacts generated throughout the pipeline to support debugging, reproducibility, and lineage analysis (the ability to track a trained model back to its training dataset, including all intermediate artifacts and metadata)</p>
		</li>
		<li>
		<p>Hyperparameters used during training</p>
		</li>
		<li>
		<p>All evaluations performed by the pipeline</p>
		</li>
		<li>
		<p>Processed data snapshots after transformation steps, if feasible</p>
		</li>
		<li>
		<p>Data summaries such as descriptive statistics, schemas, and feature <span class="keep-together">distributions</span></p>
		</li>
	</ul>

	<p>Documenting these metrics in the evaluation report and model card enhances transparency. Aligning them with the model’s intended use is crucial for producing relevant and meaningful assessments.</p>
	</dd>
	<dt>Business goal alignment</dt>
	<dd>
	<p>Verify that the model’s performance aligns with the original business goals and success criteria. Confirm that the model’s outputs are relevant and valuable for the intended use case.</p>
	</dd>
	<dt>Compliance verification</dt>
	<dd>
	<p>Verify that the system’s functionalities and documentation comply with the EU AI Act and any other applicable laws or industry standards.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_6_smactr_integration_1748539923608667">
<h3>SMACTR integration</h3>

<p>Here are the main SMACTR integration activities for the evaluation phase:</p>

<dl>
	<dt>Artifact Collection (test results)</dt>
	<dd>
	<ul>
		<li>
		<p>Gather all test results and performance data to be used in the impact assessment.</p>
		</li>
		<li>
		<p>Ensure all AI system artifacts are well documented and clearly presented.</p>
		</li>
		<li>
		<p>Collect comprehensive data on the model’s behavior and performance under a variety of conditions.</p>
		</li>
	</ul>
	</dd>
	<dt>Testing (comprehensive system testing)</dt>
	<dd>
	<ul>
		<li>
		<p>Conduct thorough testing of the entire AI system, including the model, data pipelines, and user interfaces. The tests should cover a range of scenarios, including edge cases and potential adversarial inputs.</p>
		</li>
		<li>
		<p>Ensure all tests are documented and results are properly recorded.</p>
		</li>
	</ul>
	</dd>
	<dt>Reflection (system impact analysis)</dt>
	<dd>
	<ul>
		<li>
		<p>Document potential risks and limitations, including sources of bias and fairness concerns.</p>
		</li>
		<li>
		<p>Assess whether the AI system aligns with the organization’s defined ethical <span class="keep-together">principles</span>.</p>
		</li>
	</ul>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Key artifacts" data-type="sect3"><div class="sect3" id="chapter_6_key_artifacts_1748539923608721">
<h3>Key artifacts</h3>

<p><a data-type="xref" href="#chapter_6_table_7_1748539923588851">Table 6-7</a> provides a summary of the artifacts that should be produced during this phase.</p>

<table class="striped" id="chapter_6_table_7_1748539923588851">
	<caption><span class="label">Table 6-7. </span>Summary of key artifacts produced during the evaluation phase</caption>
	<thead>
		<tr>
			<th>Artifact</th>
			<th>Content</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Evaluation reports</p>
			</td>
			<td>
			<p>Document all evaluation activities, including model validation, performance metrics, and business goal alignment. Include a summary of the model’s strengths, weaknesses, and limitations.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Compliance verification documents</p>
			</td>
			<td>
			<p>Provide evidence of compliance with the transparency requirements of Article 50 and other relevant regulations. Clearly demonstrate how the system meets legal and ethical obligations.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Test results</p>
			</td>
			<td>
			<p>Compile the findings of model and system tests, including inputs, outputs, and any errors found. Include performance data and error analysis.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Impact assessments</p>
			</td>
			<td>
			<p>Analyze the system’s effects on stakeholders, considering potential ethical and societal implications. Document any identified biases and risks.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Audit reports</p>
			</td>
			<td>
			<p>Summarize findings from the evaluation phase, including compliance checks and ethical assessments. Provide recommendations for improvements, and include a social impact analysis.</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="Available tools and technologies" data-type="sect3"><div class="sect3" id="chapter_6_available_tools_and_technologies_1748539923608778">
<h3>Available tools and technologies</h3>

<p>Integrating transparency into MLOps workflows requires robust tools for explainability, monitoring, and validation. The following tools and technologies can be used to implement best practices and support compliance with the EU AI Act:</p>

<dl>
	<dt>Explainability tools</dt>
	<dd>
	<p>These help make the model’s decision-making process more interpretable and understandable. Examples include:</p>

	<ul>
		<li>
		<p>SHAP, for understanding feature importance</p>
		</li>
		<li>
		<p>LIME, for explaining individual predictions</p>
		</li>
		<li>
		<p>Explainable Boosting Machines (EBMs), for creating inherently interpretable models</p>
		</li>
	</ul>
	</dd>
	<dt>Monitoring and logging tools</dt>
	<dd>
	<p>These are essential for tracking system performance in real time and monitoring for fairness, and potential biases. Examples include:</p>

	<ul>
		<li>
		<p>Prometheus, for time-series data and alerts</p>
		</li>
		<li>
		<p>Grafana, for data visualization and dashboards</p>
		</li>
		<li>
		<p>Elastic Stack, for log management and analysis</p>
		</li>
		<li>
		<p>Model Card Toolkit, for standardizing and documenting key information about AI models via structured model cards</p>
		</li>
	</ul>
	</dd>
	<dt>Testing frameworks</dt>
	<dd>
	<p>These facilitate thorough testing of the entire AI system. Examples include:</p>

	<ul>
		<li>
		<p>PyTest, for writing and running tests of Python-based AI applications</p>
		</li>
		<li>
		<p>TensorFlow Testing, for testing TensorFlow models and pipelines</p>
		</li>
	</ul>
	</dd>
</dl>
</div></section>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="Deployment Phase" data-type="sect2"><div class="sect2" id="chapter_6_deployment_phase_1748539923608841">
<h2 class="less_space">Deployment Phase</h2>

<p>The deployment phase marks the transition of the AI system into a live production environment. The following actions are essential for meeting transparency requirements under Article 50 during this lifecycle phase:</p>

<dl>
	<dt>Deploy notification systems</dt>
	<dd>
	<p>Users must be informed that they are interacting with an AI system before they engage with the system (unless it is obvious). These disclosures must be clearly visible within the system interface and include an explanation of the system’s purpose, data sources, and any known limitations. User disclosures must be clear, concise, and accessible to everyone. Test the notification system thoroughly to ensure it works properly in the production environment.</p>
	</dd>
	<dt>Implement content marking</dt>
	<dd>
	<p>Use content marking mechanisms that clearly identify synthetic content (audio, images, video, or text) as artificially generated or manipulated. For systems that produce deepfakes, explicit disclosure is required to inform users that the content is synthetic.</p>
	</dd>
	<dt>Enable monitoring</dt>
	<dd>
	<p>Activate monitoring tools to observe the system’s real-time performance. Monitor for data drift, model performance degradation, and the emergence of biases. The monitoring system should also track user interactions, content generation events, and model deployments.</p>
	</dd>
	<dt>Establish audit logging</dt>
	<dd>
	<p>Implement a comprehensive audit logging framework to record all relevant system activities (user interactions, content generation or manipulation events, errors, etc.). The logs should include model versioning details, transparency notifications, and compliance verification results. The audit logging system should be robust and secure while adhering to data protection regulations.</p>
	</dd>
</dl>

<section data-pdf-bookmark="CRISP-ML(Q) activities" data-type="sect3"><div class="sect3" id="chapter_6_crisp_ml_q_activities_1748539923608910">
<h3>CRISP-ML(Q) activities</h3>

<p>This phase typically involves the following key activities:</p>

<dl>
	<dt>System integration</dt>
	<dd>
	<p>Integrate the AI model with the rest of the production environment. This requires end-to-end testing to ensure that all components are correctly connected and functioning as intended.</p>
	</dd>
	<dt>Production deployment</dt>
	<dd>
	<p>Deploy the AI model into the production environment using established CI/CD pipelines. Validate that the deployed system is functioning as expected by running tests in the live environment.</p>
	</dd>
	<dt>Monitoring setup</dt>
	<dd>
	<p>Configure monitoring tools to track key metrics and performance indicators (defined in the earlier CRISP-ML(Q) phases) such as accuracy, fairness, robustness, and explainability. Set up alerting mechanisms to notify stakeholders of any issues or anomalies that arise.</p>
	</dd>
	<dt>Documentation finalization</dt>
	<dd>
	<p>Ensure that all relevant documentation, including user manuals, system architecture diagrams, dataset documentation, and model cards, is complete and accurately reflects the current state of the deployed system. Finalizing documentation is an often overlooked step that is essential for transparency and providing end users and stakeholders with a clear understanding of the deployed system.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_6_smactr_integration_1748539923608963">
<h3>SMACTR integration</h3>

<p>The deployment phase incorporates SMACTR’s Artifact Collection, Testing, and Reflection stages through the following activities:</p>

<dl>
	<dt>Artifact Collection (final documentation)</dt>
	<dd>
	<ul>
		<li>
		<p>Gather final versions of all documentation, including system architecture diagrams, data flow schematics, dataset and model cards, and compliance records.</p>
		</li>
		<li>
		<p>Confirm that the documentation reflects the complete process, from development to deployment.</p>
		</li>
	</ul>
	</dd>
	<dt>Testing (production verification)</dt>
	<dd>
	<ul>
		<li>
		<p>Perform a final verification of the system’s performance in the live environment.</p>
		</li>
		<li>
		<p>Ensure that the system continues to operate as expected and that all transparency mechanisms are functioning correctly.</p>
		</li>
		<li>
		<p>Continuously monitor the model for performance degradation, fairness issues, and potential bias drift.</p>
		</li>
	</ul>
	</dd>
	<dt>Reflection (deployment readiness assessment)</dt>
	<dd>
	<ul>
		<li>
		<p>Ensure all necessary safeguards are in place to protect users and support responsible AI deployment.</p>
		</li>
		<li>
		<p>Conduct a final review of the entire system to confirm it meets all technical, ethical, and legal requirements.</p>
		</li>
	</ul>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Key artifacts" data-type="sect3"><div class="sect3" id="chapter_6_key_artifacts_1748539923609017">
<h3>Key artifacts</h3>

<p>By the end of the deployment phase, several key artifacts should be available to meet the transparency obligations of the EU AI Act<a contenteditable="false" data-primary="transparency obligations" data-type="indexterm" id="id604"/>. As usual, I’ve summarized them in <a data-type="xref" href="#chapter_6_table_8_1748539923588877">Table 6-8</a>.</p>

<table class="striped" id="chapter_6_table_8_1748539923588877">
	<caption><span class="label">Table 6-8. </span>Summary of key artifacts produced during the deployment phase</caption>
	<thead>
		<tr>
			<th>Artifact</th>
			<th>Content</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Deployment configurations</p>
			</td>
			<td>
			<p>Document the configurations and settings used for deploying the AI system into production. Include infrastructure setup, software versions, and any dependencies.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Monitoring dashboards</p>
			</td>
			<td>
			<p>Set up monitoring dashboards to provide real-time visibility into the system’s performance. Dashboards should display key metrics and include alerting mechanisms for potential issues and anomalies.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>System documentation</p>
			</td>
			<td>
			<p>Compile all documentation related to the deployed system, including architecture diagrams, deployment details, API specifications, and user guides. Make sure this documentation is accessible to all stakeholders.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Compliance records</p>
			</td>
			<td>
			<p>Maintain records demonstrating compliance with Article 50 transparency obligations and any other applicable regulations. Ensure these records are accessible for audit purposes.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Incident response plans</p>
			</td>
			<td>
			<p>Establish clear procedures for identifying, containing, and resolving incidents in the production environment. Include communication protocols for notifying relevant stakeholders.</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="Available tools and technologies" data-type="sect3"><div class="sect3" id="chapter_6_available_tools_and_technologies_1748539923609075">
<h3>Available tools and technologies</h3>

<p>Here are some tools and technologies that can support the implementation of transparency during the deployment phase:</p>

<dl>
	<dt>Metadata tracking systems</dt>
	<dd>
	<p>These enable traceability by tracking user interactions, content generation events, model deployments, and compliance verification results. Examples include:</p>

	<ul>
		<li>
		<p>Databases: PostgreSQL (for structured metadata), MongoDB (for unstructured data)</p>
		</li>
		<li>
		<p>Searchable index: Elasticsearch (for quick retrieval)</p>
		</li>
	</ul>
	</dd>
	<dt>MLOps platforms</dt>
	<dd>
	<p>These support model deployment, versioning, and lifecycle management. Popular examples include:</p>

	<ul>
		<li>
		<p>Kubeflow, an open source platform for deploying and managing machine learning workflows on Kubernetes</p>
		</li>
		<li>
		<p>MLflow, for managing the complete ML lifecycle</p>
		</li>
		<li>
		<p>Vertex AI, Google Cloud’s managed ML platform for building, deploying, and scaling ML models</p>
		</li>
	</ul>
	</dd>
	<dt>CI/CD tools</dt>
	<dd>
	<p>These automate the deployment pipeline, ensuring consistency and traceability. They include:</p>

	<ul>
		<li>
		<p>GitLab CI/CD, an integrated solution for version control and automated deployments</p>
		</li>
		<li>
		<p>GitHub Actions, for automating ML workflows within the GitHub ecosystem</p>
		</li>
	</ul>
	</dd>
	<dt>Monitoring and logging tools</dt>
	<dd>
	<p>These provide real-time observability into system behavior, supporting transparency and bias detection. Examples include:</p>

	<ul>
		<li>
		<p>Prometheus, for time-series data and alerts</p>
		</li>
		<li>
		<p>Grafana, for data visualization and dashboards</p>
		</li>
		<li>
		<p>Elastic Stack, for log management and analysis</p>
		</li>
	</ul>
	</dd>
	<dt>Explainability tools</dt>
	<dd>
	<p>These help make the AI system’s decisions interpretable in a live environment. They include:</p>

	<ul>
		<li>
		<p>SHAP, for understanding feature importance</p>
		</li>
		<li>
		<p>LIME, for explaining individual predictions</p>
		</li>
	</ul>
	</dd>
</dl>
</div></section>
</div></section>

<section data-pdf-bookmark="Monitoring and Maintenance Phase" data-type="sect2"><div class="sect2" id="chapter_6_monitoring_and_maintenance_phase_1748539923609145">
<h2>Monitoring and Maintenance Phase</h2>

<p>The monitoring and maintenance phase is crucial for ensuring continued compliance with the EU AI Act and the reliability and ethical performance of the AI system in the long term. Relevant transparency requirements include:</p>

<dl>
	<dt>Monitor notification delivery</dt>
	<dd>
	<p>Regularly verify that user-facing transparency notifications are clear, accessible, and delivered at the appropriate moments. Track delivery frequency and success rates to identify gaps and areas for improvement, and use these insights to improve the user experience and build trust in the AI system.</p>
	</dd>
	<dt>Track content marking effectiveness</dt>
	<dd>
	<p>Continuously validate that synthetic content is consistently and accurately marked in a machine-readable, detectable format and that this marking persists even after any modifications to the content. Monitor for instances when content is not marked correctly, and take corrective actions to address these issues.</p>
	</dd>
	<dt>Log compliance status</dt>
	<dd>
	<p>Record any system configuration changes or behaviors that could impact compliance. Ensure that compliance logs are stored securely and are readily available for audits.</p>
	</dd>
	<dt>Maintain an audit trail</dt>
	<dd>
	<p>Keep a detailed record of all system activities, including user actions, content creation events, model updates, and configuration changes. The audit logs should include timestamps, user IDs, and clear descriptions of what actions were taken. Review these logs regularly to detect any unusual activity or potential problems, to maintain the system’s reliability and performance and guide necessary improvements.</p>
	</dd>
</dl>

<section data-pdf-bookmark="CRISP-ML(Q) activities" data-type="sect3"><div class="sect3" id="chapter_6_crisp_ml_q_activities_1748539923609212">
<h3>CRISP-ML(Q) activities</h3>

<p>Core activities in this phase of the lifecycle include:</p>

<dl>
	<dt>Data drift and model decay detection</dt>
	<dd>
	<ul>
		<li>
		<p>Implement mechanisms for detecting data drift, where the characteristics of the input data change over time.</p>
		</li>
		<li>
		<p>Monitor for changes in the distribution of input features and flag any significant shifts that could impact the model’s performance.</p>
		</li>
	</ul>
	</dd>
	<dt>Performance monitoring</dt>
	<dd>
	<ul>
		<li>
		<p>Continuously track key performance metrics such as accuracy, precision, recall, and F1 score, fairness indicators, feature attributions, and error rates. Also include serving efficiency metrics such as latency (responsiveness and health of the model service), throughput (volume of predictions handled within a given time), and resource utilization (CPU, GPU, and memory usage, for cost and performance optimization).</p>
		</li>
		<li>
		<p>Monitor these metrics over time to identify performance degradation or drift.</p>
		</li>
		<li>
		<p>Establish thresholds for acceptable performance and set up alerts to notify stakeholders of any significant deviations.</p>
		</li>
	</ul>
	</dd>
	<dt>Model updates</dt>
	<dd>
	<ul>
		<li>
		<p>Establish a structured process for updating models in response to performance degradation, data drift, or changing business requirements.</p>
		</li>
		<li>
		<p>Use version control to track model updates and roll back to previous versions if necessary.</p>
		</li>
		<li>
		<p>Ensure that all model updates are thoroughly tested and validated before deployment to production.</p>
		</li>
	</ul>
	</dd>
	<dt>System maintenance</dt>
	<dd>
	<ul>
		<li>
		<p>Perform regular maintenance to address bugs, security vulnerabilities, and performance issues.</p>
		</li>
		<li>
		<p>Schedule downtime as necessary and communicate maintenance activities to users in advance.</p>
		</li>
	</ul>
	</dd>
</dl>

<p>Keep detailed records of all monitoring and maintenance activities and ensure that system documentation remains up-to-date.</p>
</div></section>

<section data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_6_smactr_integration_1748539923609265">
<h3>SMACTR integration</h3>

<p>Here are the main SMACTR integration activities for the monitoring and maintenance phase:</p>

<dl>
	<dt>Artifact Collection (ongoing documentation)</dt>
	<dd>
	<ul>
		<li>
		<p>Continuously collect and update all relevant documentation, including monitoring reports, maintenance logs, and update records.</p>
		</li>
	</ul>
	</dd>
	<dt>Testing (continuous monitoring)</dt>
	<dd>
	<ul>
		<li>
		<p>Implement continuous monitoring of the AI system, with a focus on ethical considerations including fairness, bias, and accountability.</p>
		</li>
		<li>
		<p>Regularly evaluate the system’s impact on different user groups to ensure it does not perpetuate or amplify existing biases.</p>
		</li>
		<li>
		<p>Set up mechanisms for reporting and addressing any ethical concerns that arise.</p>
		</li>
	</ul>
	</dd>
	<dt>Reflection (regular assessments)</dt>
	<dd>
	<ul>
		<li>
		<p>Conduct regular assessments of the system’s performance and compliance with transparency obligations.</p>
		</li>
		<li>
		<p>Review the ethical and social implications of the system while it is in production.</p>
		</li>
		<li>
		<p>Use insights from these assessments to identify areas for improvement and guide updates to the design, deployment, or governance strategies.</p>
		</li>
	</ul>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Key artifacts" data-type="sect3"><div class="sect3" id="chapter_6_key_artifacts_1748539923609498">
<h3>Key artifacts</h3>

<p><a data-type="xref" href="#chapter_6_table_9_1748539923588898">Table 6-9</a> summarizes the artifacts that should be produced and maintained during this phase.</p>

<table class="striped" id="chapter_6_table_9_1748539923588898">
	<caption><span class="label">Table 6-9. </span>Summary of key artifacts produced during the monitoring and maintenance phase</caption>
	<thead>
		<tr>
			<th>Artifact</th>
			<th>Content</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Monitoring reports</p>
			</td>
			<td>
			<ul>
				<li>Generate regular reports summarizing system performance and providing details on any identified issues or anomalies, such as data drift.</li>
				<li>Include visualizations and metrics to illustrate the system’s overall health and performance.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Maintenance logs</p>
			</td>
			<td>
			<ul>
				<li>Maintain detailed logs of all maintenance activities, including changes to system configurations, data, or code.</li>
				<li>Record the purpose of each activity, along with the steps taken and the outcome.</li>
				<li>Ensure the logs are securely stored and available for audits.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Update records</p>
			</td>
			<td>
			<ul>
				<li>Maintain version control for all model and system updates.</li>
				<li>Ensure that you can roll back to previous AI model versions if needed or have a rule-based fallback mechanism in place.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Compliance reports</p>
			</td>
			<td>
			<ul>
				<li>Produce periodic reports documenting adherence to Article 50 and other applicable regulations.</li>
				<li>Include metrics related to notification delivery, content marking effectiveness, and audit trail maintenance.</li>
				<li>Highlight any areas of noncompliance and actions taken to address these.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Audit trails</p>
			</td>
			<td>
			<ul>
				<li>Maintain a complete audit trail of all system activities, including user interactions, content generation events, model deployments, and changes to system configurations.</li>
				<li>Ensure audit logs are securely stored and accessible to relevant stakeholders for audit and compliance verification purposes.</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="Available tools and technologies" data-type="sect3"><div class="sect3" id="chapter_6_available_tools_and_technologies_1748539923609566">
<h3>Available tools and technologies</h3>

<p>In addition to the tools mentioned in the previous sections, supporting SMACTR integration during the monitoring and maintenance phase requires the following:</p>

<dl>
	<dt>Tools for data drift detection</dt>
	<dd>
	<p>These monitor input data and model predictions over time to detect shifts that could impact performance or fairness. Examples include:</p>

	<ul>
		<li>
		<p>TensorFlow Data Validation, for automatically detecting data anomalies and drift in TensorFlow pipelines</p>
		</li>
		<li>
		<p>Evidently AI, an open source tool for monitoring data quality and model performance</p>
		</li>
		<li>
		<p>NannyML, a library for detecting data drift and model degradation</p>
		</li>
	</ul>
	</dd>
	<dt>Compliance and auditing tools</dt>
	<dd>
	<p>These help organizations define, enforce, and monitor compliance policies while maintaining secure audit trails. Examples include:</p>

	<ul>
		<li>
		<p>Open Policy Agent (OPA), for defining and enforcing governance and compliance controls</p>
		</li>
		<li>
		<p>Cloud Security Command Center, for compliance monitoring and security checks in cloud-based environments like Google Cloud</p>
		</li>
	</ul>
	</dd>
</dl>

<p>Congratulations! You’ve just walked through the complete process of integrating the SMACTR framework into the CRISP-ML(Q) AI system development lifecycle. This integration extends the technical workflow with an internal algorithmic audit process, enabling teams to proactively identify and mitigate potential harms prior to deployment. It ensures transparency and supports compliance throughout every phase, from business and data understanding to ongoing monitoring and maintenance.</p>

<p>For reference, <a data-type="xref" href="#chapter_6_table_10_1748539923588918">Table 6-10</a> maps the SMACTR stages to the CRISP-ML(Q) phases.</p>

<table id="chapter_6_table_10_1748539923588918">
	<caption><span class="label">Table 6-10. </span>SMACTR and CRISP-ML(Q) integration</caption>
	<thead>
		<tr>
			<th rowspan="2">CRISP-ML(Q) phase</th>
			<th colspan="5">SMACTR stage</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td class="subheading"> </td>
			<td class="subheading">Scoping</td>
			<td class="subheading">Mapping</td>
			<td class="subheading">Artifact Collection</td>
			<td class="subheading">Testing</td>
			<td class="subheading">Reflection</td>
		</tr>
		<tr>
			<td class="subheading">Business and data understanding</td>
			<td class="center">✓</td>
			<td class="center">✓</td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td class="subheading">Data preparation</td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td class="center">✓</td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td class="subheading">Modeling</td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td class="center">✓</td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td class="subheading">Evaluation</td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td class="center">✓</td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td class="subheading">Deployment</td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td class="center">✓</td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td class="subheading">Monitoring and maintenance</td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td class="center">✓</td>
			<td class="center">✓</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>
</div></section>

<section data-pdf-bookmark="Technology Trend: AI Governance Platforms" data-type="sect1"><div class="sect1" id="chapter_6_technology_trend_ai_governance_platforms_1748539923609632">
<h1>Technology Trend: AI Governance Platforms</h1>

<p>An AI governance platform<a contenteditable="false" data-primary="AI governance platforms" data-type="indexterm" id="ai-gov-plat-1"/> is a comprehensive, distributed system designed to manage complex workflows, handle large volumes of data, and integrate with diverse AI systems. It comprises a set of rules, processes, frameworks, and tools that organizations can use to support the ethical and responsible development and deployment of AI systems.</p>

<p>The primary function of an AI governance platform is to align AI initiatives with organizational principles, legal requirements (such as the EU AI Act), and ethical standards. You can think of it as a central control system that helps organizations track and manage all their AI-related activities—much like how a financial management system tracks monetary transactions and ensures regulatory compliance.</p>

<p>These platforms typically provide a centralized inventory of all AI systems in use across the organization and assist in classifying them according to the EU AI Act’s risk levels. This is crucial because different risk classifications require different compliance measures.</p>

<p>Governance platforms offer built-in risk assessment tools that help identify potential compliance gaps and suggest remediation measures. For instance, if an AI system is used for hiring and classified as high risk, the platform can help track whether it’s being regularly tested for bias and fairness as required by the legislation.</p>

<p>AI governance platforms also provide tools for documenting and monitoring AI systems throughout their lifecycle. This includes tracking how AI models are developed, trained, and deployed, what data they use, and how they perform in production. They often include features for automatically generating technical documentation and maintaining audit trails as well, which is helpful as the EU AI Act requires extensive documentation, particularly for high-risk AI systems.</p>

<p>Governance workflows are another critical feature. These help manage the approvals required before deploying AI systems, ensure regular reviews are conducted, and maintain records of key decisions and changes. To give a practical example, imagine a large bank using AI for credit scoring. Its AI governance platform might support the following functions:</p>

<ul>
	<li>
	<p>Document how the credit scoring model was developed.</p>
	</li>
	<li>
	<p>Track the data used during training.</p>
	</li>
	<li>
	<p>Monitor performance for accuracy and fairness.</p>
	</li>
	<li>
	<p>Generate required compliance reports.</p>
	</li>
	<li>
	<p>Alert relevant stakeholders if the system starts showing signs of bias.</p>
	</li>
	<li>
	<p>Maintain records of all model updates and approval processes.</p>
	</li>
</ul>

<p>In the context of the EU AI Act, these platforms are becoming increasingly valuable. The Act introduces strict requirements for documentation, risk management, human oversight, and accountability. While it doesn’t explicitly require organizations to use an AI governance platform, having one in place can significantly ease the compliance burden, especially for organizations managing multiple AI systems.</p>

<section data-pdf-bookmark="Leading Platforms" data-type="sect2"><div class="sect2" id="chapter_6_leading_platforms_1748539923609689">
<h2>Leading Platforms</h2>

<p>Several notable AI governance platforms<a contenteditable="false" data-primary="AI governance platforms" data-secondary="leading platforms" data-type="indexterm" id="id605"/> are available at the time of writing, including the following:</p>

<dl>
	<dt>Credo AI</dt>
	<dd>
	<p>Specializes in model risk management and compliance assessments, with a strong focus on generative AI</p>
	</dd>
	<dt>Monitaur</dt>
	<dd>
	<p>Provides an ML Assurance platform for building scalable AI governance <span class="keep-together">programs</span></p>
	</dd>
	<dt>Fairly AI</dt>
	<dd>
	<p>Offers tools for real-time monitoring, auditing, and automated compliance <span class="keep-together">management</span></p>
	</dd>
	<dt>Modulos</dt>
	<dd>
	<p>Features an agentic AI governance platform that automates compliance with trustworthy AI standards</p>
	</dd>
	<dt>Holistic AI</dt>
	<dd>
	<p>Delivers enterprise-level oversight of AI projects and inventory <a contenteditable="false" data-primary="limited-risk AI systems" data-secondary="AI engineering for" data-startref="l-r-ai-eng-for-1" data-type="indexterm" id="id606"/>management</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Further Reading" data-type="sect2"><div class="sect2" id="chapter_6_further_reading_1748539923609743">
<h2>Further Reading</h2>

<p>To learn more about AI governance platforms<a contenteditable="false" data-primary="AI governance platforms" data-startref="ai-gov-plat-1" data-type="indexterm" id="id607"/>, check out the following resources:</p>

<ul>
	<li>
	<p>Domo’s <a href="https://oreil.ly/tuYhZ">Top 8 AI Governance Platforms for 2025</a></p>
	</li>
	<li>
	<p><a href="https://oreil.ly/FtsGt">“What Is an AI Alignment Platform?”</a> by Andrew Burt, Mike Schiller, and Ben Lorica</p>
	</li>
	<li>
	<p>Edwin Wenink’s <a href="https://oreil.ly/rn8_p">Ethical Tools Landscape</a></p>
	</li>
</ul>
</div></section>
</div></section>

<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="chapter_6_conclusion_1748539923609794">
<h1>Conclusion</h1>

<p>Transparency obligations under the EU AI Act focus on user awareness and preventing deception. Unlike the conformity assessments required for high-risk AI systems, which are formal processes that verify legal compliance, transparency obligations apply to all AI systems that interact directly with humans, regardless of their risk <span class="keep-together">classification</span>. Their primary goal is to ensure that individuals are clearly informed when they are interacting with an AI system and understand its purpose and <span class="keep-together">limitations</span>.</p>

<p>This chapter has provided a practical, detailed guide for AI engineers seeking to achieve proactive compliance with the transparency requirements set out in Article 50 of the Act. As you’ve seen, integrating the SMACTR framework with the CRISP-ML(Q) methodology provides a robust and auditable process for responsibly developing and deploying AI systems and supports transparency throughout the AI lifecycle.</p>

<p>The next chapter will discuss the implications of the EU AI Act on developing and deploying general-purpose AI models and systems. We’ll examine the specific rules and obligations for GPAI and explore the conditions under which these systems might be classified as high risk, with corresponding stringent requirements.</p>
</div></section>
</div></section></body></html>