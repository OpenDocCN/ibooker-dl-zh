- en: Chapter 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI, especially generative AI, has followed a similar adoption cycle to many
    new technologies. Those organizations producing cutting-edge technologies and
    those in or adjacent to the tech space typically have been quicker to adopt this
    technology, while other industries have had a slower rate of adoption. However,
    more recent advancements have helped business leaders realize that they must figure
    out how to leverage generative AI for their businesses or risk being left behind
    by their competitors.
  prefs: []
  type: TYPE_NORMAL
- en: Now, enterprises are expending more resources to leverage AI for their businesses.
    This often takes the form of teams of data scientists implementing proofs of concept
    (POCs) of AI-based applications for their businesses.
  prefs: []
  type: TYPE_NORMAL
- en: A particularly common proof of concept project in the enterprise is building
    chatbots. Typically, these projects make use of retrieval-augmented generation
    (RAG), combining proprietary data with off-the-shelf large language models (LLMs)
    to give the chatbot expertise on a specific problem domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'But these enterprises are often facing a difficult challenge: they have POCs
    of AI-based applications for their businesses, but they can’t move them into production.
    In fact, the [vast majority of these projects never make it to production](https://oreil.ly/X1kML).'
  prefs: []
  type: TYPE_NORMAL
- en: To improve the success rate of these POCs and improve the return on investment
    of AI initiatives in the enterprise, businesses must develop a better understanding
    of the challenges that arise when running AI-enabled applications in production.
    With this understanding, leaders will be better able to architect solutions for
    promoting POCs into production and managing their product lifecycles.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing and overcoming these challenges is at the core of the relatively
    recent discipline of *machine learning operations (MLOps)*. This publication will
    walk you through why this is a critical next step and how to leverage MLOps on
    Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this report, we’ll unpack four fundamentals of building AI-powered applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Training models in the experimental phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making model creation repeatable and declarative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating models in production as a part of AI-powered applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring that models you create are trustworthy and built responsibly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This report will take a Kubernetes-centric approach, highlighting projects that
    are built to be Kubernetes native and when used together allow you to apply MLOps
    principles and practices to building AI-powered applications.
  prefs: []
  type: TYPE_NORMAL
- en: What Is MLOps?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLOps has its origin in the world of *DevOps*, a best-practice development model
    that seeks to deliver high-quality software to production quickly. It seeks to
    do this by bringing development and operations roles closer together. This fosters
    collaboration and shared knowledge across the software development and production
    lifecycles while bringing awareness of production issues to the teams and individuals
    best equipped to solve them. This approach requires that developers concern themselves
    with how the software they write performs in production and that they’re actively
    involved in operating that software in production as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the proliferation of AI/ML and an ever-increasing number of models being
    created, MLOps has emerged as a new paradigm for delivering high-quality models
    to production quickly, applying [DevOps principles](https://oreil.ly/0QBop) to
    AI models and AI-powered applications instead of traditional software applications.
    However, MLOps doesn’t just apply DevOps principles to the AI development lifecycle
    but builds upon them to define foundational best practices for building and running
    AI-powered applications. A team implementing MLOps practices should adhere to
    the following core principles, which are expanded upon and explained in-depth
    in the book [*Designing Machine Learning Systems*](https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956)
    by Chip Huyen (O’Reilly, 2022):'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration and delivery (CI/CD)
  prefs: []
  type: TYPE_NORMAL
- en: A robust suite of CI/CD automation tools to repeatably build, test, and deploy
    AI-powered applications.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory workflow orchestration
  prefs: []
  type: TYPE_NORMAL
- en: A robust data science workflow orchestration tool to automate the end-to-end
    model development lifecycle from data preparation through model training, tuning,
    and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducible artifacts
  prefs: []
  type: TYPE_NORMAL
- en: Artifacts from a given version of an intelligent application must be made reproducible,
    and all components used to create these artifacts must be versioned and well-documented.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-team collaboration
  prefs: []
  type: TYPE_NORMAL
- en: Building AI-powered applications requires strong collaboration between multiple
    roles consisting of, at a minimum, data engineers, data scientists, application
    developers, and operations teams. MLOps emphasizes close communication and collaboration
    between these groups.
  prefs: []
  type: TYPE_NORMAL
- en: Model and data lineage
  prefs: []
  type: TYPE_NORMAL
- en: Model and data lineage along with other key metadata for an intelligent application
    must be well tracked, especially for the purpose of building trust in AI applications
    but also for debugging and explainability.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs: []
  type: TYPE_NORMAL
- en: MLOps requires that AI-powered applications be monitored across their production
    lifecycle. In addition to traditional application monitoring, AI-powered applications
    must be monitored for data distribution drifts, model degradation, bias, compliance,
    and more. Because many models also use expensive specialized hardware, such as
    GPU clusters, monitoring for the efficient use of this hardware is critically
    important as well.
  prefs: []
  type: TYPE_NORMAL
- en: Iteration-supporting process
  prefs: []
  type: TYPE_NORMAL
- en: MLOps processes must allow for frequent iterations throughout the development
    and production lifecycles of an intelligent application. Data scientists must
    be able to train a model, evaluate its performance, and quickly retrain the model
    based on results of the evaluation. Similarly, models must be periodically retrained
    after they are released to production in order to incorporate new data in their
    training sets, as new data may diverge from the original training data. This divergence
    can be caught as it happens by adhering to the previously mentioned monitoring
    principle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the foundational principles, let’s consider the AI development
    lifecycle. While this lifecycle can take many forms, all tend to roughly follow
    this pattern (illustrated in [Figure 1-1](#ch01_figure_1_1738498450402392)):'
  prefs: []
  type: TYPE_NORMAL
- en: Project initiation
  prefs: []
  type: TYPE_NORMAL
- en: Business stakeholders, application developers, data scientists, and data engineers
    collaborate to identify the desired business outcome for the intelligent application,
    the raw data that can be used to build the model that will achieve this outcome,
    and the architecture for how the resulting AI model will be integrated into a
    software application to deliver the desired solution.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs: []
  type: TYPE_NORMAL
- en: 'Data engineers and data scientists produce the necessary training and tuning
    data artifacts upon which to build the model. These artifacts do not need to be
    static: they could be real-time data or database entries that are constantly updating
    from data ingestion pipelines, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: Model experimentation
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists consume the prepared feature data to create a sufficiently performant
    model, frequently iterating on different model architectures, training hyperparameters,
    and feature data combinations.
  prefs: []
  type: TYPE_NORMAL
- en: Application integration
  prefs: []
  type: TYPE_NORMAL
- en: Application developers work closely with data scientists to integrate a trained
    model into application code, which will consume the trained model via an API.
  prefs: []
  type: TYPE_NORMAL
- en: Production service
  prefs: []
  type: TYPE_NORMAL
- en: The application is promoted to production, where it adds value and is continuously
    iterated upon to improve its performance and add new features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/skia_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. This flowchart demonstrates the iterative nature of the AI development
    lifecycle
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given the strong demand for a platform that enables this lifecycle and MLOps
    best practices, it isn’t surprising that a dizzying number of AI development platforms
    exist on the market today. This report will dive into the open source Kubernetes
    container orchestration platform, highlighting how it can be used across the AI
    development lifecycle to apply the foundational MLOps principles to your workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Why Use Kubernetes for Your MLOps Platform?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has many strengths, which make it an excellent platform on which
    to build and run AI-powered applications while adhering to MLOps principles. Because
    Kubernetes applications are written in a declarative manner, it allows teams to
    consistently produce repeatable results when building AI models. This, combined
    with built-in robust [GitOps tooling](https://oreil.ly/Fjk7z), makes it very easy
    to version control model training artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: Creating models requires access to specialized hardware, and how that hardware
    is used is often unpredictable and bursty in nature. Kubernetes is able to abstract
    away the process of provisioning hardware resources such that a data scientist
    can focus on developing the model instead of on configuring the hardware environment.
    On the production side, running AI-powered applications requires separately scaling
    different parts of the application, including the compute resources serving the
    backend model and frontend APIs that provide a user access to the model. Because
    Kubernetes abstracts away hardware provisioning, scaling different pieces of the
    deployed application becomes easier.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to manual scaling, Kubernetes allows users to automatically scale
    highly specialized and expensive compute resources. This fine-grained resource
    management is critical for properly managing costs.
  prefs: []
  type: TYPE_NORMAL
- en: Another consideration is specialized hardware such as accelerators, particularly
    in large-scale training and tuning jobs, which can be quite fragile. In a model
    training or fine-tuning job that requires multiple days to execute, a hardware
    failure that forces you to restart training can be quite costly. Kubernetes has
    self-healing features, which, coupled with checkpointing support in common training
    libraries, eliminate this problem, making Kubernetes a robust fault-tolerant platform.
  prefs: []
  type: TYPE_NORMAL
- en: While many AI development platforms are tied to a specific cloud platform, Kubernetes
    is able to run anywhere you need it. This includes cloud providers, private datacenters,
    edge locations, and hybrid configurations, which allow Kubernetes to serve as
    the single deployment platform upon which to build your applications.
  prefs: []
  type: TYPE_NORMAL
- en: On the monitoring side, Kubernetes integrates with several monitoring tools
    such as Prometheus, DataDog, and Grafana, which can help track performance and
    resource usage of models. This is especially important for LLMs due to their size
    and cost to operate. These deep integrations provide MLOps administrators with
    proactive monitoring and alerts to ensure that models run optimally for critical
    AI workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, rolling out updates to models, especially LLMs, can be a difficult
    and costly practice. Kubernetes simplifies this process with features like rolling
    updates (which pushes updates incrementally) and canary deployments, helping to
    minimize the downtime of these models.
  prefs: []
  type: TYPE_NORMAL
