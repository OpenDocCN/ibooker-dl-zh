<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 10. Optimizing AI Services" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch10">
<h1><span class="label">Capitolo 10. </span>Ottimizzazione dei servizi AI</h1><div data-type="note"><p>Questo lavoro è stato tradotto utilizzando l'AI. Siamo lieti di ricevere il tuo feedback e i tuoi commenti: <a href="mailto:translation-feedback@oreilly.com">translation-feedback@oreilly.com</a></p></div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1067">
<h1>Obiettivi del capitolo</h1>
<p><a data-primary="optimizing AI services" data-type="indexterm" id="ix_ch10-asciidoc0"/>In questo capitolo imparerai a conoscere:</p>
<ul>
<li>
<p>Tecniche di ottimizzazione come la cache di parole chiave, semantica e contestuale</p>
</li>
<li>
<p>Tecniche avanzate di prompt engineering per massimizzare la qualità e la coerenza della generazione del modello</p>
</li>
<li>
<p>La quantizzazione dei modelli e la differenza che i modelli quantizzati fanno nel servizio dei modelli</p>
</li>
<li>
<p>Utilizzare le API per l'elaborazione batch per carichi di lavoro AI di grandi dimensioni</p>
</li>
<li>
<p>I vantaggi, gli svantaggi e i casi d'uso della messa a punto del modello</p>
</li>
</ul>
</div></aside>
<p>In questo capitolo imparerai a ottimizzare ulteriormente i tuoi servizi attraverso l'ingegneria del prompt, la quantificazione dei modelli e i meccanismi di caching.</p>
<section data-pdf-bookmark="Optimization Techniques" data-type="sect1"><div class="sect1" id="id256">
<h1>Tecniche di ottimizzazione</h1>
<p><a data-primary="optimizing AI services" data-secondary="basics" data-type="indexterm" id="id1068"/>Gli obiettivi dell'ottimizzazione di un servizio di IA sono il miglioramento della qualità dell'output o delle prestazioni (latenza, throughput, costi, ecc.).</p>
<p>Le ottimizzazioni relative alle prestazioni includono le seguenti:</p>
<ul>
<li>
<p>Utilizzare le API per l'elaborazione batch</p>
</li>
<li>
<p>Caching (parola chiave, semantico, contesto o prompt)</p>
</li>
<li>
<p>Quantizzazione del modello</p>
</li>
</ul>
<p>Le ottimizzazioni relative alla qualità includono le seguenti:</p>
<ul>
<li>
<p>Utilizzo di output strutturati</p>
</li>
<li>
<p>Ingegneria prompt</p>
</li>
<li>
<p>Messa a punto del modello</p>
</li>
</ul>
<p>Esaminiamo ciascuna di esse in modo più dettagliato.</p>
<section data-pdf-bookmark="Batch Processing" data-type="sect2"><div class="sect2" id="id145">
<h2>Elaborazione in batch</h2>
<p><a data-primary="batch processing" data-type="indexterm" id="ix_ch10-asciidoc1"/><a data-primary="optimizing AI services" data-secondary="batch processing" data-type="indexterm" id="ix_ch10-asciidoc2"/>Spesso vuoi che un LLM elabori lotti di voci contemporaneamente. La soluzione più ovvia è quella di inviare più chiamate API per ogni voce. Tuttavia, l'approccio ovvio può essere costoso e lento e può portare il tuo fornitore di modelli a limitare le tariffe.</p>
<p>In questi casi, puoi sfruttare due tecniche distinte per elaborare i dati in batch attraverso un LLM:</p>
<ul>
<li>
<p>Aggiornare gli schemi di output strutturato per restituire più esempi contemporaneamente</p>
</li>
<li>
<p>Identificare e utilizzare le API dei fornitori di modelli progettate per l'elaborazione in batch.</p>
</li>
</ul>
<p>La prima soluzione prevede che tu aggiorni i modelli o i prompt di Pydantic in modo da richiedere un elenco di output per ogni richiesta. In questo caso, puoi elaborare i dati in batch con una manciata di richieste invece di una per voce.</p>
<p>Un'implementazione della prima soluzione è mostrata nell'<a data-type="xref" href="#batch_processing_structured_outputs">Esempio 10-1</a>.</p>
<div data-type="example" id="batch_processing_structured_outputs">
<h5><span class="label">Esempio 10-1. </span>Aggiornamento dello schema di output strutturato per l'analisi di più elementi</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">pydantic</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">BaseModel</code>

<code class="k" translate="no">class</code> <code class="nc" translate="no">BatchDocumentClassification</code><code class="p" translate="no">(</code><code class="n" translate="no">BaseModel</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code>
    <code class="k" translate="no">class</code> <code class="nc" translate="no">Category</code><code class="p" translate="no">(</code><code class="n" translate="no">BaseModel</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code>
        <code class="n" translate="no">document_id</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code>
        <code class="n" translate="no">category</code><code class="p" translate="no">:</code> <code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">str</code><code class="p" translate="no">]</code>

    <code class="n" translate="no">categories</code><code class="p" translate="no">:</code> <code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="n" translate="no">Category</code><code class="p" translate="no">]</code> <a class="co" href="#callout_optimizing_ai_services_CO1-1" id="co_optimizing_ai_services_CO1-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO1-1" id="callout_optimizing_ai_services_CO1-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Aggiornare il modello Pydantic per includere un elenco di modelli <code translate="no">Category</code>.</p></dd>
</dl></div>
<p>Ora puoi passare il nuovo schema insieme a un elenco di titoli di documenti al client OpenAI per elaborare più voci in un'unica chiamata API. Tuttavia, un'alternativa e forse la soluzione migliore sarà quella di utilizzare un'API batch, se disponibile.</p>
<p>Fortunatamente, i fornitori di modelli come OpenAI forniscono già delle API adatte a questi casi d'uso. Sotto il cofano, questi fornitori possono eseguire delle code di attività per elaborare un singolo lavoro batch in background, fornendo aggiornamenti sullo stato fino al completamento del batch per recuperare i risultati.</p>
<p>Rispetto all'utilizzo diretto degli endpoint standard, sarai in grado di inviare gruppi di richieste asincrone a costi inferiori (fino al 50% con OpenAI), di usufruire di limiti di velocità più elevati e di garantire tempi di completamento.<sup><a data-type="noteref" href="ch10.html#id1069" id="id1069-marker" translate="no">1</a></sup>Il servizio batch job è ideale per l'elaborazione di lavori che non richiedono risposte immediate, come l'utilizzo di OpenAI LLMs per analizzare, classificare o tradurre grandi volumi di documenti in background.</p>
<p>Per inviare un lavoro batch, avrai bisogno di un file <code translate="no">jsonl</code> in cui ogni riga contiene i dettagli di una singola richiesta all'API, come mostrato nell'<a data-type="xref" href="#jsonl">Esempio 10-2</a>. Inoltre, come si vede in questo esempio, per creare il file JSONL, puoi iterare le voci e generare dinamicamente il file.</p>
<div data-type="example" id="jsonl">
<h5><span class="label">Esempio 10-2. </span>Creare un file JSONL dalle voci</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code> <code class="nn" translate="no">json</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">uuid</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">UUID</code>

<code class="k" translate="no">def</code> <code class="nf" translate="no">create_batch_file</code><code class="p" translate="no">(</code>
    <code class="n" translate="no">entries</code><code class="p" translate="no">:</code> <code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">str</code><code class="p" translate="no">],</code>
    <code class="n" translate="no">system_prompt</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">model</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code> <code class="o" translate="no">=</code> <code class="s2" translate="no">"gpt-4o-mini"</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">filepath</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code> <code class="o" translate="no">=</code> <code class="s2" translate="no">"batch.jsonl"</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">max_tokens</code><code class="p" translate="no">:</code> <code class="nb" translate="no">int</code> <code class="o" translate="no">=</code> <code class="mi" translate="no">1024</code><code class="p" translate="no">,</code>
<code class="p" translate="no">)</code> <code class="o" translate="no">-&gt;</code> <code class="kc" translate="no">None</code><code class="p" translate="no">:</code>
    <code class="k" translate="no">with</code> <code class="nb" translate="no">open</code><code class="p" translate="no">(</code><code class="n" translate="no">filepath</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"w"</code><code class="p" translate="no">)</code> <code class="k" translate="no">as</code> <code class="n" translate="no">file</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">for</code> <code class="n" translate="no">_</code><code class="p" translate="no">,</code> <code class="n" translate="no">entry</code> <code class="ow" translate="no">in</code> <code class="nb" translate="no">enumerate</code><code class="p" translate="no">(</code><code class="n" translate="no">entries</code><code class="p" translate="no">,</code> <code class="n" translate="no">start</code><code class="o" translate="no">=</code><code class="mi" translate="no">1</code><code class="p" translate="no">):</code>
            <code class="n" translate="no">request</code> <code class="o" translate="no">=</code> <code class="p" translate="no">{</code>
                <code class="s2" translate="no">"custom_id"</code><code class="p" translate="no">:</code> <code class="sa" translate="no">f</code><code class="s2" translate="no">"request-</code><code class="si" translate="no">{</code><code class="n" translate="no">UUID</code><code class="p" translate="no">()</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code>
                <code class="s2" translate="no">"method"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"POST"</code><code class="p" translate="no">,</code>
                <code class="s2" translate="no">"url"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"/v1/chat/completions"</code><code class="p" translate="no">,</code>
                <code class="s2" translate="no">"body"</code><code class="p" translate="no">:</code> <code class="p" translate="no">{</code>
                    <code class="s2" translate="no">"model"</code><code class="p" translate="no">:</code> <code class="n" translate="no">model</code><code class="p" translate="no">,</code>
                    <code class="s2" translate="no">"messages"</code><code class="p" translate="no">:</code> <code class="p" translate="no">[</code>
                        <code class="p" translate="no">{</code>
                            <code class="s2" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"system"</code><code class="p" translate="no">,</code>
                            <code class="s2" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="n" translate="no">system_prompt</code><code class="p" translate="no">,</code>
                        <code class="p" translate="no">},</code>
                        <code class="p" translate="no">{</code><code class="s2" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"user"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="n" translate="no">entry</code><code class="p" translate="no">},</code>
                    <code class="p" translate="no">],</code>
                    <code class="s2" translate="no">"max_tokens"</code><code class="p" translate="no">:</code> <code class="n" translate="no">max_tokens</code><code class="p" translate="no">,</code>
                <code class="p" translate="no">},</code>
            <code class="p" translate="no">}</code>
            <code class="n" translate="no">file</code><code class="o" translate="no">.</code><code class="n" translate="no">write</code><code class="p" translate="no">(</code><code class="n" translate="no">json</code><code class="o" translate="no">.</code><code class="n" translate="no">dumps</code><code class="p" translate="no">(</code><code class="n" translate="no">request</code><code class="p" translate="no">)</code> <code class="o" translate="no">+</code> <code class="s2" translate="no">"</code><code class="se" translate="no">\n</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code></pre></div>
<p>Una volta creato, puoi inviare il file all'API batch per l'elaborazione, come mostrato nell'<a data-type="xref" href="#batch_processing_api">esempio 10-3</a>.</p>
<div data-type="example" id="batch_processing_api">
<h5><span class="label">Esempio 10-3. </span>Elaborazione di lavori batch con l'API OpenAI Batch</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">loguru</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">logger</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">openai</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">AsyncOpenAI</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">openai.types</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">Batch</code>

<code class="n" translate="no">client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">AsyncOpenAI</code><code class="p" translate="no">()</code>

<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">submit_batch_job</code><code class="p" translate="no">(</code><code class="n" translate="no">filepath</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code><code class="p" translate="no">)</code> <code class="o" translate="no">-&gt;</code> <code class="n" translate="no">Batch</code><code class="p" translate="no">:</code>
    <code class="k" translate="no">if</code> <code class="s2" translate="no">".jsonl"</code> <code class="ow" translate="no">not</code> <code class="ow" translate="no">in</code> <code class="n" translate="no">filepath</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">raise</code> <code class="ne" translate="no">FileNotFoundError</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"JSONL file not provided at </code><code class="si" translate="no">{</code><code class="n" translate="no">filepath</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>

    <code class="n" translate="no">file_response</code> <code class="o" translate="no">=</code> <code class="k" translate="no">await</code> <code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">files</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code>
        <code class="n" translate="no">file</code><code class="o" translate="no">=</code><code class="nb" translate="no">open</code><code class="p" translate="no">(</code><code class="n" translate="no">filepath</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"rb"</code><code class="p" translate="no">),</code> <code class="n" translate="no">purpose</code><code class="o" translate="no">=</code><code class="s2" translate="no">"batch"</code>
    <code class="p" translate="no">)</code>

    <code class="n" translate="no">batch_job_response</code> <code class="o" translate="no">=</code> <code class="k" translate="no">await</code> <code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">batches</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code>
        <code class="n" translate="no">input_file_id</code><code class="o" translate="no">=</code><code class="n" translate="no">file_response</code><code class="o" translate="no">.</code><code class="n" translate="no">id</code><code class="p" translate="no">,</code>
        <code class="n" translate="no">endpoint</code><code class="o" translate="no">=</code><code class="s2" translate="no">"/v1/chat/completions"</code><code class="p" translate="no">,</code>
        <code class="n" translate="no">completion_window</code><code class="o" translate="no">=</code><code class="s2" translate="no">"24h"</code><code class="p" translate="no">,</code>
        <code class="n" translate="no">metadata</code><code class="o" translate="no">=</code><code class="p" translate="no">{</code><code class="s2" translate="no">"description"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"document classification job"</code><code class="p" translate="no">},</code>
    <code class="p" translate="no">)</code>
    <code class="k" translate="no">return</code> <code class="n" translate="no">batch_job_response</code>

<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">retrieve_batch_results</code><code class="p" translate="no">(</code><code class="n" translate="no">batch_id</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code><code class="p" translate="no">):</code>
    <code class="n" translate="no">batch</code> <code class="o" translate="no">=</code> <code class="k" translate="no">await</code> <code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">batches</code><code class="o" translate="no">.</code><code class="n" translate="no">retrieve</code><code class="p" translate="no">(</code><code class="n" translate="no">batch_id</code><code class="p" translate="no">)</code>
    <code class="k" translate="no">if</code> <code class="p" translate="no">(</code>
        <code class="n" translate="no">status</code> <code class="o" translate="no">:=</code> <code class="n" translate="no">batch</code><code class="o" translate="no">.</code><code class="n" translate="no">status</code> <code class="o" translate="no">==</code> <code class="s2" translate="no">"completed"</code>
        <code class="ow" translate="no">and</code> <code class="n" translate="no">batch</code><code class="o" translate="no">.</code><code class="n" translate="no">output_file_id</code> <code class="ow" translate="no">is</code> <code class="ow" translate="no">not</code> <code class="kc" translate="no">None</code>
    <code class="p" translate="no">):</code>
        <code class="n" translate="no">file_content</code> <code class="o" translate="no">=</code> <code class="k" translate="no">await</code> <code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">files</code><code class="o" translate="no">.</code><code class="n" translate="no">content</code><code class="p" translate="no">(</code><code class="n" translate="no">batch</code><code class="o" translate="no">.</code><code class="n" translate="no">output_file_id</code><code class="p" translate="no">)</code>
        <code class="k" translate="no">return</code> <code class="n" translate="no">file_content</code>
    <code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">warning</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"Batch </code><code class="si" translate="no">{</code><code class="n" translate="no">batch_id</code><code class="si" translate="no">}</code><code class="s2" translate="no"> is in </code><code class="si" translate="no">{</code><code class="n" translate="no">status</code><code class="si" translate="no">}</code><code class="s2" translate="no"> status"</code><code class="p" translate="no">)</code></pre></div>
<p>Ora puoi sfruttare gli endpoint batch offline per elaborare più voci in una sola volta, con tempi di consegna garantiti e un notevole risparmio sui costi.</p>
<p>Oltre a sfruttare gli output strutturati e le API batch per ottimizzare i tuoi servizi, puoi anche sfruttare le tecniche di caching per accelerare in modo significativo i tempi di risposta e i costi delle risorse dei tuoi server.<a data-startref="ix_ch10-asciidoc2" data-type="indexterm" id="id1070"/><a data-startref="ix_ch10-asciidoc1" data-type="indexterm" id="id1071"/></p>
</div></section>
<section data-pdf-bookmark="Caching" data-type="sect2"><div class="sect2" id="id146">
<h2>Caching</h2>
<p><a data-primary="caching" data-type="indexterm" id="ix_ch10-asciidoc3"/><a data-primary="caching" data-secondary="for optimizing AI services" data-secondary-sortas="optimizing AI services" data-type="indexterm" id="ix_ch10-asciidoc4"/><a data-primary="optimizing AI services" data-secondary="caching" data-type="indexterm" id="ix_ch10-asciidoc5"/>Nei servizi GenAI, spesso ti affidi alla risposta di dati/modelli che richiedono calcoli significativi o lunghe elaborazioni. Se ci sono più utenti che richiedono gli stessi dati, ripetere le stesse operazioni può essere dispendioso. Invece, puoi usare le tecniche di caching per memorizzare e recuperare i dati a cui si accede di frequente per aiutarti a ottimizzare i tuoi servizi accelerando i tempi di risposta, riducendo il carico del server e risparmiando larghezza di banda e costi operativi.</p>
<p>Ad esempio, in un chatbot FAQ pubblico in cui gli utenti pongono per lo più le stesse domande, potresti voler riutilizzare le risposte nella cache per periodi più lunghi. D'altra parte, per chatbot più personalizzati e dinamici, puoi aggiornare frequentemente (cioè invalidare) la risposta nella cache.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Dovresti sempre considerare la frequenza di aggiornamento della cache in base alla natura dei dati e al livello accettabile di staleness.</p>
</div>
<p>Le strategie di caching più importanti per i servizi GenAI includono:</p>
<ul>
<li>
<p>Caching delle parole chiave</p>
</li>
<li>
<p>Caching semantico</p>
</li>
<li>
<p>Caching del contesto o del prompt</p>
</li>
</ul>
<p>Esaminiamo ciascuna di esse in modo più dettagliato.</p>
<section data-pdf-bookmark="Keyword caching" data-type="sect3"><div class="sect3" id="id147">
<h3>Caching delle parole chiave</h3>
<p><a data-primary="caching" data-secondary="keyword caching" data-type="indexterm" id="ix_ch10-asciidoc6"/><a data-primary="keyword caching" data-type="indexterm" id="ix_ch10-asciidoc7"/><a data-primary="optimizing AI services" data-secondary="caching" data-tertiary="keyword caching" data-type="indexterm" id="ix_ch10-asciidoc8"/>Se hai bisogno solo di un semplice meccanismo di caching per memorizzare le funzioni o le risposte degli endpoint, puoi usare il <em>keyword caching</em>, che consiste nel memorizzare le risposte in base alle corrispondenze esatte delle query di input come coppie chiave-valore.</p>
<p>In FastAPI, librerie come <code translate="no">fastapi-cache</code> possono aiutarti a implementare la cache delle parole chiave in poche righe di codice, su qualsiasi funzione o endpoint. Le cache FastAPI ti danno anche la possibilità di collegare backend di archiviazione come Redis per centralizzare l'archivio della cache in tutte le tue istanze.</p>
<div class="less_space pagebreak-before" data-type="tip"><h6>Suggerimento</h6>
<p>In alternativa, puoi implementare un meccanismo di caching personalizzato con un cache store utilizzando pacchetti di livello inferiore come <code translate="no">cachetools</code>.</p>
</div>
<p>Per iniziare, tutto ciò che devi fare è inizializzare e configurare il sistema di caching come parte del ciclo di vita dell'applicazione, come mostrato nell'<a data-type="xref" href="#caching_lifespan">esempio 10-4</a>. Puoi installare la cache FastAPI utilizzando il seguente comando:</p>
<pre data-type="programlisting" translate="no">$ pip install "fastapi-cache2[redis]"</pre>
<div data-type="example" id="caching_lifespan">
<h5><span class="label">Esempio 10-4. </span>Configurazione della durata della cache FastAPI</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">collections</code><code class="nn" translate="no">.</code><code class="nn" translate="no">abc</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">AsyncIterator</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">contextlib</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">asynccontextmanager</code>

<code class="kn" translate="no">from</code> <code class="nn" translate="no">fastapi</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">FastAPI</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">fastapi_cache</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">FastAPICache</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">fastapi_cache</code><code class="nn" translate="no">.</code><code class="nn" translate="no">backends</code><code class="nn" translate="no">.</code><code class="nn" translate="no">redis</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">RedisBackend</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">redis</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">asyncio</code> <code class="k" translate="no">as</code> <code class="n" translate="no">aioredis</code>

<code class="nd" translate="no">@asynccontextmanager</code>
<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">lifespan</code><code class="p" translate="no">(</code><code class="n" translate="no">_</code><code class="p" translate="no">:</code> <code class="n" translate="no">FastAPI</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="n" translate="no">AsyncIterator</code><code class="p" translate="no">[</code><code class="kc" translate="no">None</code><code class="p" translate="no">]</code><code class="p" translate="no">:</code>
    <code class="n" translate="no">redis</code> <code class="o" translate="no">=</code> <code class="n" translate="no">aioredis</code><code class="o" translate="no">.</code><code class="n" translate="no">from_url</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">redis://localhost</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
    <code class="n" translate="no">FastAPICache</code><code class="o" translate="no">.</code><code class="n" translate="no">init</code><code class="p" translate="no">(</code><code class="n" translate="no">RedisBackend</code><code class="p" translate="no">(</code><code class="n" translate="no">redis</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code> <code class="n" translate="no">prefix</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">fastapi-cache</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code> <a class="co" href="#callout_optimizing_ai_services_CO2-1" id="co_optimizing_ai_services_CO2-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
    <code class="k" translate="no">yield</code>

<code class="n" translate="no">app</code> <code class="o" translate="no">=</code> <code class="n" translate="no">FastAPI</code><code class="p" translate="no">(</code><code class="n" translate="no">lifespan</code><code class="o" translate="no">=</code><code class="n" translate="no">lifespan</code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO2-1" id="callout_optimizing_ai_services_CO2-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Inizializza <code translate="no">FastAPICache</code> con un <code translate="no">RedisBackend</code> che non decodifica le risposte in modo che i dati in cache siano memorizzati come byte (binari). Questo perché la decodifica delle risposte romperebbe la cache alterando il formato originale della risposta.</p></dd>
</dl></div>
<p>Una volta configurato il sistema di caching, puoi decorare le tue funzioni o i gestori di endpoint per mettere in cache i loro output, come mostrato nell'<a data-type="xref" href="#caching_functions_endpoint">Esempio 10-5</a>.</p>
<div data-type="example" id="caching_functions_endpoint">
<h5><span class="label">Esempio 10-5. </span>Caching dei risultati delle funzioni e degli endpoint</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">fastapi</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">APIRouter</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">fastapi_cache</code><code class="nn" translate="no">.</code><code class="nn" translate="no">decorator</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">cache</code>

<code class="n" translate="no">router</code> <code class="o" translate="no">=</code> <code class="n" translate="no">APIRouter</code><code class="p" translate="no">(</code><code class="n" translate="no">prefix</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">/generate</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <code class="n" translate="no">tags</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Resource</code><code class="s2" translate="no">"</code><code class="p" translate="no">]</code><code class="p" translate="no">)</code>

<code class="nd" translate="no">@cache</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code>
<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">classify_document</code><code class="p" translate="no">(</code><code class="n" translate="no">title</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="nb" translate="no">str</code><code class="p" translate="no">:</code>
    <code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code>

<code class="nd" translate="no">@router</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">/text</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
<code class="nd" translate="no">@cache</code><code class="p" translate="no">(</code><code class="n" translate="no">expire</code><code class="o" translate="no">=</code><code class="mi" translate="no">60</code><code class="p" translate="no">)</code> <a class="co" href="#callout_optimizing_ai_services_CO3-1" id="co_optimizing_ai_services_CO3-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">serve_text_to_text_controller</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code>
    <code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO3-1" id="callout_optimizing_ai_services_CO3-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Il decoratore <code translate="no">cache()</code> deve sempre arrivare per ultimo. Invalida la cache tra 60 secondi impostando <code translate="no">expires=60</code> per ricompilare gli output.</p></dd>
</dl></div>
<p>Il decoratore <code translate="no">cache()</code> mostrato nell'<a data-type="xref" href="#caching_functions_endpoint">Esempio 10-5</a> inietta le dipendenze per gli oggetti <code translate="no">Request</code> e <code translate="no">Response</code> in modo da poter aggiungere intestazioni di controllo della cache alla risposta in uscita. Queste intestazioni di controllo della cache istruiscono i client su come mettere in cache le risposte da parte loro, specificando un insieme di direttive (cioè istruzioni).</p>
<p>Queste sono alcune direttive comuni per il controllo della cache durante l'invio delle risposte:</p>
<dl>
<dt><code translate="no">max-age</code></dt>
<dd>
<p>Definisce il tempo massimo (in secondi) in cui una risposta è considerata fresca.</p>
</dd>
<dt><code translate="no">no-cache</code></dt>
<dd>
<p>Forza la riconvalida in modo che i client controllino gli aggiornamenti costanti con il server.</p>
</dd>
<dt><code translate="no">no-store</code></dt>
<dd>
<p>Impedisce completamente il caching</p>
</dd>
<dt><code translate="no">private</code></dt>
<dd>
<p>Memorizza le risposte in una cache privata (ad esempio, la cache locale dei browser).</p>
</dd>
</dl>
<p>Una risposta potrebbe avere intestazioni di controllo della cache come <code translate="no">Cache-Control: max-age=180, private</code> per impostare queste direttive.<sup><a data-type="noteref" href="ch10.html#id1072" id="id1072-marker" translate="no">2</a></sup></p>
<p>Dato che il caching delle parole chiave lavora sulle corrispondenze esatte, è più adatto alle funzioni e alle API che prevedono input con corrispondenze frequenti. Tuttavia, nei servizi GenAI che accettano query variabili da parte dell'utente, potresti voler prendere in considerazione altri meccanismi di caching che si basano sul significato degli input quando restituiscono una risposta in cache. È qui che la cache semantica può rivelarsi utile.<a data-startref="ix_ch10-asciidoc8" data-type="indexterm" id="id1073"/><a data-startref="ix_ch10-asciidoc7" data-type="indexterm" id="id1074"/><a data-startref="ix_ch10-asciidoc6" data-type="indexterm" id="id1075"/></p>
</div></section>
<section data-pdf-bookmark="Semantic caching" data-type="sect3"><div class="sect3" id="id148">
<h3>Caching semantico</h3>
<p><a data-primary="caching" data-secondary="semantic caching" data-type="indexterm" id="ix_ch10-asciidoc9"/><a data-primary="optimizing AI services" data-secondary="caching" data-tertiary="semantic caching" data-type="indexterm" id="ix_ch10-asciidoc10"/><a data-primary="semantic caching" data-type="indexterm" id="ix_ch10-asciidoc11"/>Il<em>caching semantico</em> è un meccanismo di caching che restituisce un valore memorizzato in base a input simili.</p>
<p>Sotto il cofano, il sistema utilizza codificatori e vettori di incorporamento per catturare la semantica e i significati degli input, quindi esegue ricerche di somiglianza tra le coppie chiave-valore memorizzate per restituire una risposta nella cache.</p>
<p>Rispetto alla memorizzazione nella cache delle parole chiave, gli input simili possono restituire la stessa risposta memorizzata nella cache. Gli input al sistema non devono essere identici per essere riconosciuti come simili. Anche se tali input hanno strutture o formulazioni di frasi diverse o contengono imprecisioni, saranno comunque considerati simili perché hanno lo stesso significato. Inoltre, viene richiesta la stessa risposta. A titolo di esempio, le seguenti query sono considerate simili perché hanno lo stesso intento:</p>
<ul>
<li>
<p>Come si costruiscono i servizi generativi con FastAPI?</p>
</li>
<li>
<p>Qual è il processo di sviluppo dei servizi FastAPI per Genai?</p>
</li>
</ul>
<p>Questo sistema di caching contribuisce a un significativo risparmio sui costi<sup><a data-type="noteref" href="ch10.html#id1076" id="id1076-marker" translate="no">3</a></sup> riducendo le chiamate API del <a href="https://oreil.ly/gjGz6">30-40%</a> (cioè con un tasso di risposta alla cache del 60-70%) a seconda del caso d'uso e delle dimensioni della base di utenti. Ad esempio, le applicazioni Q&amp;A RAG che ricevono domande frequenti da una vasta base di utenti possono ridurre le chiamate API del 69% utilizzando una cache semantica.</p>
<p>In un tipico sistema RAG, ci sono due punti in cui la cache può ridurre le operazioni che richiedono risorse e tempo:</p>
<ul>
<li>
<p><em>Prima che il LLM</em> restituisca una risposta nella cache invece di generarne una nuova</p>
</li>
<li>
<p><em>Prima dell'archivio vettoriale</em> per arricchire i prompt con documenti memorizzati nella cache, invece di cercarne e recuperarne di nuovi.</p>
</li>
</ul>
<p>Quando integri un componente di cache semantica nel tuo sistema RAG, devi considerare se la restituzione di una risposta in cache potrebbe avere un impatto negativo sull'esperienza dell'utente dell'applicazione. Ad esempio, se le risposte di LLM vengono memorizzate nella cache, entrambe le query seguenti restituirebbero la stessa risposta a causa della loro elevata somiglianza semantica, inducendo il sistema di cache semantica a trattarle come quasi identiche:</p>
<ul>
<li>
<p>Riassumi questo testo in 100 parole</p>
</li>
<li>
<p>Riassumi questo testo in 50 parole</p>
</li>
</ul>
<p>In questo modo si ha l'impressione che i servizi non rispondano alle query. Dal momento che potresti ancora volere diversi output LLM nella tua applicazione, implementeremo una cache semantica per il recupero dei documenti per il tuo sistema RAG.<a data-type="xref" href="#semantic_cache">La Figura 10-1</a> mostra l'architettura completa del sistema.</p>
<figure><div class="figure" id="semantic_cache">
<img alt="bgai 1001" src="assets/bgai_1001.png" width="1392" height="463"/>
<h6><span class="label">Figura 10-1. </span>Caching semantico nell'architettura del sistema RAG</h6>
</div></figure>
<p>Cominciamo a implementare il sistema di caching semantico da zero e poi vedremo come scaricare la funzionalità a una libreria esterna come<code translate="no">gptcache</code>.</p>
<section data-pdf-bookmark="Building a semantic caching service from scratch" data-type="sect4"><div class="sect4" id="id149">
<h4>Costruire un servizio di caching semantico da zero</h4>
<p><a data-primary="caching" data-secondary="semantic caching" data-tertiary="building a semantic caching service from scratch" data-type="indexterm" id="ix_ch10-asciidoc12"/><a data-primary="semantic caching" data-secondary="building a semantic caching service from scratch" data-type="indexterm" id="ix_ch10-asciidoc13"/>Puoi implementare un sistema di caching semantico implementando i seguenti componenti:</p>
<ul>
<li>
<p>Un client di cache store</p>
</li>
<li>
<p>Un client per l'archiviazione vettoriale di documenti</p>
</li>
<li>
<p>Un modello di incorporazione</p>
</li>
</ul>
<p>L<a data-type="xref" href="#semantic_cache_cache_store">'esempio 10-6</a> mostra come implementare il client del cache store.</p>
<div data-type="example" id="semantic_cache_cache_store">
<h5><span class="label">Esempio 10-6. </span>Client del negozio di cache</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code> <code class="nn" translate="no">uuid</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">qdrant_client</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">AsyncQdrantClient</code><code class="p" translate="no">,</code> <code class="n" translate="no">models</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">qdrant_client</code><code class="nn" translate="no">.</code><code class="nn" translate="no">http</code><code class="nn" translate="no">.</code><code class="nn" translate="no">models</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">Distance</code><code class="p" translate="no">,</code> <code class="n" translate="no">PointStruct</code><code class="p" translate="no">,</code> <code class="n" translate="no">ScoredPoint</code>

<code class="k" translate="no">class</code> <code class="nc" translate="no">CacheClient</code><code class="p" translate="no">:</code>
    <code class="k" translate="no">def</code> <code class="fm" translate="no">__init__</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code>
        <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db</code> <code class="o" translate="no">=</code> <code class="n" translate="no">AsyncQdrantClient</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">:memory:</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code> <a class="co" href="#callout_optimizing_ai_services_CO4-1" id="co_optimizing_ai_services_CO4-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
        <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">cache_collection_name</code> <code class="o" translate="no">=</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">cache</code><code class="s2" translate="no">"</code>

    <code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">initialize_database</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="kc" translate="no">None</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">await</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db</code><code class="o" translate="no">.</code><code class="n" translate="no">create_collection</code><code class="p" translate="no">(</code>
            <code class="n" translate="no">collection_name</code><code class="o" translate="no">=</code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">cache_collection_name</code><code class="p" translate="no">,</code>
            <code class="n" translate="no">vectors_config</code><code class="o" translate="no">=</code><code class="n" translate="no">models</code><code class="o" translate="no">.</code><code class="n" translate="no">VectorParams</code><code class="p" translate="no">(</code>
                <code class="n" translate="no">size</code><code class="o" translate="no">=</code><code class="mi" translate="no">384</code><code class="p" translate="no">,</code> <code class="n" translate="no">distance</code><code class="o" translate="no">=</code><code class="n" translate="no">Distance</code><code class="o" translate="no">.</code><code class="n" translate="no">EUCLID</code>
            <code class="p" translate="no">)</code><code class="p" translate="no">,</code>
        <code class="p" translate="no">)</code>

    <code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">insert</code><code class="p" translate="no">(</code>
        <code class="bp" translate="no">self</code><code class="p" translate="no">,</code> <code class="n" translate="no">query_vector</code><code class="p" translate="no">:</code> <code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">float</code><code class="p" translate="no">]</code><code class="p" translate="no">,</code> <code class="n" translate="no">documents</code><code class="p" translate="no">:</code> <code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">str</code><code class="p" translate="no">]</code>
    <code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="kc" translate="no">None</code><code class="p" translate="no">:</code>
        <code class="n" translate="no">point</code> <code class="o" translate="no">=</code> <code class="n" translate="no">PointStruct</code><code class="p" translate="no">(</code>
            <code class="nb" translate="no">id</code><code class="o" translate="no">=</code><code class="nb" translate="no">str</code><code class="p" translate="no">(</code><code class="n" translate="no">uuid</code><code class="o" translate="no">.</code><code class="n" translate="no">uuid4</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code>
            <code class="n" translate="no">vector</code><code class="o" translate="no">=</code><code class="n" translate="no">query_vector</code><code class="p" translate="no">,</code>
            <code class="n" translate="no">payload</code><code class="o" translate="no">=</code><code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">documents</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="n" translate="no">documents</code><code class="p" translate="no">}</code><code class="p" translate="no">,</code>
        <code class="p" translate="no">)</code>
        <code class="k" translate="no">await</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db</code><code class="o" translate="no">.</code><code class="n" translate="no">upload_points</code><code class="p" translate="no">(</code>
            <code class="n" translate="no">collection_name</code><code class="o" translate="no">=</code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">cache_collection_name</code><code class="p" translate="no">,</code> <code class="n" translate="no">points</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code><code class="n" translate="no">point</code><code class="p" translate="no">]</code>
        <code class="p" translate="no">)</code>

    <code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">search</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code> <code class="n" translate="no">query_vector</code><code class="p" translate="no">:</code> <code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">float</code><code class="p" translate="no">]</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="n" translate="no">ScoredPoint</code><code class="p" translate="no">]</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">return</code> <code class="k" translate="no">await</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db</code><code class="o" translate="no">.</code><code class="n" translate="no">search</code><code class="p" translate="no">(</code>
            <code class="n" translate="no">collection_name</code><code class="o" translate="no">=</code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">cache_collection_name</code><code class="p" translate="no">,</code>
            <code class="n" translate="no">query_vector</code><code class="o" translate="no">=</code><code class="n" translate="no">query_vector</code><code class="p" translate="no">,</code>
            <code class="n" translate="no">limit</code><code class="o" translate="no">=</code><code class="mi" translate="no">1</code><code class="p" translate="no">,</code>
        <code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO4-1" id="callout_optimizing_ai_services_CO4-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Inizializza un client Qdrant in esecuzione sulla memoria che funge da cache store.</p></dd>
</dl></div>
<p>Una volta inizializzato il client dell'archivio cache, puoi configurare l'archivio vettoriale dei documenti seguendo l'<a data-type="xref" href="#semantic_cache_doc_store">Esempio 10-7</a>.</p>
<div data-type="example" id="semantic_cache_doc_store">
<h5><span class="label">Esempio 10-7. </span>Client del negozio di documenti</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">qdrant_client</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">AsyncQdrantClient</code><code class="p" translate="no">,</code> <code class="n" translate="no">models</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">qdrant_client</code><code class="nn" translate="no">.</code><code class="nn" translate="no">http</code><code class="nn" translate="no">.</code><code class="nn" translate="no">models</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">Distance</code><code class="p" translate="no">,</code> <code class="n" translate="no">ScoredPoint</code>

<code class="n" translate="no">documents</code> <code class="o" translate="no">=</code> <code class="p" translate="no">[</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="p" translate="no">]</code> <a class="co" href="#callout_optimizing_ai_services_CO5-1" id="co_optimizing_ai_services_CO5-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>

<code class="k" translate="no">class</code> <code class="nc" translate="no">DocumentStoreClient</code><code class="p" translate="no">:</code>
    <code class="k" translate="no">def</code> <code class="fm" translate="no">__init__</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code> <code class="n" translate="no">host</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">localhost</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <code class="n" translate="no">port</code><code class="o" translate="no">=</code><code class="mi" translate="no">6333</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code>
        <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">AsyncQdrantClient</code><code class="p" translate="no">(</code><code class="n" translate="no">host</code><code class="o" translate="no">=</code><code class="n" translate="no">host</code><code class="p" translate="no">,</code> <code class="n" translate="no">port</code><code class="o" translate="no">=</code><code class="n" translate="no">port</code><code class="p" translate="no">)</code>
        <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">collection_name</code> <code class="o" translate="no">=</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">docs</code><code class="s2" translate="no">"</code>

    <code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">initialize_database</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="kc" translate="no">None</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">await</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">create_collection</code><code class="p" translate="no">(</code>
            <code class="n" translate="no">collection_name</code><code class="o" translate="no">=</code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">collection_name</code><code class="p" translate="no">,</code>
            <code class="n" translate="no">vectors_config</code><code class="o" translate="no">=</code><code class="n" translate="no">models</code><code class="o" translate="no">.</code><code class="n" translate="no">VectorParams</code><code class="p" translate="no">(</code>
                <code class="n" translate="no">size</code><code class="o" translate="no">=</code><code class="mi" translate="no">384</code><code class="p" translate="no">,</code> <code class="n" translate="no">distance</code><code class="o" translate="no">=</code><code class="n" translate="no">Distance</code><code class="o" translate="no">.</code><code class="n" translate="no">EUCLID</code>
            <code class="p" translate="no">)</code><code class="p" translate="no">,</code>
        <code class="p" translate="no">)</code>
        <code class="k" translate="no">await</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">add</code><code class="p" translate="no">(</code>
            <code class="n" translate="no">documents</code><code class="o" translate="no">=</code><code class="n" translate="no">documents</code><code class="p" translate="no">,</code> <code class="n" translate="no">collection_name</code><code class="o" translate="no">=</code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">collection_name</code>
        <code class="p" translate="no">)</code>

    <code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">search</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code> <code class="n" translate="no">query_vector</code><code class="p" translate="no">:</code> <code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">float</code><code class="p" translate="no">]</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="n" translate="no">ScoredPoint</code><code class="p" translate="no">]</code><code class="p" translate="no">:</code>
        <code class="n" translate="no">results</code> <code class="o" translate="no">=</code> <code class="k" translate="no">await</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">search</code><code class="p" translate="no">(</code>
            <code class="n" translate="no">query_vector</code><code class="o" translate="no">=</code><code class="n" translate="no">query_vector</code><code class="p" translate="no">,</code>
            <code class="n" translate="no">limit</code><code class="o" translate="no">=</code><code class="mi" translate="no">3</code><code class="p" translate="no">,</code>
            <code class="n" translate="no">collection_name</code><code class="o" translate="no">=</code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">collection_name</code><code class="p" translate="no">,</code>
        <code class="p" translate="no">)</code>
        <code class="k" translate="no">return</code> <code class="n" translate="no">results</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO5-1" id="callout_optimizing_ai_services_CO5-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Carica una collezione di documenti nell'archivio vettoriale di Qdrant.</p></dd>
</dl></div>
<p>Con i client della cache e dell'archivio vettoriale di documenti pronti, puoi ora implementare il servizio di cache semantica, come mostrato nell'<a data-type="xref" href="#semantic_cache_service">Esempio 10-8</a>, con i metodi per calcolare gli embeddings ed eseguire le ricerche nella cache.</p>
<div data-type="example" id="semantic_cache_service">
<h5><span class="label">Esempio 10-8. </span>Sistema di caching semantico</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code> <code class="nn" translate="no">time</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">loguru</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">logger</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">transformers</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">AutoModel</code>

<code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code>

<code class="k" translate="no">class</code> <code class="nc" translate="no">SemanticCacheService</code><code class="p" translate="no">:</code>
    <code class="k" translate="no">def</code> <code class="fm" translate="no">__init__</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code> <code class="n" translate="no">threshold</code><code class="p" translate="no">:</code> <code class="nb" translate="no">float</code> <code class="o" translate="no">=</code> <code class="mf" translate="no">0.35</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code>
        <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">embedder</code> <code class="o" translate="no">=</code> <code class="n" translate="no">AutoModel</code><code class="o" translate="no">.</code><code class="n" translate="no">from_pretrained</code><code class="p" translate="no">(</code>
            <code class="s2" translate="no">"</code><code class="s2" translate="no">jinaai/jina-embeddings-v2-base-en</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <code class="n" translate="no">trust_remote_code</code><code class="o" translate="no">=</code><code class="kc" translate="no">True</code>
        <code class="p" translate="no">)</code>
        <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">euclidean_threshold</code> <code class="o" translate="no">=</code> <code class="n" translate="no">threshold</code>
        <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">cache_client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">CacheClient</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code>
        <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">doc_db_client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">DocumentStoreClient</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code>

    <code class="k" translate="no">def</code> <code class="nf" translate="no">get_embedding</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code> <code class="n" translate="no">question</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">float</code><code class="p" translate="no">]</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">return</code> <code class="nb" translate="no">list</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">embedder</code><code class="o" translate="no">.</code><code class="n" translate="no">embed</code><code class="p" translate="no">(</code><code class="n" translate="no">question</code><code class="p" translate="no">)</code><code class="p" translate="no">)</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">]</code>

    <code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">initialize_databases</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">await</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">cache_client</code><code class="o" translate="no">.</code><code class="n" translate="no">initialize_databases</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code>
        <code class="k" translate="no">await</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">doc_db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">initialize_databases</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code>

    <code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">ask</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code> <code class="n" translate="no">query</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="nb" translate="no">str</code><code class="p" translate="no">:</code>
        <code class="n" translate="no">start_time</code> <code class="o" translate="no">=</code> <code class="n" translate="no">time</code><code class="o" translate="no">.</code><code class="n" translate="no">time</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code>
        <code class="n" translate="no">vector</code> <code class="o" translate="no">=</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">get_embedding</code><code class="p" translate="no">(</code><code class="n" translate="no">query</code><code class="p" translate="no">)</code>
        <code class="k" translate="no">if</code> <code class="n" translate="no">search_results</code> <code class="o" translate="no">:=</code> <code class="k" translate="no">await</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">cache_client</code><code class="o" translate="no">.</code><code class="n" translate="no">search</code><code class="p" translate="no">(</code><code class="n" translate="no">vector</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code>
            <code class="k" translate="no">for</code> <code class="n" translate="no">s</code> <code class="ow" translate="no">in</code> <code class="n" translate="no">search_results</code><code class="p" translate="no">:</code>
                <code class="k" translate="no">if</code> <code class="n" translate="no">s</code><code class="o" translate="no">.</code><code class="n" translate="no">score</code> <code class="o" translate="no">&lt;</code><code class="o" translate="no">=</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">euclidean_threshold</code><code class="p" translate="no">:</code> <a class="co" href="#callout_optimizing_ai_services_CO6-1" id="co_optimizing_ai_services_CO6-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
                    <code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Found cache with score </code><code class="si" translate="no">{</code><code class="n" translate="no">s</code><code class="o" translate="no">.</code><code class="n" translate="no">score</code><code class="si" translate="no">:</code><code class="s2" translate="no">.3f</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
                    <code class="n" translate="no">elapsed_time</code> <code class="o" translate="no">=</code> <code class="n" translate="no">time</code><code class="o" translate="no">.</code><code class="n" translate="no">time</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code> <code class="n" translate="no">start_time</code>
                    <code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Time taken: </code><code class="si" translate="no">{</code><code class="n" translate="no">elapsed_time</code><code class="si" translate="no">:</code><code class="s2" translate="no">.3f</code><code class="si" translate="no">}</code><code class="s2" translate="no"> seconds</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
                    <code class="k" translate="no">return</code> <code class="n" translate="no">s</code><code class="o" translate="no">.</code><code class="n" translate="no">payload</code><code class="p" translate="no">[</code><code class="s2" translate="no">"</code><code class="s2" translate="no">content</code><code class="s2" translate="no">"</code><code class="p" translate="no">]</code>

        <code class="k" translate="no">if</code> <code class="n" translate="no">db_results</code> <code class="o" translate="no">:=</code> <code class="k" translate="no">await</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">doc_db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">search</code><code class="p" translate="no">(</code><code class="n" translate="no">vector</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code> <a class="co" href="#callout_optimizing_ai_services_CO6-2" id="co_optimizing_ai_services_CO6-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a>
            <code class="n" translate="no">documents</code> <code class="o" translate="no">=</code> <code class="p" translate="no">[</code><code class="n" translate="no">r</code><code class="o" translate="no">.</code><code class="n" translate="no">payload</code><code class="p" translate="no">[</code><code class="s2" translate="no">"</code><code class="s2" translate="no">content</code><code class="s2" translate="no">"</code><code class="p" translate="no">]</code> <code class="k" translate="no">for</code> <code class="n" translate="no">r</code> <code class="ow" translate="no">in</code> <code class="n" translate="no">db_results</code><code class="p" translate="no">]</code>
            <code class="k" translate="no">await</code> <code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">cache_client</code><code class="o" translate="no">.</code><code class="n" translate="no">insert</code><code class="p" translate="no">(</code><code class="n" translate="no">vector</code><code class="p" translate="no">,</code> <code class="n" translate="no">documents</code><code class="p" translate="no">)</code>
            <code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Query context inserted to Cache.</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
            <code class="n" translate="no">elapsed_time</code> <code class="o" translate="no">=</code> <code class="n" translate="no">time</code><code class="o" translate="no">.</code><code class="n" translate="no">time</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code> <code class="n" translate="no">start_time</code>
            <code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Time taken: </code><code class="si" translate="no">{</code><code class="n" translate="no">elapsed_time</code><code class="si" translate="no">:</code><code class="s2" translate="no">.3f</code><code class="si" translate="no">}</code><code class="s2" translate="no"> seconds</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>

        <code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">No answer found in Cache or Database.</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
        <code class="n" translate="no">elapsed_time</code> <code class="o" translate="no">=</code> <code class="n" translate="no">time</code><code class="o" translate="no">.</code><code class="n" translate="no">time</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code> <code class="n" translate="no">start_time</code>
        <code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Time taken: </code><code class="si" translate="no">{</code><code class="n" translate="no">elapsed_time</code><code class="si" translate="no">:</code><code class="s2" translate="no">.3f</code><code class="si" translate="no">}</code><code class="s2" translate="no"> seconds</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
        <code class="k" translate="no">return</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">No answer available.</code><code class="s2" translate="no">"</code> <a class="co" href="#callout_optimizing_ai_services_CO6-3" id="co_optimizing_ai_services_CO6-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO6-1" id="callout_optimizing_ai_services_CO6-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Imposta una soglia di somiglianza. Qualsiasi punteggio superiore a questa soglia sarà un hit della cache.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO6-2" id="callout_optimizing_ai_services_CO6-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Interroga l'archivio dei documenti se non c'è un riscontro nella cache. Memorizza nella cache i documenti recuperati con l'incorporazione vettoriale della query come chiave della cache.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO6-3" id="callout_optimizing_ai_services_CO6-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Se non ci sono documenti correlati o cache disponibili per la query data, restituisce una risposta in scatola.</p></dd>
</dl></div>
<p>Ora che hai un servizio di caching semantico, puoi usarlo per recuperare i documenti in cache dalla memoria seguendo l'<a data-type="xref" href="#semantic_cache_qdrant_usage">Esempio 10-9</a>.</p>
<div data-type="example" id="semantic_cache_qdrant_usage">
<h5><span class="label">Esempio 10-9. </span>Implementazione di una cache semantica in un sistema RAG con Qdrant</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">main</code><code class="p" translate="no">():</code>
    <code class="n" translate="no">cache_service</code> <code class="o" translate="no">=</code> <code class="n" translate="no">SemanticCacheService</code><code class="p" translate="no">()</code>
    <code class="n" translate="no">query_1</code> <code class="o" translate="no">=</code> <code class="s2" translate="no">"How to build GenAI services?"</code>
    <code class="n" translate="no">query_2</code> <code class="o" translate="no">=</code> <code class="s2" translate="no">"What is the process for developing GenAI services?"</code>

    <code class="n" translate="no">cache_service</code><code class="o" translate="no">.</code><code class="n" translate="no">ask</code><code class="p" translate="no">(</code><code class="n" translate="no">query_1</code><code class="p" translate="no">)</code>
    <code class="n" translate="no">cache_service</code><code class="o" translate="no">.</code><code class="n" translate="no">ask</code><code class="p" translate="no">(</code><code class="n" translate="no">query_2</code><code class="p" translate="no">)</code>

<code class="n" translate="no">asyncio</code><code class="o" translate="no">.</code><code class="n" translate="no">run</code><code class="p" translate="no">(</code><code class="n" translate="no">main</code><code class="p" translate="no">())</code>

<code class="c1" translate="no"># Query 1:</code>
<code class="c1" translate="no"># Query added to Cache.</code>
<code class="c1" translate="no"># Time taken: 0.822 seconds</code>

<code class="c1" translate="no"># Query 2:</code>
<code class="c1" translate="no"># Found cache with score 0.329</code>
<code class="c1" translate="no"># Time taken: 0.016 seconds</code></pre></div>
<p>Ora dovresti aver capito meglio come implementare i tuoi sistemi di caching semantico personalizzati utilizzando un client di database vettoriale.<a data-startref="ix_ch10-asciidoc13" data-type="indexterm" id="id1077"/><a data-startref="ix_ch10-asciidoc12" data-type="indexterm" id="id1078"/></p>
</div></section>
<section data-pdf-bookmark="Semantic caching with GPT cache" data-type="sect4"><div class="sect4" id="id150">
<h4>Caching semantico con la cache GPT</h4>
<p><a data-primary="caching" data-secondary="semantic caching" data-tertiary="GPT cache" data-type="indexterm" id="ix_ch10-asciidoc14"/><a data-primary="GPT cache" data-type="indexterm" id="ix_ch10-asciidoc15"/><a data-primary="semantic caching" data-secondary="GPT cache" data-type="indexterm" id="ix_ch10-asciidoc16"/>Se non hai bisogno di sviluppare il tuo servizio di caching semantico da zero, puoi anche utilizzare la libreria modulare <code translate="no">gptcache</code> che ti dà la possibilità di scambiare vari componenti di archiviazione, caching e incorporazione.</p>
<p>Per configurare una cache semantica con <code translate="no">gptcache</code>, devi prima installare il pacchetto:</p>
<pre data-type="programlisting" translate="no">$ pip install gptcache</pre>
<p class="less_space pagebreak-before">Quindi carica il sistema all'avvio dell'applicazione, come mostrato nell'<a data-type="xref" href="#configrue_gptcache">Esempio 10-10</a>.</p>
<div data-type="example" id="configrue_gptcache">
<h5><span class="label">Esempio 10-10. </span>Configurazione della cache GPT</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">contextlib</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">asynccontextmanager</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">typing</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">AsyncIterator</code>

<code class="kn" translate="no">from</code> <code class="nn" translate="no">fastapi</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">FastAPI</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">gptcache</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">Config</code><code class="p" translate="no">,</code> <code class="n" translate="no">cache</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">gptcache</code><code class="nn" translate="no">.</code><code class="nn" translate="no">embedding</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">Onnx</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">gptcache</code><code class="nn" translate="no">.</code><code class="nn" translate="no">processor</code><code class="nn" translate="no">.</code><code class="nn" translate="no">post</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">random_one</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">gptcache</code><code class="nn" translate="no">.</code><code class="nn" translate="no">processor</code><code class="nn" translate="no">.</code><code class="nn" translate="no">pre</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">last_content</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">gptcache</code><code class="nn" translate="no">.</code><code class="nn" translate="no">similarity_evaluation</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">OnnxModelEvaluation</code>

<code class="nd" translate="no">@asynccontextmanager</code>
<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">lifespan</code><code class="p" translate="no">(</code><code class="n" translate="no">_</code><code class="p" translate="no">:</code> <code class="n" translate="no">FastAPI</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="n" translate="no">AsyncIterator</code><code class="p" translate="no">[</code><code class="kc" translate="no">None</code><code class="p" translate="no">]</code><code class="p" translate="no">:</code>
    <code class="n" translate="no">cache</code><code class="o" translate="no">.</code><code class="n" translate="no">init</code><code class="p" translate="no">(</code>
        <code class="n" translate="no">post_func</code><code class="o" translate="no">=</code><code class="n" translate="no">random_one</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO7-1" id="co_optimizing_ai_services_CO7-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
        <code class="n" translate="no">pre_embedding_func</code><code class="o" translate="no">=</code><code class="n" translate="no">last_content</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO7-2" id="co_optimizing_ai_services_CO7-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a>
        <code class="n" translate="no">embedding_func</code><code class="o" translate="no">=</code><code class="n" translate="no">Onnx</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="o" translate="no">.</code><code class="n" translate="no">to_embeddings</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO7-3" id="co_optimizing_ai_services_CO7-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a>
        <code class="n" translate="no">similarity_evaluation</code><code class="o" translate="no">=</code><code class="n" translate="no">OnnxModelEvaluation</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO7-4" id="co_optimizing_ai_services_CO7-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a>
        <code class="n" translate="no">config</code><code class="o" translate="no">=</code><code class="n" translate="no">Config</code><code class="p" translate="no">(</code><code class="n" translate="no">similarity_threshold</code><code class="o" translate="no">=</code><code class="mf" translate="no">0.75</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO7-5" id="co_optimizing_ai_services_CO7-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a>
    <code class="p" translate="no">)</code>
    <code class="n" translate="no">cache</code><code class="o" translate="no">.</code><code class="n" translate="no">set_openai_key</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code> <a class="co" href="#callout_optimizing_ai_services_CO7-6" id="co_optimizing_ai_services_CO7-6"><img alt="6" src="assets/6.png" width="12" height="12"/></a>
    <code class="k" translate="no">yield</code>

<code class="n" translate="no">app</code> <code class="o" translate="no">=</code> <code class="n" translate="no">FastAPI</code><code class="p" translate="no">(</code><code class="n" translate="no">lifespan</code><code class="o" translate="no">=</code><code class="n" translate="no">lifespan</code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO7-1" id="callout_optimizing_ai_services_CO7-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Seleziona una funzione di callback post-elaborazione per selezionare un elemento casuale tra quelli restituiti nella cache.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO7-2" id="callout_optimizing_ai_services_CO7-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Seleziona una funzione di callback pre-inclusione per utilizzare l'ultima query per impostare una nuova cache.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO7-3" id="callout_optimizing_ai_services_CO7-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Usa il modello di incorporazione ONNX per calcolare i vettori di incorporazione.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO7-4" id="callout_optimizing_ai_services_CO7-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza <code translate="no">OnnxModelEvaluation</code> per calcolare i punteggi di somiglianza tra gli elementi in cache e una determinata query.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO7-5" id="callout_optimizing_ai_services_CO7-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a></dt>
<dd><p>Imposta le opzioni di configurazione della cache, come ad esempio la soglia di somiglianza.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO7-6" id="callout_optimizing_ai_services_CO7-6"><img alt="6" src="assets/6.png" width="12" height="12"/></a></dt>
<dd><p>Fornisci una chiave API del client OpenAI per GPT Cache per eseguire automaticamente la cache semantica sulle risposte dell'API LLM.</p></dd>
</dl></div>
<p class="less_space pagebreak-before">Una volta inizializzato, <code translate="no">gptcache</code> si integrerà perfettamente con il client LLM di OpenAI nella tua applicazione. Ora puoi effettuare query multiple LLM, come mostrato nell'<a data-type="xref" href="#semantic_caching_gptcache">Esempio 10-11</a>, sapendo che <code translate="no">gptcache</code> metterà in cache le risposte LLM.</p>
<div data-type="example" id="semantic_caching_gptcache">
<h5><span class="label">Esempio 10-11. </span>Caching semantico con la cache GPT</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code> <code class="nn" translate="no">time</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">openai</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">OpenAI</code>
<code class="n" translate="no">client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">OpenAI</code><code class="p" translate="no">()</code>

<code class="n" translate="no">question</code> <code class="o" translate="no">=</code> <code class="s2" translate="no">"what's FastAPI"</code>
<code class="k" translate="no">for</code> <code class="n" translate="no">_</code> <code class="ow" translate="no">in</code> <code class="nb" translate="no">range</code><code class="p" translate="no">(</code><code class="mi" translate="no">2</code><code class="p" translate="no">):</code>
    <code class="n" translate="no">start_time</code> <code class="o" translate="no">=</code> <code class="n" translate="no">time</code><code class="o" translate="no">.</code><code class="n" translate="no">time</code><code class="p" translate="no">()</code>
    <code class="n" translate="no">response</code> <code class="o" translate="no">=</code> <code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">chat</code><code class="o" translate="no">.</code><code class="n" translate="no">completions</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code>
        <code class="n" translate="no">model</code><code class="o" translate="no">=</code><code class="s2" translate="no">"gpt-4o"</code><code class="p" translate="no">,</code>
        <code class="n" translate="no">messages</code><code class="o" translate="no">=</code><code class="p" translate="no">[{</code><code class="s2" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"user"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="n" translate="no">question</code><code class="p" translate="no">}],</code>
    <code class="p" translate="no">)</code>
    <code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"Question: </code><code class="si" translate="no">{</code><code class="n" translate="no">question</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
    <code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="s2" translate="no">"Time consuming: </code><code class="si" translate="no">{:.2f}</code><code class="s2" translate="no">s"</code><code class="o" translate="no">.</code><code class="n" translate="no">format</code><code class="p" translate="no">(</code><code class="n" translate="no">time</code><code class="o" translate="no">.</code><code class="n" translate="no">time</code><code class="p" translate="no">()</code> <code class="o" translate="no">-</code> <code class="n" translate="no">start_time</code><code class="p" translate="no">))</code>
    <code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"Answer: </code><code class="si" translate="no">{</code><code class="n" translate="no">response</code><code class="o" translate="no">.</code><code class="n" translate="no">choices</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">]</code><code class="o" translate="no">.</code><code class="n" translate="no">message</code><code class="o" translate="no">.</code><code class="n" translate="no">content</code><code class="si" translate="no">}</code><code class="se" translate="no">\n</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code></pre></div>
<p>L'utilizzo di librerie esterne come <code translate="no">gptcache</code>, come mostrato nell'<a data-type="xref" href="#semantic_caching_gptcache">Esempio 10-11</a>, rende semplice l'implementazione del caching semantico.</p>
<p>Una volta che il sistema di caching è attivo e funzionante, puoi regolare <em>le soglie di somiglianza</em> per ottimizzare le percentuali di successo della cache.<a data-startref="ix_ch10-asciidoc16" data-type="indexterm" id="id1079"/><a data-startref="ix_ch10-asciidoc15" data-type="indexterm" id="id1080"/><a data-startref="ix_ch10-asciidoc14" data-type="indexterm" id="id1081"/></p>
</div></section>
<section data-pdf-bookmark="Similarity threshold" data-type="sect4"><div class="sect4" id="id151">
<h4>Soglia di somiglianza</h4>
<p><a data-primary="caching" data-secondary="semantic caching" data-tertiary="similarity threshold adjustment" data-type="indexterm" id="ix_ch10-asciidoc17"/><a data-primary="semantic caching" data-secondary="similarity threshold adjustment" data-type="indexterm" id="ix_ch10-asciidoc18"/><a data-primary="similarity thresholds" data-type="indexterm" id="ix_ch10-asciidoc19"/>Quando crei un servizio di cache semantica, potresti dover regolare la soglia di somiglianza in base alle query fornite per ottenere tassi di risposta alla cache elevati e accurati. Puoi fare riferimento alla <a href="https://semanticcachehit.com">visualizzazione interattiva dei cluster della cache semantica</a> mostrata nella <a data-type="xref" href="#semantic_cache_visualization">Figura 10-2</a> per capire meglio il concetto di<span class="keep-together">soglia</span> di somiglianza.</p>
<p>Aumentando il valore della soglia nella <a data-type="xref" href="#semantic_cache_visualization">Figura 10-2</a> si otterrà un grafo meno connesso, mentre riducendolo al minimo si possono ottenere dei falsi positivi. Pertanto, è consigliabile eseguire alcuni esperimenti per mettere a punto la soglia di somiglianza per la propria applicazione.</p>
<figure><div class="figure" id="semantic_cache_visualization">
<img alt="bgai 1002" src="assets/bgai_1002.png" width="1551" height="1212"/>
<h6><span class="label">Figura 10-2. </span>Visualizzazione del caching semantico (Fonte: <a class="orm:hideurl" href="https://semanticcachehit.com">semanticcachehit.com</a>)</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Eviction policies" data-type="sect4"><div class="sect4" id="id152">
<h4>Politiche di sfratto</h4>
<p><a data-primary="caching" data-secondary="semantic caching" data-tertiary="eviction policies" data-type="indexterm" id="id1082"/><a data-primary="eviction policies" data-type="indexterm" id="id1083"/><a data-primary="semantic caching" data-secondary="eviction policies" data-type="indexterm" id="id1084"/>Un altro concetto rilevante per la cache è quello delle <em>politiche di eviction</em> che controllano il comportamento della cache quando il meccanismo di caching raggiunge la sua capacità massima. La selezione della politica di eviction appropriata deve essere adatta al tuo caso d'uso.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Dato che le dimensioni della memoria cache sono spesso limitate, puoi aggiungere un metodo <code translate="no">evict()</code> al <code translate="no">SemanticCachingService</code> che hai implementato nell'<a data-type="xref" href="#semantic_cache_service">Esempio 10-8</a>.</p>
</div>
<p class="less_space pagebreak-before">La<a data-type="xref" href="#eviction_policies">Tabella 10-1</a> mostra alcuni criteri di sfratto che puoi scegliere.</p>
<table class="striped" id="eviction_policies">
<caption><span class="label">Tabella 10-1. </span>Politiche di sfratto</caption>
<thead>
<tr>
<th>Politica</th>
<th>Descrizione</th>
<th>Caso d'uso</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Primo entrato, primo uscito (FIFO)</p></td>
<td><p>Rimuove gli elementi più vecchi</p></td>
<td><p>Quando tutti gli elementi hanno la stessa priorità</p></td>
</tr>
<tr>
<td><p>Ultimo usato (LRU)</p></td>
<td><p>Traccia l'utilizzo della cache nel tempo e rimuove l'ultimo elemento consultato.</p></td>
<td><p>Quando è più probabile che gli elementi consultati di recente vengano consultati nuovamente</p></td>
</tr>
<tr>
<td><p>Utilizzato meno frequentemente (LFU)</p></td>
<td><p>Traccia l'utilizzo della cache nel tempo e rimuove l'elemento a cui si accede meno frequentemente.</p></td>
<td><p>Quando gli articoli usati meno frequentemente devono essere rimossi per primi</p></td>
</tr>
<tr>
<td><p>Utilizzato più di recente (MRU)</p></td>
<td><p>Traccia l'utilizzo della cache nel tempo e rimuove l'ultimo elemento consultato.</p></td>
<td><p>Utilizzato raramente, quando gli articoli usati più di recente hanno meno probabilità di essere consultati di nuovo.</p></td>
</tr>
<tr>
<td><p>Sostituzione casuale (RR)</p></td>
<td><p>Rimuove un elemento casuale dalla cache</p></td>
<td><p>Semplice e veloce, da usare quando non ha un impatto sulle prestazioni</p></td>
</tr>
</tbody>
</table>
<p>La scelta del giusto criterio di sfratto dipende dal tuo caso d'uso e dai requisiti dell'applicazione. In generale, puoi iniziare con il criterio LRU prima di passare alle<span class="keep-together">alternative.</span></p>
<p>Ora dovresti sentirti più sicuro nell'implementazione di meccanismi di caching semantico che si applicano al recupero di documenti o alle risposte dei modelli. Poi, impariamo a conoscere il caching contestuale o prompt, che ottimizza le query ai modelli in base ai loro input.<a data-startref="ix_ch10-asciidoc11" data-type="indexterm" id="id1085"/><a data-startref="ix_ch10-asciidoc10" data-type="indexterm" id="id1086"/><a data-startref="ix_ch10-asciidoc9" data-type="indexterm" id="id1087"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Context/prompt caching" data-type="sect3"><div class="sect3" id="id153">
<h3>Caching del contesto e del prompt</h3>
<p><a data-primary="caching" data-secondary="context/prompt caching" data-type="indexterm" id="ix_ch10-asciidoc20"/><a data-primary="context (prompt) caching" data-type="indexterm" id="ix_ch10-asciidoc21"/><a data-primary="optimizing AI services" data-secondary="caching" data-tertiary="context/prompt caching" data-type="indexterm" id="ix_ch10-asciidoc22"/><a data-primary="prompt (context) caching" data-type="indexterm" id="ix_ch10-asciidoc23"/>Il<em>caching del contesto</em>, noto anche come <em>prompt caching</em>, è un meccanismo di caching adatto a scenari in cui si fa riferimento a grandi quantità di contesto ripetutamente all'interno di piccole richieste. È stato progettato per riutilizzare gli stati di attenzione precalcolati da prompt frequentemente riutilizzati, eliminando la necessità di ricompilare in modo ridondante l'intero contesto di input ogni volta che viene effettuata una nuova richiesta.</p>
<p>Dovresti prendere in considerazione l'utilizzo di una cache contestuale quando i tuoi servizi prevedono quanto segue:</p>
<ul>
<li>
<p>Chatbot con istruzioni di sistema estese e lunghe conversazioni a più turni</p>
</li>
<li>
<p>Analisi ripetitiva di file video lunghi</p>
</li>
<li>
<p>Query ricorrenti su set di documenti di grandi dimensioni</p>
</li>
<li>
<p>Analisi frequente del repository di codice o correzione di bug</p>
</li>
<li>
<p>Riassunti di documenti, libri, documenti, trascrizioni di podcast e altri contenuti di lunga durata.</p>
</li>
<li>
<p>Fornire un gran numero di esempi in prompt (cioè l'apprendimento nel contesto).</p>
</li>
</ul>
<p>Secondo Anthropic, il caching dei prompt può ridurre i costi fino al 90% e la latenza fino all'85% per i prompt lunghi.</p>
<p>Gli autori del <a href="https://oreil.ly/augpd">documento sul prompt caching</a> che presenta questa tecnica affermano anche che:</p>
<blockquote>
<p>Abbiamo scoperto che la Prompt Cache riduce significativamente la latenza nel time-to-first-token, soprattutto per i prompt più lunghi come le risposte alle domande basate su documenti e le raccomandazioni. I miglioramenti vanno da 8× per l'inferenza basata su GPU a 60× per l'inferenza basata su CPU, il tutto mantenendo l'accuratezza dell'output e senza la necessità di modificare i parametri del modello.</p></blockquote>
<p>La<a data-type="xref" href="#context_caching_architecture">Figura 10-3</a> visualizza l'architettura del sistema di caching del contesto.</p>
<figure><div class="figure" id="context_caching_architecture">
<img alt="bgai 1003" src="assets/bgai_1003.png" width="1044" height="448"/>
<h6><span class="label">Figura 10-3. </span>Architettura del sistema per il caching del contesto</h6>
</div></figure>
<p>Al momento in cui scriviamo, OpenAI implementa automaticamente il prompt caching per tutte le richieste API senza richiedere modifiche al codice o costi aggiuntivi. L<a data-type="xref" href="#context_cachin_anthropic">'esempio 10-12</a> mostra un esempio di utilizzo del prompt caching quando si utilizza l'API Anthropic.</p>
<div data-type="example" id="context_cachin_anthropic">
<h5><span class="label">Esempio 10-12. </span>Caching del contesto e del prompt con l'API Antropica</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">anthropic</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">Anthropic</code>

<code class="n" translate="no">client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">Anthropic</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code>

<code class="n" translate="no">response</code> <code class="o" translate="no">=</code> <code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">messages</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code>
    <code class="n" translate="no">model</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">claude-3-7-sonnet-20250219</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO8-1" id="co_optimizing_ai_services_CO8-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
    <code class="n" translate="no">max_tokens</code><code class="o" translate="no">=</code><code class="mi" translate="no">1024</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">system</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code>
        <code class="p" translate="no">{</code>
            <code class="s2" translate="no">"</code><code class="s2" translate="no">type</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">text</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code>
            <code class="s2" translate="no">"</code><code class="s2" translate="no">text</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">You are an AI assistant</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code>
        <code class="p" translate="no">}</code><code class="p" translate="no">,</code>
        <code class="p" translate="no">{</code>
            <code class="s2" translate="no">"</code><code class="s2" translate="no">type</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">text</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code>
            <code class="s2" translate="no">"</code><code class="s2" translate="no">text</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">&lt;the entire content of a large document&gt;</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code>
            <code class="s2" translate="no">"</code><code class="s2" translate="no">cache_control</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">type</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">ephemeral</code><code class="s2" translate="no">"</code><code class="p" translate="no">}</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO8-2" id="co_optimizing_ai_services_CO8-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a>
        <code class="p" translate="no">}</code><code class="p" translate="no">,</code>
    <code class="p" translate="no">]</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">messages</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code><code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">role</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">user</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">content</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">Summarize the documents in ...</code><code class="s2" translate="no">"</code><code class="p" translate="no">}</code><code class="p" translate="no">]</code><code class="p" translate="no">,</code>
<code class="p" translate="no">)</code>
<code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="n" translate="no">response</code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO8-1" id="callout_optimizing_ai_services_CO8-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Il prompt caching è disponibile solo per alcuni modelli, tra cui Claude 3.5 Sonnet, Claude 3 Haiku e Claude 3 Opus.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO8-2" id="callout_optimizing_ai_services_CO8-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza il parametro <code translate="no">cache_control</code> per riutilizzare il contenuto del documento di grandi dimensioni in più chiamate API senza elaborarlo ogni volta.</p>
<p>Sotto il cofano, il client Anthropic aggiunge <code translate="no">anthropic-beta: prompt-caching-2024-07-31</code> alle intestazioni delle richieste.</p>
<p>Al momento in cui scriviamo, <code translate="no">ephemeral</code> è l'unico tipo di cache supportato, che corrisponde a una durata della cache di 5 minuti.</p></dd>
</dl></div>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>Quando adotti una cache del contesto, introduci la statualità nelle richieste preservando i token in tutte le richieste. Ciò significa che i dati inviati in una richiesta influenzeranno le richieste successive, in quanto il server del provider del modello può utilizzare il contesto memorizzato nella cache per mantenere la continuità tra le interazioni.</p>
</div>
<p>Con la funzione di caching del contesto dell'API Gemini, puoi fornire il contenuto al modello una sola volta, mettere in cache i token di input e fare riferimento a questi token in cache per le richieste future.</p>
<p>L'uso di questi token nella cache può farti risparmiare una spesa significativa se eviti di passare ripetutamente lo stesso corpus di token in volumi elevati.<a data-primary="time to live (TTL)" data-type="indexterm" id="id1088"/><a data-primary="TTL (time to live)" data-type="indexterm" id="id1089"/>Il costo della cache dipenderà dalle dimensioni dei token in ingresso e dalla durata di memorizzazione del time to live (TTL) desiderato.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Quando metti in cache un insieme di token, puoi specificare la durata del TTL, ossia il tempo in cui la cache deve esistere prima che i token vengano eliminati automaticamente. Per impostazione predefinita, il TTL è normalmente impostato su 1 ora.</p>
</div>
<p>Puoi vedere come utilizzare un'istruzione di sistema nella cache nell'<a data-type="xref" href="#context_caching_google">Esempio 10-13</a>. Ti servirà anche il Gemini API Python SDK:</p>
<pre data-type="programlisting" translate="no">$ pip install google-generativeai</pre>
<div data-type="example" id="context_caching_google">
<h5><span class="label">Esempio 10-13. </span>Caching del contesto con l'API Google Gemini</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code> <code class="nn" translate="no">datetime</code>
<code class="kn" translate="no">import</code> <code class="nn" translate="no">google</code><code class="nn" translate="no">.</code><code class="nn" translate="no">generativeai</code> <code class="k" translate="no">as</code> <code class="nn" translate="no">genai</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">google</code><code class="nn" translate="no">.</code><code class="nn" translate="no">generativeai</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">caching</code>

<code class="n" translate="no">genai</code><code class="o" translate="no">.</code><code class="n" translate="no">configure</code><code class="p" translate="no">(</code><code class="n" translate="no">api_key</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">your_gemini_api_key</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>

<code class="n" translate="no">corpus</code> <code class="o" translate="no">=</code> <code class="n" translate="no">genai</code><code class="o" translate="no">.</code><code class="n" translate="no">upload_file</code><code class="p" translate="no">(</code><code class="n" translate="no">path</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">corpus.txt</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
<code class="n" translate="no">cache</code> <code class="o" translate="no">=</code> <code class="n" translate="no">caching</code><code class="o" translate="no">.</code><code class="n" translate="no">CachedContent</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code>
    <code class="n" translate="no">model</code><code class="o" translate="no">=</code><code class="s1" translate="no">'</code><code class="s1" translate="no">models/gemini-1.5-flash-001</code><code class="s1" translate="no">'</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">display_name</code><code class="o" translate="no">=</code><code class="s1" translate="no">'</code><code class="s1" translate="no">fastapi</code><code class="s1" translate="no">'</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO9-1" id="co_optimizing_ai_services_CO9-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
    <code class="n" translate="no">system_instruction</code><code class="o" translate="no">=</code><code class="p" translate="no">(</code>
        <code class="s2" translate="no">"</code><code class="s2" translate="no">You are an expert AI engineer, and your job is to answer </code><code class="s2" translate="no">"</code>
        <code class="s2" translate="no">"</code><code class="s2" translate="no">the user</code><code class="s2" translate="no">'</code><code class="s2" translate="no">s query based on the files you have access to.</code><code class="s2" translate="no">"</code>
    <code class="p" translate="no">)</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">contents</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code><code class="n" translate="no">corpus</code><code class="p" translate="no">]</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO9-2" id="co_optimizing_ai_services_CO9-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a>
    <code class="n" translate="no">ttl</code><code class="o" translate="no">=</code><code class="n" translate="no">datetime</code><code class="o" translate="no">.</code><code class="n" translate="no">timedelta</code><code class="p" translate="no">(</code><code class="n" translate="no">minutes</code><code class="o" translate="no">=</code><code class="mi" translate="no">5</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code>
<code class="p" translate="no">)</code>

<code class="n" translate="no">model</code> <code class="o" translate="no">=</code> <code class="n" translate="no">genai</code><code class="o" translate="no">.</code><code class="n" translate="no">GenerativeModel</code><code class="o" translate="no">.</code><code class="n" translate="no">from_cached_content</code><code class="p" translate="no">(</code><code class="n" translate="no">cached_content</code><code class="o" translate="no">=</code><code class="n" translate="no">cache</code><code class="p" translate="no">)</code>
<code class="n" translate="no">response</code> <code class="o" translate="no">=</code> <code class="n" translate="no">model</code><code class="o" translate="no">.</code><code class="n" translate="no">generate_content</code><code class="p" translate="no">(</code>
    <code class="p" translate="no">[</code>
        <code class="p" translate="no">(</code>
            <code class="s2" translate="no">"</code><code class="s2" translate="no">Introduce different characters in the movie by describing </code><code class="s2" translate="no">"</code>
            <code class="s2" translate="no">"</code><code class="s2" translate="no">their personality, looks, and names. Also list the timestamps </code><code class="s2" translate="no">"</code>
            <code class="s2" translate="no">"</code><code class="s2" translate="no">they were introduced for the first time.</code><code class="s2" translate="no">"</code>
        <code class="p" translate="no">)</code>
    <code class="p" translate="no">]</code>
<code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO9-1" id="callout_optimizing_ai_services_CO9-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Fornisci un nome visualizzato come chiave o identificatore della cache.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO9-2" id="callout_optimizing_ai_services_CO9-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Passa il corpus al sistema di cache contestuale. La dimensione minima di una cache contestuale è di 32.768 token.</p></dd>
</dl></div>
<p>Se esegui l'<a data-type="xref" href="#context_caching_google">Esempio 10-13</a> e stampi il sito <code translate="no">response.usage_metadata</code>, dovresti ricevere il seguente output:</p>
<pre data-type="programlisting" translate="no">&gt;&gt; print(response.usage_metadata)

prompt_token_count: 696219
cached_content_token_count: 696190
candidates_token_count: 214
total_token_count: 696433</pre>
<p>Si noti come la maggior parte di <code translate="no">prompt_token_count</code> viene ora messa in cache se la si confronta con <code translate="no">cached_content_token_count</code>. <code translate="no">candidates_token_count</code> si riferisce al conteggio dei token di output o di risposta provenienti dal modello, che non viene influenzato dal sistema di cache.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>I modelli Gemini non fanno distinzione tra i token memorizzati nella cache e i normali token di input. Il contenuto memorizzato nella cache viene anteposto al prompt. Per questo motivo il numero di token del prompt non viene ridotto quando si utilizza la cache.</p>
</div>
<p class="less_space pagebreak-before">Con la cache del contesto, non vedrai una drastica riduzione dei tempi di risposta, ma ridurrai in modo significativo i costi operativi perché eviterai di inviare nuovamente prompt di sistema e token contestuali. Pertanto, questa strategia di cache è più adatta quando hai un contesto ampio su cui lavorare, ad esempio quando elabori file batch con istruzioni ed esempi completi.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>L'utilizzo della stessa cache di contesto e dello stesso prompt non garantisce risposte coerenti del modello perché le risposte degli LLMs non sono deterministiche. Una cache di contesto non memorizza alcun output.</p>
</div>
<p>Il context caching rimane un'area di ricerca attiva. Se vuoi evitare il vendor lock-in, ci sono già alcuni progressi in questo campo con strumenti open source come <a href="https://oreil.ly/PXm6B"><em>MemServe</em></a>, che implementa il context caching con un pool di memoria elastica<a data-startref="ix_ch10-asciidoc23" data-type="indexterm" id="id1090"/><a data-startref="ix_ch10-asciidoc22" data-type="indexterm" id="id1091"/><a data-startref="ix_ch10-asciidoc21" data-type="indexterm" id="id1092"/><a data-startref="ix_ch10-asciidoc20" data-type="indexterm" id="id1093"/> .<a data-startref="ix_ch10-asciidoc5" data-type="indexterm" id="id1094"/><a data-startref="ix_ch10-asciidoc4" data-type="indexterm" id="id1095"/><a data-startref="ix_ch10-asciidoc3" data-type="indexterm" id="id1096"/></p>
<p>Oltre alla cache, puoi anche esaminare le opzioni per ridurre le dimensioni del modello per accelerare i tempi di risposta utilizzando tecniche come la <em>quantizzazione del modello</em>.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Model Quantization" data-type="sect2"><div class="sect2" id="id154">
<h2>Quantizzazione del modello</h2>
<p><a data-primary="model quantization" data-type="indexterm" id="ix_ch10-asciidoc24"/><a data-primary="optimizing AI services" data-secondary="model quantization" data-type="indexterm" id="ix_ch10-asciidoc25"/>Se hai intenzione di utilizzare tu stesso modelli come gli LLMs, dovresti prendere in considerazione l'idea di <em>quantizzare</em> (cioè comprimere/ridurre) i tuoi modelli, se possibile. Spesso, i repository di modelli open source forniscono anche versioni quantizzate che puoi scaricare e utilizzare immediatamente senza dover affrontare il processo di quantizzazione.</p>
<p>La<em>quantizzazione del modello</em> è un processo di regolazione dei pesi e delle attivazioni del modello in cui i parametri del modello ad alta precisione vengono proiettati statisticamente in valori a bassa precisione attraverso un'operazione di regolazione fine che utilizza fattori di scala sulla distribuzione originale dei parametri. È quindi possibile eseguire tutte le operazioni di inferenza critiche con una precisione inferiore, dopodiché è possibile convertire gli output a una precisione superiore per mantenere la qualità e migliorare le prestazioni.</p>
<p>Riducendo la precisione si riducono anche i requisiti di memoria, riducendo teoricamente il consumo energetico e velocizzando operazioni come la moltiplicazione di matrici attraverso l'aritmetica intera. Questo permette anche di eseguire i modelli su dispositivi embedded, che possono supportare solo tipi di dati interi.</p>
<p><a data-type="xref" href="#quantization_process">La Figura 10-4</a> mostra il processo di quantizzazione completo.</p>
<figure><div class="figure" id="quantization_process">
<img alt="bgai 1004" src="assets/bgai_1004.png" width="699" height="613"/>
<h6><span class="label">Figura 10-4. </span>Processo di quantizzazione</h6>
</div></figure>
<p>È possibile risparmiare più di una manciata di gigabyte nel consumo di memoria della GPU, poiché i tipi di dati a bassa precisione come gli interi a 8 bit richiedono una quantità di RAM per parametro significativamente inferiore rispetto a un tipo di dati come i float a 32 bit.</p>
<section data-pdf-bookmark="Precision versus quality trade-off" data-type="sect3"><div class="sect3" id="id155">
<h3>Il compromesso tra precisione e qualità</h3>
<p><a data-primary="model quantization" data-secondary="precision versus quality trade-off" data-type="indexterm" id="ix_ch10-asciidoc26"/><a data-primary="optimizing AI services" data-secondary="model quantization" data-tertiary="precision versus quality trade-off" data-type="indexterm" id="ix_ch10-asciidoc27"/>La<a data-type="xref" href="#quantization">Figura 10-5</a> mette a confronto un modello non quantizzato e un modello quantizzato.</p>
<figure><div class="figure" id="quantization">
<img alt="bgai 1005" src="assets/bgai_1005.png" width="1419" height="701"/>
<h6><span class="label">Figura 10-5. </span>Quantizzazione</h6>
</div></figure>
<p>Poiché ogni parametro float a 32 bit ad alta precisione consuma 4 byte di memoria della GPU, un modello a 1B parametri richiederebbe 4 GB di memoria solo per l'inferenza. Se intendi riqualificare o perfezionare lo stesso modello, avrai bisogno di almeno 24 GB di VRAM della GPU. Questo perché ogni parametro richiederebbe anche la memorizzazione di informazioni come i gradienti, gli stati dell'ottimizzatore dell'addestramento, le attivazioni e lo spazio di memoria temporaneo, consumando complessivamente altri 24 byte.
Questo stima fino a 6 volte il fabbisogno di memoria rispetto al solo caricamento dei pesi del modello. Lo stesso modello 1B richiederebbe quindi una GPU da 24 GB, che le migliori e più costose schede grafiche consumer come la NVIDIA RTX 4090 potrebbero ancora faticare a soddisfare.</p>
<p>Invece di utilizzare il formato standard 32-float, puoi selezionare uno dei seguenti formati:</p>
<ul>
<li>
<p>La<em>virgola mobile a 16 bit (FP16) di</em> mezza l'utilizzo della memoria senza compromettere la qualità del modello.</p>
</li>
<li>
<p>Gli<em>interi a 8 bit (INT8)</em> offrono un enorme risparmio di memoria, ma con una significativa perdita di qualità.</p>
</li>
<li>
<p>Il<em>brain floating-point a 16 bit (BFLOAT16)</em> con un intervallo simile all'FP32 bilancia il compromesso tra memoria e qualità.</p>
</li>
<li>
<p>Il<em>numero intero a 4 bit (INT4)</em> offre un equilibrio tra l'efficienza della memoria e la precisione di calcolo, rendendolo adatto ai dispositivi a basso consumo.</p>
</li>
<li>
<p>L'<em>intero a 1 bit (INT1)</em> utilizza il tipo di dati a più bassa precisione con la massima riduzione delle dimensioni del modello. La ricerca per la creazione di <a href="https://oreil.ly/QH9nH">LLMs a 1 bit</a> di alta qualità è attualmente in corso.</p>
</li>
</ul>
<p>A titolo di confronto, la <a data-type="xref" href="#quantization_comparison">Tabella 10-2</a> mostra la riduzione delle dimensioni del modello quando si quantizzano i modelli della famiglia Llama.</p>
<table id="quantization_comparison">
<caption><span class="label">Tabella 10-2. </span>Impatto della quantizzazione sulle dimensioni dei modelli di Llama<sup><a data-type="noteref" href="ch10.html#id1097" id="id1097-marker" translate="no">a</a></sup></caption>
<thead>
<tr>
<th>Modello</th>
<th>Originale</th>
<th>FP16</th>
<th>8 Bit</th>
<th>6 Bit</th>
<th>4 Bit</th>
<th>2Bit</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Llama 2 70B</p></td>
<td><p>140 GB</p></td>
<td><p>128,5 GB</p></td>
<td><p>73,23 GB</p></td>
<td><p>52,70 GB</p></td>
<td><p>36,20 GB</p></td>
<td><p>28,59 GB</p></td>
</tr>
<tr>
<td><p>Llama 3 8B</p></td>
<td><p>16,07 GB</p></td>
<td><p>14,97 GB</p></td>
<td><p>7,96 GB</p></td>
<td><p>4,34 GB</p></td>
<td><p>4,34 GB</p></td>
<td><p>2,96 GB</p></td>
</tr>
</tbody>
<tbody><tr class="footnotes"><td colspan="7"><p data-type="footnote" id="id1097"><sup><a href="ch10.html#id1097-marker">a</a></sup> Fonti: <a href="https://oreil.ly/9iYtL">Llama.cpp repository GitHub</a> e la <a href="https://oreil.ly/BMDtR">scheda modello Hugging Face Llama 2 70B di Tom Jobbins</a></p></td></tr></tbody></table>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Oltre alla VRAM della GPU necessaria per montare il modello, avrai bisogno di altri 5-8 GB di VRAM della GPU per l'overhead durante il caricamento del modello.</p>
</div>
<p>Allo stato attuale della ricerca, mantenere l'accuratezza con i tipi di dati INT4 e INT1 solo interi è una sfida e il miglioramento delle prestazioni con INT32 o FP16 non è significativo. Pertanto, il tipo di dati a bassa precisione più diffuso è INT8 per l'inferenza.</p>
<p>Secondo la <a href="https://oreil.ly/C7Lz3">ricerca</a>, l'utilizzo dell'aritmetica dei soli numeri interi per l'inferenza sarà più efficiente rispetto ai numeri a virgola mobile. Tuttavia, quantificare i numeri a virgola mobile in numeri interi può essere complicato: ad esempio, solo 256 valori possono essere rappresentati in INT8, mentre float32 può rappresentare un'ampia gamma di valori.<a data-startref="ix_ch10-asciidoc27" data-type="indexterm" id="id1098"/><a data-startref="ix_ch10-asciidoc26" data-type="indexterm" id="id1099"/></p>
</div></section>
<section data-pdf-bookmark="Floating-point numbers" data-type="sect3"><div class="sect3" id="id156">
<h3>Numeri in virgola mobile</h3>
<p><a data-primary="model quantization" data-secondary="floating-point numbers" data-type="indexterm" id="ix_ch10-asciidoc28"/><a data-primary="optimizing AI services" data-secondary="model quantization" data-tertiary="floating-point numbers" data-type="indexterm" id="ix_ch10-asciidoc29"/>Per capire perché proiettare i float a 32 bit in altri formati farebbe risparmiare così tanto in termini di memoria della GPU, analizziamo come si articola il tutto.</p>
<p>Un numero in virgola mobile a 32 bit è composto dai seguenti tipi di bit:</p>
<ul>
<li>
<p>Bit di<em>segno</em> che descrive se un numero è positivo o negativo</p>
</li>
<li>
<p>Bit dell<em>'esponente</em> che controllano la scala del numero</p>
</li>
<li>
<p>Bit di<em>mantissa</em> che contengono le cifre effettive che determinano la sua precisione (noti anche come bit <em>di frazione</em> )</p>
</li>
</ul>
<p>Nella <a data-type="xref" href="#quantization_bits">Figura 10-6</a> puoi vedere una visualizzazione dei bit nei numeri in virgola mobile sopra citati.</p>
<figure><div class="figure" id="quantization_bits">
<img alt="bgai 1006" src="assets/bgai_1006.png" width="1447" height="1007"/>
<h6><span class="label">Figura 10-6. </span>Bit dei numeri float a 32 bit, float a 16 bit e bfloat16</h6>
</div></figure>
<p>Quando proietti un numero FP32 in altri formati, in effetti, lo schiacci in intervalli più piccoli, perdendo la maggior parte dei bit della mantissa e adattando i bit dell'esponente, ma senza perdere gran parte della precisione. Puoi vedere questo fenomeno in azione facendo riferimento alla <a data-type="xref" href="#quantization_floating_numbers">Figura 10-7</a>.</p>
<figure><div class="figure" id="quantization_floating_numbers">
<img alt="bgai 1007" src="assets/bgai_1007.png" width="1170" height="853"/>
<h6><span class="label">Figura 10-7. </span>Quantizzazione di numeri in virgola mobile in numeri interi</h6>
</div></figure>
<p>In effetti, la <a href="https://oreil.ly/Swfz7">ricerca sulle strategie di quantizzazione per i modelli LLM pre-addestrati</a> ha dimostrato che gli LLM con quantizzazione a 4 bit possono mantenere prestazioni simili alle loro controparti non quantizzate. Tuttavia, se da un lato la quantizzazione consente di risparmiare memoria, dall'altro può ridurre la velocità di inferenza degli LLM.<a data-startref="ix_ch10-asciidoc29" data-type="indexterm" id="id1100"/><a data-startref="ix_ch10-asciidoc28" data-type="indexterm" id="id1101"/></p>
</div></section>
<section data-pdf-bookmark="How to quantize pretrained LLMs" data-type="sect3"><div class="sect3" id="id157">
<h3>Come quantizzare le LLMs preaddestrate</h3>
<p><a data-primary="GPTQ technique" data-type="indexterm" id="ix_ch10-asciidoc30"/><a data-primary="large language models (LLMs)" data-secondary="quantizing pretrained LLMs" data-type="indexterm" id="ix_ch10-asciidoc31"/><a data-primary="model quantization" data-secondary="quantizing pretrained LLMs" data-type="indexterm" id="ix_ch10-asciidoc32"/><a data-primary="optimizing AI services" data-secondary="model quantization" data-tertiary="quantizing pretrained LLMs" data-type="indexterm" id="ix_ch10-asciidoc33"/>Una di queste tecniche, chiamata <a href="https://oreil.ly/rHYKZ"><em>GPTQ</em></a>, è in grado di quantizzare LLMs con 175 miliardi di parametri in circa 4 ore di GPU, riducendo la larghezza dei bit a 3 o 4 bit per peso, con un calo di precisione trascurabile rispetto al modello non compresso.</p>
<p>Gli autori delle librerie Hugging Face <code translate="no">transformers</code> e <code translate="no">optimum</code> hanno collaborato strettamente con gli sviluppatori della libreria <code translate="no">auto-gptq</code> per fornire una semplice API per applicare la quantizzazione GPTQ su LLMs open source. Optimum è una libreria che fornisce API per eseguire la quantizzazione utilizzando diversi strumenti.</p>
<p>Con la quantizzazione GPTQ, puoi quantizzare il tuo modello linguistico preferito a 8, 4, 3 o addirittura 2 bit senza un grosso calo di prestazioni, mantenendo una velocità di inferenza superiore a quella supportata dalla maggior parte dell'hardware GPT. Puoi seguire l'<a data-type="xref" href="#gptq_quantization">Esempio 10-14</a> per quantizzare un modello pre-addestrato sulla tua GPU.</p>
<p>Le dipendenze che devi installare per eseguire l'<a data-type="xref" href="#gptq_quantization">Esempio 10-14</a> includono<span class="keep-together">le seguenti:</span></p>
<pre data-type="programlisting" translate="no">$ pip install auto-gptq optimum transformers accelerate</pre>
<div data-type="example" id="gptq_quantization">
<h5><span class="label">Esempio 10-14. </span>Quantizzazione del modello GPTQ con le librerie Hugging Face e AutoGPTQ</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code> <code class="nn" translate="no">torch</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">optimum</code><code class="nn" translate="no">.</code><code class="nn" translate="no">gptq</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">GPTQQuantizer</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">transformers</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">AutoModelForCausalLM</code><code class="p" translate="no">,</code> <code class="n" translate="no">AutoTokenizer</code>

<code class="n" translate="no">model_name</code> <code class="o" translate="no">=</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">facebook/opt-125m</code><code class="s2" translate="no">"</code> <a class="co" href="#callout_optimizing_ai_services_CO10-1" id="co_optimizing_ai_services_CO10-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
<code class="n" translate="no">tokenizer</code> <code class="o" translate="no">=</code> <code class="n" translate="no">AutoTokenizer</code><code class="o" translate="no">.</code><code class="n" translate="no">from_pretrained</code><code class="p" translate="no">(</code><code class="n" translate="no">model_name</code><code class="p" translate="no">)</code>
<code class="n" translate="no">model</code> <code class="o" translate="no">=</code> <code class="n" translate="no">AutoModelForCausalLM</code><code class="o" translate="no">.</code><code class="n" translate="no">from_pretrained</code><code class="p" translate="no">(</code>
    <code class="n" translate="no">model_name</code><code class="p" translate="no">,</code> <code class="n" translate="no">torch_dtype</code><code class="o" translate="no">=</code><code class="n" translate="no">torch</code><code class="o" translate="no">.</code><code class="n" translate="no">float16</code>
<code class="p" translate="no">)</code>

<code class="n" translate="no">quantizer</code> <code class="o" translate="no">=</code> <code class="n" translate="no">GPTQQuantizer</code><code class="p" translate="no">(</code>
    <code class="n" translate="no">bits</code><code class="o" translate="no">=</code><code class="mi" translate="no">4</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">dataset</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">c4</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO10-2" id="co_optimizing_ai_services_CO10-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a>
    <code class="n" translate="no">block_name_to_quantize</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">model.decoder.layers</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO10-3" id="co_optimizing_ai_services_CO10-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a>
    <code class="n" translate="no">model_seqlen</code><code class="o" translate="no">=</code><code class="mi" translate="no">2048</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO10-4" id="co_optimizing_ai_services_CO10-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a>
<code class="p" translate="no">)</code>
<code class="n" translate="no">quantized_model</code> <code class="o" translate="no">=</code> <code class="n" translate="no">quantizer</code><code class="o" translate="no">.</code><code class="n" translate="no">quantize_model</code><code class="p" translate="no">(</code><code class="n" translate="no">model</code><code class="p" translate="no">,</code> <code class="n" translate="no">tokenizer</code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO10-1" id="callout_optimizing_ai_services_CO10-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Carica la versione <code translate="no">float16</code> del modello preaddestrato <code translate="no">facebook/opt-125m</code> prima della quantizzazione.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO10-2" id="callout_optimizing_ai_services_CO10-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Usa il dataset <code translate="no">c4</code> per calibrare la quantizzazione.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO10-3" id="callout_optimizing_ai_services_CO10-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Quantizza solo i blocchi del livello di decodifica del modello.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO10-4" id="callout_optimizing_ai_services_CO10-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Usa la lunghezza della sequenza modello di <code translate="no">2048</code> per elaborare il set di dati.</p></dd>
</dl></div>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Come riferimento, un modello 175B richiederà 4 ore di GPU su NVIDIA A100 per essere quantizzato. Tuttavia, vale la pena cercare nel repository dei modelli di Hugging Face i modelli prequantizzati, perché potresti scoprire che qualcuno ha già fatto il lavoro<a data-startref="ix_ch10-asciidoc33" data-type="indexterm" id="id1102"/><a data-startref="ix_ch10-asciidoc32" data-type="indexterm" id="id1103"/><a data-startref="ix_ch10-asciidoc31" data-type="indexterm" id="id1104"/><a data-startref="ix_ch10-asciidoc30" data-type="indexterm" id="id1105"/> .<a data-startref="ix_ch10-asciidoc25" data-type="indexterm" id="id1106"/><a data-startref="ix_ch10-asciidoc24" data-type="indexterm" id="id1107"/></p>
</div>
<p>Ora che hai compreso le tecniche di ottimizzazione delle prestazioni, vediamo come migliorare la qualità dei tuoi servizi GenAI utilizzando metodi come gli output strutturati.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Structured Outputs" data-type="sect2"><div class="sect2" id="id158">
<h2>Uscite strutturate</h2>
<p><a data-primary="optimizing AI services" data-secondary="structured outputs" data-type="indexterm" id="ix_ch10-asciidoc34"/><a data-primary="structured outputs" data-type="indexterm" id="ix_ch10-asciidoc35"/>I modelli fondamentali come gli LLMs possono essere utilizzati come componenti di una pipeline di dati o collegati ad applicazioni a valle. Ad esempio, puoi usare questi modelli per estrarre e analizzare informazioni da documenti o per generare codice che può essere eseguito su altri sistemi.</p>
<p>Puoi chiedere a LLM di fornire una risposta testuale contenente informazioni JSON. Dovrai quindi estrarre e analizzare questa stringa JSON utilizzando strumenti come regex e Pydantic. Tuttavia, non c'è alcuna garanzia che il modello si attenga sempre alle tue istruzioni. Poiché i tuoi sistemi a valle potrebbero basarsi su output JSON, potrebbero lanciare eccezioni e gestire in modo errato input non validi.</p>
<p>Sono stati rilasciati diversi pacchetti di utilità come Instructor per migliorare la robustezza delle risposte LLM, prendendo uno schema ed effettuando diverse chiamate API con vari modelli di prompt per raggiungere l'output desiderato. Se da un lato queste soluzioni migliorano la robustezza, dall'altro aggiungono costi significativi alla tua soluzione a causa delle successive chiamate API ai fornitori di modelli.</p>
<p>Recentemente, i fornitori di modelli hanno aggiunto una funzione per richiedere output strutturati fornendo schemi quando si effettuano chiamate API al modello, come puoi vedere nell'<a data-type="xref" href="#structured_outputs">Esempio 10-15</a>. Questo aiuta a ridurre il lavoro di template del prompt che devi fare tu stesso e mira a migliorare l'<em>allineamento</em> del modello alle tue intenzioni quando restituisce una risposta.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Al momento in cui scriviamo, solo il più recente OpenAI SDK supporta i modelli Pydantic per abilitare gli output strutturati.</p>
</div>
<div data-type="example" id="structured_outputs">
<h5><span class="label">Esempio 10-15. </span>Uscite strutturate</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">openai</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">AsyncOpenAI</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">pydantic</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">BaseModel</code><code class="p" translate="no">,</code> <code class="n" translate="no">Field</code>

<code class="n" translate="no">client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">AsyncOpenAI</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code>

<code class="k" translate="no">class</code> <code class="nc" translate="no">DocumentClassification</code><code class="p" translate="no">(</code><code class="n" translate="no">BaseModel</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code> <a class="co" href="#callout_optimizing_ai_services_CO11-1" id="co_optimizing_ai_services_CO11-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
    <code class="n" translate="no">category</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code> <code class="o" translate="no">=</code> <code class="n" translate="no">Field</code><code class="p" translate="no">(</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="p" translate="no">,</code> <code class="n" translate="no">description</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">The category of the classification</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>

<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">get_document_classification</code><code class="p" translate="no">(</code>
    <code class="n" translate="no">title</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code><code class="p" translate="no">,</code>
<code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="n" translate="no">DocumentClassification</code> <code class="o" translate="no">|</code> <code class="nb" translate="no">str</code> <code class="o" translate="no">|</code> <code class="kc" translate="no">None</code><code class="p" translate="no">:</code>
    <code class="n" translate="no">response</code> <code class="o" translate="no">=</code> <code class="k" translate="no">await</code> <code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">beta</code><code class="o" translate="no">.</code><code class="n" translate="no">chat</code><code class="o" translate="no">.</code><code class="n" translate="no">completions</code><code class="o" translate="no">.</code><code class="n" translate="no">parse</code><code class="p" translate="no">(</code>
        <code class="n" translate="no">model</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">gpt-4o</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code>
        <code class="n" translate="no">messages</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code>
            <code class="p" translate="no">{</code>
                <code class="s2" translate="no">"</code><code class="s2" translate="no">role</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">system</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code>
                <code class="s2" translate="no">"</code><code class="s2" translate="no">content</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">classify the provided document into the following: ...</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code>
            <code class="p" translate="no">}</code><code class="p" translate="no">,</code>
            <code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">role</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">user</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">content</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="n" translate="no">title</code><code class="p" translate="no">}</code><code class="p" translate="no">,</code>
        <code class="p" translate="no">]</code><code class="p" translate="no">,</code>
        <code class="n" translate="no">response_format</code><code class="o" translate="no">=</code><code class="n" translate="no">DocumentClassification</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO11-2" id="co_optimizing_ai_services_CO11-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a>
    <code class="p" translate="no">)</code>

    <code class="n" translate="no">message</code> <code class="o" translate="no">=</code> <code class="n" translate="no">response</code><code class="o" translate="no">.</code><code class="n" translate="no">choices</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">]</code><code class="o" translate="no">.</code><code class="n" translate="no">message</code>
    <code class="k" translate="no">return</code> <code class="n" translate="no">message</code><code class="o" translate="no">.</code><code class="n" translate="no">parsed</code> <code class="k" translate="no">if</code> <code class="n" translate="no">message</code><code class="o" translate="no">.</code><code class="n" translate="no">parsed</code> <code class="ow" translate="no">is</code> <code class="ow" translate="no">not</code> <code class="kc" translate="no">None</code> <code class="k" translate="no">else</code> <code class="n" translate="no">message</code><code class="o" translate="no">.</code><code class="n" translate="no">refusal</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO11-1" id="callout_optimizing_ai_services_CO11-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Specifica un modello Pydantic per gli output strutturati.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO11-2" id="callout_optimizing_ai_services_CO11-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Fornisce lo schema definito al client del modello quando effettua la chiamata API.</p></dd>
</dl></div>
<p>Se il tuo fornitore di modelli non supporta in modo nativo gli output strutturati, puoi comunque sfruttare le funzionalità di completamento delle chat del modello per aumentare la robustezza degli output strutturati, come mostrato nell'<a data-type="xref" href="#structured_outputs_completions">Esempio 10-16</a>.</p>
<div data-type="example" id="structured_outputs_completions">
<h5><span class="label">Esempio 10-16. </span>Uscite strutturate basate sul prefill delle chat</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code> <code class="nn" translate="no">json</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">loguru</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">logger</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">openai</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">AsyncOpenAI</code>
<code class="n" translate="no">client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">AsyncOpenAI</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code>

<code class="n" translate="no">system_template</code> <code class="o" translate="no">=</code> <code class="s2" translate="no">"""</code>
<code class="s2" translate="no">Classify the provided document into the following: ...</code>

<code class="s2" translate="no">Provide responses in the following manner json: </code><code class="s2" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">category</code><code class="s2" translate="no">"</code><code class="s2" translate="no">: </code><code class="s2" translate="no">"</code><code class="s2" translate="no">string</code><code class="s2" translate="no">"</code><code class="s2" translate="no">}</code>
<code class="s2" translate="no">"""</code>

<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">get_document_classification</code><code class="p" translate="no">(</code><code class="n" translate="no">title</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code><code class="p" translate="no">)</code> <code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code> <code class="nb" translate="no">dict</code><code class="p" translate="no">:</code>
    <code class="n" translate="no">response</code> <code class="o" translate="no">=</code> <code class="k" translate="no">await</code> <code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">chat</code><code class="o" translate="no">.</code><code class="n" translate="no">completions</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code>
        <code class="n" translate="no">model</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">gpt-4o</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code>
        <code class="n" translate="no">max_tokens</code><code class="o" translate="no">=</code><code class="mi" translate="no">1024</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO12-1" id="co_optimizing_ai_services_CO12-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
        <code class="n" translate="no">messages</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code>
            <code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">role</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">system</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">content</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="n" translate="no">system_template</code><code class="p" translate="no">}</code><code class="p" translate="no">,</code>
            <code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">role</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">user</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">content</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="n" translate="no">title</code><code class="p" translate="no">}</code><code class="p" translate="no">,</code>
            <code class="p" translate="no">{</code>
                <code class="s2" translate="no">"</code><code class="s2" translate="no">role</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">assistant</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code>
                <code class="s2" translate="no">"</code><code class="s2" translate="no">content</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">The document classification JSON is </code><code class="s2" translate="no">{</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <a class="co" href="#callout_optimizing_ai_services_CO12-2" id="co_optimizing_ai_services_CO12-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a>
            <code class="p" translate="no">}</code><code class="p" translate="no">,</code>
        <code class="p" translate="no">]</code><code class="p" translate="no">,</code>
    <code class="p" translate="no">)</code>
    <code class="n" translate="no">message</code> <code class="o" translate="no">=</code> <code class="n" translate="no">response</code><code class="o" translate="no">.</code><code class="n" translate="no">choices</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">]</code><code class="o" translate="no">.</code><code class="n" translate="no">message</code><code class="o" translate="no">.</code><code class="n" translate="no">content</code> <code class="ow" translate="no">or</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">"</code>
    <code class="k" translate="no">try</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">return</code> <code class="n" translate="no">json</code><code class="o" translate="no">.</code><code class="n" translate="no">loads</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">{</code><code class="s2" translate="no">"</code> <code class="o" translate="no">+</code> <code class="n" translate="no">message</code><code class="p" translate="no">[</code><code class="p" translate="no">:</code> <code class="n" translate="no">message</code><code class="o" translate="no">.</code><code class="n" translate="no">rfind</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code> <code class="o" translate="no">+</code> <code class="mi" translate="no">1</code><code class="p" translate="no">]</code><code class="p" translate="no">)</code> <a class="co" href="#callout_optimizing_ai_services_CO12-3" id="co_optimizing_ai_services_CO12-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a>
    <code class="k" translate="no">except</code> <code class="n" translate="no">json</code><code class="o" translate="no">.</code><code class="n" translate="no">JSONDecodeError</code><code class="p" translate="no">:</code> <a class="co" href="#callout_optimizing_ai_services_CO12-4" id="co_optimizing_ai_services_CO12-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a>
        <code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">warning</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Failed to parse the response: </code><code class="si" translate="no">{</code><code class="n" translate="no">message</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
    <code class="k" translate="no">return</code> <code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">error</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"</code><code class="s2" translate="no">Refusal response</code><code class="s2" translate="no">"</code><code class="p" translate="no">}</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO12-1" id="callout_optimizing_ai_services_CO12-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Limitare i token di output per migliorare la robustezza e la velocità delle risposte strutturate e per ridurre i costi.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO12-2" id="callout_optimizing_ai_services_CO12-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Salta il preambolo e restituisce direttamente un JSON precompilando la risposta dell'assistente e includendo il carattere <code translate="no">{</code>.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO12-3" id="callout_optimizing_ai_services_CO12-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Aggiungiamo di nuovo il prefiltrato <code translate="no">{</code> e poi troviamo la chiusura <code translate="no">}</code> ed estraiamo la<span class="keep-together">sottostringa</span> JSON.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO12-4" id="callout_optimizing_ai_services_CO12-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Gestisce i casi in cui non c'è JSON nella risposta, ad esempio se c'è un rifiuto.</p></dd>
</dl></div>
<p>Seguire le tecniche sopra descritte dovrebbe aiutarti a migliorare la robustezza delle tue pipeline di dati se queste utilizzano gli LLMs come componenti.<a data-startref="ix_ch10-asciidoc35" data-type="indexterm" id="id1108"/><a data-startref="ix_ch10-asciidoc34" data-type="indexterm" id="id1109"/></p>
</div></section>
<section data-pdf-bookmark="Prompt Engineering" data-type="sect2"><div class="sect2" id="id159">
<h2>Ingegneria prompt</h2>
<p><a data-primary="optimizing AI services" data-secondary="prompt engineering" data-type="indexterm" id="ix_ch10-asciidoc36"/><a data-primary="prompt engineering" data-type="indexterm" id="ix_ch10-asciidoc37"/>L'ingegneria dei prompt è la pratica di creare e perfezionare le query per i modelli generativi in modo da produrre i risultati più utili e ottimizzati. Senza perfezionare i prompt, dovresti perfezionare i modelli o addestrare un modello da zero per ottimizzare la qualità dei risultati.</p>
<p>Molti sostengono che questo campo non abbia il rigore scientifico necessario per essere considerato una disciplina ingegneristica. Tuttavia, è possibile affrontare il problema da una prospettiva ingegneristica quando si affinano i prompt per ottenere la migliore qualità di output dai modelli.</p>
<p>In modo simile a come comunichi con gli altri per ottenere le cose, con i prompt ottimizzati puoi comunicare in modo più efficace le tue intenzioni al modello per aumentare le possibilità di ottenere le risposte che desideri. Pertanto, il prompt non diventa solo un problema di ingegneria ma anche di comunicazione. Un modello può essere paragonato a un collega esperto con molta esperienza ma con una conoscenza limitata del dominio, pronto ad aiutarti ma che ha bisogno che tu fornisca istruzioni ben documentate, possibilmente con alcuni esempi da seguire e da abbinare.</p>
<p>Se i tuoi prompt sono vaghi e generici, otterrai anche una risposta media.</p>
<p>Un altro modo di pensare a questo problema di ottimizzazione è quello di paragonare l'attività di prompt del modello alla programmazione. Invece di scrivere il codice da solo, stai effettivamente "codificando" un modello in modo che sia un componente ben integrato di un'applicazione più grande o di una pipeline di dati. Puoi adottare approcci di sviluppo test-driven (TDD) e perfezionare i prompt fino a quando i test non passano. Oppure, sperimentare diversi modelli per vedere quale <em>allinea</em> meglio i suoi output alle tue intenzioni.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>La massimizzazione dell'<em>allineamento</em> dei modelli rimane un obiettivo prioritario per molti fornitori di modelli, in modo che i loro risultati soddisfino al meglio l'intento dell'utente.</p>
</div>
<section data-pdf-bookmark="Prompt templates" data-type="sect3"><div class="sect3" id="id160">
<h3>Modelli di prompt</h3>
<p><a data-primary="prompt templates" data-type="indexterm" id="id1110"/><a data-primary="optimizing AI services" data-secondary="prompt engineering" data-tertiary="prompt templates" data-type="indexterm" id="id1111"/><a data-primary="prompt engineering" data-secondary="prompt templates" data-type="indexterm" id="id1112"/>Se le istruzioni del sistema non sono metodiche, chiare e non seguono le migliori pratiche di prompt, potresti lasciare sul tavolo potenziali ottimizzazioni della qualità e delle prestazioni.</p>
<p>Come minimo, dovresti avere dei prompt chiari che forniscano compiti specifici al modello. La pratica migliore è quella di seguire un modello sistematico.<a data-primary="RCT (role, context, and task) template" data-type="indexterm" id="id1113"/>Per esempio, redigi le istruzioni del modello seguendo il modello di <em>ruolo, contesto e compito</em> (RCT):</p>
<dl>
<dt>Ruolo</dt>
<dd>
<p>La ricerca ha dimostrato che specificare i ruoli per i LLMs tende a influenzare in modo significativo i loro risultati. Ad esempio, un modello potrebbe essere più indulgente nella valutazione di un saggio se gli dai il ruolo di un insegnante di scuola elementare. Senza un ruolo specifico, il modello potrebbe presumere che tu voglia che la valutazione segua gli standard accademici di livello universitario.</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>Puoi ampliare ulteriormente il ruolo del modello e descrivere in modo dettagliato una <em>persona</em> che il modello dovrà adottare. Utilizzando una persona, il modello saprà esattamente come comportarsi e fare previsioni perché avrà un contesto più ampio su ciò che il ruolo dovrebbe comportare.</p>
</div>
<dl>
<dt>Contesto</dt>
<dd>
<p>Definisce lo scenario, dipinge il quadro e fornisce tutte le informazioni rilevanti e utili che il modello può utilizzare come riferimento per fare previsioni. Senza un contesto esplicito, il modello può utilizzare solo un contesto implicito che conterrà informazioni medie dei suoi dati di addestramento. In un'applicazione RAG, il contesto potrebbe essere la concatenazione del prompt del sistema con i pezzi di documenti recuperati da un archivio di conoscenza.</p>
</dd>
<dt>Compito</dt>
<dd>
<p>Quando descrivi il compito, assicurati di pensare al modello come a un apprendista brillante e preparato, pronto a entrare in azione ma che ha bisogno di istruzioni molto chiare e non ambigue da seguire, potenzialmente con una manciata di esempi.</p>
</dd>
</dl>
<p>Seguendo il modello di sistema sopra descritto, dovresti migliorare la qualità dei risultati del tuo modello con il minimo sforzo.</p>
</div></section>
<section data-pdf-bookmark="Advanced prompting techniques" data-type="sect3"><div class="sect3" id="id257">
<h3>Tecniche di prompt avanzate</h3>
<p><a data-primary="optimizing AI services" data-secondary="prompt engineering" data-tertiary="advanced prompting techniques" data-type="indexterm" id="ix_ch10-asciidoc38"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-type="indexterm" id="ix_ch10-asciidoc39"/>Oltre ai principi fondamentali del prompt, puoi utilizzare tecniche più avanzate che potrebbero adattarsi meglio al tuo caso d'uso. In base a una <a href="https://oreil.ly/xynPC">recente indagine sistematica sulle tecniche di prompt</a>, puoi raggruppare i prompt di LLM nei seguenti tipi:</p>
<ul>
<li>
<p>Apprendimento in contesto</p>
</li>
<li>
<p>Generazione di pensieri</p>
</li>
<li>
<p>Decomposizione</p>
</li>
<li>
<p>Assemblaggio</p>
</li>
<li>
<p>Autocritica</p>
</li>
<li>
<p>Agente</p>
</li>
</ul>
<p>Esaminiamo ciascuna di esse in modo più dettagliato.</p>
<section data-pdf-bookmark="In-context learning" data-type="sect4"><div class="sect4" id="id161">
<h4>Apprendimento in contesto</h4>
<p><a data-primary="in-context learning" data-type="indexterm" id="id1114"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="in-context learning" data-type="indexterm" id="id1115"/>Ciò che distingue i modelli fondamentali come gli LLMs dai tradizionali modelli di apprendimento automatico è la loro capacità di rispondere a input dinamici senza la necessità costante di una messa a punto o di una riqualificazione.</p>
<p>Quando fornisci delle istruzioni di sistema a un LLM, puoi fornire anche diversi esempi (ad esempio, delle inquadrature) per guidare la generazione dell'output.</p>
<p><a data-primary="zero-shot prompting" data-type="indexterm" id="id1116"/>Il<a href="https://oreil.ly/3F4wb"><em>prompt a zero colpi</em></a> si riferisce a un approccio di prompt che non specifica esempi di riferimento, ma il modello può comunque completare con successo il compito dato.<a data-primary="few-shot prompting" data-type="indexterm" id="id1117"/>Se il modello ha difficoltà senza esempi di riferimento, potresti dover usare il <a href="https://oreil.ly/pOSj8"><em>prompt a pochi colpi</em></a>, in cui fornisci una manciata di esempi.<a data-primary="dynamic few-shot prompting" data-type="indexterm" id="id1118"/>Ci sono anche casi d'uso in cui vuoi usare il prompt <em>dinamico a pochi colpi</em>, in cui inserisci dinamicamente gli esempi dai dati recuperati da un database o da un archivio vettoriale.</p>
<p>Gli approcci basati sul prompt, in cui si specificano gli esempi, sono definiti anche <em>apprendimento in contesto</em>. In effetti, stai mettendo a punto i risultati del modello in base ai tuoi esempi e al compito dato senza modificare effettivamente i pesi/parametri del modello, mentre altri modelli di ML richiederebbero la modifica dei loro pesi.</p>
<p>Questo è ciò che rende gli LLMs e i modelli fondazionali così potenti, in quanto non richiedono sempre una regolazione del peso per adattarsi ai dati e ai compiti che gli vengono assegnati. Puoi conoscere diverse tecniche di apprendimento in contesto facendo riferimento alla <a data-type="xref" href="#prompting_techniques_incontext_learning">Tabella 10-3</a>.</p>
<table class="striped" id="prompting_techniques_incontext_learning">
<caption><span class="label">Tabella 10-3. </span>Tecniche di prompt per l'apprendimento contestuale</caption>
<thead>
<tr>
<th>Tecnica del prompt</th>
<th>Esempi</th>
<th>Casi d'uso</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Colpo zero</p></td>
<td><p>Riassumi i seguenti punti...</p></td>
<td><p>Riassunto, domande e risposte senza esempi di formazione specifici</p></td>
</tr>
<tr>
<td><p>Pochi colpi</p></td>
<td><p>Classifica i documenti in base agli esempi riportati di seguito:</p><p>[Esempi]</p></td>
<td><p>Classificazione del testo, analisi del sentiment, estrazione dei dati con alcuni esempi</p></td>
</tr>
<tr>
<td><p>Scatti dinamici di pochi secondi</p></td>
<td><p>Classifica i seguenti documenti in base agli esempi riportati di seguito:</p><p>&lt;Inietta esempi da un archivio di vettori basato su una query&gt;.</p></td>
<td><p>Risposte personalizzate, risoluzione di problemi complessi</p></td>
</tr>
</tbody>
</table>
<p>I prompt di apprendimento contestuali sono semplici, efficaci e rappresentano un ottimo punto di partenza per completare una serie di compiti. Per i compiti più complessi, puoi utilizzare approcci di prompt più avanzati come la generazione del pensiero, la scomposizione, l'assemblaggio, l'autocritica o gli approcci agici.</p>
</div></section>
<section data-pdf-bookmark="Thought generation" data-type="sect4"><div class="sect4" id="id162">
<h4>Generazione di pensieri</h4>
<p><a data-primary="CoT (chain of thought) prompting" data-type="indexterm" id="id1119"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="thought generation" data-type="indexterm" id="id1120"/><a data-primary="thought generation techniques" data-type="indexterm" id="id1121"/>Le tecniche di generazione del pensiero, come la <a href="https://oreil.ly/BWUYQ">catena del pensiero (CoT),</a> hanno dimostrato di migliorare significativamente la capacità delle LLMs di eseguire ragionamenti complessi.</p>
<p>Nel prompt COT si chiede al modello di spiegare il suo processo di pensiero e il suo ragionamento mentre fornisce una risposta. Le varianti della CoT includono la <a href="https://oreil.ly/1gjSH">CoT</a> a zero o a <a href="https://oreil.ly/1gjSH">pochi colpi</a>, a seconda che si forniscano o meno degli esempi. Una tecnica di generazione del pensiero più avanzata è il <a href="https://oreil.ly/1KyO4">thread of thought (ThoT)</a> che segmenta e analizza sistematicamente informazioni o compiti caotici e molto complessi.</p>
<p>La<a data-type="xref" href="#prompting_techniques_thought_generation">Tabella 10-4</a> elenca le tecniche di generazione del pensiero.</p>
<table class="striped" id="prompting_techniques_thought_generation">
<caption><span class="label">Tabella 10-4. </span>Tecniche di prompt per la generazione del pensiero</caption>
<thead>
<tr>
<th>Tecnica del prompt</th>
<th>Esempi</th>
<th>Casi d'uso</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Catena di pensiero a colpo zero (CoT)</p></td>
<td><p>Pensiamo passo dopo passo...</p></td>
<td><p>Risoluzione di problemi matematici, ragionamento logico e processo decisionale in più fasi.</p></td>
</tr>
<tr>
<td><p>CoT a pochi colpi</p></td>
<td><p>Pensiamo passo dopo passo... Ecco alcuni esempi:</p><p>[ESEMPI]</p></td>
<td><p>Scenari in cui alcuni esempi possono guidare il modello verso prestazioni migliori, come la classificazione di testi sfumati, la risposta a domande complesse e i prompt di scrittura creativa.</p></td>
</tr>
<tr>
<td><p>Filo del pensiero (ThoT)</p></td>
<td><p>Esamina il problema in parti gestibili, passo dopo passo, riassumendo e analizzando man mano...</p></td>
<td><p>Mantenere il contesto nel corso di interazioni multiple, come i sistemi di dialogo, la narrazione interattiva e la generazione di contenuti di lunga durata.</p></td>
</tr>
</tbody>
</table>
</div></section>
<section data-pdf-bookmark="Decomposition" data-type="sect4"><div class="sect4" id="id163">
<h4>Decomposizione</h4>
<p><a data-primary="decomposition prompting techniques" data-type="indexterm" id="id1122"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="decomposition" data-type="indexterm" id="id1123"/>Le tecniche di prompt a scomposizione si concentrano sulla suddivisione di compiti complessi in sottocompiti più piccoli, in modo che il modello possa affrontarli passo dopo passo e in modo logico. Puoi sperimentare questi approcci insieme alla generazione del pensiero per identificare quelli che producono i risultati migliori per il tuo caso d'uso.</p>
<p>Queste sono le tecniche di prompt di scomposizione più comuni:</p>
<dl>
<dt><a href="https://oreil.ly/HmsSN">Da meno a più</a></dt>
<dd>
<p>Chiedi al modello di suddividere un problema complesso in problemi più piccoli tramite una riduzione logica, senza risolverli. Puoi poi chiedere al modello di risolvere ogni compito uno per uno.</p>
</dd>
<dt><a href="https://oreil.ly/aWTzf">Pianifica e risolvi</a></dt>
<dd>
<p>Dato un compito, chiedi che venga elaborato un piano e poi chiedi al modello di risolverlo.</p>
</dd>
<dt><a href="https://oreil.ly/IZdj1">L'albero dei pensieri (ToT)</a></dt>
<dd>
<p>Crea un problema di ricerca ad albero in cui un compito è suddiviso in più rami di passi come un albero. Poi, richiama il modello per valutare e risolvere ogni ramo di passi.</p>
</dd>
</dl>
<p><a data-type="xref" href="#prompting_techniques_decomposition">La Tabella 10-5</a> mostra queste tecniche di scomposizione.</p>
<table class="striped" id="prompting_techniques_decomposition">
<caption><span class="label">Tabella 10-5. </span>Tecniche di prompt della decomposizione</caption>
<thead>
<tr>
<th>Tecnica del prompt</th>
<th>Esempi</th>
<th>Casi d'uso</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Da meno a più</p></td>
<td><p>Suddividi il compito di... in compiti più piccoli.</p></td>
<td><p>Risoluzione di problemi complessi, gestione di progetti, decomposizione dei compiti</p></td>
</tr>
<tr>
<td><p>Pianifica e risolvi</p></td>
<td><p>Ideare un piano per...</p></td>
<td><p>Sviluppo di algoritmi, progettazione di software, pianificazione strategica</p></td>
</tr>
<tr>
<td><p>L'albero dei pensieri (ToT)</p></td>
<td><p>Crea un albero decisionale per la scelta di un...</p></td>
<td><p>Processo decisionale, risoluzione di problemi con soluzioni multiple, pianificazione strategica con alternative.</p></td>
</tr>
</tbody>
</table>
</div></section>
<section data-pdf-bookmark="Ensembling" data-type="sect4"><div class="sect4" id="id164">
<h4>Assemblaggio</h4>
<p><a data-primary="ensembling, prompting" data-type="indexterm" id="id1124"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="ensembling" data-type="indexterm" id="id1125"/>L<em>'assemblaggio</em> è il processo che prevede l'utilizzo di più prompt per risolvere lo stesso problema e l'aggregazione delle risposte in un output finale. Puoi generare queste risposte utilizzando lo stesso modello o modelli diversi.</p>
<p>L'idea principale alla base dell'ensembling è quella di ridurre la varianza dei risultati di LLM migliorando l'accuratezza in cambio di costi di utilizzo più elevati.</p>
<p>Le tecniche di prompt dell'ensemble più conosciute sono le seguenti:</p>
<dl>
<dt><a href="https://oreil.ly/_85WS">Autoconsistenza</a></dt>
<dd>
<p>Genera più percorsi di ragionamento e seleziona l'output più coerente come risultato finale utilizzando una votazione a maggioranza.</p>
</dd>
<dt><a href="https://oreil.ly/xllKs">Miscela di esperti di ragionamento (MoRE)</a></dt>
<dd>
<p>Combina i risultati di più LLMs con prompt specializzati per migliorare la qualità delle risposte. Ogni LLMs agisce come un esperto di un'area focalizzata su diversi compiti di ragionamento come ragionamento fattuale, riduzione logica, controlli di buon senso, ecc.</p>
</dd>
<dt><a href="https://oreil.ly/lPEPz">Dimostrazione di assemblaggio (DENSE)</a></dt>
<dd>
<p>Crea prompt multipli di pochi secondi a partire dai dati, quindi genera un output finale aggregando le risposte.</p>
</dd>
<dt><a href="https://oreil.ly/yP_ka">Parafrasi prompt</a></dt>
<dd>
<p>Formula il prompt originale in più varianti attraverso la formulazione.</p>
</dd>
</dl>
<p><a data-type="xref" href="#prompting_techniques_ensemling">La Tabella 10-6</a> mostra esempi e casi d'uso di queste tecniche di assemblaggio.</p>
<table class="striped" id="prompting_techniques_ensemling">
<caption><span class="label">Tabella 10-6. </span>Tecniche di prompt di gruppo</caption>
<thead>
<tr>
<th>Tecnica del prompt</th>
<th>Esempi</th>
<th>Casi d'uso</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Autoconsistenza</p></td>
<td><p>prompt #1 (da eseguire più volte): Pensiamo passo dopo passo e completiamo la seguente attività...</p><p>prompt n. 2: Tra le seguenti risposte, scegli quella migliore/comune assegnando un punteggio utilizzando...</p></td>
<td><p>Ridurre gli errori o le distorsioni nei compiti di aritmetica, di senso comune e di ragionamento simbolico.</p></td>
</tr>
<tr>
<td><p>Miscela di esperti di ragionamento (MoRE)</p></td>
<td><p>prompt n. 1 (da eseguire per ogni esperto): Sei un recensore di ..., dai un punteggio a quanto segue in base a...</p><p>prompt n. 2: scegli la migliore risposta dell'esperto in base al punteggio dell'accordo...</p></td>
<td><p>Contabilità per aree o domini di conoscenza specializzati</p></td>
</tr>
<tr>
<td><p>Dimostrazione di assemblaggio (DENSE)</p></td>
<td><p>Crea diversi esempi di traduzione di questo testo e aggrega le risposte migliori.</p><p>Genera diversi prompt per riassumere l'articolo e combina i risultati.</p></td>
<td><ul><li><p>Migliorare l'affidabilità della produzione</p></li>
<li><p>
Aggregare prospettive diverse</p></li></ul></td>
</tr>
<tr>
<td><p>Parafrasi prompt</p></td>
<td><p>Prompt #1a: Riformula questa proposta...</p><p>prompt #1b: Chiarisci questa proposta...</p><p>prompt #1c: Fai una modifica a questa proposta...</p><p>prompt n. 2: Scegli la proposta migliore tra le seguenti risposte in base a...</p></td>
<td><ul><li><p>Esplorare diverse interpretazioni</p></li>
<li>
<p>Aumento dei dati per l'ensembling</p></li></ul></td>
</tr>
</tbody>
</table>
</div></section>
<section data-pdf-bookmark="Self-criticism" data-type="sect4"><div class="sect4" id="id165">
<h4>Autocritica</h4>
<p><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="self-criticism" data-type="indexterm" id="id1126"/><a data-primary="self-criticism prompting" data-type="indexterm" id="id1127"/>Le tecniche di<em>autocritica</em> si concentrano sull'utilizzo dei modelli come giudici, valutatori o revisori dell'intelligenza artificiale, sia per eseguire autocontrolli che per valutare i risultati di altri modelli. La critica o il feedback del primo prompt possono essere utilizzati per migliorare la qualità delle risposte nei prompt successivi.</p>
<p>Queste sono diverse strategie di prompt dell'autocritica:</p>
<dl>
<dt><a href="https://oreil.ly/_4YEr">Autocalibrazione</a></dt>
<dd>
<p>Chiedi al LLM di valutare la correttezza di una risposta rispetto a una domanda/risposta.</p>
</dd>
<dt><a href="https://oreil.ly/bTQJI">Auto-raffinare</a></dt>
<dd>
<p>Affina le risposte in modo iterativo attraverso l'autocontrollo e la fornitura di feedback.</p>
</dd>
<dt><a href="https://oreil.ly/6ojtr">Inversione della catena del pensiero (RCoT)</a></dt>
<dd>
<p>Ricostruire il problema a partire da una risposta generata e poi generare confronti a grana fine tra il problema originale e quello ricostruito per identificare le incongruenze.</p>
</dd>
<dt><a href="https://oreil.ly/Fz3JH">Autoverifica</a></dt>
<dd>
<p>Genera potenziali soluzioni con la tecnica CoT e poi assegna un punteggio a ciascuna di esse mascherando parti della domanda e fornendo ogni risposta.</p>
</dd>
<dt><a href="https://oreil.ly/WrrLP">Catena di verifica (COVE)</a></dt>
<dd>
<p>Crea un elenco di query/domande correlate per verificare la correttezza di una risposta.</p>
</dd>
<dt><a href="https://oreil.ly/3Hb-6">Ragionamento cumulativo</a></dt>
<dd>
<p>Genera potenziali passi per rispondere a una query e poi chiedi al modello di accettare/rifiutare ogni passo. Infine, controlla se è arrivato alla risposta finale per terminare il processo; altrimenti, ripeti il processo.</p>
</dd>
</dl>
<p>Puoi vedere degli esempi di ciascuna tecnica di prompt dell'autocritica nella <a data-type="xref" href="#prompting_techniques_self_criticism">Tabella 10-7</a>.</p>
<table class="striped" id="prompting_techniques_self_criticism">
<caption><span class="label">Tabella 10-7. </span>Tecniche di prompt per l'autocritica</caption>
<thead>
<tr>
<th>Tecnica del prompt</th>
<th>Esempi</th>
<th>Casi d'uso</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Autocalibrazione</p></td>
<td><p>Valuta la correttezza della seguente risposta: [risposta] per la seguente domanda: [domanda]</p></td>
<td><p>Valuta la fiducia nelle risposte per accettare o rivedere la risposta originale.</p></td>
</tr>
<tr>
<td><p>Auto-raffinare</p></td>
<td><p>prompt n. 1: Qual è la tua opinione sulla risposta...</p><p>prompt n. 2: Utilizzando il feedback [Feedback], perfeziona la tua risposta su...</p></td>
<td><p>Attività di ragionamento, codifica e generazione.</p></td>
</tr>
<tr>
<td><p>Inversione della catena del pensiero (RCoT)</p></td>
<td><p>prompt #1: Ricostruisci il problema a partire da questa risposta...</p><p>prompt n. 2: Genera un confronto a grana fine tra queste query...</p></td>
<td><p>Identificare le incongruenze e rivedere le risposte.</p></td>
</tr>
<tr>
<td><p>Autoverifica</p></td>
<td><p>prompt #1 (da eseguire più volte): Pensiamo passo dopo passo - genera una soluzione per il seguente problema...</p><p>prompt n. 2: assegna un punteggio a ciascuna soluzione in base al [problema mascherato]...</p></td>
<td><p>Migliorare i compiti di ragionamento.</p></td>
</tr>
<tr>
<td><p>Catena di verifica (COVE)</p></td>
<td><p>prompt n. 1: rispondi alla seguente domanda...</p><p>prompt #2: formula delle domande correlate per verificare questa risposta: ...</p><p>prompt #3 (da eseguire per ogni nuova domanda correlata): Rispondi alla seguente domanda: ...</p><p>prompt n. 4: in base alle seguenti informazioni, scegli la risposta migliore...</p></td>
<td><p>Attività di risposta alle domande e di generazione di testi.</p></td>
</tr>
<tr>
<td><p>Ragionamento cumulativo</p></td>
<td><p>prompt n. 1: delinea i passaggi per rispondere alla query: ...</p><p>Prompt #2: Controlla il seguente piano e accetta/rifiuta i passaggi rilevanti per rispondere alla query: ...</p><p>prompt #3: verifica di essere arrivato alla risposta finale con le seguenti informazioni...</p></td>
<td><p>Convalida passo dopo passo di query complesse, inferenze logiche e problemi matematici.</p></td>
</tr>
</tbody>
</table>
</div></section>
<section data-pdf-bookmark="Agentic" data-type="sect4"><div class="sect4" id="id166">
<h4>Agente</h4>
<p><a data-primary="agentic prompting techniques" data-type="indexterm" id="ix_ch10-asciidoc40"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="agentic techniques" data-type="indexterm" id="ix_ch10-asciidoc41"/>Puoi fare un ulteriore passo avanti rispetto alle tecniche di prompt discusse finora e aggiungere l'accesso a strumenti esterni con algoritmi di valutazione complessi. Questo processo specializza gli LLMs come <em>agenti</em>, consentendo loro di fare piani, intraprendere azioni e utilizzare<span class="keep-together">sistemi</span> esterni.</p>
<p>I prompt o le <em>sequenze di prompt (catene)</em> guidano i sistemi agenziali con un focus ingegneristico sulla creazione di comportamenti simili a quelli degli agenti da LLMs. Questi flussi di lavoro agenziali servono gli utenti eseguendo azioni sui sistemi che si interfacciano con i modelli GenAI, che sono per lo più LLMs. Gli strumenti, siano essi <em>simbolici</em> come una calcolatrice o <em>neurali</em> come un altro modello di IA, costituiscono una componente fondamentale dei sistemi agenziali.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Se crei una pipeline di più chiamate al modello con un output inoltrato allo stesso modello o a modelli diversi come input, hai costruito una <em>catena di prompt</em>. In linea di principio, quando sfrutti le catene di prompt stai utilizzando la tecnica di prompt della CoT.</p>
</div>
<p>Alcune tecniche di prompt agenziale includono:</p>
<dl>
<dt><a href="https://oreil.ly/aWeQu">Ragionamento, conoscenza e linguaggio modulari (MRKL)</a></dt>
<dd>
<p>Il sistema agenziale più semplice consiste in un LLM che utilizza diversi strumenti per ottenere e combinare le informazioni per generare una risposta.</p>
</dd>
<dt><a href="https://oreil.ly/M-9YL">Autocorrezione con critica interattiva dello strumento (CRITIC)</a></dt>
<dd>
<p>Risponde alle query e poi auto-verifica la risposta senza utilizzare strumenti esterni. Infine, utilizza strumenti per verificare o modificare le risposte.</p>
</dd>
<dt><a href="https://oreil.ly/0WtKv">Modello linguistico assistito da programma (PAL)</a></dt>
<dd>
<p>Genera codice dalle query e lo invia direttamente a interpreti di codice come Python per generare una risposta.<sup><a data-type="noteref" href="ch10.html#id1128" id="id1128-marker" translate="no">4</a></sup></p>
</dd>
<dt><a href="https://oreil.ly/pbfv_">Agente di ragionamento integrato negli strumenti (ToRA)</a></dt>
<dd>
<p>L'AMICO compie un ulteriore passo in avanti, intercalando le fasi di generazione del codice e di ragionamento per tutto il tempo necessario a fornire una risposta soddisfacente.</p>
</dd>
<dt><a href="https://oreil.ly/aDubr">Ragionare e agire (React)</a></dt>
<dd>
<p>Dato un problema, genera pensieri, compie azioni, riceve osservazioni e ripete il ciclo con le informazioni precedenti (cioè la memoria) finché il problema non viene risolto.</p>
</dd>
</dl>
<p>Se vuoi permettere ai tuoi LLMs di utilizzare degli strumenti, puoi sfruttare le funzioni <em>di chiamata di funzione</em> dei fornitori di modelli, come nell'<a data-type="xref" href="#function_calling">Esempio 10-17</a>.</p>
<div data-type="example" id="function_calling">
<h5><span class="label">Esempio 10-17. </span>Chiamata di funzione per il fetching</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">openai</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">OpenAI</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">scraper</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">fetch</code>
<code class="n" translate="no">client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">OpenAI</code><code class="p" translate="no">()</code>

<code class="n" translate="no">tools</code> <code class="o" translate="no">=</code> <code class="p" translate="no">[</code>
    <code class="p" translate="no">{</code>
        <code class="s2" translate="no">"type"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"function"</code><code class="p" translate="no">,</code>
        <code class="s2" translate="no">"function"</code><code class="p" translate="no">:</code> <code class="p" translate="no">{</code>
            <code class="s2" translate="no">"name"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"fetch"</code><code class="p" translate="no">,</code>
            <code class="s2" translate="no">"description"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"Read the content of url and provide a summary"</code><code class="p" translate="no">,</code>
            <code class="s2" translate="no">"parameters"</code><code class="p" translate="no">:</code> <code class="p" translate="no">{</code>
                <code class="s2" translate="no">"type"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"object"</code><code class="p" translate="no">,</code>
                <code class="s2" translate="no">"properties"</code><code class="p" translate="no">:</code> <code class="p" translate="no">{</code>
                    <code class="s2" translate="no">"url"</code><code class="p" translate="no">:</code> <code class="p" translate="no">{</code>
                        <code class="s2" translate="no">"type"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"string"</code><code class="p" translate="no">,</code>
                        <code class="s2" translate="no">"description"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"The url to fetch"</code><code class="p" translate="no">,</code>
                    <code class="p" translate="no">},</code>
                <code class="p" translate="no">},</code>
                <code class="s2" translate="no">"required"</code><code class="p" translate="no">:</code> <code class="p" translate="no">[</code><code class="s2" translate="no">"url"</code><code class="p" translate="no">],</code>
                <code class="s2" translate="no">"additionalProperties"</code><code class="p" translate="no">:</code> <code class="kc" translate="no">False</code><code class="p" translate="no">,</code>
            <code class="p" translate="no">},</code>
        <code class="p" translate="no">},</code>
    <code class="p" translate="no">}</code>
<code class="p" translate="no">]</code>

<code class="n" translate="no">messages</code> <code class="o" translate="no">=</code> <code class="p" translate="no">[</code>
    <code class="p" translate="no">{</code>
        <code class="s2" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"system"</code><code class="p" translate="no">,</code>
        <code class="s2" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"You are a helpful customer support assistant"</code>
        <code class="s2" translate="no">"Use the supplied tools to assist the user."</code><code class="p" translate="no">,</code>
    <code class="p" translate="no">},</code>
    <code class="p" translate="no">{</code>
        <code class="s2" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"user"</code><code class="p" translate="no">,</code>
        <code class="s2" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"Summarize this paper: https://arxiv.org/abs/2207.05221"</code><code class="p" translate="no">,</code>
    <code class="p" translate="no">},</code>
<code class="p" translate="no">]</code>

<code class="n" translate="no">response</code> <code class="o" translate="no">=</code> <code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">chat</code><code class="o" translate="no">.</code><code class="n" translate="no">completions</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code>
    <code class="n" translate="no">model</code><code class="o" translate="no">=</code><code class="s2" translate="no">"gpt-4o"</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">messages</code><code class="o" translate="no">=</code><code class="n" translate="no">messages</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">tools</code><code class="o" translate="no">=</code><code class="n" translate="no">tools</code><code class="p" translate="no">,</code>
<code class="p" translate="no">)</code></pre></div>
<p>Come hai visto nell'<a data-type="xref" href="#function_calling">Esempio 10-17</a>, puoi creare sistemi agenziali configurando LLMs specializzati che hanno accesso a strumenti personalizzati e alle funzioni di<a data-startref="ix_ch10-asciidoc41" data-type="indexterm" id="id1129"/><a data-startref="ix_ch10-asciidoc40" data-type="indexterm" id="id1130"/><a data-startref="ix_ch10-asciidoc39" data-type="indexterm" id="id1131"/><a data-startref="ix_ch10-asciidoc38" data-type="indexterm" id="id1132"/> .<a data-startref="ix_ch10-asciidoc37" data-type="indexterm" id="id1133"/><a data-startref="ix_ch10-asciidoc36" data-type="indexterm" id="id1134"/></p>
</div></section>
</div></section>
</div></section>
<section data-pdf-bookmark="Fine-Tuning" data-type="sect2"><div class="sect2" id="id167">
<h2>Messa a punto</h2>
<p><a data-primary="fine-tuning (optimization technique)" data-type="indexterm" id="ix_ch10-asciidoc42"/><a data-primary="optimizing AI services" data-secondary="fine-tuning" data-type="indexterm" id="ix_ch10-asciidoc43"/>Ci sono casi in cui l'ingegneria del prompt da sola non è in grado di fornire la qualità di risposta che stai cercando. La messa a punto è una tecnica di ottimizzazione che richiede di regolare i parametri del tuo modello GenAI per adattarlo meglio ai tuoi dati. Ad esempio, puoi mettere a punto un modello linguistico per imparare il contenuto di basi di conoscenza private o per rispondere sempre con un certo tono seguendo le linee guida del tuo marchio.</p>
<p>Spesso non è la prima tecnica da provare perché richiede uno sforzo per raccogliere e preparare i dati, oltre che per addestrare e valutare i modelli.</p>
<section data-pdf-bookmark="When should you consider fine-tuning?" data-type="sect3"><div class="sect3" id="id168">
<h3>Quando dovresti prendere in considerazione la messa a punto?</h3>
<p><a data-primary="fine-tuning (optimization technique)" data-secondary="when to consider" data-type="indexterm" id="ix_ch10-asciidoc44"/><a data-primary="optimizing AI services" data-secondary="fine-tuning" data-tertiary="when to consider" data-type="indexterm" id="ix_ch10-asciidoc45"/>Potresti prendere in considerazione la possibilità di perfezionare i modelli GenAI pre-addestrati se si verifica uno dei seguenti scenari:</p>
<ul>
<li>
<p>Hai dei costi significativi per l'utilizzo dei token, ad esempio perché devi richiedere istruzioni di sistema dettagliate o fornire molti esempi in ogni prompt.</p>
</li>
<li>
<p>Il tuo caso d'uso si basa su competenze specialistiche del dominio che il modello deve imparare.</p>
</li>
<li>
<p>È necessario ridurre il numero di allucinazioni nelle risposte con un modello conservativo più preciso.</p>
</li>
<li>
<p>Hai bisogno di risposte di qualità superiore e di dati sufficienti per la messa a punto.</p>
</li>
<li>
<p>Hai bisogno di una latenza minore nelle risposte.</p>
</li>
</ul>
<p>Una volta messo a punto il modello, non sarà più necessario fornire tanti esempi nel prompt, risparmiando sui costi e consentendo richieste a bassa latenza.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Evita il più possibile la messa a punto.</p>
<p>L'iterazione sui prompt ha un ciclo di feedback molto più rapido rispetto all'iterazione sulla messa a punto, che si basa sulla creazione di set di dati e sull'esecuzione di lavori di formazione.</p>
<p>Tuttavia, se alla fine avrai bisogno di una messa a punto, noterai che gli sforzi iniziali di prompt engineering contribuiranno a produrre dati di formazione di qualità superiore.</p>
</div>
<p>Ecco alcuni casi in cui la messa a punto può essere utile:</p>
<ul>
<li>
<p>Insegnare a un modello a rispondere a uno stile, a un tono, a un formato o a un'altra metrica qualitativa, ad esempio per produrre rapporti standardizzati conformi ai requisiti normativi e ai protocolli interni.</p>
</li>
<li>
<p>Migliorare l'affidabilità della produzione degli output desiderati, come ad esempio avere sempre risposte conformi a un determinato output strutturato.</p>
</li>
<li>
<p>Ottenere risultati corretti per query complesse come la classificazione di documenti e l'etichettatura di centinaia di classi.</p>
</li>
<li>
<p>Esecuzione di compiti specializzati in un dominio specifico, come la classificazione di articoli o l'interpretazione e l'aggregazione di dati specifici per il settore.</p>
</li>
<li>
<p>Gestione più accurata dei casi limite</p>
</li>
<li>
<p>Eseguire abilità o compiti difficili da articolare nei prompt, come l'estrazione di date da testi non strutturati.</p>
</li>
<li>
<p>Ridurre i costi utilizzando <code translate="no">gpt-40-mini</code> o addirittura <code translate="no">gpt-3.5-turbo</code> al posto di <code translate="no">gpt-4o</code></p>
</li>
<li>
<p>Insegnare a un modello a utilizzare strumenti e API complesse quando si usa la chiamata di funzione<a data-startref="ix_ch10-asciidoc45" data-type="indexterm" id="id1135"/><a data-startref="ix_ch10-asciidoc44" data-type="indexterm" id="id1136"/></p>
</li>
</ul>
</div></section>
<section data-pdf-bookmark="How to fine-tune a pretrained model" data-type="sect3"><div class="sect3" id="id169">
<h3>Come mettere a punto un modello preaddestrato</h3>
<p><a data-primary="fine-tuning (optimization technique)" data-secondary="for pretrained model" data-type="indexterm" id="ix_ch10-asciidoc46"/><a data-primary="optimizing AI services" data-secondary="fine-tuning" data-tertiary="for pretrained model" data-type="indexterm" id="ix_ch10-asciidoc47"/>Per qualsiasi lavoro di messa a punto, dovrai seguire questi passaggi:</p>
<ol>
<li>
<p>Preparare e caricare i dati della formazione.</p>
</li>
<li>
<p>Invia un lavoro di formazione per la messa a punto.</p>
</li>
<li>
<p>Valutare e utilizzare il modello perfezionato.</p>
</li>
</ol>
<p>A seconda del modello che stai utilizzando, i dati devono essere preparati in base alle istruzioni del fornitore del modello.</p>
<p>Ad esempio, per mettere a punto un tipico modello di chat come <code translate="no">gpt-4o-2024-08-06</code>, devi preparare i tuoi dati in un formato di messaggio, come mostrato nell'<a data-type="xref" href="#fine_tune_prepare">Esempio 10-18</a>. Al momento in cui scriviamo, il <a href="https://oreil.ly/MmCNq">prezzo dell'API OpenAI</a> per la messa a punto di questo modello è di $25/1M di token di addestramento.</p>
<div data-type="example" id="fine_tune_prepare">
<h5><span class="label">Esempio 10-18. </span>Esempio di dati di allenamento per un lavoro di messa a punto</h5>
<pre data-code-language="json" data-type="programlisting" translate="no"><code class="c1" translate="no">// training_data.jsonl</code><code class="w" translate="no"/>

<code class="p" translate="no">{</code><code class="w" translate="no"/>
    <code class="nt" translate="no">"messages"</code><code class="p" translate="no">:</code> <code class="p" translate="no">[</code><code class="w" translate="no"/>
        <code class="p" translate="no">{</code><code class="w" translate="no"/>
            <code class="nt" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"system"</code><code class="p" translate="no">,</code><code class="w" translate="no"/>
            <code class="nt" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"&lt;text&gt;"</code><code class="w" translate="no"/>
        <code class="p" translate="no">},</code><code class="w" translate="no"/>
        <code class="p" translate="no">{</code><code class="w" translate="no"/>
            <code class="nt" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"user"</code><code class="p" translate="no">,</code><code class="w" translate="no"/>
            <code class="nt" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"&lt;text&gt;"</code><code class="w" translate="no"/>
        <code class="p" translate="no">},</code><code class="w" translate="no"/>
        <code class="p" translate="no">{</code><code class="w" translate="no"/>
            <code class="nt" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"assistant"</code><code class="p" translate="no">,</code><code class="w" translate="no"/>
            <code class="nt" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"&lt;text&gt;"</code><code class="w" translate="no"/>
        <code class="p" translate="no">}</code><code class="w" translate="no"/>
    <code class="p" translate="no">]</code><code class="w" translate="no"/>
<code class="p" translate="no">}</code><code class="w" translate="no"/>
<code class="c1" translate="no">// more entries</code><code class="w" translate="no"/></pre></div>
<p>Una volta preparati i dati, devi caricare il file <code translate="no">jsonl</code>, ottenere un ID del file e fornirlo quando invii un lavoro di messa a punto, come puoi vedere nell'<a data-type="xref" href="#fine_tuning_training">Esempio 10-19</a>.</p>
<div class="less_space pagebreak-before" data-type="example" id="fine_tuning_training">
<h5><span class="label">Esempio 10-19. </span>Invio di un lavoro di formazione per la messa a punto</h5>
<pre data-type="programlisting" translate="no">from openai import OpenAI
client = OpenAI()

response = client.files.create(
    file=open("mydata.jsonl", "rb"), purpose="fine-tune"
)

client.fine_tuning.jobs.create(
    training_file=response.id, model="gpt-4o-mini-2024-07-18"
)</pre></div>
<p>I fornitori di modelli che ti permettono di inviare lavori di messa a punto forniranno anche API per controllare lo stato dei lavori inviati e per ottenere i risultati.</p>
<p>Una volta che il modello è stato messo a punto, puoi recuperare l'ID del modello messo a punto e passarlo al client LLM, come mostrato nell'<a data-type="xref" href="#fine_tuning_usage">Esempio 10-20</a>. Assicurati di valutare il modello prima di utilizzarlo in produzione.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Puoi anche utilizzare le tecniche di test discusse nel <a data-type="xref" href="ch11.html#ch11">Capitolo 11</a> per valutare i modelli perfezionati.</p>
</div>
<div data-type="example" id="fine_tuning_usage">
<h5><span class="label">Esempio 10-20. </span>Utilizzo di un modello ottimizzato</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">openai</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">OpenAI</code>
<code class="n" translate="no">client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">OpenAI</code><code class="p" translate="no">()</code>

<code class="n" translate="no">fine_tuning_job_id</code> <code class="o" translate="no">=</code> <code class="s2" translate="no">"ftjob-abc123"</code>
<code class="n" translate="no">response</code> <code class="o" translate="no">=</code> <code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">fine_tuning</code><code class="o" translate="no">.</code><code class="n" translate="no">jobs</code><code class="o" translate="no">.</code><code class="n" translate="no">retrieve</code><code class="p" translate="no">(</code><code class="n" translate="no">fine_tuning_job_id</code><code class="p" translate="no">)</code>
<code class="n" translate="no">fine_tuned_model</code> <code class="o" translate="no">=</code> <code class="n" translate="no">response</code><code class="o" translate="no">.</code><code class="n" translate="no">fine_tuned_model</code>

<code class="k" translate="no">if</code> <code class="n" translate="no">fine_tuned_model</code> <code class="ow" translate="no">is</code> <code class="kc" translate="no">None</code><code class="p" translate="no">:</code>
    <code class="k" translate="no">raise</code> <code class="ne" translate="no">ValueError</code><code class="p" translate="no">(</code>
        <code class="sa" translate="no">f</code><code class="s2" translate="no">"Failed to retrieve the fine-tuned model - "</code>
        <code class="sa" translate="no">f</code><code class="s2" translate="no">"Job ID: </code><code class="si" translate="no">{</code><code class="n" translate="no">fine_tuning_job_id</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code>
    <code class="p" translate="no">)</code>

<code class="n" translate="no">completion</code> <code class="o" translate="no">=</code> <code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">chat</code><code class="o" translate="no">.</code><code class="n" translate="no">completions</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code>
    <code class="n" translate="no">model</code><code class="o" translate="no">=</code><code class="n" translate="no">fine_tuned_model</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">messages</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code>
        <code class="p" translate="no">{</code><code class="s2" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"system"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"You are a helpful assistant."</code><code class="p" translate="no">},</code>
        <code class="p" translate="no">{</code><code class="s2" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"user"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"Hello!"</code><code class="p" translate="no">},</code>
    <code class="p" translate="no">],</code>
<code class="p" translate="no">)</code>
<code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="n" translate="no">completion</code><code class="o" translate="no">.</code><code class="n" translate="no">choices</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">]</code><code class="o" translate="no">.</code><code class="n" translate="no">message</code><code class="p" translate="no">)</code></pre></div>
<p>Sebbene questi esempi mostrino il processo di messa a punto con OpenAI, il processo sarà simile con altri fornitori anche se i dettagli dell'implementazione possono essere diversi.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Se decidi di sfruttare il fine-tuning, tieni presente che non sarai in grado di sfruttare gli ultimi miglioramenti o le ultime ottimizzazioni dei nuovi LLMs, rendendo potenzialmente il processo di fine-tuning uno spreco di tempo e denaro.</p>
</div>
<p>Con questa fase finale di ottimizzazione, dovresti essere sicuro di poter costruire servizi GenAI che non solo soddisfino i tuoi requisiti di sicurezza e qualità, ma che raggiungano anche le metriche di throughput e latenza desiderate<a data-startref="ix_ch10-asciidoc47" data-type="indexterm" id="id1137"/><a data-startref="ix_ch10-asciidoc46" data-type="indexterm" id="id1138"/>.<a data-startref="ix_ch10-asciidoc43" data-type="indexterm" id="id1139"/><a data-startref="ix_ch10-asciidoc42" data-type="indexterm" id="id1140"/></p>
</div></section>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id170">
<h1>Sommario</h1>
<p>In questo capitolo hai appreso diverse strategie di ottimizzazione per migliorare il throughput e la qualità dei tuoi servizi. Alcune ottimizzazioni che hai aggiunto hanno riguardato la cache (parole chiave, semantica, contesto), l'ingegneria del prompt, la quantizzazione del modello e la messa a punto.<a data-startref="ix_ch10-asciidoc0" data-type="indexterm" id="id1141"/></p>
<p>Nel prossimo capitolo ci concentreremo sull'ultima fase della costruzione di servizi di AI: la distribuzione della tua soluzione GenAI, che comprende l'esplorazione dei modelli di distribuzione per i servizi di AI e la containerizzazione con Docker.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1069"><sup><a href="ch10.html#id1069-marker">1</a></sup> Vedi l'API OpenAI Batch disponibile nella <a href="https://oreil.ly/0t59w">documentazione</a> dell'<a href="https://oreil.ly/0t59w">API OpenAI</a>.</p><p data-type="footnote" id="id1072"><sup><a href="ch10.html#id1072-marker">2</a></sup> Per saperne di più sulle intestazioni di controllo della cache, visita il <a href="https://oreil.ly/-Y5JP">sito MDN</a>.</p><p data-type="footnote" id="id1076"><sup><a href="ch10.html#id1076-marker">3</a></sup> Per ottenere un risparmio significativo sui costi, potrebbe essere necessario un modello embedder addestrato, in quanto le frequenti chiamate API a un modello embedder off-the-shelf potrebbero comportare costi aggiuntivi, riducendo il risparmio complessivo.</p><p data-type="footnote" id="id1128"><sup><a href="ch10.html#id1128-marker">4</a></sup> Per una maggiore sicurezza, devi comunque sanificare il codice generato da LLM prima di inoltrarlo ai sistemi a valle per l'esecuzione.</p></div></div></section></div></div></body></html>