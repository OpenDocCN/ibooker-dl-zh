<html><head></head><body><section data-pdf-bookmark="Chapter 10. Optimizing AI Services" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch10">
<h1><span class="label">Chapter 10. </span>Optimizing AI Services</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1067">
<h1>Chapter Goals</h1>
<p><a data-primary="optimizing AI services" data-type="indexterm" id="ix_ch10-asciidoc0"/>In this chapter, you will learn about:</p>

<ul>
<li>
<p>Optimization techniques such as keyword, semantic, and context caching</p>
</li>
<li>
<p>Advanced prompt engineering techniques to maximize model generation quality and coherence</p>
</li>
<li>
<p>Model quantization and the difference that quantized models make in model serving</p>
</li>
<li>
<p>Using batch processing APIs for larger AI workloads</p>
</li>
<li>
<p>The benefits, drawbacks, and use cases of model fine-tuning</p>
</li>
</ul>
</div></aside>

<p>In this chapter, you’ll learn to further optimize your services via prompt engineering, model quantization, and caching mechanisms.</p>






<section data-pdf-bookmark="Optimization Techniques" data-type="sect1"><div class="sect1" id="id256">
<h1>Optimization Techniques</h1>

<p><a data-primary="optimizing AI services" data-secondary="basics" data-type="indexterm" id="id1068"/>The objectives of optimizing an AI service are to either improve output quality or performance (latency, throughput, costs, etc.).</p>

<p>Performance-related optimizations include the following:</p>

<ul>
<li>
<p>Using batch processing APIs</p>
</li>
<li>
<p>Caching (keyword, semantic, context, or prompt)</p>
</li>
<li>
<p>Model quantization</p>
</li>
</ul>

<p>Quality-related optimizations include the following:</p>

<ul>
<li>
<p>Using structured outputs</p>
</li>
<li>
<p>Prompt engineering</p>
</li>
<li>
<p>Model fine-tuning</p>
</li>
</ul>

<p>Let’s review each in more detail.</p>








<section data-pdf-bookmark="Batch Processing" data-type="sect2"><div class="sect2" id="id145">
<h2>Batch Processing</h2>

<p><a data-primary="batch processing" data-type="indexterm" id="ix_ch10-asciidoc1"/><a data-primary="optimizing AI services" data-secondary="batch processing" data-type="indexterm" id="ix_ch10-asciidoc2"/>Often you want an LLM to process batches of entries at the same time.
The most obvious solution is to submit multiple API calls per entry.
However, the obvious approach can be costly and slow and may lead to your model provider rate limiting you.</p>

<p>In such cases, you can leverage two separate techniques for batch processing your data through an LLM:</p>

<ul>
<li>
<p>Updating your structured output schemas to return multiple examples at the same time</p>
</li>
<li>
<p>Identifying and using model provider APIs that are designed for batch processing</p>
</li>
</ul>

<p>The first solution requires you to update your Pydantic models or template prompts to request a list of outputs per request.
In this case, you can batch process your data within a handful of requests instead of one per entry.</p>

<p>An implementation of the first solution is shown in <a data-type="xref" href="#batch_processing_structured_outputs">Example 10-1</a>.</p>
<div data-type="example" id="batch_processing_structured_outputs">
<h5><span class="label">Example 10-1. </span>Updating structured output schema for parsing multiple items</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pydantic</code> <code class="kn">import</code> <code class="n">BaseModel</code>

<code class="k">class</code> <code class="nc">BatchDocumentClassification</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">)</code><code class="p">:</code>
    <code class="k">class</code> <code class="nc">Category</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">)</code><code class="p">:</code>
        <code class="n">document_id</code><code class="p">:</code> <code class="nb">str</code>
        <code class="n">category</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code>

    <code class="n">categories</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="n">Category</code><code class="p">]</code> <a class="co" href="#callout_optimizing_ai_services_CO1-1" id="co_optimizing_ai_services_CO1-1"><img alt="1" src="assets/1.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO1-1" id="callout_optimizing_ai_services_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Update the Pydantic model to include a list of <code>Category</code> models.</p></dd>
</dl></div>

<p>You can now pass the new schema alongside a list of document titles to the OpenAI client to process multiple entries in a single API call.
However, an alternative and possibly the best solution will be to use a batch API, if available.</p>

<p>Luckily, model providers such as OpenAI already supply relevant APIs for such use cases.
Under the hood, these providers may run task queues to process any single batch job in the background while providing you with status updates until the batch is complete to retrieve the results.</p>

<p>Compared to using standard endpoints directly, you’ll be able to send asynchronous groups of requests with lower costs (up to 50% with OpenAI<sup><a data-type="noteref" href="ch10.html#id1069" id="id1069-marker">1</a></sup>), enjoy higher rate limits, and guarantee completion times.
The batch job service is ideal for processing jobs that don’t require immediate responses such as using OpenAI LLMs to parse, classify, or translate large volumes of documents in the background.</p>

<p>To submit a batch job, you’ll need a <code>jsonl</code> file where each line contains the details of an individual request to the API, as shown in <a data-type="xref" href="#jsonl">Example 10-2</a>. Also as seen in this example, to create the JSONL file, you can iterate over your entries and dynamically generate the file.</p>
<div data-type="example" id="jsonl">
<h5><span class="label">Example 10-2. </span>Creating a JSONL file from entries</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">json</code>
<code class="kn">from</code> <code class="nn">uuid</code> <code class="kn">import</code> <code class="n">UUID</code>

<code class="k">def</code> <code class="nf">create_batch_file</code><code class="p">(</code>
    <code class="n">entries</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">str</code><code class="p">],</code>
    <code class="n">system_prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>
    <code class="n">model</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">"gpt-4o-mini"</code><code class="p">,</code>
    <code class="n">filepath</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">"batch.jsonl"</code><code class="p">,</code>
    <code class="n">max_tokens</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">1024</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="kc">None</code><code class="p">:</code>
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">filepath</code><code class="p">,</code> <code class="s2">"w"</code><code class="p">)</code> <code class="k">as</code> <code class="n">file</code><code class="p">:</code>
        <code class="k">for</code> <code class="n">_</code><code class="p">,</code> <code class="n">entry</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">entries</code><code class="p">,</code> <code class="n">start</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>
            <code class="n">request</code> <code class="o">=</code> <code class="p">{</code>
                <code class="s2">"custom_id"</code><code class="p">:</code> <code class="sa">f</code><code class="s2">"request-</code><code class="si">{</code><code class="n">UUID</code><code class="p">()</code><code class="si">}</code><code class="s2">"</code><code class="p">,</code>
                <code class="s2">"method"</code><code class="p">:</code> <code class="s2">"POST"</code><code class="p">,</code>
                <code class="s2">"url"</code><code class="p">:</code> <code class="s2">"/v1/chat/completions"</code><code class="p">,</code>
                <code class="s2">"body"</code><code class="p">:</code> <code class="p">{</code>
                    <code class="s2">"model"</code><code class="p">:</code> <code class="n">model</code><code class="p">,</code>
                    <code class="s2">"messages"</code><code class="p">:</code> <code class="p">[</code>
                        <code class="p">{</code>
                            <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code>
                            <code class="s2">"content"</code><code class="p">:</code> <code class="n">system_prompt</code><code class="p">,</code>
                        <code class="p">},</code>
                        <code class="p">{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code> <code class="s2">"content"</code><code class="p">:</code> <code class="n">entry</code><code class="p">},</code>
                    <code class="p">],</code>
                    <code class="s2">"max_tokens"</code><code class="p">:</code> <code class="n">max_tokens</code><code class="p">,</code>
                <code class="p">},</code>
            <code class="p">}</code>
            <code class="n">file</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">json</code><code class="o">.</code><code class="n">dumps</code><code class="p">(</code><code class="n">request</code><code class="p">)</code> <code class="o">+</code> <code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code></pre></div>

<p>Once created, you can submit the file to the batch API for processing, as shown in <a data-type="xref" href="#batch_processing_api">Example 10-3</a>.</p>
<div data-type="example" id="batch_processing_api">
<h5><span class="label">Example 10-3. </span>Processing batch jobs with the OpenAI Batch API</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">loguru</code> <code class="kn">import</code> <code class="n">logger</code>
<code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">AsyncOpenAI</code>
<code class="kn">from</code> <code class="nn">openai.types</code> <code class="kn">import</code> <code class="n">Batch</code>

<code class="n">client</code> <code class="o">=</code> <code class="n">AsyncOpenAI</code><code class="p">()</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">submit_batch_job</code><code class="p">(</code><code class="n">filepath</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Batch</code><code class="p">:</code>
    <code class="k">if</code> <code class="s2">".jsonl"</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">filepath</code><code class="p">:</code>
        <code class="k">raise</code> <code class="ne">FileNotFoundError</code><code class="p">(</code><code class="sa">f</code><code class="s2">"JSONL file not provided at </code><code class="si">{</code><code class="n">filepath</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

    <code class="n">file_response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">client</code><code class="o">.</code><code class="n">files</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="n">file</code><code class="o">=</code><code class="nb">open</code><code class="p">(</code><code class="n">filepath</code><code class="p">,</code> <code class="s2">"rb"</code><code class="p">),</code> <code class="n">purpose</code><code class="o">=</code><code class="s2">"batch"</code>
    <code class="p">)</code>

    <code class="n">batch_job_response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">client</code><code class="o">.</code><code class="n">batches</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="n">input_file_id</code><code class="o">=</code><code class="n">file_response</code><code class="o">.</code><code class="n">id</code><code class="p">,</code>
        <code class="n">endpoint</code><code class="o">=</code><code class="s2">"/v1/chat/completions"</code><code class="p">,</code>
        <code class="n">completion_window</code><code class="o">=</code><code class="s2">"24h"</code><code class="p">,</code>
        <code class="n">metadata</code><code class="o">=</code><code class="p">{</code><code class="s2">"description"</code><code class="p">:</code> <code class="s2">"document classification job"</code><code class="p">},</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">batch_job_response</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">retrieve_batch_results</code><code class="p">(</code><code class="n">batch_id</code><code class="p">:</code> <code class="nb">str</code><code class="p">):</code>
    <code class="n">batch</code> <code class="o">=</code> <code class="k">await</code> <code class="n">client</code><code class="o">.</code><code class="n">batches</code><code class="o">.</code><code class="n">retrieve</code><code class="p">(</code><code class="n">batch_id</code><code class="p">)</code>
    <code class="k">if</code> <code class="p">(</code>
        <code class="n">status</code> <code class="o">:=</code> <code class="n">batch</code><code class="o">.</code><code class="n">status</code> <code class="o">==</code> <code class="s2">"completed"</code>
        <code class="ow">and</code> <code class="n">batch</code><code class="o">.</code><code class="n">output_file_id</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code>
    <code class="p">):</code>
        <code class="n">file_content</code> <code class="o">=</code> <code class="k">await</code> <code class="n">client</code><code class="o">.</code><code class="n">files</code><code class="o">.</code><code class="n">content</code><code class="p">(</code><code class="n">batch</code><code class="o">.</code><code class="n">output_file_id</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">file_content</code>
    <code class="n">logger</code><code class="o">.</code><code class="n">warning</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Batch </code><code class="si">{</code><code class="n">batch_id</code><code class="si">}</code><code class="s2"> is in </code><code class="si">{</code><code class="n">status</code><code class="si">}</code><code class="s2"> status"</code><code class="p">)</code></pre></div>

<p>You can now leverage offline batch endpoints to process multiple entries in one go with guaranteed turnaround times and significant cost savings.</p>

<p>Alongside leveraging structured outputs and batch APIs to optimize your services, you can also leverage caching techniques to significantly speed up response times and resource costs of your servers.<a data-startref="ix_ch10-asciidoc2" data-type="indexterm" id="id1070"/><a data-startref="ix_ch10-asciidoc1" data-type="indexterm" id="id1071"/></p>
</div></section>








<section data-pdf-bookmark="Caching" data-type="sect2"><div class="sect2" id="id146">
<h2>Caching</h2>

<p><a data-primary="caching" data-type="indexterm" id="ix_ch10-asciidoc3"/><a data-primary="caching" data-secondary="for optimizing AI services" data-secondary-sortas="optimizing AI services" data-type="indexterm" id="ix_ch10-asciidoc4"/><a data-primary="optimizing AI services" data-secondary="caching" data-type="indexterm" id="ix_ch10-asciidoc5"/>In GenAI services, you’ll often rely on data/model response that require significant computations or long processing durations.
If you have multiple users requesting the same data, repeating the same operations can be wasteful.
Instead, you can use caching techniques for storing and retrieving frequently accessed data to help you optimize your services by speeding up response times, reducing server load, and saving bandwidth and operational costs.</p>

<p>For example, in a public FAQ chatbot where users ask mostly the same questions, you may want to reuse the cached responses for longer periods.
On the other hand, for more personalized and dynamic chatbots, you can frequently refresh (i.e., invalidate) the cached response.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You should always consider the frequency of cache refreshes based on the nature of the data and the acceptable level of staleness.</p>
</div>

<p>The most relevant caching strategies for GenAI services include:</p>

<ul>
<li>
<p>Keyword caching</p>
</li>
<li>
<p>Semantic caching</p>
</li>
<li>
<p>Context or prompt caching</p>
</li>
</ul>

<p>Let’s review each in more detail.</p>










<section data-pdf-bookmark="Keyword caching" data-type="sect3"><div class="sect3" id="id147">
<h3>Keyword caching</h3>

<p><a data-primary="caching" data-secondary="keyword caching" data-type="indexterm" id="ix_ch10-asciidoc6"/><a data-primary="keyword caching" data-type="indexterm" id="ix_ch10-asciidoc7"/><a data-primary="optimizing AI services" data-secondary="caching" data-tertiary="keyword caching" data-type="indexterm" id="ix_ch10-asciidoc8"/>If all you need is a simple caching mechanism for storing functions or endpoint responses, you can use <em>keyword caching</em>, which involves caching responses based on exact matches of input queries as key-value pairs.</p>

<p>In FastAPI, libraries such as <code>fastapi-cache</code> can help you implement keyword caching in a few lines of code, on any functions or endpoints.
FastAPI caches also give you the option to attach storage backends such as Redis for centralizing the cache store across your instances.</p>
<div class="less_space pagebreak-before" data-type="tip"><h6>Tip</h6>
<p>Alternatively, you can implement your own custom caching mechanism with a cache store using lower-level packages such as <code>cachetools</code>.</p>
</div>

<p>To get started, all you have to do is to initialize and configure the caching system as part of the application lifespan, as shown in <a data-type="xref" href="#caching_lifespan">Example 10-4</a>. You can install FastAPI cache using the following command:</p>

<pre data-type="programlisting">$ pip install "fastapi-cache2[redis]"</pre>
<div data-type="example" id="caching_lifespan">
<h5><span class="label">Example 10-4. </span>Configuring FastAPI cache lifespan</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">collections</code><code class="nn">.</code><code class="nn">abc</code> <code class="kn">import</code> <code class="n">AsyncIterator</code>
<code class="kn">from</code> <code class="nn">contextlib</code> <code class="kn">import</code> <code class="n">asynccontextmanager</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code>
<code class="kn">from</code> <code class="nn">fastapi_cache</code> <code class="kn">import</code> <code class="n">FastAPICache</code>
<code class="kn">from</code> <code class="nn">fastapi_cache</code><code class="nn">.</code><code class="nn">backends</code><code class="nn">.</code><code class="nn">redis</code> <code class="kn">import</code> <code class="n">RedisBackend</code>
<code class="kn">from</code> <code class="nn">redis</code> <code class="kn">import</code> <code class="n">asyncio</code> <code class="k">as</code> <code class="n">aioredis</code>

<code class="nd">@asynccontextmanager</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">lifespan</code><code class="p">(</code><code class="n">_</code><code class="p">:</code> <code class="n">FastAPI</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">AsyncIterator</code><code class="p">[</code><code class="kc">None</code><code class="p">]</code><code class="p">:</code>
    <code class="n">redis</code> <code class="o">=</code> <code class="n">aioredis</code><code class="o">.</code><code class="n">from_url</code><code class="p">(</code><code class="s2">"</code><code class="s2">redis://localhost</code><code class="s2">"</code><code class="p">)</code>
    <code class="n">FastAPICache</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">RedisBackend</code><code class="p">(</code><code class="n">redis</code><code class="p">)</code><code class="p">,</code> <code class="n">prefix</code><code class="o">=</code><code class="s2">"</code><code class="s2">fastapi-cache</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_optimizing_ai_services_CO2-1" id="co_optimizing_ai_services_CO2-1"><img alt="1" src="assets/1.png"/></a>
    <code class="k">yield</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="n">lifespan</code><code class="o">=</code><code class="n">lifespan</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO2-1" id="callout_optimizing_ai_services_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Initialize <code>FastAPICache</code> with a <code>RedisBackend</code> that doesn’t decode responses so that cached data is stored as bytes (binary).
This is because decoding responses would break caching by altering the original response format.</p></dd>
</dl></div>

<p>Once the caching system is configured, you can decorate your functions or endpoint handlers to cache their outputs, as shown in <a data-type="xref" href="#caching_functions_endpoint">Example 10-5</a>.</p>
<div data-type="example" id="caching_functions_endpoint">
<h5><span class="label">Example 10-5. </span>Function and endpoint results caching</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">APIRouter</code>
<code class="kn">from</code> <code class="nn">fastapi_cache</code><code class="nn">.</code><code class="nn">decorator</code> <code class="kn">import</code> <code class="n">cache</code>

<code class="n">router</code> <code class="o">=</code> <code class="n">APIRouter</code><code class="p">(</code><code class="n">prefix</code><code class="o">=</code><code class="s2">"</code><code class="s2">/generate</code><code class="s2">"</code><code class="p">,</code> <code class="n">tags</code><code class="o">=</code><code class="p">[</code><code class="s2">"</code><code class="s2">Resource</code><code class="s2">"</code><code class="p">]</code><code class="p">)</code>

<code class="nd">@cache</code><code class="p">(</code><code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">classify_document</code><code class="p">(</code><code class="n">title</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="nd">@router</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">/text</code><code class="s2">"</code><code class="p">)</code>
<code class="nd">@cache</code><code class="p">(</code><code class="n">expire</code><code class="o">=</code><code class="mi">60</code><code class="p">)</code> <a class="co" href="#callout_optimizing_ai_services_CO3-1" id="co_optimizing_ai_services_CO3-1"><img alt="1" src="assets/1.png"/></a>
<code class="k">async</code> <code class="k">def</code> <code class="nf">serve_text_to_text_controller</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO3-1" id="callout_optimizing_ai_services_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>The <code>cache()</code> decorator must always come last.
Invalidate the cache in 60 seconds by setting <code>expires=60</code> to recompute the outputs.</p></dd>
</dl></div>

<p>The <code>cache()</code> decorator shown in <a data-type="xref" href="#caching_functions_endpoint">Example 10-5</a> injects dependencies for the <code>Request</code> and <code>Response</code> objects so that it can add cache control headers to the outgoing response.
These cache control headers instruct clients how to cache the responses on their side by specifying a set of directives (i.e., instructions).</p>

<p>These are a few common cache control directives when sending responses:</p>
<dl>
<dt><code>max-age</code></dt>
<dd>
<p>Defines the maximum amount of time (in seconds) that a response is considered fresh</p>
</dd>
<dt><code>no-cache</code></dt>
<dd>
<p>Forces revalidation so that the clients check for constant updates with the server</p>
</dd>
<dt><code>no-store</code></dt>
<dd>
<p>Prevents caching entirely</p>
</dd>
<dt><code>private</code></dt>
<dd>
<p>Stores responses in a private cache (e.g., local caches in browsers)</p>
</dd>
</dl>

<p>A response could have cache control headers like <code>Cache-Control: max-age=180, private</code> to set these directives.<sup><a data-type="noteref" href="ch10.html#id1072" id="id1072-marker">2</a></sup></p>

<p>Since keyword caching works on exact matches, it’s more suitable for functions and APIs that expect frequently repeated matching inputs.
However, in GenAI services that accept variable user queries, you may want to consider other caching mechanisms that rely on the meaning of inputs when returning a cached response. This is where semantic caching can prove useful.<a data-startref="ix_ch10-asciidoc8" data-type="indexterm" id="id1073"/><a data-startref="ix_ch10-asciidoc7" data-type="indexterm" id="id1074"/><a data-startref="ix_ch10-asciidoc6" data-type="indexterm" id="id1075"/></p>
</div></section>










<section data-pdf-bookmark="Semantic caching" data-type="sect3"><div class="sect3" id="id148">
<h3>Semantic caching</h3>

<p><a data-primary="caching" data-secondary="semantic caching" data-type="indexterm" id="ix_ch10-asciidoc9"/><a data-primary="optimizing AI services" data-secondary="caching" data-tertiary="semantic caching" data-type="indexterm" id="ix_ch10-asciidoc10"/><a data-primary="semantic caching" data-type="indexterm" id="ix_ch10-asciidoc11"/><em>Semantic caching</em> is a caching mechanism that returns a stored value based on similar inputs.</p>

<p>Under the hood, the system uses encoders and embedding vectors to capture semantics and meanings of inputs.
It then performs similarity searches across stored key-value pairs to return a cached response.</p>

<p>In comparison to keyword caching, similar inputs can return the same cached response.
Inputs to the system don’t have to be identical to be recognized as similar.
Even if such inputs have different sentence structures or formulations or contain inaccuracies, they’ll still be captured as similar for carrying the same meanings.
And, the same response is being requested.
As an example, the following queries are considered similar for carrying the same intent:</p>

<ul>
<li>
<p>How do you build generative services with FastAPI?</p>
</li>
<li>
<p>What is the process of developing FastAPI services for GenAI?</p>
</li>
</ul>

<p>This caching system contributes to significant cost savings<sup><a data-type="noteref" href="ch10.html#id1076" id="id1076-marker">3</a></sup> by reducing API calls to <a href="https://oreil.ly/gjGz6">30–40%</a> (i.e., 60–70% cache hit rate) depending on the use case and size of the user base.
For instance, Q&amp;A RAG applications that receive frequently asked questions across a large user base could reduce API calls by 69% using a semantic cache.</p>

<p>Within a typical RAG system, there are two places where having a cache can reduce resource-intensive and time-consuming operations:</p>

<ul>
<li>
<p><em>Before the LLM</em> to return a cached response instead of generating a new one</p>
</li>
<li>
<p><em>Before the vector store</em> to enrich prompts with cached documents instead of searching and retrieving fresh ones</p>
</li>
</ul>

<p>When integrating a semantic cache component into your RAG system, you should consider whether returning a cached response could negatively impact your application’s user experience.
For instance, if caching the LLM responses, both of the following queries would return the same response due to their high semantic similarity, causing the semantic caching system to treat them as nearly identical:</p>

<ul>
<li>
<p>Summarize this text in 100 words</p>
</li>
<li>
<p>Summarize this text in 50 words</p>
</li>
</ul>

<p>This makes it feel like your services aren’t responding to queries. As you may still want varied LLM outputs in your application, we’re going to implement a document retrieval semantic cache for your RAG system.
<a data-type="xref" href="#semantic_cache">Figure 10-1</a> shows the full system architecture.</p>

<figure><div class="figure" id="semantic_cache">
<img alt="bgai 1001" src="assets/bgai_1001.png"/>
<h6><span class="label">Figure 10-1. </span>Semantic caching in RAG system architecture</h6>
</div></figure>

<p>Let’s start by implementing the semantic caching system from scratch first, and then we’ll review how to offload the functionality to an external library such as
<code>gptcache</code>.</p>












<section data-pdf-bookmark="Building a semantic caching service from scratch" data-type="sect4"><div class="sect4" id="id149">
<h4>Building a semantic caching service from scratch</h4>

<p><a data-primary="caching" data-secondary="semantic caching" data-tertiary="building a semantic caching service from scratch" data-type="indexterm" id="ix_ch10-asciidoc12"/><a data-primary="semantic caching" data-secondary="building a semantic caching service from scratch" data-type="indexterm" id="ix_ch10-asciidoc13"/>You can implement a semantic caching system by implementing the following components:</p>

<ul>
<li>
<p>A cache store client</p>
</li>
<li>
<p>A document vector store client</p>
</li>
<li>
<p>An embedding model</p>
</li>
</ul>

<p><a data-type="xref" href="#semantic_cache_cache_store">Example 10-6</a> shows how to implement the cache store client.</p>
<div data-type="example" id="semantic_cache_cache_store">
<h5><span class="label">Example 10-6. </span>Cache store client</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">uuid</code>
<code class="kn">from</code> <code class="nn">qdrant_client</code> <code class="kn">import</code> <code class="n">AsyncQdrantClient</code><code class="p">,</code> <code class="n">models</code>
<code class="kn">from</code> <code class="nn">qdrant_client</code><code class="nn">.</code><code class="nn">http</code><code class="nn">.</code><code class="nn">models</code> <code class="kn">import</code> <code class="n">Distance</code><code class="p">,</code> <code class="n">PointStruct</code><code class="p">,</code> <code class="n">ScoredPoint</code>

<code class="k">class</code> <code class="nc">CacheClient</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">db</code> <code class="o">=</code> <code class="n">AsyncQdrantClient</code><code class="p">(</code><code class="s2">"</code><code class="s2">:memory:</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_optimizing_ai_services_CO4-1" id="co_optimizing_ai_services_CO4-1"><img alt="1" src="assets/1.png"/></a>
        <code class="bp">self</code><code class="o">.</code><code class="n">cache_collection_name</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">cache</code><code class="s2">"</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">initialize_database</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="kc">None</code><code class="p">:</code>
        <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db</code><code class="o">.</code><code class="n">create_collection</code><code class="p">(</code>
            <code class="n">collection_name</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">cache_collection_name</code><code class="p">,</code>
            <code class="n">vectors_config</code><code class="o">=</code><code class="n">models</code><code class="o">.</code><code class="n">VectorParams</code><code class="p">(</code>
                <code class="n">size</code><code class="o">=</code><code class="mi">384</code><code class="p">,</code> <code class="n">distance</code><code class="o">=</code><code class="n">Distance</code><code class="o">.</code><code class="n">EUCLID</code>
            <code class="p">)</code><code class="p">,</code>
        <code class="p">)</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">insert</code><code class="p">(</code>
        <code class="bp">self</code><code class="p">,</code> <code class="n">query_vector</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">float</code><code class="p">]</code><code class="p">,</code> <code class="n">documents</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code>
    <code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="kc">None</code><code class="p">:</code>
        <code class="n">point</code> <code class="o">=</code> <code class="n">PointStruct</code><code class="p">(</code>
            <code class="nb">id</code><code class="o">=</code><code class="nb">str</code><code class="p">(</code><code class="n">uuid</code><code class="o">.</code><code class="n">uuid4</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="p">,</code>
            <code class="n">vector</code><code class="o">=</code><code class="n">query_vector</code><code class="p">,</code>
            <code class="n">payload</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">documents</code><code class="s2">"</code><code class="p">:</code> <code class="n">documents</code><code class="p">}</code><code class="p">,</code>
        <code class="p">)</code>
        <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db</code><code class="o">.</code><code class="n">upload_points</code><code class="p">(</code>
            <code class="n">collection_name</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">cache_collection_name</code><code class="p">,</code> <code class="n">points</code><code class="o">=</code><code class="p">[</code><code class="n">point</code><code class="p">]</code>
        <code class="p">)</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">search</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">query_vector</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">float</code><code class="p">]</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">list</code><code class="p">[</code><code class="n">ScoredPoint</code><code class="p">]</code><code class="p">:</code>
        <code class="k">return</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db</code><code class="o">.</code><code class="n">search</code><code class="p">(</code>
            <code class="n">collection_name</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">cache_collection_name</code><code class="p">,</code>
            <code class="n">query_vector</code><code class="o">=</code><code class="n">query_vector</code><code class="p">,</code>
            <code class="n">limit</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
        <code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO4-1" id="callout_optimizing_ai_services_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Initialize a Qdrant client running on memory acting as a cache store.</p></dd>
</dl></div>

<p>Once the cache store client is initialized, you can configure the document vector store by following <a data-type="xref" href="#semantic_cache_doc_store">Example 10-7</a>.</p>
<div data-type="example" id="semantic_cache_doc_store">
<h5><span class="label">Example 10-7. </span>Document store client</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">qdrant_client</code> <code class="kn">import</code> <code class="n">AsyncQdrantClient</code><code class="p">,</code> <code class="n">models</code>
<code class="kn">from</code> <code class="nn">qdrant_client</code><code class="nn">.</code><code class="nn">http</code><code class="nn">.</code><code class="nn">models</code> <code class="kn">import</code> <code class="n">Distance</code><code class="p">,</code> <code class="n">ScoredPoint</code>

<code class="n">documents</code> <code class="o">=</code> <code class="p">[</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">]</code> <a class="co" href="#callout_optimizing_ai_services_CO5-1" id="co_optimizing_ai_services_CO5-1"><img alt="1" src="assets/1.png"/></a>

<code class="k">class</code> <code class="nc">DocumentStoreClient</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">host</code><code class="o">=</code><code class="s2">"</code><code class="s2">localhost</code><code class="s2">"</code><code class="p">,</code> <code class="n">port</code><code class="o">=</code><code class="mi">6333</code><code class="p">)</code><code class="p">:</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code> <code class="o">=</code> <code class="n">AsyncQdrantClient</code><code class="p">(</code><code class="n">host</code><code class="o">=</code><code class="n">host</code><code class="p">,</code> <code class="n">port</code><code class="o">=</code><code class="n">port</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">collection_name</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">docs</code><code class="s2">"</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">initialize_database</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="kc">None</code><code class="p">:</code>
        <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code><code class="o">.</code><code class="n">create_collection</code><code class="p">(</code>
            <code class="n">collection_name</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">collection_name</code><code class="p">,</code>
            <code class="n">vectors_config</code><code class="o">=</code><code class="n">models</code><code class="o">.</code><code class="n">VectorParams</code><code class="p">(</code>
                <code class="n">size</code><code class="o">=</code><code class="mi">384</code><code class="p">,</code> <code class="n">distance</code><code class="o">=</code><code class="n">Distance</code><code class="o">.</code><code class="n">EUCLID</code>
            <code class="p">)</code><code class="p">,</code>
        <code class="p">)</code>
        <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code><code class="o">.</code><code class="n">add</code><code class="p">(</code>
            <code class="n">documents</code><code class="o">=</code><code class="n">documents</code><code class="p">,</code> <code class="n">collection_name</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">collection_name</code>
        <code class="p">)</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">search</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">query_vector</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">float</code><code class="p">]</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">list</code><code class="p">[</code><code class="n">ScoredPoint</code><code class="p">]</code><code class="p">:</code>
        <code class="n">results</code> <code class="o">=</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code><code class="o">.</code><code class="n">search</code><code class="p">(</code>
            <code class="n">query_vector</code><code class="o">=</code><code class="n">query_vector</code><code class="p">,</code>
            <code class="n">limit</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
            <code class="n">collection_name</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">collection_name</code><code class="p">,</code>
        <code class="p">)</code>
        <code class="k">return</code> <code class="n">results</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO5-1" id="callout_optimizing_ai_services_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Load a collection of documents into the Qdrant vector store.</p></dd>
</dl></div>

<p>With both the cache and document vector store clients ready, you can now implement the semantic cache service, as shown in <a data-type="xref" href="#semantic_cache_service">Example 10-8</a>, with methods to compute embeddings and performing cache searches.</p>
<div data-type="example" id="semantic_cache_service">
<h5><span class="label">Example 10-8. </span>Semantic caching system</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">time</code>
<code class="kn">from</code> <code class="nn">loguru</code> <code class="kn">import</code> <code class="n">logger</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoModel</code>

<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="k">class</code> <code class="nc">SemanticCacheService</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">threshold</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.35</code><code class="p">)</code><code class="p">:</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">embedder</code> <code class="o">=</code> <code class="n">AutoModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
            <code class="s2">"</code><code class="s2">jinaai/jina-embeddings-v2-base-en</code><code class="s2">"</code><code class="p">,</code> <code class="n">trust_remote_code</code><code class="o">=</code><code class="kc">True</code>
        <code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">euclidean_threshold</code> <code class="o">=</code> <code class="n">threshold</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">cache_client</code> <code class="o">=</code> <code class="n">CacheClient</code><code class="p">(</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">doc_db_client</code> <code class="o">=</code> <code class="n">DocumentStoreClient</code><code class="p">(</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">get_embedding</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">question</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">list</code><code class="p">[</code><code class="nb">float</code><code class="p">]</code><code class="p">:</code>
        <code class="k">return</code> <code class="nb">list</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">embedder</code><code class="o">.</code><code class="n">embed</code><code class="p">(</code><code class="n">question</code><code class="p">)</code><code class="p">)</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">initialize_databases</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code>
        <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">cache_client</code><code class="o">.</code><code class="n">initialize_databases</code><code class="p">(</code><code class="p">)</code>
        <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">doc_db_client</code><code class="o">.</code><code class="n">initialize_databases</code><code class="p">(</code><code class="p">)</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">ask</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">query</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code>
        <code class="n">start_time</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">(</code><code class="p">)</code>
        <code class="n">vector</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">get_embedding</code><code class="p">(</code><code class="n">query</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">search_results</code> <code class="o">:=</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">cache_client</code><code class="o">.</code><code class="n">search</code><code class="p">(</code><code class="n">vector</code><code class="p">)</code><code class="p">:</code>
            <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> <code class="n">search_results</code><code class="p">:</code>
                <code class="k">if</code> <code class="n">s</code><code class="o">.</code><code class="n">score</code> <code class="o">&lt;</code><code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">euclidean_threshold</code><code class="p">:</code> <a class="co" href="#callout_optimizing_ai_services_CO6-1" id="co_optimizing_ai_services_CO6-1"><img alt="1" src="assets/1.png"/></a>
                    <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Found cache with score </code><code class="si">{</code><code class="n">s</code><code class="o">.</code><code class="n">score</code><code class="si">:</code><code class="s2">.3f</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
                    <code class="n">elapsed_time</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">(</code><code class="p">)</code> <code class="o">-</code> <code class="n">start_time</code>
                    <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Time taken: </code><code class="si">{</code><code class="n">elapsed_time</code><code class="si">:</code><code class="s2">.3f</code><code class="si">}</code><code class="s2"> seconds</code><code class="s2">"</code><code class="p">)</code>
                    <code class="k">return</code> <code class="n">s</code><code class="o">.</code><code class="n">payload</code><code class="p">[</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">]</code>

        <code class="k">if</code> <code class="n">db_results</code> <code class="o">:=</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">doc_db_client</code><code class="o">.</code><code class="n">search</code><code class="p">(</code><code class="n">vector</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_optimizing_ai_services_CO6-2" id="co_optimizing_ai_services_CO6-2"><img alt="2" src="assets/2.png"/></a>
            <code class="n">documents</code> <code class="o">=</code> <code class="p">[</code><code class="n">r</code><code class="o">.</code><code class="n">payload</code><code class="p">[</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">]</code> <code class="k">for</code> <code class="n">r</code> <code class="ow">in</code> <code class="n">db_results</code><code class="p">]</code>
            <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">cache_client</code><code class="o">.</code><code class="n">insert</code><code class="p">(</code><code class="n">vector</code><code class="p">,</code> <code class="n">documents</code><code class="p">)</code>
            <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="s2">"</code><code class="s2">Query context inserted to Cache.</code><code class="s2">"</code><code class="p">)</code>
            <code class="n">elapsed_time</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">(</code><code class="p">)</code> <code class="o">-</code> <code class="n">start_time</code>
            <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Time taken: </code><code class="si">{</code><code class="n">elapsed_time</code><code class="si">:</code><code class="s2">.3f</code><code class="si">}</code><code class="s2"> seconds</code><code class="s2">"</code><code class="p">)</code>

        <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="s2">"</code><code class="s2">No answer found in Cache or Database.</code><code class="s2">"</code><code class="p">)</code>
        <code class="n">elapsed_time</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">(</code><code class="p">)</code> <code class="o">-</code> <code class="n">start_time</code>
        <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Time taken: </code><code class="si">{</code><code class="n">elapsed_time</code><code class="si">:</code><code class="s2">.3f</code><code class="si">}</code><code class="s2"> seconds</code><code class="s2">"</code><code class="p">)</code>
        <code class="k">return</code> <code class="s2">"</code><code class="s2">No answer available.</code><code class="s2">"</code> <a class="co" href="#callout_optimizing_ai_services_CO6-3" id="co_optimizing_ai_services_CO6-3"><img alt="3" src="assets/3.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO6-1" id="callout_optimizing_ai_services_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Set a similarity threshold.
Any score above this threshold will be a cache hit.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO6-2" id="callout_optimizing_ai_services_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Query the document store if there is no cache hit.
Cache the retrieved documents against the vector embedding of the query as the cache key.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO6-3" id="callout_optimizing_ai_services_CO6-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>If there is no related document or cache available for the given query, return a canned answer.</p></dd>
</dl></div>

<p>Now that you have a semantic caching service, you can use it to retrieve cached documents from memory by following <a data-type="xref" href="#semantic_cache_qdrant_usage">Example 10-9</a>.</p>
<div data-type="example" id="semantic_cache_qdrant_usage">
<h5><span class="label">Example 10-9. </span>Implementing a semantic cache in a RAG system with Qdrant</h5>

<pre data-code-language="python" data-type="programlisting"><code class="k">async</code> <code class="k">def</code> <code class="nf">main</code><code class="p">():</code>
    <code class="n">cache_service</code> <code class="o">=</code> <code class="n">SemanticCacheService</code><code class="p">()</code>
    <code class="n">query_1</code> <code class="o">=</code> <code class="s2">"How to build GenAI services?"</code>
    <code class="n">query_2</code> <code class="o">=</code> <code class="s2">"What is the process for developing GenAI services?"</code>

    <code class="n">cache_service</code><code class="o">.</code><code class="n">ask</code><code class="p">(</code><code class="n">query_1</code><code class="p">)</code>
    <code class="n">cache_service</code><code class="o">.</code><code class="n">ask</code><code class="p">(</code><code class="n">query_2</code><code class="p">)</code>

<code class="n">asyncio</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">main</code><code class="p">())</code>

<code class="c1"># Query 1:</code>
<code class="c1"># Query added to Cache.</code>
<code class="c1"># Time taken: 0.822 seconds</code>

<code class="c1"># Query 2:</code>
<code class="c1"># Found cache with score 0.329</code>
<code class="c1"># Time taken: 0.016 seconds</code></pre></div>

<p>You should now have a better understanding of how to implement your own custom semantic caching systems using a vector database client.<a data-startref="ix_ch10-asciidoc13" data-type="indexterm" id="id1077"/><a data-startref="ix_ch10-asciidoc12" data-type="indexterm" id="id1078"/></p>
</div></section>












<section data-pdf-bookmark="Semantic caching with GPT cache" data-type="sect4"><div class="sect4" id="id150">
<h4>Semantic caching with GPT cache</h4>

<p><a data-primary="caching" data-secondary="semantic caching" data-tertiary="GPT cache" data-type="indexterm" id="ix_ch10-asciidoc14"/><a data-primary="GPT cache" data-type="indexterm" id="ix_ch10-asciidoc15"/><a data-primary="semantic caching" data-secondary="GPT cache" data-type="indexterm" id="ix_ch10-asciidoc16"/>If you don’t need to develop your own semantic caching service from scratch, you can also use the modular <code>gptcache</code> library that gives you the option to swap various storage, caching, and embedding components.</p>

<p>To configure a semantic cache with <code>gptcache</code>, you first need to install the package:</p>

<pre data-type="programlisting">$ pip install gptcache</pre>

<p class="less_space pagebreak-before">Then load the system on application start, as shown in <a data-type="xref" href="#configrue_gptcache">Example 10-10</a>.</p>
<div data-type="example" id="configrue_gptcache">
<h5><span class="label">Example 10-10. </span>Configuring the GPT cache</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">contextlib</code> <code class="kn">import</code> <code class="n">asynccontextmanager</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">AsyncIterator</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code>
<code class="kn">from</code> <code class="nn">gptcache</code> <code class="kn">import</code> <code class="n">Config</code><code class="p">,</code> <code class="n">cache</code>
<code class="kn">from</code> <code class="nn">gptcache</code><code class="nn">.</code><code class="nn">embedding</code> <code class="kn">import</code> <code class="n">Onnx</code>
<code class="kn">from</code> <code class="nn">gptcache</code><code class="nn">.</code><code class="nn">processor</code><code class="nn">.</code><code class="nn">post</code> <code class="kn">import</code> <code class="n">random_one</code>
<code class="kn">from</code> <code class="nn">gptcache</code><code class="nn">.</code><code class="nn">processor</code><code class="nn">.</code><code class="nn">pre</code> <code class="kn">import</code> <code class="n">last_content</code>
<code class="kn">from</code> <code class="nn">gptcache</code><code class="nn">.</code><code class="nn">similarity_evaluation</code> <code class="kn">import</code> <code class="n">OnnxModelEvaluation</code>

<code class="nd">@asynccontextmanager</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">lifespan</code><code class="p">(</code><code class="n">_</code><code class="p">:</code> <code class="n">FastAPI</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">AsyncIterator</code><code class="p">[</code><code class="kc">None</code><code class="p">]</code><code class="p">:</code>
    <code class="n">cache</code><code class="o">.</code><code class="n">init</code><code class="p">(</code>
        <code class="n">post_func</code><code class="o">=</code><code class="n">random_one</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO7-1" id="co_optimizing_ai_services_CO7-1"><img alt="1" src="assets/1.png"/></a>
        <code class="n">pre_embedding_func</code><code class="o">=</code><code class="n">last_content</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO7-2" id="co_optimizing_ai_services_CO7-2"><img alt="2" src="assets/2.png"/></a>
        <code class="n">embedding_func</code><code class="o">=</code><code class="n">Onnx</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">to_embeddings</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO7-3" id="co_optimizing_ai_services_CO7-3"><img alt="3" src="assets/3.png"/></a>
        <code class="n">similarity_evaluation</code><code class="o">=</code><code class="n">OnnxModelEvaluation</code><code class="p">(</code><code class="p">)</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO7-4" id="co_optimizing_ai_services_CO7-4"><img alt="4" src="assets/4.png"/></a>
        <code class="n">config</code><code class="o">=</code><code class="n">Config</code><code class="p">(</code><code class="n">similarity_threshold</code><code class="o">=</code><code class="mf">0.75</code><code class="p">)</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO7-5" id="co_optimizing_ai_services_CO7-5"><img alt="5" src="assets/5.png"/></a>
    <code class="p">)</code>
    <code class="n">cache</code><code class="o">.</code><code class="n">set_openai_key</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_optimizing_ai_services_CO7-6" id="co_optimizing_ai_services_CO7-6"><img alt="6" src="assets/6.png"/></a>
    <code class="k">yield</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="n">lifespan</code><code class="o">=</code><code class="n">lifespan</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO7-1" id="callout_optimizing_ai_services_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Select a post-processing callback function to select a random item from the returned cached items.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO7-2" id="callout_optimizing_ai_services_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Select a pre-embedding callback function to use the last query for setting a new cache.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO7-3" id="callout_optimizing_ai_services_CO7-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Use the ONNX embedding model for computing embedding vectors.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO7-4" id="callout_optimizing_ai_services_CO7-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Use <code>OnnxModelEvaluation</code> to compute similarity scores between cached items and a given query.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO7-5" id="callout_optimizing_ai_services_CO7-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Set the caching configuration options such as a similarity threshold.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO7-6" id="callout_optimizing_ai_services_CO7-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>Provide an OpenAI client API key for GPT Cache to automatically perform semantic caching on LLM API responses.</p></dd>
</dl></div>

<p class="less_space pagebreak-before">Once <code>gptcache</code> is initialized, it will integrate seamlessly with the OpenAI LLM client across your application.
You can now make multiple LLM queries, as shown in <a data-type="xref" href="#semantic_caching_gptcache">Example 10-11</a>, knowing that <code>gptcache</code> will be caching your LLM responses.</p>
<div data-type="example" id="semantic_caching_gptcache">
<h5><span class="label">Example 10-11. </span>Semantic caching with the GPT cache</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">time</code>
<code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">OpenAI</code>
<code class="n">client</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">()</code>

<code class="n">question</code> <code class="o">=</code> <code class="s2">"what's FastAPI"</code>
<code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">):</code>
    <code class="n">start_time</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-4o"</code><code class="p">,</code>
        <code class="n">messages</code><code class="o">=</code><code class="p">[{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code> <code class="s2">"content"</code><code class="p">:</code> <code class="n">question</code><code class="p">}],</code>
    <code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Question: </code><code class="si">{</code><code class="n">question</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Time consuming: </code><code class="si">{:.2f}</code><code class="s2">s"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">start_time</code><code class="p">))</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Answer: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code><code class="si">}</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code></pre></div>

<p>Using external libraries like <code>gptcache</code>, as shown in <a data-type="xref" href="#semantic_caching_gptcache">Example 10-11</a>, makes implementing semantic caching straightforward.</p>

<p>Once the caching system is up and running, you can adjust <em>similarity thresholds</em> to tune the system’s cache hit rates.<a data-startref="ix_ch10-asciidoc16" data-type="indexterm" id="id1079"/><a data-startref="ix_ch10-asciidoc15" data-type="indexterm" id="id1080"/><a data-startref="ix_ch10-asciidoc14" data-type="indexterm" id="id1081"/></p>
</div></section>












<section data-pdf-bookmark="Similarity threshold" data-type="sect4"><div class="sect4" id="id151">
<h4>Similarity threshold</h4>

<p><a data-primary="caching" data-secondary="semantic caching" data-tertiary="similarity threshold adjustment" data-type="indexterm" id="ix_ch10-asciidoc17"/><a data-primary="semantic caching" data-secondary="similarity threshold adjustment" data-type="indexterm" id="ix_ch10-asciidoc18"/><a data-primary="similarity thresholds" data-type="indexterm" id="ix_ch10-asciidoc19"/>When building a semantic caching service, you may need to adjust the similarity threshold based on provided queries to achieve high cache hit rates that are accurate.
You can refer to the <a href="https://semanticcachehit.com">interactive visualization of semantic cache clusters</a> shown in <a data-type="xref" href="#semantic_cache_visualization">Figure 10-2</a> to better understand the concept of similarity 
<span class="keep-together">threshold.</span></p>

<p>Increasing the threshold value in <a data-type="xref" href="#semantic_cache_visualization">Figure 10-2</a> will result in a less connected graph, while minimizing can produce false positives.
Therefore, you may want to run a few experiments to fine-tune the similarity threshold for your own application.</p>

<figure><div class="figure" id="semantic_cache_visualization">
<img alt="bgai 1002" src="assets/bgai_1002.png"/>
<h6><span class="label">Figure 10-2. </span>Visualization of semantic caching (Source: <a class="orm:hideurl" href="https://semanticcachehit.com">semanticcachehit.com</a>)</h6>
</div></figure>
</div></section>












<section data-pdf-bookmark="Eviction policies" data-type="sect4"><div class="sect4" id="id152">
<h4>Eviction policies</h4>

<p><a data-primary="caching" data-secondary="semantic caching" data-tertiary="eviction policies" data-type="indexterm" id="id1082"/><a data-primary="eviction policies" data-type="indexterm" id="id1083"/><a data-primary="semantic caching" data-secondary="eviction policies" data-type="indexterm" id="id1084"/>Another concept relevant to caching is <em>eviction policies</em> that control the caching behavior when the caching mechanism reaches its maximum capacity.
Selecting the appropriate eviction policy should be appropriate for your own use case.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Since the size of cache memory stores is often limited, you can add an <code>evict()</code> method to the <code>SemanticCachingService</code> you implemented in <a data-type="xref" href="#semantic_cache_service">Example 10-8</a>.</p>
</div>

<p class="less_space pagebreak-before"><a data-type="xref" href="#eviction_policies">Table 10-1</a> shows a few eviction policies you can choose from.</p>
<table class="striped" id="eviction_policies">
<caption><span class="label">Table 10-1. </span>Eviction policies</caption>
<thead>
<tr>
<th>Policy</th>
<th>Description</th>
<th>Use case</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>First in, first out (FIFO)</p></td>
<td><p>Removes oldest items</p></td>
<td><p>When all items have the same priority</p></td>
</tr>
<tr>
<td><p>Least recently used (LRU)</p></td>
<td><p>Tracks cache usage across time and removes the least recently accessed item</p></td>
<td><p>When recently accessed items are more likely to be accessed again</p></td>
</tr>
<tr>
<td><p>Least frequently used (LFU)</p></td>
<td><p>Tracks cache usage across time and removes the least frequently accessed item</p></td>
<td><p>When less frequently used items should be removed first</p></td>
</tr>
<tr>
<td><p>Most recently used (MRU)</p></td>
<td><p>Tracks cache usage across time and removes the most recently accessed item</p></td>
<td><p>Rarely used, used when most recently used items are less likely to be accessed again</p></td>
</tr>
<tr>
<td><p>Random replacement (RR)</p></td>
<td><p>Removes a random item from the cache</p></td>
<td><p>Simple and fast, used when it doesn’t impact performance</p></td>
</tr>
</tbody>
</table>

<p>Choosing the right eviction policy will depend on your use case and application requirements.
Generally, you can start with the LRU policy before switching to 
<span class="keep-together">alternatives.</span></p>

<p>You should now feel more confident in implementing semantic caching mechanisms that apply to document retrieval or model responses.
Next, let’s learn about context or prompt caching, which optimizes queries to models based on their inputs.<a data-startref="ix_ch10-asciidoc11" data-type="indexterm" id="id1085"/><a data-startref="ix_ch10-asciidoc10" data-type="indexterm" id="id1086"/><a data-startref="ix_ch10-asciidoc9" data-type="indexterm" id="id1087"/></p>
</div></section>
</div></section>










<section data-pdf-bookmark="Context/prompt caching" data-type="sect3"><div class="sect3" id="id153">
<h3>Context/prompt caching</h3>

<p><a data-primary="caching" data-secondary="context/prompt caching" data-type="indexterm" id="ix_ch10-asciidoc20"/><a data-primary="context (prompt) caching" data-type="indexterm" id="ix_ch10-asciidoc21"/><a data-primary="optimizing AI services" data-secondary="caching" data-tertiary="context/prompt caching" data-type="indexterm" id="ix_ch10-asciidoc22"/><a data-primary="prompt (context) caching" data-type="indexterm" id="ix_ch10-asciidoc23"/><em>Context caching</em>, also known as <em>prompt caching</em>, is a caching mechanism suitable for scenarios where you’re referencing large amounts of context repeatedly within small requests.
It’s designed to reuse precomputed attention states from frequently reused prompts, eliminating the need for redundant recomputation of the entire input context each time a new request is made.</p>

<p>You should consider using a context cache when your services involve the following:</p>

<ul>
<li>
<p>Chatbots with extensive system instructions and long multiturn conversations</p>
</li>
<li>
<p>Repetitive analysis of lengthy video files</p>
</li>
<li>
<p>Recurring queries against large document sets</p>
</li>
<li>
<p>Frequent code repository analysis or bug fixing</p>
</li>
<li>
<p>Document summarizations, talking to books, papers, documentation, podcast transcripts and other long form content</p>
</li>
<li>
<p>Providing a large number of examples in prompt (i.e., in-context learning)</p>
</li>
</ul>

<p>This type of caching can help you to substantially reduce token usage costs by caching large context tokens.
According to Anthropic, prompt caching can reduce costs by up to 90% and latency by up to 85% for long prompts.</p>

<p>The authors of the <a href="https://oreil.ly/augpd">prompt caching paper</a> that presents this technique also claim that:</p>
<blockquote>
<p>We find that Prompt Cache significantly reduces latency in time-to-first-token, especially for longer prompts such as document-based question answering and recommendations.
The improvements range from 8× for GPU-based inference to 60× for CPU-based inference, all while maintaining output accuracy and without the need for model parameter modifications.</p></blockquote>

<p><a data-type="xref" href="#context_caching_architecture">Figure 10-3</a> visualizes the context caching system architecture.</p>

<figure><div class="figure" id="context_caching_architecture">
<img alt="bgai 1003" src="assets/bgai_1003.png"/>
<h6><span class="label">Figure 10-3. </span>System architecture for context caching</h6>
</div></figure>

<p>At the time of writing, OpenAI automatically implements prompt caching for all API requests without requiring any code changes or additional costs.
<a data-type="xref" href="#context_cachin_anthropic">Example 10-12</a> shows an example of how to use prompt caching when using the Anthropic API.</p>
<div data-type="example" id="context_cachin_anthropic">
<h5><span class="label">Example 10-12. </span>Context/prompt caching with Anthropic API</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">anthropic</code> <code class="kn">import</code> <code class="n">Anthropic</code>

<code class="n">client</code> <code class="o">=</code> <code class="n">Anthropic</code><code class="p">(</code><code class="p">)</code>

<code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">messages</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="s2">"</code><code class="s2">claude-3-7-sonnet-20250219</code><code class="s2">"</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO8-1" id="co_optimizing_ai_services_CO8-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">max_tokens</code><code class="o">=</code><code class="mi">1024</code><code class="p">,</code>
    <code class="n">system</code><code class="o">=</code><code class="p">[</code>
        <code class="p">{</code>
            <code class="s2">"</code><code class="s2">type</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">text</code><code class="s2">"</code><code class="p">,</code>
            <code class="s2">"</code><code class="s2">text</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">You are an AI assistant</code><code class="s2">"</code><code class="p">,</code>
        <code class="p">}</code><code class="p">,</code>
        <code class="p">{</code>
            <code class="s2">"</code><code class="s2">type</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">text</code><code class="s2">"</code><code class="p">,</code>
            <code class="s2">"</code><code class="s2">text</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">&lt;the entire content of a large document&gt;</code><code class="s2">"</code><code class="p">,</code>
            <code class="s2">"</code><code class="s2">cache_control</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">type</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">ephemeral</code><code class="s2">"</code><code class="p">}</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO8-2" id="co_optimizing_ai_services_CO8-2"><img alt="2" src="assets/2.png"/></a>
        <code class="p">}</code><code class="p">,</code>
    <code class="p">]</code><code class="p">,</code>
    <code class="n">messages</code><code class="o">=</code><code class="p">[</code><code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">Summarize the documents in ...</code><code class="s2">"</code><code class="p">}</code><code class="p">]</code><code class="p">,</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">response</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO8-1" id="callout_optimizing_ai_services_CO8-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Prompt caching is available only with a handful of models including Claude 3.5 Sonnet, Claude 3 Haiku, and Claude 3 Opus.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO8-2" id="callout_optimizing_ai_services_CO8-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Use the <code>cache_control</code> parameter to reuse the large document content across multiple API calls without processing it each time.</p>

<p>Under the hood, the Anthropic client will add <code>anthropic-beta: prompt-caching-2024-07-31</code> to the request headers.</p>

<p>At the time of writing, <code>ephemeral</code> is the only supported cache type, which corresponds to a 5-minute cache lifetime.</p></dd>
</dl></div>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>As soon as you adopt a context cache, you’re introducing statefulness in requests by preserving tokens across them.
This means the data you submit in one request will affect later requests, as the model provider server can use the cached context to maintain continuity between interactions.</p>
</div>

<p>With the Gemini API’s context caching feature, you can provide content to the model once, cache the input tokens, and reference these cached tokens for future requests.</p>

<p>Using these cached tokens can save you significant expenses if you avoid repeatedly passing in the same corpus of tokens in high volumes.
<a data-primary="time to live (TTL)" data-type="indexterm" id="id1088"/><a data-primary="TTL (time to live)" data-type="indexterm" id="id1089"/>The caching cost will depend on the size of the input tokens and the desired time to live (TTL) storage duration.</p>
<div data-type="tip"><h6>Tip</h6>
<p>When you cache a set of tokens, you can specify a TTL duration, which is how long the cache should exist before the tokens are automatically deleted.
By default, TTL is normally set to 1 hour.</p>
</div>

<p>You can see how to use a cached system instruction in <a data-type="xref" href="#context_caching_google">Example 10-13</a>.
You will also need the Gemini API Python SDK:</p>

<pre data-type="programlisting">$ pip install google-generativeai</pre>
<div data-type="example" id="context_caching_google">
<h5><span class="label">Example 10-13. </span>Context caching with the Google Gemini API</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">datetime</code>
<code class="kn">import</code> <code class="nn">google</code><code class="nn">.</code><code class="nn">generativeai</code> <code class="k">as</code> <code class="nn">genai</code>
<code class="kn">from</code> <code class="nn">google</code><code class="nn">.</code><code class="nn">generativeai</code> <code class="kn">import</code> <code class="n">caching</code>

<code class="n">genai</code><code class="o">.</code><code class="n">configure</code><code class="p">(</code><code class="n">api_key</code><code class="o">=</code><code class="s2">"</code><code class="s2">your_gemini_api_key</code><code class="s2">"</code><code class="p">)</code>

<code class="n">corpus</code> <code class="o">=</code> <code class="n">genai</code><code class="o">.</code><code class="n">upload_file</code><code class="p">(</code><code class="n">path</code><code class="o">=</code><code class="s2">"</code><code class="s2">corpus.txt</code><code class="s2">"</code><code class="p">)</code>
<code class="n">cache</code> <code class="o">=</code> <code class="n">caching</code><code class="o">.</code><code class="n">CachedContent</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="s1">'</code><code class="s1">models/gemini-1.5-flash-001</code><code class="s1">'</code><code class="p">,</code>
    <code class="n">display_name</code><code class="o">=</code><code class="s1">'</code><code class="s1">fastapi</code><code class="s1">'</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO9-1" id="co_optimizing_ai_services_CO9-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">system_instruction</code><code class="o">=</code><code class="p">(</code>
        <code class="s2">"</code><code class="s2">You are an expert AI engineer, and your job is to answer </code><code class="s2">"</code>
        <code class="s2">"</code><code class="s2">the user</code><code class="s2">'</code><code class="s2">s query based on the files you have access to.</code><code class="s2">"</code>
    <code class="p">)</code><code class="p">,</code>
    <code class="n">contents</code><code class="o">=</code><code class="p">[</code><code class="n">corpus</code><code class="p">]</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO9-2" id="co_optimizing_ai_services_CO9-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">ttl</code><code class="o">=</code><code class="n">datetime</code><code class="o">.</code><code class="n">timedelta</code><code class="p">(</code><code class="n">minutes</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">genai</code><code class="o">.</code><code class="n">GenerativeModel</code><code class="o">.</code><code class="n">from_cached_content</code><code class="p">(</code><code class="n">cached_content</code><code class="o">=</code><code class="n">cache</code><code class="p">)</code>
<code class="n">response</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">generate_content</code><code class="p">(</code>
    <code class="p">[</code>
        <code class="p">(</code>
            <code class="s2">"</code><code class="s2">Introduce different characters in the movie by describing </code><code class="s2">"</code>
            <code class="s2">"</code><code class="s2">their personality, looks, and names. Also list the timestamps </code><code class="s2">"</code>
            <code class="s2">"</code><code class="s2">they were introduced for the first time.</code><code class="s2">"</code>
        <code class="p">)</code>
    <code class="p">]</code>
<code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO9-1" id="callout_optimizing_ai_services_CO9-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Provide a display name as a cache key or identifier.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO9-2" id="callout_optimizing_ai_services_CO9-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Pass the corpus to the context caching system.
The minimum size of a context cache is 32,768 tokens.</p></dd>
</dl></div>

<p>If you run <a data-type="xref" href="#context_caching_google">Example 10-13</a> and print the <code>response.usage_metadata</code>, you should receive the following output:</p>

<pre data-type="programlisting">&gt;&gt; print(response.usage_metadata)

prompt_token_count: 696219
cached_content_token_count: 696190
candidates_token_count: 214
total_token_count: 696433</pre>

<p>Notice how much of the <code>prompt_token_count</code> is now being cached when you compare it with the <code>cached_content_token_count</code>.
The <code>candidates_token_count</code> refers to count of output or response tokens coming from the model, which isn’t affected by the caching system.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Gemini models don’t make any distinction between cached tokens and regular input tokens.
Cached content will be prefixed to the prompt.
This is why the prompt token count isn’t reduced when using caching.</p>
</div>

<p class="less_space pagebreak-before">With context caching, you won’t see a drastic reduction in response times but instead will significantly reduce operational costs as you avoid resending extensive system prompts and context tokens.
Therefore, this caching strategy is most suitable when you have a large context to work with—for instance, when batch processing files with extensive instructions and examples.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Using the same context cache and prompt doesn’t guarantee consistent model responses because the responses from LLMs are nondeterministic.
A context cache doesn’t cache any output.</p>
</div>

<p>Context caching remains an active area of research.
If you want to avoid any vendor lock-in, there is already some progress in this field with open source tools such as <a href="https://oreil.ly/PXm6B"><em>MemServe</em></a>, which implements context caching with an elastic memory pool<a data-startref="ix_ch10-asciidoc23" data-type="indexterm" id="id1090"/><a data-startref="ix_ch10-asciidoc22" data-type="indexterm" id="id1091"/><a data-startref="ix_ch10-asciidoc21" data-type="indexterm" id="id1092"/><a data-startref="ix_ch10-asciidoc20" data-type="indexterm" id="id1093"/>.<a data-startref="ix_ch10-asciidoc5" data-type="indexterm" id="id1094"/><a data-startref="ix_ch10-asciidoc4" data-type="indexterm" id="id1095"/><a data-startref="ix_ch10-asciidoc3" data-type="indexterm" id="id1096"/></p>

<p>Beyond caching, you can also review your options for reducing model size to speed up response times using techniques such as <em>model quantization</em>.</p>
</div></section>
</div></section>








<section data-pdf-bookmark="Model Quantization" data-type="sect2"><div class="sect2" id="id154">
<h2>Model Quantization</h2>

<p><a data-primary="model quantization" data-type="indexterm" id="ix_ch10-asciidoc24"/><a data-primary="optimizing AI services" data-secondary="model quantization" data-type="indexterm" id="ix_ch10-asciidoc25"/>If you’re going to be serving models such as LLMs yourself, you should consider <em>quantizing</em> (i.e., compressing/shrinking) your models if possible.
Often, open source model repositories will also supply quantized versions that you can download and use straightaway without having to go through the quantization process yourself.</p>

<p><em>Model quantization</em> is the adjustment process on the model weights and activations where high-precision model parameters are statistically projected into lower-precision values through a fine-tuning operation using scaling factors on the original parameter distribution.
You can then perform all critical inference operations with lower precision, after which you can convert the outputs to higher precision to maintain the quality while improving performance.</p>

<p>Reducing the precision also decreases the memory storage requirements, theoretically lowering energy consumption and speeding up operations like matrix multiplication through integer arithmetic.
This also enables models to run on embedded devices, which may only support integer data types.</p>

<p><a data-type="xref" href="#quantization_process">Figure 10-4</a> shows the full quantization process.</p>

<figure><div class="figure" id="quantization_process">
<img alt="bgai 1004" src="assets/bgai_1004.png"/>
<h6><span class="label">Figure 10-4. </span>Quantization process</h6>
</div></figure>

<p>You can save more than a handful of gigabytes in GPU memory consumption as low-precision data types such as 8-bit integer would require significantly less RAM per parameter than a data type like 32-bit float.</p>










<section data-pdf-bookmark="Precision versus quality trade-off" data-type="sect3"><div class="sect3" id="id155">
<h3>Precision versus quality trade-off</h3>

<p><a data-primary="model quantization" data-secondary="precision versus quality trade-off" data-type="indexterm" id="ix_ch10-asciidoc26"/><a data-primary="optimizing AI services" data-secondary="model quantization" data-tertiary="precision versus quality trade-off" data-type="indexterm" id="ix_ch10-asciidoc27"/><a data-type="xref" href="#quantization">Figure 10-5</a> compares a nonquantized model and a quantized model.</p>

<figure><div class="figure" id="quantization">
<img alt="bgai 1005" src="assets/bgai_1005.png"/>
<h6><span class="label">Figure 10-5. </span>Quantization</h6>
</div></figure>

<p>As each high-precision 32-bit float parameter consumes 4 bytes of GPU memory, a 1B-parameter model would require 4 GB of memory just for inference.
If you plan on retraining or fine-tuning the same model, you’ll require at least 24 GB of GPU VRAM.
This is because each parameter would also require storing information like gradients, training optimizer states, activations, and temporary memory space, consuming an additional 24 bytes together.
This estimates up to 6 times the memory requirement compared to just loading the model weights.
The same 1B model would then require a 24 GB GPU, which the best and most expensive consumer graphics cards such as NVIDIA RTX 4090 may still struggle to meet.</p>

<p>Instead of using the standard 32-float, you can select any of the following formats:</p>

<ul>
<li>
<p><em>16-bit floating-point (FP16)</em> cuts memory usage in half without much of a hit to model output quality.</p>
</li>
<li>
<p><em>8-bit integer (INT8)</em> offers huge savings in memory but with a significant loss in quality.</p>
</li>
<li>
<p><em>16-bit brain floating-point (BFLOAT16)</em> with a similar range to FP32 balances the memory and quality trade-off.</p>
</li>
<li>
<p><em>4-bit integer (INT4)</em> provides a balance between memory efficiency and computational precision, making it suitable for low-power devices.</p>
</li>
<li>
<p><em>1-bit integer (INT1)</em> uses the lowest precision data type with maximum model size reduction.
Research for creating high-quality <a href="https://oreil.ly/QH9nH">1-bit LLMs</a> is currently under way.</p>
</li>
</ul>

<p>For comparison, <a data-type="xref" href="#quantization_comparison">Table 10-2</a> shows the reduction in model size when you quantize the Llama family models.</p>
<table id="quantization_comparison">
<caption><span class="label">Table 10-2. </span>Impact of quantization on the size of Llama models<sup><a data-type="noteref" href="ch10.html#id1097" id="id1097-marker">a</a></sup></caption>
<thead>
<tr>
<th>Model</th>
<th>Original</th>
<th>FP16</th>
<th>8 Bit</th>
<th>6 Bit</th>
<th>4 Bit</th>
<th>2Bit</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Llama 2 70B</p></td>
<td><p>140 GB</p></td>
<td><p>128.5 GB</p></td>
<td><p>73.23 GB</p></td>
<td><p>52.70 GB</p></td>
<td><p>36.20 GB</p></td>
<td><p>28.59 GB</p></td>
</tr>
<tr>
<td><p>Llama 3 8B</p></td>
<td><p>16.07 GB</p></td>
<td><p>14.97 GB</p></td>
<td><p>7.96 GB</p></td>
<td><p>4.34 GB</p></td>
<td><p>4.34 GB</p></td>
<td><p>2.96 GB</p></td>
</tr>
</tbody>
<tbody><tr class="footnotes"><td colspan="7"><p data-type="footnote" id="id1097"><sup><a href="ch10.html#id1097-marker">a</a></sup> Sources: <a href="https://oreil.ly/9iYtL">Llama.cpp GitHub repository</a> and <a href="https://oreil.ly/BMDtR">Tom Jobbins’s Hugging Face Llama 2 70B model card</a></p></td></tr></tbody></table>
<div data-type="tip"><h6>Tip</h6>
<p>In addition to the GPU VRAM needed to fit the model, you will also need an extra 5 to 8 GB of GPU VRAM for overhead during model loading.</p>
</div>

<p>As per the current state of research, maintaining accuracy with integer-only INT4 and INT1 data types is a challenge, and the performance improvement with INT32 or FP16 is not significant.
Therefore, the most popular lower-precision data type is INT8 for inference.</p>

<p>According to <a href="https://oreil.ly/C7Lz3">research</a>, using integer-only arithmetic for inference will be more efficient than floating-point numbers.
However, quantizing floating numbers to integers can be tricky.
For instance, only 256 values can be represented in INT8, while float32 can represent a wide range of values.<a data-startref="ix_ch10-asciidoc27" data-type="indexterm" id="id1098"/><a data-startref="ix_ch10-asciidoc26" data-type="indexterm" id="id1099"/></p>
</div></section>










<section data-pdf-bookmark="Floating-point numbers" data-type="sect3"><div class="sect3" id="id156">
<h3>Floating-point numbers</h3>

<p><a data-primary="model quantization" data-secondary="floating-point numbers" data-type="indexterm" id="ix_ch10-asciidoc28"/><a data-primary="optimizing AI services" data-secondary="model quantization" data-tertiary="floating-point numbers" data-type="indexterm" id="ix_ch10-asciidoc29"/>To understand why projecting 32-bit floats to other formats would save so much in GPU memory, let’s look at how it breaks down.</p>

<p>A 32-bit floating-point number consists of the following types of bits:</p>

<ul>
<li>
<p><em>Sign</em> bit describing whether a number is positive or negative</p>
</li>
<li>
<p><em>Exponent</em> bits controlling the scale of the number</p>
</li>
<li>
<p><em>Mantissa</em> bits holding the actual digits determining its precision (also known as <em>fraction</em> bits)</p>
</li>
</ul>

<p>You can see a visualization of bits in the aforementioned floating-point numbers in <a data-type="xref" href="#quantization_bits">Figure 10-6</a>.</p>

<figure><div class="figure" id="quantization_bits">
<img alt="bgai 1006" src="assets/bgai_1006.png"/>
<h6><span class="label">Figure 10-6. </span>Bits in 32-bit float, 16-bit float, and bfloat16 numbers</h6>
</div></figure>

<p>When you project the FP32 number into other formats, in effect, you’re squeezing it into smaller ranges, losing most of its mantissa bits and adjusting its exponent bits but without losing much of the precision.
You can see such a phenomenon in action by referring to <a data-type="xref" href="#quantization_floating_numbers">Figure 10-7</a>.</p>

<figure><div class="figure" id="quantization_floating_numbers">
<img alt="bgai 1007" src="assets/bgai_1007.png"/>
<h6><span class="label">Figure 10-7. </span>Quantization of floating-point numbers to integers</h6>
</div></figure>

<p>In fact, <a href="https://oreil.ly/Swfz7">research on the quantization strategies for pretrained LLM models</a> has shown that LLMs with 4-bit quantization can maintain performance similar to their nonquantized counterparts.
However, while quantization saves memory, it can also reduce the inference speed of LLMs.<a data-startref="ix_ch10-asciidoc29" data-type="indexterm" id="id1100"/><a data-startref="ix_ch10-asciidoc28" data-type="indexterm" id="id1101"/></p>
</div></section>










<section data-pdf-bookmark="How to quantize pretrained LLMs" data-type="sect3"><div class="sect3" id="id157">
<h3>How to quantize pretrained LLMs</h3>

<p><a data-primary="GPTQ technique" data-type="indexterm" id="ix_ch10-asciidoc30"/><a data-primary="large language models (LLMs)" data-secondary="quantizing pretrained LLMs" data-type="indexterm" id="ix_ch10-asciidoc31"/><a data-primary="model quantization" data-secondary="quantizing pretrained LLMs" data-type="indexterm" id="ix_ch10-asciidoc32"/><a data-primary="optimizing AI services" data-secondary="model quantization" data-tertiary="quantizing pretrained LLMs" data-type="indexterm" id="ix_ch10-asciidoc33"/>Quantization is the process of compressing large models by weight adjustment.
One such technique called <a href="https://oreil.ly/rHYKZ"><em>GPTQ</em></a> can quantize LLMs with 175 billion parameters in approximately 4 GPU hours, reducing the bit width to 3 or 4 bits per weight, with a negligible accuracy drop relative to the uncompressed model.</p>

<p>The Hugging Face <code>transformers</code> and <code>optimum</code> library authors have collaborated closely with the <code>auto-gptq</code> library developers to provide a simple API for applying GPTQ quantization on open source LLMs.
Optimum is a library that provides APIs to perform quantization using different tools.</p>

<p>With the GPTQ quantization, you can quantize your favorite language model to 8, 4, 3, or even 2 bits without a big drop in performance, while maintaining faster inference speeds that are supported by most GPT hardware.
You can follow <a data-type="xref" href="#gptq_quantization">Example 10-14</a> to quantize a pretrained model on your own GPU.</p>

<p>The dependencies you need to install to run <a data-type="xref" href="#gptq_quantization">Example 10-14</a> will include the 
<span class="keep-together">following:</span></p>

<pre data-type="programlisting">$ pip install auto-gptq optimum transformers accelerate</pre>
<div data-type="example" id="gptq_quantization">
<h5><span class="label">Example 10-14. </span>GPTQ model quantization with Hugging Face and AutoGPTQ libraries</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">optimum</code><code class="nn">.</code><code class="nn">gptq</code> <code class="kn">import</code> <code class="n">GPTQQuantizer</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoModelForCausalLM</code><code class="p">,</code> <code class="n">AutoTokenizer</code>

<code class="n">model_name</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">facebook/opt-125m</code><code class="s2">"</code> <a class="co" href="#callout_optimizing_ai_services_CO10-1" id="co_optimizing_ai_services_CO10-1"><img alt="1" src="assets/1.png"/></a>
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_name</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">AutoModelForCausalLM</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="n">model_name</code><code class="p">,</code> <code class="n">torch_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code>
<code class="p">)</code>

<code class="n">quantizer</code> <code class="o">=</code> <code class="n">GPTQQuantizer</code><code class="p">(</code>
    <code class="n">bits</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>
    <code class="n">dataset</code><code class="o">=</code><code class="s2">"</code><code class="s2">c4</code><code class="s2">"</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO10-2" id="co_optimizing_ai_services_CO10-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">block_name_to_quantize</code><code class="o">=</code><code class="s2">"</code><code class="s2">model.decoder.layers</code><code class="s2">"</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO10-3" id="co_optimizing_ai_services_CO10-3"><img alt="3" src="assets/3.png"/></a>
    <code class="n">model_seqlen</code><code class="o">=</code><code class="mi">2048</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO10-4" id="co_optimizing_ai_services_CO10-4"><img alt="4" src="assets/4.png"/></a>
<code class="p">)</code>
<code class="n">quantized_model</code> <code class="o">=</code> <code class="n">quantizer</code><code class="o">.</code><code class="n">quantize_model</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO10-1" id="callout_optimizing_ai_services_CO10-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Load the <code>float16</code> version of the <code>facebook/opt-125m</code> pretrained model prior to quantization.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO10-2" id="callout_optimizing_ai_services_CO10-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Use the <code>c4</code> dataset to calibrate the quantization.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO10-3" id="callout_optimizing_ai_services_CO10-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Quantize only the model’s decoder layer blocks.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO10-4" id="callout_optimizing_ai_services_CO10-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Use model sequence length of <code>2048</code> to process the dataset.</p></dd>
</dl></div>
<div data-type="tip"><h6>Tip</h6>
<p>For reference, a 175B model will require 4 GPU hours on NVIDIA A100 to quantize.
However, it’s worth searching the Hugging Face model repository for prequantized models, as you might find that someone has already done the work<a data-startref="ix_ch10-asciidoc33" data-type="indexterm" id="id1102"/><a data-startref="ix_ch10-asciidoc32" data-type="indexterm" id="id1103"/><a data-startref="ix_ch10-asciidoc31" data-type="indexterm" id="id1104"/><a data-startref="ix_ch10-asciidoc30" data-type="indexterm" id="id1105"/>.<a data-startref="ix_ch10-asciidoc25" data-type="indexterm" id="id1106"/><a data-startref="ix_ch10-asciidoc24" data-type="indexterm" id="id1107"/></p>
</div>

<p>Now that you understand performance optimization techniques, let’s explore how to enhance the quality of your GenAI services using methods like structured outputs.</p>
</div></section>
</div></section>








<section data-pdf-bookmark="Structured Outputs" data-type="sect2"><div class="sect2" id="id158">
<h2>Structured Outputs</h2>

<p><a data-primary="optimizing AI services" data-secondary="structured outputs" data-type="indexterm" id="ix_ch10-asciidoc34"/><a data-primary="structured outputs" data-type="indexterm" id="ix_ch10-asciidoc35"/>Foundational models such as LLMs may be used as a component of a data pipeline or connected to downstream applications.
For instance, you can use these models to extract and parse information from documents or to generate code that can be executed on other systems.</p>

<p>You can ask the LLM to provide a textual response containing JSON information. You will then have to extract and parse this JSON string using tools like regex and Pydantic. However, there is no guarantee that the model will always adhere to your instructions.
Since your downstream systems may rely on JSON outputs, they may throw exceptions and incorrectly handle invalid inputs.</p>

<p>Several utility packages like Instructor have been released to improve the robustness of LLM responses by taking a schema and making several API calls under the hood with various prompt templates to reach a desired output.
While these solutions improve robustness, they also add significant costs to your solution due to subsequent API calls to the model providers.</p>

<p>Most recently, model providers have added a feature for requesting structured outputs by supplying schemas when making API calls to the model, as you can see in <a data-type="xref" href="#structured_outputs">Example 10-15</a>.
This helps to reduce the prompting templating work you have to do yourself and aims to improve the model’s <em>alignment</em> to your intent when returning a response.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>At the time of writing, only the most recent OpenAI SDK supports Pydantic models for enabling structured outputs.</p>
</div>
<div data-type="example" id="structured_outputs">
<h5><span class="label">Example 10-15. </span>Structured outputs</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">AsyncOpenAI</code>
<code class="kn">from</code> <code class="nn">pydantic</code> <code class="kn">import</code> <code class="n">BaseModel</code><code class="p">,</code> <code class="n">Field</code>

<code class="n">client</code> <code class="o">=</code> <code class="n">AsyncOpenAI</code><code class="p">(</code><code class="p">)</code>

<code class="k">class</code> <code class="nc">DocumentClassification</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_optimizing_ai_services_CO11-1" id="co_optimizing_ai_services_CO11-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">category</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"</code><code class="s2">The category of the classification</code><code class="s2">"</code><code class="p">)</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">get_document_classification</code><code class="p">(</code>
    <code class="n">title</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">DocumentClassification</code> <code class="o">|</code> <code class="nb">str</code> <code class="o">|</code> <code class="kc">None</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">client</code><code class="o">.</code><code class="n">beta</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">parse</code><code class="p">(</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"</code><code class="s2">gpt-4o</code><code class="s2">"</code><code class="p">,</code>
        <code class="n">messages</code><code class="o">=</code><code class="p">[</code>
            <code class="p">{</code>
                <code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">system</code><code class="s2">"</code><code class="p">,</code>
                <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">classify the provided document into the following: ...</code><code class="s2">"</code><code class="p">,</code>
            <code class="p">}</code><code class="p">,</code>
            <code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="n">title</code><code class="p">}</code><code class="p">,</code>
        <code class="p">]</code><code class="p">,</code>
        <code class="n">response_format</code><code class="o">=</code><code class="n">DocumentClassification</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO11-2" id="co_optimizing_ai_services_CO11-2"><img alt="2" src="assets/2.png"/></a>
    <code class="p">)</code>

    <code class="n">message</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code>
    <code class="k">return</code> <code class="n">message</code><code class="o">.</code><code class="n">parsed</code> <code class="k">if</code> <code class="n">message</code><code class="o">.</code><code class="n">parsed</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code> <code class="k">else</code> <code class="n">message</code><code class="o">.</code><code class="n">refusal</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO11-1" id="callout_optimizing_ai_services_CO11-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Specify a Pydantic model for structured outputs.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO11-2" id="callout_optimizing_ai_services_CO11-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Provide the defined schema to the model client when making the API call.</p></dd>
</dl></div>

<p>If your model provider doesn’t support structured outputs natively, you can still leverage the model’s chat completion capabilities to increase robustness of structured outputs, as shown in <a data-type="xref" href="#structured_outputs_completions">Example 10-16</a>.</p>
<div data-type="example" id="structured_outputs_completions">
<h5><span class="label">Example 10-16. </span>Structured outputs based on chat completions prefill</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">json</code>
<code class="kn">from</code> <code class="nn">loguru</code> <code class="kn">import</code> <code class="n">logger</code>
<code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">AsyncOpenAI</code>
<code class="n">client</code> <code class="o">=</code> <code class="n">AsyncOpenAI</code><code class="p">(</code><code class="p">)</code>

<code class="n">system_template</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">Classify the provided document into the following: ...</code>

<code class="s2">Provide responses in the following manner json: </code><code class="s2">{</code><code class="s2">"</code><code class="s2">category</code><code class="s2">"</code><code class="s2">: </code><code class="s2">"</code><code class="s2">string</code><code class="s2">"</code><code class="s2">}</code>
<code class="s2">"""</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">get_document_classification</code><code class="p">(</code><code class="n">title</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">dict</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"</code><code class="s2">gpt-4o</code><code class="s2">"</code><code class="p">,</code>
        <code class="n">max_tokens</code><code class="o">=</code><code class="mi">1024</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO12-1" id="co_optimizing_ai_services_CO12-1"><img alt="1" src="assets/1.png"/></a>
        <code class="n">messages</code><code class="o">=</code><code class="p">[</code>
            <code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">system</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="n">system_template</code><code class="p">}</code><code class="p">,</code>
            <code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="n">title</code><code class="p">}</code><code class="p">,</code>
            <code class="p">{</code>
                <code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">assistant</code><code class="s2">"</code><code class="p">,</code>
                <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">The document classification JSON is </code><code class="s2">{</code><code class="s2">"</code><code class="p">,</code> <a class="co" href="#callout_optimizing_ai_services_CO12-2" id="co_optimizing_ai_services_CO12-2"><img alt="2" src="assets/2.png"/></a>
            <code class="p">}</code><code class="p">,</code>
        <code class="p">]</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="n">message</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code> <code class="ow">or</code> <code class="s2">"</code><code class="s2">"</code>
    <code class="k">try</code><code class="p">:</code>
        <code class="k">return</code> <code class="n">json</code><code class="o">.</code><code class="n">loads</code><code class="p">(</code><code class="s2">"</code><code class="s2">{</code><code class="s2">"</code> <code class="o">+</code> <code class="n">message</code><code class="p">[</code><code class="p">:</code> <code class="n">message</code><code class="o">.</code><code class="n">rfind</code><code class="p">(</code><code class="s2">"</code><code class="s2">}</code><code class="s2">"</code><code class="p">)</code> <code class="o">+</code> <code class="mi">1</code><code class="p">]</code><code class="p">)</code> <a class="co" href="#callout_optimizing_ai_services_CO12-3" id="co_optimizing_ai_services_CO12-3"><img alt="3" src="assets/3.png"/></a>
    <code class="k">except</code> <code class="n">json</code><code class="o">.</code><code class="n">JSONDecodeError</code><code class="p">:</code> <a class="co" href="#callout_optimizing_ai_services_CO12-4" id="co_optimizing_ai_services_CO12-4"><img alt="4" src="assets/4.png"/></a>
        <code class="n">logger</code><code class="o">.</code><code class="n">warning</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Failed to parse the response: </code><code class="si">{</code><code class="n">message</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"</code><code class="s2">error</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">Refusal response</code><code class="s2">"</code><code class="p">}</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_optimizing_ai_services_CO12-1" id="callout_optimizing_ai_services_CO12-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Limit the output tokens to improve robustness and speed of the structured responses and to reduce costs.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO12-2" id="callout_optimizing_ai_services_CO12-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Skip the preamble and directly return a JSON by prefilling the assistant response and including a <code>{</code> character.</p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO12-3" id="callout_optimizing_ai_services_CO12-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Add back the prefilled <code>{</code> and then find the closing <code>}</code> and extract the JSON 
<span class="keep-together">substring.</span></p></dd>
<dt><a class="co" href="#co_optimizing_ai_services_CO12-4" id="callout_optimizing_ai_services_CO12-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Handle cases where there is no JSON in the response—e.g., if there is a refusal.</p></dd>
</dl></div>

<p>Following the aforementioned techniques should help you improve the robustness of your data pipelines if they leverage LLMs as a component.<a data-startref="ix_ch10-asciidoc35" data-type="indexterm" id="id1108"/><a data-startref="ix_ch10-asciidoc34" data-type="indexterm" id="id1109"/></p>
</div></section>








<section data-pdf-bookmark="Prompt Engineering" data-type="sect2"><div class="sect2" id="id159">
<h2>Prompt Engineering</h2>

<p><a data-primary="optimizing AI services" data-secondary="prompt engineering" data-type="indexterm" id="ix_ch10-asciidoc36"/><a data-primary="prompt engineering" data-type="indexterm" id="ix_ch10-asciidoc37"/>Prompt engineering is the practice of crafting and refining queries to generative models to produce the most useful and optimized outputs.
Without refining prompts, you’d either have to fine-tune models or train a model from scratch to optimize the output quality.</p>

<p>Many argue that the field lacks the scientific rigor to consider it an engineering discipline.
However, you can approach the problem from an engineering perspective when refining prompts to get the best quality outputs from your models.</p>

<p>Similar to how you communicate with others to get things done, with the optimized prompts, you can most effectively communicate your intent with the model to improve the chances of getting the responses you want.
Therefore, prompting becomes not just an engineering problem but a communication one as well.
A model can be compared to a knowledgeable colleague with lots of experience but limited domain knowledge, ready to help you but needs you to provide well-documented instructions, possibly with a few examples to follow and pattern match.</p>

<p>If your prompts are vague and generic, you’ll also get an average response.</p>

<p>Another way of thinking about this optimization problem is to compare the task of model prompting to programming.
Instead of writing the code yourself, you’re effectively “coding” a model to be a well-integrated component of a larger application or data pipeline.
You can adopt test-driven development (TDD) approaches and refine your prompts until your tests pass.
Or, experiment with different models to see which one <em>aligns</em> its outputs to your intent the best.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Maximizing model <em>alignment</em> remains a high-priority objective of many model providers so that their model outputs best satisfy the user’s intent.</p>
</div>










<section data-pdf-bookmark="Prompt templates" data-type="sect3"><div class="sect3" id="id160">
<h3>Prompt templates</h3>

<p><a data-primary="prompt templates" data-type="indexterm" id="id1110"/><a data-primary="optimizing AI services" data-secondary="prompt engineering" data-tertiary="prompt templates" data-type="indexterm" id="id1111"/><a data-primary="prompt engineering" data-secondary="prompt templates" data-type="indexterm" id="id1112"/>If your system instructions aren’t methodical, clear, and don’t follow best prompting practices, you may be leaving potential quality and performance optimizations on the table.</p>

<p>As a minimum, you should have clear system prompts that provide specific tasks to the model.
Best practice is to follow a systematic template.
<a data-primary="RCT (role, context, and task) template" data-type="indexterm" id="id1113"/>For instance, draft the model instructions following the <em>role, context, and task</em> (RCT) template:</p>
<dl>
<dt>Role</dt>
<dd>
<p>Describes how the model should behave given a scenario and a task.
Research has shown that specifying roles for LLMs tends to significantly affect their outputs.
As an example, a model may be more forgiving in grading an essay if you give it the role of a primary school teacher.
Without a specific role, the model may assume you want the grading to follow  university-level academic standards.</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can expand on the model’s role even further and describe a <em>persona</em> in detail for the model to adopt.
Using a persona, the model will exactly know how to behave and make predictions as it has more context on what the role should entail.</p>
</div>
<dl>
<dt>Context</dt>
<dd>
<p>Sets the scenario, paints the picture, and provides any relevant and useful information that the model can use as a reference to making predictions.
Without an explicit context, the model can only use an implicit context that’ll contain average information of its training data.
In a RAG application, the context could be the concatenation of system prompt with the retrieved document chunks from a knowledge store.</p>
</dd>
<dt>Task</dt>
<dd>
<p>Provides clear instructions on what you want the model to perform given a context and a role.
When describing the task, make sure you think of the model as a bright and knowledgeable apprentice, ready to jump into action but needs highly clear and unambiguous instructions to follow, potentially with a handful of examples.</p>
</dd>
</dl>

<p>Following the aforementioned system template, you should enhance the quality of your model outputs with minimal effort.</p>
</div></section>










<section data-pdf-bookmark="Advanced prompting techniques" data-type="sect3"><div class="sect3" id="id257">
<h3>Advanced prompting techniques</h3>

<p><a data-primary="optimizing AI services" data-secondary="prompt engineering" data-tertiary="advanced prompting techniques" data-type="indexterm" id="ix_ch10-asciidoc38"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-type="indexterm" id="ix_ch10-asciidoc39"/>Beyond the prompting fundamentals, you can use more advanced techniques that may better fit your use case.
Based on a <a href="https://oreil.ly/xynPC">recent systematic survey of prompting techniques</a>, you can group LLM prompts into the following:</p>

<ul>
<li>
<p>In-context learning</p>
</li>
<li>
<p>Thought generation</p>
</li>
<li>
<p>Decomposition</p>
</li>
<li>
<p>Ensembling</p>
</li>
<li>
<p>Self-criticism</p>
</li>
<li>
<p>Agentic</p>
</li>
</ul>

<p>Let’s review each in more detail.</p>












<section data-pdf-bookmark="In-context learning" data-type="sect4"><div class="sect4" id="id161">
<h4>In-context learning</h4>

<p><a data-primary="in-context learning" data-type="indexterm" id="id1114"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="in-context learning" data-type="indexterm" id="id1115"/>What sets foundational models such as LLMs apart from traditional machine learning models is their ability to respond to dynamic inputs without the constant need for fine-tuning or retraining.</p>

<p>When you give system instructions to an LLM, you can additionally supply several examples, (i.e., shots) to guide the output generation.</p>

<p><a data-primary="zero-shot prompting" data-type="indexterm" id="id1116"/><a href="https://oreil.ly/3F4wb"><em>Zero-shot prompting</em></a> refers to a prompting approach that doesn’t specify reference examples, yet the model can still successfully complete the given task.
<a data-primary="few-shot prompting" data-type="indexterm" id="id1117"/>If the model struggles without reference examples, you may have to use <a href="https://oreil.ly/pOSj8"><em>few-shot prompting</em></a> where you provide a handful of examples.
<a data-primary="dynamic few-shot prompting" data-type="indexterm" id="id1118"/>There are also use cases where you want to use <em>dynamic few-shot</em> prompting where you dynamically insert examples from data fetched from a database or vector store.</p>

<p>Prompting approaches where you specify examples are also termed <em>in-context learning</em>.
You’re effectively fine-tuning the model’s outputs to your examples and the given task without actually modifying its model weights/parameters, whereas other ML models would require adjustments to their weights.</p>

<p>This is what makes LLMs and foundational models so powerful, since they don’t always require weight adjustment to fit your data and tasks you give them.
You can learn about several in-context learning techniques by referring to <a data-type="xref" href="#prompting_techniques_incontext_learning">Table 10-3</a>.</p>
<table class="striped" id="prompting_techniques_incontext_learning">
<caption><span class="label">Table 10-3. </span>In-context learning prompting techniques</caption>
<thead>
<tr>
<th>Prompting technique</th>
<th>Examples</th>
<th>Use cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Zero-shot</p></td>
<td><p>Summarize the following…​</p></td>
<td><p>Summarization, Q&amp;A without specific training examples</p></td>
</tr>
<tr>
<td><p>Few-shot</p></td>
<td><p>Classify documents based on examples below:</p><p>[Examples]</p></td>
<td><p>Text classification, sentiment analysis, data extraction with few examples</p></td>
</tr>
<tr>
<td><p>Dynamic few-shot</p></td>
<td><p>Classify the following documents based on examples below:</p><p>&lt;Inject examples from a vector store based on a query&gt;</p></td>
<td><p>Personalized responses, complex problem-solving</p></td>
</tr>
</tbody>
</table>

<p>In-context learning prompts are straightforward, effective, and a great starting point for completing a variety of tasks.
For more complex tasks, you can use more advanced prompting approaches like thought generation, decomposition, ensembling, self-criticism, or agentic approaches.</p>
</div></section>












<section data-pdf-bookmark="Thought generation" data-type="sect4"><div class="sect4" id="id162">
<h4>Thought generation</h4>

<p><a data-primary="CoT (chain of thought) prompting" data-type="indexterm" id="id1119"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="thought generation" data-type="indexterm" id="id1120"/><a data-primary="thought generation techniques" data-type="indexterm" id="id1121"/>Thought generation techniques like <a href="https://oreil.ly/BWUYQ">chain of thought (CoT)</a> have shown to significantly improve the ability of LLMs to perform complex reasoning.</p>

<p>In COT prompting you ask the model to explain its thought process and reasoning as it provides a response.
Variants of CoT include zero-shot or <a href="https://oreil.ly/1gjSH">few-shot CoT</a> depending on whether you supply examples.
A more advanced thought generation technique is <a href="https://oreil.ly/1KyO4">thread of thought (ThoT)</a> that systematically segments and analyzes chaotic and very complex information or tasks.</p>

<p><a data-type="xref" href="#prompting_techniques_thought_generation">Table 10-4</a> lists thought generation techniques.</p>
<table class="striped" id="prompting_techniques_thought_generation">
<caption><span class="label">Table 10-4. </span>Thought generation prompting techniques</caption>
<thead>
<tr>
<th>Prompting technique</th>
<th>Examples</th>
<th>Use cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Zero-shot chain of thought (CoT)</p></td>
<td><p>Let’s think step by step…​</p></td>
<td><p>Mathematical problem-solving, logical reasoning, and multi-step decision-making.</p></td>
</tr>
<tr>
<td><p>Few-shot CoT</p></td>
<td><p>Let’s think step by step…​ Here are a few examples:</p><p>[EXAMPLES]</p></td>
<td><p>Scenarios where a few examples can guide the model to perform better, such as nuanced text classification, complex question answering, and creative writing prompts.</p></td>
</tr>
<tr>
<td><p>Thread of thought (ThoT)</p></td>
<td><p>Walk me through the problem
in manageable parts step by step, summarizing and
analyzing as you go…​</p></td>
<td><p>Maintaining context over multiple interactions, such as dialogue systems, interactive storytelling, and long-form content generation.</p></td>
</tr>
</tbody>
</table>
</div></section>












<section data-pdf-bookmark="Decomposition" data-type="sect4"><div class="sect4" id="id163">
<h4>Decomposition</h4>

<p><a data-primary="decomposition prompting techniques" data-type="indexterm" id="id1122"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="decomposition" data-type="indexterm" id="id1123"/>Decomposition prompting techniques focus on breaking down complex tasks into smaller subtasks so that the model can work through them step-by-step and logically.
You can experiment with these approaches alongside thought generation to identify which ones produce the best results for your use case.</p>

<p>These are the most common decomposition prompting techniques:</p>
<dl>
<dt><a href="https://oreil.ly/HmsSN">Least-to-most</a></dt>
<dd>
<p>Ask the model to break a complex problem into smaller problems via logical reduction without solving them.
You can then reprompt the model to solve each task one by one.</p>
</dd>
<dt><a href="https://oreil.ly/aWTzf">Plan-and-solve</a></dt>
<dd>
<p>Given a task, ask for a plan to be devised, and then request the model to solve it.</p>
</dd>
<dt><a href="https://oreil.ly/IZdj1">Tree of thoughts (ToT)</a></dt>
<dd>
<p>Create a tree-search problem where a task is broken into multiple branches of steps like a tree.
Then, reprompt the model to evaluate and solve each branch of steps.</p>
</dd>
</dl>

<p><a data-type="xref" href="#prompting_techniques_decomposition">Table 10-5</a> shows these decomposition techniques.</p>
<table class="striped" id="prompting_techniques_decomposition">
<caption><span class="label">Table 10-5. </span>Decomposition prompting techniques</caption>
<thead>
<tr>
<th>Prompting technique</th>
<th>Examples</th>
<th>Use cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Least-to-most</p></td>
<td><p>Break down the task of…​into smaller tasks.</p></td>
<td><p>Complex problem-solving, project management, task decomposition</p></td>
</tr>
<tr>
<td><p>Plan-and-solve</p></td>
<td><p>Devise a plan to…​</p></td>
<td><p>Algorithm development, software design, strategic planning</p></td>
</tr>
<tr>
<td><p>Tree of thoughts (ToT)</p></td>
<td><p>Create a decision tree for choosing a…​</p></td>
<td><p>Decision-making, problem-solving with multiple solutions, strategic planning with alternatives</p></td>
</tr>
</tbody>
</table>
</div></section>












<section data-pdf-bookmark="Ensembling" data-type="sect4"><div class="sect4" id="id164">
<h4>Ensembling</h4>

<p><a data-primary="ensembling, prompting" data-type="indexterm" id="id1124"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="ensembling" data-type="indexterm" id="id1125"/><em>Ensembling</em> is the process of using multiple prompts to solve the same problem and then aggregating the responses into a final output.
You can generate these responses using the same or different models.</p>

<p>The main idea behind ensembling is to reduce the variance of LLM outputs by improving accuracy in exchange for higher usage costs.</p>

<p>Well-known ensembling prompting techniques include the following:</p>
<dl>
<dt><a href="https://oreil.ly/_85WS">Self-consistency</a></dt>
<dd>
<p>Generates multiple reasoning paths and selects the most consistent output as the final result using a majority vote.</p>
</dd>
<dt><a href="https://oreil.ly/xllKs">Mixture of reasoning experts (MoRE)</a></dt>
<dd>
<p>Combines outputs from multiple LLMs with specialized prompts to improve response quality.
Each LLM acts as an expert on an area focused on different reasoning tasks such as factual reasoning, logical reduction, common-sense checks, etc.</p>
</dd>
<dt><a href="https://oreil.ly/lPEPz">Demonstration ensembling (DENSE)</a></dt>
<dd>
<p>Creates multiple few-shot prompts from data, then generates a final output by aggregating over the responses.</p>
</dd>
<dt><a href="https://oreil.ly/yP_ka">Prompt paraphrasing</a></dt>
<dd>
<p>Formulates the original prompt into multiple variants via wording.</p>
</dd>
</dl>

<p><a data-type="xref" href="#prompting_techniques_ensemling">Table 10-6</a> shows examples and use cases of these ensembling techniques.</p>
<table class="striped" id="prompting_techniques_ensemling">
<caption><span class="label">Table 10-6. </span>Ensembling prompting techniques</caption>
<thead>
<tr>
<th>Prompting technique</th>
<th>Examples</th>
<th>Use cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Self-consistency</p></td>
<td><p>Prompt #1 (run multiple times): Let’s think step by step and complete the following task…​</p><p>Prompt #2: From the following responses, choose the best/common one by scoring them using…​</p></td>
<td><p>Reducing errors or bias in arithmetic, common-sense tasks, and symbolic reasoning tasks</p></td>
</tr>
<tr>
<td><p>Mixture of reasoning experts (MoRE)</p></td>
<td><p>Prompt #1 (run for each expert): You are a reviewer for …​, score the following based on…​</p><p>Prompt #2: Choose the best expert answer based on an agreement score…​</p></td>
<td><p>Accounting for specialized knowledge areas or domains</p></td>
</tr>
<tr>
<td><p>Demonstration ensembling (DENSE)</p></td>
<td><p>Create multiple few-shot examples for translating this text and aggregate the best responses.</p><p>Generate several few-shot prompts for summarizing this article and combine the outputs.</p></td>
<td><ul><li><p>Improving output reliability</p></li>
<li><p>
Aggregating diverse perspectives</p></li></ul></td>
</tr>
<tr>
<td><p>Prompt paraphrasing</p></td>
<td><p>Prompt #1a: Reword this proposal…​</p><p>Prompt #1b: Clarify this proposal…​</p><p>Prompt #1c: Make adjustment to this proposal…​</p><p>Prompt #2: Choose the best proposal from the following responses based on…​</p></td>
<td><ul><li><p>Exploring different interpretations</p></li>
<li>
<p>Data augmentation for ensembling</p></li></ul></td>
</tr>
</tbody>
</table>
</div></section>












<section data-pdf-bookmark="Self-criticism" data-type="sect4"><div class="sect4" id="id165">
<h4>Self-criticism</h4>

<p><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="self-criticism" data-type="indexterm" id="id1126"/><a data-primary="self-criticism prompting" data-type="indexterm" id="id1127"/><em>Self-criticism</em> prompting techniques focus on using models as AI judges, assessors, or reviewers, either to perform self-checks or to assess the outputs of other models.
The criticism or feedback from the first prompt can then be used to improve the response quality in follow-on prompts.</p>

<p>These are several self-criticism prompting strategies:</p>
<dl>
<dt><a href="https://oreil.ly/_4YEr">Self-calibration</a></dt>
<dd>
<p>Ask the LLM to assess the correctness of a response/answer against a question/answer.</p>
</dd>
<dt><a href="https://oreil.ly/bTQJI">Self-refine</a></dt>
<dd>
<p>Refine responses iteratively through self-checks and providing feedback.</p>
</dd>
<dt><a href="https://oreil.ly/6ojtr">Reversing chain of thought (RCoT)</a></dt>
<dd>
<p>Reconstruct the problem from a generated answer, and then generate fine-grained comparisons between the original problem and the reconstructed one to identify inconsistencies.</p>
</dd>
<dt><a href="https://oreil.ly/Fz3JH">Self-verification</a></dt>
<dd>
<p>Generate potential solutions with the CoT technique, and then score each by masking parts of the question and supplying each answer.</p>
</dd>
<dt><a href="https://oreil.ly/WrrLP">Chain of verification (COVE)</a></dt>
<dd>
<p>Create a list of related queries/questions to help verify the correctness of an answer/response.</p>
</dd>
<dt><a href="https://oreil.ly/3Hb-6">Cumulative reasoning</a></dt>
<dd>
<p>Generate potential steps in responding to a query, and then ask the model to accept/reject each step.
Finally, check whether it has arrived at the final answer to terminate the process; otherwise, repeat the process.</p>
</dd>
</dl>

<p>You can see examples of each self-criticism prompting technique in <a data-type="xref" href="#prompting_techniques_self_criticism">Table 10-7</a>.</p>
<table class="striped" id="prompting_techniques_self_criticism">
<caption><span class="label">Table 10-7. </span>Self-criticism prompting techniques</caption>
<thead>
<tr>
<th>Prompting technique</th>
<th>Examples</th>
<th>Use cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Self-calibration</p></td>
<td><p>Assess the correctness of the following response: [response]
for the following question: [question]</p></td>
<td><p>Gauge confidence of the answers to accept or revise the original answer.</p></td>
</tr>
<tr>
<td><p>Self-refine</p></td>
<td><p>Prompt #1: What is your feedback on the response…​</p><p>Prompt #2: Using the feedback [Feedback], refine your response on…​</p></td>
<td><p>Reasoning, coding, and generation tasks.</p></td>
</tr>
<tr>
<td><p>Reversing chain-of-thought (RCoT)</p></td>
<td><p>Prompt #1: Reconstruct the problem from this answer…​</p><p>Prompt #2: Generate fine-grained comparison between these queries…​</p></td>
<td><p>Identifying inconsistencies and revising answers.</p></td>
</tr>
<tr>
<td><p>Self-verification</p></td>
<td><p>Prompt #1 (run multiple times): Let’s think step by step - generate solution for the following problem…​</p><p>Prompt #2: Score each solution based on the [masked problem]…​</p></td>
<td><p>Improve on reasoning tasks.</p></td>
</tr>
<tr>
<td><p>Chain of verification (COVE)</p></td>
<td><p>Prompt #1: Answer the following question…​</p><p>Prompt #2: Formulate related questions to check this response: …​</p><p>Prompt #3 (run for each new related question): Answer the following question: …​</p><p>Prompt #4: Based on the following information, pick the best answer…​</p></td>
<td><p>Question answering and text-generation
tasks.</p></td>
</tr>
<tr>
<td><p>Cumulative reasoning</p></td>
<td><p>Prompt #1: Outline steps to respond to the query: …​</p><p>Prompt #2: Check the following plan and accept/reject steps relevant in responding to the query: …​</p><p>Prompt #3: Check you’ve arrived at the final answer given the following information…​</p></td>
<td><p>Step-by-step validation of complex queries, logical inference, and mathematical problems.</p></td>
</tr>
</tbody>
</table>
</div></section>












<section data-pdf-bookmark="Agentic" data-type="sect4"><div class="sect4" id="id166">
<h4>Agentic</h4>

<p><a data-primary="agentic prompting techniques" data-type="indexterm" id="ix_ch10-asciidoc40"/><a data-primary="prompt engineering" data-secondary="advanced prompting techniques" data-tertiary="agentic techniques" data-type="indexterm" id="ix_ch10-asciidoc41"/>You can take the prompting techniques discussed so far one step further and add access to external tools with complex evaluation algorithms.
This process specializes LLMs as <em>agents</em>, allowing them to make plans, take actions, and use external 
<span class="keep-together">systems.</span></p>

<p>Prompts or <em>prompt sequences (chains)</em> drive agentic systems with an engineering focus on creating agent-like behavior from LLMs.
These agentic workflows serve users by performing actions on systems that interface with the GenAI models, which are mostly LLMs.
Tools, whether <em>symbolic</em> like a calculator, or <em>neural</em> such as another AI model, form a core component of agentic systems.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you create a pipeline of multiple model calls with one output forwarded to the same or different model as input, you’ve constructed a <em>prompt chain</em>.
In principle, you’re using the CoT prompting technique when you leverage prompt chains.</p>
</div>

<p>A few agentic prompting techniques include:</p>
<dl>
<dt><a href="https://oreil.ly/aWeQu">Modular reasoning, knowledge, and language (MRKL)</a></dt>
<dd>
<p>Simplest agentic system consisting of an LLM using multiple tools to get and combine information for generating an answer.</p>
</dd>
<dt><a href="https://oreil.ly/M-9YL">Self-correcting with tool-interactive critiquing (CRITIC)</a></dt>
<dd>
<p>Responds to queries, and then self-checks its answer without using external tools.
Finally, uses tools to verify or amend responses.</p>
</dd>
<dt><a href="https://oreil.ly/0WtKv">Program-aided language model (PAL)</a></dt>
<dd>
<p>Generates code from queries and sends directly to code interpreters such as Python to generate an answer.<sup><a data-type="noteref" href="ch10.html#id1128" id="id1128-marker">4</a></sup></p>
</dd>
<dt><a href="https://oreil.ly/pbfv_">Tool-integrated reasoning agent (ToRA)</a></dt>
<dd>
<p>Takes PAL a few steps further by interleaving code generation and reasoning steps as long as needed to provide a satisfactory response.</p>
</dd>
<dt><a href="https://oreil.ly/aDubr">Reasoning and acting (ReAct)</a></dt>
<dd>
<p>Given a problem, generates thoughts, takes actions, receives observations, and repeats the loop with previous information, (i.e., memory) until the problem is solved.</p>
</dd>
</dl>

<p>If you want to enable your LLMs to use tools, you can take advantage of <em>function calling</em> features from model providers, as in <a data-type="xref" href="#function_calling">Example 10-17</a>.</p>
<div data-type="example" id="function_calling">
<h5><span class="label">Example 10-17. </span>Function calling for fetching</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">OpenAI</code>
<code class="kn">from</code> <code class="nn">scraper</code> <code class="kn">import</code> <code class="n">fetch</code>
<code class="n">client</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">()</code>

<code class="n">tools</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">{</code>
        <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"function"</code><code class="p">,</code>
        <code class="s2">"function"</code><code class="p">:</code> <code class="p">{</code>
            <code class="s2">"name"</code><code class="p">:</code> <code class="s2">"fetch"</code><code class="p">,</code>
            <code class="s2">"description"</code><code class="p">:</code> <code class="s2">"Read the content of url and provide a summary"</code><code class="p">,</code>
            <code class="s2">"parameters"</code><code class="p">:</code> <code class="p">{</code>
                <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"object"</code><code class="p">,</code>
                <code class="s2">"properties"</code><code class="p">:</code> <code class="p">{</code>
                    <code class="s2">"url"</code><code class="p">:</code> <code class="p">{</code>
                        <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"string"</code><code class="p">,</code>
                        <code class="s2">"description"</code><code class="p">:</code> <code class="s2">"The url to fetch"</code><code class="p">,</code>
                    <code class="p">},</code>
                <code class="p">},</code>
                <code class="s2">"required"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"url"</code><code class="p">],</code>
                <code class="s2">"additionalProperties"</code><code class="p">:</code> <code class="kc">False</code><code class="p">,</code>
            <code class="p">},</code>
        <code class="p">},</code>
    <code class="p">}</code>
<code class="p">]</code>

<code class="n">messages</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">{</code>
        <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code>
        <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"You are a helpful customer support assistant"</code>
        <code class="s2">"Use the supplied tools to assist the user."</code><code class="p">,</code>
    <code class="p">},</code>
    <code class="p">{</code>
        <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code>
        <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"Summarize this paper: https://arxiv.org/abs/2207.05221"</code><code class="p">,</code>
    <code class="p">},</code>
<code class="p">]</code>

<code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-4o"</code><code class="p">,</code>
    <code class="n">messages</code><code class="o">=</code><code class="n">messages</code><code class="p">,</code>
    <code class="n">tools</code><code class="o">=</code><code class="n">tools</code><code class="p">,</code>
<code class="p">)</code></pre></div>

<p>As you saw in <a data-type="xref" href="#function_calling">Example 10-17</a>, you can create agentic systems by configuring specialized LLMs that have access to custom tools and<a data-startref="ix_ch10-asciidoc41" data-type="indexterm" id="id1129"/><a data-startref="ix_ch10-asciidoc40" data-type="indexterm" id="id1130"/> functions<a data-startref="ix_ch10-asciidoc39" data-type="indexterm" id="id1131"/><a data-startref="ix_ch10-asciidoc38" data-type="indexterm" id="id1132"/>.<a data-startref="ix_ch10-asciidoc37" data-type="indexterm" id="id1133"/><a data-startref="ix_ch10-asciidoc36" data-type="indexterm" id="id1134"/></p>
</div></section>
</div></section>
</div></section>








<section data-pdf-bookmark="Fine-Tuning" data-type="sect2"><div class="sect2" id="id167">
<h2>Fine-Tuning</h2>

<p><a data-primary="fine-tuning (optimization technique)" data-type="indexterm" id="ix_ch10-asciidoc42"/><a data-primary="optimizing AI services" data-secondary="fine-tuning" data-type="indexterm" id="ix_ch10-asciidoc43"/>There are cases where prompt engineering alone won’t give the response quality you’re looking for.
Fine-tuning is an optimization technique that requires you to adjust the parameters of your GenAI model to better fit your data.
For instance, you may fine-tune a language model to learn content of private knowledge bases or to always respond with a certain tone following your brand guidelines.</p>

<p>It’s often not the first technique you should try since it requires effort to collect and prepare data, in addition to training and evaluating models.</p>










<section data-pdf-bookmark="When should you consider fine-tuning?" data-type="sect3"><div class="sect3" id="id168">
<h3>When should you consider fine-tuning?</h3>

<p><a data-primary="fine-tuning (optimization technique)" data-secondary="when to consider" data-type="indexterm" id="ix_ch10-asciidoc44"/><a data-primary="optimizing AI services" data-secondary="fine-tuning" data-tertiary="when to consider" data-type="indexterm" id="ix_ch10-asciidoc45"/>You may want to consider fine-tuning pretrained GenAI models if one of the following scenarios is true:</p>

<ul>
<li>
<p>You have significant token usage costs—for instance, due to requiring extensive system instructions or providing lots of examples in every prompt.</p>
</li>
<li>
<p>Your use case relies on specialized domain expertise that the model needs to learn.</p>
</li>
<li>
<p>You need to reduce the number of hallucinations in responses with a more fine-tuned conservative model.</p>
</li>
<li>
<p>You require higher-quality responses and have sufficient data for fine-tuning.</p>
</li>
<li>
<p>You require lower latency in responses.</p>
</li>
</ul>

<p>Once a model has been fine-tuned, you won’t need to provide as many examples in the prompt.
This saves costs and enables lower-latency requests.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Avoid fine-tuning as much as you can.</p>

<p>There are many tasks where prompt engineering alone can help you optimize the quality of your outputs.
Iterating over prompts has a much faster feedback loop than iterating over fine-tuning, which relies on creating datasets and running training jobs.</p>

<p>However, if you do end up needing to fine-tune, you’ll notice that the initial prompt engineering efforts would contribute to producing higher-quality training data.</p>
</div>

<p>Here are a few cases where fine-tuning can be useful:</p>

<ul>
<li>
<p>Teaching a model to respond in a brand style, tone, format, or some other qualitative metric—for instance, to produce standardized reports that comply with regulatory requirements and internal protocols</p>
</li>
<li>
<p>Improving reliability of producing desired outputs such as always having responses conform to a given structured output</p>
</li>
<li>
<p>Achieving correct results to complex queries such as document classification and tagging from hundreds of classes</p>
</li>
<li>
<p>Performing domain-specific specialized tasks such as item classification or industry-specific data interpretation and aggregation</p>
</li>
<li>
<p>Nuanced handling of edge cases</p>
</li>
<li>
<p>Performing skills or tasks that are hard to articulate in prompts such as datetime extraction from unstructured texts</p>
</li>
<li>
<p>Reducing costs by using <code>gpt-40-mini</code> or even <code>gpt-3.5-turbo</code> instead of <code>gpt-4o</code></p>
</li>
<li>
<p>Teaching a model to use complex tools and APIs when using function calling<a data-startref="ix_ch10-asciidoc45" data-type="indexterm" id="id1135"/><a data-startref="ix_ch10-asciidoc44" data-type="indexterm" id="id1136"/></p>
</li>
</ul>
</div></section>










<section data-pdf-bookmark="How to fine-tune a pretrained model" data-type="sect3"><div class="sect3" id="id169">
<h3>How to fine-tune a pretrained model</h3>

<p><a data-primary="fine-tuning (optimization technique)" data-secondary="for pretrained model" data-type="indexterm" id="ix_ch10-asciidoc46"/><a data-primary="optimizing AI services" data-secondary="fine-tuning" data-tertiary="for pretrained model" data-type="indexterm" id="ix_ch10-asciidoc47"/>For any fine-tuning job, you will need to follow these steps:</p>
<ol>
<li>
<p>Prepare and upload training data.</p>
</li>
<li>
<p>Submit a fine-tuning training job.</p>
</li>
<li>
<p>Evaluate and use fine-tuned model.</p>
</li>

</ol>

<p>Depending on the model you’re using, the data must be prepared based on the model provider’s instruction.</p>

<p>For instance, to fine-tune a typical chat model like <code>gpt-4o-2024-08-06</code>, you need to prepare your data as a message format, as shown in <a data-type="xref" href="#fine_tune_prepare">Example 10-18</a>.
At the time of writing, <a href="https://oreil.ly/MmCNq">OpenAI API pricing</a> for fine-tuning this model is $25/1M training tokens.</p>
<div data-type="example" id="fine_tune_prepare">
<h5><span class="label">Example 10-18. </span>Example training data for a fine-tuning job</h5>

<pre data-code-language="json" data-type="programlisting"><code class="c1">// training_data.jsonl</code><code class="w"/>

<code class="p">{</code><code class="w"/>
    <code class="nt">"messages"</code><code class="p">:</code> <code class="p">[</code><code class="w"/>
        <code class="p">{</code><code class="w"/>
            <code class="nt">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code><code class="w"/>
            <code class="nt">"content"</code><code class="p">:</code> <code class="s2">"&lt;text&gt;"</code><code class="w"/>
        <code class="p">},</code><code class="w"/>
        <code class="p">{</code><code class="w"/>
            <code class="nt">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code><code class="w"/>
            <code class="nt">"content"</code><code class="p">:</code> <code class="s2">"&lt;text&gt;"</code><code class="w"/>
        <code class="p">},</code><code class="w"/>
        <code class="p">{</code><code class="w"/>
            <code class="nt">"role"</code><code class="p">:</code> <code class="s2">"assistant"</code><code class="p">,</code><code class="w"/>
            <code class="nt">"content"</code><code class="p">:</code> <code class="s2">"&lt;text&gt;"</code><code class="w"/>
        <code class="p">}</code><code class="w"/>
    <code class="p">]</code><code class="w"/>
<code class="p">}</code><code class="w"/>
<code class="c1">// more entries</code><code class="w"/></pre></div>

<p>Once your data is prepared, you need to upload the <code>jsonl</code> file, get a file ID, and supply that when submitting a fine-tuning job, as you can see in <a data-type="xref" href="#fine_tuning_training">Example 10-19</a>.</p>
<div class="less_space pagebreak-before" data-type="example" id="fine_tuning_training">
<h5><span class="label">Example 10-19. </span>Submitting a fine-tuning training job</h5>

<pre data-type="programlisting">from openai import OpenAI
client = OpenAI()

response = client.files.create(
    file=open("mydata.jsonl", "rb"), purpose="fine-tune"
)

client.fine_tuning.jobs.create(
    training_file=response.id, model="gpt-4o-mini-2024-07-18"
)</pre></div>

<p>Model providers that allow you to submit fine-tuning jobs will also provide APIs for checking the status of submitted jobs and for getting results.</p>

<p>Once the model is fine-tuned, you can retrieve the fine-tuned model ID and pass it to the LLM client, as shown in <a data-type="xref" href="#fine_tuning_usage">Example 10-20</a>.
Make sure to evaluate the model first before using it in production.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You can also use the testing techniques discussed in <a data-type="xref" href="ch11.html#ch11">Chapter 11</a> when evaluating fine-tuned models.</p>
</div>
<div data-type="example" id="fine_tuning_usage">
<h5><span class="label">Example 10-20. </span>Using a fine-tuned model</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">OpenAI</code>
<code class="n">client</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">()</code>

<code class="n">fine_tuning_job_id</code> <code class="o">=</code> <code class="s2">"ftjob-abc123"</code>
<code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">fine_tuning</code><code class="o">.</code><code class="n">jobs</code><code class="o">.</code><code class="n">retrieve</code><code class="p">(</code><code class="n">fine_tuning_job_id</code><code class="p">)</code>
<code class="n">fine_tuned_model</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">fine_tuned_model</code>

<code class="k">if</code> <code class="n">fine_tuned_model</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">:</code>
    <code class="k">raise</code> <code class="ne">ValueError</code><code class="p">(</code>
        <code class="sa">f</code><code class="s2">"Failed to retrieve the fine-tuned model - "</code>
        <code class="sa">f</code><code class="s2">"Job ID: </code><code class="si">{</code><code class="n">fine_tuning_job_id</code><code class="si">}</code><code class="s2">"</code>
    <code class="p">)</code>

<code class="n">completion</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="n">fine_tuned_model</code><code class="p">,</code>
    <code class="n">messages</code><code class="o">=</code><code class="p">[</code>
        <code class="p">{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code> <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"You are a helpful assistant."</code><code class="p">},</code>
        <code class="p">{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code> <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"Hello!"</code><code class="p">},</code>
    <code class="p">],</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">completion</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="p">)</code></pre></div>

<p>While these examples show the fine-tuning process with OpenAI, the process will be similar with other providers even if the implementation details may differ.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If you decide to leverage fine-tuning, be mindful that you won’t be able to take advantage of the latest improvements or optimizations in new LLMs, potentially making the fine-tuning process a waste of your time and money.</p>
</div>

<p>With this final optimization step, you should now feel confident in building GenAI services that not only meet your security and quality requirements but also achieve your desired throughput and latency metrics<a data-startref="ix_ch10-asciidoc47" data-type="indexterm" id="id1137"/><a data-startref="ix_ch10-asciidoc46" data-type="indexterm" id="id1138"/>.<a data-startref="ix_ch10-asciidoc43" data-type="indexterm" id="id1139"/><a data-startref="ix_ch10-asciidoc42" data-type="indexterm" id="id1140"/></p>
</div></section>
</div></section>
</div></section>






<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id170">
<h1>Summary</h1>

<p>In this chapter, you learned about several optimization strategies to improve the throughput and quality of your services.
A few optimizations you added covered various caching (keyword, semantic, context), prompt engineering, model quantization, and fine-tuning.<a data-startref="ix_ch10-asciidoc0" data-type="indexterm" id="id1141"/></p>

<p>In the next chapter, we will shift focus to the last step in building AI services: deploying your GenAI solution.
This includes exploring deployment patterns for AI services and containerization with Docker.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1069"><sup><a href="ch10.html#id1069-marker">1</a></sup> See the OpenAI Batch API available in the <a href="https://oreil.ly/0t59w">OpenAI API documentation</a>.</p><p data-type="footnote" id="id1072"><sup><a href="ch10.html#id1072-marker">2</a></sup> Learn more about cache control headers at the <a href="https://oreil.ly/-Y5JP">MDN website</a>.</p><p data-type="footnote" id="id1076"><sup><a href="ch10.html#id1076-marker">3</a></sup> You may still require a trained embedder model for significant cost savings, as making frequent API calls to an off-the-shelf embedder model could incur additional costs, diminishing your overall savings.</p><p data-type="footnote" id="id1128"><sup><a href="ch10.html#id1128-marker">4</a></sup> For better security, you still need to sanitize any LLM-generated code before forwarding it to downstream systems for execution.</p></div></div></section></body></html>