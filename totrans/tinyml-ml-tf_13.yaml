- en: Chapter 13\. TensorFlow Lite for Microcontrollers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we look at the software framework we’ve been using for all
    of the examples in the book: TensorFlow Lite for Microcontrollers. We go into
    a lot of detail, but you don’t need to understand everything we cover to use it
    in an application. If you’re not interested in what’s happening under the hood,
    feel free to skip this chapter; you can always return to it when you have questions.
    If you do want to better understand the tool you’re using to run machine learning,
    we cover the history and inner workings of the library here.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: What Is TensorFlow Lite for Microcontrollers?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first question you might ask is what the framework actually does. To understand
    that, it helps to break the (rather long) name down a bit and explain the components.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may well have heard of TensorFlow itself if you’ve looked into machine learning.
    [TensorFlow](https://tensorflow.org) is Google’s open source machine learning
    library, with the motto “An Open Source Machine Learning Framework for Everyone.”
    It was developed internally at Google and first released to the public in 2015\.
    Since then a large external community has grown up around the software, with more
    contributors outside Google than inside. It’s aimed at Linux, Windows, and macOS
    desktop and server platforms and offers a lot of tools, examples, and optimizations
    around training and deploying models in the cloud. It’s the main machine learning
    library used within Google to power its products, and the core code itself is
    the same across the internal and published versions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: There are also a large number of examples and tutorials available from Google
    and other sources. These can show you how to train and use models for everything
    from speech recognition to data center power management or video analysis.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: The biggest need when TensorFlow was launched was the ability to train models
    and run them in desktop environments. This influenced a lot of the design decisions,
    such as trading the size of the executable for lower latency and more functionality—on
    a cloud server where even RAM is measured in gigabytes and there are terabytes
    of storage space, having a binary that’s a couple of hundred megabytes in size
    is not a problem. Another example is that its main interface language at launch
    was Python, a scripting language widely used on servers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: These engineering trade-offs weren’t as appropriate for other platforms, though.
    On Android and iPhone devices, adding even a few megabytes to the size of an app
    can decrease the number of downloads and customer satisfaction dramatically. You
    can build TensorFlow for these phone platforms, but by default it adds 20 MB to
    the application size, and even with some work never shrinks below 2 MB.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Lite
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To meet these lower size requirements for mobile platforms, in 2017 Google started
    a companion project to mainline TensorFlow called TensorFlow Lite. This library
    is aimed at running neural network models efficiently and easily on mobile devices.
    To reduce the size and complexity of the framework, it drops features that are
    less common on these platforms. For example, it doesn’t support training, just
    running inference on models that were previously trained on a cloud platform.
    It also doesn’t support the full range of data types (such as `double`) available
    in mainline TensorFlow. Additionally, some less-used operations aren’t present,
    like `tf.depth_to_space`. You can find the latest compatibility information on
    the [TensorFlow website](https://oreil.ly/otEIp).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In return for these trade-offs, TensorFlow Lite can fit within just a few hundred
    kilobytes, making it much easier to fit into a size-constrained application. It
    also has highly optimized libraries for Arm Cortex-A-series CPUs, along with support
    for Android’s Neural Network API for accelerators, and GPUs through OpenGL. Another
    key advantage is that it has good support for 8-bit quantization of networks.
    Because a model might have millions of parameters, the 75% size reduction from
    32-bit floats to 8-bit integers alone makes it worthwhile, but there are also
    specialized code paths that allow inference to run much faster on the smaller
    data type.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这些折衷的回报，TensorFlow Lite可以适应几百千字节，使其更容易适应大小受限的应用程序。它还具有针对Arm Cortex-A系列CPU高度优化的库，以及通过OpenGL支持Android的神经网络API的加速器和GPU。另一个关键优势是它对网络的8位量化有很好的支持。因为一个模型可能有数百万个参数，仅从32位浮点数到8位整数的75%大小减少就是值得的，但还有专门的代码路径，使得推断在较小的数据类型上运行得更快。
- en: TensorFlow Lite for Microcontrollers
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微控制器的TensorFlow Lite
- en: TensorFlow Lite has been widely adopted by mobile developers, but its engineering
    trade-offs didn’t meet the requirements of all platforms. The team noticed that
    there were a lot of Google and external products that could benefit from machine
    learning being build on embedded platforms, on which the existing TensorFlow Lite
    library wouldn’t fit. Again, the biggest constraint was binary size. For these
    environments even a few hundred kilobytes was too large; they needed something
    that would fit within 20 KB or less. A lot of the dependencies that mobile developers
    take for granted, like the C Standard Library, weren’t present either, so no code
    that relied on these libraries could be used. A lot of the requirements were very
    similar, though. Inference was the primary use case, quantized networks were important
    for performance, and having a code base that was simple enough for developers
    to explore and modify was a priority.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite已被移动开发人员广泛采用，但其工程折衷并不符合所有平台的要求。团队注意到有很多谷歌和外部产品可以从在嵌入式平台上构建的机器学习中受益，而现有的TensorFlow
    Lite库则不适用。再次，最大的限制是二进制大小。对于这些环境来说，即使几百千字节也太大了；他们需要适合在20 KB或更小范围内的东西。许多移动开发人员认为理所当然的依赖项，如C标准库，也不存在，因此不能使用依赖于这些库的代码。然而，许多要求非常相似。推断是主要用例，量化网络对性能很重要，并且具有足够简单以供开发人员探索和修改的代码库是首要任务。
- en: With those needs in mind, in 2018 a team at Google (including the authors of
    this book) started experimenting with a specialized version of TensorFlow Lite
    aimed just at these embedded platforms. The goal was to reuse as much of the code,
    tooling, and documentation from the mobile project as possible, while satisfying
    the tough requirements of embedded environments. To make sure Google was building
    something practical, the team focused on the real-world use case of recognizing
    a spoken “wake word,” similar to the “Hey Google” or “Alexa” examples from commercial
    voice interfaces. Aiming at an end-to-end example of how to tackle this problem,
    Google worked to ensure the system we designed was usable for production systems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些需求，2018年，谷歌团队（包括本书的作者）开始尝试专门针对这些嵌入式平台的TensorFlow Lite的特殊版本。目标是尽可能重用移动项目中的代码、工具和文档，同时满足嵌入式环境的严格要求。为了确保谷歌正在构建实用的东西，团队专注于识别口头“唤醒词”的真实用例，类似于商业语音界面中的“Hey
    Google”或“Alexa”示例。旨在提供一个端到端的示例来解决这个问题，谷歌努力确保我们设计的系统适用于生产系统。
- en: Requirements
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 要求
- en: 'The Google team knew that running in embedded environments imposed a lot of
    constraints on how the code could be written, so it identified some key requirements
    for the library:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌团队知道在嵌入式环境中运行对代码编写有很多限制，因此确定了库的一些关键要求：
- en: No operating system dependencies
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 没有操作系统依赖
- en: A machine learning model is fundamentally a mathematical black box where numbers
    are fed in, and numbers are returned as the results. Access to the rest of the
    system shouldn’t be necessary to perform these operations, so it’s possible to
    write a machine learning framework without calls to the underlying operating system.
    Some of the targeted platforms don’t have an OS at all, and avoiding any references
    to files or devices in the basic code made it possible to port to those chips.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型基本上是一个数学黑匣子，其中输入数字，输出结果也是数字。执行这些操作不需要访问系统的其余部分，因此可以编写一个不调用底层操作系统的机器学习框架。一些目标平台根本没有操作系统，避免在基本代码中引用文件或设备使得可以将其移植到这些芯片上。
- en: No standard C or C++ library dependencies at linker time
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在链接时没有标准的C或C++库依赖
- en: This is a bit subtler than the OS requirement, but the team was aiming to deploy
    on devices that might have only a few tens of kilobytes of memory to store a program,
    so the binary size was very important. Even apparently simple functions like `sprintf()`
    can easily take up 20 KB by themselves, so the team aimed to avoid anything that
    had to be pulled in from the library archives that hold the implementations of
    the C and C++ standard libraries. This was tricky because there’s no well-defined
    boundary between header-only dependencies (like *stdint.h*, which holds the sizes
    of data types) and linker-time parts of the standard libraries (such as many string
    functions or `sprintf()`. In practice the team had to use some common sense to
    understand that, generally, compile-time constants and macros were fine, but anything
    more complex should be avoided. The one exception to this linker avoidance is
    the standard C `math` library, which is relied on for things like trigonometric
    functions that do need to be linked in.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这比操作系统要求更微妙一些，但团队的目标是部署在可能只有几十KB内存来存储程序的设备上，因此二进制大小非常重要。即使看似简单的函数如`sprintf()`本身可能就需要20KB的空间，因此团队的目标是避免从包含C和C++标准库实现的库存档案中提取任何内容。这很棘手，因为头文件依赖（如*stdint.h*，其中包含数据类型的大小）和标准库的链接时部分（如许多字符串函数或`sprintf()`）之间没有明确定义的边界。实际上，团队必须运用一些常识来理解，通常情况下，编译时常量和宏是可以接受的，但应避免使用更复杂的内容。唯一的例外是标准C
    `math`库，它被用于需要链接的三角函数等功能。
- en: No floating-point hardware expected
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要浮点硬件
- en: Many embedded platforms don’t have support for floating-point arithmetic in
    hardware, so the code had to avoid any performance-critical uses of floats. This
    meant focusing on models with 8-bit integer parameters, and using 8-bit arithmetic
    within operations (though for compatibility the framework also supports float
    ops if they’re needed).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多嵌入式平台不支持硬件浮点运算，因此代码必须避免对浮点数的性能关键使用。这意味着专注于具有8位整数参数的模型，并在操作中使用8位算术（尽管为了兼容性，该框架还支持浮点运算，如果需要的话）。
- en: No dynamic memory allocation
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不支持动态内存分配
- en: A lot of applications using microcontrollers need to run continuously for months
    or years. If the main loop of a program is allocating and deallocating memory
    using `malloc()`/`new` and `free()`/`delete`, it’s very difficult to guarantee
    that the heap won’t eventually end up in a fragmented state, causing an allocation
    failure and a crash. There’s also very little memory available on most embedded
    systems, so upfront planning of this limited resource is more important than on
    other platforms, and without an OS there might not even be a heap and allocation
    routines. This means that embedded applications often avoid using dynamic memory
    allocation entirely. Because the library was designed to be used by those applications,
    it needed do the same. In practice the framework asks the calling application
    to pass in a small, fixed-size arena that the framework can use for temporary
    allocations (like activation buffers) at initialization time. If the arena is
    too small, the library will return an error immediately and the client will need
    to recompile with a larger arena. Otherwise, the calls to perform inference happen
    with no further memory allocations, so they can be made repeatedly with no risk
    of heap fragmentation or memory errors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 许多使用微控制器的应用程序需要连续运行数月或数年。如果程序的主循环使用`malloc()`/`new`和`free()`/`delete`来分配和释放内存，很难保证堆最终不会处于碎片化状态，导致分配失败和崩溃。大多数嵌入式系统上可用的内存非常有限，因此提前规划这种有限资源比其他平台更为重要，而且没有操作系统可能甚至没有堆和分配例程。这意味着嵌入式应用程序通常完全避免使用动态内存分配。因为该库是为这些应用程序设计的，所以它也需要这样做。实际上，该框架要求调用应用程序在初始化时传入一个小型、固定大小的区域，框架可以在其中进行临时分配（如激活缓冲区）。如果区域太小，库将立即返回错误，客户端需要重新编译以使用更大的区域。否则，进行推理调用时不会有进一步的内存分配，因此可以反复进行，而不会出现堆碎片化或内存错误的风险。
- en: 'The team also decided against some other constraints that are common in the
    embedded community because they would make sharing code and maintaining compatibility
    with mobile TensorFlow Lite too difficult. Therefore:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 团队还决定不采用嵌入式社区中常见的其他一些约束，因为这将使共享代码和与移动TensorFlow Lite的兼容性维护变得太困难。因此：
- en: It requires C++11
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 它需要C++11
- en: It’s common to write embedded programs in C, and some platforms don’t have toolchains
    that support C++ at all, or support older versions than the 2011 revision of the
    standard. TensorFlow Lite is mostly written in C++, with some plain C APIs, which
    makes calling it from other languages easier. It doesn’t rely on advanced features
    like complex templates; its style is in the spirit of a “better C” with classes
    to help modularize the code. Rewriting the framework in C would have taken a lot
    of work and been a step backward for users on mobile platforms, and when we surveyed
    the most popular platforms we found, they all had C++11 support already, so the
    team decided to trade support for older devices against making it easier to share
    code across all flavors of TensorFlow Lite.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在C中编写嵌入式程序很常见，有些平台根本不支持C++，或者支持的版本比2011年的标准修订版旧。TensorFlow Lite主要是用C++编写的，具有一些纯C
    API，这使得从其他语言调用它更容易。它不依赖于复杂的模板等高级功能；其风格是“更好的C”，使用类来帮助模块化代码。将框架重写为C将需要大量工作，并且对于移动平台上的用户来说是一种倒退，当我们调查最受欢迎的平台时，我们发现，它们都已经支持C++11，因此团队决定牺牲对旧设备的支持，以使代码更容易在所有版本的TensorFlow
    Lite之间共享。
- en: It expects 32-bit processors
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 它需要32位处理器
- en: There are a massive number of different hardware platforms available in the
    embedded world, but the trend in recent years has been toward 32-bit processors,
    rather than the 16-bit or 8-bit chips that used to be common. After surveying
    the ecosystem, Google decided to focus its development on the newer 32-bit devices
    because that kept assumptions like the C `int` data type being 32 bits the same
    across mobile and embedded versions of the framework. We have had reports of successful
    ports to some 16-bit platforms, but these rely on modern toolchains that compensate
    for the limitations, and are not our main priority.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式世界中有大量不同的硬件平台可用，但近年来的趋势是向32位处理器发展，而不是以前常见的16位或8位芯片。在调查了生态系统之后，Google决定将开发重点放在更新的32位设备上，因为这样可以保持假设，例如C
    `int`数据类型为32位，这样可以使移动和嵌入式版本的框架保持一致。我们已经收到了一些成功移植到一些16位平台的报告，但这些平台依赖于弥补限制的现代工具链，并不是我们的主要重点。
- en: Why Is the Model Interpreted?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么要解释模型？
- en: One question that comes up a lot is why we chose to interpret models at runtime
    rather than doing code generation from a model ahead of time. Explaining that
    decision involves teasing apart some of the benefits and problems of the different
    approaches involved.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 经常出现的一个问题是，为什么我们选择在运行时解释模型，而不是提前从模型生成代码。解释该决定涉及分析涉及的不同方法的一些好处和问题。
- en: 'Code generation involves converting a model directly into C or C++ code, with
    all of the parameters stored as data arrays in the code and the architecture expressed
    as a series of function calls that pass activations from one layer to the next.
    This code is often output into a single large source file with a handful of entry
    points. That file can then be included in an IDE or toolchain directly, and compiled
    like any other code. Here are a few of the key advantages of code generation:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成涉及将模型直接转换为C或C++代码，其中所有参数都存储为代码中的数据数组，架构表示为一系列函数调用，这些函数调用将激活从一层传递到下一层。这些代码通常输出到一个单独的大型源文件中，其中包含少量入口点。然后可以直接将该文件包含在IDE或工具链中，并像任何其他代码一样进行编译。以下是代码生成的一些关键优势：
- en: Ease of building
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 易于构建
- en: Users told us the number one benefit was how easy it makes integrating into
    build systems. If all you have is a few C or C++ files, with no external library
    dependencies, you can easily drag and drop them into almost any IDE and get a
    project built with few chances for things to go wrong.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 用户告诉我们，最大的好处是它有多么容易集成到构建系统中。如果您只有几个C或C++文件，没有外部库依赖项，您可以轻松地将它们拖放到几乎任何IDE中，并构建一个项目，几乎没有出错的机会。
- en: Modifiability
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 可修改性
- en: When you have a small amount of code in a single implementation file, it’s much
    simpler to step through and change the code if you need to, at least compared
    to a large library for which you first need to establish what implementations
    are even being used.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当您有少量代码在单个实现文件中时，如果需要，通过代码进行步进和更改会更简单，至少与首先需要确定哪些实现正在使用的大型库相比是如此。
- en: Inline data
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 内联数据
- en: The data for the model itself can be stored as part of the implementation source
    code, so no additional files are required. It can also be stored directly as an
    in-memory data structure, so no loading or parsing step is required.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 模型本身的数据可以存储为实现源代码的一部分，因此不需要额外的文件。它也可以直接存储为内存中的数据结构，因此不需要加载或解析步骤。
- en: Code size
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 代码大小
- en: If you know what model and platform you’re building for ahead of time, you can
    avoid including code that will never be called, so the size of the program segment
    can be kept minimal.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您提前知道要构建的模型和平台，可以避免包含永远不会被调用的代码，因此可以保持程序段的大小最小化。
- en: 'Interpreting a model is a different approach, and relies on loading a data
    structure that defines the model. The executed code is static; only the model
    data changes, and the information in the model controls which operations are executed
    and where parameters are drawn from. This is more like running a script in an
    interpreted language like Python, whereas you can see code generation as being
    closer to traditional compiled languages like C. Here are some of the drawbacks
    of code generation, compared to interpreting a model data structure:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 解释模型是一种不同的方法，依赖于加载定义模型的数据结构。执行的代码是静态的；只有模型数据发生变化，模型中的信息控制执行哪些操作以及从哪里提取参数。这更像是在解释语言（如Python）中运行脚本，而将代码生成视为更接近传统编译语言（如C）。以下是与解释模型数据结构相比，代码生成的一些缺点：
- en: Upgradability
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 可升级性
- en: What happens if you’ve locally modified the generated code but you want to upgrade
    to a newer version of the overall framework to get new functionality or optimizations?
    You’ll either need to manually cherry-pick changes into your local files or regenerate
    them entirely and try to patch back in your local changes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在本地修改了生成的代码，但想要升级到整体框架的新版本以获得新功能或优化，会发生什么？您要么需要手动将更改挑选到本地文件中，要么完全重新生成它们，然后尝试将本地更改补丁回去。
- en: Multiple models
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多个模型
- en: It’s difficult to support more than one model at a time through code generation
    without a lot of source duplication.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过代码生成很难支持多个模型，而不会有大量源代码重复。
- en: Replacing models
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 替换模型
- en: Each model is expressed as a mixture of source code and data arrays within the
    program, so it’s difficult to change the model without recompiling the entire
    program.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都表示为程序中源代码和数据数组的混合，因此很难在不重新编译整个程序的情况下更改模型。
- en: What the team realized was that it’s possible to get a lot of the benefits of
    code generation, without incurring the drawbacks, using what we term *project
    generation*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 团队意识到的是，可以通过使用我们所谓的*项目生成*来获得代码生成的许多好处，而不会遇到缺点。
- en: Project Generation
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目生成
- en: 'In TensorFlow Lite, project generation is a process that creates a copy of
    just the source files you need to build a particular model, without making any
    changes to them, and also optionally sets up any IDE-specific project files so
    that they can be built easily. It retains most of the benefits of code generation,
    but it has some key advantages:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow Lite中，项目生成是一个过程，它创建了构建特定模型所需的源文件副本，而不对其进行任何更改，并且还可以选择设置任何特定于IDE的项目文件，以便可以轻松构建。它保留了大部分代码生成的好处，但它具有一些关键优势：
- en: Upgradability
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 可升级性
- en: All of the source files are just copies of originals from the main TensorFlow
    Lite code base, and they appear in the same location in the folder hierarchy,
    so if you make local modifications they can easily be ported back to the original
    source, and library upgrades can be merged simply using standard merge tools.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的源文件都只是主要TensorFlow Lite代码库中原始文件的副本，并且它们出现在文件夹层次结构中的相同位置，因此如果您进行本地修改，可以轻松地将其移植回原始源，并且可以简单地使用标准合并工具合并库升级。
- en: Multiple and replacement models
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 多个和替换模型
- en: The underlying code is an interpreter, so you can have more than one model or
    swap out a data file easily without recompiling.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 底层代码是一个解释器，因此您可以拥有多个模型或轻松更换数据文件而无需重新编译。
- en: Inline data
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 内联数据
- en: The model parameters themselves can still be compiled into the program as a
    C data array if needed, and the use of the FlatBuffers serialization format means
    that this representation can be used directly in memory with no unpacking or parsing
    required.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，模型参数本身仍然可以编译到程序中作为C数据数组，并且使用FlatBuffers序列化格式意味着这种表示可以直接在内存中使用，无需解包或解析。
- en: External dependencies
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 外部依赖
- en: All of the header and source files required to build the project are copied
    into the folder alongside the regular TensorFlow code, so no dependencies need
    to be downloaded or installed separately.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 构建项目所需的所有头文件和源文件都复制到与常规TensorFlow代码相邻的文件夹中，因此不需要单独下载或安装任何依赖项。
- en: The biggest advantage that doesn’t come automatically is code size, because
    the interpreter structure makes it more difficult to spot code paths that will
    never be called. This is addressed separately in TensorFlow Lite by manually using
    the `OpResolver` mechanism to register only the kernel implementations that you
    expect to use in your application.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的优势并不是自动获得的代码大小，因为解释器结构使得更难以发现永远不会被调用的代码路径。在TensorFlow Lite中，通过手动使用`OpResolver`机制来注册您在应用程序中期望使用的内核实现，可以单独解决这个问题。
- en: Build Systems
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建系统
- en: TensorFlow Lite was originally developed in a Linux environment, so a lot of
    our tooling is based around traditional Unix tools like shell scripts, Make, and
    Python. We know that’s not a common combination for embedded developers, though,
    so we aim to support other platforms and compilation toolchains as first-class
    citizens.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite最初是在Linux环境中开发的，因此我们的许多工具基于传统的Unix工具，如shell脚本、Make和Python。我们知道这对于嵌入式开发人员来说并不常见，因此我们旨在支持其他平台和编译工具链作为一流公民。
- en: 'The way we do that is through the aforementioned project generation. If you
    grab the TensorFlow source code from GitHub, you can build for a lot of platforms
    using a standard Makefile approach on Linux. For example, this command line should
    compile and test an x86 version of the library:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过上述项目生成来实现这一点。如果您从GitHub获取TensorFlow源代码，可以使用Linux上的标准Makefile方法为许多平台构建。例如，这个命令行应该编译和测试库的x86版本：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can build a specific target, like the speech wake-word example for the
    SparkFun Edge platform, with a command like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以构建特定目标，比如为SparkFun Edge平台构建语音唤醒示例，使用以下命令：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'What if you’re running on a Windows machine or want to use an IDE like Keil,
    Mbed, Arduino, or another specialized build system? That’s where the project generation
    comes in. You can generate a folder that’s ready to use with the Mbed IDE by running
    the following command line from Linux:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Windows机器上运行或想要使用Keil、Mbed、Arduino或其他专门的构建系统，那么项目生成就派上用场了。您可以通过在Linux上运行以下命令行来生成一个准备在Mbed
    IDE中使用的文件夹：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You should now see a set of source files in *tensorflow/lite/micro/tools/make/gen/disco_f746ng_x86_64/prj/micro_speech/mbed/*,
    along with all the dependencies and project files you need to build within the
    Mbed environment. The same approach works for Keil and Arduino, and there’s a
    generic version that just outputs the folder hierarchy of source files without
    project metainformation (though it does include a Visual Studio Code file that
    defines a couple of build rules).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该在*tensorflow/lite/micro/tools/make/gen/disco_f746ng_x86_64/prj/micro_speech/mbed/*中看到一组源文件，以及在Mbed环境中构建所需的所有依赖项和项目文件。同样的方法适用于Keil和Arduino，还有一个通用版本，只输出源文件的文件夹层次结构，不包括项目元信息（尽管它包括一个定义了一些构建规则的Visual
    Studio Code文件）。
- en: You might be wondering how this Linux command-line approach helps people on
    other platforms. We automatically run this project-generation process as part
    of our nightly continuous integration workflow and whenever we do a major release.
    Whenever it’s run, it automatically puts the resulting files up on a public web
    server. This means that users on all platforms should be able to find a version
    for their preferred IDE, and download the project as a self-contained folder instead
    of through GitHub.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道这种Linux命令行方法如何帮助其他平台上的用户。我们会自动将此项目生成过程作为我们每晚的持续集成工作流的一部分以及每次进行重大发布时运行。每次运行时，它会自动将生成的文件放在公共Web服务器上。这意味着所有平台上的用户应该能够找到适合其首选IDE的版本，并且可以下载该项目作为一个独立的文件夹，而不是通过GitHub。
- en: Specializing Code
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专门化代码
- en: One of the benefits of code generation is that it’s easy to rewrite part of
    the library to work well on a particular platform, or even just optimize a function
    for a particular set of parameters that you know are common in your use case.
    We didn’t want to lose this ease of modification, but we also wanted to make it
    as easy as possible for more generally useful changes to be merged back into the
    main framework’s source code. We had the additional constraint that some build
    environments don’t make it easy to pass in custom `#define` macros during compilation,
    so we couldn’t rely on switching to different implementations at compile time
    using macro guards.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成的一个好处是很容易重写库的部分，使其在特定平台上运行良好，甚至只是针对你知道在你的用例中很常见的一组特定参数进行函数优化。我们不想失去这种修改的便利性，但我们也希望尽可能地使更普遍有用的更改能够轻松地合并回主框架的源代码中。我们还有一个额外的约束条件，即一些构建环境在编译过程中不容易传递自定义的`#define`宏，因此我们不能依赖于在编译时使用宏保护切换到不同的实现。
- en: To solve this problem we’ve broken the library into small modules, each of which
    has a single C++ file implementing a default version of its functionality, along
    with a C++ header that defines the interface that other code can call to use the
    module. We then adopted a convention that if you want to write a specialized version
    of a module, you save your new version out as a C++ implementation file with the
    same name as the original but in a subfolder of the directory that the original
    is in. This subfolder should have the name of the platform or feature you’re specializing
    for (see [Figure 13-1](#specialize_screenshot)), and will be automatically used
    by the Makefile or generated projects instead of the original implementation when
    you’re building for that platform or feature. This probably sounds pretty complicated,
    so let’s walk through a couple of concrete examples.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将库拆分为小模块，每个模块都有一个实现其功能的单个C++文件，以及一个定义其他代码可以调用以使用该模块的接口的C++头文件。然后我们采用了一个约定，如果您想编写一个模块的专门版本，您将您的新版本保存为与原始文件同名但在原始文件所在目录的子文件夹中的C++实现文件。这个子文件夹应该有您专门为其进行特化的平台或功能的名称（参见[图13-1](#specialize_screenshot)），并且在为该平台或功能构建时将自动使用Makefile或生成的项目而不是原始实现。这可能听起来很复杂，所以让我们通过几个具体的例子来解释一下。
- en: 'The speech wake-word sample code needs to grab audio data from a microphone,
    but unfortunately there’s no cross-platform way to capture audio. Because we need
    to at least compile across a wide range of devices, we wrote a default implementation
    that just returns a buffer full of zero values, without using a microphone. Here’s
    what the interface to that module looks like, from [*audio_provider.h*](https://oreil.ly/J5N0N):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 语音唤醒词示例代码需要从麦克风中获取音频数据，但不幸的是没有跨平台的方法来捕获音频。因为我们至少需要在各种设备上进行编译，所以我们编写了一个默认实现，它只返回一个充满零值的缓冲区，而不使用麦克风。以下是该模块的接口是什么样子的，来自[*audio_provider.h*](https://oreil.ly/J5N0N)：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Screenshot of a specialized audio provider file](Images/timl_1301.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![专门音频提供者文件的截图](Images/timl_1301.png)'
- en: Figure 13-1\. Screenshot of a specialized audio provider file
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-1\. 专门音频提供者文件的截图
- en: The first function outputs a buffer filled with audio data for a given time
    period, returning an error if something goes wrong. The second function returns
    when the most recent audio data was captured, so the client can ask for the correct
    range of time, and know when new data has arrived.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数为给定时间段输出填充有音频数据的缓冲区，如果出现问题则返回错误。第二个函数返回最近捕获到的音频数据的时间戳，因此客户端可以请求正确的时间范围，并知道何时有新数据到达。
- en: 'Because the default implementation can’t rely on a microphone being present,
    the implementations of the two functions in [*audio_provider.cc*](https://oreil.ly/8V1Ll)
    are very simple:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因为默认实现不能依赖于麦克风的存在，所以[*audio_provider.cc*](https://oreil.ly/8V1Ll)中的两个函数的实现非常简单：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The timestamp is incremented automatically every time the function is called,
    so that clients will behave as if new data were coming in, but the same array
    of zeros is returned every time by the capture routine. The benefit of this is
    that it allows you to prototype and experiment with the sample code even before
    you have a microphone working on a system. `kMaxAudioSampleSize` is defined in
    the model header and is the largest number of samples that the function will ever
    be asked for.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳在每次调用函数时自动递增，以便客户端表现得好像有新数据进来，但捕获例程每次都返回相同的零数组。这样做的好处是可以让您在系统上的麦克风工作之前就可以对示例代码进行原型设计和实验。`kMaxAudioSampleSize`在模型头文件中定义，是函数将被要求的最大样本数。
- en: 'On a real device the code needs to be a lot more complex, so we need a new
    implementation. Earlier, we compiled this example for the STM32F746NG Discovery
    kit board, which has microphones built in and uses a separate Mbed library to
    access them. The code is in [*disco_f746ng/audio_provider.cc*](https://oreil.ly/KrdSO).
    It’s not included inline here because it’s too big, but if you look at that file,
    you’ll see it implements the same two public functions as the default *audio_provider.cc*:
    `GetAudioSamples()` and `LatestAudioTimestamp()`. The definitions of the functions
    are a lot more complex, but their behavior from a client’s perspective is the
    same. The complexity is hidden, and the calling code can remain the same despite
    the change in platform—and now, instead of receiving an array of zeros every time,
    captured audio will show up in the returned buffer.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实设备上，代码需要更复杂，因此我们需要一个新的实现。早些时候，我们为STM32F746NG Discovery kit开发板编译了这个示例，该开发板内置了麦克风，并使用单独的Mbed库来访问它们。代码在[*disco_f746ng/audio_provider.cc*](https://oreil.ly/KrdSO)中。这里没有内联包含它，因为它太大了，但如果您查看该文件，您会看到它实现了与默认*audio_provider.cc*相同的两个公共函数：`GetAudioSamples()`和`LatestAudioTimestamp()`。这些函数的定义要复杂得多，但从客户端的角度来看，它们的行为是相同的。复杂性被隐藏起来，调用代码可以保持不变，尽管平台发生了变化，现在，而不是每次都接收到一个零数组，捕获的音频将显示在返回的缓冲区中。
- en: If you look at the full path of this specialized implementation, *tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc*,
    you’ll see it’s almost identical to that of the default implementation at *tensorflow/lite/micro/examples/micro_speech/audio_provider.cc*,
    but it’s inside a *disco_f746ng* subfolder at the same level as the original *.cc*
    file. If you look back at the command line for building the STM32F746NG Mbed project,
    you’ll see we passed in `TARGET=disco_f746ng` to specify what platform we want.
    The build system always looks for *.cc* files in subfolders with the target name
    for possible specialized implementations, so in this case *disco_f746ng/audio_provider.cc*
    is used instead of the default *audio_provider.cc* version in the parent folder.
    When the source files are being assembled for the Mbed project copy, that parent-level
    *.cc* file is ignored, and the one in the subfolder is copied over; thus, the
    specialized version is used by the resulting project.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看这个专门实现的完整路径，*tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc*，你会发现它几乎与默认实现的*tensorflow/lite/micro/examples/micro_speech/audio_provider.cc*相同，但它位于与原始*.cc*文件相同级别的*disco_f746ng*子文件夹内。如果你回顾一下用于构建STM32F746NG
    Mbed项目的命令行，你会看到我们传入了`TARGET=disco_f746ng`来指定我们想要的平台。构建系统总是在目标名称的子文件夹中寻找*.cc*文件，以便可能的专门实现，因此在这种情况下，*disco_f746ng/audio_provider.cc*被用来代替父文件夹中的默认*audio_provider.cc*版本。在为Mbed项目复制源文件时，会忽略父级*.cc*文件，并复制子文件夹中的文件；因此，生成的项目将使用专门版本。
- en: Capturing audio is done differently on almost every platform, so we have a lot
    of different specialized implementations of this module. There’s even a macOS
    version, [*osx/audio_provider.cc*](https://oreil.ly/ZaMtF), which is useful if
    you’re debugging locally on a Mac laptop.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在几乎每个平台上，音频捕获的方式都不同，因此我们有许多不同的专门实现这个模块。甚至还有一个macOS版本，[*osx/audio_provider.cc*](https://oreil.ly/ZaMtF)，如果你在Mac笔记本上本地调试，这将非常有用。
- en: This mechanism isn’t just used for portability, though; it’s also flexible enough
    to use for optimizations. We actually use this approach in the speech wake-word
    example to help speed up the depthwise convolution operation. If you look in [*tensorflow/lite/micro/kernels*](https://oreil.ly/0yHNd)
    you’ll see implementations of all the operations that TensorFlow Lite for Microcontrollers
    supports. These default implementations are written to be short, be easy to understand,
    and run on any platform, but meeting those goals means that they often miss opportunities
    to run as fast as they could. Optimization usually involves making the algorithms
    more complicated and more difficult to understand, so these reference implementations
    are expected to be comparatively slow. The idea is that we want to enable developers
    to get code running in the simplest possible way first and ensure that they’re
    getting correct results, and then be able to incrementally change the code to
    improve performance. This means that every small change can be tested to make
    sure it doesn’t break correctness, making debugging much easier.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这种机制不仅用于可移植性，还足够灵活以用于优化。实际上，我们在语音唤醒词示例中使用这种方法来加速深度卷积操作。如果你查看[*tensorflow/lite/micro/kernels*](https://oreil.ly/0yHNd)，你会看到TensorFlow
    Lite for Microcontrollers支持的所有操作的实现。这些默认实现被设计为简短、易于理解，并在任何平台上运行，但是为了达到这些目标，它们通常会错过提高运行速度的机会。优化通常涉及使算法更复杂、更难理解，因此这些参考实现预计会相对较慢。我们的想法是要让开发人员能够以最简单的方式运行代码，并确保他们获得正确的结果，然后逐步更改代码以提高性能。这意味着每个小改变都可以进行测试，以确保它不会破坏正确性，从而使调试变得更加容易。
- en: 'The model used in the speech wake-word example relies heavily on the depthwise
    convolution operation, which has an unoptimized implementation at [*tensorflow/lite/micro/kernels/depthwise_conv.cc*](https://oreil.ly/a16dw).
    The core algorithm is implemented in [*tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h*](https://oreil.ly/2gQ-e),
    and it’s written as a straightforward set of nested loops. Here’s the code itself:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 语音唤醒词示例中使用的模型严重依赖深度卷积操作，该操作在[*tensorflow/lite/micro/kernels/depthwise_conv.cc*](https://oreil.ly/a16dw)中有一个未经优化的实现。核心算法在[*tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h*](https://oreil.ly/2gQ-e)中实现，并被写成一组嵌套循环。以下是代码本身：
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You might be able to see lots of opportunities to speed this up just from a
    quick look, like precalculating all the array indices that we figure out every
    time in the inner loop. Those changes would add to the complexity of the code,
    so for this reference implementation we’ve avoided them. The speech wake-word
    example needs to run multiple times a second on a microcontroller, though, and
    it turns out that this naive implementation is the main speed bottleneck preventing
    that on the SparkFun Edge Cortex-M4 processor. To make the example run at a usable
    speed, we needed to add some optimizations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会从快速查看中看到许多加速的机会，比如在内部循环中每次计算的所有数组索引都预先计算出来。这些改变会增加代码的复杂性，因此对于这个参考实现，我们避免了它们。然而，语音唤醒词示例需要在微控制器上多次运行，结果发现这种朴素的实现是阻碍SparkFun
    Edge Cortex-M4处理器实现这一目标的主要速度瓶颈。为了使示例以可用的速度运行，我们需要添加一些优化。
- en: To provide an optimized implementation, we created a new subfolder called *portable_optimized*
    inside *tensorflow/lite/micro/kernels*, and added a new C++ source file called
    [*depthwise_conv.cc*](https://oreil.ly/BYRho). This is much more complex than
    the reference implementation, and takes advantage of particular features of the
    speech model to enable specialized optimizations. For example, the convolution
    windows are multiples of 8 wide, so we can load the values as two 32-bit words
    from memory, rather than as 8 individual bytes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个优化的实现，我们在*tensorflow/lite/micro/kernels*内创建了一个名为*portable_optimized*的新子文件夹，并添加了一个名为[*depthwise_conv.cc*](https://oreil.ly/BYRho)的新的C++源文件。这比参考实现复杂得多，并利用了语音模型的特定特性来实现专门的优化。例如，卷积窗口的宽度是8的倍数，因此我们可以将值作为两个32位字从内存中加载，而不是作为8个单独的字节。
- en: You’ll notice that we’ve named the subfolder *portable_optimized*, rather than
    something platform-specific as we did for the previous example. This is because
    none of the changes we’ve made are tied to a particular chip or library; they’re
    generic optimizations that are expected to help across a wide variety of processors,
    such as precalculating array indices or loading multiple byte values as larger
    words. We then specify that this implementation should be used inside the `make`
    project files, by adding `portable_optimized` to the [`ALL_TAGS` list](https://oreil.ly/XSWFk).
    Because this tag is present, and there’s an implementation of *depthwise_conv.cc*
    inside the subfolder with the same name, the optimized implementation is linked
    in rather than the default reference version.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到我们将子文件夹命名为*portable_optimized*，而不是像前面的示例那样特定于平台。这是因为我们所做的更改都不与特定芯片或库绑定；它们是通用优化，预计将有助于各种处理器，例如预先计算数组索引或将多个字节值加载为更大的字。然后，我们通过将`portable_optimized`添加到[`ALL_TAGS`列表](https://oreil.ly/XSWFk)中来指定应在`make`项目文件中使用此实现。由于存在此标签，并且在具有相同名称的子文件夹中存在*depthwise_conv.cc*的实现，因此链接了优化实现，而不是默认的参考版本。
- en: Hopefully these examples show how you can use the subfolder mechanism to extend
    and optimize the library code while keeping the core implementations small and
    easy to understand.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这些示例展示了如何利用子文件夹机制来扩展和优化库代码，同时保持核心实现简洁易懂。
- en: Makefiles
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Makefiles
- en: On the topic of being easy to understand, Makefiles aren’t. The [Make build
    system](https://oreil.ly/8Ft1J) is now more than 40 years old and has a lot of
    features that can be confusing, such as its use of tabs as meaningful syntax or
    the indirect specification of build targets through declarative rules. We chose
    to use Make over alternatives such as Bazel or Cmake because it was flexible enough
    to implement complex behaviors like project generation, and we hope that most
    users of TensorFlow Lite for Microcontrollers will use those generated projects
    in more modern IDEs rather than interacting with Makefiles directly.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 说到易于理解，Makefiles并不是。[Make构建系统](https://oreil.ly/8Ft1J)现在已经有40多年的历史，具有许多令人困惑的特性，比如使用制表符作为有意义的语法或通过声明性规则间接指定构建目标。我们选择使用Make而不是Bazel或Cmake等替代方案，因为它足够灵活，可以实现像项目生成这样的复杂行为，我们希望大多数TensorFlow
    Lite for Microcontrollers的用户会在更现代的IDE中使用这些生成的项目，而不是直接与Makefiles交互。
- en: If you’re making changes to the core library, you might need to understand more
    about what’s going on under the hood in the Makefiles, though, so this section
    covers some of the conventions and helper functions that you’ll need to be familiar
    with to make modifications.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对核心库进行更改，可能需要更深入了解Makefiles中的内部情况，因此，本节涵盖了一些您需要熟悉的约定和辅助函数，以便进行修改。
- en: Note
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re using a bash terminal on Linux or macOS, you should be able to see
    all of the available targets (names of things you can build) by typing the normal
    `make -f tensorflow/lite/micro/tools/make/Makefile` command and then pressing
    the Tab key. This autocomplete feature can be very useful when finding or debugging
    targets.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Linux或macOS上使用bash终端，可以通过键入正常的`make -f tensorflow/lite/micro/tools/make/Makefile`命令，然后按Tab键来查看所有可用的目标（可以构建的内容的名称）。在查找或调试目标时，此自动完成功能非常有用。
- en: 'If you’re just adding a specialized version of a module or operation, you shouldn’t
    need to update the Makefile at all. There’s a custom function called [`specialize()`](https://oreil.ly/teIF6)
    that automatically takes the `ALL_TAGS` list of strings (populated with the platform
    name, along with any custom tags) and a list of source files, and returns the
    list with the correct specialized versions substituted for the originals. This
    does also give you the flexibility to manually specify tags on the command line
    if you want to. For example, this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只是添加一个模块或操作的专门版本，您根本不需要更新Makefile。有一个名为[`specialize()`](https://oreil.ly/teIF6)的自定义函数，它会自动获取字符串（包含平台名称以及任何自定义标签）的`ALL_TAGS`列表和源文件列表，并返回替换原始版本的正确专门版本的列表。这也使您有灵活性，在命令行上手动指定标签。例如，这样：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: would produce an `ALL_TAGS` list that looked like “bluepill portable_optimized
    foo,” and for every source file the subfolders would be searched in order to find
    any specialized versions to substitute.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成一个看起来像“bluepill portable_optimized foo”的`ALL_TAGS`列表，对于每个源文件，将按顺序搜索子文件夹以查找任何专门的版本来替换。
- en: You also don’t need to alter the Makefile if you’re just adding new C++ files
    to standard folders, because most of these are automatically picked up by wildcard
    rules, like the definition of [`MICROLITE_CC_BASE_SRCS`](https://oreil.ly/QAtDk).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只是向标准文件夹添加新的C++文件，也不需要修改Makefile，因为大多数情况下这些文件会被通配符规则自动捕捉，比如[`MICROLITE_CC_BASE_SRCS`](https://oreil.ly/QAtDk)的定义。
- en: 'The Makefile relies on defining lists of source and header files to build at
    the root level and then modifying them depending on which platform and tags are
    specified. These modifications happen in sub-Makefiles included from the parent
    build project. For example, all of the *.inc* files in the [*tensorflow/lite/micro/tools/make/targets*](https://oreil.ly/79zOB)
    folder are automatically included. If you look in one of these, like the [*apollo3evb_makefile.inc*](https://oreil.ly/gKKXO)
    used for Ambiq and SparkFun Edge platforms, you can see that it checks whether
    the chips it’s targeting have been specified for this build; if they have, it
    defines a lot of flags and modifies the source lists. Here’s an abbreviated version
    including some of the most interesting bits:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Makefile依赖于在根级别定义要构建的源文件和头文件列表，然后根据指定的平台和标签进行修改。这些修改发生在从父构建项目包含的子Makefiles中。例如，[*tensorflow/lite/micro/tools/make/targets*](https://oreil.ly/79zOB)文件夹中的所有*.inc*文件都会自动包含。如果您查看其中一个，比如用于Ambiq和SparkFun
    Edge平台的[*apollo3evb_makefile.inc*](https://oreil.ly/gKKXO)，您会看到它检查了是否已为此构建指定了目标芯片；如果有，它会定义许多标志并修改源列表。以下是包含一些最有趣部分的简化版本：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is where all of the customizations for a particular platform happen. In
    this snippet, we’re indicating to the build system where to find the compiler
    that we want to use, and what architecture to specify. We’re specifying some extra
    external libraries to download, like the GCC toolchain and Arm’s CMSIS library.
    We’re setting up compilation flags for the build, and arguments to pass to the
    linker, including extra library archives to link in and include paths to look
    in for headers. We’re also adding some extra C files we need to build successfully
    on Ambiq platforms.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这是特定平台的所有定制发生的地方。在这段代码中，我们指示构建系统在哪里找到我们想要使用的编译器，并指定要使用的架构。我们指定了一些额外的外部库要下载，如GCC工具链和Arm的CMSIS库。我们为构建设置编译标志，并传递给链接器的参数，包括要链接的额外库归档文件和要查找头文件的包含路径。我们还添加了一些我们需要在Ambiq平台上成功构建的额外C文件。
- en: A similar kind of sub-Makefile inclusion is used for building the examples.
    The speech wake-word sample code has its own Makefile at [*micro_speech/Makefile.inc*](https://oreil.ly/XjuJP),
    and it defines its own lists of source code files to compile, along with extra
    external dependencies to download.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 构建示例时也使用了类似的子Makefile包含。语音唤醒词示例代码在[*micro_speech/Makefile.inc*](https://oreil.ly/XjuJP)中有自己的Makefile，并定义了要编译的源代码文件列表，以及要下载的额外外部依赖项。
- en: You can generate standalone projects for different IDEs by using the [`generate_microlite_projects()`](https://oreil.ly/iv94T)
    function. This takes a list of source files and flags and then copies the required
    files to a new folder, together with any additional project files that are needed
    by the build system. For some IDEs this is very simple, but the Arduino, for example,
    requires all *.cc* files to be renamed to *.cpp* and some include paths to be
    altered in the source files as they are copied.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用[`generate_microlite_projects()`](https://oreil.ly/iv94T)函数为不同的IDE生成独立项目。这将接受一组源文件和标志，然后将所需文件复制到一个新文件夹中，以及构建系统所需的任何其他项目文件。对于某些IDE，这非常简单，但例如Arduino需要将所有*.cc*文件重命名为*.cpp*，并且在复制时需要更改源文件中的一些包含路径。
- en: External libraries such as the C++ toolchain for embedded Arm processors are
    automatically downloaded as part of the Makefile build process. This happens because
    of the [`add_third_party_download`](https://oreil.ly/E9tS-) rule that’s invoked
    for every needed library, passing in a URL to pull from and an MD5 sum to check
    the archive against to ensure that it’s correct. These are expected to be ZIP,
    GZIP, BZ2, or TAR files, and the appropriate unpacker will be called depending
    on the file extension. If headers or source files from any of these are needed
    by build targets, they should be explicitly included in the file lists in the
    Makefile so that they can be copied over to any generated projects, so each project’s
    source tree is self-contained. This is easy to forget with headers because setting
    up include paths is enough to get the Makefile compilation working without explicitly
    mentioning each included file, but the generated projects will then fail to build.
    You should also ensure that any license files are included in your file lists,
    so that the copies of the external libraries retain the proper attribution.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 外部库，如用于嵌入式Arm处理器的C++工具链，将作为Makefile构建过程的一部分自动下载。这是因为对每个所需库调用的[`add_third_party_download`](https://oreil.ly/E9tS-)规则，传入一个URL以拉取文件，并传入一个MD5校验和以检查归档文件以确保正确性。这些文件应为ZIP、GZIP、BZ2或TAR文件，根据文件扩展名将调用适当的解压程序。如果构建目标需要这些文件中的头文件或源文件，则应明确包含在Makefile中的文件列表中，以便将其复制到任何生成的项目中，因此每个项目的源树都是自包含的。这很容易被忽略，因为设置包含路径足以使Makefile编译正常工作，而无需明确提及每个包含的文件，但生成的项目将无法构建。您还应确保包含任何许可文件在您的文件列表中，以便外部库的副本保留正确的归属。
- en: Writing Tests
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写测试
- en: TensorFlow aims to have unit tests for all of its code, and we’ve already covered
    some of these tests in detail in [Chapter 5](ch05.xhtml#chapter_building_an_application).
    The tests are usually arranged as *_test.cc* files in the same folder as the module
    that’s being tested, and with the same prefix as the original source file. For
    example, the implementation of the depthwise convolution operation is tested by
    [*tensorflow/lite/micro/kernels/depthwise_conv_test.cc*](https://oreil.ly/eIiRO).
    If you’re adding a new source file, you must add an accompanying unit test that
    exercises it if you want to submit your modifications back into the main tree.
    This is because we need to support a lot of different platforms and models and
    many people are building complex systems on top of our code, so it’s important
    that our core components can be checked for correctness.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow旨在为其所有代码编写单元测试，我们已经在[第5章](ch05.xhtml#chapter_building_an_application)中详细介绍了其中一些测试。这些测试通常安排为与正在测试的模块相同文件夹中的*_test.cc*文件，并具有与原始源文件相同的前缀。例如，深度卷积操作的实现通过[*tensorflow/lite/micro/kernels/depthwise_conv_test.cc*](https://oreil.ly/eIiRO)进行测试。如果要添加新的源文件，如果要将修改提交回主树，则必须添加一个相应的单元测试来测试它。这是因为我们需要支持许多不同的平台和模型，许多人正在我们的代码之上构建复杂系统，因此重要的是我们的核心组件可以检查正确性。
- en: 'If you add a file in a direct subfolder of *tensorflow/tensorflow/lite/experimental/micro*,
    you should be able to name it *<something>_test.cc* and it will be picked up automatically.
    If you’re testing a module inside an example, you’ll need to add an explicit call
    to the `microlite_test` Makefile helper function, like [this](https://oreil.ly/wkYgu):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在*tensorflow/tensorflow/lite/experimental/micro*的直接子文件夹中添加文件，您应该能够将其命名为*<something>_test.cc*，并且它将被自动捕获。如果您正在测试示例内的模块，则需要向`microlite_test`
    Makefile辅助函数添加显式调用，例如[此处](https://oreil.ly/wkYgu)：
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The tests themselves need to be run on microcontrollers, so they must stick
    to the same constraints around dynamic memory allocation, avoiding OS and external
    library dependencies that the framework aims to satisfy. Unfortunately, this means
    that popular unit test systems like [Google Test](https://oreil.ly/GZWdj) aren’t
    acceptable. Instead, we’ve written our own very minimal test framework, defined
    and implemented in the [*micro_test.h*](https://oreil.ly/GcIbP) header.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 测试本身需要在微控制器上运行，因此它们必须遵守围绕动态内存分配、避免OS和外部库依赖的相同约束，这是框架旨在满足的。不幸的是，这意味着像[Google
    Test](https://oreil.ly/GZWdj)这样的流行单元测试系统是不可接受的。相反，我们编写了自己非常简化的测试框架，定义和实现在[*micro_test.h*](https://oreil.ly/GcIbP)头文件中。
- en: 'To use it, create a *.cc* file that includes the header. Start with a `TF_LITE_MICRO_TESTS_BEGIN`
    statement on a new line, and then define a series of test functions, each with
    a `TF_LITE_MICRO_TEST()` macro. Inside each test, you call macros like `TF_LITE_MICRO_EXPECT_EQ()`
    to assert the expected results that you want to see from the functions being tested.
    At the end of all the test functions you’ll need `TF_LITE_MICRO_TESTS_END`. Here’s
    a basic example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用它，创建一个包含头文件的*.cc*文件。在新行上以`TF_LITE_MICRO_TESTS_BEGIN`语句开始，然后定义一系列测试函数，每个函数都有一个`TF_LITE_MICRO_TEST()`宏。在每个测试中，您调用像`TF_LITE_MICRO_EXPECT_EQ()`这样的宏来断言您希望从正在测试的函数中看到的预期结果。在所有测试函数的末尾，您将需要`TF_LITE_MICRO_TESTS_END`。这里是一个基本示例：
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you compile this for your platform, you’ll get a normal binary that you
    should be able to run. Executing it will output logging information like this
    to `stderr` (or whatever equivalent is available and written to by `ErrorReporter`
    on your platform):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您为您的平台编译此代码，您将获得一个正常的二进制文件，您应该能够运行它。执行它将输出类似于这样的日志信息到`stderr`（或者在您的平台上由`ErrorReporter`写入的任何等效内容）：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is designed to be human-readable, so you can just run tests manually, but
    the string `~~~ALL TESTS PASSED~~~` should appear only if all of the tests do
    actually pass. This makes it possible to integrate with automated test systems
    by scanning the output logs and looking for that magic value. This is how we’re
    able to run tests on microcontrollers. As long as there’s some debug logging connection
    back, the host can flash the binary and then monitor the output log to ensure
    the expected string appears to indicate whether the tests succeeded.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为了便于人类阅读，因此您可以手动运行测试，但只有在所有测试确实通过时，字符串`~~~ALL TESTS PASSED~~~`才应该出现。这使得可以通过扫描输出日志并查找该魔术值来与自动化测试系统集成。这就是我们能够在微控制器上运行测试的方式。只要有一些调试日志连接回来，主机就可以刷新二进制文件，然后监视输出日志以确保预期的字符串出现以指示测试是否成功。
- en: Supporting a New Hardware Platform
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持新硬件平台
- en: One of the main goals of the TensorFlow Lite for Microcontrollers project is
    to make it easy to run machine learning models across many different devices,
    operating systems, and architectures. The core code is designed to be as portable
    as possible, and the build system is written to make bringing up new environments
    straightforward. In this section, we present a step-by-step guide to getting TensorFlow
    Lite for Microcontrollers running on a new platform.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite for Microcontrollers项目的主要目标之一是使在许多不同设备、操作系统和架构上运行机器学习模型变得容易。核心代码被设计为尽可能可移植，构建系统编写为使引入新环境变得简单。在本节中，我们提供了一个逐步指南，以在新平台上运行TensorFlow
    Lite for Microcontrollers。
- en: Printing to a Log
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打印到日志
- en: The only platform dependency that TensorFlow Lite absolutely requires is the
    ability to print strings to a log that can be inspected externally, typically
    from a desktop host machine. This is so that we can see whether tests have been
    run successfully and generally debug what’s happening inside the programs we’re
    running. Because this is a difficult requirement, the first thing you will need
    to do on your platform is determine what kind of logging facilities are available
    and then write a small program to print something out to exercise them.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite绝对需要的唯一平台依赖是能够将字符串打印到可以从桌面主机机器外部检查的日志中。这样我们就可以看到测试是否成功运行，并通常调试我们正在运行的程序内部发生的情况。由于这是一个困难的要求，您在您的平台上需要做的第一件事是确定可用的日志记录设施类型，然后编写一个小程序来打印一些内容以测试它们。
- en: 'On Linux and most other desktop operating systems, this would be the canonical
    “hello world” example that begins many C training curriculums. It would typically
    look something like this:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux和大多数其他桌面操作系统上，这将是许多C培训课程的经典“hello world”示例。它通常看起来像这样：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you compile and build this on Linux, macOS, or Windows and then run the executable
    from the command line, you should see “Hello World!” printed to the terminal.
    It might also work on a microcontroller if it’s running an advanced OS, but at
    the very least you’ll need to figure out where the text itself appears given that
    embedded systems don’t have displays or terminals themselves. Typically you’ll
    need to connect to a desktop machine over USB or another debugging connection
    to see any logs, even if `fprintf()` is supported when compiling.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Linux、macOS或Windows上编译和构建此代码，然后从命令行运行可执行文件，您应该会在终端上看到“Hello World!”打印出来。如果微控制器正在运行高级操作系统，它可能也会工作，但至少您需要弄清楚文本本身出现在哪里，因为嵌入式系统本身没有显示器或终端。通常，您需要通过USB或其他调试连接连接到桌面机器才能查看任何日志，即使在编译时支持`fprintf()`。
- en: There are a few tricky parts about this code from a microcontroller perspective.
    One of them is that the *stdio.h* library requires functions to be linked in,
    and some of them are quite large, which can increase the binary size beyond the
    resources available on a small device. The library also assumes that there are
    all the normal C standard library facilities available, like dynamic memory allocation
    and string functions. And there’s no natural definition for where `stderr` should
    go on an embedded system, so the API is unclear.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从微控制器的角度来看，这段代码有一些棘手的部分。其中一个问题是，*stdio.h*库需要链接函数，其中一些函数非常庞大，可能会使二进制文件大小超出小型设备可用的资源。该库还假定所有常规的C标准库设施都可用，如动态内存分配和字符串函数。而在嵌入式系统上，`stderr`应该放在哪里并没有自然的定义，因此API不清晰。
- en: 'Instead, most platforms define their own debug logging interfaces. How these
    are called often depends on what kind of connection is being used between the
    host and microcontroller, as well as the hardware architecture and the OS (if
    any) being run on the embedded system. For example, Arm Cortex-M microcontrollers
    support [*semihosting*](https://oreil.ly/LmC4k), which is a standard for communicating
    between the host and target systems during the development process. If you’re
    using a connection like [OpenOCD](https://oreil.ly/lSn0n) from your host machine,
    calling the [`SYS_WRITE0`](https://oreil.ly/6IyrK) system call from the microcontroller
    will cause the zero-terminated string argument in register 1 to be shown on the
    OpenOCD terminal. In this case, the code for an equivalent “hello world” program
    would look like this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，大多数平台定义了自己的调试日志接口。这些接口的调用方式通常取决于主机和微控制器之间使用的连接类型，以及嵌入式系统上运行的硬件架构和操作系统（如果有）。例如，Arm
    Cortex-M微控制器支持[*semihosting*](https://oreil.ly/LmC4k)，这是在开发过程中在主机和目标系统之间通信的标准。如果你正在使用类似[OpenOCD](https://oreil.ly/lSn0n)的连接从主机机器上，从微控制器调用[`SYS_WRITE0`](https://oreil.ly/6IyrK)系统调用将导致寄存器1中的零终止字符串参数显示在OpenOCD终端上。在这种情况下，等效“hello
    world”程序的代码将如下所示：
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The need for assembly here shows how platform-specific this solution is, but
    it does avoid the need to bring in any external libraries at all (even the standard
    C library).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要汇编的原因显示了这个解决方案有多么特定于平台，但它确实避免了完全不引入任何外部库的需要（甚至是标准C库）。
- en: 'Exactly how to do this will vary widely across different platforms, but one
    common approach is to use a serial UART connection to the host. Here’s how you
    do that on Mbed:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如何做到这一点在不同平台上会有很大差异，但一个常见的方法是使用串行UART连接到主机。这是在Mbed上如何做的：
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And here’s a slightly more complex example for Arduino:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个稍微复杂一点的Arduino示例：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Both of these examples create a serial object, and then expect that the user
    will hook up a serial connection to the microcontroller over USB to their host
    machine.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个示例都创建了一个串行对象，然后期望用户将串行连接到微控制器上的主机机器上。
- en: The key first step in the porting effort is to create a minimal example for
    your platform, running in the IDE you want to use, that gets a string printed
    to the host console somehow. If you can get this working, the code you use will
    become the basis of a specialized function that you’ll add to the TensorFlow Lite
    code.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 移植工作的关键第一步是为你的平台创建一个最小示例，在你想要使用的IDE中运行，以某种方式将一个字符串打印到主机控制台。如果你能让这个工作起来，你使用的代码将成为你将添加到TensorFlow
    Lite代码中的专门函数的基础。
- en: Implementing DebugLog()
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现DebugLog()
- en: If you look in [*tensorflow/lite/micro/debug_log.cc*](https://oreil.ly/Lka3T),
    you’ll see that there’s an implementation of the `DebugLog()` function that looks
    very similar to the first “hello world” example we showed, using *stdio.h* and
    `fprintf()` to output a string to the console. If your platform supports the standard
    C library fully and you don’t mind the extra binary size, you can just use this
    default implementation and ignore the rest of this section. It’s more likely that
    you’ll need to use a different approach, though, unfortunately.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看[*tensorflow/lite/micro/debug_log.cc*](https://oreil.ly/Lka3T)，你会看到`DebugLog()`函数的实现，看起来与我们展示的第一个“hello
    world”示例非常相似，使用*stdio.h*和`fprintf()`将字符串输出到控制台。如果你的平台完全支持标准C库，并且不介意额外的二进制文件大小，你可以使用这个默认实现，忽略本节的其余部分。不过，更有可能的是你需要使用不同的方法。
- en: 'As a first step, we’ll use the test that already exists for the `DebugLog()`
    function. To begin, run this command line:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将使用已经存在的`DebugLog()`函数的测试。首先，运行以下命令行：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When you look inside *tensorflow/lite/micro/tools/make/gen/linux_x86_64/prj/micro_error_reporter_test/make/*
    (replacing *linux* with *osx* or *windows* if you’re on a different host platform)
    you should see some folders like *tensorflow* and *third_party*. These folders
    contain C++ source code, and if you drag them into your IDE or build system and
    compile all the files, you should end up with an executable that tests out the
    error reporting functionality we need to create. It’s likely that your first attempt
    to build this code will fail, because it’s still using the default `DebugLog()`
    implementation in [*debug_log.cc*](https://oreil.ly/fDkLh), which relies on *stdio.h*
    and the C standard library. To work around that problem, change *debug_log.cc*
    to remove the `#include` `<cstdio>` statement and replace the `DebugLog()` implementation
    with one that does nothing:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看*tensorflow/lite/micro/tools/make/gen/linux_x86_64/prj/micro_error_reporter_test/make/*（如果你在不同的主机平台上，请将*linux*替换为*osx*或*windows*），你应该会看到一些像*tensorflow*和*third_party*这样的文件夹。这些文件夹包含C++源代码，如果你将它们拖入你的IDE或构建系统并编译所有文件，你应该会得到一个可执行文件，用于测试我们需要创建的错误报告功能。你第一次尝试构建这段代码很可能会失败，因为它仍在使用[*debug_log.cc*](https://oreil.ly/fDkLh)中的默认`DebugLog()`实现，依赖于*stdio.h*和C标准库。为了解决这个问题，修改*debug_log.cc*，删除`#include`
    `<cstdio>`语句，并用一个什么都不做的实现替换`DebugLog()`：
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With that changed, try to get the set of source files successfully compiling.
    After you’ve done that, take the resulting binary and load it onto your embedded
    system. If you can, check that the program runs without crashing, even though
    you won’t be able to see any output yet.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: When the program seems to build and run correctly, see whether you can get the
    debug logging working. Take the code that you used for the “hello world” program
    in the previous section and put it into the `DebugLog()` implementation inside
    *debug_log.cc*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual test code itself exists in [*tensorflow/lite/micro/micro_error_reporter_test.cc*](https://oreil.ly/0jD00),
    and it looks like this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It’s not calling `DebugLog()` directly—it goes through the `ErrorReporter`
    interface that handles things like variable numbers of arguments first—but it
    does rely on the code you’ve just written as its underlying implementation. You
    should see something like this in your debug console if everything’s working correctly:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After you have that working, you’ll want to put your implementation of `DebugLog()`
    back into the main source tree. To do this, you’ll use the subfolder specialization
    technique that we discussed earlier. You’ll need to decide on a short name (with
    no capital letters, spaces, or other special characters) to use to identify your
    platform. For example, we use *arduino*, *sparkfun_edge*, and *linux* for some
    of the platforms we already support. For the purposes of this tutorial, we’ll
    use *my_mcu*. Start by creating a new subfolder in *tensorflow/lite/micro/* called
    *my_mcu* in the copy of the source code you checked out from GitHub (not the one
    you just generated or downloaded). Copy the *debug_log.cc* file with your implementation
    into that *my_mcu* folder, and add it to source tracking using Git. Copy your
    generated project files to a backup location and then run the following commands:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you now look in *tensorflow/lite/micro/tools/make/gen/my_mcu_x86_64/prj/micro_error_reporter_test/make/tensorflow/lite/micro/*
    you should see that the default *debug_log.cc* is no longer present, but your
    implementation is in the *my_mcu* subfolder. If you drag this set of source files
    back into your IDE or build system, you should now see a program that successfully
    builds, runs, and outputs to the debug console.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Running All the Targets
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If that works, congratulations: you’ve now enabled all of the TensorFlow test
    and executable targets! Implementing debug logging is the only required platform-specific
    change you need to make; everything else in the code base should be written in
    a portable enough way that it will build and run on any C++11-supporting toolchain,
    with no need for standard library linking beyond the `math` library. To create
    all of the targets so that you can try them in your IDE, you can run the following
    command from the terminal:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This creates a large number of folders in similar locations to the generated
    error reporter test, each exercising different parts of the library. If you want
    to get the speech wake-word example running on your platform, you can look at
    *tensorflow/lite/micro/tools/make/gen/my_mcu_x86_64/prj/micro_speech/make/*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have `DebugLog()` implemented, it should run on your platform,
    but it won’t do anything useful because the default *audio_provider.cc* implementation
    is always returning arrays full of zeros. To get it working properly, you’ll need
    to create a specialized *audio_provider.cc* module that returns captured sound,
    using the subfolder specialization approach described earlier. If you don’t care
    about a working demonstration, you can still look at things like the inference
    latency of neural networks on your platform using the same sample code, or some
    of the other tests.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: As well as hardware support for sensors and output devices like LEDs, you may
    well want to implement versions of the neural network operators that run faster
    by taking advantage of special features of your platform. We welcome this kind
    of specialized optimization and hope that the subfolder specialization technique
    will be a good way to integrate them back into the main source tree if they prove
    to be useful.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with the Makefile Build
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we’ve talked only about using your own IDE, given that it’s often simpler
    and more familiar to many embedded programmers than using our Make system. If
    you want to be able to have your code tested by our continuous integration builds,
    or have it available outside of a particular IDE, you might want to integrate
    your changes more fully with our Makefiles. One of the essentials for this is
    finding a publicly downloadable toolchain for your platform, along with public
    downloads for any SDKs or other dependencies, so that a shell script can automatically
    grab everything it needs to build without having to worry about website logins
    or registrations. For example, we download the macOS and Linux versions of the
    GCC Embedded toolchain from Arm, with the URLs in [*tensorflow/lite/micro/tools/make/third_party_downloads.inc*](https://oreil.ly/WBrIy).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: You’ll then need to determine the correct command-line flags to pass into the
    compiler and linker, along with any extra source files you need that aren’t found
    using subfolder specialization, and encode that information into a sub-Makefile
    in [*tensorflow/lite/micro/tools/make/targets*](https://oreil.ly/zusVM). If you
    want extra credit, you can then figure out how to emulate your microcontroller
    on an x86 server using a tool like [Renode](https://renode.io/) so that we can
    run the tests during our continuous integration, not just confirm the build. You
    can see an example of the script we run to test the “Bluepill” binaries using
    Renode at [*tensorflow/lite/micro/testing/test_bluepill_binary.sh*](https://oreil.ly/A80CN).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have all of the build settings configured correctly, you’ll be able
    to run something like this to generate a flashable binary (setting the target
    as appropriate for your platform):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If you have the script and environment for running tests working correctly,
    you can do this to run all the tests for the platform:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Supporting a New IDE or Build System
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow Lite for Microcontrollers can create standalone projects for Arduino,
    Mbed, and Keil toolchains, but we know that a lot of other development environments
    are used by embedded engineers. If you need to run the framework in a new environment,
    the first thing we recommend is seeing whether the “raw” set of files that are
    generated when you generate a Make project can be imported into your IDE. This
    kind of project archive contains only the source files needed for a particular
    target, including any third-party dependencies, so in many cases you can just
    point your toolchain at the root folder and ask it to include everything.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you have only a few files, it can seem odd to keep them in the nested subfolders
    (like *tensorflow/lite/micro/examples/micro_speech*) of the original source tree
    when you export them to a generated project. Wouldn’t it make more sense to flatten
    out the directory hierarchy?
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The reason we chose to keep the deeply nested folders is to make merging back
    into the main source tree as straightforward as possible, even if it is a little
    less convenient when working with the generated project files. If the paths always
    match between the original code checked out of GitHub and the copies in each project,
    keeping track of changes and updates is a lot easier.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: This approach won’t work for all IDEs, unfortunately. For example, Arduino libraries
    require all C++ source code files to have the suffix *.cpp* rather than TensorFlow’s
    default of *.cc,* and they’re also unable to specify include paths, so we need
    to change the paths in the code when we copy over the original files to the Arduino
    destination. To support these more complex transformations we have some rules
    and scripts in the Makefile build, with the root function [`generate_microlite_projects()`](https://oreil.ly/YYoHm)
    calling into specialized versions for each IDE, which then rely on more [rules](https://oreil.ly/KHo7G),
    [Python scripts](https://oreil.ly/BKLhn), and [template files](https://oreil.ly/tDFhh)
    to create the final output. If you need to do something similar for your own IDE,
    you’ll need to add similar functionality using the Makefile, which won’t be straightforward
    to implement because the build system is quite complex to work with.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Code Changes Between Projects and Repositories
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the biggest disadvantages of a code generation system is that you end
    up with multiple copies of the source scattered in different locations, which
    makes dealing with code updates very tricky. To minimize the cost of merging changes,
    we’ve adopted some conventions and recommended procedures that should help. The
    most common use case is that you’ve made some modifications to files within the
    local copy of your project, and you’d like to update to a newer version of the
    TensorFlow Lite framework to get extra features or bug fixes. Here’s how we suggest
    handling that process:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Either download a prebuilt archive of the project file for your IDE and target
    or generate one manually from the Makefile using the version of the framework
    you’re interested in.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unpack this new set of files into a folder and make sure that the folder structures
    match between the new folder and the folder containing the project files that
    you’ve been modifying. For example, both should have *tensorflow* subfolders at
    the top level.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a merge tool between the two folders. Which tool you use will depend on
    your OS, but [Meld](https://meldmerge.org/) is a good choice that works on Linux,
    Windows, and macOS. The complexity of the merge process will depend on how many
    files you’ve changed locally, but it’s expected most of the differences will be
    updates on the framework side, so you should usually be able to choose the equivalent
    of “accept theirs.”
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have changed only one or two files locally, it might be easier to just
    copy the modified code from the old version and manually merge it into the new
    exported project.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: You could also get more advanced by checking your modified code into Git, importing
    the latest project files as a new branch, and then using Git’s built-in merging
    facilities to handle integration. We’re still not advanced enough [Git masters](https://oreil.ly/sIe1F)
    to offer advice on this approach, so we haven’t used it ourselves.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: The big difference between this process and doing the same with more traditional
    code-generation approaches is that the code is still separated into many logical
    files whose paths remain constant over time. Typical code generation will concatenate
    all of the source into a single file, which makes merging or tracking changes
    very difficult because trivial changes to the order or layout make historical
    comparisons impossible.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you might want to port changes in the other direction, merging from
    project files to the main source tree. This main source tree doesn’t need to be
    [the official repository on GitHub](https://oreil.ly/o8Ytb); it could be a local
    fork that you maintain and don’t distribute. We love to get pull requests to the
    main repository with fixes or upgrades, but we know that’s not always possible
    with proprietary embedded development, so we’re also happy to help keep forks
    healthy. The key thing to watch is that you try to keep a single “source of truth”
    for your development files. Especially if you have multiple developers, it’s easy
    to have incompatible changes being made in different local copies of the source
    files inside project archives, which makes updating and debugging a nightmare.
    Whether it’s only internal or shared publicly, we highly recommend having a source-control
    system that has a single copy of each file, rather than checking in multiple versions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: To handle migrating changes back to the source of truth repository, you’ll need
    to keep track of which files you’ve modified. If you don’t have that information
    handy, you can always go back to the project files you originally downloaded or
    generated and run a diff to see what has changed. As soon as you know what files
    are modified or new, just copy them into the Git (or other source-control system)
    repository at the same paths they occur at in the project files.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The only exceptions to this approach are files that are part of third-party
    libraries, given that these don’t exist in the TensorFlow repository. Getting
    changes to those files submitted is beyond the scope of this book—the process
    will depend on the rules of each individual repository—but as a last resort, if
    you have changes that aren’t being accepted, you can often fork the project on
    GitHub and point your platform’s build system to that new URL rather than the
    original. Assuming that you’re changing just TensorFlow source files, you should
    now have a locally modified repository that contains your changes. To verify that
    the modifications have been successfully integrated, you’ll need to run `generate_projects()`
    using Make and then ensure that the project for your IDE and target has your updates
    applied as you’d expect. When that’s complete, and you’ve run tests to ensure
    nothing else has been broken, you can submit your changes to your fork of TensorFlow.
    As soon as that’s done, the final stage is to submit a pull request if you’d like
    to see your changes made public.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Contributing Back to Open Source
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are already more contributors to TensorFlow outside of Google than inside,
    and the microcontroller work has a larger reliance on collaboration than most
    other areas. We’re very keen to get help from the community, and one of the most
    important ways of helping is through pull requests (though there are plenty of
    other ways, like [Stack Overflow](https://oreil.ly/7btPw)) or creating your own
    example projects). GitHub has great [documentation](https://oreil.ly/8rDKL) covering
    the basics of pull requests, but there are some details that are helpful to know
    when working with TensorFlow:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: We have a code review process run by project maintainers inside and outside
    Google. This is managed through GitHub’s code review system, so you should expect
    to see a discussion about your submission there.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes that are more than just a bug fix or optimization usually need a design
    document first. There’s a group called [SIG Micro](https://oreil.ly/JKiwD) that’s
    run by external contributors to help define our priorities and roadmap, so that’s
    a good forum to talk about new designs. The document can be just a page or two
    for a smaller change; it’s helpful to understand the context and motivation behind
    a pull request.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintaining a public fork can be a great way of getting feedback on experimental
    changes before they’re submitted to the main branch because you can make changes
    with any cumbersome processes to slow you down.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are automated tests that run against all pull requests, both publicly
    and with some additional Google internal tools that check the integration against
    our own projects that depend on this. The results of these tests can sometimes
    be difficult to interpret, unfortunately, and even worse, they’re occasionally
    “flakey,” with tests failing for reasons unrelated to your changes. We’re constantly
    trying to improve this process because we know it’s a bad experience, but please
    do ping the maintainers in the conversation thread if you’re having trouble understanding
    test failures.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We aim for 100% test coverage, so if a change isn’t exercised by an existing
    test, we’ll ask you for a new one. These tests can be quite simple; we just want
    to make sure there’s some coverage of everything we do.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For readability’s sake, we use the Google style guide for C and C++ code formatting
    consistently across the entire TensorFlow code base, so we request any new or
    modified code be in this style. You can use [`clang-format`](https://oreil.ly/KkRKL)
    with the `google` style argument to automatically format your code.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks in advance for any contributions you can make to TensorFlow, and for
    your patience with the work involved in getting changes submitted. It’s not always
    easy, but you’ll be making a difference to many developers around the world!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Supporting New Hardware Accelerators
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the goals of TensorFlow Lite for Microcontrollers is to be a reference
    software platform to help hardware developers make faster progress with their
    designs. What we’ve observed is that a lot of the work around getting a new chip
    doing something useful with machine learning is in tasks like writing exporters
    from the training environment, especially with regard to tricky details like quantization
    and implementing the “long tail” of operations that are needed for typical machine
    learning models. These tasks take so little time that they aren’t good candidates
    for hardware optimization.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: To address these problems, we hope that the first step that hardware developers
    will take is getting the unoptimized reference code for TensorFlow Lite for Microcontrollers
    running on their platform and producing the correct results. This will demonstrate
    that everything but the hardware optimization is working, so that can be the focus
    of the remaining work. One challenge might be if the chip is an accelerator that
    doesn’t support general-purpose C++ compilation, because it only has specialized
    functionality rather than a traditional CPU. For embedded use cases, we’ve found
    that it’s almost always necessary to have some general-purpose computation available,
    even if it’s slow (like a small microcontroller), because many users’ graphs have
    operations that can’t be compactly expressed except as arbitrary C++ implementations.
    We’ve also made the design decision that the TensorFlow Lite for Microcontrollers
    interpreter won’t support asynchronous execution of subgraphs, because that would
    complicate the code considerably and also seems uncommon in the embedded domain
    (unlike the mobile world, where Android’s Neural Network API is popular).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: This means that the kinds of architectures TensorFlow Lite for Microcontrollers
    supports look more like synchronous coprocessors that run in lockstep with a traditional
    processor, with the accelerator speeding up compute-intensive functions that would
    otherwise take a long time but deferring the smaller ops with more flexible requirements
    to a CPU. The result in practice is that we recommend starting off by replacing
    individual operator implementations at the kernel level with calls to any specialized
    hardware. This does mean that the results and inputs are expected to be in normal
    memory addressable by the CPU (because you don’t have any guarantees about what
    processor subsequent ops will run on), and you will either need to wait for the
    accelerator to complete before continuing or use platform-specific code to switch
    to threads outside of the Micro framework. These restrictions should at least
    enable some quick prototyping, though, and hopefully offer the ability to make
    incremental changes while always being able to test the correctness of each small
    modification.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the File Format
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The format TensorFlow Lite uses to store its models has many virtues, but unfortunately
    simplicity is not one of them. Don’t be put off by the complexity, though; it’s
    actually fairly straightforward to work with after you understand some of the
    fundamentals.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: As we touched on in [Chapter 3](ch03.xhtml#chapter_get_up_to_speed), neural
    network models are graphs of operations with inputs and outputs. Some of the inputs
    to an operation might be large arrays of learned values, known as weights, and
    others will come from the results of earlier operations, or input value arrays
    fed in by the application layer. These inputs might be image pixels, audio sample
    data, or accelerometer time-series data. At the end of running a single pass of
    the model, the final operations will leave arrays of values in their outputs,
    typically representing things like classification predictions for different categories.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Models are usually trained on desktop machines, so we need a way of transferring
    them to other devices like phones or microcontrollers. In the TensorFlow world,
    we do this using a converter that can take a trained model from Python and export
    it as a TensorFlow Lite file. This exporting stage can be fraught with problems,
    because it’s easy to create a model in TensorFlow that relies on features of the
    desktop environment (like being able to execute Python code snippets or use advanced
    operations) that are not supported on simpler platforms. It’s also necessary to
    convert all the values that are variable in training (such as weights) into constants,
    remove operations that are needed only for gradient backpropagation, and perform
    optimizations like fusing neighboring ops or folding costly operations like batch
    normalization into less expensive forms. What makes this even trickier is that
    there are more than 800 operations in mainline TensorFlow, and more are being
    added all the time. This means that it’s fairly straightforward to write your
    own converter for a small set of models, but handling the broader range of networks
    that users can create in TensorFlow reliably is much more difficult. Just keeping
    up to date with new operations is a full-time job.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow Lite file that you get out of the conversion process doesn’t
    suffer from most of these issues. We try to produce a simpler and more stable
    representation of a trained model with clear inputs and outputs, variables that
    are *frozen* into weights, and common graph optimizations like fusing already
    applied. This means that even if you’re not intending to use TensorFlow Lite for
    Microcontrollers, we recommend using the TensorFlow Lite file format as the way
    you access TensorFlow models for inference instead of writing your own converter
    from the Python layer.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: FlatBuffers
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use [FlatBuffers](https://oreil.ly/jfoBx) as our serialization library. It
    was designed for applications for which performance is critical, so it’s a good
    fit for embedded systems. One of the nice features is that its runtime in-memory
    representation is exactly the same as its serialized form, so models can be embedded
    directly into flash memory and accessed immediately, with no need for any parsing
    or copying. This does mean that the generated code classes to read properties
    can be a bit difficult to follow because there are a couple of layers of indirection,
    but the important data (such as weights) is stored directly as little-endian blobs
    that can be accessed like raw C arrays. There’s also very little wasted space,
    so you aren’t paying a size penalty by using FlatBuffers.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: FlatBuffers work using a *schema* that defines the data structures we want to
    serialize, together with a compiler that turns that schema into native C++ (or
    C, Python, Java, etc.) code for reading and writing the information. For TensorFlow
    Lite, the schema is in [*tensorflow/lite/schema/schema.fbs*](https://oreil.ly/JoDE9),
    and we cache the generated C++ accessor code at [*tensorflow/lite/schema/schema_generated.h*](https://oreil.ly/LjxOp).
    We could generate the C++ code every time we do a fresh build rather than storing
    it in source control, but this would require every platform we build on to include
    the `flatc` compiler as well as the rest of the toolchain, and we decided to trade
    the convenience of automatic generation for ease of porting.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: If you want to understand the format at the byte level, we recommend looking
    at the [internals page](https://oreil.ly/EBg3-) of the FlatBuffers C++ project
    or the equivalent for the [C library](https://oreil.ly/xXkZg). We’re hopeful that
    most needs will be met through the various high-level language interfaces, though,
    and you won’t need to work at that granularity. To introduce you to the concepts
    behind the format, we’re going to walk through the schema and the code in `MicroInterpreter`
    that reads a model; hopefully, having some concrete examples will help it all
    make sense.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Ironically, to get started we need to scroll to the [very end of the schema](https://oreil.ly/aHYM-).
    Here we see a line declaring that the `root_type` is `Model`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'FlatBuffers need a single container object that acts as the root for the tree
    of other data structures held within the file. This statement tells us that the
    root of this format is going to be a `Model`. To find out what that means, we
    scroll up a few more lines to the definition of `Model`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This tells us that `Model` is what FlatBuffers calls a `table`. You can think
    of this like a `Dict` in Python or a `struct` in C or C++ (though it’s more flexible
    than that). It defines what properties an object can have, along with their names
    and types. There’s also a less-flexible type in FlatBuffers called `struct` that’s
    more memory-efficient for arrays of objects, but we don’t currently use this in
    TensorFlow Lite.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see how this is used in practice by looking at the [`micro_speech`
    example’s `main()` function](https://oreil.ly/StkFf):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `g_tiny_conv_micro_features_model_data` variable is a pointer to an area
    of memory containing a serialized TensorFlow Lite model, and the call to `::tflite::GetModel()`
    is effectively just a cast to get a C++ object backed up by that underlying memory.
    It doesn’t require any memory allocation or walking of data structures, so it’s
    a very quick and efficient call. To understand how we can use it, look at the
    next operation we perform on the data structure:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If you look at [the start of the `Model` definition in the schema](https://oreil.ly/vPpDw),
    you can see the definition of the `version` property this code is referring to:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This informs us that the `version` property is a 32-bit unsigned integer, so
    the C++ code generated for `model->version()` returns that type of value. Here
    we’re just doing error checking to make sure the version is one that we can understand,
    but the same kind of accessor function is generated for all the properties that
    are defined in the schema.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the more complex parts of the file format, it’s worth following
    the flow of the `MicroInterpreter` class as it loads a model and prepares to execute
    it. The constructor is passed a pointer to a model in memory, such as the previous
    example’s `g_tiny_conv_micro_features_model_data`. The first property it accesses
    is [buffers](https://oreil.ly/nQjwY):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You might see the `Vector` name in the type definition, and be worried we’re
    trying to use objects similar to Standard Template Library (STL) types inside
    an embedded environment without dynamic memory management, which would be a bad
    idea. Happily, though, the FlatBuffers `Vector` class is just a read-only wrapper
    around the underlying memory, so just like with the root `Model` object, there’s
    no parsing or memory allocation required to create it.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand more about what this `buffers` array represents, it’s worth taking
    a look at [the schema definition](https://oreil.ly/QOTlY):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Each buffer is defined as a raw array of unsigned 8-bit values, with the first
    value 16-byte-aligned in memory. This is the container type used for all of the
    arrays of weights (and any other constant values) held in the graph. The type
    and shape of the tensors are held separately; this array just holds the raw bytes
    that back up the data inside the arrays. Operations refer to these constant buffers
    by index inside this top-level vector.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'The next property we access is [a list of subgraphs](https://oreil.ly/9Fa9V):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'A subgraph is a set of operators, the connections between them, and the buffers,
    inputs, and outputs that they use. There are some advanced models that might require
    multiple subgraphs in the future—for example, to support control flow—but all
    of the networks we want to support on microcontrollers at the moment have a single
    subgraph, so we can simplify our subsequent code by making sure the current model
    meets that requirement. To get more of an idea of what’s in a subgraph, we can
    look back at [the schema](https://oreil.ly/Z9mLn):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The first property every subgraph has is a list of tensors, and the `MicroInterpreter`
    code accesses it [like this](https://oreil.ly/EsO7M):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As we mentioned earlier, the `Buffer` objects just hold raw values for weights,
    without any metadata about their types or shapes. Tensors are the place where
    this extra information is stored for constant buffers. They also hold the same
    information for temporary arrays like inputs, outputs, or activation layers. You
    can see this metadata in their definition [near the top of the schema file](https://oreil.ly/mH0IL):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The `shape` is a simple list of integers that indicates the tensor’s dimensions,
    whereas `type` is an enum mapping to the possible data types that are supported
    in TensorFlow Lite. The `buffer` property indicates which `Buffer` in the root-level
    list has the actual values backing up this tensor if it’s a constant read from
    a file, or is zero if the values are calculated dynamically (for example, for
    an activation layer). The `name` is there only to give a human-readable label
    for the tensor, which can help with debugging, and the `quantization` property
    defines how to map low-precision values into real numbers. Finally, the `is_variable`
    member exists to support future training and other advanced applications, but
    it doesn’t need to be used on microcontroller units (MCUs).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the `MicroInterpreter` code, the second major property we pull
    from the subgraph is [a list of operators](https://oreil.ly/6Yl8d):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This list holds the graph structure of the model. To understand how this is
    encoded, we can go back to [the schema definition of `Operator`](https://oreil.ly/xTs7j):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `opcode_index` member is an index into the root-level `operator_codes` vector
    inside `Model`. Because a particular kind of operator, like `Conv2D`, might show
    up many times in one graph, and some ops require a string to define them, it saves
    serialization size to keep all of the op definitions in one top-level array and
    refer to them indirectly from subgraphs.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: The `inputs` and `outputs` arrays define the connections between an operator
    and its neighbors in the graph. These are lists of integers that refer to the
    tensor array in the parent subgraph, and may refer to constant buffers read from
    the model, inputs fed into the network by the application, the results of running
    other operations, or output destination buffers that will be read by the application
    after calculations have finished.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: One important thing to know about this list of operators held in the subgraph
    is that they are always in topological order, so that if you execute them from
    the beginning of the array to the end, all of the inputs for a given operation
    that rely on previous operations will have been calculated by the time that operation
    is reached. This makes writing interpreters much simpler, because the execution
    loop doesn’t need to do any graph operations beforehand and can just execute the
    operations in the order they’re listed. It does mean that running the same subgraph
    in different orders (for example, to use back-propagation with training) is not
    straightforward, but TensorFlow Lite’s focus is on inference so this is a worthwhile
    trade-off.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Operators also usually require parameters, like the shape and stride for the
    filters for a `Conv2D` kernel. The representation of these is unfortunately pretty
    complex. For historical reasons, TensorFlow Lite supports two different families
    of operations. Built-in operations came first, and are the most common ops that
    are used in mobile applications. You can see a list [in the schema](https://oreil.ly/HjdHn).
    As of November 2019 there are only 122, but TensorFlow supports more than 800
    operations—so what can we do about the remainder? Custom operations are defined
    by a string name instead of a fixed enum like built-ins, so they can be added
    more easily without touching the schema.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'For built-in ops, the parameter structures are listed in the schema. Here’s
    an example for `Conv2D`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Hopefully most of the members listed look somewhat familiar, and they are accessed
    in the same way as other FlatBuffers objects: through the `builtin_options` union
    of each `Operator` object, with the appropriate type picked based on the operator
    code (though the code to do so is based on [a monster `switch` statement)](https://oreil.ly/SkzaA).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'If the operator code turns out to indicate a custom operator, we don’t know
    the structure of the parameter list ahead of time, so we can’t generate a code
    object. Instead, the argument information is packed into a [FlexBuffer](https://oreil.ly/qPwo9).
    This is a format that the FlatBuffer library offers for encoding arbitrary data
    when you don’t know the structure in advance, which means the code implementing
    the operator needs to access the resulting data specifying what the type is, and
    with messier syntax than a built-in’s. Here’s an example from [some object detection
    code](https://oreil.ly/xQoTR):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The buffer pointer being referenced in this example ultimately comes from the
    `custom_options` member of the `Operator` table, showing how you can access parameter
    data from this property.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The final member of `Operator` is `mutating_variable_inputs`. This is an experimental
    feature to help manage Long Short-Term Memory (LSTM) and other ops that might
    want to treat their inputs as variables, and shouldn’t be relevant for most MCU
    applications.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Those are the key parts of the TensorFlow Lite serialization format. There are
    a few other members we haven’t covered (like `metadata_buffer` in `Model`), but
    these are for nonessential features that are optional and so can usually be ignored.
    Hopefully this overview will be enough to get you started on reading, writing,
    and debugging your own model files.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Porting TensorFlow Lite Mobile Ops to Micro
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are more than one hundred “built-in” operations in the mainline TensorFlow
    Lite version targeting mobile devices. TensorFlow Lite for Microcontrollers reuses
    most of the code, but because the default implementations of these ops bring in
    dependencies like pthreads, dynamic memory allocation, or other features unavailable
    on embedded systems, the op implementations (also known as kernels) require some
    work to make them available on Micro.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, we hope to unify the two branches of op implementations, but that
    effort requires some design and API changes across the framework, so it won’t
    be happening in the short term. Most ops should already have Micro implementations,
    but if you discover one that’s available on mobile TensorFlow Lite but not through
    the embedded version, this section walks you through the conversion process. After
    you’ve identified the operation you’re going to port, there are several stages.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Separate the Reference Code
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of the ops listed should already have reference code, but the functions
    are likely to be in [*reference_ops.h*](https://oreil.ly/QmW4H). This is a monolithic
    header file that’s almost 5,000 lines long. Because it covers so many operations,
    it pulls in a lot of dependencies that are not available on embedded platforms.
    To begin the porting process, you first need to extract the reference functions
    that are required for the operation you’re working on into a separate header file.
    You can see examples of these smaller headers in [*https://oreil.ly/*](https://oreil.ly/)*vH-6[_conv.h*]
    and [*pooling.h*](https://oreil.ly/pwP_0). The reference functions themselves
    should have names that match the operation they implement, and there will typically
    be multiple implementations for different data types, sometimes using templates.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as the file is separated from the larger header, you’ll need to include
    it from *reference_ops.h* so that all the existing users of that header still
    see the functions you’ve moved (though our Micro code will include only the separated
    headers individually). You can see how we do this for `conv2d` [here](https://oreil.ly/jtXLU).
    You’ll also need to add the header to the `kernels/internal/BUILD:reference_base`
    and `kernels/internal/BUILD:legacy_reference_base` build rules. After you’ve made
    those changes, you should be able to run the test suite and see all of the existing
    mobile tests passing:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This is a good point to create an initial pull request for review. You haven’t
    ported anything to the `micro` branch yet, but you’ve prepared the existing code
    for the change, so it’s worth trying to get this work reviewed and submitted while
    you work on the following steps.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Create a Micro Copy of the Operator
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each micro operator implementation is a modified copy of a mobile version held
    in *tensorflow/lite/kernels/*. For example, the micro *conv.cc* is based on the
    mobile *conv.cc*. There are a few big differences. First, dynamic memory allocation
    is trickier in embedded environments, so the creation of the OpData structure
    that caches calculated values for the calculations used during inference is moved
    into a separate function so that it can be called during `Invoke()` rather than
    returned from `Prepare()`. This involves a little more work for each `Invoke()`
    call, but the reduction in memory overhead usually makes sense for microcontrollers.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Second, most of the parameter-checking code in `Prepare()` is usually removed.
    It might be better to enclose this in `#if defined(DEBUG)` rather than removing
    it entirely, but the removal keeps the code size to a minimum. All references
    to external frameworks (`Eigen`, `gemmlowp`, `cpu_backend_support`) should be
    removed from the includes and the code. In the `Eval()` function, everything but
    the path that calls the function in the `reference_ops::` namespace should be
    removed.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: The resulting modified operator implementation should be saved in a file with
    the same name as the mobile version (usually the lowercase version of the operator
    name) in the *tensorflow/lite/micro/kernels/* folder.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Port the Test to the Micro Framework
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can’t run the full Google Test framework on embedded platforms, so we need
    to use the Micro Test library instead. This should look familiar to users of GTest,
    but it avoids any constructs that require dynamic memory allocation or C++ global
    initialization. There’s more documentation elsewhere in this book.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll want to run the same tests that you run on mobile in the embedded environment,
    so you’ll need to use the version in *tensorflow/lite/kernels/<`your op name`>_test.cc*
    as a starting point. For example, look at [*tensorflow/lite/kernels/conv_test.cc*](https://oreil.ly/76KXK)
    and the ported version [*tensorflow/lite/micro/kernels/conv_test.cc*](https://oreil.ly/r1wKh).
    Here are the big differences:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: The mobile code relies on C++ STL classes like `std::map` and `std::vector`,
    which require dynamic memory allocation.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mobile code also uses helper classes and passes in data objects in a way
    that involves allocations.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The micro version allocates all of its data on the stack, using `std::initializer_list`
    to pass down objects that look a lot like `std::vectors`, but do not require dynamic
    memory allocation.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calls to run a test are expressed as function calls rather than object allocations
    because this helps reuse a lot of code without hitting allocation issues.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most standard error checking macros are available, but with the `TF_LITE_MICRO_`
    suffix. For example, `EXPECT_EQ` becomes `TF_LITE_MICRO_EXPECT_EQ`.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tests all have to live in one file, and be surrounded by a single `TF_LITE_MICRO_TESTS_BEGIN/TF_LITE_MICRO_TESTS_END`
    pair. Under the hood this actually creates a `main()` function so that the tests
    can be run as a standalone binary.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: We also try to ensure that the tests rely on only the kernel code and API, not
    bringing in other classes like the interpreter. The tests should call into the
    kernel implementations directly, using the C API returned from `GetRegistration()`.
    This is because we want to ensure that the kernels can be used completely standalone,
    without needing the rest of the framework, so the testing code should avoid those
    dependencies, too.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Build a Bazel Test
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you have created the operator implementation and test files, you’ll
    want to check whether they work. You’ll need to use the Bazel open source build
    system to do this. Add a `tflite_micro_cc_test` rule to the [*BUILD* file](https://oreil.ly/CbwMI)
    and then try building and running this command line (replacing `conv` with your
    operator name):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: No doubt there will be compilation errors and test failures, so expect to spend
    some time iterating on fixing those.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Add Your Op to AllOpsResolver
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applications can choose to pull in only certain operator implementations for
    binary size reasons, but there’s an op resolver that pulls in all available operators,
    to make getting started easy. You should add a call to register your operator
    implementation in the constructor of [*all_ops_resolver.cc*](https://oreil.ly/0Nq06),
    and make sure the implementation and header files are included in the *BUILD*
    rules, too.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Build a Makefile Test
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, everything you’ve been doing has been within the `micro` branch of TensorFlow
    Lite, but you’ve been building and testing on x86\. This is the easiest way to
    develop, and the initial task is to create portable, unoptimized implementations
    of all the ops, so we recommend doing as much as you can in this domain. At this
    point, though, you should have a completely working and tested operator implementation
    running on desktop Linux, so it’s time to begin compiling and testing on embedded
    devices.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: The standard build system for Google open source projects is Bazel, but unfortunately
    it’s not easy to implement cross-compilation and support for embedded toolchains
    using it, so we’ve had to turn to the venerable Make for deployment. The Makefile
    itself is very complicated internally, but hopefully your new operator should
    be automatically picked up based on the name and location of its implementation
    file and test. The only manual step should be adding the reference header you
    created to the `MICROLITE_CC_HDRS` file list.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'To test your operator in this environment, `cd` to the folder, and run this
    command (with your own operator name instead of `conv`):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Hopefully this will compile and the test will pass. If not, run through the
    normal debugging procedures to work out what’s going wrong.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'This is still running natively on your local Intel x86 desktop machine, though
    it’s using the same build machinery as the embedded targets. You can try compiling
    and flashing your code onto a real microcontroller like the SparkFun Edge now
    (just passing in `TARGET=sparkfun_edge` on the Makefile line should be enough),
    but to make life easier we also have software emulation of a Cortex-M3 device
    available. You should be able to run your test through this by executing the following
    command:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This can be a little flakey because sometimes the emulator takes too long to
    execute and the process times out, but hopefully giving it a second try will fix
    it. If you’ve gotten this far, we encourage you to contribute your changes back
    to the open source build if you can. The full process of open-sourcing your code
    can be a bit involved, but [the TensorFlow Community guide](https://oreil.ly/YcbFB)
    is a good place to start.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After finishing this chapter, you might be feeling like you’ve been trying to
    drink from a fire hose. We’ve given you a lot of information about how TensorFlow
    Lite for Microcontrollers works. Don’t worry if you don’t understand it all, or
    even most of it—we just wanted to give you enough background so that if you do
    need to delve under the hood, you know where to begin looking. The code is all
    open source and is the ultimate guide to how the framework operates, but we hope
    this commentary will help you navigate its structure and understand why some of
    its design decisions were made.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: After seeing how to run some prebuilt examples and taking a deep dive into how
    the library works, you’re probably wondering how you can apply what you’ve learned
    to your own applications. The remainder of the book concentrates on the skills
    you need to be able to deploy custom machine learning in your own products, covering
    optimization, debugging, and porting models, along with privacy and security.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
