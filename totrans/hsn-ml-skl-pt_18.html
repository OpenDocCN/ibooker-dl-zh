<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 16. Vision and Multimodal Transformers"><div class="chapter" id="vit_chapter">
<h1><span class="label">Chapter 16. </span>Vision and Multimodal Transformers</h1>


<p>In the previous chapter<a data-type="indexterm" data-primary="transformers" id="xi_transformers16324_1"/>, we implemented a transformer from scratch and turned it into a translation system, then we explored encoder-only models for NLU, decoder-only models for NLG, and we even built a little chatbot—that was quite a journey! Yet, there’s still a lot more to say about transformers. In particular, we have only dealt with text so far, but transformers actually turned out to be exceptionally good at processing all sorts of inputs. In this chapter we will cover <em>vision transformers</em> (ViTs), capable of processing images, followed<a data-type="indexterm" data-primary="multimodal transformers" id="id3720"/> by <em>multimodal transformers</em>, capable of handling multiple modalities, including text, images, audio, videos, robot sensors and actuators, and really any kind of data.</p>

<p>In the first part of this chapter, we will discuss some of the most influential pure-vision transformers:</p>
<dl>
<dt>DETR (Detection Transformer)</dt>
<dd>
<p>An early encoder-decoder transformer for object detection.</p>
</dd>
<dt>The original ViT (Vision Transformer)</dt>
<dd>
<p>This landmark encoder-only transformer treats image patches like word tokens and reaches the state of the art if trained on a large dataset.</p>
</dd>
<dt>DeiT (Data-Efficient Image Transformer)</dt>
<dd>
<p>A more data-efficient ViT trained at scale using distillation.</p>
</dd>
<dt>PVT (Pyramid Vision Transformer)</dt>
<dd>
<p>A hierarchical model that can produce multiscale feature maps for semantic segmentation and other dense prediction tasks.</p>
</dd>
<dt>Swin Transformer (Shifted Windows Transformer)</dt>
<dd>
<p>A much faster hierarchical model.</p>
</dd>
<dt>DINO (self-Distillation with NO labels)</dt>
<dd>
<p>This introduced a novel self-supervised technique for visual representation 
<span class="keep-together">learning.</span></p>
</dd>
</dl>

<p>In the second part of this chapter, we will dive into multimodal transformers:</p>
<dl>
<dt>VideoBERT</dt>
<dd>
<p>A BERT model trained to process both text and video tokens.</p>
</dd>
<dt>ViLBERT (Visio-Linguistic BERT)</dt>
<dd>
<p>A dual-encoder model for image plus text, which introduced co-attention (i.e., two-way cross-attention).</p>
</dd>
<dt>CLIP (Contrastive Language–Image Pretraining)</dt>
<dd>
<p>This is another image plus text dual-encoder model trained using contrastive pretraining.</p>
</dd>
<dt>DALL·E (a pun on the names of the artist Salvador Dali and the Pixar character Wall-E)</dt>
<dd>
<p>A model capable of generating images from text prompts.</p>
</dd>
<dt>Perceiver</dt>
<dd>
<p>This efficiently compresses any high-resolution modality into a short sequence using a cross-attention trick.</p>
</dd>
<dt>Perceiver IO (Input/Output)</dt>
<dd>
<p>Adds a flexible output mechanism to the Perceiver, using a similar cross-attention trick.</p>
</dd>
<dt>Flamingo</dt>
<dd>
<p>Rather than starting from scratch, it reuses two large pretrained models—one for vision and one for language (both frozen)—and connects them using a Perceiver-style adapter named a Resampler. This architecture enables open-ended visual dialogue.</p>
</dd>
<dt>BLIP-2 (Bootstrapping Language-Image Pretraining)</dt>
<dd>
<p>This is another open-ended visual dialogue model that reuses two large pretrained models, connects them using a lightweight querying transformer (Q-Former), and uses a powerful two-stage training approach with multiple training objectives.</p>
</dd>
</dl>

<p>So turn on the lights, transformers are about to open their eyes.</p>






<section data-type="sect1" data-pdf-bookmark="Vision Transformers"><div class="sect1" id="id311">
<h1>Vision Transformers</h1>

<p>Vision transformers<a data-type="indexterm" data-primary="transformers" data-secondary="vision transformers" id="xi_transformersvisiontransformers164220_1"/><a data-type="indexterm" data-primary="vision transformers (ViTs)" id="xi_visiontransformersViTs164220_1"/> didn’t pop out of a vacuum: before they were invented, there were RNNs with visual attention<a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="and vision transformers" data-secondary-sortas="vision transformers" id="xi_recurrentneuralnetworksRNNsandvisiontransformers1642113_1"/>, and hybrid CNN-Transformer models. Let’s take a look at these ViT ancestors before we dive into some of the most influential ViTs.</p>








<section data-type="sect2" data-pdf-bookmark="RNNs with Visual Attention"><div class="sect2" id="id312">
<h2>RNNs with Visual Attention</h2>

<p>One of the first applications of attention mechanisms<a data-type="indexterm" data-primary="vision transformers (ViTs)" data-secondary="RNNs with visual attention" id="xi_visiontransformersViTsRNNswithvisualattention164554_1"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="RNNs with visual attention" id="xi_attentionmechanismsRNNswithvisualattention164554_1"/><a data-type="indexterm" data-primary="visual attention, RNNs with" id="xi_visualattentionRNNswith164554_1"/><a data-type="indexterm" data-primary="decoder-only transformers" data-secondary="RNNs with visual attention" id="xi_decoderonlytransformersRNNswithvisualattention164554_1"/><a data-type="indexterm" data-primary="transformers" data-secondary="decoder-only" id="xi_transformersdecoderonly164554_1"/> beyond NLP was to generate image captions<a data-type="indexterm" data-primary="captioning of images and video" id="id3721"/> using <a href="https://homl.info/visualattention">visual attention</a>.⁠<sup><a data-type="noteref" id="id3722-marker" href="ch16.html#id3722">1</a></sup> Here a convolutional neural network<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="and vision transformers" data-secondary-sortas="vision transformers" id="id3723"/> first processes the image and outputs some feature maps, then a decoder RNN equipped with an attention mechanism generates the caption, one token at a time.</p>

<p>The decoder uses an attention layer at each decoding step to focus on just the right part of the image. For example, in <a data-type="xref" href="#visual_attention_diagram">Figure 16-1</a>, the model generated the caption “A woman is throwing a Frisbee in a park”, and you can see what part of the input image the decoder focused its attention on when it was about to output the word “Frisbee”: clearly, most of its attention was focused on the Frisbee.</p>

<figure><div id="visual_attention_diagram" class="figure">
<img src="assets/hmls_1601.png" alt="A woman in green throws a red Frisbee in a park, and the attention layer highlights the Frisbee when producing the word &quot;Frisbee&quot; in a caption." width="1439" height="689"/>
<h6><span class="label">Figure 16-1. </span>Visual attention: an input image (left) and the model’s focus before producing the word “Frisbee” (right)⁠<sup><a data-type="noteref" id="id3724-marker" href="ch16.html#id3724">2</a></sup></h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id3725">
<h1>Explainability</h1>
<p>One extra benefit of attention mechanisms<a data-type="indexterm" data-primary="explainability, attention mechanisms" id="id3726"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="explainability" id="id3727"/> is that they make it easier to understand what led the model to produce its output. This is called <em>explainability</em>. It can be especially useful when the model makes a mistake; for example, if an image of a dog walking in the snow is labeled as “a wolf walking in the snow”, then you can go back and check what the model focused on when it output the word “wolf”. You may find that it was paying attention not only to the dog, but also to the snow, hinting at a possible explanation: perhaps he model learned to distinguish dogs from wolves by checking whether there’s a lot of snow around. You can then fix this by training the model with more images of wolves without snow, and dogs with snow. This example comes from a great <a href="https://homl.info/explainclass">2016 paper</a>⁠<sup><a data-type="noteref" id="id3728-marker" href="ch16.html#id3728">3</a></sup> by Marco Tulio Ribeiro et al. that uses a different approach to explainability: learning an interpretable model locally around a classifier’s prediction.</p>

<p>In some applications, explainability is not just a tool to debug a model; it can be a legal requirement—think of a system deciding whether it should grant you a loan<a data-type="indexterm" data-startref="xi_visiontransformersViTsRNNswithvisualattention164554_1" id="id3729"/><a data-type="indexterm" data-startref="xi_attentionmechanismsRNNswithvisualattention164554_1" id="id3730"/><a data-type="indexterm" data-startref="xi_visualattentionRNNswith164554_1" id="id3731"/><a data-type="indexterm" data-startref="xi_decoderonlytransformersRNNswithvisualattention164554_1" id="id3732"/><a data-type="indexterm" data-startref="xi_transformersdecoderonly164554_1" id="id3733"/>.</p>
</div></aside>

<p>Once transformers were invented, they were quickly applied to visual tasks, generally by replacing RNNs in existing architectures (e.g., for image captioning)<a data-type="indexterm" data-startref="xi_recurrentneuralnetworksRNNsandvisiontransformers1642113_1" id="id3734"/>. However, the bulk of the visual work was still performed by a CNN, so although they were transformers used for visual tasks, we usually don’t consider them as ViTs. The <em>detection transformer</em> (DETR) is a good example of this.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="DETR: A CNN-Transformer Hybrid for Object Detection"><div class="sect2" id="id313">
<h2>DETR: A CNN-Transformer Hybrid for Object Detection</h2>

<p>In May 2020, a team of Facebook researchers proposed a hybrid CNN–transformer architecture<a data-type="indexterm" data-primary="vision transformers (ViTs)" data-secondary="DETR" id="id3735"/><a data-type="indexterm" data-primary="detection transformer (DETR)" id="id3736"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="DETR transformer hybrid" id="id3737"/><a data-type="indexterm" data-primary="object detection" id="id3738"/><a data-type="indexterm" data-primary="dense prediction, transformers for" data-secondary="object detection" id="id3739"/> for object detection, named <a href="https://homl.info/detr"><em>detection transformer</em></a> (DETR, see <a data-type="xref" href="#detr_diagram">Figure 16-2</a>).⁠<sup><a data-type="noteref" id="id3740-marker" href="ch16.html#id3740">4</a></sup> The CNN first processes the input images and outputs a set of feature maps, then these feature maps are turned into a sequence of visual tokens that are fed to an encoder-decoder transformer, and finally the transformer outputs a sequence of bounding box predictions.</p>

<p>At one point, someone was bound to wonder whether we could get rid of the CNN entirely. After all, attention is all you need, right? This happened a few months after DETR: the original ViT was born.</p>

<figure class="width-70"><div id="detr_diagram" class="figure">
<img src="assets/hmls_1602.png" alt="Diagram of the detection transformer (DETR) process illustrating how visual tokens and positional encoding are used to identify objects like a temple and a tree with confidence levels." width="969" height="776"/>
<h6><span class="label">Figure 16-2. </span>The detection transformer (DETR) for object detection</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The Original ViT"><div class="sect2" id="id314">
<h2>The Original ViT</h2>

<p>In October 2020<a data-type="indexterm" data-primary="vision transformers (ViTs)" data-secondary="origins at Google" id="xi_visiontransformersViTsoriginsatGoogle167316_1"/>, a team of Google researchers released <a href="https://homl.info/vit">a paper</a>⁠<sup><a data-type="noteref" id="id3741-marker" href="ch16.html#id3741">5</a></sup> that introduced the first vision transformer without a CNN (see <a data-type="xref" href="#vit_diagram">Figure 16-3</a>). It was simply named the <em>vision transformer</em> (ViT). The idea is surprisingly simple: chop the image into little 16 × 16 patches, and treat the sequence of patches as if it is a sequence of word representations. In fact, the paper’s title is “An Image Is Worth 16 × 16 Words”.</p>

<p>To be more precise, the patches are first flattened into 16 × 16 × 3 = 768-dimensional vectors (the 3 is for the RGB color channels). For example, a 224 × 224 image gets chopped into 14 × 14 = 196 patches, so we get 196 vectors of 768 dimensions each. These vectors then go through a linear layer that projects the vectors to the transformer’s embedding size. The resulting sequence of vectors can then be treated just like a sequence of word embeddings: add learnable positional embeddings, and pass the result to the transformer, which is a regular encoder-only model<a data-type="indexterm" data-primary="encoder-only transformers" data-secondary="and ViTs" id="id3742"/><a data-type="indexterm" data-primary="transformers" data-secondary="encoder-only" id="id3743"/>. A class token with a trainable representation is inserted at the start of the sequence, and a classification head is added on top of the corresponding output (i.e., this is BERT-style classification).</p>

<p>And that’s it! This model beat the state of the art on ImageNet image classification, but to be fair the authors had to use over 300 million additional images for training. This makes sense since transformers don’t have as many <em>inductive biases</em> as convolution neural nets, so they need extra data just to learn things that CNNs implicitly assume.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>An inductive bias<a data-type="indexterm" data-primary="inductive bias" id="id3744"/><a data-type="indexterm" data-primary="bias" data-secondary="inductive bias" id="id3745"/> is an implicit assumption made by the model, due to its architecture. For example, linear models implicitly assume that the data is, well, linear. CNNs<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="biases of" id="id3746"/><a data-type="indexterm" data-primary="bias" data-secondary="convolutional layers" id="id3747"/> are translation invariant, so they implicitly assume that patterns learned in one location will likely be useful in other locations as well. They also have a strong bias toward locality. RNNs<a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="inductive biases of" id="id3748"/> implicitly assume that the inputs are ordered, and that recent tokens are more important than older ones. The more inductive biases a model has, assuming they are correct, the less training data the model will require. But if the implicit assumptions are wrong, then the model may perform poorly even if it is trained on a large dataset.</p>
</div>

<figure class="width-75"><div id="vit_diagram" class="figure">
<img src="assets/hmls_1603.png" alt="Diagram illustrating the Vision Transformer (ViT) model, showing the process of converting image patches into token embeddings for classification." width="921" height="631"/>
<h6><span class="label">Figure 16-3. </span>Vision transformer (ViT) for classification</h6>
</div></figure>

<p>Now you know everything you need to implement a ViT from scratch<a data-type="indexterm" data-startref="xi_visiontransformersViTsoriginsatGoogle167316_1" id="id3749"/>!</p>










<section data-type="sect3" data-pdf-bookmark="Implementing a ViT from scratch using PyTorch"><div class="sect3" id="id315">
<h3>Implementing a ViT from scratch using PyTorch</h3>

<p>We will start by implementing<a data-type="indexterm" data-primary="vision transformers (ViTs)" data-secondary="implementing a ViT using PyTorch" id="xi_visiontransformersViTsimplementingaViTusingPyTorch168830_1"/><a data-type="indexterm" data-primary="PyTorch" data-secondary="implementing a ViT" id="xi_PyTorchimplementingaViT168830_1"/> a custom module to take care of patch embedding<a data-type="indexterm" data-primary="patch embedding" id="xi_patchembedding168878_1"/><a data-type="indexterm" data-primary="embeddings" data-secondary="patch" id="xi_embeddingspatch168878_1"/>. For this, we can actually use an <code translate="no">nn.Conv2d</code> module with <code translate="no">kernel_size</code> and <code translate="no">stride</code> both set to the patch size (16). This is equivalent to chopping the image into patches, flattening them, and passing them through a linear layer (then reshaping the result). Just what we need!</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>

<code class="k">class</code> <code class="nc">PatchEmbedding</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">in_channels</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">,</code> <code class="n">patch_size</code><code class="o">=</code><code class="mi">16</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">conv2d</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">in_channels</code><code class="p">,</code>
                                <code class="n">kernel_size</code><code class="o">=</code><code class="n">patch_size</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="n">patch_size</code><code class="p">)</code>
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">X</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">conv2d</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>  <code class="c1"># shape [B=Batch, C=Channels, H=Height, W=Width]</code>
        <code class="n">X</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">flatten</code><code class="p">(</code><code class="n">start_dim</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>  <code class="c1"># shape [B, C, H * W]</code>
        <code class="k">return</code> <code class="n">X</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>  <code class="c1"># shape [B, H * W, C]</code></pre>

<p>After the convolutional layer, we must flatten the spatial dimensions and transpose the last two dimensions to ensure the embedding dimension ends up last, which is what the <code translate="no">nn.TransformerEncoder</code> module expects. Now we’re ready to implement our ViT model:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ViT</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">img_size</code><code class="o">=</code><code class="mi">224</code><code class="p">,</code> <code class="n">patch_size</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code> <code class="n">in_channels</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
                 <code class="n">num_classes</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">embed_dim</code><code class="o">=</code><code class="mi">768</code><code class="p">,</code> <code class="n">depth</code><code class="o">=</code><code class="mi">12</code><code class="p">,</code> <code class="n">num_heads</code><code class="o">=</code><code class="mi">12</code><code class="p">,</code>
                 <code class="n">ff_dim</code><code class="o">=</code><code class="mi">3072</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.1</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">patch_embed</code> <code class="o">=</code> <code class="n">PatchEmbedding</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">in_channels</code><code class="p">,</code> <code class="n">patch_size</code><code class="p">)</code>
        <code class="n">cls_init</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">)</code> <code class="o">*</code> <code class="mf">0.02</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">cls_token</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Parameter</code><code class="p">(</code><code class="n">cls_init</code><code class="p">)</code>  <code class="c1"># shape [1, 1, E=embed_dim]</code>
        <code class="n">num_patches</code> <code class="o">=</code> <code class="p">(</code><code class="n">img_size</code> <code class="o">//</code> <code class="n">patch_size</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code>  <code class="c1"># num_patches (denoted L)</code>
        <code class="n">pos_init</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">num_patches</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">)</code> <code class="o">*</code> <code class="mf">0.02</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">pos_embed</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Parameter</code><code class="p">(</code><code class="n">pos_init</code><code class="p">)</code>  <code class="c1"># shape [1, 1 + L, E]</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">p</code><code class="o">=</code><code class="n">dropout</code><code class="p">)</code>
        <code class="n">encoder_layer</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">TransformerEncoderLayer</code><code class="p">(</code>
            <code class="n">d_model</code><code class="o">=</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">nhead</code><code class="o">=</code><code class="n">num_heads</code><code class="p">,</code> <code class="n">dim_feedforward</code><code class="o">=</code><code class="n">ff_dim</code><code class="p">,</code>
            <code class="n">dropout</code><code class="o">=</code><code class="n">dropout</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"gelu"</code><code class="p">,</code> <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">TransformerEncoder</code><code class="p">(</code><code class="n">encoder_layer</code><code class="p">,</code> <code class="n">num_layers</code><code class="o">=</code><code class="n">depth</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">layer_norm</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LayerNorm</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">num_classes</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">patch_embed</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>  <code class="c1"># shape [B, L, E]</code>
        <code class="n">cls_expd</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">cls_token</code><code class="o">.</code><code class="n">expand</code><code class="p">(</code><code class="n">Z</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># shape [B, 1, E]</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">((</code><code class="n">cls_expd</code><code class="p">,</code> <code class="n">Z</code><code class="p">),</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># shape [B, 1 + L, E]</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">Z</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">pos_embed</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code>  <code class="c1"># shape [B, 1 + L, E]</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">layer_norm</code><code class="p">(</code><code class="n">Z</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">])</code>  <code class="c1"># shape [B, E]</code>
        <code class="n">logits</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code> <code class="c1"># shape [B, C]</code>
        <code class="k">return</code> <code class="n">logits</code></pre>

<p>Let’s go through this code:</p>

<ul>
<li>
<p>The constructor starts by creating the <code translate="no">PatchEmbedding</code> module.</p>
</li>
<li>
<p>Then it creates the class token’s trainable embedding,  initialized using a normal distribution with a small standard deviation (0.02 is common). Its shape is [1, 1, <em>E</em>], where <em>E</em> is the embedding dimension.</p>
</li>
<li>
<p>Next, we initialize the positional embeddings, of shape [1, 1 + <em>L</em>, <em>E</em>], where <em>L</em> is the number of patch tokens. We need one more positional embedding for the class token, hence the 1 + <em>L</em>. Again, we initialize it using a normal distribution with a small standard deviation.</p>
</li>
<li>
<p>Next, we create the other modules: <code translate="no">nn.Dropout</code>, <code translate="no">nn.TransformerEncoder</code> (based on an <code translate="no">nn.TransformerEncoderLayer</code>), <code translate="no">nn.LayerNorm</code>, and the output linear layer that we will use as a classification head.</p>
</li>
<li>
<p>In the <code translate="no">forward()</code> method, we start by creating the patch tokens.</p>
</li>
<li>
<p>Then we replicate the class token along the batch axis, using the <code translate="no">expand()</code> method, and we concatenate the patch tokens. This ensures that each sequence of patch tokens starts with the class token.</p>
</li>
<li>
<p>The rest is straightforward: we add the positional embeddings, apply some dropout, run the encoder, keep only the class token’s output (<code translate="no">Z[:, 0]</code>) and normalize it, and lastly pass it through the output layer, which produces the logits.</p>
</li>
</ul>

<p>You can create the model and test it with a random batch of images, like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">vit_model</code> <code class="o">=</code> <code class="n">ViT</code><code class="p">(</code>
    <code class="n">img_size</code><code class="o">=</code><code class="mi">224</code><code class="p">,</code> <code class="n">patch_size</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code> <code class="n">in_channels</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">num_classes</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">embed_dim</code><code class="o">=</code><code class="mi">768</code><code class="p">,</code>
    <code class="n">depth</code><code class="o">=</code><code class="mi">12</code><code class="p">,</code> <code class="n">num_heads</code><code class="o">=</code><code class="mi">12</code><code class="p">,</code> <code class="n">ff_dim</code><code class="o">=</code><code class="mi">3072</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
<code class="n">batch</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">224</code><code class="p">,</code> <code class="mi">224</code><code class="p">)</code>
<code class="n">logits</code> <code class="o">=</code> <code class="n">vit_model</code><code class="p">(</code><code class="n">batch</code><code class="p">)</code>  <code class="c1"># shape [4, 1000]</code></pre>

<p>You can then train this model using the <code translate="no">nn.CrossEntropyLoss</code>, as usual<a data-type="indexterm" data-startref="xi_visiontransformersViTsimplementingaViTusingPyTorch168830_1" id="id3750"/><a data-type="indexterm" data-startref="xi_PyTorchimplementingaViT168830_1" id="id3751"/><a data-type="indexterm" data-startref="xi_patchembedding168878_1" id="id3752"/><a data-type="indexterm" data-startref="xi_embeddingspatch168878_1" id="id3753"/>. This would take quite a while, however, so unless your image dataset is very domain-specific, you’re usually better off downloading a pretrained ViT using the Transformers library and then fine-tuning it on your dataset. Let’s see how.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Fine-tuning a pretrained ViT using the Transformers library"><div class="sect3" id="id316">
<h3>Fine-tuning a pretrained ViT using the Transformers library</h3>

<p>Let’s download a small pretrained ViT<a data-type="indexterm" data-primary="vision transformers (ViTs)" data-secondary="fine-tuning a pretrained Vit" id="xi_visiontransformersViTsfinetuningapretrainedVit1616538_1"/><a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="fine-tuning a pretrained ViT" id="xi_pretrainingandpretrainedlayersfinetuningapretrainedViT1616538_1"/><a data-type="indexterm" data-primary="Transformers library" data-secondary="fine-tuning a pretrained ViT" id="xi_TransformerslibraryfinetuningapretrainedViT1616538_1"/> and fine-tune it on the Oxford-IIIT Pet dataset<a data-type="indexterm" data-primary="Oxford-IIIT Pet dataset" id="id3754"/>, which contains over 7,000 pictures of pets grouped into 37 different classes. First, let’s download the dataset:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">load_dataset</code>

<code class="n">pets</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"timm/oxford-iiit-pet"</code><code class="p">)</code></pre>

<p class="pagebreak-before">Next, let’s download the ViT:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">ViTForImageClassification</code><code class="p">,</code> <code class="n">AutoImageProcessor</code>

<code class="n">model_id</code> <code class="o">=</code> <code class="s2">"google/vit-base-patch16-224-in21k"</code>
<code class="n">vit_model</code> <code class="o">=</code> <code class="n">ViTForImageClassification</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_id</code><code class="p">,</code> <code class="n">num_labels</code><code class="o">=</code><code class="mi">37</code><code class="p">)</code>
<code class="n">vit_processor</code> <code class="o">=</code> <code class="n">AutoImageProcessor</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_id</code><code class="p">,</code> <code class="n">use_fast</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>We’re loading a base ViT model that was pretrained on the ImageNet-21k dataset<a data-type="indexterm" data-primary="ImageNet-21k dataset" id="id3755"/>. This dataset contains roughly 14 million images across over 21,800 classes. We’re using the <code translate="no">ViTForImageClassification</code> class which automatically replaces the original classification head with a new one (untrained) for the desired number of classes. That’s the part we now need to train.</p>

<p>We also loaded the image processor for this model. We will use it to preprocess each image as the model expects: it will be rescaled to 224 × 224, pixel values will be normalized to range between –1 and 1, and the channel dimension will be moved in front of the spatial dimensions. We also set <code translate="no">use_fast=True</code> because a fast implementation of the image processor is available, so we might as well use it. The processor takes an image as input and returns a dictionary containing a “pixel_values” entry equal to the preprocessed image.</p>

<p>Next, we need a data collator that will preprocess all the images in a batch and return the images and labels as PyTorch tensors:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">vit_collate_fn</code><code class="p">(</code><code class="n">batch</code><code class="p">):</code>
    <code class="n">images</code> <code class="o">=</code> <code class="p">[</code><code class="n">example</code><code class="p">[</code><code class="s2">"image"</code><code class="p">]</code> <code class="k">for</code> <code class="n">example</code> <code class="ow">in</code> <code class="n">batch</code><code class="p">]</code>
    <code class="n">labels</code> <code class="o">=</code> <code class="p">[</code><code class="n">example</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]</code> <code class="k">for</code> <code class="n">example</code> <code class="ow">in</code> <code class="n">batch</code><code class="p">]</code>
    <code class="n">inputs</code> <code class="o">=</code> <code class="n">vit_processor</code><code class="p">(</code><code class="n">images</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">,</code> <code class="n">do_convert_rgb</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
    <code class="n">inputs</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">]</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">labels</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">inputs</code></pre>

<p>We set <code translate="no">do_convert_rgb=True</code> because the model expects RGB images, but some images in the dataset are RGBA (i.e., they have an extra transparency channel), so we must force the conversion to RGB to avoid an error in the middle of training. And now we’re ready to train our model using the familiar Hugging Face training API:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">Trainer</code><code class="p">,</code> <code class="n">TrainingArguments</code>

<code class="n">args</code> <code class="o">=</code> <code class="n">TrainingArguments</code><code class="p">(</code><code class="s2">"my_pets_vit"</code><code class="p">,</code> <code class="n">per_device_train_batch_size</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code>
                         <code class="n">eval_strategy</code><code class="o">=</code><code class="s2">"epoch"</code><code class="p">,</code> <code class="n">num_train_epochs</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
                         <code class="n">remove_unused_columns</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
<code class="n">trainer</code> <code class="o">=</code> <code class="n">Trainer</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="n">vit_model</code><code class="p">,</code> <code class="n">args</code><code class="o">=</code><code class="n">args</code><code class="p">,</code> <code class="n">data_collator</code><code class="o">=</code><code class="n">vit_collate_fn</code><code class="p">,</code>
                  <code class="n">train_dataset</code><code class="o">=</code><code class="n">pets</code><code class="p">[</code><code class="s2">"train"</code><code class="p">],</code> <code class="n">eval_dataset</code><code class="o">=</code><code class="n">pets</code><code class="p">[</code><code class="s2">"test"</code><code class="p">])</code>
<code class="n">train_output</code> <code class="o">=</code> <code class="n">trainer</code><code class="o">.</code><code class="n">train</code><code class="p">()</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>By default, the trainer will automatically remove input attributes that are not used by the <code translate="no">forward()</code> method<a data-type="indexterm" data-primary="torch" data-secondary="forward()" id="id3756"/>: our model expects <code translate="no">pixel_values</code> and optionally <code translate="no">labels</code>, but anything else will be dropped, including the <code translate="no">"image"</code> attribute. Since the unused attributes are dropped before the <code translate="no">collate_fn()</code> function is called, the code <code translate="no">example["image"]</code> will cause an error. This is why we must set <code translate="no">remove_unused_columns=False</code>.</p>
</div>

<p>After just 3 epochs, our ViT model reaches about 91.8% accuracy<a data-type="indexterm" data-startref="xi_visiontransformersViTsfinetuningapretrainedVit1616538_1" id="id3757"/><a data-type="indexterm" data-startref="xi_pretrainingandpretrainedlayersfinetuningapretrainedViT1616538_1" id="id3758"/><a data-type="indexterm" data-startref="xi_TransformerslibraryfinetuningapretrainedViT1616538_1" id="id3759"/>. With some data augmentation and more training, you could reach 93 to 95% accuracy, which is close to the state of the art. Great! But we’re just getting started: ViTs have been improved in many ways since 2020. In particular, it’s possible to train them from scratch in a much more efficient way using distillation. Let’s see how.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Data-Efficient Image Transformer"><div class="sect2" id="id317">
<h2>Data-Efficient Image Transformer</h2>

<p>Just two months after Google’s ViT paper was published<a data-type="indexterm" data-primary="data-efficient image transformers (DeiT)" id="xi_dataefficientimagetransformersDeiT1622055_1"/><a data-type="indexterm" data-primary="DeiT (data-efficient image transformers)" id="xi_DeiTdataefficientimagetransformers1622055_1"/><a data-type="indexterm" data-primary="distillation, model" data-secondary="DeiT" id="xi_distillationmodelDeiT1622055_1"/>, a team of Facebook researchers<a data-type="indexterm" data-primary="vision transformers (ViTs)" data-secondary="data-efficient image transformer" id="xi_visiontransformersViTsdataefficientimagetransformer1622087_1"/> released <a href="https://homl.info/deit"><em>data-efficient image transformers</em></a> (DeiT).⁠<sup><a data-type="noteref" id="id3760-marker" href="ch16.html#id3760">6</a></sup> Their DeiT model achieved competitive results on ImageNet without requiring any additional data for training. The model’s architecture is virtually the same as the original ViT (see <a data-type="xref" href="#deit_diagram">Figure 16-4</a>), but the authors used a distillation technique to transfer knowledge from a teacher model to their student ViT model (distillation was introduced in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>).</p>

<p>The authors used a frozen, state-of-the-art CNN as the teacher model. During training, they added a special distillation token to the student ViT model. Just like the class token, the distillation token representation is trainable, and its output goes through a dedicated classification head. Both classification heads (for the class token and for the distillation token) are trained simultaneously, both using the cross-entropy loss, but the class token’s classification head is trained using the normal hard targets (i.e., one-hot vectors), while the distillation head is trained using soft targets output by a teacher model. The final loss is a weighted sum of both classification losses (typically with equal weights). At inference time, the distillation token is dropped, along with its classification head. And that’s all there is to it! If you fine-tune a DeiT model on the same pets dataset, using <code translate="no">model_id = "facebook/deit-base-distilled-patch16-224"</code> and <code translate="no">DeiTForImageClassification</code>, you should get around 94.4% validation accuracy after just three epochs.</p>

<figure class="width-85"><div id="deit_diagram" class="figure">
<img src="assets/hmls_1604.png" alt="Diagram illustrating the data-efficient image transformer (DeiT) process, showing the flow from image patches through linear transformation, encoding with position and class tokens, distillation token inclusion, to classifier heads with hard and soft target outputs from a CNN." width="1158" height="767"/>
<h6><span class="label">Figure 16-4. </span>Data-efficient image transformer (DeiT) = ViT + distillation</h6>
</div></figure>

<p>So far, we have only used ViTs for classification tasks, but what about dense prediction tasks such as object detection or semantic segmentation (introduced in <a data-type="xref" href="ch12.html#cnn_chapter">Chapter 12</a>)? For this, the ViT architecture needs to be tweaked a bit; welcome to hierarchical vision transformers<a data-type="indexterm" data-startref="xi_dataefficientimagetransformersDeiT1622055_1" id="id3761"/><a data-type="indexterm" data-startref="xi_DeiTdataefficientimagetransformers1622055_1" id="id3762"/><a data-type="indexterm" data-startref="xi_visiontransformersViTsdataefficientimagetransformer1622087_1" id="id3763"/><a data-type="indexterm" data-startref="xi_distillationmodelDeiT1622055_1" id="id3764"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Pyramid Vision Transformer for Dense Prediction Tasks"><div class="sect2" id="id318">
<h2>Pyramid Vision Transformer for Dense Prediction Tasks</h2>

<p>The year 2021 was a year of plenty for ViTs: new models<a data-type="indexterm" data-primary="vision transformers (ViTs)" data-secondary="PVT" id="xi_visiontransformersViTsPVT1623156_1"/><a data-type="indexterm" data-primary="Pyramid Vision Transformer (PVT) for dense prediction" id="xi_PyramidVisionTransformerPVTfordenseprediction1623156_1"/><a data-type="indexterm" data-primary="dense prediction, transformers for" data-secondary="PVT" id="xi_densepredictiontransformersforPVT1623156_1"/><a data-type="indexterm" data-primary="encoder-only transformers" data-secondary="PVT" id="xi_encoderonlytransformersPVT1623156_1"/><a data-type="indexterm" data-primary="transformers" data-secondary="encoder-only" id="xi_transformersencoderonly1623156_1"/> advanced the state of the art almost every other week. An important milestone was the release of the <a href="https://homl.info/pvt">Pyramid Vision Transformer (PVT)</a> in February 2021,⁠<sup><a data-type="noteref" id="id3765-marker" href="ch16.html#id3765">7</a></sup> developed by a team of researchers from Nanjing University, HKU, IIAI, and SenseTime Research. They pointed out that the original ViT architecture was good at classification tasks, but not so much at dense prediction tasks, where fine-grained resolution is needed. To solve this issue, they proposed a pyramidal architecture in which the image is processed into a gradually smaller but deeper image (i.e., semantically richer), much like in a CNN. <a data-type="xref" href="#pvt_diagram">Figure 16-5</a> shows how a 256 × 192 image with 3 channels (RGB) is first turned into a 64 × 48 image with 64 channels, then into a 32 × 24 image with 128 channels, then a 16 × 12 image with 320 channels, and lastly an 8 × 6 image with 512 channels.</p>

<figure><div id="pvt_diagram" class="figure">
<img src="assets/hmls_1605.png" alt="Diagram illustrating the Pyramid Vision Transformer process, showing how an image is transformed through multiple stages with increasing channels and reduced spatial resolution, ultimately used for dense prediction tasks." width="1378" height="861"/>
<h6><span class="label">Figure 16-5. </span>Pyramid Vision Transformer for dense prediction tasks</h6>
</div></figure>

<p>At each pyramid level, the input image is processed very much like in a regular ViT. It is first chopped into patches and turned into a sequence of patch tokens, then trainable positional embeddings are added, and the resulting tokens are passed through an encoder-only transformer, composed of multiple encoder layers.</p>

<p>Since the encoder outputs a sequence of vectors (i.e., contextualized embeddings), this sequence must be reshaped into a <em>grid</em> of vectors, which can then be treated as an image (with many channels) and passed on to the next level of the pyramid. For example, the encoder at the first level receives a sequence of 3,072 patch tokens, since the image was chopped into a 64 × 48 grid of 4 × 4 patches (and 64 × 48 = 3,072). Each patch token is represented as a 64-dimensional vector. The encoder also outputs 3,072 vectors (i.e., contextualized embeddings), each 64-dimensional, and they are organized into a 64 × 48 grid once again. This gives us a 64 × 48 image with 64 channels, which can be passed on to the next level. In levels 2, 3, and 4 of the pyramid, the patch tokens are 128-, 320-, and 512-dimensional, respectively.</p>

<p>Importantly, the patches are much smaller than in the original ViT: instead of 16 × 16, they are just 4 × 4 at level 1, and 2 × 2 at levels 2, 3, and 4. These tiny patches offer a much higher spatial resolution, which is crucial for dense prediction tasks. However, this comes at a cost: smaller patches means many more of them, and since multi-head attention has quadratic complexity, a naive adaptation of ViT would require vastly more computation. This is why the PVT authors<a data-type="indexterm" data-primary="spatial reduction attention (SRA)" id="id3766"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="SRA" id="id3767"/><a data-type="indexterm" data-primary="SRA (spatial reduction attention)" id="id3768"/><a data-type="indexterm" data-primary="multi-head attention (MHA)" data-secondary="versus SRA" data-secondary-sortas="SRA" id="id3769"/> introduced <em>spatial reduction attention</em> (SRA): it’s just like MHA except that the keys and values are first spatially reduced (but not the queries). For this, the authors proposed a sequence of operations that is usually implemented as a strided convolutional layer, followed by layer norm (although some implementations use an average pooling layer instead).</p>

<p>Let’s look at the impact of SRA at the first level of the pyramid. There are 3,072 patch tokens. In regular MHA, each of these tokens would attend to every token, so we would have to compute 3,072<sup>2</sup> attention scores: that’s over 9 million scores! In SRA, the query is unchanged so it still involves 3,072 tokens, but the keys and values are reduced spatially by a factor of 8, both horizontally and vertically (in levels 2, 3, and 4 of the pyramid, the reduction factor is 4, 2, and 1, respectively). So instead of 3,072 tokens representing a 64 × 48 grid, the keys and values are only composed of 48 tokens representing an 8 × 6 grid (because 64 / 8 = 8 and 48 / 8 = 6). So we only need to compute 3,072 × 48 = 147,456 attention scores: that’s 64 times less computationally expensive. And the good news is that this doesn’t affect the output resolution since we didn’t reduce the query at all: the encoder still output 3,072 tokens, representing a 
<span class="keep-together">64 × 48 image.</span></p>

<p>OK, so the PVT model takes an image and outputs four gradually smaller and deeper images. Now what? How do we use these multiscale feature maps to implement object detection or other dense prediction tasks? Well, no need to reinvent the wheel: existing solutions generally involve a CNN backbone<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="and PVT model" data-secondary-sortas="PVT model" id="id3770"/> that produces multiscale feature maps, so we can simply swap out this backbone for a PVT (often pretrained on ImageNet). For example, we can use an FCN approach for semantic segmentation (introduced at the end of <a data-type="xref" href="ch12.html#cnn_chapter">Chapter 12</a>) by upscaling and combining the multiscale feature maps output by the PVT, and add a final classification head to output one class per pixel. Similarly, we can use a Mask R-CNN for object detection and instance segmentation, replacing its CNN backbone with a PVT<a data-type="indexterm" data-startref="xi_visiontransformersViTsPVT1623156_1" id="id3771"/><a data-type="indexterm" data-startref="xi_PyramidVisionTransformerPVTfordenseprediction1623156_1" id="id3772"/><a data-type="indexterm" data-startref="xi_densepredictiontransformersforPVT1623156_1" id="id3773"/><a data-type="indexterm" data-startref="xi_encoderonlytransformersPVT1623156_1" id="id3774"/><a data-type="indexterm" data-startref="xi_transformersencoderonly1623156_1" id="id3775"/>.</p>

<p>In short, the PVT’s hierarchical structure was a big milestone for vision transformers, but despite spatial reduction attention, it’s still computationally expensive. The Swin Transformer, released one month later, is much more scalable. Let’s see why.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The Swin Transformer: A Fast and Versatile ViT"><div class="sect2" id="id319">
<h2>The Swin Transformer: A Fast and Versatile ViT</h2>

<p>In March 2021, a team of Microsoft researchers<a data-type="indexterm" data-primary="vision transformers (ViTs)" data-secondary="Swin Transformer" id="xi_visiontransformersViTsSwinTransformer1625047_1"/><a data-type="indexterm" data-primary="Swin Transformer (vision)" id="xi_SwinTransformervision1625047_1"/><a data-type="indexterm" data-primary="dense prediction, transformers for" data-secondary="Swin Transformer (vision)" id="xi_densepredictiontransformersforSwinTransformervision1625047_1"/> released the <a href="https://homl.info/swin">Swin Transformer</a>.⁠<sup><a data-type="noteref" id="id3776-marker" href="ch16.html#id3776">8</a></sup> Just like PVT, it has a hierarchical structure, producing multiscale feature maps which can be used for dense prediction tasks. But Swin uses a very different variant of multi-head attention<a data-type="indexterm" data-primary="multi-head attention (MHA)" data-secondary="versus W-MSA" data-secondary-sortas="W-MSA" id="id3777"/>: each patch only attends to patches located within the same window<a data-type="indexterm" data-primary="window-based multi-head self-attention (W-MSA)" id="xi_windowbasedmultiheadselfattentionWMSA16250503_1"/><a data-type="indexterm" data-primary="self-attention" data-secondary="W-MSA" id="xi_selfattentionWMSA16250503_1"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="self-attention" id="xi_attentionmechanismsselfattention16250503_1"/>. This is called <em>window-based multi-head self-attention</em> (W-MSA), and it allows the cost of self-attention<a data-type="indexterm" data-primary="self-distillation" id="id3778"/><a data-type="indexterm" data-primary="distillation, model" data-secondary="DINO (self-distillation)" id="id3779"/> to scale linearly with the image size (meaning its area), instead of quadratically.</p>

<p>For example, on the lefthand side of <a data-type="xref" href="#swin_diagram">Figure 16-6</a>, the image is chopped into a 28 × 28 grid of patches, and these patches are grouped into nonoverlapping windows. At the first level of the Swin pyramid, the patches are usually 4 × 4 pixels, and each window contains a 7 × 7 grid of patches. So there’s a total of 784 patch tokens (28 × 28), but each token only attends to 49 tokens (7 × 7), so the W-MSA layer only needs to compute 784 × 49 = 38,416 attention scores, instead of 784<sup>2</sup> = 614,656 scores for regular MHA.</p>

<p>Most importantly, if we double the width and the height of the image, we quadruple the number of patch tokens, but each token still attends to only 49 tokens, so we just need to compute 4 times more attention scores: the Swin Transformer’s computational cost scales linearly with the image’s area, so it can handle large images. Conversely, ViT, DeiT, and PVT all scale quadratically: if you double the image width and height, the area is quadrupled, and the computational cost is multiplied by 16! As a result, these models are way too slow for very large images, meaning you must first downsample the image, which may hurt the model’s accuracy, especially for dense prediction tasks.</p>

<figure><div id="swin_diagram" class="figure">
<img src="assets/hmls_1606.png" alt="Diagram of the Swin Transformer architecture showing regular multi-head self-attention (S-MSA), shifted window multi-head self-attention (SW-MSA), and optimized SW-MSA, highlighting how different window configurations cover the same image." width="2393" height="951"/>
<h6><span class="label">Figure 16-6. </span>Swin Transformer: alternates W-MSA (left) and SW-MSA (center); SW-MSA can be optimized to require the same number of windows as W-MSA (right)</h6>
</div></figure>

<p>But wait a minute! If each token only attends to patches within the same window, how can we hope to capture long-range patterns? The answer is in the name of the architecture, Swin, which stands<a data-type="indexterm" data-primary="shifted windows (SW-MSA)" id="id3780"/> for <em>shifted windows</em>: every other encoder layer uses <em>shifted W-MSA</em> (SW-MSA), which is just like W-MSA except the windows are offset by half a window size. As you can see in the middle of <a data-type="xref" href="#swin_diagram">Figure 16-6</a>, the windows are shifted by 3 patches toward the bottom right (because half of 7 is 3.5, which we round down to 3). Why does this help? Well, nearby patches that were in separate windows in the previous layer are now in the same window, so they can see each other. By alternating W-MSA and SW-MSA, information from any part of the image can gradually propagate throughout the whole image. Moreover, since the architecture is hierarchical, the patches get coarser and coarser as we go up the pyramid, so the information can propagate faster and faster.</p>

<p>A naive implementation of SW-MSA would require handling many extra windows. For example, if you compare W-MSA and SW-MSA in <a data-type="xref" href="#swin_diagram">Figure 16-6</a>, you can see that W-MSA uses 16 windows, while SW-MSA uses 25 (at least in this example). To avoid this extra cost, the authors proposed an optimized implementation: instead of shifting the windows, we shift the image itself and wrap it around the borders, as shown in the righthand side of <a data-type="xref" href="#swin_diagram">Figure 16-6</a>. This way, we’re back to 16 windows. However, this requires careful masking for the border windows that contain the wrapped patches; for example, the regions labeled ①, ②, ③, ④ should not see each other, even though they are within the same window, so an appropriate attention mask must be applied.</p>

<p>Overall, Swin is harder to implement than PVT, but its linear scaling and excellent performance make it one of the best vision transformers out there. But the year 2021 wasn’t over: <a href="https://homl.info">Swin v2</a> was released in November 2021.⁠<sup><a data-type="noteref" id="id3781-marker" href="ch16.html#id3781">9</a></sup> It improved Swin across the board: more stable training for large ViTs, easier to fine-tune on large images, reduced need for labeled data, and more. Swin v2 is still widely used in vision tasks today<a data-type="indexterm" data-startref="xi_visiontransformersViTsSwinTransformer1625047_1" id="id3782"/><a data-type="indexterm" data-startref="xi_SwinTransformervision1625047_1" id="id3783"/><a data-type="indexterm" data-startref="xi_densepredictiontransformersforSwinTransformervision1625047_1" id="id3784"/><a data-type="indexterm" data-startref="xi_windowbasedmultiheadselfattentionWMSA16250503_1" id="id3785"/><a data-type="indexterm" data-startref="xi_selfattentionWMSA16250503_1" id="id3786"/><a data-type="indexterm" data-startref="xi_attentionmechanismsselfattention16250503_1" id="id3787"/>.</p>

<p>Our toolbox now contains vision transformers for classification (e.g., ViT and DeiT) and for dense prediction tasks (e.g., PVT and Swin). Let’s now explore one last pure-vision transformer, DINO, which introduced a revolutionary self-supervision technique for visual representation learning.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="DINO: Self-Supervised Visual Representation Learning"><div class="sect2" id="id320">
<h2>DINO: Self-Supervised Visual Representation Learning</h2>

<p>In April 2021, Mathilde Caron et al. introduced <a href="https://homl.info/dino">DINO</a>,<sup><a data-type="noteref" id="id3788-marker" href="ch16.html#id3788">10</a></sup> an impressive self-supervised training technique<a data-type="indexterm" data-primary="vision transformers (ViTs)" data-secondary="DINO" id="xi_visiontransformersViTsDINO16269262_1"/><a data-type="indexterm" data-primary="DINO, visual representation learning" id="xi_DINOvisualrepresentationlearning16269262_1"/><a data-type="indexterm" data-primary="representation learning" id="xi_representationlearning16269262_1"/><a data-type="indexterm" data-primary="self-supervised learning" id="xi_selfsupervisedlearning16269262_1"/> that produces models capable of generating excellent image representations. These representations can then be used for classification and other tasks.</p>

<p>Here’s how it works: the model is duplicated during training, with one network acting as a teacher and the other acting as a student (see <a data-type="xref" href="#dino_diagram">Figure 16-7</a>). Gradient descent only affects the student, while the teacher’s weights are just an exponential moving average (EMA) of the student’s weights. This<a data-type="indexterm" data-primary="momentum teacher" id="id3789"/> is called a <em>momentum teacher</em>. The student is trained to match the teacher’s predictions: since they’re almost the same model, this<a data-type="indexterm" data-primary="self-distillation" id="id3790"/><a data-type="indexterm" data-primary="distillation, model" data-secondary="DINO (self-distillation)" id="id3791"/> is called <em>self-distillation</em> (hence the name of the model: self-<strong>di</strong>stillation with <strong>no</strong> labels).</p>

<p>At each training step, the input images are augmented in various ways: color jitter, grayscale, Gaussian blur, horizontal flipping, and more. Importantly, they are augmented in different ways for the teacher and the student: the teacher always sees the full image, only slightly augmented, while the student often sees only a zoomed-in section of the image, with stronger augmentations. In short, the teacher and the student don’t see the same variant of the original image, yet their predictions must still match. This forces them to agree on high-level representations.</p>

<p>With this mechanism, however, there’s a strong risk<a data-type="indexterm" data-primary="mode collapse" id="id3792"/> of <em>mode collapse</em>. This is when both the student and the teacher always output the exact same thing, completely ignoring the input images. To prevent this, DINO keeps track of a moving average of the teacher’s predicted logits, and it subtracts this average from the predicted logits. This<a data-type="indexterm" data-primary="centering, self-distillation" id="id3793"/> is called <em>centering</em>,  forcing the teacher to distribute its predictions evenly across all classes (on average, over time).</p>

<p>But centering alone might cause the teacher to simply output the same probability for every class, all the time, still ignoring the image. To avoid this, DINO also forces the teacher to have high confidence in its highest predictions<a data-type="indexterm" data-primary="sharpening, NLP transformers" id="id3794"/>: this<a data-type="indexterm" data-primary="sharpening, self-distillation" id="id3795"/> is called <em>sharpening</em>. It’s implemented by applying a low temperature to the teacher’s logits (i.e., dividing them by a temperature smaller than 1). Together, centering and sharpening preserve the diversity in the teacher’s outputs; this leaves no easy shortcut for the model. It must base its predictions on the actual content of the image.</p>

<figure class="width-85"><div id="dino_diagram" class="figure">
<img src="assets/hmls_1607.png" alt="Diagram illustrating the DINO model's use of teacher and student Vision Transformers (ViTs) with centering and sharpening to enhance predictive accuracy without labels." width="1108" height="475"/>
<h6><span class="label">Figure 16-7. </span>DINO, or self-distillation with no labels</h6>
</div></figure>

<p>After training, you can drop the teacher: the student is the final DINO model. If you feed it a new image, it will output a sequence of contextualized patch embeddings. These can be used in various ways. For example, you can train a classifier head on top of the class token’s output embedding. In fact, you don’t even need a new classifier head: you can run DINO on every training image to get their representation (i.e., the output of the class token), then compute the mean representation per class. Then, when given a new image, use DINO to compute its representation and look for the class with the nearest mean representation. This simple approach reaches 78.3% top-1 accuracy on ImageNet, which is pretty impressive.</p>

<p>But it’s not just about classification! Interestingly, the DINO authors noticed that the class token’s attention maps in the last layer often focus on the main object of interest in the image, even though they were trained entirely without labels! In fact, each attention head seems to focus on a different part of the object<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="transformer models" id="id3796"/>, as you can see in <a data-type="xref" href="#unsupervised_segmentation_diagram">Figure 16-8</a>.⁠<sup><a data-type="noteref" id="id3797-marker" href="ch16.html#id3797">11</a></sup> See the notebook for a code example that uses DINO to plot a similar attention map.</p>

<figure><div id="unsupervised_segmentation_diagram" class="figure">
<img src="assets/hmls_1608.png" alt="Diagram showing unsupervised image segmentation using DINO, with different attention heads highlighting various parts of objects like vegetables, a zebra, and a truck." width="1760" height="1172"/>
<h6><span class="label">Figure 16-8. </span>Unsupervised image segmentation using DINO—different attention heads attend to different parts of the main object</h6>
</div></figure>

<p>Later techniques<a data-type="indexterm" data-primary="TokenCut" id="id3798"/> such as <a href="https://homl.info/tokencut">TokenCut</a>⁠<sup><a data-type="noteref" id="id3799-marker" href="ch16.html#id3799">12</a></sup> built upon DINO to detect and segment objects in images and videos. Then, in April 2023, Meta<a data-type="indexterm" data-primary="DINOv2" id="id3800"/> released <a href="https://homl.info/dino2">DINOv2</a>,⁠<sup><a data-type="noteref" id="id3801-marker" href="ch16.html#id3801">13</a></sup> which was trained on a curated and much larger dataset, and was tweaked to output per-patch features, making it a great foundation model not just for classification, but also for dense prediction tasks<a data-type="indexterm" data-startref="xi_visiontransformersViTsDINO16269262_1" id="id3802"/><a data-type="indexterm" data-startref="xi_DINOvisualrepresentationlearning16269262_1" id="id3803"/><a data-type="indexterm" data-startref="xi_representationlearning16269262_1" id="id3804"/><a data-type="indexterm" data-startref="xi_selfsupervisedlearning16269262_1" id="id3805"/>.</p>

<p>Let’s step back: we’ve covered CNN-based transformers such as DETR, followed by the original ViT (image patches through an encoder), DeiT (a distilled ViT), PVT (a hierarchical ViT with spatial reduction attention), Swin (a hierarchical ViT with window-based attention), and DINO (self-distillation with no labels). Before we move on to multimodal transformers, let’s quickly go through a few more pure-vision transformer models and techniques.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Other Major Vision Models and Techniques"><div class="sect2" id="id321">
<h2>Other Major Vision Models and Techniques</h2>

<p>Progress in vision transformers<a data-type="indexterm" data-primary="vision transformers (ViTs)" data-secondary="other major models and techniques" id="xi_visiontransformersViTsothermajormodelsandtechniques1629632_1"/> has continued steadily to this day. Here is a brief overview of some landmark papers:</p>
<dl>
<dt><a href="https://homl.info/scalingvits">“Scaling Vision Transformers”</a>,⁠<sup><a data-type="noteref" id="id3806-marker" href="ch16.html#id3806">14</a></sup> June 2021</dt>
<dd>
<p>Google researchers showed how to scale<a data-type="indexterm" data-primary="scaling of vision transformers" id="id3807"/> ViTs up or down, depending on the amount of available data. They managed to create a huge 2 billion parameter model that reached over 90.4% top-1 accuracy on ImageNet. Conversely, they also trained a scaled-down model that reached over 84.8% top-1 accuracy on ImageNet, using only 10,000 images: that’s just 10 images per class!</p>
</dd>
<dt><a href="https://homl.info/beit">“BEiT: BERT Pre-Training of Image Transformers”</a>,⁠<sup><a data-type="noteref" id="id3808-marker" href="ch16.html#id3808">15</a></sup> June 2021</dt>
<dd>
<p>Hangbo Bao et al. proposed a <em>masked image modeling</em> (MIM)<a data-type="indexterm" data-primary="masked image model (MIM)" id="id3809"/><a data-type="indexterm" data-primary="MIM (masked image modeling)" id="id3810"/> approach inspired from BERT’s masked language modeling (MLM)<a data-type="indexterm" data-primary="masked language model (MLM)" id="id3811"/><a data-type="indexterm" data-primary="MLM (masked language model)" id="id3812"/>. BEiT<a data-type="indexterm" data-primary="BEiT vision transformer" id="id3813"/> is pretrained to reconstruct masked image patches from the visible ones. This pretraining technique significantly improves downstream tasks.</p>

<p>Note that BEiT is not trained to predict the raw pixels of the masked patches; instead, it must predict the masked token IDs. But where do these token IDs come from? Well, the original image is passed<a data-type="indexterm" data-primary="discrete variational autoencoders (dVAEs)" id="id3814"/><a data-type="indexterm" data-primary="dVAEs (discrete variational autoencoders)" id="id3815"/> through a <em>discrete variational autoencoder</em> (dVAE, see <a data-type="xref" href="ch18.html#autoencoders_chapter">Chapter 18</a>) which encodes each patch into a visual token ID (an integer), from a fixed vocabulary. These are the IDs that BEiT tries to predict. The goal is to avoid wasting the model’s capacity on unnecessary details.</p>
</dd>
<dt><a href="https://homl.info/mae">“Masked Autoencoders Are Scalable Vision Learners”</a>,⁠<sup><a data-type="noteref" id="id3816-marker" href="ch16.html#id3816">16</a></sup> November 2021</dt>
<dd>
<p>This paper by a team of Facebook researchers (led by the prolific Kaiming He) also proposes a pretraining technique based on masked image modeling, but it removes the complexity of BEiT’s dVAE: masked autoencoder (MAE)<a data-type="indexterm" data-primary="masked autoencoder (MAE)" id="id3817"/><a data-type="indexterm" data-primary="MAE (masked autoencoder)" id="id3818"/><a data-type="indexterm" data-primary="autoencoders" data-secondary="MAE" id="id3819"/> directly predicts raw pixel values. Crucially, it uses an asymmetric encoder-decoder 
<span class="keep-together">architecture:</span> a large encoder processes only the visible patches, while a lightweight decoder reconstructs the entire image. Since 75% of patches are masked, this design dramatically reduces computational cost and allows MAE to be pretrained on very large datasets. This leads to strong performance on downstream tasks.</p>
</dd>
<dt><a href="https://homl.info/modelsoups">“Model Soups”</a>,⁠<sup><a data-type="noteref" id="id3820-marker" href="ch16.html#id3820">17</a></sup> March 2022</dt>
<dd>
<p>This paper demonstrated that it’s possible to first train multiple transformers, then average their weights to create a new and improved model. This is similar to an ensemble (see <a data-type="xref" href="ch06.html#ensembles_chapter">Chapter 6</a>), except there’s just one model in the end, which means there’s no inference cost.</p>
</dd>
<dt><a href="https://homl.info/eva">“EVA: Exploring the Limits of Masked Visual Representation Learning at Scale”</a>,⁠<sup><a data-type="noteref" id="id3821-marker" href="ch16.html#id3821">18</a></sup> May 2022</dt>
<dd>
<p>EVA<a data-type="indexterm" data-primary="EVA family of ViTs" id="id3822"/> is a family of large ViTs pretrained at scale, using enhanced MAE and strong augmentations. It’s one of the leading foundation models for ViTs. EVA-02, released in March 2023, does just as well or better despite having fewer parameters. The large variant has 304M parameters and reaches an impressive 90.0% on ImageNet.</p>
</dd>
<dt><a href="https://homl.info/ijepa">I-JEPA</a>,⁠<sup><a data-type="noteref" id="id3823-marker" href="ch16.html#id3823">19</a></sup> January 2023</dt>
<dd>
<p>Yann LeCun proposed the joint-embedding predictive architecture (JEPA)<a data-type="indexterm" data-primary="joint-embedding predictive architecture (JEPA)" id="id3824"/><a data-type="indexterm" data-primary="embeddings" data-secondary="JEPA" id="id3825"/> in a <a href="https://homl.info/jepa">2022 paper</a>,⁠<sup><a data-type="noteref" id="id3826-marker" href="ch16.html#id3826">20</a></sup> as part of his world-model framework, which aims to deepen AI’s understanding of the world and improve the reliability of its predictions. I-JEPA<a data-type="indexterm" data-primary="I-JEPA" id="id3827"/> is an implementation of JEPA for images. It was soon followed by <a href="https://homl.info/vjepa">V-JEPA</a> in 2024, and <a href="https://homl.info/vjepa2">V-JEPA 2</a> in 2025, both of which process videos<a data-type="indexterm" data-primary="V-JEPA" id="id3828"/>.</p>

<p>During training, JEPA involves two encoders and a predictor: the teacher encoder sees the full input (e.g., a photo of a cat) while the student encoder sees only part of the input (e.g., the same cat photo but without the ears). Both encoders convert their inputs to embeddings, then the predictor tries to predict the teacher embedding for the missing part (e.g., the ears) given the student embeddings for the rest of the input (e.g., the cat without ears). The student encoder and the predictor are trained jointly, while the teacher encoder is just a moving average of the student encoder (much like in DINO). JEPA mostly works in embedding space rather than pixel space, which makes it fast, parameter efficient, and more semantic.</p>

<p>After training, the teacher encoder and the predictor are no longer needed, but the student encoder can be used to generate excellent, meaningful representations for downstream tasks.</p>
</dd>
</dl>

<p>The list could go on and on:</p>

<ul>
<li>
<p>NesT or DeiT-III for image classification</p>
</li>
<li>
<p>MobileViT, EfficientFormer, EfficientViT, or TinyViT for small and efficient image classification models (e.g., for mobile devices)</p>
</li>
<li>
<p>Hierarchical transformers like Twins-SVT, FocalNet, MaxViT, and InternImage, often used as backbones for dense prediction tasks</p>
</li>
<li>
<p>Mask2Former or OneFormer for general-purpose segmentation, SEEM for universal segmentation, and SAM or MobileSAM for interactive segmentation</p>
</li>
<li>
<p>ViTDet or RT-DETR for object detection</p>
</li>
<li>
<p>TimeSformer, VideoMAE, or OmniMAE for video understanding</p>
</li>
</ul>

<p>There are also techniques<a data-type="indexterm" data-primary="token merging (ToMe)" id="id3829"/><a data-type="indexterm" data-primary="ToMe (token merging)" id="id3830"/> like <em>token merging</em> (ToMe) which speeds up inference by merging similar tokens on the fly, <em>token pruning</em> to drop unimportant tokens<a data-type="indexterm" data-primary="token pruning" id="id3831"/> during processing (i.e., with low attention scores), <em>early exiting</em> to only compute deep layers<a data-type="indexterm" data-primary="early exiting" id="id3832"/> for the most important tokens, <em>patch selection</em> to select<a data-type="indexterm" data-primary="patch selection" id="id3833"/> only the most informative patches for processing, and self-supervised training techniques like SimMIM, iBOT, CAE, or DINOv2, and more.</p>

<p>Hopefully we’ve covered a wide enough variety of models and techniques for you to be able to explore further on your own.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Some of these vision-only models were pretrained on multimodal data (e.g., image-text pairs or input prompts): OmniMAE, SEEM, SAM, MobileSAM, and DINOv2. Which leads us nicely to the second part of this chapter<a data-type="indexterm" data-startref="xi_transformersvisiontransformers164220_1" id="id3834"/><a data-type="indexterm" data-startref="xi_visiontransformersViTs164220_1" id="id3835"/><a data-type="indexterm" data-startref="xi_visiontransformersViTsothermajormodelsandtechniques1629632_1" id="id3836"/>.</p>
</div>

<p>We already had transformers that could read and write (and chat!), and now we have vision transformers that can see. It’s time to build transformers that can handle both text and images at the same time, as well as other modalities.</p>
</div></section>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Multimodal Transformers"><div class="sect1" id="id322">
<h1>Multimodal Transformers</h1>

<p>Humans are multimodal<a data-type="indexterm" data-primary="transformers" data-secondary="multimodal transformers" id="xi_transformersmultimodaltransformers1634022_1"/><a data-type="indexterm" data-primary="multimodal transformers" id="xi_multimodaltransformers1634022_1"/> creatures: we perceive the world through multiple 
<span class="keep-together">senses—sight,</span> hearing, smell, taste, touch, sense of balance, proprioception (i.e., sense of body position), and several others—and we act upon the world through movement, speech, writing, etc. Each of these modalities can be considered at a very low level (e.g., sound waves) or at a higher level (e.g., words, intonations, melody). Importantly, modalities are heterogeneous: one modality may be continuous while another is discrete, one may be temporal while the other is spatial, one may be high-resolution (e.g., 48 kHz audio) while the other is not (e.g., text), one may be noisy while the other is clean, and so on.</p>

<p>Moreover, modalities may interact in various ways. For example, when we chat with someone, we may listen to their voice while also watching the movement of their lips: these two modalities (auditory and visual) carry overlapping information, which helps our brain better parse words. But multimodality is not just about improving the signal/noise ratio: facial expressions may carry their own meaning (e.g., smiles and frowns), and different modalities may combine to produce a new meaning. For example, if you say “he’s an expert” while rolling your eyes or gesturing air quotes, you’re clearly being ironic, which inverts the meaning of your sentence and conveys extra information (e.g., humor or disdain) which neither modality possesses on its own.</p>

<p>So multimodal machine learning requires designing models that can handle very heterogeneous data and capture their interactions. There are two main challenges for this. The first<a data-type="indexterm" data-primary="fusion challenge for multimodal ML" id="id3837"/> is called <em>fusion</em>, and it’s about finding a way to combine different modalities, for example, by encoding them into the same representation space. The second<a data-type="indexterm" data-primary="alignment model" id="id3838"/> is called <em>alignment</em>, where the goal is to discover the relationships between modalities. For example, perhaps you have a recording of a speech, as well as a text transcription, and you want to find the timestamp of each word. Or you want to find the most relevant object in an image given a text query such as “the dog next to the tree” (this<a data-type="indexterm" data-primary="visual grounding" id="id3839"/> is called <em>visual grounding</em>). Many other common tasks involve two or more modalities, such as image captioning, image search, visual question answering (VQA), speech-to-text (STT), text-to-speech (TTS), embodied AI (i.e., a model capable of physically interacting with the environment), and much more.</p>

<p>Multimodal machine learning has been around for decades, but progress has recently accelerated thanks to deep learning, and particularly since the rise of transformers. Indeed, transformers can ingest pretty much any modality, as long as you can chop it into a sequence of meaningful tokens (e.g., text into words, images into small patches, audio or video into short clips, etc.). Once you have prepared a sequence of token embeddings, you’re ready to feed it to a transformer. Embeddings from different modalities can be fused in various ways: summed up, concatenated, passed through a fusion encoder, and more. This can take care of the fusion problem. And transformers also have multi-head attention, which is a powerful tool to detect and exploit complex patterns, both within and across modalities. This can take care of the alignment problem.</p>

<p>Researchers quickly understood the potential of transformers for multimodal architectures. The first multimodal transformers were released just months after the original Transformer paper was released in early 2018 with image captioning, video captioning, and more. Let’s look at some of the most impactful multimodal transformer architectures, starting with VideoBERT.</p>








<section data-type="sect2" data-pdf-bookmark="VideoBERT: A BERT Variant for Text plus Video"><div class="sect2" id="id323">
<h2>VideoBERT: A BERT Variant for Text plus Video</h2>

<p>In April 2019, Google researchers<a data-type="indexterm" data-primary="multimodal transformers" data-secondary="VideoBERT" id="xi_multimodaltransformersVideoBERT1635134_1"/><a data-type="indexterm" data-primary="VideoBERT" id="xi_VideoBERT1635134_1"/><a data-type="indexterm" data-primary="text plus image/video tools" data-secondary="VideoBERT" id="xi_textplusimagevideotoolsVideoBERT1635134_1"/> released <a href="https://homl.info/videobert">VideoBERT</a>.⁠<sup><a data-type="noteref" id="id3840-marker" href="ch16.html#id3840">21</a></sup> As its name suggests, this model is very similar to BERT, except it can handle both text and videos. In fact, the authors just took a pretrained BERT-large model, extended its embedding matrix to allow for extra video tokens (more on this shortly), and continued training the model using self-supervision on a text plus video training set. This dataset was built from a large collection of instructional YouTube videos, particularly cooking videos. These videos typically involve someone describing a sequence of actions while performing them (e.g., “Cut the tomatoes into thin slices like this”). To feed these videos to VideoBERT, the authors had to encode the videos into both text and visual sequences (see <a data-type="xref" href="#videobert_encoding_diagram">Figure 16-9</a>):</p>

<ul>
<li>
<p>For the visual modality, they extracted nonoverlapping 1.5-second clips at 20 frames per second (i.e., 30 frames each), and they passed these clips through a 3D CNN named S3D. This CNN is based on Inception modules and separable convolutions (see <a data-type="xref" href="ch12.html#cnn_chapter">Chapter 12</a>), and it was pretrained on the Kinetics dataset composed of many YouTube videos of people performing a wide range of actions. The authors added a 3D average pooling layer on top of S3D to get a 1,024-dimensional vector for each video clip. Each vector encodes fairly high-level information about the video clip.</p>
</li>
<li>
<p>To extract the text from the videos, the authors used YouTube’s internal speech-to-text software, after which they dropped the audio tracks from the videos. Then they separated the text into sentences by adding punctuation using an off-the-shelf LSTM model. Finally, they preprocessed and tokenized the text just like for BERT.</p>
</li>
</ul>

<figure><div id="videobert_encoding_diagram" class="figure">
<img src="assets/hmls_1609.png" alt="Diagram illustrating the VideoBERT process, showing how actions in a video are converted into text and visual tokens using S3D for video clips and speech-to-text for audio." width="1368" height="484"/>
<h6><span class="label">Figure 16-9. </span>VideoBERT—encoding a video into a text sequence and a visual sequence</h6>
</div></figure>

<p>Great! We now have a text token sequence describing some actions, and a sequence of vectors representing video clips of these actions. However, we have a problem. Recall that BERT is pretrained using MLM, where the model must predict masked tokens from a fixed vocabulary. We do have a fixed vocabulary for the text tokens, but not for the video tokens. So let’s build one! For this, the authors gathered all the visual vectors produced by S3D over their training set, and they clustered these vectors into <em>k</em> = 12 clusters using <em>k</em>-means (see <a data-type="xref" href="ch08.html#unsupervised_learning_chapter">Chapter 8</a>). Then they used <em>k</em>-means again on each cluster to get 12<sup>2</sup> = 144 clusters, then again and again to get 12<sup>4</sup> = 20,736 clusters. This process<a data-type="indexterm" data-primary="hierarchical k-means" id="id3841"/> is called <em>hierarchical k-means</em>, and it’s much faster than running <em>k</em>-means just once using <em>k</em> = 20,736, plus it typically produces much better clusters. Now each vector can be replaced with its cluster ID: this way, each video clip is represented by a single ID from a fixed visual vocabulary, so the whole video is now represented as one sequence of visual token IDs (e.g., 194, 3912, …​), exactly like tokenized text. In short, we’ve gone from a continuous 1,024-dimensional space down to a discrete space with just 20,736 possible values. There’s a lot of information loss at this step, but VideoBERT’s excellent performance suggests that much of the important information remains.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Since the authors used a pretrained BERT-large model, the text token embeddings were already excellent before VideoBERT’s additional training even started. For the visual token embeddings, rather than using trainable embeddings initialized from scratch, the authors used frozen embeddings initialized using the 1,024-dimensional vector representations of the <em>k</em>-means cluster 
<span class="keep-together">centroids.</span></p>
</div>

<p>The authors used three different training regimes: text-only, video-only, and text plus video. In text-only and video-only modes, VideoBERT was fed a single modality and trained to predict masked tokens (either text tokens or video tokens). For text plus video, the model was fed both text tokens and video tokens, simply concatenated (plus an unimportant separation token in between), and it had to predict whether the text tokens and video tokens came from the same part of the original video. This<a data-type="indexterm" data-primary="linguistic-visual alignment" id="id3842"/> is called <em>linguistic-visual alignment</em>. For this, the authors added a binary classification head on top of the class token’s output (this replaces BERT’s next sentence prediction head). For negative examples, the authors just sampled random sentences and video segments. <a data-type="xref" href="#videobert_pretraining_diagram">Figure 16-10</a> shows all three modes at once, but keep in mind that they are actually separate.</p>

<figure class="width-90"><div id="videobert_pretraining_diagram" class="figure">
<img src="assets/hmls_1610.png" alt="Diagram illustrating the VideoBERT model, highlighting masked token prediction and linguistic-visual alignment processes." width="1084" height="844"/>
<h6><span class="label">Figure 16-10. </span>VideoBERT—pretraining using masked token prediction and linguistic-visual alignment (shown together but actually separate)</h6>
</div></figure>

<p>Linguistic-visual alignment is a noisy task since the cook may explain something that they have already finished or will do later, so the authors concatenated random neighboring sentences to give the model more context. The authors had a few more tricks up their sleeves, such as randomly changing the video sampling rate to make the model more robust to different action speeds, since some cooks are faster than others; see the paper for more details.</p>

<p>This was a lot of work, but the authors were finally done: they had a fully trained VideoBERT model. To demonstrate its effectiveness, they evaluated VideoBERT on some downstream tasks, including:</p>
<dl>
<dt>Zero-shot action classification</dt>
<dd>
<p>Given a video clip<a data-type="indexterm" data-primary="zero-shot learning (ZSL)" id="id3843"/>, figure out which action is performed, without fine-tuning VideoBERT. The authors achieved this by feeding the video to VideoBERT, along with the following masked sentence: “Now let me show you how to [MASK] the [MASK]”. Then they looked at the output probabilities for both masked tokens, for each possible pair of verb and noun. If the video shows a cook slicing some tomatoes, then the probability of “slice” and “tomatoes” will be much higher than “bake” and “cake” or “boil” and “egg”.</p>
</dd>
<dt>Video captioning</dt>
<dd>
<p>Given a video clip<a data-type="indexterm" data-primary="video captioning, VideoBERT" id="id3844"/><a data-type="indexterm" data-primary="captioning of images and video" id="id3845"/>, generate a caption. To do this, the authors used the earliest <a href="https://homl.info/videocaption">video-captioning transformer architecture</a>,⁠<sup><a data-type="noteref" id="id3846-marker" href="ch16.html#id3846">22</a></sup> but they replaced the input to the encoder with visual features output by VideoBERT. More specifically, they took an average of VideoBERT’s final output representations, including the representations of all of the visual tokens and the masked-out text tokens. The masked sentence they used was: “now let’s [MASK] the [MASK] to the [MASK], and then [MASK] the [MASK]”. After fine-tuning this new model, they obtained improved results over the original captioning model.</p>
</dd>
</dl>

<p>Using similar approaches, VideoBERT can be adapted for many other tasks, such as multiple-choice visual question answering<a data-type="indexterm" data-primary="multiple-choice question answering (MCQA)" id="id3847"/><a data-type="indexterm" data-primary="MCQA (multiple-choice question answering)" id="id3848"/>: given an image, a question, and multiple possible answers, the model must find the correct answer. For example: “What is the cook doing?” → “Slicing tomatoes”. For this, one approach is to simply run VideoBERT on each possible answer, along with the video, and compare the linguistic-visual alignment scores: the correct answer should have the highest score.</p>

<p>The success of VideoBERT inspired many other BERT-based multimodal transformers, many of which were released in August and September 2019: ViLBERT, VisualBERT, Unicoder-VL, LXMERT, VL-BERT, and UNITER. Most of these are single-stream models like VideoBERT, meaning that the modalities are fused very early in the network, typically by simply concatenating the sequences. However, ViLBERT and LXMERT are dual-stream transformers, meaning that each modality is processed by its own encoder, with a mechanism allowing the encoders to influence each other. This lets the model better understand each modality before trying to make sense of the interactions between them. VilBERT was particularly influential, so let’s look at it more closely<a data-type="indexterm" data-startref="xi_multimodaltransformersVideoBERT1635134_1" id="id3849"/><a data-type="indexterm" data-startref="xi_VideoBERT1635134_1" id="id3850"/><a data-type="indexterm" data-startref="xi_textplusimagevideotoolsVideoBERT1635134_1" id="id3851"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="ViLBERT: A Dual-Stream Transformer for Text plus Image"><div class="sect2" id="id324">
<h2>ViLBERT: A Dual-Stream Transformer for Text plus Image</h2>

<p>ViLBERT<a data-type="indexterm" data-primary="multimodal transformers" data-secondary="ViLBERT" id="xi_multimodaltransformersViLBERT163848_1"/><a data-type="indexterm" data-primary="ViLBERT" id="xi_ViLBERT163848_1"/><a data-type="indexterm" data-primary="text plus image/video tools" data-secondary="ViLBERT" id="xi_textplusimagevideotoolsViLBERT163848_1"/> was <a href="https://homl.info/vilbert">proposed in August 2019</a>⁠<sup><a data-type="noteref" id="id3852-marker" href="ch16.html#id3852">23</a></sup> by a team of researchers from the Georgia Institute of Technology, Facebook AI Research, and Oregon State University. They pointed out that the single-stream approach (used by VideoBERT and many others) treats both modalities identically, even though they may require different levels of processing. For example, if the visual features come from a deep CNN, then we already have good high-level visual features, whereas the text will need much more processing before the model has access to high-level text features. Moreover, the researchers hypothesized that “image regions may have weaker relations than words in a sentence”.⁠<sup><a data-type="noteref" id="id3853-marker" href="ch16.html#id3853">24</a></sup> Lastly, BERT was initially pretrained using text only, so forcing it to process other modalities may give suboptimal results and even damage its weights during multimodal training.</p>

<p>So the authors chose a dual-stream approach instead: each modality goes through its own encoder, and in the upper layers the two encoders are connected and exchange information through a new bidirectional cross-attention<a data-type="indexterm" data-primary="cross-attention" id="id3854"/><a data-type="indexterm" data-primary="co-attention, ViLBERT" id="id3855"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="co-attention for ViLBERT" id="id3856"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="cross-attention" id="id3857"/> mechanism called <em>co-attention</em> (see <a data-type="xref" href="#co_attention_diagram">Figure 16-11</a>). Specifically, in each pair of connected encoder layers, the MHA query of one encoder is used as the MHA key/value by the other encoder.</p>

<figure><div id="co_attention_diagram" class="figure">
<img src="assets/hmls_1611.png" alt="Diagram of connected vision and text co-encoder layers using co-attention, where the multi-head attention (MHA) query from one encoder is used as the key/value by the other encoder." width="774" height="689"/>
<h6><span class="label">Figure 16-11. </span>Two encoder layers connected through co-attention: the MHA query of one encoder is used as the MHA key/value by the other encoder</h6>
</div></figure>

<p>The lower layers of the text encoder are initialized with BERT’s weights (the authors used a BERT base, which has 12 layers), and 6 co-attention layers sit on top (see the lower-right quadrant of <a data-type="xref" href="#vilbert_diagram">Figure 16-12</a>). The visual features are produced by a pretrained and frozen Faster R-CNN model<a data-type="indexterm" data-primary="Faster R-CNN" id="id3858"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="Faster R-CNN" id="id3859"/>, and it is assumed that these features are sufficiently high level so no further processing is needed; therefore, the visual encoder is exclusively composed of six co-attention layers, paired up with the text encoder’s six co-attention layers (see the lower-left quadrant of the figure). The Faster R-CNN model’s outputs go through a mean pooling layer for each detected region, so we get one feature vector per region, and low-confidence regions are dropped: each image ends up represented by 10 to 36 vectors.</p>

<figure class="width-90"><div id="vilbert_diagram" class="figure">
<img src="assets/hmls_1612.png" alt="Diagram illustrating the ViLBERT model's pretraining process for masked token prediction and linguistic-visual alignment, highlighting the interaction between the visual and text encoders and their integration for classification tasks." width="1253" height="880"/>
<h6><span class="label">Figure 16-12. </span>ViLBert pretraining using masked token prediction and linguistic-visual alignment (again, shown together but actually separate)</h6>
</div></figure>

<p>Since regions don’t have a natural order like words do, the visual encoder does not use positional encoding. Instead, it uses spatial encodings<a data-type="indexterm" data-primary="spatial encodings for image interpretation" id="id3860"/> that are computed like this: each region’s bounding box is encoded as a 5D vector containing the normalized upper-left and lower-right coordinates, and the ratio of the image covered by the bounding box. Then this 5D vector is linearly projected up to the same dimensionality as the visual vector, and simply added to it.</p>

<p>Lastly, a special [IMG] token is prepended to the visual sequence: it serves the same purpose as the class token (i.e., to produce a representation of the whole sequence), but instead of being a trainable embedding, it’s computed as the average of the feature vectors (before spatial encoding), plus the spatial encoding for a bounding box covering the whole image.</p>

<p>Now on to training! Similar to VideoBERT, the authors used masked token prediction<a data-type="indexterm" data-primary="masked token prediction, ViLBERT" id="id3861"/><a data-type="indexterm" data-primary="predictions" data-secondary="masked token, ViLBERT" id="id3862"/> and linguistic-visual alignment<a data-type="indexterm" data-primary="linguistic-visual alignment" id="id3863"/><a data-type="indexterm" data-primary="alignment model" id="id3864"/>:</p>

<ul>
<li>
<p>For masked token prediction, the authors used regular BERT-like MLM for the text encoder. However, for the visual encoder, since ViLBERT does not use a fixed-size visual vocabulary (there’s no clustering step), the model is trained to predict the class distribution that the CNN predicts for the given image region (this is a soft target). The authors chose this task rather than predicting raw pixels because the regions can be quite large and there’s typically not enough information in the surrounding regions and in the text to reconstruct the masked region correctly: it’s better to aim for a higher-level target.</p>
</li>
<li>
<p>For linguistic-visual alignment, the model takes the outputs of the [IMG] and [CLS] tokens, then computes their itemwise product and passes the result to a binary classification head that must predict whether the text and image match. Multiplication is preferred over addition because it amplifies features that are strong in both representations (a bit like a logical AND gate), so it better captures alignment.</p>
</li>
</ul>

<p>And that’s it. This model significantly beat the state of the art for several downstream tasks, including image grounding, caption-based image retrieval (even zero-shot), visual question answering, and <em>visual commonsense reasoning</em> (VCR)<a data-type="indexterm" data-primary="visual commonsense reasoning (VCR)" id="id3865"/><a data-type="indexterm" data-primary="VCR (visual commonsense reasoning)" id="id3866"/> which involves answering a multiple-choice question about an image (like VQA), then selecting the appropriate justification. For example, given an image of a waiter serving some pancakes at a table, along with the question “Why is person #4 pointing at person #1”, the model must choose the correct answer “He is telling person #3 that person #1 ordered the pancakes”, then it must choose the justification “Person #3 is serving food and they might not know whose order is whose”<a data-type="indexterm" data-startref="xi_multimodaltransformersViLBERT163848_1" id="id3867"/><a data-type="indexterm" data-startref="xi_ViLBERT163848_1" id="id3868"/><a data-type="indexterm" data-startref="xi_textplusimagevideotoolsViLBERT163848_1" id="id3869"/>.</p>

<p>ViLBERT had a strong influence on the field of multimodal machine learning thanks to its dual-stream architecture, the invention of co-attention, and its excellent results on many downstream tasks. It was another great demonstration of the power of large-scale self-supervised pretraining using transformers. The next major milestone came in 2021, and it approached the problem very differently, using contrastive pretraining: meet CLIP.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="CLIP: A Dual-Encoder Text plus Image Model Trained &#10;with Contrastive Pretraining"><div class="sect2" id="id325">
<h2>CLIP: A Dual-Encoder Text plus Image Model Trained 
<span class="keep-together">with Contrastive Pretraining</span></h2>

<p>OpenAI’s<a data-type="indexterm" data-primary="multimodal transformers" data-secondary="CLIP" id="xi_multimodaltransformersCLIP164129_1"/><a data-type="indexterm" data-primary="CLIP (contrastive language-image pretraining)" id="xi_CLIPcontrastivelanguageimagepretraining164129_1"/><a data-type="indexterm" data-primary="encoder-decoder models" data-secondary="CLIP text-plus-image model" id="xi_encoderdecodermodelsCLIPtextplusimagemodel164129_1"/><a data-type="indexterm" data-primary="text plus image/video tools" data-secondary="CLIP" id="xi_textplusimagevideotoolsCLIP164129_1"/><a data-type="indexterm" data-primary="contrastive language-image pretraining (CLIP)" id="xi_contrastivelanguageimagepretrainingCLIP164129_1"/> January 2021 release of <a href="https://homl.info/clip">contrastive language–image pretraining (CLIP)</a>⁠<sup><a data-type="noteref" id="id3870-marker" href="ch16.html#id3870">25</a></sup> was a major breakthrough, not just for its astounding capabilities, but also because of its surprisingly straightforward approach based on <em>contrastive learning</em>: the model learns<a data-type="indexterm" data-primary="contrastive learning" id="id3871"/> to encode text and images into vector 
<span class="keep-together">representations</span> that are similar when the text and image match, and dissimilar when they don’t match.</p>

<p>Once trained, the model can be used for many tasks, particularly zero-shot image classification<a data-type="indexterm" data-primary="zero-shot image classification, CLIP" id="id3872"/>. For example, CLIP can be used as an insect classifier without any additional training: just start by feeding all the possible class names to CLIP, such as “cricket”, “ladybug”, “spider”, and so on, to get one vector representation for each class name. Then, whenever you want to classify an image, feed it to CLIP to get a vector representation, and find the most similar class name representation using cosine similarity. This usually works even better if the text resembles typical image captions found on the web, since this is what CLIP was trained on, for example, “This is a photo of a ladybug” instead of just “ladybug”. A bit of prompt engineering can help (i.e., experimenting with various prompt templates).</p>

<p>The good news is that CLIP is fully open source,⁠<sup><a data-type="noteref" id="id3873-marker" href="ch16.html#id3873">26</a></sup>, several pretrained models are available on the Hugging Face Hub, and the Transformers library provides a convenient pipeline for zero-shot image classification:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">pipeline</code>

<code class="n">model_id</code> <code class="o">=</code> <code class="s2">"openai/clip-vit-base-patch32"</code>
<code class="n">clip_pipeline</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="n">task</code><code class="o">=</code><code class="s2">"zero-shot-image-classification"</code><code class="p">,</code> <code class="n">model</code><code class="o">=</code><code class="n">model_id</code><code class="p">,</code>
                         <code class="n">device_map</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">)</code>
<code class="n">candidate_labels</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"cricket"</code><code class="p">,</code> <code class="s2">"ladybug"</code><code class="p">,</code> <code class="s2">"spider"</code><code class="p">]</code>
<code class="n">image_url</code> <code class="o">=</code> <code class="s2">"https://homl.info/ladybug"</code>  <code class="c1"># a photo of a ladybug on a dandelion</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">clip_pipeline</code><code class="p">(</code><code class="n">image_url</code><code class="p">,</code> <code class="n">candidate_labels</code><code class="o">=</code><code class="n">candidate_labels</code><code class="p">,</code>
                        <code class="n">hypothesis_template</code><code class="o">=</code><code class="s2">"This is a photo of a </code><code class="si">{}</code><code class="s2">."</code><code class="p">)</code></pre>

<p>Note that we provided a prompt template, so the model will actually encode “This is a photo of a ladybug”, not just “ladybug” (if you don’t provide any template, the pipeline actually defaults to “This is a photo of a {}”.). Now let’s look at the results, which are sorted by score:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="p">[{</code><code class="s1">'score'</code><code class="p">:</code> <code class="mf">0.9972853660583496</code><code class="p">,</code> <code class="s1">'label'</code><code class="p">:</code> <code class="s1">'ladybug'</code><code class="p">},</code>
 <code class="p">{</code><code class="s1">'score'</code><code class="p">:</code> <code class="mf">0.0016511697322130203</code><code class="p">,</code> <code class="s1">'label'</code><code class="p">:</code> <code class="s1">'spider'</code><code class="p">},</code>
 <code class="p">{</code><code class="s1">'score'</code><code class="p">:</code> <code class="mf">0.0010634352220222354</code><code class="p">,</code> <code class="s1">'label'</code><code class="p">:</code> <code class="s1">'cricket'</code><code class="p">}]</code></pre>

<p>Great! CLIP predicts ladybug with over 99.7% confidence. Now if you want a flower classifier instead, just replace the candidate labels with names of flowers. If you include “dandelion” in the list and classify the same image, the model should choose “dandelion” with high confidence (ignoring the ladybug). Impressive!</p>

<p>So how does this magic work? Well, CLIP’s architecture is based on a regular text encoder and a regular vision encoder, no co-attention or anything fancy (see <a data-type="xref" href="#clip_diagram">Figure 16-13</a>). You can actually use pretty much any text and vision encoders you want, as long as they can produce a vector representation of the text or image. The authors experimented with various encoders, including several ResNet and ViT models for vision, and a GPT-2-like model for text, all trained from scratch. What’s that I hear you say, GPT-2 is not an encoder? That’s true, it’s a decoder-only model, but we’re not pretraining it for next token prediction, so the last token’s output is free to be used as a representation of the entire input sequence, which is what CLIP does. You may wonder why we’re not using a regular text encoder like BERT? Well, we could, but OpenAI created GPT—Alex Radford is the lead author of both GPT and CLIP—so that’s most likely why GPT-2 was chosen: the authors simply had more experience with this model and a good training infrastructure already in place. Using a causal encoder also makes it possible to cache the intermediate state of the model when multiple texts start in the same way; for example, “This is a photo of a”.</p>

<figure><div id="clip_diagram" class="figure">
<img src="assets/hmls_1613.png" alt="Diagram illustrating the CLIP model with text and vision encoders processing image-caption pairs into vectors to match corresponding pairs and differentiate mismatched ones." width="1060" height="420"/>
<h6><span class="label">Figure 16-13. </span>CLIP: a batch of image-caption pairs is encoded as vectors, then matching pairs are pulled closer while mismatched pairs are pushed away</h6>
</div></figure>

<p>Also note that a pooling layer is added on top of the vision encoder to ensure it outputs a single vector for the whole image instead of feature maps. Moreover, a linear layer is added on top of each encoder to project the final representation into the same output space (i.e., with the same number of dimensions). So given a batch of <em>m</em> image-caption pairs<a data-type="indexterm" data-primary="captioning of images and video" id="xi_captioningofimagesandvideo16444359_1"/>, we get <em>m</em> vector representations for the images and <em>m</em> vector representations for the captions, and all vectors have the same number of dimensions. <a data-type="xref" href="#clip_diagram">Figure 16-13</a> shows <em>m</em> = 4, but the authors used a shockingly large batch size of <em>m</em> = 2<sup>15</sup> = 32,768 during training.</p>

<p>The model was then pretrained on a large dataset of 400 million image-caption pairs scraped from the internet, using a contrastive loss⁠<sup><a data-type="noteref" id="id3874-marker" href="ch16.html#id3874">27</a></sup> that pulls together the representations of matching pairs, while also pushing apart representations of mismatched pairs. Here’s how it works:</p>

<ul>
<li>
<p>All vectors are first ℓ<sub>2</sub> normalized, meaning they are rescaled to unit vectors: we only care about their orientation, not their length.</p>
</li>
<li>
<p>Next, we compute the cosine similarity of the image representation and the text representation for every possible image-caption pair. The result is an <em>m</em> × <em>m</em> matrix containing numbers between –1 for opposite vectors, and +1 for identical vectors. In <a data-type="xref" href="#clip_diagram">Figure 16-13</a>, this matrix is represented by the 4 × 4 grid (black is +1, white is –1). Each column measures how much each image in the batch matches a given caption in the same batch, while each row measures how much each caption matches a given image.</p>
</li>
<li>
<p>Since the <em>i</em><sup>th</sup> image corresponds to the <em>i</em><sup>th</sup> caption, we want the main diagonal of this matrix to contain similarity scores close to +1, while all other scores should be close to 0. Why not close to –1? Well, if an image and a text are totally unrelated, we can think of their representations as two random vectors. Recall that two random high-dimensional vectors are highly likely to be close to orthogonal (as discussed in <a data-type="xref" href="ch07.html#dimensionality_chapter">Chapter 7</a>), so their cosine similarity will be close to 0, not –1. In other words, it makes sense to assume that the text and image representations of a mismatched pair are unrelated (score close to 0), not opposite (score close to –1).</p>
</li>
<li>
<p>In the <em>i</em><sup>th</sup> row, we know that the matching caption is in the <em>i</em><sup>th</sup> column, so we want the model to produce a high similarity score in that column, and a low score elsewhere. This resembles a classification task where the target class is the <em>i</em><sup>th</sup> class. Indeed, we can treat each similarity score as class logit and simply compute the cross-entropy loss for that row with <em>i</em> as the target. We can follow the exact same rationale for each column. If we compute the cross-entropy loss for each row and each column (using class <em>i</em> as the target for the <em>i</em><sup>th</sup> row and the <em>i</em><sup>th</sup> column), and evaluate the mean, we get the final loss.</p>
</li>
<li>
<p>There’s just one extra technical detail: the similarity scores range between –1 and +1, which is unlikely to be the ideal logit scale for the task, so CLIP divides all the similarity scores by a trainable temperature (a scalar) before computing the loss<a data-type="indexterm" data-startref="xi_captioningofimagesandvideo16444359_1" id="id3875"/>.</p>
</li>
</ul>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>This loss requires a large batch size to ensure the model sees enough negative examples to contrast with the positive examples, or else it could overfit details in the positive examples. CLIP’s success is due in part to the gigantic batch size that the authors were able to implement.</p>
</div>

<p>The authors evaluated CLIP on many image classification datasets, and for roughly 60% of these, it performed better without any extra training (i.e., zero-shot) than a <em>linear probe</em> trained on ResNet-50<a data-type="indexterm" data-primary="ResNet-50" id="id3876"/> features (that’s a linear classifier trained on features output by a pretrained and frozen ResNet-50 model), including on ImageNet, despite the fact that the ResNet-50 model was actually pretrained on ImageNet. CLIP is particularly strong on datasets with few examples per class, with pictures of everyday scenes (i.e., the kind of pictures you find on the web). In fact, CLIP even beat the state of the art on the Stanford Cars dataset<a data-type="indexterm" data-primary="Stanford Cars dataset" id="id3877"/>, ahead of the best ViTs specifically trained on this dataset, because pictures of cars are very common on the web and the dataset doesn’t have many examples per class. However, CLIP doesn’t perform as well on domain-specific images, such as satellite or medical images.</p>

<p>Importantly, the visual features output by CLIP are also highly robust to perturbations, making them excellent for downstream tasks, such as image retrieval: if you store images in a vector database, indexing them by their CLIP-encoded visual features, you can then search for them given either a text query or an image query. For this, just run the query through CLIP to get a vector representation, then search the database for images with a similar representation.</p>

<p>To get the text and visual features using the Transformers library, you must run the CLIP model directly, without going through a pipeline:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">PIL</code>
<code class="kn">import</code> <code class="nn">urllib.request</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">CLIPProcessor</code><code class="p">,</code> <code class="n">CLIPModel</code>

<code class="n">clip_processor</code> <code class="o">=</code> <code class="n">CLIPProcessor</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_id</code><code class="p">)</code>
<code class="n">clip_model</code> <code class="o">=</code> <code class="n">CLIPModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_id</code><code class="p">)</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">PIL</code><code class="o">.</code><code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">urllib</code><code class="o">.</code><code class="n">request</code><code class="o">.</code><code class="n">urlopen</code><code class="p">(</code><code class="n">image_url</code><code class="p">))</code><code class="o">.</code><code class="n">convert</code><code class="p">(</code><code class="s2">"RGB"</code><code class="p">)</code>
<code class="n">captions</code> <code class="o">=</code> <code class="p">[</code><code class="sa">f</code><code class="s2">"This is a photo of a </code><code class="si">{</code><code class="n">label</code><code class="si">}</code><code class="s2">."</code> <code class="k">for</code> <code class="n">label</code> <code class="ow">in</code> <code class="n">candidate_labels</code><code class="p">]</code>
<code class="n">inputs</code> <code class="o">=</code> <code class="n">clip_processor</code><code class="p">(</code><code class="n">text</code><code class="o">=</code><code class="n">captions</code><code class="p">,</code> <code class="n">images</code><code class="o">=</code><code class="p">[</code><code class="n">image</code><code class="p">],</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">,</code>
                        <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">outputs</code> <code class="o">=</code> <code class="n">clip_model</code><code class="p">(</code><code class="o">**</code><code class="n">inputs</code><code class="p">)</code>

<code class="n">text_features</code> <code class="o">=</code> <code class="n">outputs</code><code class="o">.</code><code class="n">text_embeds</code>    <code class="c1"># shape [3, 512]  # 3 captions</code>
<code class="n">image_features</code> <code class="o">=</code> <code class="n">outputs</code><code class="o">.</code><code class="n">image_embeds</code>  <code class="c1"># shape [1, 512]  # 1 image (ladybug)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If you need to encode the images and text separately, you can use the CLIP model’s <code translate="no">get_image_features()</code> and <code translate="no">get_text_features()</code> methods. You must first tokenize the text using a <code translate="no">CLIPTokenizer</code> and process the images using a <code translate="no">CLIPImageProcessor</code>. The resulting features are not ℓ<sub>2</sub> normalized, so you must divide them by <code translate="no">features.norm(dim=1, keepdim=True)</code> (see the notebook for a code example).</p>
</div>

<p>The features are already ℓ<sub>2</sub> normalized, so if you want to compute similarity scores, a single matrix multiplication is all you need:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">similarities</code> <code class="o">=</code> <code class="n">image_features</code> <code class="o">@</code> <code class="n">text_features</code><code class="o">.</code><code class="n">T</code>  <code class="c1"># shape [1, 3]</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">similarities</code><code class="w"/>
<code class="go">tensor([[0.2337, 0.3021, 0.2381]])</code></pre>

<p>This works because matrix multiplication computes the dot products of every row vector in the first matrix with every column vector in the second, and each dot product is equal to the cosine of the angle between the vectors multiplied by the norms of the vectors. Since the vectors have been ℓ<sub>2</sub> normalized in this case, the norms are equal to 1, so the result is just the cosine of the angle, which is the similarity score we’re after. As you can see, the most similar representation is the second one, for the ladybug class. If you prefer estimated probabilities rather than similarity scores, you must first rescale the similarities using the model’s learned temperature, then pass the result through the softmax function (it’s nice to see that we get the same result as the pipeline):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">temperature</code> <code class="o">=</code> <code class="n">clip_model</code><code class="o">.</code><code class="n">logit_scale</code><code class="o">.</code><code class="n">detach</code><code class="p">()</code><code class="o">.</code><code class="n">exp</code><code class="p">()</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">rescaled_similarities</code> <code class="o">=</code> <code class="n">similarities</code> <code class="o">*</code> <code class="n">temperature</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">probabilities</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">rescaled_similarities</code> <code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">probabilities</code><code class="w"/>
<code class="go">tensor([[0.0011, 0.9973, 0.0017]])</code></pre>

<p>CLIP wasn’t the only surprise OpenAI had in stock in 2021<a data-type="indexterm" data-startref="xi_multimodaltransformersCLIP164129_1" id="id3878"/><a data-type="indexterm" data-startref="xi_CLIPcontrastivelanguageimagepretraining164129_1" id="id3879"/><a data-type="indexterm" data-startref="xi_encoderdecodermodelsCLIPtextplusimagemodel164129_1" id="id3880"/><a data-type="indexterm" data-startref="xi_textplusimagevideotoolsCLIP164129_1" id="id3881"/><a data-type="indexterm" data-startref="xi_contrastivelanguageimagepretrainingCLIP164129_1" id="id3882"/>. Just the following month, OpenAI announced DALL·E, which can generate impressive images given a text description. Let’s discuss it now.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="DALL·E: Generating Images from Text Prompts"><div class="sect2" id="id326">
<h2>DALL·E: Generating Images from Text Prompts</h2>

<p>OpenAI <a href="https://homl.info/dalle">DALL·E</a>,⁠<sup><a data-type="noteref" id="id3883-marker" href="ch16.html#id3883">28</a></sup> released in February 2021, is a model<a data-type="indexterm" data-primary="multimodal transformers" data-secondary="DALL·E" id="xi_multimodaltransformersDALLE16510192_1"/><a data-type="indexterm" data-primary="DALL·E" id="xi_DALLE16510192_1"/><a data-type="indexterm" data-primary="prompt engineering, chatbot training" id="xi_promptengineeringchatbottraining16510192_1"/><a data-type="indexterm" data-primary="generating images from text, DALL·E" id="xi_generatingimagesfromtextDALLE16510192_1"/> capable of generating images based on text prompts, such as “an armchair in the shape of an avocado”. Its architecture is quite simple (see the lefthand side of <a data-type="xref" href="#dalle_diagram">Figure 16-14</a>): a GPT-like model trained to predict the next token, but unlike GPT, it was pretrained on millions of image-caption pairs, and fed input sequences composed on text tokens followed by visual tokens. At inference time, you only feed it the text tokens, and the model then generates the visual tokens, one at a time, until you get the full image. The visual tokens are generated by a dVAE model<a data-type="indexterm" data-primary="dVAEs (discrete variational autoencoders)" id="id3884"/><a data-type="indexterm" data-primary="discrete variational autoencoders (dVAEs)" id="id3885"/>, which takes an image and outputs a sequence of tokens from a fixed vocabulary. Sadly, the model was never released to the public, but the paper was detailed enough so some open source replications are available, such as <a href="https://huggingface.co/dalle-mini">DALL·E mini</a>, also known as Craiyon.</p>

<p>One year later, in April 2022, OpenAI released <a href="https://homl.info/dalle2">DALL·E 2</a>,⁠<sup><a data-type="noteref" id="id3886-marker" href="ch16.html#id3886">29</a></sup> able to generate even higher quality images. Its architecture is actually very different: the text is fed to a CLIP model which outputs a text embedding, then this text embedding is fed<a data-type="indexterm" data-primary="diffusion models" id="id3887"/> to a <em>diffusion model</em> which uses it to guide its image generation process (we will discuss diffusion models in <a data-type="xref" href="ch18.html#autoencoders_chapter">Chapter 18</a>). The model is not open source, but it’s available through a paid API, and via some products such as Microsoft Designer, Bing Image Creator, Canva, ChatGPT, and more.</p>

<figure class="width-80"><div id="dalle_diagram" class="figure">
<img src="assets/hmls_1614.png" alt="Diagram comparing the architectures of DALL·E and DALL·E 2, illustrating the transition from text embedding via a decoder and dVAE (for DALL·E) to CLIP and diffusion models (for DALL·E 2) in generating the image of a Chinese tower." width="1103" height="427"/>
<h6><span class="label">Figure 16-14. </span>DALL·E (left) and DALL·E 2 (right)</h6>
</div></figure>

<p>DALL·E 3 was released in October 2023. Sadly, by then OpenAI had fully shifted away from its initial openness: there was no peer-reviewed paper, no code, no weights, no data. Like the previous version, DALL·E 3 is available through an API and via some products. We know it’s diffusion-based, it doesn’t use CLIP, and it’s tightly integrated with GPT-4<a data-type="indexterm" data-primary="GPT-4, and DALL·E" id="id3888"/>, which rewrites the prompt before generating the image. It works impressively well: it outputs stunning images which match the prompts much more precisely than previous versions. The difference is particularly striking<a data-type="indexterm" data-primary="compositional prompts, DALL·E" id="id3889"/> for <em>compositional prompts</em> (e.g., “A fluffy white cat sitting on a red velvet cushion, with a vase of sunflowers behind it, bathed in golden hour light. The cat is looking directly at the viewer”.). DALL·E 1 and 2 would generally follow only one or two elements of such prompts, whereas DALL·E 3 follows instructions much more closely. The image quality, realism, artistic style, and consistency are astounding. Lastly, DALL·E 3 also integrates some moderation capabilities<a data-type="indexterm" data-startref="xi_multimodaltransformersDALLE16510192_1" id="id3890"/><a data-type="indexterm" data-startref="xi_DALLE16510192_1" id="id3891"/><a data-type="indexterm" data-startref="xi_promptengineeringchatbottraining16510192_1" id="id3892"/><a data-type="indexterm" data-startref="xi_generatingimagesfromtextDALLE16510192_1" id="id3893"/>.</p>

<p>The next landmark in our multimodal journey came one month after the first DALL·E model: the Perceiver.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Perceiver: Bridging High-Resolution Modalities with Latent Spaces"><div class="sect2" id="id327">
<h2>Perceiver: Bridging High-Resolution Modalities with Latent Spaces</h2>

<p>Every transformer<a data-type="indexterm" data-primary="Perceiver" id="xi_Perceiver1652318_1"/><a data-type="indexterm" data-primary="multimodal transformers" data-secondary="Perceiver" id="xi_multimodaltransformersPerceiver1652318_1"/> so far has required chopping the inputs into meaningful tokens<a data-type="indexterm" data-primary="tokenization" id="id3894"/><a data-type="indexterm" data-primary="inductive bias" id="id3895"/><a data-type="indexterm" data-primary="bias" data-secondary="inductive bias" id="id3896"/>. In the case of text, tokens represent words or subwords. In the case of ViTs, they represent 16 × 16 pixel patches. In VideoBERT, it’s short 1.5-second clips. In audio transformers, it’s short audio clips. If we fed individual characters, pixels, or audio frames directly into a transformer, the input sequence would be extremely long, and we would run into the quadratic attention problem. Also, we would lose important inductive biases: for example, by chopping an image into patches, we enforce a strong inductive bias toward proximity (i.e., nearby pixels are assumed to be more strongly correlated than distant pixels).</p>

<p>However, such tokenization is modality-specific, which makes it harder to deal with new modalities or mix them in the model. Moreover, inductive biases are great when you don’t have a lot of training data (assuming the biases are correct), but if your dataset is large, you will often get better performance by using unbiased models with very few implicit assumptions. Sure, the model will have to figure out on its own that nearby pixels are generally related, but on the other hand, it will be flexible enough to discover patterns that might otherwise go unnoticed.</p>

<p>This is why DeepMind<a data-type="indexterm" data-primary="DeepMind" data-secondary="multimodal transformers" id="id3897"/> introduced the <a href="https://homl.info/perceiver"><em>Perceiver</em></a>⁠<sup><a data-type="noteref" id="id3898-marker" href="ch16.html#id3898">30</a></sup> in March 2021.  This architecture is capable of directly handling any modality at the lowest level: characters, pixels, audio frames, and more. Moreover, it does so with a modality-agnostic<a data-type="indexterm" data-primary="modality-agnostic design, Perceiver transformer" id="id3899"/> design, so the same model can handle different modalities. The Perceiver architecture is shown in <a data-type="xref" href="#perceiver_diagram">Figure 16-15</a>.</p>

<figure class="width-85"><div id="perceiver_diagram" class="figure">
<img src="assets/hmls_1615.png" alt="Diagram of the Perceiver architecture showing the flow of input pixels through Fourier positional encoding, linear encoding, and conversion into pixel tokens, which interact with latent tokens through cross-attention layers, leading to a classification head." width="1098" height="585"/>
<h6><span class="label">Figure 16-15. </span>Perceiver architecture: inputs are ingested through cross-attention layers, while the main input is a sequence of learned latent tokens</h6>
</div></figure>

<p>Let’s walk through this architecture:</p>

<ul>
<li>
<p>The input is first chopped into its smallest constituents. In this example, the input is an image, so it is chopped into individual pixels: we now have a sequence of 3D vectors (red, green, blue).</p>
</li>
<li>
<p>Positional encodings<a data-type="indexterm" data-primary="positional encodings" id="id3900"/> are concatenated to these feature vectors. Perceiver uses Fourier positional encodings, which are very similar to the sinusoidal positional encodings of the original Transformer, except they encode all of the input’s dimensions. Since an image is 2D, each pixel’s horizontal and vertical coordinates are encoded; for example, if a pixel is located at coordinates <em>x</em> and <em>y</em> (normalized between –1 and 1), then the positional encoding vector will include <em>x</em> and <em>y</em>, followed by sin(π_fx_), sin(π_fy_), and cos(π_fx_), cos(π_fy_) repeated <em>K</em> times (typically 6) with the frequency <em>f</em> starting at 1 and going up to <em>μ</em> / 2 (spaced equally), where <em>μ</em> is the target resolution (e.g., if the image is 224 × 224 pixels, then <em>μ</em> = 224).⁠<sup><a data-type="noteref" id="id3901-marker" href="ch16.html#id3901">31</a></sup> The dimensionality of the positional encoding vector is <em>d</em>(2_K_ + 1), where <em>d</em> is the number of input dimensions (i.e., 1 for audio, 2 for images, 3 for videos, etc.).</p>
</li>
<li>
<p>The pixel tokens now have 3 + 2 × (2 × 6 + 1) = 29 dimensions. We then pass them through a linear layer to project them to the Perceiver’s dimensionality (e.g., 512).</p>
</li>
<li>
<p>The Perceiver’s architecture itself is composed of repeated processing blocks (e.g., eight), where each block is composed of a single cross-attention<a data-type="indexterm" data-primary="cross-attention" id="id3902"/><a data-type="indexterm" data-primary="multi-head attention (MHA)" data-secondary="Perceiver’s use of" id="id3903"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="cross-attention" id="id3904"/> multi-head attention layer (MHA) followed by a regular transformer encoder (e.g., with six encoder layers). The final block is composed of a single cross-attention MHA layer and an average pooling layer to reduce the input sequence into a single vector, which is then fed to a classification head (i.e., linear plus softmax).</p>
</li>
<li>
<p>The pixel tokens are fed to the Perceiver exclusively through the MHA layers, and they play the role of the keys and values. In other words, the Perceiver attends to the pixel tokens through cross-attention only.</p>
</li>
<li>
<p>Crucially, the Perceiver’s main input is a fairly short<a data-type="indexterm" data-primary="latent tokens, Perceiver" id="id3905"/> sequence of <em>latent tokens</em> (e.g., 512). These tokens<a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="and Perceiver" data-secondary-sortas="Perceiver" id="id3906"/> are similar to an RNN’s hidden state: an initial sequence (learned during training) is fed to the Perceiver, and it gradually gets updated as the model learns more and more about the pixel tokens via cross-attention. Since it’s a short sequence, it doesn’t suffer much from the quadratic attention problem. This is called<a data-type="indexterm" data-primary="latent bottleneck trick, Perceiver" id="id3907"/> the <em>latent bottleneck trick</em>, and is the key to the success of the Perceiver.</p>
</li>
<li>
<p>The authors experimented sharing weights across processing blocks (excluding the first cross-attention layer), and they got good results. When the processing blocks share the same weights, the Perceiver is effectively a recurrent neural network, and the latent tokens really are its hidden state.</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>As we saw in <a data-type="xref" href="ch07.html#dimensionality_chapter">Chapter 7</a>, the manifold hypothesis<a data-type="indexterm" data-primary="manifold hypothesis" id="id3908"/> states that most real-world data lives near a low-dimensional manifold, much like a rolled piece of paper lives in 3D but is essentially a 2D object. This 2D space is latent (i.e., hidden, potential) until we unroll the paper. Similarly, the Perceiver’s goal is to “unroll” its high-dimensional inputs so the model can work in the latent space<a data-type="indexterm" data-primary="latent space" id="id3909"/>, using low-dimensional representations.</p>
</div>

<p>Importantly, this architecture can efficiently process high-resolution inputs. For example, a 224 × 224 image has 50,176 pixels, so if we tried to feed such a long sequence of pixel tokens directly to a regular encoder, each self-attention layer would have to compute 50,176<sup>2</sup> ≈ 2.5 billion attention scores! But since the Perceiver only attends to the pixel tokens through cross-attention, it just needs to compute 50,176 times the number of latent tokens. Even for the biggest Perceiver variant, that’s just a total of 50,176 × 512 ≈ 25.7 million attention scores, which is roughly 100 times less compute.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Thanks to the latent bottleneck, the Perceiver scales linearly with the number of pixel tokens, instead of quadratically.</p>
</div>

<p>The authors trained the Perceiver using regular supervised learning on various classification tasks across several modalities, including image-only (ImageNet), audio plus video (AudioSet),⁠<sup><a data-type="noteref" id="id3910-marker" href="ch16.html#id3910">32</a></sup> or point clouds (ModelNet40),⁠<sup><a data-type="noteref" id="id3911-marker" href="ch16.html#id3911">33</a></sup> all using the same model architecture. They got competitive results, in some cases even reaching the state of the art.</p>

<p>The videos in the AudioSet dataset were downsampled to 224 × 224 pixels at 25 frames per second (fps), with a 48 kHz audio sample rate. You could theoretically feed each pixel and each audio frame individually to the Perceiver, but this would be a bit extreme, as each 10s video would be represented as a sequence of 224 × 224 × 25 × 10 ≈ 12.5 million pixel tokens, and 48,000 × 10 = 480,000 audio tokens.</p>

<p>So the authors had to compromise. They trained on 32-frame clips (at 25 fps, that’s 1.28s each, instead of 10s) and they chopped the video into 2 × 8 × 8 patches (i.e., 2 frames × 8 × 8 pixels), resulting in 224 × 224 × 32 / (2 × 8 × 8) = 12,544 video tokens of 128 RGB pixels each (plus the position encoding). They also chopped the audio into clips of 128 frames each, resulting in 480 audio tokens. They also tried converting the audio to a mel spectrogram (which resulted in 4,800 audio tokens). Using a spectrogram instead of raw audio is a standard practice in audio processing, but it made very little difference to the model’s performance, which shows that the Perceiver is able to extract useful features from the raw data without any help.</p>

<p>Then they simply concatenated the video and audio token sequences (after positional encoding), and also concatenated a modality embedding to help the model distinguish the modalities<a data-type="indexterm" data-startref="xi_multimodaltransformersPerceiver1652318_1" id="id3912"/>.</p>

<p>One limitation of the Perceiver architecture is that it was only designed for multimodal classification. That said, instead of averaging the latent tokens and feeding them to a classification head, we could try to use them for other downstream tasks<a data-type="indexterm" data-startref="xi_Perceiver1652318_1" id="id3913"/>. Of course, the DeepMind researchers thought of that, and just a few months later they published the Perceiver IO architecture.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Perceiver IO: A Flexible Output Mechanism for the Perceiver"><div class="sect2" id="id328">
<h2>Perceiver IO: A Flexible Output Mechanism for the Perceiver</h2>

<p>DeepMind<a data-type="indexterm" data-primary="multimodal transformers" data-secondary="Perceiver IO" id="xi_multimodaltransformersPerceiverIO165609_1"/><a data-type="indexterm" data-primary="Perceiver IO" id="xi_PerceiverIO165609_1"/><a data-type="indexterm" data-primary="DeepMind" data-secondary="multimodal transformers" id="id3914"/> released <a href="https://homl.info/perceiverio">Perceiver IO</a> in July 2021.⁠<sup><a data-type="noteref" id="id3915-marker" href="ch16.html#id3915">34</a></sup> It can perform classification tasks like the Perceiver, but also many other tasks such as masked language modeling (MLM) better than BERT, <em>optical flow</em> (i.e., predicting where each pixel will move in the next video frame)<a data-type="indexterm" data-primary="optical flow, Perceiver IO’s use of" id="id3916"/><a data-type="indexterm" data-primary="masked language model (MLM)" id="id3917"/><a data-type="indexterm" data-primary="MLM (masked language model)" id="id3918"/>, actually beating the state of the art, and even playing StarCraft II.</p>

<p>The model is identical to Perceiver up to the output latent tokens, but the pooling layer and the classification head are replaced by a very flexible output mechanism (see <a data-type="xref" href="#perceiverio_diagram">Figure 16-16</a>):</p>

<ul>
<li>
<p>A new cross-attention layer is added, which acts as a decoder by attending to the output latent tokens and producing the final output representations. These output representations can then go through a task-specific head, or even multiple heads if we’re doing multitask learning.</p>
</li>
<li>
<p>The number and nature of the output tokens is task-specific:</p>

<ul>
<li>
<p>For classification, we only need one output vector, which we can feed to a classification head. Therefore, we need one output query token, which can just be a learned embedding.</p>
</li>
<li>
<p>For masked language modeling, we can use one output query token per masked token, and add a classification head on top of the output representations (i.e., linear plus softmax) to get one estimated token probability for each masked token. To help the model locate each masked token, the output query tokens are learnable positional embeddings based on the masked token’s position. For example, given the masked sentence “The dog [MASK] the [MASK]”, the masked tokens are located at positions #2 and #4, so we use the positional embedding #2 as the first output query token, and #4 as the second output query token. This same approach works for any other modality: just predict the masked tokens. It can also be extended to multiple modalities at once, typically by adding a modality embedding to the output query token before feeding it to the output cross-attention layer.</p>
</li>
<li>
<p>For optical flow, the authors actually used one output token per pixel, using the same pixel representations both as the inputs to the Perceiver and as the output query tokens. This representation includes a Fourier positional encoding.</p>
</li>
</ul>
</li>
</ul>

<figure class="width-75"><div id="perceiverio_diagram" class="figure">
<img src="assets/hmls_1616.png" alt="Diagram of Perceiver IO architecture showing input tokens processed into latent tokens, which connect through a cross-attention layer to output query tokens, leading to task-specific head(s)." width="952" height="517"/>
<h6><span class="label">Figure 16-16. </span>Perceiver IO architecture: one output query token per desired output token is fed to a cross-attention layer that attends to the Perceiver’s output latent tokens</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Because the output query tokens only ever attend to the latent tokens, the Perceiver IO can handle a very large number of output query tokens. The latent bottleneck allows the model to scale linearly for both the inputs and outputs.</p>
</div>

<p>The Perceiver IO is a bidirectional architecture; there’s no causal masking, so it’s not well suited for autoregressive tasks. In particular, it cannot efficiently perform next token prediction, so it’s not well suited for text generation tasks such as image captioning. Sure, you could feed it an image and some text with a mask token at the end, and make it predict which token was masked, then start over to get the next token, and so on, but it would be horribly inefficient compared to a causal model (which can cache the previous state).</p>

<p>For this reason, Google and DeepMind researchers released the <a href="https://homl.info/perceiverar">Perceiver AR architecture</a> in February 2022 to address this limitation (AR stands for autoregressive)<a data-type="indexterm" data-primary="Perceiver AR" id="id3919"/>. The model works very much like the Perceiver, except the last tokens of the input sequence are used as the latent tokens, the model is causal over these latent tokens, and it is trained using next token prediction. Perceiver AR didn’t quite have the same impact as Perceiver and Perceiver IO, but it got excellent results on very long input sequences, thanks to its linear scaling capability<a data-type="indexterm" data-startref="xi_multimodaltransformersPerceiverIO165609_1" id="id3920"/><a data-type="indexterm" data-startref="xi_PerceiverIO165609_1" id="id3921"/>.</p>

<p>But DeepMind<a data-type="indexterm" data-primary="DeepMind" data-secondary="multimodal transformers" id="id3922"/> researchers weren’t done with multimodal ML; they soon released yet another amazing multimodal model, partly based on the Perceiver: Flamingo.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Flamingo: Open-Ended Visual Dialogue"><div class="sect2" id="id329">
<h2>Flamingo: Open-Ended Visual Dialogue</h2>

<p>DeepMind’s <a href="https://homl.info/flamingo">Flamingo paper</a>, published in April 2022<a data-type="indexterm" data-primary="multimodal transformers" data-secondary="Flamingo" id="xi_multimodaltransformersFlamingo1658379_1"/><a data-type="indexterm" data-primary="Flamingo" id="xi_Flamingo1658379_1"/><a data-type="indexterm" data-primary="open-ended visual dialogue" id="xi_openendedvisualdialogue1658379_1"/>, introduced a visual-language model (VLM)<a data-type="indexterm" data-primary="visual-language model (VLM), Flamingo" id="id3923"/> that can take arbitrary sequences of text and images as input and generate coherent free-form text. Most importantly, its few-shot<a data-type="indexterm" data-primary="few-shot learning (FSL)" id="id3924"/><a data-type="indexterm" data-primary="FSL (few-shot learning)" id="id3925"/> performance is excellent on a wide variety of tasks.</p>

<p>For example, suppose you want to build a model that takes a picture and outputs a poem about that image: no need to train a new model; you can just feed a few examples to Flamingo, add the new image at the end, and it will happily generate a poem about this new image. If you want it to detect license plate numbers on car photos, just give it a few photos along with the corresponding license plate numbers (as text), then add a new car photo, and Flamingo will output its license plate number. You can just as easily use Flamingo for image captioning. Or visual question answering. Or you can ask it to compare two images. In fact, you can even give the model several frames from a video and ask it to describe the action. It’s an incredibly versatile and powerful model out of the box, without any fine-tuning.</p>

<p>Let’s look at Flamingo’s architecture (see <a data-type="xref" href="#flamingo_diagram">Figure 16-17</a>):</p>

<ul>
<li>
<p>Instead of starting from scratch, Flamingo is based on two large pretrained models, which are both frozen: a vision model and a decoder-only language model. The authors used Chinchilla and CLIP, respectively, but many other powerful models would work fine too.</p>
</li>
<li>
<p>Each input image is fed to the vision model, and the outputs go through a Perceiver model, called<a data-type="indexterm" data-primary="Resampler" id="id3926"/> a <em>Resampler</em>, which produces a sequence of latent token representations. This ensures that every image gets represented as a fairly short sequence of latent representations<a data-type="indexterm" data-primary="latent representation of inputs" id="id3927"/> (typically much shorter than the output of the vision model). This works around the quadratic attention problem.</p>
</li>
<li>
<p>The sequences output by the Resampler are fed as the keys/values<a data-type="indexterm" data-primary="gated xattn-dense modules" id="id3928"/> to many <em>gated xattn-dense</em> modules, which are inserted before every block in the frozen LLM:</p>

<ul>
<li>
<p>Each gated xattn-dense module is composed of a masked multi-head attention layer followed by a feedforward module, with a skip connection each, just like the cross-attention half of a vanilla Transformer’s decoder layer.</p>
</li>
<li>
<p>However, both the masked MHA layer and the feedforward module are followed<a data-type="indexterm" data-primary="tanh gate" id="id3929"/> by a <em>tanh gate</em>. These gates multiply their input by tanh(<em>α</em>), where <em>α</em> is a learnable scalar parameter initialized to 0 (one per gate). Since tan(0) = 0, training starts with all gates closed, so the inputs can only flow through the skip connections, and the gated xattn-dense modules have no impact on the LLM. But as training progresses, the model gradually learns to open the gates, allowing the gated modules to influence the LLM’s outputs.</p>
</li>
<li>
<p>In the gated xattn-dense module, each text token can only attend to visual tokens from the closest image located before it; visual tokens from all other images are masked. For example, the last text token (“is”) can only attend to the Chinese tower photo, it cannot directly attend to the flower photo. However, since previous text tokens have information about the flower photo, the last token does have indirect access to the flower photo via the frozen LLM’s self-attention layers.</p>
</li>
</ul>
</li>
<li>
<p>The text is tokenized as the LLM expects (e.g., Chinchilla expects start-of-sequence and end-of-sequence tokens, which I denoted as &lt;s&gt; and &lt;/s&gt;), but a couple new special tokens are added. Each image-text chunk ends with an end-of-chunk token (which I denoted as &lt;/c&gt;), and each image is replaced with an image token (which I denoted as &lt;i&gt;). Both are represented using trainable embeddings.</p>
</li>
</ul>

<figure class="width-95"><div id="flamingo_diagram" class="figure">
<img src="assets/hmls_1617.png" alt="Diagram illustrating the Flamingo model architecture, showing the flow of image and text inputs through a vision encoder, Resampler, and gated xattn-dense modules, which integrate into the LLM blocks." width="1274" height="594"/>
<h6><span class="label">Figure 16-17. </span>Flamingo takes any sequence of text and images, and outputs coherent free-form text</h6>
</div></figure>

<p class="pagebreak-before">The bad news is that DeepMind did not release Flamingo to the public. The good news is that open source replications and variants are available:</p>

<ul>
<li>
<p><a href="https://homl.info/openflamingo">OpenFlamingo</a>, created by the MLFoundations team<a data-type="indexterm" data-primary="OpenFlamingo" id="id3930"/>, which is part of the non-profit organization LAION. It is fully open source and available on the Hugging Face Hub (e.g., openflamingo/OpenFlamingo-9B-vitl-mpt7b, based on a CLIP ViT-L/14 vision encoder and a MPT-7B LLM).</p>
</li>
<li>
<p><a href="https://homl.info/idefics">IDEFICS</a> by Hugging Face, trained on a huge dataset named OBELICS,⁠<sup><a data-type="noteref" id="id3931-marker" href="ch16.html#id3931">35</a></sup> composed of 141 million interleaved text-image documents gathered from Common Crawl (including 350 million images and 115 billion text tokens). Both IDEFICS and OBELICS are available on the hub (e.g., Idefics3-8B-Llama3 and OBELICS by HuggingFaceM4). The architecture includes a few improvements over Flamingo; for example, you can more easily swap in different LLMs or vision encoders. IDEFICS itself is open source, but the models it is based on may have licensing limitations. In particular, IDEFICS 1 and 3 are based on Llama, which has some limitations for commercial use, while IDEFICS 2 is based on Mistral, which is fully open source.</p>
</li>
<li>
<p><a href="https://homl.info/audioflamingo">AudioFlamingo</a> by Nvidia<a data-type="indexterm" data-primary="IDEFICS" id="id3932"/><a data-type="indexterm" data-primary="OBELICS" id="id3933"/><a data-type="indexterm" data-primary="AudioFlamingo" id="id3934"/>, which is very similar to Flamingo but handles audio instead of images.</p>
</li>
<li>
<p>Other variants are available, such as domain-specific models like <a href="https://homl.info/medflamingo">Med-Flamingo</a>, an OpenFlamingo model trained on medical documents<a data-type="indexterm" data-startref="xi_multimodaltransformersFlamingo1658379_1" id="id3935"/><a data-type="indexterm" data-startref="xi_Flamingo1658379_1" id="id3936"/><a data-type="indexterm" data-startref="xi_openendedvisualdialogue1658379_1" id="id3937"/>.</p>
</li>
</ul>

<p>The last multimodal architecture we will discuss is bootstrapping language-image pretraining, or BLIP, by Salesforce. Its second version, BLIP-2, also successfully reuses two large pretrained models—a vision model and an LLM—to create a VLM that can ingest both images and text, and generate free-form text. Let’s see how.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="BLIP and BLIP-2"><div class="sect2" id="id330">
<h2>BLIP and BLIP-2</h2>

<p>The original <a href="https://homl.info/blip">BLIP model</a> is an excellent visual-language model<a data-type="indexterm" data-primary="multimodal transformers" data-secondary="BLIP and BLIP-2" id="xi_multimodaltransformersBLIPandBLIP21661186_1"/><a data-type="indexterm" data-primary="BLIP (bootstrapping language-image pretraining)" id="xi_BLIPbootstrappinglanguageimagepretraining1661186_1"/><a data-type="indexterm" data-primary="bootstrapping language-image pretraining (BLIP)" id="xi_bootstrappinglanguageimagepretrainingBLIP1661186_1"/> released by Salesforce in January 2022.⁠<sup><a data-type="noteref" id="id3938-marker" href="ch16.html#id3938">36</a></sup> Its architecture<a data-type="indexterm" data-primary="mixture of encoder-decoder (MED) model, BLIP" id="id3939"/><a data-type="indexterm" data-primary="MED (mixture of encoder-decoder) model, BLIP" id="id3940"/><a data-type="indexterm" data-primary="encoder-decoder models" data-secondary="BLIP and MED" id="id3941"/> is a <em>mixture of encoder-decoder</em> (MED) composed of a text-only encoder, a vision-only encoder, an image-grounded text encoder, and an image-grounded text decoder, sharing many layers. This flexible architecture made it possible to train the model simultaneously on three distinct objectives<a data-type="indexterm" data-primary="image-text matching (ITM), BLIP" id="id3942"/><a data-type="indexterm" data-primary="ITM (image-text matching), BLIP" id="id3943"/><a data-type="indexterm" data-primary="ITC (image-text contrastive) loss, BLIP" id="id3944"/><a data-type="indexterm" data-primary="image-text contrastive (ITC) loss, BLIP" id="id3945"/>: <em>image-text matching</em> (ITM), an <em>image-text contrastive</em> (ITC) loss to align image and text representations (similar to CLIP), and language modeling (LM)<a data-type="indexterm" data-primary="language modeling (LM), BLIP" id="id3946"/><a data-type="indexterm" data-primary="LM (language modeling), BLIP" id="id3947"/> where the model must try to generate the caption<a data-type="indexterm" data-primary="captioning of images and video" id="id3948"/> using next token prediction.</p>

<p>Another important reason for BLIP’s success is the fact that it was pretrained on a very large and clean dataset. To build this dataset, the authors simultaneously trained a <em>captioning module</em> to generate synthetic captions for images<a data-type="indexterm" data-primary="filtering module, BLIP" id="id3949"/>, and a <em>filtering module</em> to remove noisy data. This approach, named <em>CapFilt</em>, removed<a data-type="indexterm" data-primary="CapFilt captioning, BLIP" id="id3950"/> poor quality captions from the original web-scraped dataset, and added many new high-quality synthetic captions. After this bootstrapping stage, the authors trained the final model on the large and clean dataset they had just built. It’s a two-stage process, hence the name BLIP: <em>bootstrapping language-image pretraining</em>.</p>

<p>One year later, in January 2023, Salesforce released <a href="https://homl.info/blip2">BLIP-2</a>,⁠<sup><a data-type="noteref" id="id3951-marker" href="ch16.html#id3951">37</a></sup> which is based on the same core ideas but greatly improves the model’s performance by reusing two large pretrained models, one vision model and one language model, both frozen. <a data-type="indexterm" data-primary="BLIP-2" id="xi_BLIP216615442_1"/>BLIP-2 even outperformed Flamingo with a much smaller model.</p>

<p>Training is split in two stages. BLIP-2’s architecture during the first stage is shown in <a data-type="xref" href="#blip2_stage1_diagram">Figure 16-18</a>.</p>

<figure class="width-90"><div id="blip2_stage1_diagram" class="figure">
<img src="assets/hmls_1618.png" alt="Diagram illustrating BLIP-2's Stage 1 pretraining architecture, focusing on the Q-Former module integrating vision and text sequences through various attention and processing layers." width="1175" height="739"/>
<h6><span class="label">Figure 16-18. </span>BLIP-2 pretraining, Stage 1: training the Q-Former</h6>
</div></figure>

<ul>
<li>
<p>The central component is called<a data-type="indexterm" data-primary="Q-Former (querying transformer), BLIP-2" id="xi_QFormerqueryingtransformerBLIP21662334_1"/> the <em>Q-Former</em> (querying transformer). Its architecture is the same as BERT-base, and in fact it’s even initialized using BERT-base’s pretrained weights, but it also has some extra cross-attention layers that let it attend to visual tokens produced by the pretrained visual encoder. The cross-attention layers are inserted in every other encoder layer, between the self-attention layer and the feedforward module, and they are initialized randomly.</p>
</li>
<li>
<p>The Q-Former processes three sequences: a sequence of text tokens (using BERT tokenization and token embeddings), a sequence of visual tokens produced by the pretrained vision encoder, and lastly a sequence of trainable Perceiver-style latent tokens. In BLIP-2, the latent tokens are called <em>query tokens</em> because<a data-type="indexterm" data-primary="query tokens, BLIP-2" id="id3952"/> their output representations will later be used to query the pretrained LLM.</p>
</li>
<li>
<p>The Q-Former is trained with the same three objectives as BLIP: ITM, ITC, and LM. For each objective, a different mask is used:</p>

<ul>
<li>
<p>For ITM<a data-type="indexterm" data-primary="ITM (image-text matching), BLIP" id="id3953"/><a data-type="indexterm" data-primary="image-text matching (ITM), BLIP" id="id3954"/>, query tokens and text tokens can attend to each other. In other words, the output representations for the query tokens represent text-grounded visual features, and the output representations for the text tokens represent image-grounded text features. The query token outputs go through a linear layer which produces two logits per query token (image-text match or mismatch), and the model computes the mean logits across all query tokens, then computes the binary cross-entropy.</p>
</li>
<li>
<p>For ITC<a data-type="indexterm" data-primary="ITC (image-text contrastive) loss, BLIP" id="id3955"/><a data-type="indexterm" data-primary="image-text contrastive (ITC) loss, BLIP" id="id3956"/>, query tokens and text tokens cannot attend to each other. In other words, the Q-Former’s outputs represent visual-only features and text-only features. For each possible image/caption pair in the batch, the model computes the maximum similarity between the query token outputs and the class token output. We get a matrix of maximum similarities, and the loss pushes the values toward +1 on the main diagonal, and pushes the other values toward 0, much like CLIP.</p>
</li>
<li>
<p>For LM<a data-type="indexterm" data-primary="LM (language modeling), BLIP" id="id3957"/><a data-type="indexterm" data-primary="language modeling (LM), BLIP" id="id3958"/>, text tokens can only attend previous tokens (i.e., we use a causal mask), but they can attend all query tokens. However, query tokens cannot attend any text token. In other words, the query token outputs represent visual-only features, while text token outputs represent image-grounded causal text features. The model is trained using next token prediction: each text token’s output goes through a classification head which must predict the next token in the caption.</p>
</li>
</ul>
</li>
</ul>

<p>You may be surprised that the Q-Former is used to encode text (for ITM and ITC) and also to generate text (for LM). Since the Q-Former is initialized using the weights of a pretrained BERT-base model, it’s pretty good at text encoding right from the start of training, but it initially doesn’t know that it has to predict the next token for the LM task. Luckily, it can learn fairly fast since it’s not starting from scratch; it has good BERT features to work with. However, we need to tell it whether we want it to encode the text or predict the next token. For this, we replace the class token with a <em>decode token</em> during LM.⁠<sup><a data-type="noteref" id="id3959-marker" href="ch16.html#id3959">38</a></sup></p>

<p>Once stage 1 is finished, the Q-Former is already a powerful model that can encode images and text into the same space, so a photo of a chimpanzee produces a very similar output representation as the caption “A photo of a chimpanzee”. But it’s even better than that: the query token outputs were trained to be most helpful for next token prediction.</p>
<div data-type="tip"><h6>Tip</h6>
<p>To produce negative examples for ITM, one strategy is to randomly pick a caption in the same batch, excluding the image’s true caption. However, this makes the task too easy, so the model doesn’t learn much. Instead, the authors used a <em>hard negative mining</em> strategy, where difficult captions are more likely to be sampled. For example, given a photo of a chimpanzee, the caption “A gorilla” is more likely to be sampled than “A spacecraft”. To find difficult captions, the algorithm uses the similarity scores from the ITC task.</p>
</div>

<p>So it’s time for the second stage of training (see <a data-type="xref" href="#blip2_stage2_diagram">Figure 16-19</a>):</p>

<ul>
<li>
<p>We keep the vision transformer and the Q-Former, but we drop the rest and we add a new linear layer, initialized randomly, on top of the Q-Former.</p>
</li>
<li>
<p>For each image/caption pair, the Q-Former attends to the visual features produced by the pretrained vision encoder, and the outputs go through the linear layer to produce a sequence of visual query tokens<a data-type="indexterm" data-startref="xi_QFormerqueryingtransformerBLIP21662334_1" id="id3960"/>.</p>
</li>
<li>
<p>The visual query tokens and the text token representations are concatenated and fed to the (frozen) pretrained LLM. We train BLIP-2 to predict the next caption token.</p>
</li>
</ul>

<p>During stage 2, the model learns to properly map the visual query tokens to the LLM’s input space. Once trained, the model can be used like in stage 2, generating visual-grounded text.</p>

<figure class="width-80"><div id="blip2_stage2_diagram" class="figure">
<img src="assets/hmls_1619.png" alt="Diagram illustrating the BLIP-2 pretraining process, showing how visual features from a vision encoder are processed by a Q-former and a linear layer to generate visual query tokens, which are then combined with text tokens and fed to a large language model (LLM)." width="1004" height="581"/>
<h6><span class="label">Figure 16-19. </span>BLIP-2 pretraining, Stage 2: training the linear layer to map the query tokens to the LLM’s input space</h6>
</div></figure>

<p>Let’s use BLIP-2 to generate a caption for an image:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">Blip2Processor</code><code class="p">,</code> <code class="n">Blip2ForConditionalGeneration</code>

<code class="n">model_id</code> <code class="o">=</code> <code class="s2">"Salesforce/blip2-opt-2.7b"</code>
<code class="n">blip2_processor</code> <code class="o">=</code> <code class="n">Blip2Processor</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_id</code><code class="p">)</code>
<code class="n">blip2_model</code> <code class="o">=</code> <code class="n">Blip2ForConditionalGeneration</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="n">model_id</code><code class="p">,</code> <code class="n">device_map</code><code class="o">=</code><code class="n">device</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">)</code>

<code class="n">image_url</code> <code class="o">=</code> <code class="s2">"http://images.cocodataset.org/val2017/000000039769.jpg"</code>  <code class="c1"># two cats</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">urllib</code><code class="o">.</code><code class="n">request</code><code class="o">.</code><code class="n">urlopen</code><code class="p">(</code><code class="n">image_url</code><code class="p">))</code>
<code class="n">inputs</code> <code class="o">=</code> <code class="n">blip2_processor</code><code class="p">(</code><code class="n">images</code><code class="o">=</code><code class="n">image</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code>
<code class="n">inputs</code> <code class="o">=</code> <code class="n">inputs</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">)</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">generated_ids</code> <code class="o">=</code> <code class="n">blip2_model</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="o">**</code><code class="n">inputs</code><code class="p">)</code>

<code class="n">generated_text</code> <code class="o">=</code> <code class="n">blip2_processor</code><code class="o">.</code><code class="n">batch_decode</code><code class="p">(</code><code class="n">generated_ids</code><code class="p">)</code></pre>

<p>What did BLIP-2 see?</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">generated_text</code><code class="w"/>
<code class="go">['&lt;image&gt;&lt;image&gt;&lt;image&gt;&lt;image&gt;[...]&lt;image&gt;&lt;/s&gt;two cats laying on a couch\n']</code></pre>

<p>It’s a good description of the photo, but it would nicer without the special tokens, so let’s get rid of them when decoding the model’s output:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">generated_text</code> <code class="o">=</code> <code class="n">blip2_processor</code><code class="o">.</code><code class="n">batch_decode</code><code class="p">(</code><code class="n">generated_ids</code><code class="p">,</code><code class="w"/>
<code class="gp">... </code>                                              <code class="n">skip_special_tokens</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">generated_text</code><code class="w"/>
<code class="gp">&gt;&gt;&gt;</code><code class="w"/>
<code class="go">['two cats laying on a couch\n']</code></pre>

<p>Perfect<a data-type="indexterm" data-startref="xi_multimodaltransformersBLIPandBLIP21661186_1" id="id3961"/><a data-type="indexterm" data-startref="xi_BLIPbootstrappinglanguageimagepretraining1661186_1" id="id3962"/><a data-type="indexterm" data-startref="xi_BLIP216615442_1" id="id3963"/><a data-type="indexterm" data-startref="xi_bootstrappinglanguageimagepretrainingBLIP1661186_1" id="id3964"/>!</p>
<div data-type="tip"><h6>Tip</h6>
<p>Also check out InstructBLIP<a data-type="indexterm" data-primary="InstructBLIP" id="id3965"/>, a BLIP-2 model with vision-language instruction tuning.</p>
</div>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Other Multimodal Models"><div class="sect1" id="id331">
<h1>Other Multimodal Models</h1>

<p>We’ve covered quite a few multimodal models<a data-type="indexterm" data-primary="multimodal transformers" data-secondary="other models" id="xi_multimodaltransformersothermodels1669444_1"/>, with very different architectures and pretraining techniques, but of course there are many others. Here is a quick overview of some of the most notable ones:</p>
<dl>
<dt>LayoutLM (Microsoft, Dec. 2019)</dt>
<dd>
<p>Document<a data-type="indexterm" data-primary="LayoutLM" id="id3966"/> understanding based on text, vision, and document layout. Version 3 was released in April 2022.</p>
</dd>
<dt>GLIP (Microsoft, Dec. 2021)</dt>
<dd>
<p>A vision-language model for visual grounding and object detection<a data-type="indexterm" data-primary="GLIP and GLIP-2" id="id3967"/>. GLIP-2 was released in 2022.</p>
</dd>
<dt>Stable Diffusion (Stability AI, Dec. 2021)</dt>
<dd>
<p>A powerful text-to-image model<a data-type="indexterm" data-primary="Stable Diffusion (SD)" id="id3968"/>.</p>
</dd>
<dt>OFA (Microsoft, Feb. 2022)</dt>
<dd>
<p>Unified (one for all)<a data-type="indexterm" data-primary="one for all (OFA)" id="id3969"/> vision-language pretraining framework handling various vision-language tasks.</p>
</dd>
<dt>CoCa (Google, May 2022)</dt>
<dd>
<p>A vision-language model pretrained<a data-type="indexterm" data-primary="CoCa" id="id3970"/> using contrastive and captioning objectives. CoCa influenced later models like PaLI-X and Flamingo-2.</p>
</dd>
<dt>PaLI (Google, Sep. 2022)</dt>
<dd>
<p>Multilingual multimodal models<a data-type="indexterm" data-primary="PaLI" id="id3971"/> for vision-language tasks like VQA and captioning, with strong zero-shot performance. The next versions, PaLI-X and PaLI-3, were released in 2023, and PaliGemma in May 2024.</p>
</dd>
<dt>Kosmos-1 (Microsoft, Feb. 2023)</dt>
<dd>
<p>A vision-language model<a data-type="indexterm" data-primary="Kosmos" id="id3972"/> with strong support for visual grounding. Kosmos-2 and Kosmos-2.5 came out in 2023.</p>
</dd>
<dt>PaLM-E (Google, Mar. 2023)</dt>
<dd>
<p>PaLM-E extends Google’s PaLM<a data-type="indexterm" data-primary="PaLM" id="id3973"/> series with visual inputs and embodied sensor data. A decoder-only LLM generates text commands like “grab the hammer”, which are interpreted and executed by a robot via a downstream system.</p>
</dd>
<dt>LLaVA (H. Liu et al., Apr. 2023)</dt>
<dd>
<p>Among the best open source vision-language chat models<a data-type="indexterm" data-primary="LLaVA" id="id3974"/>.</p>
</dd>
<dt>ImageBind (Meta, May 2023)</dt>
<dd>
<p>A CLIP-style model<a data-type="indexterm" data-primary="ImageBind" id="id3975"/> extended to six modalities (image, text, audio, IMU,⁠<sup><a data-type="noteref" id="id3976-marker" href="ch16.html#id3976">39</a></sup> depth, and thermal).</p>
</dd>
<dt>RT-2 (DeepMind, Jul. 2023)</dt>
<dd>
<p>A vision-language model<a data-type="indexterm" data-primary="RT-2" id="id3977"/> capable of robotic control as well, trained on a large-scale instruction-following dataset.</p>
</dd>
<dt>SeamlessM4T (Meta, Aug. 2023)</dt>
<dd>
<p>A single model<a data-type="indexterm" data-primary="SeamlessM4T" id="id3978"/> that can perform speech-to-text, speech-to-speech, text-to-speech, and text-to-text translation across close to 100 languages.</p>
</dd>
<dt>Qwen-VL (Alibaba, Sep. 2023)</dt>
<dd>
<p>Open vision-language family (7B to 72B)<a data-type="indexterm" data-primary="Qwen" id="id3979"/> that became one of the strongest open multimodal baselines. Led to Qwen2-VL (Aug. 2024) and Qwen3-Omni (Sep. 2025), which expanded to video and audio and reached trillion-parameter scale.</p>
</dd>
<dt>Fuyu (Adept AI, Oct. 2023)</dt>
<dd>
<p>Processes interleaved image<a data-type="indexterm" data-primary="Fuyu" id="id3980"/> and text in real time with a unified transformer.</p>
</dd>
<dt>EMO (Alibaba, Feb. 2024)</dt>
<dd>
<p>Takes an image of a person<a data-type="indexterm" data-primary="EMO" id="id3981"/>, plus an audio recording of someone speaking or singing, and the model generates a video of that person, matching the audio. EMO-2 was released in January 2025.</p>
</dd>
<dt>GLaMM (H. Rasheed et al., Jun. 2024)</dt>
<dd>
<p>A visual dialogue model<a data-type="indexterm" data-primary="GLaMM" id="id3982"/> which generates text responses mixed with object segmentation masks.</p>
</dd>
<dt>LaViDa (UCLA, Panasonic, Adobe, Salesforce, May 2025)</dt>
<dd>
<p>A family of open,<a data-type="indexterm" data-primary="LaViDa" id="id3983"/> diffusion-based vision-language models.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>I’ve created homl.info short links for all the models discussed in this chapter; just use the lowercase name without hyphens, for example, <a href="https://homl.info/qwen2vl" class="bare"><em class="hyperlink">https://homl.info/qwen2vl</em></a>.</p>
</div>

<p>There are also several commercial multimodal models whose detailed architectures were not disclosed, such as GPT-4.1<a data-type="indexterm" data-primary="GPT-4.1" id="id3984"/><a data-type="indexterm" data-primary="Sora" id="id3985"/> and Sora by OpenAI, Gemini 2.5 Pro by Google, Veo-3 by DeepMind, and Claude 4 Opus by Anthropic<a data-type="indexterm" data-primary="Gemini 2.5" id="id3986"/><a data-type="indexterm" data-primary="Claude 4" id="id3987"/>. To access these models, you first need to create an account and get a subscription (or use the free tier), then you can either use the provided apps (e.g., Google AI Studio, <a href="https://aistudio.google.com" class="bare"><em class="hyperlink">https://aistudio.google.com</em></a>), or query the model via an API. For example, following is a short code example showing how to query Gemini 2.5 Pro via the API. You first need to get an API key in Google AI Studio, then you can use any secret management method you prefer to store it and load it in your code (e.g., if you are using Colab, I recommend you use Colab’s secret manager, as we saw in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>).</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">google</code> <code class="kn">import</code> <code class="n">genai</code>

<code class="n">gemini_api_key</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># load from Colab secrets, or from a file, or hardcode</code>
<code class="n">gemini_client</code> <code class="o">=</code> <code class="n">genai</code><code class="o">.</code><code class="n">Client</code><code class="p">(</code><code class="n">api_key</code><code class="o">=</code><code class="n">gemini_api_key</code><code class="p">)</code>
<code class="n">cats_photo</code> <code class="o">=</code> <code class="n">gemini_client</code><code class="o">.</code><code class="n">files</code><code class="o">.</code><code class="n">upload</code><code class="p">(</code><code class="n">file</code><code class="o">=</code><code class="s2">"my_cats_photo.jpg"</code><code class="p">)</code>
<code class="n">question</code> <code class="o">=</code> <code class="s2">"What animal and how many? Format: [animal, number]"</code>
<code class="n">response</code> <code class="o">=</code> <code class="n">gemini_client</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">generate_content</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="s2">"gemini-2.5-flash"</code><code class="p">,</code>  <code class="c1"># or "gemini-2.5-pro"</code>
    <code class="n">contents</code><code class="o">=</code><code class="p">[</code><code class="n">cats_photo</code><code class="p">,</code> <code class="n">question</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>  <code class="c1"># prints: "[cat, 2]"</code></pre>

<p>This code uses the <code translate="no">google-genai</code> library<a data-type="indexterm" data-primary="GenAI library" id="id3988"/><a data-type="indexterm" data-primary="Google GenAI library" id="id3989"/>, which is already installed on Colab. It also assumes that a file named <em>my_cats_photo.jpg</em> is present in the same directory as the notebook<a data-type="indexterm" data-startref="xi_multimodaltransformersothermodels1669444_1" id="id3990"/>.</p>

<p>This wraps up this chapter; I hope you enjoyed it. Transformers can now see, hear, touch, and more! In the next chapter, we will explore some fairly advanced techniques designed to speed up and scale transformers<a data-type="indexterm" data-startref="xi_transformersmultimodaltransformers1634022_1" id="id3991"/><a data-type="indexterm" data-startref="xi_multimodaltransformers1634022_1" id="id3992"/>. As Daft Punk put it: harder, better, faster, stronger.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id332">
<h1>Exercises</h1>
<ol>
<li>
<p>Can you describe the original ViT’s architecture? Why does it matter?</p>
</li>
<li>
<p>What tasks are regular ViTs (meaning nonhierarchical) best used for? What are their limitations?</p>
</li>
<li>
<p>What is the main innovation in DeiT? Is this idea generalizable to other architectures?</p>
</li>
<li>
<p>What are some examples of hierarchical ViTs? What kind of tasks are they good for?</p>
</li>
<li>
<p>How do PVTs and Swin Transformers reduce the computational cost of processing high-resolution images?</p>
</li>
<li>
<p>How does DINO work? What changed in DINOv2? When would you want to use DINOv2?</p>
</li>
<li>
<p>What is the objective of the JEPA architecture? How does it work?</p>
</li>
<li>
<p>What is a multimodal model? Can you give five examples of multimodal tasks?</p>
</li>
<li>
<p>Explain what the fusion and alignment problems are in multimodal learning. Why are transformers well suited to tackle them?</p>
</li>
<li>
<p>Can you write a one-line summary of the main ideas in VideoBERT, ViLBERT, CLIP, DALL·E, Perceiver IO, Flamingo, and BLIP-2?</p>
</li>
<li>
<p>If you are using a Perceiver IO model and you double the length of the inputs and the outputs, approximately how much more computation will be required?</p>
</li>
<li>
<p>Try fine-tuning a pretrained ViT model on the <a href="https://homl.info/food101">Food 101 dataset</a> (<code translate="no">torchvision.datasets.Food101</code>). What accuracy can you reach? How about using a CLIP model, zero-shot?</p>
</li>
<li>
<p>Create a simple search engine for your own photos: first, write a function that uses a CLIP model to embed all of your photos and saves the resulting vectors. Next, write a function that takes a search query (text or image), embeds it using CLIP, then finds the most similar photo embeddings and displays the corresponding photos<a data-type="indexterm" data-startref="xi_transformers16324_1" id="id3993"/>. You can manually implement the similarity search algorithm, or a dedicated library such as the <a href="https://github.com/facebookresearch/faiss">FAISS library</a> or even a full-blown vector database.</p>
</li>
<li>
<p>Use BLIP-2 to automatically caption all of your photos.</p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id3722"><sup><a href="ch16.html#id3722-marker">1</a></sup> Kelvin Xu et al., “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention”, <em>Proceedings of the 32nd International Conference on Machine Learning</em> (2015): 2048–2057.</p><p data-type="footnote" id="id3724"><sup><a href="ch16.html#id3724-marker">2</a></sup> This is a part of Figure 3 from the paper. It is reproduced with the kind authorization of the authors.</p><p data-type="footnote" id="id3728"><sup><a href="ch16.html#id3728-marker">3</a></sup> Marco Tulio Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier”, <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (2016): 1135–1144.</p><p data-type="footnote" id="id3740"><sup><a href="ch16.html#id3740-marker">4</a></sup> Nicolas Carion et al., “End-to-End Object Detection with Transformers”, arXiv preprint arXiv:2005.12872 (2020).</p><p data-type="footnote" id="id3741"><sup><a href="ch16.html#id3741-marker">5</a></sup> Alexey Dosovitskiy et al., “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale”, arXiv preprint arXiv:2010.11929 (2020).</p><p data-type="footnote" id="id3760"><sup><a href="ch16.html#id3760-marker">6</a></sup> Hugo Touvron et al., “Training Data-Efficient Image Transformers &amp; Distillation Through Attention”, arXiv preprint arXiv:2012.12877 (2020).</p><p data-type="footnote" id="id3765"><sup><a href="ch16.html#id3765-marker">7</a></sup> Wenhai Wang et al., “Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions”, arXiv preprint arXiv:2102.12122 (2021).</p><p data-type="footnote" id="id3776"><sup><a href="ch16.html#id3776-marker">8</a></sup> Ze Liu et al., “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows”, arXiv preprint arXiv:2103.14030 (2021).</p><p data-type="footnote" id="id3781"><sup><a href="ch16.html#id3781-marker">9</a></sup> Ze Liu et al., “Swin Transformer V2: Scaling Up Capacity and Resolution”, arXiv preprint arXiv:2111.09883 (2021).</p><p data-type="footnote" id="id3788"><sup><a href="ch16.html#id3788-marker">10</a></sup> Mathilde Caron et al., “Emerging Properties in Self-Supervised Vision Transformers”, arXiv preprint arXiv:2104.14294 (2021).</p><p data-type="footnote" id="id3797"><sup><a href="ch16.html#id3797-marker">11</a></sup> This is the righthand part of Figure 3 of the DINO paper, reproduced with the kind authorization of the authors.</p><p data-type="footnote" id="id3799"><sup><a href="ch16.html#id3799-marker">12</a></sup> Yangtao Wang et al., “TokenCut: Segmenting Objects in Images and Videos with Self-supervised Transformer and Normalized Cut”, arXiv preprint arXiv:2209.00383 (2022).</p><p data-type="footnote" id="id3801"><sup><a href="ch16.html#id3801-marker">13</a></sup> “DINOv2: Learning Robust Visual Features without Supervision”, arXiv preprint arXiv:2304.07193 (2023).</p><p data-type="footnote" id="id3806"><sup><a href="ch16.html#id3806-marker">14</a></sup> Xiaohua Zhai et al., “Scaling Vision Transformers”, arXiv preprint arXiv:2106.04560 (2021).</p><p data-type="footnote" id="id3808"><sup><a href="ch16.html#id3808-marker">15</a></sup> Hangbo Bao et al., “BEiT: BERT Pre-Training of Image Transformers”, arXiv preprint arXiv:2106.08254 (2021).</p><p data-type="footnote" id="id3816"><sup><a href="ch16.html#id3816-marker">16</a></sup> Kaiming He et al., “Masked Autoencoders Are Scalable Vision Learners”, arXiv preprint arXiv:2111.06377 (2021).</p><p data-type="footnote" id="id3820"><sup><a href="ch16.html#id3820-marker">17</a></sup> Mitchell Wortsman et al., “Model Soups: Averaging Weights of Multiple Fine-tuned Models Improves Accuracy Without Increasing Inference Time”, arXiv preprint arXiv:2203.05482 (2022).</p><p data-type="footnote" id="id3821"><sup><a href="ch16.html#id3821-marker">18</a></sup> Yuxin Fang et al., “EVA: Exploring the Limits of Masked Visual Representation Learning at Scale”, arXiv preprint arXiv:2211.07636 (2022).</p><p data-type="footnote" id="id3823"><sup><a href="ch16.html#id3823-marker">19</a></sup> “Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture”, arXiv preprint arXiv:2301.08243 (2023).</p><p data-type="footnote" id="id3826"><sup><a href="ch16.html#id3826-marker">20</a></sup> Yann LeCun, “A Path Towards Autonomous Machine Intelligence” (2022).</p><p data-type="footnote" id="id3840"><sup><a href="ch16.html#id3840-marker">21</a></sup> Chen Sun et al., “VideoBERT: A Joint Model for Video and Language Representation Learning”, arXiv preprint arXiv:1904.01766 (2019).</p><p data-type="footnote" id="id3846"><sup><a href="ch16.html#id3846-marker">22</a></sup> L. Zhou, Y. Zhou et al., “End-to-End Dense Video Captioning with Masked Transformer”, <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2018).</p><p data-type="footnote" id="id3852"><sup><a href="ch16.html#id3852-marker">23</a></sup> Jiasen Lu et al., “ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks”, <em>Advances in Neural Information Processing Systems</em> 32 (2019).</p><p data-type="footnote" id="id3853"><sup><a href="ch16.html#id3853-marker">24</a></sup> Jize Cao et al. later provided some empirical evidence supporting this claim in their paper <a href="https://homl.info/probing">“Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models”</a>: in particular, they found that more attention heads focus on the text modality than on the visual modality.</p><p data-type="footnote" id="id3870"><sup><a href="ch16.html#id3870-marker">25</a></sup> Alec Radford et al., “Learning Transferable Visual Models From Natural Language Supervision”, arXiv preprint arXiv:2103.00020 (2021).</p><p data-type="footnote" id="id3873"><sup><a href="ch16.html#id3873-marker">26</a></sup> The training code and data were not released by OpenAI, but Gabriel Ilharco et al. created <a href="https://homl.info/openclip">OpenCLIP</a> which is a flexible open source replication of CLIP with the full training code and data.</p><p data-type="footnote" id="id3874"><sup><a href="ch16.html#id3874-marker">27</a></sup> This contrastive loss was first introduced as the <em>multiclass n-pair loss</em> in a <a href="https://homl.info/npairloss">2016 paper by Kihyuk Sohn</a>, then used for contrastive representation learning and renamed to <em>InfoNCE</em> (information noise-contrastive estimation) in a <a href="https://homl.info/infonce">2018 paper by Aaron van den Oord et al</a>.</p><p data-type="footnote" id="id3883"><sup><a href="ch16.html#id3883-marker">28</a></sup> Aditya Ramesh et al., “Zero-Shot Text-to-Image Generation”, arXiv preprint arXiv:2102.12092 (2021).</p><p data-type="footnote" id="id3886"><sup><a href="ch16.html#id3886-marker">29</a></sup> Aditya Ramesh et al., “Hierarchical Text-Conditional Image Generation with CLIP Latents”, arXiv preprint arXiv:2204.06125 (2022).</p><p data-type="footnote" id="id3898"><sup><a href="ch16.html#id3898-marker">30</a></sup> Andrew Jaegle et al., “Perceiver: General Perception with Iterative Attention”, arXiv preprint arXiv:2103.03206 (2021).</p><p data-type="footnote" id="id3901"><sup><a href="ch16.html#id3901-marker">31</a></sup> If Δ is the spacing between samples, then the Nyquist–Shannon sampling theorem tells us that the maximum frequency we can measure is <em>f</em> = 1 / 2Δ. This is why <em>f</em> stops at <em>μ</em> / 2 rather than <em>μ</em>: sampling at a higher resolution would not add any information, and it might introduce aliasing artifacts.</p><p data-type="footnote" id="id3910"><sup><a href="ch16.html#id3910-marker">32</a></sup> <a href="https://homl.info/audioset">AudioSet</a> contains over 2 million video segments of 10s each, sorted into over 500 classes.</p><p data-type="footnote" id="id3911"><sup><a href="ch16.html#id3911-marker">33</a></sup> <a href="https://homl.info/modelnet">ModelNet40</a> is a synthetic dataset of 3D point clouds of various shapes, such as airplanes or cars. A common source of point clouds in real life is LiDAR sensors.</p><p data-type="footnote" id="id3915"><sup><a href="ch16.html#id3915-marker">34</a></sup> Andrew Jaegle et al., “Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs”, arXiv preprint arXiv:2107.14795 (2021).</p><p data-type="footnote" id="id3931"><sup><a href="ch16.html#id3931-marker">35</a></sup> In the French comic series <em>Astérix</em>, Obélix is a big and friendly Gaul, and Idéfix is his clever little dog.</p><p data-type="footnote" id="id3938"><sup><a href="ch16.html#id3938-marker">36</a></sup> Junnan Li et al., “BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation”, arXiv preprint arXiv:2201.12086 (2022).</p><p data-type="footnote" id="id3951"><sup><a href="ch16.html#id3951-marker">37</a></sup> Junnan Li et al., “BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models”, arXiv preprint arXiv:2301.12597 (2023).</p><p data-type="footnote" id="id3959"><sup><a href="ch16.html#id3959-marker">38</a></sup> The idea of training a single model capable of both encoding and generating text was introduced in 2019 by Microsoft researchers Li Dong et al. with their <a href="https://homl.info/unilm">UniLM model</a>.</p><p data-type="footnote" id="id3976"><sup><a href="ch16.html#id3976-marker">39</a></sup> Most modern smartphones contain an inertial measurement unit (IMU) sensor: it measures acceleration, angular velocity, and often the magnetic field strength.</p></div></div></section></div></div></body></html>