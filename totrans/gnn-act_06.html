<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">5</span></span> <span class="chapter-title-text">Graph autoencoders</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Distinguishing between discriminative and generative models </li>
<li class="readable-text" id="p3">Applying autoencoders and variational autoencoders to graphs</li>
<li class="readable-text" id="p4">Building graph autoencoders with PyTorch Geometric </li>
<li class="readable-text" id="p5">Over-squashing and graph neural networks</li>
<li class="readable-text" id="p6">Link prediction and graph generation </li>
</ul>
</div>
<div class="readable-text" id="p7">
<p>So far, we’ve covered how classical deep learning architectures can be extended to work on graph-structured data. In chapter 3, we considered convolutional graph neural networks (GNNs), which apply the convolutional operator to identify patterns within the data. In chapter 4, we explored the attention mechanism and how this can be used to improve performance for graph-learning tasks such as node classification.</p>
</div>
<div class="readable-text intended-text" id="p8">
<p>Both convolutional GNNs and attention GNNs are examples of <em>discriminative </em><em>models</em>, as they learn to discriminate between different instances of data, such as whether a photo is of a cat or a dog. In this chapter, we introduce the topic of <em>generative</em><em> models</em> and explore them through two of the most common architectures, autoencoders and variational autoencoders (VAEs). Generative models aim to learn the entire dataspace rather than separating boundaries <em>within </em>the dataspace, as do discriminative models. For example, a generative model learns how to generate images of cats and dogs (learning to reproduce aspects of a cat or dog, rather than learning just the features that separates two or more classes, such as the pointed ears of a cat or the long ears of a spaniel). </p>
</div>
<div class="readable-text intended-text" id="p9">
<p>As we’ll discover, discriminative models learn to separate boundaries in dataspace, whereas generative models learn to model the dataspace itself. By approximating the dataspace, we can sample from a generative model to create new examples of our training data. In the preceding example, we can use our generative model to make new images of a cat or dog, or even some hybrid version that has features of both. This is a very powerful tool and important knowledge for both beginner and established data scientists. In recent years, deep generative models, generative models that use artificial neural networks, have shown amazing ability in many language and vision tasks. For example, the family of DALL-E models are able to generate new images from text prompts while models such as OpenAI’s GPT models have dramatically changed the capabilities of chatbots.</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>In this chapter, you’ll learn how to extend generative architectures to act on graph-structured data, leading to graph autoencoders (GAEs) and variational graph autoencoders (VGAEs). These models are distinct from previous chapters, which focused on discriminative models. As we’ll see, generative models model the entire dataspace and can be combined with discriminative models for downstream machine learning tasks.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>To demonstrate the power of generative approaches to learning tasks, we return to the Amazon Product Co-Purchaser Network introduced in chapter 3. However, in chapter 3, you learned how to predict what category an item might belong to given its position in the network. In this chapter, we’ll show how to predict where an item should be placed in the network, given its description. This is known as <em>edge </em>(or link)<em> prediction</em> and comes up frequently, for example, when designing recommendation systems. We’ll put our understanding of GAEs to work here to perform edge prediction, building a model that can predict when nodes in a graph are connected. We’ll also discuss the problems of over-squashing, a specific consideration for GNNs, and how we can apply a GNN to generate potential chemical graphs.</p>
</div>
<div class="readable-text intended-text" id="p12">
<p>By the end of this chapter, you should know the basics of when and where to use generative models of graphs (rather than discriminative ones) and how to implement them when we need to.</p>
</div>
<div class="readable-text print-book-callout" id="p13">
<p><span class="print-book-callout-head">NOTE</span>  Code from this chapter can be found in notebook form at the GitHub repository (<a href="https://mng.bz/4aGQ">https://mng.bz/4aGQ</a>). Colab links and data from this chapter can be accessed in the same location.</p>
</div>
<div class="readable-text" id="p14">
<h2 class="readable-text-h2"><span class="num-string">5.1</span> Generative models: Learning how to generate</h2>
</div>
<div class="readable-text" id="p15">
<p>A classic example of deep learning is, given a set of labeled images, how to train models to <em>learn</em> what label to give to new and unseen images. If we consider the example of a set of images of boats and airplanes, we want our model to distinguish between these different images. If we then pass the model a new image, we want our model to correctly identify this as, for example, a boat. Discriminative models learn to discriminate between classes based on their specific target labels. Both convolutional architectures (discussed in chapter 3) and attention-based architectures (covered in chapter 4) are typically used to create discriminative models. However, as we’ll see, they can also be incorporated into generative models. To understand this, we first have to understand the difference between discriminative and generative modeling approaches. </p>
</div>
<div class="readable-text" id="p16">
<h3 class="readable-text-h3"><span class="num-string">5.1.1</span> Generative and discriminative models</h3>
</div>
<div class="readable-text" id="p17">
<p>As described in previous chapters, the original dataset that we use to train a model is referred to as our <em>training data,</em> and the labels that we seek to predict are our <em>training targets</em>. The unseen data is our <em>test data</em>, and we want to learn the <em>target labels </em>(from training) to classify the test data. Another way to describe this is using conditional probability. We want our models to return the probability of some target, Y, given an instance of data, X. We can write this as P(Y|X), where the vertical bar means that Y is “conditioned” on X. </p>
</div>
<div class="readable-text intended-text" id="p18">
<p>As we’ve said, discriminative models learn to discriminate <em>between</em> classes. This is equivalent to learning the separating boundaries of the data in the dataspace. In contrast, generative models learn to model the dataspace itself. They capture the entire distribution of data in the dataspace, and, when presented with a new example, they tell us how likely the new example is. Using the language of probability, we say that they model the <em>joint probability </em>between data and targets, P(X,Y). A typical example of a generative model might be a model that is used to predict the next word in a sentence (e.g., the autocomplete feature in many modern mobile phones). The generative model assigns a probability to each possible next word and returns those words that have the highest probability. Discriminative models can tell you how likely a word has some specific sentiment, while a generative model will suggest a word to use. </p>
</div>
<div class="readable-text intended-text" id="p19">
<p>Returning to our image example, a generative model approximates the overall distribution of images. This can be seen in figure 5.1, where the generative model has learned where the points are positioned in the dataspace (rather than how they are separated). This means that generative models must learn more complicated correlations in the data than their discriminative counterparts. For example, a generative model learns that “airplanes have wings” and “boats appear near water.” On the other hand, discriminative models just have to learn the difference between “boat” and “not boat.” They can do this by looking for telltale signs such as a mast, keel, or boom in the image. They can then largely ignore the rest of the image. As a result, generative models can be more computationally expensive to train and can require larger network architectures. (In section 5.5, we’ll describe over-squashing, which is a particular problem for large GNNs.)<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p20">
<img alt="figure" height="518" src="../Images/5-1.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.1</span> Comparison of generative and discriminative tasks. On the left, the discriminative model learns to separate different images of boats and airplanes. On the right, the generative model attempts to learn the entire dataspace, which allows for new synthetic examples to be created such as a boat in the sky or an airplane on water. </h5>
</div>
<div class="readable-text" id="p21">
<h3 class="readable-text-h3"><span class="num-string">5.1.2</span> Synthetic data</h3>
</div>
<div class="readable-text" id="p22">
<p>Given that discriminative models are computationally cheaper to train and more robust to outliers than generative models, you might wonder why we want to use a generative model at all. Generative models, however, are efficient tools when labeling data is relatively expensive but generating datasets is easy to do. For example, generative models are increasingly being used in drug discovery where they generate new candidate drugs that might match certain properties, such as the ability to reduce the effects of some disease. In a sense, generative models attempt to learn how to create synthetic data, which allows us to create new data instances. For example, none of the people shown in figure 5.2 exist and were instead created by sampling from the dataspace, approximated using a generative model.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p23">
<img alt="figure" height="553" src="../Images/5-2.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.2</span> Figure showing synthetic faces (Source: [1])</h5>
</div>
<div class="readable-text intended-text" id="p24">
<p>Synthetic examples created by generative models can be used to augment a dataset, which is expensive to collect. Rather than taking lots of pictures of faces under every condition, we can use generative models to create new data examples (e.g., a person wearing a hat, glasses, and a mask) to increase our dataset to contain tricky edge cases. These synthetic examples can then be used to further improve our other models (e.g., one that identifies when someone is wearing a mask). However, when introducing synthetic data, we must also be careful about introducing other biases or noise into our dataset. </p>
</div>
<div class="readable-text intended-text" id="p25">
<p>In addition, discriminative models are often used downstream of generative models. This is because generative models are typically trained in a “self-supervised” way, without relying on data labels. They learn to compress (or <em>encode) </em>complex high-dimensional data to lower dimensions. These low-dimensional representations can be used to better tease out underlying patterns within our data. This is known as<em> dimension reduction</em> and can be helpful in clustering data or in classification tasks. Later, we’ll see how generative models can separate graphs into different classes without ever seeing their labels. In cases where annotating each data point is expensive, generative models can be huge cost savers. Let’s get on to meeting our first generative GNN model. </p>
</div>
<div class="readable-text" id="p26">
<h2 class="readable-text-h2"><span class="num-string">5.2</span> Graph autoencoders for link prediction</h2>
</div>
<div class="readable-text" id="p27">
<p>One of the fundamental and popular models for deep generative models is the autoencoder. The reason the autoencoder framework is so widely used is because it’s incredibly adaptive. Just as the attention mechanisms in chapter 3 can be used to improve on many different models, autoencoders can be combined with many different models, including different types of GNNs. Once the autoencoder structure is understood, the encoder and decoder can be replaced with any type of neural network, including different GNNs such as the graph convolutional network (GCN) and GraphSAGE architectures from chapter 2. </p>
</div>
<div class="readable-text intended-text" id="p28">
<p>However, we need to take care when applying autoencoders to graph-based data. When reconstructing our data, we also have to reconstruct our adjacency matrix. <span class="aframe-location"/>In this section, we’ll look at implementing a GAE using the Amazon Products dataset from chapter 3 [2]. We’ll build a GAE for the task of link prediction, which is a common problem when working with graphs. This allows us to reconstruct the adjacency matrix and is especially useful when we’re dealing with a dataset that has missing data. We’ll follow this process: </p>
</div>
<ol>
<li class="readable-text buletless-item" id="p29"> Define the model: 
    <ol style="list-style: lower-alpha">
<li> Create both an encoder and decoder. </li>
<li> Use the encoder to create a latent space to sample from. </li>
</ol></li>
<li class="readable-text" id="p30"> Define the training and testing loop by including a loss suitable for constructing a generative model. </li>
<li class="readable-text" id="p31"> Prepare the data as a graph, with edge lists and node features. </li>
<li class="readable-text" id="p32"> Train the model, passing the edge data to compute the loss. </li>
<li class="readable-text" id="p33"> Test the model using the test dataset. </li>
</ol>
<div class="readable-text" id="p34">
<h3 class="readable-text-h3"><span class="num-string">5.2.1</span> Review of the Amazon Products dataset from chapter 3</h3>
</div>
<div class="readable-text" id="p35">
<p>In chapter 3, we learned about the Amazon Products dataset with co-purchaser information. This dataset contains information about a range of different items that were purchased, details about who purchased them and how, and categories for the items, which were the labels in chapter 3. We’ve already learned about how we can turn this tabular dataset into a graph structure and, by doing so, make our learning algorithms more efficient and more powerful. We’ve also already used some dimension reduction without realizing it. Principal component analysis (PCA) was applied to the Amazon Products dataset to create the features. Each product description was converted into numerical values using the bag-of-words algorithm, and PCA is then applied to reduce the (now numerical) description to 100 features. </p>
</div>
<div class="readable-text intended-text" id="p36">
<p>In this chapter, we’re going to revisit the Amazon Products dataset but with a different aim in mind. We’re going to use our dataset to learn link predictions. Essentially, this means learning the <em>relations between </em>nodes in our graph. This has many use cases, such as predicting what movies or TV shows users would like to watch next, suggesting new connections on social media platforms, or even predicting customers who are more likely to default on credit. Here, we’re going to use it to predict which products in the Amazon Electronics dataset should be connected together, as we show in figure 5.3. For further details about link prediction, check out section 5.5 at the end of this chapter.</p>
</div>
<div class="browsable-container figure-container" id="p38">
<img alt="figure" height="753" src="../Images/5-3.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.3</span> The Amazon Electronics dataset, where different products such as cameras and lenses are connected based on whether they have been bought together in the past</h5>
</div>
<div class="readable-text intended-text" id="p37">
<p>As with all data science projects, it’s worth first taking a look at the dataset and understanding what the problem is. We start by loading the data, the same way as we did in chapter 3, which we show in listing 5.1. The data is preprocessed and labeled so it can be loaded using NumPy. Further details on the dataset can be found in [2]. <span class="aframe-location"/></p>
</div>
<div class="browsable-container listing-container" id="p39">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.1</span> Loading the data</h5>
<div class="code-area-container">
<pre class="code-area">import numpy as np

filename = 'data/new_AMZN_electronics.npz'

data = np.load(filename)

loader = dict(data)
print(loader)</pre>
</div>
</div>
<div class="readable-text" id="p40">
<p>The preceding output prints the following: </p>
</div>
<div class="browsable-container listing-container" id="p41">
<div class="code-area-container">
<pre class="code-area">{'adj_data': array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]],\
 dtype=float32), 'attr_data': \
array([[0., 0., 0., ..., 0., 1., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 1., 0., ..., 0., 0., 0.],
       [1., 1., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 1.]],\
 dtype=float32), 'labels': \
array([6, 4, 3, ..., 1, 2, 3]),\
 'class_names': array(['Film Photography',\
 'Digital Cameras', 'Binoculars &amp; Scopes',
       'Lenses', 'Tripods &amp; Monopods', 'Video Surveillance',
       'Lighting &amp; Studio', 'Flashes'], dtype='&lt;U19')}</pre>
</div>
</div>
<div class="readable-text" id="p42">
<p>With the data loaded, we can next look at some basic statistics and details of the data. We’re interested in edge or link prediction, so it’s worth understanding how many different edges exist. We might also want to know how many components there are and the average degree to understand how connected our graph is. We show the code to calculate this in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p43">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.2</span> Exploratory data analysis</h5>
<div class="code-area-container">
<pre class="code-area">adj_matrix = torch.tensor(loader['adj_data'])
if not adj_matrix.is_sparse:
    adj_matrix = adj_matrix.to_sparse()

feature_matrix = torch.tensor(loader['attr_data'])
labels = loader['labels']

class_names = loader.get('class_names')
metadata = loader.get('metadata')

num_nodes = adj_matrix.size(0)
num_edges = adj_matrix.coalesce().values().size(0)  <span class="aframe-location"/> #1
density = num_edges / (num_nodes \
* (num_nodes - 1) / 2) if num_nodes \
&gt; 1 else 0 <span class="aframe-location"/> #2</pre>
<div class="code-annotations-overlay-container">
     #1 This is only possible because the adjacency matrix is undirected.
     <br/>#2 Ratio of actual edges to possible edges
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p44">
<p>We also plot the distribution of the degree to see how connections vary, as shown in the following listing and in figure 5.4. </p>
</div>
<div class="browsable-container listing-container" id="p45">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.3</span> Plotting the graph</h5>
<div class="code-area-container">
<pre class="code-area">degrees = adj_matrix.coalesce().indices().numpy()[0]   <span class="aframe-location"/> #1
degree_count = np.bincount(degrees, minlength=num_nodes)

plt.figure(figsize=(10, 5))
plt.hist(degree_count, bins=25, alpha=0.75, color='blue')
plt.xlabel('Degree')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()</pre>
<div class="code-annotations-overlay-container">
     #1 Gets row indices for each nonzero value
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p46">
<p>We find that there are 7,650 nodes, more than 143,000 edges, and an overall density of 0.0049. Therefore, our graph is medium size (~10,000 nodes) but very sparse (density much less than 0.05). We see that the majority of nodes have a low degree (less than 10), but that there is a second peak of edges with a higher degree (around 30) and a longer tail. In total, we see very few nodes with a high degree, which we would expect given the low density of the graph. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p47">
<img alt="figure" height="567" src="../Images/5-4.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.4</span> Degree distribution for the Amazon Electronics co-purchaser graph</h5>
</div>
<div class="readable-text" id="p48">
<h3 class="readable-text-h3"><span class="num-string">5.2.2</span> Defining a graph autoencoder</h3>
</div>
<div class="readable-text" id="p49">
<p>Next, we’ll use a generative model, the autoencoder, to estimate and predict links in the Amazon Electronics dataset. In doing so, we’re in good company, as link prediction was the problem that GAEs were applied to when first published by Kipf and Welling in 2012 [3]. In their seminal paper, they introduced the GAE and its variational extension, which we’ll be discussing shortly, and then applied these models to three classic benchmarks in graph deep learning, the Cora dataset, CiteSeer, and PubMed. Today, most graph deep learning libraries make it very easy to create and begin training GAEs, as these have become one of the most popular graph-based deep generative models. We’ll look at the steps required to build one in more detail in this section. </p>
</div>
<div class="readable-text intended-text" id="p50">
<p>The GAE model is similar to a typical autoencoder. The only difference is that each individual layer of our network is a GNN, such as a GCN or GraphSAGE network. In figure 5.5, we show a schematic for a GAE’s architecture. Broadly, we’ll be taking our edge data and compressing it into a low-dimensional representation using an encoder network.</p>
</div>
<div class="browsable-container figure-container" id="p52">
<img alt="figure" height="579" src="../Images/5-5.png" width="1007"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.5</span> Schematic for the GAE showing the key elements of the model, such as the encoder, latent space, and decoder</h5>
</div>
<div class="readable-text intended-text" id="p51">
<p>The first thing we need to define for our GAE is the encoder, which will take our data and transform it into a latent representation. The code snippet for implementing the encoder is given in listing 5.4. We first import our libraries and then build a GNN where each layer is progressively smaller. <span class="aframe-location"/></p>
</div>
<div class="browsable-container listing-container" id="p53">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.4</span> Graph encoder</h5>
<div class="code-area-container">
<pre class="code-area">from torch_geometric.nn import GCNConv  <span class="aframe-location"/> #1

class GCNEncoder(torch.nn.Module):                        <span class="aframe-location"/> #2
    def __init__(self, input_size, layers, latent_dim):    #2
        super().__init__() #2
        self.conv0 = GCNConv(input_size, layers[0])   <span class="aframe-location"/> #3
        self.conv1 = GCNConv(layers[0], layers[1])     #3
        self.conv2 = GCNConv(layers[1], latent_dim)    #3

    def forward(self, x, edge_index):          <span class="aframe-location"/> #4
        x = self.conv0(x, edge_index).relu()    #4
        x = self.conv1(x, edge_index).relu()    #4
        return self.conv2(x, edge_index)        #4</pre>
<div class="code-annotations-overlay-container">
     #1 Loads GCNConv models from PyG
     <br/>#2 Defines the encoder layer and initializes it with a predefined size
     <br/>#3 Defines each of the encoder layer networks
     <br/>#4 Forward pass for the encoder with edge data
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p54">
<p>Note that we also have to make sure that our forward pass can return the edge data from our graph because we’ll be using our autoencoder to reconstruct the graph from the latent space. To put this another way, the autoencoder will be learning how to reconstruct the adjacency matrix from a low-dimensional representation of our feature space. This means it’s also learning to predict edges from new data. To do this, we need to make the autoencoder structure learn to reconstruct edges, specifically by changing the decoder. Here, we’ll use the inner product to predict edges from the latent space. This is shown in listing 5.5. (To understand why we use the inner product, see the technical details in section 5.5.) </p>
</div>
<div class="browsable-container listing-container" id="p55">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.5</span> Graph decoder</h5>
<div class="code-area-container">
<pre class="code-area">class InnerProductDecoder(torch.nn.Module):    <span class="aframe-location"/> #1
    def __init__(self):                        
         super().__init__()                    

def forward(self, z, edge_index):    <span class="aframe-location"/> #2
        value = (z[edge_index[0]] * \
z[edge_index[1]]).sum(dim=1)  <span class="aframe-location"/> #3
        return torch.sigmoid(value)</pre>
<div class="code-annotations-overlay-container">
     #1 Defines the decoder layer
     <br/>#2 States the shape and size of the decoder (which, again, is the reverse of the encoder)
     <br/>#3 Forward pass for the decoder
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p56">
<p>Now we’re ready to combine both encoder and decoder together in the GAE class, which contains both submodels (see listing 5.6). Note that we don’t initialize the decoder with any input or output sizes now as this is just applying the inner product to the output of our encoder with the edge data. </p>
</div>
<div class="browsable-container listing-container" id="p57">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.6</span> Graph autoencoder</h5>
<div class="code-area-container">
<pre class="code-area">   class GraphAutoEncoder(torch.nn.Module):
        def __init__(self, input_size, layers, latent_dims):
            super().__init__()
            self.encoder = GCNEncoder(input_size, \
   layers, latent_dims)    <span class="aframe-location"/> #1
            self.decoder = InnerProductDecoder()     <span class="aframe-location"/> #2

        def forward(self, x):
            z = self.encoder(x)
            return self.decoder(z)</pre>
<div class="code-annotations-overlay-container">
     #1 Defines the encoder for the GAE
     <br/>#2 Defines the decoder
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p58">
<p>In PyTorch Geometric (PyG), the GAE model can be made even easier by just importing the GAE class, which automatically builds both decoder and autoencoder once passed to the encoder. We’ll use this functionality when we build a VGAE later in the chapter. </p>
</div>
<div class="readable-text" id="p59">
<h3 class="readable-text-h3"><span class="num-string">5.2.3</span> Training a graph autoencoder to perform link prediction</h3>
</div>
<div class="readable-text" id="p60">
<p>Having built our GAE, we can proceed to use this to perform edge prediction for the sub models Amazon Products dataset. The overall framework will follow a typical deep learning problem format, where we first load the data, prepare the data, and split this data into train, test, and validation datasets; define our training parameters; and then train and test our model. These steps are shown in figure 5.6. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p61">
<img alt="figure" height="170" src="../Images/5-6.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.6</span> Overall steps for training our model for link prediction</h5>
</div>
<div class="readable-text intended-text" id="p62">
<p>We begin by loading the dataset and preparing it for our learning algorithms, which we’ve already done in listing 5.1. For us to use the PyG models for GAE and VGAE, we need to construct an edge index from the adjacency matrix, which is easily done using one of PyG’s utility functions, <code>to_edge_index</code>, as we describe in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p63">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.7</span> Construct Edge Index</h5>
<div class="code-area-container">
<pre class="code-area">from torch_geometrics.utils import to_edge_index  <span class="aframe-location"/> #1

edge_index, edge_attr = to_edge_index(adj_matrix)  <span class="aframe-location"/> #2
num_nodes = adj_matrix.size(0)</pre>
<div class="code-annotations-overlay-container">
     #1 Loads the to_edge_index from the PyG utility libraries 
     <br/>#2 Converts the adjacency matrix to an edge index and edge attribute vector
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p64">
<p>We then load the PyG libraries and convert our data into a PyG data object. We can also apply transformations to our dataset, where the features and adjacency matrix are loaded as in chapter 3. First, we normalize our features and then split our dataset into training, testing, and validation sets based on the edges or links of the graph, as shown in listing 5.8. This is a vital step when carrying out link prediction to ensure we correctly split our data. In the code, we’ve used 5% of the data for validation and 10% for test data, noting that our graph is undirected. Here, we don’t add any negative training samples. </p>
</div>
<div class="browsable-container listing-container" id="p65">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.8</span> Convert to a PyG object </h5>
<div class="code-area-container">
<pre class="code-area">data = Data(x=feature_matrix,        <span class="aframe-location"/> #1
            edge_index=edge_index,   
            edge_attr=edge_attr,     
            y=labels)                

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

transform = T.Compose([\
     T.NormalizeFeatures(),\                 <span class="aframe-location"/> #2
     T.ToDevice(device),                     
     T.RandomLinkSplit(num_val=0.05,\
     num_test=0.1, is_undirected=True,       
     add_negative_train_samples=False)])     
train_data, val_data, test_data = transform(data)</pre>
<div class="code-annotations-overlay-container">
     #1 Converts our data to a PyG data object
     <br/>#2 Transforms our data and splits the links into train, test, and validation sets
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p66">
<p>With everything in place, we can now apply GAE to the Amazon Products dataset. First, we define our model, as well as our optimizer and our loss. We apply the binary cross-entropy loss to the predicted values from the decoder and compare against our true edge index to see whether our model has reconstructed the adjacency matrix correctly, as shown in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p67">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.9</span> Define the model</h5>
<div class="code-area-container">
<pre class="code-area">input_size, latent_dims = feature_matrix.shape[1], 16  <span class="aframe-location"/> #1
layers = [512, 256]                                    
model = GraphAutoEncoder(input_size, layers, latent_dims)  <span class="aframe-location"/> #2
model = model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.BCEWithLogitsLoss()  <span class="aframe-location"/> #3</pre>
<div class="code-annotations-overlay-container">
     #1 Specifies the shape of our encoder
     <br/>#2 Defines a GAE with the correct shape
     <br/>#3 Our loss now is binary cross-entropy.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p68">
<p>It’s important to use a binary cross-entropy loss because we want to calculate the probabilities that each edge is a true edge, where true edges correspond to the ones that aren’t being hidden and don’t need to be predicted (i.e., the positive samples). The encoder learns to compress the edge data but doesn’t change the number of edges, whereas the decoder learns to predict edges. In a sense, we’re combining both the discriminative and generative steps here. Therefore, the binary cross-entropy gives us a probability where there is likely to be an edge between these nodes. It’s binary, as either an edge should exist (label 1) or shouldn’t (label 0). We can compare all of those edges that have a binary cross-entropy probability greater than 0.5 to the actual true edges in each epoch of our training loop, as shown in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p69">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.10</span> Training function</h5>
<div class="code-area-container">
<pre class="code-area">def train(model, criterion, optimizer):

    model.train() 

    optimizer.zero_grad() 
    z = model.encoder(train_data.x,\
    train_data.edge_index)  <span class="aframe-location"/> #1

    neg_edge_index = negative_sampling(\        <span class="aframe-location"/> #2
    edge_index=train_data.edge_index,\
    num_nodes=train_data.num_nodes,             
    num_neg_samples=train_data.\
    edge_label_index.size(1), method='sparse')  

    edge_label_index = torch.cat(                    <span class="aframe-location"/> #3
    [train_data.edge_label_index, neg_edge_index],    #3
    dim=-1,)                                          #3

    out = model.decoder(z, edge_label_index).view(-1)  <span class="aframe-location"/> #4

    edge_label = torch.cat([       <span class="aframe-location"/> #5
    train_data.edge_label,         
train_data.edge_label.new_zeros\
(neg_edge_index.size(1))           
    ], dim=0)                      
    loss = criterion(out, edge_label)  <span class="aframe-location"/> #6
    loss.backward()                     #6
    optimizer.step() #6

    return loss</pre>
<div class="code-annotations-overlay-container">
     #1 Encodes graph into latent representation
     <br/>#2 Performs a new round of negative sampling
     <br/>#3 Combines new negative samples with the edge label index
     <br/>#4 Generates edge predictions
     <br/>#5 Combines edge labels with 0s for negative samples
     <br/>#6 Computes loss and backpropagates
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p70">
<p>Here, we first encoded our graph into a latent representation. We then perform a round of negative sampling, with new samples drawn for each epoch. Negative sampling takes a random subset of nonexistent labels rather than existing positive ones during training to account for the class imbalance between real labels and nonexistent ones. Once we have these new negative samples, we concatenate them with our original edge labels index and pass these to our decoder to get a reconstructed graph. Finally, we concatenate our true edge labels with the 0 labels for our negative edges and compute the loss between our predicted edges and our true edges. Note that we’re not doing batch learning here; instead, we’re choosing to train on all data during each epoch. </p>
</div>
<div class="readable-text intended-text" id="p71">
<p>Our test function, shown in listing 5.11, is much simpler than our training function as it doesn’t have to perform any negative sampling. Instead, we just use the true and predicted edges and return a Receiver Operating Characteristic (ROC)/Area Under the Curve (AUC) score to measure the accuracy of our model. Recall that the ROC/AUC curves will range between 0 and 1, and a perfect model, whose predictions are 100% correct, will have an AUC of 1.</p>
</div>
<div class="browsable-container listing-container" id="p72">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.11</span> Test function</h5>
<div class="code-area-container">
<pre class="code-area">from sklearn.metrics import roc_auc_score

@torch.no_grad() 
def test(data):
    model.eval() 
    z = model.encode(data.x, data.edge_index)  <span class="aframe-location"/> #1
    out = model.decode(z, \
    data.edge_label_index).view(-1).sigmoid()  <span class="aframe-location"/> #2
    loss = roc_auc_score(data.edge_label.cpu().numpy(),  <span class="aframe-location"/> #3
                        out.cpu().numpy())                #3
    return loss</pre>
<div class="code-annotations-overlay-container">
     #1 Encodes the graph into a latent representation
     <br/>#2 Decodes the graph using the full edge label index
     <br/>#3 Calculates the overall ROC/AUC score
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p73">
<p>At each time step, we’ll calculate the overall success of a model using all our edge data from our validation data. After training is complete, we then use the test data to calculate the final test accuracy, as shown in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p74">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.12</span> Training loop</h5>
<div class="code-area-container">
<pre class="code-area">best_val_auc = final_test_auc = 0 
for epoch in range(1, 201): 
    loss = train(model, criterion, optimizer) <span class="aframe-location"/> #1
    val_auc = test(val_data)  <span class="aframe-location"/> #2
    if val_auc &gt; best_val_auc: 
        best_val_auc = val_auc 
test_auc = test(test_data)  <span class="aframe-location"/> #3</pre>
<div class="code-annotations-overlay-container">
     #1 Performs a training step
     <br/>#2 Tests our updated model on validation data
     <br/>#3 Tests our final model on test data
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p75">
<p>We find that after 200 epochs, we achieve an accuracy of more than 83%. Even better, when we then use our test set to see how well our model is able to predict edges, we get an accuracy of 86%. We can interpret our model performance as being able to suggest a meaningful item to the purchaser 86% of the time, assuming that all future data is the same as our current dataset. This is a great result and demonstrates how useful GNNs are for recommender systems. We can also use our model to better understand how the dataset is structured or apply additional classification and feature engineering tasks by exploring our newly constructed latent space. Next, we’re going to learn about one of the most common extensions to the graph autoencoder model—the VGAE.</p>
</div>
<div class="readable-text" id="p76">
<h2 class="readable-text-h2"><span class="num-string">5.3</span> Variational graph autoencoders</h2>
</div>
<div class="readable-text" id="p77">
<p>Autoencoders map data onto discrete points in the latent space. To sample outside of the training dataset and generate new synthetic data, we can interpolate between these discrete points. This is exactly the process that we described in figure 5.1, where we generated unseen combinations of data such as a flying boat. However, autoencoders are deterministic, where each input maps to a specific point in the latent space. This can lead to sharp discontinuities when sampling, which can affect performance for data generation resulting in synthetic data that doesn’t reproduce the original dataset as well. To improve our generative process, we need to ensure that our latent space is well-structured, or <em>regular.</em> In figure 5.7, for example, we show how to use the Kullback-Liebler divergence (KL divergence) to restructure the latent space to improve reconstruction. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p78">
<img alt="figure" height="324" src="../Images/5-7.png" width="507"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.7</span> Regular spaces are continuous and compact, but data regions may become less separated. Alternatively, high reconstruction loss typically means data is well separated, but the latent space might be less covered leading to worse generative samples. Here, KL Divergence refers to the Kullback-Liebler divergence. </h5>
</div>
<div class="readable-text" id="p79">
<p>The KL divergence is a measure of how one probability distribution differs from another. It calculates how much “extra information” is needed to encode values from one distribution (the original data distribution) into another (the latent space). On the left, the data groups (<em>x</em><sub><em>i</em></sub>) don’t overlap much, which means the KL divergence is higher. On the right, there is more overlap (similarity) between the different data groups, meaning the KL divergence is lower. When building a more regular latent space that has a high KL divergence, we can get very good reconstruction but poor interpolation, while we get the opposite for low KL divergence. More details on this are provided in section 5.5.</p>
</div>
<div class="readable-text intended-text" id="p80">
<p><em>Regular</em> means that the space fulfills two properties: continuity and compactness. <em>Continuity</em> means that nearby points in the latent space are decoded into approximately similar things, while <em>compactness</em> means that any point in the latent space should lead to a meaningful decoded representation. These terms, approximately similar and meaningful, have precise definitions, which you can read more about in <em>Learn Generative AI with PyTorch</em> (Manning, 2024; <a href="https://mng.bz/AQBg">https://mng.bz/AQBg</a>). However, for this chapter, all you need to know is that these properties make it easier to sample from the latent space, resulting in cleaner generated samples and potentially higher model accuracy. </p>
</div>
<div class="readable-text intended-text" id="p81">
<p>When we regularize a latent space, we use variational methods that model the entire dataspace in terms of probability distributions (or densities). As we’ll see, the main benefit of using variational methods is that the latent space is well structured. However, variational methods don’t necessarily guarantee higher performance, so it’s often important to test both the autoencoder and the variational counterpart when using these types of models. This can be done by either looking at the reconstruction score (e.g., mean squared error) on the test dataset, applying some dimension reduction method to the latent encodings (e.g., t-SNE or Uniform Manifold Approximation and Projection [UMAP]), or using task-specific measures (e.g., the Inception Score for images or ROUGE/METEOR for text generation). Specifically for graphs, measures such as the maximum mean discrepancy (MMD), graph statistics, or graph kernel methods can all be used to compare against different synthetically generated graph copies. </p>
</div>
<div class="readable-text intended-text" id="p82">
<p>In the next few sections, we’ll go into more detail on what it means to model a dataspace as a probability density and how we can transform our graph autoencoder into a VGAE with just a few lines. These depend on some key probabilistic machine learning concepts such as the KL divergence and the reparameterization trick, which we give an overview of in section 5.5. For more of a deep dive into these concepts, we recommend <em>Probabilistic Deep Learning </em>(Manning, 2020). Let’s build a VGAE architecture and apply it to the same Amazon Products dataset as before.</p>
</div>
<div class="readable-text" id="p83">
<h3 class="readable-text-h3"><span class="num-string">5.3.1</span> Building a variational graph autoencoder</h3>
</div>
<div class="readable-text" id="p84">
<p>The VGAE architecture is similar to the GAE model. The main difference is that the output of a <em>variational</em> graph encoder is generated by sampling from a probability density. We can characterize density in terms of its mean and variance. Therefore, the output of the encoder will now be the mean and variance for each dimension of our previous space. The decoder then takes this sampled latent representation and decodes it to appear like the input data. This can be seen in figure 5.8, where the high-level model is that we now extend our previous autoencoder to output mean and variance rather than point estimates from the latent space. This allows our model to make probabilistic samples from the latent space. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p85">
<img alt="figure" height="149" src="../Images/5-8.png" width="844"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.8</span> Structure of a general VAE, where we now sample from a probability density in the latent space rather than a point estimate as with typical autoencoders. VGAEs extend the VAE architecture to apply to graph-structured data. </h5>
</div>
<div class="readable-text" id="p86">
<p>We have to adapt our architecture and also change our loss to include an additional term for regularizing the latent space. Listing 5.13 provides a code snippet for the VGAE. The similarities between listing 5.4 and the <code>VariationalGCNEncoder</code> layer in listing 5.13 include that we’ve doubled the dimensionality of our latent space and now return the mean and the log variance from our encoder at the end of our forward pass. </p>
</div>
<div class="browsable-container listing-container" id="p87">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.13</span> <code>VariationalGCNEncoder</code> </h5>
<div class="code-area-container">
<pre class="code-area">class VariationalGCNEncoder(torch.nn.Module):           <span class="aframe-location"/> #1
  def __init__(self, input_size, layers, latent_dims):
    super().__init__()
    self.layer0 = GCNConv(input_size, layers[0])
    self.layer1 = GCNConv(layers[0], layers[1])
    self.mu = GCNConv(layers[1], latent_dims)           
    self.logvar = GCNConv(layers[1], latent_dims)       

  def forward(self, x, edge_index):
    x = self.layer0(x, edge_index).relu()
    x = self.layer1(x, edge_index).relu()
    mu = self.mu(x, edge_index)
    logvar = self.logvar(x, edge_index)
    return mu, logvar                     <span class="aframe-location"/> #2</pre>
<div class="code-annotations-overlay-container">
     #1 Adds in mean and log variance variables to sample from
     <br/>#2 Forward pass returns mean and log variance variables
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p88">
<p>When we discussed the GAE, we learned that the decoder uses the inner product to return the adjacency matrix, or edge list. Previously we explicitly implemented the inner dot product. However, in PyG, this functionality is built in. To build a VGAE structure, we can call the <code>VGAE</code> function, shown in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p89">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.14</span> Variational graph autoencoder (<code>VGAE</code>) </h5>
<div class="code-area-container">
<pre class="code-area">from torch_geometric.nn import VGAE  <span class="aframe-location"/> #1
model = VGAE(VariationalGCNEncoder(input_size,\
 layers, latent_dims))</pre>
<div class="code-annotations-overlay-container">
     #1 Uses the VGAE function from the PyG library to build the autoencoder
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p90">
<p>This functionality makes it much simpler to build a VGAE, where the VGAE function in PyG takes care of the reparameterization trick. Now that we have our VGAE model, the next thing we need to do is amend the training and testing functions to include the KL divergence loss. The training function is shown in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p91">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.15</span> Training function</h5>
<div class="code-area-container">
<pre class="code-area">def train(model, criterion, optimizer):
    model.train() 
    optimizer.zero_grad() 
    z = model.encode(train_data.x, train_data.edge_index)     <span class="aframe-location"/> #1

    neg_edge_index = negative_sampling( 
    edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,
    num_neg_samples=train_data.edge_label_index.size(1), method='sparse')

    edge_label_index = torch.cat( 
    [train_data.edge_label_index, neg_edge_index], 
    dim=-1,) 
    out = model.decode(z, edge_label_index).view(-1)          

    edge_label = torch.cat([ 
    train_data.edge_label,
    train_data.edge_label.new_zeros(neg_edge_index.size(1))
    ], dim=0)

    loss = criterion(out, edge_label)           <span class="aframe-location"/> #2
+ (1 / train_data.num_nodes) * model.kl_loss()  

    loss.backward() 
    optimizer.step()

    return loss</pre>
<div class="code-annotations-overlay-container">
     #1 As we are using the PyG VGAE function, we need to use the encode and decode methods.
     <br/>#2 Adds in the regularizing term of the loss given by the KL divergence
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p92">
<p>This is the same training loop that we used in listing 5.12 to train our GAE model. The only differences are that we include an additional term to our loss that minimizes the KL divergence and we change the <code>encoder</code> and <code>decoder</code> method calls to <code>encode</code> and <code>decode</code> (which we also need to update in our test function). Otherwise, the training remains unchanged. Note that thanks to the added PyG functionality, these changes are considerably less involved than when we made the changes in PyTorch earlier. However, going through each of those extra steps gives us more intuition about the underlying architecture for a GAE. </p>
</div>
<div class="readable-text intended-text" id="p93">
<p>We can now apply our VGAE to the Amazon Products dataset and use this to perform edge prediction, which yields an overall test accuracy of 88%. This is slightly higher than our accuracy for GAE. It’s important to note that VGAEs won’t necessarily give higher accuracy. As a result, you should always try a GAE as well as a VGAE and run careful model validation when using this architecture. </p>
</div>
<div class="readable-text" id="p94">
<h3 class="readable-text-h3"><span class="num-string">5.3.2</span> When to use a variational graph autoencoder</h3>
</div>
<div class="readable-text" id="p95">
<p>Given that the accuracy for the VGAE was similar to the GAE, it’s important to realize the limitations of both methods. In general, GAEs and VGAEs are great models to use when you want to build a generative model or where you want to use one aspect of your data to learn another aspect. For example, we might want to make a graph-based model for pose prediction. We can use both GAE and VGAE architectures to predict future poses based on video footage. (We’ll see a similar example in later chapters.) When we do so, we’re using the GAE/VGAE to learn a graph of the body, conditioned on what the future positions of each body part will be. However, if we’re specifically interested in generating new data, such as new chemical graphs for drug discovery, VGAEs are often better as the latent space is more structured. </p>
</div>
<div class="readable-text intended-text" id="p96">
<p>In general, GAEs are great for specific reconstruction tasks such as link prediction or node classification, while VGAEs are better for where the tasks require a larger or more diverse range of synthetic samples, such as where you want to generate entirely new subgraphs or small graphs. VGAEs are also often better suited for when the underlying dataset is noisy, compared to GAEs which are faster and more suitable for graph data with clear structure. Finally, note that VGAEs are less prone to overfitting due to their variational approach, and they may generalize better as a result. As always, your choice of architecture depends on the problem at hand. </p>
</div>
<div class="readable-text intended-text" id="p97">
<p>In this chapter, we’ve learned about two examples of generative models, the GAE and VGAE models, and how to implement these models to work with graph-structured data. To better understand how to use this model class, we applied our models to an edge prediction task. However, this is only one step in applying a generative model. </p>
</div>
<div class="readable-text intended-text" id="p98">
<p>In many instances where we require a generative model, we use successive layers of autoencoders to further reduce the dimensionality of our system and increase our reconstruction power. In the context of drug discovery and chemical science, GAEs allow us to reconstruct the adjacency matrix (as we did here) as well as reconstruct types of molecules and even the number of molecules. GAEs are used frequently in many sciences and industries. Now you have the tools to try them out too.</p>
</div>
<div class="readable-text intended-text" id="p99">
<p>In the next section, we’ll demonstrate how to use the VGAE to generate new graphs with specific qualities, such as novel molecules that have a high property indicating usefulness as a potential drug candidate. </p>
</div>
<div class="readable-text" id="p100">
<h2 class="readable-text-h2"><span class="num-string">5.4</span> Generating graphs using GNNs</h2>
</div>
<div class="readable-text" id="p101">
<p>So far, we’ve considered how to use a generative model of our graph to estimate edges between nodes. However, sometimes we’re also interested in generating not just a node or an edge but the entire graph. This can be particularly important when trying to understand or predict graph-level data. In this example, we’ll do exactly that by using our GAE and VGAEs to generate new potential molecules to synthesize, which have certain properties. </p>
</div>
<div class="readable-text intended-text" id="p102">
<p>One of the fields that GNNs have had the largest effect on has been drug discovery, especially for the identification of new molecules or potential drugs. In 2020, a new antibiotic was proposed that was discovered using a GNN, and, in 2021, a new method for identifying carcinogens in food was published that also made use of GNNs. Since then, there have been many other papers that use GNNs as tools to accelerate the drug discovery pipeline. </p>
</div>
<div class="readable-text" id="p103">
<h3 class="readable-text-h3"><span class="num-string">5.4.1</span> Molecular graphs</h3>
</div>
<div class="readable-text" id="p104">
<p>We’re going to be considering small molecules that have previously been screened for drugs, as described in the ZINC dataset of around 250,000 individual molecules. Each molecule in this dataset has additional data including the following: </p>
</div>
<ul>
<li class="readable-text" id="p105"> <em/><em>Simplified Molecular Input Line Entry System (SMILES)</em> —A description of the molecular structure or the molecular <em>graph </em>in ASCII format. </li>
<li class="readable-text" id="p106"> <em>Important properties</em> —Synthetic accessibility score (SAS), water-octanol partition coefficient (<code>logP</code>), and, most importantly, a measure of the quantitative estimate of druglikeness (QED), which highlights how likely this molecule could be as a potential drug.  </li>
</ul>
<div class="readable-text" id="p107">
<p>To make this dataset usable by our GNN models, we need to convert this into a suitable graph structure. Here, we’re going to be using PyG for defining our model and running our deep learning routines. Therefore, we first download the data and then convert the dataset into graph objects using NetworkX. We download our dataset in listing 5.16, which generates the following output: </p>
</div>
<div class="browsable-container listing-container" id="p108">
<div class="code-area-container">
<pre class="code-area">     smiles     logP     qed     SAS
0     CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1
     5.05060     0.702012     2.084095
1     C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1
     3.11370     0.928975     3.432004
2     N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...
     4.96778     0.599682     2.470633
3     CCOC(=O)[C@@H]1CCCN(C(=O)c2nc
      (-c3ccc(C)cc3)n3c...     
      4.00022     0.690944     2.822753
4     N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])
      [C@H](C#...     3.60956     0.789027     4.035182</pre>
</div>
</div>
<div class="browsable-container listing-container" id="p109">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.16</span> Create a molecular graph dataset</h5>
<div class="code-area-container">
<pre class="code-area">import requests
import pandas as pd

def download_file(url, filename):
     response = requests.get(url)
     response.raise_for_status() 
     with open(filename, 'wb') as f:
     f.write(response.content)

url = "https://raw.githubusercontent.com/
aspuru-guzikgroup/chemical_vae/master/models/
zinc_properties/250k_rndm_zinc_drugs_clean_3.csv"
filename = "250k_rndm_zinc_drugs_clean_3.csv"

download_file(url, filename)

df = pd.read_csv(filename)
df["smiles"] = df["smiles"].apply(lambda s: s.replace("\n", ""))</pre>
</div>
</div>
<div class="readable-text" id="p110">
<p>In listing 5.17, we define a function to convert the SMILES into small graphs, which we then use to create a PyG dataset. We also add some additional information to each object in our dataset, such as the number of heavy atoms that we can use for further data exploration. Here, we use the recursive SMILES depth-first search (DFS) toolkit (RDKit) package (<a href="http://www.rdkit.org/docs/index.xhtml">www.rdkit.org/docs/index.xhtml</a>), which is a great open source tool for cheminformatics. </p>
</div>
<div class="browsable-container listing-container" id="p111">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.17</span> Create the molecular graph dataset</h5>
<div class="code-area-container">
<pre class="code-area">   from torch_geometric.data import Data
   import torch
   from rdkit import Chem


   def smiles_to_graph(smiles, qed):
     mol = Chem.MolFromSmiles(smiles)
        if not mol:
             return None

        edges = []
        edge_features = []
        for bond in mol.GetBonds():
             edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])
             bond_type = bond.GetBondTypeAsDouble()
             bond_feature = [1 if i == bond_type\
             else 0 for i in range(4)]
             edge_features.append(bond_feature)

        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
        edge_attr = torch.tensor(edge_features, dtype=torch.float)
        x = torch.tensor([atom.GetAtomicNum()\
 for atom in mol.GetAtoms()], \
 dtype=torch.float).view(-1, 1)

        num_heavy_atoms = mol.GetNumHeavyAtoms()

        return Data(x=x, edge_index=edge_index,\
 edge_attr=edge_attr, \
qed=torch.tensor([qed], \
dtype=torch.float), \
num_heavy_atoms=num_heavy_atoms)</pre>
</div>
</div>
<div class="readable-text" id="p112">
<p>A random sample from our dataset is shown in figure 5.9, which highlights how varied our molecular graphs are and their small size, where each one has less than 100 nodes and edges.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p113">
<img alt="figure" height="505" src="../Images/5-9.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.9</span> Example molecular graphs with quantitative estimate of druglikeness (QED) </h5>
</div>
<div class="readable-text" id="p114">
<h3 class="readable-text-h3"><span class="num-string">5.4.2</span> Identifying new drug candidates</h3>
</div>
<div class="readable-text" id="p115">
<p>In figure 5.10, we start to see how QED can vary with different molecular structures. One of the main obstacles to drug discovery is the number of different potential combinations of molecules and how to know which ones to synthesize and then test for drug efficacy. This is far before the stage of introducing the drug to human, animal (in vivo), or sometimes even cellular (in vitro) trials. Even evaluating things such as a molecule’s solubility can be a challenge if we use the molecular graph alone. Here, we’re going to be focusing on predicting the molecules’ QED, to see which ones are most likely to have potential use as a drug. To give an example of how the QED can vary, see figure 5.10, which has four molecules with high (~0.95) and low (~0.12) QED. We can see some qualitative differences between these molecules, such as the increased number of strong bonds for those with low QED. However, estimating the QED directly from the graph is a challenge. To help us with this task, we’ll use a GNN to both generate and evaluate new potential drugs. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p116">
<img alt="figure" height="566" src="../Images/5-10.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.10</span> Molecules with high QED (top) and low QED (bottom)</h5>
</div>
<div class="readable-text intended-text" id="p117">
<p>Our work will be based on two important papers that demonstrated how generative models can be effective tools for identifying new molecules (Gómez-Bombarelli et al. [4] and De Cao et al. [5]). Specifically, Gómez-Bombarelli et al. showed that by constructing a smooth representation of the dataspace, which is the latent space we described earlier in this chapter, it’s possible to optimize to find new candidates with specific properties of interest. This work borrows heavily from an equivalent implementation in the Keras library, outlined in a posting by Victor Basu [6]. Figure 5.11 reproduces the basic idea from [5]. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p118">
<img alt="figure" height="459" src="../Images/5-11.png" width="904"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.11</span> Example of how a graph autoencoder that is trained to re-create small graphs can also be used to make property predictions. The property prediction is applied in the latent space and creates a learned gradient of a specific graph property—in our case, the QED value.</h5>
</div>
<div class="readable-text" id="p119">
<p>In figure 5.11, we can see that the underlying model structure is an autoencoder, just like the ones we’ve been discussing in this chapter. Here, we pass the SMILES of the molecule as input to the encoder, and this is then used to construct the latent space of different molecular representations. This is shown as regions with different colors representing different groups of molecules. Then, the decoder is designed to faithfully translate the latent space back into the original molecule. This is similar to the autoencoder structure that we showed earlier in figure 5.5.</p>
</div>
<div class="readable-text intended-text" id="p120">
<p>Alongside the latent space, we now also have an additional function, which is going to predict the property of the molecule. In figure 5.11, the property we’ll predict is also the property we’re optimizing for. Therefore, by learning how to encode both the molecule and the property, which in our case is QED, into the latent space, we can optimize drug discovery to generate new candidate molecules with a high QED. </p>
</div>
<div class="readable-text intended-text" id="p121">
<p>In our example, we’ll use the VGAE. This model includes two losses: a reconstruction loss that measures the difference between the original input data passed to the encoder and the output from the decoder, as well as a measure of the structure of the latent space, where we use the KL divergence.</p>
</div>
<div class="readable-text intended-text" id="p122">
<p>Along with these two loss functions, we’ll add one more function: a property prediction loss. The property prediction loss estimates the MSE between predicted and actual properties after running the latent representation through a property prediction model, as shown in the middle of figure 5.11. </p>
</div>
<div class="readable-text intended-text" id="p123">
<p>To train our GNN, we adapt the training loop provided earlier in listing 5.15 to include these individual losses. This is shown in listing 5.18. Here, we have the reconstruction loss as the binary cross-entropy (BCE) for the adjacency matrix, while the property prediction loss considers only QED and can be based on the MSE. </p>
</div>
<div class="browsable-container listing-container" id="p124">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.18</span> Loss for molecule graph generation</h5>
<div class="code-area-container">
<pre class="code-area">        def calculate_loss(self, pred_adj, \
   true_adj, qed_pred, qed_true, mu, logvar):
             adj_loss = F.binary_cross_entropy\
   (pred_adj, true_adj)  <span class="aframe-location"/> #1

             qed_loss = F.mse_loss\
(qed_pred.view(-1), qed_true.view(-1))    <span class="aframe-location"/> #2

             kl_loss = -0.5 * torch.mean\
(torch.sum(1 + logvar - mu.pow(2)\    <span class="aframe-location"/> #3
 - logvar.exp(), dim=1))

             return adj_loss + qed_loss + kl_loss</pre>
<div class="code-annotations-overlay-container">
     #1 Reconstruction loss
     <br/>#2 Property prediction loss
     <br/>#3 KL divergence loss
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p125">
<h3 class="readable-text-h3"><span class="num-string">5.4.3</span> VGAEs for generating graphs</h3>
</div>
<div class="readable-text" id="p126">
<p>Now that we have both our training data and loss, we can start to think about the model. Overall, this model will be similar to the ones discussed earlier in the chapter, both GAE and VGAE. However, we need to make some subtle changes to our model to ensure that it’s well applied to the problem at hand: </p>
</div>
<ul>
<li class="readable-text" id="p127"> Use a heterogenous GCN to account for different edge types. </li>
<li class="readable-text" id="p128"> Train the decoder to generate the entire graph. </li>
<li class="readable-text" id="p129"> Introduce a property prediction layer. </li>
</ul>
<div class="readable-text" id="p130">
<p>Let’s look at each of these in turn. </p>
</div>
<div class="readable-text" id="p131">
<h4 class="readable-text-h4">Heterogeneous GCN</h4>
</div>
<div class="readable-text" id="p132">
<p>The small graphs that we’re generating will have different edge types that connect the nodes of our graphs. Specifically, we can have a different number of bonds between the atoms such as a single bond, double bond, triple bond, or even <em>aromatic bonds</em>, which relate to molecules that are formed into a ring. Graphs with more than one edge type are known as heterogeneous graphs, so we’ll need to make our GNN applicable to heterogeneous graphs. </p>
</div>
<div class="readable-text intended-text" id="p133">
<p>So far, all the graphs we’ve been considering have been homogenous (only one edge type). In listing 5.19, we show how the GCN, which we discussed in chapter 3, can be adapted to heterogeneous graphs. Here, we explicitly map out some of the different features for heterogeneous graphs. However, it’s important to note that many GNN packages already support models for heterogeneous graphs out of the box. For example, PyG has a specific class of models known as <code>HeteroConv</code>. </p>
</div>
<div class="readable-text intended-text" id="p134">
<p>Listing 5.19 shows the code to create a heterogenous GCN. This builds off the message-passing class in PyG, which is fundamental to all GNN models. We also use the PyTorch <code>Parameter</code> class to create a new subset of parameters that are specific to the different edge types (relations). Finally, we also specify here that the aggregation operation in the message-passing framework is based on summation (<code>'add'</code>). If you’re interested, feel free to try other aggregation operations. </p>
</div>
<div class="browsable-container listing-container" id="p135">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.19</span> Heterogenous GCN</h5>
<div class="code-area-container">
<pre class="code-area">from torch.nn import Parameter
from torch_geometric.nn import MessagePassing

   class HeterogeneousGraphConv(MessagePassing):
def __init__(self, in_channels, out_channels, num_relations, bias=True):
        super(HeterogeneousGraphConv, self).\
__init__(aggr='add')      <span class="aframe-location"/> #1
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_relations = num_relations

        self.weight = Parameter(torch.\
Tensor(num_relations, in_channels, \
out_channels)) <span class="aframe-location"/> #2
        if bias:
             self.bias = Parameter(torch.Tensor(out_channels))
        else:
             self.register_parameter('bias', None)

        self.reset_parameters()

        def reset_parameters(self):
             torch.nn.init.xavier_uniform_(self.weight)
             if self.bias is not None:
                  torch.nn.init.zeros_(self.bias)

        def forward(self, x, edge_index, edge_type):

       return self.propagate\
(edge_index, size=(x.size(0), 
x.size(0)), x=x, edge_type=edge_type) <span class="aframe-location"/> #3

       def message(self, x_j, edge_type, index, size):  <span class="aframe-location"/> #4

            W = self.weight[edge_type]   <span class="aframe-location"/> #5
            x_j = torch.matmul(x_j.unsqueeze(1), W).squeeze(1)

            return x_j

       def update(self, aggr_out):
            if self.bias is not None:
                 aggr_out += self.bias
            return aggr_out</pre>
<div class="code-annotations-overlay-container">
     #1 "Add" aggregation
     <br/>#2 Parameter for weights
     <br/>#3 edge_type is used to select the weights.
     <br/>#4 x_j has shape [E, in_channels], and edge_type has shape [E].
     <br/>#5 Select the corresponding weights.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p136">
<p>With the preceding GNN, we can then compose our encoder as a combination of these individual GNN layers. This is shown in listing 5.20, where we follow the same logic as when we defined our edge encoder (refer to listing 5.13), except that we now switch out our GCN layers for the heterogeneous GCN layers. As we have different edge types, we must now also specify the number of different types (relations) as well as passing the specific edge type into the forward function for our graph encoder. Again, we return both log variance and mean to ensure that the latent space is constructed using distributions rather than point samples.</p>
</div>
<div class="browsable-container listing-container" id="p137">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.20</span> Small graph encoder</h5>
<div class="code-area-container">
<pre class="code-area">   class VariationalGCEncoder(torch.nn.Module):
        def __init__(self, input_size, layers, latent_dims, num_relations):
             super().__init__()
             self.layer0 = HeterogeneousGraphConv(input_size, 
   layers[0], num_relations)                                    <span class="aframe-location"/> #1
             self.layer1 = HeterogeneousGraphConv(layers[0], 
   layers[1], num_relations)                                    
             self.layer2 = HeterogeneousGraphConv(layers[1], 
   latent_dims, num_relations)                                  

        def forward(self, x, edge_index, edge_type):
             x = F.relu(self.layer0\
(x, edge_index, edge_type))             <span class="aframe-location"/> #2
             x = F.relu(self.layer1\
(x, edge_index, edge_type))             
             mu = self.mu(x, edge_index) 
             logvar = self.logvar(x, edge_index)
             return mu, logvar</pre>
<div class="code-annotations-overlay-container">
     #1 Heterogeneous GCNs
     <br/>#2 Forward pass GCNs
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p138">
<h4 class="readable-text-h4">Graph decoders</h4>
</div>
<div class="readable-text" id="p139">
<p>In our previous examples, we used GAEs to generate and predict edges between nodes in a single graph. However, we’re now interested in using our autoencoder to generate entire graphs. Therefore, we no longer just consider the inner product decoder to account for the presence of an edge in the graph but instead decode both the adjacency matrix and feature matrix for each small molecular graph. This is shown in listing 5.21.  </p>
</div>
<div class="browsable-container listing-container" id="p140">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.21</span> Small graph decoder</h5>
<div class="code-area-container">
<pre class="code-area">class GraphDecoder(nn.Module):
        def __init__(self, latent_dim, adjacency_shape, feature_shape):
        super(GraphDecoder, self).__init__()

        self.dense1 = nn.Linear(latent_dim, 128)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(0.1)

        self.dense2 = nn.Linear(128, 256)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(0.1)

        self.dense3 = nn.Linear(256, 512)
        self.relu3 = nn.ReLU()
        self.dropout3 = nn.Dropout(0.1)

        self.adjacency_output = nn.Linear(512,\
torch.prod(torch.tensor(adjacency_shape)).item())
        self.feature_output = nn.Linear(512,\
torch.prod(torch.tensor(feature_shape)).item())

        def forward(self, z):
             x = self.dropout1(self.relu1(self.dense1(z)))
             x = self.dropout2(self.relu2(self.dense2(x)))
             x = self.dropout3(self.relu3(self.dense3(x)))

             adj = self.adjacency_output(x)  <span class="aframe-location"/> #1
             adj = adj.view(-1, *self.adjacency_shape)
             adj = (adj + adj.transpose(-1, -2)) / 2   <span class="aframe-location"/> #2
             adj = F.softmax(adj, dim=-1)                    <span class="aframe-location"/> #3

             features = self.feature_output(x)  <span class="aframe-location"/> #4
             features = features.view(-1, *self.feature_shape)
             features = F.softmax(features, dim=-1)   <span class="aframe-location"/> #5

             return adj, features</pre>
<div class="code-annotations-overlay-container">
     #1 Generates the adjacency matrix 
     <br/>#2 Symmetrizes the adjacency matrix
     <br/>#3 Applies softmax
     <br/>#4 Generates features
     <br/>#5 Applies softmax
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p141">
<p>The majority of this code is typical for decoder style networks. We begin with a small network that matches the dimension for the latent space created using the encoder. We then progressively increase the size of the graph through subsequent layers of the network. Here, we can use simple linear networks, where we include network dropout for performance. At the final layer, we reshape the decoder output into the adjacency and feature matrices. We also ensure that the adjacency matrix is symmetric before applying softmax. We symmetrize the adjacency matrix by adding it to its transpose and dividing by 2. This ensures that node <code>i</code> is connected to <code>j</code> and that <code>j</code> is also connected to <code>i</code>. We then apply softmax to normalize the adjacency matrix, ensuring all outgoing edges from each node sum to 1. There are other choices we could make here such as using the maximum value, applying a threshold, or using the sigmoid function instead of softmax. In general, averaging + softmax is a good approach.</p>
</div>
<div class="readable-text" id="p142">
<h4 class="readable-text-h4">Property prediction layer</h4>
</div>
<div class="readable-text" id="p143">
<p>All that’s left is to combine both encoder and decoder networks into a final model that can be used for molecular graph generation, as shown in listing 5.22. Overall, this follows the same steps as in listing 5.14 earlier, where we define both our encoder and decoder as well as use the reparameterization trick. The only difference is that we also include a simple linear network to predict the property of the graphs, in this case, the QED. This is applied on the latent representation (<code>z</code>), after being reparameterized.</p>
</div>
<div class="browsable-container listing-container" id="p144">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.22</span> VGAE for molecular graph generation</h5>
<div class="code-area-container">
<pre class="code-area">   import torch
   import torch.nn as nn
   import torch.nn.functional as F
   from torch_geometric.nn import MessagePassing

   class VGAEWithPropertyPrediction(nn.Module):
        def __init__(self, encoder, decoder, latent_dim):
             super(VGAEWithPropertyPrediction, self).__init__()
             self.encoder = encoder
             self.decoder = decoder
             self.property_prediction_layer = nn.Linear(latent_dim, 1)

        def reparameterize(self, mu, logvar):
             std = torch.exp(logvar / 2)
             eps = torch.randn_like(std)
             return eps.mul(std).add_(mu)

        def forward(self, data):
             mu, logvar = self.encoder(data.x, \
data.edge_index, data.edge_attr)
             z = self.reparameterize(mu, logvar)
             adj_recon, x_recon = self.decoder(z)
             qed_pred = self.property_prediction_layer(z)
             return adj_recon, x_recon, qed_pred, mu, logvar, z</pre>
</div>
</div>
<div class="readable-text" id="p145">
<p>The output for the model is then both the mean and log variance, which are passed to the KL divergence; the reconstructed adjacency matrix and feature matrix, passed to the reconstruction loss; and the predicted QED values, which are used in the prediction loss. Using these, we can then compute the loss for our network and backpropagate the loss through the network weights to refine the generated graphs to have specifically high QED values. Next, we show how to achieve just that in our training and test loops. </p>
</div>
<div class="readable-text" id="p146">
<h3 class="readable-text-h3"><span class="num-string">5.4.4</span> Generating molecules using a GNN</h3>
</div>
<div class="readable-text" id="p147">
<p>In the previous section, we discussed all the individual parts needed to use a GNN to generate molecules. We’ll now bring the different elements together and demonstrate how to use a GNN to create novel graphs that are optimized for a specific property. In figure 5.12, we show the steps to generate molecules with a GNN that have a high QED. These steps include creating suitable graphs to represent small molecules, passing these through our autoencoder, predicting specific molecular features such as QED, and then repeating those steps until we’re able to recreate new novel molecular graphs with specific features. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p148">
<img alt="figure" height="427" src="../Images/5-12.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.12</span> Steps to generate molecules with a GNN</h5>
</div>
<div class="readable-text" id="p149">
<p>The key element that remains is to combine our loss functions with our adapted VGAE model. This is shown in listing 5.23, which defines our training loop. This is similar to previous training loops that you’ve seen in earlier chapters and examples. The main idea is that our model is used to predict some property of the graph. However, here we’re predicting the entire graph, as defined in the predicted adjacency matrix (<code>pred_adj</code>) and the predicted feature matrix (<code>pred_feat</code>). </p>
</div>
<div class="readable-text intended-text" id="p150">
<p>The output from our model and the real data are passed to our method for calculating the loss, which contains the reconstruction loss, KL divergence loss, and property prediction loss. Finally, we compute the gradient penalty, which acts as a further regularizer for our model (and defined in more detail in section 5.5). With both loss and gradient calculated, we backpropagate through our model, step our optimizer forwards, and return the loss. </p>
</div>
<div class="browsable-container listing-container" id="p151">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 5.23</span> Training function for molecule graph generation</h5>
<div class="code-area-container">
<pre class="code-area">   def train(model, optimizer, data, test=False):
        model.train()
        optimizer.zero_grad()

        pred_adj, pred_feat, pred_qed, mu, logvar, _ = model(data)

        real_adj = create_adjacency_matrix\
(data.edge_index, data.edge_attr, \
num_nodes=NUM_ATOMS)
        real_x = data.x
        real_qed = data.qed


        loss = calculate_loss\
(pred_adj[0], real_adj, pred_qed, \
real_qed, mu, logvar)  <span class="aframe-location"/> #1

        total_loss = loss

        if not test:
             total_loss.backward()
        optimizer.step()
        return total_loss.item()</pre>
<div class="code-annotations-overlay-container">
     #1 Compute losses
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p152">
<p>During training time, we find that the model loss decreases, demonstrating that the model is effectively learning how to reproduce novel molecules. We show some of these molecules in figure 5.13.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p153">
<img alt="figure" height="335" src="../Images/5-13.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.13</span> Small molecule graphs generated using a GNN</h5>
</div>
<div class="readable-text" id="p154">
<p>To better understand the distribution of the predicted QED property in our latent space, we apply our encoder to a new subset of data and look at the first two axes of the data as represented in the latent space, as shown in figure 5.14. Here, we can see that the latent space has been constructed to cluster molecules with higher QED together. Therefore, by sampling from regions around this area, we can identify new molecules to test. Future work will be needed to verify our results, but as a first step toward the discovery of new molecules, we’ve demonstrated that a GNN model may well be used to propose new and potentially valuable drug candidates.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p155">
<img alt="figure" height="933" src="../Images/5-14.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.14</span> Latent space of drug molecules</h5>
</div>
<div class="readable-text" id="p156">
<p>In this chapter, we’ve focused on generative tasks rather than classical discriminative models. We’ve shown generative models, such as GAEs and VGAEs, can be used for edge prediction, learning to identify connections between nodes where information is potentially not available. We then went on to show that generative GNNs can be used to discover not just unknown parts of a graph, such as a node or edge, but also entirely new and complicated graphs, when we applied our GNNs to generate new small molecules with a high QED. These results highlight that GNNs are vital tools for those working in chemistry, life sciences, and many other disciplines that deal with many individual graphs. </p>
</div>
<div class="readable-text intended-text" id="p157">
<p>Moreover, we’ve learned that GNNs are extremely useful for both discriminative and generative tasks. Here, we consider the topic of small molecule graphs, but GNNs have also been applied to knowledge graphs and small social clusters. In the next chapter, we’ll look at how we can learn to generate graphs that are consistent over time by combining generative GNNs with temporal encodings. In that spirit, we take a further step forward and learn how GNNs can be taught how to walk. </p>
</div>
<div class="readable-text" id="p158">
<h2 class="readable-text-h2"><span class="num-string">5.5</span> Under the hood</h2>
</div>
<div class="readable-text" id="p159">
<p>Deep generative models use artificial neural networks to model the dataspace. One of the classic examples of a deep generative model is the autoencoder. Autoencoders contain two key components, the encoder and the decoder, both represented by neural networks. They learn how to take data and encode (compress) it into a low dimensional representation as well as decode (uncompress) it again. Figure 5.15 shows a basic autoencoder taking an image as input and compressing it (step 1). This results in the low dimensional representation, or latent space (step 2). The autoencoder then reconstructs the image (step 3), and the process is repeated until the reconstruction error between input image (x) and output image (x*) is as small as possible. The autoencoder is the basic idea behind GAEs and VGAEs. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p160">
<img alt="figure" height="419" src="../Images/5-15.png" width="557"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.15</span> Structure of an autoencoder [9]</h5>
</div>
<div class="readable-text" id="p161">
<h3 class="readable-text-h3"><span class="num-string">5.5.1</span> Understanding link prediction tasks</h3>
</div>
<div class="readable-text" id="p162">
<p>Link prediction is a common problem in graph-based learning, especially in situations where we have incomplete knowledge of our data. This might be because the graph changes over time, for example, where we expect new customers to use an e-commerce service, and we want a model that can give the best suggested products to buy at that time. Alternatively, it may be costly to acquire this knowledge, for example, if we want our model to predict which combinations of drugs lead to specific disease outcomes. Finally, our data may contain incorrect or purposefully hidden details, such as fake accounts on a social media platform. Link prediction allows us to infer relations <em>between</em> nodes in our graph. Essentially, this means creating a model that predicts when and how nodes are connected, as shown in figure 5.16. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p163">
<img alt="figure" height="339" src="../Images/5-16.png" width="777"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.16</span> Schematic explaining how link prediction is performed in practice. Subsections of the input graph (subgraphs) are passed to the GNN with different links missing, and the model learns to predict when to recreate a link.</h5>
</div>
<div class="readable-text" id="p164">
<p>For link prediction, a model will take pairs of nodes as input and predict whether these nodes are connected (whether they should be <em>linked)</em>. To train a model, we’ll also need ground-truth targets. We generate these by hiding a subset of links within the graph. These hidden links become the missing data that we’ll learn to infer, which are known as <em>negative samples</em>. However, we also need a way to encode the information about pairs of nodes. Both of these parts can be solved simultaneously using GAEs, as autoencoders both encode information about the edge as well as predict whether an edge exists.</p>
</div>
<div class="readable-text" id="p165">
<h3 class="readable-text-h3"><span class="num-string">5.5.2</span> The inner product decoder</h3>
</div>
<div class="readable-text" id="p166">
<p>Inner product decoders are used for graphs because we want to reconstruct the adjacency matrix from the latent representation of our feature data. The GAE learns how to rebuild a graph (to infer the edges) given a latent representation of the nodes. The inner product in high dimensional space calculates the distance between two positions. We use the inner product, rescaled by the sigmoid function, to gain a probability for an edge between nodes. Essentially, we use the distance between points in the latent space as a probability that a node will be connected when decoded. This allows us to build a decoder that takes samples from our latent space and returns probabilities of whether an edge exists, namely, to perform edge prediction, as shown in figure 5.17.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p167">
<img alt="figure" height="613" src="../Images/5-17.png" width="992"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.17</span> Overall steps for training our model for link prediction</h5>
</div>
<div class="readable-text" id="p168">
<p>The inner product decoder works by taking the latent representation of our data and applying the inner product of this data using the passed edge index of our data. We then apply the sigmoid function to this value, which returns a matrix where each value represents the probability that there is an edge between the two nodes. </p>
</div>
<div class="readable-text" id="p169">
<h4 class="readable-text-h4">Regularizing the latent space </h4>
</div>
<div class="readable-text" id="p170">
<p>Put plainly, the KL divergence tells us how much worse we would be doing if we used the wrong probability density when estimating a probability of something. Suppose we have two coins and want to guess how well one coin (which we know is fair) matches the other coin (which we don’t know is fair). We’re trying to use the coin with the known probability to predict the probability for the coin with unknown probability. If it’s a good predictor (the unknown coin actually is fair), then the KL divergence will be zero. The probability densities of both coins are the same; however, if we find that the coin is a bad predictor, then the KL divergence will be large. This is because the two probability densities will be far from each other. In figure 5.18, we can see this explicitly. We’re trying to model the unknown probability density Q(z) using the conditional probability density P(Z|X). As the densities overlap, the KL divergence here will be low. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p171">
<img alt="figure" height="309" src="../Images/5-18.png" width="717"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.18</span> KL divergence calculates the degree to which two probability densities are distinct. High KL divergence means that they are well separated, whereas low KL divergence means they are not. </h5>
</div>
<div class="readable-text" id="p172">
<p>Practically, we convert an autoencoder to a VGAE by introducing the KL divergence in the loss. Our intention here is to both minimize the discrepancy between our encoder and decoder as in the autoencoder loss, as well as minimize the difference between the probability distribution given by our encoder and the “true” distribution that was used to generate our data. This is done by adding the KL divergence to the loss. For many standard VGAE, this is given by </p>
</div>
<div class="browsable-container equation-container" id="p173">
<h5 class="browsable-container-h5"><span class="num-string">(5.1)</span> <span class="aframe-location"/></h5>
<img alt="figure" height="67" src="../Images/2.png" width="679"/>
</div>
<div class="readable-text" id="p174">
<p>where (<em>p</em>||<em>q</em>) denotes the divergence of probability <em>p</em> with respect to probability <em>q</em>. The term <em>m</em> is the mean value of the latent features, and log(var) is the logarithm of the variance. We use this in the loss function whenever we build a VGAE, ensuring that the forward pass returns both the mean and variance to our decoder. </p>
</div>
<div class="readable-text" id="p175">
<h4 class="readable-text-h4">Over-squashing</h4>
</div>
<div class="readable-text" id="p176">
<p>We’ve discussed how GNNs can be used to find out information about a node by propagating node and edge representations through message passing. These are used to make embeddings of individual nodes or edges, which help guide the model to perform some specific tasks. In this chapter, we discussed how to construct a model that constructs latent representations by propagating all of the embeddings created by the message-passing layers into a latent space. Both perform dimension reduction and representation learning of graph-specific data. </p>
</div>
<div class="readable-text intended-text" id="p177">
<p>However, GNNs have a specific limitation in how much information they can use to make representations. GNNs suffer from something known as <em>over-squashing</em>, which refers to how information that is spread many hops across the graph (i.e., message passing) causes a considerable drop in performance. This is because the neighborhood that each node receives information from, also known as its receptive field, grows exponentially with the number of layers of the GNN. As more information is aggregated through message passing across these layers, the important signals from distant nodes become diluted compared to the information coming from nearer nodes. This causes the node representations to become similar or more homogenous, and eventually to converge to the same representations, also known as over-smoothing, which we discussed in chapter 4. </p>
</div>
<div class="readable-text intended-text" id="p178">
<p>Empirical evidence has shown that this can start to occur with as few as three or four layers [7], as you can see in figure 5.19. This highlights one of the key differences between GNNs and other deep learning architectures: we rarely want to make a very deep model with many layers stacked on top of each other. For models with many layers, other methods are often also introduced to ensure long-range information is included such as skip connections or attention mechanisms.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p179">
<img alt="figure" height="408" src="../Images/5-19.png" width="588"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 5.19</span> Visualization of over-squashing (Source: Alon and Yahav [7])</h5>
</div>
<div class="readable-text intended-text" id="p180">
<p>In the previous example in this chapter, we spoke about using GNNs for drug discovery. Here, we considered an example where the graphs were relatively small. However, when graphs become larger, there is an increasing risk that long-range interactions become important. This is particularly true in chemistry and biology, where nodes at extreme ends of a graph can have an outsized influence on the overall properties of the graph. In the context of chemistry, these might be two atoms that are either ends of a large molecule and which decide the overall properties of the molecule such as its toxicity. The range of interactions or information flow that we need to consider to effectively model a problem is known as the <em>problem radius</em>. When designing a GNN, we need to make sure that the number of layers is at least as large as the problem radius.</p>
</div>
<div class="readable-text intended-text" id="p181">
<p>In general, there are several methods for addressing over-squashing for GNNs:</p>
</div>
<ul>
<li class="readable-text" id="p182"> Ensure that not too many layers are stacked together. </li>
<li class="readable-text" id="p183"> Add in new “fake” edges between nodes that are very far/many hops apart or introduce a single node that is attached to all other nodes so that the problem radius is reduced to 2. </li>
<li class="readable-text" id="p184"> Use sampling, such as GraphSAGE, which samples from the neighborhood or introduces skip connections, which similarly skip some local neighbors. For sampling methods, it’s important to balance the loss of local information with the gain of more long-range information. </li>
</ul>
<div class="readable-text" id="p185">
<p>All of these methods are highly problem specific, and you should think carefully about the type of interactions between nodes in your graph when deciding whether long-range interactions are important. For example, in the next chapter, we consider motion prediction where the head has likely little influence on the foot compared to the knee. Alternatively, molecular graphs as described in this chapter will likely have large influences from more distant nodes. Therefore, the most important part in resolving problems such as over-squashing is making sure you have a solid understanding of both your problem and data. </p>
</div>
<div class="readable-text" id="p186">
<h2 class="readable-text-h2">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p187"> Discriminative models learn to separate data classes, while generative models learn to model the entire dataspace. </li>
<li class="readable-text" id="p188"> Generative models are often used to perform dimension reduction. Principal component analysis (PCA) is a form of linear dimension reduction. </li>
<li class="readable-text" id="p189"> Autoencoders contain two key components, the encoder and the decoder, both represented by neural networks. They learn how to take data and encode (compress) it into a low dimensional representation as well as decode (uncompress) it again. For autoencoders, the low dimensional representation is known as the latent space. </li>
<li class="readable-text" id="p190"> VAEs extend autoencoders to have a regularizing term in the loss. This regularizing term is typically the Kullback-Liebler (KL) divergence, which measures the difference between two distributions—the learned latent distribution and a prior distribution. The latent space of VAEs is more structured and continuous, where each point represents a probability density rather than a fixed-point encoding. </li>
<li class="readable-text" id="p191"> Autoencoders and variational autoencoders (VAEs) can also be applied to graphs. These are, respectively, graph autoencoders (GAE) and variational graph autoencoders (VGAE). They are similar to typical autoencoders and VAEs, but the decoder element is typically the dot product applied to the edge list. </li>
<li class="readable-text" id="p192"> GAEs and VGAEs are useful for edge prediction tasks. They can help us predict where there might be hidden edges in our graph. </li>
</ul>
</div></body></html>