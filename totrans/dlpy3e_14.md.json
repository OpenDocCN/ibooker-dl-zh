["```py\nThe quick brown fox jumped over the lazy dog. \n```", "```py\nimport regex as re\n\ndef split_chars(text):\n    return re.findall(r\".\", text) \n```", "```py\n>>> chars = split_chars(\"The quick brown fox jumped over the lazy dog.\")\n>>> chars[:12]\n[\"T\", \"h\", \"e\", \" \", \"q\", \"u\", \"i\", \"c\", \"k\", \" \", \"b\", \"r\"]\n```", "```py\ndef split_words(text):\n    return re.findall(r\"[\\w]+|[.,!?;]\", text) \n```", "```py\n>>> split_words(\"The quick brown fox jumped over the dog.\")\n[\"The\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"dog\", \".\"]\n```", "```py\nvocabulary = {\n    \"[UNK]\": 0,\n    \"the\": 1,\n    \"quick\": 2,\n    \"brown\": 3,\n    \"fox\": 4,\n    \"jumped\": 5,\n    \"over\": 6,\n    \"dog\": 7,\n    \".\": 8,\n}\nwords = split_words(\"The quick brown fox jumped over the lazy dog.\")\nindices = [vocabulary.get(word, 0) for word in words] \n```", "```py\n[0, 2, 3, 4, 5, 6, 1, 0, 7, 8] \n```", "```py\nclass CharTokenizer:\n    def __init__(self, vocabulary):\n        self.vocabulary = vocabulary\n        self.unk_id = vocabulary[\"[UNK]\"]\n\n    def standardize(self, inputs):\n        return inputs.lower()\n\n    def split(self, inputs):\n        return re.findall(r\".\", inputs)\n\n    def index(self, tokens):\n        return [self.vocabulary.get(t, self.unk_id) for t in tokens]\n\n    def __call__(self, inputs):\n        inputs = self.standardize(inputs)\n        tokens = self.split(inputs)\n        indices = self.index(tokens)\n        return indices \n```", "```py\nimport collections\n\ndef compute_char_vocabulary(inputs, max_size):\n    char_counts = collections.Counter()\n    for x in inputs:\n        x = x.lower()\n        tokens = re.findall(r\".\", x)\n        char_counts.update(tokens)\n    vocabulary = [\"[UNK]\"]\n    most_common = char_counts.most_common(max_size - len(vocabulary))\n    for token, count in most_common:\n        vocabulary.append(token)\n    return dict((token, i) for i, token in enumerate(vocabulary)) \n```", "```py\nclass WordTokenizer:\n    def __init__(self, vocabulary):\n        self.vocabulary = vocabulary\n        self.unk_id = vocabulary[\"[UNK]\"]\n\n    def standardize(self, inputs):\n        return inputs.lower()\n\n    def split(self, inputs):\n        return re.findall(r\"[\\w]+|[.,!?;]\", inputs)\n\n    def index(self, tokens):\n        return [self.vocabulary.get(t, self.unk_id) for t in tokens]\n\n    def __call__(self, inputs):\n        inputs = self.standardize(inputs)\n        tokens = self.split(inputs)\n        indices = self.index(tokens)\n        return indices \n```", "```py\ndef compute_word_vocabulary(inputs, max_size):\n    word_counts = collections.Counter()\n    for x in inputs:\n        x = x.lower()\n        tokens = re.findall(r\"[\\w]+|[.,!?;]\", x)\n        word_counts.update(tokens)\n    vocabulary = [\"[UNK]\"]\n    most_common = word_counts.most_common(max_size - len(vocabulary))\n    for token, count in most_common:\n        vocabulary.append(token)\n    return dict((token, i) for i, token in enumerate(vocabulary)) \n```", "```py\nimport keras\n\nfilename = keras.utils.get_file(\n    origin=\"https://www.gutenberg.org/files/2701/old/moby10b.txt\",\n)\nmoby_dick = list(open(filename, \"r\"))\n\nvocabulary = compute_char_vocabulary(moby_dick, max_size=100)\nchar_tokenizer = CharTokenizer(vocabulary) \n```", "```py\n>>> print(\"Vocabulary length:\", len(vocabulary))\nVocabulary length: 64\n>>> print(\"Vocabulary start:\", list(vocabulary.keys())[:10])\nVocabulary start: [\"[UNK]\", \" \", \"e\", \"t\", \"a\", \"o\", \"n\", \"i\", \"s\", \"h\"]\n>>> print(\"Vocabulary end:\", list(vocabulary.keys())[-10:])\nVocabulary end: [\"@\", \"$\", \"%\", \"#\", \"=\", \"~\", \"&\", \"+\", \"<\", \">\"]\n>>> print(\"Line length:\", len(char_tokenizer(\n...    \"Call me Ishmael. Some years ago--never mind how long precisely.\"\n... )))\nLine length: 63\n```", "```py\nvocabulary = compute_word_vocabulary(moby_dick, max_size=2_000)\nword_tokenizer = WordTokenizer(vocabulary) \n```", "```py\n>>> print(\"Vocabulary length:\", len(vocabulary))\nVocabulary length: 2000\n>>> print(\"Vocabulary start:\", list(vocabulary.keys())[:5])\nVocabulary start: [\"[UNK]\", \",\", \"the\", \".\", \"of\"]\n>>> print(\"Vocabulary end:\", list(vocabulary.keys())[-5:])\nVocabulary end: [\"tambourine\", \"subtle\", \"perseus\", \"elevated\", \"repose\"]\n>>> print(\"Line length:\", len(word_tokenizer(\n...    \"Call me Ishmael. Some years ago--never mind how long precisely.\"\n... )))\nLine length: 13\n```", "```py\ndata = [\n    \"the quick brown fox\",\n    \"the slow brown fox\",\n    \"the quick brown foxhound\",\n] \n```", "```py\ndef count_and_split_words(data):\n    counts = collections.Counter()\n    for line in data:\n        line = line.lower()\n        for word in re.findall(r\"[\\w]+|[.,!?;]\", line):\n            chars = re.findall(r\".\", word)\n            split_word = \" \".join(chars)\n            counts[split_word] += 1\n    return dict(counts)\n\ncounts = count_and_split_words(data) \n```", "```py\n>>> counts\n{\"t h e\": 3,\n \"q u i c k\": 2,\n \"b r o w n\": 3,\n \"f o x\": 2,\n \"s l o w\": 1,\n \"f o x h o u n d\": 1}\n```", "```py\ndef count_pairs(counts):\n    pairs = collections.Counter()\n    for word, freq in counts.items():\n        symbols = word.split()\n        for pair in zip(symbols[:-1], symbols[1:]):\n            pairs[pair] += freq\n    return pairs\n\ndef merge_pair(counts, first, second):\n    # Matches an unmerged pair\n    split = re.compile(f\"(?<!\\S){first} {second}(?!\\S)\")\n    # Replaces all occurances with a merged version\n    merged = f\"{first}{second}\"\n    return {split.sub(merged, word): count for word, count in counts.items()}\n\nfor i in range(10):\n    pairs = count_pairs(counts)\n    first, second = max(pairs, key=pairs.get)\n    counts = merge_pair(counts, first, second)\n    print(list(counts.keys())) \n```", "```py\n[\"t h e\", \"q u i c k\", \"b r ow n\", \"f o x\", \"s l ow\", \"f o x h o u n d\"]\n[\"th e\", \"q u i c k\", \"b r ow n\", \"f o x\", \"s l ow\", \"f o x h o u n d\"]\n[\"the\", \"q u i c k\", \"b r ow n\", \"f o x\", \"s l ow\", \"f o x h o u n d\"]\n[\"the\", \"q u i c k\", \"br ow n\", \"f o x\", \"s l ow\", \"f o x h o u n d\"]\n[\"the\", \"q u i c k\", \"brow n\", \"f o x\", \"s l ow\", \"f o x h o u n d\"]\n[\"the\", \"q u i c k\", \"brown\", \"f o x\", \"s l ow\", \"f o x h o u n d\"]\n[\"the\", \"q u i c k\", \"brown\", \"fo x\", \"s l ow\", \"fo x h o u n d\"]\n[\"the\", \"q u i c k\", \"brown\", \"fox\", \"s l ow\", \"fox h o u n d\"]\n[\"the\", \"qu i c k\", \"brown\", \"fox\", \"s l ow\", \"fox h o u n d\"]\n[\"the\", \"qui c k\", \"brown\", \"fox\", \"s l ow\", \"fox h o u n d\"] \n```", "```py\ndef compute_sub_word_vocabulary(dataset, vocab_size):\n    counts = count_and_split_words(dataset)\n\n    char_counts = collections.Counter()\n    for word in counts:\n        for char in word.split():\n            char_counts[char] += counts[word]\n    most_common = char_counts.most_common()\n    vocab = [\"[UNK]\"] + [char for char, freq in most_common]\n    merges = []\n\n    while len(vocab) < vocab_size:\n        pairs = count_pairs(counts)\n        if not pairs:\n            break\n        first, second = max(pairs, key=pairs.get)\n        counts = merge_pair(counts, first, second)\n        vocab.append(f\"{first}{second}\")\n        merges.append(f\"{first} {second}\")\n\n    vocab = dict((token, index) for index, token in enumerate(vocab))\n    merges = dict((token, rank) for rank, token in enumerate(merges))\n    return vocab, merges \n```", "```py\nclass SubWordTokenizer:\n    def __init__(self, vocabulary, merges):\n        self.vocabulary = vocabulary\n        self.merges = merges\n        self.unk_id = vocabulary[\"[UNK]\"]\n\n    def standardize(self, inputs):\n        return inputs.lower()\n\n    def bpe_merge(self, word):\n        while True:\n            # Matches all symbol pairs in the text\n            pairs = re.findall(r\"(?<!\\S)\\S+ \\S+(?!\\S)\", word, overlapped=True)\n            if not pairs:\n                break\n            # We apply merge rules in \"rank\" order. More frequent pairs\n            # are merged first.\n            best = min(pairs, key=lambda pair: self.merges.get(pair, 1e9))\n            if best not in self.merges:\n                break\n            first, second = best.split()\n            split = re.compile(f\"(?<!\\S){first} {second}(?!\\S)\")\n            merged = f\"{first}{second}\"\n            word = split.sub(merged, word)\n        return word\n\n    def split(self, inputs):\n        tokens = []\n        # Split words\n        for word in re.findall(r\"[\\w]+|[.,!?;]\", inputs):\n            # Joins all characters with a space\n            word = \" \".join(re.findall(r\".\", word))\n            # Applies byte-pair encoding merge rules\n            word = self.bpe_merge(word)\n            tokens.extend(word.split())\n        return tokens\n\n    def index(self, tokens):\n        return [self.vocabulary.get(t, self.unk_id) for t in tokens]\n\n    def __call__(self, inputs):\n        inputs = self.standardize(inputs)\n        tokens = self.split(inputs)\n        indices = self.index(tokens)\n        return indices \n```", "```py\nvocabulary, merges = compute_sub_word_vocabulary(moby_dick, 2_000)\nsub_word_tokenizer = SubWordTokenizer(vocabulary, merges) \n```", "```py\n>>> print(\"Vocabulary length:\", len(vocabulary))\nVocabulary length: 2000\n>>> print(\"Vocabulary start:\", list(vocabulary.keys())[:10])\nVocabulary start: [\"[UNK]\", \"e\", \"t\", \"a\", \"o\", \"n\", \"i\", \"s\", \"h\", \"r\"]\n>>> print(\"Vocabulary end:\", list(vocabulary.keys())[-7:])\nVocabulary end: [\"bright\", \"pilot\", \"sco\", \"ben\", \"dem\", \"gale\", \"ilo\"]\n>>> print(\"Line length:\", len(sub_word_tokenizer(\n...    \"Call me Ishmael. Some years ago--never mind how long precisely.\"\n... )))\nLine length: 16\n```", "```py\nimport os, pathlib, shutil, random\n\nzip_path = keras.utils.get_file(\n    origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n    fname=\"imdb\",\n    extract=True,\n)\n\nimdb_extract_dir = pathlib.Path(zip_path) / \"aclImdb\" \n```", "```py\n>>> for path in imdb_extract_dir.glob(\"*/*\"):\n...     if path.is_dir():\n...         print(path)\n~/.keras/datasets/aclImdb/train/pos\n~/.keras/datasets/aclImdb/train/unsup\n~/.keras/datasets/aclImdb/train/neg\n~/.keras/datasets/aclImdb/test/pos\n~/.keras/datasets/aclImdb/test/neg\n```", "```py\n>>> print(open(imdb_extract_dir / \"train\" / \"pos\" / \"4077_10.txt\", \"r\").read())\nI first saw this back in the early 90s on UK TV, i did like it then but i missed\nthe chance to tape it, many years passed but the film always stuck with me and i\nlost hope of seeing it TV again, the main thing that stuck with me was the end,\nthe hole castle part really touched me, its easy to watch, has a great story,\ngreat music, the list goes on and on, its OK me saying how good it is but\neveryone will take there own best bits away with them once they have seen it,\nyes the animation is top notch and beautiful to watch, it does show its age in a\nvery few parts but that has now become part of it beauty, i am so glad it has\ncame out on DVD as it is one of my top 10 films of all time. Buy it or rent it\njust see it, best viewing is at night alone with drink and food in reach so you\ndon't have to stop the film.<br /><br />Enjoy\n```", "```py\ntrain_dir = pathlib.Path(\"imdb_train\")\ntest_dir = pathlib.Path(\"imdb_test\")\nval_dir = pathlib.Path(\"imdb_val\")\n\n# Moves the test data unaltered\nshutil.copytree(imdb_extract_dir / \"test\", test_dir)\n\n# Splits the training data into a train set and a validation set\nval_percentage = 0.2\nfor category in (\"neg\", \"pos\"):\n    src_dir = imdb_extract_dir / \"train\" / category\n    src_files = os.listdir(src_dir)\n    random.Random(1337).shuffle(src_files)\n    num_val_samples = int(len(src_files) * val_percentage)\n\n    os.makedirs(val_dir / category)\n    for file in src_files[:num_val_samples]:\n        shutil.copy(src_dir / file, val_dir / category / file)\n    os.makedirs(train_dir / category)\n    for file in src_files[num_val_samples:]:\n        shutil.copy(src_dir / file, train_dir / category / file) \n```", "```py\nfrom keras.utils import text_dataset_from_directory\n\nbatch_size = 32\ntrain_ds = text_dataset_from_directory(train_dir, batch_size=batch_size)\nval_ds = text_dataset_from_directory(val_dir, batch_size=batch_size)\ntest_ds = text_dataset_from_directory(test_dir, batch_size=batch_size) \n```", "```py\n\"this movie made me cry\"\n\n{\"[UNK]\": 0, \"movie\": 1, \"film\": 2, \"made\": 3, \"laugh\": 4, \"cry\": 5} \n```", "```py\n[0, 1, 3, 0, 5] \n```", "```py\n{0, 1, 3, 5} \n```", "```py\n[1, 1, 0, 1, 0, 1] \n```", "```py\nfrom keras import layers\n\nmax_tokens = 20_000\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    # Learns a word-level vocabulary\n    split=\"whitespace\",\n    output_mode=\"multi_hot\",\n)\ntrain_ds_no_labels = train_ds.map(lambda x, y: x)\ntext_vectorization.adapt(train_ds_no_labels)\n\nbag_of_words_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n)\nbag_of_words_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n)\nbag_of_words_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n) \n```", "```py\n>>> x, y = next(bag_of_words_train_ds.as_numpy_iterator())\n>>> x.shape\n(32, 20000)\n>>> y.shape\n(32, 1)\n```", "```py\ndef build_linear_classifier(max_tokens, name):\n    inputs = keras.Input(shape=(max_tokens,))\n    outputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\n    model = keras.Model(inputs, outputs, name=name)\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"binary_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\nmodel = build_linear_classifier(max_tokens, \"bag_of_words_classifier\") \n```", "```py\n>>> model.summary()\nModel: \"bag_of_words_classifier\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)          │ (None, 20000)            │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense (Dense)                     │ (None, 1)                │        20,001 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 20,001 (78.13 KB)\n Trainable params: 20,001 (78.13 KB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    restore_best_weights=True,\n    patience=2,\n)\nhistory = model.fit(\n    bag_of_words_train_ds,\n    validation_data=bag_of_words_val_ds,\n    epochs=10,\n    callbacks=[early_stopping],\n) \n```", "```py\nimport matplotlib.pyplot as plt\n\naccuracy = history.history[\"accuracy\"]\nval_accuracy = history.history[\"val_accuracy\"]\nepochs = range(1, len(accuracy) + 1)\n\nplt.plot(epochs, accuracy, \"r--\", label=\"Training accuracy\")\nplt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.show() \n```", "```py\n>>> test_loss, test_acc = model.evaluate(bag_of_words_test_ds)\n>>> test_acc\n0.88388\n```", "```py\nmax_tokens = 30_000\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    # Learns a word-level vocabulary\n    split=\"whitespace\",\n    output_mode=\"multi_hot\",\n    # Considers all unigrams and bigrams\n    ngrams=2,\n)\ntext_vectorization.adapt(train_ds_no_labels)\n\nbigram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n)\nbigram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n)\nbigram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n) \n```", "```py\n>>> x, y = next(bigram_train_ds.as_numpy_iterator())\n>>> x.shape\n(32, 30000)\n```", "```py\n>>> text_vectorization.get_vocabulary()[100:108]\n[\"in a\", \"most\", \"him\", \"dont\", \"it was\", \"one of\", \"for the\", \"made\"]\n```", "```py\nmodel = build_linear_classifier(max_tokens, \"bigram_classifier\")\nmodel.fit(\n    bigram_train_ds,\n    validation_data=bigram_val_ds,\n    epochs=10,\n    callbacks=[early_stopping],\n) \n```", "```py\n>>> test_loss, test_acc = model.evaluate(bigram_test_ds)\n>>> test_acc\n0.90116\n```", "```py\n\"the quick brown fox jumped over the lazy dog\"\n\n\"the slow brown badger\" \n```", "```py\n[\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\"]\n[\"the\", \"slow\", \"brown\", \"badger\", \"[PAD]\", \"[PAD]\", \"[PAD]\", \"[PAD]\"] \n```", "```py\nmax_length = 600\nmax_tokens = 30_000\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    # Learns a word-level vocabulary\n    split=\"whitespace\",\n    # Outputs a integer sequence of token IDs\n    output_mode=\"int\",\n    # Pads and truncates to 600 tokens\n    output_sequence_length=max_length,\n)\ntext_vectorization.adapt(train_ds_no_labels)\n\nsequence_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n)\nsequence_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n)\nsequence_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n) \n```", "```py\n>>> x, y = next(sequence_test_ds.as_numpy_iterator())\n>>> x.shape\n(32, 600)\n>>> x\narray([[   11,    29,     7, ...,     0,     0,     0],\n       [  132,   115,    35, ...,     0,     0,     0],\n       [ 1825,     3, 25819, ...,     0,     0,     0],\n       ...,\n       [    4,   576,    56, ...,     0,     0,     0],\n       [   30,   203,     4, ...,     0,     0,     0],\n       [ 5104,     1,    14, ...,     0,     0,     0]])\n```", "```py\nfrom keras import ops\n\nclass OneHotEncoding(keras.Layer):\n    def __init__(self, depth, **kwargs):\n        super().__init__(**kwargs)\n        self.depth = depth\n\n    def call(self, inputs):\n        # Flattens the inputs\n        flat_inputs = ops.reshape(ops.cast(inputs, \"int\"), [-1])\n        # Builds an identity matrix with all possible one-hot vectors\n        one_hot_vectors = ops.eye(self.depth)\n        # Uses our input token IDs to gather the correct vector for\n        # each token\n        outputs = ops.take(one_hot_vectors, flat_inputs, axis=0)\n        # Unflattens the output\n        return ops.reshape(outputs, ops.shape(inputs) + (self.depth,))\n\none_hot_encoding = OneHotEncoding(max_tokens) \n```", "```py\n>>> x, y = next(sequence_train_ds.as_numpy_iterator())\n>>> one_hot_encoding(x).shape\n(32, 600, 30000)\n```", "```py\nhidden_dim = 64\ninputs = keras.Input(shape=(max_length,), dtype=\"int32\")\nx = one_hot_encoding(inputs)\nx = layers.Bidirectional(layers.LSTM(hidden_dim))(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs, name=\"lstm_with_one_hot\")\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n) \n```", "```py\n>>> model.summary()\nModel: \"lstm_with_one_hot\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_2 (InputLayer)        │ (None, 600)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ one_hot_encoding (OneHotEncoding) │ (None, 600, 30000)       │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ bidirectional (Bidirectional)     │ (None, 128)              │    15,393,280 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout (Dropout)                 │ (None, 128)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ (None, 1)                │           129 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 15,393,409 (58.72 MB)\n Trainable params: 15,393,409 (58.72 MB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nmodel.fit(\n    sequence_train_ds,\n    validation_data=sequence_val_ds,\n    epochs=10,\n    callbacks=[early_stopping],\n) \n```", "```py\n>>> test_loss, test_acc = model.evaluate(sequence_test_ds)\n>>> test_acc\n0.84811\n```", "```py\nhidden_dim = 64\ninputs = keras.Input(shape=(max_length,), dtype=\"int32\")\nx = keras.layers.Embedding(\n    input_dim=max_tokens,\n    output_dim=hidden_dim,\n    mask_zero=True,\n)(inputs)\nx = keras.layers.Bidirectional(keras.layers.LSTM(hidden_dim))(x)\nx = keras.layers.Dropout(0.5)(x)\noutputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs, name=\"lstm_with_embedding\")\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n) \n```", "```py\n[\"the\", \"movie\", \"was\", \"awful\", \"[PAD]\", \"[PAD]\", \"[PAD]\", \"[PAD]\"] \n```", "```py\n>>> model.summary()\nModel: \"lstm_with_embedding\"\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_3         │ (None, 600)       │           0 │ -                  │\n│ (InputLayer)          │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ embedding (Embedding) │ (None, 600, 64)   │   1,920,000 │ input_layer_6[0][… │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ not_equal (NotEqual)  │ (None, 600)       │           0 │ input_layer_6[0][… │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ bidirectional_1       │ (None, 128)       │      66,048 │ embedding[0][0],   │\n│ (Bidirectional)       │                   │             │ not_equal[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dropout_1 (Dropout)   │ (None, 128)       │           0 │ bidirectional_2[0… │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_3 (Dense)       │ (None, 1)         │         129 │ dropout_2[0][0]    │\n└───────────────────────┴───────────────────┴─────────────┴────────────────────┘\n Total params: 1,986,177 (7.58 MB)\n Trainable params: 1,986,177 (7.58 MB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\n>>> model.fit(\n...     sequence_train_ds,\n...     validation_data=sequence_val_ds,\n...     epochs=10,\n...     callbacks=[early_stopping],\n... )\n>>> test_loss, test_acc = model.evaluate(sequence_test_ds)\n>>> test_acc\n0.8443599939346313\n```", "```py\nimdb_vocabulary = text_vectorization.get_vocabulary()\ntokenize_no_padding = keras.layers.TextVectorization(\n    vocabulary=imdb_vocabulary,\n    split=\"whitespace\",\n    output_mode=\"int\",\n) \n```", "```py\nimport tensorflow as tf\n\n# Words to the left or right of label\ncontext_size = 4\n# Total window size\nwindow_size = 9\n\ndef window_data(token_ids):\n    num_windows = tf.maximum(tf.size(token_ids) - context_size * 2, 0)\n    windows = tf.range(window_size)[None, :]\n    windows = windows + tf.range(num_windows)[:, None]\n    windowed_tokens = tf.gather(token_ids, windows)\n    return tf.data.Dataset.from_tensor_slices(windowed_tokens)\n\ndef split_label(window):\n    left = window[:context_size]\n    right = window[context_size + 1 :]\n    bag = tf.concat((left, right), axis=0)\n    label = window[4]\n    return bag, label\n\n# Uses all training data, including the unsup/ directory\ndataset = keras.utils.text_dataset_from_directory(\n    imdb_extract_dir / \"train\", batch_size=None\n)\n# Drops label\ndataset = dataset.map(lambda x, y: x, num_parallel_calls=8)\n# Tokenizes\ndataset = dataset.map(tokenize_no_padding, num_parallel_calls=8)\n# Creates context windows\ndataset = dataset.interleave(window_data, cycle_length=8, num_parallel_calls=8)\n# Splits middle wonder into a label\ndataset = dataset.map(split_label, num_parallel_calls=8) \n```", "```py\nhidden_dim = 64\ninputs = keras.Input(shape=(2 * context_size,))\ncbow_embedding = layers.Embedding(\n    max_tokens,\n    hidden_dim,\n)\nx = cbow_embedding(inputs)\nx = layers.GlobalAveragePooling1D()(x)\noutputs = layers.Dense(max_tokens, activation=\"sigmoid\")(x)\ncbow_model = keras.Model(inputs, outputs)\ncbow_model.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"sparse_categorical_accuracy\"],\n) \n```", "```py\n>>> cbow_model.summary()\nModel: \"functional_1\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_4 (InputLayer)        │ (None, 8)                │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ embedding_1 (Embedding)           │ (None, 8, 64)            │     1,920,000 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ global_average_pooling1d_2        │ (None, 64)               │             0 │\n│ (GlobalAveragePooling1D)          │                          │               │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_4 (Dense)                   │ (None, 30000)            │     1,950,000 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 3,870,000 (14.76 MB)\n Trainable params: 3,870,000 (14.76 MB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\ndataset = dataset.batch(1024).cache()\ncbow_model.fit(dataset, epochs=4) \n```", "```py\ninputs = keras.Input(shape=(max_length,))\nlstm_embedding = layers.Embedding(\n    input_dim=max_tokens,\n    output_dim=hidden_dim,\n    mask_zero=True,\n)\nx = lstm_embedding(inputs)\nx = layers.Bidirectional(layers.LSTM(hidden_dim))(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs, name=\"lstm_with_cbow\") \n```", "```py\nlstm_embedding.embeddings.assign(cbow_embedding.embeddings) \n```", "```py\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(\n    sequence_train_ds,\n    validation_data=sequence_val_ds,\n    epochs=10,\n    callbacks=[early_stopping],\n) \n```", "```py\n>>> test_loss, test_acc = model.evaluate(sequence_test_ds)\n>>> test_acc\n0.89139\n```"]