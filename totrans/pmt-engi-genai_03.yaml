- en: Chapter 3\. Standard Practices for Text Generation with ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple prompting techniques will help you to maximize the output and formats
    from LLMs. You’ll start by tailoring the prompts to explore all of the common
    practices used for text generation.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Lists
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatically generating lists is incredibly powerful and enables you to focus
    on higher-level tasks while GPT can automatically generate, refine, rerank, and
    de-duplicate lists on your behalf.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'GPT-4 is perfectly capable of providing a list of characters. However, there
    are some pitfalls with this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT has decided to provide 30 examples as a numbered list, separated by `\n`
    characters. However, if your downstream Python code was expecting to split on
    bullet points, then you’ll likely end up with undesirable results or a runtime
    error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT has provided preceding commentary; removing any preceding/succeeding commentary
    would make parsing the output easier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list size wasn’t controlled and was left to the language model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the characters have the name of their corresponding film within brackets—for
    example, *Bagheera (The Jungle Book)*—and others don’t. This makes names harder
    to extract because you would need to remove the movie titles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No filtering or selection has been applied to the LLM generation based on our
    desired result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following you’ll find an optimized prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Provide Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simply rephrasing your prompt to include examples *(few-shot prompting)* can
    greatly impact the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: 'By optimizing the prompt, you’ve achieved the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Restricted the list to a fixed size of five
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generated only male characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correctly formatted the list with bullet points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removed any preceding commentary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple lists are fine for most tasks; however, they are less structured, and
    for some tasks it’s beneficial to obtain nested data structures from a GPT-4 output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three typical data structures include:'
  prefs: []
  type: TYPE_NORMAL
- en: Nested text data (hierarchical lists)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YAML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical List Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hierarchical lists are useful for when your desired output is nested. A good
    example of this would be a detailed article structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To generate an effective article outline in the preceding output, you’ve included
    two key phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical
  prefs: []
  type: TYPE_NORMAL
- en: To suggest that the article outline needs to produce a nested structure.
  prefs: []
  type: TYPE_NORMAL
- en: Incredibly detailed
  prefs: []
  type: TYPE_NORMAL
- en: To guide the language model towards producing a larger output. Other words that
    you could include that have the same effect would be *very long* or by specifying
    a large number of subheadings, *include at least 10 top-level headings*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Asking a language model for a fixed number of items doesn’t guarantee the language
    model will produce the same length. For example, if you ask for 10 headings, you
    might receive only 8\. Therefore, your code should either validate that 10 headings
    exist or be flexible to handle varying lengths from the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: So you’ve successfully produced a hierarchical article outline, but how could
    you parse the string into structured data?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore [Example 3-1](#parsing_a_hierarchical_list_one) using Python,
    where you’ve previously made a successful API call against OpenAI’s GPT-4\. Two
    regular expressions are used to extract the headings and subheadings from `openai_result`.
    The `re` module in Python is used for working with regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-1\. [Parsing a hierarchical list](https://oreil.ly/A0otS)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The use of regular expressions allows for efficient pattern matching, making
    it possible to handle variations in the input text, such as the presence or absence
    of leading spaces or tabs. Let’s explore how these patterns work:'
  prefs: []
  type: TYPE_NORMAL
- en: '`heading_pattern = r''\* (.+)''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This pattern is designed to extract the main headings and consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: '`\*` matches the asterisk `(*)` symbol at the beginning of a heading. The backslash
    is used to escape the asterisk, as the asterisk has a special meaning in regular
    expressions (zero or more occurrences of the preceding character).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A space character will match after the asterisk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(.+)`: matches one or more characters, and the parentheses create a capturing
    group. The `.` is a wildcard that matches any character except a newline, and
    the `+` is a quantifier that means *one or more* occurrences of the preceding
    element (the dot, in this case).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By applying this pattern you can easily extract all of the main headings into
    a list without the asterisk.
  prefs: []
  type: TYPE_NORMAL
- en: '`subheading_pattern = r''\s+[a-z]\. (.+)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `subheading pattern` will match all of the subheadings within the `openai_result`
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '`\s+` matches one or more whitespace characters (spaces, tabs, and so on).
    The `+` means *one or more* occurrences of the preceding element (the `\s`, in
    this case).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[a-z]` matches a single lowercase letter from *a* to *z*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`\.` matches a period character. The backslash is used to escape the period,
    as it has a special meaning in regular expressions (matches any character except
    a newline).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A space character will match after the period.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(.+)` matches one or more characters, and the parentheses create a capturing
    group. The `.` is a wildcard that matches any character except a newline, and
    the `+` is a quantifier that means *one or more* occurrences of the preceding
    element (the dot, in this case).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally the `re.findall()` function is used to find all non-overlapping
    matches of the patterns in the input string and return them as a list. The extracted
    headings and subheadings are then printed.
  prefs: []
  type: TYPE_NORMAL
- en: So now you’re able to extract headings and subheadings from hierarchical article
    outlines; however, you can further refine the regular expressions so that each
    heading is associated with corresponding `subheadings`.
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 3-2](#parsing_a_hierarchical_list_two), the regex has been slightly
    modified so that each subheading is attached directly with its appropriate subheading.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-2\. [Parsing a hierarchical list into a Python dictionary](https://oreil.ly/LcMtv)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The section title regex, `r'\* (.+)'`, matches an asterisk followed by a space
    and then one or more characters. The parentheses capture the text following the
    asterisk and space to be used later in the code.
  prefs: []
  type: TYPE_NORMAL
- en: The subsection regex, `r'\s*([a-z]\..+)'`, starts with `\s*`, which matches
    zero or more whitespace characters (spaces or tabs). This allows the regex to
    match subsections with or without leading spaces or tabs. The following part,
    `([a-z]\..+)`, matches a lowercase letter followed by a period and then one or
    more characters. The parentheses capture the entire matched subsection text for
    later use in the code.
  prefs: []
  type: TYPE_NORMAL
- en: The `for` loop iterates over each line in the input string, `openai_result`.
    Upon encountering a line that matches the section title regex, the loop sets the
    matched title as the current section and assigns an empty list as its value in
    the `result_dict` dictionary. When a line matches the subsection regex, the matched
    subsection text is appended to the list corresponding to the current section.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, the loop processes the *input string line by line*, categorizes
    lines as section titles or subsections, and constructs the intended dictionary
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: When to Avoid Using Regular Expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you work to extract more structured data from LLM responses, relying solely
    on regular expressions can make the control flow *become increasingly complicated.*
    However, there are other formats that can facilitate the parsing of structured
    data from LLM responses with ease. Two common formats are *.json* and *.yml* files.
  prefs: []
  type: TYPE_NORMAL
- en: Generating JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start by experimenting with some prompt design that will direct an LLM
    to return a JSON response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Give Direction and Provide Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Notice that in the preceding prompt, you’ve provided direction on the type of
    task, the format, and an example JSON output.
  prefs: []
  type: TYPE_NORMAL
- en: Common errors that you’ll encounter when working with JSON involve invalid payloads,
    or the JSON being wrapped within triple backticks ([PRE12]
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure here''s the JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '{"Name": "John Smith"}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You must follow the following principles:'
  prefs: []
  type: TYPE_NORMAL
- en: '* Only return valid JSON'
  prefs: []
  type: TYPE_NORMAL
- en: '* Never include backtick symbols such as: `'
  prefs: []
  type: TYPE_NORMAL
- en: '* The response will be parsed with json.loads(), therefore it must be valid
    JSON.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: import json
  prefs: []
  type: TYPE_NORMAL
- en: openai_json_result = generate_article_outline(prompt)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: openai_json_result = """
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"Introduction": ['
  prefs: []
  type: TYPE_NORMAL
- en: '"a. Overview of coding and programming languages",'
  prefs: []
  type: TYPE_NORMAL
- en: '"b. Importance of coding in today''s technology-driven world"],'
  prefs: []
  type: TYPE_NORMAL
- en: '"Conclusion": ['
  prefs: []
  type: TYPE_NORMAL
- en: '"a. Recap of the benefits of learning code",'
  prefs: []
  type: TYPE_NORMAL
- en: '"b. The ongoing importance of coding skills in the modern world"]'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: parsed_json_payload = json.loads(openai_json_result)
  prefs: []
  type: TYPE_NORMAL
- en: print(parsed_json_payload)
  prefs: []
  type: TYPE_NORMAL
- en: '''''''{''Introduction'': [''a. Overview of coding and programming languages'','
  prefs: []
  type: TYPE_NORMAL
- en: '"b. Importance of coding in today''s technology-driven world"],'
  prefs: []
  type: TYPE_NORMAL
- en: '''Conclusion'': [''a. Recap of the benefits of learning code'','
  prefs: []
  type: TYPE_NORMAL
- en: '''b. The ongoing importance of coding skills in the modern world'']}'''''''
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '- Below you''ll find the current yaml schema.'
  prefs: []
  type: TYPE_NORMAL
- en: '- You can update the quantities based on a User Query.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Filter the User Query based on the schema below, if it doesn''t match and'
  prefs: []
  type: TYPE_NORMAL
- en: there are no items left then return `"No Items"`.
  prefs: []
  type: TYPE_NORMAL
- en: '- If there is a partial match, then return only the items that are'
  prefs: []
  type: TYPE_NORMAL
- en: 'within the schema below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'schema:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- item: Apple Slices'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 5'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: pieces'
  prefs: []
  type: TYPE_NORMAL
- en: '- item: Milk'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: gallon'
  prefs: []
  type: TYPE_NORMAL
- en: '- item: Bread'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: loaves'
  prefs: []
  type: TYPE_NORMAL
- en: '- item: Eggs'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: dozen'
  prefs: []
  type: TYPE_NORMAL
- en: 'User Query: "5 apple slices, and 2 dozen eggs."'
  prefs: []
  type: TYPE_NORMAL
- en: Given the schema below, please return only a valid .yml based on the User
  prefs: []
  type: TYPE_NORMAL
- en: Query.If there's no match, return `"No Items"`. Do not provide any
  prefs: []
  type: TYPE_NORMAL
- en: commentary or explanations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '- item: Apple Slices'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 5'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: pieces'
  prefs: []
  type: TYPE_NORMAL
- en: '- item: Eggs'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: dozen'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'User Query:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- item: Apple Slices'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 5'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: pieces'
  prefs: []
  type: TYPE_NORMAL
- en: '- item: Bananas'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: pieces'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Updated yaml list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- item: Apple Slices'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 5'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: pieces'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'User Query:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- item: Bananas'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: pieces'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: No Items
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'class InvalidResponse(Exception):'
  prefs: []
  type: TYPE_NORMAL
- en: pass
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'class InvalidItemType(Exception):'
  prefs: []
  type: TYPE_NORMAL
- en: pass
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'class InvalidItemKeys(Exception):'
  prefs: []
  type: TYPE_NORMAL
- en: pass
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'class InvalidItemName(Exception):'
  prefs: []
  type: TYPE_NORMAL
- en: pass
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'class InvalidItemQuantity(Exception):'
  prefs: []
  type: TYPE_NORMAL
- en: pass
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'class InvalidItemUnit(Exception):'
  prefs: []
  type: TYPE_NORMAL
- en: pass
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Provided schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: schema = """
  prefs: []
  type: TYPE_NORMAL
- en: '- item: Apple Slices'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 5'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: pieces'
  prefs: []
  type: TYPE_NORMAL
- en: '- item: Milk'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: gallon'
  prefs: []
  type: TYPE_NORMAL
- en: '- item: Bread'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: loaves'
  prefs: []
  type: TYPE_NORMAL
- en: '- item: Eggs'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: dozen'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: import yaml
  prefs: []
  type: TYPE_NORMAL
- en: 'def validate_response(response, schema):'
  prefs: []
  type: TYPE_NORMAL
- en: Parse the schema
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: schema_parsed = yaml.safe_load(schema)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: maximum_quantity = 10
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Check if the response is a list
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'if not isinstance(response, list):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: raise InvalidResponse("Response is not a list")
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Check if each item in the list is a dictionary
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'for item in response:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if not isinstance(item, dict):'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: raise InvalidItemType('''Item is not a dictionary''')
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Check if each dictionary has the keys "item", "quantity", and "unit"
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'if not all(key in item for key in ("item", "quantity", "unit")):'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: raise InvalidItemKeys("Item does not have the correct keys")
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Check if the values associated with each key are the correct type
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'if not isinstance(item["item"], str):'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: raise InvalidItemName("Item name is not a string")
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if not isinstance(item["quantity"], int):'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: raise InvalidItemQuantity("Item quantity is not an integer")
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if not isinstance(item["unit"], str):'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: raise InvalidItemUnit("Item unit is not a string")
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Check if the values associated with each key are the correct value
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'if item["item"] not in [x["item"] for x in schema_parsed]:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: raise InvalidItemName("Item name is not in schema")
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if item["quantity"] >  maximum_quantity:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: raise InvalidItemQuantity(f'''Item quantity is greater than
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '{maximum_quantity}'''''')'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if item["unit"] not in ["pieces", "dozen"]:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: raise InvalidItemUnit("Item unit is not pieces or dozen")
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Fake responses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: fake_response_1 = """
  prefs: []
  type: TYPE_NORMAL
- en: '- item: Apple Slices'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 5'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: pieces'
  prefs: []
  type: TYPE_NORMAL
- en: '- item: Eggs'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: dozen'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: fake_response_2 = """
  prefs: []
  type: TYPE_NORMAL
- en: Updated yaml list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- item: Apple Slices'
  prefs: []
  type: TYPE_NORMAL
- en: 'quantity: 5'
  prefs: []
  type: TYPE_NORMAL
- en: 'unit: pieces'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: fake_response_3 = """Unmatched"""
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parse the fake responses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: response_1_parsed = yaml.safe_load(fake_response_1)
  prefs: []
  type: TYPE_NORMAL
- en: response_2_parsed = yaml.safe_load(fake_response_2)
  prefs: []
  type: TYPE_NORMAL
- en: response_3_parsed = yaml.safe_load(fake_response_3)
  prefs: []
  type: TYPE_NORMAL
- en: Validate the responses against the schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'try:'
  prefs: []
  type: TYPE_NORMAL
- en: validate_response(response_1_parsed, schema)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print("Response 1 is valid")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'except Exception as e:'
  prefs: []
  type: TYPE_NORMAL
- en: print("Response 1 is invalid:", str(e))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'try:'
  prefs: []
  type: TYPE_NORMAL
- en: validate_response(response_2_parsed, schema)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print("Response 2 is valid")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'except Exception as e:'
  prefs: []
  type: TYPE_NORMAL
- en: print("Response 2 is invalid:", str(e))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'try:'
  prefs: []
  type: TYPE_NORMAL
- en: validate_response(response_3_parsed, schema)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print("Response 3 is valid")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'except Exception as e:'
  prefs: []
  type: TYPE_NORMAL
- en: print("Response 3 is invalid:", str(e))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Response 1 is valid
  prefs: []
  type: TYPE_NORMAL
- en: Response 2 is valid
  prefs: []
  type: TYPE_NORMAL
- en: 'Response 3 is invalid: Response is not a list'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Create a flowchart diagram in mermaid syntax showing a simple food ordering
  prefs: []
  type: TYPE_NORMAL
- en: process that starts with "Choose food" and ends with "Pay for meal."
  prefs: []
  type: TYPE_NORMAL
- en: Include steps for "Add food to cart" and "Confirm cart."
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: graph TD
  prefs: []
  type: TYPE_NORMAL
- en: ChooseFood[Choose Food] --> AddToCart[Add to Cart]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: AddToCart --> ConfirmCart[Confirm Cart]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ConfirmCart --> PayForMeal[Pay for Meal]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a sample CSV data for five students with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: name, age, grade.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: name,age,grade
  prefs: []
  type: TYPE_NORMAL
- en: John,16,A
  prefs: []
  type: TYPE_NORMAL
- en: Alex,17,B
  prefs: []
  type: TYPE_NORMAL
- en: Emma,16,A
  prefs: []
  type: TYPE_NORMAL
- en: Sophia,16,B
  prefs: []
  type: TYPE_NORMAL
- en: Liam,17,A
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Explain the following text as if I was 5 years old. Explain it like I'm 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Text: This is a review regarding different types of cancer treatments.'
  prefs: []
  type: TYPE_NORMAL
- en: We aimed at analyzing the tumor microenvironment and the recent trends
  prefs: []
  type: TYPE_NORMAL
- en: for the therapeutic applications and effectiveness for several kinds
  prefs: []
  type: TYPE_NORMAL
- en: of cancers. Traditionally the cancer treatment was based on the
  prefs: []
  type: TYPE_NORMAL
- en: neoplastic cells.
  prefs: []
  type: TYPE_NORMAL
- en: '...(shortened for brevity)...'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Cancer is a sickness that makes your body's cells grow and divide in a way
  prefs: []
  type: TYPE_NORMAL
- en: that is not normal. Doctors have different ways to treat cancer, like
  prefs: []
  type: TYPE_NORMAL
- en: cutting it out, giving you medicine, or using special rays to kill the
  prefs: []
  type: TYPE_NORMAL
- en: cancer cells...
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Create some simple text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The sun gently rises, painting the sky with hues of orange and pink. Birds
  prefs: []
  type: TYPE_NORMAL
- en: chirp merrily as they welcome the new day. The cool breeze whispers through
  prefs: []
  type: TYPE_NORMAL
- en: the trees, carrying the scent of fresh grass and blooming flowers. Life
  prefs: []
  type: TYPE_NORMAL
- en: awakens with a sense of tranquility and promise.
  prefs: []
  type: TYPE_NORMAL
- en: '...(shortened for brevity)...'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now make the text incredibly complex and hard to read.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In the ethereal realm of celestial ascendancy, wherein the resplendent
  prefs: []
  type: TYPE_NORMAL
- en: orb of the sun commences its gradual ascent, an exquisite phenomenon
  prefs: []
  type: TYPE_NORMAL
- en: unfolds,engendering a symphony of chromatic brilliance upon the
  prefs: []
  type: TYPE_NORMAL
- en: expansive canopy of the firmament. Avian denizens, endowed with
  prefs: []
  type: TYPE_NORMAL
- en: mellifluous voices, bespeak their jubilant salutations, punctuating
  prefs: []
  type: TYPE_NORMAL
- en: the aural tapestry with notes of felicity. The zephyr, tenderly
  prefs: []
  type: TYPE_NORMAL
- en: wafting amidst arboreal sentinels, carries upon its gentle currents
  prefs: []
  type: TYPE_NORMAL
- en: the redolence of verdant blades and blossoming flora.
  prefs: []
  type: TYPE_NORMAL
- en: '...(shortened for brevity)...'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now transform the text into a simple, easy to read format that's in Spanish
  prefs: []
  type: TYPE_NORMAL
- en: ', make sure that it is very easy to understand and read. You must ensure'
  prefs: []
  type: TYPE_NORMAL
- en: that it is as simple to comprehend as possible.
  prefs: []
  type: TYPE_NORMAL
- en: '- Use simple language.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Make it concise.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: En un lugar mágico y celestial, cuando el sol empieza a salir, ocurre algo
  prefs: []
  type: TYPE_NORMAL
- en: hermoso en el cielo. Los pájaros cantan alegremente, llenando el aire con
  prefs: []
  type: TYPE_NORMAL
- en: sus dulces melodías. Una brisa suave sopla entre los árboles, llevando
  prefs: []
  type: TYPE_NORMAL
- en: consigo el olor fresco de la naturaleza. Todo cobra vida y promete momentos
  prefs: []
  type: TYPE_NORMAL
- en: felices. Nos invita a disfrutar de las cosas simples y a encontrar la
  prefs: []
  type: TYPE_NORMAL
- en: felicidad en los pequeños detalles. En este escenario encantador,
  prefs: []
  type: TYPE_NORMAL
- en: encontramos paz y aprendemos que hasta los momentos más simples pueden
  prefs: []
  type: TYPE_NORMAL
- en: ser mágicos.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Finally translate it back into English.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In a magical and celestial place, when the sun begins to rise, something
  prefs: []
  type: TYPE_NORMAL
- en: beautiful happens in the sky. The birds sing cheerfully, filling the air
  prefs: []
  type: TYPE_NORMAL
- en: with their sweet melodies. A gentle breeze blows among the trees, carrying
  prefs: []
  type: TYPE_NORMAL
- en: the fresh scent of nature. Everything comes alive and promises happy
  prefs: []
  type: TYPE_NORMAL
- en: moments. It invites us to enjoy the simple things and find happiness in the
  prefs: []
  type: TYPE_NORMAL
- en: little details. In this enchanting setting, we find peace and learn that
  prefs: []
  type: TYPE_NORMAL
- en: even the simplest moments can be magical.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Should I use MongoDB or PostgreSQL for my project?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: It's difficult to make a recommendation without knowing more about your
  prefs: []
  type: TYPE_NORMAL
- en: specific software requirements. Both MongoDB and PostgreSQL have their
  prefs: []
  type: TYPE_NORMAL
- en: advantages, but the best choice depends on the specific needs of your
  prefs: []
  type: TYPE_NORMAL
- en: project.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Should I use MongoDB or PostgreSQL for my project?
  prefs: []
  type: TYPE_NORMAL
- en: Provide a list of requirements that would help you to make an informed
  prefs: []
  type: TYPE_NORMAL
- en: and high-quality decision about which database solution I should choose?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: To make an informed decision between MongoDB and PostgreSQL for your
  prefs: []
  type: TYPE_NORMAL
- en: 'project, consider the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Data Structure: Evaluate the structure of your data. If your data'
  prefs: []
  type: TYPE_NORMAL
- en: is highly unstructured or schema-less, MongoDB's flexible document model
  prefs: []
  type: TYPE_NORMAL
- en: may be more suitable. On the other hand, if your data has a fixed schema
  prefs: []
  type: TYPE_NORMAL
- en: or requires complex relationships, PostgreSQL's relational model might
  prefs: []
  type: TYPE_NORMAL
- en: be a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: '10\. Cost: Assess the cost implications of each database solution. MongoDB'
  prefs: []
  type: TYPE_NORMAL
- en: offers both a community edition (free) and a commercial edition with
  prefs: []
  type: TYPE_NORMAL
- en: additional features and support.PostgreSQL is open-source and free to use,
  prefs: []
  type: TYPE_NORMAL
- en: with optional commercial support available from various vendors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Should I use MongoDB or PostgreSQL for my project?
  prefs: []
  type: TYPE_NORMAL
- en: 'Context:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Data Structure: Highly unstructured data with varying document formats'
  prefs: []
  type: TYPE_NORMAL
- en: 'and no fixed schema. Scalability: Project requires the ability to handle'
  prefs: []
  type: TYPE_NORMAL
- en: massive amounts of data and distribute it across multiple servers.
  prefs: []
  type: TYPE_NORMAL
- en: '- ACID Compliance: Strong data consistency and integrity are crucial for'
  prefs: []
  type: TYPE_NORMAL
- en: the project's financial transactions.
  prefs: []
  type: TYPE_NORMAL
- en: '- Querying Capabilities: Complex geospatial queries and aggregations on'
  prefs: []
  type: TYPE_NORMAL
- en: location-based data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Community and Ecosystem: Active community with numerous third-party'
  prefs: []
  type: TYPE_NORMAL
- en: libraries and tools specifically tailored for the project's industry.
  prefs: []
  type: TYPE_NORMAL
- en: '- Data Integrity and Constraints: Enforcing strict foreign key'
  prefs: []
  type: TYPE_NORMAL
- en: relationships and complex data validation rules.
  prefs: []
  type: TYPE_NORMAL
- en: '- Transactions and Concurrency: Requires handling high levels of'
  prefs: []
  type: TYPE_NORMAL
- en: concurrent read and write operations while maintaining data consistency.
  prefs: []
  type: TYPE_NORMAL
- en: '- Maturity and Stability: Prioritizing a well-established and extensively'
  prefs: []
  type: TYPE_NORMAL
- en: tested database for long-term stability.
  prefs: []
  type: TYPE_NORMAL
- en: '- Development and Operational Ease: Agile development approach with a need'
  prefs: []
  type: TYPE_NORMAL
- en: for rapid prototyping and iterative schema design.
  prefs: []
  type: TYPE_NORMAL
- en: '- Cost: Budget constraints necessitate using an open-source solution with'
  prefs: []
  type: TYPE_NORMAL
- en: no additional licensing or support costs.
  prefs: []
  type: TYPE_NORMAL
- en: If you need more context, please specify what would help you to make
  prefs: []
  type: TYPE_NORMAL
- en: a better decision.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Based on these considerations, both MongoDB and PostgreSQL have their
  prefs: []
  type: TYPE_NORMAL
- en: strengths, but given the emphasis on strong data consistency, complex
  prefs: []
  type: TYPE_NORMAL
- en: querying, and data integrity constraints, PostgreSQL seems to be a more
  prefs: []
  type: TYPE_NORMAL
- en: suitable choice for your project.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Analyze the following text and identify core features that will allow
  prefs: []
  type: TYPE_NORMAL
- en: further documents to imitate the same style, consistency, and formatting
  prefs: []
  type: TYPE_NORMAL
- en: as the document below.
  prefs: []
  type: TYPE_NORMAL
- en: Provide all of the features as a style writing guide that will be used
  prefs: []
  type: TYPE_NORMAL
- en: for future documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Analyze the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence is rapidly transforming the world around us,
  prefs: []
  type: TYPE_NORMAL
- en: opening up new possibilities and revolutionizing industries. From
  prefs: []
  type: TYPE_NORMAL
- en: health care to entertainment, AI is making our lives better and more
  prefs: []
  type: TYPE_NORMAL
- en: convenient in countless ways. However, with great power comes great
  prefs: []
  type: TYPE_NORMAL
- en: responsibility. We must ensure that AI technologies are developed and
  prefs: []
  type: TYPE_NORMAL
- en: deployed ethically and responsibly to protect our privacy and uphold our
  prefs: []
  type: TYPE_NORMAL
- en: values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Style Writing Guide:'
  prefs: []
  type: TYPE_NORMAL
- en: Tone of Voice
  prefs: []
  type: TYPE_NORMAL
- en: '- Maintain an informative and optimistic tone throughout the document.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Emphasize the positive impact of the subject matter while acknowledging'
  prefs: []
  type: TYPE_NORMAL
- en: any potential concerns or responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Length:'
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: 'Vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: 'Structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: 'Content:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Focus on the transformative nature and positive impacts of the subject'
  prefs: []
  type: TYPE_NORMAL
- en: matter.
  prefs: []
  type: TYPE_NORMAL
- en: '- Highlight specific industries or areas where the subject matter has'
  prefs: []
  type: TYPE_NORMAL
- en: made a difference.
  prefs: []
  type: TYPE_NORMAL
- en: '- Acknowledge any associated responsibilities or potential risks,'
  prefs: []
  type: TYPE_NORMAL
- en: emphasizing the need for ethical and responsible practices.
  prefs: []
  type: TYPE_NORMAL
- en: Stress the importance of privacy protection and upholding values in
  prefs: []
  type: TYPE_NORMAL
- en: relation to the subject matter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Analyze the following text to identify and list the entities mentioned:'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence is rapidly transforming the world around us,
  prefs: []
  type: TYPE_NORMAL
- en: opening up new possibilities and revolutionizing industries. From
  prefs: []
  type: TYPE_NORMAL
- en: health care to entertainment, AI is making our lives better and more
  prefs: []
  type: TYPE_NORMAL
- en: convenient in countless ways. However, with great power comes great
  prefs: []
  type: TYPE_NORMAL
- en: responsibility. We must ensure that AI technologies are developed
  prefs: []
  type: TYPE_NORMAL
- en: and deployed ethically and responsibly to protect our privacy and
  prefs: []
  type: TYPE_NORMAL
- en: uphold our values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Artificial Intelligence (AI)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Health care'
  prefs: []
  type: TYPE_NORMAL
- en: '- Entertainment'
  prefs: []
  type: TYPE_NORMAL
- en: '- Privacy'
  prefs: []
  type: TYPE_NORMAL
- en: '- Values'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Summarize the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: Whereas the party of the first part, hereinafter referred to as the
  prefs: []
  type: TYPE_NORMAL
- en: '"Purchaser," and the party of the second part, hereinafter referred to'
  prefs: []
  type: TYPE_NORMAL
- en: as the "Vendor," being fully cognizant of their respective legal
  prefs: []
  type: TYPE_NORMAL
- en: obligations and responsibilities pursuant to the applicable statutory
  prefs: []
  type: TYPE_NORMAL
- en: provisions and common law doctrines, have duly entered into a legally
  prefs: []
  type: TYPE_NORMAL
- en: binding agreement, hereinafter referred to as the "Agreement," for the
  prefs: []
  type: TYPE_NORMAL
- en: purpose of effectuating a transaction involving the transfer of goods,
  prefs: []
  type: TYPE_NORMAL
- en: hereinafter referred to as the "Goods," between the Purchaser and the
  prefs: []
  type: TYPE_NORMAL
- en: Vendor, subject to the terms and conditions set forth herein, the Parties
  prefs: []
  type: TYPE_NORMAL
- en: 'do hereby solemnly acknowledge and affirm the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: This is a legally binding agreement between the Purchaser and the Vendor
  prefs: []
  type: TYPE_NORMAL
- en: for the transfer of goods. The agreement incorporates the recitals as an
  prefs: []
  type: TYPE_NORMAL
- en: integral part, but in case of any conflict, the operative provisions of the
  prefs: []
  type: TYPE_NORMAL
- en: agreement prevail. The interpretation clause states that singular words
  prefs: []
  type: TYPE_NORMAL
- en: include the plural and vice versa, and words referring to a gender include
  prefs: []
  type: TYPE_NORMAL
- en: all genders. References to statutory provisions include any amendments or
  prefs: []
  type: TYPE_NORMAL
- en: replacements.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The local council has decided to increase the budget for education by 10%
  prefs: []
  type: TYPE_NORMAL
- en: this year, a move that has been welcomed by parents and teachers alike. The
  prefs: []
  type: TYPE_NORMAL
- en: additional funds will be used to improve school infrastructure, hire more
  prefs: []
  type: TYPE_NORMAL
- en: teachers, and provide better resources for students. However, some critics
  prefs: []
  type: TYPE_NORMAL
- en: argue that the increase is not enough to address the growing demands of the
  prefs: []
  type: TYPE_NORMAL
- en: education system.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '["The", "local", "council", "has", "decided", "to", "increase", "the",'
  prefs: []
  type: TYPE_NORMAL
- en: '"budget", ...]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '["""The local council has decided to increase the budget for education'
  prefs: []
  type: TYPE_NORMAL
- en: by 10% this year, a move that has been welcomed by parents and teachers alike.
  prefs: []
  type: TYPE_NORMAL
- en: '""",'
  prefs: []
  type: TYPE_NORMAL
- en: '"""The additional funds will be used to improve school infrastructure,'
  prefs: []
  type: TYPE_NORMAL
- en: hire more teachers, and provide better resources for students.""",
  prefs: []
  type: TYPE_NORMAL
- en: '""""However, some critics argue that the increase is not enough to'
  prefs: []
  type: TYPE_NORMAL
- en: address the growing demands of the education system."""]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: import spacy
  prefs: []
  type: TYPE_NORMAL
- en: nlp = spacy.load("en_core_web_sm")
  prefs: []
  type: TYPE_NORMAL
- en: text = "This is a sentence. This is another sentence."
  prefs: []
  type: TYPE_NORMAL
- en: doc = nlp(text)
  prefs: []
  type: TYPE_NORMAL
- en: 'for sent in doc.sents:'
  prefs: []
  type: TYPE_NORMAL
- en: print(sent.text)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: This is a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: This is another sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'with open("hubspot_blog_post.txt", "r") as f:'
  prefs: []
  type: TYPE_NORMAL
- en: text = f.read()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'chunks = [text[i : i + 200] for i in range(0, len(text), 200)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'for chunk in chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: print("-" * 20)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print(chunk)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: search engine optimization strategy for many local businesses is an optimized
  prefs: []
  type: TYPE_NORMAL
- en: Google My Business profile to appear in local search results when people look
    for
  prefs: []
  type: TYPE_NORMAL
- en: products or services related to what yo
  prefs: []
  type: TYPE_NORMAL
- en: '--------------------'
  prefs: []
  type: TYPE_NORMAL
- en: u offer.
  prefs: []
  type: TYPE_NORMAL
- en: For Keeps Bookstore, a local bookstore in Atlanta, GA, has optimized its
  prefs: []
  type: TYPE_NORMAL
- en: Google My Business profile for local SEO so it appears in queries for
  prefs: []
  type: TYPE_NORMAL
- en: “atlanta bookstore.”
  prefs: []
  type: TYPE_NORMAL
- en: '--------------------'
  prefs: []
  type: TYPE_NORMAL
- en: '...(shortened for brevity)...'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'def sliding_window(text, window_size, step_size):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if window_size > len(text) or step_size < 1:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return []
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return [text[i:i+window_size] for i
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: in range(0, len(text) - window_size + 1, step_size)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: text = "This is an example of sliding window text chunking."
  prefs: []
  type: TYPE_NORMAL
- en: window_size = 20
  prefs: []
  type: TYPE_NORMAL
- en: step_size = 5
  prefs: []
  type: TYPE_NORMAL
- en: chunks = sliding_window(text, window_size, step_size)
  prefs: []
  type: TYPE_NORMAL
- en: 'for idx, chunk in enumerate(chunks):'
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"Chunk {idx + 1}: {chunk}")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Chunk 1: This is an example o'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunk 2: is an example of sli'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunk 3:  example of sliding'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunk 4: ple of sliding windo'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunk 5: f sliding window tex'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunk 6: ding window text chu'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunk 7: window text chunking'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '1\. Import the package:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import tiktoken
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Load an encoding with tiktoken.get_encoding()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: encoding = tiktoken.get_encoding("cl100k_base")
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Turn some text into tokens with encoding.encode()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: while turning tokens into text with encoding.decode()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: print(encoding.encode("Learning how to use Tiktoken is fun!"))
  prefs: []
  type: TYPE_NORMAL
- en: print(encoding.decode([1061, 15009, 374, 264, 2294, 1648,
  prefs: []
  type: TYPE_NORMAL
- en: 311, 4048, 922, 15592, 0]))
  prefs: []
  type: TYPE_NORMAL
- en: '[48567, 1268, 311, 1005, 73842, 5963, 374, 2523, 0]'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Data engineering is a great way to learn about AI!"'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'def count_tokens(text_string: str, encoding_name: str) -> int:'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns the number of tokens in a text string using a given encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Args:'
  prefs: []
  type: TYPE_NORMAL
- en: 'text: The text string to be tokenized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'encoding_name: The name of the encoding to be used for tokenization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of tokens in the text string.
  prefs: []
  type: TYPE_NORMAL
- en: 'Raises:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ValueError: If the encoding name is not recognized.'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: encoding = tiktoken.get_encoding(encoding_name)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: num_tokens = len(encoding.encode(text_string))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return num_tokens
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4\. Use the function to count the number of tokens in a text string.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: text_string = "Hello world! This is a test."
  prefs: []
  type: TYPE_NORMAL
- en: print(count_tokens(text_string, "cl100k_base"))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0613"):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Return the number of tokens used by a list of messages."""'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'try:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: encoding = tiktoken.encoding_for_model(model)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'except KeyError:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'print("Warning: model not found. Using cl100k_base encoding.")'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: encoding = tiktoken.get_encoding("cl100k_base")
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: if model in {
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"gpt-3.5-turbo-0613",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"gpt-3.5-turbo-16k-0613",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"gpt-4-0314",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"gpt-4-32k-0314",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"gpt-4-0613",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"gpt-4-32k-0613",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: tokens_per_message = 3
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: tokens_per_name = 1
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'elif model == "gpt-3.5-turbo-0301":'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'tokens_per_message = 4  # every message follows'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: <|start|>{role/name}\n{content}<|end|>\n
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'tokens_per_name = -1  # if there''s a name, the role is omitted'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'elif "gpt-3.5-turbo" in model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'print(''''''Warning: gpt-3.5-turbo may update over time. Returning'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: num tokens assuming gpt-3.5-turbo-0613.''')
  prefs: []
  type: TYPE_NORMAL
- en: return num_tokens_from_messages(messages, model="gpt-3.5-turbo-0613")
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'elif "gpt-4" in model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'print(''''''Warning: gpt-4 may update over time.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Returning num tokens assuming gpt-4-0613.''')
  prefs: []
  type: TYPE_NORMAL
- en: return num_tokens_from_messages(messages, model="gpt-4-0613")
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'else:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: raise NotImplementedError(
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: f"""num_tokens_from_messages() is not implemented for model
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '{model}."""'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: num_tokens = 0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'for message in messages:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: num_tokens += tokens_per_message
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'for key, value in message.items():'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: num_tokens += len(encoding.encode(value))
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if key == "name":'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: num_tokens += tokens_per_name
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'num_tokens += 3  # every reply is primed with'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <|start|>assistant<|message|>
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: return num_tokens
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: example_messages = [
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"role": "system",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"content": ''''''You are a helpful, pattern-following assistant that'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: translates corporate jargon into plain English.''',
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"role": "system",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"name": "example_user",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"content": "New synergies will help drive top-line growth.",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '},'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"role": "system",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"name": "example_assistant",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"content": "Things working well together will increase revenue.",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '},'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"role": "system",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"name": "example_user",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"content": ''''''Let''s circle back when we have more bandwidth to touch'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: base on opportunities for increased leverage.''',
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"role": "system",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"name": "example_assistant",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"content": ''''''Let''s talk later when we''re less busy about how to'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: do better.''',
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"role": "user",'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"content": ''''''This late pivot means we don''t have'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: time to boil the ocean for the client deliverable.''',
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ']'
  prefs: []
  type: TYPE_NORMAL
- en: 'for model in ["gpt-3.5-turbo-0301", "gpt-4-0314"]:'
  prefs: []
  type: TYPE_NORMAL
- en: print(model)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: example token count from the function defined above
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: print(f'''{num_tokens_from_messages(example_messages, model)} `prompt tokens
    counted by num_tokens_from_messages().'''``)`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]`# Sentiment Analysis    *Sentiment analysis* is a widely used NLP technique
    that helps in identifying, extracting, and understanding the emotions, opinions,
    or sentiments expressed in a piece of text. By leveraging the power of LLMs like
    GPT-4, sentiment analysis has become an essential tool for businesses, researchers,
    and developers across various industries.    The primary goal of sentiment analysis
    is to determine the attitude or emotional tone conveyed in a text, whether it’s
    positive, negative, or neutral. This information can provide valuable insights
    into consumer opinions about products or services, help monitor brand reputation,
    and even assist in predicting market trends.    The following are several prompt
    engineering techniques for creating effective sentiment analysis prompts:    Input:    [PRE68]    Output:    [PRE69]    Although
    GPT-4 identifies a “mixed tone,” the outcome is a result of several shortcomings
    in the prompt:    Lack of clarity      The prompt does not clearly define the
    desired output format.      Insufficient examples      The prompt does not include
    any examples of positive, negative, or neutral sentiments, which could help guide
    the LLM in understanding the distinctions between them.      No guidance on handling
    mixed sentiments      The prompt does not specify how to handle cases where the
    text contains a mix of positive and negative sentiments.      Input:    [PRE70]    Output:    [PRE71]    This
    prompt is much better because it:    Provides clear instructions      The prompt
    clearly states the task, which is to classify the sentiment of the given text
    into one of three categories: positive, negative, or neutral.      Offers examples      The
    prompt provides examples for each of the sentiment categories, which helps in
    understanding the context and desired output.      Defines the output format      The
    prompt specifies that the output should be a single word, ensuring that the response
    is concise and easy to understand.      ## Techniques for Improving Sentiment
    Analysis    To enhance sentiment analysis accuracy, preprocessing the input text
    is a vital step. This involves the following:    Special characters removal      Exceptional
    characters such as emojis, hashtags, and punctuation may skew the rule-based sentiment
    algorithm’s judgment. Besides, these characters might not be recognized by machine
    learning and deep learning models, resulting in misclassification.      Lowercase
    conversion      Converting all the characters to lowercase aids in creating uniformity.
    For instance, words like *Happy* and *happy* are treated as different words by
    models, which can cause duplication and inaccuracies.      Spelling correction      Spelling
    errors can cause misinterpretation and misclassification. Creating a spell-check
    pipeline can significantly reduce such errors and improve results.      For industry-
    or domain-specific text, embedding domain-specific content in the prompt helps
    in navigating the LLM’s sense of the text’s framework and sentiment. It enhances
    accuracy in the classification and provides a heightened understanding of particular
    jargon and expressions.    ## Limitations and Challenges in Sentiment Analysis    Despite
    the advancements in LLMs and the application of prompt engineering techniques,
    sentiment analysis still faces some limitations and challenges:    Handling sarcasm
    and irony      Detecting sarcasm and irony in text can be difficult for LLMs,
    as it often requires understanding the context and subtle cues that humans can
    easily recognize. Misinterpreting sarcastic or ironic statements may lead to inaccurate
    sentiment classification.      Identifying context-specific sentiment      Sentiment
    analysis can be challenging when dealing with context-specific sentiments, such
    as those related to domain-specific jargon or cultural expressions. LLMs may struggle
    to accurately classify sentiments in these cases without proper guidance or domain-specific
    examples.      # Least to Most    The *least to most* technique in prompt engineering
    is a powerful method for sequentially generating or extracting increasingly detailed
    knowledge on a given topic. This method is particularly effective when dealing
    with complex subjects or when a high level of detail is necessary.    Least to
    most uses a *chain* of prompts where each new prompt is based on the last answer.
    This step-by-step approach helps gather more detailed information each time, making
    it easier to dive deeper into any topic.    This technique can also be applied
    to code generation, as demonstrated in a Flask `Hello World` app example.    ##
    Planning the Architecture    Before diving into the architecture, let’s briefly
    understand what Flask is. [Flask](https://oreil.ly/7N-bs) is a lightweight web
    application framework in Python, widely used for creating web applications quickly
    and with minimal code. (Flask is only used for demonstration purposes here and
    isn’t included within the [*requirements.txt* file](https://oreil.ly/TRK0i) for
    the book.    Now, let’s ask an LLM to outline the basic architecture for a simple
    Flask “Hello World” application.    Input:    [PRE72]    Output:    [PRE73]    ##
    Coding Individual Functions    Before coding, let’s clarify what a Flask route
    is: it’s a function linked to a URL pattern that determines what users see on
    a specific web page. Next, we’ll provide the code snippet for a simple “Hello
    World” route in Flask.    Input:    [PRE74]    Output:    [PRE75]    In this code,
    we import the Flask module and create a Flask application instance named `app`.
    We then define a route using the `@app.route` decorator, specifying the root URL
    path `''/''`. The `hello_world` function is associated with this route, and it
    returns the string `''Hello, World!''` as the response.    The Flask application
    will start by calling `app.run()` when your Python script is executed directly.    ##
    Adding Tests    Finally, let’s ask it to generate a test case for the “Hello World”
    route in the Flask application.    Input:    [PRE76]    Output:    [PRE77]    By
    using the least to most prompt technique, you can gradually build up the knowledge
    and understanding required for the task at hand.    ## Benefits of the Least to
    Most Technique    This method is particularly useful for complex tasks, as it
    allows an LLM to generate relevant knowledge that will subsequently be used as
    context for future tasks.    Let’s dive deeper into the benefits of using this
    approach in various other scenarios:    Progressive exploration      Breaking
    a complex problem into smaller tasks allows an LLM to provide more detailed and
    accurate information at each step. This approach is especially helpful when working
    with a new subject matter or a multifaceted problem.      Flexibility      The
    least to most technique offers flexibility in addressing different aspects of
    a problem. It enables you to pivot, explore alternative solutions, or dive deeper
    into specific areas as needed.      Improved comprehension      By breaking down
    a task into smaller steps, an LLM can deliver information in a more digestible
    format, making it easier for you to understand and follow.      Collaborative
    learning      This technique promotes collaboration between you and an LLM, as
    it encourages an iterative process of refining the output and adjusting your responses
    to achieve the desired outcome.      ## Challenges with the Least to Most Technique    Overreliance
    on previously generated knowledge      Using previous chat history to store the
    state may lead to larger tasks forgetting their initial tasks/outputs due to limitations
    in context length.      Dependence on prior prompts      Since each prompt is
    built upon preceding outputs, it is imperative to ensure that the quality and
    responses of previous prompts provide ample information for the next step.      #
    Evaluate Quality    In the process of designing prompts, make sure to evaluate
    the quality of each prior LLM response. The performance of the next task depends
    upon the quality of information provided by the preceding one.    # Role Prompting    *Role
    prompting* is a technique in which the AI is given a specific role or character
    to assume while generating a response. This helps guide the AI’s response style
    and content to better align with the user’s needs or desired outcome.    ######
    Note    [Awesome ChatGPT prompts](https://oreil.ly/8pf40) provides an extensive
    list of role prompts that you can use.    Let’s ask ChatGPT to take on the role
    of a tech reviewer for MongoDB.    Input:    [PRE78]    Output:    [PRE79]    The
    prompt is an excellent example of role prompting, as it clearly defines the role
    the AI should assume (a tech reviewer) and sets expectations for the type of response
    desired (an in-depth review of MongoDB).    # Give Direction    When crafting
    prompts, consider assigning a specific role to the AI. This sets the proper context
    for the response, creating a more focused and relevant output.    # Benefits of
    Role Prompting    Role prompting helps narrow down the AI’s responses, ensuring
    more focused, contextually appropriate, and tailored results. It can also enhance
    creativity by pushing the AI to think and respond from unique perspectives.    #
    Challenges of Role Prompting    Role prompting can pose certain challenges. There
    might be potential risks for bias or stereotyping based on the role assigned.
    Assigning stereotyped roles can lead to generating biased responses, which could
    harm usability or offend individuals. Additionally, maintaining consistency in
    the role throughout an extended interaction can be difficult. The model might
    drift off-topic or respond with information irrelevant to the assigned role.    #
    Evaluate Quality    Consistently check the quality of the LLM’s responses, especially
    when role prompting is in play. Monitor if the AI is sticking to the role assigned
    or if it is veering off-topic.    # When to Use Role Prompting    Role prompting
    is particularly useful when you want to:    Elicit specific expertise      If
    you need a response that requires domain knowledge or specialized expertise, role
    prompting can help guide the LLM to generate more informed and accurate responses.      Tailor
    response style      Assigning a role can help an LLM generate responses that match
    a specific tone, style, or perspective, such as a formal, casual, or humorous
    response.      Encourage creative responses      Role prompting can be used to
    create fictional scenarios or generate imaginative answers by assigning roles
    like a storyteller, a character from a novel, or a historical figure.    *   *Explore
    diverse perspectives*: If you want to explore different viewpoints on a topic,
    role prompting can help by asking the AI to assume various roles or personas,
    allowing for a more comprehensive understanding of the subject.           *   *Enhance
    user engagement*: Role prompting can make interactions more engaging and entertaining
    by enabling an LLM to take on characters or personas that resonate with the user.                If
    you’re using OpenAI, then the best place to add a role is within the `System Message`
    for chat models.    # GPT Prompting Tactics    So far you’ve already covered several
    prompting tactics, including asking for context, text style bundling, least to
    most, and role prompting.    Let’s cover several more tactics, from managing potential
    hallucinations with appropriate reference text, to providing an LLM with critical
    *thinking time*, to understanding the concept of *task decomposition*—we have
    plenty for you to explore.    These methodologies have been designed to significantly
    boost the precision of your AI’s output and are recommended by [OpenAI](https://oreil.ly/QZE8n).
    Also, each tactic utilizes one or more of the prompt engineering principles discussed
    in [Chapter 1](ch01.html#five_principles_01).    ## Avoiding Hallucinations with
    Reference    The first method for avoiding text-based hallucinations is to instruct
    the model to *only answer using reference text.*    By supplying an AI model with
    accurate and relevant information about a given query, the model can be directed
    to use this information to generate its response.    Input:    [PRE80]    Output:    [PRE81]    If
    you were to ask the same reference text this question:    Input:    [PRE82]    Output:    [PRE83]    #
    Give Direction and Specify Format    The preceding prompt is excellent as it both
    instructs the model on how to find answers and also sets a specific response format
    for any unanswerable questions.    Considering the constrained context windows
    of GPTs, a method for dynamically retrieving information relevant to the asked
    query might be necessary to utilize this strategy.    Another approach is to direct
    the model to *incorporate references* from a given text in its response. When
    the input is enhanced with relevant information, the model can be guided to include
    citations in its responses by referring to sections of the supplied documents.
    This approach has the added benefit that citations in the output can be *authenticated
    automatically by matching strings* within the given documents.    Input:    [PRE84]    Output:    [PRE85]    ##
    Give GPTs “Thinking Time”    Often, by explicitly guiding an LLM to *derive solutions
    from first principles* before reaching a verdict, you can garner more accurate
    responses. Providing an LLM with *thinking time* can often lead to better results.    Input:    [PRE86]    Output:    [PRE87]    In
    some cases, when using GPT-4 or other generative AI models, you may not want the
    model’s reasoning process to be visible to the user.    You can achieve this by
    asking an LLM to *generate an inner monologue*. This is particularly useful in
    tutoring applications, where revealing the model’s reasoning might give away the
    solution prematurely.    ## The Inner Monologue Tactic    The *inner monologue
    tactic* instructs the model to structure parts of the output that should be hidden
    from the user in a specific format. This makes it easy to remove these parts before
    presenting the final output to the user.    Here’s how you can utilize this tactic
    to answer user queries effectively.    Input:    [PRE88]    Output:    [PRE89]    ##
    Self-Eval LLM Responses    Another tactic you can use is to *critque a generated
    LLM output* and ask whether the LLM missed any information or important facts.
    You’re essentially asking an LLM to *evaluate itself* based on its previous output.    First,
    create a prompt to generate a `"Hello World"` function.    Input:    [PRE90]    Output:    [PRE91]    The
    LLM returns a simple Python function called `print_hello_world()` that prints
    the traditional “Hello, World!” greeting.    Then make a second LLM request with
    the previous chat history, asking for the initial output to be improved.    Input:    [PRE92]    Output:    [PRE93]    Consequently
    GPT-4 returns an upgraded version of the function, now furnished with Python type
    hints and a default argument. This enriches the function with greater flexibility,
    allowing it to print not just `"Hello, World!"` but any user-specified message.    These
    prompt-response exchanges illustrate how you can easily refine generated LLM outputs
    until you’re satisfied with the final output.    ###### Note    It’s possible
    to critique an LLM’s response multiple times, until no further refinement is provided
    by the LLM.    # Classification with LLMs    Classifying, in the context of AI,
    refers to the process of predicting the class or category of a given data point
    or sample. It’s a common task in machine learning where models are trained to
    assign predefined labels to unlabeled data based on learned patterns.    LLMs
    are powerful assets when it comes to classification, even with zero or only a
    small number of examples provided within a prompt. Why? That’s because LLMs, like
    GPT-4, have been previously trained on an extensive dataset and now possess a
    degree of reasoning.    There are two overarching strategies in solving classification
    problems with LLMs: *zero-shot learning* and *few-shot learning*.    Zero-shot
    learning      In this process, the LLM classifies data with exceptional accuracy,
    without the aid of any prior specific examples. It’s akin to acing a project without
    any preparation—impressive, right?      Few-shot learning      Here, you provide
    your LLM with a small number of examples. This strategy can significantly influence
    the structure of your output format and enhance the overall classification accuracy.      Why
    is this groundbreaking for you?    Leveraging LLMs lets you sidestep lengthy processes
    that traditional machine learning processes demand. Therefore, you can quickly
    prototype a classification model, determine a base level accuracy, and create
    immediate business value.    ###### Warning    Although an LLM can perform classification,
    depending upon your problem and training data you might find that using a traditional
    machine learning process could yield better results.    # Building a Classification
    Model    Let’s explore a few-shot learning example to determine the sentiment
    of text into either `''Compliment''`, `''Complaint''`, or `''Neutral''`.    [PRE94]    [PRE95]    Several
    good use cases for LLM classification include:    Customer reviews      Classify
    user reviews into categories like “Positive,” “Negative,” or “Neutral.” Dive deeper
    by further identifying subthemes such as “Usability,” “Customer Support,” or “Price.”      Email
    filtering      Detect the intent or purpose of emails and classify them as “Inquiry,”
    “Complaint,” “Feedback,” or “Spam.” This can help businesses prioritize responses
    and manage communications efficiently.      Social media sentiment analysis      Monitor
    brand mentions and sentiment across social media platforms. Classify posts or
    comments as “Praise,” “Critic,” “Query,” or “Neutral.” Gain insights into public
    perception and adapt marketing or PR strategies accordingly.      News article
    categorization      Given the vast amount of news generated daily, LLMs can classify
    articles by themes or topics such as “Politics,” “Technology,” “Environment,”
    or “Entertainment.”      Résumé screening      For HR departments inundated with
    résumés, classify them based on predefined criteria like “Qualified,” “Overqualified,”
    “Underqualified,” or categorize by expertise areas such as “Software Development,”
    “Marketing,” or “Sales.”      ###### Warning    Be aware that exposing emails,
    résumés, or sensitive data does run the risk of data being leaked into OpenAI’s
    future models as training data.    # Majority Vote for Classification    Utilizing
    multiple LLM requests can help in reducing the variance of your classification
    labels. This process, known as *majority vote*, is somewhat like choosing the
    most common fruit out of a bunch. For instance, if you have 10 pieces of fruit
    and 6 out of them are apples, then apples are the majority. The same principle
    goes for choosing the majority vote in classification labels.    By soliciting
    several classifications and taking the *most frequent classification*, you’re
    able to reduce the impact of potential outliers or unusual interpretations from
    a single model inference. However, do bear in mind that there can be significant
    downsides to this approach, including the increased time required and cost for
    multiple API calls.    Let’s classify the same piece of text three times, and
    then take the majority vote:    [PRE96]    Calling the `most_frequent_classification(responses)`
    function should pinpoint `''Neutral''` as the dominant sentiment. You’ve now learned
    how to use the OpenAI package for majority vote classification.    # Criteria
    Evaluation    In [Chapter 1](ch01.html#five_principles_01), a human-based evaluation
    system was used with a simple thumbs-up/thumbs-down rating system to identify
    how often a response met our expectations. Rating manually can be expensive and
    tedious, requiring a qualified human to judge quality or identify errors. While
    this work can be outsourced to low-cost raters on services such as [Mechanical
    Turk](https://www.mturk.com), designing such a task in a way that gets valid results
    can itself be time-consuming and error prone. One increasingly common approach
    is to use a more sophisticated LLM to evaluate the responses of a smaller model.    The
    evidence is mixed on whether LLMs can act as effective evaluators, with some studies
    [claiming LLMs are human-level evaluators](https://oreil.ly/nfc3f) and others
    [identifying inconsistencies in how LLMs evaluate](https://oreil.ly/ykkzY). In
    our experience, GPT-4 is a useful evaluator with consistent results across a diverse
    set of tasks. In particular, GPT-4 is effective and reliable in evaluating the
    responses from smaller, less sophisticated models like GPT-3.5-turbo. In the example
    that follows, we generate concise and verbose examples of answers to a question
    using GPT-3.5-turbo, ready for rating with GPT-4.    Input:    [PRE97]    Output:    [PRE98]    This
    script is a Python program that interacts with the OpenAI API to generate and
    evaluate responses based on their conciseness. Here’s a step-by-step explanation:    1.  `responses
    = []` creates an empty list named `responses` to store the responses generated
    by the OpenAI API.           2.  The `for` loop runs 10 times, generating a response
    for each iteration.           3.  Inside the loop, `style` is determined based
    on the current iteration number (`i`). It alternates between “concise” and “verbose”
    for even and odd iterations, respectively.           4.  Depending on the `style`,
    a `prompt` string is formatted to ask, “What is the meaning of life?” in either
    a concise or verbose manner.           5.  `response = client.chat.completions.create(...)`
    makes a request to the OpenAI API to generate a response based on the `prompt`.
    The model used here is specified as “gpt-3.5-turbo.”           6.  The generated
    response is then stripped of any leading or trailing whitespace and added to the
    `responses` list.           7.  `system_prompt = """You are assessing..."""` sets
    up a prompt used for evaluating the conciseness of the generated responses.           8.  `ratings
    = []` initializes an empty list to store the conciseness ratings.           9.  Another
    `for` loop iterates over each response in `responses`.           10.  For each
    response, the script sends it along with the `system_prompt` to the OpenAI API,
    requesting a conciseness evaluation. This time, the model used is “gpt-4.”           11.  The
    evaluation rating (either 1 for concise or 0 for not concise) is then stripped
    of whitespace and added to the `ratings` list.           12.  The final `for`
    loop iterates over the `ratings` list. For each rating, it prints the `style`
    of the response (either “concise” or “verbose”) and its corresponding conciseness
    `rating`.              For simple ratings like conciseness, GPT-4 performs with
    near 100% accuracy; however, for more complex ratings, it’s important to spend
    some time evaluating the evaluator. For example, by setting test cases that contain
    an issue, as well as test cases that do not contain an issue, you can identify
    the accuracy of your evaluation metric. An evaluator can itself be evaluated by
    counting the number of false positives (when the LLM hallucinates an issue in
    a test case that is known not to contain an issue), as well as the number of false
    negatives (when the LLM misses an issue in a test case that is known to contain
    an issue). In our example we generated the concise and verbose examples, so we
    can easily check the rating accuracy, but in more complex examples you may need
    human evaluators to validate the ratings.    # Evaluate Quality    Using GPT-4
    to evaluate the responses of less sophisticated models is an emerging standard
    practice, but care must be taken that the results are reliable and consistent.    Compared
    to human-based evaluation, LLM-based or synthetic evaluation typically costs an
    order of magnitude less and completes in a few minutes rather than taking days
    or weeks. Even in important or sensitive cases where a final manual review by
    a human is necessary, rapid iteration and A/B testing of the prompt through synthetic
    reviews can save significant time and improve results considerably. However, the
    cost of running many tests at scale can add up, and the latency or rate limits
    of GPT-4 can be a blocker. If at all possible, a prompt engineer should first
    test using programmatic techniques that don’t require a call to an LLM, such as
    simply measuring the length of the response, which runs near instantly for close
    to zero cost.    # Meta Prompting    *Meta prompting* is a technique that involves
    the creation of text prompts that, in turn, generate other text prompts. These
    text prompts are then used to generate new assets in many mediums such as images,
    videos, and more text.    To better understand meta prompting, let’s take the
    example of authoring a children’s book with the assistance of GPT-4\. First, you
    direct the LLM to generate the text for your children’s book. Afterward, you invoke
    meta prompting by instructing GPT-4 to produce prompts that are suitable for image-generation
    models. This could mean creating situational descriptions or specific scenes based
    on the storyline of your book, which then can be given to AI models like Midjourney
    or Stable Diffusion. These image-generation models can, therefore, deliver images
    in harmony with your AI-crafted children’s story.    [Figure 3-8](#figure-3-8)
    visually describes the process of meta prompting in the context of crafting a
    children’s book.  ![Creating image prompts from an LLM that will later be used
    by MidJourney for image creation.](assets/pega_0308.png)  ###### Figure 3-8\.
    Utilizing an LLM to generate image prompts for MidJourney’s image creation in
    the process of crafting a children’s book    Meta prompts offer a multitude of
    benefits for a variety of applications:    Image generation from product descriptions      Meta
    prompts can be employed to derive an image generation prompt for image models
    like [Midjourney](https://www.midjourney.com), effectively creating a visual representation
    of product descriptions.      Generating style/feature prompts      Let’s consider
    you are a copywriter needing to develop a unique style guide prompt from a couple
    of blog posts. Given each client has a distinctive tone and style, it’s beneficial
    to utilize a *meta prompt* that encapsulates all the varied features, rather than
    producing a single prompt output.      Optimizing prompts to achieve specific
    goals      A common approach is to ask ChatGPT or another language model to refine
    or improve `Prompt A` in order to attain `Goal 1`, given `Prompt A` and `Goal
    1`. This method aids in identifying any missed input features, that by adding
    could assist the language model in generating a more detailed and desirable response.      Let’s
    experiment with creating a meta prompt for the copywriting example.    Input:    [PRE99]    Output:    [PRE100]    As
    previously mentioned, it can be beneficial to *critique the model’s output* and
    to ask for a refined prompt either via the interface or API.    ChatGPT can perform
    web searches and can make use of the content of web pages, along with the previously
    generated copywriting style guide. Let’s provide two URLs that ChatGPT will read
    to enhance the output.    Input:    [PRE101]    [Figure 3-9](#figure-3-9) shows
    the output.  ![pega 0309](assets/pega_0309.png)  ###### Figure 3-9\. ChatGPT refining
    a meta prompt by two URL web pages    [PRE102]    Meta prompting offers a dynamic
    and innovative way to harness the power of generative AI models, fostering the
    creation of complex, multifaceted prompts and even prompts that generate other
    prompts. It broadens the application scope, from text and image generation to
    style and feature prompts, and optimization toward specific goals. As you continue
    to refine and explore the potential of meta prompting, it promises to be a game
    changer in how you utilize, interact with, and benefit from using LLMs.    # Summary    After
    reading this chapter, you are now aware of how crucial it is to give clear directions
    and examples to generate desired outputs. Also, you have hands-on experience extracting
    structured data from a hierarchical list using regular expressions in Python,
    and you’ve learned to utilize nested data structures like JSON and YAML to produce
    robust, parsable outputs.    You’ve learned several best practices and effective
    prompt engineering techniques, including the famous “Explain it like I’m five”,
    role prompting, and meta prompting techniques. In the next chapter, you will learn
    how to use a popular LLM package called LangChain that’ll help you to create more
    advanced prompt engineering workflows.````'
  prefs: []
  type: TYPE_NORMAL
