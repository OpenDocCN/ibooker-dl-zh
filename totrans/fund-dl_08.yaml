- en: Chapter 8\. Embedding and Representation Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Lower-Dimensional Representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we motivated the convolutional architecture using a
    simple argument. The larger our input vector, the larger our model. Large models
    with lots of parameters are expressive, but they’re also increasingly data hungry.
    This means that without sufficiently large volumes of training data, we will likely
    overfit. Convolutional architectures help us cope with the curse of dimensionality
    by reducing the number of parameters in our models without necessarily diminishing
    expressiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless, convolutional networks still require large amounts of labeled training
    data. And for many problems, labeled data is scarce and expensive to generate.
    Our goal in this chapter will be to develop effective learning models in situations
    where labeled data is scarce, but wild, unlabeled data is plentiful. We’ll approach
    this problem by learning *embeddings*, or low-dimensional representations, in
    an unsupervised fashion. Because these unsupervised models allow us to offload
    all of the heavy lifting of automated feature selection, we can use the generated
    embeddings to solve learning problems using smaller models that require less data.
    This process is summarized in [Figure 8-1](#using_embeddings_to_automate_feature_selection).
  prefs: []
  type: TYPE_NORMAL
- en: In the process of developing algorithms that learn good embeddings, we’ll also
    explore other applications of learning lower-dimensional representations, such
    as visualization and semantic hashing. We’ll start by considering situations where
    all of the important information is already contained within the original input
    vector itself. In this case, learning embeddings is equivalent to developing an
    effective compression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Using embeddings to automate feature selection in the face of scarce
    labeled data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the next section, we’ll introduce *principal component analysis* (PCA), a
    classic method for dimensionality reduction. In subsequent sections, we’ll explore
    more powerful neural methods for learning compressive embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic concept behind PCA is to find a set of axes that communicates the
    most information about our dataset. More specifically, if we have  *d*-dimensional
    data, we’d like to find a new set of  <math alttext="m less-than d"><mrow><mi>m</mi>
    <mo><</mo> <mi>d</mi></mrow></math>  dimensions that conserves as much valuable
    information from the original dataset as possible. For simplicity, let’s choose 
    <math alttext="d equals 2 comma m equals 1"><mrow><mi>d</mi> <mo>=</mo> <mn>2</mn>
    <mo>,</mo> <mi>m</mi> <mo>=</mo> <mn>1</mn></mrow></math> . Assuming that variance
    corresponds to information, we can perform this transformation through an iterative
    process. First, we find a unit vector along which the dataset has maximum variance.
    Because this direction contains the most information, we select this direction
    as our first axis. Then from the set of vectors orthogonal to this first choice,
    we pick a new unit vector along which the dataset has maximum variance. This is
    our second axis.
  prefs: []
  type: TYPE_NORMAL
- en: We continue this process until we have found a total of *d* new vectors that
    represent new axes. We project our data onto this new set of axes. We then decide
    a good value for *m* and toss out all but the first *m* axes (the principal components,
    which store the most information). The result is shown in [Figure 8-2](#illustration_of_pca).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. An illustration of PCA for dimensionality reduction to capture
    the dimension with the most information (as proxied by variance)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For the mathematically inclined, we can view this operation as a projection
    onto the vector space spanned by the top *m* eigenvectors of the dataset’s correlation
    matrix, which is equivalent to the dataset’s covariance matrix when the dataset
    has been z-score normalized (zero-mean and unit-variance per input dimension).
    Let us represent the dataset as a matrix **X** with dimensions  <math alttext="n
    times d"><mrow><mi>n</mi> <mo>×</mo> <mi>d</mi></mrow></math> (i.e., *n* inputs
    of  *d* dimensions). We’d like to create an embedding matrix **T** with dimensions
    <math alttext="n times m"><mrow><mi>n</mi> <mo>×</mo> <mi>m</mi></mrow></math>
    . We can compute the matrix using the relationship **T** = **X**, where each column
    of **W** corresponds to an eigenvector of the matrix <math alttext="StartFraction
    1 Over n EndFraction"><mfrac><mn>1</mn> <mi>n</mi></mfrac></math> **X**^Τ**X**.
    Those with linear algebra background or core data science experience may be seeing
    a striking parallel between PCA and the singular value decomposition (SVD), which
    we cover in more depth in [“Theory: PCA and SVD”](#theory_sidebar).'
  prefs: []
  type: TYPE_NORMAL
- en: While PCA has been used for decades for dimensionality reduction, it spectacularly
    fails to capture important relationships that are piecewise linear or nonlinear.
    Take, for instance, the example illustrated in [Figure 8-3](#situation_in_which_pca).
  prefs: []
  type: TYPE_NORMAL
- en: The example shows data points selected at random from two concentric circles.
    We hope that PCA will transform this dataset so that we can pick a single new
    axis that allows us to easily separate the dots. Unfortunately for us, there is
    no linear direction that contains more information here than another (we have
    equal variance in all directions). Instead, as human beings, we notice that information
    is being encoded in a nonlinear way, in terms of how far points are from the origin.
    With this information in mind, we notice that the polar transformation (expressing
    points as their distance from the origin, as the new horizontal axis, and their
    angle bearing from the original x-axis, as the new vertical axis) does just the
    trick.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-3](#situation_in_which_pca) highlights the shortcomings of an approach
    like PCA in capturing important relationships in complex datasets. Because most
    of the datasets we are likely to encounter in the wild (images, text, etc.) are
    characterized by nonlinear relationships, we must develop a theory that will perform
    nonlinear dimensionality reduction. Deep learning practitioners have closed this
    gap using neural models, which we’ll cover in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. A situation in which PCA fails to optimally transform the data
    for dimensionality reduction
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Motivating the Autoencoder Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talked about feed-forward networks, we discussed how each layer learned
    progressively more relevant representations of the input. In fact, in [Chapter 7](ch07.xhtml#convolutional_neural_networks),
    we took the output of the final convolutional layer and used that as a lower-dimensional
    representation of the input image. Putting aside the fact that we want to generate
    these low-dimensional representations in an unsupervised fashion, there are fundamental
    problems with these approaches in general. Specifically, while the selected layer
    does contain information from the input, the network has been trained to pay attention
    to the aspects of the input that are critical to solving the task at hand. As
    a result, there’s a significant amount of information loss with respect to elements
    of the input that may be important for other classification tasks, but potentially
    less important than the one immediately at hand.
  prefs: []
  type: TYPE_NORMAL
- en: However, the fundamental intuition here still applies. We define a new network
    architecture that we call the *autoencoder*. We first take the input and compress
    it into a low-dimensional vector. This part of the network is called the *encoder*
    because it is responsible for producing the low-dimensional embedding or *code*.
    The second part of the network, instead of mapping the embedding to an arbitrary
    label as we would in a feed-forward network, tries to invert the computation of
    the first half of the network and reconstruct the original input. This piece is
    known as the *decoder*. The overall architecture is illustrated in [Figure 8-4](#autoencoder_architecture_attempts_to_construct).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. The autoencoder architecture attempts to construct a high-dimensional
    input into a low-dimensional embedding and then uses that low-dimensional embedding
    to reconstruct the input
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To demonstrate the surprising effectiveness of autoencoders, we’ll build and
    visualize the autoencoder architecture in [Figure 8-4](#autoencoder_architecture_attempts_to_construct).
    Specifically, we will highlight its superior ability to separate MNIST digits
    as compared to PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an Autoencoder in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The seminal paper “Reducing the Dimensionality of Data with Neural Networks,”
    which describes the autoencoder, was written by Hinton and Salakhutdinov in 2006.^([1](ch08.xhtml#idm45934168575888))
    Their hypothesis was that the nonlinear complexities afforded by a neural model
    would allow them to capture structure that linear methods, such as PCA, would
    miss. To demonstrate this point, they ran an experiment on MNIST using both an
    autoencoder and PCA to reduce the dataset into 2D data points. In this section,
    we will recreate their experimental setup to validate this hypothesis and further
    explore the architecture and properties of feed-forward autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: 'The setup shown in [Figure 8-5](#experimental_setup_for_dimensionality_reduction)
    is built with the same principle, but the 2D embedding is now treated as the input,
    and the network attempts to reconstruct the original image. Because we are essentially
    applying an inverse operation, we architect the decoder network so that the autoencoder
    has the shape of an hourglass. The output of the decoder network is a 784-dimensional
    vector that can be reconstructed into a 28 × 28 image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/fdl2_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. The experimental setup for dimensionality reduction of the MNIST
    dataset employed by Hinton and Salakhutdinov, 2006
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In order to accelerate training, we’ll reuse the batch normalization strategy
    we employed in [Chapter 7](ch07.xhtml#convolutional_neural_networks). Also, because
    we’d like to visualize the results, we’ll avoid introducing sharp transitions
    in our neurons. In this example, we’ll use sigmoidal neurons instead of our usual
    ReLU neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to construct a measure (or objective function) that describes
    how well our model functions. Specifically, we want to measure how close the reconstruction
    is to the original image. We can measure this simply by computing the distance
    between the original 784-dimensional input and the reconstructed 784-dimensional
    output. More specifically, given an input vector  <math alttext="upper I"><mi>I</mi></math>
     and a reconstruction  <math alttext="upper O"><mi>O</mi></math> , we’d like to
    minimize the value of  <math alttext="parallel-to upper I minus upper O parallel-to
    equals StartRoot sigma-summation Underscript i Endscripts left-parenthesis upper
    I Subscript i Baseline minus upper O Subscript i Baseline right-parenthesis squared
    EndRoot"><mrow><mrow><mo>∥</mo> <mi>I</mi> <mo>-</mo> <mi>O</mi> <mo>∥</mo></mrow>
    <mo>=</mo> <msqrt><mrow><msub><mo>∑</mo> <mi>i</mi></msub> <msup><mfenced separators=""
    open="(" close=")"><msub><mi>I</mi> <mi>i</mi></msub> <mo>-</mo><msub><mi>O</mi>
    <mi>i</mi></msub></mfenced> <mn>2</mn></msup></mrow></msqrt></mrow></math> , also
    known as the L2 norm of the difference between the two vectors. We average this
    function over the whole minibatch to generate our final objective function. Finally,
    we’ll train the network using the Adam optimizer, logging a scalar summary of
    the error incurred at every minibatch using `torch.utils.tensorboard.SummaryWriter`.
    In PyTorch, we can concisely express the loss and training operations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we’ll need a method to evaluate the generalizability of our model.
    As usual, we’ll use a validation dataset and compute the same L2 norm measurement
    for model evaluation. In addition, we’ll collect image summaries so that we can
    compare both the input images and the reconstructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize the model graph, the training and validation costs, and the
    image summaries using TensorBoard. Simply run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then navigate your browser to *http://localhost:6006/*. The results of the “Graph”
    tab are shown in [Figure 8-6](#tensorflow_allows_us_to_neatly_view).
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to how we’ve namespaced the components of our model graph, our model
    is nicely organized. We can easily click through the components and delve deeper,
    tracing how data flows up through the various layers of the encoder and through
    the decoder, how the optimizer reads the output of our training module, and how
    gradients in turn affect all of the components of the model.
  prefs: []
  type: TYPE_NORMAL
- en: We also visualize both the training (after each minibatch) and validation costs
    (after each epoch), closely monitoring the curves for potential overfitting. The
    TensorBoard visualizations of the costs over the span of training are shown in
    [Figure 8-7](#cost_incurred_on_the_training_set). As we would expect for a successful
    model, both the training and validation curves decrease until they flatten off
    asymptotically. After approximately 200 epochs, we attain a validation cost of
    4.78\. While the curves look promising, it’s difficult, upon first glance, to
    understand whether we’ve reached a plateau at a “good” cost, or whether our model
    is still doing a poor job of reconstructing the original inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. TensorBoard allows us to neatly view the high-level components
    and data flow of our computation graph (top) and also click through to more closely
    inspect the data flows of individual subcomponents (bottom)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. The cost incurred on the training set (logged after each minibatch)
    and on the validation set (logged after each epoch)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To get a sense of what that means, let’s explore the MNIST dataset. We pick
    an arbitrary image of a 1 from the dataset and call it *X*. In [Figure 8-8](#compared_to_all_other_digits),
    we compare the image to all other images in the dataset. Specifically, for each
    digit class, we compute the average of the L2 costs, comparing *X* to each instance
    of the digit class. As a visual aid, we also include the average of all of the
    instances for each digit class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. The image of the 1 on the left is compared to all of the other
    digits in the MNIST dataset; each digit class is represented visually with the
    average of all of its members and labeled with the average of the L2 costs, comparing
    the 1 on the left with all of the class members
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On average, *X* is 5.75 units away from other 1s in MNIST. In terms of L2 distance,
    the non-1 digits closest to the *X* are the 7s (8.94 units) and the digits farthest
    are the 0s (11.05 units). Given these measurements, it’s quite apparent that with
    an average cost of 4.78, our autoencoder is producing high-quality reconstructions.
  prefs: []
  type: TYPE_NORMAL
- en: Because we are collecting image summaries, we can confirm this hypothesis directly
    by inspecting the input images and reconstructions directly. The reconstructions
    for three randomly chosen samples from the test set are shown in [Figure 8-9](#side_by_side_comparison_of_original_inputs).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0809.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. A side-by-side comparison of the original inputs (from the validation
    set) and reconstructions after 5, 100, and 200 epochs of training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After five epochs, we can start to make out some of the critical strokes of
    the original image that are being picked by the autoencoder, but for the most
    part, the reconstructions are still hazy mixtures of closely related digits. By
    100 epochs, the 0 and 4 are reconstructed with strong strokes, but it looks like
    the autoencoder is still having trouble differentiating between 5s, 3s, and possibly
    8s. However, by 200 epochs, it’s clear that even this more difficult ambiguity
    is clarified, and all of the digits are crisply reconstructed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we’ll complete the section by exploring the 2D codes produced by traditional
    PCA and autoencoders. We’ll want to show that autoencoders produce better visualizations.
    In particular, we’ll want to show that autoencoders do a much better job of visually
    separating instances of different digit classes than PCA. We’ll start by quickly
    covering the code we use to produce 2D PCA codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We first pull up the MNIST dataset. We’ve set the flag `one_hot=False` because
    we’d like the labels to be provided as integers instead of one-hot vectors (as
    a quick reminder, a one-hot vector representing an MNIST label would be a vector
    of size 10 with the <math alttext="i Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    component set to one to represent digit <math alttext="i"><mi>i</mi></math> and
    the rest of the components set to zero). We use the commonly used machine learning
    library *scikit-learn* to perform the PCA, setting the `n_components=2` flat so
    that scikit-learn knows to generate 2D codes. We can also reconstruct the original
    images from the 2D codes and visualize the reconstructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The code snippet shows how to visualize the first image in the test dataset,
    but we can easily modify the code to visualize any arbitrary subset of the dataset.
    Comparing the PCA reconstructions to the autoencoder reconstructions in [Figure 8-10](#comparing_reconstructions_by_pca_and_autoencoder),
    it’s quite clear that the autoencoder vastly outperforms PCA with 2D codes. In
    fact, the PCA’s performance is somewhat reminiscent of the autoencoder only five
    epochs into training. It has trouble distinguishing 5s from 3s and 8s, 0s from
    8s, and 4s from 9s. Repeating the same experiment with 30-dimensional codes provides
    significant improvement to the PCA reconstructions, but they are still significantly
    worse than the 30-dimensional autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0810.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. Comparing the reconstructions by both PCA and autoencoder side
    by side
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, to complete the experiment, we must load up a saved PyTorch model, retrieve
    the 2D codes, and plot both the PCA and autoencoder codes. We’re careful to rebuild
    the PyTorch graph exactly how we set it up during training. We pass the path to
    the model checkpoint we saved during training as a command-line argument to the
    script. Finally, we use a custom plotting function to generate a legend and appropriately
    color data points of different digit classes.
  prefs: []
  type: TYPE_NORMAL
- en: In the resulting visualization in [Figure 8-11](#two_dimensional_embeddings_produced_by_pca),
    it is extremely difficult to make out separable clusters in the 2D PCA codes;
    the autoencoder has clearly done a spectacular job at clustering codes of different
    digit classes. This means that a simple machine learning model is going to be
    able to much more effectively classify data points consisting of autoencoder embeddings
    as compared to PCA embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0811.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. 2D embeddings produced by PCA (top) and by an autoencoder (bottom)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this section, we successfully set up and trained a feed-forward autoencoder
    and demonstrated that the resulting embeddings were superior to PCA, a classical
    dimensionality reduction method. In the next section, we’ll explore a concept
    known as denoising, which acts as a form of regularization by making our embeddings
    more robust.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising to Force Robust Representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Denoising* improves the ability of the autoencoder to generate embeddings
    that are resistant to noise. The human ability for perception is surprisingly
    resistant to noise. Take [Figure 8-12](#despite_the_corruption), for example.
    Despite the fact that I’ve corrupted half of the pixels in each image, you still
    have no problem making out the digit. In fact, even easily confused digits (like
    the 2 and the 7) are still distinguishable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0812.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12\. Human perception allows us to identify even obscured digits
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One way to look at this phenomenon is probabilistically. Even if we’re exposed
    to a random sampling of pixels from an image, if we have enough information, our
    brain is still capable of concluding the ground truth of what the pixels represent
    with maximal probability. Our mind is able to, quite literally, fill in the blanks
    to draw a conclusion. Even though only a corrupted version of a digit hits our
    retina, our brain is still able to reproduce the set of activations (i.e., the
    code or embedding) that we normally would use to represent the image of that digit.
    This is a property we might hope to enforce in our embedding algorithm, and it
    was first explored by Vincent et al. in 2008, when they introduced the *denoising
    autoencoder*.^([2](ch08.xhtml#idm45934167915920))
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic principles behind denoising are quite simple. We corrupt some fixed
    percentage of the pixels in the input image by setting them to zero. Given an
    original input *X*, let’s call the corrupted version  <math alttext="upper C left-parenthesis
    upper X right-parenthesis"><mrow><mi>C</mi> <mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></math>
    . The denoising autoencoder is identical to the vanilla autoencoder except for
    one detail: the input to the encoder network is the corrupted  <math alttext="upper
    C left-parenthesis upper X right-parenthesis"><mrow><mi>C</mi> <mo>(</mo> <mi>X</mi>
    <mo>)</mo></mrow></math>  instead of *X*. In other words, the autoencoder is forced
    to learn a code for each input that is resistant to the corruption mechanism and
    is able to interpolate through the missing information to recreate the original,
    uncorrupted image.'
  prefs: []
  type: TYPE_NORMAL
- en: We can also think about this process more geometrically. Let’s say we had a
    2D dataset with various labels. Let’s take all of the data points in a particular
    category (i.e., with some fixed label), and call this subset of data points *S*.
    While any arbitrary sampling of points could end up taking any form while visualized,
    we presume that for real-life categories, there is some underlying structure that
    unifies all of the points in *S*. This underlying, unifying geometric structure
    is known as a *manifold*. The manifold is the shape that we want to capture when
    we reduce the dimensionality of our data; and as Bengio et al. described in 2013,
    our autoencoder is implicitly learning this manifold as it learns how to reconstruct
    data after pushing it through a bottleneck (the code layer).^([3](ch08.xhtml#idm45934167906880))
    The autoencoder must figure out whether a point belongs to one manifold or another
    when trying to generate a reconstruction of an instance with potentially different
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: As an illustration, let’s consider the scenario in [Figure 8-13](#denoising_objective),
    where the points in *S* are a simple low-dimensional manifold (a solid circle
    in the diagram). In part A, we see our data points in *S* (black xs) and the manifold
    that best describes them. We also observe an approximation of our corruption operation.
    Specifically, the arrow and nonconcentric circle demonstrate all the ways in which
    the corruption could possibly move or modify a data point. Given that we are applying
    this corruption operation to every data point (i.e., along the entire manifold),
    this corruption operation artificially expands the dataset to not only include
    the manifold but also all of the points in space around the manifold, up to a
    maximum margin of error. This margin is demonstrated by the dashed circles in
    A, and the dataset expansion is illustrated by the x’s in part B. Finally the
    autoencoder is forced to learn to collapse all of the data points in this space
    back to the manifold. In other words, by learning which aspects of a data point
    are generalizable, broad strokes, and which aspects are “noise,” the denoising
    autoencoder learns to approximate the underlying manifold of *S*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0813.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13\. The denoising objective enables our model to learn the manifold
    (dark circle) by learning to map corrupted data (light x’s in B and C) to uncorrupted
    data (dark x’s) by minimizing the error (arrows in C) between their representations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With the philosophical motivations of denoising in mind, we can now make a
    small modification to our autoencoder script to build a denoising autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet corrupts the input if the `corrupt` variable is equal to 1,
    and it refrains from corrupting the input if the `corrupt` variable is equal to
    0\. After making this modification, we can rerun our autoencoder, resulting in
    the reconstructions shown in [Figure 8-14](#apply_a_corruption_operation). It’s
    quite apparent that the denoising autoencoder has faithfully replicated our incredible
    human ability to fill in the missing pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0814.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-14\. We apply a corruption operation to the dataset and train a denoising
    autoencoder to reconstruct the original, uncorrupted images
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sparsity in Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most difficult aspects of deep learning is a problem known as *interpretability*.
    Interpretability is a property of a machine learning model that measures how easy
    it is to inspect and explain its process and/or output. Deep models are generally
    difficult to interpret because of the nonlinearities and massive numbers of parameters
    that make up a model. While deep models are generally more accurate, a lack of
    interpretability often hinders their adoption in highly valuable, but highly risky,
    applications. For example, if a machine learning model is predicting that a patient
    has or does not have cancer, the doctor will likely want an explanation to confirm
    the model’s conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: We can address one aspect of interpretability by exploring the characteristics
    of the output of an autoencoder. In general, an autoencoder’s representations
    are dense, and this has implications with respect to how the representation changes
    as we make coherent modifications to the input. Consider the situation in [Figure 8-15](#activations_of_a_dense_representation).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0815.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-15\. The activations of a dense representation combine and overlay
    information from multiple features in ways that are difficult to interpret
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The autoencoder produces a *dense* representation, that is, the representation
    of the original image is highly compressed. Because we have only so many dimensions
    to work with in the representation, the activations of the representation combine
    information from multiple features in ways that are extremely difficult to disentangle.
    The result is that as we add components or remove components, the output representation
    changes in unexpected ways. It’s virtually impossible to interpret how and why
    the representation is generated in the way it is.
  prefs: []
  type: TYPE_NORMAL
- en: The ideal outcome for us is if we can build a representation where there is
    a 1-to-1 correspondence, or close to a 1-to-1 correspondence, between high-level
    features and individual components in the code. When we are able to achieve this,
    we get very close to the system described in [Figure 8-16](#right_combo_of_space_and_sparsity),
    which shows how the representation changes as we add and remove components. The
    representation is the sum of the individual strokes in the image. With the right
    combination of space and sparsity, a representation is more interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0816.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-16\. How activations in the representation change with the addition
    and removal of strokes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While this is the ideal outcome, we’ll have to think through what mechanisms
    we can leverage to enable this interpretability in the representation. The issue
    here is clearly the bottlenecked capacity of the code layer; but unfortunately,
    increasing the capacity of the code layer alone is not sufficient. In the medium
    case, while we can increase the size of the code layer, there is no mechanism
    that prevents each individual feature picked up by the autoencoder from affecting
    a large fraction of the components with smaller magnitudes. In the more extreme
    case, where the features that are picked up are more complex and therefore more
    bountiful, the capacity of the code layer may be even larger than the dimensionality
    of the input. In this case, the code layer has so much capacity that the model
    could quite literally perform a “copy” operation where the code layer learns no
    useful representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we really want is to force the autoencoder to utilize as few components
    of the representation vector as possible, while still effectively reconstructing
    the input. This is similar to the rationale behind using regularization to prevent
    overfitting in simple neural networks, as we discussed in [Chapter 4](ch04.xhtml#training_feed_forward),
    except we want as many components to be zero (or extremely close to zero) as possible.
    As in [Chapter 4](ch04.xhtml#training_feed_forward), we’ll achieve this by modifying
    the objective function with a sparsity penalty, which increases the cost of any
    representation that has a large number of nonzero components:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper E Subscript Sparse Baseline equals upper E plus beta dot
    SparsityPenalty"><mrow><msub><mi>E</mi> <mtext>Sparse</mtext></msub> <mo>=</mo>
    <mi>E</mi> <mo>+</mo> <mi>β</mi> <mo>·</mo> <mtext>SparsityPenalty</mtext></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The value of <math alttext="beta"><mi>β</mi></math> determines how strongly
    we favor sparsity at the expense of generating better reconstructions. For the
    mathematically inclined, you would do this by treating the values of each of the
    components of every representation as the outcome of a random variable with an
    unknown mean. We would then employ a measure of divergence comparing the distribution
    of observations of this random variable (the values of each component) and the
    distribution of a random variable whose mean is known to be 0\. A measure that
    is often used to this end is the Kullback-Leibler (often referred to as KL) divergence.
    Further discussion on sparsity in autoencoders is beyond the scope of this text,
    but they are covered by Ranzato et al. (2007^([4](ch08.xhtml#idm45934167858016)) and
    2008^([5](ch08.xhtml#idm45934167856400))). More recently, the theoretical properties
    and empirical effectiveness of introducing an intermediate function before the
    code layer that zeroes out all but  <math alttext="k"><mi>k</mi></math>  of the
    maximum activations in the representation were investigated by Makhzani and Frey
    (2014).^([6](ch08.xhtml#idm45934167853584)) These *k-Sparse autoencoders* were
    shown to be just as effective as other mechanisms of sparsity despite being shockingly
    simple to implement and understand (as well as computationally more efficient).
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of autoencoders. We’ve explored how we can use
    autoencoders to find strong representations of data points by summarizing their
    content. This mechanism of dimensionality reduction works well when the independent
    data points are rich and contain all of the relevant information pertaining to
    their structure in their original representation. In the next section, we’ll explore
    strategies that we can use when the main source of information is in the context
    of the data point instead of the data point itself.
  prefs: []
  type: TYPE_NORMAL
- en: When Context Is More Informative than the Input Vector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve mostly focused on the concept of dimensionality reduction. In
    dimensionality reduction, we generally have rich inputs that contain lots of noise
    on top of the core, structural information that we care about. In these situations,
    we want to extract this underlying information while ignoring the variations and
    noise that are extraneous to this fundamental understanding of the data.
  prefs: []
  type: TYPE_NORMAL
- en: In other situations, we have input representations that say very little at all
    about the content that we are trying to capture. In these situations, our goal
    is not to extract information but rather to gather information from context to
    build useful representations. All of this probably sounds too abstract to be useful
    at this point, so let’s concretize these ideas with a real example.
  prefs: []
  type: TYPE_NORMAL
- en: Building models for language is a tricky business. The first problem we have
    to overcome when building language models is finding a good way to represent individual
    words. At first glance, it’s not entirely clear how one builds a good representation.
    Let’s start with the naive approach, considering [Figure 8-17](#example_of_generating_one_hot_vector_reps).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0817.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-17\. Generating one-hot vector representations for words using a simple
    document
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If a document has a vocabulary  <math alttext="upper V"><mi>V</mi></math>  with 
    <math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo> <mi>V</mi>
    <mo>|</mo></mrow></math>  words, we can represent the words with one-hot vectors.
    We have  <math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>V</mi> <mo>|</mo></mrow></math> -dimensional representation vectors, and we
    associate each unique word with an index in this vector. To represent unique word 
    <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math> , we set
    the  <math alttext="i Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
     component of the vector to be 1, and zero out all of the other components.
  prefs: []
  type: TYPE_NORMAL
- en: However, this representation scheme seems rather arbitrary. This vectorization
    does not make similar words into similar vectors. This is problematic, because
    we’d like our models to know that the words “jump” and “leap” have similar meanings.
    Similarly, we’d like our models to know when words are verbs or nouns or prepositions.
    The naive one-hot encoding of words to vectors does not capture any of these characteristics.
    To address this challenge, we’ll need to find some way of discovering these relationships
    and encoding this information into a vector.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that one way to discover relationships between words is by analyzing
    their surrounding context. For example, synonyms such as “jump” and “leap” can
    be used interchangeably in their respective contexts. In addition, both words
    generally appear when a subject is performing the action over a direct object.
    We use this principle all the time when we run across new vocabulary while reading.
    For example, if we read the sentence “The warmonger argued with the crowd,” we
    can immediately draw conclusions about the word “warmonger” even if we don’t already
    know the dictionary definition. In this context, “warmonger” precedes a word we
    know to be a verb, which makes it likely that “warmonger” is a noun and the subject
    of this sentence. Also, the “warmonger” is “arguing,” which might imply that a
    “warmonger” is generally a combative or argumentative individual. Overall, as
    illustrated in [Figure 8-18](#id_words_with_similar_meanings), by analyzing the
    context (i.e., a fixed window of words surrounding a target word), we can quickly
    surmise the meaning of the word.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0818.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-18\. Analyzing context to determine a word’s meaning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It turns out we can use the same principles we used when building the autoencoder
    to build a network that builds strong, distributed representations. Two strategies
    are shown in [Figure 8-19](#general_architectures_for_designing_encoders). One
    possible method (shown in A) passes the target through an encoder network to create
    an embedding. Then we have a decoder network take this embedding; but instead
    of trying to reconstruct the original input as we did with the autoencoder, the
    decoder attempts to construct a word from the context. The second possible method
    (shown in B) does exactly the reverse: the encoder takes a word from the context
    as input, producing the target.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0819.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-19\. General architectures for designing encoders and decoders that
    generate embeddings by mapping words to their respective contexts (A) or vice
    versa (B)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the next section, we’ll describe how we use this strategy (along with some
    slight modifications for performance) to produce word embeddings in practice.
  prefs: []
  type: TYPE_NORMAL
- en: The Word2Vec Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2Vec, a framework for generating word embeddings, was pioneered by Mikolov
    et al. The original paper detailed two strategies for generating embeddings, similar
    to the two strategies for encoding context we discussed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: The first flavor of Word2Vec that Mikolov et al. introduced was the *Continuous
    Bag of Words* (CBOW) model.^([7](ch08.xhtml#idm45934167802832)) This model is
    much like strategy B from [Figure 8-19](#general_architectures_for_designing_encoders).
    The CBOW model used the encoder to create an embedding from the full context (treated
    as one input) and predict the target word. It turns out this strategy works best
    for smaller datasets, an attribute that is further discussed in the original paper.
  prefs: []
  type: TYPE_NORMAL
- en: The second flavor of Word2Vec is the *Skip-Gram model*, introduced by Mikolov
    et al.^([8](ch08.xhtml#idm45934167799008)) The Skip-Gram model does the inverse
    of CBOW, taking the target word as an input, and then attempting to predict one
    of the words in the context. Let’s walk through a toy example to explore what
    the dataset for a Skip-Gram model looks like.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the sentence “the boy went to the bank.” If we broke this sentence
    down into a sequence of (context, target) pairs, we would obtain [([the, went],
    boy), ([boy, to], went), ([went, the], to), ([to, bank], the)]. Taking this a
    step further, we have to split each (context, target) pair into (input, output)
    pairs where the input is the target and the output is one of the words from the
    context. From the first pair ([the, went], boy), we would generate the two pairs
    (boy, the) and (boy, went). We continue to apply this operation to every (context,
    target) pair to build our dataset. Finally, we replace each word with its unique
    index  <math alttext="i element-of StartSet 0 comma 1 comma ellipsis comma StartAbsoluteValue
    upper V EndAbsoluteValue minus 1 EndSet"><mrow><mi>i</mi> <mo>∈</mo> <mo>{</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>,</mo> <mo>...</mo> <mo>,</mo> <mo>|</mo>
    <mi>V</mi> <mo>|</mo> <mo>-</mo> <mn>1</mn> <mo>}</mo></mrow></math> corresponding
    to its index in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of the encoder is surprisingly simple. It is essentially a lookup
    table with <math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>V</mi> <mo>|</mo></mrow></math> rows, where the <math alttext="i Superscript
    t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>  row is
    the embedding corresponding to the <math alttext="i Superscript t h"><msup><mi>i</mi>
    <mrow><mi>t</mi><mi>h</mi></mrow></msup></math> vocabulary word. All the encoder
    has to do is take the index of the input word and output the appropriate row in
    the lookup table. This an efficient operation because on a GPU, this operation
    can be represented as a product of the transpose of the lookup table and the one-hot
    vector representing the input word. We can implement this simply in PyTorch with
    the following PyTorch function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Where `out` is the embedding matrix, and `x` is a tensor of indices we want
    to look up. For information on optional parameters, we refer you to the [PyTorch
    API documentation](https://oreil.ly/NaQWV).
  prefs: []
  type: TYPE_NORMAL
- en: The decoder is slightly trickier because we make some modifications for performance.
    The naive way to construct the decoder would be to attempt to reconstruct the
    one-hot encoding vector for the output, which we could implement with a run-of-the-mill
    feed-forward layer coupled with a softmax. The only concern is that it’s inefficient
    because we have to produce a probability distribution over the whole vocabulary
    space.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the number of parameters, Mikolov et al. used a strategy for implementing
    the decoder known as noise-contrastive estimation (NCE). The strategy is illustrated
    in [Figure 8-20](#illustration_of_noise_contrastive_esimation). A binary logistic
    regression compares the embedding of the target with the embedding of a context
    word and randomly sampled noncontext words. We construct a loss function describing
    how effectively the embeddings enable identification of words in the context of
    the target versus words outside the context of the target.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0820.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-20\. The NCE strategy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The NCE strategy uses the lookup table to find the embedding for the output,
    as well as embeddings for random selections from the vocabulary that are not in
    the context of the input. We then employ a binary logistic regression model that,
    one at a time, takes the input embedding and the embedding of the output or random
    selection, and then outputs a value between 0 to 1 corresponding to the probability
    that the comparison embedding represents a vocabulary word present in the input’s
    context. We then take the sum of the probabilities corresponding to the noncontext
    comparisons and subtract the probability corresponding to the context comparison.
    This value is the objective function that we want to minimize (in the optimal
    scenario where the model has perfect performance, the value will be –1).
  prefs: []
  type: TYPE_NORMAL
- en: An example of implementing NCE in PyTorch can be found on [GitHub](https://oreil.ly/lH2ip).
  prefs: []
  type: TYPE_NORMAL
- en: While Word2Vec is admittedly not a deep machine learning model, we discuss it
    here for many reasons. First, it thematically represents a strategy (finding embeddings
    using context) that generalizes to many deep learning models. When we learn about
    models for sequence analysis in [Chapter 9](ch09.xhtml#ch07), we’ll see this strategy
    employed for generating skip-thought vectors to embed sentences. Moreover, when
    we start building more and more models for language starting in [Chapter 9](ch09.xhtml#ch07),
    we’ll find that using Word2Vec embeddings instead of one-hot vectors to represent
    words will yield far superior results.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to architect the Skip-Gram model and its importance,
    we can start implementing it in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Skip-Gram Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build the dataset for our Skip-Gram model, we’ll utilize a modified version
    of the PyTorch Word2Vec data reader in `input_word_data.py`. We’ll start off by
    setting a couple of important parameters for training and regularly inspecting
    our model. Of particular note, we employ a minibatch size of 32 examples and train
    for 5 epochs (full passes through the dataset). We’ll use embeddings of size 128\.
    We’ll use a context window of five words to the left and to the right of each
    target word, and sample four context words from this window. Finally, we’ll use
    64 randomly chosen noncontext words for NCE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing the embedding layer is not particularly complicated. We merely
    have to initialize the lookup table with a matrix of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'PyTorch does not currently have a built-in NCE loss function. However, there
    are some implementations on the internet.  One example is the *info-nce-pytorch*
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We utilize `InfoNCE` to compute the NCE cost for each training example, and
    then compile all of the results in the minibatch into a single measurement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our objective function expressed as a mean of the NCE costs,
    we set up the training as usual. Here, we follow in the footsteps of Mikolov et
    al. and employ stochastic gradient descent with a learning rate of 0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We also inspect the model regularly using a validation function, which normalizes
    the embeddings in the lookup table and uses cosine similarity to compute distances
    for a set of validation words from all other words in the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting all of these components together, we’re finally ready to run the Skip-Gram
    model. We skim over this portion of the code because it is very similar to how
    we constructed models in the past. The only difference is the additional code
    during the inspection step. We randomly select 20 validation words out of the
    500 most common words in our vocabulary of 10,000 words. For each of these words,
    we use the cosine similarity function we built to find the nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The code starts to run, and we can begin to see how the model evolves over
    time. At the beginning, the model does a poor job of embedding (as is apparent
    from the inspection step). However, by the time training completes, the model
    has clearly found representations that effectively capture the meanings of individual
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: While not perfect, there are some strikingly meaningful clusters captured here.
    Numbers, countries, and cultures are clustered close together. The pronoun “I”
    is clustered with other pronouns. The word “world” is interestingly close to both
    “championship” and “war.” And the word “written” is found to be similar to “translated,”
    “poetry,” “alphabet,” “letters,” and “words.”
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we conclude this section by visualizing our word embeddings in [Figure 8-21](#viz_of_skip_gram_embeddings).
    To display our 128-dimensional embeddings in 2D space, we’ll use a visualization
    method known as t-SNE. If you’ll recall, we also used t-SNE in [Chapter 7](ch07.xhtml#convolutional_neural_networks)
    to visualize the relationships between images in ImageNet. Using t-SNE is quite
    simple, as it has a built-in function in the commonly used machine learning library
    scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can construct the visualization using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In [Figure 8-21](#viz_of_skip_gram_embeddings), we notice that similar concepts
    are closer together than disparate concepts, indicating that our embeddings encode
    meaningful information about the functions and definitions of individual words.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0821.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-21\. Skip-Gram embeddings using t-SNE
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For a more detailed exploration of the properties of word embeddings and interesting
    patterns (verb tenses, countries and capitals, analogy completion, etc.), we refer
    you to the original Mikolov et al. paper.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various methods in representation learning. We
    learned about how we can perform effective dimensionality reduction using autoencoders.
    We also learned about denoising and sparsity, which augment autoencoders with
    useful properties. After discussing autoencoders, we shifted our attention to
    representation learning when context of an input is more informative than the
    input itself. We learned how to generate embeddings for English words using the
    Skip-Gram model, which will prove useful as we explore deep learning models for
    understanding language. In the next chapter, we will build on this tangent to
    analyze language and other sequences using deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch08.xhtml#idm45934168575888-marker)) Hinton, Geoffrey E., and Ruslan
    R. Salakhutdinov. “Reducing the Dimensionality of Data with Neural Networks.”
    *Science* 313.5786 (2006): 504-507.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.xhtml#idm45934167915920-marker)) Vincent, Pascal, et al. “Extracting
    and Composing Robust Features with Denoising Autoencoders.” *Proceedings of the
    25th International Conference on Machine Learning*. ACM, 2008.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch08.xhtml#idm45934167906880-marker)) Bengio, Yoshua, et al. “Generalized
    Denoising Auto-Encoders as Generative Models.” *Advances in Neural Information
    Processing Systems*. 2013.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.xhtml#idm45934167858016-marker)) Ranzato, Marc’Aurelio, et al. “Efficient
    Learning of Sparse Representations with an Energy-Based Model.” *Proceedings of
    the 19th International Conference on Neural Information Processing Systems*. MIT
    Press, 2006.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch08.xhtml#idm45934167856400-marker)) Ranzato, Marc’Aurelio, and Martin
    Szummer. “Semi-supervised Learning of Compact Document Representations with Deep
    Networks.” *Proceedings of the 25th International Conference on Machine Learning*.
    ACM, 2008.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch08.xhtml#idm45934167853584-marker)) Makhzani, Alireza, and Brendan Frey.
    “k-Sparse Autoencoders.” *arXiv preprint arXiv*:1312.5663 (2013).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch08.xhtml#idm45934167802832-marker)) Mikolov, Tomas, et al. “Distributed
    Representations of Words and Phrases and their Compositionality.” *Advances in
    Neural Information Processing Systems*. 2013.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch08.xhtml#idm45934167799008-marker)) Tomas Mikolov, Kai Chen, Greg Corrado,
    and Jeffrey Dean. “Efficient Estimation of Word Representations in Vector Space.”
    *ICLR Workshop*, 2013.
  prefs: []
  type: TYPE_NORMAL
