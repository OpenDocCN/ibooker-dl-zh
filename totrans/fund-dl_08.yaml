- en: Chapter 8\. Embedding and Representation Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。嵌入和表示学习
- en: Learning Lower-Dimensional Representations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习低维表示
- en: In the previous chapter, we motivated the convolutional architecture using a
    simple argument. The larger our input vector, the larger our model. Large models
    with lots of parameters are expressive, but they’re also increasingly data hungry.
    This means that without sufficiently large volumes of training data, we will likely
    overfit. Convolutional architectures help us cope with the curse of dimensionality
    by reducing the number of parameters in our models without necessarily diminishing
    expressiveness.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们用一个简单的论点来激励卷积架构。我们的输入向量越大，我们的模型就越大。具有大量参数的大型模型具有表现力，但它们也越来越需要数据。这意味着如果没有足够大量的训练数据，我们很可能会过拟合。卷积架构通过减少模型中的参数数量而不一定减少表现力来帮助我们应对维度灾难。
- en: Regardless, convolutional networks still require large amounts of labeled training
    data. And for many problems, labeled data is scarce and expensive to generate.
    Our goal in this chapter will be to develop effective learning models in situations
    where labeled data is scarce, but wild, unlabeled data is plentiful. We’ll approach
    this problem by learning *embeddings*, or low-dimensional representations, in
    an unsupervised fashion. Because these unsupervised models allow us to offload
    all of the heavy lifting of automated feature selection, we can use the generated
    embeddings to solve learning problems using smaller models that require less data.
    This process is summarized in [Figure 8-1](#using_embeddings_to_automate_feature_selection).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，卷积网络仍然需要大量标记的训练数据。对于许多问题来说，标记数据是稀缺且昂贵的。本章的目标是在标记数据稀缺但野生的无标记数据丰富的情况下开发有效的学习模型。我们将通过无监督学习*嵌入*或低维表示来解决这个问题。因为这些无监督模型可以帮助我们摆脱自动特征选择的繁重工作，我们可以使用生成的嵌入来使用需要更少数据的较小模型解决学习问题。这个过程总结在[图8-1](#using_embeddings_to_automate_feature_selection)中。
- en: In the process of developing algorithms that learn good embeddings, we’ll also
    explore other applications of learning lower-dimensional representations, such
    as visualization and semantic hashing. We’ll start by considering situations where
    all of the important information is already contained within the original input
    vector itself. In this case, learning embeddings is equivalent to developing an
    effective compression algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发学习良好嵌入的算法过程中，我们还将探索学习低维表示的其他应用，如可视化和语义哈希。我们将从考虑所有重要信息已经包含在原始输入向量中的情况开始。在这种情况下，学习嵌入等同于开发一种有效的压缩算法。
- en: '![](Images/fdl2_0801.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0801.png)'
- en: Figure 8-1\. Using embeddings to automate feature selection in the face of scarce
    labeled data
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. 在面对稀缺标记数据时使用嵌入来自动化特征选择的示例
- en: In the next section, we’ll introduce *principal component analysis* (PCA), a
    classic method for dimensionality reduction. In subsequent sections, we’ll explore
    more powerful neural methods for learning compressive embeddings.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍*主成分分析*（PCA），这是一种经典的降维方法。在接下来的章节中，我们将探索更强大的神经方法来学习压缩嵌入。
- en: Principal Component Analysis
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: The basic concept behind PCA is to find a set of axes that communicates the
    most information about our dataset. More specifically, if we have  *d*-dimensional
    data, we’d like to find a new set of  <math alttext="m less-than d"><mrow><mi>m</mi>
    <mo><</mo> <mi>d</mi></mrow></math>  dimensions that conserves as much valuable
    information from the original dataset as possible. For simplicity, let’s choose 
    <math alttext="d equals 2 comma m equals 1"><mrow><mi>d</mi> <mo>=</mo> <mn>2</mn>
    <mo>,</mo> <mi>m</mi> <mo>=</mo> <mn>1</mn></mrow></math> . Assuming that variance
    corresponds to information, we can perform this transformation through an iterative
    process. First, we find a unit vector along which the dataset has maximum variance.
    Because this direction contains the most information, we select this direction
    as our first axis. Then from the set of vectors orthogonal to this first choice,
    we pick a new unit vector along which the dataset has maximum variance. This is
    our second axis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的基本概念是找到一组轴，这些轴传达了关于我们数据集的最多信息。更具体地说，如果我们有*d*维数据，我们希望找到一个新的 <math alttext="m小于d"><mrow><mi>m</mi>
    <mo><</mo> <mi>d</mi></mrow></math> 维度的集合，尽可能保留原始数据集中的有价值信息。为简单起见，让我们选择 <math
    alttext="d等于2，m等于1"><mrow><mi>d</mi> <mo>=</mo> <mn>2</mn> <mo>,</mo> <mi>m</mi>
    <mo>=</mo> <mn>1</mn></mrow></math>。假设方差对应于信息，我们可以通过迭代过程执行这种转换。首先，我们找到一个沿着数据集具有最大方差的单位向量。因为这个方向包含最多信息，我们选择这个方向作为我们的第一个轴。然后从与这个第一个选择正交的向量集中，我们选择一个沿着数据集具有最大方差的新单位向量。这是我们的第二个轴。
- en: We continue this process until we have found a total of *d* new vectors that
    represent new axes. We project our data onto this new set of axes. We then decide
    a good value for *m* and toss out all but the first *m* axes (the principal components,
    which store the most information). The result is shown in [Figure 8-2](#illustration_of_pca).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续这个过程，直到找到一共*d*个代表新轴的新向量。我们将数据投影到这组新轴上。然后我们决定一个好的值*m*，并且丢弃除了前*m*个轴（存储最多信息的主成分）之外的所有轴。结果显示在[图8-2](#illustration_of_pca)中。
- en: '![](Images/fdl2_0802.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0802.png)'
- en: Figure 8-2\. An illustration of PCA for dimensionality reduction to capture
    the dimension with the most information (as proxied by variance)
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. 主成分分析的示例，用于降维以捕获包含最多信息的维度（通过方差代表）
- en: 'For the mathematically inclined, we can view this operation as a projection
    onto the vector space spanned by the top *m* eigenvectors of the dataset’s correlation
    matrix, which is equivalent to the dataset’s covariance matrix when the dataset
    has been z-score normalized (zero-mean and unit-variance per input dimension).
    Let us represent the dataset as a matrix **X** with dimensions  <math alttext="n
    times d"><mrow><mi>n</mi> <mo>×</mo> <mi>d</mi></mrow></math> (i.e., *n* inputs
    of  *d* dimensions). We’d like to create an embedding matrix **T** with dimensions
    <math alttext="n times m"><mrow><mi>n</mi> <mo>×</mo> <mi>m</mi></mrow></math>
    . We can compute the matrix using the relationship **T** = **X**, where each column
    of **W** corresponds to an eigenvector of the matrix <math alttext="StartFraction
    1 Over n EndFraction"><mfrac><mn>1</mn> <mi>n</mi></mfrac></math> **X**^Τ**X**.
    Those with linear algebra background or core data science experience may be seeing
    a striking parallel between PCA and the singular value decomposition (SVD), which
    we cover in more depth in [“Theory: PCA and SVD”](#theory_sidebar).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数学倾向的人来说，我们可以将这个操作视为投影到由数据集的相关矩阵的前*m*个特征向量张成的向量空间上，当数据集已经进行了z-score标准化（每个输入维度的零均值和单位方差）时，这等同于数据集的协方差矩阵。让我们将数据集表示为一个维度为
    <math alttext="n times d"><mrow><mi>n</mi> <mo>×</mo> <mi>d</mi></mrow></math>
    的矩阵**X**（即，*n*个输入，*d*个维度）。我们希望创建一个维度为 <math alttext="n times m"><mrow><mi>n</mi>
    <mo>×</mo> <mi>m</mi></mrow></math> 的嵌入矩阵**T**。我们可以使用关系**T** = **X**计算矩阵，其中**W**的每一列对应于矩阵
    <math alttext="StartFraction 1 Over n EndFraction"><mfrac><mn>1</mn> <mi>n</mi></mfrac></math>
    **X**^Τ**X**的特征向量。具有线性代数背景或核心数据科学经验的人可能会看到PCA和奇异值分解（SVD）之间的惊人相似之处，我们将在[“理论：PCA和SVD”](#theory_sidebar)中更深入地讨论。
- en: While PCA has been used for decades for dimensionality reduction, it spectacularly
    fails to capture important relationships that are piecewise linear or nonlinear.
    Take, for instance, the example illustrated in [Figure 8-3](#situation_in_which_pca).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PCA在几十年来一直被用于降维，但它在捕捉重要的分段线性或非线性关系方面表现得非常糟糕。例如，看看[图8-3](#situation_in_which_pca)中所示的例子。
- en: The example shows data points selected at random from two concentric circles.
    We hope that PCA will transform this dataset so that we can pick a single new
    axis that allows us to easily separate the dots. Unfortunately for us, there is
    no linear direction that contains more information here than another (we have
    equal variance in all directions). Instead, as human beings, we notice that information
    is being encoded in a nonlinear way, in terms of how far points are from the origin.
    With this information in mind, we notice that the polar transformation (expressing
    points as their distance from the origin, as the new horizontal axis, and their
    angle bearing from the original x-axis, as the new vertical axis) does just the
    trick.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了从两个同心圆中随机选择的数据点。我们希望PCA将转换这个数据集，以便我们可以选择一个新的轴，使我们能够轻松地分开这些点。不幸的是，这里没有一个线性方向包含比另一个更多的信息（在所有方向上方差相等）。相反，作为人类，我们注意到信息以非线性方式进行编码，即点距离原点的远近。有了这些信息，我们注意到极坐标变换（将点表示为它们距离原点的距离，作为新的水平轴，以及它们相对于原始x轴的角度，作为新的垂直轴）正好起到了作用。
- en: '[Figure 8-3](#situation_in_which_pca) highlights the shortcomings of an approach
    like PCA in capturing important relationships in complex datasets. Because most
    of the datasets we are likely to encounter in the wild (images, text, etc.) are
    characterized by nonlinear relationships, we must develop a theory that will perform
    nonlinear dimensionality reduction. Deep learning practitioners have closed this
    gap using neural models, which we’ll cover in the next section.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-3](#situation_in_which_pca)突出了像PCA这样的方法在捕捉复杂数据集中重要关系方面的缺点。因为我们在现实中可能遇到的大多数数据集（图像、文本等）都具有非线性关系，所以我们必须开发一种能够进行非线性降维的理论。深度学习从业者通过使用神经模型来弥补这一差距，我们将在下一节中介绍。'
- en: '![](Images/fdl2_0803.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/fdl2_0803.png)
- en: Figure 8-3\. A situation in which PCA fails to optimally transform the data
    for dimensionality reduction
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3。PCA在数据降维方面无法进行最佳转换的情况
- en: Motivating the Autoencoder Architecture
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激励自动编码器架构
- en: When we talked about feed-forward networks, we discussed how each layer learned
    progressively more relevant representations of the input. In fact, in [Chapter 7](ch07.xhtml#convolutional_neural_networks),
    we took the output of the final convolutional layer and used that as a lower-dimensional
    representation of the input image. Putting aside the fact that we want to generate
    these low-dimensional representations in an unsupervised fashion, there are fundamental
    problems with these approaches in general. Specifically, while the selected layer
    does contain information from the input, the network has been trained to pay attention
    to the aspects of the input that are critical to solving the task at hand. As
    a result, there’s a significant amount of information loss with respect to elements
    of the input that may be important for other classification tasks, but potentially
    less important than the one immediately at hand.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论前馈网络时，我们讨论了每一层如何逐渐学习更相关的输入表示。事实上，在[第7章](ch07.xhtml#convolutional_neural_networks)中，我们取出了最终卷积层的输出，并将其用作输入图像的低维表示。暂且不谈我们希望以无监督的方式生成这些低维表示，总体上这些方法存在根本问题。具体来说，虽然所选层确实包含来自输入的信息，但网络已经被训练为关注解决手头任务关键的输入方面。因此，与输入的一些可能对其他分类任务重要但可能比当前任务不太重要的元素相关的信息丢失是相当显著的。
- en: However, the fundamental intuition here still applies. We define a new network
    architecture that we call the *autoencoder*. We first take the input and compress
    it into a low-dimensional vector. This part of the network is called the *encoder*
    because it is responsible for producing the low-dimensional embedding or *code*.
    The second part of the network, instead of mapping the embedding to an arbitrary
    label as we would in a feed-forward network, tries to invert the computation of
    the first half of the network and reconstruct the original input. This piece is
    known as the *decoder*. The overall architecture is illustrated in [Figure 8-4](#autoencoder_architecture_attempts_to_construct).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里的基本直觉仍然适用。我们定义了一个称为*自动编码器*的新网络架构。我们首先将输入压缩成一个低维向量。网络的这一部分被称为*编码器*，因为它负责生成低维嵌入或*编码*。网络的第二部分，而不是将嵌入映射到任意标签，而是尝试反转网络前半部分的计算并重构原始输入。这部分被称为*解码器*。整体架构如[图8-4](#autoencoder_architecture_attempts_to_construct)所示。
- en: '![](Images/fdl2_0804.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0804.png)'
- en: Figure 8-4\. The autoencoder architecture attempts to construct a high-dimensional
    input into a low-dimensional embedding and then uses that low-dimensional embedding
    to reconstruct the input
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4。自动编码器架构试图将高维输入构建成低维嵌入，然后使用该低维嵌入来重构输入
- en: To demonstrate the surprising effectiveness of autoencoders, we’ll build and
    visualize the autoencoder architecture in [Figure 8-4](#autoencoder_architecture_attempts_to_construct).
    Specifically, we will highlight its superior ability to separate MNIST digits
    as compared to PCA.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示自动编码器的惊人有效性，我们将构建并可视化自动编码器架构，如[图8-4](#autoencoder_architecture_attempts_to_construct)所示。具体来说，我们将突出其与PCA相比更好地分离MNIST数字的能力。
- en: Implementing an Autoencoder in PyTorch
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中实现自动编码器
- en: The seminal paper “Reducing the Dimensionality of Data with Neural Networks,”
    which describes the autoencoder, was written by Hinton and Salakhutdinov in 2006.^([1](ch08.xhtml#idm45934168575888))
    Their hypothesis was that the nonlinear complexities afforded by a neural model
    would allow them to capture structure that linear methods, such as PCA, would
    miss. To demonstrate this point, they ran an experiment on MNIST using both an
    autoencoder and PCA to reduce the dataset into 2D data points. In this section,
    we will recreate their experimental setup to validate this hypothesis and further
    explore the architecture and properties of feed-forward autoencoders.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年Hinton和Salakhutdinov撰写的开创性论文“使用神经网络降低数据的维度”描述了自动编码器。他们的假设是，神经模型提供的非线性复杂性将使他们能够捕捉线性方法（如PCA）所忽略的结构。为了证明这一点，他们在MNIST上进行了一个实验，使用自动编码器和PCA将数据集减少为2D数据点。在本节中，我们将重新创建他们的实验设置，以验证这一假设，并进一步探索前馈自动编码器的架构和属性。
- en: 'The setup shown in [Figure 8-5](#experimental_setup_for_dimensionality_reduction)
    is built with the same principle, but the 2D embedding is now treated as the input,
    and the network attempts to reconstruct the original image. Because we are essentially
    applying an inverse operation, we architect the decoder network so that the autoencoder
    has the shape of an hourglass. The output of the decoder network is a 784-dimensional
    vector that can be reconstructed into a 28 × 28 image:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-5](#experimental_setup_for_dimensionality_reduction)中显示的设置是基于相同原则构建的，但现在将2D嵌入视为输入，并且网络试图重构原始图像。因为我们实质上是应用一个逆操作，所以我们设计解码器网络，使得自动编码器的形状像一个沙漏。解码器网络的输出是一个784维向量，可以重构为一个28×28的图像：'
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](Images/fdl2_0805.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0805.png)'
- en: Figure 8-5\. The experimental setup for dimensionality reduction of the MNIST
    dataset employed by Hinton and Salakhutdinov, 2006
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5。Hinton和Salakhutdinov在2006年使用的MNIST数据集降维实验设置
- en: 'In order to accelerate training, we’ll reuse the batch normalization strategy
    we employed in [Chapter 7](ch07.xhtml#convolutional_neural_networks). Also, because
    we’d like to visualize the results, we’ll avoid introducing sharp transitions
    in our neurons. In this example, we’ll use sigmoidal neurons instead of our usual
    ReLU neurons:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快训练速度，我们将重用我们在[第7章](ch07.xhtml#convolutional_neural_networks)中使用的批量归一化策略。此外，因为我们想要可视化结果，我们将避免在我们的神经元中引入尖锐的转变。在这个例子中，我们将使用S形神经元而不是我们通常的ReLU神经元：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, we need to construct a measure (or objective function) that describes
    how well our model functions. Specifically, we want to measure how close the reconstruction
    is to the original image. We can measure this simply by computing the distance
    between the original 784-dimensional input and the reconstructed 784-dimensional
    output. More specifically, given an input vector  <math alttext="upper I"><mi>I</mi></math>
     and a reconstruction  <math alttext="upper O"><mi>O</mi></math> , we’d like to
    minimize the value of  <math alttext="parallel-to upper I minus upper O parallel-to
    equals StartRoot sigma-summation Underscript i Endscripts left-parenthesis upper
    I Subscript i Baseline minus upper O Subscript i Baseline right-parenthesis squared
    EndRoot"><mrow><mrow><mo>∥</mo> <mi>I</mi> <mo>-</mo> <mi>O</mi> <mo>∥</mo></mrow>
    <mo>=</mo> <msqrt><mrow><msub><mo>∑</mo> <mi>i</mi></msub> <msup><mfenced separators=""
    open="(" close=")"><msub><mi>I</mi> <mi>i</mi></msub> <mo>-</mo><msub><mi>O</mi>
    <mi>i</mi></msub></mfenced> <mn>2</mn></msup></mrow></msqrt></mrow></math> , also
    known as the L2 norm of the difference between the two vectors. We average this
    function over the whole minibatch to generate our final objective function. Finally,
    we’ll train the network using the Adam optimizer, logging a scalar summary of
    the error incurred at every minibatch using `torch.utils.tensorboard.SummaryWriter`.
    In PyTorch, we can concisely express the loss and training operations as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要构建一个描述我们模型功能如何的度量（或目标函数）。具体来说，我们想要衡量重构与原始图像之间的接近程度。我们可以通过简单地计算原始784维输入和重构的784维输出之间的距离来衡量这一点。更具体地说，给定一个输入向量I和一个重构O，我们希望最小化I和O之间的差值的值，也称为两个向量之间的L2范数。我们将这个函数平均到整个小批次上以生成我们的最终目标函数。最后，我们将使用Adam优化器训练网络，使用`torch.utils.tensorboard.SummaryWriter`在每个小批次记录所产生的错误的标量摘要。在PyTorch中，我们可以简洁地表示损失和训练操作如下：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we’ll need a method to evaluate the generalizability of our model.
    As usual, we’ll use a validation dataset and compute the same L2 norm measurement
    for model evaluation. In addition, we’ll collect image summaries so that we can
    compare both the input images and the reconstructions:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一种方法来评估我们模型的泛化能力。像往常一样，我们将使用一个验证数据集，并计算相同的L2范数测量来评估模型。此外，我们将收集图像摘要，以便我们可以比较输入图像和重构图像：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can visualize the model graph, the training and validation costs, and the
    image summaries using TensorBoard. Simply run the following command:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用TensorBoard可视化模型图、训练和验证成本以及图像摘要。只需运行以下命令：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then navigate your browser to *http://localhost:6006/*. The results of the “Graph”
    tab are shown in [Figure 8-6](#tensorflow_allows_us_to_neatly_view).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将浏览器导航到*http://localhost:6006/*。 “Graph”选项卡的结果显示在[图8-6](#tensorflow_allows_us_to_neatly_view)中。
- en: Thanks to how we’ve namespaced the components of our model graph, our model
    is nicely organized. We can easily click through the components and delve deeper,
    tracing how data flows up through the various layers of the encoder and through
    the decoder, how the optimizer reads the output of our training module, and how
    gradients in turn affect all of the components of the model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对模型图的组件进行了命名空间处理，我们的模型组织得很好。我们可以轻松地点击组件并深入研究，追踪数据如何通过编码器的各个层和解码器的各个层流动，优化器如何读取我们训练模块的输出，以及梯度如何影响模型的所有组件。
- en: We also visualize both the training (after each minibatch) and validation costs
    (after each epoch), closely monitoring the curves for potential overfitting. The
    TensorBoard visualizations of the costs over the span of training are shown in
    [Figure 8-7](#cost_incurred_on_the_training_set). As we would expect for a successful
    model, both the training and validation curves decrease until they flatten off
    asymptotically. After approximately 200 epochs, we attain a validation cost of
    4.78\. While the curves look promising, it’s difficult, upon first glance, to
    understand whether we’ve reached a plateau at a “good” cost, or whether our model
    is still doing a poor job of reconstructing the original inputs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可视化训练（每个小批次后）和验证成本（每个时代后），密切监控曲线以防止过拟合。训练期间成本的TensorBoard可视化显示在[图8-7](#cost_incurred_on_the_training_set)中。正如我们所期望的那样，对于一个成功的模型，训练和验证曲线都会下降，直到渐近平稳。大约在200个时代之后，我们获得了一个验证成本为4.78。虽然曲线看起来很有希望，但乍一看很难理解我们是否已经达到了“好”的成本平台，还是我们的模型仍然在重构原始输入方面做得很差。
- en: '![](Images/fdl2_0806.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0806.png)'
- en: Figure 8-6\. TensorBoard allows us to neatly view the high-level components
    and data flow of our computation graph (top) and also click through to more closely
    inspect the data flows of individual subcomponents (bottom)
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-6. TensorBoard允许我们清晰地查看计算图的高级组件和数据流（顶部），并且还可以点击查看各个子组件的数据流（底部）
- en: '![](Images/fdl2_0807.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0807.png)'
- en: Figure 8-7\. The cost incurred on the training set (logged after each minibatch)
    and on the validation set (logged after each epoch)
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-7. 训练集上产生的成本（每个小批次后记录）和验证集上产生的成本（每个时代后记录）
- en: To get a sense of what that means, let’s explore the MNIST dataset. We pick
    an arbitrary image of a 1 from the dataset and call it *X*. In [Figure 8-8](#compared_to_all_other_digits),
    we compare the image to all other images in the dataset. Specifically, for each
    digit class, we compute the average of the L2 costs, comparing *X* to each instance
    of the digit class. As a visual aid, we also include the average of all of the
    instances for each digit class.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0808.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. The image of the 1 on the left is compared to all of the other
    digits in the MNIST dataset; each digit class is represented visually with the
    average of all of its members and labeled with the average of the L2 costs, comparing
    the 1 on the left with all of the class members
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On average, *X* is 5.75 units away from other 1s in MNIST. In terms of L2 distance,
    the non-1 digits closest to the *X* are the 7s (8.94 units) and the digits farthest
    are the 0s (11.05 units). Given these measurements, it’s quite apparent that with
    an average cost of 4.78, our autoencoder is producing high-quality reconstructions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Because we are collecting image summaries, we can confirm this hypothesis directly
    by inspecting the input images and reconstructions directly. The reconstructions
    for three randomly chosen samples from the test set are shown in [Figure 8-9](#side_by_side_comparison_of_original_inputs).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0809.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. A side-by-side comparison of the original inputs (from the validation
    set) and reconstructions after 5, 100, and 200 epochs of training
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After five epochs, we can start to make out some of the critical strokes of
    the original image that are being picked by the autoencoder, but for the most
    part, the reconstructions are still hazy mixtures of closely related digits. By
    100 epochs, the 0 and 4 are reconstructed with strong strokes, but it looks like
    the autoencoder is still having trouble differentiating between 5s, 3s, and possibly
    8s. However, by 200 epochs, it’s clear that even this more difficult ambiguity
    is clarified, and all of the digits are crisply reconstructed.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we’ll complete the section by exploring the 2D codes produced by traditional
    PCA and autoencoders. We’ll want to show that autoencoders produce better visualizations.
    In particular, we’ll want to show that autoencoders do a much better job of visually
    separating instances of different digit classes than PCA. We’ll start by quickly
    covering the code we use to produce 2D PCA codes:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We first pull up the MNIST dataset. We’ve set the flag `one_hot=False` because
    we’d like the labels to be provided as integers instead of one-hot vectors (as
    a quick reminder, a one-hot vector representing an MNIST label would be a vector
    of size 10 with the <math alttext="i Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    component set to one to represent digit <math alttext="i"><mi>i</mi></math> and
    the rest of the components set to zero). We use the commonly used machine learning
    library *scikit-learn* to perform the PCA, setting the `n_components=2` flat so
    that scikit-learn knows to generate 2D codes. We can also reconstruct the original
    images from the 2D codes and visualize the reconstructions:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The code snippet shows how to visualize the first image in the test dataset,
    but we can easily modify the code to visualize any arbitrary subset of the dataset.
    Comparing the PCA reconstructions to the autoencoder reconstructions in [Figure 8-10](#comparing_reconstructions_by_pca_and_autoencoder),
    it’s quite clear that the autoencoder vastly outperforms PCA with 2D codes. In
    fact, the PCA’s performance is somewhat reminiscent of the autoencoder only five
    epochs into training. It has trouble distinguishing 5s from 3s and 8s, 0s from
    8s, and 4s from 9s. Repeating the same experiment with 30-dimensional codes provides
    significant improvement to the PCA reconstructions, but they are still significantly
    worse than the 30-dimensional autoencoder.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0810.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0810.png)'
- en: Figure 8-10\. Comparing the reconstructions by both PCA and autoencoder side
    by side
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10。将PCA和自动编码器的重建结果并排比较
- en: Now, to complete the experiment, we must load up a saved PyTorch model, retrieve
    the 2D codes, and plot both the PCA and autoencoder codes. We’re careful to rebuild
    the PyTorch graph exactly how we set it up during training. We pass the path to
    the model checkpoint we saved during training as a command-line argument to the
    script. Finally, we use a custom plotting function to generate a legend and appropriately
    color data points of different digit classes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了完成实验，我们必须加载一个保存的PyTorch模型，检索2D代码，并绘制PCA和自动编码器代码。我们小心地重建PyTorch图，确保与训练期间设置的一样。我们将训练期间保存的模型检查点路径作为命令行参数传递给脚本。最后，我们使用自定义绘图函数生成图例，并适当着色不同数字类别的数据点。
- en: In the resulting visualization in [Figure 8-11](#two_dimensional_embeddings_produced_by_pca),
    it is extremely difficult to make out separable clusters in the 2D PCA codes;
    the autoencoder has clearly done a spectacular job at clustering codes of different
    digit classes. This means that a simple machine learning model is going to be
    able to much more effectively classify data points consisting of autoencoder embeddings
    as compared to PCA embeddings.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图8-11](#two_dimensional_embeddings_produced_by_pca)中的可视化结果中，很难看出2D PCA代码中的可分离聚类；自动编码器显然在对不同数字类别的代码进行聚类方面做得非常出色。这意味着一个简单的机器学习模型将能够更有效地对由自动编码器嵌入组成的数据点进行分类，与PCA嵌入相比。
- en: '![](Images/fdl2_0811.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0811.png)'
- en: Figure 8-11\. 2D embeddings produced by PCA (top) and by an autoencoder (bottom)
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-11。PCA生成的2D嵌入（顶部）和自动编码器生成的2D嵌入（底部）
- en: In this section, we successfully set up and trained a feed-forward autoencoder
    and demonstrated that the resulting embeddings were superior to PCA, a classical
    dimensionality reduction method. In the next section, we’ll explore a concept
    known as denoising, which acts as a form of regularization by making our embeddings
    more robust.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们成功地建立并训练了一个前馈自动编码器，并证明了生成的嵌入优于PCA，这是一种经典的降维方法。在下一节中，我们将探讨一种称为去噪的概念，它作为一种正则化形式，使我们的嵌入更加健壮。
- en: Denoising to Force Robust Representations
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪以强制生成稳健的表示形式
- en: '*Denoising* improves the ability of the autoencoder to generate embeddings
    that are resistant to noise. The human ability for perception is surprisingly
    resistant to noise. Take [Figure 8-12](#despite_the_corruption), for example.
    Despite the fact that I’ve corrupted half of the pixels in each image, you still
    have no problem making out the digit. In fact, even easily confused digits (like
    the 2 and the 7) are still distinguishable.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*去噪* 提高了自动编码器生成对噪声具有抗性的嵌入的能力。人类的感知能力对噪声非常有抵抗力。例如，看看[图8-12](#despite_the_corruption)。尽管我在每个图像中破坏了一半的像素，但您仍然可以轻松辨认出数字。事实上，即使是容易混淆的数字（如2和7）仍然可以区分开来。'
- en: '![](Images/fdl2_0812.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0812.png)'
- en: Figure 8-12\. Human perception allows us to identify even obscured digits
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-12。人类感知能力使我们能够识别甚至被遮挡的数字
- en: One way to look at this phenomenon is probabilistically. Even if we’re exposed
    to a random sampling of pixels from an image, if we have enough information, our
    brain is still capable of concluding the ground truth of what the pixels represent
    with maximal probability. Our mind is able to, quite literally, fill in the blanks
    to draw a conclusion. Even though only a corrupted version of a digit hits our
    retina, our brain is still able to reproduce the set of activations (i.e., the
    code or embedding) that we normally would use to represent the image of that digit.
    This is a property we might hope to enforce in our embedding algorithm, and it
    was first explored by Vincent et al. in 2008, when they introduced the *denoising
    autoencoder*.^([2](ch08.xhtml#idm45934167915920))
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这种现象的一种方式是从概率的角度来看。即使我们暴露于图像的随机像素采样中，如果我们有足够的信息，我们的大脑仍然能够以最大概率得出像素代表的真实情况。我们的大脑能够，确实地，填补空白以得出结论。即使我们的视网膜只接收到一个数字的损坏版本，我们的大脑仍然能够重现我们通常用来表示该数字图像的一组激活（即代码或嵌入）。这是我们希望在我们的嵌入算法中强制执行的一种属性，这是由Vincent等人在2008年首次探索的，当时他们引入了*去噪自动编码器*。
- en: 'The basic principles behind denoising are quite simple. We corrupt some fixed
    percentage of the pixels in the input image by setting them to zero. Given an
    original input *X*, let’s call the corrupted version  <math alttext="upper C left-parenthesis
    upper X right-parenthesis"><mrow><mi>C</mi> <mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></math>
    . The denoising autoencoder is identical to the vanilla autoencoder except for
    one detail: the input to the encoder network is the corrupted  <math alttext="upper
    C left-parenthesis upper X right-parenthesis"><mrow><mi>C</mi> <mo>(</mo> <mi>X</mi>
    <mo>)</mo></mrow></math>  instead of *X*. In other words, the autoencoder is forced
    to learn a code for each input that is resistant to the corruption mechanism and
    is able to interpolate through the missing information to recreate the original,
    uncorrupted image.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪背后的基本原理非常简单。我们通过将输入图像中的一定百分比的像素设为零来破坏输入图像。给定原始输入*X*，让我们称破坏版本为 <math alttext="upper
    C left-parenthesis upper X right-parenthesis"><mrow><mi>C</mi> <mo>(</mo> <mi>X</mi>
    <mo>)</mo></mrow></math> 。去噪自动编码器与普通自动编码器相同，除了一个细节：编码器网络的输入是破坏的 <math alttext="upper
    C left-parenthesis upper X right-parenthesis"><mrow><mi>C</mi> <mo>(</mo> <mi>X</mi>
    <mo>)</mo></mrow></math> 而不是*X*。换句话说，自动编码器被迫学习对每个输入都具有抗干扰机制的代码，并且能够通过缺失的信息插值来重新创建原始的未损坏图像。
- en: We can also think about this process more geometrically. Let’s say we had a
    2D dataset with various labels. Let’s take all of the data points in a particular
    category (i.e., with some fixed label), and call this subset of data points *S*.
    While any arbitrary sampling of points could end up taking any form while visualized,
    we presume that for real-life categories, there is some underlying structure that
    unifies all of the points in *S*. This underlying, unifying geometric structure
    is known as a *manifold*. The manifold is the shape that we want to capture when
    we reduce the dimensionality of our data; and as Bengio et al. described in 2013,
    our autoencoder is implicitly learning this manifold as it learns how to reconstruct
    data after pushing it through a bottleneck (the code layer).^([3](ch08.xhtml#idm45934167906880))
    The autoencoder must figure out whether a point belongs to one manifold or another
    when trying to generate a reconstruction of an instance with potentially different
    labels.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: As an illustration, let’s consider the scenario in [Figure 8-13](#denoising_objective),
    where the points in *S* are a simple low-dimensional manifold (a solid circle
    in the diagram). In part A, we see our data points in *S* (black xs) and the manifold
    that best describes them. We also observe an approximation of our corruption operation.
    Specifically, the arrow and nonconcentric circle demonstrate all the ways in which
    the corruption could possibly move or modify a data point. Given that we are applying
    this corruption operation to every data point (i.e., along the entire manifold),
    this corruption operation artificially expands the dataset to not only include
    the manifold but also all of the points in space around the manifold, up to a
    maximum margin of error. This margin is demonstrated by the dashed circles in
    A, and the dataset expansion is illustrated by the x’s in part B. Finally the
    autoencoder is forced to learn to collapse all of the data points in this space
    back to the manifold. In other words, by learning which aspects of a data point
    are generalizable, broad strokes, and which aspects are “noise,” the denoising
    autoencoder learns to approximate the underlying manifold of *S*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0813.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13\. The denoising objective enables our model to learn the manifold
    (dark circle) by learning to map corrupted data (light x’s in B and C) to uncorrupted
    data (dark x’s) by minimizing the error (arrows in C) between their representations
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With the philosophical motivations of denoising in mind, we can now make a
    small modification to our autoencoder script to build a denoising autoencoder:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code snippet corrupts the input if the `corrupt` variable is equal to 1,
    and it refrains from corrupting the input if the `corrupt` variable is equal to
    0\. After making this modification, we can rerun our autoencoder, resulting in
    the reconstructions shown in [Figure 8-14](#apply_a_corruption_operation). It’s
    quite apparent that the denoising autoencoder has faithfully replicated our incredible
    human ability to fill in the missing pixels.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0814.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: Figure 8-14\. We apply a corruption operation to the dataset and train a denoising
    autoencoder to reconstruct the original, uncorrupted images
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sparsity in Autoencoders
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most difficult aspects of deep learning is a problem known as *interpretability*.
    Interpretability is a property of a machine learning model that measures how easy
    it is to inspect and explain its process and/or output. Deep models are generally
    difficult to interpret because of the nonlinearities and massive numbers of parameters
    that make up a model. While deep models are generally more accurate, a lack of
    interpretability often hinders their adoption in highly valuable, but highly risky,
    applications. For example, if a machine learning model is predicting that a patient
    has or does not have cancer, the doctor will likely want an explanation to confirm
    the model’s conclusion.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: We can address one aspect of interpretability by exploring the characteristics
    of the output of an autoencoder. In general, an autoencoder’s representations
    are dense, and this has implications with respect to how the representation changes
    as we make coherent modifications to the input. Consider the situation in [Figure 8-15](#activations_of_a_dense_representation).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过探索自动编码器的输出特征来解决可解释性的一个方面。一般来说，自动编码器的表示是密集的，这对于我们在对输入进行连贯修改时表示如何变化具有影响。考虑在[图8-15](#activations_of_a_dense_representation)中的情况。
- en: '![](Images/fdl2_0815.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0815.png)'
- en: Figure 8-15\. The activations of a dense representation combine and overlay
    information from multiple features in ways that are difficult to interpret
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-15\. 密集表示的激活以难以解释的方式结合和叠加多个特征的信息
- en: The autoencoder produces a *dense* representation, that is, the representation
    of the original image is highly compressed. Because we have only so many dimensions
    to work with in the representation, the activations of the representation combine
    information from multiple features in ways that are extremely difficult to disentangle.
    The result is that as we add components or remove components, the output representation
    changes in unexpected ways. It’s virtually impossible to interpret how and why
    the representation is generated in the way it is.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器产生*密集*表示，即原始图像的表示被高度压缩。由于表示中只有有限的维度可用，表示的激活以极其难以分解的方式结合了多个特征的信息。结果是，当我们添加组件或删除组件时，输出表示以意想不到的方式变化。几乎不可能解释表示是如何生成的以及为什么生成的。
- en: The ideal outcome for us is if we can build a representation where there is
    a 1-to-1 correspondence, or close to a 1-to-1 correspondence, between high-level
    features and individual components in the code. When we are able to achieve this,
    we get very close to the system described in [Figure 8-16](#right_combo_of_space_and_sparsity),
    which shows how the representation changes as we add and remove components. The
    representation is the sum of the individual strokes in the image. With the right
    combination of space and sparsity, a representation is more interpretable.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，理想的结果是我们能够构建一个表示，其中高级特征与代码中的各个组件之间存在一对一的对应，或接近一对一的对应。当我们能够实现这一点时，我们就非常接近[图8-16](#right_combo_of_space_and_sparsity)中描述的系统，该系统显示了随着添加和删除组件表示的变化。表示是图像中各个笔画的总和。通过正确的空间和稀疏组合，表示更具可解释性。
- en: '![](Images/fdl2_0816.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0816.png)'
- en: Figure 8-16\. How activations in the representation change with the addition
    and removal of strokes
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-16\. 表示中的激活随着笔画的添加和删除而变化
- en: While this is the ideal outcome, we’ll have to think through what mechanisms
    we can leverage to enable this interpretability in the representation. The issue
    here is clearly the bottlenecked capacity of the code layer; but unfortunately,
    increasing the capacity of the code layer alone is not sufficient. In the medium
    case, while we can increase the size of the code layer, there is no mechanism
    that prevents each individual feature picked up by the autoencoder from affecting
    a large fraction of the components with smaller magnitudes. In the more extreme
    case, where the features that are picked up are more complex and therefore more
    bountiful, the capacity of the code layer may be even larger than the dimensionality
    of the input. In this case, the code layer has so much capacity that the model
    could quite literally perform a “copy” operation where the code layer learns no
    useful representation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是理想的结果，但我们必须考虑可以利用哪些机制来实现表示中的可解释性。问题显然在于代码层的瓶颈容量；但不幸的是，仅增加代码层的容量是不够的。在中等情况下，虽然我们可以增加代码层的大小，但没有机制可以阻止自动编码器捕捉到的每个单独特征影响具有较小幅度的大部分组件。在更极端的情况下，捕捉到的特征更复杂，因此更丰富，代码层的容量可能甚至大于输入的维度。在这种情况下，代码层的容量非常大，以至于模型可能实际上执行“复制”操作，其中代码层学习不到任何有用的表示。
- en: 'What we really want is to force the autoencoder to utilize as few components
    of the representation vector as possible, while still effectively reconstructing
    the input. This is similar to the rationale behind using regularization to prevent
    overfitting in simple neural networks, as we discussed in [Chapter 4](ch04.xhtml#training_feed_forward),
    except we want as many components to be zero (or extremely close to zero) as possible.
    As in [Chapter 4](ch04.xhtml#training_feed_forward), we’ll achieve this by modifying
    the objective function with a sparsity penalty, which increases the cost of any
    representation that has a large number of nonzero components:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正想要的是强制自动编码器尽可能少地利用表示向量的组件，同时有效地重建输入。这类似于在简单神经网络中使用正则化来防止过拟合的原理，正如我们在[第4章](ch04.xhtml#training_feed_forward)中讨论的那样，只是我们希望尽可能多的组件为零（或非常接近零）。与[第4章](ch04.xhtml#training_feed_forward)一样，我们将通过在目标函数中添加稀疏惩罚来实现这一点，这会增加具有大量非零组件的任何表示的成本：
- en: <math alttext="upper E Subscript Sparse Baseline equals upper E plus beta dot
    SparsityPenalty"><mrow><msub><mi>E</mi> <mtext>Sparse</mtext></msub> <mo>=</mo>
    <mi>E</mi> <mo>+</mo> <mi>β</mi> <mo>·</mo> <mtext>SparsityPenalty</mtext></mrow></math>
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper E Subscript Sparse Baseline equals upper E plus beta dot
    SparsityPenalty"><mrow><msub><mi>E</mi> <mtext>Sparse</mtext></msub> <mo>=</mo>
    <mi>E</mi> <mo>+</mo> <mi>β</mi> <mo>·</mo> <mtext>SparsityPenalty</mtext></mrow></math>
- en: The value of <math alttext="beta"><mi>β</mi></math> determines how strongly
    we favor sparsity at the expense of generating better reconstructions. For the
    mathematically inclined, you would do this by treating the values of each of the
    components of every representation as the outcome of a random variable with an
    unknown mean. We would then employ a measure of divergence comparing the distribution
    of observations of this random variable (the values of each component) and the
    distribution of a random variable whose mean is known to be 0\. A measure that
    is often used to this end is the Kullback-Leibler (often referred to as KL) divergence.
    Further discussion on sparsity in autoencoders is beyond the scope of this text,
    but they are covered by Ranzato et al. (2007^([4](ch08.xhtml#idm45934167858016)) and
    2008^([5](ch08.xhtml#idm45934167856400))). More recently, the theoretical properties
    and empirical effectiveness of introducing an intermediate function before the
    code layer that zeroes out all but  <math alttext="k"><mi>k</mi></math>  of the
    maximum activations in the representation were investigated by Makhzani and Frey
    (2014).^([6](ch08.xhtml#idm45934167853584)) These *k-Sparse autoencoders* were
    shown to be just as effective as other mechanisms of sparsity despite being shockingly
    simple to implement and understand (as well as computationally more efficient).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of autoencoders. We’ve explored how we can use
    autoencoders to find strong representations of data points by summarizing their
    content. This mechanism of dimensionality reduction works well when the independent
    data points are rich and contain all of the relevant information pertaining to
    their structure in their original representation. In the next section, we’ll explore
    strategies that we can use when the main source of information is in the context
    of the data point instead of the data point itself.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: When Context Is More Informative than the Input Vector
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve mostly focused on the concept of dimensionality reduction. In
    dimensionality reduction, we generally have rich inputs that contain lots of noise
    on top of the core, structural information that we care about. In these situations,
    we want to extract this underlying information while ignoring the variations and
    noise that are extraneous to this fundamental understanding of the data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: In other situations, we have input representations that say very little at all
    about the content that we are trying to capture. In these situations, our goal
    is not to extract information but rather to gather information from context to
    build useful representations. All of this probably sounds too abstract to be useful
    at this point, so let’s concretize these ideas with a real example.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Building models for language is a tricky business. The first problem we have
    to overcome when building language models is finding a good way to represent individual
    words. At first glance, it’s not entirely clear how one builds a good representation.
    Let’s start with the naive approach, considering [Figure 8-17](#example_of_generating_one_hot_vector_reps).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0817.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: Figure 8-17\. Generating one-hot vector representations for words using a simple
    document
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If a document has a vocabulary  <math alttext="upper V"><mi>V</mi></math>  with 
    <math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo> <mi>V</mi>
    <mo>|</mo></mrow></math>  words, we can represent the words with one-hot vectors.
    We have  <math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>V</mi> <mo>|</mo></mrow></math> -dimensional representation vectors, and we
    associate each unique word with an index in this vector. To represent unique word 
    <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math> , we set
    the  <math alttext="i Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
     component of the vector to be 1, and zero out all of the other components.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: However, this representation scheme seems rather arbitrary. This vectorization
    does not make similar words into similar vectors. This is problematic, because
    we’d like our models to know that the words “jump” and “leap” have similar meanings.
    Similarly, we’d like our models to know when words are verbs or nouns or prepositions.
    The naive one-hot encoding of words to vectors does not capture any of these characteristics.
    To address this challenge, we’ll need to find some way of discovering these relationships
    and encoding this information into a vector.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that one way to discover relationships between words is by analyzing
    their surrounding context. For example, synonyms such as “jump” and “leap” can
    be used interchangeably in their respective contexts. In addition, both words
    generally appear when a subject is performing the action over a direct object.
    We use this principle all the time when we run across new vocabulary while reading.
    For example, if we read the sentence “The warmonger argued with the crowd,” we
    can immediately draw conclusions about the word “warmonger” even if we don’t already
    know the dictionary definition. In this context, “warmonger” precedes a word we
    know to be a verb, which makes it likely that “warmonger” is a noun and the subject
    of this sentence. Also, the “warmonger” is “arguing,” which might imply that a
    “warmonger” is generally a combative or argumentative individual. Overall, as
    illustrated in [Figure 8-18](#id_words_with_similar_meanings), by analyzing the
    context (i.e., a fixed window of words surrounding a target word), we can quickly
    surmise the meaning of the word.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0818.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Figure 8-18\. Analyzing context to determine a word’s meaning
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It turns out we can use the same principles we used when building the autoencoder
    to build a network that builds strong, distributed representations. Two strategies
    are shown in [Figure 8-19](#general_architectures_for_designing_encoders). One
    possible method (shown in A) passes the target through an encoder network to create
    an embedding. Then we have a decoder network take this embedding; but instead
    of trying to reconstruct the original input as we did with the autoencoder, the
    decoder attempts to construct a word from the context. The second possible method
    (shown in B) does exactly the reverse: the encoder takes a word from the context
    as input, producing the target.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0819.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Figure 8-19\. General architectures for designing encoders and decoders that
    generate embeddings by mapping words to their respective contexts (A) or vice
    versa (B)
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the next section, we’ll describe how we use this strategy (along with some
    slight modifications for performance) to produce word embeddings in practice.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The Word2Vec Framework
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2Vec, a framework for generating word embeddings, was pioneered by Mikolov
    et al. The original paper detailed two strategies for generating embeddings, similar
    to the two strategies for encoding context we discussed in the previous section.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The first flavor of Word2Vec that Mikolov et al. introduced was the *Continuous
    Bag of Words* (CBOW) model.^([7](ch08.xhtml#idm45934167802832)) This model is
    much like strategy B from [Figure 8-19](#general_architectures_for_designing_encoders).
    The CBOW model used the encoder to create an embedding from the full context (treated
    as one input) and predict the target word. It turns out this strategy works best
    for smaller datasets, an attribute that is further discussed in the original paper.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The second flavor of Word2Vec is the *Skip-Gram model*, introduced by Mikolov
    et al.^([8](ch08.xhtml#idm45934167799008)) The Skip-Gram model does the inverse
    of CBOW, taking the target word as an input, and then attempting to predict one
    of the words in the context. Let’s walk through a toy example to explore what
    the dataset for a Skip-Gram model looks like.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Consider the sentence “the boy went to the bank.” If we broke this sentence
    down into a sequence of (context, target) pairs, we would obtain [([the, went],
    boy), ([boy, to], went), ([went, the], to), ([to, bank], the)]. Taking this a
    step further, we have to split each (context, target) pair into (input, output)
    pairs where the input is the target and the output is one of the words from the
    context. From the first pair ([the, went], boy), we would generate the two pairs
    (boy, the) and (boy, went). We continue to apply this operation to every (context,
    target) pair to build our dataset. Finally, we replace each word with its unique
    index  <math alttext="i element-of StartSet 0 comma 1 comma ellipsis comma StartAbsoluteValue
    upper V EndAbsoluteValue minus 1 EndSet"><mrow><mi>i</mi> <mo>∈</mo> <mo>{</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>,</mo> <mo>...</mo> <mo>,</mo> <mo>|</mo>
    <mi>V</mi> <mo>|</mo> <mo>-</mo> <mn>1</mn> <mo>}</mo></mrow></math> corresponding
    to its index in the vocabulary.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of the encoder is surprisingly simple. It is essentially a lookup
    table with <math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>V</mi> <mo>|</mo></mrow></math> rows, where the <math alttext="i Superscript
    t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>  row is
    the embedding corresponding to the <math alttext="i Superscript t h"><msup><mi>i</mi>
    <mrow><mi>t</mi><mi>h</mi></mrow></msup></math> vocabulary word. All the encoder
    has to do is take the index of the input word and output the appropriate row in
    the lookup table. This an efficient operation because on a GPU, this operation
    can be represented as a product of the transpose of the lookup table and the one-hot
    vector representing the input word. We can implement this simply in PyTorch with
    the following PyTorch function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Where `out` is the embedding matrix, and `x` is a tensor of indices we want
    to look up. For information on optional parameters, we refer you to the [PyTorch
    API documentation](https://oreil.ly/NaQWV).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The decoder is slightly trickier because we make some modifications for performance.
    The naive way to construct the decoder would be to attempt to reconstruct the
    one-hot encoding vector for the output, which we could implement with a run-of-the-mill
    feed-forward layer coupled with a softmax. The only concern is that it’s inefficient
    because we have to produce a probability distribution over the whole vocabulary
    space.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the number of parameters, Mikolov et al. used a strategy for implementing
    the decoder known as noise-contrastive estimation (NCE). The strategy is illustrated
    in [Figure 8-20](#illustration_of_noise_contrastive_esimation). A binary logistic
    regression compares the embedding of the target with the embedding of a context
    word and randomly sampled noncontext words. We construct a loss function describing
    how effectively the embeddings enable identification of words in the context of
    the target versus words outside the context of the target.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0820.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: Figure 8-20\. The NCE strategy
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The NCE strategy uses the lookup table to find the embedding for the output,
    as well as embeddings for random selections from the vocabulary that are not in
    the context of the input. We then employ a binary logistic regression model that,
    one at a time, takes the input embedding and the embedding of the output or random
    selection, and then outputs a value between 0 to 1 corresponding to the probability
    that the comparison embedding represents a vocabulary word present in the input’s
    context. We then take the sum of the probabilities corresponding to the noncontext
    comparisons and subtract the probability corresponding to the context comparison.
    This value is the objective function that we want to minimize (in the optimal
    scenario where the model has perfect performance, the value will be –1).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: An example of implementing NCE in PyTorch can be found on [GitHub](https://oreil.ly/lH2ip).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: While Word2Vec is admittedly not a deep machine learning model, we discuss it
    here for many reasons. First, it thematically represents a strategy (finding embeddings
    using context) that generalizes to many deep learning models. When we learn about
    models for sequence analysis in [Chapter 9](ch09.xhtml#ch07), we’ll see this strategy
    employed for generating skip-thought vectors to embed sentences. Moreover, when
    we start building more and more models for language starting in [Chapter 9](ch09.xhtml#ch07),
    we’ll find that using Word2Vec embeddings instead of one-hot vectors to represent
    words will yield far superior results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to architect the Skip-Gram model and its importance,
    we can start implementing it in PyTorch.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Skip-Gram Architecture
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build the dataset for our Skip-Gram model, we’ll utilize a modified version
    of the PyTorch Word2Vec data reader in `input_word_data.py`. We’ll start off by
    setting a couple of important parameters for training and regularly inspecting
    our model. Of particular note, we employ a minibatch size of 32 examples and train
    for 5 epochs (full passes through the dataset). We’ll use embeddings of size 128\.
    We’ll use a context window of five words to the left and to the right of each
    target word, and sample four context words from this window. Finally, we’ll use
    64 randomly chosen noncontext words for NCE.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing the embedding layer is not particularly complicated. We merely
    have to initialize the lookup table with a matrix of values:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'PyTorch does not currently have a built-in NCE loss function. However, there
    are some implementations on the internet.  One example is the *info-nce-pytorch*
    library:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We utilize `InfoNCE` to compute the NCE cost for each training example, and
    then compile all of the results in the minibatch into a single measurement:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now that we have our objective function expressed as a mean of the NCE costs,
    we set up the training as usual. Here, we follow in the footsteps of Mikolov et
    al. and employ stochastic gradient descent with a learning rate of 0.1:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We also inspect the model regularly using a validation function, which normalizes
    the embeddings in the lookup table and uses cosine similarity to compute distances
    for a set of validation words from all other words in the vocabulary:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Putting all of these components together, we’re finally ready to run the Skip-Gram
    model. We skim over this portion of the code because it is very similar to how
    we constructed models in the past. The only difference is the additional code
    during the inspection step. We randomly select 20 validation words out of the
    500 most common words in our vocabulary of 10,000 words. For each of these words,
    we use the cosine similarity function we built to find the nearest neighbors:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The code starts to run, and we can begin to see how the model evolves over
    time. At the beginning, the model does a poor job of embedding (as is apparent
    from the inspection step). However, by the time training completes, the model
    has clearly found representations that effectively capture the meanings of individual
    words:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: While not perfect, there are some strikingly meaningful clusters captured here.
    Numbers, countries, and cultures are clustered close together. The pronoun “I”
    is clustered with other pronouns. The word “world” is interestingly close to both
    “championship” and “war.” And the word “written” is found to be similar to “translated,”
    “poetry,” “alphabet,” “letters,” and “words.”
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we conclude this section by visualizing our word embeddings in [Figure 8-21](#viz_of_skip_gram_embeddings).
    To display our 128-dimensional embeddings in 2D space, we’ll use a visualization
    method known as t-SNE. If you’ll recall, we also used t-SNE in [Chapter 7](ch07.xhtml#convolutional_neural_networks)
    to visualize the relationships between images in ImageNet. Using t-SNE is quite
    simple, as it has a built-in function in the commonly used machine learning library
    scikit-learn.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'We can construct the visualization using the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In [Figure 8-21](#viz_of_skip_gram_embeddings), we notice that similar concepts
    are closer together than disparate concepts, indicating that our embeddings encode
    meaningful information about the functions and definitions of individual words.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0821.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 8-21\. Skip-Gram embeddings using t-SNE
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For a more detailed exploration of the properties of word embeddings and interesting
    patterns (verb tenses, countries and capitals, analogy completion, etc.), we refer
    you to the original Mikolov et al. paper.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various methods in representation learning. We
    learned about how we can perform effective dimensionality reduction using autoencoders.
    We also learned about denoising and sparsity, which augment autoencoders with
    useful properties. After discussing autoencoders, we shifted our attention to
    representation learning when context of an input is more informative than the
    input itself. We learned how to generate embeddings for English words using the
    Skip-Gram model, which will prove useful as we explore deep learning models for
    understanding language. In the next chapter, we will build on this tangent to
    analyze language and other sequences using deep learning.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch08.xhtml#idm45934168575888-marker)) Hinton, Geoffrey E., and Ruslan
    R. Salakhutdinov. “Reducing the Dimensionality of Data with Neural Networks.”
    *Science* 313.5786 (2006): 504-507.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.xhtml#idm45934167915920-marker)) Vincent, Pascal, et al. “Extracting
    and Composing Robust Features with Denoising Autoencoders.” *Proceedings of the
    25th International Conference on Machine Learning*. ACM, 2008.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch08.xhtml#idm45934167906880-marker)) Bengio, Yoshua, et al. “Generalized
    Denoising Auto-Encoders as Generative Models.” *Advances in Neural Information
    Processing Systems*. 2013.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.xhtml#idm45934167858016-marker)) Ranzato, Marc’Aurelio, et al. “Efficient
    Learning of Sparse Representations with an Energy-Based Model.” *Proceedings of
    the 19th International Conference on Neural Information Processing Systems*. MIT
    Press, 2006.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch08.xhtml#idm45934167856400-marker)) Ranzato, Marc’Aurelio, and Martin
    Szummer. “Semi-supervised Learning of Compact Document Representations with Deep
    Networks.” *Proceedings of the 25th International Conference on Machine Learning*.
    ACM, 2008.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch08.xhtml#idm45934167853584-marker)) Makhzani, Alireza, and Brendan Frey.
    “k-Sparse Autoencoders.” *arXiv preprint arXiv*:1312.5663 (2013).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch08.xhtml#idm45934167802832-marker)) Mikolov, Tomas, et al. “Distributed
    Representations of Words and Phrases and their Compositionality.” *Advances in
    Neural Information Processing Systems*. 2013.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch08.xhtml#idm45934167799008-marker)) Tomas Mikolov, Kai Chen, Greg Corrado,
    and Jeffrey Dean. “Efficient Estimation of Word Representations in Vector Space.”
    *ICLR Workshop*, 2013.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
