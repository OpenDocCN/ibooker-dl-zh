- en: Chapter 8\. Embedding and Representation Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。嵌入和表示学习
- en: Learning Lower-Dimensional Representations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习低维表示
- en: In the previous chapter, we motivated the convolutional architecture using a
    simple argument. The larger our input vector, the larger our model. Large models
    with lots of parameters are expressive, but they’re also increasingly data hungry.
    This means that without sufficiently large volumes of training data, we will likely
    overfit. Convolutional architectures help us cope with the curse of dimensionality
    by reducing the number of parameters in our models without necessarily diminishing
    expressiveness.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们用一个简单的论点来激励卷积架构。我们的输入向量越大，我们的模型就越大。具有大量参数的大型模型具有表现力，但它们也越来越需要数据。这意味着如果没有足够大量的训练数据，我们很可能会过拟合。卷积架构通过减少模型中的参数数量而不一定减少表现力来帮助我们应对维度灾难。
- en: Regardless, convolutional networks still require large amounts of labeled training
    data. And for many problems, labeled data is scarce and expensive to generate.
    Our goal in this chapter will be to develop effective learning models in situations
    where labeled data is scarce, but wild, unlabeled data is plentiful. We’ll approach
    this problem by learning *embeddings*, or low-dimensional representations, in
    an unsupervised fashion. Because these unsupervised models allow us to offload
    all of the heavy lifting of automated feature selection, we can use the generated
    embeddings to solve learning problems using smaller models that require less data.
    This process is summarized in [Figure 8-1](#using_embeddings_to_automate_feature_selection).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，卷积网络仍然需要大量标记的训练数据。对于许多问题来说，标记数据是稀缺且昂贵的。本章的目标是在标记数据稀缺但野生的无标记数据丰富的情况下开发有效的学习模型。我们将通过无监督学习*嵌入*或低维表示来解决这个问题。因为这些无监督模型可以帮助我们摆脱自动特征选择的繁重工作，我们可以使用生成的嵌入来使用需要更少数据的较小模型解决学习问题。这个过程总结在[图8-1](#using_embeddings_to_automate_feature_selection)中。
- en: In the process of developing algorithms that learn good embeddings, we’ll also
    explore other applications of learning lower-dimensional representations, such
    as visualization and semantic hashing. We’ll start by considering situations where
    all of the important information is already contained within the original input
    vector itself. In this case, learning embeddings is equivalent to developing an
    effective compression algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发学习良好嵌入的算法过程中，我们还将探索学习低维表示的其他应用，如可视化和语义哈希。我们将从考虑所有重要信息已经包含在原始输入向量中的情况开始。在这种情况下，学习嵌入等同于开发一种有效的压缩算法。
- en: '![](Images/fdl2_0801.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0801.png)'
- en: Figure 8-1\. Using embeddings to automate feature selection in the face of scarce
    labeled data
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. 在面对稀缺标记数据时使用嵌入来自动化特征选择的示例
- en: In the next section, we’ll introduce *principal component analysis* (PCA), a
    classic method for dimensionality reduction. In subsequent sections, we’ll explore
    more powerful neural methods for learning compressive embeddings.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍*主成分分析*（PCA），这是一种经典的降维方法。在接下来的章节中，我们将探索更强大的神经方法来学习压缩嵌入。
- en: Principal Component Analysis
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: The basic concept behind PCA is to find a set of axes that communicates the
    most information about our dataset. More specifically, if we have  *d*-dimensional
    data, we’d like to find a new set of  <math alttext="m less-than d"><mrow><mi>m</mi>
    <mo><</mo> <mi>d</mi></mrow></math>  dimensions that conserves as much valuable
    information from the original dataset as possible. For simplicity, let’s choose 
    <math alttext="d equals 2 comma m equals 1"><mrow><mi>d</mi> <mo>=</mo> <mn>2</mn>
    <mo>,</mo> <mi>m</mi> <mo>=</mo> <mn>1</mn></mrow></math> . Assuming that variance
    corresponds to information, we can perform this transformation through an iterative
    process. First, we find a unit vector along which the dataset has maximum variance.
    Because this direction contains the most information, we select this direction
    as our first axis. Then from the set of vectors orthogonal to this first choice,
    we pick a new unit vector along which the dataset has maximum variance. This is
    our second axis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的基本概念是找到一组轴，这些轴传达了关于我们数据集的最多信息。更具体地说，如果我们有*d*维数据，我们希望找到一个新的 <math alttext="m小于d"><mrow><mi>m</mi>
    <mo><</mo> <mi>d</mi></mrow></math> 维度的集合，尽可能保留原始数据集中的有价值信息。为简单起见，让我们选择 <math
    alttext="d等于2，m等于1"><mrow><mi>d</mi> <mo>=</mo> <mn>2</mn> <mo>,</mo> <mi>m</mi>
    <mo>=</mo> <mn>1</mn></mrow></math>。假设方差对应于信息，我们可以通过迭代过程执行这种转换。首先，我们找到一个沿着数据集具有最大方差的单位向量。因为这个方向包含最多信息，我们选择这个方向作为我们的第一个轴。然后从与这个第一个选择正交的向量集中，我们选择一个沿着数据集具有最大方差的新单位向量。这是我们的第二个轴。
- en: We continue this process until we have found a total of *d* new vectors that
    represent new axes. We project our data onto this new set of axes. We then decide
    a good value for *m* and toss out all but the first *m* axes (the principal components,
    which store the most information). The result is shown in [Figure 8-2](#illustration_of_pca).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续这个过程，直到找到一共*d*个代表新轴的新向量。我们将数据投影到这组新轴上。然后我们决定一个好的值*m*，并且丢弃除了前*m*个轴（存储最多信息的主成分）之外的所有轴。结果显示在[图8-2](#illustration_of_pca)中。
- en: '![](Images/fdl2_0802.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0802.png)'
- en: Figure 8-2\. An illustration of PCA for dimensionality reduction to capture
    the dimension with the most information (as proxied by variance)
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. 主成分分析的示例，用于降维以捕获包含最多信息的维度（通过方差代表）
- en: 'For the mathematically inclined, we can view this operation as a projection
    onto the vector space spanned by the top *m* eigenvectors of the dataset’s correlation
    matrix, which is equivalent to the dataset’s covariance matrix when the dataset
    has been z-score normalized (zero-mean and unit-variance per input dimension).
    Let us represent the dataset as a matrix **X** with dimensions  <math alttext="n
    times d"><mrow><mi>n</mi> <mo>×</mo> <mi>d</mi></mrow></math> (i.e., *n* inputs
    of  *d* dimensions). We’d like to create an embedding matrix **T** with dimensions
    <math alttext="n times m"><mrow><mi>n</mi> <mo>×</mo> <mi>m</mi></mrow></math>
    . We can compute the matrix using the relationship **T** = **X**, where each column
    of **W** corresponds to an eigenvector of the matrix <math alttext="StartFraction
    1 Over n EndFraction"><mfrac><mn>1</mn> <mi>n</mi></mfrac></math> **X**^Τ**X**.
    Those with linear algebra background or core data science experience may be seeing
    a striking parallel between PCA and the singular value decomposition (SVD), which
    we cover in more depth in [“Theory: PCA and SVD”](#theory_sidebar).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数学倾向的人来说，我们可以将这个操作视为投影到由数据集的相关矩阵的前*m*个特征向量张成的向量空间上，当数据集已经进行了z-score标准化（每个输入维度的零均值和单位方差）时，这等同于数据集的协方差矩阵。让我们将数据集表示为一个维度为
    <math alttext="n times d"><mrow><mi>n</mi> <mo>×</mo> <mi>d</mi></mrow></math>
    的矩阵**X**（即，*n*个输入，*d*个维度）。我们希望创建一个维度为 <math alttext="n times m"><mrow><mi>n</mi>
    <mo>×</mo> <mi>m</mi></mrow></math> 的嵌入矩阵**T**。我们可以使用关系**T** = **X**计算矩阵，其中**W**的每一列对应于矩阵
    <math alttext="StartFraction 1 Over n EndFraction"><mfrac><mn>1</mn> <mi>n</mi></mfrac></math>
    **X**^Τ**X**的特征向量。具有线性代数背景或核心数据科学经验的人可能会看到PCA和奇异值分解（SVD）之间的惊人相似之处，我们将在[“理论：PCA和SVD”](#theory_sidebar)中更深入地讨论。
- en: While PCA has been used for decades for dimensionality reduction, it spectacularly
    fails to capture important relationships that are piecewise linear or nonlinear.
    Take, for instance, the example illustrated in [Figure 8-3](#situation_in_which_pca).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PCA在几十年来一直被用于降维，但它在捕捉重要的分段线性或非线性关系方面表现得非常糟糕。例如，看看[图8-3](#situation_in_which_pca)中所示的例子。
- en: The example shows data points selected at random from two concentric circles.
    We hope that PCA will transform this dataset so that we can pick a single new
    axis that allows us to easily separate the dots. Unfortunately for us, there is
    no linear direction that contains more information here than another (we have
    equal variance in all directions). Instead, as human beings, we notice that information
    is being encoded in a nonlinear way, in terms of how far points are from the origin.
    With this information in mind, we notice that the polar transformation (expressing
    points as their distance from the origin, as the new horizontal axis, and their
    angle bearing from the original x-axis, as the new vertical axis) does just the
    trick.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了从两个同心圆中随机选择的数据点。我们希望PCA将转换这个数据集，以便我们可以选择一个新的轴，使我们能够轻松地分开这些点。不幸的是，这里没有一个线性方向包含比另一个更多的信息（在所有方向上方差相等）。相反，作为人类，我们注意到信息以非线性方式进行编码，即点距离原点的远近。有了这些信息，我们注意到极坐标变换（将点表示为它们距离原点的距离，作为新的水平轴，以及它们相对于原始x轴的角度，作为新的垂直轴）正好起到了作用。
- en: '[Figure 8-3](#situation_in_which_pca) highlights the shortcomings of an approach
    like PCA in capturing important relationships in complex datasets. Because most
    of the datasets we are likely to encounter in the wild (images, text, etc.) are
    characterized by nonlinear relationships, we must develop a theory that will perform
    nonlinear dimensionality reduction. Deep learning practitioners have closed this
    gap using neural models, which we’ll cover in the next section.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-3](#situation_in_which_pca)突出了像PCA这样的方法在捕捉复杂数据集中重要关系方面的缺点。因为我们在现实中可能遇到的大多数数据集（图像、文本等）都具有非线性关系，所以我们必须开发一种能够进行非线性降维的理论。深度学习从业者通过使用神经模型来弥补这一差距，我们将在下一节中介绍。'
- en: '![](Images/fdl2_0803.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/fdl2_0803.png)
- en: Figure 8-3\. A situation in which PCA fails to optimally transform the data
    for dimensionality reduction
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3。PCA在数据降维方面无法进行最佳转换的情况
- en: Motivating the Autoencoder Architecture
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激励自动编码器架构
- en: When we talked about feed-forward networks, we discussed how each layer learned
    progressively more relevant representations of the input. In fact, in [Chapter 7](ch07.xhtml#convolutional_neural_networks),
    we took the output of the final convolutional layer and used that as a lower-dimensional
    representation of the input image. Putting aside the fact that we want to generate
    these low-dimensional representations in an unsupervised fashion, there are fundamental
    problems with these approaches in general. Specifically, while the selected layer
    does contain information from the input, the network has been trained to pay attention
    to the aspects of the input that are critical to solving the task at hand. As
    a result, there’s a significant amount of information loss with respect to elements
    of the input that may be important for other classification tasks, but potentially
    less important than the one immediately at hand.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论前馈网络时，我们讨论了每一层如何逐渐学习更相关的输入表示。事实上，在[第7章](ch07.xhtml#convolutional_neural_networks)中，我们取出了最终卷积层的输出，并将其用作输入图像的低维表示。暂且不谈我们希望以无监督的方式生成这些低维表示，总体上这些方法存在根本问题。具体来说，虽然所选层确实包含来自输入的信息，但网络已经被训练为关注解决手头任务关键的输入方面。因此，与输入的一些可能对其他分类任务重要但可能比当前任务不太重要的元素相关的信息丢失是相当显著的。
- en: However, the fundamental intuition here still applies. We define a new network
    architecture that we call the *autoencoder*. We first take the input and compress
    it into a low-dimensional vector. This part of the network is called the *encoder*
    because it is responsible for producing the low-dimensional embedding or *code*.
    The second part of the network, instead of mapping the embedding to an arbitrary
    label as we would in a feed-forward network, tries to invert the computation of
    the first half of the network and reconstruct the original input. This piece is
    known as the *decoder*. The overall architecture is illustrated in [Figure 8-4](#autoencoder_architecture_attempts_to_construct).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里的基本直觉仍然适用。我们定义了一个称为*自动编码器*的新网络架构。我们首先将输入压缩成一个低维向量。网络的这一部分被称为*编码器*，因为它负责生成低维嵌入或*编码*。网络的第二部分，而不是将嵌入映射到任意标签，而是尝试反转网络前半部分的计算并重构原始输入。这部分被称为*解码器*。整体架构如[图8-4](#autoencoder_architecture_attempts_to_construct)所示。
- en: '![](Images/fdl2_0804.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0804.png)'
- en: Figure 8-4\. The autoencoder architecture attempts to construct a high-dimensional
    input into a low-dimensional embedding and then uses that low-dimensional embedding
    to reconstruct the input
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4。自动编码器架构试图将高维输入构建成低维嵌入，然后使用该低维嵌入来重构输入
- en: To demonstrate the surprising effectiveness of autoencoders, we’ll build and
    visualize the autoencoder architecture in [Figure 8-4](#autoencoder_architecture_attempts_to_construct).
    Specifically, we will highlight its superior ability to separate MNIST digits
    as compared to PCA.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示自动编码器的惊人有效性，我们将构建并可视化自动编码器架构，如[图8-4](#autoencoder_architecture_attempts_to_construct)所示。具体来说，我们将突出其与PCA相比更好地分离MNIST数字的能力。
- en: Implementing an Autoencoder in PyTorch
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中实现自动编码器
- en: The seminal paper “Reducing the Dimensionality of Data with Neural Networks,”
    which describes the autoencoder, was written by Hinton and Salakhutdinov in 2006.^([1](ch08.xhtml#idm45934168575888))
    Their hypothesis was that the nonlinear complexities afforded by a neural model
    would allow them to capture structure that linear methods, such as PCA, would
    miss. To demonstrate this point, they ran an experiment on MNIST using both an
    autoencoder and PCA to reduce the dataset into 2D data points. In this section,
    we will recreate their experimental setup to validate this hypothesis and further
    explore the architecture and properties of feed-forward autoencoders.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年Hinton和Salakhutdinov撰写的开创性论文“使用神经网络降低数据的维度”描述了自动编码器。他们的假设是，神经模型提供的非线性复杂性将使他们能够捕捉线性方法（如PCA）所忽略的结构。为了证明这一点，他们在MNIST上进行了一个实验，使用自动编码器和PCA将数据集减少为2D数据点。在本节中，我们将重新创建他们的实验设置，以验证这一假设，并进一步探索前馈自动编码器的架构和属性。
- en: 'The setup shown in [Figure 8-5](#experimental_setup_for_dimensionality_reduction)
    is built with the same principle, but the 2D embedding is now treated as the input,
    and the network attempts to reconstruct the original image. Because we are essentially
    applying an inverse operation, we architect the decoder network so that the autoencoder
    has the shape of an hourglass. The output of the decoder network is a 784-dimensional
    vector that can be reconstructed into a 28 × 28 image:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-5](#experimental_setup_for_dimensionality_reduction)中显示的设置是基于相同原则构建的，但现在将2D嵌入视为输入，并且网络试图重构原始图像。因为我们实质上是应用一个逆操作，所以我们设计解码器网络，使得自动编码器的形状像一个沙漏。解码器网络的输出是一个784维向量，可以重构为一个28×28的图像：'
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](Images/fdl2_0805.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0805.png)'
- en: Figure 8-5\. The experimental setup for dimensionality reduction of the MNIST
    dataset employed by Hinton and Salakhutdinov, 2006
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5。Hinton和Salakhutdinov在2006年使用的MNIST数据集降维实验设置
- en: 'In order to accelerate training, we’ll reuse the batch normalization strategy
    we employed in [Chapter 7](ch07.xhtml#convolutional_neural_networks). Also, because
    we’d like to visualize the results, we’ll avoid introducing sharp transitions
    in our neurons. In this example, we’ll use sigmoidal neurons instead of our usual
    ReLU neurons:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快训练速度，我们将重用我们在[第7章](ch07.xhtml#convolutional_neural_networks)中使用的批量归一化策略。此外，因为我们想要可视化结果，我们将避免在我们的神经元中引入尖锐的转变。在这个例子中，我们将使用S形神经元而不是我们通常的ReLU神经元：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, we need to construct a measure (or objective function) that describes
    how well our model functions. Specifically, we want to measure how close the reconstruction
    is to the original image. We can measure this simply by computing the distance
    between the original 784-dimensional input and the reconstructed 784-dimensional
    output. More specifically, given an input vector  <math alttext="upper I"><mi>I</mi></math>
     and a reconstruction  <math alttext="upper O"><mi>O</mi></math> , we’d like to
    minimize the value of  <math alttext="parallel-to upper I minus upper O parallel-to
    equals StartRoot sigma-summation Underscript i Endscripts left-parenthesis upper
    I Subscript i Baseline minus upper O Subscript i Baseline right-parenthesis squared
    EndRoot"><mrow><mrow><mo>∥</mo> <mi>I</mi> <mo>-</mo> <mi>O</mi> <mo>∥</mo></mrow>
    <mo>=</mo> <msqrt><mrow><msub><mo>∑</mo> <mi>i</mi></msub> <msup><mfenced separators=""
    open="(" close=")"><msub><mi>I</mi> <mi>i</mi></msub> <mo>-</mo><msub><mi>O</mi>
    <mi>i</mi></msub></mfenced> <mn>2</mn></msup></mrow></msqrt></mrow></math> , also
    known as the L2 norm of the difference between the two vectors. We average this
    function over the whole minibatch to generate our final objective function. Finally,
    we’ll train the network using the Adam optimizer, logging a scalar summary of
    the error incurred at every minibatch using `torch.utils.tensorboard.SummaryWriter`.
    In PyTorch, we can concisely express the loss and training operations as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要构建一个描述我们模型功能如何的度量（或目标函数）。具体来说，我们想要衡量重构与原始图像之间的接近程度。我们可以通过简单地计算原始784维输入和重构的784维输出之间的距离来衡量这一点。更具体地说，给定一个输入向量I和一个重构O，我们希望最小化I和O之间的差值的值，也称为两个向量之间的L2范数。我们将这个函数平均到整个小批次上以生成我们的最终目标函数。最后，我们将使用Adam优化器训练网络，使用`torch.utils.tensorboard.SummaryWriter`在每个小批次记录所产生的错误的标量摘要。在PyTorch中，我们可以简洁地表示损失和训练操作如下：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we’ll need a method to evaluate the generalizability of our model.
    As usual, we’ll use a validation dataset and compute the same L2 norm measurement
    for model evaluation. In addition, we’ll collect image summaries so that we can
    compare both the input images and the reconstructions:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一种方法来评估我们模型的泛化能力。像往常一样，我们将使用一个验证数据集，并计算相同的L2范数测量来评估模型。此外，我们将收集图像摘要，以便我们可以比较输入图像和重构图像：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can visualize the model graph, the training and validation costs, and the
    image summaries using TensorBoard. Simply run the following command:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用TensorBoard可视化模型图、训练和验证成本以及图像摘要。只需运行以下命令：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then navigate your browser to *http://localhost:6006/*. The results of the “Graph”
    tab are shown in [Figure 8-6](#tensorflow_allows_us_to_neatly_view).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将浏览器导航到*http://localhost:6006/*。 “Graph”选项卡的结果显示在[图8-6](#tensorflow_allows_us_to_neatly_view)中。
- en: Thanks to how we’ve namespaced the components of our model graph, our model
    is nicely organized. We can easily click through the components and delve deeper,
    tracing how data flows up through the various layers of the encoder and through
    the decoder, how the optimizer reads the output of our training module, and how
    gradients in turn affect all of the components of the model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对模型图的组件进行了命名空间处理，我们的模型组织得很好。我们可以轻松地点击组件并深入研究，追踪数据如何通过编码器的各个层和解码器的各个层流动，优化器如何读取我们训练模块的输出，以及梯度如何影响模型的所有组件。
- en: We also visualize both the training (after each minibatch) and validation costs
    (after each epoch), closely monitoring the curves for potential overfitting. The
    TensorBoard visualizations of the costs over the span of training are shown in
    [Figure 8-7](#cost_incurred_on_the_training_set). As we would expect for a successful
    model, both the training and validation curves decrease until they flatten off
    asymptotically. After approximately 200 epochs, we attain a validation cost of
    4.78\. While the curves look promising, it’s difficult, upon first glance, to
    understand whether we’ve reached a plateau at a “good” cost, or whether our model
    is still doing a poor job of reconstructing the original inputs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可视化训练（每个小批次后）和验证成本（每个时代后），密切监控曲线以防止过拟合。训练期间成本的TensorBoard可视化显示在[图8-7](#cost_incurred_on_the_training_set)中。正如我们所期望的那样，对于一个成功的模型，训练和验证曲线都会下降，直到渐近平稳。大约在200个时代之后，我们获得了一个验证成本为4.78。虽然曲线看起来很有希望，但乍一看很难理解我们是否已经达到了“好”的成本平台，还是我们的模型仍然在重构原始输入方面做得很差。
- en: '![](Images/fdl2_0806.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0806.png)'
- en: Figure 8-6\. TensorBoard allows us to neatly view the high-level components
    and data flow of our computation graph (top) and also click through to more closely
    inspect the data flows of individual subcomponents (bottom)
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-6. TensorBoard允许我们清晰地查看计算图的高级组件和数据流（顶部），并且还可以点击查看各个子组件的数据流（底部）
- en: '![](Images/fdl2_0807.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0807.png)'
- en: Figure 8-7\. The cost incurred on the training set (logged after each minibatch)
    and on the validation set (logged after each epoch)
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-7. 训练集上产生的成本（每个小批次后记录）和验证集上产生的成本（每个时代后记录）
- en: To get a sense of what that means, let’s explore the MNIST dataset. We pick
    an arbitrary image of a 1 from the dataset and call it *X*. In [Figure 8-8](#compared_to_all_other_digits),
    we compare the image to all other images in the dataset. Specifically, for each
    digit class, we compute the average of the L2 costs, comparing *X* to each instance
    of the digit class. As a visual aid, we also include the average of all of the
    instances for each digit class.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这意味着什么，让我们探索MNIST数据集。我们从数据集中选择一个任意的数字1的图像，并将其称为*X*。在[图8-8](#compared_to_all_other_digits)中，我们将这个图像与数据集中的所有其他图像进行比较。具体来说，对于每个数字类别，我们计算与*X*与该类别的每个实例的L2成本的平均值。作为视觉辅助，我们还包括每个数字类别的所有实例的平均值。
- en: '![](Images/fdl2_0808.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0808.png)'
- en: Figure 8-8\. The image of the 1 on the left is compared to all of the other
    digits in the MNIST dataset; each digit class is represented visually with the
    average of all of its members and labeled with the average of the L2 costs, comparing
    the 1 on the left with all of the class members
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8。左侧的数字1图像与MNIST数据集中的所有其他数字进行比较；每个数字类别在视觉上用其所有成员的平均值表示，并用与左侧的1与该类别所有成员的L2成本的平均值标记
- en: On average, *X* is 5.75 units away from other 1s in MNIST. In terms of L2 distance,
    the non-1 digits closest to the *X* are the 7s (8.94 units) and the digits farthest
    are the 0s (11.05 units). Given these measurements, it’s quite apparent that with
    an average cost of 4.78, our autoencoder is producing high-quality reconstructions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST中，*X*平均与其他1之间的距离为5.75个单位。以L2距离来看，与*X*最接近的非1数字是7（8.94个单位），最远的数字是0（11.05个单位）。根据这些测量结果，很明显，我们的自动编码器产生了高质量的重构，平均成本为4.78。
- en: Because we are collecting image summaries, we can confirm this hypothesis directly
    by inspecting the input images and reconstructions directly. The reconstructions
    for three randomly chosen samples from the test set are shown in [Figure 8-9](#side_by_side_comparison_of_original_inputs).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们正在收集图像摘要，我们可以通过直接检查输入图像和重构来直接确认这一假设。来自测试集的三个随机选择样本的重构显示在[图8-9](#side_by_side_comparison_of_original_inputs)中。
- en: '![](Images/fdl2_0809.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0809.png)'
- en: Figure 8-9\. A side-by-side comparison of the original inputs (from the validation
    set) and reconstructions after 5, 100, and 200 epochs of training
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-9。原始输入（来自验证集）和经过5、100和200个epoch训练后的重构的并排比较
- en: After five epochs, we can start to make out some of the critical strokes of
    the original image that are being picked by the autoencoder, but for the most
    part, the reconstructions are still hazy mixtures of closely related digits. By
    100 epochs, the 0 and 4 are reconstructed with strong strokes, but it looks like
    the autoencoder is still having trouble differentiating between 5s, 3s, and possibly
    8s. However, by 200 epochs, it’s clear that even this more difficult ambiguity
    is clarified, and all of the digits are crisply reconstructed.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 经过五个epoch，我们可以开始看出自动编码器捕捉到原始图像的一些关键笔画，但在大部分情况下，重构仍然是与相关数字的模糊混合。到了100个epoch，0和4被强烈的笔画重构，但看起来自动编码器仍然难以区分5、3和可能8。然而，到了200个epoch，很明显即使是这种更困难的模糊也被澄清，所有数字都被清晰地重构。
- en: 'Finally, we’ll complete the section by exploring the 2D codes produced by traditional
    PCA and autoencoders. We’ll want to show that autoencoders produce better visualizations.
    In particular, we’ll want to show that autoencoders do a much better job of visually
    separating instances of different digit classes than PCA. We’ll start by quickly
    covering the code we use to produce 2D PCA codes:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过探索传统PCA和自动编码器生成的2D代码来完成本节。我们希望展示自动编码器产生更好的可视化效果。特别是，我们希望展示自动编码器在视觉上更好地区分不同数字类别的实例。我们将从快速介绍生成2D
    PCA代码的代码开始：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We first pull up the MNIST dataset. We’ve set the flag `one_hot=False` because
    we’d like the labels to be provided as integers instead of one-hot vectors (as
    a quick reminder, a one-hot vector representing an MNIST label would be a vector
    of size 10 with the <math alttext="i Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    component set to one to represent digit <math alttext="i"><mi>i</mi></math> and
    the rest of the components set to zero). We use the commonly used machine learning
    library *scikit-learn* to perform the PCA, setting the `n_components=2` flat so
    that scikit-learn knows to generate 2D codes. We can also reconstruct the original
    images from the 2D codes and visualize the reconstructions:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们拉取MNIST数据集。我们设置了标志`one_hot=False`，因为我们希望标签以整数形式提供，而不是作为one-hot向量（简单提醒一下，表示MNIST标签的one-hot向量将是一个大小为10的向量，其中第i个分量设置为1以表示数字i，其余分量设置为零）。我们使用常用的机器学习库*scikit-learn*执行PCA，设置`n_components=2`参数，以便scikit-learn知道生成2D代码。我们还可以从2D代码重构原始图像并可视化重构结果：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The code snippet shows how to visualize the first image in the test dataset,
    but we can easily modify the code to visualize any arbitrary subset of the dataset.
    Comparing the PCA reconstructions to the autoencoder reconstructions in [Figure 8-10](#comparing_reconstructions_by_pca_and_autoencoder),
    it’s quite clear that the autoencoder vastly outperforms PCA with 2D codes. In
    fact, the PCA’s performance is somewhat reminiscent of the autoencoder only five
    epochs into training. It has trouble distinguishing 5s from 3s and 8s, 0s from
    8s, and 4s from 9s. Repeating the same experiment with 30-dimensional codes provides
    significant improvement to the PCA reconstructions, but they are still significantly
    worse than the 30-dimensional autoencoder.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段展示了如何可视化测试数据集中的第一幅图像，但我们可以轻松修改代码以可视化数据集的任意子集。将PCA重构与自动编码器重构在[图8-10](#comparing_reconstructions_by_pca_and_autoencoder)中进行比较，很明显自动编码器远远优于具有2D代码的PCA。事实上，PCA的性能有点像自动编码器训练五个epoch后。它难以区分5和3、8，0和8，4和9。使用30维代码重复相同实验，对PCA重构提供了显著改进，但仍然明显差于30维自动编码器。
- en: '![](Images/fdl2_0810.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0810.png)'
- en: Figure 8-10\. Comparing the reconstructions by both PCA and autoencoder side
    by side
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10。将PCA和自动编码器的重建结果并排比较
- en: Now, to complete the experiment, we must load up a saved PyTorch model, retrieve
    the 2D codes, and plot both the PCA and autoencoder codes. We’re careful to rebuild
    the PyTorch graph exactly how we set it up during training. We pass the path to
    the model checkpoint we saved during training as a command-line argument to the
    script. Finally, we use a custom plotting function to generate a legend and appropriately
    color data points of different digit classes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了完成实验，我们必须加载一个保存的PyTorch模型，检索2D代码，并绘制PCA和自动编码器代码。我们小心地重建PyTorch图，确保与训练期间设置的一样。我们将训练期间保存的模型检查点路径作为命令行参数传递给脚本。最后，我们使用自定义绘图函数生成图例，并适当着色不同数字类别的数据点。
- en: In the resulting visualization in [Figure 8-11](#two_dimensional_embeddings_produced_by_pca),
    it is extremely difficult to make out separable clusters in the 2D PCA codes;
    the autoencoder has clearly done a spectacular job at clustering codes of different
    digit classes. This means that a simple machine learning model is going to be
    able to much more effectively classify data points consisting of autoencoder embeddings
    as compared to PCA embeddings.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图8-11](#two_dimensional_embeddings_produced_by_pca)中的可视化结果中，很难看出2D PCA代码中的可分离聚类；自动编码器显然在对不同数字类别的代码进行聚类方面做得非常出色。这意味着一个简单的机器学习模型将能够更有效地对由自动编码器嵌入组成的数据点进行分类，与PCA嵌入相比。
- en: '![](Images/fdl2_0811.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0811.png)'
- en: Figure 8-11\. 2D embeddings produced by PCA (top) and by an autoencoder (bottom)
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-11。PCA生成的2D嵌入（顶部）和自动编码器生成的2D嵌入（底部）
- en: In this section, we successfully set up and trained a feed-forward autoencoder
    and demonstrated that the resulting embeddings were superior to PCA, a classical
    dimensionality reduction method. In the next section, we’ll explore a concept
    known as denoising, which acts as a form of regularization by making our embeddings
    more robust.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们成功地建立并训练了一个前馈自动编码器，并证明了生成的嵌入优于PCA，这是一种经典的降维方法。在下一节中，我们将探讨一种称为去噪的概念，它作为一种正则化形式，使我们的嵌入更加健壮。
- en: Denoising to Force Robust Representations
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪以强制生成稳健的表示形式
- en: '*Denoising* improves the ability of the autoencoder to generate embeddings
    that are resistant to noise. The human ability for perception is surprisingly
    resistant to noise. Take [Figure 8-12](#despite_the_corruption), for example.
    Despite the fact that I’ve corrupted half of the pixels in each image, you still
    have no problem making out the digit. In fact, even easily confused digits (like
    the 2 and the 7) are still distinguishable.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*去噪* 提高了自动编码器生成对噪声具有抗性的嵌入的能力。人类的感知能力对噪声非常有抵抗力。例如，看看[图8-12](#despite_the_corruption)。尽管我在每个图像中破坏了一半的像素，但您仍然可以轻松辨认出数字。事实上，即使是容易混淆的数字（如2和7）仍然可以区分开来。'
- en: '![](Images/fdl2_0812.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0812.png)'
- en: Figure 8-12\. Human perception allows us to identify even obscured digits
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-12。人类感知能力使我们能够识别甚至被遮挡的数字
- en: One way to look at this phenomenon is probabilistically. Even if we’re exposed
    to a random sampling of pixels from an image, if we have enough information, our
    brain is still capable of concluding the ground truth of what the pixels represent
    with maximal probability. Our mind is able to, quite literally, fill in the blanks
    to draw a conclusion. Even though only a corrupted version of a digit hits our
    retina, our brain is still able to reproduce the set of activations (i.e., the
    code or embedding) that we normally would use to represent the image of that digit.
    This is a property we might hope to enforce in our embedding algorithm, and it
    was first explored by Vincent et al. in 2008, when they introduced the *denoising
    autoencoder*.^([2](ch08.xhtml#idm45934167915920))
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这种现象的一种方式是从概率的角度来看。即使我们暴露于图像的随机像素采样中，如果我们有足够的信息，我们的大脑仍然能够以最大概率得出像素代表的真实情况。我们的大脑能够，确实地，填补空白以得出结论。即使我们的视网膜只接收到一个数字的损坏版本，我们的大脑仍然能够重现我们通常用来表示该数字图像的一组激活（即代码或嵌入）。这是我们希望在我们的嵌入算法中强制执行的一种属性，这是由Vincent等人在2008年首次探索的，当时他们引入了*去噪自动编码器*。
- en: 'The basic principles behind denoising are quite simple. We corrupt some fixed
    percentage of the pixels in the input image by setting them to zero. Given an
    original input *X*, let’s call the corrupted version  <math alttext="upper C left-parenthesis
    upper X right-parenthesis"><mrow><mi>C</mi> <mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></math>
    . The denoising autoencoder is identical to the vanilla autoencoder except for
    one detail: the input to the encoder network is the corrupted  <math alttext="upper
    C left-parenthesis upper X right-parenthesis"><mrow><mi>C</mi> <mo>(</mo> <mi>X</mi>
    <mo>)</mo></mrow></math>  instead of *X*. In other words, the autoencoder is forced
    to learn a code for each input that is resistant to the corruption mechanism and
    is able to interpolate through the missing information to recreate the original,
    uncorrupted image.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪背后的基本原理非常简单。我们通过将输入图像中的一定百分比的像素设为零来破坏输入图像。给定原始输入*X*，让我们称破坏版本为 <math alttext="upper
    C left-parenthesis upper X right-parenthesis"><mrow><mi>C</mi> <mo>(</mo> <mi>X</mi>
    <mo>)</mo></mrow></math> 。去噪自动编码器与普通自动编码器相同，除了一个细节：编码器网络的输入是破坏的 <math alttext="upper
    C left-parenthesis upper X right-parenthesis"><mrow><mi>C</mi> <mo>(</mo> <mi>X</mi>
    <mo>)</mo></mrow></math> 而不是*X*。换句话说，自动编码器被迫学习对每个输入都具有抗干扰机制的代码，并且能够通过缺失的信息插值来重新创建原始的未损坏图像。
- en: We can also think about this process more geometrically. Let’s say we had a
    2D dataset with various labels. Let’s take all of the data points in a particular
    category (i.e., with some fixed label), and call this subset of data points *S*.
    While any arbitrary sampling of points could end up taking any form while visualized,
    we presume that for real-life categories, there is some underlying structure that
    unifies all of the points in *S*. This underlying, unifying geometric structure
    is known as a *manifold*. The manifold is the shape that we want to capture when
    we reduce the dimensionality of our data; and as Bengio et al. described in 2013,
    our autoencoder is implicitly learning this manifold as it learns how to reconstruct
    data after pushing it through a bottleneck (the code layer).^([3](ch08.xhtml#idm45934167906880))
    The autoencoder must figure out whether a point belongs to one manifold or another
    when trying to generate a reconstruction of an instance with potentially different
    labels.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: As an illustration, let’s consider the scenario in [Figure 8-13](#denoising_objective),
    where the points in *S* are a simple low-dimensional manifold (a solid circle
    in the diagram). In part A, we see our data points in *S* (black xs) and the manifold
    that best describes them. We also observe an approximation of our corruption operation.
    Specifically, the arrow and nonconcentric circle demonstrate all the ways in which
    the corruption could possibly move or modify a data point. Given that we are applying
    this corruption operation to every data point (i.e., along the entire manifold),
    this corruption operation artificially expands the dataset to not only include
    the manifold but also all of the points in space around the manifold, up to a
    maximum margin of error. This margin is demonstrated by the dashed circles in
    A, and the dataset expansion is illustrated by the x’s in part B. Finally the
    autoencoder is forced to learn to collapse all of the data points in this space
    back to the manifold. In other words, by learning which aspects of a data point
    are generalizable, broad strokes, and which aspects are “noise,” the denoising
    autoencoder learns to approximate the underlying manifold of *S*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0813.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13\. The denoising objective enables our model to learn the manifold
    (dark circle) by learning to map corrupted data (light x’s in B and C) to uncorrupted
    data (dark x’s) by minimizing the error (arrows in C) between their representations
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With the philosophical motivations of denoising in mind, we can now make a
    small modification to our autoencoder script to build a denoising autoencoder:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code snippet corrupts the input if the `corrupt` variable is equal to 1,
    and it refrains from corrupting the input if the `corrupt` variable is equal to
    0\. After making this modification, we can rerun our autoencoder, resulting in
    the reconstructions shown in [Figure 8-14](#apply_a_corruption_operation). It’s
    quite apparent that the denoising autoencoder has faithfully replicated our incredible
    human ability to fill in the missing pixels.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0814.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: Figure 8-14\. We apply a corruption operation to the dataset and train a denoising
    autoencoder to reconstruct the original, uncorrupted images
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sparsity in Autoencoders
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most difficult aspects of deep learning is a problem known as *interpretability*.
    Interpretability is a property of a machine learning model that measures how easy
    it is to inspect and explain its process and/or output. Deep models are generally
    difficult to interpret because of the nonlinearities and massive numbers of parameters
    that make up a model. While deep models are generally more accurate, a lack of
    interpretability often hinders their adoption in highly valuable, but highly risky,
    applications. For example, if a machine learning model is predicting that a patient
    has or does not have cancer, the doctor will likely want an explanation to confirm
    the model’s conclusion.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: We can address one aspect of interpretability by exploring the characteristics
    of the output of an autoencoder. In general, an autoencoder’s representations
    are dense, and this has implications with respect to how the representation changes
    as we make coherent modifications to the input. Consider the situation in [Figure 8-15](#activations_of_a_dense_representation).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过探索自动编码器的输出特征来解决可解释性的一个方面。一般来说，自动编码器的表示是密集的，这对于我们在对输入进行连贯修改时表示如何变化具有影响。考虑在[图8-15](#activations_of_a_dense_representation)中的情况。
- en: '![](Images/fdl2_0815.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0815.png)'
- en: Figure 8-15\. The activations of a dense representation combine and overlay
    information from multiple features in ways that are difficult to interpret
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-15\. 密集表示的激活以难以解释的方式结合和叠加多个特征的信息
- en: The autoencoder produces a *dense* representation, that is, the representation
    of the original image is highly compressed. Because we have only so many dimensions
    to work with in the representation, the activations of the representation combine
    information from multiple features in ways that are extremely difficult to disentangle.
    The result is that as we add components or remove components, the output representation
    changes in unexpected ways. It’s virtually impossible to interpret how and why
    the representation is generated in the way it is.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器产生*密集*表示，即原始图像的表示被高度压缩。由于表示中只有有限的维度可用，表示的激活以极其难以分解的方式结合了多个特征的信息。结果是，当我们添加组件或删除组件时，输出表示以意想不到的方式变化。几乎不可能解释表示是如何生成的以及为什么生成的。
- en: The ideal outcome for us is if we can build a representation where there is
    a 1-to-1 correspondence, or close to a 1-to-1 correspondence, between high-level
    features and individual components in the code. When we are able to achieve this,
    we get very close to the system described in [Figure 8-16](#right_combo_of_space_and_sparsity),
    which shows how the representation changes as we add and remove components. The
    representation is the sum of the individual strokes in the image. With the right
    combination of space and sparsity, a representation is more interpretable.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，理想的结果是我们能够构建一个表示，其中高级特征与代码中的各个组件之间存在一对一的对应，或接近一对一的对应。当我们能够实现这一点时，我们就非常接近[图8-16](#right_combo_of_space_and_sparsity)中描述的系统，该系统显示了随着添加和删除组件表示的变化。表示是图像中各个笔画的总和。通过正确的空间和稀疏组合，表示更具可解释性。
- en: '![](Images/fdl2_0816.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0816.png)'
- en: Figure 8-16\. How activations in the representation change with the addition
    and removal of strokes
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-16\. 表示中的激活随着笔画的添加和删除而变化
- en: While this is the ideal outcome, we’ll have to think through what mechanisms
    we can leverage to enable this interpretability in the representation. The issue
    here is clearly the bottlenecked capacity of the code layer; but unfortunately,
    increasing the capacity of the code layer alone is not sufficient. In the medium
    case, while we can increase the size of the code layer, there is no mechanism
    that prevents each individual feature picked up by the autoencoder from affecting
    a large fraction of the components with smaller magnitudes. In the more extreme
    case, where the features that are picked up are more complex and therefore more
    bountiful, the capacity of the code layer may be even larger than the dimensionality
    of the input. In this case, the code layer has so much capacity that the model
    could quite literally perform a “copy” operation where the code layer learns no
    useful representation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是理想的结果，但我们必须考虑可以利用哪些机制来实现表示中的可解释性。问题显然在于代码层的瓶颈容量；但不幸的是，仅增加代码层的容量是不够的。在中等情况下，虽然我们可以增加代码层的大小，但没有机制可以阻止自动编码器捕捉到的每个单独特征影响具有较小幅度的大部分组件。在更极端的情况下，捕捉到的特征更复杂，因此更丰富，代码层的容量可能甚至大于输入的维度。在这种情况下，代码层的容量非常大，以至于模型可能实际上执行“复制”操作，其中代码层学习不到任何有用的表示。
- en: 'What we really want is to force the autoencoder to utilize as few components
    of the representation vector as possible, while still effectively reconstructing
    the input. This is similar to the rationale behind using regularization to prevent
    overfitting in simple neural networks, as we discussed in [Chapter 4](ch04.xhtml#training_feed_forward),
    except we want as many components to be zero (or extremely close to zero) as possible.
    As in [Chapter 4](ch04.xhtml#training_feed_forward), we’ll achieve this by modifying
    the objective function with a sparsity penalty, which increases the cost of any
    representation that has a large number of nonzero components:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正想要的是强制自动编码器尽可能少地利用表示向量的组件，同时有效地重建输入。这类似于在简单神经网络中使用正则化来防止过拟合的原理，正如我们在[第4章](ch04.xhtml#training_feed_forward)中讨论的那样，只是我们希望尽可能多的组件为零（或非常接近零）。与[第4章](ch04.xhtml#training_feed_forward)一样，我们将通过在目标函数中添加稀疏惩罚来实现这一点，这会增加具有大量非零组件的任何表示的成本：
- en: <math alttext="upper E Subscript Sparse Baseline equals upper E plus beta dot
    SparsityPenalty"><mrow><msub><mi>E</mi> <mtext>Sparse</mtext></msub> <mo>=</mo>
    <mi>E</mi> <mo>+</mo> <mi>β</mi> <mo>·</mo> <mtext>SparsityPenalty</mtext></mrow></math>
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper E Subscript Sparse Baseline equals upper E plus beta dot
    SparsityPenalty"><mrow><msub><mi>E</mi> <mtext>Sparse</mtext></msub> <mo>=</mo>
    <mi>E</mi> <mo>+</mo> <mi>β</mi> <mo>·</mo> <mtext>SparsityPenalty</mtext></mrow></math>
- en: The value of <math alttext="beta"><mi>β</mi></math> determines how strongly
    we favor sparsity at the expense of generating better reconstructions. For the
    mathematically inclined, you would do this by treating the values of each of the
    components of every representation as the outcome of a random variable with an
    unknown mean. We would then employ a measure of divergence comparing the distribution
    of observations of this random variable (the values of each component) and the
    distribution of a random variable whose mean is known to be 0\. A measure that
    is often used to this end is the Kullback-Leibler (often referred to as KL) divergence.
    Further discussion on sparsity in autoencoders is beyond the scope of this text,
    but they are covered by Ranzato et al. (2007^([4](ch08.xhtml#idm45934167858016)) and
    2008^([5](ch08.xhtml#idm45934167856400))). More recently, the theoretical properties
    and empirical effectiveness of introducing an intermediate function before the
    code layer that zeroes out all but  <math alttext="k"><mi>k</mi></math>  of the
    maximum activations in the representation were investigated by Makhzani and Frey
    (2014).^([6](ch08.xhtml#idm45934167853584)) These *k-Sparse autoencoders* were
    shown to be just as effective as other mechanisms of sparsity despite being shockingly
    simple to implement and understand (as well as computationally more efficient).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="beta"><mi>β</mi></math>的值决定了我们在追求稀疏性的同时牺牲生成更好重建的程度。对于数学倾向的人来说，您可以将每个表示的每个组件的值视为具有未知均值的随机变量的结果。然后，我们将使用一个衡量观察这个随机变量（每个组件的值）的分布和已知均值为0的随机变量的分布之间差异的度量。用于此目的的常用度量是Kullback-Leibler（通常称为KL）散度。关于自动编码器中稀疏性的进一步讨论超出了本文的范围，但已被Ranzato等人（2007年^([4](ch08.xhtml#idm45934167858016))和2008年^([5](ch08.xhtml#idm45934167856400))）涵盖。最近，Makhzani和Frey（2014年）^([6](ch08.xhtml#idm45934167853584))研究了在编码层之前引入一个中间函数的理论性质和经验有效性，该函数将表示中的最大激活值之外的所有值都归零。这些*k-稀疏自动编码器*被证明与其他稀疏机制一样有效，尽管实现和理解起来非常简单（以及在计算上更有效）。
- en: This concludes our discussion of autoencoders. We’ve explored how we can use
    autoencoders to find strong representations of data points by summarizing their
    content. This mechanism of dimensionality reduction works well when the independent
    data points are rich and contain all of the relevant information pertaining to
    their structure in their original representation. In the next section, we’ll explore
    strategies that we can use when the main source of information is in the context
    of the data point instead of the data point itself.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对自动编码器的讨论。我们已经探讨了如何使用自动编码器通过总结其内容来找到数据点的强表示。当独立数据点丰富并包含有关其结构的所有相关信息时，这种降维的机制效果很好。在下一节中，我们将探讨当主要信息源是数据点的上下文而不是数据点本身时，我们可以使用的策略。
- en: When Context Is More Informative than the Input Vector
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当上下文比输入向量更具信息性时
- en: So far, we’ve mostly focused on the concept of dimensionality reduction. In
    dimensionality reduction, we generally have rich inputs that contain lots of noise
    on top of the core, structural information that we care about. In these situations,
    we want to extract this underlying information while ignoring the variations and
    noise that are extraneous to this fundamental understanding of the data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要关注了降维的概念。在降维中，我们通常有包含大量噪音的丰富输入，这些噪音覆盖了我们关心的核心结构信息。在这些情况下，我们希望提取这些基本信息，同时忽略与数据的基本理解无关的变化和噪音。
- en: In other situations, we have input representations that say very little at all
    about the content that we are trying to capture. In these situations, our goal
    is not to extract information but rather to gather information from context to
    build useful representations. All of this probably sounds too abstract to be useful
    at this point, so let’s concretize these ideas with a real example.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，我们有输入表示几乎没有关于我们试图捕捉的内容的信息。在这些情况下，我们的目标不是提取信息，而是从上下文中收集信息以构建有用的表示。在这一点上，所有这些可能听起来太抽象而不实用，所以让我们用一个真实的例子来具体化这些想法。
- en: Building models for language is a tricky business. The first problem we have
    to overcome when building language models is finding a good way to represent individual
    words. At first glance, it’s not entirely clear how one builds a good representation.
    Let’s start with the naive approach, considering [Figure 8-17](#example_of_generating_one_hot_vector_reps).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为语言构建模型是一项棘手的工作。构建语言模型时我们必须克服的第一个问题是找到表示单词的好方法。乍一看，如何构建一个好的表示并不完全清楚。让我们从天真的方法开始，考虑[图8-17](#example_of_generating_one_hot_vector_reps)。
- en: '![](Images/fdl2_0817.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0817.png)'
- en: Figure 8-17\. Generating one-hot vector representations for words using a simple
    document
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-17。使用简单文档生成单热向量表示单词
- en: If a document has a vocabulary  <math alttext="upper V"><mi>V</mi></math>  with 
    <math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo> <mi>V</mi>
    <mo>|</mo></mrow></math>  words, we can represent the words with one-hot vectors.
    We have  <math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>V</mi> <mo>|</mo></mrow></math> -dimensional representation vectors, and we
    associate each unique word with an index in this vector. To represent unique word 
    <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math> , we set
    the  <math alttext="i Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
     component of the vector to be 1, and zero out all of the other components.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个文档有一个词汇表<math alttext="upper V"><mi>V</mi></math>，其中有<math alttext="StartAbsoluteValue
    upper V EndAbsoluteValue"><mrow><mo>|</mo> <mi>V</mi> <mo>|</mo></mrow></math>个单词，我们可以用单热向量表示这些单词。我们有<math
    alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo> <mi>V</mi>
    <mo>|</mo></mrow></math>维表示向量，并将每个唯一的单词与此向量中的一个索引关联起来。为了表示唯一的单词<math alttext="w
    Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>，我们将向量的第<math alttext="i
    Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>个分量设置为1，并将所有其他分量归零。
- en: However, this representation scheme seems rather arbitrary. This vectorization
    does not make similar words into similar vectors. This is problematic, because
    we’d like our models to know that the words “jump” and “leap” have similar meanings.
    Similarly, we’d like our models to know when words are verbs or nouns or prepositions.
    The naive one-hot encoding of words to vectors does not capture any of these characteristics.
    To address this challenge, we’ll need to find some way of discovering these relationships
    and encoding this information into a vector.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种表示方案似乎相当任意。这种向量化并不会使相似的词成为相似的向量。这是有问题的，因为我们希望我们的模型知道“jump”和“leap”这两个词有相似的含义。同样，我们希望我们的模型知道哪些词是动词、名词或介词。将单词进行朴素的独热编码到向量中并不捕捉这些特征。为了解决这一挑战，我们需要找到一种方法来发现这些关系，并将这些信息编码到一个向量中。
- en: It turns out that one way to discover relationships between words is by analyzing
    their surrounding context. For example, synonyms such as “jump” and “leap” can
    be used interchangeably in their respective contexts. In addition, both words
    generally appear when a subject is performing the action over a direct object.
    We use this principle all the time when we run across new vocabulary while reading.
    For example, if we read the sentence “The warmonger argued with the crowd,” we
    can immediately draw conclusions about the word “warmonger” even if we don’t already
    know the dictionary definition. In this context, “warmonger” precedes a word we
    know to be a verb, which makes it likely that “warmonger” is a noun and the subject
    of this sentence. Also, the “warmonger” is “arguing,” which might imply that a
    “warmonger” is generally a combative or argumentative individual. Overall, as
    illustrated in [Figure 8-18](#id_words_with_similar_meanings), by analyzing the
    context (i.e., a fixed window of words surrounding a target word), we can quickly
    surmise the meaning of the word.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，发现单词之间关系的一种方法是分析它们周围的上下文。例如，同义词“jump”和“leap”可以在各自的上下文中互换使用。此外，这两个词通常出现在主语执行动作的直接对象上。当我们阅读时，我们一直在使用这个原则。例如，如果我们读到句子“The
    warmonger argued with the crowd”，即使我们不知道字典定义，我们也可以立即对“warmonger”这个词做出推断。在这个上下文中，“warmonger”在我们知道是一个动词之前出现，这使得“warmonger”很可能是一个名词，是这个句子的主语。此外，“warmonger”在“arguing”，这可能意味着“warmonger”通常是一个好斗或好争论的个体。总的来说，如[图8-18](#id_words_with_similar_meanings)所示，通过分析上下文（即围绕目标词的固定窗口词），我们可以快速推断单词的含义。
- en: '![](Images/fdl2_0818.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0818.png)'
- en: Figure 8-18\. Analyzing context to determine a word’s meaning
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-18. 分析上下文以确定单词含义
- en: 'It turns out we can use the same principles we used when building the autoencoder
    to build a network that builds strong, distributed representations. Two strategies
    are shown in [Figure 8-19](#general_architectures_for_designing_encoders). One
    possible method (shown in A) passes the target through an encoder network to create
    an embedding. Then we have a decoder network take this embedding; but instead
    of trying to reconstruct the original input as we did with the autoencoder, the
    decoder attempts to construct a word from the context. The second possible method
    (shown in B) does exactly the reverse: the encoder takes a word from the context
    as input, producing the target.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，我们可以使用构建自动编码器时使用的相同原则来构建一个生成强大、分布式表示的网络。[图8-19](#general_architectures_for_designing_encoders)展示了两种策略。一种可能的方法（如A所示）通过编码器网络将目标传递，以创建一个嵌入。然后，我们有一个解码器网络接受这个嵌入；但是与自动编码器中尝试重建原始输入不同，解码器尝试从上下文中构建一个词。第二种可能的方法（如B所示）完全相反：编码器将上下文中的一个词作为输入，生成目标。
- en: '![](Images/fdl2_0819.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0819.png)'
- en: Figure 8-19\. General architectures for designing encoders and decoders that
    generate embeddings by mapping words to their respective contexts (A) or vice
    versa (B)
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-19. 设计编码器和解码器的一般架构，通过将单词映射到它们各自的上下文（A）或反之（B）生成嵌入
- en: In the next section, we’ll describe how we use this strategy (along with some
    slight modifications for performance) to produce word embeddings in practice.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将描述如何使用这种策略（以及一些性能上的轻微修改）来实际产生单词嵌入。
- en: The Word2Vec Framework
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec框架
- en: Word2Vec, a framework for generating word embeddings, was pioneered by Mikolov
    et al. The original paper detailed two strategies for generating embeddings, similar
    to the two strategies for encoding context we discussed in the previous section.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是由Mikolov等人开创的用于生成单词嵌入的框架。原始论文详细介绍了生成嵌入的两种策略，类似于我们在前一节讨论的编码上下文的两种策略。
- en: The first flavor of Word2Vec that Mikolov et al. introduced was the *Continuous
    Bag of Words* (CBOW) model.^([7](ch08.xhtml#idm45934167802832)) This model is
    much like strategy B from [Figure 8-19](#general_architectures_for_designing_encoders).
    The CBOW model used the encoder to create an embedding from the full context (treated
    as one input) and predict the target word. It turns out this strategy works best
    for smaller datasets, an attribute that is further discussed in the original paper.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Mikolov等人介绍的Word2Vec的第一种模型是Continuous Bag of Words（CBOW）模型。这个模型与[图8-19](#general_architectures_for_designing_encoders)中的策略B非常相似。CBOW模型使用编码器从完整上下文（作为一个输入）创建嵌入并预测目标词。事实证明，这种策略对于较小的数据集效果最好，这一属性在原始论文中进一步讨论。
- en: The second flavor of Word2Vec is the *Skip-Gram model*, introduced by Mikolov
    et al.^([8](ch08.xhtml#idm45934167799008)) The Skip-Gram model does the inverse
    of CBOW, taking the target word as an input, and then attempting to predict one
    of the words in the context. Let’s walk through a toy example to explore what
    the dataset for a Skip-Gram model looks like.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec的第二种模型是Skip-Gram模型，由Mikolov等人介绍。Skip-Gram模型与CBOW相反，将目标词作为输入，然后尝试预测上下文中的一个词。让我们通过一个玩具示例来探索Skip-Gram模型的数据集是什么样子的。
- en: Consider the sentence “the boy went to the bank.” If we broke this sentence
    down into a sequence of (context, target) pairs, we would obtain [([the, went],
    boy), ([boy, to], went), ([went, the], to), ([to, bank], the)]. Taking this a
    step further, we have to split each (context, target) pair into (input, output)
    pairs where the input is the target and the output is one of the words from the
    context. From the first pair ([the, went], boy), we would generate the two pairs
    (boy, the) and (boy, went). We continue to apply this operation to every (context,
    target) pair to build our dataset. Finally, we replace each word with its unique
    index  <math alttext="i element-of StartSet 0 comma 1 comma ellipsis comma StartAbsoluteValue
    upper V EndAbsoluteValue minus 1 EndSet"><mrow><mi>i</mi> <mo>∈</mo> <mo>{</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>,</mo> <mo>...</mo> <mo>,</mo> <mo>|</mo>
    <mi>V</mi> <mo>|</mo> <mo>-</mo> <mn>1</mn> <mo>}</mo></mrow></math> corresponding
    to its index in the vocabulary.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑句子“the boy went to the bank.”如果我们将这个句子分解成一系列的（上下文，目标）对，我们将得到[([the, went],
    boy), ([boy, to], went), ([went, the], to), ([to, bank], the)]。进一步地，我们必须将每个（上下文，目标）对拆分成（输入，输出）对，其中输入是目标，输出是上下文中的一个词。从第一个对([the,
    went], boy)开始，我们将生成两对(boy, the)和(boy, went)。我们继续将这个操作应用到每个（上下文，目标）对，以构建我们的数据集。最后，我们用词汇表中的索引替换每个单词的唯一索引
    <math alttext="i element-of StartSet 0 comma 1 comma ellipsis comma StartAbsoluteValue
    upper V EndAbsoluteValue minus 1 EndSet"><mrow><mi>i</mi> <mo>∈</mo> <mo>{</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>,</mo> <mo>...</mo> <mo>,</mo> <mo>|</mo>
    <mi>V</mi> <mo>|</mo> <mo>-</mo> <mn>1</mn> <mo>}</mo></mrow></math>。
- en: 'The structure of the encoder is surprisingly simple. It is essentially a lookup
    table with <math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>V</mi> <mo>|</mo></mrow></math> rows, where the <math alttext="i Superscript
    t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>  row is
    the embedding corresponding to the <math alttext="i Superscript t h"><msup><mi>i</mi>
    <mrow><mi>t</mi><mi>h</mi></mrow></msup></math> vocabulary word. All the encoder
    has to do is take the index of the input word and output the appropriate row in
    the lookup table. This an efficient operation because on a GPU, this operation
    can be represented as a product of the transpose of the lookup table and the one-hot
    vector representing the input word. We can implement this simply in PyTorch with
    the following PyTorch function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的结构非常简单。它本质上是一个查找表，有<math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>V</mi> <mo>|</mo></mrow></math>行，其中第<math alttext="i Superscript t h"><msup><mi>i</mi>
    <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>行是对应于第<math alttext="i Superscript
    t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>个词汇的嵌入。编码器所需做的就是获取输入单词的索引，并输出查找表中的适当行。这是一个高效的操作，因为在GPU上，这个操作可以表示为查找表的转置和表示输入单词的独热向量的乘积。我们可以在PyTorch中简单实现这一点，使用以下PyTorch函数：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Where `out` is the embedding matrix, and `x` is a tensor of indices we want
    to look up. For information on optional parameters, we refer you to the [PyTorch
    API documentation](https://oreil.ly/NaQWV).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`out`是嵌入矩阵，`x`是我们想要查找的索引张量。有关可选参数的信息，请参阅[PyTorch API文档](https://oreil.ly/NaQWV)。
- en: The decoder is slightly trickier because we make some modifications for performance.
    The naive way to construct the decoder would be to attempt to reconstruct the
    one-hot encoding vector for the output, which we could implement with a run-of-the-mill
    feed-forward layer coupled with a softmax. The only concern is that it’s inefficient
    because we have to produce a probability distribution over the whole vocabulary
    space.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器稍微复杂，因为我们对性能进行了一些修改。构建解码器的朴素方法是尝试重建输出的独热编码向量，我们可以使用一个普通的前馈层和softmax来实现。唯一的问题是效率低下，因为我们必须在整个词汇空间上产生一个概率分布。
- en: To reduce the number of parameters, Mikolov et al. used a strategy for implementing
    the decoder known as noise-contrastive estimation (NCE). The strategy is illustrated
    in [Figure 8-20](#illustration_of_noise_contrastive_esimation). A binary logistic
    regression compares the embedding of the target with the embedding of a context
    word and randomly sampled noncontext words. We construct a loss function describing
    how effectively the embeddings enable identification of words in the context of
    the target versus words outside the context of the target.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少参数数量，Mikolov等人使用了一种实现解码器的策略，称为噪声对比估计（NCE）。该策略在[图8-20](#illustration_of_noise_contrastive_esimation)中有所说明。二元逻辑回归比较目标的嵌入与上下文单词的嵌入以及随机抽样的非上下文单词。我们构建了一个损失函数，描述了嵌入如何有效地使得识别目标上下文中的单词与目标外的单词成为可能。
- en: '![](Images/fdl2_0820.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0820.png)'
- en: Figure 8-20\. The NCE strategy
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-20。NCE策略
- en: The NCE strategy uses the lookup table to find the embedding for the output,
    as well as embeddings for random selections from the vocabulary that are not in
    the context of the input. We then employ a binary logistic regression model that,
    one at a time, takes the input embedding and the embedding of the output or random
    selection, and then outputs a value between 0 to 1 corresponding to the probability
    that the comparison embedding represents a vocabulary word present in the input’s
    context. We then take the sum of the probabilities corresponding to the noncontext
    comparisons and subtract the probability corresponding to the context comparison.
    This value is the objective function that we want to minimize (in the optimal
    scenario where the model has perfect performance, the value will be –1).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: NCE策略使用查找表来找到输出的嵌入，以及来自词汇表的非上下文输入的嵌入。然后，我们使用二元逻辑回归模型，一次一个，获取输入嵌入和输出或随机选择的嵌入，然后输出一个值介于0到1之间，对应于比较嵌入表示词汇单词是否存在于输入上下文中的概率。然后，我们取与非上下文比较对应的概率之和，并减去与上下文比较对应的概率。这个值是我们要最小化的目标函数（在模型表现完美的最佳情况下，该值将为-1）。
- en: An example of implementing NCE in PyTorch can be found on [GitHub](https://oreil.ly/lH2ip).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中实现NCE的示例可以在[GitHub](https://oreil.ly/lH2ip)上找到。
- en: While Word2Vec is admittedly not a deep machine learning model, we discuss it
    here for many reasons. First, it thematically represents a strategy (finding embeddings
    using context) that generalizes to many deep learning models. When we learn about
    models for sequence analysis in [Chapter 9](ch09.xhtml#ch07), we’ll see this strategy
    employed for generating skip-thought vectors to embed sentences. Moreover, when
    we start building more and more models for language starting in [Chapter 9](ch09.xhtml#ch07),
    we’ll find that using Word2Vec embeddings instead of one-hot vectors to represent
    words will yield far superior results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Word2Vec毫无疑问不是一个深度机器学习模型，但我们在这里讨论它有很多原因。首先，它在主题上代表了一种策略（使用上下文找到嵌入），这种策略可以推广到许多深度学习模型。当我们在[第9章](ch09.xhtml#ch07)学习序列分析模型时，我们将看到这种策略用于生成skip-thought向量以嵌入句子。此外，当我们从[第9章](ch09.xhtml#ch07)开始构建更多语言模型时，我们将发现使用Word2Vec嵌入代替独热向量来表示单词将产生更好的结果。
- en: Now that we understand how to architect the Skip-Gram model and its importance,
    we can start implementing it in PyTorch.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了如何设计Skip-Gram模型及其重要性，我们可以开始在PyTorch中实现它。
- en: Implementing the Skip-Gram Architecture
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Skip-Gram架构
- en: To build the dataset for our Skip-Gram model, we’ll utilize a modified version
    of the PyTorch Word2Vec data reader in `input_word_data.py`. We’ll start off by
    setting a couple of important parameters for training and regularly inspecting
    our model. Of particular note, we employ a minibatch size of 32 examples and train
    for 5 epochs (full passes through the dataset). We’ll use embeddings of size 128\.
    We’ll use a context window of five words to the left and to the right of each
    target word, and sample four context words from this window. Finally, we’ll use
    64 randomly chosen noncontext words for NCE.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的Skip-Gram模型的数据集，我们将使用`input_word_data.py`中的PyTorch Word2Vec数据读取器的修改版本。我们将首先设置一些重要的训练参数，并定期检查我们的模型。特别值得注意的是，我们使用32个示例的小批量大小，并训练5个时期（完整通过数据集）。我们将使用大小为128的嵌入。我们将使用每个目标词左右各五个单词的上下文窗口，并从该窗口中随机选择四个上下文单词。最后，我们将使用64个随机选择的非上下文单词进行NCE。
- en: 'Implementing the embedding layer is not particularly complicated. We merely
    have to initialize the lookup table with a matrix of values:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 实现嵌入层并不特别复杂。我们只需用一个值矩阵初始化查找表：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'PyTorch does not currently have a built-in NCE loss function. However, there
    are some implementations on the internet.  One example is the *info-nce-pytorch*
    library:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch目前没有内置的NCE损失函数。但是，互联网上有一些实现。一个例子是*info-nce-pytorch*库：
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We utilize `InfoNCE` to compute the NCE cost for each training example, and
    then compile all of the results in the minibatch into a single measurement:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用`InfoNCE`来计算每个训练样本的NCE成本，然后将所有结果编译到一个小批量中进行单一测量：
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now that we have our objective function expressed as a mean of the NCE costs,
    we set up the training as usual. Here, we follow in the footsteps of Mikolov et
    al. and employ stochastic gradient descent with a learning rate of 0.1:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将我们的目标函数表达为NCE成本的平均值，我们像往常一样设置训练。在这里，我们跟随Mikolov等人的脚步，使用学习率为0.1的随机梯度下降：
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We also inspect the model regularly using a validation function, which normalizes
    the embeddings in the lookup table and uses cosine similarity to compute distances
    for a set of validation words from all other words in the vocabulary:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定期使用验证函数检查模型，该函数将查找表中的嵌入归一化，并使用余弦相似度计算一组验证单词与词汇表中所有其他单词之间的距离：
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Putting all of these components together, we’re finally ready to run the Skip-Gram
    model. We skim over this portion of the code because it is very similar to how
    we constructed models in the past. The only difference is the additional code
    during the inspection step. We randomly select 20 validation words out of the
    500 most common words in our vocabulary of 10,000 words. For each of these words,
    we use the cosine similarity function we built to find the nearest neighbors:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些组件放在一起，我们终于准备好运行Skip-Gram模型。我们略过这部分代码，因为它与我们过去构建模型的方式非常相似。唯一的区别是在检查步骤中的额外代码。我们从我们的词汇表中最常见的500个单词中随机选择20个验证单词。对于这些单词中的每一个，我们使用我们构建的余弦相似度函数找到最近的邻居：
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The code starts to run, and we can begin to see how the model evolves over
    time. At the beginning, the model does a poor job of embedding (as is apparent
    from the inspection step). However, by the time training completes, the model
    has clearly found representations that effectively capture the meanings of individual
    words:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 代码开始运行，我们可以开始看到模型随时间的演变。在开始时，模型在嵌入方面表现不佳（从检查步骤中可以看出）。然而，当训练完成时，模型显然已经找到了有效捕捉单词含义的表示：
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: While not perfect, there are some strikingly meaningful clusters captured here.
    Numbers, countries, and cultures are clustered close together. The pronoun “I”
    is clustered with other pronouns. The word “world” is interestingly close to both
    “championship” and “war.” And the word “written” is found to be similar to “translated,”
    “poetry,” “alphabet,” “letters,” and “words.”
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不完美，但这里捕捉到了一些引人注目的有意义的聚类。数字、国家和文化被紧密聚集在一起。代词“I”与其他代词聚集在一起。单词“world”有趣地接近“championship”和“war”。而单词“written”被发现与“translated”、“poetry”、“alphabet”、“letters”和“words”相似。
- en: Finally, we conclude this section by visualizing our word embeddings in [Figure 8-21](#viz_of_skip_gram_embeddings).
    To display our 128-dimensional embeddings in 2D space, we’ll use a visualization
    method known as t-SNE. If you’ll recall, we also used t-SNE in [Chapter 7](ch07.xhtml#convolutional_neural_networks)
    to visualize the relationships between images in ImageNet. Using t-SNE is quite
    simple, as it has a built-in function in the commonly used machine learning library
    scikit-learn.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过在[图8-21](#viz_of_skip_gram_embeddings)中可视化我们的词嵌入来结束本节。为了在2D空间中显示我们的128维嵌入，我们将使用一种称为t-SNE的可视化方法。如果您还记得，我们在[第7章](ch07.xhtml#convolutional_neural_networks)中也使用t-SNE来可视化ImageNet中图像之间的关系。使用t-SNE非常简单，因为它在常用的机器学习库scikit-learn中有一个内置函数。
- en: 'We can construct the visualization using the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码构建可视化：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In [Figure 8-21](#viz_of_skip_gram_embeddings), we notice that similar concepts
    are closer together than disparate concepts, indicating that our embeddings encode
    meaningful information about the functions and definitions of individual words.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图8-21](#viz_of_skip_gram_embeddings)中，我们注意到相似的概念比不同的概念更接近，表明我们的嵌入对于单词的功能和定义编码了有意义的信息。
- en: '![](Images/fdl2_0821.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0821.png)'
- en: Figure 8-21\. Skip-Gram embeddings using t-SNE
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-21。使用t-SNE的Skip-Gram嵌入
- en: For a more detailed exploration of the properties of word embeddings and interesting
    patterns (verb tenses, countries and capitals, analogy completion, etc.), we refer
    you to the original Mikolov et al. paper.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有关单词嵌入的属性和有趣模式（动词时态、国家和首都、类比完成等）的更详细探讨，我们建议您参考原始的Mikolov等人的论文。
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored various methods in representation learning. We
    learned about how we can perform effective dimensionality reduction using autoencoders.
    We also learned about denoising and sparsity, which augment autoencoders with
    useful properties. After discussing autoencoders, we shifted our attention to
    representation learning when context of an input is more informative than the
    input itself. We learned how to generate embeddings for English words using the
    Skip-Gram model, which will prove useful as we explore deep learning models for
    understanding language. In the next chapter, we will build on this tangent to
    analyze language and other sequences using deep learning.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了表示学习中的各种方法。我们了解了如何使用自动编码器进行有效的降维。我们还学习了去噪和稀疏性，这些增强了自动编码器的有用属性。在讨论完自动编码器后，我们将注意力转向当输入的上下文比输入本身更具信息性时的表示学习。我们学习了如何使用Skip-Gram模型为英语单词生成嵌入，这将在我们探索用于理解语言的深度学习模型时非常有用。在下一章中，我们将在此基础上分析语言和其他序列使用深度学习。
- en: '^([1](ch08.xhtml#idm45934168575888-marker)) Hinton, Geoffrey E., and Ruslan
    R. Salakhutdinov. “Reducing the Dimensionality of Data with Neural Networks.”
    *Science* 313.5786 (2006): 504-507.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.xhtml#idm45934168575888-marker)) Hinton, Geoffrey E., and Ruslan
    R. Salakhutdinov. “使用神经网络降低数据的维度。”*科学*313.5786（2006）：504-507。
- en: ^([2](ch08.xhtml#idm45934167915920-marker)) Vincent, Pascal, et al. “Extracting
    and Composing Robust Features with Denoising Autoencoders.” *Proceedings of the
    25th International Conference on Machine Learning*. ACM, 2008.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch08.xhtml#idm45934167915920-marker)) Vincent, Pascal, et al. “使用去噪自动编码器提取和组合稳健特征。”*第25届国际机器学习会议论文集*。ACM，2008年。
- en: ^([3](ch08.xhtml#idm45934167906880-marker)) Bengio, Yoshua, et al. “Generalized
    Denoising Auto-Encoders as Generative Models.” *Advances in Neural Information
    Processing Systems*. 2013.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch08.xhtml#idm45934167906880-marker)) Bengio, Yoshua, et al. “广义去噪自动编码器作为生成模型。”*神经信息处理系统进展*。2013年。
- en: ^([4](ch08.xhtml#idm45934167858016-marker)) Ranzato, Marc’Aurelio, et al. “Efficient
    Learning of Sparse Representations with an Energy-Based Model.” *Proceedings of
    the 19th International Conference on Neural Information Processing Systems*. MIT
    Press, 2006.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch08.xhtml#idm45934167858016-marker)) Ranzato, Marc’Aurelio, et al. “使用基于能量的模型高效学习稀疏表示。”*第19届神经信息处理系统国际会议论文集*。MIT出版社，2006年。
- en: ^([5](ch08.xhtml#idm45934167856400-marker)) Ranzato, Marc’Aurelio, and Martin
    Szummer. “Semi-supervised Learning of Compact Document Representations with Deep
    Networks.” *Proceedings of the 25th International Conference on Machine Learning*.
    ACM, 2008.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch08.xhtml#idm45934167856400-marker)) Ranzato, Marc’Aurelio, and Martin
    Szummer. “半监督学习中的紧凑文档表示与深度网络。”*第25届国际机器学习会议论文集*。ACM，2008年。
- en: ^([6](ch08.xhtml#idm45934167853584-marker)) Makhzani, Alireza, and Brendan Frey.
    “k-Sparse Autoencoders.” *arXiv preprint arXiv*:1312.5663 (2013).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch08.xhtml#idm45934167853584-marker)) Makhzani, Alireza, and Brendan Frey.
    “k-稀疏自动编码器。”*arXiv预印本arXiv*：1312.5663（2013）。
- en: ^([7](ch08.xhtml#idm45934167802832-marker)) Mikolov, Tomas, et al. “Distributed
    Representations of Words and Phrases and their Compositionality.” *Advances in
    Neural Information Processing Systems*. 2013.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch08.xhtml#idm45934167802832-marker)) Mikolov, Tomas, et al. “单词和短语的分布式表示及其组合性。”*神经信息处理系统进展*。2013年。
- en: ^([8](ch08.xhtml#idm45934167799008-marker)) Tomas Mikolov, Kai Chen, Greg Corrado,
    and Jeffrey Dean. “Efficient Estimation of Word Representations in Vector Space.”
    *ICLR Workshop*, 2013.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch08.xhtml#idm45934167799008-marker)) Tomas Mikolov, Kai Chen, Greg Corrado,
    and Jeffrey Dean. “在向量空间中高效估计单词表示。”*ICLR研讨会*，2013年。
