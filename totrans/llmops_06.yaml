- en: Chapter 6\. API-First LLM Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing the right tools for deploying LLMs can make or break your project.
  prefs: []
  type: TYPE_NORMAL
- en: Open source tools give you more control but require you to do more work, while
    managed services are easier to set up and scale but often come at a higher cost.
    A popular repository of open source tools and data is HuggingFace, which contains
    a lot of pretrained models and tools to help with tasks like tokenization, fine-tuning,
    and data processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The business model you choose will impact your revenue, costs, and user experience
    and, thus, also your deployment decision. By understanding your users’ needs,
    evaluating your costs, and considering your competition, you can choose a business
    model that meets your needs and provides value to your users. Options include:'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as a service (IaaS)
  prefs: []
  type: TYPE_NORMAL
- en: This model is suitable for organizations that want to build and deploy their
    own LLM applications but don’t want to manage the underlying infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: With IaaS, organizations can provision and configure computing resources quickly
    and easily, without the need for significant up-front investment. It provides
    flexibility and control over the infrastructure, allowing organizations to customize
    and optimize the environment for their specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: IaaS is a good fit for organizations that have the expertise and resources to
    manage their own applications and infrastructure. However, it requires a higher
    level of technical expertise and management than do other business models.
  prefs: []
  type: TYPE_NORMAL
- en: Platform as a service (PaaS)
  prefs: []
  type: TYPE_NORMAL
- en: This model is suitable for organizations that want to build and deploy LLM applications
    quickly and easily, without worrying about the underlying infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: With PaaS, organizations can focus on building and deploying their applications,
    without the need for significant up-front investment or technical expertise. It
    provides a simplified and streamlined development and deployment process, allowing
    organizations to quickly build and deploy applications.
  prefs: []
  type: TYPE_NORMAL
- en: PaaS is a good fit for organizations that want to quickly build and deploy LLM
    applications. However, it may not provide the same level of flexibility and control
    as other business models.
  prefs: []
  type: TYPE_NORMAL
- en: Software as a service (SaaS)
  prefs: []
  type: TYPE_NORMAL
- en: With SaaS, organizations can access the LLM’s capabilities through a web interface
    or API, without the need for significant up-front investment or technical expertise.
    This model provides a simplified and streamlined user experience, allowing organizations
    to quickly and easily access LLM capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: SaaS is a good fit for organizations that want to quickly and easily access
    LLM capabilities without significant technical expertise or management. However,
    it may not provide the same level of flexibility and control as other business
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Most companies today are somewhere between using LLMs as IaaS or SaaS offerings
    via APIs, in which case the integration is pretty straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter walks you through the deployment steps one by one and then offers
    tips on APIs, knowledge graphs, latency, and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Your Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deploying an LLM from a cloud service is straightforward. For example, to deploy
    a model by OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the OpenAI website and create an account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the API keys page and create a new API key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the API key securely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the OpenAI Python library using `pip install openai`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the OpenAI library in your code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Call the client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this chapter, I will assume that you want to deploy your own models. While
    the principles of MLOps apply to some extent, LLMOps requires specific adjustments
    to handle the unique challenges of large-scale models.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the application, LLMOps workflows may involve pre- and postprocessing,
    chaining models, inference optimization, and integrating external systems like
    knowledge bases or APIs. Also, it requires handling large-scale text data, vectorized
    embeddings, and often RAG techniques for improving context in predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how to do that using an example project. Let’s say you have a
    model you have already developed, called `my-llm-model`. The next step is to deploy
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Set Up Your Environment'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is to ensure the necessary tools are installed. Some recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins for automating CI/CD pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker to containerize the model and its dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes for orchestrating scalable and fault-tolerant deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZenML or MLFlow for more complex workflow orchestration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Containerize the LLM'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Containerization ensures that your LLM and its dependencies will be portable
    and consistent across environments. Create a `Dockerfile` in the project directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the Docker image and test the container locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Automate Pipelines with Jenkins'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Automating deployment pipelines allows for reliable and repeatable processes.
    I recommend using Jenkins for CI/CD automation. Here’s how to implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: Install Jenkins and configure it to connect with your repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a `Jenkinsfile` to define the pipeline stages. This pipeline builds
    the Docker image, pushes it to a container registry, and deploys it to Kubernetes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Step 4: Workflow Orchestration'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For complex workflows, tools like ZenML and MLFlow let you define modular steps
    and manage dependencies. Here’s how to install ZenML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: Set Up Monitoring'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once deployed, monitoring is the key to making sure your LLM application is
    performing as expected. Tools like Prometheus and Grafana can track model latency,
    system resource usage, and error rates, or you can use an LLM-specific tool, like
    Log10.io.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to deploy an LLM, you might want to provide your model
    to other users without making it open source. The next section looks into APIs
    for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Developing APIs for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: APIs provide users a standardized way for clients to interact with their LLM
    and for developers to access and consume LLM services and models from a variety
    of sources. Following the best practices of LLMOps, as we’ll show you in this
    section, will help you make your APIs secure, reliable, easy to use, and ensure
    that they provide the functionality and performance that LLM-based applications
    need.
  prefs: []
  type: TYPE_NORMAL
- en: APIs have been around since the 1960s and 1970s. These early APIs were primarily
    used for system-level programming, allowing different components to communicate
    with each other within a single OS. With the rise of the internet in the 1990s,
    people began using APIs for web-based applications as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Web APIs allow different websites and web applications to communicate and exchange
    data with each other, based on two core rules of software development: high cohesion
    and loose coupling. *High cohesion* means that the components of an API are closely
    related and focused on a single task. This makes the API easier to understand
    and maintain. *Loose coupling* means that the components of an API are independent
    of each other, allowing them to change without affecting other parts. This increases
    flexibility and reduces dependencies.'
  prefs: []
  type: TYPE_NORMAL
- en: Today, web APIs are an essential component of modern web-based applications,
    enabling developers to create powerful, integrated systems that can be accessed
    from anywhere at any time. Some common web APIs used by LLM-based applications
    include NLP APIs and LLMs-as-APIs.
  prefs: []
  type: TYPE_NORMAL
- en: '*NLP APIs* provide access to natural language processing functionalities such
    as tokenization, part-of-speech tagging, and named-entity recognition libraries.
    Tools include Hugging Face and spaCy.'
  prefs: []
  type: TYPE_NORMAL
- en: '*LLMs-as-APIs* provide access to LLMs and make predictions based on user prompts.
    They can be divided into two main categories. *LLM platform APIs* provide access
    to LLM platforms and services that enable developers to build, train, and deploy
    LLM models. Examples include Google Cloud LLM, Amazon SageMaker, and Microsoft
    Azure Machine Learning. *LLM model APIs* provide access to pretrained LLM models
    that can be used to make inferences on text, images, or speech. Model APIs are
    typically used for text generation, classification, and language translation.
    This category includes all the proprietary model APIs: OpenAI, Cohere, Anthropic,
    Ollama, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Platform APIs* provide a range of services and tools for building, training,
    and deploying LLM models including end-to-end deployment tooling for data preparation,
    model training, model deployment, and model monitoring. LLM platform APIs’ most
    important benefit is that they allow developers to reuse existing LLM models and
    services, reducing the amount of time and effort required to build new applications.
    For example, Google Studio (with the Gemini family of models) is a suite of LLM
    services that enables developers to build, train, and deploy LLM models.'
  prefs: []
  type: TYPE_NORMAL
- en: API-Led Architecture Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An *API-led architecture strategy* is a design approach for deploying LLM-based
    applications using APIs to create complex, integrated systems that are scalable,
    flexible, and reusable; that can be accessed from anywhere, at any time; and that
    can handle large volumes of data and traffic. It involves using APIs to expose
    the functionality and data of different systems and services.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two kinds of web APIs: stateful and stateless. A *stateful* API maintains
    and manages the state of a client or user session. The server keeps track of the
    state of the client or user and uses this information to provide personalized
    and context-aware responses based on the state of the client or user. This can
    improve the user experience by providing more relevant and useful information.
    A stateful API can also provide secure access and authentication to protect against
    unauthorized access and use. Examples of stateful APIs are shopping-cart APIs,
    user authentication APIs, content management APIs, and real-time communication
    APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Stateless* APIs do not store any information about previous requests. Each
    request is independent and contains all the necessary data to be processed. If
    one request fails, it doesn’t affect others because there’s no stored state. This
    means you can use stateless APIs across different environments or platforms without
    worrying about session continuity.'
  prefs: []
  type: TYPE_NORMAL
- en: REST APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: REST APIs are not inherently stateful or stateless, but they can be used to
    create both, depending on the requirements and the techniques you use.
  prefs: []
  type: TYPE_NORMAL
- en: Representational State Transfer (REST) is a type of web API that follows the
    RESTful architectural style. REST APIs are stateless, meaning each request contains
    all the information needed to complete the request. However, they can still maintain
    and manage the state of a client or user using techniques such as sessions, cookies,
    or tokens.
  prefs: []
  type: TYPE_NORMAL
- en: By using REST APIs, you can create scalable, flexible, and reusable systems
    that can handle large volumes of data and traffic. They can also provide the functionality
    and performance that modern web-based applications need.
  prefs: []
  type: TYPE_NORMAL
- en: API Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s look into how to implement an API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Define Your API’s Endpoints'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Common endpoints include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/generate`: For generating text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/summarize`: For summarization tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/embed`: For retrieving embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Choose an API Development Framework'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we will use FastAPI, a Python framework that simplifies API
    development while supporting asynchronous operations. Let’s implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Test the API'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start the FastAPI server using `python app.py`. Once you have created your API,
    it’s important to manage it effectively to keep it secure, reliable, and performant.
    *API management* is a set of practices and tools for monitoring, maintaining,
    and improving your API. You should consider your API management approach before
    you even start developing your API. Good API management reduces the risk of security
    breaches and provides valuable insights into how your API is being used, and it
    makes the API a valuable asset that delivers value for your organization and users.
  prefs: []
  type: TYPE_NORMAL
- en: 'API management activities include monitoring performance, handling errors,
    implementing security measures, and regularly updating and maintaining the API.
    Managing the API for an LLM-based application involves several steps. The following
    list is high level and not all-inclusive:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify your application’s key functionalities and define the API endpoints
    you’ll use to access them. For example, you might have endpoints for generating
    text, retrieving model information, and/or managing user accounts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decide on the API design, such as whether to use a RESTful or GraphQL API, and
    what data format to use (for example, JSON). Make sure to follow best practices
    for API design, such as using meaningful endpoint names, providing clear and concise
    documentation, and using appropriate HTTP status codes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the API using a web framework (such as Flask or Django for Python
    or Express for Node.js). Make sure to handle errors gracefully, validate input
    data, and implement appropriate security measures, such as authentication and
    rate limiting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate the LLM into your API by creating a wrapper around the LLM library
    or API. This wrapper should handle input/output formatting, error handling, and
    any other necessary functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thoroughly test the API using automated testing tools such as PyTest or Jest.
    Make sure to test all endpoints, input validation, error handling, and performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy the API to a production environment using a cloud provider such as AWS,
    Google Cloud, or Azure. Make sure to use best practices for deployment, such as
    using continuous integration/continuous deployment (CI/CD), monitoring performance,
    and implementing security measures such as firewalls and access controls.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the API for performance issues, errors, and security vulnerabilities.
    Implement logging and alerting mechanisms to notify you of any issues. Regularly
    maintain the API by updating dependencies, fixing bugs, and adding new features
    as needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Credential Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most ignored yet most critical components of API management is *credential
    management*. Credentials include any sensitive information, such as API keys,
    authentication tokens, or user passwords, that are used to access your application
    or API. To manage credentials effectively, make sure to store them securely, such
    as by using a secure vault or encryption. Avoid hard-coding credentials into your
    code or configuration files, as this can increase the risk of exposure. Instead,
    use environment variables or secure configuration files that are not committed
    to version control.
  prefs: []
  type: TYPE_NORMAL
- en: You should also implement access controls to limit who can access credentials.
    This can include using role-based access control (RBAC) or attribute-based access
    control (ABAC) to restrict access to sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, regularly rotate credentials to reduce the risk of exposure. This can
    include setting expiration dates for API keys or tokens, or requiring users to
    change their passwords periodically.
  prefs: []
  type: TYPE_NORMAL
- en: API Gateways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An *API gateway* is a critical component of your LLM-based application. It provides
    a single entry point for all API requests and handles multiple services. It routes
    requests and handles load balancing, authentication, and sometimes caching or
    logging, acting as a middle layer between clients and microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up an API gateway for your LLM-based application:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose an API gateway provider that meets your needs in terms of features, scalability,
    and cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define your API by specifying the endpoints, methods, and request/response formats.
    Make sure to use meaningful endpoint names and provide clear, concise documentation
    and appropriate HTTP status codes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement authentication and authorization mechanisms, such as OAuth or JWT,
    to ensure that only authorized users can access your API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement rate limiting to prevent abuse (such as denial-of-service, or DoS,
    attacks) and ensure fair use of your API. This might include setting a maximum
    number of requests per minute or hour or implementing more advanced rate-limiting
    algorithms. Monitor and log API activity to detect and respond to security threats,
    performance issues, or errors. This can include the implementation of logging
    and alerting mechanisms to notify you of any issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test your API thoroughly to ensure that it meets your functional and nonfunctional
    requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy it to a production environment using AWS, Google Cloud, or Azure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up an API gateway for an LLM-based application has several advantages.
    It provides a single entry point for all API requests, making it easier to manage
    and monitor API traffic. This helps you identify and respond to security threats,
    performance issues, and errors faster. API gateways can handle authentication
    and authorization tasks, such as verifying API keys or tokens, and enforce access
    controls. They can also log and monitor API activity, providing valuable insights
    into how your LLM-based application is being used. Most importantly, an API gateway
    can implement rate limiting to prevent abuse and ensure fair use of your API.
  prefs: []
  type: TYPE_NORMAL
- en: API Versioning and Lifecycle Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*API versioning* is the process of maintaining multiple versions of an API
    to ensure backward compatibility and minimize the impact of changes on existing
    users.'
  prefs: []
  type: TYPE_NORMAL
- en: To version your API, first include the version number in the API endpoint or
    request header. This makes it easy to identify which version of the API is being
    used. Then use semantic versioning to indicate the level of backward compatibility,
    which can help users understand the impact of changes and plan accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to document all changes between versions, including any breaking changes
    or deprecated features. This can help users understand how to migrate to the new
    version. You can include providing tools or scripts to help users update their
    code or configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'But it doesn’t stop at versioning. Your LLMOps strategy also needs to define
    your approach to *API lifecycle management*, from design and development to deployment
    and retirement. The first step is to define the API lifecycle stages, such as
    planning, development, testing, deployment, and retirement. From there, the components
    you’ll need include:'
  prefs: []
  type: TYPE_NORMAL
- en: Governance model
  prefs: []
  type: TYPE_NORMAL
- en: A governance model establishes roles and responsibilities, defines processes
    and workflows, and determines which tools and technologies are acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: Change management process
  prefs: []
  type: TYPE_NORMAL
- en: Defining a change management process will help ensure that any future changes
    to the API are planned, tested, and communicated to users effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and alerting
  prefs: []
  type: TYPE_NORMAL
- en: You need a monitoring and alerting system to detect and respond to issues or
    errors. This can include setting up alerts for performance issues, security threats,
    or errors. Most API deployment platforms offer this as a service. An example is
    Azure Application Insights, a tool that checks how long each step of your API
    calls is taking and automatically alerts you to performance problems or errors.
  prefs: []
  type: TYPE_NORMAL
- en: Retirement process
  prefs: []
  type: TYPE_NORMAL
- en: Finally, agree on and document a retirement process to decommission the API
    when it is no longer needed. This can include notifying users, providing a migration
    path, and archiving data.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Deployment Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The two most common deployment architectures for software applications and LLM-based
    applications are modular and monolithic*.*
  prefs: []
  type: TYPE_NORMAL
- en: Modular and Monolithic Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each architecture has its strengths and use cases, and both require careful
    planning. *Modular architectures* break the system down into its components. Modular
    designs are easier to update and scale, making them ideal for applications requiring
    flexibility. *Monolithic architectures* handle everything within a single framework.
    These models offer simplicity and tightly integrated workflows.
  prefs: []
  type: TYPE_NORMAL
- en: For modular systems, you’ll train components like retrievers, re-rankers, and
    generators independently. This approach allows you to focus on optimizing each
    module. It requires defining the communication between modules extremely well;
    most issues in modular systems happen when modules communicate incorrectly with
    one another. In contrast, monolithic architectures often involve end-to-end training,
    which simplifies dependencies but demands significant computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: After training, save your model in a format that supports the architecture;
    for example, use open formats like ONNX for interoperability or native formats
    like PyTorch or TensorFlow for custom pipelines. Validation is crucial for both
    approaches. In terms of testing, modular systems require component-specific tests
    to ensure compatibility and performance, while monolithic architectures need comprehensive
    end-to-end evaluation to confirm their robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Microservices-Based Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say you’ve decided to adopt a *microservices-based architecture* for your
    LLM application. This is a modular architectural style that breaks a large application
    down into smaller, independent services that communicate with each other through
    APIs. Its benefits include improved scalability, flexibility, and maintainability.
  prefs: []
  type: TYPE_NORMAL
- en: In a microservice-based architecture, APIs serve as connectors between the different
    services. Each service exposes an API that allows other services to interact with
    it. APIs *decouple* the different services, allowing them to evolve independently.
    This means that changes to one service do not impact other services, reducing
    the risk of breaking changes.
  prefs: []
  type: TYPE_NORMAL
- en: APIs also enable services to scale independently, allowing you to allocate resources
    more efficiently. For example, you could scale your language translation service
    independently of your text-to-speech service. With APIs, you can build different
    services using different technologies and programming languages. This means that
    you can choose the best technology for each service, improving development speed
    and reducing technical debt.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use different APIs as connectors for a microservice-based architecture for
    LLM applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Define clear and consistent APIs for each service, defining the input and output
    formats, authentication and authorization mechanisms, and error handling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement standard API communication protocols, such as HTTP or gRPC, for compatibility
    and interoperability between services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement security mechanisms, such as OAuth or JWT, to authenticate and authorize
    API requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement monitoring and logging mechanisms to track API usage and detect issues.
    This can help you identify and resolve issues quickly and improve the user experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement versioning mechanisms to manage changes to the APIs and minimize their
    impact on existing applications and users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach can help you build a scalable, flexible, and maintainable LLM
    application with multiple APIs that meets the needs of your users and enables
    distributed functionality for large-scale applications. Let’s look at how to implement
    your microservices architecture in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Decompose the application into its components'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Preprocessing service to tokenize and clean input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference service to perform LLM inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Postprocessing service to format or enrich model outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at sample code for a preprocessing service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Establish communication between services'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use HTTP for simplicity or gRPC (Google’s Remote Procedure Call) for
    high performance. Add a message broker like RabbitMQ or Kafka for asynchronous
    communication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Coordinate the microservices to keep the workflows seamless'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can use tools like Consul or Eureka to register and discover services dynamically,
    or you might implement an API Gateway (such as Kong or NGINX) to route client
    requests to the appropriate microservice. Here’s an NGINX example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you plan to use a tool like MLFlow or BentoML to manage service dependencies
    and task execution, you can also implement it at this step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Create a Dockerfile for each microservice'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s​ an example using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s another example for deploying to Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to test your Kubernetes deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Automating RAG with Retriever Re-ranker Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building efficient retriever re-ranker pipelines is a key step in implementing
    RAG pipeline workflows. *Retriever re-ranker* pipelines retrieve relevant context
    and rank it for input to the LLM. As you’ve seen throughout this book, automation
    is critical to ensuring a system’s scalability and reliability. As we move through
    this section, you’ll get pointers about how using frameworks like LangChain and
    LlamaIndex can simplify this process.
  prefs: []
  type: TYPE_NORMAL
- en: Start with the *retriever*, which fetches relevant data based on a query. You
    can use dense vector embeddings and store them in a vector database, like Pinecone
    or Milvus. Once it retrieves results, the *re-ranker* reorders those results by
    relevance. LangChain provides modular components to integrate these steps seamlessly,
    allowing you to create pipelines to automate data retrieval and ranking with minimal
    intervention. LlamaIndex adds features for integrating retrieval systems with
    structured data sources, offering flexibility in managing knowledge sources.
  prefs: []
  type: TYPE_NORMAL
- en: Automation ensures that your retriever re-ranker pipeline is always up-to-date.
    This is especially useful when dealing with dynamic data, like user-generated
    content or frequently updated knowledge bases. Regular validation and retraining
    improve the accuracy of these pipelines over time.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an implementation that retrieves documents, re-ranks them, and
    feeds the most relevant context to an LLM ([Example 6-1](#ch06_example_1_1748919660048653)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-1\. Building a retriever re-ranker pipeline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: At step 2, you use Pinecone to fetch the top-*k* relevant documents based on
    the query embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: In step 4, a simple function ranks retrieved documents by semantic similarity
    using the embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: For better results, you can replace the simple scoring with neural re-rankers
    like T5 or BERT, add memory to the pipeline to handle multi-turn queries, or automate
    database updates using scheduled tasks for dynamic content.
  prefs: []
  type: TYPE_NORMAL
- en: Automating Knowledge Graph Updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keeping your knowledge graphs (KGs) up-to-date is essential for maintaining
    accurate insights. Automation simplifies this process, especially for tasks like
    entity linking and generating graph embeddings. It reduces manual effort, increases
    accuracy, and ensures that your knowledge graphs remain a reliable source of information.
  prefs: []
  type: TYPE_NORMAL
- en: '*Entity linking* ensures that new information connects to the correct nodes
    in the KG. For example, if a document references “Paris,” entity linking determines
    whether that refers to the city or a person’s name. Automated pipelines handle
    this by combining neural natural language processing (NNLP) models with preexisting
    graph structures and by using embeddings to understand relationships and context.
    Tools like spaCy and specialized libraries for entity resolution can help you
    build robust linking systems.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Graph embeddings* are numerical representations of nodes, edges, and their
    relationships. They enable tasks like graph search, recommendation, and reasoning.
    To make sure your KG reflects the latest data, it’s wise to automate embedding
    creation and updates. That way, the pipelines can schedule updates whenever new
    data arrives, ensuring the KG stays accurate and ready for downstream applications.
    Libraries like PyTorch Geometric and DGL (Deep Graph Library) provide embedding
    generation tools. Regularly validate your pipelines to prevent errors from propagating
    through the graph.'
  prefs: []
  type: TYPE_NORMAL
- en: The next example walks you through how to automate KG updates by building a
    pipeline using Python. The libraries used here are spaCy for entity linking and
    PyTorch Geometric and DGL for graph embeddings. For the KG itself, a Neo4j graph
    database is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In step 3, the `link_entities_and_update_kg()` function uses spaCy to extract
    named entities from the input text. It then updates the Neo4j knowledge graph
    by linking each entity (e.g., “John von Neumann,” “computer science”) as a node.
    The `MERGE` clause ensures that entities are created only if they don’t already
    exist in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: In step 4, we use PyTorch Geometric to compute graph embeddings using graph
    convolutional networks (GCNs). Nodes and edges are defined manually, and the GCNConv
    layer is applied to compute new embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'At step 5, the `automate_kg_update()` function combines the two steps: it first
    links entities and updates the KG, and then it computes the graph embeddings to
    keep the knowledge graph up-to-date with the latest entity information and structure.
    To automate the process, schedule the `automate_kg_update()` function to run periodically
    via cron jobs or a task scheduler like Celery.'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment Latency Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reducing latency is one of the most important considerations when deploying
    LLMs. Latency directly impacts performance and responsiveness. Some applications,
    such as chatbots, search engines, and real-time decision-making systems, require
    especially low latency, making it essential to find ways to minimize the time
    it takes for the system to return results.
  prefs: []
  type: TYPE_NORMAL
- en: One effective approach is to use Triton Inference Server, an open source platform
    designed specifically for high-performance model inference. It supports a wide
    variety of model types, including TensorFlow, PyTorch, ONNX, and others. Triton
    significantly optimizes LLM execution, making it possible to handle multiple concurrent
    inference requests with minimal delay.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few reasons for this. First, it supports model concurrency and can
    run models on GPUs. It can also dynamically load and unload models based on demand,
    which is useful for applications requiring low latency, such as chatbots, search
    engines, or real-time decision-making systems. Triton also supports *batching*,
    which allows it to combine multiple inference requests into a single operation,
    further improving throughput and reducing the overall response time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy an LLM using Triton Inference Server for optimized execution, first
    install Triton:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, prepare the model directory. Make sure to save your model in a directory
    that Triton can access and in a format like TensorFlow SavedModel or PyTorch TorchScript:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run Triton from the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, query the server for inference. You can use a client library like
    `tritonclient` to send requests to the Triton server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Orchestrating Multiple Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To achieve efficiency and good response times in systems that require multiple
    models to work together, you’ll need to use *multi-model orchestration,* which
    involves breaking models down into microservices. You’ll then deploy each model
    as an independent service, and they can interact through APIs or message queues.
    There are multiple ready-to-use orchestrators out there including Multi-Agent
    Orchestrator by AWS, and proxy tools like LiteLLM that allow you to switch between
    multiple models and APIs. But as with everything else in software, the higher
    the dependencies, the higher the debugging complexity when inferencing fails in
    mission-critical tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you might have separate models for different stages of processing:
    one for text preprocessing, another for text-to-speech, and another for generating
    responses. Orchestration can ensure that each part of the process happens concurrently
    and efficiently, reducing bottlenecks and speeding up the overall system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use container orchestration tools like Kubernetes or Docker Compose
    to manage multiple models running as microservices. Here’s how to create a `docker-compose.yml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Orchestrate the communication between the models using a message queue like
    RabbitMQ or through direct API calls. Each service listens for input and processes
    it sequentially or concurrently as needed.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll also need to set up *load balancing* to manage traffic between models
    and distribute requests efficiently. You’ll need to configure Kubernetes or Docker
    Swarm to run multiple instances of your models and balance the incoming traffic.
    Kubernetes uses a service to route requests to the appropriate pod, while Docker
    Swarm uses Docker’s built-in load balancer to automatically distribute traffic
    between containers. Let’s assume you have a Docker container running a model;
    for instance, a `model_image` Docker image. You want to deploy multiple instances
    of this model and use Kubernetes to load-balance the incoming requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a Kubernetes deployment configuration file, which will define
    the model container, and specify how many replicas of it you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This configuration will deploy three replicas. The Kubernetes Deployment will
    manage the *pods* running these models (the smallest deployable units in Kubernetes)
    and will automatically balance traffic. To distribute traffic among them, you
    need to expose them using a Kubernetes Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This service will expose the three model replicas on port 80 and balance the
    traffic among them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can deploy the model and the service to your Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: There are a few advantages to this level of modularity. For one thing, it allows
    you to scale each model independently, based on the demand for its specific task.
    For instance, if you have more requests for text generation than for entity recognition,
    you can scale up the text generation model without affecting the other models.
    Additionally, if one model fails, the other models can continue to operate, keeping
    the system available. That means you can swap out one model for a newer version
    or replace it with a different model to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing RAG Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimizing RAG pipelines is crucial for achieving efficiency and low latency
    in information retrieval and text generation tasks. Their performance depends
    heavily on how well you optimized the retrieval pipeline. This section will show
    you several techniques to significantly improve RAG performance.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous Querying
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Asynchronous querying* is a powerful optimization technique that allows multiple
    queries to be processed concurrently, reducing the waiting time for each query.
    In traditional synchronous retrieval systems, each query is processed sequentially,
    but this causes delays when multiple requests are made at once. Asynchronous querying
    addresses this bottleneck by allowing the system to send queries to the vector
    store simultaneously and then wait for the responses in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how you might implement asynchronous querying using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `asyncio.gather()` sends all queries to Facebook AI Similarity
    Search (FAISS) at once and waits for the responses asynchronously. This allows
    the system to process multiple queries in parallel, reducing the overall latency.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Dense and Sparse Retrieval Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Dense retrieval* leverages embeddings to represent both queries and documents
    in a vector space, allowing for similarity searches based on vector distances.
    *Sparse retrieval* methods, such as TF-IDF, rely on term-based matching, which
    can capture more nuanced keyword-based relevance. Dense retrieval is particularly
    useful for capturing semantic relevance, while sparse retrieval excels at exact
    keyword matching. Combining both methods allows you to leverage the strengths
    of each for more accurate and comprehensive results. To do so, try the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In this example, FAISS handles dense vector-based retrieval, while Whoosh handles
    the sparse, keyword-based search. The results are then combined, offering both
    semantic and exact-match retrieval, which can improve the overall accuracy and
    completeness of the system’s responses.
  prefs: []
  type: TYPE_NORMAL
- en: Cache Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of recomputing embeddings for frequently queried data, use *embedding
    caching* to let the system store embeddings and reuse them for subsequent queries.
    If the embeddings for a query are already stored in the cache, the system retrieves
    them; otherwise, it computes the embeddings and stores them for future use. This
    reduces the need to reprocess the same data, significantly decreasing response
    times and improving efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how to implement embedding caching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Key–Value Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Key–value (KV) caching* works similarly to embedding caching. It stores the
    results of key-value pairs, where the key is a query or an intermediate result
    and the value is the corresponding response or computed result. This allows the
    system to retrieve precomputed results instead of recalculating them each time
    it processes a repeated query. KV caching speeds up both the retrieval and generation,
    especially in large-scale, high-traffic systems.'
  prefs: []
  type: TYPE_NORMAL
- en: In RAG systems, KV caching is typically applied during the retrieval phase to
    speed up the query–response cycle. In the generation phase, the model may use
    versions or parts of cached documents and responses to build its final output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how to implement it in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In step 1, you connect to a Redis instance running locally to store the key-value
    pairs for quick lookup.
  prefs: []
  type: TYPE_NORMAL
- en: Then step 3 stipulates that when a query is received, the code checks if the
    embeddings for that query are already cached in Redis by checking the key (`embedding:<query>`).
    If the cache contains the embeddings (called a *cache hit*), it directly retrieves
    and returns them. If not (a *cache miss*), the embeddings are computed using `SentenceTransformer`
    and then stored. The embeddings are stored in Redis as bytes using `tobytes()`
    to ensure they can be retrieved in the same format.
  prefs: []
  type: TYPE_NORMAL
- en: By reducing the need to recompute embeddings or model responses, KV caching
    can help lower compute costs and reduce the strain on both the retrieval and generation
    components, ensuring that the system remains responsive even under heavy load.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability and Reusability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scalability and reusability are essential for handling high-traffic systems.
    In large-scale environments, the ability to scale your infrastructure efficiently
    is critical. *Distributed inference orchestration* allows the system to distribute
    the load across multiple nodes as traffic increases, with each handling a portion
    of the overall request. This reduces the chances of any single machine becoming
    overwhelmed.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is usually used to manage the scaling process by automating task
    distribution and adjusting resources as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Reusable components make it easier to scale and manage your pipeline. They can
    be replicated across different services or projects quickly because they don’t
    require significant modifications. This is especially important in environments
    with constant updates and iterations. ZenML and similar orchestration tools allow
    you to create reusable pipelines, which you can modify or extend without disrupting
    the entire system. As you build new models or add new tasks, you can reuse existing
    components to maintain consistency and reduce development time.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed inference orchestration and reusable components work hand in hand
    to ensure that your system is both scalable and maintainable. When traffic spikes
    or new use cases arise, it’s important to know that you can rely on your existing
    infrastructure to handle the demands. This makes the entire system more resilient
    and agile in adapting to new challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability and reusability are not just nice-to-haves but necessary features
    for high-traffic LLM systems. Distributed inference orchestration ensures that
    your system can scale to meet demand, while reusable components make it easier
    to maintain and expand the system over time. Together, they allow for efficient
    and effective handling of large-scale LLM deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The right stack will depend on your project goals. Open source tools are great
    if you need flexibility and have technical resources to manage the setup. Managed
    services are perfect for teams that prioritize speed and simplicity. Carefully
    assess your needs before committing to a stack, as the right choice will save
    time, improve performance, and help you deploy more effectively.
  prefs: []
  type: TYPE_NORMAL
