- en: Chapter 6\. API-First LLM Deployment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章\. API优先的LLM部署
- en: Choosing the right tools for deploying LLMs can make or break your project.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的工具来部署LLM可以成就或毁掉你的项目。
- en: Open source tools give you more control but require you to do more work, while
    managed services are easier to set up and scale but often come at a higher cost.
    A popular repository of open source tools and data is HuggingFace, which contains
    a lot of pretrained models and tools to help with tasks like tokenization, fine-tuning,
    and data processing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 开源工具赋予你更多的控制权，但需要你做更多的工作，而托管服务更容易设置和扩展，但通常成本更高。一个流行的开源工具和数据存储库是HuggingFace，其中包含大量预训练模型和用于任务如分词、微调和数据处理等工具。
- en: 'The business model you choose will impact your revenue, costs, and user experience
    and, thus, also your deployment decision. By understanding your users’ needs,
    evaluating your costs, and considering your competition, you can choose a business
    model that meets your needs and provides value to your users. Options include:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择的企业模式将影响你的收入、成本和用户体验，从而也会影响你的部署决策。通过了解用户需求、评估成本和考虑竞争，你可以选择满足你需求并为用户提供价值的商业模式。选项包括：
- en: Infrastructure as a service (IaaS)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施即服务（IaaS）
- en: This model is suitable for organizations that want to build and deploy their
    own LLM applications but don’t want to manage the underlying infrastructure.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式适合那些希望构建和部署自己的LLM应用程序但不想管理底层基础设施的组织。
- en: With IaaS, organizations can provision and configure computing resources quickly
    and easily, without the need for significant up-front investment. It provides
    flexibility and control over the infrastructure, allowing organizations to customize
    and optimize the environment for their specific needs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 使用IaaS，组织可以快速轻松地配置计算资源，无需进行大量前期投资。它提供了对基础设施的灵活性和控制，使组织能够根据特定需求定制和优化环境。
- en: IaaS is a good fit for organizations that have the expertise and resources to
    manage their own applications and infrastructure. However, it requires a higher
    level of technical expertise and management than do other business models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: IaaS适合那些拥有管理和维护自身应用程序和基础设施的专业知识和资源的组织。然而，它比其他商业模式需要更高水平的专业技术和管理。
- en: Platform as a service (PaaS)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 平台即服务（PaaS）
- en: This model is suitable for organizations that want to build and deploy LLM applications
    quickly and easily, without worrying about the underlying infrastructure.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式适合那些希望快速轻松地构建和部署LLM应用程序，而不必担心底层基础设施的组织。
- en: With PaaS, organizations can focus on building and deploying their applications,
    without the need for significant up-front investment or technical expertise. It
    provides a simplified and streamlined development and deployment process, allowing
    organizations to quickly build and deploy applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PaaS，组织可以专注于构建和部署他们的应用程序，无需进行大量前期投资或专业技术。它提供了一个简化和流程化的开发和部署过程，使组织能够快速构建和部署应用程序。
- en: PaaS is a good fit for organizations that want to quickly build and deploy LLM
    applications. However, it may not provide the same level of flexibility and control
    as other business models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PaaS适合那些希望快速构建和部署LLM应用程序的组织。然而，它可能不会提供与其他商业模式相同水平的灵活性和控制。
- en: Software as a service (SaaS)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 软件即服务（SaaS）
- en: With SaaS, organizations can access the LLM’s capabilities through a web interface
    or API, without the need for significant up-front investment or technical expertise.
    This model provides a simplified and streamlined user experience, allowing organizations
    to quickly and easily access LLM capabilities.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SaaS，组织可以通过网页界面或API访问LLM的功能，无需进行大量前期投资或专业技术。这种模式提供了一个简化和流程化的用户体验，使组织能够快速轻松地访问LLM功能。
- en: SaaS is a good fit for organizations that want to quickly and easily access
    LLM capabilities without significant technical expertise or management. However,
    it may not provide the same level of flexibility and control as other business
    models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SaaS适合那些希望快速轻松地访问LLM功能，而不需要大量专业技术或管理的组织。然而，它可能不会提供与其他商业模式相同水平的灵活性和控制。
- en: Most companies today are somewhere between using LLMs as IaaS or SaaS offerings
    via APIs, in which case the integration is pretty straightforward.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的大多数公司都在使用LLM作为IaaS或SaaS通过API提供的产品之间，在这种情况下，集成相当直接。
- en: This chapter walks you through the deployment steps one by one and then offers
    tips on APIs, knowledge graphs, latency, and optimization.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将逐步向您介绍部署步骤，然后提供有关API、知识图谱、延迟和优化的技巧。
- en: Deploying Your Model
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署您的模型
- en: 'Deploying an LLM from a cloud service is straightforward. For example, to deploy
    a model by OpenAI:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从云服务部署LLM很简单。例如，要使用OpenAI部署模型：
- en: Go to the OpenAI website and create an account.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往OpenAI网站并创建一个账户。
- en: Navigate to the API keys page and create a new API key.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到API密钥页面并创建一个新的API密钥。
- en: Save the API key securely.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安全地保存API密钥。
- en: Install the OpenAI Python library using `pip install openai`.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pip install openai`安装OpenAI Python库。
- en: Import the OpenAI library in your code.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的代码中导入OpenAI库。
- en: 'Call the client:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用客户端：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this chapter, I will assume that you want to deploy your own models. While
    the principles of MLOps apply to some extent, LLMOps requires specific adjustments
    to handle the unique challenges of large-scale models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将假设您想部署自己的模型。虽然MLOps的原则在一定程度上适用，但LLMOps需要针对大规模模型的独特挑战进行特定调整。
- en: Depending on the application, LLMOps workflows may involve pre- and postprocessing,
    chaining models, inference optimization, and integrating external systems like
    knowledge bases or APIs. Also, it requires handling large-scale text data, vectorized
    embeddings, and often RAG techniques for improving context in predictions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用的不同，LLMOps工作流程可能涉及预处理和后处理、模型链式操作、推理优化以及集成外部系统，如知识库或API。此外，它还需要处理大规模文本数据、向量嵌入以及通常用于提高预测上下文的RAG技术。
- en: Let’s look at how to do that using an example project. Let’s say you have a
    model you have already developed, called `my-llm-model`. The next step is to deploy
    it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例项目来看看如何做到这一点。假设您已经开发了一个名为`my-llm-model`的模型。下一步是部署它。
- en: 'Step 1: Set Up Your Environment'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：设置您的环境
- en: 'The first step is to ensure the necessary tools are installed. Some recommendations:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是确保安装了必要的工具。以下是一些建议：
- en: Jenkins for automating CI/CD pipelines
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Jenkins自动化CI/CD管道
- en: Docker to containerize the model and its dependencies
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker对模型及其依赖项进行容器化
- en: Kubernetes for orchestrating scalable and fault-tolerant deployments
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes编排可扩展和容错部署
- en: ZenML or MLFlow for more complex workflow orchestration
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ZenML或MLFlow进行更复杂的流程编排
- en: 'Step 2: Containerize the LLM'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步：容器化LLM
- en: 'Containerization ensures that your LLM and its dependencies will be portable
    and consistent across environments. Create a `Dockerfile` in the project directory:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 容器化确保您的LLM及其依赖项可以在不同环境中便携和一致。在项目目录中创建一个`Dockerfile`：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Build the Docker image and test the container locally:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 构建Docker镜像并在本地测试容器：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Step 3: Automate Pipelines with Jenkins'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：使用Jenkins自动化管道
- en: 'Automating deployment pipelines allows for reliable and repeatable processes.
    I recommend using Jenkins for CI/CD automation. Here’s how to implement it:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化部署管道可以实现可靠和可重复的过程。我建议使用Jenkins进行CI/CD自动化。以下是实现方法：
- en: Install Jenkins and configure it to connect with your repository.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装Jenkins并将其配置为与您的仓库连接。
- en: 'Create a `Jenkinsfile` to define the pipeline stages. This pipeline builds
    the Docker image, pushes it to a container registry, and deploys it to Kubernetes:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`Jenkinsfile`来定义管道阶段。此管道构建Docker镜像，将其推送到容器注册库，并将其部署到Kubernetes：
- en: '[PRE3]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 4: Workflow Orchestration'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步：工作流程编排
- en: 'For complex workflows, tools like ZenML and MLFlow let you define modular steps
    and manage dependencies. Here’s how to install ZenML:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂的流程，像ZenML和MLFlow这样的工具可以让您定义模块化步骤并管理依赖项。以下是安装ZenML的方法：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 5: Set Up Monitoring'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五步：设置监控
- en: Once deployed, monitoring is the key to making sure your LLM application is
    performing as expected. Tools like Prometheus and Grafana can track model latency,
    system resource usage, and error rates, or you can use an LLM-specific tool, like
    Log10.io.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦部署，监控是确保您的LLM应用程序按预期运行的关键。像Prometheus和Grafana这样的工具可以跟踪模型延迟、系统资源使用情况和错误率，或者您可以使用像Log10.io这样的特定于LLM的工具。
- en: Now that you know how to deploy an LLM, you might want to provide your model
    to other users without making it open source. The next section looks into APIs
    for LLMs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经知道如何部署LLM，您可能希望将模型提供给其他用户，而不使其开源。下一节将探讨LLM的API。
- en: Developing APIs for LLMs
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为大型语言模型（LLM）开发API
- en: APIs provide users a standardized way for clients to interact with their LLM
    and for developers to access and consume LLM services and models from a variety
    of sources. Following the best practices of LLMOps, as we’ll show you in this
    section, will help you make your APIs secure, reliable, easy to use, and ensure
    that they provide the functionality and performance that LLM-based applications
    need.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: API 为用户提供了一种标准化的方式，让客户端可以与他们的 LLM 交互，并让开发者能够从各种来源访问和消费 LLM 服务和模型。遵循 LLMOps 的最佳实践，正如我们将在本节中向您展示的，将有助于您使您的
    API 安全、可靠、易于使用，并确保它们提供 LLM 基于的应用程序所需的性能和功能。
- en: APIs have been around since the 1960s and 1970s. These early APIs were primarily
    used for system-level programming, allowing different components to communicate
    with each other within a single OS. With the rise of the internet in the 1990s,
    people began using APIs for web-based applications as well.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: API 自 20 世纪 60 年代和 70 年代以来一直存在。这些早期的 API 主要用于系统级编程，允许不同组件在单个操作系统内相互通信。随着 20
    世纪 90 年代互联网的兴起，人们开始将 API 用于基于 Web 的应用程序。
- en: 'Web APIs allow different websites and web applications to communicate and exchange
    data with each other, based on two core rules of software development: high cohesion
    and loose coupling. *High cohesion* means that the components of an API are closely
    related and focused on a single task. This makes the API easier to understand
    and maintain. *Loose coupling* means that the components of an API are independent
    of each other, allowing them to change without affecting other parts. This increases
    flexibility and reduces dependencies.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Web API 允许不同的网站和 Web 应用程序根据软件开发的两项核心规则：高内聚和松耦合进行通信和交换数据。*高内聚* 意味着 API 的组件紧密相关，专注于单一任务。这使得
    API 更易于理解和维护。*松耦合* 意味着 API 的组件相互独立，允许它们在不影响其他部分的情况下进行更改。这增加了灵活性并减少了依赖性。
- en: Today, web APIs are an essential component of modern web-based applications,
    enabling developers to create powerful, integrated systems that can be accessed
    from anywhere at any time. Some common web APIs used by LLM-based applications
    include NLP APIs and LLMs-as-APIs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，Web API 是现代基于 Web 的应用程序的一个基本组成部分，使开发者能够创建强大、集成的系统，可以从任何地方在任何时间访问。一些常见的由 LLM
    基于的应用程序使用的 Web API 包括 NLP API 和 LLMs-as-APIs。
- en: '*NLP APIs* provide access to natural language processing functionalities such
    as tokenization, part-of-speech tagging, and named-entity recognition libraries.
    Tools include Hugging Face and spaCy.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*NLP API* 提供访问自然语言处理功能，如分词、词性标注和命名实体识别库。工具包括 Hugging Face 和 spaCy。'
- en: '*LLMs-as-APIs* provide access to LLMs and make predictions based on user prompts.
    They can be divided into two main categories. *LLM platform APIs* provide access
    to LLM platforms and services that enable developers to build, train, and deploy
    LLM models. Examples include Google Cloud LLM, Amazon SageMaker, and Microsoft
    Azure Machine Learning. *LLM model APIs* provide access to pretrained LLM models
    that can be used to make inferences on text, images, or speech. Model APIs are
    typically used for text generation, classification, and language translation.
    This category includes all the proprietary model APIs: OpenAI, Cohere, Anthropic,
    Ollama, and so on.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLMs-as-APIs* 提供访问 LLMs 的途径，并根据用户提示进行预测。它们可以分为两大类。*LLM 平台 API* 提供访问 LLM 平台和服务，使开发者能够构建、训练和部署
    LLM 模型。例如，包括 Google Cloud LLM、Amazon SageMaker 和 Microsoft Azure Machine Learning。*LLM
    模型 API* 提供访问预训练的 LLM 模型，可用于对文本、图像或语音进行推理。模型 API 通常用于文本生成、分类和语言翻译。这一类别包括所有专有模型
    API：OpenAI、Cohere、Anthropic、Ollama 等等。'
- en: '*Platform APIs* provide a range of services and tools for building, training,
    and deploying LLM models including end-to-end deployment tooling for data preparation,
    model training, model deployment, and model monitoring. LLM platform APIs’ most
    important benefit is that they allow developers to reuse existing LLM models and
    services, reducing the amount of time and effort required to build new applications.
    For example, Google Studio (with the Gemini family of models) is a suite of LLM
    services that enables developers to build, train, and deploy LLM models.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*平台 API* 提供一系列用于构建、训练和部署 LLM 模型的服务和工具，包括数据准备、模型训练、模型部署和模型监控的端到端部署工具。LLM 平台
    API 的最大好处是它们允许开发者重用现有的 LLM 模型和服务，从而减少了构建新应用程序所需的时间和精力。例如，Google Studio（带有 Gemini
    系列模型）是一套 LLM 服务，使开发者能够构建、训练和部署 LLM 模型。'
- en: API-Led Architecture Strategies
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API 领导的架构策略
- en: An *API-led architecture strategy* is a design approach for deploying LLM-based
    applications using APIs to create complex, integrated systems that are scalable,
    flexible, and reusable; that can be accessed from anywhere, at any time; and that
    can handle large volumes of data and traffic. It involves using APIs to expose
    the functionality and data of different systems and services.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*API 领导的架构策略* 是一种设计方法，用于通过使用 API 来部署基于 LLM 的应用程序，创建复杂、集成的系统，这些系统可扩展、灵活且可重用；可以从任何地方、任何时间访问；并且可以处理大量数据和流量。这涉及到使用
    API 来暴露不同系统和服务的功能和数据。'
- en: 'There are two kinds of web APIs: stateful and stateless. A *stateful* API maintains
    and manages the state of a client or user session. The server keeps track of the
    state of the client or user and uses this information to provide personalized
    and context-aware responses based on the state of the client or user. This can
    improve the user experience by providing more relevant and useful information.
    A stateful API can also provide secure access and authentication to protect against
    unauthorized access and use. Examples of stateful APIs are shopping-cart APIs,
    user authentication APIs, content management APIs, and real-time communication
    APIs.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的 Web API：有状态的和无状态的。*有状态的* API 维护和管理客户端或用户会话的状态。服务器跟踪客户端或用户的状态，并使用这些信息根据客户端或用户的状态提供个性化的、上下文感知的响应。这可以通过提供更相关和有用的信息来改善用户体验。有状态的
    API 还可以提供安全的访问和身份验证，以防止未经授权的访问和使用。有状态 API 的例子包括购物车 API、用户身份验证 API、内容管理 API 和实时通信
    API。
- en: '*Stateless* APIs do not store any information about previous requests. Each
    request is independent and contains all the necessary data to be processed. If
    one request fails, it doesn’t affect others because there’s no stored state. This
    means you can use stateless APIs across different environments or platforms without
    worrying about session continuity.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*无状态的* API 不存储任何关于先前请求的信息。每个请求都是独立的，并包含处理所需的所有必要数据。如果一个请求失败，它不会影响其他请求，因为没有存储的状态。这意味着您可以在不同的环境或平台上使用无状态的
    API，而不用担心会话连续性。'
- en: REST APIs
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: REST API
- en: REST APIs are not inherently stateful or stateless, but they can be used to
    create both, depending on the requirements and the techniques you use.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: REST API 本身既不是有状态的也不是无状态的，但根据需求和使用的技巧，它们可以用来创建这两种状态。
- en: Representational State Transfer (REST) is a type of web API that follows the
    RESTful architectural style. REST APIs are stateless, meaning each request contains
    all the information needed to complete the request. However, they can still maintain
    and manage the state of a client or user using techniques such as sessions, cookies,
    or tokens.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表示性状态转移（REST）是一种遵循 RESTful 架构风格的 Web API。REST API 是无状态的，意味着每个请求都包含完成请求所需的所有信息。然而，它们仍然可以使用诸如会话、cookies
    或令牌等技术来维护和管理客户端或用户的状态。
- en: By using REST APIs, you can create scalable, flexible, and reusable systems
    that can handle large volumes of data and traffic. They can also provide the functionality
    and performance that modern web-based applications need.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 REST API，您可以创建可扩展、灵活且可重用的系统，这些系统可以处理大量数据和流量。它们还可以提供现代基于 Web 的应用程序所需的性能和功能。
- en: API Implementation
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: API 实现
- en: Let’s look into how to implement an API.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何实现一个 API。
- en: 'Step 1: Define Your API’s Endpoints'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步：定义您的 API 端点
- en: 'Common endpoints include:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的端点包括：
- en: '`/generate`: For generating text'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/generate`：用于生成文本'
- en: '`/summarize`: For summarization tasks'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/summarize`：用于摘要任务'
- en: '`/embed`: For retrieving embeddings'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/embed`：用于检索嵌入'
- en: 'Step 2: Choose an API Development Framework'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：选择一个API开发框架
- en: 'In this example, we will use FastAPI, a Python framework that simplifies API
    development while supporting asynchronous operations. Let’s implement it:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用 FastAPI，这是一个简化 API 开发同时支持异步操作的 Python 框架。让我们来实现它：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Step 3: Test the API'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步：测试 API
- en: Start the FastAPI server using `python app.py`. Once you have created your API,
    it’s important to manage it effectively to keep it secure, reliable, and performant.
    *API management* is a set of practices and tools for monitoring, maintaining,
    and improving your API. You should consider your API management approach before
    you even start developing your API. Good API management reduces the risk of security
    breaches and provides valuable insights into how your API is being used, and it
    makes the API a valuable asset that delivers value for your organization and users.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`python app.py`启动FastAPI服务器。一旦创建了API，有效地管理它以保持其安全、可靠和高效至关重要。*API管理*是一套用于监控、维护和改进API的实践和工具。在开始开发API之前，你应该考虑你的API管理方法。良好的API管理可以降低安全漏洞的风险，并为API的使用提供有价值的见解，使API成为为你的组织和用户创造价值的宝贵资产。
- en: 'API management activities include monitoring performance, handling errors,
    implementing security measures, and regularly updating and maintaining the API.
    Managing the API for an LLM-based application involves several steps. The following
    list is high level and not all-inclusive:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: API管理活动包括监控性能、处理错误、实施安全措施以及定期更新和维护API。为基于LLM的应用程序管理API涉及几个步骤。以下列表是高级概述，并不全面：
- en: Identify your application’s key functionalities and define the API endpoints
    you’ll use to access them. For example, you might have endpoints for generating
    text, retrieving model information, and/or managing user accounts.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别你应用程序的关键功能，并定义你将用于访问它们的API端点。例如，你可能会有用于生成文本、检索模型信息以及/或管理用户账户的端点。
- en: Decide on the API design, such as whether to use a RESTful or GraphQL API, and
    what data format to use (for example, JSON). Make sure to follow best practices
    for API design, such as using meaningful endpoint names, providing clear and concise
    documentation, and using appropriate HTTP status codes.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定API设计，例如是否使用RESTful或GraphQL API，以及使用哪种数据格式（例如JSON）。确保遵循API设计的最佳实践，例如使用有意义的端点名称，提供清晰简洁的文档，并使用适当的HTTP状态码。
- en: Implement the API using a web framework (such as Flask or Django for Python
    or Express for Node.js). Make sure to handle errors gracefully, validate input
    data, and implement appropriate security measures, such as authentication and
    rate limiting.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网络框架（例如Python的Flask或Django或Node.js的Express）实现API。确保优雅地处理错误，验证输入数据，并实施适当的安全措施，例如身份验证和速率限制。
- en: Integrate the LLM into your API by creating a wrapper around the LLM library
    or API. This wrapper should handle input/output formatting, error handling, and
    any other necessary functionality.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过围绕LLM库或API创建包装器将LLM集成到你的API中。这个包装器应该处理输入/输出格式化、错误处理以及任何其他必要的功能。
- en: Thoroughly test the API using automated testing tools such as PyTest or Jest.
    Make sure to test all endpoints, input validation, error handling, and performance.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动化测试工具（如PyTest或Jest）彻底测试API。确保测试所有端点、输入验证、错误处理和性能。
- en: Deploy the API to a production environment using a cloud provider such as AWS,
    Google Cloud, or Azure. Make sure to use best practices for deployment, such as
    using continuous integration/continuous deployment (CI/CD), monitoring performance,
    and implementing security measures such as firewalls and access controls.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用云服务提供商（如AWS、Google Cloud或Azure）将API部署到生产环境。确保使用最佳部署实践，例如使用持续集成/持续部署（CI/CD）、监控性能以及实施安全措施，如防火墙和访问控制。
- en: Monitor the API for performance issues, errors, and security vulnerabilities.
    Implement logging and alerting mechanisms to notify you of any issues. Regularly
    maintain the API by updating dependencies, fixing bugs, and adding new features
    as needed.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控API的性能问题、错误和安全漏洞。实施日志记录和警报机制以通知任何问题。定期维护API，更新依赖项、修复错误并根据需要添加新功能。
- en: Credential Management
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 凭证管理
- en: One of the most ignored yet most critical components of API management is *credential
    management*. Credentials include any sensitive information, such as API keys,
    authentication tokens, or user passwords, that are used to access your application
    or API. To manage credentials effectively, make sure to store them securely, such
    as by using a secure vault or encryption. Avoid hard-coding credentials into your
    code or configuration files, as this can increase the risk of exposure. Instead,
    use environment variables or secure configuration files that are not committed
    to version control.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: API管理中最被忽视但最关键的组件之一是*凭证管理*。凭证包括任何敏感信息，例如API密钥、认证令牌或用户密码，这些信息用于访问您的应用程序或API。为了有效地管理凭证，请确保安全地存储它们，例如使用安全保险库或加密。避免将凭证硬编码到代码或配置文件中，因为这会增加泄露的风险。相反，使用未提交到版本控制的环境变量或安全配置文件。
- en: You should also implement access controls to limit who can access credentials.
    This can include using role-based access control (RBAC) or attribute-based access
    control (ABAC) to restrict access to sensitive information.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应该实施访问控制以限制谁可以访问凭证。这可以包括使用基于角色的访问控制（RBAC）或基于属性的访问控制（ABAC）来限制对敏感信息的访问。
- en: Finally, regularly rotate credentials to reduce the risk of exposure. This can
    include setting expiration dates for API keys or tokens, or requiring users to
    change their passwords periodically.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，定期轮换凭证以降低泄露风险。这可以包括为API密钥或令牌设置过期日期，或要求用户定期更改密码。
- en: API Gateways
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: API网关
- en: An *API gateway* is a critical component of your LLM-based application. It provides
    a single entry point for all API requests and handles multiple services. It routes
    requests and handles load balancing, authentication, and sometimes caching or
    logging, acting as a middle layer between clients and microservices.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*API网关*是您基于LLM的应用程序的一个关键组件。它为所有API请求提供一个单一的入口点，并处理多个服务。它路由请求并处理负载均衡、身份验证，有时还处理缓存或日志记录，作为客户端和微服务之间的中间层。'
- en: 'To set up an API gateway for your LLM-based application:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要为基于LLM的应用程序设置API网关：
- en: Choose an API gateway provider that meets your needs in terms of features, scalability,
    and cost.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个满足您在功能、可扩展性和成本方面的需求的API网关提供商。
- en: Define your API by specifying the endpoints, methods, and request/response formats.
    Make sure to use meaningful endpoint names and provide clear, concise documentation
    and appropriate HTTP status codes.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过指定端点、方法和请求/响应格式来定义您的API。请确保使用有意义的端点名称，并提供清晰、简洁的文档和适当的HTTP状态代码。
- en: Implement authentication and authorization mechanisms, such as OAuth or JWT,
    to ensure that only authorized users can access your API.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施身份验证和授权机制，如OAuth或JWT，以确保只有授权用户才能访问您的API。
- en: Implement rate limiting to prevent abuse (such as denial-of-service, or DoS,
    attacks) and ensure fair use of your API. This might include setting a maximum
    number of requests per minute or hour or implementing more advanced rate-limiting
    algorithms. Monitor and log API activity to detect and respond to security threats,
    performance issues, or errors. This can include the implementation of logging
    and alerting mechanisms to notify you of any issues.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施速率限制以防止滥用（如拒绝服务攻击或DoS攻击）并确保API的公平使用。这可能包括设置每分钟或每小时的最大请求数量或实施更高级的速率限制算法。监控和记录API活动以检测和应对安全威胁、性能问题或错误。这可能包括实施日志记录和警报机制，以通知您任何问题。
- en: Test your API thoroughly to ensure that it meets your functional and nonfunctional
    requirements.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细测试您的API以确保它满足您的功能和非功能需求。
- en: Deploy it to a production environment using AWS, Google Cloud, or Azure.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS、Google Cloud或Azure将其部署到生产环境。
- en: Setting up an API gateway for an LLM-based application has several advantages.
    It provides a single entry point for all API requests, making it easier to manage
    and monitor API traffic. This helps you identify and respond to security threats,
    performance issues, and errors faster. API gateways can handle authentication
    and authorization tasks, such as verifying API keys or tokens, and enforce access
    controls. They can also log and monitor API activity, providing valuable insights
    into how your LLM-based application is being used. Most importantly, an API gateway
    can implement rate limiting to prevent abuse and ensure fair use of your API.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为基于LLM的应用程序设置API网关有多个优点。它为所有API请求提供了一个单一的入口点，这使得管理监控API流量变得更容易。这有助于您更快地识别和响应安全威胁、性能问题和错误。API网关可以处理身份验证和授权任务，例如验证API密钥或令牌，并执行访问控制。它们还可以记录和监控API活动，提供有关您的LLM基于的应用程序如何被使用的宝贵见解。最重要的是，API网关可以实现速率限制，以防止滥用并确保API的公平使用。
- en: API Versioning and Lifecycle Management
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: API版本化和生命周期管理
- en: '*API versioning* is the process of maintaining multiple versions of an API
    to ensure backward compatibility and minimize the impact of changes on existing
    users.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*API版本化*是维护API多个版本的过程，以确保向后兼容性并最小化对现有用户的影响。'
- en: To version your API, first include the version number in the API endpoint or
    request header. This makes it easy to identify which version of the API is being
    used. Then use semantic versioning to indicate the level of backward compatibility,
    which can help users understand the impact of changes and plan accordingly.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要对API进行版本化，首先在API端点或请求头中包含版本号。这使得识别正在使用哪个API版本变得容易。然后使用语义版本化来指示向后兼容性的级别，这可以帮助用户了解更改的影响并据此进行规划。
- en: Make sure to document all changes between versions, including any breaking changes
    or deprecated features. This can help users understand how to migrate to the new
    version. You can include providing tools or scripts to help users update their
    code or configuration.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 确保记录所有版本之间的更改，包括任何破坏性更改或已弃用功能。这可以帮助用户了解如何迁移到新版本。您还可以包括提供工具或脚本来帮助用户更新他们的代码或配置。
- en: 'But it doesn’t stop at versioning. Your LLMOps strategy also needs to define
    your approach to *API lifecycle management*, from design and development to deployment
    and retirement. The first step is to define the API lifecycle stages, such as
    planning, development, testing, deployment, and retirement. From there, the components
    you’ll need include:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但版本化并不止于此。您的LLMOps策略还需要定义您对*API生命周期管理*的方法，从设计和发展到部署和退役。第一步是定义API生命周期阶段，例如规划、开发、测试、部署和退役。从那里开始，您将需要的组件包括：
- en: Governance model
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 管理模型
- en: A governance model establishes roles and responsibilities, defines processes
    and workflows, and determines which tools and technologies are acceptable.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 管理模型建立角色和责任，定义流程和工作流程，并确定哪些工具和技术是可接受的。
- en: Change management process
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 变更管理流程
- en: Defining a change management process will help ensure that any future changes
    to the API are planned, tested, and communicated to users effectively.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个变更管理流程将有助于确保对API的任何未来变更都得到规划、测试，并且能够有效地通知用户。
- en: Monitoring and alerting
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 监控和警报
- en: You need a monitoring and alerting system to detect and respond to issues or
    errors. This can include setting up alerts for performance issues, security threats,
    or errors. Most API deployment platforms offer this as a service. An example is
    Azure Application Insights, a tool that checks how long each step of your API
    calls is taking and automatically alerts you to performance problems or errors.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要一个监控和警报系统来检测和响应问题或错误。这可以包括设置性能问题、安全威胁或错误的警报。大多数API部署平台都提供这项服务。例如，Azure Application
    Insights是一个工具，它可以检查您的API调用每个步骤所花费的时间，并自动提醒您性能问题或错误。
- en: Retirement process
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 退役流程
- en: Finally, agree on and document a retirement process to decommission the API
    when it is no longer needed. This can include notifying users, providing a migration
    path, and archiving data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，同意并记录一个退役流程，以便在API不再需要时将其停用。这可能包括通知用户、提供迁移路径和存档数据。
- en: LLM Deployment Architectures
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM部署架构
- en: The two most common deployment architectures for software applications and LLM-based
    applications are modular and monolithic*.*
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 软件应用和基于LLM的应用程序最常用的两种部署架构是模块化和单体架构*。
- en: Modular and Monolithic Architectures
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模块化和单体架构
- en: Each architecture has its strengths and use cases, and both require careful
    planning. *Modular architectures* break the system down into its components. Modular
    designs are easier to update and scale, making them ideal for applications requiring
    flexibility. *Monolithic architectures* handle everything within a single framework.
    These models offer simplicity and tightly integrated workflows.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 每种架构都有其优势和用例，并且都需要仔细规划。*模块化架构*将系统分解为其组件。模块化设计更容易更新和扩展，使其适用于需要灵活性的应用程序。*单体架构*在单个框架内处理所有内容。这些模型提供了简单性和紧密集成的流程。
- en: For modular systems, you’ll train components like retrievers, re-rankers, and
    generators independently. This approach allows you to focus on optimizing each
    module. It requires defining the communication between modules extremely well;
    most issues in modular systems happen when modules communicate incorrectly with
    one another. In contrast, monolithic architectures often involve end-to-end training,
    which simplifies dependencies but demands significant computational resources.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模块化系统，你将独立训练检索器、重新排序器和生成器等组件。这种方法允许你专注于优化每个模块。它需要非常精确地定义模块之间的通信；模块化系统中的大多数问题都发生在模块之间通信错误时。相比之下，单体架构通常涉及端到端训练，这简化了依赖关系，但需要大量的计算资源。
- en: After training, save your model in a format that supports the architecture;
    for example, use open formats like ONNX for interoperability or native formats
    like PyTorch or TensorFlow for custom pipelines. Validation is crucial for both
    approaches. In terms of testing, modular systems require component-specific tests
    to ensure compatibility and performance, while monolithic architectures need comprehensive
    end-to-end evaluation to confirm their robustness.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，以支持架构的格式保存你的模型；例如，使用如ONNX这样的开放格式以实现互操作性，或使用如PyTorch或TensorFlow这样的本地格式以实现自定义管道。验证对于两种方法都至关重要。在测试方面，模块化系统需要针对组件的特定测试以确保兼容性和性能，而单体架构需要全面的端到端评估以确认其稳健性。
- en: Implementing a Microservices-Based Architecture
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施基于微服务的架构
- en: Let’s say you’ve decided to adopt a *microservices-based architecture* for your
    LLM application. This is a modular architectural style that breaks a large application
    down into smaller, independent services that communicate with each other through
    APIs. Its benefits include improved scalability, flexibility, and maintainability.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经决定为你的LLM应用采用*基于微服务的架构*。这是一种模块化架构风格，将大型应用程序分解为更小、独立的、通过API相互通信的服务。它的好处包括提高可扩展性、灵活性和可维护性。
- en: In a microservice-based architecture, APIs serve as connectors between the different
    services. Each service exposes an API that allows other services to interact with
    it. APIs *decouple* the different services, allowing them to evolve independently.
    This means that changes to one service do not impact other services, reducing
    the risk of breaking changes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于微服务的架构中，API作为不同服务之间的连接器。每个服务都暴露一个API，允许其他服务与之交互。API *解耦*了不同的服务，使它们能够独立演进。这意味着对某个服务的更改不会影响其他服务，从而降低了破坏性更改的风险。
- en: APIs also enable services to scale independently, allowing you to allocate resources
    more efficiently. For example, you could scale your language translation service
    independently of your text-to-speech service. With APIs, you can build different
    services using different technologies and programming languages. This means that
    you can choose the best technology for each service, improving development speed
    and reducing technical debt.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: API还使服务能够独立扩展，允许你更有效地分配资源。例如，你可以独立于你的语音合成服务扩展你的语言翻译服务。使用API，你可以使用不同的技术和编程语言构建不同的服务。这意味着你可以为每个服务选择最佳技术，提高开发速度并减少技术债务。
- en: 'To use different APIs as connectors for a microservice-based architecture for
    LLM applications:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要将不同的API作为LLM应用的基于微服务的架构的连接器：
- en: Define clear and consistent APIs for each service, defining the input and output
    formats, authentication and authorization mechanisms, and error handling.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个服务定义清晰和一致的API，包括输入和输出格式、认证和授权机制以及错误处理。
- en: Implement standard API communication protocols, such as HTTP or gRPC, for compatibility
    and interoperability between services.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施标准API通信协议，如HTTP或gRPC，以实现服务之间的兼容性和互操作性。
- en: Implement security mechanisms, such as OAuth or JWT, to authenticate and authorize
    API requests.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施安全机制，如OAuth或JWT，以认证和授权API请求。
- en: Implement monitoring and logging mechanisms to track API usage and detect issues.
    This can help you identify and resolve issues quickly and improve the user experience.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施监控和日志记录机制以跟踪API使用情况并检测问题。这可以帮助您快速识别和解决问题，并改善用户体验。
- en: Implement versioning mechanisms to manage changes to the APIs and minimize their
    impact on existing applications and users.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施版本控制机制以管理API的变化并最小化其对现有应用程序和用户的影响。
- en: This approach can help you build a scalable, flexible, and maintainable LLM
    application with multiple APIs that meets the needs of your users and enables
    distributed functionality for large-scale applications. Let’s look at how to implement
    your microservices architecture in more detail.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以帮助您构建一个可扩展、灵活且易于维护的具有多个API的LLM应用程序，满足用户需求并使大型应用程序能够实现分布式功能。让我们更详细地看看如何实现您的微服务架构。
- en: 'Step 1: Decompose the application into its components'
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第1步：将应用程序分解为其组件
- en: Preprocessing service to tokenize and clean input
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理服务用于标记化和清理输入
- en: Inference service to perform LLM inference
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理服务执行LLM推理
- en: Postprocessing service to format or enrich model outputs
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后处理服务用于格式化或丰富模型输出
- en: 'Let’s look at sample code for a preprocessing service:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看预处理服务的示例代码：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Step 2: Establish communication between services'
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第2步：建立服务之间的通信
- en: You can use HTTP for simplicity or gRPC (Google’s Remote Procedure Call) for
    high performance. Add a message broker like RabbitMQ or Kafka for asynchronous
    communication.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用HTTP进行简单操作，或使用gRPC（谷歌的远程过程调用）进行高性能操作。添加一个消息代理，如RabbitMQ或Kafka，以实现异步通信。
- en: 'Step 3: Coordinate the microservices to keep the workflows seamless'
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第3步：协调微服务以保持工作流程的流畅
- en: 'You can use tools like Consul or Eureka to register and discover services dynamically,
    or you might implement an API Gateway (such as Kong or NGINX) to route client
    requests to the appropriate microservice. Here’s an NGINX example:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Consul或Eureka等工具动态注册和发现服务，或者您可能实现一个API网关（如Kong或NGINX），将客户端请求路由到适当的微服务。以下是一个NGINX示例：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you plan to use a tool like MLFlow or BentoML to manage service dependencies
    and task execution, you can also implement it at this step.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划使用MLFlow或BentoML等工具来管理服务依赖和任务执行，您也可以在这一步实现它。
- en: 'Step 4: Create a Dockerfile for each microservice'
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第4步：为每个微服务创建Dockerfile
- en: 'Here’s​ an example using Python:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用Python的示例：
- en: '[PRE8]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here’s another example for deploying to Kubernetes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个部署到Kubernetes的示例：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, to test your Kubernetes deployment:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了测试您的Kubernetes部署：
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Automating RAG with Retriever Re-ranker Pipelines
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用检索器重新排序器管道自动化RAG
- en: Building efficient retriever re-ranker pipelines is a key step in implementing
    RAG pipeline workflows. *Retriever re-ranker* pipelines retrieve relevant context
    and rank it for input to the LLM. As you’ve seen throughout this book, automation
    is critical to ensuring a system’s scalability and reliability. As we move through
    this section, you’ll get pointers about how using frameworks like LangChain and
    LlamaIndex can simplify this process.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 构建高效的检索器重新排序器管道是实现RAG管道工作流程的关键步骤。*检索器重新排序器*管道检索相关上下文并将其排序以供LLM输入。正如您在本章中看到的，自动化对于确保系统的可扩展性和可靠性至关重要。随着我们进入本节，您将获得有关如何使用LangChain和LlamaIndex等框架简化此过程的提示。
- en: Start with the *retriever*, which fetches relevant data based on a query. You
    can use dense vector embeddings and store them in a vector database, like Pinecone
    or Milvus. Once it retrieves results, the *re-ranker* reorders those results by
    relevance. LangChain provides modular components to integrate these steps seamlessly,
    allowing you to create pipelines to automate data retrieval and ranking with minimal
    intervention. LlamaIndex adds features for integrating retrieval systems with
    structured data sources, offering flexibility in managing knowledge sources.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从*检索器*开始，它根据查询检索相关数据。您可以使用密集向量嵌入并将它们存储在向量数据库中，如Pinecone或Milvus。一旦检索到结果，*重新排序器*就会根据相关性重新排序这些结果。LangChain提供了模块化组件，以无缝集成这些步骤，让您能够创建管道来自动化数据检索和排序，最小化干预。LlamaIndex增加了将检索系统与结构化数据源集成的功能，提供了管理知识源时的灵活性。
- en: Automation ensures that your retriever re-ranker pipeline is always up-to-date.
    This is especially useful when dealing with dynamic data, like user-generated
    content or frequently updated knowledge bases. Regular validation and retraining
    improve the accuracy of these pipelines over time.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化确保你的检索重排管道始终是最新的。这对于处理动态数据，如用户生成内容或频繁更新的知识库特别有用。定期的验证和再训练可以随着时间的推移提高这些管道的准确性。
- en: Let’s look at an implementation that retrieves documents, re-ranks them, and
    feeds the most relevant context to an LLM ([Example 6-1](#ch06_example_1_1748919660048653)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个实现，它检索文档，重新排名它们，并将最相关的上下文提供给LLM（[示例 6-1](#ch06_example_1_1748919660048653)）。
- en: Example 6-1\. Building a retriever re-ranker pipeline
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. 构建检索重排管道
- en: '[PRE11]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: At step 2, you use Pinecone to fetch the top-*k* relevant documents based on
    the query embeddings.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步，你使用Pinecone根据查询嵌入检索最相关的*前k*个文档。
- en: In step 4, a simple function ranks retrieved documents by semantic similarity
    using the embedding model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，一个简单的函数使用嵌入模型根据语义相似度对检索到的文档进行排名。
- en: For better results, you can replace the simple scoring with neural re-rankers
    like T5 or BERT, add memory to the pipeline to handle multi-turn queries, or automate
    database updates using scheduled tasks for dynamic content.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好的结果，你可以用T5或BERT等神经重排器替换简单的评分，向管道添加内存以处理多轮查询，或使用计划任务自动化数据库更新以处理动态内容。
- en: Automating Knowledge Graph Updates
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化知识图谱更新
- en: Keeping your knowledge graphs (KGs) up-to-date is essential for maintaining
    accurate insights. Automation simplifies this process, especially for tasks like
    entity linking and generating graph embeddings. It reduces manual effort, increases
    accuracy, and ensures that your knowledge graphs remain a reliable source of information.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 保持你的知识图谱（KG）更新是维持准确洞察力的关键。自动化简化了这一过程，特别是对于实体链接和生成图嵌入等任务。它减少了人工工作量，提高了准确性，并确保你的知识图谱始终是可靠的信息来源。
- en: '*Entity linking* ensures that new information connects to the correct nodes
    in the KG. For example, if a document references “Paris,” entity linking determines
    whether that refers to the city or a person’s name. Automated pipelines handle
    this by combining neural natural language processing (NNLP) models with preexisting
    graph structures and by using embeddings to understand relationships and context.
    Tools like spaCy and specialized libraries for entity resolution can help you
    build robust linking systems.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*实体链接* 确保新信息与KG中的正确节点连接。例如，如果文档引用“巴黎”，实体链接将确定这指的是城市还是人名。自动管道通过结合神经自然语言处理（NNLP）模型与现有的图结构，并使用嵌入来理解关系和上下文来处理这个问题。spaCy和用于实体解析的专用库等工具可以帮助你构建健壮的链接系统。'
- en: '*Graph embeddings* are numerical representations of nodes, edges, and their
    relationships. They enable tasks like graph search, recommendation, and reasoning.
    To make sure your KG reflects the latest data, it’s wise to automate embedding
    creation and updates. That way, the pipelines can schedule updates whenever new
    data arrives, ensuring the KG stays accurate and ready for downstream applications.
    Libraries like PyTorch Geometric and DGL (Deep Graph Library) provide embedding
    generation tools. Regularly validate your pipelines to prevent errors from propagating
    through the graph.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*图嵌入* 是节点、边及其关系的数值表示。它们使图搜索、推荐和推理等任务成为可能。为了确保你的KG反映最新数据，自动嵌入创建和更新是明智的。这样，管道可以在新数据到达时安排更新，确保KG保持准确并准备好下游应用。PyTorch
    Geometric 和 DGL（深度图库）等库提供了嵌入生成工具。定期验证你的管道以防止错误在图中传播。'
- en: The next example walks you through how to automate KG updates by building a
    pipeline using Python. The libraries used here are spaCy for entity linking and
    PyTorch Geometric and DGL for graph embeddings. For the KG itself, a Neo4j graph
    database is used.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例将指导你如何通过构建使用Python的管道来自动化KG更新。这里使用的库是spaCy用于实体链接和PyTorch Geometric以及DGL用于图嵌入。对于KG本身，使用Neo4j图数据库。
- en: 'First, install the libraries:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，安装库：
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now you can implement:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以实现：
- en: '[PRE13]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In step 3, the `link_entities_and_update_kg()` function uses spaCy to extract
    named entities from the input text. It then updates the Neo4j knowledge graph
    by linking each entity (e.g., “John von Neumann,” “computer science”) as a node.
    The `MERGE` clause ensures that entities are created only if they don’t already
    exist in the graph.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 步中，`link_entities_and_update_kg()` 函数使用 spaCy 从输入文本中提取命名实体。然后，它通过将每个实体（例如，“约翰·冯·诺伊曼”，“计算机科学”）作为节点链接到
    Neo4j 知识图谱来更新图谱。`MERGE` 子句确保只有在实体在图中不存在时才创建实体。
- en: In step 4, we use PyTorch Geometric to compute graph embeddings using graph
    convolutional networks (GCNs). Nodes and edges are defined manually, and the GCNConv
    layer is applied to compute new embeddings.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 步中，我们使用 PyTorch Geometric 通过图卷积网络 (GCNs) 计算图嵌入。节点和边是手动定义的，并将 GCNConv 层应用于计算新的嵌入。
- en: 'At step 5, the `automate_kg_update()` function combines the two steps: it first
    links entities and updates the KG, and then it computes the graph embeddings to
    keep the knowledge graph up-to-date with the latest entity information and structure.
    To automate the process, schedule the `automate_kg_update()` function to run periodically
    via cron jobs or a task scheduler like Celery.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5 步中，`automate_kg_update()` 函数结合了两个步骤：它首先链接实体并更新 KG，然后计算图嵌入以保持知识图谱与最新的实体信息和结构同步。为了自动化此过程，通过
    cron 作业或 Celery 等任务调度器定期安排 `automate_kg_update()` 函数的运行。
- en: Deployment Latency Optimization
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署延迟优化
- en: Reducing latency is one of the most important considerations when deploying
    LLMs. Latency directly impacts performance and responsiveness. Some applications,
    such as chatbots, search engines, and real-time decision-making systems, require
    especially low latency, making it essential to find ways to minimize the time
    it takes for the system to return results.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 降低延迟是部署 LLM 时最重要的考虑因素之一。延迟直接影响性能和响应速度。一些应用程序，如聊天机器人、搜索引擎和实时决策系统，需要特别低的延迟，因此找到减少系统返回结果所需时间的方法至关重要。
- en: One effective approach is to use Triton Inference Server, an open source platform
    designed specifically for high-performance model inference. It supports a wide
    variety of model types, including TensorFlow, PyTorch, ONNX, and others. Triton
    significantly optimizes LLM execution, making it possible to handle multiple concurrent
    inference requests with minimal delay.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有效的方法是使用 Triton Inference Server，这是一个专门为高性能模型推理设计的开源平台。它支持多种模型类型，包括 TensorFlow、PyTorch、ONNX
    等。Triton 显著优化了 LLM 的执行，使其能够以最小的延迟处理多个并发推理请求。
- en: There are a few reasons for this. First, it supports model concurrency and can
    run models on GPUs. It can also dynamically load and unload models based on demand,
    which is useful for applications requiring low latency, such as chatbots, search
    engines, or real-time decision-making systems. Triton also supports *batching*,
    which allows it to combine multiple inference requests into a single operation,
    further improving throughput and reducing the overall response time.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这有几个原因。首先，它支持模型并发，可以在 GPU 上运行模型。它还可以根据需求动态加载和卸载模型，这对于需要低延迟的应用程序非常有用，例如聊天机器人、搜索引擎或实时决策系统。Triton
    还支持 *批处理*，这允许它将多个推理请求组合成一个操作，从而进一步提高吞吐量并减少整体响应时间。
- en: 'To deploy an LLM using Triton Inference Server for optimized execution, first
    install Triton:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Triton Inference Server 部署 LLM 以实现优化执行，首先安装 Triton：
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, prepare the model directory. Make sure to save your model in a directory
    that Triton can access and in a format like TensorFlow SavedModel or PyTorch TorchScript:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，准备模型目录。确保将你的模型保存在 Triton 可以访问的目录中，并且使用 TensorFlow SavedModel 或 PyTorch TorchScript
    等格式：
- en: '[PRE15]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now run Triton from the terminal:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从终端运行 Triton：
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, query the server for inference. You can use a client library like
    `tritonclient` to send requests to the Triton server:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，查询服务器进行推理。你可以使用 `tritonclient` 等客户端库向 Triton 服务器发送请求：
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Orchestrating Multiple Models
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模型编排
- en: To achieve efficiency and good response times in systems that require multiple
    models to work together, you’ll need to use *multi-model orchestration,* which
    involves breaking models down into microservices. You’ll then deploy each model
    as an independent service, and they can interact through APIs or message queues.
    There are multiple ready-to-use orchestrators out there including Multi-Agent
    Orchestrator by AWS, and proxy tools like LiteLLM that allow you to switch between
    multiple models and APIs. But as with everything else in software, the higher
    the dependencies, the higher the debugging complexity when inferencing fails in
    mission-critical tasks.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在需要多个模型协同工作的系统中实现效率和良好的响应时间，你需要使用 *多模型编排*，这涉及到将模型分解成微服务。然后，将每个模型作为独立的服务部署，它们可以通过
    API 或消息队列进行交互。市面上有多个现成的编排器，包括 AWS 的 Multi-Agent Orchestrator，以及像 LiteLLM 这样的代理工具，允许你在多个模型和
    API 之间切换。但就像软件中的其他一切一样，依赖性越高，当推理在关键任务中失败时，调试的复杂性就越高。
- en: 'For example, you might have separate models for different stages of processing:
    one for text preprocessing, another for text-to-speech, and another for generating
    responses. Orchestration can ensure that each part of the process happens concurrently
    and efficiently, reducing bottlenecks and speeding up the overall system.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可能为处理的不同阶段有不同的模型：一个用于文本预处理，另一个用于文本转语音，还有一个用于生成响应。编排可以确保过程的每个部分都并发且高效地发生，减少瓶颈并加快整体系统的速度。
- en: 'You can use container orchestration tools like Kubernetes or Docker Compose
    to manage multiple models running as microservices. Here’s how to create a `docker-compose.yml`
    file:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 Kubernetes 或 Docker Compose 等容器编排工具来管理作为微服务运行的多个模型。以下是如何创建 `docker-compose.yml`
    文件的方法：
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Orchestrate the communication between the models using a message queue like
    RabbitMQ or through direct API calls. Each service listens for input and processes
    it sequentially or concurrently as needed.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像 RabbitMQ 这样的消息队列或直接通过 API 调用来编排模型之间的通信。每个服务都监听输入并按需顺序或并发处理它。
- en: You’ll also need to set up *load balancing* to manage traffic between models
    and distribute requests efficiently. You’ll need to configure Kubernetes or Docker
    Swarm to run multiple instances of your models and balance the incoming traffic.
    Kubernetes uses a service to route requests to the appropriate pod, while Docker
    Swarm uses Docker’s built-in load balancer to automatically distribute traffic
    between containers. Let’s assume you have a Docker container running a model;
    for instance, a `model_image` Docker image. You want to deploy multiple instances
    of this model and use Kubernetes to load-balance the incoming requests.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要设置 *负载均衡* 来管理模型之间的流量并高效地分配请求。你需要配置 Kubernetes 或 Docker Swarm 来运行你模型的多个实例并平衡传入的流量。Kubernetes
    使用服务将请求路由到适当的 pod，而 Docker Swarm 则使用 Docker 内置的负载均衡器来自动在容器之间分配流量。假设你有一个运行模型的 Docker
    容器；例如，一个 `model_image` Docker 镜像。你希望部署此模型的多个实例并使用 Kubernetes 来负载均衡传入的请求。
- en: 'First, create a Kubernetes deployment configuration file, which will define
    the model container, and specify how many replicas of it you want:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个 Kubernetes 部署配置文件，它将定义模型容器，并指定你想要多少个副本：
- en: '[PRE19]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This configuration will deploy three replicas. The Kubernetes Deployment will
    manage the *pods* running these models (the smallest deployable units in Kubernetes)
    and will automatically balance traffic. To distribute traffic among them, you
    need to expose them using a Kubernetes Service:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置将部署三个副本。Kubernetes 部署将管理运行这些模型的 *pods*（Kubernetes 中最小的可部署单元）并自动平衡流量。为了在他们之间分配流量，你需要使用
    Kubernetes 服务来公开它们：
- en: '[PRE20]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This service will expose the three model replicas on port 80 and balance the
    traffic among them.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此服务将在端口 80 上公开三个模型副本并平衡它们之间的流量。
- en: 'Now you can deploy the model and the service to your Kubernetes cluster:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将模型和服务部署到你的 Kubernetes 集群：
- en: '[PRE21]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There are a few advantages to this level of modularity. For one thing, it allows
    you to scale each model independently, based on the demand for its specific task.
    For instance, if you have more requests for text generation than for entity recognition,
    you can scale up the text generation model without affecting the other models.
    Additionally, if one model fails, the other models can continue to operate, keeping
    the system available. That means you can swap out one model for a newer version
    or replace it with a different model to improve performance.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模块化程度有几个优点。首先，它允许你根据特定任务的需求独立扩展每个模型。例如，如果你对文本生成的请求多于实体识别，你可以扩展文本生成模型而不影响其他模型。此外，如果一个模型失败，其他模型可以继续运行，保持系统可用。这意味着你可以用新版本替换一个模型，或者用不同的模型替换它以提高性能。
- en: Optimizing RAG Pipelines
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化RAG管道
- en: Optimizing RAG pipelines is crucial for achieving efficiency and low latency
    in information retrieval and text generation tasks. Their performance depends
    heavily on how well you optimized the retrieval pipeline. This section will show
    you several techniques to significantly improve RAG performance.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 优化RAG管道对于在信息检索和文本生成任务中实现效率和低延迟至关重要。它们的性能很大程度上取决于你如何优化检索管道。本节将向你展示几种显著提高RAG性能的技术。
- en: Asynchronous Querying
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步查询
- en: '*Asynchronous querying* is a powerful optimization technique that allows multiple
    queries to be processed concurrently, reducing the waiting time for each query.
    In traditional synchronous retrieval systems, each query is processed sequentially,
    but this causes delays when multiple requests are made at once. Asynchronous querying
    addresses this bottleneck by allowing the system to send queries to the vector
    store simultaneously and then wait for the responses in parallel.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*异步查询*是一种强大的优化技术，它允许同时处理多个查询，从而减少了每个查询的等待时间。在传统的同步检索系统中，每个查询都是顺序处理的，但一次有多个请求时会导致延迟。异步查询通过允许系统同时向向量存储发送查询并并行等待响应来解决这个瓶颈。'
- en: 'Here’s an example of how you might implement asynchronous querying using Python:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个使用Python实现异步查询的例子：
- en: '[PRE22]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In this example, `asyncio.gather()` sends all queries to Facebook AI Similarity
    Search (FAISS) at once and waits for the responses asynchronously. This allows
    the system to process multiple queries in parallel, reducing the overall latency.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`asyncio.gather()`一次将所有查询发送到Facebook AI Similarity Search (FAISS)并异步等待响应。这允许系统并行处理多个查询，从而减少整体延迟。
- en: Combining Dense and Sparse Retrieval Methods
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合密集和稀疏检索方法
- en: '*Dense retrieval* leverages embeddings to represent both queries and documents
    in a vector space, allowing for similarity searches based on vector distances.
    *Sparse retrieval* methods, such as TF-IDF, rely on term-based matching, which
    can capture more nuanced keyword-based relevance. Dense retrieval is particularly
    useful for capturing semantic relevance, while sparse retrieval excels at exact
    keyword matching. Combining both methods allows you to leverage the strengths
    of each for more accurate and comprehensive results. To do so, try the following
    code:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*密集检索*利用嵌入在向量空间中表示查询和文档，允许基于向量距离进行相似度搜索。*稀疏检索*方法，如TF-IDF，依赖于基于术语的匹配，可以捕捉到更细微的关键词相关性。密集检索特别适用于捕捉语义相关性，而稀疏检索在精确关键词匹配方面表现出色。结合这两种方法可以使你利用各自的优势，以获得更准确和全面的结果。为此，请尝试以下代码：'
- en: '[PRE23]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this example, FAISS handles dense vector-based retrieval, while Whoosh handles
    the sparse, keyword-based search. The results are then combined, offering both
    semantic and exact-match retrieval, which can improve the overall accuracy and
    completeness of the system’s responses.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，FAISS处理基于密集向量的检索，而Whoosh处理基于关键词的稀疏搜索。然后将结果合并，提供语义和精确匹配检索，这可以提高系统响应的整体准确性和完整性。
- en: Cache Embeddings
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存嵌入
- en: Instead of recomputing embeddings for frequently queried data, use *embedding
    caching* to let the system store embeddings and reuse them for subsequent queries.
    If the embeddings for a query are already stored in the cache, the system retrieves
    them; otherwise, it computes the embeddings and stores them for future use. This
    reduces the need to reprocess the same data, significantly decreasing response
    times and improving efficiency.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于频繁查询的数据，而不是重新计算嵌入，可以使用*嵌入缓存*让系统存储嵌入，并在后续查询中重用它们。如果查询的嵌入已经存储在缓存中，系统将检索它们；否则，它将计算嵌入并将它们存储以供将来使用。这减少了重新处理相同数据的需求，显著降低了响应时间并提高了效率。
- en: 'Here’s an example of how to implement embedding caching:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个如何实现嵌入缓存（embedding caching）的例子：
- en: '[PRE24]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Key–Value Caching
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 键值（Key-Value）缓存
- en: '*Key–value (KV) caching* works similarly to embedding caching. It stores the
    results of key-value pairs, where the key is a query or an intermediate result
    and the value is the corresponding response or computed result. This allows the
    system to retrieve precomputed results instead of recalculating them each time
    it processes a repeated query. KV caching speeds up both the retrieval and generation,
    especially in large-scale, high-traffic systems.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*键值（KV）缓存*与嵌入缓存的工作方式类似。它存储键值对的结果，其中键是一个查询或中间结果，值是对应的响应或计算结果。这允许系统检索预先计算的结果，而不是每次处理重复查询时都重新计算。键值缓存加快了检索和生成的速度，尤其是在大规模、高流量系统中。'
- en: In RAG systems, KV caching is typically applied during the retrieval phase to
    speed up the query–response cycle. In the generation phase, the model may use
    versions or parts of cached documents and responses to build its final output.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG系统中，键值（KV）缓存通常在检索阶段应用，以加快查询-响应周期。在生成阶段，模型可能使用缓存的文档和响应的版本或部分来构建其最终输出。
- en: 'Let’s look at how to implement it in Python:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Python中实现它：
- en: '[PRE25]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In step 1, you connect to a Redis instance running locally to store the key-value
    pairs for quick lookup.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，你连接到本地运行的Redis实例，以存储用于快速查找的关键值对。
- en: Then step 3 stipulates that when a query is received, the code checks if the
    embeddings for that query are already cached in Redis by checking the key (`embedding:<query>`).
    If the cache contains the embeddings (called a *cache hit*), it directly retrieves
    and returns them. If not (a *cache miss*), the embeddings are computed using `SentenceTransformer`
    and then stored. The embeddings are stored in Redis as bytes using `tobytes()`
    to ensure they can be retrieved in the same format.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然后第3步规定，当接收到查询时，代码会检查该查询的嵌入是否已经缓存到Redis中，通过检查键（`embedding:<query>`）。如果缓存包含嵌入（称为*缓存命中*），则直接检索并返回它们。如果没有（称为*缓存未命中*），则使用`SentenceTransformer`计算嵌入，然后存储。嵌入使用`tobytes()`存储在Redis中，以确保它们可以以相同的格式检索。
- en: By reducing the need to recompute embeddings or model responses, KV caching
    can help lower compute costs and reduce the strain on both the retrieval and generation
    components, ensuring that the system remains responsive even under heavy load.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少重新计算嵌入或模型响应的需求，键值（KV）缓存可以帮助降低计算成本，并减少检索和生成组件的负担，确保系统即使在重负载下也能保持响应。
- en: Scalability and Reusability
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展性和可重用性
- en: Scalability and reusability are essential for handling high-traffic systems.
    In large-scale environments, the ability to scale your infrastructure efficiently
    is critical. *Distributed inference orchestration* allows the system to distribute
    the load across multiple nodes as traffic increases, with each handling a portion
    of the overall request. This reduces the chances of any single machine becoming
    overwhelmed.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性和可重用性对于处理高流量系统至关重要。在大规模环境中，有效地扩展你的基础设施的能力是至关重要的。*分布式推理编排*允许系统在流量增加时将负载分配到多个节点，每个节点处理整体请求的一部分。这减少了任何单台机器过载的可能性。
- en: Kubernetes is usually used to manage the scaling process by automating task
    distribution and adjusting resources as needed.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes通常用于通过自动化任务分配和根据需要调整资源来管理扩展过程。
- en: Reusable components make it easier to scale and manage your pipeline. They can
    be replicated across different services or projects quickly because they don’t
    require significant modifications. This is especially important in environments
    with constant updates and iterations. ZenML and similar orchestration tools allow
    you to create reusable pipelines, which you can modify or extend without disrupting
    the entire system. As you build new models or add new tasks, you can reuse existing
    components to maintain consistency and reduce development time.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 可重用组件使您更容易扩展和管理您的管道。由于它们不需要进行重大修改，因此可以快速在不同服务或项目中复制。这在需要不断更新和迭代的环境中尤为重要。ZenML和类似的编排工具允许您创建可重用管道，您可以在不破坏整个系统的情况下对其进行修改或扩展。随着您构建新的模型或添加新的任务，您可以重用现有组件以保持一致性并减少开发时间。
- en: Distributed inference orchestration and reusable components work hand in hand
    to ensure that your system is both scalable and maintainable. When traffic spikes
    or new use cases arise, it’s important to know that you can rely on your existing
    infrastructure to handle the demands. This makes the entire system more resilient
    and agile in adapting to new challenges.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式推理编排和可重用组件协同工作，以确保您的系统既可扩展又易于维护。当流量激增或出现新的用例时，了解您可以依赖现有基础设施来处理需求是很重要的。这使得整个系统在面对新挑战时更具弹性和敏捷性。
- en: Scalability and reusability are not just nice-to-haves but necessary features
    for high-traffic LLM systems. Distributed inference orchestration ensures that
    your system can scale to meet demand, while reusable components make it easier
    to maintain and expand the system over time. Together, they allow for efficient
    and effective handling of large-scale LLM deployments.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性和可重用性不仅仅是锦上添花的功能，对于高流量LLM系统来说是必需的功能。分布式推理编排确保您的系统可以扩展以满足需求，而可重用组件使您更容易随着时间的推移维护和扩展系统。共同作用，它们允许高效有效地处理大规模LLM部署。
- en: Conclusion
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The right stack will depend on your project goals. Open source tools are great
    if you need flexibility and have technical resources to manage the setup. Managed
    services are perfect for teams that prioritize speed and simplicity. Carefully
    assess your needs before committing to a stack, as the right choice will save
    time, improve performance, and help you deploy more effectively.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的堆栈将取决于您的项目目标。如果您需要灵活性和有技术资源来管理设置，开源工具非常出色。对于优先考虑速度和简单性的团队，托管服务是完美的选择。在承诺使用某个堆栈之前，仔细评估您的需求，因为正确的选择将节省时间、提高性能，并帮助您更有效地部署。
