<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">appendix C</span></span> <span class="chapter-title-text">Exercise solutions</span></h1> 
  </div> 
  <div class="readable-text" id="p2"> 
   <p>The complete code examples for the exercises’ answers can be found in the supplementary GitHub repository at <a href="https://github.com/rasbt/LLMs-from-scratch">https://github.com/rasbt/LLMs-from-scratch</a>.</p> 
  </div> 
  <div class="readable-text" id="p3"> 
   <h2 class=" readable-text-h2">Chapter 2</h2> 
  </div> 
  <div class="readable-text" id="p4"> 
   <h3 class=" readable-text-h3">Exercise 2.1</h3> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>You can obtain the individual token IDs by prompting the encoder with one string at a time:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p6"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(tokenizer.encode("Ak"))
print(tokenizer.encode("w"))
# ...</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p8"> 
   <div class="code-area-container"> 
    <pre class="code-area">[33901]
[86]
# ...</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>You can then use the following code to assemble the original string:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p10"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(tokenizer.decode([33901, 86, 343, 86, 220, 959]))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>This returns</p> 
  </div> 
  <div class="browsable-container listing-container" id="p12"> 
   <div class="code-area-container"> 
    <pre class="code-area">'Akwirw ier'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p13"> 
   <h3 class=" readable-text-h3">Exercise 2.2</h3> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>The code for the data loader with <code>max_length=2</code> <code>and</code> <code>stride=2</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p15"> 
   <div class="code-area-container"> 
    <pre class="code-area">dataloader = create_dataloader(
    raw_text, batch_size=4, max_length=2, stride=2
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>It produces batches of the following format:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p17"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[  40,  367],
        [2885, 1464],
        [1807, 3619],
        [ 402,  271]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>The code of the second data loader with <code>max_length=8</code> <code>and</code> <code>stride=2</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p19"> 
   <div class="code-area-container"> 
    <pre class="code-area">dataloader = create_dataloader(
    raw_text, batch_size=4, max_length=8, stride=2
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>An example batch looks like</p> 
  </div> 
  <div class="browsable-container listing-container" id="p21"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],
        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138],
        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],
        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p22"> 
   <h2 class=" readable-text-h2">Chapter 3</h2> 
  </div> 
  <div class="readable-text" id="p23"> 
   <h3 class=" readable-text-h3">Exercise 3.1</h3> 
  </div> 
  <div class="readable-text" id="p24"> 
   <p>The correct weight assignment is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p25"> 
   <div class="code-area-container"> 
    <pre class="code-area">sa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)
sa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)
sa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p26"> 
   <h3 class=" readable-text-h3">Exercise 3.2</h3> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>To achieve an output dimension of 2, similar to what we had in single-head attention, we need to change the projection dimension <code>d_out</code> to 1.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p28"> 
   <div class="code-area-container"> 
    <pre class="code-area">d_out = 1
mha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p29"> 
   <h3 class=" readable-text-h3">Exercise 3.3</h3> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>The initialization for the smallest GPT-2 model is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p31"> 
   <div class="code-area-container"> 
    <pre class="code-area">block_size = 1024
d_in, d_out = 768, 768
num_heads = 12
mha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p32"> 
   <h2 class=" readable-text-h2">Chapter 4</h2> 
  </div> 
  <div class="readable-text" id="p33"> 
   <h3 class=" readable-text-h3">Exercise 4.1</h3> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>We can calculate the number of parameters in the feed forward and attention modules as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p35"> 
   <div class="code-area-container"> 
    <pre class="code-area">block = TransformerBlock(GPT_CONFIG_124M)

total_params = sum(p.numel() for p in block.ff.parameters())
print(f"Total number of parameters in feed forward module: {total_params:,}")

total_params = sum(p.numel() for p in block.att.parameters())
print(f"Total number of parameters in attention module: {total_params:,}")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>As we can see, the feed forward module contains approximately twice as many parameters as the attention module:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p37"> 
   <div class="code-area-container"> 
    <pre class="code-area">Total number of parameters in feed forward module: 4,722,432
Total number of parameters in attention module: 2,360,064</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p38"> 
   <h3 class=" readable-text-h3">Exercise 4.2</h3> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>To instantiate the other GPT model sizes, we can modify the configuration dictionary as follows (here shown for GPT-2 XL):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p40"> 
   <div class="code-area-container"> 
    <pre class="code-area">GPT_CONFIG = GPT_CONFIG_124M.copy()
GPT_CONFIG["emb_dim"] = 1600
GPT_CONFIG["n_layers"] = 48
GPT_CONFIG["n_heads"] = 25
model = GPTModel(GPT_CONFIG)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>Then, reusing the code from section 4.6 to calculate the number of parameters and RAM requirements, we find</p> 
  </div> 
  <div class="browsable-container listing-container" id="p42"> 
   <div class="code-area-container"> 
    <pre class="code-area">gpt2-xl:
Total number of parameters: 1,637,792,000
Number of trainable parameters considering weight tying: 1,557,380,800
Total size of the model: 6247.68 MB</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p43"> 
   <h3 class=" readable-text-h3">Exercise 4.3</h3> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>There are three distinct places in chapter 4 where we used dropout layers: the embedding layer, shortcut layer, and multi-head attention module. We can control the dropout rates for each of the layers by coding them separately in the config file and then modifying the code implementation accordingly. </p> 
  </div> 
  <div class="readable-text intended-text" id="p45"> 
   <p>The modified configuration is as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p46"> 
   <div class="code-area-container"> 
    <pre class="code-area">GPT_CONFIG_124M = {
    "vocab_size": 50257,
    "context_length": 1024,
    "emb_dim": 768,
    "n_heads": 12,
    "n_layers": 12,
    "drop_rate_attn": 0.1,     <span class="aframe-location"/> #1
    "drop_rate_shortcut": 0.1,     <span class="aframe-location"/> #2
    "drop_rate_emb": 0.1,     <span class="aframe-location"/> #3
    "qkv_bias": False
}</pre> 
    <div class="code-annotations-overlay-container">
     #1 Dropout for multi-head attention
     <br/>#2 Dropout for shortcut connections
     <br/>#3 Dropout for embedding layer
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>The modified <code>TransformerBlock</code> and <code>GPTModel</code> look like</p> 
  </div> 
  <div class="browsable-container listing-container" id="p48"> 
   <div class="code-area-container"> 
    <pre class="code-area">class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"], 
            dropout=cfg["drop_rate_attn"],     <span class="aframe-location"/> #1
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(       <span class="aframe-location"/> #2
            cfg["drop_rate_shortcut"]           #2
        )                                       #2

    def forward(self, x):
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)
        x = self.drop_shortcut(x)
        x = x + shortcut

        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut
        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(
            cfg["vocab_size"], cfg["emb_dim"]
        )
        self.pos_emb = nn.Embedding(
            cfg["context_length"], cfg["emb_dim"]
        )
        self.drop_emb = nn.Dropout(cfg["drop_rate_emb"])   <span class="aframe-location"/> #3

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(
            cfg["emb_dim"], cfg["vocab_size"], bias=False
        )

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(
            torch.arange(seq_len, device=in_idx.device)
        )
        x = tok_embeds + pos_embeds
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logitss</pre> 
    <div class="code-annotations-overlay-container">
     #1 Dropout for multi-head attention
     <br/>#2 Dropout for shortcut connections
     <br/>#3 Dropout for embedding layer
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p49"> 
   <h2 class=" readable-text-h2">Chapter 5</h2> 
  </div> 
  <div class="readable-text" id="p50"> 
   <h3 class=" readable-text-h3">Exercise 5.1</h3> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>We can print the number of times the token (or word) “pizza” is sampled using the <code>print_sampled_tokens</code> function we defined in this section. Let’s start with the code we defined in section 5.3.1.</p> 
  </div> 
  <div class="readable-text intended-text" id="p52"> 
   <p>The “pizza” token is sampled 0x if the temperature is 0 or 0.1, and it is sampled 32× if the temperature is scaled up to 5. The estimated probability is 32/1000 × 100% = 3.2%.</p> 
  </div> 
  <div class="readable-text intended-text" id="p53"> 
   <p>The actual probability is 4.3% and is contained in the rescaled softmax probability tensor (<code>scaled_probas[2][6]</code>).</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <h3 class=" readable-text-h3">Exercise 5.2</h3> 
  </div> 
  <div class="readable-text" id="p55"> 
   <p>Top-k sampling and temperature scaling are settings that have to be adjusted based on the LLM and the desired degree of diversity and randomness in the output. </p> 
  </div> 
  <div class="readable-text intended-text" id="p56"> 
   <p>When using relatively small top-k values (e.g., smaller than 10) and when the temperature is set below 1, the model’s output becomes less random and more deterministic. This setting is useful when we need the generated text to be more predictable, coherent, and closer to the most likely outcomes based on the training data. </p> 
  </div> 
  <div class="readable-text intended-text" id="p57"> 
   <p>Applications for such low k and temperature settings include generating formal documents or reports where clarity and accuracy are most important. Other examples of applications include technical analysis or code-generation tasks, where precision is crucial. Also, question answering and educational content require accurate answers where a temperature below 1 is helpful.</p> 
  </div> 
  <div class="readable-text intended-text" id="p58"> 
   <p>On the other hand, larger top-k values (e.g., values in the range of 20 to 40) and temperature values above 1 are useful when using LLMs for brainstorming or generating creative content, such as fiction.</p> 
  </div> 
  <div class="readable-text" id="p59"> 
   <h3 class=" readable-text-h3">Exercise 5.3</h3> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>There are multiple ways to force deterministic behavior with the <code>generate</code> function:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p61"> Setting to <code>top_k=None</code> and applying no temperature scaling </li> 
   <li class="readable-text" id="p62"> Setting <code>top_k=1</code> </li> 
  </ol> 
  <div class="readable-text" id="p63"> 
   <h3 class=" readable-text-h3">Exercise 5.4</h3> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>In essence, we have to load the model and optimizer that we saved in the main chapter:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p65"> 
   <div class="code-area-container"> 
    <pre class="code-area">checkpoint = torch.load("model_and_optimizer.pth")
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint["model_state_dict"])
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>Then, call the <code>train_simple_function</code> with <code>num_epochs=1</code> to train the model for another epoch.</p> 
  </div> 
  <div class="readable-text" id="p67"> 
   <h3 class=" readable-text-h3">Exercise 5.5</h3> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>We can use the following code to calculate the training and validation set losses of the GPT model:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p69"> 
   <div class="code-area-container"> 
    <pre class="code-area">train_loss = calc_loss_loader(train_loader, gpt, device)
val_loss = calc_loss_loader(val_loader, gpt, device)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>The resulting losses for the 124-million parameter are as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p71"> 
   <div class="code-area-container"> 
    <pre class="code-area">Training loss: 3.754748503367106
Validation loss: 3.559617757797241</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>The main observation is that the training and validation set performances are in the same ballpark. This can have multiple explanations:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p73"> “The Verdict” was not part of the pretraining dataset when OpenAI trained GPT-2. Hence, the model is not explicitly overfitting to the training set and performs similarly well on the training and validation set portions of “The Verdict.” (The validation set loss is slightly lower than the training set loss, which is unusual in deep learning. However, it’s likely due to random noise since the dataset is relatively small. In practice, if there is no overfitting, the training and validation set performances are expected to be roughly identical). </li> 
   <li class="readable-text" id="p74"> “The Verdict” was part of GPT-2’s training dataset. In this case, we can’t tell whether the model is overfitting the training data because the validation set would have been used for training as well. To evaluate the degree of overfitting, we’d need a new dataset generated after OpenAI finished training GPT-2 to make sure that it couldn’t have been part of the pretraining. </li> 
  </ol> 
  <div class="readable-text" id="p75"> 
   <h3 class=" readable-text-h3">Exercise 5.6</h3> 
  </div> 
  <div class="readable-text" id="p76"> 
   <p>In the main chapter, we experimented with the smallest GPT-2 model, which has only 124-million parameters. The reason was to keep the resource requirements as low as possible. However, you can easily experiment with larger models with minimal code changes. For example, instead of loading the 1,558 million instead of 124 million model weights in chapter 5, the only two lines of code that we have to change are the following:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p77"> 
   <div class="code-area-container"> 
    <pre class="code-area">hparams, params = download_and_load_gpt2(model_size="124M", models_dir="gpt2")
model_name = "gpt2-small (124M)"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>The updated code is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p79"> 
   <div class="code-area-container"> 
    <pre class="code-area">hparams, params = download_and_load_gpt2(model_size="1558M", models_dir="gpt2")
model_name = "gpt2-xl (1558M)"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p80"> 
   <h2 class=" readable-text-h2">Chapter 6</h2> 
  </div> 
  <div class="readable-text" id="p81"> 
   <h3 class=" readable-text-h3">Exercise 6.1</h3> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>We can pad the inputs to the maximum number of tokens the model supports by setting the max length to <code>max_length</code> <code>=</code> <code>1024</code> when initializing the datasets:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p83"> 
   <div class="code-area-container"> 
    <pre class="code-area">train_dataset = SpamDataset(..., max_length=1024, ...)
val_dataset = SpamDataset(..., max_length=1024, ...)
test_dataset = SpamDataset(..., max_length=1024, ...)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>However, the additional padding results in a substantially worse test accuracy of 78.33% (vs. the 95.67% in the main chapter).</p> 
  </div> 
  <div class="readable-text" id="p85"> 
   <h3 class=" readable-text-h3">Exercise 6.2</h3> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>Instead of fine-tuning just the final transformer block, we can fine-tune the entire model by removing the following lines from the code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p87"> 
   <div class="code-area-container"> 
    <pre class="code-area">for param in model.parameters():
    param.requires_grad = False</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>This modification results in a 1% improved test accuracy of 96.67% (vs. the 95.67% in the main chapter).</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <h3 class=" readable-text-h3">Exercise 6.3</h3> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>Rather than fine-tuning the last output token, we can fine-tune the first output token by changing <code>model(input_batch)[:,</code> <code>-1,</code> <code>:]</code> to <code>model(input_batch)[:,</code> <code>0,</code> <code>:]</code> everywhere in the code.</p> 
  </div> 
  <div class="readable-text intended-text" id="p91"> 
   <p>As expected, since the first token contains less information than the last token, this change results in a substantially worse test accuracy of 75.00% (vs. the 95.67% in the main chapter).</p> 
  </div> 
  <div class="readable-text" id="p92"> 
   <h2 class=" readable-text-h2">Chapter 7</h2> 
  </div> 
  <div class="readable-text" id="p93"> 
   <h3 class=" readable-text-h3">Exercise 7.1</h3> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>The Phi-3 prompt format, which is shown in figure 7.4, looks like the following for a given example input:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p95"> 
   <div class="code-area-container"> 
    <pre class="code-area">&lt;user&gt;
Identify the correct spelling of the following word: 'Occasion'

&lt;assistant&gt;
The correct spelling is 'Occasion'.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>To use this template, we can modify the <code>format_input</code> function as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p97"> 
   <div class="code-area-container"> 
    <pre class="code-area">def format_input(entry):
    instruction_text = (
        f"&lt;|user|&gt;\n{entry['instruction']}"
    )
    input_text = f"\n{entry['input']}" if entry["input"] else ""
    return instruction_text + input_text</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>Lastly, we also have to update the way we extract the generated response when we collect the test set responses:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p99"> 
   <div class="code-area-container"> 
    <pre class="code-area">for i, entry in tqdm(enumerate(test_data), total=len(test_data)):
    input_text = format_input(entry)
    tokenizer=tokenizer
    token_ids = generate(
        model=model,
        idx=text_to_token_ids(input_text, tokenizer).to(device),
        max_new_tokens=256,
        context_size=BASE_CONFIG["context_length"],
        eos_id=50256
    )
    generated_text = token_ids_to_text(token_ids, tokenizer)
    response_text = (                      <span class="aframe-location"/> #1
        generated_text[len(input_text):]
        .replace("&lt;|assistant|&gt;:", "")
        .strip()
    )
    test_data[i]["model_response"] = response_text</pre> 
    <div class="code-annotations-overlay-container">
     #1 New: Adjust ###Response to &lt;|assistant|&gt;
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>Fine-tuning the model with the Phi-3 template is approximately 17% faster since it results in shorter model inputs. The score is close to 50, which is in the same ballpark as the score we previously achieved with the Alpaca-style prompts.</p> 
  </div> 
  <div class="readable-text" id="p101"> 
   <h3 class=" readable-text-h3">Exercise 7.2</h3> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>To mask out the instructions as shown in figure 7.13, we need to make slight modifications to the <code>InstructionDataset</code> class and <code>custom_collate_fn</code> function. We can modify the <code>InstructionDataset</code> class to collect the lengths of the instructions, which we will use in the collate function to locate the instruction content positions in the targets when we code the collate function, as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p103"> 
   <div class="code-area-container"> 
    <pre class="code-area">class InstructionDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data
        self.instruction_lengths = []    <span class="aframe-location"/> #1
        self.encoded_texts = []

        for entry in data:
            instruction_plus_input = format_input(entry)
            response_text = f"\n\\n### Response:\\n{entry['output']}"
            full_text = instruction_plus_input + response_text

            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )
            instruction_length = ( 
                len(tokenizer.encode(instruction_plus_input)
            )
            self.instruction_lengths.append(instruction_length)     <span class="aframe-location"/> #2

    def __getitem__(self, index):   <span class="aframe-location"/> #3
        return self.instruction_lengths[index], self.encoded_texts[index]

    def __len__(self):
        return len(self.data)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Separate list for instruction lengths
     <br/>#2 Collects instruction lengths
     <br/>#3 Returns both instruction lengths and texts separately
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>Next, we update the <code>custom_collate_fn</code> where each <code>batch</code> is now a tuple containing <code>(instruction_length,</code> <code>item)</code> instead of just <code>item</code> due to the changes in the <code>InstructionDataset</code> dataset. In addition, we now mask the corresponding instruction tokens in the target ID list:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p105"> 
   <div class="code-area-container"> 
    <pre class="code-area">def custom_collate_fn(
    batch,
    pad_token_id=50256,
    ignore_index=-100,
    allowed_max_length=None,
    device="cpu"
):

    batch_max_length = max(len(item)+1 for instruction_length, item in batch)
    inputs_lst, targets_lst = [], []         <span class="aframe-location"/> #1

    for instruction_length, item in batch:   
        new_item = item.copy()
        new_item += [pad_token_id]
        padded = (
            new_item + [pad_token_id] * (batch_max_length - len(new_item)
        )
        inputs = torch.tensor(padded[:-1])
        targets = torch.tensor(padded[1:])
        mask = targets == pad_token_id
        indices = torch.nonzero(mask).squeeze()
        if indices.numel() &gt; 1:
            targets[indices[1:]] = ignore_index

        targets[:instruction_length-1] = -100      <span class="aframe-location"/> #2

        if allowed_max_length is not None:
            inputs = inputs[:allowed_max_length]
            targets = targets[:allowed_max_length]

        inputs_lst.append(inputs)
        targets_lst.append(targets)

    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)

    return inputs_tensor, targets_tensor</pre> 
    <div class="code-annotations-overlay-container">
     #1 batch is now a tuple.
     <br/>#2 Masks all input and instruction tokens in the targets
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>When evaluating a model fine-tuned with this instruction masking method, it performs slightly worse (approximately 4 points using the Ollama Llama 3 method from chapter 7). This is consistent with observations in the “Instruction Tuning With Loss Over Instructions” paper (<a href="https://arxiv.org/abs/2405.14394">https://arxiv.org/abs/2405.14394</a>).</p> 
  </div> 
  <div class="readable-text" id="p107"> 
   <h3 class=" readable-text-h3">Exercise 7.3</h3> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>To fine-tune the model on the original Stanford Alpaca dataset (<a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a>), we just have to change the file URL from</p> 
  </div> 
  <div class="browsable-container listing-container" id="p109"> 
   <div class="code-area-container"> 
    <pre class="code-area">url = "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>to</p> 
  </div> 
  <div class="browsable-container listing-container" id="p111"> 
   <div class="code-area-container"> 
    <pre class="code-area">url = "https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>Note that the dataset contains 52,000 entries (50x more than in chapter 7), and the entries are longer than the ones we worked with in chapter 7.</p> 
  </div> 
  <div class="readable-text intended-text" id="p113"> 
   <p>Thus, it’s highly recommended that the training be run on a GPU.</p> 
  </div> 
  <div class="readable-text intended-text" id="p114"> 
   <p>If you encounter out-of-memory errors, consider reducing the batch size from 8 to 4, 2, or 1. In addition to lowering the batch size, you may also want to consider lowering the <code>allowed_max_length</code> from 1024 to 512 or 256.</p> 
  </div> 
  <div class="readable-text intended-text" id="p115"> 
   <p>Below are a few examples from the Alpaca dataset, including the generated model responses:</p> 
  </div> 
  <div class="readable-text" id="p116"> 
   <h3 class=" readable-text-h3">Exercise 7.4</h3> 
  </div> 
  <div class="readable-text" id="p117"> 
   <p>To instruction fine-tune the model using LoRA, use the relevant classes and functions from appendix E:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p118"> 
   <div class="code-area-container"> 
    <pre class="code-area">from appendix_E import LoRALayer, LinearWithLoRA, replace_linear_with_lora</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p119"> 
   <p>Next, add the following lines of code below the model loading code in section 7.5:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p120"> 
   <div class="code-area-container"> 
    <pre class="code-area">total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total trainable parameters before: {total_params:,}")

for param in model.parameters():
    param.requires_grad = False

total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total trainable parameters after: {total_params:,}")
replace_linear_with_lora(model, rank=16, alpha=16)

total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total trainable LoRA parameters: {total_params:,}")
model.to(device)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p121"> 
   <p>Note that, on an Nvidia L4 GPU, the fine-tuning with LoRA takes 1.30 min to run on an L4. On the same GPU, the original code takes 1.80 minutes to run. So, LoRA is approximately 28% faster in this case. The score, evaluated with the Ollama Llama 3 method from chapter 7, is around 50, which is in the same ballpark as the original model.</p> 
  </div> 
  <div class="readable-text" id="p122"> 
   <h2 class=" readable-text-h2">Appendix A</h2> 
  </div> 
  <div class="readable-text" id="p123"> 
   <h3 class=" readable-text-h3">Exercise A.1</h3> 
  </div> 
  <div class="readable-text" id="p124"> 
   <p>The optional Python Setup Tips document (<a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences">https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences</a>) contains additional recommendations and tips if you need additional help to set up your Python environment.</p> 
  </div> 
  <div class="readable-text" id="p133"> 
   <h3 class=" readable-text-h3">Exercise A.2</h3> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>The the optional "Installing Libraries Used In This Book" document (<a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/02_installing-python-libraries">https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/02_installing-python-libraries</a>) contains utilities to check whether your environment is set up correctly.</p> 
  </div> 
  <div class="readable-text" id="p143"> 
   <h3 class=" readable-text-h3">Exercise A.3</h3> 
  </div> 
  <div class="readable-text" id="p144"> 
   <p>The network has two inputs and two outputs. In addition, there are 2 hidden layers with 30 and 20 nodes, respectively. Programmatically, we can calculate the number of parameters as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p145"> 
   <div class="code-area-container"> 
    <pre class="code-area">model = NeuralNetwork(2, 2)
num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print("Total number of trainable model parameters:", num_params)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p146"> 
   <p>This returns</p> 
  </div> 
  <div class="browsable-container listing-container" id="p147"> 
   <div class="code-area-container"> 
    <pre class="code-area">752</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p148"> 
   <p>We can also calculate this manually as follows:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p149"> <em>First hidden layer:</em> 2 inputs times 30 hidden units plus 30 bias units </li> 
   <li class="readable-text" id="p150"> <em>Second hidden layer:</em> 30 incoming units times 20 nodes plus 20 bias units </li> 
   <li class="readable-text" id="p151"> <em>Output layer:</em> 20 incoming nodes times 2 output nodes plus 2 bias units </li> 
  </ul> 
  <div class="readable-text" id="p152"> 
   <p>Then, adding all the parameters in each layer results in 2×30+30 + 30×20+20 + 20×2+2 = 752.</p> 
  </div> 
  <div class="readable-text" id="p153"> 
   <h3 class=" readable-text-h3">Exercise A.4</h3> 
  </div> 
  <div class="readable-text" id="p154"> 
   <p>The exact run-time results will be specific to the hardware used for this experiment. In my experiments, I observed significant speed-ups even for small matrix multiplications when using a Google Colab instance connected to a V100 GPU:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p155"> 
   <div class="code-area-container"> 
    <pre class="code-area">a = torch.rand(100, 200)
b = torch.rand(200, 300)
%timeit a@b</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p156"> 
   <p>On the CPU this resulted in</p> 
  </div> 
  <div class="browsable-container listing-container" id="p157"> 
   <div class="code-area-container"> 
    <pre class="code-area">63.8 µs ± 8.7 µs per loop</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p158"> 
   <p>When executed on a GPU</p> 
  </div> 
  <div class="browsable-container listing-container" id="p159"> 
   <div class="code-area-container"> 
    <pre class="code-area">a, b = a.to("cuda"), b.to("cuda")
%timeit a @ b</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p160"> 
   <p>The result was</p> 
  </div> 
  <div class="browsable-container listing-container" id="p161"> 
   <div class="code-area-container"> 
    <pre class="code-area">13.8 µs ± 425 ns per loop</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p162"> 
   <p>In this case, on a V100, the computation was approximately four times faster.</p> 
  </div>
 </div></div></body></html>