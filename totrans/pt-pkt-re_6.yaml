- en: Chapter 6\. PyTorch Acceleration and Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you learned how to use the built-in capabilities of
    PyTorch and extend those capabilities by creating your own custom components for
    deep learning. Doing so enables you to quickly design new models and algorithms
    to train them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: However, when dealing with very large datasets or more complex models, training
    your models on a single CPU or GPU can take an extremely long time—it may take
    days or even weeks to get preliminary results. Longer training times can become
    frustrating, especially when you want to conduct many experiments using different
    hyperparameter configurations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore state-of-the-art techniques to accelerate and
    optimize your model development with PyTorch. First, we’ll look at using tensor
    processing units (TPUs) instead of GPU devices and consider instances in which
    using TPUs can improve performance. Next, I’ll show you how to use PyTorch’s built-in
    capabilities for parallel processing and distributed training. This will provide
    a quick reference for training models across multiple GPUs and multiple machines
    so you can quickly scale your training when more hardware resources are available.
    After exploring ways to accelerate training, we’ll look at how to optimize your
    models using advanced techniques like hyperparameter tuning, quantization, and
    pruning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The chapter will also provide reference code to make getting started easy, and
    references to the key packages and libraries we’ve used. Once you create your
    models and training loops, you can return to this chapter for tips on how to accelerate
    and optimize your training process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by exploring how to run your models on TPUs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch on a TPU
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As deep learning and AI are increasingly deployed, companies are developing
    custom hardware chips or ASICs aimed at optimizing model performance in hardware.
    Google developed its own ASIC for NN acceleration called the TPU. Since the TPU
    was designed for NNs, it does not have some of the downfalls of the GPU, which
    was designed for graphics processing. Google’s TPU is now available for you to
    use as part of Google Cloud TPU. You can also run Google Colab with a TPU.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapters, I showed you how to test and train your deep models
    using a GPU. You should continue to use CPUs and GPUs for training if the following
    conditions apply to your use case:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: You have small- or medium-size models with small batch sizes.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your models do not take long to train.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving data in and out is your main bottleneck.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your calculations are frequently branching or mostly done element-wise, or you
    use sparse memory access.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to use high precision. Doubles are not suitable for TPUs.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, there are several reasons why you may want to use a TPU
    instead of a GPU for training. TPUs are very fast at performing dense vector and
    matrix computations. They are optimized for specific workloads. You should strongly
    consider using TPUs when the following apply to your use case:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Your model is dominated by matrix computations.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your model has long training times.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to run multiple iterations of your entire training loop on TPUs.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Running on a TPU is very similar to running on a CPU or a GPU. Let’s revisit
    how we would train the model on a GPU in the following code:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO1-1)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Configure the device to a GPU if it’s available.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO1-2)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Send the model to the device.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_pytorch_acceleration_and_optimization_CO1-3)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Send inputs and labels to the GPU.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Namely, we move the model, inputs, and labels to the GPU, and the rest is done
    for us. Training your network on a TPU is almost the same as training it on a
    GPU except you will need to use the *PyTorch/XLA* (Accelerated Linear Algebra)
    package as TPUs are not currently supported natively by PyTorch.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Let’s train our model on a Cloud TPU using Google Colab. Open a new Colab notebook
    and select Change Runtime Type from the Runtime menu. Then select TPU from the
    “Hardware accelerator” drop-down menu, as shown in [Figure 6-1](#fig_colab_tpu).
    Google Colab provides a free Cloud TPU system, including a remote CPU host and
    four TPU chips with two cores each.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Google Colab上使用Cloud TPU训练我们的模型。打开一个新的Colab笔记本，并从运行时菜单中选择更改运行时类型。然后从“硬件加速器”下拉菜单中选择TPU，如[图6-1](#fig_colab_tpu)所示。Google
    Colab提供免费的Cloud TPU系统，包括远程CPU主机和每个具有两个核心的四个TPU芯片。
- en: '![ptpr 0601](Images/ptpr_0601.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![ptpr 0601](Images/ptpr_0601.png)'
- en: Figure 6-1\. Using a TPU in Google Colab
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1。在Google Colab中使用TPU
- en: 'Since Colab does not have PyTorch/XLA installed by default we’ll need to install
    that first, using the following commands. This installs the latest “nightly” version,
    but you can select another version if needed:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Colab默认未安装PyTorch/XLA，我们需要首先安装它，使用以下命令。这将安装最新的“夜间”版本，但如果需要，您可以选择其他版本：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: <1>These are commands that are intended to run in a notebook. Omit the “!” to
    run them on the command line.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <1>这些是打算在笔记本中运行的命令。在命令行上运行时，请省略“!”。
- en: 'Once PyTorch/XLA is installed, we can import the package and move our data
    to the TPU:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 安装PyTorch/XLA后，我们可以导入该软件包并将数据移动到TPU：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that we do not use `torch.cuda.is_available()` here, since it only works
    for GPUs. Unfortunately, there is no `is_available()` method for TPUs. If your
    environment is not configured for TPUs, you will get an error.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们这里不使用`torch.cuda.is_available()`，因为它仅适用于GPU。不幸的是，TPU没有`is_available()`方法。如果您的环境未配置为TPU，您将收到错误消息。
- en: 'Once the device is set, the rest of the code is exactly the same:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 设备设置完成后，其余代码完全相同：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO2-1)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO2-1)'
- en: If Colab is configured for TPUs, you should see `xla:1`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Colab配置为TPU，您应该看到 `xla:1`。
- en: PyTorch/XLA is a general library for XLA operations and may support other specialized
    ASICs in addition to TPUs. For more information on PyTorch/XLA, visit the [PyTorch/XLA
    GitHub repository](https://pytorch.tips/xla).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch/XLA是一个用于XLA操作的通用库，可能支持除TPU之外的其他专用ASIC。有关PyTorch/XLA的更多信息，请访问[PyTorch/XLA
    GitHub存储库](https://pytorch.tips/xla)。
- en: There are still many limitations for running on TPUs, and GPU support is more
    widespread. Therefore, most PyTorch developers will benchmark their code using
    a single GPU at first and then explore using a single TPU or multiple GPUs to
    accelerate their code.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在TPU上运行仍然存在许多限制，GPU支持更加普遍。因此，大多数PyTorch开发人员将首先使用单个GPU对其代码进行基准测试，然后再探索使用单个TPU或多个GPU加速其代码。
- en: We’ve already covered using a single GPU earlier in this book. In the next section,
    I’ll show you how to train your models on machines with multiple GPUs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在本书的前面部分介绍了如何使用单个GPU。在下一节中，我将向您展示如何在具有多个GPU的机器上训练您的模型。
- en: PyTorch on Multiple GPUs (Single Machine)
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在多个GPU上的PyTorch（单台机器）
- en: When accelerating your training and development, it’s important to make the
    most of the hardware resources you have available. If you have a local machine
    or a network server with access to multiple GPUs, this section will show you how
    to fully utilize the GPUs on your system. In addition, you may want to scale your
    GPU resources by using cloud GPUs on a single instance. This is usually the first
    level of scaling before considering a distributed training approach.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在加速训练和开发时，充分利用您可用的硬件资源非常重要。如果您有一台本地计算机或网络服务器可以访问多个GPU，本节将向您展示如何充分利用系统上的GPU。此外，您可能希望通过在单个实例上使用云GPU来扩展GPU资源。这通常是在考虑分布式训练方法之前的第一级扩展。
- en: 'Running your code across multiple GPUs is often called *parallel processing*.
    There are two approaches to parallel processing: *data* parallel processing and
    *model* parallel processing. During data parallel processing, the data batches
    are split between multiple GPUs while each GPU runs a copy of the model. During
    model parallel processing, the model is split up between multiple GPUs and the
    data batches are pipelined into each portion.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个GPU上运行代码通常称为*并行处理*。并行处理有两种方法：*数据*并行处理和*模型*并行处理。在数据并行处理期间，数据批次在多个GPU之间分割，而每个GPU运行模型的副本。在模型并行处理期间，模型在多个GPU之间分割，数据批次被管道传送到每个部分。
- en: Data parallel processing is more commonly used in practice. Model parallel processing
    is often reserved for cases in which the model does not fit on a single GPU. I’ll
    show you how to perform both types of processing in this section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行处理在实践中更常用。模型并行处理通常保留用于模型不适合单个GPU的情况。我将在本节中向您展示如何执行这两种类型的处理。
- en: Data Parallel Processing
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据并行处理
- en: '[Figure 6-2](#fig_data_parallel) illustrates how data parallel processing works.
    In this process, each data batch is split into *N* parts (*N* is the number of
    GPUs available on the host). *N* is typically a power of two. Each GPU holds a
    copy of the model, and the gradients and loss are computed for each portion of
    the batch. The gradients and loss are combined at the end of each iteration. This
    approach is good for larger batch sizes and use cases in which the model will
    fit on a single GPU.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-2](#fig_data_parallel)说明了数据并行处理的工作原理。在此过程中，每个数据批次被分成*N*部分（*N*是主机上可用的GPU数量）。*N*通常是2的幂。每个GPU持有模型的副本，并且为批次的每个部分计算梯度和损失。在每次迭代结束时，梯度和损失被合并。这种方法适用于较大的批次大小和模型适合单个GPU的用例。'
- en: Data parallel processing can be implemented in PyTorch using a *single-process,
    multithreaded approach* or by using a *multiprocess* approach. The single-process,
    multithreaded approach requires only one additional line of code but does not
    perform well in many cases.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch可以使用*单进程，多线程方法*或使用*多进程*方法来实现数据并行处理。单进程，多线程方法只需要一行额外的代码，但在许多情况下性能不佳。
- en: '![ptpr 0602](Images/ptpr_0602.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![ptpr 0602](Images/ptpr_0602.png)'
- en: Figure 6-2\. Data parallel processing
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2。数据并行处理
- en: Unfortunately, multithreading performs poorly due to Python’s Global Interpreter
    Lock (GIL) contention across threads, the per-iteration replication of the model,
    and the additional overhead introduced by scattering inputs and gathering outputs.
    You may want to try this approach because it’s so simple, but in most cases, you
    will probably use the multiprocess approach.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于Python的全局解释器锁（GIL）在线程之间的争用、模型的每次迭代复制以及输入散布和输出收集引入的额外开销，多线程性能较差。您可能想尝试这种方法，因为它非常简单，但在大多数情况下，您可能会使用多进程方法。
- en: The multithreaded approach using nn.DataParallel
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用nn.DataParallel的多线程方法
- en: 'The multithreaded approach to data parallel processing is natively supported
    by PyTorch’s `nn` module. All you need to do is wrap your model in `nn.DataParallel`
    before sending it to the GPU, as shown in the following code. Here we assume you
    have already instantiated your model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的`nn`模块原生支持多线程的数据并行处理。您只需要在将模型发送到GPU之前将其包装在`nn.DataParallel`中，如下面的代码所示。在这里，我们假设您已经实例化了您的模型：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: First we check to make sure we have multiple GPUs, and then we use `nn.DataParallel()`
    to set up data parallel processing before sending the model to the GPU with `to(*device*)`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查确保我们有多个GPU，然后我们使用`nn.DataParallel()`在将模型发送到GPU之前设置数据并行处理。
- en: This multithreaded approach is the simplest way to run on multiple GPUs; however,
    the multiprocess approach usually performs better, even on a single machine. In
    addition, the multiprocess approach can also be used to run across multiple machines,
    as we’ll see later in this chapter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多线程方法是在多个GPU上运行的最简单方式；然而，多进程方法通常在单台机器上表现更好。此外，多进程方法也可以用于跨多台机器运行，我们将在本章后面看到。
- en: The multiprocess approach using DDP (preferred)
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用DDP的多进程方法（首选）
- en: Training your models across multiple GPUs is best accomplished using a multiprocess
    approach. PyTorch supports this with its `nn.parallel.DistributedDataProcessing`
    module. Distributed data processing (DDP) can be used with multiple processes
    on a single machine or with multiple processes across multiple machines. We’ll
    start with a single machine.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最好使用多进程方法在多个GPU上训练您的模型。PyTorch通过其`nn.parallel.DistributedDataProcessing`模块支持这一点。分布式数据处理（DDP）可以在单台机器上的多个进程或跨多台机器的多个进程中使用。我们将从单台机器开始。
- en: 'There are four steps you need to do to modify your code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个步骤需要修改您的代码：
- en: Initialize a process group using *torch.distributed*.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*torch.distributed*初始化一个进程组。
- en: Create a local model using *torch.nn.to()*.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*torch.nn.to()*创建一个本地模型。
- en: Wrap the model with DDP using *torch.nn.parallel*.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*torch.nn.parallel*将模型包装在DDP中。
- en: Spawn processes using *torch.multiprocessing*.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*torch.multiprocessing*生成进程。
- en: 'The following code demonstrates how you can convert your model for DDP training.
    We’ll break it down into steps. First, import the necessary libraries:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何将您的模型转换为DDP训练。我们将其分解为步骤。首先，导入必要的库：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Notice that we’re using three new libraries—*torch.distributed*, *torch.multiprocessing*,
    and *torch.nn.parallel*. The following code shows you how to create a distributed
    training loop:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在使用三个新库—*torch.distributed*、*torch.multiprocessing*和*torch.nn.parallel*。以下代码向您展示如何创建一个分布式训练循环：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO3-1)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO3-1)'
- en: Set up a process group with `world_size` processes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`world_size`进程设置一个进程组。
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO3-2)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO3-2)'
- en: Move the model to a GPU with the ID of `rank`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型移动到ID为`rank`的GPU。
- en: '[![3](Images/3.png)](#co_pytorch_acceleration_and_optimization_CO3-3)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_pytorch_acceleration_and_optimization_CO3-3)'
- en: Wrap the model in DDP.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型包装在DDP中。
- en: '[![4](Images/4.png)](#co_pytorch_acceleration_and_optimization_CO3-4)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_pytorch_acceleration_and_optimization_CO3-4)'
- en: Move inputs and labels to the GPU with the ID of `rank`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入和标签移动到ID为`rank`的GPU。
- en: '[![5](Images/5.png)](#co_pytorch_acceleration_and_optimization_CO3-5)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_pytorch_acceleration_and_optimization_CO3-5)'
- en: Call the DDP model for the forward pass.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 调用DDP模型进行前向传递。
- en: DDP broadcasts the model states from the `rank0` process to all the other processes,
    so we don’t have to worry about the different processes having models with different
    initialized weights.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: DDP将模型状态从`rank0`进程广播到所有其他进程，因此我们不必担心不同进程具有具有不同初始化权重的模型。
- en: DDP handles the lower-level interprocess communications that allow you to treat
    the model as if it was a local model. During the backward pass, DDP automatically
    synchronizes the gradients and places the synchronized gradient tensor in `params.grad`
    when `loss.backward()` returns.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: DDP处理了低级别的进程间通信，使您可以将模型视为本地模型。在反向传播过程中，当`loss.backward()`返回时，DDP会自动同步梯度并将同步的梯度张量放在`params.grad`中。
- en: 'Now that we have the process defined, we need to create these processes using
    the `spawn()` function, as shown in the following code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了进程，我们需要使用`spawn()`函数创建这些进程，如下面的代码所示：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we run the code as `main` to spawn two processes, each with its own GPU.
    And that’s how you run data parallel processing on multiple GPUs on a single machine.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将代码作为`main`运行，生成两个进程，每个进程都有自己的GPU。这就是如何在单台机器上的多个GPU上运行数据并行处理。
- en: Warning
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: GPU devices cannot be shared across processes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: GPU设备不能在进程之间共享。
- en: If your model does not fit into a single GPU or you are using smaller batch
    sizes, you may consider using model parallel processing instead of data parallel
    processing. We’ll look at that next.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型不适合单个GPU或者使用较小的批量大小，您可以考虑使用模型并行处理而不是数据并行处理。接下来我们将看看这个。
- en: Model Parallel Processing
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型并行处理
- en: '[Figure 6-3](#fig_model_parallel) illustrates how model parallel processing
    works. In this process, the model is split across *N* GPUs on the same machine.
    If we process data batches in sequence, the next GPU will always be waiting for
    the previous GPU to finish, and this defeats the purpose of parallel processing.
    Therefore, we need to pipeline the data processing so that every GPU is running
    at any given moment. When we pipeline the data, only the first *N* batches are
    run in sequence, and then each subsequent run activates all the GPUs.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-3](#fig_model_parallel)展示了模型并行处理的工作原理。在这个过程中，模型被分割到同一台机器上的 *N* 个 GPU 中。如果我们按顺序处理数据批次，下一个
    GPU 将始终等待前一个 GPU 完成，这违背了并行处理的目的。因此，我们需要对数据处理进行流水线处理，以便每个 GPU 在任何给定时刻都在运行。当我们对数据进行流水线处理时，只有前
    *N* 个批次按顺序运行，然后每个后续运行会激活所有 GPU。'
- en: '![ptpr 0603](Images/ptpr_0603.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![ptpr 0603](Images/ptpr_0603.png)'
- en: Figure 6-3\. Model parallel processing
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 模型并行处理
- en: Implementing model parallel processing is not as simple as data parallel processing,
    and it requires you to rewrite your models. You’ll need to define how your models
    are split across multiple GPUs and how the data will be pipelined in the forward
    pass. This is typically done by writing a subclass for your model with a multi-GPU
    implementation for a specific number of GPUs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实现模型并行处理并不像数据并行处理那样简单，它需要您重新编写模型。您需要定义模型如何跨多个 GPU 分割以及数据在前向传递中如何进行流水线处理。通常通过为模型编写一个子类，具有特定数量的
    GPU 的多 GPU 实现来完成这一点。
- en: 'The following code demonstrates a two-GPU implementation of AlexNet:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了 AlexNet 的双 GPU 实现：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO4-1)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO4-1)'
- en: '`s_prev` runs on `cuda:1`.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`s_prev` 在 `cuda:1` 上运行。'
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO4-2)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO4-2)'
- en: '`s_next` runs on `cuda:0`, which can run concurrently with `s_prev`.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`s_next` 在 `cuda:0` 上运行，可以与 `s_prev` 并行运行。'
- en: 'Because we are deriving a subclass from the `AlexNet` class, we inherit its
    model structure, so there’s no need to create our layers. Instead, we need to
    describe which pieces of the model go on GPU0 and which pieces go on GPU1 in the
    `__init__()` constructor. Then we need to pipeline the data through each GPU in
    the `forward()` method to implement GPU pipelining. When you train your model,
    you will need to put labels on the last GPU, as shown in the following code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们从 `AlexNet` 类派生一个子类，我们继承了它的模型结构，所以不需要创建我们自己的层。相反，我们需要描述模型的哪些部分放在 GPU0 上，哪些部分放在
    GPU1 上。然后我们需要在 `forward()` 方法中通过每个 GPU 管道传递数据来实现 GPU 流水线。当训练模型时，您需要将标签放在最后一个 GPU
    上，如下面的代码所示：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO5-1)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO5-1)'
- en: Send inputs to GPU0 and labels to GPU1.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入发送到 GPU0，将标签发送到 GPU1。
- en: As you can see, the training loop requires changing one line of code to make
    sure the labels are on the last GPU since that’s where the outputs will be before
    calculating the loss.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，训练循环需要更改一行代码，以确保标签位于最后一个 GPU 上，因为在计算损失之前输出将位于那里。
- en: Data parallel processing and model parallel processing are two effective paradigms
    for leveraging multiple GPUs for accelerated training. Wouldn’t it be great if
    we could combine the two approaches and achieve even better results? Let’s see
    how to implement the combined approach.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行处理和模型并行处理是利用多个 GPU 进行加速训练的两种有效范式。如果我们能够将这两种方法结合起来并取得更好的结果，那将是多么美妙呢？让我们看看如何实现结合的方法。
- en: Combined Data Parallel Processing and Model Parallel Processing
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合数据并行处理和模型并行处理
- en: You can combine data parallel processing with model parallel processing to further
    improve performance. In this case, you will wrap your model using DDP to distribute
    your data batches among multiple processes. Each process will use multiple GPUs,
    and your model will be partitioned among each of those GPUs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将数据并行处理与模型并行处理结合起来，以进一步提高性能。在这种情况下，您将使用 DDP 包装您的模型，将数据批次分发给多个进程。每个进程将使用多个
    GPU，并且您的模型将被分割到每个 GPU 中。
- en: 'There are only two changes we need to make:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要做两个更改。
- en: Change our multi-GPU model class to accept devices as inputs.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的多 GPU 模型类更改为接受设备作为输入。
- en: Omit setting the output device during the forward pass. DDP will determine where
    the input and output data will be placed.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前向传递期间省略设置输出设备。DDP 将确定输入和输出数据的放置位置。
- en: 'The following code shows how to modify the multi-GPU model:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了如何修改多 GPU 模型：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the `__init__()` constructor we pass in the GPU device objects, `dev0` and
    `dev1`, and describe which parts of the model reside in which GPUs. This allows
    us to instantiate new models on different processes, each with two GPUs. The `forward()`
    method moves the data from one GPU to the next at the proper point in the model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `__init__()` 构造函数中，我们传入 GPU 设备对象 `dev0` 和 `dev1`，并描述模型的哪些部分位于哪些 GPU 中。这使我们能够在不同进程上实例化新模型，每个模型都有两个
    GPU。`forward()` 方法在模型的适当位置将数据从一个 GPU 移动到下一个 GPU。
- en: 'The training loop changes are shown in the following code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了训练循环的更改：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO6-1)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO6-1)'
- en: Wrap the model in `DDP`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型包装在 `DDP` 中。
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO6-2)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO6-2)'
- en: Move the inputs and labels to the appropriate device IDs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入和标签移动到适当的设备 ID。
- en: '[![3](Images/3.png)](#co_pytorch_acceleration_and_optimization_CO6-3)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_pytorch_acceleration_and_optimization_CO6-3)'
- en: The output is on `dev1`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出在 `dev1` 上。
- en: To recap, you have a few options when using PyTorch across multiple GPUs. You
    can use the reference code in this section to implement data parallel, model parallel,
    or combined parallel processing to accelerate your model training and inference.
    So far, we’ve only discussed multiple GPUs on a single machine or a cloud instance.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, parallel processing across multiple GPUs on a single machine
    can reduce training times by half or more—all you need to do is upgrade your GPU
    card or utilize a larger cloud GPU instance. However, if you are training very
    complex models or using extremely large datasets, you may want to use multiple
    machines or cloud instances to speed up your training.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that DDP on multiple machines is not much different than DDP
    on a single machine. The next section shows how this is done.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Training (Multiple Machines)
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If training your NN models on a single machine does not meet your needs and
    you have access to a cluster of servers, you can use PyTorch’s distributed processing
    capabilities to scale your training across multiple machines. PyTorch’s distributed
    subpackage, `torch.distributed`, provides a rich set of capabilities to suit a
    variety of training architectures and hardware platforms.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torch.distributed` subpackage consists of three components: DDP, RPC-based
    distributed training (RPC), and collective communication (c10d). We used DDP in
    the previous section to run multiple processes on a single machine, and it’s best
    suited for the data parallel processing paradigm. RPC was created to support more
    general training architectures and can be used for distributed architectures other
    than the data parallel processing paradigm.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The c10d component is a communications library used to transfer tensors across
    processes. c10d is used by both the DDP and RPC components as a backend, and PyTorch
    provides a c10d API so you can use it in custom distributed applications.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we’ll focus on using DDP for distributed training. However, if
    you have a more advanced use case, you may want to use RPC or c10d. You can find
    out more about these by reading the [PyTorch documentation](https://pytorch.tips/rpc).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: For distributed training with DDP, we will follow the same DDP procedure as
    we did for a single machine with multiple processes. However, in this case, we
    will run each process on a separate machine or instance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'To run across multiple machines, we run DDP with a launch script that specifies
    our configuration. The launch script is contained in `torch.distributed` and can
    be executed as shown in the following code. Let’s assume you have two nodes, Node
    0 and Node 1\. Node 0 is the master node and has an IP address of 192.168.1.1
    and a free port at 1234\. On Node 0, you would run the following script:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO7-1)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '`node_rank` is set to Node 0.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'On Node 1, you would run this next script. Notice that this node’s rank is
    `1`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO8-1)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '`node_rank` is set to Node 1.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’d like to explore the optional parameters in this script, run the following
    command:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Remember, if you are not using a DDP paradigm, you should consider using the
    RPC or c10d API for your use case. Parallel processing and distributed training
    can significantly speed up model performance and reduce development time. In the
    next section, we’ll consider other ways to improve NN performance by implementing
    techniques that optimize the model itself.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Model Optimization
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model optimization is an advanced topic that focuses on the underlying implementation
    of NN models and how they are trained. As research in this space continues to
    evolve, PyTorch has added various capabilities for model optimization. In this
    section, we’ll explore three areas of optimization—hyperparameter tuning, quantization,
    and pruning—and provide reference code for you to use in your own designs.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Tuning
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning model development often involves selecting many variables that
    are used to design a model and how it’s trained. These variables are called *hyperparameters*
    and can include architecture variations like the number of layers, layer depth,
    and kernel sizes, as well as optional stages like pooling or batch normalization.
    Hyperparameters may also include variations of loss functions or optimization
    parameters, such as LRs or weight decay rates.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I’ll show you how to use a package called Ray Tune to manage
    your hyperparameter optimization. Researchers often test a small set of hyperparameters
    manually. However, Ray Tune allows you to configure your hyperparameters and determines
    which settings are best for performance.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Ray Tune supports state-of-the-art hyperparameter search algorithms and distributed
    training. It is constantly being updated with new capabilities. Let’s see how
    we can use Ray Tune to perform hyperparameter tuning.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Remember the LeNet5 model we trained for image classification back in [Chapter 3](ch03.xhtml#deep_learning_development_with_pytorch)?
    Let’s experiment with different model configurations and training parameters to
    see if we can use hyperparameter tuning to improve our model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use Ray Tune, we need to make the following changes to our model:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Define our hyperparameters and their search space.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a function to wrap our training loop.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run Ray Tune hyperparameter tuning.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s redefine our model so that we can configure the number of nodes in the
    fully connected layers, as shown in the following code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO9-1)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Configure nodes in `fc1`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO9-2)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Configure nodes in `fc2`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: So far we have two hyperparameters, `nodes_1` and `nodes_2`. Let’s also define
    two more hyperparameters, `lr` and `batch_size`, so we can vary the learning rate
    and batch size used in our training.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we import the `ray` package and define the hyperparameter
    configuration:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: During each run, the values for these parameters are chosen from the specified
    search space. You can use the method `tune.sample_from()` and a `lambda` function
    to define a search space, or you can use built-in sampling functions. In this
    case, `layer_1` and `layer_2` are each set to a random value from `2` to `9` using
    `sample_from()`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The `lr` and `batch_size` use built-in functions in which `lr` is randomly chosen
    to be a double from 1e-4 to 1e-1 with uniform distribution, and `batch_size` is
    randomly chosen to be either `2`, `4`, `8`, or `16`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Next we need to wrap our training loop with a function that takes the configuration
    dictionary as an input. This training loop function will be called by Ray Tune.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we write our training loop, let’s define a function that loads the CIFAR-10
    data so we can reuse the data from the same directory during training. The following
    code is similar to the data-loading code we used in [Chapter 3](ch03.xhtml#deep_learning_development_with_pytorch):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we can wrap our training loop into a function, *train_model()*, as shown
    in the following code. This is a large snippet of code; however, it should be
    familiar to you:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO10-1)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Make the model layers configurable.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO10-2)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Make the learning rate configurable.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_pytorch_acceleration_and_optimization_CO10-3)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Make the batch size configurable.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we want to run Ray Tune, but we first need to determine the scheduler
    and the reporter that we want to use. The scheduler determines how Ray Tune searches
    and selects the hyperparameters, while the reporter specifies how we’d like to
    view the results. Let’s set them up in the following code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For the scheduler, we’ll use the asynchronous successive halving algorithm (ASHA)
    for hyperparameter searches and instruct it to minimize loss. For the reporter,
    we’ll configure a CLI reporter to report the loss, accuracy, training iteration,
    and selected hyperparameters on the CLI for each run.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally we can run Ray Tune using the `run()` method as shown in the following
    code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We provision the resources and specify the configuration. We pass in our configuration
    dictionary, specify the number of samples or runs, and pass in our `scheduler`
    and `reporter` functions.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Ray Tune will report the results. The `get_best_trial()` method returns an
    object that contains information about the best trial. We can print out the hyperparameter
    settings that yielded the best results, as shown in the following code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You may find other features of the Ray Tune API useful. [Table 6-1](#table_ray_tune_schedulers)
    lists the available schedulers in `tune.schedulers`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Ray Tune schedulers
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '| Scheduling method | Description |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| ASHA | Scheduler that runs the asynchronous successive halving algorithm
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| HyperBand | Scheduler that runs the HyperBand early stopping algorithm |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| Median Stopping Rule | Scheduler based on the median stopping rule, as described
    in [“Google Vizier: A Service for Black-Box Optimization”](https://research.google.com/pubs/pub46180.html).
    |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| Population Based Training | Scheduler based on the Population Based Training
    algorithm |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| Population Based Training Replay | Scheduler that replays a Population Based
    Training run |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| BOHB | Scheduler that uses Bayesian optimization and HyperBand |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| FIFOScheduler | Simple scheduler that just runs trials in submission order
    |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| TrialScheduler | Scheduler based on trials |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| Shim Instantiation | Scheduler based on the provided string |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: More information can be found in the [Ray Tune documentation.](https://pytorch.tips/ray)
    As you can see, Ray Tune has a rich set of capabilities, but there are other hyperparameter
    packages that support PyTorch as well. These include [Allegro Trains](https://pytorch.tips/allegro)
    and [Optuna](https://pytorch.tips/optuna).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyperparameter tuning can significantly improve the performance of an NN model
    by finding the settings that work best. Next, we’ll explore another technique
    to optimize a model: quantization.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NNs are implemented as computational graphs, and their computations often use
    32-bit (or in some cases, 64-bit) floating-point numbers. However, we can enable
    our computations to use lower-precision numbers and still achieve comparable results
    by applying quantization.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '*Quantization* refers to techniques for computing and accessing memory with
    lower-precision data. These techniques can decrease model size, reduce memory
    bandwidth, and perform faster inference due to savings in memory bandwidth and
    faster computing with int8 arithmetic.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick quantization method is to reduce all computation precision by half.
    Let’s consider our LeNet5 model example again, as shown in the following code:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'By default, all computations and memory are implemented as float32\. We can
    inspect the data types of our model’s parameters using the following code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As expected, our data types are float32\. However, we can reduce the model
    to half precision in one line of code using the `half()` method:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now our computation and memory values are float16 . Using `half()` is often
    a quick and easy way to quantize your models. It’s worth a try to see if the performance
    is adequate for your use case.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in many cases, we don’t want to quantize every computation in the
    same way, and we may need to quantize beyond float16 values. For these other cases,
    PyTorch provides three additional modes of quantization: dynamic quantization,
    post-training static quantization, and quantization-aware training (QAT).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic quantization is used when throughput is limited by compute or memory
    bandwidth for weights. This is often true for LSTM, RNN, Bidirectional Encoder
    Representations from Transformers (BERT), or Transformer networks. Static quantization
    is used when throughput is limited by memory bandwidth for activations and often
    applies for CNNs. QAT is used when accuracy requirements cannot be achieved by
    static quantization.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当权重的计算或内存带宽限制吞吐量时，使用动态量化。这通常适用于LSTM、RNN、双向编码器表示来自变压器（BERT）或变压器网络。当激活的内存带宽限制吞吐量时，通常适用于CNN的静态量化。当静态量化无法满足精度要求时，使用QAT。
- en: Let’s provide some reference code for each type. All types convert weights to
    int8\. They vary in handle activations and memory access.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为每种类型提供一些参考代码。所有类型将权重转换为int8。它们在处理激活和内存访问方面有所不同。
- en: '*Dynamic quantization* is the easiest type. It converts the activations to
    int8 on the fly. Computations use efficient int8 values, but the activations are
    read and written to memory in floating-point format.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*动态量化*是最简单的类型。它会将激活即时转换为int8。计算使用高效的int8值，但激活以浮点格式读取和写入内存。'
- en: 'The following code shows you how to quantize a model with dynamic quantization:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码向您展示了如何使用动态量化量化模型：
- en: '[PRE25]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: All we need to do is pass in our model and specify the quantized layers and
    the quantization level.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所需做的就是传入我们的模型并指定量化层和量化级别。
- en: Warning
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Quantization depends on the backend being used to run quantized models. Currently,
    quantized operators are supported for CPU inference only in the following backends:
    x86 (*fbgemm*) and ARM (`qnnpack`). However, quantization-aware training occurs
    in full floating point and can run on either GPUs or CPUs.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 量化取决于用于运行量化模型的后端。目前，量化运算符仅在以下后端中支持CPU推断：x86（*fbgemm*）和ARM（`qnnpack`）。然而，量化感知训练在完全浮点数上进行，并且可以在GPU或CPU上运行。
- en: '*Post-training static quantization* can be used to further reduce latency by
    observing the distributions of different activations during training and by deciding
    how those activations should be quantized at the time of inference. This type
    of quantization allows us to pass quantized values between operations without
    converting back and forth between floats and ints in memory:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*后训练静态量化*可通过观察训练期间不同激活的分布，并决定在推断时如何量化这些激活来进一步降低延迟。这种类型的量化允许我们在操作之间传递量化值，而无需在内存中来回转换浮点数和整数：'
- en: '[PRE26]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Post-training static quantization requires configuration and training to prepare
    it before its use. We configure the backend to use x86 (`fbgemm`) and call `torch.quantization.prepare`
    to insert observers to calibrate the model and collect statistics. Then we convert
    the model to a quantized version.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练静态量化需要配置和训练以准备使用。我们配置后端以使用x86（`fbgemm`），并调用`torch.quantization.prepare`来插入观察器以校准模型并收集统计信息。然后我们将模型转换为量化版本。
- en: '*Quantization-aware training* typically results in the best accuracy. In this
    case, all weights and activations are “fake quantized” during the forward and
    backward pass of training. Float values are rounded to the int8 equivalent, but
    the computations are still done in floating point. That is, the weight adjustments
    are made “aware” that they will be quantized during training. The following code
    shows how to quantize a model with QAT:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*量化感知训练*通常会产生最佳精度。在这种情况下，所有权重和激活在训练的前向和后向传递期间都被“伪量化”。浮点值四舍五入为int8等效值，但计算仍然以浮点数进行。也就是说，在训练期间进行量化时，权重调整是“知道”的。以下代码显示了如何使用QAT量化模型：'
- en: '[PRE27]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Again we need to configure the backend and prepare the model, and then we call
    `convert()` to quantize the model.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们需要配置后端并准备模型，然后调用`convert()`来量化模型。
- en: PyTorch’s quantization capabilities are continuing to evolve, and they currently
    exist in beta. Please refer to [the PyTorch documentation](https://pytorch.tips/quantization)
    for the latest information on how to use the Quantization package.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的量化功能正在不断发展，目前处于测试阶段。请参考[PyTorch文档](https://pytorch.tips/quantization)获取有关如何使用量化包的最新信息。
- en: Pruning
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修剪
- en: Modern deep learning models can have millions of parameters and can be difficult
    to deploy. However, models are over-parameterized, and parameters can often be
    reduced without affecting the accuracy or model performance much. *Pruning* is
    a technique that reduces the number of model parameters with minimal effect on
    performance. This allows you to deploy models with less memory, lower power usage,
    and reduced hardware resources.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习模型可能具有数百万个参数，并且可能难以部署。但是，模型是过度参数化的，参数通常可以减少而几乎不影响准确性或模型性能。*修剪*是一种通过最小影响性能来减少模型参数数量的技术。这使您可以部署具有更少内存、更低功耗和减少硬件资源的模型。
- en: Pruning model example
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修剪模型示例
- en: 'Pruning can be applied to an `nn.module`. Since an `nn.module` may consist
    of a single layer, multiple layers, or an entire model, pruning can be applied
    to a single layer, multiple layers, or the entire model itself. Let’s consider
    our example LeNet5 model:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪可以应用于`nn.module`。由于`nn.module`可能包含单个层、多个层或整个模型，因此可以将修剪应用于单个层、多个层或整个模型本身。让我们考虑我们的LeNet5模型示例：
- en: '[PRE28]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Our LeNet5 model has five submodules—`conv1`, `conv2`, `fc1`, `fc2`, and `fc3`.
    The model parameters consist of its weights and biases and can be shown using
    the `named_parameters()` method. Let’s look at the parameters of the `conv1` layer:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的LeNet5模型有五个子模块——`conv1`、`conv2`、`fc1`、`fc2`和`fc3`。模型参数包括其权重和偏差，可以使用`named_parameters()`方法显示。让我们看看`conv1`层的参数：
- en: '[PRE29]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Local and global pruning
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地和全局修剪
- en: '*Local pruning* is when we only prune a specific piece of our model. With this
    technique we can apply local pruning to a single layer or module. Just call your
    pruning method, passing in the layer, and set its options as shown in the following
    code:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*本地修剪*是指我们仅修剪模型的特定部分。通过这种技术，我们可以将本地修剪应用于单个层或模块。只需调用修剪方法，传入层，并设置其选项如下代码所示：'
- en: '[PRE30]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This example applies random unstructured pruning to the parameters named `weight`
    in the `conv1` layer in our model. This only prunes the weight parameters. We
    can prune the bias parameters as well with the following code:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Pruning can be applied iteratively, so you can further prune the same parameters
    using other pruning methods across different dimensions.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'You can prune modules and parameters differently. For example, you may want
    to prune by module or layer type and apply pruning to convolutional layers differently
    than linear layers. The following code illustrates one way to do so:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO11-1)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Prune all 2D convolutional layers by 30%.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO11-2)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Prune all linear layers by 50%.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Another use of the pruning API is to apply *global pruning*, in which we apply
    a pruning method to the entire model. For example, we could prune 25% of our model’s
    parameters globally, which would probably result in different pruning rates for
    each layer. The following code illustrates one way to apply global pruning:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here we prune 25% of all the parameters in the entire model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Pruning API
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch provides built-in support for pruning in its `torch.nn.utils.prune`
    module. [Table 6-2](#table_pruning) lists the available functions in the pruning
    API.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-2\. Pruning functions
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| `is_pruned(*module*)` | Checks whether the module is pruned |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| `remove(*module*, *name*)` | Removes the pruning reparameterization from
    a module and the pruning method from the forward hook |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| `custom_from_mask(*module*, *name*, *mask*)` | Prunes the tensor corresponding
    to the parameter called `name` in `module` by applying the precomputed mask in
    `mask` |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| `global_unstructured(*params*, *pruning_method*)` | Globally prunes tensors
    corresponding to all parameters in `params` by applying the specified `pruning_method`
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| `ln_structured(*module*, *name*, *amount*, *n*, *dim*)` | Prunes the tensor
    corresponding to the parameter called `name` in `module` by removing the specified
    `amount` of (currently unpruned) channels along the specified `dim` with the lowest
    L`n`-norm |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| `random_structured(*module*, *name*, *amount*, *dim*)` | Prunes the tensor
    corresponding to the parameter called `name` in `module` by removing the specified
    `amount` of (currently unpruned) channels along the specified `dim` selected at
    random |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| `l1_unstructured(*module*, *name*, *amount*)` | Prunes the tensor corresponding
    to the parameter called `name` in `module` by removing the specified `amount`
    of (currently unpruned) units with the lowest L1-norm |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| `random_unstructured(*module*, *name*, *amount*)` | Prunes tensor corresponding
    to the parameter called `name` in `module` by removing the specified `amount`
    of (currently unpruned) units selected at random |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: Custom pruning methods
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you can’t find a pruning method that suits your needs, you can create your
    own pruning method. To do so, create a subclass from the `BasePruningMethod` class
    provided in `torch.nn.utils.prune`. In most cases, you can leave the `call()`,
    `apply_mask()`, `apply()`, `prune()`, and `remove()` methods as they are.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'However, you will need to write your own `__init__()` constructor and `compute_mask()`
    method to describe how your pruning method computes the mask. In addition, you’ll
    need to specify the type of pruning (`structured`, `unstructured`, or `global`).
    The following code shows an example:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'First we define the class. This example prunes every other parameter, as defined
    by the code in `compute_mask()`. The `PRUNING_TYPE` is used to configure the pruning
    type as `unstructured`. Then we include and apply a function that instantiates
    the method. You would apply this pruning to your model in the following way:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: You’ve now created your own custom pruning method and can apply it locally or
    globally.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: This chapter showed you how to accelerate your training and optimize your models
    using PyTorch. The next step is to deploy your models and innovations into the
    world. In the next chapter, you’ll learn how to deploy your models to the cloud
    and to mobile and edge devices, and I’ll provide some reference code to build
    quick applications to showcase your designs.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您展示了如何使用PyTorch加速培训并优化模型。下一步是将您的模型和创新部署到世界上。在下一章中，您将学习如何将您的模型部署到云端、移动设备和边缘设备，并且我将提供一些参考代码来构建快速应用程序，展示您的设计。
