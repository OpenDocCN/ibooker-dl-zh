- en: Chapter 6\. PyTorch Acceleration and Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you learned how to use the built-in capabilities of
    PyTorch and extend those capabilities by creating your own custom components for
    deep learning. Doing so enables you to quickly design new models and algorithms
    to train them.
  prefs: []
  type: TYPE_NORMAL
- en: However, when dealing with very large datasets or more complex models, training
    your models on a single CPU or GPU can take an extremely long time—it may take
    days or even weeks to get preliminary results. Longer training times can become
    frustrating, especially when you want to conduct many experiments using different
    hyperparameter configurations.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore state-of-the-art techniques to accelerate and
    optimize your model development with PyTorch. First, we’ll look at using tensor
    processing units (TPUs) instead of GPU devices and consider instances in which
    using TPUs can improve performance. Next, I’ll show you how to use PyTorch’s built-in
    capabilities for parallel processing and distributed training. This will provide
    a quick reference for training models across multiple GPUs and multiple machines
    so you can quickly scale your training when more hardware resources are available.
    After exploring ways to accelerate training, we’ll look at how to optimize your
    models using advanced techniques like hyperparameter tuning, quantization, and
    pruning.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter will also provide reference code to make getting started easy, and
    references to the key packages and libraries we’ve used. Once you create your
    models and training loops, you can return to this chapter for tips on how to accelerate
    and optimize your training process.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by exploring how to run your models on TPUs.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch on a TPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As deep learning and AI are increasingly deployed, companies are developing
    custom hardware chips or ASICs aimed at optimizing model performance in hardware.
    Google developed its own ASIC for NN acceleration called the TPU. Since the TPU
    was designed for NNs, it does not have some of the downfalls of the GPU, which
    was designed for graphics processing. Google’s TPU is now available for you to
    use as part of Google Cloud TPU. You can also run Google Colab with a TPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapters, I showed you how to test and train your deep models
    using a GPU. You should continue to use CPUs and GPUs for training if the following
    conditions apply to your use case:'
  prefs: []
  type: TYPE_NORMAL
- en: You have small- or medium-size models with small batch sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your models do not take long to train.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving data in and out is your main bottleneck.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your calculations are frequently branching or mostly done element-wise, or you
    use sparse memory access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to use high precision. Doubles are not suitable for TPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, there are several reasons why you may want to use a TPU
    instead of a GPU for training. TPUs are very fast at performing dense vector and
    matrix computations. They are optimized for specific workloads. You should strongly
    consider using TPUs when the following apply to your use case:'
  prefs: []
  type: TYPE_NORMAL
- en: Your model is dominated by matrix computations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your model has long training times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to run multiple iterations of your entire training loop on TPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Running on a TPU is very similar to running on a CPU or a GPU. Let’s revisit
    how we would train the model on a GPU in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Configure the device to a GPU if it’s available.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Send the model to the device.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_pytorch_acceleration_and_optimization_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Send inputs and labels to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Namely, we move the model, inputs, and labels to the GPU, and the rest is done
    for us. Training your network on a TPU is almost the same as training it on a
    GPU except you will need to use the *PyTorch/XLA* (Accelerated Linear Algebra)
    package as TPUs are not currently supported natively by PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s train our model on a Cloud TPU using Google Colab. Open a new Colab notebook
    and select Change Runtime Type from the Runtime menu. Then select TPU from the
    “Hardware accelerator” drop-down menu, as shown in [Figure 6-1](#fig_colab_tpu).
    Google Colab provides a free Cloud TPU system, including a remote CPU host and
    four TPU chips with two cores each.
  prefs: []
  type: TYPE_NORMAL
- en: '![ptpr 0601](Images/ptpr_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Using a TPU in Google Colab
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since Colab does not have PyTorch/XLA installed by default we’ll need to install
    that first, using the following commands. This installs the latest “nightly” version,
    but you can select another version if needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <1>These are commands that are intended to run in a notebook. Omit the “!” to
    run them on the command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once PyTorch/XLA is installed, we can import the package and move our data
    to the TPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we do not use `torch.cuda.is_available()` here, since it only works
    for GPUs. Unfortunately, there is no `is_available()` method for TPUs. If your
    environment is not configured for TPUs, you will get an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the device is set, the rest of the code is exactly the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: If Colab is configured for TPUs, you should see `xla:1`.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch/XLA is a general library for XLA operations and may support other specialized
    ASICs in addition to TPUs. For more information on PyTorch/XLA, visit the [PyTorch/XLA
    GitHub repository](https://pytorch.tips/xla).
  prefs: []
  type: TYPE_NORMAL
- en: There are still many limitations for running on TPUs, and GPU support is more
    widespread. Therefore, most PyTorch developers will benchmark their code using
    a single GPU at first and then explore using a single TPU or multiple GPUs to
    accelerate their code.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already covered using a single GPU earlier in this book. In the next section,
    I’ll show you how to train your models on machines with multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch on Multiple GPUs (Single Machine)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When accelerating your training and development, it’s important to make the
    most of the hardware resources you have available. If you have a local machine
    or a network server with access to multiple GPUs, this section will show you how
    to fully utilize the GPUs on your system. In addition, you may want to scale your
    GPU resources by using cloud GPUs on a single instance. This is usually the first
    level of scaling before considering a distributed training approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running your code across multiple GPUs is often called *parallel processing*.
    There are two approaches to parallel processing: *data* parallel processing and
    *model* parallel processing. During data parallel processing, the data batches
    are split between multiple GPUs while each GPU runs a copy of the model. During
    model parallel processing, the model is split up between multiple GPUs and the
    data batches are pipelined into each portion.'
  prefs: []
  type: TYPE_NORMAL
- en: Data parallel processing is more commonly used in practice. Model parallel processing
    is often reserved for cases in which the model does not fit on a single GPU. I’ll
    show you how to perform both types of processing in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Data Parallel Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 6-2](#fig_data_parallel) illustrates how data parallel processing works.
    In this process, each data batch is split into *N* parts (*N* is the number of
    GPUs available on the host). *N* is typically a power of two. Each GPU holds a
    copy of the model, and the gradients and loss are computed for each portion of
    the batch. The gradients and loss are combined at the end of each iteration. This
    approach is good for larger batch sizes and use cases in which the model will
    fit on a single GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: Data parallel processing can be implemented in PyTorch using a *single-process,
    multithreaded approach* or by using a *multiprocess* approach. The single-process,
    multithreaded approach requires only one additional line of code but does not
    perform well in many cases.
  prefs: []
  type: TYPE_NORMAL
- en: '![ptpr 0602](Images/ptpr_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Data parallel processing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unfortunately, multithreading performs poorly due to Python’s Global Interpreter
    Lock (GIL) contention across threads, the per-iteration replication of the model,
    and the additional overhead introduced by scattering inputs and gathering outputs.
    You may want to try this approach because it’s so simple, but in most cases, you
    will probably use the multiprocess approach.
  prefs: []
  type: TYPE_NORMAL
- en: The multithreaded approach using nn.DataParallel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The multithreaded approach to data parallel processing is natively supported
    by PyTorch’s `nn` module. All you need to do is wrap your model in `nn.DataParallel`
    before sending it to the GPU, as shown in the following code. Here we assume you
    have already instantiated your model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: First we check to make sure we have multiple GPUs, and then we use `nn.DataParallel()`
    to set up data parallel processing before sending the model to the GPU with `to(*device*)`.
  prefs: []
  type: TYPE_NORMAL
- en: This multithreaded approach is the simplest way to run on multiple GPUs; however,
    the multiprocess approach usually performs better, even on a single machine. In
    addition, the multiprocess approach can also be used to run across multiple machines,
    as we’ll see later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The multiprocess approach using DDP (preferred)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training your models across multiple GPUs is best accomplished using a multiprocess
    approach. PyTorch supports this with its `nn.parallel.DistributedDataProcessing`
    module. Distributed data processing (DDP) can be used with multiple processes
    on a single machine or with multiple processes across multiple machines. We’ll
    start with a single machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four steps you need to do to modify your code:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a process group using *torch.distributed*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a local model using *torch.nn.to()*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrap the model with DDP using *torch.nn.parallel*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spawn processes using *torch.multiprocessing*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code demonstrates how you can convert your model for DDP training.
    We’ll break it down into steps. First, import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we’re using three new libraries—*torch.distributed*, *torch.multiprocessing*,
    and *torch.nn.parallel*. The following code shows you how to create a distributed
    training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Set up a process group with `world_size` processes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Move the model to a GPU with the ID of `rank`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_pytorch_acceleration_and_optimization_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Wrap the model in DDP.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_pytorch_acceleration_and_optimization_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Move inputs and labels to the GPU with the ID of `rank`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_pytorch_acceleration_and_optimization_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Call the DDP model for the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: DDP broadcasts the model states from the `rank0` process to all the other processes,
    so we don’t have to worry about the different processes having models with different
    initialized weights.
  prefs: []
  type: TYPE_NORMAL
- en: DDP handles the lower-level interprocess communications that allow you to treat
    the model as if it was a local model. During the backward pass, DDP automatically
    synchronizes the gradients and places the synchronized gradient tensor in `params.grad`
    when `loss.backward()` returns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the process defined, we need to create these processes using
    the `spawn()` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, we run the code as `main` to spawn two processes, each with its own GPU.
    And that’s how you run data parallel processing on multiple GPUs on a single machine.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: GPU devices cannot be shared across processes.
  prefs: []
  type: TYPE_NORMAL
- en: If your model does not fit into a single GPU or you are using smaller batch
    sizes, you may consider using model parallel processing instead of data parallel
    processing. We’ll look at that next.
  prefs: []
  type: TYPE_NORMAL
- en: Model Parallel Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 6-3](#fig_model_parallel) illustrates how model parallel processing
    works. In this process, the model is split across *N* GPUs on the same machine.
    If we process data batches in sequence, the next GPU will always be waiting for
    the previous GPU to finish, and this defeats the purpose of parallel processing.
    Therefore, we need to pipeline the data processing so that every GPU is running
    at any given moment. When we pipeline the data, only the first *N* batches are
    run in sequence, and then each subsequent run activates all the GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![ptpr 0603](Images/ptpr_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Model parallel processing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Implementing model parallel processing is not as simple as data parallel processing,
    and it requires you to rewrite your models. You’ll need to define how your models
    are split across multiple GPUs and how the data will be pipelined in the forward
    pass. This is typically done by writing a subclass for your model with a multi-GPU
    implementation for a specific number of GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates a two-GPU implementation of AlexNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`s_prev` runs on `cuda:1`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: '`s_next` runs on `cuda:0`, which can run concurrently with `s_prev`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we are deriving a subclass from the `AlexNet` class, we inherit its
    model structure, so there’s no need to create our layers. Instead, we need to
    describe which pieces of the model go on GPU0 and which pieces go on GPU1 in the
    `__init__()` constructor. Then we need to pipeline the data through each GPU in
    the `forward()` method to implement GPU pipelining. When you train your model,
    you will need to put labels on the last GPU, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Send inputs to GPU0 and labels to GPU1.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the training loop requires changing one line of code to make
    sure the labels are on the last GPU since that’s where the outputs will be before
    calculating the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Data parallel processing and model parallel processing are two effective paradigms
    for leveraging multiple GPUs for accelerated training. Wouldn’t it be great if
    we could combine the two approaches and achieve even better results? Let’s see
    how to implement the combined approach.
  prefs: []
  type: TYPE_NORMAL
- en: Combined Data Parallel Processing and Model Parallel Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can combine data parallel processing with model parallel processing to further
    improve performance. In this case, you will wrap your model using DDP to distribute
    your data batches among multiple processes. Each process will use multiple GPUs,
    and your model will be partitioned among each of those GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are only two changes we need to make:'
  prefs: []
  type: TYPE_NORMAL
- en: Change our multi-GPU model class to accept devices as inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Omit setting the output device during the forward pass. DDP will determine where
    the input and output data will be placed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code shows how to modify the multi-GPU model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the `__init__()` constructor we pass in the GPU device objects, `dev0` and
    `dev1`, and describe which parts of the model reside in which GPUs. This allows
    us to instantiate new models on different processes, each with two GPUs. The `forward()`
    method moves the data from one GPU to the next at the proper point in the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training loop changes are shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Wrap the model in `DDP`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Move the inputs and labels to the appropriate device IDs.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_pytorch_acceleration_and_optimization_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The output is on `dev1`.
  prefs: []
  type: TYPE_NORMAL
- en: To recap, you have a few options when using PyTorch across multiple GPUs. You
    can use the reference code in this section to implement data parallel, model parallel,
    or combined parallel processing to accelerate your model training and inference.
    So far, we’ve only discussed multiple GPUs on a single machine or a cloud instance.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, parallel processing across multiple GPUs on a single machine
    can reduce training times by half or more—all you need to do is upgrade your GPU
    card or utilize a larger cloud GPU instance. However, if you are training very
    complex models or using extremely large datasets, you may want to use multiple
    machines or cloud instances to speed up your training.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that DDP on multiple machines is not much different than DDP
    on a single machine. The next section shows how this is done.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Training (Multiple Machines)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If training your NN models on a single machine does not meet your needs and
    you have access to a cluster of servers, you can use PyTorch’s distributed processing
    capabilities to scale your training across multiple machines. PyTorch’s distributed
    subpackage, `torch.distributed`, provides a rich set of capabilities to suit a
    variety of training architectures and hardware platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torch.distributed` subpackage consists of three components: DDP, RPC-based
    distributed training (RPC), and collective communication (c10d). We used DDP in
    the previous section to run multiple processes on a single machine, and it’s best
    suited for the data parallel processing paradigm. RPC was created to support more
    general training architectures and can be used for distributed architectures other
    than the data parallel processing paradigm.'
  prefs: []
  type: TYPE_NORMAL
- en: The c10d component is a communications library used to transfer tensors across
    processes. c10d is used by both the DDP and RPC components as a backend, and PyTorch
    provides a c10d API so you can use it in custom distributed applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we’ll focus on using DDP for distributed training. However, if
    you have a more advanced use case, you may want to use RPC or c10d. You can find
    out more about these by reading the [PyTorch documentation](https://pytorch.tips/rpc).
  prefs: []
  type: TYPE_NORMAL
- en: For distributed training with DDP, we will follow the same DDP procedure as
    we did for a single machine with multiple processes. However, in this case, we
    will run each process on a separate machine or instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run across multiple machines, we run DDP with a launch script that specifies
    our configuration. The launch script is contained in `torch.distributed` and can
    be executed as shown in the following code. Let’s assume you have two nodes, Node
    0 and Node 1\. Node 0 is the master node and has an IP address of 192.168.1.1
    and a free port at 1234\. On Node 0, you would run the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`node_rank` is set to Node 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On Node 1, you would run this next script. Notice that this node’s rank is
    `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`node_rank` is set to Node 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’d like to explore the optional parameters in this script, run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Remember, if you are not using a DDP paradigm, you should consider using the
    RPC or c10d API for your use case. Parallel processing and distributed training
    can significantly speed up model performance and reduce development time. In the
    next section, we’ll consider other ways to improve NN performance by implementing
    techniques that optimize the model itself.
  prefs: []
  type: TYPE_NORMAL
- en: Model Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model optimization is an advanced topic that focuses on the underlying implementation
    of NN models and how they are trained. As research in this space continues to
    evolve, PyTorch has added various capabilities for model optimization. In this
    section, we’ll explore three areas of optimization—hyperparameter tuning, quantization,
    and pruning—and provide reference code for you to use in your own designs.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning model development often involves selecting many variables that
    are used to design a model and how it’s trained. These variables are called *hyperparameters*
    and can include architecture variations like the number of layers, layer depth,
    and kernel sizes, as well as optional stages like pooling or batch normalization.
    Hyperparameters may also include variations of loss functions or optimization
    parameters, such as LRs or weight decay rates.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I’ll show you how to use a package called Ray Tune to manage
    your hyperparameter optimization. Researchers often test a small set of hyperparameters
    manually. However, Ray Tune allows you to configure your hyperparameters and determines
    which settings are best for performance.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Tune supports state-of-the-art hyperparameter search algorithms and distributed
    training. It is constantly being updated with new capabilities. Let’s see how
    we can use Ray Tune to perform hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Remember the LeNet5 model we trained for image classification back in [Chapter 3](ch03.xhtml#deep_learning_development_with_pytorch)?
    Let’s experiment with different model configurations and training parameters to
    see if we can use hyperparameter tuning to improve our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use Ray Tune, we need to make the following changes to our model:'
  prefs: []
  type: TYPE_NORMAL
- en: Define our hyperparameters and their search space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a function to wrap our training loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run Ray Tune hyperparameter tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s redefine our model so that we can configure the number of nodes in the
    fully connected layers, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Configure nodes in `fc1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Configure nodes in `fc2`.
  prefs: []
  type: TYPE_NORMAL
- en: So far we have two hyperparameters, `nodes_1` and `nodes_2`. Let’s also define
    two more hyperparameters, `lr` and `batch_size`, so we can vary the learning rate
    and batch size used in our training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we import the `ray` package and define the hyperparameter
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: During each run, the values for these parameters are chosen from the specified
    search space. You can use the method `tune.sample_from()` and a `lambda` function
    to define a search space, or you can use built-in sampling functions. In this
    case, `layer_1` and `layer_2` are each set to a random value from `2` to `9` using
    `sample_from()`.
  prefs: []
  type: TYPE_NORMAL
- en: The `lr` and `batch_size` use built-in functions in which `lr` is randomly chosen
    to be a double from 1e-4 to 1e-1 with uniform distribution, and `batch_size` is
    randomly chosen to be either `2`, `4`, `8`, or `16`.
  prefs: []
  type: TYPE_NORMAL
- en: Next we need to wrap our training loop with a function that takes the configuration
    dictionary as an input. This training loop function will be called by Ray Tune.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we write our training loop, let’s define a function that loads the CIFAR-10
    data so we can reuse the data from the same directory during training. The following
    code is similar to the data-loading code we used in [Chapter 3](ch03.xhtml#deep_learning_development_with_pytorch):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can wrap our training loop into a function, *train_model()*, as shown
    in the following code. This is a large snippet of code; however, it should be
    familiar to you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Make the model layers configurable.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO10-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Make the learning rate configurable.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_pytorch_acceleration_and_optimization_CO10-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Make the batch size configurable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we want to run Ray Tune, but we first need to determine the scheduler
    and the reporter that we want to use. The scheduler determines how Ray Tune searches
    and selects the hyperparameters, while the reporter specifies how we’d like to
    view the results. Let’s set them up in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For the scheduler, we’ll use the asynchronous successive halving algorithm (ASHA)
    for hyperparameter searches and instruct it to minimize loss. For the reporter,
    we’ll configure a CLI reporter to report the loss, accuracy, training iteration,
    and selected hyperparameters on the CLI for each run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally we can run Ray Tune using the `run()` method as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We provision the resources and specify the configuration. We pass in our configuration
    dictionary, specify the number of samples or runs, and pass in our `scheduler`
    and `reporter` functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ray Tune will report the results. The `get_best_trial()` method returns an
    object that contains information about the best trial. We can print out the hyperparameter
    settings that yielded the best results, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You may find other features of the Ray Tune API useful. [Table 6-1](#table_ray_tune_schedulers)
    lists the available schedulers in `tune.schedulers`.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Ray Tune schedulers
  prefs: []
  type: TYPE_NORMAL
- en: '| Scheduling method | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ASHA | Scheduler that runs the asynchronous successive halving algorithm
    |'
  prefs: []
  type: TYPE_TB
- en: '| HyperBand | Scheduler that runs the HyperBand early stopping algorithm |'
  prefs: []
  type: TYPE_TB
- en: '| Median Stopping Rule | Scheduler based on the median stopping rule, as described
    in [“Google Vizier: A Service for Black-Box Optimization”](https://research.google.com/pubs/pub46180.html).
    |'
  prefs: []
  type: TYPE_TB
- en: '| Population Based Training | Scheduler based on the Population Based Training
    algorithm |'
  prefs: []
  type: TYPE_TB
- en: '| Population Based Training Replay | Scheduler that replays a Population Based
    Training run |'
  prefs: []
  type: TYPE_TB
- en: '| BOHB | Scheduler that uses Bayesian optimization and HyperBand |'
  prefs: []
  type: TYPE_TB
- en: '| FIFOScheduler | Simple scheduler that just runs trials in submission order
    |'
  prefs: []
  type: TYPE_TB
- en: '| TrialScheduler | Scheduler based on trials |'
  prefs: []
  type: TYPE_TB
- en: '| Shim Instantiation | Scheduler based on the provided string |'
  prefs: []
  type: TYPE_TB
- en: More information can be found in the [Ray Tune documentation.](https://pytorch.tips/ray)
    As you can see, Ray Tune has a rich set of capabilities, but there are other hyperparameter
    packages that support PyTorch as well. These include [Allegro Trains](https://pytorch.tips/allegro)
    and [Optuna](https://pytorch.tips/optuna).
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyperparameter tuning can significantly improve the performance of an NN model
    by finding the settings that work best. Next, we’ll explore another technique
    to optimize a model: quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NNs are implemented as computational graphs, and their computations often use
    32-bit (or in some cases, 64-bit) floating-point numbers. However, we can enable
    our computations to use lower-precision numbers and still achieve comparable results
    by applying quantization.
  prefs: []
  type: TYPE_NORMAL
- en: '*Quantization* refers to techniques for computing and accessing memory with
    lower-precision data. These techniques can decrease model size, reduce memory
    bandwidth, and perform faster inference due to savings in memory bandwidth and
    faster computing with int8 arithmetic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick quantization method is to reduce all computation precision by half.
    Let’s consider our LeNet5 model example again, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, all computations and memory are implemented as float32\. We can
    inspect the data types of our model’s parameters using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, our data types are float32\. However, we can reduce the model
    to half precision in one line of code using the `half()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now our computation and memory values are float16 . Using `half()` is often
    a quick and easy way to quantize your models. It’s worth a try to see if the performance
    is adequate for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in many cases, we don’t want to quantize every computation in the
    same way, and we may need to quantize beyond float16 values. For these other cases,
    PyTorch provides three additional modes of quantization: dynamic quantization,
    post-training static quantization, and quantization-aware training (QAT).'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic quantization is used when throughput is limited by compute or memory
    bandwidth for weights. This is often true for LSTM, RNN, Bidirectional Encoder
    Representations from Transformers (BERT), or Transformer networks. Static quantization
    is used when throughput is limited by memory bandwidth for activations and often
    applies for CNNs. QAT is used when accuracy requirements cannot be achieved by
    static quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s provide some reference code for each type. All types convert weights to
    int8\. They vary in handle activations and memory access.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dynamic quantization* is the easiest type. It converts the activations to
    int8 on the fly. Computations use efficient int8 values, but the activations are
    read and written to memory in floating-point format.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows you how to quantize a model with dynamic quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: All we need to do is pass in our model and specify the quantized layers and
    the quantization level.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Quantization depends on the backend being used to run quantized models. Currently,
    quantized operators are supported for CPU inference only in the following backends:
    x86 (*fbgemm*) and ARM (`qnnpack`). However, quantization-aware training occurs
    in full floating point and can run on either GPUs or CPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Post-training static quantization* can be used to further reduce latency by
    observing the distributions of different activations during training and by deciding
    how those activations should be quantized at the time of inference. This type
    of quantization allows us to pass quantized values between operations without
    converting back and forth between floats and ints in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Post-training static quantization requires configuration and training to prepare
    it before its use. We configure the backend to use x86 (`fbgemm`) and call `torch.quantization.prepare`
    to insert observers to calibrate the model and collect statistics. Then we convert
    the model to a quantized version.
  prefs: []
  type: TYPE_NORMAL
- en: '*Quantization-aware training* typically results in the best accuracy. In this
    case, all weights and activations are “fake quantized” during the forward and
    backward pass of training. Float values are rounded to the int8 equivalent, but
    the computations are still done in floating point. That is, the weight adjustments
    are made “aware” that they will be quantized during training. The following code
    shows how to quantize a model with QAT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Again we need to configure the backend and prepare the model, and then we call
    `convert()` to quantize the model.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch’s quantization capabilities are continuing to evolve, and they currently
    exist in beta. Please refer to [the PyTorch documentation](https://pytorch.tips/quantization)
    for the latest information on how to use the Quantization package.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern deep learning models can have millions of parameters and can be difficult
    to deploy. However, models are over-parameterized, and parameters can often be
    reduced without affecting the accuracy or model performance much. *Pruning* is
    a technique that reduces the number of model parameters with minimal effect on
    performance. This allows you to deploy models with less memory, lower power usage,
    and reduced hardware resources.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning model example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pruning can be applied to an `nn.module`. Since an `nn.module` may consist
    of a single layer, multiple layers, or an entire model, pruning can be applied
    to a single layer, multiple layers, or the entire model itself. Let’s consider
    our example LeNet5 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Our LeNet5 model has five submodules—`conv1`, `conv2`, `fc1`, `fc2`, and `fc3`.
    The model parameters consist of its weights and biases and can be shown using
    the `named_parameters()` method. Let’s look at the parameters of the `conv1` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Local and global pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Local pruning* is when we only prune a specific piece of our model. With this
    technique we can apply local pruning to a single layer or module. Just call your
    pruning method, passing in the layer, and set its options as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This example applies random unstructured pruning to the parameters named `weight`
    in the `conv1` layer in our model. This only prunes the weight parameters. We
    can prune the bias parameters as well with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Pruning can be applied iteratively, so you can further prune the same parameters
    using other pruning methods across different dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can prune modules and parameters differently. For example, you may want
    to prune by module or layer type and apply pruning to convolutional layers differently
    than linear layers. The following code illustrates one way to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_pytorch_acceleration_and_optimization_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Prune all 2D convolutional layers by 30%.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_pytorch_acceleration_and_optimization_CO11-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Prune all linear layers by 50%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another use of the pruning API is to apply *global pruning*, in which we apply
    a pruning method to the entire model. For example, we could prune 25% of our model’s
    parameters globally, which would probably result in different pruning rates for
    each layer. The following code illustrates one way to apply global pruning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Here we prune 25% of all the parameters in the entire model.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch provides built-in support for pruning in its `torch.nn.utils.prune`
    module. [Table 6-2](#table_pruning) lists the available functions in the pruning
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-2\. Pruning functions
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `is_pruned(*module*)` | Checks whether the module is pruned |'
  prefs: []
  type: TYPE_TB
- en: '| `remove(*module*, *name*)` | Removes the pruning reparameterization from
    a module and the pruning method from the forward hook |'
  prefs: []
  type: TYPE_TB
- en: '| `custom_from_mask(*module*, *name*, *mask*)` | Prunes the tensor corresponding
    to the parameter called `name` in `module` by applying the precomputed mask in
    `mask` |'
  prefs: []
  type: TYPE_TB
- en: '| `global_unstructured(*params*, *pruning_method*)` | Globally prunes tensors
    corresponding to all parameters in `params` by applying the specified `pruning_method`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ln_structured(*module*, *name*, *amount*, *n*, *dim*)` | Prunes the tensor
    corresponding to the parameter called `name` in `module` by removing the specified
    `amount` of (currently unpruned) channels along the specified `dim` with the lowest
    L`n`-norm |'
  prefs: []
  type: TYPE_TB
- en: '| `random_structured(*module*, *name*, *amount*, *dim*)` | Prunes the tensor
    corresponding to the parameter called `name` in `module` by removing the specified
    `amount` of (currently unpruned) channels along the specified `dim` selected at
    random |'
  prefs: []
  type: TYPE_TB
- en: '| `l1_unstructured(*module*, *name*, *amount*)` | Prunes the tensor corresponding
    to the parameter called `name` in `module` by removing the specified `amount`
    of (currently unpruned) units with the lowest L1-norm |'
  prefs: []
  type: TYPE_TB
- en: '| `random_unstructured(*module*, *name*, *amount*)` | Prunes tensor corresponding
    to the parameter called `name` in `module` by removing the specified `amount`
    of (currently unpruned) units selected at random |'
  prefs: []
  type: TYPE_TB
- en: Custom pruning methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you can’t find a pruning method that suits your needs, you can create your
    own pruning method. To do so, create a subclass from the `BasePruningMethod` class
    provided in `torch.nn.utils.prune`. In most cases, you can leave the `call()`,
    `apply_mask()`, `apply()`, `prune()`, and `remove()` methods as they are.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, you will need to write your own `__init__()` constructor and `compute_mask()`
    method to describe how your pruning method computes the mask. In addition, you’ll
    need to specify the type of pruning (`structured`, `unstructured`, or `global`).
    The following code shows an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'First we define the class. This example prunes every other parameter, as defined
    by the code in `compute_mask()`. The `PRUNING_TYPE` is used to configure the pruning
    type as `unstructured`. Then we include and apply a function that instantiates
    the method. You would apply this pruning to your model in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: You’ve now created your own custom pruning method and can apply it locally or
    globally.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter showed you how to accelerate your training and optimize your models
    using PyTorch. The next step is to deploy your models and innovations into the
    world. In the next chapter, you’ll learn how to deploy your models to the cloud
    and to mobile and edge devices, and I’ll provide some reference code to build
    quick applications to showcase your designs.
  prefs: []
  type: TYPE_NORMAL
