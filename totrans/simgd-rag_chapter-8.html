<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>chapter-8</title>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css" />
 </head>
 <body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">8</span> </span><span class="chapter-title-text">Graph, multimodal, agentic, and other RAG variants</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header"><span class="CharOverride-2">This chapter covers</span><span class="CharOverride-2"></span></h3> 
   <ul> 
    <li class="readable-text" id="p2"><span class="CharOverride-3">Introducing RAG variants</span></li> 
    <li class="readable-text" id="p3"><span class="CharOverride-3">Knowledge graph RAG</span> </li> 
    <li class="readable-text" id="p4"><span class="CharOverride-3">Multimodal RAG</span></li> 
    <li class="readable-text" id="p5"><span class="CharOverride-3">Agentic RAG</span></li> 
    <li class="readable-text" id="p6"><span class="CharOverride-3">Other RAG variants</span> </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>The first part of the book introduced retrieval-augmented generation (RAG) and the core idea behind it. The second part dealt with building and evaluating basic RAG systems. Part 3 took RAG beyond the na&iuml;ve approach and discussed advanced techniques and the technology stack that supports a RAG system. The last part of the book looks at more RAG patterns, and we conclude our discussion with a few best practices and some areas for further exploration.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>Chapter 8 looks at some popular RAG variants. These variants adapt different stages of RAG (i.e., indexing, retrieval, augmentation, and generation) to specific use case requirements. The chapter begins by discussing the emergence of these variants and the purpose they serve. We then continue talking about three important variants that have gained prominence in applied RAG. These are knowledge-graph-enhanced, multimodal, and agentic RAG. We also briefly examine other RAG variants that significantly contribute to the evolution of RAG in practical applications. We discuss the purpose and motivation behind each variant. This chapter also breaks down the workflow, features, and technical details of the variants along with their strengths and weaknesses. For simplicity, the code for these variants is not included in this chapter but can be found in the book’s code repository.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>By the end of this chapter, you should </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p10">Be familiar with the idea and motivation behind RAG variants. </li> 
   <li class="readable-text" id="p11">Have an in-depth understanding of graph, multimodal, and agentic RAG.</li> 
   <li class="readable-text" id="p12">Be aware of several popular RAG variants and the use cases they solve.</li> 
  </ul> 
  <div class="readable-text" id="p13"> 
   <p>There are several limitations of a na&iuml;ve approach to RAG that affect the overall usability of a standard RAG system. These limitations range from difficulties in understanding relationships across different documents to challenges in handling various data types, as well as concerns regarding system cost and efficiency. Chapter 6 discussed several pre-retrieval, retrieval, and post-retrieval techniques, such as index optimization, query optimization, hybrid and iterative retrieval strategies, compression, and re-ranking, which address different limitations and improve the accuracy of a RAG system. Several RAG patterns that incorporate one or more of these techniques have emerged over time to solve specific use challenges. We refer to them as RAG variants.</p> 
  </div> 
  <div class="readable-text" id="p14"> 
   <h2 class=" readable-text-h2"><span class="num-string">8.1</span> What are RAG variants, and why do we need them?</h2> 
  </div> 
  <div class="readable-text" id="p15"> 
   <p>The universe of applications that rely on RAG is expanding every day. Some of these applications process not just text, but different data modalities such as image, video, and audio as well. Others are being applied in domains such as healthcare and finance, where the effects of inaccurate results are catastrophic. The emerging domain of using LLMs as decision-making agents has also enabled a more adaptive and intelligent RAG system. Apart from factual accuracy, practical RAG applications demand low latency and low costs to enhance user experience and adoption. As the range of applications for RAG has expanded, so need specialized variations of RAG—known as RAG variants—designed to address unique challenges across different tasks and data types. </p> 
  </div> 
  <div class="readable-text intended-text" id="p16"> 
   <p>These RAG variants are adaptations of the standard RAG framework that extend its functionality to meet demands of diverse and complex use cases. By employing advanced pre-retrieval, retrieval and post-retrieval techniques, these variants enhance RAG with capabilities such as handling multimodal data, providing higher accuracy, and better relational understanding. The evolution of these RAG variants makes the system both flexible and domain aware.</p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>While several RAG variants have emerged, the three that we are going to discuss in-depth in the subsequent sections have gained prominence: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p18"><em>Multimodal RA</em><em>G</em>—Extends capabilities of the standard RAG beyond text data and incorporates other data types such as images, video, and audio. This characteristic enables the system to fetch information from nontextual documents and provide additional context. </li> 
   <li class="readable-text" id="p19"><em>Knowledge graph RA</em><em>G</em>—Integrates knowledge graphs into the retrieval process. This idea was introduced in chapter 6 as part of improving the indexing structure. Knowledge graphs help establish relationships between entities, providing better context, especially in multi-hop queries.</li> 
   <li class="readable-text" id="p20"><em>Agentic RA</em><em>G</em>—Incorporates LLM agents into the RAG framework. These agents enable autonomous decision making across the RAG value chain from indexing to generation. Simultaneously, all components become adaptive to the user query.</li> 
  </ul> 
  <div class="readable-text" id="p21"> 
   <p>In addition to these three, we also touch upon additional variants, such as corrective RAG, self-RAG, and more, but first, we begin by discussing multimodality.</p> 
  </div> 
  <div class="readable-text" id="p22"> 
   <h2 class=" readable-text-h2"><span class="num-string">8.2</span> Multimodal RAG</h2> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>Until now, we have seen that standard RAG systems are effective in managing and retrieving textual data to generate context-aware and grounded responses. However, the scope of enterprise data extends beyond text to image, audio, and video. Standard RAG systems fall short when attempting to interpret nontextual data formats. This is the core motivation behind a multimodal variant of RAG, which extends the capabilities to more data formats. </p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.2.1</span> Data modality</h3> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>Multimodality can be a confusing term for the uninitiated, especially because “modality” varies in meaning across different fields. Grammatical modality relates to the expression of the speaker’s attitude, while treatment modality may refer to the medical approach in medicine. In RAG, and AI in general, modality refers to data format. Text is a modality, image is a modality, video and audio are different modalities, and we can also consider tables and code as distinct modalities. Figure 8.1 shows some data modalities, including less common ones such as genomic and 3D data.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p26">  
   <img src="../Images/CH08_F01_Kimothi.png" alt="A group of icons with text

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.1</span><span class=""> </span><span class="">Examples of different data modalities</span></h5>
  </div> 
  <div class="readable-text" id="p27"> 
   <p>Multimodal RAG is, therefore, the extended variant of standard RAG with the capability to process multiple data modalities. Before diving into the requirements and architectural details of multimodal RAG, let’s ponder over the use cases where multimodal RAG is necessary. </p> 
  </div> 
  <div class="readable-text" id="p28"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.2.2</span> Multimodal RAG use cases</h3> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>There are several industries and functions where a multimodal variant of RAG is required, such as </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p30"><em>Medical diagnosi</em><em>s</em>—A diagnostic assistant can work with patient records that may include medical history (in text form), lab results (in tabular form), and diagnostic images (like X-rays, MRIs, etc.), along with studies and research papers that include graphs, charts, or microscopic images. When the patient comes in for a consultation, this assistant can provide a holistic analysis to the doctor.</li> 
   <li class="readable-text" id="p31"><em>Investment analysi</em><em>s</em>—Working with financial reports and other filings that have charts showing trends, earnings, and projections along with balance sheets and income statements in tabular form, apart from the usual text commentary, an investment research assistant can provide analysts with crucial information needed to make investment decisions.</li> 
   <li class="readable-text" id="p32"><em>Buying assistanc</em><em>e</em>—Through an analysis of product images, textual descriptions, product specifications (in tabular form), and customer reviews, a shopping assistant can help the shoppers on an e-commerce website with personalized recommendations.</li> 
   <li class="readable-text" id="p33"><em>Coding assistanc</em><em>e</em>—Coding assistants retrieve relevant documentation, function usage examples, and code snippets from repositories based on the query context. For example, when a developer asks how to implement a certain API function. The RAG system retrieves precise code snippets and explanations from the documentation, helping the developer avoid time-consuming searches.</li> 
   <li class="readable-text" id="p34"><em>Equipment maintenanc</em><em>e</em>—Using historical text reports with visual inspection images or video feed, sensor data, and performance tables, a maintenance assistant can provide maintenance recommendations and trends.</li> 
  </ul> 
  <div class="readable-text" id="p35"> 
   <p>These are just a few examples. While standard text-only RAG finds acceptability in the initial stages of a use case, a large proportion of production-grade RAG systems incorporate at least one other modality of data.</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.2.3</span> Multimodal RAG pipelines</h3> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>Let’s now explore how developing a multimodal RAG pipeline differs from a standard text-only RAG pipeline you have learned so far. An obvious change will be in loading and indexing the data of nontext modalities. </p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <h4 class=" readable-text-h4">Multimodal indexing pipeline</h4> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>Developing the knowledge base for multimodal RAG requires enhancement in each of the four components of the indexing pipelines. Apart from loading and chunking files of different modalities, creating embeddings for multimodal data requires special attention. Let’s look at each of the components one by one. </p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p><em>The data-loading step</em> is quite like the standard text-only RAG but now includes connectors and data loaders for nontext modalities. There are several options available. <code>Pillow</code>, also known as <code>PIL</code>, is a popular Python library for loading images. <code>Unstructured</code> is an open source library that includes components for ingesting a variety of data formats. <code>Pydub</code> is another Python library that allows the loading of audio files such as WAV and MP3. LangChain provides an integration with the unstructured library. <code>UnstructuredImageLoader</code> is a class available in LangChain document loaders for loading images. For audio and video transcription, libraries such as <code>OpenAIWhisperParser</code>, <code>AssemblyAIAudioTranscriptLoader</code>, and <code>YoutubeLoader</code> can be used. Likewise, for tabular data <code>CSVLoader</code> and <code>DataFrameLoader</code> come in handy. For simplicity, sometimes data of different modalities is transcribed into text.  </p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p><em>Chunking</em> for multimodal data largely follows a process similar to text chunking in cases where audio/video data is transcribed and stored as text. However, for raw audio and video data, specific chunking methods can be employed. Voice activity detection (VAD) chunks the data based on silences or background noise in the audio. Scene-detection-based chunking identifies major changes in the scene to segment the video. For tabular data, sometimes row/column-level chunking can be incorporated, and for code, the chunking can be carried out at a function, a class, or a logical unit level. All strategies used for chunking text data such as context enrichment, semantic chunking, and similar are also held here. For images, chunking is generally not done. <code>semantic_chunkers</code> is a multimodal chunking library for intelligent chunking of text, video, and audio. It makes AI and data processing more efficient and accurate. </p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p><em>Embeddings </em>is where nuance begins in multimodal RAG. In standard text-only RAG, there are several embeddings models available to vectorize the chunks. But how does one vectorize data of different modalities, such as an image? There are three approaches to deal with this complexity: shared or joint embedding models, modality-specific embeddings, and conversion of all non-text data into text. </p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>Shared or joint embeddings models map diverse data types into a unified embeddings space. By doing this, cross-modal retrieval is enabled, such as finding images based on textual descriptions or generating text from images. Google Vertex AI offers shared embeddings models that generate vectors for all data modalities in the unified embeddings space. Shared embeddings models are also called multimodal embeddings models. While efficient at understanding general image data, multimodal embeddings sometimes fall short when granular understanding is needed, as in charts and tables represented as images and infographics. In figure 8.2, image, text, audio, and video data are plotted in the same 3D vector space.</p> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>The modality-specific embeddings approach resemble multimodal embeddings, except that instead of a single embeddings space for all modalities, the embeddings space maps only two modalities. In such a scenario, we need an image–text embeddings model to process text, image, and audio data (e.g., Contrastive Language–Image Pretraining, or CLIP) and an audio-text embeddings model (e.g., Contrastive Language–Audio Pretraining, or CLAP). The knowledge base has text, image, and audio embeddings in different embeddings spaces and stored separately. Figure 8.3 is an example of CLIP image–text embeddings where image and text embeddings are projected onto a shared embeddings space.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p45">  
   <img src="../Images/CH08_F02_Kimothi.png" alt="A diagram of a dog

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5">Figure 8.2 Images, text, video, and audio are plotted on the same embeddings space. Dog, bark, and dog’s image are close to each other.</h5>
  </div> 
  <div class="browsable-container figure-container " id="p46">  
   <img src="../Images/CH08_F03_Kimothi.png" alt="A diagram of a number of letters

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.3</span><span class=""> </span><span class="">CLIP uses multimodal pre-training to convert classification into a retrieval task, which enables pre-trained models to tackle zero-shot recognition.</span></h5>
  </div> 
  <div class="readable-text" id="p47"> 
   <p>Conversion of all non-text data into text is employed to first convert all nontext (image) data into text using a multimodal LLM and then follow the standard text-only RAG approach. (A multimodal LLM is a large language model that processes all modalities of data. You will read more about multimodal LLMs later in this section.) In this strategy, you may notice that we may not be entirely using multimodal data as information loss is bound to occur when converting nontext to text data. In a variation of this strategy, instead of converting all multimodal data into text and using it as text, a two-pronged approach is employed. Here all multimodal data is summarized in text using a multimodal LLM. Embeddings of this text are used to search for during the retrieval process. However, for generation, not only the summary but the actual multimodal file (e.g., a .jpeg) is retrieved and passed to the multimodal LLM for generation. This reduces the loss of information when converting to text. </p> 
  </div> 
  <div class="readable-text intended-text" id="p48"> 
   <p>Embeddings, either multimodal or text, are <em>stored</em> in vector databases such as standard text-only RAG. In addition to vector storage, document storage is required to store raw files that can be retrieved and passed to the LLM for generation. Document stores such as Redis can be used to store raw files. When text summaries are used, a key mapping of the summary embeddings to the raw documents must be created. Figure 8.4 shows the indexing pipeline with all three options for embeddings.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p49">  
   <img src="../Images/CH08_F04_Kimothi.png" alt="A diagram of a diagram

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.4</span><span class=""> </span><span class="">Multimodal indexing pipeline presenting three options</span></h5>
  </div> 
  <div class="readable-text" id="p50"> 
   <p>While the loading, chunking, and storage components are similar, the embedding component presents several options in multimodal RAG. Table 8.1 compares the indexing pipelines of text-only RAG and multimodal RAG.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p51"> 
   <h5 class=" browsable-container-h5">Table 8.1 Indexing pipelines of text-only vs. multimodal RAG</h5> 
   <table id="table001" class="No-Table-Style _idGenTablePara-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-2" /> 
     <col class="_idGenTableRowColumn-3" /> 
    </colgroup> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-1"> <p class="_TableHead">Indexing component</p> </td> 
      <td class="No-Table-Style CellOverride-2"> <p class="_TableHead">Text-only RAG</p> </td> 
      <td class="No-Table-Style CellOverride-3"> <p class="_TableHead">Multimodal RAG</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-5"> 
      <td class="No-Table-Style CellOverride-4"> <p class="_TableBody">Loading</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Standard text data loaders are used to load documents, such as plain text files, PDFs, and other text-based formats.</p> </td> 
      <td class="No-Table-Style CellOverride-6"> <p class="_TableBody">Requires connectors for additional data types. For images, libraries such as <code>Pillow</code> (<code>PIL</code>) and <code>Unstructured&shy;ImageLoader</code> in LangChain are used; for audio, we use libraries such as <code>Pydub</code> or <code>OpenAIWhisperParser</code>, whereas <code>CSVLoader</code> and <code>DataFrameLoader</code> are used for tabular data. Audio and video transcription tools such as AssemblyAI and YoutubeLoader are also incorporated to preprocess audio/video content.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-6"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Chunking</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Text data is divided into segments (chunks) based on context or structure (e.g., sentences, paragraphs) and optionally enriched semantically.</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody ParaOverride-1">Follows text chunking when data is transcribed to text (audio/video). For raw audio, voice activity detection (VAD) can be used to chunk by pauses. For videos, scene detection identifies visual transitions, and tabular data can be chunked row/column-wise. Image chunking is typically skipped.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-7"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Embeddings</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Text embeddings are created using a single-modality text embeddings model (e.g., OpenAI embeddings or BERT), which vectorizes each chunk for storage and retrieval.</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Embeddings can be generated via multimodal embeddings models, which unify all data types in a shared vector space for cross-modal retrieval, modality-specific embeddings such as CLIP and CLAP or converting multimodal data to text first and use text embeddings, although this may cause information loss.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-8"> 
      <td class="No-Table-Style CellOverride-10"> <p class="_TableBody">Storage</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Embeddings are stored in vector databases.</p> </td> 
      <td class="No-Table-Style CellOverride-12"> <p class="_TableBody">Embeddings are stored in vector databases, but additional document storage for raw multimodal files may be used.</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p52"> 
   <p>Once the knowledge base is created, such as in text-only RAG, the generation pipeline is responsible for real-time interaction with the knowledge base. Depending on the embedding strategy used, the generation pipeline components adapt to incorporate multimodal data.</p> 
  </div> 
  <div class="readable-text" id="p53"> 
   <h4 class=" readable-text-h4">Multimodal generation pipeline </h4> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>Once the knowledge base is created by the indexing pipeline, the generation pipeline needs to search, retrieve, process, and generate multimodal data. This requires variations in retrieval approach and a multimodal LLM:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p55"><em>Retrieva</em><em>l</em>—Depending on the embeddings strategy, the retrieval technique varies: 
    <ul> 
     <li>In case a shared multimodal embeddings model is used, the retrieval process follows a similarity search approach, where the user query is converted into a vector form using the same multimodal embeddings, and the documents are retrieved based on their cosine similarity value irrespective of their modality.</li> 
     <li>In the modality-specific embedding approach, because multiple embeddings are present, a multi-vector retrieval approach is employed. For a single query, documents are retrieved from each modality-specific embeddings space based on similarity. These documents may later be re-ranked before augmentation and generation. </li> 
     <li>When nontext data is converted into text, the retrieval process is the same as the standard text-only RAG. In the variation where both text summaries and raw files are used, the retriever first retrieves the relevant summaries from the text embeddings space, and then the files from the document stores mapped to those summaries are also retrieved.</li> 
    </ul> </li> 
   <li class="readable-text" id="p56"><em>Augmentatio</em><em>n</em>—The augmentation step remains the same as text-only RAG, except that the augmented prompt now includes the raw multimodal file accompanying the text prompt.</li> 
   <li class="readable-text" id="p57"><em>Generatio</em><em>n</em>—Like multimodal embeddings, for processing and generating multimodal data, multimodal LLMs are used. LLMs are limited by their ability to process text data only. Multimodal LLMs are transformers-based models, too, but have been trained on data of all modalities, in addition to text data. There are nuanced differences in the training process of multimodal LLMs, and the readers are encouraged to explore them. However, for building RAG systems, we can use the available foundation multimodal LLMs. OpenAI’s GPT 4o and GPT 4o mini and Google’s Gemini are popular proprietary multimodal LLMs, while Meta’s Llama 3.2 and Mistral AI’s Pixtral are open source multimodal LLMs. </li> 
  </ul> 
  <div class="readable-text" id="p58"> 
   <p>While the augmentation step remains similar to text-only RAG, the retrieval step adapts based on the embeddings strategy used, and the generation step swaps the LLMs with multimodal LLMs. The differences in the generation pipelines are highlighted in <br />table 8.2. </p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p59"> 
   <h5 class=" browsable-container-h5">Table 8.2 Indexing pipelines of text-only vs. multimodal RAG</h5> 
   <table id="table002" class="No-Table-Style _idGenTablePara-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-9" /> 
     <col class="_idGenTableRowColumn-10" /> 
     <col class="_idGenTableRowColumn-11" /> 
    </colgroup> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-1"> <p class="_TableHead">Generation component</p> </td> 
      <td class="No-Table-Style CellOverride-2"> <p class="_TableHead">Text-only RAG</p> </td> 
      <td class="No-Table-Style CellOverride-3"> <p class="_TableHead">Multimodal RAG</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-12"> 
      <td class="No-Table-Style CellOverride-4"> <p class="_TableBody">Retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Retrieves similar text embeddings to the query using similarity search</p> </td> 
      <td class="No-Table-Style CellOverride-6"> <p class="_TableBody">Varies by embedding strategy—in shared embeddings model, a similarity search is employed regardless of modality, converting the query into a multimodal vector. In modality-specific embeddings, multi-vector retrieval is used for modality-specific results, and in text-converted nontext data, a standard text retrieval along with raw files mapped to text summaries is used.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-13"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Augmentation</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Adds retrieved text to the prompt</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Similar to text-only but includes the raw multimodal files alongside the text in the prompt.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-14"> 
      <td class="No-Table-Style CellOverride-10"> <p class="_TableBody">Generation</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Uses LLMs to generate responses</p> </td> 
      <td class="No-Table-Style CellOverride-12"> <p class="_TableBody">Uses multimodal LLMs instead of text-only LLMs.</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>By tweaking the indexing and generation pipelines, a standard text-only RAG system can be upgraded to a multimodal RAG system, as illustrated in figure 8.5.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p61">  
   <img src="../Images/CH08_F05_Kimothi.png" alt="A diagram of a multilevel system

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.5</span><span class=""> </span><span class="">For each of the three approaches, the generation pipeline also adapts.</span></h5>
  </div> 
  <div class="readable-text" id="p62"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.2.4</span> Challenges and best practices</h3> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>Multimodal RAG systems are gaining prominence owing to the diversity present in enterprise data. However, one must note that with multimodality, the complexity of the system increases along with higher latency and more expenditure on multimodal embeddings and generation. Some of the common challenges associated with multimodal RAG are </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p64">Ensuring coherent alignment between different data modalities (e.g., text and images) can be difficult. Utilizing multimodal embeddings projecting different modalities into a common embedding space does create better integration, but these embeddings models can still lead to inaccuracies and must be evaluated.</li> 
   <li class="readable-text" id="p65">Handling multiple data types may increase computational requirements and processing time. Robust preprocessing pipelines to standardize and align data from various modalities are essential. Sometimes, converting multimodal data to text and following a text-only RAG approach may be enough to generate the desired results.</li> 
   <li class="readable-text" id="p66">Not all models are capable of effectively processing and integrating multimodal data of all modalities. Incorporate only those that add significant value to the task to optimize performance and resource utilization. </li> 
  </ul> 
  <div class="readable-text" id="p67"> 
   <p>We have looked at a RAG variant that extends the capability of RAG to different data modalities. However, standard RAG is still deficient when the information is dispersed across different documents. Let’s now look at a pattern in which knowledge graphs are used to establish higher-order relationships.</p> 
  </div> 
  <div class="readable-text" id="p68"> 
   <h2 class=" readable-text-h2"><span class="num-string">8.3</span> Knowledge graph RAG</h2> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>Imagine summarizing a large report or answering complex questions that draw information from diverse sources. For example, a question such as, “What are the main themes in this report?” or “Which products in the catalogue are endorsed by the same celebrities?” are questions that are difficult for standard RAG systems to answer. </p> 
  </div> 
  <div class="readable-text intended-text" id="p70"> 
   <p>In a summarization task such as the “main themes” in a report, there is no chunk of the document that can answer the question completely. Likewise, “endorsed by the same celebrities” is not likely to be present in the data for the retriever to search through. </p> 
  </div> 
  <div class="readable-text intended-text" id="p71"> 
   <p>To answer these kinds of complex questions requiring multi-hop reasoning, identifying contextual relationships, and addressing higher-order queries, a powerful RAG pattern that incorporates knowledge graphs has been widely successful. </p> 
  </div> 
  <div class="readable-text intended-text" id="p72"> 
   <p>This pattern is called <span class="IndexSee">knowledge graph RAG</span> or simply <em>graph RAG</em> (not to be confused with Microsoft’s GraphRAG, which is a specific framework of knowledge graph RAG). It must be noted here that graph RAG is not necessarily a replacement for standard vector-based RAG, but a hybrid approach in which both vectors and graphs are used to retrieve context. Before moving forward, The following sections explain what knowledge graphs are and what benefits are inherent to them.</p> 
  </div> 
  <div class="readable-text" id="p73"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.3.1</span> Knowledge graphs</h3> 
  </div> 
  <div class="readable-text" id="p74"> 
   <p>The term <em>knowledge graph</em> was popularized by Google somewhere around 2012 by integrating an entity-relationship structure into its search engine to deliver more accurate and context-aware results. The simplest way to understand knowledge graphs is through the node-and-edge structure. Nodes may represent entities such as people, organizations, products, and events, and edges represent relationships between the nodes, such as <em>is a part of</em>, <em>works at</em>, <em>is related to</em>, and so on. The nodes and edges can also have attributes such as id, timestamp, and similar. Knowledge graphs, therefore, rely on semantics or meaning to create a shared, human-like, understanding of data. Figure 8.6 illustrates a simple knowledge graph with nodes, edges, and attributes for customer data.</p> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>Knowledge graphs offer several advantages over standard structured databases such as SQL by prioritizing relationships and context, which results in deeper data exploration. A standard row–column or a document storage does not allow for context a knowledge graph does. </p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <p>The storage and data processing in knowledge graphs is unique. Specialized databases such as Neo4j, Amazon Neptune, and TigerGraph are used to store knowledge graph</p> 
  </div> 
  <div class="browsable-container figure-container " id="p77">  
   <img src="../Images/CH08_F06_Kimothi.png" alt="A diagram of a product

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.6</span><span class=""> </span><span class="">Knowledge graph representation of customer activity where nodes (circles) represent entities, edges (arrows) represent relationships, and attributes (rectangles) are the properties.</span></h5>
  </div> 
  <div class="readable-text" id="p78"> 
   <p>data, and query languages such as Cypher, Gremlin, and SparkQL are used for graph traversal. Readers are encouraged to learn more about graph databases, but some key concepts to keep in mind are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p79"><em>Nodes and edge</em><em>s</em><em>—</em>Nodes represent entities, and edges represent relationships to form the graph structure and enable a visual structure to the knowledge.</li> 
   <li class="readable-text" id="p80"><em>Attribute</em><em>s</em>—Attributes are properties of entities(nodes) and relationships(edges).</li> 
   <li class="readable-text" id="p81"><em>Triplet</em><em>s</em>—Knowledge is represented in triplets such as “customer A purchased product X” (node–edge–node). Here the two entities, “customer A” and “product X,” and one relationship, “purchased,” form a triplet. These triples are the building blocks of knowledge graphs, capturing facts and relationships in a structured way. </li> 
   <li class="readable-text" id="p82"><em>Ontolog</em><em>y</em>—An ontology defines the schema or structure of a knowledge graph, specifying the types of entities, relationships, and their properties.</li> 
   <li class="readable-text" id="p83"><em>Graph embedding</em><em>s</em>—Graph embeddings are vector representations of nodes and edges that capture graph structure.</li> 
   <li class="readable-text" id="p84"><em>Graph query languag</em><em>e</em>—SPARQL, Cypher, and similar languages allow users to retrieve information from the graph, formulating complex queries to find patterns, connections, and insights.</li> 
   <li class="readable-text" id="p85"><em>Graph traversa</em><em>l</em>—This is the method of navigating through nodes and edges to discover paths, patterns, and insights, essential for algorithms such as shortest path or recommendation systems. </li> 
  </ul> 
  <div class="readable-text" id="p86"> 
   <p>Because of their inherent focus on relationships and context, knowledge graphs enhance standard RAG for a superior context-aware retrieval. </p> 
  </div> 
  <div class="readable-text" id="p87"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.3.2</span> Knowledge graph RAG use cases</h3> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>Knowledge graphs can be useful in a variety of use cases where the ability to handle multi-hop relationships, entity disambiguation, and complex networks is required. Standard RAG systems are limited to retrieving isolated information chunks, while knowledge graph RAG can dynamically connect and analyze data points within a network, making it ideal for applications requiring a deep understanding of interrelated data. Here are some examples: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p89"><em>Personalized treatment plan</em><em>s</em>—Knowledge graph RAG can link drugs, treatments, and conditions in a networked format, which allows it to identify potential interactions and customize treatment recommendations based on multiple factors. Standard RAG can retrieve information about a specific drug or treatment but struggles to cross-reference interactions across a network of symptoms, conditions, and treatments.</li> 
   <li class="readable-text" id="p90"><em>Personalized product recommendation</em><em>s</em>—Standard RAG can retrieve individual touchpoints or customer reviews but fails to capture the interconnected path a customer follows across their journey. Knowledge graph RAG allows for multi-hop reasoning across transactions, browsing history, and customer feedback, enabling a more holistic analysis of the journey and providing highly relevant recommendations based on relationships between customer behaviors and preferences.</li> 
   <li class="readable-text" id="p91"><em>Contract analysi</em><em>s</em>—Standard RAG can retrieve text from individual contracts or clauses but cannot map relationships among contracts, parties, or compliance requirements. Knowledge graph RAG can link contracts, clauses, and parties in a relational network, enabling it to identify conflicts, dependencies, and compliance risks across interconnected legal documents.</li> 
  </ul> 
  <div class="readable-text" id="p92"> 
   <p>While standard RAG can solve simple queries, for processes that require analysis and reasoning on data from multiple sources, knowledge graph can prove to be advantageous.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.3.3</span> Graph RAG approaches</h3> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>Knowledge graph is a powerful data pattern. The approach to using knowledge graphs can be determined by the complexity of the use case and the diversity of data. This section discusses three common approaches that can be followed. </p> 
  </div> 
  <div class="readable-text" id="p95"> 
   <h4 class=" readable-text-h4">Structure awareness through graphs</h4> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>This is the simplest approach to incorporating knowledge graphs. Recall that in the standard vector-based RAG approach, documents are chunked, and embeddings are created then and stored for retrieval. The problem that may arise is that the information in the adjacent chunks might not be retrieved, and a certain degree of context loss may happen. In section 6.2.1, we discussed a hierarchical indexing structure such as a parent–child structure. The parent document contains overarching themes or summaries, while child documents delve into specific details. During retrieval, the system can first locate the most relevant child documents and then refer to the parent documents for additional context if required. This approach enhances the precision of retrieval, while maintaining the broader context. </p> 
  </div> 
  <div class="readable-text intended-text" id="p97"> 
   <p>An efficient way to store documents in a hierarchical structure is in graphs. Parent and child documents can be stored in the nodes with a relationship “is child of.” More levels of hierarchies can be created. In figure 8.7, there are three levels of indexing hierarchy, and while the search happens at the lowest level, parent documents at a higher hierarchy level are retrieved for deeper context.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p98">  
   <img src="../Images/CH08_F07_Kimothi.png" alt="A screenshot of a diagram

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.7</span><span class=""> </span><span class="">While search in a hierarchical index structure happens at the lowest level, retrieved documents are more contextually complete from a higher level of hierarchy.</span></h5>
  </div> 
  <div class="readable-text" id="p99"> 
   <h4 class=" readable-text-h4">Graph-enhanced vector search</h4> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>Graphs are not mandatory when implementing hierarchical indexing. The true value of knowledge graphs is realized when connections can be made across chunks. Standard vector-based search on a collection of chunks can be enhanced by traversing a knowledge graph to retrieve related chunks. To do this, a set of entities and relationships are extracted from the chunks using an LLM. </p> 
  </div> 
  <div class="readable-text intended-text" id="p101"> 
   <p>In the retrieval stage, the first step is a usual vector search executed based on the user query. An initial set of chunks is identified that has a high similarity with the user query. In the next step, the knowledge graph is traversed to fetch-related entities around the entities of the chunks identified in the first step. By doing this, the retriever fetches not only the chunks similar to the user query but also related chunks, which leads to deeper context and can be quite effective in solving multi-hop queries. This is often coupled with hierarchical structures and a re-ranking of retrieved documents. Figure 8.8 shows an enhanced knowledge graph, where chuwnks also have the extracted entities and relationships. During retrieval, in addition to similar chunks, the parent chunks of related entities are also retrieved.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p102">  
   <img src="../Images/CH08_F08_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.8</span><span class=""> </span><span class="">Entities and relationships extracted from the chunks play a crucial role. When chunks similar to the user query are retrieved, the chunks that have entities related to the entities of similar chunks are also retrieved.</span></h5>
  </div> 
  <div class="readable-text" id="p103"> 
   <h4 class=" readable-text-h4">Graph communities and community summaries</h4> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>As discussed before, knowledge graphs are about entities and their relationships. Depending on the process, there may be patterns in which certain entities interact more with each other. Graph communities are a subset of entities connected more densely. For example, communities of customers with similar demographics and buying patterns can be identified or clusters of product features that appear together can be discovered. Community detection algorithms such as the Leiden and the Louvain algorithm are employed to detect communities within a knowledge graph. After detecting these communities, an LLM is used to generate summaries of the entities and the relationship information in the community. The retrieval process can be similar to vector search, where initial nodes are identified using a similarity score and community summaries related to the nodes are fetched, or vector search can be employed directly on the community summaries since they already contain a deeper context of several entities. This approach is particularly useful when queries relate to the broader themes within the knowledge base. Figure 8.9 shows how the retrieval at a community level is sufficient to answer questions at a broader thematic level.</p> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>In any of these approaches, both the indexing and the retrieval pipeline need to be modified to incorporate the graph and create a hybrid retrieval system where both vector databases and graph databases exist.</p> 
  </div> 
  <div class="readable-text" id="p106"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.3.4</span> Graph RAG pipelines</h3> 
  </div> 
  <div class="readable-text" id="p107"> 
   <p>As we have been discussing, knowledge graph is a unique data pattern that requires specific processing and storage. RAG pipelines need to be customized to incorporate knowledge graphs. Depending on the approach used, both the indexing and the generation pipelines need tweaking.</p> 
  </div> 
  <div class="readable-text" id="p108"> 
   <h4 class=" readable-text-h4">Knowledge graph RAG indexing pipeline</h4> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>The knowledge base in graph RAG requires a different kind of parsing and storage. New components are introduced in the indexing pipeline to create knowledge graphs, extract summaries, and store the data for generation. While the loading and chunking components remain similar, the remaining components change significantly:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p110"><em>Data loadin</em><em>g</em>—There is no difference in the loading of the documents from the standard vector-based RAG. </li> 
   <li class="readable-text" id="p111"><em>Data chunkin</em><em>g</em>—To create knowledge graphs from the documents, large documents are chunked in the same way as the vector RAG approach. These chunks are then passed to an LLM to extract entities and their relationships. </li> 
   <li class="readable-text" id="p112"><em>Entity relationship attribute extraction (for graph-enhanced RAG</em><em>)</em>—This is a crucial step in graph enhancement because the quality of responses will depend on how well the entities and relationships have been identified. This step can be customized </li> 
  </ul> 
  <div class="browsable-container figure-container " id="p113">  
   <img src="../Images/CH08_F09_Kimothi.png" alt="A diagram of a community

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.9</span><span class=""> </span><span class="">Communities club entities under a consistent theme and summarize the information at this group level. Since the summaries are created from a high number of thematically related chunks, these summaries can answer broad queries.</span></h5>
  </div> 
  <ul> 
   <li class="readable-text" style="list-style: none;" id="p114">according to the need and complexity of the use case. The simplest approach can be to ask an LLM directly to do the extraction. The exact kind of entities and relationships can also be predetermined, say, allowed entities are “people,” “country,” and “organization,” and allowed relationships are “nationality,” “located at,” and “works at.” There can be another approach in which an LLM is used to identify the schema of the knowledge graph. Attributes can also be added to the entities and relationships. There can be multiple passes of this step to ensure that an exhaustive list has been created. Another step can be employed to remove redundancies and duplication. In LangChain, <code>LLMGraphTransformer</code> class is available in the <code>langchain_experimental</code> library that abstracts the entity relationship extraction from documents. </li> 
   <li class="readable-text" id="p115"><em>Storag</em><em>e</em>—Once the entities, relationships, and attributes have been extracted, these can be stored in a graph database such as Neo4j. LangChain has integration with the Neo4j graph database, and the <code>Neo4jGraph</code> library from the <code>langchain_community</code> can be used. Since the entity relationship extraction is done at a chunk level, the storage is also iterative, and the graph database is updated after each pass. In LangChain, the <code>add_graph_documents()</code> function of the <code>Neo4jGraph</code> library can be used to directly update the knowledge graph. </li> 
   <li class="readable-text" id="p116"><em>Creating community summarie</em><em>s</em>—As discussed previously, once the knowledge graph is created, an algorithm is used to detect communities, and an LLM is used to create a summary of the community. <code>Graphrag</code>, a library developed by Microsoft, provides end-to-end knowledge graph and community summary creation from documents. Another approach is to just use the community summaries and store the summaries in a vector database and use the standard vector RAG on the community summaries.</li> 
  </ul> 
  <div class="readable-text" id="p117"> 
   <p>This graph database can be used as the complete knowledge base or be treated as an addition to the regular vector database in the knowledge base. Figure 8.10 illustrates the indexing pipeline with each step.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p118">  
   <img src="../Images/CH08_F10_Kimothi.png" alt="A diagram of a graph

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.10</span><span class=""> </span><span class="">Indexing pipeline for graph RAG. Chunks can directly be stored for simple structure-aware indexing, and community summaries can be created and stored with the graph.</span></h5>
  </div> 
  <div class="readable-text" id="p119"> 
   <h4 class=" readable-text-h4">Generation pipeline</h4> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>Since the nature of the knowledge base in graph RAG is quite unlike standard RAG, it requires significant changes in the generation pipeline. The retrieval process becomes slightly more nuanced than vector retrieval because of an additional step of graph traversal. Graph databases such as Neo4j have introduced vector indexes, via the Neo4j vector search plugin, which represent nodes and attributes as embeddings and enable similarity search. For effective retrieval, the user query (in natural language) is converted into a graph query that can be used to traverse the knowledge graph. Neo4j uses a graph query language called Cypher. For using the Cypher query language, there are a couple of approaches: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p121"><em>Template base</em><em>d</em>—Several pre-defined Cypher templates are created and based on the user query, an LLM selects which template to use. This is an extremely rigid and limiting approach.</li> 
   <li class="readable-text" id="p122"><em>LLM-generated quer</em><em>y</em>—An LLM generates the Cypher query directly based on the natural language user query. Prompt engineering techniques such as few-shot prompting are employed. This approach is more flexible than a template-based approach, but not 100% reliable. </li> 
  </ul> 
  <div class="readable-text" id="p123"> 
   <p>In LangChain, the <code>GraphCypherQAChain</code> class is from the <code>langchain.chains</code> library. For better querying, the schema of the knowledge graph is also provided to the LLM: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p124"><em>Augmentatio</em><em>n</em>—Depending on the graph query, the response received from the graph database is processed to extract the text that can be augmented to the original user query. Apart from this, the augmentation step is the same as in vector RAG.</li> 
   <li class="readable-text" id="p125"><em>Generatio</em><em>n</em>—The augmented prompt is sent to the LLM like in the standard vector RAG approach.</li> 
  </ul> 
  <div class="readable-text" id="p126"> 
   <p>While the final generation step and initial data loading and chunking do not require any special adjustment, the rest of the process changes significantly. Table 8.3 summarizes the differences between vector and graph RAG.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p127"> 
   <h5 class=" browsable-container-h5">Table 8.3 Differences between vector RAG and graph RAG</h5> 
   <table id="table003" class="No-Table-Style TableOverride-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-15" /> 
     <col class="_idGenTableRowColumn-16" /> 
    </colgroup> 
    <thead> 
     <tr class="No-Table-Style _idGenTableRowColumn-17"> 
      <td class="No-Table-Style CellOverride-13" scope="col"> <p class="_TableHead">Step</p> </td> 
      <td class="No-Table-Style CellOverride-14" scope="col"> <p class="_TableHead">Vector RAG</p> </td> 
      <td class="No-Table-Style CellOverride-15" scope="col"> <p class="_TableHead">Graph RAG</p> </td> 
     </tr> 
    </thead> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-8"> 
      <td class="No-Table-Style CellOverride-16"> <p class="_TableBody">Data loading</p> </td> 
      <td class="No-Table-Style CellOverride-17"> <p class="_TableBody">Loads documents without specialized preprocessing for relationships</p> </td> 
      <td class="No-Table-Style CellOverride-18"> <p class="_TableBody">Similar to vector RAG; documents are loaded without special graph handling. </p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-8"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">Data chunking</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Divides large documents into smaller chunks for embedding and vector storage </p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Documents are chunked similarly; each chunk is then processed to extract entities and relationships, building a relational structure.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-18"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">Entity and relationship extraction</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Not applicable; focuses on <br />creating embeddings from chunks</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Entities, relationships, and attributes are extracted from each chunk using an LLM, potentially in multiple passes to refine and de-duplicate entities and relationships.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-19"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">Storage</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Stores embeddings in a vector database</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Entities and relationships are stored in a graph database (e.g., Neo4j), with the option to update the graph iteratively. Tools such as LangChain’s Neo4jGraph can automate this process.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-19"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">Community summaries</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Not applicable; primarily relies on similarity search on individual embeddings </p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Detects communities within the knowledge graph and uses an LLM to create summaries for each community. These summaries can be stored as vectors for a hybrid graph–vector RAG approach.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-19"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">Retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Performs direct similarity searches on embeddings</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Involves graph traversal using Cypher queries, generated either from pre-defined templates or dynamically by an LLM. Neo4j’s vector indexes can enhance similarity-based node searches.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-20"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">Augmentation</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Uses retrieved embeddings to augment the user’s query</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Retrieved nodes, relationships, or summaries augment the user’s query. Additional LLM processing might be used to refine responses based on the retrieved graph content.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-21"> 
      <td class="No-Table-Style CellOverride-22"> <p class="_TableBody">Generation</p> </td> 
      <td class="No-Table-Style CellOverride-23"> <p class="_TableBody">Sends the augmented prompt to an LLM for response generation</p> </td> 
      <td class="No-Table-Style CellOverride-24"> <p class="_TableBody">Like vector RAG but relies on augmented data with graph-derived insights, relationships, and context from the knowledge graph to enrich the response.</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p128"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.3.5</span> Challenges and best practices</h3> 
  </div> 
  <div class="readable-text" id="p129"> 
   <p>Despite all the benefits of graph RAG, there are certain challenges that must be considered carefully: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p130">Merging diverse data sources into a cohesive knowledge graph can be intricate and time-consuming. Start with a focused domain and gradually expand the knowledge graph to manage complexity.</li> 
   <li class="readable-text" id="p131">Due to the iterative LLM processing at different stages, large-scale knowledge graph generation and community summarization from documents are computationally expensive. Therefore, the data for graph RAG must be selected carefully.</li> 
   <li class="readable-text" id="p132">Current similarity measurement techniques may not fully capture the nuanced relationships or structural dependencies in graphs, leading to potential mismatches in retrieved information. Careful use of case-specific evaluation is warranted for acceptable accuracy.</li> 
   <li class="readable-text" id="p133">Each deployment may need custom graph data construction, indexing, and retrieval adaptations, which makes generalization difficult. Keeping the knowledge graph updated with accurate and current information requires continuous effort. Consequently, graph RAG may not be the default RAG strategy. </li> 
  </ul> 
  <div class="readable-text" id="p134"> 
   <p>So far, we have looked at two RAG variants that extend standard RAG capabilities by including multimodal data and graph structures. Next, we discuss one of the most significant concepts in the field of generative AI: agents.</p> 
  </div> 
  <div class="readable-text" id="p135"> 
   <h2 class=" readable-text-h2"><span class="num-string">8.4</span> Agentic RAG</h2> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>By now, you understand that challenges exist with standard RAG systems. They may struggle with reasoning, answering complex questions, and multistep processes. One of the key aspects of comprehensive RAG systems is the ability to search through multiple sources of data. This can be internal company documents, the open internet, third-party applications, and even structured data sources like an SQL database. So far in this book, we have built systems that can search through a single knowledge base, and for any query, the entire knowledge base is searched through. </p> 
  </div> 
  <div class="readable-text intended-text" id="p137"> 
   <p>Two challenges arise with this approach. First, all information must be indexed and stored in a single vector store, which leads to storage problems at scale. Second, for any query, the entire knowledge base needs to be searched, which is highly inefficient for large knowledge bases. To overcome this challenge, a module that can understand the user’s query and route the query to a relevant source is needed. This is one of the limitations addressed by agentic RAG that uses one or more LLM agents for decision-making. Let’s first understand what is meant by the term<em> agent</em>.</p> 
  </div> 
  <div class="readable-text" id="p138"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.4.1</span> LLM agents</h3> 
  </div> 
  <div class="readable-text" id="p139"> 
   <p>The use of agents in AI predates the popularity of LLMs. The overarching meaning of an AI agent is a software system that can autonomously perceive the environment it is in, make decisions, and perform actions to achieve a goal. Traditionally, AI agents have been developed to execute specific tasks and rely on predefined rules or learned behaviors, like in the fields of autonomous vehicles or robotics. Due to the ability to process and understand language (and now even multimodal data), LLMs are now being seen as a general-purpose technology that can help build autonomous decision-making without explicitly defining rules or environment data. While there is no common definition of an LLM-based AI agent, there are four key components of the system that enable autonomous decision-making and task execution. </p> 
  </div> 
  <div class="readable-text intended-text" id="p140"> 
   <p>The<strong> </strong><em>core LLM brain</em> is an LLM that assigned a certain role and a task. This component is responsible for understanding the user request and interacting with other components to respond to the user. For example, an AI agent built for travel assistance may have to deal with different types of tasks such as searching for information, creating itineraries, booking tickets, or managing previous bookings. </p> 
  </div> 
  <div class="readable-text intended-text" id="p141"> 
   <p>The<strong> </strong><em>memory</em> component manages the agent’s past experiences. It can be short-term like the chat history of the current conversation or long-term where important pieces of information from previous interactions are stored. For a travel assistant AI agent, short-term memory will hold the current context of the user query, while the ticket booking history or previous travel searches can be fetched from long-term memory.</p> 
  </div> 
  <div class="readable-text intended-text" id="p142"> 
   <p>The<strong> </strong><em>planning</em> component creates a step-by-step sequence of tasks that will be followed to respond to the user’s request. Task decomposition or breaking down complex tasks into smaller, manageable subtasks. ReAct, which stands for reasoning and acting, or reflection, where the agent does a self-assessment of the outcomes, can be part of the planning component.</p> 
  </div> 
  <div class="readable-text intended-text" id="p143"> 
   <p><em>Tools</em> assist the agent in performing actions on resources external to it. This can be conducting a web search on the internet, querying an external database such as an SQL database, invoking a third-party API such as a weather API, and similar. The core LLM brain is responsible for sending the payload request to the tools in the accepted format. These four components and their interactions are shown in figure 8.11. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p144">  
   <img src="../Images/CH08_F11_Kimothi.png" alt="A diagram of a process

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.11</span><span class=""> </span><span class="">An LLM agent’s four components break down the user’s query, recall the history of interaction with the user, and employ external tools to accomplish tasks and respond to the user.</span></h5>
  </div> 
  <div class="readable-text" id="p145"> 
   <p>Since the definition of AI agents continues evolving, these components are not set in stone but are generally agreed upon. To help understand how these components interact, let’s take an example of an AI agent built for travel assistance, like the customer service agent of an online travel agency. </p> 
  </div> 
  <div class="readable-text intended-text" id="p146"> 
   <p>Suppose a customer asks a question like, “Is my flight on schedule?” The core LLM brain receives this input and understands that the user intent is to check a specific flight status. At this stage, the core LLM brain can invoke the planning module to decide the course of action required to answer queries of this intent. The planning module may respond with steps such as retrieving booking information from previous interactions (memory), querying the latest flight information from a database, comparing it with previous details from memory, and conveying the result to the user. Here, retrieving the information from the database will require a tool such as an API, which is a prebuilt module that the core LLM brain has access to. The planning module can also bring in conditional steps—for example, if the previous booking information cannot be retrieved from memory, the core LLM brain must prompt the user to provide this information. When the core LLM brain gets the plan from the planning module, it retrieves previous booking information, invokes the tool to retrieve flight information, compares the new information with the old information in memory, and crafts a response based on this analysis. This simple workflow of the agent is illustrated in figure 8.12.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p147">  
   <img src="../Images/CH08_F12_Kimothi.png" alt="A diagram of a flight process

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.12</span><span class=""> </span><span class="">A simple task of responding to a user query on flight schedule responded to by an LLM agent by using the planning, memory, and tools modules</span></h5>
  </div> 
  <div class="readable-text" id="p148"> 
   <p>This is an example of a simple task. Multiple agents can come together to solve tasks of a higher level of complexity, such as “Plan and book a holiday for me.” The field of LLM-based AI agents is quite promising, and readers are encouraged to read more about this evolving domain. For our discussion on agentic RAG in this section, we focus on a few aspects, specifically on tool usage and a little bit of planning. The use cases for agentic RAG span across industries, so it makes more sense to look at the capabilities of agentic RAG.</p> 
  </div> 
  <div class="readable-text" id="p149"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.4.2</span> Agentic RAG capabilities</h3> 
  </div> 
  <div class="readable-text" id="p150"> 
   <p>In our introduction to agentic RAG, we highlighted the challenge in standard RAG using a single knowledge base. Agentic RAG infuses abilities in the RAG system that make the system more efficient and accurate.</p> 
  </div> 
  <div class="readable-text" id="p151"> 
   <h4 class=" readable-text-h4">Query understanding and routing</h4> 
  </div> 
  <div class="readable-text" id="p152"> 
   <p>Based on the user query, an LLM agent can be tasked with deciding which knowledge base to search through. For example, assume a programming assistant that can not only search the codebase but also the product documentation, along with searching the web. Depending on the question that the developer asks, the agent can decide which database to query. For generic messages such as greetings, the agent can also decide not to invoke the retriever and send the message directly to the LLM for a response. </p> 
  </div> 
  <div class="readable-text" id="p153"> 
   <h4 class=" readable-text-h4">Tools usage</h4> 
  </div> 
  <div class="readable-text" id="p154"> 
   <p>In the previous example, the system was also required to search the web. The internet cannot be stored in a knowledge base and is usually accessed through an API that returns search results. This search API is an example of a tool the agent can use. Similarly, other APIs, such as Notion or Google Drive, can be used to access information sources. One of the features of tools like APIs is that they have fixed query and response formats. The job of the agent is to process natural language information into the format structure and parse the response to use it for generation.</p> 
  </div> 
  <div class="readable-text" id="p155"> 
   <h4 class=" readable-text-h4">Adaptive retrieval</h4> 
  </div> 
  <div class="readable-text" id="p156"> 
   <p>Recall adaptive retrieval discussed in chapter 6. An LLM is enabled to determine the most appropriate moment and content for retrieval. This is an extension of query routing, where after deciding the most appropriate source to query, an agent can also determine whether the retrieved information is good enough to generate responses or whether another iteration of retrieval is required. For the next iteration, the agent can also form fresh queries based on the retrieved context. This enables the RAG system to solve complex queries. </p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>These capabilities enable agentic RAG systems to be comprehensive and work on a scale. While the indexing and generation pipelines do not change in structure, agents can be invoked throughout the two pipelines.</p> 
  </div> 
  <div class="readable-text" id="p158"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.4.3</span> Agentic RAG pipelines</h3> 
  </div> 
  <div class="readable-text" id="p159"> 
   <p>The capability of LLM-based agents to understand the context and invoke tools can be used to elevate each stage of the RAG pipeline. </p> 
  </div> 
  <div class="readable-text" id="p160"> 
   <h4 class=" readable-text-h4">Indexing pipeline</h4> 
  </div> 
  <div class="readable-text" id="p161"> 
   <p>The idea of the knowledge base in agentic RAG is no different from standard RAG. Agents can be used across components to enhance the indexing pipeline:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p162"><em>Data loadin</em><em>g</em>—Loading data and extracting information is the first and incredibly crucial step of RAG system development. Accurate parsing of information is critical in building an accurate RAG system. Parsing complex documents such as PDF reports can be tough. While there are libraries and tools present for these tasks, LLM agents can be used for high-precision parsing. The importance of metadata in RAG cannot be overstated. It is useful for filtering, more contextual mapping, and source citation. In most scenarios, it is difficult to source rich metadata. LLM agents can be used to build metadata architecture and extract contextual metadata. </li> 
   <li class="readable-text" id="p163"><em>Chunkin</em><em>g</em>—In agentic chunking, chunks from the text are created based on a goal or a task. Consider an e-commerce platform wanting to analyze customer reviews. The best way for the reviews to be chunked is if the reviews about a particular topic are put in the same chunk. Similarly, the critical and positive reviews may be put in different chunks. To achieve this kind of chunking, we will need to do sentiment analysis, entity extraction, and some kind of clustering. This can be achieved by a multiagent system. Agentic chunking is still an active area of research and improvement.</li> 
   <li class="readable-text" id="p164"><em>Embedding</em><em>s</em>—The role of agents in embeddings can be the selection of the right embeddings model, depending on the context of the chunks. For example, if there is information from multiple domains in the loaded data, there may be a case for using domain-specific embeddings for different chunks. Apart from this, quality control agents can validate embeddings by measuring similarity or alignment with predefined standards or use case requirements. You may also recall from the discussion on graph RAG that agents can also decide to use graph structures for certain chunks.</li> 
   <li class="readable-text" id="p165"><em>Storag</em><em>e</em>—There is also a possibility to store chunk embeddings from the same document in different collections owing to the nature of the information. For example, the information related to the installation and troubleshooting of a product can be stored in one collection of a vector database, and product features and advantages can be stored in another. This helps in setting the retrieval up for higher precision. You may notice that the use of agents in chunking, embeddings, and storage are closely related.</li> 
  </ul> 
  <div class="readable-text" id="p166"> 
   <p>Figure 8.13 summarizes how the use of agents can embellish the indexing pipeline. The nature of the knowledge base itself doesn’t change, but the process of creation is embellished with agents.</p> 
  </div> 
  <div class="readable-text" id="p167"> 
   <h4 class=" readable-text-h4">Generation Pipeline</h4> 
  </div> 
  <div class="readable-text" id="p168"> 
   <p>The true advantage of an agentic system lies in how it transforms the entire generation pipeline across all three stages: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p169"><em>Retrieval</em>—Perhaps the most significant use of agents is in the retrieval stage. Query routing to the most appropriate source and the integration of tools to query external sources of information is a crucial feature of agentic RAG. </li> 
  </ul> 
  <div class="browsable-container figure-container " id="p170">  
   <img src="../Images/CH08_F13_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.13</span><span class=""> </span><span class="">Agentic embellishment to the indexing pipeline enhances the quality of the knowledge base.</span></h5>
  </div> 
  <ul> 
   <li class="readable-text" style="list-style: none;" id="p171">Adaptive retrieval strategies also bring significant improvement in the retrieval stage. </li> 
   <li class="readable-text" id="p172"><em>Augmentation</em>—Agents can choose the correct prompting technique for augmentation, depending on the nature of the query and the retrieved context. Prompts can also be generated dynamically by an agent. </li> 
   <li class="readable-text" id="p173"><em>Generation</em>—One of the uses of agentic RAG is also in multistep generation such as IterRetGen or iterative-retrieval generation. In this approach, an agent is used to review the response generated by the LLM in the first pass, and it decides if any further iteration of retrieval and generation is required to completely respond to the user query. This is particularly useful in multi-hop reasoning and fact verification.</li> 
  </ul> 
  <div class="readable-text" id="p174"> 
   <p>Another way to think about agentic RAG is that wherever dynamic decision-making can improve the RAG system, an agent can be used to autonomously make those decisions. From the previous discussion, you may conclude that agentic RAG is a superior version of standard RAG. Table 8.4 summarizes the advantages of agentic over standard RAG.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p175"> 
   <h5 class=" browsable-container-h5">Table 8.4 Advantages of agentic RAG</h5> 
   <table id="table004" class="No-Table-Style TableOverride-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-22" /> 
     <col class="_idGenTableRowColumn-23" /> 
    </colgroup> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-17"> 
      <td class="No-Table-Style CellOverride-25"> <p class="_TableHead">Aspect</p> </td> 
      <td class="No-Table-Style CellOverride-26"> <p class="_TableHead">Standard RAG</p> </td> 
      <td class="No-Table-Style CellOverride-27"> <p class="_TableHead">Agentic RAG</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-14"> 
      <td class="No-Table-Style CellOverride-16"> <p class="_TableBody">Retrieval process</p> </td> 
      <td class="No-Table-Style CellOverride-17"> <p class="_TableBody">Passive retrieval based on initial query</p> </td> 
      <td class="No-Table-Style CellOverride-18"> <p class="_TableBody">Adaptive retrieval with intelligent agents routing and reformulating queries as needed</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">Handling complex queries</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Struggles with multistep reasoning and complex queries</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Can be used to break down and address complex, multifaceted queries</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">Tool integration</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Limited integration with external tools and APIs</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Seamless integration with various external tools and APIs for enhanced information gathering</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">Scalability</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Challenges in scaling due to static processes</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Scalable through modular agent-based architecture, allowing for easy expansion </p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-8"> 
      <td class="No-Table-Style CellOverride-22"> <p class="_TableBody">Accuracy and relevance</p> </td> 
      <td class="No-Table-Style CellOverride-23"> <p class="_TableBody">Dependent on initial query quality; may retrieve less relevant information</p> </td> 
      <td class="No-Table-Style CellOverride-24"> <p class="_TableBody">Higher accuracy and relevance due to agents’ ability to refine queries and validate information</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p176"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.4.4</span> Challenges and pest practices</h3> 
  </div> 
  <div class="readable-text" id="p177"> 
   <p>LLM based agents are still evolving and are not foolproof. There are also concerns around the planning and reasoning abilities of LLMs. For implementing agentic abilities into the RAG pipelines, a few aspects should be evaluated carefully:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p178">The accuracy of tool selection diminishes when a single agent is responsible for invoking a high number of tools. Therefore, the number of decision choices for the agent needs to be controlled. </li> 
   <li class="readable-text" id="p179">No agent can be expected to be accurate all the time. Error rates in multiagent systems can also increase. It is important to establish a failsafe at every stage. The choice of the use case should also be guided by the expected accuracy levels.</li> 
   <li class="readable-text" id="p180">Increased autonomy in decision-making can lead to unintended actions if not properly controlled. In other words, agents can misfire, and establishing explicit boundaries and guidelines for agent behavior is critical.</li> 
  </ul> 
  <div class="readable-text" id="p181"> 
   <p>Multimodal, graph, and agentic RAG patterns have demonstrated significant improvements over the standard RAG pipelines. Multimodal RAG opens the RAG systems to different modalities, graph RAG introduces relational understanding, and agentic RAG infuses RAG systems with intelligence and autonomous decision making. Apart from these three, ongoing research on RAG has resulted in several other frameworks and variations to the standard RAG systems. The next section discusses variants that show significant promise.</p> 
  </div> 
  <div class="readable-text" id="p182"> 
   <h2 class=" readable-text-h2"><span class="num-string">8.5</span> Other RAG variants</h2> 
  </div> 
  <div class="readable-text" id="p183"> 
   <p>We have talked about the three major RAG variants in this chapter. Research in the field is bustling, and every week, several papers are released by researchers about their experiments and key findings. Out of these papers, quite a few demonstrate RAG variants that find relevance in practical applications. We close this chapter by briefly discussing four such RAG variants.</p> 
  </div> 
  <div class="readable-text" id="p184"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.5.1</span> Corrective RAG</h3> 
  </div> 
  <div class="readable-text" id="p185"> 
   <p>The effectiveness of a RAG system depends on the quality of retrieval. Inaccuracies in retrieval negate all RAG benefits. To address this, the corrective RAG (CRAG) approach evaluates the quality of retrieved documents. It uses a lightweight evaluator and triggers corrective action if the retrieved information is found to be inaccurate. The key CRAG components are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p186"><em>Retrieval evaluato</em><em>r</em>—A model that evaluates the relevance of the retrieved documents and assigns a relevance score to each retrieved document. In the original CRAG paper (<a href="https://arxiv.org/abs/2401.15884"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/abs/2401.15884</span></a>), the evaluator is a fine-tuned T5 model that assigns a score of being correct, incorrect, or ambiguous.  </li> 
   <li class="readable-text" id="p187"><em>Web search supplementatio</em><em>n</em>—If a retrieved document is classified as incorrect, the system conducts a web search to supplement the knowledge base, ensuring more accurate, up-to-date information. </li> 
   <li class="readable-text" id="p188"><em>Knowledge refinemen</em><em>t</em>—Retrieved documents classified as correct by the evaluator and the content retrieved from web search are broken down further into smaller knowledge strips, and each strip undergoes evaluation. </li> 
  </ul> 
  <div class="readable-text" id="p189"> 
   <p>Figure 8.14 illustrates the CRAG workflow with the evaluator, knowledge refinement, and web search added to the standard RAG flow.</p> 
  </div> 
  <div class="readable-text" id="p190"> 
   <p>As for its advantages and limitations, CRAG secures accurate, context-relevant knowledge for generation, particularly in cases where initial retrieval may be flawed. The corrective actions enhance the factual accuracy of the generated content. CRAG is a solution that can be integrated with all RAG pipelines and other RAG variants without causing any disruptions. There are also a couple of factors that need to be considered: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p191">The additional corrective actions and web search integration may increase response time. </li> 
   <li class="readable-text" id="p192">The performance of the system is closely tied to the accuracy of the evaluator model.</li> 
  </ul> 
  <div class="readable-text" id="p193"> 
   <p>CRAG is an improvement over standard RAG, which uses the retrieved documents as is. The corrective approach makes it effective for accuracy-sensitive applications that demand data verification.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p194">  
   <img src="../Images/CH08_F14_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 8.14</span><span class=""> </span><span class="">CRAG corrects the knowledge at the most granular level, hence the name corrective RAG. Source: </span><a href="https://arxiv.org/abs/2401.15884"><span class=""><span class="Hyperlink CharOverride-4">https:</span></span><span class=""><span class="Hyperlink CharOverride-4">/</span></span><span class=""><span class="Hyperlink CharOverride-4">/arxiv.org/abs/2401.15884</span></span></a><span class="">.</span> </h5>
  </div> 
  <div class="readable-text" id="p195"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.5.2</span> Speculative RAG</h3> 
  </div> 
  <div class="readable-text" id="p196"> 
   <p>Latency and redundancy are ubiquitous concerns in RAG systems. Speculative RAG addresses these in a two-step approach. First, small language models parallelly generate multiple answer drafts, each based on diverse subsets of documents. Then, a larger LLM verifies and selects the most accurate draft. The key components of speculative RAG are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p197"><em>Document clusterin</em><em>g</em>—Retrieved documents are clustered into topic-related groups, each offering a unique perspective. </li> 
   <li class="readable-text" id="p198"><em>RAG drafte</em><em>r</em>—A smaller LLM produces initial answer drafts based on each cluster subset, generating responses and rationales in parallel for efficiency.</li> 
   <li class="readable-text" id="p199"><em>RAG verifie</em><em>r</em>—A larger LLM evaluates each draft’s accuracy and coherence, assigning confidence scores based on self-consistency and rationale support.</li> 
  </ul> 
  <div class="readable-text" id="p200"> 
   <p>The key advantage of speculative RAG is faster response generation by reducing the workload on the generator LLM and performing parallel draft generation. However, some of the following limitations require careful consideration: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p201">Involves managing a two-model setup and document clustering, which may increase initial setup complexity. </li> 
   <li class="readable-text" id="p202">Document clustering directly affects draft diversity, and poor clustering can lead to redundant drafts by grouping highly similar or repetitive documents into multiple clusters. </li> 
   <li class="readable-text" id="p203">The smaller LLM may require training for effective draft and rationale generation.</li> 
  </ul> 
  <div class="readable-text" id="p204"> 
   <p>Unlike standard RAG, which incorporates all retrieved data into a single prompt, speculative RAG uses parallel draft generation for efficiency and a dedicated verification step for accuracy, which leads to a reduction in latency, while improving the factual efficiency of the responses.</p> 
  </div> 
  <div class="readable-text" id="p205"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.5.3</span> Self-reflective (self RAG)</h3> 
  </div> 
  <div class="readable-text" id="p206"> 
   <p>Self-reflection in an LLM is the ability of the LLM to analyze its actions, identify potential errors or flaws in its reasoning process, and then use that feedback to improve its responses and decision-making. Self RAG incorporates reflection to dynamically decide whether to retrieve relevant information, evaluate retrieved content, and to critique its output. The key components of self RAG are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p207"><em>Reflection token</em><em>s</em>—Self RAG trains an LLM to use “reflection tokens,” which help it assess the relevance, support, and usefulness of retrieved passages. These tokens are designed to guide the model in judging the quality of both the retrieved content and its generated response, adding layers of control and adaptability. A <em>retrieve token</em> indicates whether retrieval is needed. Similarly, the <em>relevance token</em> determines whether a passage is relevant, the <em>support token</em> verifies whether the generated response is fully supported by retrieved content, and the <em>utility token</em> scores the usefulness of the response.</li> 
   <li class="readable-text" id="p208"><em>Dynamic retrieval decisio</em><em>n</em>—The model uses reflection tokens to determine if retrieval is necessary based on each segment of the response and skips retrieval if it is unnecessary at any step.</li> 
   <li class="readable-text" id="p209"><em>Self-critiqu</em><em>e</em>—The model critiques its output at each generation step, applying reflection tokens to guide retrieval and refine the response in real time.</li> 
  </ul> 
  <div class="readable-text" id="p210"> 
   <p>Adaptive retrieval in self RAG reduces unnecessary retrievals, and self-reflection results in better accuracy, factual consistency, and relevance. However, some limitations need to be considered:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p211">Processing multiple passages in parallel and self-reflection may increase computational demands.</li> 
   <li class="readable-text" id="p212">The additional training and use of reflection tokens require fine-tuning of thresholds.</li> 
  </ul> 
  <div class="readable-text" id="p213"> 
   <p>Self RAG is one of the most cited techniques in research on RAG. Its dynamic adjustment of retrieval based on task needs evaluates output quality, achieving superior accuracy.</p> 
  </div> 
  <div class="readable-text" id="p214"> 
   <h3 class=" readable-text-h3"><span class="num-string">8.5.4</span> RAPTOR</h3> 
  </div> 
  <div class="readable-text" id="p215"> 
   <p>Recursive abstractive processing for tree-organized retrieval, or RAPTOR, is a RAG variant designed to handle hierarchical relationships in data. It creates a multilevel, tree-based structure of recursive summaries, capturing both granular details and overarching themes in long documents. Like graph RAG, RAPTOR uses a tree structure to achieve similar objectives. Here are the key RAPTOR components:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p216"><em>Chunk clustering and summarizatio</em><em>n</em>—Chunk embeddings are clustered based on similarity, and an LLM is used to summarize the clusters. Soft clustering with Gaussian mixture models allows text segments to belong to multiple clusters.</li> 
   <li class="readable-text" id="p217"><em>Recursive tree constructio</em><em>n</em>—RAPTOR builds a multilayered tree by using chunks, clusters, and summaries in a bottom-up process.</li> 
   <li class="readable-text" id="p218"><em>Dual querying mechanism</em><em>s</em>—A top-down approach starts traversing down to select the most relevant nodes at each level based on cosine similarity to the query. Another single-layer search retrieves context across all tree nodes irrespective of the levels. </li> 
  </ul> 
  <div class="readable-text" id="p219"> 
   <p>Like graph RAG, RAPTOR enables better multi-hop reasoning and thematic question answering by incorporating both granular and high-level summaries. However, tree structures are complex to manage and RAPTOR comes with its set of challenges: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p220">The recursive clustering and summarization steps can be computationally intensive, especially for very large documents.</li> 
   <li class="readable-text" id="p221">Effective retrieval hinges on the quality of the clustering; errors in initial clustering can propagate up the tree.</li> 
  </ul> 
  <div class="readable-text" id="p222"> 
   <p>Unlike standard RAG, which may struggle with multilayered content, RAPTOR’s hierarchical model allows targeted retrieval, optimizing for both specificity and contextual relevance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p223"> 
   <p>This chapter explored RAG variants that use advanced techniques to improve RAG systems for specific use cases. Multimodal pipelines give RAG systems access to previously unusable data, graph RAG provides the ability of relational analysis, and agentic RAG introduces autonomous decision-making for complex tasks. Each RAG variant addresses a certain aspect of improvement in standard RAG systems. Corrective RAG focuses on factual relevance, RAPTOR builds relational intelligence for hierarchical data, speculative RAG is built for efficiency, and self RAG makes the system adaptive. </p> 
  </div> 
  <div class="readable-text intended-text" id="p224"> 
   <p>With this chapter, we are almost at the end of our discussion on RAG. The last chapter discusses some of the independent considerations and best practices across different stages of RAG system lifecycle. </p> 
  </div> 
  <div class="readable-text" id="p225"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <div class="readable-text" id="p226"> 
   <h3 class=" readable-text-h3">Introducing RAG variants</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p227">RAG variants are adaptations of the na&iuml;ve RAG framework that extend its functionality to specific use cases.</li> 
   <li class="readable-text" id="p228">These variants address challenges, such as processing nontextual data, improving relational understanding, enhancing accuracy, and enabling autonomous decision-making.</li> 
   <li class="readable-text" id="p229">Three major RAG variants were discussed in depth: multimodal, graph, and agentic RAG.</li> 
   <li class="readable-text" id="p230">Other promising RAG variants are corrective RAG, speculative RAG, self RAG, and RAPTOR.</li> 
  </ul> 
  <div class="readable-text" id="p231"> 
   <h3 class=" readable-text-h3">Multimodal rag</h3> 
  </div> 
  <ul> 
   <li class="readable-text buletless-item" id="p232">It extends RAG capabilities to handle multiple data modalities such as text, images, audio, and video. It can be used for 
    <ul> 
     <li><em>Medical diagnosi</em><em>s</em>—Analyzing text, images (X-rays), and tabular data (lab results)</li> 
     <li><em>Investment analysi</em><em>s</em>—Processing financial documents, charts, and balance sheets</li> 
     <li><em>Equipment maintenanc</em><em>e</em>—Combining text reports, visual inspections, and sensor data</li> 
    </ul> </li> 
   <li class="readable-text" id="p233">As for the pipeline enhancements, multimodal RAG introduces multimodal embeddings (shared or modality specific), transcription tools, and specialized chunking methods to indexing pipeline. In the generation pipeline, it employs multimodal LLMs (e.g., GPT-4o, Google Gemini). </li> 
   <li class="readable-text" id="p234">Multimodal RAG has high computational requirements and increased latency. Information loss is possible during text conversion of nontext modalities.</li> 
  </ul> 
  <div class="readable-text" id="p235"> 
   <h3 class=" readable-text-h3">Knowledge graph RAG</h3> 
  </div> 
  <ul> 
   <li class="readable-text buletless-item" id="p236">It enhances retrieval and reasoning through relationships represented in a graph structure. It can be used for 
    <ul> 
     <li><em>Personalized treatment plan</em><em>s</em>—Linking drugs, conditions, and symptoms for customized recommendations</li> 
     <li><em>Contract analysi</em><em>s</em>—Identifying dependencies and compliance risks across interconnected legal documents</li> 
    </ul> </li> 
   <li class="readable-text" id="p237">As for the pipeline enhancements, the knowledge graph RAG extracts entities, relationships, and attributes from chunks to create a graph in the indexing pipeline. As for the generation pipeline, it incorporates graph traversal using graph query languages such as Cypher.</li> 
   <li class="readable-text" id="p238">Building and maintaining knowledge graphs is complex and computationally expensive. It also requires custom adaptations for each deployment.</li> 
  </ul> 
  <div class="readable-text" id="p239"> 
   <h3 class=" readable-text-h3">Agentic RAG</h3> 
  </div> 
  <ul> 
   <li class="readable-text buletless-item" id="p240">It introduces LLM-based agents for autonomous decision-making and dynamic query routing. Agentic RAG can be used for 
    <ul> 
     <li>Query understanding and routing to relevant data sources</li> 
     <li>Adaptive retrieval and multistep generation</li> 
     <li>Integration with tools such as web search APIs and external databases</li> 
    </ul> </li> 
   <li class="readable-text" id="p241">With regard to pipeline enhancements, agentic RAG enhances chunking, metadata extraction, and embeddings selection with agentic decision-making in the indexing pipeline. In the generation pipeline, it dynamically augments prompts and employs iterative retrieval-generation workflows.</li> 
   <li class="readable-text" id="p242">Agentic RAG requires robust controls to prevent unintended actions by agents. High computational overhead and multiplied error rates in multiagent systems.</li> 
  </ul> 
  <div class="readable-text" id="p243"> 
   <h3 class=" readable-text-h3">Other RAG variants</h3> 
  </div> 
  <ul> 
   <li class="readable-text buletless-item" id="p244">Corrective RAG (CRAG) Focuses on factual accuracy by evaluating retrieved content. It also adds corrective steps such as web search supplementation and knowledge refinement. 
    <ul> 
     <li><em>Advantage</em><em>s</em>—Enhances accuracy and can integrate seamlessly with other RAG pipelines </li> 
     <li><em>Challenge</em><em>s</em>—Increased response time and dependency on the evaluator model</li> 
    </ul> </li> 
   <li class="readable-text buletless-item" id="p245">Speculative RAG reduces latency by generating multiple drafts in parallel using smaller LLMs. A larger LLM verifies and selects the most accurate draft. 
    <ul> 
     <li><em>Advantage</em><em>s</em>—Faster response generation</li> 
     <li><em>Challenge</em><em>s</em>—Requires careful document clustering and draft diversity</li> 
    </ul> </li> 
   <li class="readable-text buletless-item" id="p246">Self RAG incorporates reflection tokens for adaptive retrieval and self-assessment of generated content. 
    <ul> 
     <li><em>Advantage</em><em>s</em>—Superior accuracy and factual consistency</li> 
     <li><em>Challenge</em><em>s</em>—Computationally demanding and requires fine-tuned thresholds</li> 
    </ul> </li> 
   <li class="readable-text buletless-item" id="p247">RAPTOR builds hierarchical relationships through tree-structured summaries. 
    <ul> 
     <li><em>Advantage</em><em>s</em>—Optimized for multi-hop reasoning and thematic queries</li> 
     <li><em>Challenge</em><em>s</em>—Computationally intensive and relies on effective clustering</li> 
    </ul> </li> 
  </ul>
 </body>
</html>