- en: Chapter 3\. Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After experimenting with the fully connected neural networks in [Chapter 2](ch02.html#image-classification-with-pytorch),
    you probably noticed a few things. If you attempted to add more layers or vastly
    increase the number of parameters, you almost certainly ran out of memory on your
    GPU. In addition, it took a while to train to anything resembling somewhat decent
    accuracy, and even that wasn’t much to shout about, especially considering the
    hype surrounding deep learning. What’s going on?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: It’s true that a fully connected or (*feed-forward*) network can function as
    a universal approximator, but the theory doesn’t say how long it’ll take you to
    train it to become that approximation to the function you’re really after. But
    we can do better, especially with images. In this chapter, you’ll learn about
    *convolutional neural networks* (CNNs) and how they form the backbone of the most
    accurate image classifiers around today (we take a look at a couple of them in
    some detail along the way). We build up a new convolutional-based architecture
    for our fish versus cat application and show that it is quicker to train *and*
    more accurate than what we were doing in the previous chapter. Let’s get started!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Our First Convolutional Model
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This time around, I’m going to share the final model architecture first, and
    then discuss all the new pieces. And as I mentioned in [Chapter 2](ch02.html#image-classification-with-pytorch),
    the training method we created is independent of the model, so you can go ahead
    and test this model out first and then come back for the explanation!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first thing to notice is the use of `nn.Sequential()`. This allows us to
    create a chain of layers. When we use one of these chains in `forward()`, the
    input goes through each element of the array of layers in succession. You can
    use this to break your model into more logical arrangements. In this network,
    we have two chains: the `features` block and the `classifier`. Let’s take a look
    at the new layers we’re introducing, starting with `Conv2d`.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Conv2d` layer is a *2D convolution*. If we have a grayscale image, it
    consists of an array, *x* pixels wide and *y* pixels high, with each entry having
    a value that indicates whether it’s black or white or somewhere in between (we
    assume an 8-bit image, so each value can vary from 0 to 255). For this example
    we look at a small, square image that’s 4 pixels high and wide:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd>
    <mtd><mn>11</mn></mtd> <mtd><mn>9</mn></mtd> <mtd><mn>3</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd>
    <mtd><mn>123</mn></mtd> <mtd><mn>4</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>45</mn></mtd>
    <mtd><mn>237</mn></mtd> <mtd><mn>23</mn></mtd> <mtd><mn>99</mn></mtd></mtr> <mtr><mtd><mn>20</mn></mtd>
    <mtd><mn>67</mn></mtd> <mtd><mn>22</mn></mtd> <mtd><mn>255</mn></mtd></mtr></mtable></mfenced></math>
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we introduce something called a *filter*, or *convolutional kernel*. This
    is another matrix, most likely smaller, which we will drag across our image. Here’s
    our 2 × 2 filter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></math>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'To produce our output, we take the smaller filter and pass it over the original
    input, like a magnifying glass over a piece of paper. Starting from the top left,
    our first calculation is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd>
    <mtd><mn>11</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd> <mtd><mn>123</mn></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'And all we do is multiply each element in the matrix by its corresponding member
    in the other matrix and sum the result: (`10` × `1`) + (`11` × `0`) + (`2` × `1`)
    + (`123` × `0`) = `12`. Having done that, we move the filter across and begin
    again. But how much should we move the filter? In this case, we move the filter
    across by 2, meaning that our second calculation is:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的就是将矩阵中的每个元素与另一个矩阵中的对应成员相乘，并求和结果：(`10` × `1`) + (`11` × `0`) + (`2` × `1`)
    + (`123` × `0`) = `12`。做完这个之后，我们将滤波器移动并重新开始。但是我们应该移动滤波器多少？在这种情况下，我们将滤波器移动2个单位，这意味着我们的第二次计算是：
- en: <math display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>9</mn></mtd>
    <mtd><mn>3</mn></mtd></mtr> <mtr><mtd><mn>4</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>9</mn></mtd>
    <mtd><mn>3</mn></mtd></tr> <mtr><mtd><mn>4</mn></mtd> <mtd><mn>0</mn></mtd></tr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</m></mtd></tr>
    <mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></tr></mtable></mfenced></mrow></math>
- en: 'This gives us an output of 13\. We now move our filter down and back to the
    left and repeat the process, giving us this final result (or *feature map*):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个输出为13。现在我们将滤波器向下移动并向左移动，重复这个过程，给出这个最终结果（或*特征图*）：
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>12</mn></mtd>
    <mtd><mn>13</mn></mtd></mtr> <mtr><mtd><mn>65</mn></mtd> <mtd><mn>45</mn></mtd></mtr></mtable></mfenced></math>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>12</mn></mtd>
    <mtd><mn>13</mn></mtd></mtr> <mtr><mtd><mn>65</mn></mtd> <mtd><mn>45</mn></m></tr></mtable></mfenced></math>
- en: In [Figure 3-1](#cnn-kernel), you can see how this works graphically, with a
    3 × 3 kernel being dragged across a 4 × 4 tensor and producing a 2 × 2 output
    (though each segment is based on nine elements instead of the four in our first
    example).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图3-1](#cnn-kernel)中，您可以看到这是如何以图形方式工作的，一个3×3卷积核被拖动到一个4×4张量上，并产生一个2×2的输出（尽管每个部分基于九个元素而不是我们第一个示例中的四个）。
- en: '![How a 3x3 kernel operates across a 4x4 input](assets/ppdl_0301.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![3x3卷积核在4x4输入上的操作](assets/ppdl_0301.png)'
- en: Figure 3-1\. How a 3 × 3 kernel operates across a 4 × 4 input
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。3×3卷积核在4×4输入上的操作
- en: 'A convolutional layer will have many of these filters, the values of which
    are filled in by the training of the network, and all the filters in the layer
    share the same bias values. Let’s go back to how we’re invoking the `Conv2d` layer
    and see some of the other options that we can set:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层将有许多这样的滤波器，这些滤波器的值是由网络的训练填充的，该层中的所有滤波器共享相同的偏置值。让我们回到如何调用`Conv2d`层并看看我们可以设置的其他选项：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `in_channels` is the number of input channels we’ll be receiving in the
    layer. At the beginning of the network, we’re taking in the RGB image as input,
    so the number of input channels is three. `out_channels` is, unsurprisingly, the
    number of output channels, which corresponds to the number of filters in our conv
    layer. Next is `kernel_size`, which describes the height and width of our filter.^([1](ch03.html#idm45762366875032))
    This can be a single scalar specifying a square (e.g., in the first conv layer,
    we’re setting up an 11 × 11 filter), or you can use a tuple (such as (3,5) for
    a 3 × 5 filter).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`in_channels`是我们在该层接收的输入通道数。在网络的开始，我们将RGB图像作为输入，因此输入通道数为三。`out_channels`是输出通道数，对应于卷积层中的滤波器数量。接下来是`kernel_size`，描述了滤波器的高度和宽度。^([1](ch03.html#idm45762366875032))
    这可以是一个指定正方形的单个标量（例如，在第一个卷积层中，我们设置了一个11×11的滤波器），或者您可以使用一个元组（例如(3,5)表示一个3×5的滤波器）。'
- en: 'The next two parameters seem harmless enough, but they can have big effects
    on the downstream layers of your network, and even what that particular layer
    ends up looking at. `stride` indicates how many steps across the input we move
    when we adjust the filter to a new position. In our example, we end up with a
    stride of 2, which has the effect of making a feature map that is half the size
    of the input. But we could have also moved with a stride of 1, which would give
    us a feature map output of 4 × 4, the same size of the input. We can also pass
    in a tuple *(a,b)* that would allow us to move *a* across and *b* down on each
    step. Now, you might be wondering, what happens when it gets to the end? Let’s
    take a look. If we drag our filter along with a stride of 1, we eventually get
    to this point:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个参数似乎无害，但它们可能对网络的下游层产生重大影响，甚至影响该特定层最终查看的内容。`stride`表示我们在调整滤波器到新位置时在输入上移动多少步。在我们的示例中，我们最终得到步幅为2，这使得特征图的大小是输入的一半。但我们也可以使用步幅为1移动，这将给我们一个4×4的特征图输出，与输入的大小相同。我们还可以传入一个元组*(a,b)*，允许我们在每一步上移动*a*个单位横向和*b*个单位纵向。现在，您可能想知道，当它到达末尾时会发生什么。让我们看看。如果我们以步幅1拖动我们的滤波器，最终会到达这一点：
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>3</mn></mtd>
    <mtd><mo>?</mo></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mo>?</mo></mtd></mtr></mtable></mfenced></math>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>3</mn></mtd>
    <mtd><mo>?</mo></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mo>?</mo></mtd></mtr></mtable></mfenced></math>
- en: 'We don’t have enough elements in our input to do a full convolution. So what
    happens? This is where the `padding` parameter comes in. If we give a `padding`
    value of 1, our input looks a bit like this:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入中没有足够的元素来进行完整的卷积。那么会发生什么？这就是`padding`参数发挥作用的地方。如果我们给出`padding`值为1，我们的输入看起来有点像这样：
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>10</mn></mtd>
    <mtd><mn>11</mn></mtd> <mtd><mn>9</mn></mtd> <mtd><mn>3</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>2</mn></mtd> <mtd><mn>123</mn></mtd> <mtd><mn>4</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>45</mn></mtd>
    <mtd><mn>237</mn></mtd> <mtd><mn>23</mn></mtd> <mtd><mn>99</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>20</mn></mtd> <mtd><mn>67</mn></mtd> <mtd><mn>22</mn></mtd>
    <mtd><mn>255</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></math>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Now when we get to the edge, our values covered by the filter are as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>3</mn></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t set padding, any edge cases that PyTorch encounters in the last
    columns of the input are simply thrown away. It’s up to you to set padding appropriately.
    Just as with `stride` and `kernel_size`, you can also pass in a tuple for `height`
    × `weight` padding instead of a single number that pads the same in both directions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: That’s what the `Conv2d` layers are doing in our model. But what about those
    `MaxPool2d` layers?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conjunction with the convolution layers, you will often see *pooling* layers.
    These layers reduce the resolution of the network from the previous input layer,
    which gives us fewer parameters in lower layers. This compression results in faster
    computation for a start, and it helps prevent overfitting in the network.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'In our model, we’re using `MaxPool2d` with a kernel size of 3 and a stride
    of 2\. Let’s have a look at how that works with an example. Here’s a 5 × 3 input:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>4</mn></mtd> <mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>5</mn></mtd> <mtd><mn>6</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>2</mn></mtd>
    <mtd><mn>5</mn></mtd></mtr> <mtr><mtd><mn>5</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>9</mn></mtd> <mtd><mn>6</mn></mtd></mtr></mtable></mfenced></math>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the kernel size of 3 × 3 and a stride of 2, we get two 3 × 3 tensors
    from the pooling:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>5</mn></mtd> <mtd><mn>6</mn></mtd>
    <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>5</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></math><math
    display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>4</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><mn>2</mn></mtd>
    <mtd><mn>5</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>9</mn></mtd> <mtd><mn>6</mn></mtd></mtr></mtable></mfenced></math>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: In `MaxPool` we take the maximum value from each of these tensors, giving us
    an output tensor of [6,9]. Just as in the convolutional layers, there’s a `padding`
    option to `MaxPool` that creates a border of zero values around the tensor in
    case the stride goes outside the tensor window.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, you can pool with other functions aside from taking the
    maximum value from a kernel. A popular alternative is to take the average of the
    tensor values, which allows all of the tensor data to contribute to the pool instead
    of just one value in the `max` case (and if you think about an image, you can
    imagine that you might want to consider the nearest neighbors of a pixel). Also,
    PyTorch provides `AdaptiveMaxPool` and `AdaptiveAvgPool` layers, which work independently
    of the incoming input tensor’s dimensions (we have an `AdaptiveAvgPool` in our
    model, for example). I recommend using these in model architectures that you construct
    over the standard `MaxPool` or `AvgPool` layers, because they allow you to create
    architectures that can work with different input dimensions; this is handy when
    working with disparate datasets.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可以想象的那样，除了从内核中取最大值之外，你还可以使用其他函数进行池化。一个流行的替代方法是取张量值的平均值，这样允许所有张量数据都参与到池中，而不仅仅是`max`情况下的一个值（如果你考虑一幅图像，你可以想象你可能想要考虑像素的最近邻）。此外，PyTorch提供了`AdaptiveMaxPool`和`AdaptiveAvgPool`层，它们独立于传入输入张量的维度工作（例如，我们的模型中有一个`AdaptiveAvgPool`）。我建议在构建模型架构时使用这些，而不是标准的`MaxPool`或`AvgPool`层，因为它们允许你创建可以处理不同输入维度的架构；在处理不同数据集时这很方便。
- en: We have one more new component to talk about, one that is incredibly simple
    yet important for training.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个新组件要讨论，这个组件非常简单但对训练非常重要。
- en: Dropout
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout
- en: 'One recurring issue with neural networks is their tendency to overfit to training
    data, and a large amount of ongoing work is done in the deep learning world to
    identify approaches that allow networks to learn and generalize to nontraining
    data without simply learning how to just respond to the training inputs. The `Dropout`
    layer is a devilishly simple way of doing this that has the benefit of being easy
    to understand and effective: what if we just don’t train a random bunch of nodes
    within the network during a training cycle? Because they won’t be updated, they
    won’t have the chance to overfit to the input data, and because it’s random, each
    training cycle will ignore a different selection of the input, which should help
    generalization even further.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个经常出现的问题是它们倾向于过拟合训练数据，深度学习领域正在进行大量工作，以确定允许网络学习和泛化到非训练数据的方法，而不仅仅是学习如何对训练输入做出响应。`Dropout`
    层是一个极其简单但重要的方法，它易于理解且有效：如果我们在训练周期内不训练网络中的一组随机节点会怎样？因为它们不会被更新，它们就不会有机会过拟合输入数据，而且因为是随机的，每个训练周期将忽略不同的输入选择，这应该进一步帮助泛化。
- en: 'By default, the `Dropout` layers in our example CNN network are initialized
    with `0.5`, meaning that 50% of the input tensor is randomly zeroed out. If you
    want to change that to 20%, add the `p` parameter to the initialization call:
    `Dropout(p=0.2)`.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们示例CNN网络中，默认情况下，`Dropout` 层的初始化为`0.5`，意味着输入张量的50%会被随机置零。如果你想将其更改为20%，请在初始化调用中添加`p`参数：`Dropout(p=0.2)`。
- en: Note
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '`Dropout` should take place only during training. If it was happening during
    inference time, you’d lose a chunk of your network’s reasoning power, which is
    not what we want! Thankfully, PyTorch’s implementation of `Dropout` works out
    which mode you’re running in and passes all the data through the `Dropout` layer
    at inference time.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dropout` 应该只在训练期间发生。如果在推理时发生，你会失去网络推理能力的一部分，这不是我们想要的！幸运的是，PyTorch的`Dropout`实现会根据你运行的模式来确定，并在推理时通过`Dropout`层传递所有数据。'
- en: Having looked at our little CNN model and examined the layer types in depth,
    let’s take a look at other models that have been made in the past ten years.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看了我们的小型CNN模型并深入研究了层类型之后，让我们看看过去十年中制作的其他模型。
- en: History of CNN Architectures
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN架构的历史
- en: Although CNN models have been around for decades (LeNet-5 was used for digit
    recognition on check in the late 1990s, for example), it wasn’t until GPUs became
    widely available that deep CNN networks became practical. Even then, it has been
    only seven years since deep learning networks started to overwhelm all other existing
    approaches in image classification. In this section, we take a little journey
    back through the last few years to talk about some milestones in CNN-based learning
    and investigate some new techniques along the way.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CNN模型已经存在几十年了（例如，LeNet-5在1990年代末用于支票上的数字识别），但直到GPU变得广泛可用，深度CNN网络才变得实用。即使是在那时，深度学习网络开始压倒所有其他现有方法在图像分类中的应用也仅有七年。在本节中，我们将回顾过去几年的一些CNN学习里程碑，并探讨一些新技术。
- en: AlexNet
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlexNet
- en: '*AlexNet* was, in many ways, the architecture that changed everything. It was
    released in 2012 and destroyed all other entries in that year’s ImageNet competition
    with a top-5 error rate of 15.3% (the second place entry had a top-5 error of
    26.2%, just to give you an idea of how much better it was than other state-of-the-art
    methods). AlexNet was one of the first architectures to introduce the concepts
    of `MaxPool` and `Dropout`, and even popularize the then less-well-known `ReLU`
    activation function. It was one of the first architectures to demonstrate that
    many layers were possible and efficient to train on a GPU. Although it’s not state
    of the art anymore, it remains an important milestone in deep learning history.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*AlexNet* 在许多方面改变了一切。它于2012年发布，并在当年的ImageNet竞赛中以15.3%的前五错误率摧毁了所有其他参赛作品（第二名的前五错误率为26.2%，这让你了解了它比其他最先进方法好多少）。AlexNet是最早引入`MaxPool`和`Dropout`概念的架构之一，甚至推广了当时不太知名的`ReLU`激活函数。它是最早证明许多层次在GPU上训练是可能且高效的架构之一。虽然它不再是最先进的，但仍然是深度学习历史上的重要里程碑。'
- en: What does the AlexNet architecture look like? Aha, well, it’s time to let you
    in on a little secret. The network we’ve been using in this chapter so far? It’s
    AlexNet. Surprise! That’s why we used the standard `MaxPool2d` instead of `AdaptiveMaxPool2d`,
    to match the original AlexNet definition.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Inception/GoogLeNet
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s skip ahead to the winner of the 2014 ImageNet competition. The GoogLeNet
    architecture introduced the *Inception* module that addressed some of the deficiencies
    of AlexNet. In that network, the kernels of the convolutional layers are fixed
    at a certain resolution. We might expect that an image will have details that
    are important at both the macro- and microscale. It may be easier to determine
    whether an object is a car with a large kernel, but to determine whether it’s
    an SUV or a hatchback may require a smaller kernel. And to determine the model,
    we might need an even smaller kernel to make out details such as logos and insignias.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The Inception network instead runs a series of convolutions of different sizes
    all on the same input, and concatenates all of the filters together to pass on
    to the next layer. Before it does any of those, though, it does a 1 × 1 convolution
    as a *bottleneck* that compresses the input tensor, meaning that the 3 × 3 and
    5 × 5 kernels operate on a fewer number of filters than they would if the 1 ×
    1 convolution wasn’t present. You can see an Inception module illustrated in [Figure 3-2](#inception-module).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of an Inception module](assets/ppdl_0302.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. An Inception module
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The original GoogLeNet architecture uses nine of these modules stacked on top
    of each other, forming a deep network. Despite the depth, it uses fewer parameters
    overall than AlexNet while delivering a human-like performance of an 6.67% top-5
    error rate.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: VGG
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second-place entry in 2014’s ImageNet was from the University of Oxford—the
    Visual Geometry Group (VGG) network. In contrast to GoogLeNet, VGG is a simpler
    stack of convolutional layers. Coming in various configurations of longer stacks
    of convolutional filters combined with two large hidden linear layers before the
    final classification layer, it shows off the power of simple deep architectures
    (scoring an 8.8% top-5 error in its VGG-16 configuration). [Figure 3-3](#vgg-16)
    shows the layers of the VGG-16 from end to end.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The downside of the VGG approach is that the final fully connected layers make
    the network balloon to a large size, weighing in at 138 million parameters in
    comparison with GoogLeNet’s 7 million. Having said that, the VGG network is still
    quite popular in the deep learning world despite its huge size, as it’s easy to
    reason about because of its simpler construction and the early availability of
    trained weights. You’ll often see it used in style transfer applications (e.g.,
    turning a photo into a Van Gogh painting) as its combination of convolutional
    filters do appear to capture that sort of information in a way that’s easier to
    observe than the more complex networks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of VGG-16](assets/ppdl_0303.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. VGG-16
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ResNet
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A year later, Microsoft’s ResNet architecture won the ImageNet 2015 competition
    with a top-5 score of 4.49% in its ResNet-152 variant and 3.57% in an ensemble
    model (essentially beyond human ability at this point). The innovation that ResNet
    brought was an improvement on the Inception-style stacking bundle of layers approach,
    wherein each bundle performed the usual CNN operations but also added the incoming
    input to the output of the block, as shown in [Figure 3-4](#resnet-block).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of this set up is that each block passes through the original
    input to the next layer, allowing the “signal” of the training data to traverse
    through deeper networks than possible in either VGG or Inception. (This loss of
    weight changes in deep networks is known as a *vanishing gradient* because of
    the gradient changes in backpropagation tending to zero during the training process.)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a ResNet block](assets/ppdl_0304.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![一个ResNet块的图表](assets/ppdl_0304.png)'
- en: Figure 3-4\. A ResNet block
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4。一个ResNet块
- en: Other Architectures Are Available!
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他架构也是可用的！
- en: Since 2015 or so, plenty of other architectures have incrementally improved
    the accuracy on ImageNet, such as DenseNet (an extension of the ResNet idea that
    allows for the construction of 1,000-layer monster architectures), but also a
    lot of work has gone into creating architectures such as SqueezeNet and MobileNet,
    which offer reasonable accuracy but are tiny compared to architectures such as
    VGG, ResNet, or Inception.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 自2015年以来，许多其他架构已经逐步提高了在ImageNet上的准确性，例如DenseNet（ResNet思想的延伸，允许构建1,000层的庞大架构），但也有很多工作致力于创建像SqueezeNet和MobileNet这样的架构，它们提供了合理的准确性，但与VGG、ResNet或Inception等架构相比，它们要小得多。
- en: Another big area of research is getting neural networks to start designing neural
    networks themselves. The most successful attempt so far is, of course, from Google,
    whose AutoML system generated an architecture called *NASNet* that has a top-5
    error rate of 3.8% on ImageNet, which is state of the art as I type this at the
    start of 2019 (along with another autogenerated architecture from Google called
    *PNAS*). In fact, the organizers of the ImageNet competition have decided to call
    a halt to further competitions in this space because the architectures have already
    gone beyond human levels of ability.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的研究领域是让神经网络开始设计神经网络。到目前为止，最成功的尝试当然来自Google，他们的AutoML系统生成了一个名为*NASNet*的架构，在ImageNet上的前五错误率为3.8%，这是我在2019年初写这篇文章时的最新技术水平（还有另一个来自Google的自动生成架构称为*PNAS*）。事实上，ImageNet比赛的组织者已经决定停止在这个领域进行进一步的比赛，因为这些架构已经超越了人类的能力水平。
- en: That brings us to the state of the art as of the time this book goes to press,
    so let’s take a look at how we can use these models instead of defining our own.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到了这本书出版时的最新技术水平，所以让我们看看我们如何可以使用这些模型而不是定义我们自己的。
- en: Using Pretrained Models in PyTorch
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中使用预训练模型
- en: 'Obviously, having to define a model each time you want to use one would be
    a chore, especially once you move away from AlexNet, so PyTorch provides many
    of the most popular models by default in the `torchvision` library. For AlexNet,
    all you need to do is this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，每次想使用一个模型都要定义一个模型将是一件麻烦事，特别是一旦你远离AlexNet，所以PyTorch在`torchvision`库中默认提供了许多最受欢迎的模型。对于AlexNet，你只需要这样做：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Definitions for VGG, ResNet, Inception, DenseNet, and SqueezeNet variants are
    also available. That gives you the model definition, but you can also go a step
    further and call `models.alexnet(pretrained=True)` to download a pretrained set
    of weights for AlexNet, allowing you to use it immediately for classification
    with no extra training. (But as you’ll see in the next chapter, you will likely
    want to do some additional training to improve the accuracy on your particular
    dataset.)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: VGG、ResNet、Inception、DenseNet和SqueezeNet变体的定义也是可用的。这给了你模型的定义，但你也可以进一步调用`models.alexnet(pretrained=True)`来下载AlexNet的预训练权重，让你可以立即用它进行分类，无需额外的训练。（但正如你将在下一章中看到的那样，你可能需要进行一些额外的训练来提高你特定数据集上的准确性。）
- en: Having said that, there is something to be said for building the models yourself
    at least once to get a feel for how they fit together. It’s a good way to get
    some practice building model architectures within PyTorch, and of course you can
    compare with the provided models to make sure that what you come up with matches
    the actual definition. But how do you find out what that structure is?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，至少建立自己的模型一次是有必要的，这样你就能感受到它们如何组合在一起。这是一个很好的练习，在PyTorch中构建模型架构的方法，当然你也可以与提供的模型进行比较，以确保你所构建的与实际定义相匹配。但是你如何找出那个结构是什么呢？
- en: Examining a Model’s Structure
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查模型的结构
- en: 'If you’re curious about how one of these models is constructed, there’s an
    easy way to get PyTorch to help you out. As an example, here’s a look at the entire
    ResNet-18 architecture, which we get by simply calling the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对其中一个模型是如何构建的感到好奇，有一个简单的方法可以让PyTorch帮助你。例如，这里是整个ResNet-18架构的一个示例，我们只需调用以下内容：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There’s almost nothing here you haven’t already seen in this chapter, with the
    exception of `BatchNorm2d`. Let’s have a look at what that does in one of those
    layers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你几乎没有看到什么新东西，除了`BatchNorm2d`。让我们看看其中一个层中的作用。
- en: BatchNorm
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BatchNorm
- en: '*BatchNorm*, short for *batch normalization*, is a simple layer that has one
    task in life: using two learned parameters (meaning that it will be trained along
    with the rest of the network) to try to ensure that each minibatch that goes through
    the network has a mean centered around zero with a variance of 1\. You might ask
    why we need to do this when we’ve already normalized our input by using the transform
    chain in [Chapter 2](ch02.html#image-classification-with-pytorch). For smaller
    networks, `BatchNorm` is indeed less useful, but as they get larger, the effect
    of any layer on another, say 20 layers down, can be vast because of repeated multiplication,
    and you may end up with either vanishing or exploding gradients, both of which
    are fatal to the training process. The `BatchNorm` layers make sure that even
    if you use a model such as ResNet-152, the multiplications inside your network
    don’t get out of hand.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*BatchNorm*，即*批量归一化*，是一个简单的层，它的生活中只有一个任务：使用两个学习参数（意味着它将与网络的其余部分一起训练）来尝试确保通过网络的每个小批量具有以零为中心的均值和方差为1。你可能会问为什么我们需要这样做，当我们已经通过使用[第2章](ch02.html#image-classification-with-pytorch)中的变换链对输入进行了归一化。对于较小的网络，`BatchNorm`确实不太有用，但随着它们变得更大，任何一层对另一层的影响，比如说20层之后，可能会很大，因为重复的乘法，你可能会得到消失或爆炸的梯度，这两者对训练过程都是致命的。`BatchNorm`层确保即使你使用像ResNet-152这样的模型，你网络内部的乘法也不会失控。'
- en: 'You might be wondering: if we have `BatchNorm` in our network, why are we normalizing
    the input at all in the training loop’s transformation chain? After all, shouldn’t
    `BatchNorm` do the work for us? And the answer here is yes, you could do that!
    But it’ll take longer for the network to learn how to get the inputs under control,
    as they’ll have to discover the initial transform themselves, which will make
    training longer.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想：如果我们的网络中有`BatchNorm`，为什么在训练循环的转换链中还要对输入进行归一化呢？毕竟，`BatchNorm`不应该为我们做这项工作吗？答案是是的，您可以这样做！但网络将需要更长的时间来学习如何控制输入，因为它们将不得不自己发现初始转换，这将使训练时间更长。
- en: 'I recommend that you instantiate all of the architectures we’ve talked about
    so far and use `print(model)` to see which layers they use and in what order operations
    happen. After that, there’s another key question: *which of these architectures
    should I use?*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您实例化我们到目前为止讨论过的所有架构，并使用`print(model)`来查看它们使用的层以及操作发生的顺序。之后，还有另一个关键问题：*我应该使用这些架构中的哪一个？*
- en: Which Model Should You Use?
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 您应该使用哪个模型？
- en: The unhelpful answer is, whichever one works best for you, naturally! But let’s
    dig in a little. First, although I suggest that you try the NASNet and PNAS architectures
    at the moment, I wouldn’t wholeheartedly recommend them, despite their impressive
    results on ImageNet. They can be surprisingly memory-hungry in operation, and
    the *transfer learning* technique, which you learn about in [Chapter 4](ch04.html#transfer-learning-and-other-tricks),
    is not quite as effective compared to the human-built architectures including
    ResNet.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 没有帮助的答案是，自然是哪个对您最有效！但让我们深入一点。首先，尽管我建议您目前尝试NASNet和PNAS架构，但我不会全力推荐它们，尽管它们在ImageNet上取得了令人印象深刻的结果。它们在操作中可能会消耗大量内存，并且*迁移学习*技术（您将在[第4章](ch04.html#transfer-learning-and-other-tricks)中了解到）与人工构建的架构（包括ResNet）相比并不那么有效。
- en: I suggest that you have a look around the image-based competitions on [Kaggle](https://www.kaggle.com),
    a website that runs hundreds of data science competitions, and see what the winning
    entries are using. More than likely you’ll end up seeing a bunch of ResNet-based
    ensembles. Personally, I like and use the ResNet architectures over and above
    any of the others listed here, first because they offer good accuracy, and second
    because it’s easy to start out experimenting with a ResNet-34 model for fast iteration
    and then move to larger ResNets (and more realistically, an ensemble of different
    ResNet architectures, just as Microsoft used in their ImageNet win in 2015) once
    I feel I have something promising.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您在[Kaggle](https://www.kaggle.com)上浏览基于图像的比赛，这是一个举办数百个数据科学比赛的网站，看看获胜作品在使用什么。很可能您会看到一堆基于ResNet的集成模型。就我个人而言，我喜欢并使用ResNet架构，因为它们提供了良好的准确性，并且很容易从ResNet-34模型开始尝试实验，然后转向更大的ResNet（更现实地说，使用不同ResNet架构的集成模型，就像微软在2015年ImageNet比赛中使用的那样），一旦我觉得有所希望。
- en: Before we end the chapter, I have some breaking news concerning downloading
    pretrained models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，我有一些关于下载预训练模型的最新消息。
- en: 'One-Stop Shopping for Models: PyTorch Hub'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型一站式购物：PyTorch Hub
- en: 'A recent announcement in the PyTorch world provides an additional route to
    get models: *PyTorch Hub*. This is supposed to become a central location for obtaining
    any published model in the future, whether it’s for operating on images, text,
    audio, video, or any other type of data. To obtain a model in this fashion, you
    use the `torch.hub` module:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch世界最近的一项公告提供了另一种获取模型的途径：*PyTorch Hub*。这将成为未来获取任何已发布模型的中心位置，无论是用于处理图像、文本、音频、视频还是其他任何类型的数据。要以这种方式获取模型，您可以使用`torch.hub`模块：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first parameter points to a GitHub owner and repository (with an optional
    *tag/branch* identifier in the string as well); the second is the model requested
    (in this case, `resnet50`); and finally, the third indicates whether to download
    pretrained weights. You can also use `torch.hub.list('pytorch/vision')` to discover
    all the models inside that repository that are available to download.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数指向一个GitHub所有者和存储库（字符串中还可以包含可选的*标签/分支*标识符）；第二个是请求的模型（在本例中为`resnet50`）；最后一个指示是否下载预训练权重。您还可以使用`torch.hub.list('pytorch/vision')`来发现该存储库中可供下载的所有模型。
- en: PyTorch Hub is brand new as of mid-2019, so there aren’t a huge number of models
    available as I write this, but I expect it to become a popular way to distribute
    and download models by the end of the year. All the models in this chapter can
    be loaded through the `pytorch/vision` repo in PytorchHub, so feel free to use
    this loading process instead of `torchvision.models`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Hub是2019年中新推出的，所以在我写这篇文章时可用的模型数量并不多，但我预计到年底它将成为一个流行的模型分发和下载方式。本章中的所有模型都可以通过PytorchHub中的`pytorch/vision`存储库加载，所以可以随意使用这种加载过程，而不是`torchvision.models`。
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you’ve taken a quick walk-through of how CNN-based neural networks
    work, including features such as `Dropout`, `MaxPool`, and `BatchNorm`. You’ve
    also looked at the most popular architectures used in industry today. Before moving
    on to the next chapter, play with the architectures we’ve been talking about and
    see how they compare. (Don’t forget, you don’t need to train them! Just download
    the weights and test the model.)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，您已经快速了解了基于CNN的神经网络是如何工作的，包括`Dropout`、`MaxPool`和`BatchNorm`等特性。您还看了当今工业中最流行的架构。在继续下一章之前，尝试一下我们讨论过的架构，看看它们之间的比较。（不要忘记，您不需要训练它们！只需下载权重并测试模型。）
- en: We’re going to close out our look at computer vision by using these pretrained
    models as a starting point for a custom solution for our cats versus fish problem
    that uses *transfer learning*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用这些预训练模型作为我们猫对鱼问题的自定义解决方案的起点来结束我们对计算机视觉的探讨，这将使用*迁移学习*。
- en: Further Reading
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[AlexNet: “ImageNet Classification with Deep Convolutional Neural Networks”](https://oreil.ly/CsoFv)
    by Alex Krizhevsky et al. (2012)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AlexNet: “使用深度卷积神经网络进行ImageNet分类”](https://oreil.ly/CsoFv) 作者：Alex Krizhevsky
    等人（2012年）'
- en: '[VGG: “Very Deep Convolutional Networks for Large-Scale Image Recognition”](https://arxiv.org/abs/1409.1556)
    by Karen Simonyan and Andrew Zisserman (2014)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[VGG: “用于大规模图像识别的非常深的卷积网络”](https://arxiv.org/abs/1409.1556) 作者：Karen Simonyan
    和 Andrew Zisserman（2014年）'
- en: '[Inception: “Going Deeper with Convolutions”](https://arxiv.org/abs/1409.4842)
    by Christian Szegedy et al. (2014)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Inception: “使用卷积进行更深层次的研究”](https://arxiv.org/abs/1409.4842) 作者：Christian
    Szegedy 等人（2014年）'
- en: '[ResNet: “Deep Residual Learning for Image Recognition”](https://arxiv.org/abs/1512.03385)
    by Kaiming He et al. (2015)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ResNet: “用于图像识别的深度残差学习”](https://arxiv.org/abs/1512.03385) 作者：Kaiming He 等人（2015年）'
- en: '[NASNet: “Learning Transferable Architectures for Scalable Image Recognition”](https://arxiv.org/abs/1707.07012)
    by Barret Zoph et al. (2017)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NASNet: “学习可迁移的架构以实现可扩展的图像识别”](https://arxiv.org/abs/1707.07012) 作者：Barret
    Zoph 等人（2017年）'
- en: ^([1](ch03.html#idm45762366875032-marker)) Kernel and filter tend to be used
    interchangeably in the literature. If you have experience in graphics processing,
    kernel is probably more familiar to you, but I prefer filter.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#idm45762366875032-marker)) 在文献中，核和滤波器往往可以互换使用。如果您有图形处理经验，核可能更熟悉，但我更喜欢滤波器。
