- en: 4 Generating Cypher queries from natural language questions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从自然语言问题生成 Cypher 查询
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The basics of query language generation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询语言生成的基础知识
- en: Where query language generation fits in the RAG pipeline
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询语言生成在 RAG 管道中的位置
- en: Useful practices for query language generation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询语言生成的实用技巧
- en: Implementing a text2cypher retriever using a base model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基础模型实现 text2cypher 检索器
- en: Specialized (finetuned) LLMs for text2cypher
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于文本2cypher的专业（微调）LLM
- en: We’ve covered a lot of ground in the previous chapters. We’ve learned how to
    build a knowledge graph, extract information from text, and use that information
    to answer questions. We’ve also looked into how we can extend and improve plain
    vector search retrieval by using hardcoded Cypher queries to get more relevant
    context to the LLM. In this chapter, we will go a step further and learn how to
    generate Cypher queries from natural language questions. This will allow us to
    build a more flexible and dynamic retrieval system that can adapt to different
    types of questions and knowledge graphs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经覆盖了很多内容。我们学习了如何构建知识图谱，从文本中提取信息，并使用这些信息来回答问题。我们还探讨了如何通过使用硬编码的 Cypher
    查询来扩展和改进普通的向量搜索检索，从而为 LLM 获取更多相关的上下文。在本章中，我们将更进一步，学习如何从自然语言问题生成 Cypher 查询。这将使我们能够构建一个更灵活和动态的检索系统，能够适应不同类型的问题和知识图谱。
- en: Note  In the implementation of this chapter, we use what we call the “Movies
    dataset.” See the appendix for more information on the dataset and various ways
    to load it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本章的实现中，我们使用所谓的“电影数据集”。有关数据集的更多信息以及各种加载方式，请参阅附录。
- en: 4.1 The basics of query language generation
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 查询语言生成的基础知识
- en: When we talk about the basics of query language generation, we are referring
    to the process of converting a natural language question into a query language
    that can be executed on a database. More specifically, we are interested in generating
    Cypher queries from natural language questions. Most LLMs know what Cypher is
    and know the basic syntax of the language. The main challenge in this process
    is to generate a query that is both correct and relevant to the question being
    asked. This requires understanding the semantics of the question, as well as the
    schema of the knowledge graph being queried.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论查询语言生成的基础知识时，我们指的是将自然语言问题转换为可以在数据库上执行的语言的过程。更具体地说，我们感兴趣的是从自然语言问题生成 Cypher
    查询。大多数 LLM 都知道 Cypher 是什么，也知道该语言的基本语法。在这个过程中，主要挑战是生成一个既正确又与所提问题相关的查询。这需要理解问题的语义以及被查询的知识图谱的模式。
- en: If we don’t provide a schema of the knowledge graph, the LLM can only assume
    the names of nodes, relationships, and properties. When a schema is provided,
    it acts as a mapping between the semantics of the user question and the graph
    model used---which labels are being used on nodes, the relationship types that
    exist, the properties that are available, and which relationship types the nodes
    are connected to.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不提供知识图谱的模式，LLM 只能假设节点、关系和属性的名称。当提供模式时，它充当用户问题的语义与所使用的图模型之间的映射——节点上使用的标签、存在的关联类型、可用的属性以及节点连接到的关联类型。
- en: 'The workflow for generating Cypher queries from natural language questions
    can be broken down into the following steps (figure 4.1):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从自然语言问题生成 Cypher 查询的工作流程可以分解为以下步骤（图 4.1）：
- en: Retrieve the question from the user.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从用户那里检索问题。
- en: Retrieve the schema of the knowledge graph.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索知识图谱的模式。
- en: Define other useful information like terminology mappings, format instructions,
    and few-shot examples.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义其他有用的信息，如术语映射、格式说明和少量示例。
- en: Generate the prompt for the LLM.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 LLM 生成提示。
- en: Pass the prompt to the LLM to generate the Cypher query.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将提示传递给 LLM 以生成 Cypher 查询。
- en: '![figure](../Images/4-1.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-1.png)'
- en: Figure 4.1 Workflow for generating Cypher queries from natural language questions
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.1 从自然语言问题生成 Cypher 查询的工作流程
- en: 4.2 Where query language generation fits in the RAG pipeline
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 查询语言生成在 RAG 管道中的位置
- en: In earlier chapters, we’ve seen how we can get relevant responses from knowledge
    graphs by performing a vector similarity search on unstructured parts of the graphs.
    We’ve also seen how we can use vector similarity search extended with hardcoded
    Cypher queries to get more relevant context to the LLM. One limitation of these
    techniques is that they’re restricted in what type of questions they can answer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了如何通过在图的无结构部分执行向量相似度搜索来从知识图谱中获得相关响应。我们还看到了如何使用扩展了硬编码Cypher查询的向量相似度搜索来为LLM提供更多相关上下文。这些技术的局限性在于它们在可以回答的问题类型上受到限制。
- en: Consider the user question, “List the top three highest-rated movies directed
    by Steven Spielberg and their average score.” This can never be answered by a
    vector similarity search, as it requires a specific type of query to be executed
    on the database where the Cypher query could be something like the following (assuming
    a reasonable schema).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑用户问题：“列出由史蒂文·斯皮尔伯格执导的前三部评分最高的电影及其平均分。”这个问题永远不能通过向量相似度搜索来回答，因为它需要在数据库上执行特定类型的查询，Cypher查询可能如下所示（假设有合理的模式）。
- en: Listing 4.1 Cypher query
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.1 Cypher查询
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This query is not so much about the most similar nodes in the graph as aggregating
    data in a specific way. What this illustrates is that we want to use generated
    Cypher for certain types of queries---when we’re looking for things other than
    just the most similar nodes in the graph or when we want to aggregate data in
    some way. In the next chapter, we will look at how we can create an agentic system
    where we can provide multiple retrievers and use the most fitting one for each
    user question to be able to deliver the best response to the user.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询更多的是关于以特定方式聚合数据，而不是关于图中最相似的节点。这表明我们希望使用生成的Cypher来执行某些类型的查询——当我们寻找的不是图中最相似的节点，或者我们想要以某种方式聚合数据时。在下一章中，我们将探讨如何创建一个代理系统，我们可以提供多个检索器，并为每个用户问题选择最合适的一个，以便能够向用户提供最佳响应。
- en: Text2cypher could also function as a “catchall” retriever for the types of questions
    where there’s no real good match for any of the other retrievers in the system.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Text2cypher也可以作为“万能”检索器，用于那些在系统中没有其他检索器能提供良好匹配的问题类型。
- en: 4.3 Useful practices for query language generation
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 查询语言生成的实用技巧
- en: When generating Cypher queries from natural language questions, there are a
    few things to keep in mind to ensure that the generated queries are correct and
    relevant. The LLMs tend to make mistakes when generating Cypher queries, especially
    when the input questions are complex or ambiguous or if the database schema elements
    aren’t semantically named.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当从自然语言问题生成Cypher查询时，有一些事情需要考虑，以确保生成的查询是正确且相关的。LLM在生成Cypher查询时容易出错，尤其是当输入问题复杂或含糊不清，或者数据库模式元素没有语义命名时。
- en: 4.3.1 Using few-shot examples for in-context learning
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 使用少量示例进行上下文学习
- en: Few-shot examples are a great way to improve the performance of LLMs for text2cypher.
    What this means is that we can provide the LLM with a few examples of questions
    and their corresponding Cypher queries, and the LLM will learn to generate similar
    queries for new questions. In contrast, zero-shot examples are when we don’t provide
    any examples to the LLM, and it has to generate the query with no hints at all.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 少量示例是提高LLM在text2cypher中性能的绝佳方式。这意味着我们可以向LLM提供一些问题和它们相应的Cypher查询的示例，LLM将学会为新的问题生成类似的查询。相比之下，零示例是在我们不向LLM提供任何示例的情况下，它必须在没有任何提示的情况下生成查询。
- en: The few-shot examples are specific to the knowledge graph being queried, so
    they need to be created manually for each knowledge graph. This is very useful
    when you recognize that the LLM misinterprets the schema or often makes the same
    type of mistake (expects a property when it should be a traversal, etc.).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 几个示例是针对查询的知识图谱特定的，因此需要为每个知识图谱手动创建。这在您意识到LLM误解了模式或经常犯相同类型的错误（期望一个属性而实际上应该是遍历等）时非常有用。
- en: 'Let’s assume that you detect that the LLM is trying to read the country of
    production of a movie, and it’s looking for a property on the movie node, but
    the country is actually a node in the graph. You can then add a few-shot example
    to the prompt to let the LLM know how to get the country name:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您检测到LLM正在尝试读取电影的制作国家，并且它在电影节点上寻找一个属性，但实际上国家是图中的一个节点。然后您可以在提示中添加少量示例，让LLM知道如何获取国家名称：
- en: In what country was the movie *The Matrix* produced?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 电影《黑客帝国》是在哪个国家制作的？
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This would be fixed by adding the following to the few-shot examples in the
    prompt to the LLM:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过在提示LLM的几个示例中添加以下内容来解决：
- en: In what country was the movie *The Matrix* produced?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 电影《黑客帝国》是在哪个国家制作的？
- en: Examples
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: 'Question: In what country was the movie *Ready Player One* produced?'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：电影《头号玩家》是在哪个国家制作的？
- en: 'Cypher: MATCH (m:Movie { title: ''Ready Player One'' })-[:PRODUCED_IN]→(c:Country)
    RETURN c.name'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cypher：MATCH (m:Movie { title: ''头号玩家'' })-[:PRODUCED_IN]→(c:Country) RETURN
    c.name'
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This would not only fix the issue for this specific question but also for similar
    questions now that we have a clear example to let the LLM see a pattern to get
    a country name.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅会解决这个具体问题，而且由于我们现在有一个清晰的例子让LLM看到模式以获取国家名称，所以也会解决类似的问题。
- en: 4.3.2 Using database schema in the prompt to show the LLM the structure of the
    knowledge graph
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 在提示中使用数据库模式向LLM展示知识图谱的结构
- en: The schema of the knowledge graph is crucial for generating correct Cypher queries.
    There are several ways to describe the knowledge graph schema to an LLM, and according
    to our internal research at Neo4j, the format doesn’t matter that much.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱的模式对于生成正确的Cypher查询至关重要。有几种方式可以向LLM描述知识图谱模式，根据Neo4j内部的研究，格式并不那么重要。
- en: 'The schema should be part of the prompt and make a clear case about what labels,
    relationship types, and properties are available in the graph:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模式应该是提示的一部分，并清楚地说明图中可用的标签、关系类型和属性：
- en: 'Graph database schema:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据库模式：
- en: Use only the provided relationship types and properties in the schema. Do not
    use any other relationship types or properties that are not provided in the schema.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在模式中提供的关系类型和属性中使用。不要使用模式中未提供的任何其他关系类型或属性。
- en: 'Node labels and properties:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 节点标签和属性：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Relationship types and properties:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 关系类型和属性：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The relationships:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 关系：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Whether you want to expose the complete knowledge graph to be queried or not
    might depend on how large the schema is and if it’s relevant for the use case.
    To automatically infer the schema from Neo4j could be expensive, depending on
    the size of the data, so it’s common to sample the database and infer the schema
    from that.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否希望公开完整的知识图谱以进行查询，可能取决于模式的大小以及它是否与用例相关。自动从Neo4j推断模式可能很昂贵，这取决于数据的大小，因此通常从数据库中采样并从中推断模式是常见的做法。
- en: To infer the schema from Neo4j, we currently need to use procedures from the
    APOC library that’s free and available both within Neo4j’s SaaS offering Aura
    and in the other distributions of Neo4j. The following listing shows how you can
    infer the schema from a Neo4j database.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Neo4j推断模式，我们目前需要使用APOC库中的过程，该库免费且在Neo4j的SaaS产品Aura和其他Neo4j发行版中均可用。以下列表显示了如何从Neo4j数据库中推断模式。
- en: 'Tip  You can read more about APOC here: [https://neo4j.com/docs/apoc/](https://neo4j.com/docs/apoc/).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：您可以在[https://neo4j.com/docs/apoc/](https://neo4j.com/docs/apoc/)了解更多关于APOC的信息。
- en: Listing 4.2 Inferring schema from Neo4j
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.2 从Neo4j推断模式
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With these queries, we can now get the schema of the graph database and use
    it in the prompt to the LLM. Let’s run the queries and store the result in a structured
    way so we can generate the previous schema string later.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些查询，我们现在可以获取图数据库的模式并将其用于提示LLM。让我们运行这些查询并以结构化的方式存储结果，这样我们就可以稍后生成前面的模式字符串。
- en: Listing 4.3 Running the schema inference queries
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.3 运行模式推断查询
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With this structured response in place, we can format the schema string as we
    want, and it’s also easy for us to explore and experiment with different formats
    in the prompt.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个结构化响应到位后，我们可以按需格式化模式字符串，并且我们也很容易在提示中探索和实验不同的格式。
- en: To get the format illustrated earlier in this chapter, we can use the function
    shown in the following listing.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得本章前面展示的格式，我们可以使用以下列表中显示的函数。
- en: Listing 4.4 Formatting the schema string
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.4 格式化模式字符串
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With this function, we can now generate the schema string that we can use in
    the prompt to the LLM.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数，我们现在可以生成可以用于提示LLM的模式字符串。
- en: 4.3.3 Adding terminology mapping to semantically map the user question to the
    schema
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 添加术语映射以语义地将用户问题映射到模式
- en: The LLM needs to know how to map the terminology used in the question to the
    terminology used in the schema. A well-designed graph schema uses nouns and verbs
    for labels and relationship types and adjectives and nouns for properties. Even
    if that’s the case, the LLMs can sometimes get confused about what to use where.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: LLM需要知道如何将问题中使用的术语映射到模式中使用的术语。一个设计良好的图模式使用名词和动词作为标签和关系类型，以及形容词和名词作为属性。即使如此，LLM有时也可能不清楚在哪里使用什么。
- en: Note  These mappings are knowledge graph specific and should be part of the
    prompt; they would be hard to reuse between different knowledge graphs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这些映射是知识图谱特定的，应该作为提示的一部分；它们在不同知识图谱之间难以重用。
- en: The terminology mappings are something that probably will evolve over time as
    you detect problems with the generated queries due to the LLM not understanding
    the schema correctly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 术语映射可能是随着时间的推移而演变的东西，因为当你发现由于LLM没有正确理解模式而导致的生成查询问题时。
- en: 'TERMINOLOGY MAPPING:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 术语映射：
- en: 'Persons: When a user asks about a person by trade, they are referring to a
    node with the label Person. Movies: When a user asks about a film or movie, they
    are referring to a node with the label Movie.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 人物：当用户询问一个职业人物时，他们指的是具有Person标签的节点。电影：当用户询问一部电影或电影时，他们指的是具有Movie标签的节点。
- en: 4.3.4 Format instructions
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.4 格式说明
- en: Different LLMs output the response in different ways. Some of them put code
    tags around the Cypher query, and some of them don’t. Some of them add text before
    the Cypher query; some of them don’t, etc.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的LLM以不同的方式输出响应。其中一些在Cypher查询周围添加代码标签，而另一些则没有。一些在Cypher查询之前添加文本；而另一些则没有，等等。
- en: To have them all output the same way, you can add format instructions to the
    prompt. Useful instructions are to try to get the LLMs to only output the Cypher
    query and nothing else.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要使它们都以相同的方式输出，你可以在提示中添加格式说明。有用的说明是尝试让LLM只输出Cypher查询，而不输出其他任何内容。
- en: 'FORMAT INSTRUCTIONS:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 格式说明：
- en: Do not include any explanations or apologies in your responses. Do not respond
    to any questions that might ask anything else than for you to construct a Cypher
    statement. Do not include any text except the generated Cypher statement. ONLY
    RESPOND WITH CYPHER, NO CODE BLOCKS.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 不要在响应中包含任何解释或道歉。不要回答任何可能要求你构建Cypher语句之外的问题。不要包含任何文本，除了生成的Cypher语句。只以CYPHER回答，不要包含代码块。
- en: 4.4 Implementing a text2cypher generator using a base model
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 使用基础模型实现text2cypher生成器
- en: Let’s put all of this into practice and implement a text2cypher generator using
    a base model. The task here is basically forming a prompt that includes the schema,
    terminology mappings, format instructions, and few-shot examples to make our intention
    clear to the LLM.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有这些应用到实践中，并使用基础模型实现一个text2cypher生成器。这里的任务基本上是形成一个包含模式、术语映射、格式说明和少量示例的提示，以便向LLM明确我们的意图。
- en: 'In the remainder of this chapter, we will implement a text2cypher generator
    using the Neo4j Python driver and the OpenAI API. To follow along, you’ll need
    access to a running, blank Neo4j instance. This can be a local installation or
    a cloud-hosted instance; just make sure it’s empty. You can follow the implementation
    directly in the accompanying Jupyter notebook available here: [https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch04.ipynb](https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch04.ipynb).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将使用Neo4j Python驱动程序和OpenAI API实现一个text2cypher生成器。为了跟随，你需要访问一个运行中的、空白的Neo4j实例。这可以是一个本地安装或云托管实例；只需确保它是空的。你可以直接在附带的Jupyter笔记本中跟随实现，笔记本地址如下：[https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch04.ipynb](https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch04.ipynb)。
- en: Let’s dive in.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨。
- en: Listing 4.5 Prompt template
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.5 提示模板
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With this prompt template, we can now generate the prompt for the LLM. Let’s
    assume we have the following user question, schema, terminology mappings, and
    few-shot examples.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个提示模板，我们现在可以为LLM生成提示。假设我们有一个以下用户问题、模式、术语映射和少量示例。
- en: Listing 4.6 Full prompt example
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.6 完整提示示例
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If we execute this example, the prompt output would look like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们执行这个示例，提示输出将看起来像这样：
- en: 'Instructions: Generate Cypher statement to query a graph database to get the
    data to answer the following user question.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 说明：生成Cypher语句以查询图数据库以获取回答以下用户问题的数据。
- en: 'Graph database schema: Use only the provided relationship types and properties
    in the schema. Do not use any other relationship types or properties that are
    not provided in the schema. Node properties:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据库模式：仅使用模式中提供的关系类型和属性。不要使用模式中未提供的任何其他关系类型或属性。节点属性：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Relationship properties:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 关系属性：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The relationships:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 关系：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Terminology mapping: This section is helpful to map terminology between the
    user question and the graph database schema.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 术语映射：本节有助于在用户问题和图数据库模式之间映射术语。
- en: 'Persons: When a user asks about a person by trade like actor, writer, director,
    producer, or reviewer, they are referring to a node with the label ''Person''.
    Movies: When a user asks about a film or movie, they are referring to a node with
    the label Movie.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 人物：当用户询问像演员、作家、导演、制片人或评论家这样的职业人物时，他们指的是带有标签'Person'的节点。电影：当用户询问电影或影片时，他们指的是带有标签Movie的节点。
- en: 'Examples: The following examples provide useful patterns for querying the graph
    database. Question: Who are the two people who have acted in the most movies together?'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：以下示例提供了查询图数据库的有用模式。问题：哪两位演员共同出演了最多的电影？
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Format instructions: Do not include any explanations or apologies in your responses.
    Do not respond to any questions that might ask anything else than for you to construct
    a Cypher statement. Do not include any text except the generated Cypher statement.
    ONLY RESPOND WITH CYPHER—NO CODE BLOCKS.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 格式说明：在您的回答中不要包含任何解释或道歉。不要回答任何可能要求您构建Cypher语句之外的问题。不要包含任何除生成的Cypher语句之外的文字。只回答CYPHER——不要使用代码块。
- en: 'User question: Who has directed the most movies?'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 用户问题：谁执导的电影最多？
- en: With this prompt, we can now generate the Cypher query for the user’s question.
    You can try this by copying the prompt to an LLM and see what it generates.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个提示，我们现在可以生成用户问题的Cypher查询。您可以尝试将提示复制到LLM中，看看它生成什么。
- en: Listing 4.7 Cypher query generated
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.7 生成的Cypher查询
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 4.5 Specialized (finetuned) LLMs for text2cypher
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 专为text2cypher优化的（微调的）LLM
- en: At Neo4j, we are continuously working on improving the performance of our LLMs
    for text2cypher via finetuning. Our open source training data at Hugging Face
    is available at [https://huggingface.co/datasets/neo4j/text2cypher](https://huggingface.co/datasets/neo4j/text2cypher).
    We also provide finetuned models based on open source LLMs (like Gemma2, Llama
    3.1) at [https://huggingface.co/neo4j](https://huggingface.co/neo4j).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在Neo4j，我们正在通过微调不断改进text2cypher的LLM性能。我们Hugging Face上的开源训练数据可在[https://huggingface.co/datasets/neo4j/text2cypher](https://huggingface.co/datasets/neo4j/text2cypher)找到。我们还提供基于开源LLM（如Gemma2、Llama
    3.1）的微调模型，可在[https://huggingface.co/neo4j](https://huggingface.co/neo4j)找到。
- en: These models are still pretty far behind the performance of finetuned larger
    models like the latest GPT and Gemini models, but they are much more efficient
    and can be used in production systems where the larger models are too slow. Go
    ahead and try them out and refer back to the few-shot examples, schema, terminology
    mappings, and format instructions to improve the performance of the models. There’s
    more information about our finetuning process and learnings at [https://mng.bz/MwDW](https://mng.bz/MwDW),
    [https://mng.bz/a9v7](https://mng.bz/a9v7), and [https://mng.bz/yNWB](https://mng.bz/yNWB).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型在性能上仍然远远落后于像最新GPT和Gemini模型这样的微调大型模型，但它们效率更高，可以在大型模型太慢的生产系统中使用。大胆尝试它们，并参考少量示例、模式、术语映射和格式说明来提高模型性能。有关我们的微调过程和学习，更多信息请参阅[https://mng.bz/MwDW](https://mng.bz/MwDW)、[https://mng.bz/a9v7](https://mng.bz/a9v7)和[https://mng.bz/yNWB](https://mng.bz/yNWB)。
- en: 4.6 What we’ve learned and what text2cypher enables
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 我们学到的东西以及text2cypher能做什么
- en: With the code and information in this chapter, you should be able to implement
    a text2cypher retriever for your knowledge graph. You should be able to get it
    to generate correct Cypher queries for a wide range of questions, and to improve
    its performance by providing it with few-shot examples, schema, terminology mappings,
    and format instructions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的代码和信息的基础上，您应该能够为您的知识图谱实现一个text2cypher检索器。您应该能够让它为广泛的问题生成正确的Cypher查询，并通过提供少量示例、模式、术语映射和格式说明来提高其性能。
- en: As you identify the types of questions it struggles with, you can add more few-shot
    examples to the prompt to help it learn how to generate the correct queries. Over
    time, you will notice that the quality of the generated queries improves and that
    the retriever becomes more reliable.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你识别出它难以应对的问题类型，你可以向提示中添加更多少样本示例，以帮助它学习如何生成正确的查询。随着时间的推移，你会发现生成的查询质量有所提高，检索器变得更加可靠。
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Query language generation fits in well with the RAG pipeline as a complement
    to other retrieval methods, especially when we want to aggregate data or get specific
    data from the graph.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询语言生成与RAG（Retrieval-Augmented Generation）管道很好地结合，作为其他检索方法的补充，尤其是在我们想要聚合数据或从图中获取特定数据时。
- en: Useful practices for query language generation include using few-shot examples,
    schema, terminology mappings, and format instructions.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询语言生成的有用实践包括使用少样本示例、模式、术语映射和格式说明。
- en: We can implement a text2cypher retriever using a base model and structure the
    prompt to the LLM.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用基础模型实现一个文本到Cypher的检索器，并将提示结构化到LLM中。
- en: We can use specialized (finetuned) LLMs for text2cypher and improve their performance.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用专门（微调）的LLM（大型语言模型）进行文本到Cypher的转换，并提高其性能。
