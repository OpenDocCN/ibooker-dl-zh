- en: 4 Generating Cypher queries from natural language questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basics of query language generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where query language generation fits in the RAG pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful practices for query language generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a text2cypher retriever using a base model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized (finetuned) LLMs for text2cypher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve covered a lot of ground in the previous chapters. We’ve learned how to
    build a knowledge graph, extract information from text, and use that information
    to answer questions. We’ve also looked into how we can extend and improve plain
    vector search retrieval by using hardcoded Cypher queries to get more relevant
    context to the LLM. In this chapter, we will go a step further and learn how to
    generate Cypher queries from natural language questions. This will allow us to
    build a more flexible and dynamic retrieval system that can adapt to different
    types of questions and knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Note  In the implementation of this chapter, we use what we call the “Movies
    dataset.” See the appendix for more information on the dataset and various ways
    to load it.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 The basics of query language generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we talk about the basics of query language generation, we are referring
    to the process of converting a natural language question into a query language
    that can be executed on a database. More specifically, we are interested in generating
    Cypher queries from natural language questions. Most LLMs know what Cypher is
    and know the basic syntax of the language. The main challenge in this process
    is to generate a query that is both correct and relevant to the question being
    asked. This requires understanding the semantics of the question, as well as the
    schema of the knowledge graph being queried.
  prefs: []
  type: TYPE_NORMAL
- en: If we don’t provide a schema of the knowledge graph, the LLM can only assume
    the names of nodes, relationships, and properties. When a schema is provided,
    it acts as a mapping between the semantics of the user question and the graph
    model used---which labels are being used on nodes, the relationship types that
    exist, the properties that are available, and which relationship types the nodes
    are connected to.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow for generating Cypher queries from natural language questions
    can be broken down into the following steps (figure 4.1):'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve the question from the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve the schema of the knowledge graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define other useful information like terminology mappings, format instructions,
    and few-shot examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate the prompt for the LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass the prompt to the LLM to generate the Cypher query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/4-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 Workflow for generating Cypher queries from natural language questions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.2 Where query language generation fits in the RAG pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In earlier chapters, we’ve seen how we can get relevant responses from knowledge
    graphs by performing a vector similarity search on unstructured parts of the graphs.
    We’ve also seen how we can use vector similarity search extended with hardcoded
    Cypher queries to get more relevant context to the LLM. One limitation of these
    techniques is that they’re restricted in what type of questions they can answer.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the user question, “List the top three highest-rated movies directed
    by Steven Spielberg and their average score.” This can never be answered by a
    vector similarity search, as it requires a specific type of query to be executed
    on the database where the Cypher query could be something like the following (assuming
    a reasonable schema).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 Cypher query
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This query is not so much about the most similar nodes in the graph as aggregating
    data in a specific way. What this illustrates is that we want to use generated
    Cypher for certain types of queries---when we’re looking for things other than
    just the most similar nodes in the graph or when we want to aggregate data in
    some way. In the next chapter, we will look at how we can create an agentic system
    where we can provide multiple retrievers and use the most fitting one for each
    user question to be able to deliver the best response to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Text2cypher could also function as a “catchall” retriever for the types of questions
    where there’s no real good match for any of the other retrievers in the system.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Useful practices for query language generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When generating Cypher queries from natural language questions, there are a
    few things to keep in mind to ensure that the generated queries are correct and
    relevant. The LLMs tend to make mistakes when generating Cypher queries, especially
    when the input questions are complex or ambiguous or if the database schema elements
    aren’t semantically named.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Using few-shot examples for in-context learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Few-shot examples are a great way to improve the performance of LLMs for text2cypher.
    What this means is that we can provide the LLM with a few examples of questions
    and their corresponding Cypher queries, and the LLM will learn to generate similar
    queries for new questions. In contrast, zero-shot examples are when we don’t provide
    any examples to the LLM, and it has to generate the query with no hints at all.
  prefs: []
  type: TYPE_NORMAL
- en: The few-shot examples are specific to the knowledge graph being queried, so
    they need to be created manually for each knowledge graph. This is very useful
    when you recognize that the LLM misinterprets the schema or often makes the same
    type of mistake (expects a property when it should be a traversal, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume that you detect that the LLM is trying to read the country of
    production of a movie, and it’s looking for a property on the movie node, but
    the country is actually a node in the graph. You can then add a few-shot example
    to the prompt to let the LLM know how to get the country name:'
  prefs: []
  type: TYPE_NORMAL
- en: In what country was the movie *The Matrix* produced?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This would be fixed by adding the following to the few-shot examples in the
    prompt to the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: In what country was the movie *The Matrix* produced?
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: In what country was the movie *Ready Player One* produced?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cypher: MATCH (m:Movie { title: ''Ready Player One'' })-[:PRODUCED_IN]→(c:Country)
    RETURN c.name'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This would not only fix the issue for this specific question but also for similar
    questions now that we have a clear example to let the LLM see a pattern to get
    a country name.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Using database schema in the prompt to show the LLM the structure of the
    knowledge graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The schema of the knowledge graph is crucial for generating correct Cypher queries.
    There are several ways to describe the knowledge graph schema to an LLM, and according
    to our internal research at Neo4j, the format doesn’t matter that much.
  prefs: []
  type: TYPE_NORMAL
- en: 'The schema should be part of the prompt and make a clear case about what labels,
    relationship types, and properties are available in the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph database schema:'
  prefs: []
  type: TYPE_NORMAL
- en: Use only the provided relationship types and properties in the schema. Do not
    use any other relationship types or properties that are not provided in the schema.
  prefs: []
  type: TYPE_NORMAL
- en: 'Node labels and properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Relationship types and properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Whether you want to expose the complete knowledge graph to be queried or not
    might depend on how large the schema is and if it’s relevant for the use case.
    To automatically infer the schema from Neo4j could be expensive, depending on
    the size of the data, so it’s common to sample the database and infer the schema
    from that.
  prefs: []
  type: TYPE_NORMAL
- en: To infer the schema from Neo4j, we currently need to use procedures from the
    APOC library that’s free and available both within Neo4j’s SaaS offering Aura
    and in the other distributions of Neo4j. The following listing shows how you can
    infer the schema from a Neo4j database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip  You can read more about APOC here: [https://neo4j.com/docs/apoc/](https://neo4j.com/docs/apoc/).'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 Inferring schema from Neo4j
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: With these queries, we can now get the schema of the graph database and use
    it in the prompt to the LLM. Let’s run the queries and store the result in a structured
    way so we can generate the previous schema string later.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 Running the schema inference queries
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With this structured response in place, we can format the schema string as we
    want, and it’s also easy for us to explore and experiment with different formats
    in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: To get the format illustrated earlier in this chapter, we can use the function
    shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 Formatting the schema string
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With this function, we can now generate the schema string that we can use in
    the prompt to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Adding terminology mapping to semantically map the user question to the
    schema
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The LLM needs to know how to map the terminology used in the question to the
    terminology used in the schema. A well-designed graph schema uses nouns and verbs
    for labels and relationship types and adjectives and nouns for properties. Even
    if that’s the case, the LLMs can sometimes get confused about what to use where.
  prefs: []
  type: TYPE_NORMAL
- en: Note  These mappings are knowledge graph specific and should be part of the
    prompt; they would be hard to reuse between different knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: The terminology mappings are something that probably will evolve over time as
    you detect problems with the generated queries due to the LLM not understanding
    the schema correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'TERMINOLOGY MAPPING:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Persons: When a user asks about a person by trade, they are referring to a
    node with the label Person. Movies: When a user asks about a film or movie, they
    are referring to a node with the label Movie.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Format instructions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different LLMs output the response in different ways. Some of them put code
    tags around the Cypher query, and some of them don’t. Some of them add text before
    the Cypher query; some of them don’t, etc.
  prefs: []
  type: TYPE_NORMAL
- en: To have them all output the same way, you can add format instructions to the
    prompt. Useful instructions are to try to get the LLMs to only output the Cypher
    query and nothing else.
  prefs: []
  type: TYPE_NORMAL
- en: 'FORMAT INSTRUCTIONS:'
  prefs: []
  type: TYPE_NORMAL
- en: Do not include any explanations or apologies in your responses. Do not respond
    to any questions that might ask anything else than for you to construct a Cypher
    statement. Do not include any text except the generated Cypher statement. ONLY
    RESPOND WITH CYPHER, NO CODE BLOCKS.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Implementing a text2cypher generator using a base model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s put all of this into practice and implement a text2cypher generator using
    a base model. The task here is basically forming a prompt that includes the schema,
    terminology mappings, format instructions, and few-shot examples to make our intention
    clear to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the remainder of this chapter, we will implement a text2cypher generator
    using the Neo4j Python driver and the OpenAI API. To follow along, you’ll need
    access to a running, blank Neo4j instance. This can be a local installation or
    a cloud-hosted instance; just make sure it’s empty. You can follow the implementation
    directly in the accompanying Jupyter notebook available here: [https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch04.ipynb](https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch04.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive in.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 Prompt template
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With this prompt template, we can now generate the prompt for the LLM. Let’s
    assume we have the following user question, schema, terminology mappings, and
    few-shot examples.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 Full prompt example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If we execute this example, the prompt output would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instructions: Generate Cypher statement to query a graph database to get the
    data to answer the following user question.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph database schema: Use only the provided relationship types and properties
    in the schema. Do not use any other relationship types or properties that are
    not provided in the schema. Node properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Relationship properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Terminology mapping: This section is helpful to map terminology between the
    user question and the graph database schema.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Persons: When a user asks about a person by trade like actor, writer, director,
    producer, or reviewer, they are referring to a node with the label ''Person''.
    Movies: When a user asks about a film or movie, they are referring to a node with
    the label Movie.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples: The following examples provide useful patterns for querying the graph
    database. Question: Who are the two people who have acted in the most movies together?'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Format instructions: Do not include any explanations or apologies in your responses.
    Do not respond to any questions that might ask anything else than for you to construct
    a Cypher statement. Do not include any text except the generated Cypher statement.
    ONLY RESPOND WITH CYPHER—NO CODE BLOCKS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'User question: Who has directed the most movies?'
  prefs: []
  type: TYPE_NORMAL
- en: With this prompt, we can now generate the Cypher query for the user’s question.
    You can try this by copying the prompt to an LLM and see what it generates.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.7 Cypher query generated
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 4.5 Specialized (finetuned) LLMs for text2cypher
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At Neo4j, we are continuously working on improving the performance of our LLMs
    for text2cypher via finetuning. Our open source training data at Hugging Face
    is available at [https://huggingface.co/datasets/neo4j/text2cypher](https://huggingface.co/datasets/neo4j/text2cypher).
    We also provide finetuned models based on open source LLMs (like Gemma2, Llama
    3.1) at [https://huggingface.co/neo4j](https://huggingface.co/neo4j).
  prefs: []
  type: TYPE_NORMAL
- en: These models are still pretty far behind the performance of finetuned larger
    models like the latest GPT and Gemini models, but they are much more efficient
    and can be used in production systems where the larger models are too slow. Go
    ahead and try them out and refer back to the few-shot examples, schema, terminology
    mappings, and format instructions to improve the performance of the models. There’s
    more information about our finetuning process and learnings at [https://mng.bz/MwDW](https://mng.bz/MwDW),
    [https://mng.bz/a9v7](https://mng.bz/a9v7), and [https://mng.bz/yNWB](https://mng.bz/yNWB).
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 What we’ve learned and what text2cypher enables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the code and information in this chapter, you should be able to implement
    a text2cypher retriever for your knowledge graph. You should be able to get it
    to generate correct Cypher queries for a wide range of questions, and to improve
    its performance by providing it with few-shot examples, schema, terminology mappings,
    and format instructions.
  prefs: []
  type: TYPE_NORMAL
- en: As you identify the types of questions it struggles with, you can add more few-shot
    examples to the prompt to help it learn how to generate the correct queries. Over
    time, you will notice that the quality of the generated queries improves and that
    the retriever becomes more reliable.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Query language generation fits in well with the RAG pipeline as a complement
    to other retrieval methods, especially when we want to aggregate data or get specific
    data from the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful practices for query language generation include using few-shot examples,
    schema, terminology mappings, and format instructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can implement a text2cypher retriever using a base model and structure the
    prompt to the LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use specialized (finetuned) LLMs for text2cypher and improve their performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
