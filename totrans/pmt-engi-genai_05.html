<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Vector Databases with FAISS and Pinecone"><div class="chapter" id="vector_databases_05">
<h1><span class="label">Chapter 5. </span>Vector Databases with FAISS and Pinecone</h1>


<p>This chapter introduces the concept of embeddings and vector databases, discussing how they can be used to provide relevant context in prompts.</p>

<p>A <em>vector database</em> is a tool most commonly <a data-type="indexterm" data-primary="vector databases" id="id930"/>used for storing text data in a way that enables querying based on similarity or semantic meaning. This technology is used to decrease hallucinations (where the AI model makes something up) by referencing data the model isn’t trained on, significantly improving the accuracy and quality of the LLM’s response. Use cases for vector databases also include reading documents, recommending similar products, or remembering past conversations.</p>

<p><em>Vectors</em> are lists of numbers <a data-type="indexterm" data-primary="vectors" id="id931"/>representing text (or images), which you might think of as coordinates for a location. The vector for the word <em>mouse</em> using OpenAI’s text-embedding-ada-002 model is a list of 1,536 numbers, each representing the value for a feature the embedding model learned in training:</p>

<pre data-type="programlisting">[-0.011904156766831875,
 -0.0323905423283577,
 0.001950666424818337,
...]</pre>

<p>When these models are trained, texts that appear together in the training data will be pushed closer together in values, and texts that are unrelated will be pushed further away. Imagine we trained a simple model with only two parameters, <code>Cartoon</code> and <code>Hygiene</code>, that must describe the entire world, but only in terms of these two variables. Starting from the word <em>mouse</em>, increasing the value for the parameter <code>Cartoon</code> we would travel toward the most famous cartoon mouse, <code>mickey mouse</code>, as shown in <a data-type="xref" href="#figure-5-1">Figure 5-1</a>. Decreasing the value for the <code>Hygiene</code> parameter would take us toward <code>rat</code>, because rats are rodents similar to mice, but are associated with plague and disease (i.e., being unhygenic).</p>

<figure><div id="figure-5-1" class="figure">
<img src="assets/pega_0501.png" alt="pega 0501" width="600" height="434"/>
<h6><span class="label">Figure 5-1. </span>2-D vector distances</h6>
</div></figure>

<p>Each location on the graph can be found by two numbers on the x- and y-axes, which represent the features of the model <code>Cartoon</code> and <code>Hygiene</code>. In reality, vectors can have thousands of <a data-type="indexterm" data-primary="vectors" data-secondary="parameters" id="id932"/>parameters, because having more parameters allows the model to capture a wider range of similarities and differences. Hygiene is not the only difference between mice and rats, and Mickey Mouse isn’t just a cartoon mouse. These features are learned from the data in a way that makes them hard for humans to interpret, and we would need a graph <a data-type="indexterm" data-primary="vector databases" data-secondary="latent space" id="id933"/><a data-type="indexterm" data-primary="latent space" id="id934"/>with thousands of axes to display a location in <em>latent space</em> (the abstract multidimensional space formed by the model’s parameters). Often there is no human-understandable explanation of what a feature means. However, we can create a simplified two-dimensional projection of the distances between vectors, as has been done in <a data-type="xref" href="#figure-5-2">Figure 5-2</a>.</p>

<p>To conduct a vector search, you first get <a data-type="indexterm" data-primary="vector databases" data-secondary="k closest records" id="id935"/>the vector (or location) of what you want to look up and find the <code>k</code> closest records in the database. In this case the word <em>mouse</em> is closest to <code>mickey mouse</code>, <code>cheese</code>, and <code>trap</code> where <code>k=3</code> (return the three nearest records). The word <em>rat</em> is excluded if <code>k=3</code>, but would be included if <code>k=4</code> as it is the next closest vector. The word <em>airplane</em> in this example is far away because it is rarely associated with the word <em>mouse</em> in the training data. The word <em>ship</em> is still colocated near the other forms of transport but is closer to <code>mouse</code> and <code>rat</code> because they are often found on ships, as per the training data.</p>

<figure><div id="figure-5-2" class="figure">
<img src="assets/pega_0502.png" alt="pega 0502" width="600" height="483"/>
<h6><span class="label">Figure 5-2. </span>Multidimensional vector distances</h6>
</div></figure>

<p>A vector database stores the text records with their vector representation as the key. This is unlike other types of databases, where you might find records based on an ID, relation, or where the text contains a string. For example, if you queried a relational database based on the text in <a data-type="xref" href="#figure-5-2">Figure 5-2</a> to find records where text contains <code>mouse</code>, you’d return the record <code>mickey mouse</code> but nothing else, as no other record contains that exact phrase. With vectors search you could also return the records <code>cheese</code> and <code>trap</code>, because they are closely associated, even though they aren’t an exact match for your query.</p>

<p>The ability to query based <a data-type="indexterm" data-primary="vector databases" data-secondary="querying" id="id936"/>on similarity is extremely useful, and vector search powers a lot of AI functionality. For example:</p>
<dl>
<dt>Document reading</dt>
<dd>
<p>Find related <a data-type="indexterm" data-primary="vector databases" data-secondary="document reading" id="id937"/>sections of text to read in order to provide a more accurate answer.</p>
</dd>
<dt>Recommendation systems</dt>
<dd>
<p>Discover similar <a data-type="indexterm" data-primary="vector databases" data-secondary="recommendation systems" id="id938"/>products or items in order to suggest them to a user.</p>
</dd>
<dt>Long-term memory</dt>
<dd>
<p>Look up relevant <a data-type="indexterm" data-primary="vector databases" data-secondary="long-term memory" id="id939"/>snippets of conversation history so a chatbot remembers past interactions.</p>
</dd>
</dl>

<p>AI models are able to handle these tasks at small scale, as long as your documents, product list, or conversation memory fits within the token limits of the model you’re using. However, at scale you quite quickly run into token limits and excess cost from passing too many tokens in each prompt. OpenAI’s <code>gpt-4-1106-preview</code> was <a href="https://oreil.ly/KMNU8">released in November 2023</a> with an enormous 128,000 token context window, but it costs 10 times more per token than <code>gpt-3.5-turbo</code>, which has 88% fewer tokens and was released a year earlier. The more efficient approach is to look up only the most relevant records to pass into the prompt at runtime in order to provide the most relevant context to form a response. This practice is typically referred to as RAG.</p>






<section data-type="sect1" data-pdf-bookmark="Retrieval Augmented Generation (RAG)"><div class="sect1" id="id85">
<h1>Retrieval Augmented Generation (RAG)</h1>

<p>Vector databases are a key component <a data-type="indexterm" data-primary="RAG (Retrieval-Augmented Generation)" id="id940"/><a data-type="indexterm" data-primary="vector searches" data-seealso="RAG (Retrieval-Augmented Generation)" id="id941"/>of RAG, which typically involves searching by similarity to the query, retrieving the most relevant documents, and inserting them into the prompt as context. This lets you stay within what fits in the current context window, while avoiding spending money on wasted tokens by inserting irrelevant text documents in the context.</p>

<p>Retrieval can also be done using traditional database searches or web browsing, and in many cases a vector search by semantic similarity is not necessary. RAG is typically used to solve hallucinations in open-ended scenarios, like a user talking to a chatbot that is prone to making things up when asked about something not in its training data. Vector search can insert documents that are semantically similar to the user query into the prompt, greatly decreasing the chances the chatbot will hallucinate.</p>

<p>For example, if your author Mike told a chatbot “My name is Mike,” then three messages later asked, “What is my name?” it can easily recall the right answer. The message containing Mike’s name is still within the context window of the chat. However, if it was 3,000 messages ago, the text of those messages may be too large to fit inside the context window. Without this important context, it might hallucinate a name or refuse to answer for lack of information. A keyword search might help but could return too many irrelevant documents or fail to recall the right context in which the information was captured in the past. There may be many times Mike mentioned the word <em>name</em> in different formats, and for different reasons. By passing the question to the vector database, it can return the top three similar messages from the chat that match what the user asked:</p>

<pre data-type="programlisting">## Context
Most relevant previous user messages:
1. "My name is Mike".
2. "My dog's name is Hercules".
3. "My coworker's name is James".

## Instructions
Please answer the user message using the context above.
User message: What is my name?
AI message:</pre>

<p>It’s impossible to pass all 3,000 past messages into the prompt for most models, and for a traditional search the AI model would have to formulate the right search query, which can be unreliable. Using the RAG pattern, you would pass the current user message to a vector search function, and return the most relevant three records as context, which the chatbot can then use to respond correctly.</p>
<div data-type="tip"><h1>Give Direction</h1>
<p>Rather than inserting static <a data-type="indexterm" data-primary="Give Direction principle" data-secondary="RAG and" id="id942"/>knowledge into the prompt, vector search allows you to dynamically insert the most relevant knowledge into the prompt.</p>
</div>

<p>Here’s how the process works for production applications <a data-type="indexterm" data-primary="RAG (Retrieval-Augmented Generation)" data-secondary="production applications" id="id943"/>using RAG:</p>
<ol>
<li>
<p>Break documents into chunks of text.</p>
</li>
<li>
<p>Index chunks in a vector database.</p>
</li>
<li>
<p>Search by vector for similar records.</p>
</li>
<li>
<p>Insert records into the prompt as context.</p>
</li>

</ol>

<p>In this instance, the documents would be all the 3,000 past user messages to serve as the chatbot’s memory, but it could also be sections of a PDF document we uploaded to give the chatbot the ability to read, or a list of all the relevant products you sell to enable the chatbot to make a recommendation. The ability of our vector search to find the most similar texts is wholly dependent on the AI model used to generate the vectors, referred to as <em>embeddings</em> when you’re dealing with semantic or contextual information.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Introducing Embeddings"><div class="sect1" id="id86">
<h1>Introducing Embeddings</h1>

<p>The word <em>embeddings</em> typically refers to the <a data-type="indexterm" data-primary="vector databases" data-secondary="embeddings" id="id944"/><a data-type="indexterm" data-primary="embeddings" id="id945"/>vector representation of the text returned from a pretrained AI model. At the time of writing, the standard model for generating embeddings is OpenAI’s text-embedding-ada-002, although embedding models have been available long before the advent of generative AI.</p>

<p>Although it is helpful to visualize vector spaces as a two-dimensional chart, as in <a data-type="xref" href="#figure-5-2">Figure 5-2</a>, in reality the embeddings returned from text-embedding-ada-002 are in 1,536 dimensions, which is difficult to depict graphically. Having more dimensions allows the model to capture deeper semantic meaning and relationships. For example, while a 2-D space might be able to separate cats from dogs, a 300-D space could capture information about the differences between breeds, sizes, colors, and other intricate details. The following code shows how to retrieve embeddings from the OpenAI API. The code for the following examples is included in the <a href="https://oreil.ly/6RzTy">GitHub repository</a> for this book.</p>

<p class="pagebreak-before" id="retrieving_openai_embeddings">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">OpenAI</code>
<code class="n">client</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">()</code>

<code class="c1"># Function to get the vector embedding for a given text</code>
<code class="k">def</code> <code class="nf">get_vector_embeddings</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">embeddings</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="nb">input</code><code class="o">=</code><code class="n">text</code><code class="p">,</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"text-embedding-ada-002"</code>
    <code class="p">)</code>
    <code class="n">embeddings</code> <code class="o">=</code> <code class="p">[</code><code class="n">r</code><code class="o">.</code><code class="n">embedding</code> <code class="k">for</code> <code class="n">r</code> <code class="ow">in</code> <code class="n">response</code><code class="o">.</code><code class="n">data</code><code class="p">]</code>
    <code class="k">return</code> <code class="n">embeddings</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>

<code class="n">get_vector_embeddings</code><code class="p">(</code><code class="s2">"Your text string goes here"</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">[
-0.006929283495992422,
-0.005336422007530928,
...
-4.547132266452536e-05,
-0.024047505110502243
]</pre>

<p>This code uses the OpenAI API to create an embedding for a given input text using a specific embedding model:</p>
<ol>
<li>
<p><code>from openai import OpenAI</code> imports the OpenAI library, and <code>client = OpenAI()</code> sets up the client. It retrieves your OpenAI API key from an environment variable <code>OPENAI_API_KEY</code> in order to charge the cost of the embeddings to your account. You need to set this in your environment (usually in an <em>.env</em> file), which can be obtained by creating an account and visiting <a href="https://oreil.ly/apikeys" class="bare"><em class="hyperlink">https://oreil.ly/apikeys</em></a>.</p>
</li>
<li>
<p><code>response = client.embeddings.create(...)</code>: This line calls the <code>create</code> method of the <code>Embedding</code> class from the <code>client</code> from the OpenAI library. The method takes two arguments:</p>

<ul>
<li>
<p><code>input</code>: This is where you provide the text string for which you want to generate an embedding.</p>
</li>
<li>
<p><code>model</code>: This specifies the embedding model you want to use. In this case, it is <code>text-embedding-ada-002</code>, which is a model within the OpenAI API.</p>
</li>
</ul>
</li>
<li>
<p><code>embeddings = [r.embedding for r in response.data]</code>: After the API call, the <code>response</code> object contains the generated embeddings in JSON format. This line extracts the actual numerical embedding from the response, by iterating through a list of embeddings in <code>response.data</code>.</p>
</li>

</ol>

<p>After executing this code, the <code>embeddings</code> variable <a data-type="indexterm" data-primary="vector databases" data-secondary="embeddings" data-tertiary="document loading" id="id946"/><a data-type="indexterm" data-primary="embeddings" data-secondary="document loading" id="id947"/><a data-type="indexterm" data-primary="document loading" id="id948"/>will hold the numerical representation (embedding) of the input text, which can then be used in various NLP tasks or machine learning models. This process of retrieving or generating embeddings is sometimes referred to as <em>document loading</em>.</p>

<p>The term <em>loading</em> in this context refers to the act of computing or retrieving the numerical (vector) representations of text from a model and storing them in a variable for later use. This is distinct from the concept of <em>chunking</em>, which typically refers to breaking down a text into smaller, manageable pieces or chunks to facilitate processing. These two techniques are regularly used in conjunction with each other, as it’s often useful to break large documents up into pages or paragraphs to facilitate more accurate matching and to only pass the most relevant tokens into the prompt.</p>

<p>There is a cost associated with retrieving embeddings from OpenAI, but it is relatively inexpensive at $0.0004 per 1,000 tokens at the time of writing. For instance, the King James version of the Bible, which comprises around 800,000 words or approximately 4,000,000 tokens, would cost about $1.60 to retrieve all the embeddings for the entire document.</p>

<p>Paying for embeddings from OpenAI is not your only option. There <a data-type="indexterm" data-primary="vector databases" data-secondary="embeddings" data-tertiary="open-source models" id="id949"/><a data-type="indexterm" data-primary="embeddings" data-secondary="open-source models and" id="id950"/><a data-type="indexterm" data-primary="vector databases" data-secondary="embeddings" data-tertiary="Sentence Transformers Library" id="id951"/><a data-type="indexterm" data-primary="embeddings" data-secondary="Sentence Transformers Library" id="id952"/><a data-type="indexterm" data-primary="Sentence Transformers Library" id="id953"/><a data-type="indexterm" data-primary="Hugging Face API" id="id954"/>are also open-source models you can use, for example, the <a href="https://oreil.ly/8OV3c">Sentence Transformers library</a> provided by Hugging Face, which has 384 dimensions.</p>

<p id="retrieving_opensource_embeddings">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">requests</code>
<code class="kn">import</code> <code class="nn">os</code>

<code class="n">model_id</code> <code class="o">=</code> <code class="s2">"sentence-transformers/all-MiniLM-L6-v2"</code>
<code class="n">hf_token</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">getenv</code><code class="p">(</code><code class="s2">"HF_TOKEN"</code><code class="p">)</code>

<code class="n">api_url</code> <code class="o">=</code> <code class="s2">"https://api-inference.huggingface.co/"</code>
<code class="n">api_url</code> <code class="o">+=</code> <code class="sa">f</code><code class="s2">"pipeline/feature-extraction/</code><code class="si">{</code><code class="n">model_id</code><code class="si">}</code><code class="s2">"</code>
<code class="n">headers</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"Authorization"</code><code class="p">:</code> <code class="sa">f</code><code class="s2">"Bearer </code><code class="si">{</code><code class="n">hf_token</code><code class="si">}</code><code class="s2">"</code><code class="p">}</code>

<code class="k">def</code> <code class="nf">query</code><code class="p">(</code><code class="n">texts</code><code class="p">):</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="n">api_url</code><code class="p">,</code> <code class="n">headers</code><code class="o">=</code><code class="n">headers</code><code class="p">,</code>
    <code class="n">json</code><code class="o">=</code><code class="p">{</code><code class="s2">"inputs"</code><code class="p">:</code> <code class="n">texts</code><code class="p">,</code>
    <code class="s2">"options"</code><code class="p">:{</code><code class="s2">"wait_for_model"</code><code class="p">:</code><code class="kc">True</code><code class="p">}})</code>
    <code class="k">return</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()</code>

<code class="n">texts</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"mickey mouse"</code><code class="p">,</code>
        <code class="s2">"cheese"</code><code class="p">,</code>
        <code class="s2">"trap"</code><code class="p">,</code>
        <code class="s2">"rat"</code><code class="p">,</code>
        <code class="s2">"ratatouille"</code>
        <code class="s2">"bus"</code><code class="p">,</code>
        <code class="s2">"airplane"</code><code class="p">,</code>
        <code class="s2">"ship"</code><code class="p">]</code>

<code class="n">output</code> <code class="o">=</code> <code class="n">query</code><code class="p">(</code><code class="n">texts</code><code class="p">)</code>
<code class="n">output</code></pre>

<p>Output:</p>

<pre data-type="programlisting">[[-0.03875632584095001, 0.04480459913611412,
0.016051070764660835, -0.01789097487926483,
-0.03518553078174591, -0.013002964667975903,
0.14877274632453918, 0.048807501792907715,
0.011848390102386475, -0.044042471796274185,
...
-0.026688814163208008, -0.0359361357986927,
-0.03237859532237053, 0.008156519383192062,
-0.10299170762300491, 0.0790356695652008,
-0.008071334101259708, 0.11919838190078735,
0.0005506130401045084, -0.03497892618179321]]</pre>

<p>This code uses the Hugging Face API to obtain embeddings <a data-type="indexterm" data-primary="vector databases" data-secondary="embeddings" data-tertiary="Hugging Face API" id="id955"/><a data-type="indexterm" data-primary="embeddings" data-secondary="Hugging Face API" id="id956"/><a data-type="indexterm" data-primary="Hugging Face API" data-secondary="embeddings" id="id957"/>for a list of text inputs using a pre-trained model. The model used here is the <code>sentence-transformers/all-MiniLM-L6-v2</code>, which is a smaller version of BERT, an open source NLP model introduced by Google in 2017 (based on the transformer model), which is optimized for sentence-level tasks. Here’s how it works step-by-step:</p>
<ol>
<li>
<p><code>model_id</code> is assigned the identifier of the pre-trained model, <code>sentence-transformers/all-MiniLM-L6-v2</code>.</p>
</li>
<li>
<p><code>hf_token = os.getenv("HF_TOKEN")</code> retrieves the API key for the Hugging Face API token from your environment. You need to set this in your environment with your own token, which can be obtained by creating an account and visiting <a href="https://hf.co/settings/tokens" class="bare"><em class="hyperlink">https://hf.co/settings/tokens</em></a>.</p>
</li>
<li>
<p>The <code>requests</code> library is imported to make HTTP requests to the API.</p>
</li>
<li>
<p><code>api_url</code> is assigned the URL for the Hugging Face API, with the model ID included in the URL.</p>
</li>
<li>
<p><code>headers</code> is a dictionary containing the authorization header with your Hugging Face API token.</p>
</li>
<li>
<p>The <code>query()</code> function is defined, which takes a list of text inputs and sends a <code>POST</code> request to the Hugging Face API with the appropriate headers and JSON payload containing the inputs and an option to wait for the model to become available. The function then returns the JSON response from the API.</p>
</li>
<li>
<p><code>texts</code> is a list of strings from your database.</p>
</li>
<li>
<p><code>output</code> is assigned the result of calling the <code>query()</code> function with the <code>texts</code> list.</p>
</li>
<li>
<p>The <code>output</code> variable is printed, which will display the feature embeddings for the input texts.</p>
</li>

</ol>

<p>When you run this code, the script will send text to the Hugging Face API, and the API will return  embeddings for each string of text sent.</p>

<p>If you pass the same text into an embedding model, you’ll get the same vector back every time. However, vectors are not usually comparable across models (or versions of models) due to differences in training. The embeddings you get from OpenAI are different from those you get from BERT or spaCy (a natural language processing library).</p>

<p>The main difference with embeddings <a data-type="indexterm" data-primary="embeddings" data-secondary="contextual vectors" id="id958"/><a data-type="indexterm" data-primary="vectors" data-secondary="contextual" id="id959"/><a data-type="indexterm" data-primary="vector databases" data-secondary="embeddings" data-tertiary="contextual vectors" id="id960"/>generated by modern transformer models is that the vectors are contextual rather than static, meaning the word <em>bank</em> would have different embeddings in the context of a <em>riverbank</em> versus <em>financial bank</em>. The embeddings you get from OpenAI Ada 002 and HuggingFace Sentence Transformers are examples of dense vectors, where each number in the array is almost always nonzero (i.e., they contain semantic information). There are also <a href="https://oreil.ly/d1cmb">sparse vectors</a>, which normally have a large number of dimensions (e.g., 100,000+) with many of the dimensions having a value of zero. This allows capturing specific important features (each feature can have its own dimension), which tends to be important for performance in keyword-based search applications. Most AI applications use dense vectors for retrieval, although hybrid search (both dense and sparse vectors) is rising in popularity, as both similarity and keyword search can be useful in combination.</p>

<p>The accuracy of the vectors is wholly reliant on the accuracy of the model you use to generate the embeddings. Whatever biases or knowledge gaps the underlying models have will also be an issue for vector search. For example, the <code>text-embedding-ada-002</code> model is currently only trained up to August 2020 and therefore is unaware of any new words or new cultural associations that formed after that cutoff date. This can cause a problem for use cases that need more recent context or niche domain knowledge not available in the training data, which may necessitate training a custom model.</p>

<p>In some instances it might make sense to train your own embedding model. For instance, you might do this if the text used has a domain-specific vocabulary where specific words have a meaning separate from the generally accepted meaning of the word. One example might be tracing the language used by toxic groups on social media like Q-Anon, who evolve the language they use in posts to bypass moderation actions.</p>

<p>Training your own embeddings can be <a data-type="indexterm" data-primary="vector databases" data-secondary="embeddings" data-tertiary="training" id="id961"/><a data-type="indexterm" data-primary="embeddings" data-secondary="training" id="id962"/><a data-type="indexterm" data-primary="word2vec" id="id963"/><a data-type="indexterm" data-primary="GloVe (Global Vectors for Word Representation)" id="id964"/>done with tools like word2vec, a method to represent words in a vector space, enabling you to capture the semantic meanings of words. More advanced models may be used, like GloVe (Global Vectors for Word Representation), which is used by spaCy for its embeddings, <a data-type="indexterm" data-primary="embeddings" data-secondary="Common Crawl dataset" id="id965"/><a data-type="indexterm" data-primary="Common Crawl dataset" id="id966"/>which are trained on the Common Crawl dataset, an open source snapshot of the web. The library Gensim offers a simple process for training your own custom <a data-type="indexterm" data-primary="embeddings" data-secondary="custom, training" id="id967"/><a data-type="indexterm" data-primary="vector databases" data-secondary="embeddings" data-tertiary="custom, training" id="id968"/>embeddings using the <a href="https://oreil.ly/RmXVR">open source algorithm</a> word2vec.</p>

<p id="generating_custom_embeddings">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">gensim.models</code> <code class="kn">import</code> <code class="n">Word2Vec</code>

<code class="c1"># Sample data: list of sentences, where each sentence is</code>
<code class="c1"># a list of words.</code>
<code class="c1"># In a real-world scenario, you'd load and preprocess your</code>
<code class="c1"># own corpus.</code>
<code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">[</code><code class="s2">"the"</code><code class="p">,</code> <code class="s2">"cake"</code><code class="p">,</code> <code class="s2">"is"</code><code class="p">,</code> <code class="s2">"a"</code><code class="p">,</code> <code class="s2">"lie"</code><code class="p">],</code>
    <code class="p">[</code><code class="s2">"if"</code><code class="p">,</code> <code class="s2">"you"</code><code class="p">,</code> <code class="s2">"hear"</code><code class="p">,</code> <code class="s2">"a"</code><code class="p">,</code> <code class="s2">"turret"</code><code class="p">,</code> <code class="s2">"sing"</code><code class="p">,</code> <code class="s2">"you're"</code><code class="p">,</code>
    <code class="s2">"probably"</code><code class="p">,</code> <code class="s2">"too"</code><code class="p">,</code> <code class="s2">"close"</code><code class="p">],</code>
    <code class="p">[</code><code class="s2">"why"</code><code class="p">,</code> <code class="s2">"search"</code><code class="p">,</code> <code class="s2">"for"</code><code class="p">,</code> <code class="s2">"the"</code><code class="p">,</code> <code class="s2">"end"</code><code class="p">,</code> <code class="s2">"of"</code><code class="p">,</code> <code class="s2">"a"</code><code class="p">,</code>
    <code class="s2">"rainbow"</code><code class="p">,</code> <code class="s2">"when"</code><code class="p">,</code> <code class="s2">"the"</code><code class="p">,</code> <code class="s2">"cake"</code><code class="p">,</code> <code class="s2">"is"</code><code class="p">,</code> <code class="s2">"a"</code><code class="p">,</code> <code class="s2">"lie?"</code><code class="p">],</code>
    <code class="c1"># ...</code>
    <code class="p">[</code><code class="s2">"there's"</code><code class="p">,</code> <code class="s2">"no"</code><code class="p">,</code> <code class="s2">"cake"</code><code class="p">,</code> <code class="s2">"in"</code><code class="p">,</code> <code class="s2">"space,"</code><code class="p">,</code> <code class="s2">"just"</code><code class="p">,</code> <code class="s2">"ask"</code><code class="p">,</code>
    <code class="s2">"wheatley"</code><code class="p">],</code>
    <code class="p">[</code><code class="s2">"completing"</code><code class="p">,</code> <code class="s2">"tests"</code><code class="p">,</code> <code class="s2">"for"</code><code class="p">,</code> <code class="s2">"cake"</code><code class="p">,</code> <code class="s2">"is"</code><code class="p">,</code> <code class="s2">"the"</code><code class="p">,</code>
    <code class="s2">"sweetest"</code><code class="p">,</code> <code class="s2">"lie"</code><code class="p">],</code>
    <code class="p">[</code><code class="s2">"I"</code><code class="p">,</code> <code class="s2">"swapped"</code><code class="p">,</code> <code class="s2">"the"</code><code class="p">,</code> <code class="s2">"cake"</code><code class="p">,</code> <code class="s2">"recipe"</code><code class="p">,</code> <code class="s2">"with"</code><code class="p">,</code> <code class="s2">"a"</code><code class="p">,</code>
    <code class="s2">"neurotoxin"</code><code class="p">,</code> <code class="s2">"formula,"</code><code class="p">,</code> <code class="s2">"hope"</code><code class="p">,</code> <code class="s2">"that's"</code><code class="p">,</code> <code class="s2">"fine"</code><code class="p">],</code>
<code class="p">]</code> <code class="o">+</code> <code class="p">[</code>
    <code class="p">[</code><code class="s2">"the"</code><code class="p">,</code> <code class="s2">"cake"</code><code class="p">,</code> <code class="s2">"is"</code><code class="p">,</code> <code class="s2">"a"</code><code class="p">,</code> <code class="s2">"lie"</code><code class="p">],</code>
    <code class="p">[</code><code class="s2">"the"</code><code class="p">,</code> <code class="s2">"cake"</code><code class="p">,</code> <code class="s2">"is"</code><code class="p">,</code> <code class="s2">"definitely"</code><code class="p">,</code> <code class="s2">"a"</code><code class="p">,</code> <code class="s2">"lie"</code><code class="p">],</code>
    <code class="p">[</code><code class="s2">"everyone"</code><code class="p">,</code> <code class="s2">"knows"</code><code class="p">,</code> <code class="s2">"that"</code><code class="p">,</code> <code class="s2">"cake"</code><code class="p">,</code> <code class="s2">"equals"</code><code class="p">,</code> <code class="s2">"lie"</code><code class="p">],</code>
    <code class="c1"># ...</code>
<code class="p">]</code> <code class="o">*</code> <code class="mi">10</code>  <code class="c1"># repeat several times to emphasize</code>


<code class="c1"># Train the word2vec model</code>
<code class="n">model</code> <code class="o">=</code>  <code class="n">Word2Vec</code><code class="p">(</code><code class="n">sentences</code><code class="p">,</code> <code class="n">vector_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">window</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>
<code class="n">min_count</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">workers</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">36</code><code class="p">)</code>

<code class="c1"># Save the model</code>
<code class="n">model</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"custom_word2vec_model.model"</code><code class="p">)</code>

<code class="c1"># To load the model later</code>
<code class="c1"># loaded_model = word2vec.load(</code>
<code class="c1"># "custom_word2vec_model.model")</code>

<code class="c1"># Get vector for a word</code>
<code class="n">vector</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">wv</code><code class="p">[</code><code class="s1">'cake'</code><code class="p">]</code>

<code class="c1"># Find most similar words</code>
<code class="n">similar_words</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">wv</code><code class="o">.</code><code class="n">most_similar</code><code class="p">(</code><code class="s2">"cake"</code><code class="p">,</code> <code class="n">topn</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Top five most similar words to 'cake': "</code><code class="p">,</code> <code class="n">similar_words</code><code class="p">)</code>

<code class="c1"># Directly query the similarity between "cake" and "lie"</code>
<code class="n">cake_lie_similarity</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">wv</code><code class="o">.</code><code class="n">similarity</code><code class="p">(</code><code class="s2">"cake"</code><code class="p">,</code> <code class="s2">"lie"</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Similarity between 'cake' and 'lie': "</code><code class="p">,</code>
<code class="n">cake_lie_similarity</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">Top 5 most similar words to 'cake':  [('lie',
0.23420444130897522), ('test', 0.23205122351646423),
('tests', 0.17178669571876526), ('GLaDOS',
0.1536172330379486), ('got', 0.14605288207530975)]
Similarity between 'cake' and 'lie':  0.23420444</pre>

<p>This code creates a word2vec model using <a data-type="indexterm" data-primary="word2vec" id="id969"/>the Gensim library and then uses the model to determine words that are similar to a given word. Let’s break it down:</p>
<ol>
<li>
<p>The variable <code>sentences</code> contains a list of sentences, where each sentence is a list of words. This is the data on which the Word2Vec model will be trained. In a real application, instead of such hardcoded sentences, you’d often load a large corpus of text and preprocess it to obtain such a list of tokenized sentences.</p>
</li>
<li>
<p>An instance of the <code>word2vec</code> class is created to represent the model. While initializing this instance, several parameters are provided:</p>

<ul>
<li>
<p><code>sentences</code>: This is the training data.</p>
</li>
<li>
<p><code>vector_size=100</code>: This defines the size of the word vectors. So each word will be represented as a 100-dimensional vector.</p>
</li>
<li>
<p><code>window=5</code>: This represents the maximum distance between the current and predicted word within a sentence.</p>
</li>
<li>
<p><code>min_count=1</code>: This ensures that even words that appear only once in the dataset will have vectors created for them.</p>
</li>
<li>
<p><code>workers=4</code>: Number of CPU cores to use during training. It speeds up training on multicore machines.</p>
</li>
<li>
<p><code>seed=36</code>: This is set for reproducibility so that the random processes in training deliver the same result each time (not guaranteed with multiple workers).</p>
</li>
</ul>
</li>
<li>
<p>After training, the model is saved to a file named <code>custom_word2vec_model.model</code> using the <code>save</code> method. This allows you to reuse the trained model later without needing to train it again.</p>
</li>
<li>
<p>There is a commented-out line that shows how to load the model back from the saved file. This is useful when you want to load a pre-trained model in a different script or session.</p>
</li>
<li>
<p>The variable <code>vector</code> is assigned the vector representation of the word <em>cake</em>. This vector can be used for various purposes, like similarity calculations, arithmetic operations, etc.</p>
</li>
<li>
<p>The <code>most_similar</code> method is used to find words that are most similar to the provided vector (in this case, the vector for <em>cake</em>). The method returns the top five (<code>topn=5</code>) most similar words.</p>
</li>
<li>
<p>The <code>similarity</code> method queries the similarity between <em>cake</em> and <em>lie</em> direction, showing a small positive value.</p>
</li>

</ol>

<p>The dataset is small and heavily repetitive, which might not provide a diverse context to properly learn the relationship between the words. Normally, word2vec benefits from larger and more diverse corpora and typically won’t get good results until you’re into the tens of millions of words. In the example we set a seed value to cherrypick one instance where <em>lie</em> came back in the top five results, but if you remove that seed, you’ll find it rarely discovers the association successfully.</p>

<p>For smaller document sizes <a data-type="indexterm" data-primary="TF-IDF (Term Frequency-Inverse Document Frequency)" id="id970"/><a data-type="indexterm" data-primary="Term Frequency-Inverse Document Frequency (TF-IDF)" id="id971"/><a data-type="indexterm" data-primary="vector databases" data-secondary="embeddings" data-tertiary="TF-IDF and" id="id972"/><a data-type="indexterm" data-primary="embeddings" data-secondary="TF-IDF and" id="id973"/>a simpler technique <em>TF-IDF</em> (Term Frequency-Inverse Document Frequency) is recommended, a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents. The TF-IDF value increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the wider corpus, which helps to adjust for the fact that some words are generally more common than others.</p>

<p>To compute the similarity between <em>cake</em> and <em>lie</em> using TF-IDF, you can use the open source <a href="https://oreil.ly/gHb3F">scientific library</a> scikit-learn and compute the <em>cosine similarity</em> (a measure of distance between two vectors). Words that are frequently colocated in sentences will have high cosine similarity (approaching 1), whereas words that appear infrequently will show a low value (or 0, if not co-located at all). This method is robust to even small documents like our toy example.</p>

<p id="word_similarity_tf-idf">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">TfidfVectorizer</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics.pairwise</code> <code class="kn">import</code> <code class="n">cosine_similarity</code>

<code class="c1"># Convert sentences to a list of strings for TfidfVectorizer</code>
<code class="n">document_list</code> <code class="o">=</code> <code class="p">[</code><code class="s1">' '</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">s</code><code class="p">)</code> <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> <code class="n">sentences</code><code class="p">]</code>

<code class="c1"># Compute TF-IDF representation</code>
<code class="n">vectorizer</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">()</code>
<code class="n">tfidf_matrix</code> <code class="o">=</code> <code class="n">vectorizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">document_list</code><code class="p">)</code>

<code class="c1"># Extract the position of the words "cake" and "lie" in</code>
<code class="c1"># the feature matrix</code>
<code class="n">cake_idx</code> <code class="o">=</code> <code class="n">vectorizer</code><code class="o">.</code><code class="n">vocabulary_</code><code class="p">[</code><code class="s1">'cake'</code><code class="p">]</code>
<code class="n">lie_idx</code> <code class="o">=</code> <code class="n">vectorizer</code><code class="o">.</code><code class="n">vocabulary_</code><code class="p">[</code><code class="s1">'lie'</code><code class="p">]</code>

<code class="c1"># Extract and reshape the vector for 'cake'</code>
<code class="n">cakevec</code> <code class="o">=</code> <code class="n">tfidf_matrix</code><code class="p">[:,</code> <code class="n">cake_idx</code><code class="p">]</code><code class="o">.</code><code class="n">toarray</code><code class="p">()</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Compute the cosine similarities</code>
<code class="n">similar_words</code> <code class="o">=</code> <code class="n">cosine_similarity</code><code class="p">(</code><code class="n">cakevec</code><code class="p">,</code> <code class="n">tfidf_matrix</code><code class="o">.</code><code class="n">T</code><code class="p">)</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>

<code class="c1"># Get the indices of the top 6 most similar words</code>
<code class="c1"># (including 'cake')</code>
<code class="n">top_indices</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argsort</code><code class="p">(</code><code class="n">similar_words</code><code class="p">)[</code><code class="o">-</code><code class="mi">6</code><code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">][::</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>

<code class="c1"># Retrieve and print the top 5 most similar words to</code>
<code class="c1"># 'cake' (excluding 'cake' itself)</code>
<code class="n">names</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">idx</code> <code class="ow">in</code> <code class="n">top_indices</code><code class="p">:</code>
    <code class="n">names</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">vectorizer</code><code class="o">.</code><code class="n">get_feature_names_out</code><code class="p">()[</code><code class="n">idx</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Top five most similar words to 'cake': "</code><code class="p">,</code> <code class="n">names</code><code class="p">)</code>

<code class="c1"># Compute cosine similarity between "cake" and "lie"</code>
<code class="n">similarity</code> <code class="o">=</code> <code class="n">cosine_similarity</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">tfidf_matrix</code><code class="p">[:,</code>
    <code class="n">cake_idx</code><code class="p">]</code><code class="o">.</code><code class="n">todense</code><code class="p">()),</code> <code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">tfidf_matrix</code><code class="p">[:,</code> <code class="n">lie_idx</code><code class="p">]</code><code class="o">.</code><code class="n">todense</code><code class="p">()))</code>
<code class="c1"># The result will be a matrix; we can take the average or</code>
<code class="c1"># max similarity value</code>
<code class="n">avg_similarity</code> <code class="o">=</code> <code class="n">similarity</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Similarity between 'cake' and 'lie'"</code><code class="p">,</code> <code class="n">avg_similarity</code><code class="p">)</code>

<code class="c1"># Show the similarity between "cake" and "elephant"</code>
<code class="n">elephant_idx</code> <code class="o">=</code> <code class="n">vectorizer</code><code class="o">.</code><code class="n">vocabulary_</code><code class="p">[</code><code class="s1">'sing'</code><code class="p">]</code>
<code class="n">similarity</code> <code class="o">=</code> <code class="n">cosine_similarity</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">tfidf_matrix</code><code class="p">[:,</code>
    <code class="n">cake_idx</code><code class="p">]</code><code class="o">.</code><code class="n">todense</code><code class="p">()),</code> <code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">tfidf_matrix</code><code class="p">[:,</code>
    <code class="n">elephant_idx</code><code class="p">]</code><code class="o">.</code><code class="n">todense</code><code class="p">()))</code>
<code class="n">avg_similarity</code> <code class="o">=</code> <code class="n">similarity</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Similarity between 'cake' and 'sing'"</code><code class="p">,</code>
    <code class="n">avg_similarity</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">Top 5 most similar words to 'cake':  ['lie', 'the', 'is',
'you', 'definitely']
Similarity between 'cake' and 'lie' 0.8926458157227388
Similarity between 'cake' and 'sing' 0.010626735901461177</pre>

<p>Let’s break down this code step-by-step:</p>
<ol>
<li>
<p>The <code>sentences</code> variable is reused from the previous example. The code converts these lists of words into full sentences (strings) using a list comprehension, resulting in <code>document_list</code>.</p>
</li>
<li>
<p>An instance of <code>TfidfVectorizer</code> is created. The <code>fit_transform</code> method of the vectorizer is then used to convert the <code>document_list</code> into a matrix of TF-IDF features, which is stored in <code>tfidf_matrix</code>.</p>
</li>
<li>
<p>The code extracts the position (or index) of the words <em>cake</em> and <em>lie</em> in the feature matrix using the <code>vocabulary_</code> attribute of the vectorizer.</p>
</li>
<li>
<p>The TF-IDF vector corresponding to the word <em>cake</em> is extracted from the matrix and reshaped.</p>
</li>
<li>
<p>The cosine similarity between the vector for <em>cake</em> and all other vectors in the TF-IDF matrix is computed. This results in a list of similarity scores.</p>

<ul>
<li>
<p>The indices of the top six most similar words (including <em>cake</em>) are identified.</p>
</li>
<li>
<p>Using these indices, the top five words (excluding <em>cake</em>) with the highest similarity to <em>cake</em> are retrieved and printed.</p>
</li>
</ul>
</li>
<li>
<p>The cosine similarity between the TF-IDF vectors of the words <em>cake</em> and <em>lie</em> is computed. Since the result is a matrix, the code computes the mean similarity value across all values in this matrix and then prints the average similarity.</p>
</li>
<li>
<p>Now we compute the similarity between <em>cake</em> and <em>sing</em>. The average similarity value is calculated and printed to show that the two words are not commonly colocated (close to zero).</p>
</li>

</ol>

<p>As well as the embedding model used, the strategy for what you embed is also important, because there is a trade-off between context and similarity. If you embed a large block of text, say an entire book, the vector you get back will be the average of the locations of the tokens that make up the full text. As you increase the size of the chunk, there is a regression to the mean where it approaches the average of all the vectors and no longer contains much semantic information.</p>

<p>Smaller chunks of text will be more specific in terms of location in vector space and as such might be more useful when you need close similarity. For example, isolating smaller sections of text from a novel may better separate comedic from tragic moments in the story, whereas embedding a whole page or chapter may mix both together. However, making the chunks of text too small might also cause them to lose meaning if the text is cut off in the middle of a sentence or paragraph. Much of the art of working with vector databases is in the way you load the document and break it into chunks.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Document Loading"><div class="sect1" id="id87">
<h1>Document Loading</h1>

<p>One common use case of AI is to be able <a data-type="indexterm" data-primary="document loading" id="id974"/><a data-type="indexterm" data-primary="vector databases" data-secondary="document loading" id="id975"/>to search across documents based on similarity to the text of the user query. For example, you may have a series of PDFs representing your employee handbook, and you want to return the correct snippet of text from those PDFs that relates to an employee question. The way you load documents into your vector database will be dictated by the structure of your documents, how many examples you want to return from each query, and the number of tokens you can afford in each prompt.</p>

<p>For example, <code>gpt-4-0613</code> has an <a href="https://oreil.ly/wbx1f">8,192 token limit</a>, which needs to be shared between the prompt template, the examples inserted into the prompt, and the completion the model provides in response. Setting aside around 2,000 words or approximately 3,000 tokens for the prompt and response, you could pull the five most similar chunks of 1,000 tokens of text each into the prompt as context. However, if you naively split the document into 1,000-token chunks, you will run into a problem. The arbitrary place where each split takes place might be in the middle of a paragraph or sentence, so you risk losing the meaning of <a data-type="indexterm" data-primary="LangChain" data-secondary="text splitters" id="id976"/><a data-type="indexterm" data-primary="text splitters" id="id977"/><a data-type="indexterm" data-primary="document loading" data-secondary="text splitters" id="id978"/>what’s being conveyed. LangChain has a series of <a href="https://oreil.ly/qsG7J">text splitters</a>, including the commonly used recursive character text splitter. It tries to split on line breaks and then spaces until the chunks are small enough. This keeps all paragraphs (and then sentences, and then words) together as much as possible to retain semantic groupings inherent in the structure of <a data-type="indexterm" data-primary="document loading" data-secondary="chunking text" data-tertiary="sentences" id="id979"/><a data-type="indexterm" data-primary="chunking text" data-secondary="sentences" id="id980"/><a data-type="indexterm" data-primary="vector databases" data-secondary="document loading" data-tertiary="chunking text" id="id981"/>the text.</p>

<p id="chunking_sentences">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain.text_splitter</code> <code class="kn">import</code> <code class="n">RecursiveCharacterTextSplitter</code>

<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="o">.</code><code class="n">from_tiktoken_encoder</code><code class="p">(</code>
    <code class="n">chunk_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="c1"># 100 tokens</code>
    <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="c1"># 20 tokens of overlap</code>
    <code class="p">)</code>

<code class="n">text</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">Welcome to the "Unicorn Enterprises: Where Magic Happens"</code>
<code class="s2">Employee Handbook! We're thrilled to have you join our team</code>
<code class="s2">of dreamers, doers, and unicorn enthusiasts. At Unicorn</code>
<code class="s2">Enterprises, we believe that work should be as enchanting as</code>
<code class="s2">it is productive. This handbook is your ticket to the</code>
<code class="s2">magical world of our company, where we'll outline the</code>
<code class="s2">principles, policies, and practices that guide us on this</code>
<code class="s2">extraordinary journey. So, fasten your seatbelts and get</code>
<code class="s2">ready to embark on an adventure like no other!</code>

<code class="s2">...</code>

<code class="s2">As we conclude this handbook, remember that at Unicorn</code>
<code class="s2">Enterprises, the pursuit of excellence is a never-ending</code>
<code class="s2">quest. Our company's success depends on your passion,</code>
<code class="s2">creativity, and commitment to making the impossible</code>
<code class="s2">possible. We encourage you to always embrace the magic</code>
<code class="s2">within and outside of work, and to share your ideas and</code>
<code class="s2">innovations to keep our enchanted journey going. Thank you</code>
<code class="s2">for being a part of our mystical family, and together, we'll</code>
<code class="s2">continue to create a world where magic and business thrive</code>
<code class="s2">hand in hand!</code>
<code class="s2">"""</code>

<code class="n">chunks</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">split_text</code><code class="p">(</code><code class="n">text</code><code class="o">=</code><code class="n">text</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">chunks</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">3</code><code class="p">])</code></pre>

<p class="pagebreak-before">Output:</p>

<pre data-type="programlisting">['Welcome to the "Unicorn Enterprises: Where Magic Happens"
Employee Handbook! We\'re thrilled to have you join our team
of dreamers, doers, and unicorn enthusiasts.',
"We're thrilled to have you join our team of dreamers,
doers, and unicorn enthusiasts. At Unicorn Enterprises, we
believe that work should be as enchanting as it is
productive.",
 ...
"Our company's success depends on your passion, creativity,
and commitment to making the impossible possible. We
encourage you to always embrace the magic within and outside
of work, and to share your ideas and innovations to keep our
enchanted journey going.",
"We encourage you to always embrace the magic within and
outside of work, and to share your ideas and innovations to
keep our enchanted journey going. Thank you for being a part
of our mystical family, and together, we'll continue to
create a world where magic and business thrive hand in
hand!"]</pre>

<p>Here’s how this code works step-by-step:</p>
<ol>
<li>
<p><em>Create text splitter instance</em>: An instance of <code>RecursiveCharacterTextSplitter</code> is created using the <code>from_tiktoken_encoder</code> method. This method is specifically designed to <a data-type="indexterm" data-primary="text splitters" data-secondary="instances" id="id982"/><a data-type="indexterm" data-primary="document loading" data-secondary="text splitters" data-tertiary="instances" id="id983"/>handle the splitting of text based on token counts.</p>

<p>The <code>chunk_size</code> parameter, set to 100, ensures that each chunk of text will contain approximately 100 <a data-type="indexterm" data-primary="document loading" data-secondary="chunking text" data-tertiary="chunk size" id="id984"/><a data-type="indexterm" data-primary="chunking text" data-secondary="chunk size" id="id985"/>tokens. This is a way of controlling the size of each text segment.</p>

<p>The <code>chunk_overlap</code> parameter, set to 20, specifies that there will be an overlap of 20 tokens between <a data-type="indexterm" data-primary="document loading" data-secondary="chunking text" data-tertiary="chunk overlap" id="id986"/><a data-type="indexterm" data-primary="chunking text" data-secondary="chunk overlap" id="id987"/>consecutive chunks. This overlap ensures that the context is not lost between chunks, which is crucial for understanding and processing the text accurately.</p>
</li>
<li>
<p><em>Prepare the text</em>: The variable <code>text</code> contains <a data-type="indexterm" data-primary="document loading" data-secondary="text prep" id="id988"/>a multiparagraph string, representing the content to be split into chunks.</p>
</li>
<li>
<p><em>Split the text</em>: The <code>split_text</code> method of the <code>text_splitter</code> instance is used to split the text into chunks based on the previously defined <code>chunk_size</code> and <code>chunk_overlap</code>. This method processes the text and returns a list of text chunks.</p>
</li>
<li>
<p><em>Output the chunks</em>: The code prints <a data-type="indexterm" data-primary="document loading" data-secondary="chunking text" data-tertiary="outputting chunks" id="id989"/><a data-type="indexterm" data-primary="chunking text" data-secondary="outputting chunks" id="id990"/>the first three chunks of the split text to demonstrate how the text has been divided. This output is helpful for verifying that the text has been split as expected, adhering to the specified chunk size and overlap.</p>
</li>

</ol>
<div data-type="tip"><h1>Specify Format</h1>
<p>The relevance of the chunk of text <a data-type="indexterm" data-primary="Specify Format principle" data-secondary="document loading" id="id991"/>provided to the prompt will depend heavily on your chunking strategy. Shorter chunks of text without overlap may not contain the right answer, whereas longer chunks of text with too much overlap may return too many irrelevant results and confuse the LLM or cost you too many tokens.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Memory Retrieval with FAISS"><div class="sect1" id="id88">
<h1>Memory Retrieval with FAISS</h1>

<p>Now that you have your documents processed <a data-type="indexterm" data-primary="memory" data-secondary="FAISS and" id="fssrsrai"/><a data-type="indexterm" data-primary="FAISS (Facebook AI Similarity Search)" id="fassslr"/><a data-type="indexterm" data-primary="vector databases" data-secondary="FAISS and" id="vcdtbfss"/>into chunks, you need to store them in a vector database. It is common practice to store vectors in a database so that you do not need to recompute them, as there is typically some cost and latency associated with doing so. If you don’t change your embedding model, the vectors won’t change, so you do not typically need to update them once stored. You can use an open source library to store and query your vectors called FAISS, a library developed by <a href="https://oreil.ly/gIcTI">Facebook AI</a> that provides efficient similarity search and clustering of dense vectors. First install FAISS in the terminal with <code>pip install faiss-cpu</code>. The code for this example is included in the <a href="https://oreil.ly/4wR7o">GitHub repository</a> for this book.</p>

<p id="faiss_vector_database">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">faiss</code>

<code class="c1">#  The get_vector_embeddings function is defined in a preceding example</code>
<code class="n">emb</code> <code class="o">=</code> <code class="p">[</code><code class="n">get_vector_embeddings</code><code class="p">(</code><code class="n">chunk</code><code class="p">)</code> <code class="k">for</code> <code class="n">chunk</code> <code class="ow">in</code> <code class="n">chunks</code><code class="p">]</code>
<code class="n">vectors</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">emb</code><code class="p">)</code>

<code class="c1"># Create a FAISS index</code>
<code class="n">index</code> <code class="o">=</code> <code class="n">faiss</code><code class="o">.</code><code class="n">IndexFlatL2</code><code class="p">(</code><code class="n">vectors</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="n">index</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">vectors</code><code class="p">)</code>

<code class="c1"># Function to perform a vector search</code>
<code class="k">def</code> <code class="nf">vector_search</code><code class="p">(</code><code class="n">query_text</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>
    <code class="n">query_vector</code> <code class="o">=</code> <code class="n">get_vector_embeddings</code><code class="p">(</code><code class="n">query_text</code><code class="p">)</code>
    <code class="n">distances</code><code class="p">,</code> <code class="n">indices</code> <code class="o">=</code> <code class="n">index</code><code class="o">.</code><code class="n">search</code><code class="p">(</code>
        <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">query_vector</code><code class="p">]),</code> <code class="n">k</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">[(</code><code class="n">chunks</code><code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="nb">float</code><code class="p">(</code><code class="n">dist</code><code class="p">))</code> <code class="k">for</code> <code class="n">dist</code><code class="p">,</code>
        <code class="n">i</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">distances</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">indices</code><code class="p">[</code><code class="mi">0</code><code class="p">])]</code>

<code class="c1"># Example search</code>
<code class="n">user_query</code> <code class="o">=</code> <code class="s2">"do we get free unicorn rides?"</code>
<code class="n">search_results</code> <code class="o">=</code> <code class="n">vector_search</code><code class="p">(</code><code class="n">user_query</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Search results for </code><code class="si">{</code><code class="n">user_query</code><code class="si">}</code><code class="s2">:"</code><code class="p">,</code> <code class="n">search_results</code><code class="p">)</code></pre>

<p class="pagebreak-before">Output:</p>

<pre data-type="programlisting">Search results for do we get free unicorn rides?: [("You'll
enjoy a treasure chest of perks, including unlimited unicorn
rides, a bottomless cauldron of coffee and potions, and
access to our company library filled with spellbinding
books. We also offer competitive health and dental plans,
ensuring your physical well-being is as robust as your
magical spirit.\n\n**5: Continuous Learning and
Growth**\n\nAt Unicorn Enterprises, we believe in continuous
learning and growth.", 0.3289167582988739)]</pre>

<p>Here is an explanation of the preceding code:</p>
<ol>
<li>
<p>Import the Facebook AI Similarity Search (FAISS) library with <code>import faiss</code>.</p>
</li>
<li>
<p><code>vectors = np.array([get_vector_embeddings(chunk) for chunk in chunks])</code> applies <code>get_vector_embeddings</code> to each element in <code>chunks</code>, which returns a vector representation (embedding) of each element. These vectors are then used to create a numpy array, which is stored in the variable <code>vectors</code>.</p>
</li>
<li>
<p>The line <code>index = faiss.IndexFlatL2(vectors.shape[1])</code> creates a FAISS index for efficient similarity search. The argument <code>vectors.shape[1]</code> is the dimension of the vectors that will be added to the index. This kind of index (<code>IndexFlatL2</code>) performs brute-force L2 distance search, which looks for the closest items to a particular item in a collection by measuring the straight-line distance between them, checking each item in the collection one by one.</p>
</li>
<li>
<p>Then you add the array of vectors to the created FAISS index with <code>index.add(vectors)</code>.</p>
</li>
<li>
<p><code>def vector_search(query_text, k=1)</code> defines a new function named <span class="keep-together"><code>vector_search</code></span> that accepts two parameters: <code>query_text</code> and <code>k</code> (with a default value of 1). The function will retrieve the embeddings for the <code>query_text</code>, and then use that to search the index for the <code>k</code> closest vectors.</p>
</li>
<li>
<p>Inside the <code>vector_search</code> function, <code>query_vector = get_vector_embeddings(query_text)</code> generates a vector embedding for the query text using the <code>get_vector_embeddings</code> function.</p>
</li>
<li>
<p>The <code>distances, indices = index.search(np.array([query_vector]), k)</code> line performs a search in the FAISS index. It looks for the <code>k</code> closest vectors to <code>query_vector</code>. The method returns two arrays: <code>distances</code> (the squared L2 distances to the query vector) and <code>indices</code> (the indices</p>
</li>
<li>
<p><code>return [(chunks[i], float(dist)) for dist, i in zip(distances[0], indices[0])]</code> returns a list of tuples. Each tuple contains a chunk (retrieved using the indices found in the search) and the corresponding distance from the query vector. Note that the distance is converted to a float before returning.</p>
</li>
<li>
<p>Finally, you perform a vector search for the string containing the user query: <code>search_results = vector_search(user_query)</code>. The result (the closest chunk and its distance) is stored in the variable <code>search_results</code>.</p>
</li>

</ol>

<p>Once the vector search is complete, the results can be injected into the prompt to provide useful context. It’s also important to set the system message so that the model is focused on answering based on the context provided rather than making an answer up. The RAG technique as demonstrated here is widely used in AI to help protect against hallucination.</p>

<p id="query_with_retrieval">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Function to perform a vector search and then ask # GPT-3.5-turbo a question</code>
<code class="k">def</code> <code class="nf">search_and_chat</code><code class="p">(</code><code class="n">user_query</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>
  <code class="c1"># Perform the vector search</code>
  <code class="n">search_results</code> <code class="o">=</code> <code class="n">vector_search</code><code class="p">(</code><code class="n">user_query</code><code class="p">,</code> <code class="n">k</code><code class="p">)</code>
  <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Search results: </code><code class="si">{</code><code class="n">search_results</code><code class="si">}</code><code class="se">\n\n</code><code class="s2">"</code><code class="p">)</code>

  <code class="n">prompt_with_context</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"""Context:</code><code class="si">{</code><code class="n">search_results</code><code class="si">}</code><code class="se">\</code>
<code class="s2">  Answer the question: </code><code class="si">{</code><code class="n">user_query</code><code class="si">}</code><code class="s2">"""</code>

  <code class="c1"># Create a list of messages for the chat</code>
  <code class="n">messages</code> <code class="o">=</code> <code class="p">[</code>
      <code class="p">{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code> <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"""Please answer the</code>
<code class="s2">      questions provided by the user. Use only the context</code>
<code class="s2">      provided to you to respond to the user, if you don't</code>
<code class="s2">      know the answer say </code><code class="se">\"</code><code class="s2">I don't know</code><code class="se">\"</code><code class="s2">."""</code><code class="p">},</code>
      <code class="p">{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code> <code class="s2">"content"</code><code class="p">:</code> <code class="n">prompt_with_context</code><code class="p">},</code>
  <code class="p">]</code>

  <code class="c1"># Get the model's response</code>
  <code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code> <code class="n">messages</code><code class="o">=</code><code class="n">messages</code><code class="p">)</code>

  <code class="c1"># Print the assistant's reply</code>
  <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"""Response:</code>
  <code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code><code class="si">}</code><code class="s2">"""</code><code class="p">)</code>

<code class="c1"># Example search and chat</code>
<code class="n">search_and_chat</code><code class="p">(</code><code class="s2">"What is Unicorn Enterprises' mission?"</code><code class="p">)</code></pre>

<p class="less_space pagebreak-before">Output:</p>

<pre data-type="programlisting">Search results: [("""As we conclude this handbook, remember that at
Unicorn Enterprises, the pursuit of excellence is a never-ending
quest. Our company's success depends on your passion,
creativity, and commitment to making the impossible
possible. We encourage you to always embrace the magic
within and outside of work, and to share your ideas and
innovations to keep our enchanted journey going. Thank you",
0.26446571946144104)]


Response:
Unicorn Enterprises' mission is to pursue excellence in their
work by encouraging their employees to embrace the magic within
and outside of work, share their ideas and innovations, and make
the impossible possible.</pre>

<p>Here is a step-by-step explanation of what the function does:</p>
<ol>
<li>
<p>Using a function named <code>vector_search</code>, the program <a data-type="indexterm" data-primary="vector searches" data-secondary="FAISS" id="vcschfss"/>performs a vector search with <code>user_query</code> as the search string and <code>k</code> as the number of search results to return. The results are stored in <code>search_results</code>.</p>
</li>
<li>
<p>The search results are then printed to the console.</p>
</li>
<li>
<p>A <code>prompt_with_context</code> is created by concatenating the <code>search_results</code> and <code>user_query</code>. The goal is to provide the model with context from the search results and a question to answer.</p>
</li>
<li>
<p>A list of messages is created. The first message is a system message that instructs the model to answer questions provided by the user using only the given context. If the model doesn’t know the answer, it’s advised to respond with <em>I don’t know</em>. The second message is a user message containing the <code>prompt_with_context</code>.</p>
</li>
<li>
<p>The <code>openai.ChatCompletion.create()</code> function is used to get the model’s response. It’s provided with the model name (<code>gpt-3.5-turbo</code>) and the list of messages.</p>
</li>
<li>
<p>At the end of the code, the <code>search_and_chat()</code> function is called with the question as the <code>user_query</code>.</p>
</li>

</ol>
<div data-type="tip"><h1>Provide Examples</h1>
<p>Without testing the writing style, it would <a data-type="indexterm" data-primary="Provide Examples principle" data-secondary="FAISS and" id="id992"/>be hard to guess which prompting strategy would win. Now you can be confident this is the correct approach.</p>
</div>

<p>Although our code is working end to end now, it doesn’t make sense to be collecting embeddings and creating a vector database with every query. Even if you’re using an open source model for embeddings, there will be a cost in terms of compute and latency. You can save the FAISS index to a file using the <code>faiss.write_index</code> function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Save the index to a file</code>
<code class="n">faiss</code><code class="o">.</code><code class="n">write_index</code><code class="p">(</code><code class="n">index</code><code class="p">,</code> <code class="s2">"data/my_index_file.index"</code><code class="p">)</code></pre>

<p>This will create a file called <em>my_index_file.index</em> in your current directory, which contains the serialized index. You can load this index back into memory later with <code>faiss.read_index</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Load the index from a file</code>
<code class="n">index</code> <code class="o">=</code> <code class="n">faiss</code><code class="o">.</code><code class="n">read_index</code><code class="p">(</code><code class="s2">"data/my_index_file.index"</code><code class="p">)</code></pre>

<p>This way, you can persist your index across different sessions, or even share it between different machines or environments. Just make sure to handle these files carefully, as they can be quite large for big indexes.</p>

<p>If you have more than one saved vector database, it’s also possible to merge them. This can be useful when serializing the loading of documents or making batch updates to your records.</p>

<p>You can merge two FAISS indices using the <code>faiss.IndexFlatL2</code> index’s <code>add</code> method:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Assuming index1 and index2 are two IndexFlatL2 indices</code>
<code class="n">index1</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">index2</code><code class="o">.</code><code class="n">reconstruct_n</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">index2</code><code class="o">.</code><code class="n">ntotal</code><code class="p">))</code></pre>

<p>In this code, <code>reconstruct_n(0, index2.ntotal)</code> is used to fetch all vectors from <code>index2</code>, and then <code>index1.add()</code> is used to add those vectors to <code>index1</code>, effectively merging the two indices.</p>

<p>This should work because <code>faiss.IndexFlatL2</code> supports the <code>reconstruct</code> method to retrieve vectors. However, please note that this process will not move any IDs associated with the vectors from <code>index2</code> to <code>index1</code>. After merging, the vectors from <code>index2</code> will have new IDs in <code>index1</code>.</p>

<p>If you need to preserve vector IDs, you’ll need to manage this externally by keeping a separate mapping from vector IDs to your data items. Then, when you merge the indices, you also merge these mappings.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Be aware that this method may not work for all types of indices, especially for those that do not support the <code>reconstruct</code> method like <code>IndexIVFFlat</code> or if the two indices have different configurations. In those cases, it may be better to keep the original vectors used to <a data-type="indexterm" data-primary="memory" data-secondary="FAISS and" data-startref="fssrsrai" id="id993"/><a data-type="indexterm" data-primary="FAISS (Facebook AI Similarity Search)" data-startref="fassslr" id="id994"/><a data-type="indexterm" data-primary="vector searches" data-secondary="FAISS" data-startref="vcschfss" id="id995"/><a data-type="indexterm" data-primary="vector databases" data-secondary="FAISS and" data-startref="vcdtbfss" id="id996"/>build each index and then merge and rebuild the index.</p>
</div>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="RAG with LangChain"><div class="sect1" id="id89">
<h1>RAG with LangChain</h1>

<p>As one of the most popular frameworks <a data-type="indexterm" data-primary="RAG (Retrieval-Augmented Generation)" data-secondary="LangChain and" id="raguglgchn"/><a data-type="indexterm" data-primary="LangChain" data-secondary="RAG (Retrieval-Augmented Generation) and" id="lcrgrgug"/><a data-type="indexterm" data-primary="LlamaIndex" id="id997"/><a data-type="indexterm" data-primary="vector databases" data-secondary="RAG and" id="vdtbssrg"/>for AI engineering, LangChain has a wide coverage of RAG techniques. Other frameworks like <a href="https://www.llamaindex.ai">LlamaIndex</a> focus specifically on RAG and are worth exploring for sophisticated use cases. As you are familiar with LangChain from <a data-type="xref" href="ch04.html#advanced_text_04">Chapter 4</a>, we’ll continue in this framework for the examples in this chapter. After manually performing RAG based on a desired context, let’s create a similar example using LCEL on four small text documents with FAISS:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_community.vectorstores.faiss</code> <code class="kn">import</code> <code class="n">FAISS</code>
<code class="kn">from</code> <code class="nn">langchain_core.output_parsers</code> <code class="kn">import</code> <code class="n">StrOutputParser</code>
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>
<code class="kn">from</code> <code class="nn">langchain_core.runnables</code> <code class="kn">import</code> <code class="n">RunnablePassthrough</code>
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code><code class="p">,</code> <code class="n">OpenAIEmbeddings</code>

<code class="c1"># 1. Create the documents:</code>
<code class="n">documents</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s2">"James Phoenix worked at JustUnderstandingData."</code><code class="p">,</code>
    <code class="s2">"James Phoenix currently is 31 years old."</code><code class="p">,</code>
    <code class="sd">"""Data engineering is the designing and building systems for collecting,</code>
<code class="sd">    storing, and analyzing data at scale."""</code><code class="p">,</code>
<code class="p">]</code>

<code class="c1"># 2. Create a vectorstore:</code>
<code class="n">vectorstore</code> <code class="o">=</code> <code class="n">FAISS</code><code class="o">.</code><code class="n">from_texts</code><code class="p">(</code><code class="n">texts</code><code class="o">=</code><code class="n">documents</code><code class="p">,</code> <code class="n">embedding</code><code class="o">=</code><code class="n">OpenAIEmbeddings</code><code class="p">())</code>
<code class="n">retriever</code> <code class="o">=</code> <code class="n">vectorstore</code><code class="o">.</code><code class="n">as_retriever</code><code class="p">()</code>

<code class="c1"># 3. Create a prompt:</code>
<code class="n">template</code> <code class="o">=</code> <code class="s2">"""Answer the question based only on the following context:</code>
<code class="s2">---</code>
<code class="s2">Context: </code><code class="si">{context}</code><code class="s2"/>
<code class="s2">---</code>
<code class="s2">Question: </code><code class="si">{question}</code><code class="s2"/>
<code class="s2">"""</code>
<code class="n">prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="n">template</code><code class="p">)</code>

<code class="c1"># 4. Create a chat model:</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code></pre>

<p>The code begins by importing necessary modules from the LangChain library and defines a list of text documents to be processed.</p>

<p>It utilizes <code>FAISS</code>, a library for efficient <a data-type="indexterm" data-primary="FAISS (Facebook AI Similarity Search)" id="id998"/>similarity search, to create a vector store from the text documents. This involves converting the texts into vector embeddings using OpenAI’s embedding model.</p>

<p>A prompt template for handling questions and a <code>ChatOpenAI</code> model are initialized for generating responses. Additionally, the prompt template enforces that the LLM only replies using the context provided from the retriever.</p>

<p>You’ll need to create an LCEL chain that will contain the <code>"context"</code> and <code>"question"</code> keys:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">chain</code> <code class="o">=</code> <code class="p">(</code>
    <code class="p">{</code><code class="s2">"context"</code><code class="p">:</code> <code class="n">retriever</code><code class="p">,</code> <code class="s2">"question"</code><code class="p">:</code> <code class="n">RunnablePassthrough</code><code class="p">()}</code>
    <code class="o">|</code> <code class="n">prompt</code>
    <code class="o">|</code> <code class="n">model</code>
    <code class="o">|</code> <code class="n">StrOutputParser</code><code class="p">()</code>
<code class="p">)</code></pre>

<p>By adding a retriever to <code>"context"</code>, it will automatically fetch four documents that are converted into a string value. Combined with the <code>"question"</code> key, these are then used to format the prompt. The LLM generates a response that is then parsed into a string value by <code>StrOutputParser()</code>.</p>

<p>You’ll invoke the chain and pass in your question that gets assigned to <code>"question"</code> and manually test three different queries:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"What is data engineering?"</code><code class="p">)</code>
<code class="c1"># 'Data engineering is the process of designing and building systems for</code>
<code class="c1"># collecting, storing, and analyzing data at scale.'</code>

<code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"Who is James Phoenix?"</code><code class="p">)</code>
<code class="c1"># 'Based on the given context, James Phoenix is a 31-year-old individual who</code>
<code class="c1"># worked at JustUnderstandingData.'</code>

<code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"What is the president of the US?"</code><code class="p">)</code>
<code class="c1"># I don't know</code></pre>

<p>Notice how the LLM only appropriately answered the first two queries because it didn’t have any relevant context contained within the vector database to answer the third query!</p>

<p>The LangChain implementation uses significantly less code, is easy to read, and allows you to rapidly implement retrieval <a data-type="indexterm" data-primary="RAG (Retrieval-Augmented Generation)" data-secondary="LangChain and" data-startref="raguglgchn" id="id999"/><a data-type="indexterm" data-primary="LangChain" data-secondary="RAG (Retrieval-Augmented Generation) and" data-startref="lcrgrgug" id="id1000"/><a data-type="indexterm" data-primary="vector databases" data-secondary="RAG and" data-startref="vdtbssrg" id="id1001"/>augmented generation.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Hosted Vector Databases with Pinecone"><div class="sect1" id="id90">
<h1>Hosted Vector Databases with Pinecone</h1>

<p>There are a number of hosted vector database providers <a data-type="indexterm" data-primary="vector databases" data-secondary="hosted, Pinecone and" id="vctdpce"/><a data-type="indexterm" data-primary="hosted vector databases, Pinecone and" id="hvdbpc"/><a data-type="indexterm" data-primary="Pinecone" id="pncne"/>emerging to support AI use cases, including <a href="https://www.trychroma.com">Chroma</a>, <a href="https://weaviate.io">Weaviate</a>, and <a href="https://www.pinecone.io">Pinecone</a>. Hosts of other types of databases are also offering vector search functionality, such as <a href="https://supabase.com">Supabase</a> with the <a href="https://oreil.ly/pgvector">pgvector add-on</a>. Examples in this book use Pinecone, as it is the current leader at the time of writing, but usage patterns are relatively consistent across providers and concepts should be transferrable.</p>

<p class="less_space pagebreak-before">Hosted vector databases offer several advantages over open source local vector stores:</p>
<dl>
<dt>Maintainance</dt>
<dd>
<p>With a hosted vector database, you don’t need to worry about setting up, managing, and maintaining the database yourself. This can save significant time and resources, especially for businesses that may not have dedicated DevOps or database management teams.</p>
</dd>
<dt>Scalability</dt>
<dd>
<p>Hosted vector databases are designed to scale with your needs. As your data grows, the database can automatically scale to handle the increased load, ensuring that your applications continue to perform efficiently.</p>
</dd>
<dt>Reliability</dt>
<dd>
<p>Managed services typically offer high availability with service-level agreements, as well as automatic backups and disaster recovery features. This can provide peace of mind and save you from potential data loss.</p>
</dd>
<dt>Performance</dt>
<dd>
<p>Hosted vector databases often have optimized infrastructure and algorithms that can provide better performance than self-managed, open source solutions. This can be particularly important for applications that rely on real-time or near-real-time vector search capabilities.</p>
</dd>
<dt>Support</dt>
<dd>
<p>With a hosted service, you typically get access to support from the company providing the service. This can be very helpful if you experience issues or need help optimizing your use of the database.</p>
</dd>
<dt>Security</dt>
<dd>
<p>Managed services often have robust security measures in place to protect your data, including things like encryption, access control, and monitoring. Major hosted providers are more likely to have the necessary compliance certificates and be in compliance with privacy legislation in regions like the EU.</p>
</dd>
</dl>

<p>Of course, this extra functionality comes at a cost, as well as a risk of overspending. As is the case with using Amazon Web Services, Microsoft Azure, or Google Cloud, stories of developers accidentally spending thousands of dollars through incorrect configuration or mistakes in code abound. There is also some risk of vendor lock-in, because although each vendor has similar functionality, they differ in certain areas, and as such it’s not quite straightforward to migrate between them. The other major consideration is privacy, because sharing data with a third party comes with security risks and potential legal implications.</p>

<p class="less_space pagebreak-before">The steps for working with a hosted vector database remain the same as when you set up your open source FAISS vector store. First, you chunk your documents and retrieve vectors; you then index your document chunks in the vector database, allowing you to retrieve similar records to your query, in order to insert into the prompt as context. First, let’s create an index in <a href="https://www.pinecone.io">Pinecone</a>, a popular commercial vector database vendor. Then log into Pinecone and retrieve your API key (visit API Keys in the side menu and click “create API Key”). The code for this example is provided in the <a href="https://oreil.ly/Q0rIw">GitHub repository</a> for this book.</p>

<p id="pinecone_vector_database">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">pinecone</code> <code class="kn">import</code> <code class="n">Pinecone</code><code class="p">,</code> <code class="n">ServerlessSpec</code>
<code class="kn">import</code> <code class="nn">os</code>

<code class="c1"># Initialize connection (get API key at app.pinecone.io):</code>
<code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="p">[</code><code class="s2">"PINECONE_API_KEY"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">"insert-your-api-key-here"</code>

<code class="n">index_name</code> <code class="o">=</code> <code class="s2">"employee-handbook"</code>
<code class="n">environment</code> <code class="o">=</code> <code class="s2">"us-west-2"</code>
<code class="n">pc</code> <code class="o">=</code> <code class="n">Pinecone</code><code class="p">()</code>  <code class="c1"># This reads the PINECONE_API_KEY env var</code>

<code class="c1"># Check if index already exists:</code>
<code class="c1"># (it shouldn't if this is first time)</code>
<code class="k">if</code> <code class="n">index_name</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">pc</code><code class="o">.</code><code class="n">list_indexes</code><code class="p">()</code><code class="o">.</code><code class="n">names</code><code class="p">():</code>
    <code class="c1"># if does not exist, create index</code>
    <code class="n">pc</code><code class="o">.</code><code class="n">create_index</code><code class="p">(</code>
        <code class="n">index_name</code><code class="p">,</code>
        <code class="c1"># Using the same vector dimensions as text-embedding-ada-002</code>
        <code class="n">dimension</code><code class="o">=</code><code class="mi">1536</code><code class="p">,</code>
        <code class="n">metric</code><code class="o">=</code><code class="s2">"cosine"</code><code class="p">,</code>
        <code class="n">spec</code><code class="o">=</code><code class="n">ServerlessSpec</code><code class="p">(</code><code class="n">cloud</code><code class="o">=</code><code class="s2">"aws"</code><code class="p">,</code> <code class="n">region</code><code class="o">=</code><code class="n">environment</code><code class="p">),</code>
    <code class="p">)</code>

<code class="c1"># Connect to index:</code>
<code class="n">index</code> <code class="o">=</code> <code class="n">pc</code><code class="o">.</code><code class="n">Index</code><code class="p">(</code><code class="n">index_name</code><code class="p">)</code>

<code class="c1"># View index stats:</code>
<code class="n">index</code><code class="o">.</code><code class="n">describe_index_stats</code><code class="p">()</code></pre>

<p>Output:</p>

<pre data-type="programlisting">{'dimension': 1536,
 'index_fullness': 0.0,
 'namespaces': {},
 'total_vector_count': 0}</pre>

<p class="less_space pagebreak-before">Let’s go through this code step-by-step:</p>
<dl>
<dt>1. Importing libraries</dt>
<dd>
<p>The script begins <a data-type="indexterm" data-primary="Pinecone" data-secondary="libraries, importing" id="id1002"/>with importing the necessary modules. <code>from pinecone import Pinecone, ServerlessSpec,</code> <code>import os</code> is used for accessing and setting environment variables.</p>
</dd>
<dt>2. Setting up the Pinecone API key</dt>
<dd>
<p>The Pinecone API key, <a data-type="indexterm" data-primary="Pinecone API key setup" id="id1003"/>which is crucial for authentication, is set as an environment variable using <code>os.environ["PINECONE_API_KEY"] = "insert-your-api-key-here"</code>. It’s important to replace <code>"insert-your-api-key-here"</code> with your actual Pinecone API key.</p>
</dd>
<dt>3. Defining index name and environment</dt>
<dd>
<p>The variables <code>index_name</code> and <code>environment</code> are set up. <code>index_name</code> is given the value <code>"employee-handbook"</code>, which is the name of the index to be created or accessed in the Pinecone database. The <code>environment</code> variable is assigned <code>"us-west-2"</code>, indicating the server’s location.</p>
</dd>
<dt>4. Initializing Pinecone connection</dt>
<dd>
<p>The connection <a data-type="indexterm" data-primary="Pinecone" data-secondary="connection initialization" id="id1004"/>to Pinecone is initialized with the <code>Pinecone()</code> constructor. This constructor automatically reads the <code>PINECONE_API_KEY</code> from the environment variable.</p>
</dd>
<dt>5. Checking for existing index</dt>
<dd>
<p>The script checks whether an index with the name <code>index_name</code> already exists in the Pinecone database. This is done through <code>pc.list_indexes().names()</code> functions, which returns a list of all existing index names.</p>
</dd>
<dt>6. Creating the index</dt>
<dd>
<p>If the index doesn’t exist, it is created using the <span><code>pc.​cre⁠ate_index()</code></span> function. This function is <a data-type="indexterm" data-primary="Pinecone" data-secondary="index" id="id1005"/>invoked with several parameters that configure the new index:</p>

<ul>
<li>
<p><code>index_name</code>: Specifies the name of the index</p>
</li>
<li>
<p><code>dimension=1536</code>: Sets the dimensionality of the vectors to be stored in the index</p>
</li>
<li>
<p><code>metric='cosine'</code>: Determines that the cosine similarity metric will be used for vector comparisons</p>
</li>
</ul>
</dd>
<dt>7. Connecting to the index</dt>
<dd>
<p>After verifying or creating the index, the script connects to it using <code>pc.Index(index_name)</code>. This connection is necessary for subsequent operations like inserting or querying data.</p>
</dd>
<dt>8. Index statistics</dt>
<dd>
<p><span>The script concludes with calling <code>index.describe_​index_stats()</code></span>, which retrieves and displays various statistics about the index, such as its dimensionality and the total count of vectors stored.</p>
</dd>
</dl>

<p>Next, you need to store your vectors in the newly created index, by looping through all the text chunks and vectors and upserting them as records in Pinecone. The database operation <code>upsert</code> is a combination of <em>update</em> and <em>insert</em>, and it either updates an existing record or inserts a new record if the record does not already exist (refer to  <a href="https://oreil.ly/YC-nV">this Jupyter Notebook</a> for the chunks variable).</p>

<p id="upserting_embeddings">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tqdm</code> <code class="kn">import</code> <code class="n">tqdm</code> <code class="c1"># For printing a progress bar</code>
<code class="kn">from</code> <code class="nn">time</code> <code class="kn">import</code> <code class="n">sleep</code>

<code class="c1"># How many embeddings you create and insert at once</code>
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">retry_limit</code> <code class="o">=</code> <code class="mi">5</code>  <code class="c1"># maximum number of retries</code>

<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">chunks</code><code class="p">),</code> <code class="n">batch_size</code><code class="p">)):</code>
    <code class="c1"># Find end of batch</code>
    <code class="n">i_end</code> <code class="o">=</code> <code class="nb">min</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">chunks</code><code class="p">),</code> <code class="n">i</code><code class="o">+</code><code class="n">batch_size</code><code class="p">)</code>
    <code class="n">meta_batch</code> <code class="o">=</code> <code class="n">chunks</code><code class="p">[</code><code class="n">i</code><code class="p">:</code><code class="n">i_end</code><code class="p">]</code>
    <code class="c1"># Get ids</code>
    <code class="n">ids_batch</code> <code class="o">=</code> <code class="p">[</code><code class="nb">str</code><code class="p">(</code><code class="n">j</code><code class="p">)</code> <code class="k">for</code> <code class="n">j</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">i</code><code class="p">,</code> <code class="n">i_end</code><code class="p">)]</code>
    <code class="c1"># Get texts to encode</code>
    <code class="n">texts</code> <code class="o">=</code> <code class="p">[</code><code class="n">x</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">meta_batch</code><code class="p">]</code>
    <code class="c1"># Create embeddings</code>
    <code class="c1"># (try-except added to avoid RateLimitError)</code>
    <code class="n">done</code> <code class="o">=</code> <code class="kc">False</code>
    <code class="k">try</code><code class="p">:</code>
        <code class="c1"># Retrieve embeddings for the whole batch at once</code>
        <code class="n">embeds</code> <code class="o">=</code> <code class="p">[]</code>
        <code class="k">for</code> <code class="n">text</code> <code class="ow">in</code> <code class="n">texts</code><code class="p">:</code>
            <code class="n">embedding</code> <code class="o">=</code> <code class="n">get_vector_embeddings</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
            <code class="n">embeds</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">embedding</code><code class="p">)</code>
        <code class="n">done</code> <code class="o">=</code> <code class="kc">True</code>
    <code class="k">except</code><code class="p">:</code>
        <code class="n">retry_count</code> <code class="o">=</code> <code class="mi">0</code>
        <code class="k">while</code> <code class="ow">not</code> <code class="n">done</code> <code class="ow">and</code> <code class="n">retry_count</code> <code class="o">&lt;</code> <code class="n">retry_limit</code><code class="p">:</code>
            <code class="k">try</code><code class="p">:</code>
                <code class="k">for</code> <code class="n">text</code> <code class="ow">in</code> <code class="n">texts</code><code class="p">:</code>
                    <code class="n">embedding</code> <code class="o">=</code> <code class="n">get_vector_embeddings</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
                    <code class="n">embeds</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">embedding</code><code class="p">)</code>
                <code class="n">done</code> <code class="o">=</code> <code class="kc">True</code>
            <code class="k">except</code><code class="p">:</code>
                <code class="n">sleep</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>
                <code class="n">retry_count</code> <code class="o">+=</code> <code class="mi">1</code>

    <code class="k">if</code> <code class="ow">not</code> <code class="n">done</code><code class="p">:</code>
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"""Failed to get embeddings after</code>
        <code class="si">{</code><code class="n">retry_limit</code><code class="si">}</code><code class="s2"> retries."""</code><code class="p">)</code>
        <code class="k">continue</code>

    <code class="c1"># Cleanup metadata</code>
    <code class="n">meta_batch</code> <code class="o">=</code> <code class="p">[{</code>
        <code class="s1">'batch'</code><code class="p">:</code> <code class="n">i</code><code class="p">,</code>
        <code class="s1">'text'</code><code class="p">:</code> <code class="n">x</code>
    <code class="p">}</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">meta_batch</code><code class="p">]</code>
    <code class="n">to_upsert</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">ids_batch</code><code class="p">,</code> <code class="n">embeds</code><code class="p">,</code> <code class="n">meta_batch</code><code class="p">))</code>

    <code class="c1"># Upsert to Pinecone</code>
    <code class="n">index</code><code class="o">.</code><code class="n">upsert</code><code class="p">(</code><code class="n">vectors</code><code class="o">=</code><code class="n">to_upsert</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">100% 13/13 [00:53&lt;00:00, 3.34s/it]</pre>

<p>Let’s break this code down:</p>
<ol>
<li>
<p>Import the necessary libraries <code>tqdm</code> and <code>time</code>. The library <code>tqdm</code> displays progress bars, and <code>time</code> provides the <code>sleep()</code> function, which is used in this script for retry logic.</p>
</li>
<li>
<p>Set the variable <code>batch_size</code> to 10 (normally set to 100 for real workloads), representing how many items will be processed at once in the upcoming loop. Also set the <code>retry_limit</code> to make sure we stop after five tries.</p>
</li>
<li>
<p>The <code>tqdm(range(0, len(chunks), batch_size))</code> part is a loop that will run from 0 to the length of <code>chunks</code> (defined previously), with a step of <code>batch_size</code>. <code>chunks</code> is a list of text to be processed. <code>tqdm</code> is used here to display a progress bar for this loop.</p>
</li>
<li>
<p>The <code>i_end</code> variable is calculated to be the smaller of the length of <code>chunks</code> or <code>i + batch_size</code>. This is used to prevent an Index Error if <code>i + batch_size</code> exceeds the length of <code>chunks</code>.</p>
</li>
<li>
<p><code>meta_batch</code> is a subset of <code>chunks</code> for the current batch. This is created by slicing the <code>chunks</code> list from index <code>i</code> to <code>i_end</code>.</p>
</li>
<li>
<p><code>ids_batch</code> is a list of string representations of the range <code>i</code> to <code>i_end</code>. These are IDs that are used to identify each item in <code>meta_batch</code>.</p>
</li>
<li>
<p><code>texts</code> list is just the text from <code>meta_batch</code>, ready for processing for embeddings.</p>
</li>
<li>
<p>Try to get the embeddings by calling <code>get_vector_embeddings()</code> with the <code>texts</code> as the argument. The result is stored in the variable <code>embeds</code>. This is done inside a <code>try-except</code> block to handle any exceptions that might be raised by this function, such as a rate limit error.</p>
</li>
<li>
<p>If an exception is raised, the script enters a <code>while</code> loop where it will sleep for five seconds and then tries again to retrieve the embeddings. It will continue this until successful or the number of retries is reached, at which point it sets <code>done = True</code> to exit the <code>while</code> loop.</p>
</li>
<li>
<p>Modify the <code>meta_batch</code> to be a list of dictionaries. Each dictionary has two keys: <code>batch</code>, which is set to the current batch number <code>i</code>, and <code>text</code>, which is set to the corresponding item in <code>meta_batch</code>. This is where you could add additional metadata for filtering queries later, such as the page, title, or chapter.</p>
</li>
<li>
<p>Create the <code>to_upsert</code> list by using the <code>zip</code> function to combine <code>ids_batch</code>, <code>embeds</code>, and <code>meta_batch</code> into tuples, and then turning that into a list. Each tuple contains the ID, the corresponding embedding, and the corresponding metadata for each item in the batch.</p>
</li>
<li>
<p>The last line of the loop calls a method <code>upsert</code> on <code>index</code>, a Pinecone (a vector database service) index. The <code>vectors=to_upsert</code> argument passes the <code>to_upsert</code> list as the data to be inserted or updated in the index. If a vector with a given ID already exists in the index, it will be updated; if it doesn’t exist, it will be inserted.</p>
</li>

</ol>

<p>Once the records are stored in Pinecone, you can query them as you need, just like when you saved your vectors locally with FAISS. Embeddings remain the same so long as you’re using the same embedding model to retrieve vectors for your query, so you do not need to update your database unless you have additional records or metadata to add.</p>

<p id="pinecone_vector_search">Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Retrieve from Pinecone</code>
<code class="n">user_query</code> <code class="o">=</code> <code class="s2">"do we get free unicorn rides?"</code>

<code class="k">def</code> <code class="nf">pinecone_vector_search</code><code class="p">(</code><code class="n">user_query</code><code class="p">,</code> <code class="n">k</code><code class="p">):</code>
    <code class="n">xq</code> <code class="o">=</code> <code class="n">get_vector_embeddings</code><code class="p">(</code><code class="n">user_query</code><code class="p">)</code>
    <code class="n">res</code> <code class="o">=</code> <code class="n">index</code><code class="o">.</code><code class="n">query</code><code class="p">(</code><code class="n">vector</code><code class="o">=</code><code class="n">xq</code><code class="p">,</code> <code class="n">top_k</code><code class="o">=</code><code class="n">k</code><code class="p">,</code> <code class="n">include_metadata</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">res</code>

<code class="n">pinecone_vector_search</code><code class="p">(</code><code class="n">user_query</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">{'matches':
    [{'id': '15',
    'metadata': {'batch': 10.0,
    'text': "You'll enjoy a treasure chest of perks, "
            'including unlimited unicorn rides, a '
            'bottomless cauldron of coffee and potions, '
            'and access to our company library filled '
            'with spellbinding books. We also offer '
            'competitive health and dental plans, '
            'ensuring your physical well-being is as '
            'robust as your magical spirit.\n'
            '\n'
            '**5: Continuous Learning and Growth**\n'
            '\n'
            'At Unicorn Enterprises, we believe in '
            'continuous learning and growth.'},
    'score': 0.835591,
    'values': []},],
 'namespace': ''}</pre>

<p>This script performs a nearest neighbors search using Pinecone’s API to identify the most similar vectors to a given input vector in a high-dimensional space. Here’s a step-by-step breakdown:</p>
<ol>
<li>
<p>The function <code>pinecone_vector_search</code> is defined with two parameters: <code>user_query</code> and <code>k</code>. <code>user_query</code> is the input text from the user, ready to be converted into a vector, and <code>k</code> indicates the number of closest vectors you want to retrieve.</p>
</li>
<li>
<p>Within the function, <code>xq</code> is defined by calling another function, <span><code>get_​vec⁠tor_embeddings(user_query)</code></span>. This function (defined previously) is responsible for transforming the <code>user_query</code> into a vector representation.</p>
</li>
<li>
<p>The next line performs a query on an object named <code>index</code>, a Pinecone index object, using the <code>query</code> method. The <code>query</code> method takes three parameters:</p>

<p>The first parameter is <code>vector=xq</code>, the vector representation of our <code>user_query</code>.</p>

<p>The second parameter, <code>top_k=k</code>, specifies that you want to return only the <code>k</code> closest vectors in the Pinecone index.</p>

<p>The third parameter, <code>include_metadata=True</code>, specifies that you want to include metadata (such as IDs or other associated data) with the returned results. If you wanted to <a href="https://oreil.ly/BBYD4">filter the results by metadata</a>, for example specifying the batch (or any other metadata you upserted), you could do this here by adding a fourth parameter: <code>filter={"batch": 1}</code>.</p>
</li>
<li>
<p>The results of the <code>query</code> method are assigned to <code>res</code> and then returned by the function.</p>
</li>
<li>
<p>Finally, the function <code>pinecone_vector_search</code> is called with arguments <code>user_query</code> and <code>k</code>, returning the response from Pinecone.</p>
</li>

</ol>

<p>You have now effectively emulated the job FAISS was doing, returning the relevant record from the handbook with a similarity search by vector. If you replace <code>vector_search(user_query, k)</code> with <code>pinecone_vector_search(user_query, k)</code> in the <code>search_and_chat</code> function (from the previous example), the chatbot will run the same, except that the vectors will be stored in a hosted Pinecone database instead of locally using FAISS.</p>

<p>When you upserted the records into Pinecone, you passed the batch number as metadata. Pinecone supports the following formats of metadata:</p>

<ul>
<li>
<p>String</p>
</li>
<li>
<p>Number (integer or floating-point, gets converted to a 64-bit floating point)</p>
</li>
<li>
<p>Booleans (true, false)</p>
</li>
<li>
<p>List of strings</p>
</li>
</ul>

<p>The metadata strategy for storing records can be just as important as the chunking strategy, as you can use metadata to filter queries. For example, if you wanted to only search for similarity limited to a specific batch number, you could add a filter to the <code>index.query</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">res</code> <code class="o">=</code> <code class="n">index</code><code class="o">.</code><code class="n">query</code><code class="p">(</code><code class="n">xq</code><code class="p">,</code> <code class="nb">filter</code><code class="o">=</code><code class="p">{</code>
        <code class="s2">"batch"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"$eq"</code><code class="p">:</code> <code class="mi">1</code><code class="p">}</code>
    <code class="p">},</code> <code class="n">top_k</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">include_metadata</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>This can be useful for limiting the scope of where you are searching for similarity. For example, it would allow you to store past conversations for all chatbots in the same vector database and then query only for past conversations related to a specific chatbot ID when querying to add context to that chatbot’s prompt. Other common uses of metadata filters include searching for more recent timestamps, for specific page numbers of documents, or for products over a certain price.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Note that more metadata storage is likely to increase storage costs, as is storing large chunks that are infrequently referenced. Understanding how vector databases work should give you license <a data-type="indexterm" data-primary="vector databases" data-secondary="hosted, Pinecone and" data-startref="vctdpce" id="id1006"/><a data-type="indexterm" data-primary="hosted vector databases, Pinecone and" data-startref="hvdbpc" id="id1007"/><a data-type="indexterm" data-primary="Pinecone" data-startref="pncne" id="id1008"/>to experiment with different chunking and metadata strategies and see what works for your use cases.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Self-Querying"><div class="sect1" id="id91">
<h1>Self-Querying</h1>

<p>Retrieval can get quite sophisticated, and <a data-type="indexterm" data-primary="vector databases" data-secondary="queries, self-querying" id="vcdbqfqy"/><a data-type="indexterm" data-primary="self-querying vector databases" id="sqyvdb"/>you’re not limited to a basic retriever that fetches documents from a vector database based purely on semantic relevance. For example, consider using metadata from within a user’s query. By recognizing and extracting such filters, your retriever can autonomously generate a new query to execute against the vector database, as in the structure depicted in <a data-type="xref" href="#figure-5-3">Figure 5-3</a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>This approach also generalizes to NoSQL, SQL, or any common database, and it is not solely limited to vector databases.</p>
</div>

<figure><div id="figure-5-3" class="figure">
<img src="assets/pega_0503.png" alt="Self querying" width="600" height="120"/>
<h6><span class="label">Figure 5-3. </span>A self-querying retriever architecture</h6>
</div></figure>

<p><a href="https://oreil.ly/39rgU">Self-querying</a> yields several significant benefits:</p>
<dl>
<dt>Schema definition</dt>
<dd>
<p>You can establish a schema reflecting anticipated user descriptions, enabling a structured understanding of the information sought by users.</p>
</dd>
<dt>Dual-layer retrieval</dt>
<dd>
<p>The retriever performs a two-tier operation. First, it gauges the semantic similarity between the user’s input and the database’s contents. Simultaneously, it discerns and applies filters based on the metadata of the stored documents or rows, ensuring an even more precise and relevant retrieval.</p>
</dd>
</dl>

<p>This method maximizes the retriever’s potential in serving user-specific requests.</p>

<p>Install <code>lark</code> on the terminal with <code>pip install lark</code>.</p>

<p>In the subsequent code, essential modules such as <code>langchain</code>, <code>lark</code>, <code>getpass</code>, and <code>chroma</code> are imported. For a streamlined experience, potential warnings are suppressed:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_core.documents</code> <code class="kn">import</code> <code class="n">Document</code>
<code class="kn">from</code> <code class="nn">langchain_community.vectorstores.chroma</code> <code class="kn">import</code> <code class="n">Chroma</code>
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">OpenAIEmbeddings</code>
<code class="kn">import</code> <code class="nn">lark</code>
<code class="kn">import</code> <code class="nn">getpass</code>
<code class="kn">import</code> <code class="nn">os</code>
<code class="kn">import</code> <code class="nn">warnings</code>

<code class="c1"># Disabling warnings:</code>
<code class="n">warnings</code><code class="o">.</code><code class="n">filterwarnings</code><code class="p">(</code><code class="s2">"ignore"</code><code class="p">)</code></pre>

<p>In the upcoming section, you’ll craft a list named <code>docs</code>, filling it with detailed instances of the <code>Document</code> class. Each <code>Document</code> lets you capture rich details of a book. Within the metadata dictionary, you’ll store valuable information such as the title, author, and genre. You’ll also include data like the ISBN, the publisher, and a concise summary to give you a snapshot of each story. The <code>"rating"</code> offers a hint of its popularity. By setting up your data this way, you’re laying the groundwork for a systematic and insightful exploration of a diverse library:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">docs</code> <code class="o">=</code> <code class="p">[</code>
    <code class="n">Document</code><code class="p">(</code>
        <code class="n">page_content</code><code class="o">=</code><code class="s2">"A tale about a young wizard and his </code><code class="se">\</code>
<code class="s2">            journey in a magical school."</code><code class="p">,</code>
        <code class="n">metadata</code><code class="o">=</code><code class="p">{</code>
            <code class="s2">"title"</code><code class="p">:</code> <code class="s2">"Harry Potter and the Philosopher's Stone"</code><code class="p">,</code>
            <code class="s2">"author"</code><code class="p">:</code> <code class="s2">"J.K. Rowling"</code><code class="p">,</code>
            <code class="s2">"year_published"</code><code class="p">:</code> <code class="mi">1997</code><code class="p">,</code>
            <code class="s2">"genre"</code><code class="p">:</code> <code class="s2">"Fiction"</code><code class="p">,</code>
            <code class="s2">"isbn"</code><code class="p">:</code> <code class="s2">"978-0747532699"</code><code class="p">,</code>
            <code class="s2">"publisher"</code><code class="p">:</code> <code class="s2">"Bloomsbury"</code><code class="p">,</code>
            <code class="s2">"language"</code><code class="p">:</code> <code class="s2">"English"</code><code class="p">,</code>
            <code class="s2">"page_count"</code><code class="p">:</code> <code class="mi">223</code><code class="p">,</code>
            <code class="s2">"summary"</code><code class="p">:</code> <code class="s2">"The first book in the Harry Potter </code><code class="se">\</code>
<code class="s2">            series where Harry discovers his magical </code><code class="se">\</code>
<code class="s2">            heritage."</code><code class="p">,</code>
            <code class="s2">"rating"</code><code class="p">:</code> <code class="mf">4.8</code><code class="p">,</code>
        <code class="p">},</code>
    <code class="p">),</code>
    <code class="c1"># ... More documents ...</code>
<code class="p">]</code></pre>

<p>You’ll import <code>ChatOpenAI</code>, <code>SelfQueryRetriever</code>, and <code>OpenAIEmbeddings</code>. Following this, you’ll create a new vector database using the <code>Chroma.from_documents(..)</code> method.</p>

<p>Next, the <code>AttributeInfo</code> class is used to structure metadata for each book. Through this class, you’ll systematically specify the attribute’s name, description, and type. By curating a list of <code>AttributeInfo</code> entries, the self-query retriever can perform metadata filtering:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langchain.retrievers.self_query.base</code> \
    <code class="kn">import</code> <code class="nn">SelfQueryRetriever</code>
<code class="kn">from</code> <code class="nn">langchain.chains.query_constructor.base</code> \
    <code class="kn">import</code> <code class="nn">AttributeInfo</code>

<code class="c1"># Create the embeddings and vectorstore:</code>
<code class="n">embeddings</code> <code class="o">=</code> <code class="n">OpenAIEmbeddings</code><code class="p">()</code>
<code class="n">vectorstore</code> <code class="o">=</code> <code class="n">Chroma</code><code class="o">.</code><code class="n">from_documents</code><code class="p">(</code><code class="n">docs</code><code class="p">,</code> <code class="n">OpenAIEmbeddings</code><code class="p">())</code>

<code class="c1"># Basic Info</code>
<code class="n">basic_info</code> <code class="o">=</code> <code class="p">[</code>
    <code class="n">AttributeInfo</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"title"</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"The title of the book"</code><code class="p">,</code>
    <code class="nb">type</code><code class="o">=</code><code class="s2">"string"</code><code class="p">),</code>
    <code class="n">AttributeInfo</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"author"</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"The author of the book"</code><code class="p">,</code>
    <code class="nb">type</code><code class="o">=</code><code class="s2">"string"</code><code class="p">),</code>
    <code class="n">AttributeInfo</code><code class="p">(</code>
        <code class="n">name</code><code class="o">=</code><code class="s2">"year_published"</code><code class="p">,</code>
        <code class="n">description</code><code class="o">=</code><code class="s2">"The year the book was published"</code><code class="p">,</code>
        <code class="nb">type</code><code class="o">=</code><code class="s2">"integer"</code><code class="p">,</code>
    <code class="p">),</code>
<code class="p">]</code>

<code class="c1"># Detailed Info</code>
<code class="n">detailed_info</code> <code class="o">=</code> <code class="p">[</code>
    <code class="n">AttributeInfo</code><code class="p">(</code>
        <code class="n">name</code><code class="o">=</code><code class="s2">"genre"</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"The genre of the book"</code><code class="p">,</code>
        <code class="nb">type</code><code class="o">=</code><code class="s2">"string or list[string]"</code>
    <code class="p">),</code>
    <code class="n">AttributeInfo</code><code class="p">(</code>
        <code class="n">name</code><code class="o">=</code><code class="s2">"isbn"</code><code class="p">,</code>
        <code class="n">description</code><code class="o">=</code><code class="s2">"The International Standard Book Number for the book"</code><code class="p">,</code>
        <code class="nb">type</code><code class="o">=</code><code class="s2">"string"</code><code class="p">,</code>
    <code class="p">),</code>
    <code class="n">AttributeInfo</code><code class="p">(</code>
        <code class="n">name</code><code class="o">=</code><code class="s2">"publisher"</code><code class="p">,</code>
        <code class="n">description</code><code class="o">=</code><code class="s2">"The publishing house that published the book"</code><code class="p">,</code>
        <code class="nb">type</code><code class="o">=</code><code class="s2">"string"</code><code class="p">,</code>
    <code class="p">),</code>
    <code class="n">AttributeInfo</code><code class="p">(</code>
        <code class="n">name</code><code class="o">=</code><code class="s2">"language"</code><code class="p">,</code>
        <code class="n">description</code><code class="o">=</code><code class="s2">"The primary language the book is written in"</code><code class="p">,</code>
        <code class="nb">type</code><code class="o">=</code><code class="s2">"string"</code><code class="p">,</code>
    <code class="p">),</code>
    <code class="n">AttributeInfo</code><code class="p">(</code>
        <code class="n">name</code><code class="o">=</code><code class="s2">"page_count"</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"Number of pages in the book"</code><code class="p">,</code>
        <code class="nb">type</code><code class="o">=</code><code class="s2">"integer"</code>
    <code class="p">),</code>
<code class="p">]</code>

<code class="c1"># Analysis</code>
<code class="n">analysis</code> <code class="o">=</code> <code class="p">[</code>
    <code class="n">AttributeInfo</code><code class="p">(</code>
        <code class="n">name</code><code class="o">=</code><code class="s2">"summary"</code><code class="p">,</code>
        <code class="n">description</code><code class="o">=</code><code class="s2">"A brief summary or description of the book"</code><code class="p">,</code>
        <code class="nb">type</code><code class="o">=</code><code class="s2">"string"</code><code class="p">,</code>
    <code class="p">),</code>
    <code class="n">AttributeInfo</code><code class="p">(</code>
        <code class="n">name</code><code class="o">=</code><code class="s2">"rating"</code><code class="p">,</code>
        <code class="n">description</code><code class="o">=</code><code class="s2">"""An average rating for the book (from reviews), ranging</code>
<code class="s2">        from 1-5"""</code><code class="p">,</code>
        <code class="nb">type</code><code class="o">=</code><code class="s2">"float"</code><code class="p">,</code>
    <code class="p">),</code>
<code class="p">]</code>

<code class="c1"># Combining all lists into metadata_field_info</code>
<code class="n">metadata_field_info</code> <code class="o">=</code> <code class="n">basic_info</code> <code class="o">+</code> <code class="n">detailed_info</code> <code class="o">+</code> <code class="n">analysis</code></pre>

<p>Let’s run this through step-by-step:</p>
<ol>
<li>
<p>Import <code>ChatOpenAI</code>, <code>SelfQueryRetriever</code>, and <code>AttributeInfo</code> from the LangChain modules for chat model integration, self-querying, and defining metadata attributes.</p>
</li>
<li>
<p>Create an <code>OpenAIEmbeddings</code> instance for handling OpenAI model embeddings.</p>
</li>
<li>
<p>A <code>Chroma</code> vector database is created from the documents.</p>
</li>
<li>
<p>Define three lists (<code>basic_info</code>, <code>detailed_info</code>, <code>analysis</code>), each containing <code>AttributeInfo</code> objects for different types of book metadata.</p>
</li>
<li>
<p>Combine these lists into a single list, <code>metadata_field_info</code>, for comprehensive book metadata management.</p>
</li>

</ol>

<p>Now, set up a <code>ChatOpenAI</code> model and assign a <code>document_content_description</code> to specify what content type you’re working with. The <code>SelfQueryRetriever</code> then uses this along with your LLM to fetch relevant documents from your <code>vectorstore</code>. With a simple query, such as asking for sci-fi books, the <code>invoke</code> method scans through the dataset and returns a list of <code>Document</code> objects.</p>

<p>Each <code>Document</code> encapsulates valuable metadata about the book, like the genre, author, and a brief summary, transforming the results into organized, rich data for your application:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">document_content_description</code> <code class="o">=</code> <code class="s2">"Brief summary of a movie"</code>
<code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">retriever</code> <code class="o">=</code> <code class="n">SelfQueryRetriever</code><code class="o">.</code><code class="n">from_llm</code><code class="p">(</code>
    <code class="n">llm</code><code class="p">,</code> <code class="n">vectorstore</code><code class="p">,</code> <code class="n">document_content_description</code><code class="p">,</code> <code class="n">metadata_field_info</code>
<code class="p">)</code>

<code class="c1"># Looking for sci-fi books</code>
<code class="n">retriever</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"What are some sci-fi books?"</code><code class="p">)</code>
<code class="c1"># [Document(page_content='''A futuristic society where firemen burn books to</code>
<code class="c1"># maintain order.''', metadata={'author': 'Ray Bradbury', 'genre': '...</code>
<code class="c1"># More documents..., truncated for brevity</code></pre>
<div data-type="tip"><h1>Evaluate Quality</h1>
<p>By setting the temperature to zero, you <a data-type="indexterm" data-primary="Evaluate Quality principle" data-secondary="self-querying vector databases" id="id1009"/>instruct the model to prioritize generating consistent metadata filtering outputs, rather than being more creative and therefore inconsistent. These metadata filters are then leveraged against the vector database to retrieve relevant documents.</p>
</div>

<p>When you want to fetch books from a specific author, you’re directing the <code>retriever</code> to pinpoint books authored by <code>J.K. Rowling</code>. The <code>Comparison</code> function with the <code>EQ</code> (equals) comparator ensures the retrieved documents have their <code>'author'</code> attribute precisely matching <code>'J.K. Rowling'</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Querying for a book by J.K. Rowling:</code>
<code class="n">retriever</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code>
    <code class="sd">'''I want some books that are published by the</code>
<code class="sd">    author J.K.Rowling'''</code>
<code class="p">)</code>
<code class="c1"># query=' ' filter=Comparison(comparator=&lt;Comparator.EQ:</code>
<code class="c1"># 'eq'&gt;, attribute='author', value='J.K. Rowling')</code>
<code class="c1"># limit=None</code>
<code class="c1"># Documents [] omitted to save space</code></pre>

<p>Initializing the <code>SelfQueryRetriever</code> with an added <code>enable_limit</code> flag set to <code>True</code> allows you to dictate the number of results returned. Then, you craft a query to obtain precisely <code>2 Fantasy</code> books. By using the <code>Comparison</code> function with the <code>EQ</code> (equals) comparator on the <code>'genre'</code> attribute, the retriever zeros in on <code>'Fantasy'</code> titles. The <code>limit</code> parameter ensures you get only two results, optimizing your output for <a data-type="indexterm" data-primary="vector databases" data-secondary="queries, self-querying" data-startref="vcdbqfqy" id="id1010"/><a data-type="indexterm" data-primary="self-querying vector databases" data-startref="sqyvdb" id="id1011"/>precision and brevity:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">retriever</code> <code class="o">=</code> <code class="n">SelfQueryRetriever</code><code class="o">.</code><code class="n">from_llm</code><code class="p">(</code>
    <code class="n">llm</code><code class="p">,</code>
    <code class="n">vectorstore</code><code class="p">,</code>
    <code class="n">document_content_description</code><code class="p">,</code>
    <code class="n">metadata_field_info</code><code class="p">,</code>
    <code class="n">enable_limit</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">retriever</code><code class="o">.</code><code class="n">get_relevant_documents</code><code class="p">(</code>
    <code class="n">query</code><code class="o">=</code><code class="s2">"Return 2 Fantasy books"</code><code class="p">,</code>
<code class="p">)</code>
<code class="c1"># query=' ' filter=Comparison(</code>
<code class="c1">#    comparator=&lt;Comparator.EQ: 'eq'&gt;, attribute='genre',</code>
<code class="c1">#   value='Fantasy') limit=2</code>
<code class="c1"># Documents [] omitted to save space</code></pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Alternative Retrieval Mechanisms"><div class="sect1" id="id92">
<h1>Alternative Retrieval Mechanisms</h1>

<p>When it comes to retrieval <a data-type="indexterm" data-primary="retrieval mechanisms" id="rtrvch"/>implementations, various intriguing methods each demonstrate their distinct approaches, advantages, and limitations:</p>
<dl>
<dt>MultiQueryRetriever</dt>
<dd>
<p>The <a href="https://oreil.ly/uuzpG">MultiQueryRetriever</a> aims to overcome the limitations <a data-type="indexterm" data-primary="vector databases" data-secondary="MultiQueryRetriever" id="id1012"/><a data-type="indexterm" data-primary="MultiQueryRetriever" id="id1013"/>of distance-based retrieval by generating multiple queries from different perspectives for a given user input query. This leads to the generation of a larger set of potentially relevant documents, offering broader insights. However, challenges may arise if the different queries produce contradicting results or overlap.</p>
</dd>
<dt>Contextual Compression</dt>
<dd>
<p>The <a href="https://oreil.ly/wzqVg">Contextual Compression  Retriever</a> handles long documents <a data-type="indexterm" data-primary="vector databases" data-secondary="Contextual Compression" id="id1014"/><a data-type="indexterm" data-primary="Contextual Compression" id="id1015"/>by compressing irrelevant parts, ensuring relevance to context. The challenge with this method is the expertise needed to determine the relevance and importance of information.</p>
</dd>
<dt>Ensemble Retriever</dt>
<dd>
<p>The <a href="https://oreil.ly/jIuJh">Ensemble Retriever</a> uses a list of retrievers and <a data-type="indexterm" data-primary="vector databases" data-secondary="Ensemble Retriever" id="id1016"/><a data-type="indexterm" data-primary="Ensemble Retriever" id="id1017"/>combines their results. It’s essentially a “hybrid” search methodology that leverages the strengths of various algorithms. However, the Ensemble Retriever implies more computational workload due to the use of multiple retrieval algorithms, potentially affecting retrieval speed.</p>
</dd>
<dt>Parent Document Retriever</dt>
<dd>
<p>The <a href="https://oreil.ly/jXSXQ">Parent Document Retriever</a> ensures the <a data-type="indexterm" data-primary="vector databases" data-secondary="Parent Document Retriever" id="id1018"/><a data-type="indexterm" data-primary="Parent Document Retriever" id="id1019"/>maintenance of rich document backgrounds by retrieving original source documents from which smaller chunks are derived. But it might increase computational requirements due to the retrieval of larger parent documents.</p>
</dd>
<dt>Time-Weighted Vector Store Retriever</dt>
<dd>
<p>The <a href="https://oreil.ly/9JbTt">Time-Weighted Vector Store Retriever</a> incorporates <em>time decay</em> into document retrieval. Despite its advantages, <a data-type="indexterm" data-primary="vector databases" data-secondary="Time-Weighted Vector Store Retriever" id="id1020"/><a data-type="indexterm" data-primary="Time-Weighted Vector Store Retriever" id="id1021"/>the time decay factor might cause overlooking of relevant older documents, risking the loss of historical context.</p>
</dd>
</dl>

<p>The key to effective retrieval is understanding the trade-offs and selecting the method, or combination of methods, that best address your specific use case. Vector search adds additional cost and latency to your application, so ensure in testing you find that the additional context is worth it. For heavy workloads, paying the up-front cost of fine-tuning a custom model may be beneficial compared to the ongoing additional cost of prompts plus embeddings plus vector storage. In other scenarios, providing static examples of correct work in the prompt may work fine. However, when you need to pull in context to a prompt dynamically, based on similarity rather than a direct keyword search, there’s no real substitute for RAG <a data-type="indexterm" data-primary="retrieval mechanisms" data-startref="rtrvch" id="id1022"/>using a vector database.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id352">
<h1>Summary</h1>

<p>In this chapter, you learned about the power of vector databases for storing and querying text based on similarity. By searching for the most similar records, vector databases can retrieve relevant information to provide context in your prompts, helping AI models stay within token limits and avoid unnecessary costs or irrelevant data. You also discovered that the accuracy of vectors depends on the underlying model and saw examples where they may fail.</p>

<p>Furthermore, you explored the process of indexing documents in a vector database, searching for similar records using vectors, and inserting records into prompts as context, called RAG. In this chapter, you went through code examples for retrieving embeddings from both the OpenAI API and open source models like the Sentence Transformers library. You also learned the cost and benefits associated with retrieving embeddings from OpenAI relative to alternative options.</p>

<p>In the next chapter on autonomous agents, you will enter the futuristic world of AI agents that can make decisions and take actions on their own. You will learn about the different types of autonomous agents, their capabilities, and how they can be trained to perform specific tasks. Additionally, you will explore the challenges and unreliability issues associated with agents.</p>
</div></section>
</div></section></div></div></body></html>