<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>chapter-6</title>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css" />
 </head>
 <body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">6</span> </span><span class="chapter-title-text">Progression of RAG systems: Na&iuml;ve, advanced, and modular RAG</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header"><span class="CharOverride-2">This chapter covers</span></h3> 
   <ul> 
    <li class="readable-text" id="p2"><span class="CharOverride-3">Limitations of the na&iuml;ve RAG approach</span></li> 
    <li class="readable-text" id="p3"><span class="CharOverride-3">Advanced RAG strategies and techniques</span></li> 
    <li class="readable-text" id="p4"><span class="CharOverride-3">Modular patterns in RAG</span></li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>In the first two parts of this book, you learned about the utility of retrieval-augmented generation (RAG), along with the development and evaluation of a basic RAG system. The basic, or the na&iuml;ve RAG approach that we have discussed is, generally, inadequate when it comes to production-grade systems. </p> 
  </div> 
  <div class="readable-text intended-text" id="p6"> 
   <p>This chapter focuses on more advanced concepts in RAG. We begin by revisiting the limitations and the points of failure of the na&iuml;ve RAG approach. Next, we discuss the failures at the retrieval, augmentation, and generation stages. Advanced strategies and techniques to address these points of failure will be elaborated on in distinct phases of the RAG pipeline.</p> 
  </div> 
  <div class="readable-text intended-text" id="p7"> 
   <p>Better indexing of the knowledge base leads to better RAG outcomes. We will look at a few data indexing strategies that build on the na&iuml;ve indexing pipeline to improve the searchability of the knowledge base. </p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>In the generation pipeline, improvements are examined in three stages: pre-retrieval, retrieval, and post-retrieval. Pre-retrieval techniques focus on manipulating and improving the input user query. Retrieval strategies focus on better matching of the user query to the documents in the knowledge base. Finally, in the post-retrieval stage, the focus is on aligning the retrieved context with the desired result and making it suitable for generation. </p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>The last part of the chapter discusses a modular approach to RAG that has been emerging to find applicability in RAG systems. The modular approach is an architectural enhancement to the basic RAG system.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>Note that the strategies and techniques for RAG improvement are expansive, and this chapter highlights a few popular ones. The chapter is interspersed with code examples, but for a more exhaustive supporting code, check out the source code repository of this book.</p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>By the end of this chapter, you should</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p12">Understand why the na&iuml;ve approach to RAG is not suitable for production.</li> 
   <li class="readable-text" id="p13">Be aware of indexing strategies that make the RAG knowledge base more efficient.</li> 
   <li class="readable-text" id="p14">Know some of the popular pre-retrieval, retrieval, and post-retrieval techniques.</li> 
   <li class="readable-text" id="p15">Be familiar with the modular approach to RAG.</li> 
  </ul> 
  <div class="readable-text" id="p16"> 
   <p>RAG powers a variety of AI applications. However, there is a certain aspect of uncertainty when it comes to outcomes. Inaccuracies in retrieval, disjointed context, and incoherence in the LLM outputs need to be addressed before taking RAG to production. In a very short time, researchers and practitioners have experimented with innovative techniques to improve the relevance and faithfulness of RAG systems. But before we look at these techniques, it is important to understand why a na&iuml;ve RAG approach often doesn’t find its way into a production environment.</p> 
  </div> 
  <div class="readable-text" id="p17"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.1</span> Limitations of na&iuml;ve RAG</h2> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>Na&iuml;ve RAG can be thought of as the earliest form of RAG, which gained popularity after the release of ChatGPT and the rise of LLM technology. As we have seen so far, it follows a linear process of indexing, retrieving, augmenting, and generation. This process falls in a “retrieve then read” framework, which means that there’s a retriever retrieving information and that there’s an LLM reading this information to generate the results, as shown in figure 6.1.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p19">  
   <img src="../Images/CH06_F01_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 6.1</span><span class=""> </span><span class="">Na&iuml;ve RAG is a sequential “retrieve then read” process.</span></h5>
  </div> 
  <div class="readable-text intended-text" id="p20"> 
   <p>The na&iuml;ve RAG approach is marred with drawbacks at each of the three stages: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p21"><em>Retrieval</em>—Na&iuml;ve retrieval is often observed to have low precision that leads to irrelevant information being retrieved. It also has a low recall, which means that relevant information is missed, which leads to incomplete results. </li> 
   <li class="readable-text" id="p22"><em>Augmentation</em>—There is a real possibility of redundancy and repetition when multiple retrieved documents have similar information. Also, when information is sourced from different documents, the context becomes disjointed. There’s also the problem of context length of the LLMs that has an effect on the volume of retrieved context that can be passed on to the LLM for generation. </li> 
   <li class="readable-text" id="p23"><em>Generation</em>—With the inadequacies of the upstream processes, the generation suffers from hallucination and lack of groundedness of the generated content. The LLM faces challenge in reconciling information. The challenges of toxicity and bias also persist. It is also noticed sometimes that the LLM becomes over-reliant on the retrieved context and forgets to draw from its own parametric memory. </li> 
  </ul> 
  <div class="readable-text" id="p24"> 
   <p>Figure 6.2 summarizes these drawbacks.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p25">  
   <img src="../Images/CH06_F02_Kimothi.png" alt="A black box with black text

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 6.2</span><span class=""> </span><span class="">Drawbacks of na&iuml;ve RAG at each stage of the process</span></h5>
  </div> 
  <div class="readable-text" id="p26"> 
   <p>In the last few years, a lot of research and experimentation has been done to address these drawbacks. Early approaches involved pre-training language models. Techniques involving fine-tuning of the LLMs, embeddings models, and retrievers have also been tried. These techniques require training data and re-computation of model weights, generally using supervised learning techniques. Since this book is a foundational guide, we will not go into these complex techniques.</p> 
  </div> 
  <div class="readable-text intended-text" id="p27"> 
   <p>This chapter covers some interventions, techniques, and strategies used at different stages of the two RAG pipelines: the indexing and generation pipeline. Although the array of such interventions is endless, some of the more popular ones are highlighted in the subsequent sections. </p> 
  </div> 
  <div class="readable-text" id="p28"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.2</span> Advanced RAG techniques</h2> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>Advanced techniques in RAG have continued to emerge since the earliest experiments with na&iuml;ve RAG. There are three stages in which we can discuss these techniques: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p30"><em>Pre-retrieval stag</em><em>e</em>—As the name suggests, certain interventions can be employed before the retriever comes into action. This broadly covers two aspects:  
    <ul> 
     <li><em>Index optimization</em>—The way documents are stored in the knowledge base</li> 
     <li><em>Query optimization</em>—Optimizing the user query so it aligns better with the retrieval and generation tasks</li> 
    </ul> </li> 
   <li class="readable-text" id="p31"><em>Retrieval stag</em><em>e</em>—Certain strategies can improve the recall and precision of the retrieval process. This goes beyond the capability of the underlying retrieval algorithms discussed in chapter 4.</li> 
   <li class="readable-text" id="p32"><em>Post-retrieval stag</em><em>e</em>—Once the information has been retrieved, the context can be further optimized to better align with the generation task and the downstream LLM. </li> 
  </ul> 
  <div class="readable-text" id="p33"> 
   <p>With techniques employed at these three stages, the advanced RAG process follows a “rewrite then retrieve then re-rank then read” frameworks. Two additional components of rewrite and re-rank are added, and the retrieve component is enhanced in comparison with na&iuml;ve RAG. This structure is presented in figure 6.3.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p34">  
   <img src="../Images/CH06_F03_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 6.3</span><span class=""> </span><span class="">Advanced RAG is a rewrite–retrieve–re-rank–read process, as compared to a retrieve–read na&iuml;ve RAG process.</span></h5>
  </div> 
  <div class="readable-text" id="p35"> 
   <p>We now explore these components one by one, beginning with the pre-retrieval stage.</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.3</span> Pre-retrieval techniques</h2> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>The primary objective of employing pre-retrieval techniques is to facilitate better retrieval. We have noted that the retrieval stage of na&iuml;ve RAG suffers from low recall and low precision—irrelevant information is retrieved, and not all relevant information is retrieved. This can happen mainly because of two reasons: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p38"><em>Knowledge base is not suited for retrieval</em>. If the information in the knowledge base is not stored in a manner that is easy to search through, then the quality of retrieval will remain suboptimal. To address this problem, <em>index optimization</em> is done in the indexing pipeline for more efficient storage of the knowledge base.</li> 
   <li class="readable-text" id="p39"><em>Retriever doesn’t completely understand the input query.</em> In generative AI applications, the control over the user query is generally limited. The level of detail a user provides is subjective. The retriever sometimes may misunderstand or not completely understand the context of the user query. <em>Query optimization</em> addresses this aspect of the challenge with the na&iuml;ve RAG.</li> 
  </ul> 
  <div class="readable-text" id="p40"> 
   <p>Both index and query optimizations are carried out before the retriever is invoked. This is the only stage that recommends interventions both in the indexing and generation pipeline. We will look at a few techniques for each of these.</p> 
  </div> 
  <div class="readable-text" id="p41"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.3.1</span> Index optimization</h3> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>Index optimization is employed in the indexing pipeline. The objective of index optimization is to set up the knowledge base for better retrieval. Some of the popular strategies are as follows.</p> 
  </div> 
  <div class="readable-text" id="p43"> 
   <h4 class=" readable-text-h4">Chunk optimization</h4> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>Chapter 3 discussed the significance of chunking in the indexing pipeline. Chunking large documents into smaller segments plays a crucial role in retrieval and handling the context length limits of LLMs. Certain techniques aim for better chunking and efficient retrieval of the chunks, such as</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p45"><em>Chunk size optimizatio</em><em>n</em>—The size of the chunks can have a significant effect on the quality of the RAG system. While large-sized chunks provide better context, they also carry a lot of noise. Smaller chunks, however, have precise information, but they might miss important information. For instance, consider a legal document that’s 10,000 words long. If we chunk it into 1,000-word segments, each chunk might contain multiple legal clauses, making it hard to retrieve specific information. Conversely, chunking it into 200-word segments allows for more precise retrieval of individual clauses, but may lose the context provided by surrounding clauses. Experimenting with chunk sizes can help find the optimal balance for accurate retrieval. The processing time also depends on the chunk size. Chunk size, therefore, has a significant effect on retrieval accuracy, processing speed, and storage efficiency. The ideal chunk size varies with the use case and depends on balancing factors such as document types and structure, complexity of user query, and the desired response time. There is no one-size-fits-all approach to optimizing chunk sizes. Experimentation and evaluation of different chunk sizes on metrics such as faithfulness, relevance, and response time (as discussed in chapter 5) can help in identifying the optimal chunk size for the RAG system. Chunk size optimization may require periodic reassessment as data or requirements change.</li> 
   <li class="readable-text" id="p46"><em>Context-enriched chunkin</em><em>g</em>—This method adds the summary of the larger document to each chunk to enrich the context of the smaller chunk. This makes more context available to the LLM without adding too much noise. It also improves the retrieval accuracy and maintains semantic coherence across chunks. This feature is particularly useful in scenarios where a more holistic view of the information is crucial. While this approach enhances the understanding of the broader context, it adds a level of complexity and comes at the cost of higher computational requirements, increased storage needs, and possible latency in retrieval. Here is an example of how context enrichment can be done using GPT-4o-mini, OpenAI embeddings, and FAISS:</li> 
  </ul> 
  <div class="browsable-container listing-container" id="p47"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_community.document_loaders #1
import AsyncHtmlLoader   #1
from langchain_community.document_transformers  #1
import Html2TextTransformer   #1
url=		 #1
https://en.wikipedia.org/wiki/2023_Cricket_World_Cup   #1
loader = AsyncHtmlLoader (url)   #1
data = loader.load()   #1
html2text = Html2TextTransformer()   #1
document_text=data_transformed[0].page_content   #1
 #1
 #1
summary_prompt = f&quot;Summarize the given 		#2
document in a single paragraph\n		 #2
document: {document_text}&quot;  	 #2
from openai import OpenAI   #2
client = OpenAI()   #2
 #2
response = client.chat.completions.create(   #2
  model=&quot;gpt-4o-mini&quot;,   #2
  messages= [   #2
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: summary_prompt}   #2
      ]   #2
)   #2
 #2
summary=response.choices[0].message.content   #2
 #2
from langchain_text_splitters import 		#3
RecursiveCharacterTextSplitter    #3
text_splitter = RecursiveCharacterTextSplitter(    #3
chunk_size=1000,    #3
chunk_overlap=200)   #3
chunks=text_splitter.split_text(					#3
data_transformed[0].page_content	#3
)    #3

context_enriched_chunks = 					#4
[answer + &quot;\n&quot; + chunk for chunk in chunks]   #4

embedding = OpenAIEmbeddings(openai_api_key=api_key)  #5
vector_store = FAISS.from_texts(			 #5
context_enriched_chunks,  #5
embedding	 #5
)   #5</pre> 
    <div class="code-annotations-overlay-container"> #1 Loads text from Wikipedia page
     <br />#2 Generates summary of the text using GPT-4o-mini model
     <br />#3 Creates chunks using recursive character splitter
     <br />#4 Enriches chunks with summary data
     <br />#5 Creates embeddings and storing in FAISS index
     <br />
    </div> 
   </div> 
  </div> 
  <ul> 
   <li class="readable-text" id="p48"><em>Fetch surrounding chunk</em><em>s</em>—In this technique, chunks are created at a granular level, say, at a sentence level, and when a relevant chunk of text is found in response to a query, the system retrieves not only that chunk but also the surrounding chunks. This makes the search granular but also performs contextual expansion by retrieving adjacent chunks. It is useful in long-form content such as books and reports where information flows across paragraphs and sections. This technique also adds a layer of processing cost and latency to the system. Apart from that, there is a possibility of diluting the relevance as the neighboring chunks may contain noise. </li> 
  </ul> 
  <div class="readable-text" id="p49"> 
   <p>Chunk optimization is an effective step toward better RAG systems. Although it presents challenges such as managing the costs, system latency, and storage efficiency, optimizing chunking can fundamentally improve the retrieval and generation process of the RAG system. </p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <h4 class=" readable-text-h4"><strong>Metadata enhancements</strong></h4> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>A common way of defining metadata is “data about data.” Metadata describes other data. It can provide information such as a description of the data, time of creation, author, and similar. While metadata is useful for managing and organizing data, in the context of RAG, metadata enhances the searchability of data. A few ways in which metadata is crucial in improving RAG systems are </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p52"><em>Metadata filtering</em>—Adding metadata such as timestamp, author, category, and similar can enhance the chunks. While retrieving, chunks can first be filtered by relevant metadata information before doing a similarity search. This improves retrieval efficiency and reduces noise in the system. For example, using the timestamp filters can help avoid outdated information in the knowledge base. If a user searches for “latest COVID-19 travel guidelines,” metadata filtering by timestamp ensures that only the most recent guidelines are retrieved, avoiding outdated information.</li> 
   <li class="readable-text" id="p53"><em>Metadata enrichment</em>—Timestamp, author, category, chapter, page number, and so forth are common metadata elements that can be extracted from documents. However, even more valuable metadata items can be constructed. This can be a summary of the chunk by extracting tags from the chunk. One particularly useful technique is reverse hypothetical document embeddings. It involves using a language model to generate potential queries that could be answered by each document or chunk. These synthetic queries are then added to metadata. During retrieval, the system compares the user’s query with these synthetic queries to find the most relevant chunks. </li> 
  </ul> 
  <div class="readable-text" id="p54"> 
   <p>Metadata is a great tool for improving the accuracy of the retrieval system. However, a degree of caution must be exercised when adding metadata to the chunks. Designing the metadata schema is important to avoid redundancies and managing processing and storage costs. Providing improved relevance and accuracy, metadata enhancement has become extremely popular in contemporary RAG systems. </p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <h4 class=" readable-text-h4">Index structures</h4> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>Another important aspect of the knowledge base is how well the information is structured. In the na&iuml;ve RAG approach, there is no structural order to documents/chunks. However, for a more efficient retrieval, a few indexing structures have become popular and effective:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p57"><em class="CharOverride-5">Parent–child document structure</em>—In a parent–child document structure, documents are organized hierarchically. The parent document contains overarching themes or summaries, while child documents delve into specific details. During retrieval, the system can first locate the most relevant child documents and then refer to the parent documents for additional context if needed. This approach enhances the precision of retrieval, while maintaining the broader context. Simultaneously, this hierarchical structure can present challenges in terms of memory requirements and computational load.</li> 
   <li class="readable-text" id="p58"><em>Knowledge graph inde</em><em>x</em>—Knowledge graphs organize data in a structured manner as entities and relationships. Using knowledge graph structures not only increases contextual understanding but also equips the system with enhanced reasoning capabilities and improved explainability. Knowledge graph creation and maintenance, however, is an expensive process. Knowledge-graph-powered RAG, also called GraphRAG, is an emerging advanced RAG pattern that has demonstrated significant improvements in RAG performance. We will discuss GraphRAG in detail in chapter 8. </li> 
  </ul> 
  <div class="readable-text" id="p59"> 
   <p>Index structure, perhaps, has the biggest effect on index optimization for retrieval. It, however, introduces storage and memory burden on the system and affects search time performance. Index structure optimization is therefore advised in large scale systems where the true potential of concepts such as GraphRAG and hierarchical index can be realized.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p60"> 
   <p><span class="print-book-callout-head">Note</span> In the previous chapters, we have discussed that embeddings are a crucial component of RAG. They are used to calculate the semantic similarity between the user query and the documents stored in the knowledge base. Generally available embeddings models have been trained on commonly spoken language. When dealing with domain-specific or specialized content, these models may not yield good results. Fine-tuning embedding models let you optimize vector representations for your specific domain or task, leading to more accurate retrieval of relevant context. Fine-tuning is a slightly complex process since it requires curation of the training dataset and resources for recalculating the embeddings model. In case you’re dealing with highly specialized domains where the vocabulary is different from commonly spoken languages, you should consider fine-tuning the embedding model for your domain.</p> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>Like the indexing pipeline, index optimization is a periodic process and does not happen in real-time. The objective of index optimization is to set up the knowledge base for better retrieval. One must also be mindful of the added complexity that leads to an increase in computational, memory, and storage requirements. Figure 6.4 is an illustrative workflow of an index-optimized knowledge base.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p62">  
   <img src="../Images/CH06_F04_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 6.4</span><span class=""> </span><span class="">Illustration of an index-optimized knowledge base</span></h5>
  </div> 
  <div class="readable-text" id="p63"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.3.2</span> Query optimization</h3> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>The second stage of pre-retrieval techniques is a part of the generation pipeline. The objective of this stage is to optimize the input user query in a manner that makes it better suited for the retrieval tasks. Some of the popular query optimization strategies are listed in the following sections. </p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <h4 class=" readable-text-h4">Query expansion</h4> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>In query expansion, the original user query is enriched to retrieve more relevant information. This helps in increasing the recall of the system and overcomes the challenge of incomplete or very brief user queries. Some of the techniques that expand user queries are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p67"><em>Multi-query expansio</em><em>n</em>—In this approach, multiple variations of the original query are generated using an LLM, and each variant query is used to search and retrieve chunks from the knowledge base. For a query “How does climate change affect polar bears?” a multi-query expansion might generate “Impact of global warming on polar bears,” “What are the consequences of climate change for polar bear habitats?” Let’s look at a simple example of multi-query generation using GPT 4o-mini model: </li> 
  </ul> 
  <div class="browsable-container listing-container" id="p68"> 
   <div class="code-area-container"> 
    <pre class="code-area">original_query=&quot;How does climate change affect polar bears?&quot;
num=5

expansion_prompt=f&quot;Generate {num} variations 	#1
of the following query: {original_query}. 		 #1
Respond in JSON format.&quot;   #1

from openai import OpenAI  #2
client = OpenAI()   #2
response = client.chat.completions.create(  #2
  model=&quot;gpt-4o-mini&quot;,   #2
  messages= [   #2
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: expansion_prompt}   #2
      ],   #2
          response_format={ &quot;type&quot;: &quot;json_object&quot; }   #2
)   #2
 #2
expanded_queries=response.choices[0].message.content  #3</pre> 
    <div class="code-annotations-overlay-container"> #1 Crafts the prompt for query expansion
     <br />#2 Uses GPT 4o-mini to generate expanded queries
     <br />#3 Extracts the text from the response object
     <br />
    </div> 
   </div> 
  </div> 
  <ul> 
   <li class="readable-text" id="p69"><em>Sub-query expansion</em>: Subquery approach is quite like the multi-query approach. In this approach, instead of generating variations of the original query, a complex query is broken down into simpler sub-queries. This approach is inspired by the least-to-most prompting technique, where complex problems are broken down into simpler sub-problems and are solved one by one. A sub-query expansion on the same query—“How does climate change affect polar bears?”—may generate “How does melting sea ice influence polar bear hunting and feeding behaviors?” and “What are the physiological and health impacts of climate change on polar bears?” The approach to sub-query is similar to that for multi-query, except for the changes to the prompt: </li> 
  </ul> 
  <div class="browsable-container listing-container" id="p70"> 
   <div class="code-area-container"> 
    <pre class="code-area">sub_query_expansion_prompt=f&quot; \
Break down the following \
query into {num} sub-queries targeting \
different aspects of the query: {original_query}. \
Respond in JSON format. &quot;</pre>  
   </div> 
  </div> 
  <ul> 
   <li class="readable-text" id="p71"><em>Step-back expansio</em><em>n</em>—The term comes from the step-back prompting approach where the original query is abstracted to a higher-level conceptual query. During retrieval, both the original query and the abstracted query are used to fetch chunks. Similar to above example, an abstracted step-back query may be “What are the ecological impacts of climate change on arctic ecosystems?” Here is an example of the prompt that can be used:</li> 
  </ul> 
  <div class="browsable-container listing-container" id="p72"> 
   <div class="code-area-container"> 
    <pre class="code-area">step_back_expansion_prompt = f&quot;	\
Given the query: {original_query}, \
generate a more abstract, \
higher-level conceptual query. &quot;</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>While multi-query expansion generates various rephrasing or synonyms of the original query to cast a wider net during retrieval, sub-query expansion breaks down a complex query into simpler, component queries to target specific pieces of information, and step-back expansion abstracts the query to a higher-level concept to capture broader context.</p> 
  </div> 
  <div class="readable-text intended-text" id="p74"> 
   <p>Query expansion also presents its own set of challenges that need to be considered while implementing this strategy. While query expansion may increase recall by matching more documents, it may reduce the precision. The expansion terms need to be carefully selected to avoid contextual drift from the original query. Overexpansion can dilute the focus from the original query. Despite the challenges, query expansion has proved to be an effective technique for improving the recall of retrieval and generating more context aware responses.</p> 
  </div> 
  <div class="readable-text" id="p75"> 
   <h4 class=" readable-text-h4">Query transformation</h4> 
  </div> 
  <div class="readable-text" id="p76"> 
   <p>Compared to query expansion, in query transformation, instead of the original user query, retrieval happens on a transformed query, which is more suitable for the retriever.</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p77"><em>Rewrit</em><em>e</em>—Queries are rewritten from the input. The input in quite a few real-world applications may not be a direct query or a query suited for retrieval. Based on the input, a language model can be trained to transform the input into a query that can be used for retrieval. A user’s statement like, “I can’t send emails from my phone” can be rewritten as “Troubleshooting steps for resolving email sending issues on smartphones,” making it more suitable for retrieval.</li> 
   <li class="readable-text" id="p78"><em>HyD</em><em>E</em>—Hypothetical document embedding, or HyDE, is a technique where the language model first generates a hypothetical answer to the user’s query without accessing the knowledge base. This generated answer is then used to perform a similarity search against the document embeddings in the knowledge base, effectively retrieving documents that are similar to the hypothetical answer rather than the query itself. Here is an example that generates hypothetical document embeddings: </li> 
  </ul> 
  <div class="browsable-container listing-container" id="p79"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Original Query</strong>
original_query=	#1
&quot;How does climate change \  #1
affect polar bears?&quot;   #1

<strong># Prompts for generating HyDE</strong>
system_prompt=&quot;You are an expert in \	#2
climate change and arctic life.&quot;   #2
hyde_prompt=f&quot;Generate an answer to the \	 #2
question: {original_query}&quot;   #2

<strong># Using OpenAI to generate a hypothetical answer</strong>

from openai import OpenAI  #3
client = OpenAI()   #3
response = client.chat.completions.create(   #3
  model=&quot;gpt-4o-mini&quot;,   #3
  messages= [   #3
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt},   #3
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: hyde_prompt}   #3
  ]   #3
)   #3
 #3
hy_answer=response.choices[0].message.content   #3

<strong># Using OpenAI Embeddings to convert hyde into embeddings</strong>
embeddings = OpenAIEmbeddings(	#4
model=&quot;text-embedding-3-large&quot;	 #4
)   #4
hyde = embeddings.embed_query(hy_answer)   #4</pre> 
    <div class="code-annotations-overlay-container"> #1 Original query
     <br />#2 Prompts for generating HyDE
     <br />#3 Uses OpenAI to generate a hypothetical answer
     <br />#4 Uses OpenAI Embeddings to convert Hyde into embeddings
     <br />
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>Challenges similar to query expansion such as drift from original query and maintaining intent also persist in query transformation strategies. Effective rewriting and transformation of the query result in enhancing the context awareness of the system.</p> 
  </div> 
  <div class="readable-text" id="p81"> 
   <h4 class=" readable-text-h4"><strong>Query routing</strong></h4> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>Different queries can demand different retrieval methods. Based on criteria such as intent, domain, language, complexity, source of information, and so forth, queries need to be classified so that they can follow the appropriate retrieval method. This is the idea behind optimizing the user query by routing it to the appropriate workflow. Types of routing techniques include:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p83"><em>Intent classificatio</em><em>n</em>—A pre-trained classification model is used to classify the intent of the user query to select the appropriate retrieval method. A modification to this technique is prompt-based classification, where instead of a pre-trained classifier, an LLM is prompted to categorize the query into an intent. </li> 
   <li class="readable-text" id="p84"><em>Metadata routin</em><em>g</em>—In this approach, keywords and tags are extracted from the user query and then filtering is done on the chunk metadata to narrow down the scope of the search.</li> 
   <li class="readable-text" id="p85"><em>Semantic routin</em><em>g</em>—In this approach, the user query is matched with a pre-defined set of queries for each retrieval method. Wherever the similarity between the user query and pre-defined queries is the highest, that retrieval method is invoked.</li> 
  </ul> 
  <div class="readable-text" id="p86"> 
   <p>In customer support chatbots, query routing ensures that technical queries are directed to databases with troubleshooting guides, while billing questions are routed to account information, enhancing user satisfaction. </p> 
  </div> 
  <div class="readable-text intended-text" id="p87"> 
   <p>Implementing query routing takes effort and skill. It introduces a whole new predictive component, bringing uncertainty to the process. Therefore, it must be carefully crafted. Query routing is a must when dealing with source data and query type variability.</p> 
  </div> 
  <div class="readable-text intended-text" id="p88"> 
   <p>Although the universe of pre-retrieval strategies and techniques is expansive and ever-evolving, we have looked at a few of the most popular and effective techniques in this section. Bear in mind that the applicability of the strategies will depend on the nature of the content in the knowledge base and the use case. However, using each of these strategies will result in incremental gains in the RAG system performance. Now that we have set up the knowledge base and the user query for better retrieval, let’s discuss important retrieval strategies in the next section.</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.4</span> Retrieval strategies</h2> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>Interventions in the pre-retrieval stage can bring significant improvements in the performance of the RAG system if the query and the knowledge base become well aligned with the retrieval algorithm. We have discussed quite a few retrieval algorithms in chapter 4. In this section, we focus on strategies that can be employed for better retrieval.</p> 
  </div> 
  <div class="readable-text" id="p91"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.4.1</span> Hybrid retrieval</h3> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>Hybrid retrieval strategy is an essential component of production-grade RAG systems. It involves combining retrieval methods for improved retrieval accuracy. This can mean simply using a keyword-based search along with semantic similarity. It can also mean combining all sparse embedding, dense embedding vector, and knowledge graph-based search. The retrieval can be a union or an intersection of all these methods, depending on the requirements of precision and recall. It generally follows a weighted approach to retrieval. Figure 6.5 shows the hybrid retriever querying graph and vector storage.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p93">  
   <img src="../Images/CH06_F05_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 6.5</span><span class=""> </span><span class="">Hybrid retriever employs multiple querying techniques and combines the results.</span></h5>
  </div> 
  <div class="readable-text" id="p94"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.4.2</span> Iterative retrieval</h3> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>Instead of using a retrieve–generate linear process, the iterative retrieval strategy searches the knowledge base repeatedly based on the original query and the generated text, which allows the system to gather more information by refining the search based on initial results. It is useful when solving multi-hop or complex queries. While effective, iterative retrieval can lead to longer processing times and may introduce challenges in managing larger amounts of retrieved information. There are examples of iterative retrieval that have demonstrated remarkably improved performance such as Iter-RetGen, which is an iterative approach that alternates between retrieval and generation steps.</p> 
  </div> 
  <div class="readable-text" id="p96"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.4.3</span> Recursive retrieval</h3> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>The recursive retrieval strategy builds on the idea of iterative retrieval by transforming the query iteratively depending on the results obtained. While the initial query is used to retrieve the chunks, new focused queries are generated based on these chunks. It, therefore, leads to a better ability to find scattered information across document chunks and a more coherent and contextual response. Iterative retrieval chain-of-thought (IRCoT) is a recursive retrieval technique that combines iterative retrieval with CoT prompting.  </p> 
  </div> 
  <div class="readable-text" id="p98"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.4.4</span> Adaptive retrieval</h3> 
  </div> 
  <div class="readable-text" id="p99"> 
   <p>Adaptive retrieval also follows the approach of repeated retrieval cycles. In adaptive retrieval strategies, an LLM is enabled to determine the most appropriate moment and content for retrieval. The objective of adaptive retrieval is to make the retrieval process more personalized to users and context. It is applied in areas such as adapting queries depending on user behavior or adjusting retrieval based on user performance. FLARE and Self-RAG are two popular examples of adaptive retrieval. Self-RAG introduces “reflection tokens” that enable the model to introspect and decide when additional retrieval is necessary. FLARE (forward-looking active retrieval-augmented generation) predicts future content needs based on the current generation and retrieves relevant information proactively. Adaptive retrieval is a part of a broader trend of agentic AI. Agentic AI refers to AI systems that can make autonomous decisions during tasks, adapting their actions based on the context. In the context of RAG, agentic RAG involves AI agents that dynamically decide when and how to retrieve information, thus enhancing the flexibility and efficiency of the retrieval process. Agentic AI is an important emerging RAG pattern. We will discuss Agentic RAG in detail in chapter 8.</p> 
  </div> 
  <div class="readable-text intended-text" id="p100"> 
   <p>Figure 6.6 compares the three retrieval strategies that focus on repeated retrieval cycles. While recursive and iterative approaches need a threshold to break out of the iterations, in the adaptive approach, a judge model decides on-demand retrieval and generation steps.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p101">  
   <img src="../Images/CH06_F06_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 6.6</span><span class=""> </span><span class="">Iterative, recursive, and adaptive retrieval incorporate repeated retrieval cycles. Source: Adapted from Gao et al., December 18, 2023. “</span><span class=""><em>Retrieval-Augmented Generation for Large Language Models: A Survey.”</em></span> </h5>
  </div> 
  <div class="readable-text" id="p102"> 
   <p>All the advanced retrieval strategies introduce overheads in terms of computational complexity, and therefore the accuracy must be balanced against the cost and latency of the system. </p> 
  </div> 
  <div class="readable-text intended-text" id="p103"> 
   <p>By employing advanced pre-retrieval techniques and a suitable retrieval strategy, we can expect that richer, deeper, and more relevant context is being retrieved from the knowledge base. Even when the relevant context is retrieved, the LLM may struggle to assimilate all the information. To address this problem, in the next section, we discuss a couple of post-retrieval strategies that help curate the context before augmenting the prompt with the necessary information.</p> 
  </div> 
  <div class="readable-text" id="p104"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.5</span> Post-retrieval techniques</h2> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>Even if the retrieval of the chunks happens in an expected manner, a point of failure still remains. The LLM might not be able to process all the information. This may be due to redundancies or disjointed nature of the context among many other reasons. At the post-retrieval stage, the approaches of re-ranking and compression help in providing better context to the LLM for generation.</p> 
  </div> 
  <div class="readable-text" id="p106"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.5.1</span> Compression</h3> 
  </div> 
  <div class="readable-text" id="p107"> 
   <p>Excessively long context has the potential of introducing noise into the system. This diminishes the LLM’s capability to process information. Consequently, hallucinations and irrelevant responses to the query may persist. In prompt compression, language models are used to detect and remove unimportant and irrelevant tokens. Apart from making the context more relevant, prompt compression also has a positive influence on cost and efficiency. Another advantage of prompt compression is being able to reduce the size of the prompt so that it can fit into the context window of the LLM. COCOM is a context compression method that compresses contexts into a small number of context embeddings. Similarly, xRAG is a method that uses document embeddings as features. Compression can lead to loss of information, and therefore, there needs to be a balance between compression and performance. A very simple prompt to compress a long-retrieved context is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p108"> 
   <div class="code-area-container"> 
    <pre class="code-area">compress_prompt = f&quot;	\
Compress the following document 	\
into a shorter version, 	\
retaining only the essential information:	\
\n\n{document}&quot;</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p109"> 
   <h4 class=" readable-text-h4">Re-ranking</h4> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>Reordering all the retrieved documents ensures that the most relevant information is prioritized for the generation step. It refines retrieval results by prioritizing documents that are more contextually appropriate for the query, improving the overall quality and accuracy of information used for generation. Re-ranking also addresses the question of prioritization when a hybrid approach to retrieval is employed and improves the overall response quality. There are commonly available re-rankers such as multi-vector, Learning to Rank (LTR), BERT-based, and even hybrid re-rankers that can be employed. Specialized APIs such as Cohere Rerank offer pre-trained models for efficient reranking integration. </p> 
  </div> 
  <div class="readable-text intended-text" id="p111"> 
   <p>In this section, we discuss some of the popular advanced RAG strategies and techniques employed at different stages of the RAG pipeline. It is important to also consider the tradeoffs that come with these techniques. Almost any advanced technique will introduce overheads to the system. These can be in the form of computational load, latency in the system, and increased storage and memory requirements. Therefore, these techniques warrant a performance versus overhead assessment catered to specific use cases. Table 6.1 provides a summary of the 12 strategies discussed so far.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p112"> 
   <h5 class=" browsable-container-h5">Table 6.1 Advanced RAG strategies with their benefits and limitations</h5> 
   <table id="table001" class="No-Table-Style TableOverride-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-1" /> 
    </colgroup> 
    <thead> 
     <tr class="No-Table-Style _idGenTableRowColumn-2"> 
      <td class="No-Table-Style CellOverride-1" scope="col"> <p class="_TableHead">Strategy</p> </td> 
      <td class="No-Table-Style CellOverride-2" scope="col"> <p class="_TableHead">Description</p> </td> 
      <td class="No-Table-Style CellOverride-2" scope="col"> <p class="_TableHead">Benefits</p> </td> 
      <td class="No-Table-Style CellOverride-3" scope="col"> <p class="_TableHead">Challenges</p> </td> 
     </tr> 
    </thead> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-4"> <p class="_TableBody">Chunk optimization</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Adjusting document chunks for optimal size and context</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Improves retrieval accuracy, processing speed, and storage</p> </td> 
      <td class="No-Table-Style CellOverride-6"> <p class="_TableBody">Requires experimentation; optimal chunk varies by use case</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Metadata enhancements</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Enriching chunks with additional metadata for better filtering and searchability</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Improves retrieval efficiency; reduces noise</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Requires careful schema design; manages processing costs</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Index structures</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Organizing data in structured formats for efficient retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Enhances accuracy and context in retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Increases memory and computational load</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Query expansion</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Enriching the user query to retrieve more relevant information</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Increases recall; overcomes brief queries</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">May reduce precision; risk of contextual drift</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Query transformation</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Modifying the user query for better retrieval suitability</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Enhances context awareness; maintains intent</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Potential for misinterpretation; drift from the original query</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Query routing</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Directing queries to appropriate retrieval methods based on classification</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Enhances retrieval by matching method to query type</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Introduces uncertainty; requires careful crafting</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Hybrid retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Combining multiple retrieval methods (e.g., keyword and semantic)</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Improves retrieval accuracy and robustness</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Increased complexity; requires method weighting</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Iterative retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Repeatedly searching based on initial results and query refinement</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Gathers more comprehensive information; refines search</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Longer processing times; managing more data</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Recursive retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Iteratively transforming the query based on obtained results</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Finds scattered information; provides coherent responses</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Similar to iterative retrieval; potential for increased load</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Adaptive retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">LLM decides when and what to retrieve during generation</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Personalized and context-aware retrieval; dynamic adaptation</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Increased computational complexity; part of agentic AI</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Compression</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Reducing context length by removing irrelevant information</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Fits within LLM context window; reduces noise and costs</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Potential loss of important information; needs balance</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-10"> <p class="_TableBody">Reranking</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Reordering retrieved documents to prioritize relevance</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Enhances response quality; ensures most relevant info is used</p> </td> 
      <td class="No-Table-Style CellOverride-12"> <p class="_TableBody">Requires additional models; may introduce overhead</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p113"> 
   <p>Figure 6.7 is an illustrative example of what a generation pipeline looks like after incorporating advanced techniques.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p114">  
   <img src="../Images/CH06_F07_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 6.7</span><span class=""> </span><span class="">Illustrative example of advanced generation pipeline</span></h5>
  </div> 
  <div class="readable-text" id="p115"> 
   <p>While these advanced strategies and techniques are extremely useful in improving performance, a RAG system also needs to provide customization and flexibility. This is because we may need to quickly adopt different techniques as the nature of data and queries evolve. A modular RAG approach discussed in the next section aims to provide greater architectural flexibility over the traditional RAG system.</p> 
  </div> 
  <div class="readable-text" id="p116"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.6</span> Modular RAG</h2> 
  </div> 
  <div class="readable-text" id="p117"> 
   <p>AI systems are becoming increasingly complex, demanding more customizable, flexible, and scalable RAG architectures. The emergence of modular RAG is a leap forward in the evolution of RAG systems. Modular RAG breaks down the traditional monolithic RAG structure into interchangeable components. This allows for tailoring of the system to specific use cases. The modular approach brings modularity to RAG components, such as retrievers, indexing, and generation, while also adding more modules such as search, memory, and fusion. We can think of the modular RAG approach in two parts:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p118">Core components of RAG developed as flexible, interchangeable modules</li> 
   <li class="readable-text" id="p119">Specialized modules to enhance the core features of retrieval, augmentation, and generation</li> 
  </ul> 
  <div class="readable-text" id="p120"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.6.1</span> Core modules</h3> 
  </div> 
  <div class="readable-text" id="p121"> 
   <p>The core components of the RAG system (i.e. indexing, retrieval, augmentation and generation), along with the advanced pre- and post-retrieval techniques, are composed as flexible, interchangeable modules in the modular RAG framework.</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p122"><em>Indexing modul</em><em>e</em>—The indexing module serves as the foundation for building the knowledge base. By modularizing this component, developers can choose from various embedding models for advanced semantic understanding. Vector stores can be interchanged based on scalability and performance needs. Additionally, chunking methods can be adapted to the data structure, whether it’s text, code, or multimedia content, ensuring optimal indexing for retrieval.</li> 
   <li class="readable-text" id="p123"><em>Retrieval modul</em><em>e</em>—The retrieval module enables the use of diverse retrieval algorithms. For instance, developers can switch between semantic similarity search using dense embeddings and traditional keyword-based search such as BM25. This flexibility allows for tailoring retrieval methods to the specific requirements of the application, such as prioritizing speed, accuracy, or resource utilization. For example, a customer support chatbot might use semantic search during off-peak hours for higher accuracy and switch to keyword search during peak hours to handle increased load. The modular retrieval component allows this dynamic interchange of retrieval strategies based on real-time needs.</li> 
   <li class="readable-text" id="p124"><em>Generation modul</em><em>e</em>—In the generation module, the choice of LLM is modular. Developers can select from models such as GPT-4 for complex language generation or smaller models for cost efficiency. This module also handles prompt engineering for augmentation to guide the LLM in generating accurate and relevant responses.</li> 
   <li class="readable-text" id="p125"><em>Pre-retrieval modul</em><em>e</em>—Allows flexibility of pre-retrieval techniques to improve the quality of indexed content and user query. </li> 
   <li class="readable-text" id="p126"><em>Post-retrieval modul</em><em>e</em>—Like the pre-retrieval module, this module allows for flexible implementation of post-retrieval techniques to refine and optimize the retrieved context. </li> 
  </ul> 
  <div class="readable-text" id="p127"> 
   <p>You may note that the first three modules complete the na&iuml;ve RAG approach, and the addition of the pre-retrieval and post-retrieval modules enhances the na&iuml;ve RAG into an advanced RAG implementation. It can also be said that na&iuml;ve RAG is a special (and limited) case of advanced RAG.</p> 
  </div> 
  <div class="readable-text" id="p128"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.6.2</span> New modules</h3> 
  </div> 
  <div class="readable-text" id="p129"> 
   <p>The modular RAG framework has introduced several new components to enhance the retrieval and generation capabilities of na&iuml;ve and advanced RAG approaches. Some of these components/modules are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p130"><em>Searc</em><em>h</em>—The search module is aimed at performing searches on different data sources. It is customized to different data sources and aimed at increasing the source data for better response generation.</li> 
   <li class="readable-text" id="p131"><em>Fusio</em><em>n</em>—RAG fusion improves traditional search systems by overcoming their limitations through a multi-query approach. The fusion module enhances retrieval by expanding the user’s query into multiple, diverse perspectives using an LLM. It then conducts parallel searches for these expanded queries, fuses the results by reranking and selecting the most relevant information, and presents a comprehensive answer. This approach captures both explicit and implicit information, uncovering deeper insights that might be missed with a single query.</li> 
   <li class="readable-text" id="p132"><em>Memor</em><em>y</em>—The memory module uses the inherent memory of the LLM, meaning the knowledge encoded within its parameters from pre-training. This module uses the LLM to recall information without explicit retrieval, guiding the system on when to retrieve additional data and when to rely on the LLM’s internal knowledge. It can involve techniques such as using reflection tokens or prompts that encourage the model to introspect and decide if more information is needed. For example, when answering a query about historical events, the memory module can decide to rely on the LLM’s knowledge about World War II to provide context, only retrieving specific dates or figures as needed. This approach reduces unnecessary retrieval and uses the model’s pre-trained knowledge.</li> 
   <li class="readable-text" id="p133"><em>Routin</em><em>g</em>—Routing in the RAG system navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, specific database searches, or merging different information streams.</li> 
   <li class="readable-text" id="p134"><em>Task adapte</em><em>r</em>—This module makes RAG adaptable to various downstream tasks allowing the development of task-specific end-to-end retrievers with minimal examples, demonstrating flexibility in handling different tasks. The task adapter module allows the RAG system to be fine-tuned for specific tasks like summarization, translation, or sentiment analysis. By incorporating a small number of task-specific examples or prompts, the module adjusts the retrieval and generation components to produce outputs tailored to the desired task, enhancing versatility without extensive retraining. </li> 
  </ul> 
  <div class="readable-text" id="p135"> 
   <p>You may observe that advanced RAG is a special case within the modular RAG framework. You also saw earlier that na&iuml;ve RAG is a special case of advanced RAG. This means that the RAG approaches (i.e., na&iuml;ve, advanced, and modular) are not competing but progressive. You may start by trying out a na&iuml;ve implementation of RAG and move to a more modular approach. Figure 6.8 shows the progression of RAG systems.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p136">  
   <img src="../Images/CH06_F08_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 6.8</span><span class=""> </span><span class="">Na&iuml;ve, advanced, and modular approaches to RAG are progressive. Na&iuml;ve RAG is a sub-component of advanced RAG, which is a sub-component of modular RAG.</span></h5>
  </div> 
  <div class="readable-text" id="p137"> 
   <p>While building a modular RAG system, remember that each module should be designed to work independently. This requires defining clear inputs and outputs. Along with the independent modules, the orchestration layer should be flexible to allow mixing and matching of modules. One should also bear in mind that a modular approach introduces complexity in the process. Managing interfaces, dependencies, configurations, and versions of modules can be complex. Ensuring compatibility and consistency between modules can be challenging. Testing each module independently and collectively requires a robust evaluation strategy. Extra modules may also add latency and inference costs to the system.</p> 
  </div> 
  <div class="readable-text intended-text" id="p138"> 
   <p>Despite the added complexities, the modular approach toward RAG is state-of-the-art in large-scale RAG systems. It enables rapid experimentation, efficient optimization, and seamless integration of new technologies as they emerge. By offering the ability to mix and match different modules, modular RAG empowers you to build more robust, accurate, and versatile AI solutions. It also facilitates easier maintenance, updates, and scalability, making it an ideal choice for managing complex, evolving knowledge bases.</p> 
  </div> 
  <div class="readable-text intended-text" id="p139"> 
   <p>This section concludes the discussion on improving RAG performance using advanced techniques and a modular framework. Interventions can be employed at different stages of the indexing and generation pipelines. Modular approaches to RAG enable rapid experimentation, flexibility, and scalable architecture. You will need to experiment to figure out the techniques that help in improving RAG for specific use cases. It is also important to be mindful of the tradeoffs. Advanced techniques introduce complexities that have an effect on computation, memory, and storage requirements. </p> 
  </div> 
  <div class="readable-text intended-text" id="p140"> 
   <p>This is one aspect of putting RAG in production. Advanced techniques are necessary for RAG systems to achieve acceptable accuracy and efficiency. The other enablers for RAG systems in production are the tools and technologies that form the backbone of the RAG stack. In the next chapter, we will look at this technology infrastructure that enables RAG systems. </p> 
  </div> 
  <div class="readable-text" id="p141"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <div class="readable-text" id="p142"> 
   <h3 class=" readable-text-h3">Limitations of na&iuml;ve RAG</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p143">Na&iuml;ve RAG follows a simple “retrieve then read” process.</li> 
   <li class="readable-text" id="p144">This approach suffers from low precision and incomplete retrieval.</li> 
   <li class="readable-text" id="p145">Retrieval often misses relevant information and pulls in irrelevant content.</li> 
   <li class="readable-text" id="p146">At the augmentation stage, there is often redundancy from similar retrieved documents.</li> 
   <li class="readable-text" id="p147">Context can become disjointed when sourced from multiple documents.</li> 
   <li class="readable-text" id="p148">The generation stage faces hallucinations and biased outputs. </li> 
   <li class="readable-text" id="p149">The model can overly rely on retrieved data and ignore its internal knowledge.</li> 
  </ul> 
  <div class="readable-text" id="p150"> 
   <h3 class=" readable-text-h3">Advanced RAG techniques</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p151">The advanced RAG process follows a “rewrite then retrieve then re-rank then read” framework, where the query is optimized through rewriting, retrieval is enhanced for better precision, results are re-ranked to prioritize relevance, and the most relevant information is used for generating the final response.</li> 
   <li class="readable-text buletless-item" id="p152">Pre-retrieval techniques include 
    <ul> 
     <li><em>Index optimizatio</em><em>n</em>—Improves document storage for better searchability</li> 
     <li><em>Chunk optimizatio</em><em>n</em>—Balances chunk sizes to avoid losing context or introducing noise </li> 
     <li><em>Context-enriched chunkin</em><em>g</em>—Adds summaries to each chunk to improve retrieval</li> 
     <li><em>Metadata enhancement</em><em>s</em>—Adds tags and metadata like timestamps or categories for better filtering</li> 
     <li><em>Query optimizatio</em><em>n</em>—Expands or rewrites user queries for improved retrieval accuracy </li> 
    </ul> </li> 
   <li class="readable-text buletless-item" id="p153">Retrieval techniques include 
    <ul> 
     <li><em>Hybrid retrieva</em><em>l</em>—Combines keyword-based and semantic searches</li> 
     <li><em>Iterative retrieva</em><em>l</em>—Refines searches by repeatedly querying based on initial results </li> 
     <li><em>Recursive retrieva</em><em>l</em>—Generates new queries based on retrieved chunks to gather more relevant information</li> 
    </ul> </li> 
   <li class="readable-text buletless-item" id="p154">Post-retrieval techniques include 
    <ul> 
     <li><em>Compressio</em><em>n</em>—Reduces unnecessary context to remove noise and fit within the model’s context window</li> 
     <li><em>Re-rankin</em><em>g</em>—Reorders retrieved documents to prioritize the most relevant ones</li> 
    </ul> </li> 
  </ul> 
  <div class="readable-text" id="p155"> 
   <h3 class=" readable-text-h3">Modular RAG framework</h3> 
  </div> 
  <ul> 
   <li class="readable-text buletless-item" id="p156">Core modules include 
    <ul> 
     <li><em>Indexing modul</em><em>e</em>—Allows flexible embedding models and vector store options</li> 
     <li><em>Retrieval modul</em><em>e</em>—Supports switching between dense and keyword-based retrieval methods </li> 
     <li><em>Generation modul</em><em>e</em>—Offers flexibility in selecting language models based on complexity and cost</li> 
    </ul> </li> 
   <li class="readable-text buletless-item" id="p157">New modules include 
    <ul> 
     <li><em>Search modul</em><em>e</em>—Tailors search to specific data sources for better results</li> 
     <li><em>Fusion modul</em><em>e</em>—Expands user queries into multiple forms and combines retrieved results for deeper insights</li> 
     <li><em>Memory modul</em><em>e</em>—Uses the model’s internal knowledge to reduce unnecessary retrieval, retrieving only when needed</li> 
     <li><em>Routing modul</em><em>e</em>—Dynamically selects the best path for handling different types of queries</li> 
     <li><em>Task adapter modul</em><em>e</em>—Adapts the system for different downstream tasks like summarization or translation</li> 
    </ul> </li> 
  </ul> 
  <div class="readable-text" id="p158"> 
   <h3 class=" readable-text-h3">Tradeoffs and best practices</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p159">Advanced techniques improve RAG accuracy but add complexity. </li> 
   <li class="readable-text" id="p160">Techniques such as hybrid retrieval or re-ranking can increase computational costs and latency.</li> 
   <li class="readable-text" id="p161">Modular RAG offers flexibility but requires careful management of interfaces and module compatibility.</li> 
   <li class="readable-text" id="p162">Testing each module independently and as a whole is important to ensure system stability and performance.</li> 
   <li class="readable-text" id="p163">Tradeoffs between performance, cost, and system complexity should be carefully assessed.</li> 
  </ul>
 </body>
</html>