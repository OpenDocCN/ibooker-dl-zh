- en: 8 Understanding agent memory and knowledge
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 理解代理记忆和知识
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Retrieval in knowledge/memory in AI functions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能功能中的知识/记忆检索
- en: Building retrieval augmented generation workflows with LangChain
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LangChain构建检索增强生成工作流程
- en: Retrieval augmented generation for agentic knowledge systems in Nexus
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nexus中用于代理知识系统的检索增强生成
- en: Retrieval patterns for memory in agents
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理中记忆的检索模式
- en: Improving augmented retrieval systems with memory and knowledge compression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用记忆和知识压缩改进增强检索系统
- en: Now that we’ve explored agent actions using external tools, such as plugins
    in the form of native or semantic functions, we can look at the role of memory
    and knowledge using retrieval in agents and chat interfaces. We’ll describe memory
    and knowledge and how they relate to prompt engineering strategies, and then,
    to understand memory knowledge, we’ll investigate document indexing, construct
    retrieval systems with LangChain, use memory with LangChain, and build semantic
    memory using Nexus.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了使用外部工具（如原生或语义函数形式的插件）进行代理动作，我们可以看看在代理和聊天界面中使用检索来处理记忆和知识的作用。我们将描述记忆和知识以及它们与提示工程策略的关系，然后，为了理解记忆知识，我们将研究文档索引，使用LangChain构建检索系统，利用LangChain使用记忆，并使用Nexus构建语义记忆。
- en: 8.1 Understanding retrieval in AI applications
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 理解人工智能应用中的检索
- en: Retrieval in agent and chat applications is a mechanism for obtaining knowledge
    to keep in storage that is typically external and long-lived. Unstructured knowledge
    includes conversation or task histories, facts, preferences, or other items necessary
    for contextualizing a prompt. Structured knowledge, typically stored in databases
    or files, is accessed through native functions or plugins.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 代理和聊天应用中的检索是一种获取知识并将其存储在通常外部且长期存在的存储中的机制。非结构化知识包括对话或任务历史、事实、偏好或其他用于上下文化的提示所需的项目。结构化知识通常存储在数据库或文件中，通过原生函数或插件访问。
- en: Memory and knowledge, as shown in figure 8.1, are elements used to add further
    context and relevant information to a prompt. Prompts can be augmented with everything
    from information about a document to previous tasks or conversations and other
    reference information.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如图8.1所示，记忆和知识是用于向提示添加更多上下文和相关信息的基本元素。提示可以通过从文档信息到先前任务或对话以及其他参考信息的一切进行增强。
- en: '![figure](../Images/8-1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-1.png)'
- en: 'Figure 8.1 Memory, retrieval, and augmentation of the prompt using the following
    prompt engineering strategies: Use External Tools and Provide Reference Text.'
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1展示了使用以下提示工程策略（使用外部工具和提供参考文本）对提示进行记忆、检索和增强。
- en: The prompt engineering strategies shown in figure 8.1 can be applied to memory
    and knowledge. Knowledge isn’t considered memory but rather an augmentation of
    the prompt from existing documents. Both knowledge and memory use retrieval as
    the basis for how unstructured information can be queried.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1中展示的提示工程策略可以应用于记忆和知识。知识并不被视为记忆，而是对现有文档提示的增强。知识和记忆都使用检索作为查询非结构化信息的基础。
- en: The retrieval mechanism, called retrieval augmented generation (RAG), has become
    a standard for providing relevant context to a prompt. The exact mechanism that
    powers RAG also powers memory/knowledge, and it’s essential to understand how
    it works. In the next section, we’ll examine what RAG is.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 检索机制，称为检索增强生成（RAG），已成为提供相关上下文的标准。驱动RAG的确切机制也驱动着记忆/知识，理解其工作原理至关重要。在下一节中，我们将检查RAG是什么。
- en: 8.2 The basics of retrieval augmented generation (RAG)
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 检索增强生成（RAG）的基本原理
- en: RAG has become a popular mechanism for supporting document chat or question-and-answer
    chat. The system typically works by a user supplying a relevant document, such
    as a PDF, and then using RAG and a large language model (LLM) to query the document.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RAG已成为支持文档聊天或问答聊天的流行机制。系统通常通过用户提供相关文档（如PDF文件），然后使用RAG和大型语言模型（LLM）查询该文档来实现。
- en: Figure 8.2 shows how RAG can allow a document to be queried using an LLM. Before
    any document can be queried, it must first be loaded, transformed into context
    chunks, embedded into vectors, and stored in a vector database.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2展示了如何使用LLM（大型语言模型）查询文档。在查询任何文档之前，它必须首先被加载，转换为上下文块，嵌入到向量中，并存储在向量数据库中。
- en: '![figure](../Images/8-2.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-2.png)'
- en: 'Figure 8.2 The two phases of RAG: first, documents must be loaded, transformed,
    embedded, and stored, and, second, they can be queried using augmented generation.'
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.2 RAG 的两个阶段：首先，文档必须被加载、转换、嵌入和存储，其次，可以使用增强生成进行查询。
- en: A user can query previously indexed documents by submitting a query. That query
    is then embedded into a vector representation to search for similar chunks in
    the vector database. Content similar to the query is then used as context and
    populated into the prompt as augmentation. The prompt is pushed to an LLM, which
    can use the context information to help answer the query.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以通过提交查询来查询先前索引的文档。然后，该查询被嵌入到向量表示中，以在向量数据库中搜索相似的片段。与查询内容相似的内容随后用作上下文，并填充到提示中以进行增强。提示被推送到一个大型语言模型
    (LLM)，该模型可以使用上下文信息来帮助回答查询。
- en: '*Unstructured* memory/knowledge concepts rely on some format of text-similarity
    search following the retrieval pattern shown in figure 8.2\. Figure 8.3 shows
    how memory uses the same embedding and vector database components. Rather than
    preload documents, conversations or parts of a conversation are embedded and saved
    to a vector database.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**非结构化**的记忆/知识概念依赖于图 8.2 所示的检索模式的一种文本相似度搜索格式。图 8.3 展示了记忆如何使用相同的嵌入和向量数据库组件。而不是预加载文档，对话或对话的一部分被嵌入并保存到向量数据库中。'
- en: '![figure](../Images/8-3.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-3.png)'
- en: Figure 8.3 Memory retrieval for augmented generation uses the same embedding
    patterns to index items to a vector database.
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.3 增强生成中的记忆检索使用相同的嵌入模式将项目索引到向量数据库中。
- en: The retrieval pattern and document indexing are nuanced and require careful
    consideration to be employed successfully. This requires understanding how data
    is stored and retrieved, which we’ll start to unfold in the next section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 检索模式和文档索引复杂且需要仔细考虑才能成功应用。这需要理解数据是如何存储和检索的，我们将在下一节开始展开讨论。
- en: 8.3 Delving into semantic search and document indexing
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 深入探讨语义搜索和文档索引
- en: Document indexing transforms a document’s information to be more easily recovered.
    How the index will be queried or searched also plays a factor, whether searching
    for a particular set of words or wanting to match phrase for phrase.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 文档索引将文档信息转换为更易于恢复的形式。索引的查询或搜索方式也会起到作用，无论是搜索特定的单词集还是想要逐词匹配。
- en: A *semantic* search is a search for content that matches the searched phrase
    by words and meaning. The ability to search by meaning, semantically, is potent
    and worth investigating in some detail. In the next section, we look at how vector
    similarity search can lay the framework for semantic search.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**语义**搜索是通过词语和意义匹配搜索短语的内容。通过语义进行搜索的能力强大且值得深入探讨。在下一节中，我们将探讨向量相似性搜索如何为语义搜索奠定框架。'
- en: 8.3.1 Applying vector similarity search
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 应用向量相似性搜索
- en: Let’s look now at how a document can be transformed into a *semantic vector,*
    or a representation of text that can then be used to perform distance or similarity
    matching. There are numerous ways to convert text into a semantic vector, so we’ll
    look at a simple one.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何将文档转换为**语义向量**，或者说是可以用于执行距离或相似度匹配的文本表示。将文本转换为语义向量的方法有很多，所以我们将探讨一个简单的方法。
- en: Open the `chapter_08` folder in a new Visual Studio Code (VS Code) workspace.
    Create a new environment and `pip` `install` the `requirements.txt` file for all
    the chapter dependencies. If you need help setting up a new Python environment,
    consult appendix B.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个新的 Visual Studio Code (VS Code) 工作区中打开 `chapter_08` 文件夹。创建一个新的环境，并使用 `pip`
    `install` 命令安装 `requirements.txt` 文件以解决所有章节的依赖项。如果您需要帮助设置新的 Python 环境，请参阅附录 B。
- en: Now open the `document_vector_similarity.py` file in VS Code, and review the
    top section in listing 8.1\. This example uses Term Frequency–Inverse Document
    Frequency (TF–IDF). This numerical statistic reflects how important a word is
    to a document in a collection or set of documents by increasing proportionally
    to the number of times a word appears in the document and offset by the frequency
    of the word in the document set. TF–IDF is a classic measure of understanding
    one document’s importance within a set of documents.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打开 VS Code 中的 `document_vector_similarity.py` 文件，并查看列表 8.1 的顶部部分。此示例使用词频-逆文档频率
    (TF-IDF)。这个数值统计量反映了单词在文档集合或文档集中的重要性，其比例与单词在文档中出现的次数成正比，并受到文档集中单词频率的影响。TF-IDF 是理解文档集合中单个文档重要性的经典度量。
- en: Listing 8.1 `document_vector_similarity` (transform to vector)
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.1 `document_vector_similarity`（转换为向量）
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Samples of documents'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 文档样本'
- en: '#2 Vectorization using TF–IDF'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用 TF–IDF 进行向量化'
- en: '#3 Vectorize the documents.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 向量化文档。'
- en: Let’s break down TF–IDF into its two components using the sample sentence, “The
    sky is blue and beautiful,” and focusing on the word *blue*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用样本句子，“The sky is blue and beautiful”，将 TF–IDF 分解为其两个组成部分，并专注于单词 *blue*。
- en: Term Frequency (TF)
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 术语频率（TF）
- en: '*Term Frequency* measures how frequently a term occurs in a document. Because
    we’re considering only a single document (our sample sentence), the simplest form
    of the TF for *blue* can be calculated as the number of times *blue* appears in
    the document divided by the total number of words in the document. Let’s calculate
    it:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*术语频率* 衡量一个术语在文档中出现的频率。因为我们只考虑一个文档（我们的样本句子），*blue* 的 TF 的最简单形式可以通过将 *blue*
    在文档中出现的次数除以文档中的总单词数来计算。让我们来计算它：'
- en: 'Number of times *blue* appears in the document: 1'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*blue* 在文档中出现的次数：1'
- en: 'Total number of words in the document: 6'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中的总单词数：6
- en: TF = 1 ÷ 6TF = .16
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: TF = 1 ÷ 6TF = .16
- en: Inverse Document Frequency (IDF)
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逆文档频率（IDF）
- en: '*Inverse Document Frequency* measures how important a term is within the entire
    corpus. It’s calculated by dividing the total number of documents by the number
    of documents containing the term and then taking the logarithm of that quotient:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*逆文档频率* 衡量一个术语在整个语料库中的重要性。它是通过将文档总数除以包含该术语的文档数，然后取该商的对数来计算的：'
- en: IDF = log(Total number of documents ÷ Number of documents containing the word)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: IDF = log(文档总数 ÷ 包含该词的文档数)
- en: In this example, the corpus is a small collection of eight documents, and *blue*
    appears in four of these documents.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，语料库是包含八个文档的小集合，其中 *blue* 出现在其中的四个文档中。
- en: IDF = log(8 ÷ 4)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: IDF = log(8 ÷ 4)
- en: TF–IDF calculation
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF–IDF 计算
- en: 'Finally, the TF–IDF score for *blue* in our sample sentence is calculated by
    multiplying the TF and the IDF scores:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过将 TF 和 IDF 分数相乘来计算样本句子中 *blue* 的 TF–IDF 分数：
- en: TF–IDF = TF × IDF
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: TF–IDF = TF × IDF
- en: 'Let’s compute the actual values for TF–IDF for the word *blue* using the example
    provided; first, the term frequency (how often the word occurs in the document)
    is computed as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用提供的示例来计算单词 *blue* 的实际 TF–IDF 值；首先，计算词频（单词在文档中出现的频率）如下：
- en: TF = 1 ÷ 6
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: TF = 1 ÷ 6
- en: 'Assuming the base of the logarithm is 10 (commonly used), the inverse document
    frequency is computed as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设对数的底数为 10（常用），则逆文档频率的计算如下：
- en: IDF = log10 (8 ÷ 4)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: IDF = log10 (8 ÷ 4)
- en: 'Now let’s calculate the exact TF–IDF value for the word *blue* in the sentence,
    “The sky is blue and beautiful”:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算句子“ The sky is blue and beautiful”中单词 *blue* 的确切 TF–IDF 值：
- en: The Term Frequency (TF) is approximately 0.1670.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 术语频率（TF）大约为 0.1670。
- en: The Inverse Document Frequency (IDF) is approximately 0.301.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 逆文档频率（IDF）大约为 0.301。
- en: Thus, the TF–IDF (TF × IDF) score for *blue* is approximately 0.050.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*blue* 的 TF–IDF（TF × IDF）分数大约为 0.050。
- en: This TF–IDF score indicates the relative importance of the word *blue* in the
    given document (the sample sentence) within the context of the specified corpus
    (eight documents, with *blue* appearing in four of them). Higher TF–IDF scores
    imply greater importance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 TF–IDF 分数表示在给定的文档（样本句子）中，在指定的语料库（八个文档，其中四个包含 *blue*）的上下文中，单词 *blue* 的相对重要性。更高的
    TF–IDF 分数意味着更大的重要性。
- en: We use TF–IDF here because it’s simple to apply and understand. Now that we
    have the elements represented as vectors, we can measure document similarity using
    cosine similarity. Cosine similarity is a measure used to calculate the cosine
    of the angle between two nonzero vectors in a multidimensional space, indicating
    how similar they are, irrespective of their size.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用 TF–IDF，因为它简单易用。现在，我们已经将元素表示为向量，我们可以使用余弦相似度来衡量文档相似度。余弦相似度是一种用于计算多维空间中两个非零向量之间角度余弦的度量，它表示它们在不考虑它们大小的情况下有多相似。
- en: Figure 8.4 shows how cosine distance compares the vector representations of
    two pieces or documents of text. Cosine similarity returns a value from –1 (not
    similar) to 1 (identical). *Cosine distance* is a normalized value ranging from
    0 to 2, derived by taking 1 minus the cosine similarity. A cosine distance of
    0 means identical items, and 2 indicates complete opposites.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 展示了余弦距离如何比较两段文本或文档的向量表示。余弦相似度返回一个从 –1（不相似）到 1（相同）的值。*余弦距离* 是一个介于 0 到 2
    之间的归一化值，通过从余弦相似度中减去 1 得到。余弦距离为 0 表示相同的项目，而 2 表示完全相反。
- en: '![figure](../Images/8-4.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-4.png)'
- en: Figure 8.4 How cosine similarity is measured
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.4 如何测量余弦相似度
- en: Listing 8.2 shows how the cosine similarities are computed using the `cosine_similarity`
    function from scikit-learn. Similarities are calculated for each document against
    all other documents in the set. The computed matrix of similarities for documents
    is stored in the `cosine_similarities` variable. Then, in the input loop, the
    user can select the document to view its similarities to the other documents.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.2 展示了如何使用 scikit-learn 的 `cosine_similarity` 函数计算余弦相似度。对集合中的每个文档与其他所有文档之间的相似度进行计算。文档的相似度矩阵存储在
    `cosine_similarities` 变量中。然后，在输入循环中，用户可以选择文档以查看其与其他文档的相似度。
- en: Listing 8.2 `document_vector_similarity` (cosine similarity)
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.2 `document_vector_similarity`（余弦相似度）
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Computes the document similarities for all vector pairs'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 计算所有向量对的文档相似度'
- en: '#2 The main input loop'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 主要输入循环'
- en: '#3 Gets the selected document index to compare with'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 获取要比较的选定文档索引'
- en: '#4 Extracts the computed similarities against all documents'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 从所有文档中提取计算出的相似度'
- en: Figure 8.5 shows the output of running the sample in VS Code (F5 for debugging
    mode). After you select a document, you’ll see the similarities between the various
    documents in the set. A document will have a cosine similarity of 1 with itself.
    Note that you won’t see a negative similarity because of the TF–IDF vectorization.
    We’ll look later at other, more sophisticated means of measuring semantic similarity.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 展示了在 VS Code 中运行示例的输出（按 F5 进入调试模式）。选择文档后，您将看到集合中各种文档之间的相似度。一个文档与其自身之间的余弦相似度为
    1。请注意，由于 TF-IDF 向量化，您不会看到负相似度。我们将在稍后探讨其他更复杂的测量语义相似度的方法。
- en: '![figure](../Images/8-5.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-5.png)'
- en: Figure 8.5 The cosine similarity between selected documents and the document
    set
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.5 选定文档与文档集之间的余弦相似度
- en: The method of vectorization will dictate the measure of semantic similarity
    between documents. Before we move on to better methods of vectorizing documents,
    we’ll examine storing vectors to perform vector similarity searches.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化方法将决定文档之间的语义相似度度量。在我们继续探讨更好的文档向量化方法之前，我们将检查存储向量以执行向量相似度搜索。
- en: 8.3.2 Vector databases and similarity search
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 向量数据库和相似度搜索
- en: After vectorizing documents, they can be stored in a vector database for later
    similarity searches. To demonstrate how this works, we can efficiently replicate
    a simple vector database in Python code.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化文档后，它们可以存储在向量数据库中以供后续的相似度搜索。为了演示其工作原理，我们可以使用 Python 代码高效地复制一个简单的向量数据库。
- en: Open `document_vector_database.py` in VS Code, as shown in listing 8.3\. This
    code demonstrates creating a vector database in memory and then allowing users
    to enter text to search the database and return results. The results returned
    show the document text and the similarity score.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VS Code 中打开 `document_vector_database.py`，如列表 8.3 所示。此代码演示了在内存中创建向量数据库，然后允许用户输入文本以搜索数据库并返回结果。返回的结果显示了文档文本和相似度分数。
- en: Listing 8.3 `document_vector_database.py`
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.3 `document_vector_database.py`
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Stores the document vectors into an array'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将文档向量存储到数组中'
- en: '#2 The function to perform similarity matching on query returns, matches, and
    similarity scores'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 执行查询返回、匹配和相似度分数的相似度匹配函数'
- en: '#3 The main input loop'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 主要输入循环'
- en: '#4 Loops through results and outputs text and similarity score'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 遍历结果并输出文本和相似度分数'
- en: Run this exercise to see the output (F5 in VS Code). Enter any text you like,
    and see the results of documents being returned. This search form works well for
    matching words and phrases with similar words and phrases. This form of search
    misses the word context and meaning from the document. In the next section, we’ll
    look at a way of transforming documents into vectors that better preserves their
    semantic meaning.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个练习以查看输出（在VS Code中按F5）。输入任何你喜欢的文本，并查看返回的文档结果。这种搜索表单对于匹配相似单词和短语非常有效。这种搜索方式会错过文档中的单词上下文和意义。在下一节中，我们将探讨一种将文档转换为向量以更好地保留其语义意义的方法。
- en: 8.3.3 Demystifying document embeddings
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 解密文档嵌入
- en: TF–IDF is a simple form that tries to capture semantic meaning in documents.
    However, it’s unreliable because it only counts word frequency and doesn’t understand
    the relationships between words. A better and more modern method uses document
    embedding, a form of document vectorizing that better preserves the semantic meaning
    of the document.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: TF–IDF是一种试图在文档中捕获语义意义的简单形式。然而，它不可靠，因为它只计算单词频率，而不理解单词之间的关系。一种更好且更现代的方法是使用文档嵌入，这是一种文档向量化形式，能更好地保留文档的语义意义。
- en: Embedding networks are constructed by training neural networks on large datasets
    to map words, sentences, or documents to high-dimensional vectors, capturing semantic
    and syntactic relationships based on context and relationships in the data. You
    typically use a pretrained model trained on massive datasets to embed documents
    and perform embeddings. Models are available from many sources, including Hugging
    Face and, of course, OpenAI.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入网络是通过在大数据集上训练神经网络来构建的，将单词、句子或文档映射到高维向量，基于数据和上下文中的关系捕获语义和句法关系。你通常使用在大量数据集上预训练的模型来嵌入文档并执行嵌入。模型可以从许多来源获得，包括Hugging
    Face和当然还有OpenAI。
- en: In our next scenario, we’ll use an OpenAI embedding model. These models are
    typically perfect for capturing the semantic context of embedded documents. Listing
    8.4 shows the relevant code that uses OpenAI to embed the documents into vectors
    that are then reduced to three dimensions and rendered into a plot.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一个场景中，我们将使用OpenAI嵌入模型。这些模型通常非常适合捕捉嵌入文档的语义上下文。列表8.4显示了使用OpenAI将文档嵌入到向量中的相关代码，然后这些向量被减少到三维并渲染成图表。
- en: Listing 8.4 `document_visualizing_embeddings.py` (relevant sections)
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.4 `document_visualizing_embeddings.py`（相关部分）
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Join all the items on the string '', ''.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将所有项目用逗号和空格连接起来。'
- en: '#2 Uses the OpenAI client to create the embedding'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用OpenAI客户端创建嵌入'
- en: '#3 Generates embeddings for each document of size 1536 dimensions'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 为每个1536维度的文档生成嵌入'
- en: '#4 Converts embeddings to a NumPy array for PCA'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将嵌入转换为NumPy数组以进行PCA'
- en: '#5 Applies PCA to reduce dimensions to 3 for plotting'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 将维度减少到3以进行绘图'
- en: When a document is embedded using an OpenAI model, it transforms the text into
    a vector with dimensions of 1536\. We can’t visualize this number of dimensions,
    so we use a dimensionality reduction technique via principal component analysis
    (PCA) to convert the vector of size 1536 to 3 dimensions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用OpenAI模型对文档进行嵌入时，它将文本转换为一个1536维度的向量。我们无法可视化这么多维度，因此我们使用主成分分析（PCA）这种降维技术，将1536维度的向量转换为3维。
- en: Figure 8.6 shows the output generated from running the file in VS Code. By reducing
    the embeddings to 3D, we can plot the output to show how semantically similar
    documents are now grouped.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6显示了在VS Code中运行文件生成的输出。通过将嵌入减少到3D，我们可以绘制输出以显示语义相似的文档是如何分组的。
- en: '![figure](../Images/8-6.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-6.png)'
- en: Figure 8.6 Embeddings in 3D, showing how similar semantic documents are grouped
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6 3D嵌入，显示相似语义文档是如何分组的
- en: The choice of which embedding model or service you use is up to you. The OpenAI
    embedding models are considered the best for general semantic similarity. This
    has made these models the standard for most memory and retrieval applications.
    With our understanding of how text can be vectorized with embeddings and stored
    in a vector database, we can move on to a more realistic example in the next section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择使用哪种嵌入模型或服务。OpenAI的嵌入模型被认为是通用语义相似度方面最好的。这使得这些模型成为大多数记忆和检索应用的标准。通过我们对于如何使用嵌入将文本向量化并存储在向量数据库中的理解，我们可以在下一节中继续一个更实际的例子。
- en: 8.3.4 Querying document embeddings from Chroma
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.4 从Chroma查询文档嵌入
- en: We can combine all the pieces and look at a complete example using a local vector
    database called Chroma DB. Many vector database options exist, but Chroma DB is
    an excellent local vector store for development or small-scale projects. There
    are also plenty of more robust options that you can consider later.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将所有部分结合起来，通过使用一个名为Chroma DB的本地向量数据库来查看一个完整的示例。存在许多向量数据库选项，但Chroma DB是一个优秀的本地向量存储，适用于开发或小规模项目。还有许多更健壮的选项，你可以在以后考虑。
- en: 'Listing 8.5 shows the new and relevant code sections from the `document_query_
    chromadb.py` file. Note that the results are scored by distance and not by similarity.
    Cosine distance is determined by this equation:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5展示了`document_query_chromadb.py`文件中的新和相关的代码部分。注意，结果是根据距离评分，而不是根据相似度评分。余弦距离由以下公式确定：
- en: Cosine Distance(A,B) = 1 – Cosine Similarity(A,B)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦距离(A,B) = 1 – 余弦相似度(A,B)
- en: This means that cosine distance will range from 0 for most similar to 2 for
    semantically opposite in meaning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着余弦距离的范围从0（最相似）到2（语义上相反）。
- en: Listing 8.5 `document_query_chromadb.py` (relevant code sections)
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.5 `document_query_chromadb.py`（相关代码部分）
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Generates embeddings for each document and assigns an ID'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为每个文档生成嵌入并分配一个ID'
- en: '#2 Creates a Chroma DB client and a collection'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建Chroma DB客户端和集合'
- en: '#3 Adds document embeddings to the collection'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将文档嵌入添加到集合中'
- en: '#4 Queries the datastore and returns the top n relevant documents'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 查询数据存储并返回最相关的n个文档'
- en: '#5 The input loop for user input and output of relevant documents/scores'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 用户输入循环和输出相关文档/分数的输入循环'
- en: As the earlier scenario demonstrated, you can now query the documents using
    semantic meaning rather than just key terms or phrases. These scenarios should
    now provide the background to see how the retrieval pattern works at a low level.
    In the next section, we’ll see how the retrieval pattern can be employed using
    LangChain.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文场景所示，你现在可以使用语义意义而不是仅仅关键词或短语来查询文档。这些场景现在应该为理解检索模式在底层是如何工作的提供背景。在下一节中，我们将看到如何使用LangChain来应用检索模式。
- en: 8.4 Constructing RAG with LangChain
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 使用LangChain构建RAG
- en: LangChain began as an open source project specializing in abstracting the retrieval
    pattern across multiple data sources and vector stores. It has since morphed into
    much more, but foundationally, it still provides excellent options for implementing
    retrieval.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain最初是一个专注于抽象多个数据源和向量存储检索模式的开源项目。它已经演变得更多，但基础层面上，它仍然为检索实现提供了优秀的选项。
- en: Figure 8.7 shows a diagram from LangChain that identifies the process of storing
    documents for retrieval. These same steps may be replicated in whole or in part
    to implement memory retrieval. The critical difference between document and memory
    retrieval is the source and how content is transformed.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7展示了LangChain中的一个流程图，该图标识了存储文档以供检索的过程。这些相同的步骤可以全部或部分复制以实现记忆检索。文档检索和记忆检索之间的关键区别在于来源以及内容是如何被转换的。
- en: '![figure](../Images/8-7.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-7.png)'
- en: Figure 8.7 Load, transform, embed, and store steps in storing documents for
    later retrieval
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.7 显示了存储文档以供后续检索的加载、转换、嵌入和存储步骤
- en: We’ll examine how to implement each of these steps using LangChain and understand
    the nuances and details accompanying this implementation. In the next section,
    we’ll start by splitting and loading documents with LangChain.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨如何使用LangChain实现这些步骤，并理解伴随此实现的细微差别和细节。在下一节中，我们将首先使用LangChain分割和加载文档。
- en: 8.4.1 Splitting and loading documents with LangChain
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 使用LangChain分割和加载文档
- en: Retrieval mechanisms augment the context of a given prompt with specific information
    relevant to the request. For example, you may request detailed information about
    a local document. With earlier language models, submitting the whole document
    as part of the prompt wasn’t an option due to token limitations.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 检索机制通过添加与请求相关的特定信息来增强给定提示的上下文。例如，你可能需要关于本地文档的详细信息。在早期的语言模型中，由于标记限制，将整个文档作为提示的一部分提交不是一个选项。
- en: Today, we could submit a whole document for many commercial LLMs, such as GPT-4
    Turbo, as part of a prompt request. However, the results may not be better and
    would likely cost more because of the increased number of tokens. Therefore, a
    better option is to split the document and use the relevant parts to request context—precisely
    what RAG and memory do.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们可以将整个文档提交给许多商业LLM，如GPT-4 Turbo，作为提示请求的一部分。然而，结果可能不会更好，并且可能会因为标记数量的增加而成本更高。因此，更好的选择是将文档分割，并使用相关部分请求上下文——这正是RAG和记忆所做的事情。
- en: Splitting a document is essential in breaking down content into semantically
    and specifically relevant sections. Figure 8.8 shows how to break down an HTML
    document containing the Mother Goose nursery rhymes. Often, splitting a document
    into contextual semantic chunks requires careful consideration.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 分割文档对于将内容分解成语义上和具体相关的部分至关重要。图8.8显示了如何分解包含老母鸡童谣的HTML文档。通常，将文档分割成上下文语义块需要仔细考虑。
- en: '![figure](../Images/8-8.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-8.png)'
- en: Figure 8.8 How the document would ideally be split into chunks for better semantic
    and contextual meaning
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.8 理想情况下文档如何分割成块以获得更好的语义和上下文意义
- en: Ideally, when we split documents into chunks, they are broken down by relevance
    and semantic meaning. While an LLM or agent could help us with this, we’ll look
    at current toolkit options within LangChain for splitting documents. Later in
    this chapter, we’ll look at a semantic function that can assist us in semantically
    dividing content for embeddings.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，当我们将文档分割成块时，它们应按相关性和语义意义分解。虽然LLM或代理可以帮助我们做这件事，但我们将查看LangChain中当前的工具包选项，用于分割文档。在本章的后面部分，我们将查看一个可以帮助我们在嵌入内容时进行语义划分的语义函数。
- en: For the next exercise, open `langchain_load_splitting.py` in VS Code, as shown
    in listing 8.6\. This code shows where we left off from listing 8.5, in the previous
    section. Instead of using the sample documents, we’re loading the Mother Goose
    nursery rhymes this time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个练习，在VS Code中打开`langchain_load_splitting.py`，如列表8.6所示。此代码显示了我们在上一节列表8.5中留下的地方。这次我们不是使用样本文档，而是加载这次的老母鸡童谣。
- en: Listing 8.6 `langchain_load_splitting.py` (sections and output)
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.6 `langchain_load_splitting.py`（部分和输出）
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 New LangChain imports'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 新的LangChain导入'
- en: '#2 Loads the document as HTML'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将文档作为HTML加载'
- en: '#3 Loads the document'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 加载文档'
- en: '#4 Splits the document into blocks of text 100 characters long with a 25-character
    overlap'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将文档分割成100个字符长、25个字符重叠的文本块'
- en: '#5 Embeds only 250 chunks, which is cheaper and faster'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 仅嵌入250个块，这更便宜且更快'
- en: '#6 Returns the embedding for each document'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 返回每个文档的嵌入'
- en: Note in listing 8.6 that the HTML document gets split into 100-character chunks
    with a 25-character overlap. The overlap allows the document’s parts not to cut
    off specific thoughts. We selected the splitter for this exercise because it was
    easy to use, set up, and understand.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在列表8.6中，HTML文档被分割成100个字符的块，并且有25个字符的重叠。这种重叠允许文档的部分不会切断特定的想法。我们选择这个分割器进行这个练习，因为它易于使用、设置和理解。
- en: Go ahead and run the `langchain_load_splitting.py` file in VS Code (F5). Enter
    a query, and see what results you get. The output in listing 8.6 shows good results
    given a specific example. Remember that we only embedded 250 document chunks to
    reduce costs and keep the exercise short. Of course, you can always try to embed
    the entire document or use a minor input document example.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，在VS Code中运行`langchain_load_splitting.py`文件（F5）。输入一个查询，看看你得到什么结果。列表8.6中的输出显示了给定特定示例的良好结果。请记住，我们只嵌入250个文档块以降低成本并使练习简短。当然，你总是可以尝试嵌入整个文档或使用较小的输入文档示例。
- en: Perhaps the most critical element to building proper retrieval is the process
    of document splitting. You can use numerous methods to split a document, including
    multiple concurrent methods. More than one method passes and splits the document
    for numerous embedding views of the same document. In the next section, we’ll
    examine a more general technique for splitting documents, using tokens and tokenization.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 构建适当的检索最关键的因素可能是文档分割的过程。你可以使用多种方法来分割文档，包括多种并发方法。超过一种方法会通过分割文档为同一文档的多个嵌入视图。在下一节中，我们将检查一种更通用的文档分割技术，使用标记和标记化。
- en: 8.4.2 Splitting documents by token with LangChain
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 使用LangChain按标记分割文档
- en: '*Tokenization* is the process of breaking text into word tokens. Where a word
    token represents a succinct element in the text, a token could be a word like
    *hold* or even a symbol like the left curly brace ({), depending on what’s relevant.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*分词* 是将文本分割成单词标记的过程。一个单词标记代表文本中的一个简洁元素，一个标记可以是一个像 *hold* 这样的单词，甚至是一个像左花括号 ({)
    这样的符号，具体取决于什么是有意义的。'
- en: Splitting documents using tokenization provides a better base for how the text
    will be interpreted by language models and for semantic similarity. Tokenization
    also allows the removal of irrelevant characters, such as whitespace, making the
    similarity matching of documents more relevant and generally providing better
    results.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分词技术分割文档为语言模型如何解释文本提供了一个更好的基础，以及语义相似性。分词技术还允许移除无关字符，如空白字符，使文档的相似性匹配更加相关，并通常提供更好的结果。
- en: For the next code exercise, open the `langchain_token_splitting.py` file in
    VS Code, as shown in listing 8.7\. Now we split the document using tokenization,
    which breaks the document into sections of unequal size. The unequal size results
    from the large sections of whitespace of the original document.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个代码练习，请在 VS Code 中打开 `langchain_token_splitting.py` 文件，如图 8.7 所示。现在我们使用分词技术将文档分割成大小不等的部分。这种不均匀的大小是由于原始文档中大量空白区域造成的。
- en: Listing 8.7 `langchain_token_splitting.py` (relevant new code)
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.7 `langchain_token_splitting.py`（相关新代码）
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Updates to 50 tokens and overlap of 10 tokens'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 更新为 50 个标记和 10 个标记的重叠'
- en: '#2 Selects just the documents that contain rhymes'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 仅选择包含押韵的文档'
- en: '#3 Uses the database’s similarity search'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用数据库的相似性搜索'
- en: '#4 Breaks into irregular size chunks because of the whitespace'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 由于空白字符而分割成不规则大小的块'
- en: Run the `langchain_token_splitting.py` code in VS Code (F5). You can use the
    query we used last time or your own. Notice how the results are significantly
    better than the previous exercise. However, the results are still suspect because
    the query uses several similar words in the same order.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VS Code 中运行 `langchain_token_splitting.py` 代码（按 F5）。您可以使用上次使用的查询或您自己的查询。注意，结果比上一个练习明显更好。然而，结果仍然可疑，因为查询使用了几个相同顺序的相似词。
- en: 'A better test would be to try a semantically similar phrase but one that uses
    different words and check the results. With the code still running, enter a new
    phrase to query: `Why` `are` `the` `girls` `crying?` Listing 8.8 shows the results
    of executing that query. If you run this example yourself and scroll down over
    the output, you’ll see Georgy Porgy appear in either the second or third returned
    document.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的测试是尝试一个语义上相似的短语，但使用不同的词，并检查结果。代码仍在运行时，输入一个新的查询短语：`为什么` `女孩们` `在哭泣？` 列表
    8.8 显示了执行该查询的结果。如果您自己运行此示例并向下滚动输出，您将看到乔治·波吉出现在返回的第二或第三份文档中。
- en: 'Listing 8.8 Query: Who made the girls cry?'
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.8 查询：谁让女孩们哭泣？
- en: '[PRE7]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This exercise shows how various retrieval methods can be employed to return
    documents semantically. With this base established, we can see how RAG can be
    applied to knowledge and memory systems. The following section will discuss RAG
    as it applies to knowledge of agents and agentic systems.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习展示了如何使用各种检索方法来返回语义上的文档。在这个基础上，我们可以看到 RAG 如何应用于知识和记忆系统。下一节将讨论 RAG 在应用于代理和代理系统知识时的应用。
- en: 8.5 Applying RAG to building agent knowledge
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 将 RAG 应用于构建代理知识
- en: Knowledge in agents encompasses employing RAG to search semantically across
    unstructured documents. These documents could be anything from PDFs to Microsoft
    Word documents and all text, including code. Agentic knowledge also includes using
    unstructured documents for Q&A, reference lookup, information augmentation, and
    other future patterns.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 代理中的知识包括使用 RAG 在非结构化文档中进行语义搜索。这些文档可以是 PDF 文件、Microsoft Word 文档以及所有文本，包括代码。代理知识还包括使用非结构化文档进行问答、参考查找、信息增强和其他未来模式。
- en: Nexus, the agent platform developed in tandem with this book and introduced
    in the previous chapter, employs complete knowledge and memory systems for agents.
    In this section, we’ll uncover how the knowledge system works.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Nexus，是与本书一起开发并在上一章中介绍的代理平台，为代理提供了完整的知识和记忆系统。在本节中，我们将揭示知识系统是如何工作的。
- en: To install Nexus for just this chapter, see listing 8.9\. Open a terminal within
    the `chapter_08` folder, and execute the commands in the listing to download,
    install, and run Nexus in normal or development mode. If you want to refer to
    the code, you should install the project in development and configure the debugger
    to run the Streamlit app from VS Code. Refer to chapter 7 if you need a refresher
    on any of these steps.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要仅为此章节安装 Nexus，请参阅列表 8.9。在 `chapter_08` 文件夹内打开一个终端，并执行列表中的命令以下载、安装和以正常或开发模式运行
    Nexus。如果您想参考代码，应将项目安装在开发模式下，并配置调试器从 VS Code 运行 Streamlit 应用。如果您需要回顾这些步骤中的任何一项，请参阅第
    7 章。
- en: Listing 8.9 Installing Nexus
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.9 安装 Nexus
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Regardless of which method you decide to run the app in after you log in, navigate
    to the Knowledge Store Manager page, as shown in figure 8.9\. Create a new Knowledge
    Store, and then upload the `sample_documents/back_to_the_future.txt` movie script.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您登录后决定使用哪种方法运行应用程序，请导航到如图 8.9 所示的知识库管理器页面。创建一个新的知识库，然后上传 `sample_documents/back_to_the_future.txt`
    电影剧本。
- en: '![figure](../Images/8-9.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-9.png)'
- en: Figure 8.9 Adding a new knowledge store and populating it with a document
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.9 添加新的知识库并填充文档
- en: The script is a large document, and it may take a while to load, chunk, and
    embed the parts into the Chroma DB vector database. Wait for the indexing to complete,
    and then you can inspect the embeddings and run a query, as shown in figure 8.10.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本是一个大型文档，加载、分块和将部分嵌入到 Chroma DB 向量数据库中可能需要一些时间。等待索引完成，然后您可以检查嵌入并运行查询，如图 8.10
    所示。
- en: '![figure](../Images/8-10.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-10.png)'
- en: Figure 8.10 The embeddings and document query views
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.10 嵌入和文档查询视图
- en: Now, we can connect the knowledge store to a supported agent and ask questions.
    Use the top-left selector to choose the chat page within the Nexus interface.
    Then, select an agent and the `time_travel` knowledge store, as shown in figure
    8.11\. You will also need to select an agent engine that supports knowledge. Each
    of the multiple agent engines requires the proper configuration to be accessible.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将知识库连接到支持的代理并提问。使用左上角的选择器在 Nexus 界面中选择聊天页面。然后，选择一个代理和 `time_travel` 知识库，如图
    8.11 所示。您还需要选择一个支持知识的代理引擎。每个代理引擎都需要适当的配置才能访问。
- en: '![figure](../Images/8-11.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-11.png)'
- en: Figure 8.11 Enabling the knowledge store for agent use
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.11 启用知识库以供代理使用
- en: Currently, as of this chapter, Nexus supports access to only a single knowledge
    store at a time. In a future version, agents may be able to select multiple knowledge
    stores at a time. This may include more advanced options, from semantic knowledge
    to employing other forms of RAG.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，截至本章，Nexus 仅支持一次访问单个知识库。在未来版本中，代理可能能够一次选择多个知识库。这可能包括从语义知识到使用其他形式的 RAG 的更高级选项。
- en: You can also configure the RAG settings within the Configuration tab of the
    Knowledge Store Manager page, as shown in figure 8.12\. As of now, you can select
    from the type of splitter (Chunking Option field) to chunk the document, along
    with the Chunk Size field and Overlap field.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在知识库管理器页面中的配置选项卡内配置 RAG 设置，如图 8.12 所示。到目前为止，您可以从拆分文档的类型（拆分选项字段）中选择，以及选择分块大小字段和重叠字段。
- en: '![figure](../Images/8-12.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-12.png)'
- en: Figure 8.12 Managing the knowledge store splitting and chunking options
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.12 管理知识库拆分和分块选项
- en: The loading, splitting, chunking, and embedding options provided are the only
    basic options supported by LangChain for now. In future versions of Nexus, more
    options and patterns will be offered. The code to support other options can be
    added directly to Nexus.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 目前提供的加载、拆分、分块和嵌入选项是唯一支持的基本选项。在 Nexus 的未来版本中，将提供更多选项和模式。支持其他选项的代码可以直接添加到
    Nexus 中。
- en: We won’t cover the code that performs the RAG as it’s very similar to what we
    already covered. Feel free to review the Nexus code, particularly the `KnowledgeManager`
    class in the `knowledge_manager.py` file.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会介绍执行 RAG 的代码，因为它与我们之前介绍的内容非常相似。您可以自由地回顾 Nexus 代码，特别是 `knowledge_manager.py`
    文件中的 `KnowledgeManager` 类。
- en: While the retrieval patterns for knowledge and memory are quite similar for
    augmentation, the two patterns differ when it comes to populating the stores.
    In the next section, we’ll explore what makes memory in agents unique.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于增强知识库和记忆库的检索模式相当相似，但在填充存储时，这两种模式有所不同。在下一节中，我们将探讨使代理中的记忆独特的原因。
- en: 8.6 Implementing memory in agentic systems
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 在代理系统中实现记忆
- en: Memory in agents and AI applications is often described in the same terms as
    cognitive memory functions. *Cognitive* memory describes the type of memory we
    use to remember what we did 30 seconds ago or how tall we were 30 years ago. Computer
    memory is also an essential element of agent memory, but one we won’t consider
    in this section.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理和人工智能应用中，记忆通常用与认知记忆功能相同的术语描述。*认知*记忆描述了我们用来记住30秒前我们做了什么或30年前我们有多高的记忆类型。计算机记忆也是代理记忆的一个基本要素，但本节不会考虑这一点。
- en: 'Figure 8.13 shows how memory is broken down into sensory, short-term, and long-term
    memory. This memory can be applied to AI agents, and this list describes how each
    form of memory maps to agent functions:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13展示了记忆是如何分解成感觉、短期和长期记忆的。这种记忆可以应用于人工智能代理，以下列表描述了每种记忆形式如何映射到代理功能：
- en: '*Sensory memory in AI* —Functions such as RAG but with images/audio/haptic
    data forms. Briefly holds input data (e.g., text and images) for immediate processing
    but not long-term storage.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人工智能中的感觉记忆* — 函数类似于RAG，但使用图像/音频/触觉数据形式。短暂地保存输入数据（例如，文本和图像）以供即时处理，但不进行长期存储。'
- en: '*Short-term/working memory in AI* —Acts as an active memory buffer of conversation
    history. We’re holding a limited amount of recent input and context for immediate
    analysis and response generation. Within Nexus, short- and long-term conversational
    memory is also held in the context of the thread.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人工智能中的短期/工作记忆* — 作为对话历史的活跃记忆缓冲区。我们正在保存有限数量的最近输入和上下文以供即时分析和响应生成。在Nexus中，短期和长期对话记忆也保存在线程的上下文中。'
- en: '*Long-term memory in AI* —Longer-term memory storage relevant to the agent’s
    or user’s life. Semantic memory provides a robust capacity to store and retrieve
    relevant global or local facts and concepts.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人工智能中的长期记忆* — 与代理或用户生活相关的长期记忆存储。语义记忆提供了强大的存储和检索相关全局或局部事实和概念的能力。'
- en: '![figure](../Images/8-13.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-13.png)'
- en: Figure 8.13 How memory is broken down into various forms
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.13 如何将记忆分解成各种形式
- en: While memory uses the exact same retrieval and augmentation mechanisms as knowledge,
    it typically differs significantly when updating or appending memories. Figure
    8.14 highlights the process of capturing and using memories to augment prompts.
    Because memories are often different from the size of complete documents, we can
    avoid using any splitting or chunking mechanisms.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然记忆使用与知识完全相同的检索和增强机制，但在更新或追加记忆时通常会有显著差异。图8.14突出了捕获和使用记忆来增强提示的过程。因为记忆通常与完整文档的大小不同，我们可以避免使用任何分割或分块机制。
- en: '![figure](../Images/8-14.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-14.png)'
- en: Figure 8.14 Basic memory retrieval and augmentation workflow
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.14 基本记忆检索和增强工作流程
- en: Nexus provides a mechanism like the knowledge store, allowing users to create
    memory stores that can be configured for various uses and applications. It also
    supports some of the more advanced memory forms highlighted in figure 8.13\. The
    following section will examine how basic memory stores work in Nexus.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Nexus提供了一个类似于知识库的机制，允许用户创建可以配置用于各种用途和应用的记忆存储。它还支持图8.13中突出显示的一些更高级的记忆形式。下一节将探讨Nexus中基本记忆存储的工作方式。
- en: 8.6.1 Consuming memory stores in Nexus
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.1 在Nexus中消费记忆存储
- en: Memory stores operate and are constructed like knowledge stores in Nexus. They
    both heavily rely on the retrieval pattern. What differs is the extra steps memory
    systems take to build new memories.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在Nexus中，记忆存储的操作和构建方式类似于知识存储。它们都高度依赖于检索模式。不同的是，记忆系统在构建新记忆时采取的额外步骤。
- en: Go ahead and start Nexus, and refer to listing 8.9 if you need to install it.
    After logging in, select the Memory page, and create a new memory store, as shown
    in figure 8.15\. Select an agent engine, and then add a few personal facts and
    preferences about yourself.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 开始运行Nexus，如果需要安装，请参考列表8.9。登录后，选择记忆页面，并创建一个新的记忆存储，如图8.15所示。选择一个代理引擎，然后添加一些关于你自己的个人事实和偏好。
- en: '![figure](../Images/8-15.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-15.png)'
- en: Figure 8.15 Adding memories to a newly created memory store
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.15 向新创建的记忆存储添加记忆
- en: The reason we need an agent (LLM) was shown in figure 8.14 earlier. When information
    is fed into a memory store, it’s generally processed through an LLM using a memory
    function, whose purpose is to process the statements/conversations into semantically
    relevant information related to the type of memory.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要代理（LLM）的原因在之前的图8.14中已经展示。当信息被输入到记忆存储中时，它通常通过使用记忆函数的LLM进行处理，该函数的目的是将陈述/对话处理成与记忆类型相关的语义相关信息。
- en: Listing 8.10 shows the conversational memory function used to extract information
    from a conversation into memories. Yes, this is just the header portion of the
    prompt sent to the LLM, instructing it how to extract information from a conversation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.10展示了用于从对话中提取信息到记忆中的对话记忆函数。是的，这只是发送给LLM的提示的标题部分，指示它如何从对话中提取信息。
- en: Listing 8.10 Conversational memory function
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.10 对话记忆函数
- en: '[PRE9]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After you generate a few relevant memories about yourself, return to the Chat
    area in Nexus, enable the `my_memory` memory store, and see how well the agent
    knows you. Figure 8.16 shows a sample conversation using a different agent engine.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成一些关于自己的相关记忆后，返回Nexus的聊天区域，启用`my_memory`记忆存储，看看代理对你了解得有多好。图8.16展示了使用不同代理引擎的示例对话。
- en: '![figure](../Images/8-16.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-16.png)'
- en: Figure 8.16 Conversing with a different agent on the same memory store
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.16 在同一记忆存储上与不同的代理进行对话
- en: This is an example of a basic memory pattern that extracts facts/preferences
    from conversations and stores them in a vector database as memories. Numerous
    other implementations of memory follow those displayed earlier in figure 8.13\.
    We’ll implement those in the next section.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个从对话中提取事实/偏好并将其作为记忆存储在向量数据库中的基本记忆模式示例。许多其他记忆的实现方式遵循之前图8.13中展示的。我们将在下一节中实现这些。
- en: 8.6.2 Semantic memory and applications to semantic, episodic, and procedural
    memory
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.2 语义记忆及其在语义、情景和程序性记忆中的应用
- en: Psychologists categorize memory into multiple forms, depending on what information
    is remembered. Semantic, episodic, and procedural memory all represent different
    types of information. *Episodic* memories are about events, *procedural* memories
    are about the process or steps, and *semantic* represents the meaning and could
    include feelings or emotions. Other forms of memory (geospatial is another), aren’t
    described here but could be.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 心理学家根据记忆中记住的信息将记忆分为多种形式。语义、情景和程序性记忆都代表不同类型的信息。*情景*记忆关于事件，*程序性*记忆关于过程或步骤，而*语义*代表意义，可能包括感觉或情感。其他形式的记忆（如地理空间记忆），这里没有描述，但可能存在。
- en: Because these memories rely on an additional level of categorization, they also
    rely on another level of semantic categorization. Some platforms, such as Semantic
    Kernel (SK), refer to this as *semantic memory*. This can be confusing because
    semantic categorization is also applied to extract episodic and procedural memories.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些记忆依赖于额外的分类级别，它们也依赖于另一个级别的语义分类。一些平台，如语义内核（SK），将这称为*语义记忆*。这可能令人困惑，因为语义分类也应用于提取情景和程序性记忆。
- en: Figure 8.17 shows the semantic memory categorization process, also sometimes
    called semantic memory. The difference between semantic memory and regular memory
    is the additional step of processing the input semantically and extracting relevant
    questions that can be used to query the memory-relevant vector database.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17展示了语义记忆分类过程，有时也称为语义记忆。语义记忆与常规记忆的区别在于多了一个处理输入语义并提取可用于查询记忆相关向量数据库的相关问题的步骤。
- en: '![figure](../Images/8-17.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-17.png)'
- en: Figure 8.17 How semantic memory augmentation works
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.17 语义记忆增强的工作原理
- en: The benefit of using semantic augmentation is the increased ability to extract
    more relevant memories. We can see this in operation by jumping back into Nexus
    and creating a new semantic memory store.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用语义增强的好处是能够提取更多相关记忆的能力增强。我们可以通过回到Nexus并创建一个新的语义记忆存储来在操作中看到这一点。
- en: Figure 8.18 shows how to configure a new memory store using semantic memory.
    As of yet, you can’t configure the specific function prompts for memory, augmentation,
    and summarization. However, it can be useful to read through each of the function
    prompts to gain a sense of how they work.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18展示了如何使用语义记忆配置新的记忆存储。到目前为止，您还不能配置记忆、增强和总结的具体功能提示。然而，阅读每个功能提示以了解它们的工作方式可能是有用的。
- en: '![figure](../Images/8-18.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-18.png)'
- en: Figure 8.18 Configuration for changing the memory store type to semantic
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.18将记忆存储类型更改为语义的配置
- en: Now, if you go back and add facts and preferences, they will convert to the
    semantics of the relevant memory type. Figure 8.19 shows an example of memories
    being populated for the same set of statements into two different forms of memory.
    Generally, the statements entered into memory would be more specific to the form
    of memory.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你回顾并添加事实和偏好，它们将转换为相关记忆类型的语义。图8.19显示了将同一组陈述填充到两种不同形式的记忆中的示例。一般来说，输入到记忆中的陈述会更具体地对应记忆的形式。
- en: '![figure](../Images/8-19.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-19.png)'
- en: Figure 8.19 Comparing memories for the same information given two different
    memory types
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.19比较两种不同记忆类型给出的相同信息的记忆
- en: Memory and knowledge can significantly assist an agent with various application
    types. Indeed, a single memory/knowledge store could feed one or multiple agents,
    allowing for further specialized interpretations of both types of stores. We’ll
    finish out the chapter by discussing memory/knowledge compression next.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆和知识可以显著帮助各种类型的代理。确实，单个记忆/知识存储可以喂养一个或多个代理，允许对这两种存储类型进行进一步的专业解释。我们将通过讨论记忆/知识压缩来结束本章。
- en: 8.7 Understanding memory and knowledge compression
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 理解记忆和知识压缩
- en: Much like our own memory, memory stores can become cluttered with redundant
    information and numerous unrelated details over time. Internally, our minds deal
    with memory clutter by compressing or summarizing memories. Our minds remember
    more significant details over less important ones, and memories accessed more
    frequently.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们自己的记忆一样，记忆存储可能会随着时间的推移变得杂乱，充满冗余信息和众多无关的细节。在内部，我们的心智通过压缩或总结记忆来处理记忆杂乱。我们的心智记住更重要的细节，而不是不那么重要的细节，以及更频繁访问的记忆。
- en: We can apply similar principles of memory compression to agent memory and other
    retrieval systems to extract significant details. The principle of compression
    is similar to semantic augmentation but adds another layer to the preclusters
    groups of related memories that can collectively be summarized.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将记忆压缩的类似原则应用于代理记忆和其他检索系统，以提取重要细节。压缩的原则与语义增强类似，但为相关记忆的预聚类组添加了另一层，这些组可以共同总结。
- en: Figure 8.20 shows the process of memory/knowledge compression. Memories or knowledge
    are first clustered using an algorithm such as k-means. Then, the groups of memories
    are passed through a compression function, which summarizes and collects the items
    into more succinct representations.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20展示了记忆/知识压缩的过程。记忆或知识首先使用如k-means之类的算法进行聚类。然后，将记忆组通过压缩函数传递，该函数总结并收集项目以形成更简洁的表示。
- en: '![figure](../Images/8-20.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-20.png)'
- en: Figure 8.20 The process of memory and knowledge compression
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.20记忆和知识压缩的过程
- en: Nexus provides for both knowledge and memory store compression using k-means
    optimal clustering. Figure 8.21 shows the compression interface for memory. Within
    the compression interface, you’ll see the items displayed in 3D and clustered.
    The size (number of items) of the clusters is shown in the left table.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Nexus通过使用k-means最优聚类为知识和记忆存储提供压缩。图8.21显示了记忆的压缩界面。在压缩界面中，您将看到以3D形式显示并聚类的项目。簇的大小（项目数量）显示在左侧的表中。
- en: '![figure](../Images/8-21.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-21.png)'
- en: Figure 8.21 The interface for compressing memories
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.21压缩记忆的界面
- en: Compressing memories and even knowledge is generally recommended if the number
    of items in a cluster is large or unbalanced. Each use case for compression may
    vary depending on the use and application of memories. Generally, though, if an
    inspection of the items in a store contains repetitive or duplicate information,
    it’s a good time for compression. The following is a summary of use cases for
    applications that would benefit from compression.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果聚类中的项目数量很大或不平衡，通常建议压缩记忆和知识。每个压缩用例可能根据记忆的使用和应用而有所不同。不过，一般来说，如果对存储中的项目进行检查发现存在重复或重复信息，那么进行压缩就是时候了。以下是对从压缩中受益的应用程序的用例总结。
- en: The case for knowledge compression
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知识压缩的案例
- en: Knowledge retrieval and augmentation have also been shown to benefit significantly
    from compression. Results will vary by use case, but generally, the more verbose
    the source of knowledge, the more it will benefit from compression. Documents
    that feature literary prose, such as stories and novels, will benefit more than,
    say, a base of code. However, if the code is likewise very repetitive, compression
    could also be shown to be beneficial.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 知识检索和增强也已被证明可以从压缩中受益显著。结果将因用例而异，但通常，知识来源越冗长，它从压缩中受益就越多。具有文学散文的文档，如故事和小说，将比代码库等受益更多。然而，如果代码同样非常重复，压缩也可能显示出其益处。
- en: The case for how often you apply compression
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 压缩应用频率的案例
- en: Memory will often benefit from the periodic compression application, whereas
    knowledge stores typically only help on the first load. How frequently you apply
    compression will greatly depend on the memory use, frequency, and quantity.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 定期应用压缩通常会从内存中受益，而知识库通常只在第一次加载时提供帮助。你应用压缩的频率将很大程度上取决于内存使用、频率和数量。
- en: The case for applying compression more than once
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多次应用压缩的案例
- en: Multiple passes of compression at the same time has been shown to improve retrieval
    performance. Other patterns have also suggested using memory or knowledge at various
    levels of compression. For example, a knowledge store is compressed two times,
    resulting in three different levels of knowledge.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 同时进行多次压缩已被证明可以提高检索性能。其他模式也建议在压缩的不同级别使用记忆或知识。例如，知识库被压缩两次，从而产生三个不同的知识级别。
- en: The case for blending knowledge and memory compression
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知识和记忆压缩融合的案例
- en: If a system is specialized to a particular source of knowledge and that system
    also employs memories, there may be further optimization to consolidate stores.
    Another approach is to populate memory with the starting knowledge of a document
    directly.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个系统专门针对特定的知识来源，并且该系统还使用记忆，那么可能还有进一步的优化来整合存储。另一种方法是直接用文档的起始知识填充记忆。
- en: The case for multiple memory or knowledge stores
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多个记忆或知识存储的案例
- en: In more advanced systems, we’ll look at agents employing multiple memory and
    knowledge stores relevant to their workflow. For example, an agent could employ
    individual memory stores as part of its conversations with individual users, perhaps
    including the ability to share different groups of memory with different groups
    of individuals. Memory and knowledge retrieval are cornerstones of agentic systems,
    and we can now summarize what we covered and review some learning exercises in
    the next section.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在更高级的系统里，我们将探讨使用与其工作流程相关的多个记忆和知识库的代理。例如，一个代理可以作为其与单个用户的对话的一部分使用单独的记忆库，也许包括能够与不同群体分享不同组记忆的能力。记忆和知识检索是代理系统的基石，我们现在可以总结我们所学的内容，并在下一节回顾一些学习练习。
- en: 8.8 Exercises
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.8 练习
- en: 'Use the following exercises to improve your knowledge of the material:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下练习来提高你对材料的了解：
- en: '*Exercise 1* —Load and Split a Different Document (Intermediate)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习 1* — 加载并拆分不同的文档（中级）'
- en: '*Objective* *—*Understand the effect of document splitting on retrieval efficiency
    by using LangChain.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标* — 使用LangChain了解文档拆分对检索效率的影响。'
- en: '*Tasks*:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务*:'
- en: Select a different document (e.g., a news article, a scientific paper, or a
    short story).
  id: totrans-240
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择不同的文档（例如，新闻文章、科学论文或短篇小说）。
- en: Use LangChain to load and split the document into chunks.
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LangChain加载并将文档拆分成块。
- en: Analyze how the document is split into chunks and how it affects the retrieval
    process.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析文档如何拆分成块以及这对检索过程的影响。
- en: '*Exercise 2* —Experiment with Semantic Search (Intermediate)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习 2* — 尝试语义搜索（中级）'
- en: '*Objective* *—*Compare the effectiveness of various vectorization techniques
    by performing semantic searches.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标* — 通过执行语义搜索比较各种向量化技术的有效性。'
- en: '*Tasks*:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务*:'
- en: Choose a set of documents for semantic search.
  id: totrans-246
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一组文档进行语义搜索。
- en: Use a vectorization method such as Word2Vec or BERT embeddings instead of TF–IDF.
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Word2Vec或BERT嵌入等向量化方法而不是TF–IDF。
- en: Perform the semantic search, and compare the results with those obtained using
    TF–IDF to understand the differences and effectiveness.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行语义搜索，并将结果与使用TF–IDF获得的结果进行比较，以了解差异和有效性。
- en: '*Exercise 3* —Implement a Custom RAG Workflow (Advanced)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习 3* — 实现自定义RAG工作流程（高级）'
- en: '*Objective* *—*Apply theoretical knowledge of RAG in a practical context using
    LangChain.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标* — 在实际环境中使用LangChain应用RAG的理论知识。'
- en: '*Tasks*:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务*：'
- en: Choose a specific application (e.g., customer service inquiries or academic
    research queries).
  id: totrans-252
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个特定的应用（例如，客户服务查询或学术研究查询）。
- en: Design and implement a custom RAG workflow using LangChain.
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LangChain设计和实现一个定制的RAG工作流程。
- en: Tailor the workflow to suit the chosen application, and test its effectiveness.
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整工作流程以适应所选应用，并测试其有效性。
- en: '*Exercise 4* —Build a Knowledge Store and Experiment with Splitting Patterns
    (Intermediate)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习4* — 构建知识库并实验分割模式（中级）'
- en: '*Objective* *—*Understand how different splitting patterns and compression
    affect knowledge retrieval.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标* — 理解不同的分割模式和压缩如何影响知识检索。'
- en: '*Tasks*:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务*：'
- en: Build a knowledge store, and populate it with a couple of documents.
  id: totrans-258
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个知识库，并用几份文档填充它。
- en: Experiment with different forms of splitting/chunking patterns, and analyze
    their effect on retrieval.
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的分割/分块模式，并分析它们对检索的影响。
- en: Compress the knowledge store, and observe the effects on query performance.
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩知识库，并观察对查询性能的影响。
- en: '*Exercise 5* —Build and Test Various Memory Stores (Advanced)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习5* — 构建和测试各种记忆存储（高级）'
- en: '*Objective* *—*Understand the uniqueness and use cases of different memory
    store types.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标* — 理解不同记忆存储类型的独特性和用例。'
- en: '*Tasks*:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务*：'
- en: Build various forms of memory stores (conversational, semantic, episodic, and
    procedural).
  id: totrans-264
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建各种形式的记忆存储（对话式、语义、情景和程序性）。
- en: Interact with an agent using each type of memory store, and observe the differences.
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用每种类型的记忆存储与代理进行交互，并观察差异。
- en: Compress the memory store, and analyze the effect on memory retrieval.
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩记忆存储，并分析对记忆检索的影响。
- en: Summary
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Memory in AI applications differentiates between unstructured and structured
    memory, highlighting their use in contextualizing prompts for more relevant interactions.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AI应用中，记忆区分了非结构化和结构化记忆，突出了它们在为更相关的交互情境化提示中的应用。
- en: Retrieval augmented generation (RAG) is a mechanism for enhancing prompts with
    context from external documents, using vector embeddings and similarity search
    to retrieve relevant content.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是一种通过使用向量嵌入和相似度搜索从外部文档中检索相关内容来增强提示的上下文机制。
- en: Semantic search with document indexing converts documents into semantic vectors
    using TF–IDF and cosine similarity, enhancing the capability to perform semantic
    searches across indexed documents.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文档索引进行语义搜索，通过TF-IDF和余弦相似度将文档转换为语义向量，增强在索引文档中执行语义搜索的能力。
- en: Vector databases and similarity search stores document vectors in a vector database,
    facilitating efficient similarity searches and improving retrieval accuracy.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量数据库和相似度搜索存储将文档向量存储在向量数据库中，便于高效的相似度搜索并提高检索准确性。
- en: Document embeddings capture semantic meanings, using models such as OpenAI’s
    models to generate embeddings that preserve a document’s context and facilitate
    semantic similarity searches.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档嵌入通过使用如OpenAI的模型等模型捕获语义含义，生成嵌入以保留文档的上下文并促进语义相似度搜索。
- en: LangChain provides several tools for performing RAG, and it abstracts the retrieval
    process, allowing for easy implementation of RAG and memory systems across various
    data sources and vector stores.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain提供了执行RAG的几个工具，并抽象了检索过程，使得在各个数据源和向量存储中轻松实现RAG和记忆系统成为可能。
- en: Short-term and long-term memory in LangChain implements conversational memory
    within LangChain, distinguishing between short-term buffering patterns and long-term
    storage solutions.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain中的短期和长期记忆实现了LangChain内的对话式记忆，区分了短期缓冲模式和长期存储解决方案。
- en: Storing document vectors in databases for efficient similarity searches is crucial
    for implementing scalable retrieval systems in AI applications.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据库中存储文档向量对于在AI应用中实现可扩展的检索系统至关重要。
- en: Agent knowledge directly relates to the general RAG pattern of performing question
    and answer on documents or other textual information.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理知识直接关联到在文档或其他文本信息上执行问答的通用RAG模式。
- en: Agent memory is a pattern related to RAG that captures the agentic interactions
    with users, itself, and other systems.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理记忆是与RAG相关的模式，它捕捉了代理与用户、自身和其他系统之间的交互。
- en: Nexus is a platform that implements agentic knowledge and memory systems, including
    setting up knowledge stores for document retrieval and memory stores for various
    forms of memory.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nexus是一个实现代理知识记忆系统的平台，包括为文档检索设置知识库和为各种形式的记忆设置记忆库。
- en: Semantic memory augmentation (semantic memory) differentiates between various
    types of memories (semantic, episodic, procedural). It implements them through
    semantic augmentation, enhancing agents’ ability to recall and use information
    relevantly specific to the nature of the memories.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义记忆增强（语义记忆）区分不同类型的记忆（语义记忆、情景记忆、程序性记忆）。它通过语义增强实现这些记忆类型，增强代理者回忆和使用与记忆性质相关的特定信息的能力。
- en: Memory and knowledge compression are techniques for condensing information stored
    in memory and knowledge systems, improving retrieval efficiency and relevancy
    through clustering and summarization.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记忆与知识压缩是用于压缩存储在记忆和知识系统中的信息的技术，通过聚类和总结来提高检索效率和相关性。
