["```py\n$ pip install gensim\n$ wget https://s3.amazonaws.com/dl4j-distribution/\n\u00a0 GoogleNews-vectors-negative300.bin.gz -O googlenews.bin.gz\n\n```", "```py\nfrom gensim.models import KeyedVectors\n\nmodel = KeyedVectors.load_word2vec_format('./googlenews.bin.gz',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0binary=True)\n\n```", "```py\n$ pip install leveldb\n```", "```py\n$ wget http://www.cnts.ua.ac.be/conll2000/chunking/train.txt.gz \n  -O - | gunzip |\n\u00a0 cut -f1,2 -d\" \" > pos.train.txt\n\n$ wget http://www.cnts.ua.ac.be/conll2000/chunking/test.txt.gz \n  -O - | gunzip |\n\u00a0 cut -f1,2 -d \" \" > pos.test.txt\n\n```", "```py\nConfidence NN\nin IN\nthe DT\npound NN\nis VBZ\nwidely RB\nexpected VBN\nto TO\ntake VB\nanother DT\nsharp JJ\ndive NN\nif IN\ntrade NN\nfigures NNS\nfor IN\nSeptember NNP\n, ,\ndue JJ\nfor IN\nrelease NN\ntomorrow NN\n...\n```", "```py\ndef create_pos_dataset(filein, fileout):\n\u00a0\u00a0dataset = []\n\u00a0\u00a0with open(filein) as f:\n\u00a0\u00a0\u00a0\u00a0dataset_raw = f.readlines()\n\u00a0\u00a0\u00a0\u00a0dataset_raw = [e.split() for e in dataset_raw\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if len(e.split()) > 0]\n\n\u00a0\u00a0counter = 0\n\u00a0\u00a0while counter < len(dataset_raw):\n\u00a0\u00a0\u00a0\u00a0pair = dataset_raw[counter]\n\u00a0\u00a0\u00a0\u00a0if counter < len(dataset_raw) - 1:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0next_pair = dataset_raw[counter + 1]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if (pair[0] + \"_\" + next_pair[0] in model) and \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0(pair[1] == next_pair[1]):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dataset.append([pair[0] + \"_\" + next_pair[0], pair[1]])\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0counter += 2\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0continue\n\n\u00a0\u00a0\u00a0\u00a0word = re.sub(\"\\d\", \"#\", pair[0])\n\u00a0\u00a0\u00a0\u00a0word = re.sub(\"-\", \"_\", word)\n\n\u00a0\u00a0\u00a0\u00a0if word in model:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dataset.append([word, pair[1]])\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0counter += 1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0continue\n\n\u00a0\u00a0\u00a0\u00a0if \"_\" in word:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0subwords = word.split(\"_\")\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for subword in subwords:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if not (subword.isspace() or len(subword) == 0):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dataset.append([subword, pair[1]])\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0counter += 1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0continue\n\n\u00a0\u00a0\u00a0\u00a0dataset.append([word, pair[1]])\n\u00a0\u00a0\u00a0\u00a0counter += 1\n\n\u00a0\u00a0with open(fileout, 'w') as processed_file:\n\u00a0\u00a0\u00a0\u00a0for item in dataset:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0processed_file.write(\"%s\\n\" % (item[0] + \" \" + item[1]))\n\n\u00a0\u00a0return dataset\n\ntrain_pos_dataset = create_pos_dataset('./pos.train.txt',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'./pos.train.processed.txt')\ntest_pos_dataset = create_pos_dataset('./pos.test.txt',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'./pos.test.processed.txt')\n\n```", "```py\nimport leveldb\ndb = leveldb.LevelDB(\"./word2vecdb\")\n\ncounter = 0\ndataset_vocab = {}\ntags_to_index = {}\nindex_to_tags = {}\nindex = 0\nfor pair in train_pos_dataset + test_pos_dataset:\n\u00a0\u00a0if pair[0] not in dataset_vocab:\n\u00a0\u00a0\u00a0\u00a0dataset_vocab[pair[0]] = index\n\u00a0\u00a0\u00a0\u00a0index += 1\n\u00a0\u00a0if pair[1] not in tags_to_index:\n\u00a0\u00a0\u00a0\u00a0tags_to_index[pair[1]] = counter\n\u00a0\u00a0\u00a0\u00a0index_to_tags[counter] = pair[1]\n\u00a0\u00a0\u00a0\u00a0counter += 1\n\nnonmodel_cache = {}\n\ncounter = 1\ntotal = len(dataset_vocab.keys())\nfor word in dataset_vocab:\n\n\u00a0\u00a0if word in model:\n\u00a0\u00a0\u00a0\u00a0db.Put(bytes(word,'utf-8'), model[word])\n\u00a0\u00a0elif word in nonmodel_cache:\n\u00a0\u00a0\u00a0\u00a0db.Put(bytes(word,'utf-8'), nonmodel_cache[word])\n\u00a0\u00a0else:\n\u00a0\u00a0\u00a0\u00a0#print(word)\n\u00a0\u00a0\u00a0\u00a0nonmodel_cache[word] = np.random.uniform(-0.25,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00.25,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0300).astype(np.float32)\n\u00a0\u00a0\u00a0\u00a0db.Put(bytes(word,'utf-8'), nonmodel_cache[word])\n\u00a0\u00a0counter += 1\n\n```", "```py\ndb = leveldb.LevelDB(\"./word2vecdb\")\n\nx = db.Get(bytes('Confidence','utf-8'))\nprint(np.frombuffer(x,dtype='float32').shape)\n# out: (300,)\n\n```", "```py\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nclass NgramPOSDataset(Dataset):\n\u00a0\u00a0def __init__(self, db, dataset, tags_to_index, n_grams):\n\u00a0\u00a0\u00a0\u00a0super(NgramPOSDataset, self).__init__()\n\u00a0\u00a0\u00a0\u00a0self.db = db\n\u00a0\u00a0\u00a0\u00a0self.dataset = dataset\n\u00a0\u00a0\u00a0\u00a0self.tags_to_index = tags_to_index\n\u00a0\u00a0\u00a0\u00a0self.n_grams = n_grams\n\n\u00a0\u00a0def __getitem__(self, index):\n\u00a0\u00a0\u00a0\u00a0ngram_vector = np.array([])\n\n\u00a0\u00a0\u00a0\u00a0for ngram_index in range(index, index + self.n_grams):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0word, _ = self.dataset[ngram_index]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0vector_bytes = self.db.Get(bytes(word, 'utf-8'))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0vector = np.frombuffer(vector_bytes, dtype='float32')\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ngram_vector = np.append(ngram_vector, vector)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0_, tag = self.dataset[index + int(np.floor(self.n_grams/2))]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0label = self.tags_to_index[tag]\n\u00a0\u00a0\u00a0\u00a0return torch.tensor(ngram_vector, dtype=torch.float32), label\n\n\u00a0\u00a0def __len__(self):\n\u00a0\u00a0\u00a0\u00a0return (len(self.dataset) - self.n_grams + 1)\n\ntrainset = NgramPOSDataset(db, train_pos_dataset, tags_to_index, 3)\ntrainloader = DataLoader(trainset, batch_size=4, shuffle=True)\n\n```", "```py\nimport torch.nn as nn\n\ncell_1 = nn.RNNCell(input_size = 10,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hidden_size = 20,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nonlinearity='tanh')\n\ncell_2 = nn.LSTMCell(input_size = 10,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hidden_size = 20)\n\ncell_3 = nn.GRUCell(input_size = 10,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hidden_size = 20)\n\n```", "```py\nmulti_layer_rnn = nn.RNN(input_size = 10,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hidden_size = 20,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0num_layers = 2,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nonlinearity = 'tanh')\n\nmulti_layer_lstm = nn.LSTM(input_size = 10,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hidden_size = 20,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0num_layers = 2)\n\n```", "```py\nmulti_layer_rnn = nn.RNN(input_size = 10,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hidden_size = 20,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0num_layers = 2,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nonlinearity = 'tanh',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_first = False,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropout = 0.5)\n\nmulti_layer_lstm = nn.LSTM(input_size = 10,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hidden_size = 20,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0num_layers = 2,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_first = False,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropout = 0.5)\n\n```", "```py\ninput = torch.randn(5, 3, 10) # (time_steps, batch, input_size)\nh_0 = torch.randn(2, 3, 20) # (n_layers, batch_size, hidden_size)\nc_0 = torch.randn(2, 3, 20) # (n_layers, batch_size, hidden_size)\n\nrnn = nn.LSTM(10, 20, 2) # (input_size, hidden_size, num_layers)\noutput_n, (hn, cn) = rnn(input, (h_0, c_0))\n\n```", "```py\n$ pip install torchtext\n\n```", "```py\nfrom torchtext.datasets import IMDB\nfrom torchtext.data.utils import get_tokenizer\n\n# Load dataset\ntrain_iter = IMDB(split=('train'))\n\n# Define tokenizer and build vocabulary\ntokenizer = get_tokenizer('basic_english')\n```", "```py\nfrom torchtext.vocab import build_vocab_from_iterator\n\ndef yield_tokens(data_iter):\n\u00a0\u00a0\u00a0\u00a0for _, text in data_iter:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0yield tokenizer(text)\n\n# build vocab from iterator and add a list of any special tokens\ntext_vocab = build_vocab_from_iterator(yield_tokens(train_iter),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0specials=['<unk>', '<pad>'])\ntext_vocab.set_default_index(text_vocab['<unk>'])\n\n```", "```py\ndef text_pipeline(x, max_size=512):\n\u00a0\u00a0\u00a0text = tokenizer(x)\n\n\u00a0\u00a0\u00a0# reduce vocab size\n\u00a0\u00a0\u00a0pruned_text = []\n\u00a0\u00a0\u00a0for token in text:\n\u00a0\u00a0\u00a0\u00a0\u00a0if text_vocab.get_stoi()[token] >= 30000:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0token = '<unk>'\n\u00a0\u00a0\u00a0\u00a0\u00a0pruned_text.append(token)\n\n\u00a0\u00a0\u00a0# pad sequence or truncate\n\u00a0\u00a0\u00a0if len(pruned_text) <= max_size:\n\u00a0\u00a0\u00a0\u00a0\u00a0pruned_text += ['<pad>'] * (max_size - len(pruned_text))\n\u00a0\u00a0\u00a0else:\n\u00a0\u00a0\u00a0\u00a0\u00a0pruned_text = pruned_text[0:max_size]\n\u00a0\u00a0\u00a0return text_vocab(pruned_text)\n\nlabel_pipeline = lambda x: (0 if (x == 'neg') else 1)\n```", "```py\ndef collate_batch(batch):\n\u00a0\u00a0label_list, text_list = [], []\n\u00a0\u00a0for label, review in batch:\n\u00a0\u00a0\u00a0\u00a0label_list.append(label_pipeline(label))\n\u00a0\u00a0\u00a0\u00a0text_list.append(text_pipeline(review))\n\u00a0\u00a0return (torch.tensor(label_list, dtype=torch.long),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0torch.tensor(text_list, dtype=torch.int32))\n\n```", "```py\nfrom torch.utils.data import DataLoader\n\ntrain_iter, val_iter = IMDB(split=('train','test'))\ntrainloader = DataLoader(train_iter,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_size = 4,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0shuffle=False,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0collate_fn=collate_batch)\nvalloader = DataLoader(val_iter,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_size = 4,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0shuffle=False,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0collate_fn=collate_batch)\n\n```", "```py\nimport torch.nn as nn\n\nembedding = nn.Embedding(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0num_embeddings=30000,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0embedding_dim=512,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0padding_idx=text_vocab.get_stoi()['<pad>'])\n\n```", "```py\nclass TextClassifier(nn.Module):\n\u00a0\u00a0def __init__(self):\n\u00a0\u00a0\u00a0\u00a0super(TextClassifier,self).__init__()\n\u00a0\u00a0\u00a0\u00a0self.layer_1 = nn.Embedding(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0num_embeddings=30000,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0embedding_dim=512,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0padding_idx=1)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0self.layer_2 = nn.LSTMCell(input_size=512, hidden_size=512)\n\u00a0\u00a0\u00a0\u00a0self.layer_3 = nn.Dropout(p=0.5)\n\u00a0\u00a0\u00a0\u00a0self.layer_4 = nn.Sequential(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(512, 2),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Sigmoid(),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.BatchNorm1d(2))\n\n\u00a0\u00a0def forward(self, x):\n\u00a0\u00a0\u00a0\u00a0x = self.layer_1(x)\n\u00a0\u00a0\u00a0\u00a0x = x.permute(1,0,2)\n\u00a0\u00a0\u00a0\u00a0h = torch.rand(x.shape[1], 512)\n\u00a0\u00a0\u00a0\u00a0c = torch.rand(x.shape[1], 512)\n\u00a0\u00a0\u00a0\u00a0for t in range(x.shape[0]):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0h, c = self.layer_2(x[t], (h,c))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0h = self.layer_3(h)\n\u00a0\u00a0\u00a0\u00a0return self.layer_4(h)\n\n```", "```py\nmodel = TextClassifier()\n\n```", "```py\nshe grabbed my hand .\n\"come on . \"\nshe fluttered her back in the air .\n\"i think we're at your place . I ca n't come get you . \"\nhe locked himself back up\n\" no . she will . \"\nkyrian shook his head\n```", "```py\nfrom torchtext.datasets import IWSLT2016\n\ntrain_iter = IWSLT2016(split=('train'),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0language_pair=('en','fr'))\n\n```", "```py\npip install -U spacy\npython -m spacy download en_core_web_sm\npython -m spacy download fr_core_news_sm\n\n```", "```py\nfrom torchtext.data.utils import get_tokenizer\n\ntokenizer_en = get_tokenizer('spacy',language='en_core_web_sm')\ntokenizer_fr = get_tokenizer('spacy',language='fr_core_news_sm')\n\n```", "```py\ndef yield_tokens(data_iter, language):\n\u00a0\u00a0\u00a0\u00a0if language == 'en':\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for data_sample in data_iter:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0yield tokenizer_en(data_sample[0])\n\u00a0\u00a0\u00a0\u00a0else:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for data_sample in data_iter:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0yield tokenizer_fr(data_sample[1])\n\nUNK_IDX, PAD_IDX, GO_IDX, EOS_IDX = 0, 1, 2, 3\nspecial_symbols = ['<unk>', '<pad>', '<go>', '<eos>']\n\n# Create Vocabs\ntrain_iter = IWSLT2016(root='.data', split=('train'),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0language_pair=('en', 'fr'))\n\nvocab_en = build_vocab_from_iterator(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0yield_tokens(train_iter, 'en'),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0min_freq=1,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0specials=special_symbols,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0special_first=True)\n\ntrain_iter = IWSLT2016(root='.data', split=('train'),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0language_pair=('en', 'fr'))\nvocab_fr = build_vocab_from_iterator(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0yield_tokens(train_iter, 'fr'),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0min_freq=1,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0specials=special_symbols,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0special_first=True)\n\n```", "```py\ndef process_tokens(source, target, bucket_sizes):\n\u00a0\u00a0# find bucket_index\n\u00a0\u00a0for i in range(len(bucket_sizes)+2):\n\u00a0\u00a0\u00a0\u00a0# truncate if we exhauset list of buckets\n\u00a0\u00a0\u00a0\u00a0if i >= len(bucket_sizes):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0bucket = bucket_sizes[i-1]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0bucket_id = i-1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if len(source) > bucket[0]:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0source = source[:bucket[0]]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if len(target) > (bucket[1]-2):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0target = target[:bucket[1]-2]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0break\n\n\u00a0\u00a0\u00a0\u00a0bucket = bucket_sizes[i]\n\u00a0\u00a0\u00a0\u00a0if (len(source) < bucket[0]) and ((len(target)+1) < bucket[1]):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0bucket_id = i\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0break\n\n\u00a0\u00a0source += ((bucket_sizes[bucket_id][0] - len(source)) * ['<pad>'])\n\u00a0\u00a0source = list(reversed(source))\n\n\u00a0\u00a0target.insert(0,'<go>')\n\u00a0\u00a0target.append('<eos>')\n\u00a0\u00a0target += (bucket_sizes[bucket_id][1] - len(target)) * ['<pad>']\n\n\u00a0\u00a0return vocab_en(source), vocab_fr(target), bucket_id\n\n```", "```py\nfrom torch.utils.data import Dataset\n\nclass BucketedDataset(Dataset):\n\u00a0\u00a0def __init__(self, bucketed_dataset, bucket_size):\n\u00a0\u00a0\u00a0\u00a0super(BucketedDataset, self).__init__()\n\u00a0\u00a0\u00a0\u00a0self.length = len(bucketed_dataset)\n\u00a0\u00a0\u00a0\u00a0self.input_len = bucket_size[0]\n\u00a0\u00a0\u00a0\u00a0self.target_len = bucket_size[1]\n\u00a0\u00a0\u00a0\u00a0self.bucketed_dataset = bucketed_dataset\n\n\u00a0\u00a0def __getitem__(self, index):\n\u00a0\u00a0\u00a0\u00a0return (torch.tensor(self.bucketed_dataset[index][0],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dtype=torch.float32),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0torch.tensor(self.bucketed_dataset[index][1],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dtype=torch.float32))\n\n\u00a0\u00a0def __len__(self):\n\u00a0\u00a0\u00a0\u00a0return self.length\n\nbucketed_datasets = []\nfor i, dataset in enumerate(datasets):\n\u00a0\u00a0bucketed_datasets.append(BucketedDataset(dataset,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0bucket_sizes[i]))\n\n```", "```py\nfrom torch.utils.data import DataLoader\n\ndataloaders = []\nfor dataset in bucketed_datasets:\n\u00a0\u00a0dataloaders.append(DataLoader(dataset,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_size=32,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0shuffle=True))\n\n```", "```py\nfor epoch in range(n_epochs):\n\u00a0\u00a0# exhaust all dataloaders randomly\n\u00a0\u00a0# keep track of when we used up all values\n\u00a0\u00a0dataloader_sizes = []\n\u00a0\u00a0for dataloader in dataloaders:\n\u00a0\u00a0\u00a0\u00a0dataloader_sizes.append(len(dataloader))\n\n\u00a0\u00a0while np.array(dataloader_sizes).sum() != 0:\n\u00a0\u00a0\u00a0\u00a0bucket_id = torch.randint(0,len(bucket_sizes),(1,1)).item()\n\u00a0\u00a0\u00a0\u00a0if dataloader_sizes[bucket_id] == 0:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0continue\n\u00a0\u00a0\u00a0\u00a0source, target = next(iter(dataloaders[bucket_id]))\n\u00a0\u00a0\u00a0\u00a0dataloader_sizes[bucket_id] -= 1\n    loss = train(encoder_inputs,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0decoder_inputs,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0target_weights,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0bucket_id)\n\n```", "```py\nloss += step_loss / steps_per_checkpoint current_step += 1\n\n```"]