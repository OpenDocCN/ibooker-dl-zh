["```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import (\n    StandardScaler,\n    OneHotEncoder,\n    OrdinalEncoder\n)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ndata = pd.read_csv(\"./AB_NYC_2019.csv\")\nexcluding_list = ['price', 'id', 'latitude', 'longitude', 'host_id', \n                  'last_review', 'name', 'host_name']            ①\nlow_card_categorical = [\n    'neighbourhood_group',\n    'room_type'\n]                                                                ②\nhigh_card_categorical = ['neighbourhood']                        ③\ncontinuous = [\n    'minimum_nights',\n    'number_of_reviews',\n    'reviews_per_month', \n    'calculated_host_listings_count',\n    'availability_365'\n]                                                                ④\ntarget_mean = (\n    (data[\"price\"] > data[\"price\"].mean())\n    .astype(int)\n)                                                                ⑤\ntarget_median = (\n    (data[\"price\"] > data[\"price\"].median())\n    .astype(int)\n)                                                                ⑥\ntarget_multiclass = pd.qcut(\n    data[\"price\"], q=5, labels=False\n)                                                                ⑦\ntarget_regression = data[\"price\"]                                ⑧\ncategorical_onehot_encoding = OneHotEncoder(handle_unknown='ignore')\ncategorical_ord_encoding = \nOrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan)\nnumeric_standardization = Pipeline([('StandardScaler', StandardScaler()), \n                                    ('Imputer', \nSimpleImputer(strategy=\"constant\", fill_value=0))])\n\ncolumn_transform = ColumnTransformer(\n    [\n        ('low_card_categories', \n         categorical_onehot_encoding, \n         low_card_categorical),\n        ('high_card_categories', \n         categorical_ord_encoding, \n         high_card_categorical),\n        ('numeric', \n         numeric_standardization, \n         continuous)\n    ],\n    remainder='drop',\n    verbose_feature_names_out=True,\n    sparse_threshold=0.0)                                        ⑨\n\nlm_column_transform = ColumnTransformer(\n    [\n        ('low_card_categories', \n         categorical_onehot_encoding, \n         low_card_categorical),\n        ('numeric', \n         numeric_standardization, \n         continuous)\n    ],\n    remainder='drop',\n    verbose_feature_names_out=True,\n    sparse_threshold=0.0)                                        ⑩\n```", "```py\nfrom sklearn.experimental import (\n    enable_iterative_imputer\n)                                                             ①\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\n\nXm = data[continuous].copy()                                  ②\nmissing_percentage = 0.05\nnp.random.seed(0)\nmask = np.random.rand(*Xm.shape) < missing_percentage         ③\nXm[mask] = np.nan\n\nsimple_imputer = SimpleImputer()\nXm_si = simple_imputer.fit_transform(Xm)                      ④\n\nrf = RandomForestRegressor(random_state=0, n_jobs=-1)         ⑤\nmultivariate_imputer = IterativeImputer(\n    estimator=rf,\n    max_iter=1,\n    tol=0.01\n)                                                             ⑥\nXm_mi = multivariate_imputer.fit_transform(Xm)                ⑦\n\nmae = pd.DataFrame(\n    {\n        \"simple\": np.mean(\n            np.abs(data[continuous] - Xm_si), axis=0\n        ),\n        \"multivariate\": np.mean(\n            np.abs(data[continuous] - Xm_mi), axis=0\n        )\n     },\n     index = continuous\n)                                                             ⑧\nprint(mae)\n```", "```py\n                                       Simple        Multivariate\nminimum_nights                         0.347355      0.260156\nnumber_of_reviews                      1.327776      0.858506\nreviews_per_month                      0.057980      0.036876\ncalculated_host_listings_count         0.579423      0.368567\navailability_365                       6.025748      4.62264\n```", "```py\ndef bin_2_cat(feature, bins=100):\n    min_value = feature.min()\n    bin_size = (feature.max() - min_value) / bins\n    bin_values = (feature - min_value) / bin_size\n    return bin_values.astype(int)                         ①\n\ndata['coordinates'] = (\n    bin_2_cat(data['latitude']) * 1000 \n    + bin_2_cat(data['longitude']\n)                                                         ②\nhigh_card_categorical += ['coordinates']\n\nprint(data[high_card_categorical].nunique())              ③\n```", "```py\nneighbourhood     221\ncoordinates      2259\n```", "```py\nfrom category_encoders.target_encoder import TargetEncoder\nfrom XGBoost import XGBClassifier\nfrom sklearn.model_selection import KFold, cross_validate\nfrom sklearn.metrics import accuracy_score, make_scorer\n\ntarget_encoder = TargetEncoder(cols=high_card_categorical,  ①\n                               smoothing=0.5)               ②\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\nxgb = XGBClassifier(booster='gbtree',\n                    objective='reg:logistic',\n                    n_estimators=300,\n                    max_depth=4,\n                    min_child_weight=3)                     ③\n\ncolumn_transform = ColumnTransformer(\n    [\n        ('low_card_categories', \n         categorical_onehot_encoding, \n         low_card_categorical),\n       ('high_card_categories', \n        target_encoder, \n        high_card_categorical),\n       ('numeric', \n        numeric_standardization, \n        continuous)\n    ],\n    remainder='drop',\n    verbose_feature_names_out=True,\n    sparse_threshold=0.0)                                   ④\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('model', xgb)])                                       ⑤\n\ncv_scores = cross_validate(estimator=model_pipeline,\n                          X=data,\n                          y=target_median,\n                          scoring=accuracy,\n                          cv=cv,\n                          return_train_score=True,\n                          return_estimator=True)        ⑥\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\",\n      f\"fit: {fit_time:0.2f}\",\n      f\"secs pred: {score_time:0.2f} secs\")                 ⑦\n```", "```py\n0.840 (0.004) fit: 4.52 secs pred: 0.06 secs\n```", "```py\nfrom XGBoost import XGBClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import PartialDependenceDisplay\n\nxgb = XGBClassifier(booster='gbtree', \n                    objective='reg:logistic', \n                    n_estimators=300, \n                    max_depth=4,\n                    min_child_weight=3)\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('XGBoost', xgb)])                                 ①\n\nmodel_pipeline.fit(X=data, y=target_median)\n\nfig, axes = plt.subplots(\n   nrows=2,\n   ncols=2,\n   figsize=(8, 4)\n)                                                       ②\nfig.subplots_adjust(hspace=0.4, wspace=0.2) \n\nPartialDependenceDisplay.from_estimator(\n    model_pipeline, \n    X=data, \n    kind='average',                                     ③\n    features=[\n        'minimum_nights',\n        'number_of_reviews', \n        'calculated_host_listings_count',\n        'availability_365'\n    ],                                                  ④\n    ax=axes\n)\nfor ax in axes.flatten():\n    ax.axhline(y=0.5, color='red', linestyle='--')      ⑤\n\nplt.show()\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import PartialDependenceDisplay\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 4))\nfig.subplots_adjust(hspace=0.4, wspace=0.2)\n\nPartialDependenceDisplay.from_estimator(model_pipeline, \n                                        X=data, \n                                        kind='both',               ①\n                                        subsample=30,              ②\n                                        features=['minimum_nights', \n                                                  'number_of_reviews', \n\n                                                  'calculated_host_listings_count', \n                                                  'availability_365'],\n                                       ax=axes)\n\nfor ax in axes.flatten():\n    ax.axhline(y=0.5, color='red', linestyle='--')\n    ax.legend().set_visible(False)\n\nplt.show()\n```", "```py\nfrom sklearn.inspection import partial_dependence         ①\nimport matplotlib.pyplot as plt\n\npd_ice = partial_dependence(model_pipeline, X=data, \n                            features=['neighbourhood_group'], \n                            kind='both')\n\nfig = plt.figure(figsize=(8, 5))\nax = fig.add_subplot(1, 1, 1)\nlabels = np.ravel(pd_ice['values'])\nplt.boxplot(\n    pd_ice[\"individual\"].squeeze(),\n    labels=labels\n)                                                         ②\nax.axhline(y=0.5, color='red', linestyle='--')            ③\nplt.show()\n```", "```py\nimport xgbfir\nxgbfir.saveXgbFI(\n    model_pipeline['XGBoost'],\n    feature_names=(\n        model_pipeline['processing']\n        .get_feature_names_out()\n    ),\n    OutputXlsxFile='fir.xlsx')                                    ①\nfir = pd.read_excel('fir.xlsx', sheet_name='Interaction Depth 1') ②\nresult = fir[[\"Interaction\", \"Gain\"]].sort_values(by=\"Gain\", \n\nascending=False).head(10).round(2)                                ③\nfor index, row in result.iterrows():\n    print(f\"{row['Interaction']}\")\n\nPartialDependenceDisplay.from_estimator(\n    model_pipeline,\n    X=data,\n    kind='average',\n    features=[(\n        'minimum_nights',\n        'calculated_host_listings_count')])                       ④\n```", "```py\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import BaggingClassifier\n\nlambda_grid=np.logspace(-4, -1, 10)                       ①\nsparse_coef = list()\n\nfor modeling_c in lambda_grid:\n    estimator = LogisticRegression(\n        solver='liblinear',\n        penalty='l1',\n        C=modeling_c\n    )                                                     ②\n    model = BaggingClassifier(\n        estimator, \n        n_estimators=100,\n        bootstrap=True\n    )                                                     ③\n    model_pipeline = Pipeline(\n        [('processing', lm_column_transform),\n         ('standardize', StandardScaler()),               ④\n         ('modeling', model)])\n    model_pipeline.fit(data, target_median)\n    sparse_coef += [estimator.coef_.ravel() for estimator in \nmodel_pipeline[\"modeling\"].estimators_]\n\nepsilon = 1e-2                                            ⑤\nthreshold = 0.5                                           ⑥\n\nnon_zero = (np.abs(sparse_coef) > epsilon).mean(axis=0)\nfeature_names = model_pipeline[\"processing\"].get_feature_names_out()\nprint(non_zero)\nprint(feature_names[non_zero > threshold])\n```", "```py\n[0.635 0\\.    0.9   0.7   0.592 1\\.    0\\.    0.6   0.593 0.444 0.6   0.506 0.7  ]\n['low_card_categories__neighbourhood_group_Bronx'\n 'low_card_categories__neighbourhood_group_Manhattan'\n 'low_card_categories__neighbourhood_group_Queens'\n 'low_card_categories__neighbourhood_group_Staten Island'\n 'low_card_categories__room_type_Entire home/apt'\n 'low_card_categories__room_type_Shared room' 'numeric__minimum_nights'\n 'numeric__reviews_per_month' 'numeric__calculated_host_listings_count'\n 'Numeric__availability_365']\n```", "```py\nfrom XGBoost import XGBClassifier\nfrom boruta import BorutaPy\n\nxgb = XGBClassifier(booster='gbtree', \n                    objective='reg:logistic', \n                    n_estimators=300, \n                    max_depth=4,\n                    min_child_weight=3)\n\nX = column_transform.fit_transform(data, target_median)                   ①\nboruta_selector = BorutaPy(estimator=xgb, n_estimators='auto', verbose=2) ②\nboruta_selector.fit(X, target_median)                                     ③\nselected_features = boruta_selector.support_                              ④\nselected_data = column_transform.get_feature_names_out()[selected_features]\nprint(selected_data)\n```", "```py\nIteration:    50 / 100\nConfirmed:    13\nTentative:    0\nRejected:    1\n['low_card_categories__neighbourhood_group_Bronx'\n 'low_card_categories__neighbourhood_group_Brooklyn'\n 'low_card_categories__neighbourhood_group_Manhattan'\n 'low_card_categories__neighbourhood_group_Queens'\n 'low_card_categories__room_type_Entire home/apt'\n 'low_card_categories__room_type_Private room'\n 'low_card_categories__room_type_Shared room'\n 'high_card_categories__neighbourhood' 'numeric__minimum_nights'\n 'numeric__number_of_reviews' 'numeric__reviews_per_month'\n 'numeric__calculated_host_listings_count' 'numeric__availability_365']\n```", "```py\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(boosting_type='gbdt', \n                      n_estimators=300, \n                      max_depth=4,\n                      min_child_samples=3)\n\nboruta_selector = BorutaPy(estimator=lgbm, n_estimators='auto', verbose=2) ①\nboruta_selector.fit(X, target_median)\nselected_features = boruta_selector.support_\nselected_data = column_transform.get_feature_names_out()[selected_features]\nprint(selected_data)\n```", "```py\nIteration:     9 / 100\nConfirmed:     8\nTentative:     0\nRejected:      6\n['low_card_categories__neighbourhood_group_Manhattan'\n 'low_card_categories__room_type_Entire home/apt'\n 'high_card_categories__neighbourhood' 'numeric__minimum_nights'\n 'numeric__number_of_reviews' 'numeric__reviews_per_month'\n 'numeric__calculated_host_listings_count' 'numeric__availability_365']\n```", "```py\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom XGBoost import XGBClassifier\n\nxgb = XGBClassifier(booster='gbtree', \n                    objective='reg:logistic', \n                    n_estimators=300, \n                    max_depth=4,\n                    min_child_weight=3)\n\ncv = KFold(5, shuffle=True, random_state=0)                  ①\naccuracy = make_scorer(accuracy_score)                       ②\nX = column_transform.fit_transform(data, target_median)\nselector = SequentialFeatureSelector(\n     estimator=xgb,\n     n_features_to_select=\"auto\",\n     tol=0.0001,                                             ③\n     direction=\"forward\",                                    ④\n     scoring=accuracy,\n     cv=cv\n)\nselector.fit(X, target_median)\nselected_features = selector.support_                        ⑤\nselected_data = column_transform.get_feature_names_out()[selected_features]\nprint(selected_data)\n```", "```py\n['low_card_categories__neighbourhood_group_Bronx'\n 'low_card_categories__room_type_Entire home/apt'\n 'low_card_categories__room_type_Shared room'\n 'high_card_categories__neighbourhood' 'numeric__minimum_nights'\n 'numeric__number_of_reviews' 'numeric__reviews_per_month'\n 'numeric__calculated_host_listings_count' 'numeric__availability_365']\n```", "```py\nselector = SequentialFeatureSelector(\n     estimator=xgb,\n     n_features_to_select=\"auto\",\n     tol=0.0001,\n     direction=\"backward\",                                  ①\n     scoring=accuracy,\n     cv=cv\n)\nselector.fit(X, target_median)\nselected_features = selector.support_\nselected_data = column_transform.get_feature_names_out()[selected_features]\nprint(selected_data)\n```", "```py\n['low_card_categories__neighbourhood_group_Bronx'\n 'low_card_categories__neighbourhood_group_Manhattan'\n 'low_card_categories__neighbourhood_group_Queens'\n 'low_card_categories__neighbourhood_group_Staten Island'\n 'low_card_categories__room_type_Entire home/apt'\n 'low_card_categories__room_type_Shared room'\n 'high_card_categories__neighbourhood' 'numeric__minimum_nights'\n 'numeric__number_of_reviews' 'numeric__reviews_per_month'\n 'numeric__calculated_host_listings_count' 'numeric__availability_365']\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.metrics import make_scorer\n\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\nmodel = LogisticRegression(solver=\"saga\", max_iter=5_000)\n\nmodel_pipeline = Pipeline(\n    [('processing', lm_column_transform),\n     ('model', model)])\n\nsearch_grid = [\n    {\"model__penalty\": [None]},\n    {\"model__penalty\": [\"l1\", \"l2\"], \"model__C\": np.logspace(-4, 4, 10)},\n    {\"model__penalty\": [\"elasticnet\"], \"model__C\": np.logspace(-4, 4, 10), \n     \"model__l1_ratio\": [.1, .3, .5, .7, .9, .95, .99]},\n]                                                             ①\n\nsearch_func = GridSearchCV(estimator=model_pipeline,          ②\n                           param_grid=search_grid, \n                           scoring=accuracy, \n                           n_jobs=-1, \n                           cv=cv)\n\nsearch_func.fit(X=data, y=target_median)\nprint (search_func.best_params_)                              ③\nprint (search_func.best_score_)                               ④\n```", "```py\n{'model__penalty': None}\n0.8210860006135597\n```", "```py\nfrom sklearn.utils.fixes import loguniform\nfrom sklearn.model_selection import KFold, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import make_scorer\nfrom XGBoost import XGBClassifier\n\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\n\nxgb = XGBClassifier(booster='gbtree', objective='reg:logistic')\nmodel_pipeline = Pipeline(\n    [('processing', column_transform), ('xgb', xgb)]\n)                                                               ①\n\nsearch_dict = {                                                 ②\n    'xgb__n_estimators': np.arange(100, 2000, 100), \n    'xgb__learning_rate': loguniform(0.01, 1),\n    'xgb__max_depth': np.arange(1, 8),\n    'xgb__subsample': np.arange(0.1, 0.9, 0.05),\n    'xgb__colsample_bytree': np.arange(0.1, 0.9, 0.05),\n    'xgb__reg_lambda': loguniform(1e-9, 100),\n    'xgb__reg_alpha': loguniform(1e-9, 100)\n}\n\nsearch_func = RandomizedSearchCV(estimator=model_pipeline, \n                                 param_distributions=search_dict, \n                                 n_iter=60,                     ③\n                                 scoring=accuracy, \n                                 n_jobs=1,                      ④\n                                 cv=cv, \n                                 random_state=0)\n\nsearch_func.fit(X=data, y=target_median)\nprint (search_func.best_params_)                                ⑤\nprint (search_func.best_score_)                                 ⑥\n```", "```py\n{'xgb__colsample_bytree': 0.3500000000000001,\n 'xgb__learning_rate': 0.020045491299569684,\n 'xgb__max_depth': 6,\n 'xgb__n_estimators': 1800,\n 'xgb__reg_alpha': 3.437821898520205e-08,\n 'xgb__reg_lambda': 0.021708909914764426,\n 'xgb__subsample': 0.1}\n0.8399836384088353\n```", "```py\nfrom sklearn.experimental import (\n    enable_halving_search_cv\n)                                                           ①\nfrom sklearn.model_selection import HalvingRandomSearchCV\n\nsearch_func = HalvingRandomSearchCV(\n    estimator=model_pipeline,\n    param_distributions=search_dict,\n    resource='n_samples',                                   ②\n    n_candidates=20,                                        ③\n    factor=3,                                               ④\n    min_resources=int(len(data) * 0.3),                     ⑤\n    max_resources=len(data),                                ⑥\n    scoring=accuracy,\n    n_jobs=1,\n    cv=cv,\n    random_state=0\n)\nsearch_func.fit(X=data, y=target_median)\nprint (search_func.best_params_)\nprint (search_func.best_score_)\n```", "```py\n{'xgb__colsample_bytree': 0.6500000000000001,\n 'xgb__learning_rate': 0.02714215181104359,\n 'xgb__max_depth': 7,\n 'xgb__n_estimators': 400,\n 'xgb__reg_alpha': 3.281921389446602,\n 'xgb__reg_lambda': 0.00039687940902191534,\n 'xgb__subsample': 0.8000000000000002}\n0.8398409090909091\n```", "```py\nimport optuna\nfrom XGBoost import XGBClassifier\nfrom sklearn.model_selection import cross_validate\n\ndef objective(trial):\n\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n        'learning_rate': trial.suggest_float(\n            'learning_rate', 0.01, 1.0, log=True\n        ),\n        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n        'colsample_bytree': trial.suggest_float(\n            'colsample_bytree', 0.1, 1.0\n        ),\n        'max_depth': trial.suggest_int('max_depth', 1, 7),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n        'reg_lambda': trial.suggest_float(\n            'reg_lambda', 1e-9, 100.0, log=True\n         ),\n        'reg_alpha': trial.suggest_float(\n            'reg_alpha', 1e-9, 100.0, log=True\n         ),\n    }                                                          ①\n\n    xgb = XGBClassifier(\n        booster='gbtree', \n        objective='reg:logistic',\n        **params\n    )                                                          ②\n    model_pipeline = Pipeline(\n        [('processing', column_transform), ('xgb', xgb)]\n    )\n    accuracy = make_scorer(accuracy_score)\n    cv = KFold(5, shuffle=True, random_state=0)\n\n    cv_scores = cross_validate(estimator=model_pipeline, \n                               X=data, \n                               y=target_median,\n                               scoring=accuracy,\n                               cv=cv)                          ③\n\n    cv_accuracy = np.mean(cv_scores['test_score'])\n    return cv_accuracy                                         ④\n\nstudy = optuna.create_study(direction=\"maximize\")              ⑤\nstudy.optimize(objective, n_trials=60)                         ⑥\nprint(study.best_value)                                        ⑦\nprint(study.best_params)                                       ⑧\n```", "```py\n{'n_estimators': 1434, \n 'learning_rate': 0.013268588739778429,\n 'subsample': 0.782534239551612,\n 'colsample_bytree': 0.9427647573058971,\n 'max_depth': 7,\n 'min_child_weight': 2,\n 'reg_lambda': 2.3123673571345327e-06,\n 'reg_alpha': 1.8176941971395193e-05}\n0.8419879333265161\n```", "```py\nsqlite_db = \"sqlite:///sqlite.db\"                                      ①\nstudy_name = \"optimize_XGBoost\"                                        ②\nstudy = optuna.create_study(storage=sqlite_db, study_name=study_name, \n                            direction=\"maximize\", load_if_exists=True) ③\nstudy.optimize(objective, n_trials=60)\n\nprint(study.best_params)\nprint(study.best_value)\n```", "```py\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show()\n```", "```py\nfig = optuna.visualization.plot_param_importances(study)\nfig.show()\n```", "```py\nimport json\nimport matplotlib.pyplot as plt\nfrom XGBoost import XGBClassifier, plot_tree\nfrom collections import namedtuple\n\nxgb = XGBClassifier(booster='gbtree',\n                    objective='reg:logistic',\n                    n_estimators=10,\n                    max_depth=3)                           ①\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('XGBoost', xgb)])                                    ②\n\nmodel_pipeline.fit(X=data, y=target_median)\nmodel = model_pipeline[\"XGBoost\"]\ntree_info = model.get_booster().dump_model(\n    \"xgb_model.json\",\n    with_stats=True,\n    dump_format=\"json\"\n)                                                          ③\n\nfig, ax = plt.subplots(figsize=(12, 15), dpi=300)\nax = plot_tree(\n    model, num_trees=0, ax=ax, rankdir='LR\n)                                                          ④\nplt.show()\n\nwith open(\"xgb_model.json\", \"r\") as f:\n    json_model = json.loads(f.read())                      ⑤\n\nprint(f\"Number of trees: {len(json_model)}\")\ntree_structure = json_model[0]                             ⑥\n\nSplit = namedtuple(\"SplitNode\", \"feature origin gain count threshold\")\nLeaf = namedtuple(\"LeafNode\", \"index origin count\")\n\ndef extract_xgb_node_info(tree):\n    return [tree['split'], tree['origin'], tree['gain'],\n            tree['cover'], tree['split_condition']]        ⑦\n\ndef extract_xgb_leaf_info(tree):\n    return (\n        [tree['nodeid'], \n         tree['origin'], \n         tree['cover']\n        ]\n)                                                          ⑧\n\ndef traverse_xgb_tree(tree):                               ⑨\n    if not 'origin' in tree:\n        tree['origin'] = \"=\"\n    if not 'children' in tree:\n        return [[Leaf(*extract_xgb_leaf_info(tree))]]\n    left_branch = tree['children'][0]\n    right_branch = tree['children'][1]\n    left_branch['origin'] = '<'\n    right_branch['origin'] = '>='\n    left_paths = traverse_xgb_tree(left_branch)\n    right_paths = traverse_xgb_tree(right_branch)\n    node_info = [Split(*extract_xgb_node_info(tree))]\n    return [node_info + path for path in left_paths + right_paths]\n\npaths = traverse_xgb_tree(tree_structure)\n\nprint(f\"Number of paths on tree: {len(paths)}\")\nprint(\"Path 0:\", paths[0])\n```", "```py\nNumber of trees: 10\nNumber of paths on tree: 8\nPath 0: [SplitNode(\n             feature='f5', \n             origin='=', \n             gain=19998.9316, \n             count=12223.75,\n             threshold=0.5),\n             SplitNode(\n                 feature='f2',\n                 origin='<',\n                 gain=965.524414,\n                 count=5871.5,\n                 threshold=0.5\n             ), \n            SplitNode(\n                 feature='f13',\n                 origin='<',\n                 gain=66.1962891,\n                 count=3756,\n                 threshold=1.88965869\n            ), \n            LeafNode(\n                 index=7,\n                 origin='<',\n                 count=3528)\n]\n```", "```py\nfrom lightgbm import LGBMClassifier, plot_tree\n\nlgbm = LGBMClassifier(boosting_type='gbdt', \n                      n_estimators=10, \n                      max_depth=3)\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('lightgbm', lgbm)])\n\nmodel_pipeline.fit(X=data, y=target_median)\nmodel = model_pipeline[\"lightgbm\"]\n\ntree_info = model._Booster.dump_model()[\"tree_info\"]           ①\ntree_structure = tree_info[0]['tree_structure']                ②\nplot_tree(\n    booster=model._Booster,\n    tree_index=0,\n    dpi=600\n)                                                              ③\n\nSplit = namedtuple(\n    \"SplitNode\",\n    \"feature origin decision_type threshold gain count\"\n)\nLeaf = namedtuple(\"LeafNode\", \"index origin count value\")\n\ndef extract_lgbm_node_info(tree):                              ④\n    return [tree['split_feature'], tree['origin'], tree['decision_type'],\n            tree['threshold'], tree['split_gain'], tree['internal_count']]\n\ndef extract_lgbm_leaf_info(tree):                              ⑤\n    return [\n         tree['leaf_index'],\n         tree['origin'],\n         tree['leaf_count'], \n         tree['leaf_value']\n    ]\n\ndef traverse_lgbm_tree(tree):                                  ⑥\n    if not 'origin' in tree:\n        tree['origin'] = \"\"\n    if not 'left_child' in tree and not 'right_child' in tree:\n        return [[Leaf(*extract_lgbm_leaf_info(tree))]]\n    left_branch = tree['left_child']\n    right_branch = tree['right_child']\n    left_branch['origin'] = 'yes'\n    right_branch['origin'] = 'no'\n    left_paths = traverse_lgbm_tree(left_branch)\n    right_paths = traverse_lgbm_tree(right_branch)\n    node_info = [Split(*extract_lgbm_node_info(tree))]\n    return [node_info + path for path in left_paths + right_paths]\n\npaths = traverse_lgbm_tree(tree_structure)\nprint(paths[0])\n```", "```py\n[SplitNode(\n    feature=5, \n    origin='',\n    decision_type='<=',\n    threshold=1.0000000180025095e-35,\n    gain=20002.19921875,\n    count=48895),\n SplitNode(\n    feature=2,\n    origin='yes',\n    decision_type='<=',\n    threshold=1.0000000180025095e-35,\n    gain=967.0560302734375,\n    count=23486),\n SplitNode(\n    feature=13,\n    origin='yes',\n    decision_type='<=',\n    threshold=1.8896587976897459,\n    gain=67.53350067138672,\n    count=15024), \n LeafNode(\n    index=0,\n    origin='yes',\n    count=14112,\n    value=-0.16892421857257725)\n]\n```", "```py\nmodel.set_param({\"predictor\": \"gpu_predictor\"})\nshap_values = model.predict(X, pred_contribs=True)\nshap_interaction_values = model.predict(X, pred_interactions=True)\n```", "```py\npip install lightgbm --install-option=--gpu\n```", "```py\nimport treelite\nimport treelite_runtime\nimport tl2cgen\n\nxgb = XGBClassifier(booster='gbtree',\n                      objective='reg:logistic',\n                      n_estimators=10,\n                      max_depth=3)\n\nmodel_pipeline = Pipeline(\n     [('processing', column_transform),\n     ('XGBoost', xgb)])\n\nmodel_pipeline.fit(X=data, y=target_median)\nmodel = model_pipeline[\"XGBoost\"]\n\nmodel.save_model(\"./xgb_model.json\")                          ①\ntreelite_model = treelite.Model.load(\"./xgb_model.json\", \nmodel_format=\"XGBoost_json\")                                  ②\ntl2cgen.generate_c_code(treelite_model, dirpath=\"./\", \nparams={\"parallel_comp\": 4})\ntl2cgen.export_lib(treelite_model, toolchain=\"gcc\", \nlibpath=\"./xgb_model.so\",                                     ③\n                   params={\"parallel_comp\": 4})\n\npredictor = tl2cgen.Predictor(\"./xgb_model.so\")\nX = model_pipeline[\"processing\"].transform(data)              ④\ndmat = tl2cgen.DMatrix(X)                                     ⑤\npredictor.predict(dmat) \n```", "```py\nimport lleaves\n\nlgbm = LGBMClassifier(boosting_type='gbdt',\n                      n_estimators=10,\n                      max_depth=3)\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('lightgbm', lgbm)])\n\nmodel_pipeline.fit(X=data, y=target_median)\nmodel = model_pipeline[\"lightgbm\"]\n\nmodel.booster_.save_model('lgb_model.txt')                     ①\n\nllvm_model = lleaves.Model(model_file=\"lgb_model.txt\")         ②\nllvm_model.compile()                                           ③\nX = model_pipeline[\"processing\"].transform(data)               ④\nllvm_model.predict(X)\n```"]