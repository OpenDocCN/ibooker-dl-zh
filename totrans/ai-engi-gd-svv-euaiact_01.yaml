- en: Chapter 1\. Understanding the AI Regulations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章：理解人工智能法规
- en: As people, organizations, and the public sector increasingly rely on AI to drive
    decision making, the technology must be trustworthy. The [EU AI Act](https://oreil.ly/-8wI7)
    aims to provide a legal framework for developing, deploying, and using AI technologies
    within the European Union, emphasizing safety, transparency, and ethical considerations.
    It is a regulatory framework for artificial intelligence that includes specific
    requirements for AI systems of different risk categories within the EU. This book
    is focused on understanding and implementing the regulatory requirements set by
    the European Union’s legislation on artificial intelligence. Please note that
    the guidance it provides is not intended as a substitute for obtaining professional
    legal advice.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人们、组织以及公共部门越来越依赖人工智能来驱动决策，这项技术必须值得信赖。《欧盟人工智能法案》（[EU AI Act](https://oreil.ly/-8wI7)）旨在为在欧洲联盟内开发、部署和使用人工智能技术提供一个法律框架，强调安全性、透明度和伦理考量。这是一个涵盖欧盟内不同风险类别人工智能系统的具体要求的人工智能监管框架。本书专注于理解和实施欧盟人工智能立法设定的监管要求。请注意，本书提供的指导不旨在替代获取专业法律建议。
- en: This chapter begins by outlining the motivation behind the EU AI Act, emphasizing
    the idea of “trustworthy AI,” and identifying the essential requirements for ensuring
    AI is trustworthy. It then describes the structure of the EU AI Act, including
    its definitions, key stakeholders, risk classifications (prohibited, high risk,
    limited risk, and minimal risk), and the implementation timeline. Finally, it
    briefly compares the EU AI Act with other international regulations and standards
    related to AI.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先概述了欧盟人工智能法案背后的动机，强调“可信赖的人工智能”这一理念，并确定了确保人工智能值得信赖的基本要求。然后，它描述了欧盟人工智能法案的结构，包括其定义、关键利益相关者、风险分类（禁止的、高风险、有限风险和低风险）以及实施时间表。最后，它简要比较了欧盟人工智能法案与其他与人工智能相关的国际法规和标准。
- en: Warning
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The author is not a lawyer, and this book does not provide legal advice. The
    intersection of law and artificial intelligence is a complex subject that requires
    expertise beyond the scope of AI, data science, and machine learning. Legal considerations
    surrounding AI systems can be complex and far-reaching. If you have any legal
    concerns related to the AI systems you are working on, seek professional legal
    advice from qualified experts in the field.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作者并非律师，本书不提供法律建议。法律与人工智能的交叉是一个复杂的主题，需要超越人工智能、数据科学和机器学习范畴的专业知识。围绕人工智能系统的法律考量可能非常复杂且影响深远。如果您对正在开发的人工智能系统有任何法律方面的疑问，请寻求该领域合格专家的专业法律建议。
- en: 'The Motivation for the EU AI Act: Trustworthy AI'
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案的动机：可信赖的人工智能
- en: As AI becomes increasingly intertwined with our daily lives, one of the challenges
    we face is learning to navigate the uncertainty that comes with it. This uncertainty
    is inherent to AI. AI models’ predictive accuracy has long been considered a core
    evaluation criterion when building an AI system. However, with the widespread
    use of AI in critical areas such as human resources, transportation, finance,
    medicine, and security, there is a growing need for these systems to be trustworthy—and
    traditional predictive accuracy alone is not sufficient to build trustworthy AI
    applications.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能与我们的日常生活日益紧密相连，我们面临的一个挑战是学会应对随之而来的不确定性。这种不确定性是人工智能固有的。长期以来，人工智能模型的预测准确性一直被视为构建人工智能系统时的核心评估标准。然而，随着人工智能在人力资源、交通、金融、医疗和安全等关键领域的广泛应用，对这些系统建立信任的需求日益增长——仅仅依靠传统的预测准确性是不足以构建可信赖的人工智能应用的。
- en: To better understand trustworthy AI, let’s start with its definition. *Trustworthy
    AI* is an umbrella term that refers to artificial intelligence systems that are
    designed and developed with principles such as fairness, privacy, and non-discrimination
    in mind, and with robust mechanisms to ensure reliability, security, and resilience.
    Within the AI community, this term is used interchangeably with *responsible AI*,
    *ethical AI*, *reliable AI*, and *values-driven AI*. Trustworthy AI systems must
    be adaptable to diverse and changing environments and robust against various types
    of disruptions, including cyber threats, data variability, and operational changes.
    They should operate transparently and be held accountable, with continuous monitoring
    and evaluation to respect human rights, including privacy and freedom from discrimination,
    and to ensure adherence to democratic values.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Trustworthy AI is a complex term that incorporates a long list of concepts and
    principles, which are visualized in [Figure 1-1](#chapter_1_figure_1_1748539916811473).
    These concepts lay the foundation for understanding the EU AI Act.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Trustworthiness in AI is grounded in the three pillars of *lawfulness*, *ethics*,
    and *robustness*. First, AI systems should be lawful, meaning they must comply
    with all relevant regulations to ensure a fair market, promote economic benefits,
    and protect citizens’ rights. Second, they must be built on ethical principles
    and values, incorporating input from all stakeholders and establishing appropriate
    feedback mechanisms. Finally, they must be robust, accounting for potential risks
    and ensuring safety at every stage of development and deployment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0101.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. The foundation and seven requirements of trustworthy AI
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Standing on these three pillars are seven key requirements that AI systems
    must implement to be deemed trustworthy:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Human agency and oversight
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Technical robustness and safety
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Privacy and data governance
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transparency
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Diversity, non-discrimination, and fairness
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Societal and environmental well-being
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accountability
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These come directly from the requirements outlined in the [Ethics Guidelines
    for Trustworthy AI](https://oreil.ly/nBhE6) developed by the European Commission’s
    High-Level Expert Group on AI (AI HLEG). In the following sections, I provide
    an explanation of each.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Human Agency and Oversight
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human agency and oversight are crucial in developing and operating AI systems.
    *Human agency* refers to the ability of individuals to make informed decisions
    and maintain control. AI systems should support this by providing transparency,
    interpretability, and mechanisms for control and intervention that enable humans
    to understand and influence the system’s decisions and actions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '*Human oversight* involves establishing governance processes and mechanisms
    that allow for human monitoring, evaluation, and intervention in the operation
    of AI systems. This includes ensuring transparency and interpretability of the
    systems’ decision-making processes based on the results produced by the AI models.
    Additionally, AI systems should provide human control mechanisms (e.g., the ability
    to override, adjust, or shut down the system when necessary). As depicted in [Figure 1-2](#chapter_1_figure_2_1748539916811507),
    I distinguish between four different modes of human oversight:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Human-in-command
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: This mode represents the highest level of human control, where humans maintain
    ultimate authority and responsibility over the AI system. Humans must explicitly
    authorize any AI system action. They are at the core of decision-making processes
    and always make the final decisions. In this mode, humans oversee the AI system’s
    overall activity and can decide when and how to use the system in specific situations.
    For example, the [“AI cockpit” concept](https://oreil.ly/HFlbo) demonstrates the
    human-in-command approach by providing a central user interface that enables operators
    to monitor AI systems’ effects on different user groups, evaluate results against
    technical and ethical criteria, and exercise the authority to disable the systems
    when necessary.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Human-in-the-loop
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: AI systems require human interaction at critical points in the decision-making
    process to add a layer of human experience and contextual understanding that AI
    currently lacks. Often, this involvement entails guiding the direction of decisions
    based on AI-generated predictions. The level of intervention and control might
    depend on the risk level of the system. Medical AI applications, for example,
    will typically require more intensive human intervention and control than less
    critical AI applications.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Human-on-the-loop
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The human plays an observer role, monitoring the actions of the AI system and
    intervening if necessary.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Human-out-of-the-loop
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The AI system operates independently, without human intervention or real-time
    supervision. This scenario is possible when decisions based on the system’s predictions
    can be programmatically implemented. Often, this mode is applied in trading applications
    that require data processing at speed, where human oversight is not realistic.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0102.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. The different levels of human oversight in AI
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The modes described here are each suitable for different AI use cases. Regardless
    of the one you use, proper implementation of the human agency and oversight requirement
    is crucial for designing and developing ethical AI systems that benefit from the
    strengths of AI while mitigating potential risks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Technical Robustness and Safety
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To be trustworthy, AI systems need to be accurate, reliable, and able to repeat
    their results. They should also have a “backup plan” if something goes wrong.
    In computer science, *robustness* means a system can keep working correctly even
    when there are mistakes, unusual inputs, or unexpected situations. For AI, robustness
    means that a machine learning (ML) model can keep making good, reliable predictions
    despite facing different conditions, tricky inputs, or changes in the type of
    data it sees.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: In simpler terms, a robust ML model can still give you the correct answers even
    if the information it receives is somewhat noisy, messy, or comes from a different
    source than the model was originally trained on. If these differences cause its
    performance to drop suddenly, then it’s not considered robust anymore. Achieving
    robustness is essential for building trust in AI—especially in areas like self-driving
    cars, cybersecurity systems, and healthcare, where a single mistake can have serious
    consequences.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three core layers to an AI system: the data it’s trained on, the
    algorithm it uses, and the software that puts them to use. Correspondingly, there
    are three levels of robustness:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '*Data robustness* requires training the system on a wide variety of data types
    and scenarios. This allows the model to “learn” to handle different situations
    and ensures it is less likely to fail when confronted with new or unexpected conditions.
    For example, a self-driving car’s AI should be trained on data from different
    weather conditions to handle rain, snow, and fog. In cybersecurity, an intrusion
    detection system should learn from many different types of network traffic and
    attack methods, so it can catch new threats as they emerge.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Algorithmic robustness* focuses on defending against attacks that try to trick
    the AI model itself. Some attacks occur at the moment of decision making by slightly
    altering the input to confuse the model. Others are executed during the training
    phase by introducing “poisoned” data that causes the model to make incorrect predictions
    later. A robust algorithm should be able to handle noisy or altered inputs, changes
    in data types, and even deliberately manipulated inputs designed to fool it. For
    example, a robust language model can deal with misspellings, slang, or strange
    grammar without losing accuracy.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*System-level robustness* looks at the bigger picture—the entire lifecycle
    of the AI product, from how data is collected to how the model is built, tested,
    and finally used in the real world. By examining every step, we can spot risks
    before they lead to serious problems. This holistic approach ensures the entire
    AI pipeline, and its interaction with other systems, remains steady and secure.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In research, we often differentiate two types of robustness:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Non-adversarial robustness
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Can the model handle everyday challenges, like noisy inputs or slight changes
    in the environment, without losing accuracy?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial robustness
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Can the model resist “tricks” that attackers use intentionally to confuse it,
    such as subtly modified inputs designed to produce the wrong answer?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure how robust an AI model is, practitioners and researchers look at
    a variety of measurements:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: How many predictions the model gets right.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Error rate
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: How many incorrect predictions the model makes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity (recall)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The model’s ability to correctly identify items that have a given condition
    (true positives). For example, if we are implementing a binary classifier, of
    all the cases that should be labeled “positive,” how many are correctly identified?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Specificity
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The model’s ability to correctly identify items that do not have a given condition
    (true negatives). For example, of all the cases that should be labeled “negative,”
    how many are correctly rejected?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Robustness curves
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Graphs that show how the model’s performance changes when we introduce challenges,
    like more noise, missing information, or unusual inputs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: All of these measurements can help us understand the model’s strengths and weaknesses.
    However, the best way to measure robustness typically depends on the specific
    problem and requirements of the AI system.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and Data Governance
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second requirement of trustworthy AI systems is that they must ensure full
    respect for privacy and personal data protection, in line with Articles 7 and
    8 of the [Charter of Fundamental Rights of the European Union](https://oreil.ly/nPDrM).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Many AI systems, such as recommender systems and systems that provide personalized
    predictions, require some amount of personally identifiable information (PII).
    Handling of PII is a subject to regulations within the EU and the US, under the
    General Data Protection Regulation (GDPR) and the California Consumer Privacy
    Act (CCPA).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: By examining how an AI system collects, stores, processes, and shares personal
    or sensitive data, *p**rivacy* *i**mpact* *a**ssessments* (PIAs) help organizations
    spot areas where privacy might be at risk and guide them in modifying the system,
    implementing safeguards, and complying with relevant privacy laws and regulations.
    PIAs should ideally be conducted early in the design process, to proactively identify,
    assess, and mitigate potential privacy risks. This is an example of *privacy by
    design*, an engineering philosophy that involves embedding privacy protections
    into every phase of an AI system’s lifecycle—from data collection and preprocessing
    to model training, deployment, and maintenance. This ensures that privacy isn’t
    just treated as a legal afterthought or a check-the-box compliance step at the
    end of the development process, but as a fundamental design principle that helps
    organizations maintain user trust and meet regulatory requirements.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Privacy by design incorporates privacy considerations into the foundational
    components of AI systems. For AI engineers, this might involve using techniques
    such as minimal data collection, deidentification, and anonymization (encryption,
    differential privacy, other methods to protect sensitive information before it
    even reaches the model); adapting model architecture choices (for example, using
    federated learning); and continuous evaluation and monitoring. The related concept
    of *data governance* ensures that privacy principles are consistently applied,
    maintained, and improved within a structured organizational framework.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 设计中的隐私将隐私考虑纳入AI系统的基本组件。对于AI工程师来说，这可能包括使用最小数据收集、去标识化、匿名化（加密、差分隐私、其他在信息甚至到达模型之前保护敏感信息的方法）；调整模型架构选择（例如，使用联邦学习）；以及持续评估和监控。相关的概念*数据治理*确保在结构化的组织框架内持续应用、维护和改进隐私原则。
- en: Data governance is a data management function that guarantees the availability,
    usability, integrity, and security of the data collected and used in an organization.
    Its key elements are visualized in [Figure 1-3](#chapter_1_figure_3_1748539916811532).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据治理是一个数据管理功能，它保证了组织收集和使用的数据的可用性、可用性、完整性和安全性。其关键要素在[图1-3](#chapter_1_figure_3_1748539916811532)中得到了可视化。
- en: '![](assets/taie_0103.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/taie_0103.png)'
- en: 'Figure 1-3\. Key elements of data governance (adapted from [Data Governance:
    The Definitive Guide](https://oreil.ly/4hIZp) by Evren Eryurek et al. [O’Reilly])'
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3. 数据治理的关键要素（改编自Evren Eryurek等人的《数据治理： definitive guide》（https://oreil.ly/4hIZp）[O’Reilly]）
- en: 'Data governance has become increasingly important as data volumes have grown,
    organizations have become more data-driven, and access to data has expanded. The
    ultimate goal of data governance is to enhance the trustworthiness of data, which
    is fundamental to trustworthy AI. There are three key aspects to this: discoverability,
    security, and accountability.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量的增加、组织变得更加数据驱动以及数据访问的扩展，数据治理变得越来越重要。数据治理的最终目标是提高数据的可信度，这是可信AI的基础。这有三个关键方面：可发现性、安全性和问责制。
- en: '*Discoverability* refers to the availability of the dataset’s metadata, data
    provenance (lineage), and glossary of domain entities. It’s essential for ensuring
    users and AI systems can easily access the data they need. Data governance establishes
    procedures that guarantee that the right data is accessed by the appropriate people
    in the organization and determine what data AI systems can access. *D**ata quality*
    is a related concept that is crucial in building trust in data: data should be
    correct, complete, timely, and integral.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*可发现性* 指的是数据集元数据、数据来源（血缘）和领域实体词汇表的可用性。这对于确保用户和AI系统可以轻松访问他们所需的数据至关重要。数据治理建立程序，确保组织内适当的人员可以访问正确的数据，并确定AI系统可以访问哪些数据。*数据质量*
    是一个相关的概念，在建立数据信任方面至关重要：数据应该是正确的、完整的、及时的、整体的。'
- en: In the context of AI engineering, data*security* means protecting the data used
    to train, validate, and run models from unauthorized access, theft, tampering,
    or loss. Together with privacy, data security is an essential aspect of protecting
    data and ensuring adherence to regulations such as the GDPR  (or CCPA).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI工程背景下，*数据安全* 指的是保护用于训练、验证和运行模型的从未经授权的访问、盗窃、篡改或丢失中。与隐私一起，数据安全是保护数据并确保遵守GDPR（或CCPA）等法规的一个基本方面。
- en: The third aspect of data governance, *accountability*, involves ensuring that
    everyone involved with an AI system, from data suppliers to model developers and
    operators, knows their role, can explain their choices, and can be held responsible
    for the system working ethically, legally, and safely.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据治理的第三个方面是*问责制*，它涉及确保与AI系统相关的每个人，从数据供应商到模型开发者和操作者，都知道他们的角色，可以解释他们的选择，并且可以因系统在道德、法律和安全方面的工作而承担责任。
- en: AI development should respect the fundamental right to privacy at all points
    of the AI application lifecycle, including data collection, processing, and storage
    and model design, development, and deployment. Privacy issues can arise in various
    aspects of AI systems, such as when social media platforms using AI to analyze
    user behaviors and preferences inadvertently expose sensitive information through
    targeted advertising or data breaches, or when AI is used to enhance the capabilities
    of surveillance systems, leading to potential overreach in monitoring activities.
    This can result in a loss of anonymity and freedom, as every action can be watched
    and recorded. For example, governmental structures using facial recognition technologies
    in public spaces can track individuals without their consent, potentially leading
    to misuse of power and privacy violations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: AI systems can also amplify biases present in their training data. When these
    biases affect how data is collected, processed, or used, they can disproportionately
    impact the privacy of certain groups. For instance, AI-driven credit scoring models
    might use biased data that discriminates against certain racial or gender groups,
    raising concerns about fairness and privacy in relation to financial data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Example metrics and mechanisms to define and track privacy and data governance
    include:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Data encryption levels
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: These determine the degree of data protection during transmission and at rest.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Access controls
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: These are policies and tools that manage who can access or alter data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Data retention and deletion policies
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: These ensure compliance with data minimization principles and regulations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Transparency
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data, AI models, and software systems that include AI components must be transparent
    and provide traceability. Regardless of the industry or use case, transparency
    is the [most commonly cited ethical principle](https://oreil.ly/_pir3) in existing
    AI guidelines.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Transparency in AI systems refers to the ability to understand how an AI system
    works internally and makes decisions. It provides comprehensible explanations
    about the AI system’s components, algorithms, decision-making process, and overall
    functioning to stakeholders such as users, developers, and regulators. Key dimensions
    of AI transparency include:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Explainability
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: The ability to clearly articulate in human-understandable terms how and why
    an AI system, encompassing its data sources, model components, and decision-making
    processes, arrived at a specific output or decision for a given case. This is
    important for building trust and accountability. As depicted in [Figure 1-4](#chapter_1_figure_4_1748539916811556),
    explainable AI (XAI) models might not be the models with the highest accuracy,
    so depending on the use case requirements, engineers might select an explainable
    but less accurate AI algorithm. Techniques for XAI are used at every stage of
    the AI lifecycle, including analyzing data for model development, incorporating
    interpretability into the system architecture, and producing post-hoc explanations
    of system behavior.^([1](ch01.html#id333))
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0104.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. Balancing explainability and accuracy is a common challenge in
    AI systems
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data transparency
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Openness about the training data used to build an AI model, including its sources,
    characteristics, and potential biases or limitations.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic transparency
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Visibility into the AI algorithms and how they process input data to generate
    outputs or decisions. This includes understanding the features, weights, and logic
    the model uses.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Governance transparency
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Documenting key decisions made during an AI system’s development process, establishing
    clear protocols and responsibilities, and ensuring organizational oversight. This
    aspect is particularly relevant to compliance with the EU AI Act.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Communication transparency
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Sharing information about the AI system’s purpose, capabilities, and limitations
    with relevant stakeholders in a timely, clear, and accessible manner.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: AI transparency aims to open the “black box” representing the internal operations
    of AI systems, which are often complex and opaque. This enables humans to understand
    the inner workings of the systems and audit them for errors or biases, fostering
    trust in their use.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of metrics to define and track transparency in AI systems include
    the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Explainability index
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: How easily the system’s decisions can be explained to users. This depends on
    the availability of explanations throughout the system’s lifecycle, the types
    of explanations provided (e.g., feature importance, counterfactual examples, visual
    aids), and the scope of those explanations (whether they address global model
    behavior or local, individual predictions).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Documentation completeness
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Availability and clarity of documentation on the AI system’s purpose, functionality,
    limitations, model training details, and feature engineering process.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic auditability
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Ease of auditing AI algorithms for compliance and performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Diversity, Non-Discrimination, and Fairness
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Discussion about bias and fairness in machine learning has become a hot topic
    over the past decade. Because data collected in an unequal manner and processed
    by non-diverse teams could potentially cause harm, incorporating diversity and
    inclusion throughout the entire AI system lifecycle is a clear requirement for
    trustworthy AI. This includes accessibility, a user-centric approach that guarantees
    that the usability of the AI system takes everyone into account—especially people
    with disabilities.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[Inclusive engineering](https://oreil.ly/n3qlT) is defined as “the process
    of ensuring that engineering products and services are accessible and inclusive
    of all users, and are as free as possible from discrimination and bias, throughout
    their lifecycle.” This approach is crucial in the development of AI systems, extending
    beyond just technical engineering to encompass broader social considerations.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Potential biases should be identified and addressed at every stage of AI system
    development. It is vital to establish a well-defined strategy or set of procedures
    to mitigate bias and promote fairness, both in the collection and use of input
    data and in the algorithm design. In this book, I focus on the [CRISP-ML(Q)](https://oreil.ly/srSRa)
    development process model to specify a fair and effective AI system development
    strategy. This model requires establishing processes for testing and monitoring
    for potential biases and detecting non-representativeness in data across all phases
    of development.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the complete end-to-end AI development workflow for fairness is important
    for building successful AI systems. Improving diversity and representativeness
    in AI systems is a step toward compliance with the EU AI Act and providing value
    for all users of those systems. Establishing a robust mechanism for flagging issues
    related to bias, discrimination, or poor performance—such as through bias detection
    tools, categorized reporting systems, and clear reporting guidance for affected
    individuals—helps developers and end users become aware of and address these issues.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: An often underestimated and less technical aspect of an organization’s data
    culture is educational and awareness initiatives for bias and fairness in AI.
    These initiatives are intended to help AI product managers, designers, and engineers
    become more aware of the possible bias they can inject while designing and developing
    AI systems.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: The EU AI Act ensures that every entity along the AI system’s value chain, from
    producer to deployer, is responsible for providing users with a fair and ethical
    experience. Several metrics are used to evaluate fairness in AI systems, and technical
    approaches for fairness improvement and bias mitigation can be applied before
    modeling (preprocessing), at the point of modeling (in-processing), or after modeling
    (post-processing), as illustrated in [Figure 1-5](#chapter_1_figure_5_1748539916811583).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0105.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-5\. Various metrics and approaches can be implemented at different
    stages of the modeling process to ensure equitable outcomes and reduce bias (source:
    [*https://oreil.ly/CGjLK*](https://oreil.ly/CGjLK)*)*'
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Table 1-1](#chapter_1_table_1_1748539916818522) lists some metrics that are
    commonly used to evaluate fairness and non-discrimination. These metrics quantify
    potential biases or disparities in the AI model’s predictions across different
    demographic groups.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-1\. Preprocessing, in-processing, and post-processing fairness metrics
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric category/name | Definition |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| Preprocessing |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| Statistical/demographic parity | Ensures equal probability of being classified
    with positive labels across groups |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| Disparate impact | Measures the ratio of positive classification rates between
    unprivileged and privileged groups |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| Individual fairness | Ensures similar individuals receive similar treatment
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| In-processing |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| Equal opportunity | Ensures equal true positive rates across different groups
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| Equalized odds | Requires equal true positive and false positive rates across
    protected groups |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| Overall accuracy equality | Compares relative accuracy rates between different
    groups |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| Treatment equality | Considers the ratio of false negatives to false positives
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| Post-processing |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| Test fairness/calibration | Ensures equal probability of a positive outcome
    given a particular score across groups |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| Well calibration | Requires predicted probabilities to match actual probabilities
    across groups |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| Balance for positive/negative class | Ensures equal expected scores for positive
    and negative classes across groups |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| Generalized entropy index | Measures individual-level fairness impacts of
    classification outcomes |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: Societal and Environmental Well-Being
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI can have positive or negative impacts depending on how it is used. The EU
    AI Act addresses the societal and environmental implications of trustworthy AI.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: AI technology has proven negative impacts on the environment. The primary environmental
    concerns associated with AI include energy consumption, carbon footprint, e-waste,
    and indirect environmental impacts. AI and machine learning models, especially
    large ones, require significant computational power (as visualized in [Figure 1-6](#chapter_1_figure_6_1748539916811608)).
    The data centers housing these processors thus require vast amounts of energy
    to run and to cool, which means large-scale AI systems have a significant carbon
    footprint. To mitigate this concern, efforts are being made to power data centers
    with renewable energy sources. The production of AI hardware also requires precious
    metals and rare earth elements, which can be environmentally damaging (in terms
    of extraction, resource depletion, and recycling challenges). Additionally, hardware
    quickly becomes obsolete, contributing to electronic waste.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0106.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-6\. The amount of computing resources used to train deep learning
    models increased 300,000-fold between 2012 and 2018 (source: [*https://oreil.ly/WJOOw*](https://oreil.ly/WJOOw))'
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Depending on its applications, AI can have indirect effects on the environment
    as well. For instance, AI-driven automation can lead to increased production capacities
    and potentially increased resource consumption. Conversely, AI can optimize systems
    to be more energy-efficient, reduce waste, or enhance resource management, thereby
    potentially having a positive impact.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: As well as model accuracy, the proposed [“Green AI” approach](https://oreil.ly/jGAsk)
    considers *efficiency*, measured as the number of floating-point operations (flops)
    required to generate a result, as a key evaluation criterion. Financial operations
    (FinOps), a cloud financial management practice that helps organizations efficiently
    manage their cloud spending, can also be leveraged to address the environmental
    and financial impacts of AI.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'With regard to the societal aspect, AI is changing the way we work in three
    key ways: by automating tasks, reshaping work processes, and affecting required
    job skills. Job displacement is an important concern, as AI has the potential
    to automate many roles—especially those that involve [repetitive tasks](https://oreil.ly/7Pdjg).
    At the same time, AI technology has the potential to create [new job roles](https://oreil.ly/zFqNJ)
    that demand advanced technological and analytical skills, such as machine learning
    engineers, data scientists, and AI ethics specialists. Additionally, AI can drive
    the development of innovative products and services, opening up new career opportunities
    in emerging sectors like AI-driven digital assistants and smart devices.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: A related concern is that the rise of AI is leading to increased [job polarization](https://oreil.ly/hW0gZ),
    with demand for high-skilled workers growing and low-skilled positions facing
    obsolescence. This worsens income inequality and creates challenges for those
    without access to education and training. As a result, AI in the workplace can
    lead to increased stress, anxiety, and job insecurity due to the fear of job loss
    and uncertainty about the future.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: AI systems also have the potential to negatively impact society at large through
    misinformation. AI—particularly generative AI—can produce misinformation and disinformation
    at scale. This can harm democracy by contributing to misinformation in elections,
    spreading propaganda, and influencing voter behavior. Tools like ChatGPT, pi.ai,
    and perplexity.ai can easily create realistic but false content, which can be
    used to spread misinformation. AI-generated “deepfakes” (convincing but fake videos
    and images) can be used to manipulate public opinion, and AI-driven content personalization
    can create “information bubbles” where individuals are exposed only to information
    that reinforces their existing beliefs, leading to increased polarization and
    social fragmentation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: The use of AI for surveillance is also an area of concern with regard to societal
    well-being and privacy. Research has shown that at least 75 countries actively
    utilize AI technologies for surveillance (see [Figure 1-6](#chapter_1_figure_6_1748539916811608))*.*
    This can lead to unauthorized data collection, privacy violations and unauthorized
    access to sensitive data, and potential misuse of personal information. In addition,
    use of AI in law enforcement, such as for predictive policing, can result in biased
    outcomes and discrimination against certain demographic groups.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0107.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-7\. AI surveillance technology is being adopted by a larger number
    of countries and at a faster pace than experts typically believe (source: [*https://oreil.ly/9_iB7*](https://oreil.ly/9_iB7))'
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Addressing these challenges requires robust ethical guidelines, transparent
    regulatory frameworks, and ongoing public dialogue to ensure that AI is developed
    and deployed to support democratic values and societal well-being.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Accountability
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI systems must be developed and operated responsibly. *AI accountability* involves
    establishing mechanisms for holding the developers and users of AI systems responsible
    for the impacts of those systems throughout the entire development cycle.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Accountability implies that information about the system’s purpose, design,
    data, and processes is available to internal and external auditors. As an example,
    we can refer to Google’s Responsible Generative AI Toolkit, which covers risk
    and mitigation techniques to address safety, privacy, fairness, and accountability
    (see [Figure 1-8](#chapter_1_figure_8_1748539916811664)). This includes maintaining
    detailed documentation and records of the AI development process, decision making,
    and outcomes to enable traceability. Logging and recordkeeping are essential for
    accountability. Additionally, accountability requires that an AI system can explain
    or justify its decisions. Finally, accountability ensures that there are mechanisms
    and processes for redress in place to minimize or correct negative impacts or
    unfair outcomes caused by AI systems. AI systems should provide appropriate opportunities
    for feedback, relevant explanations, and defined procedures for escalating concerns.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0108.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-8\. Google’s Responsible Generative AI Toolkit takes a holistic approach
    to accountability (source: [*https://oreil.ly/maEI5*](https://oreil.ly/maEI5))'
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the context of trustworthy AI, key mechanisms that should be implemented
    to create accountability include:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Clear responsibility guidelines and processes
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Establish guidelines and clear responsibilities for various stakeholders involved
    in the AI system lifecycle, including developers, deployers, and users.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Transparency and explainability
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Maintain detailed documentation of the AI development process, training data,
    algorithms used, and decision-making criteria to enable traceability. Ensure that
    AI systems are transparent about their capabilities, limitations, and decision-making
    processes. Provide clear explanations for decisions made based on the AI predictions.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Human oversight and intervention
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Implement human checks and oversight, especially for high-stakes AI decisions,
    with the ability to override the AI when needed.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Auditing and evaluation
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Conduct regular internal and third-party audits to identify and eliminate biases,
    ensuring compliance with regulations and ethical standards. By systematically
    auditing their AI systems using the CRISP-ML(Q) process model, organizations can
    assess the systems’ quality, reliability, and trustworthiness. The audit depth
    and focus areas can be tailored based on the risk and criticality of the use case.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Redress and complaint mechanisms
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Establish user-friendly channels for submitting complaints, feedback, or requests
    for explanations about AI decisions. Provide clear processes for affected individuals
    to challenge decisions and seek remedy.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Trustworthy AI is the foundational concept behind the legislation of the EU
    AI Act. You should now have a general understanding of the seven key requirements
    of trustworthy AI systems. Next, we’ll take a look at how the Act itself is structured.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The EU AI Act in a Nutshell
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The EU AI Act  focuses  on promoting human-centered and trustworthy artificial
    intelligence. Its primary objective is to foster innovation while ensuring a high
    level of protection for health, safety, and fundamental rights as outlined in
    the Charter—including democracy, the rule of law, and environmental protection—and
    mitigating the potential harmful effects of AI systems in use in the European
    Union. To fully understand the EU AI Act, it is essential to grasp its scope,
    who it applies to, and the timeline for compliance with the AI system requirements.
    This section provides a high-level overview of the EU AI Act. I’ll explain the
    detailed requirements for different risk categories later in the book.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Per the [General Provisions](https://oreil.ly/6MMz1), the Act establishes:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: “Harmonized rules for the placing on the market, the putting into service, and
    the use of AI systems in the Union;
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prohibitions of certain AI practices;
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specific requirements for high-risk AI systems and obligations for operators
    of such systems;
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Harmonized transparency rules for certain AI systems;
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Harmonized rules for the placing on the market of general-purpose AI models;
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rules on market monitoring, market surveillance, governance and enforcement;
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measures to support innovation, with a particular focus on [small and medium-sized
    enterprises], including startups.”
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It is structured into the following main sections:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Chapters
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: The Act is divided into 13 chapters covering different aspects, such as general
    provisions, prohibited AI practices, requirements for high-risk AI systems, governance,
    etc.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Articles
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Each chapter contains one or more articles laying out the specific rules and
    obligations. The articles are numbered sequentially throughout the Act and may
    be grouped into sections within each chapter. For example, Article 6 under Section
    1 of Chapter III covers the “Classification Rules for High-Risk AI Systems.”
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Annexes
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The Act has 13 annexes that provide supplementary information such as lists,
    definitions, and procedures. You can navigate to a specific Annex by its number
    or title, e.g., “Annex III” or “High-Risk AI Systems Referred to in Article 6(2).”
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Recitals
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The recitals are numbered paragraphs that explain the rationale and context
    behind the Act’s provisions. They can help readers interpret the articles but
    are not binding.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Definitions
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chapter I, Article 3 of the EU AI Act provides a set of definitions for terms
    used in the legislation. Perhaps most importantly for the purposes of this book,
    it defines an *AI system* as:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: A machine-based system that is designed to operate with varying levels of autonomy
    and that may exhibit adaptiveness after deployment, and that, for explicit or
    implicit objectives, infers, from the input it receives, how to generate outputs
    such as predictions, content, recommendations, or decisions that can influence
    physical or virtual environments.
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note, however, that given the rapid pace and unpredictability of technological
    and AI development, this definition is not entirely static, and a dynamic regulatory
    tool has been integrated into the Act to allow it to adapt over time.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition covers a wide range of AI techniques and approaches, including:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning methods (such as supervised, unsupervised, and semi-supervised
    machine learning)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning methods
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logic- and knowledge-based methods (such as logic programming, expert systems,
    inference and deductive engines, reasoning engines)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical approaches
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian methods
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search and optimization approaches
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pretrained generative models such as BERT, DALL·E, Claude, Mistral, and GPT
    have become increasingly popular in recent years. They are developed using large-scale
    self-supervised learning on massive datasets and can be adjusted to various downstream
    tasks. These are commonly referred to as *general-purpose AI (GPAI) model**s*,
    which the Act defines as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: An AI model . . . trained with a large amount of data using self-supervision
    at scale, that displays significant generality and is capable of competently performing
    a wide range of distinct tasks regardless of the way the model is placed on the
    market and that can be integrated into a variety of downstream systems or applications,
    except AI models that are used for research, development or prototyping activities
    before they are placed on the market
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The EU AI Act aims to ensure that the previously mentioned techniques used in
    digital and physical products, services, or systems are safe and respect existing
    laws on fundamental rights and European Union values.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Key Players from Creation to Market Operation
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The EU AI Act is a legal framework for developing, distributing, and using AI
    in the EU. This regulatory framework applies to companies and persons that make,
    bring in, or distribute AI systems or general-purpose AI models in the EU, even
    if those entities are located in a country outside the EU. [Figure 1-9](#chapter_1_figure_10_1748539916811704)
    visualizes the key players who are affected by the EU AI Act throughout the complete
    AI system lifecycle.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0109.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: Figure 1-9\. Key players
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Article 3 defines these roles as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: A *provider* is a natural or legal person (company, organization, or other body)
    that develops an AI system or GPAI model (or has one developed) and puts it on
    the market or uses it, whether for payment or for free, under their own name or
    brand.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An *importer* is a natural or legal person in the EU who places on the market
    an AI system bearing the name or trademark of an entity in a non-EU country.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *distributor* is a natural or legal person in the supply chain, other than
    the provider or the importer, that makes an AI system available on the EU market.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An *authorized representative* is a natural or legal person in the EU who has
    been given written permission by the provider of an AI system or GPAI model to
    carry out the responsibilities and procedures outlined in the Act on their behalf.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *deployer* is a natural or legal person using an AI system for professional
    activities.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *user* is considered to be any natural person or group of persons who use
    or are otherwise affected by an AI system.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Classification of AI Systems by Risk Levels
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another important aspect of the EU AI Act is its risk-based approach. According
    to the regulation’s classification system, AI systems are categorized into four
    risk levels:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Prohibited
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: High risk
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Limited risk
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimal risk
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is also important to note that GPAI models are specifically regulated and
    classified under this Act. In [Chapter 7](ch07.html#chapter_7_toward_trustworthy_general_purpose_ai_and_generati_1748539924538638),
    I will lay out the foundation for understanding the requirements for compliance
    with the Act for GPAI models, and their relation to the currently popular generative
    AI.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: The EU AI Act prohibits certain AI systems that are considered to pose an unacceptable
    risk. The bans are aimed at AI systems that could heavily influence or harm people’s
    decision making or infringe upon their rights. Specifically, the Act prohibits
    AI practices such as using manipulative subliminal techniques, exploiting vulnerabilities
    based on age, disability, or social circumstances, and making high-stakes assessments
    based on profiling or predictive traits without sufficient human oversight. The
    legislation also prohibits the unregulated use of “real-time” biometric identification
    in public spaces, unless under strict conditions for law enforcement purposes
    related to significant public safety concerns. (See [Chapter II, Article 5](https://oreil.ly/0KGOH),
    for more information.)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-10](#chapter_1_figure_11_1748539916811723) illustrates the risk categories
    and their expected distribution. Roughly 20% of all AI systems are expected to
    be classified as high risk.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0110.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: Figure 1-10\. AI system risk categories
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: High-risk AI systems, as defined by the EU AI Act, either are intended to be
    used as safety components of products or are products themselves (see [Chapter
    III](https://oreil.ly/_o2pw)). These AI systems are subject to specific legislation
    and require third-party conformity assessments. In addition, high-risk AI systems
    specified in a designated list (Annex III) are subject to stringent compliance
    requirements due to their potential impact on the health, safety, or fundamental
    rights of individuals. These include systems in areas such as biometrics, critical
    infrastructure, education, etc.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Like many products traded on the extended Single Market in the European Economic
    Area (EEA), AI systems classified as high risk must receive the CE marking ([Figure 1-11](#chapter_1_figure_12_1748539916811741))
    to be certified within the EU. This mark indicates that products sold in the EEA
    have been evaluated to meet stringent safety, health, and environmental protection
    requirements.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0111.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Figure 1-11\. The CE mark
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Limited-risk AI systems pose lower risks, mostly in the form of manipulation,
    deception, or impersonation. This category includes systems like chatbots and
    generative AI systems capable of producing deepfakes. For limited-risk AI systems,
    the main obligation is transparency—providers must disclose that the output is
    AI-generated and users must be made aware they are interacting with an AI system.
    There are also requirements to label deepfakes clearly.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: The last category is minimal-risk AI systems. According to the EU AI Act, these
    are systems that pose little to no risk to people’s safety, fundamental rights,
    or privacy. They include AI applications like video games, spam filters, or simple
    image editing tools that perform narrow tasks with limited decision-making capabilities.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that the EU AI Act classifies use cases (AI systems
    and GPAI models) and not the AI/ML technologies or algorithms themselves. Proper
    classification has an impact on the estimation of the AI system’s requirements,
    because different risk categories imply different governance and MLOps architectural
    decisions and obligations. However, determining the risk level of an AI system
    can be challenging as it relies on various factors and involves classifying how
    the capabilities of a nondeterministic system will affect users and systems that
    may interact with it in the future. In [Chapter 4](ch04.html#chapter_4_ai_system_assessment_and_tailoring_ai_engineering_1748539919034657),
    I outline a risk classification framework for AI systems.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Note that scientific research and military applications of AI fall outside the
    scope of the EU AI Act. However, so-called “dual use” technologies—systems that
    can be used for both civilian and military purposes—may still be subject to the
    Act if used in civilian contexts.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Enforcement and Implementation
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The implementation of the EU AI Act follows a structured timeline. Here are
    the key milestones and deadlines:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: EU AI Act enters into force (August 2024)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The Act is effective 20 days after it is published in the Official Journal of
    the EU.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Six months after entry into force (Q4 2024–Q1 2025)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Prohibitions on unacceptable-risk AI systems are effective.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Nine months after entry into force (Q1 2025)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Codes of practice for GPAI models must be finalized.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Twelve months after entry into force (Q2–Q3 2025)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Obligations on providers of GPAI models are effective.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Eighteen months after entry into force (Q4 2025–Q1 2026)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The Commission implements acts on post-market monitoring for high-risk AI systems.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Twenty-four months after entry into force (Q2–Q3 2026)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Obligations on high-risk AI systems listed in Annex III become applicable. Member
    states must have implemented rules on penalties and established AI regulatory
    sandboxes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Thirty-six months after entry into force (Q4 2026–Q1 2027)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Obligations for high-risk AI systems not prescribed in Annex III but subject
    to existing EU product safety laws are effective.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: By the end of 2030
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Obligations for specific AI systems that are components of large-scale EU IT
    systems in areas like security and justice become applicable.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: This timeline, depicted in [Figure 1-12](#chapter_1_figure_13_1748539916811763),
    implies that organizations that create high-risk AI systems today have a grace
    period to prepare their internal processes. They are responsible for complying
    fully with the provisions of the Act by the end of the grace period (June 2026).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0112.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Figure 1-12\. Timeline outlining the key milestones and deadlines for the implementation
    of the EU AI Act
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Full Picture of Compliance
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many organizations using and embedding AI technology into their products are
    asking the same question: “What does the EU AI Act mean for us?” The Act aims
    to ensure that all digital and physical products that incorporate AI—whether as
    a feature or as a core function—are used in a safe and ethical manner, in line
    with EU fundamental rights. To this end, the Act treats AI systems as regulated
    products. Like other products in the EU market, AI systems that are considered
    high risk must be CE marked to indicate compliance, as described earlier, or they
    are not permitted for use. [Table 1-2](#chapter_1_table_2_1748539916818552) briefly
    outlines the process steps needed to meet the EU AI Act’s requirements for this
    category of AI systems.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Again, the author is not a lawyer. This book does not provide legal advice.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-2\. The end-to-end process for EU AI Act compliance
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '| EU AI Act compliance step | Guiding questions |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| AI system inventory and risk classification |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: How many AI systems are in place already or are intended to be put in production?
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What risk categories do each of these AI systems belong to?
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do any of them utilize models that are classified as systemic risk GPAI models?
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '| Identification of compliance requirements |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: What requirements do we need to fulfill?
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '| Compliance operationalization |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: What processes, structures, engineering practices, and roles need to be established
    to comply with the Act?
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '| Pre-market compliance verification |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: What has to be done before placing AI systems on the market and putting them
    into service?
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What internal and external conformity assessments are required for compliance
    verification?
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we CE-mark our AI systems?
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where should our AI systems be registered?
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '| Post-market continuous compliance |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: What has to be done to ensure compliance after putting the AI systems into service?
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at these essential steps for providers and deployers
    of AI systems  to ensure conformity with the EU AI Act:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 1\. AI system inventory and risk classification
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Before setting up the technical and organizational processes for achieving compliance,
    you must determine the required scope of compliance measures. Start with an inventory
    of all AI systems that are currently in use or in development. Then assign a risk
    level to each of those systems (I discuss risk classification for AI systems in
    detail in [Chapter 4](ch04.html#chapter_4_ai_system_assessment_and_tailoring_ai_engineering_1748539919034657)).
    Any AI use cases that fall into the prohibited category must be discontinued.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Identification of compliance requirements
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: The compliance measures to implement will depend on the results of the risk
    classification. It’s important to distinguish between obligations for AI systems
    and for GPAI models used in those systems, which may be subject to transparency
    obligations or systemic risk requirements. The Act lays out concrete requirements
    for both categories.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Compliance operationalization
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Identifying compliance requirements allows you to define the scope of the technical
    and organizational processes needed for conformity with the EU AI Act. This includes
    establishing governance structures, engineering practices, and clearly defined
    roles. The Act outlines specific obligations, such as implementing risk management
    measures, maintaining technical documentation, ensuring human oversight, and guaranteeing
    the accuracy, robustness, and security of AI systems. This phase, which is the
    focus of this book, also involves setting up a quality management system that
    covers testing, incident reporting, data governance, record retention, and logging.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Pre-market compliance verification
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to demonstrate compliance through internal and, where necessary,
    external conformity assessments. Once these are completed, the AI systems must
    be CE marked and registered in the EU database. These steps are mandatory before
    the systems can be placed on the market or put into service.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Post-market continuous compliance
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: The final phase focuses on ensuring that the AI systems continue to meet the
    requirements of the Act throughout their lifecycle. This involves demonstrating
    continued adherence to compliance standards, regardless of changes to the system,
    through continuous monitoring of system performance and addressing any issues
    that arise to maintain alignment with regulatory requirements.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Penalties for EU AI Act Violation
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The EU AI Act introduces significant penalties and fines for companies that
    violate its rules and requirements. [Table 1-3](#chapter_1_table_3_1748539916818575)
    provides an overview.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-3\. Penalties for violating the terms of EU AI Act
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '| Violation | Penalties |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| Prohibited AI practices (AI practices listed in Article 5, such as exploiting
    vulnerabilities, social scoring, real-time biometric identification in public
    spaces, etc.) | Administrative fines of up to €35 million or 7% of total worldwide
    annual turnover for the preceding financial year, whichever is higher |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| Noncompliance with requirements for high-risk AI systems under Article 10
    | Administrative fines of up to €30 million or 6% of total worldwide annual turnover,
    whichever is higher |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| Noncompliance with other obligations (apart from Articles 5 and 10) | Administrative
    fines of up to €20 million or 4% of total worldwide annual turnover, whichever
    is higher |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| Providing incorrect, incomplete, or misleading information to authorities
    | Administrative fines of up to €10 million or 2% of total worldwide annual turnover,
    whichever is higher |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: Existing AI Regulations and Standards
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The EU AI Act serves as a comprehensive regulatory model for non-EU nations,
    underscoring the balance between innovation and trustworthy AI. Its strict regulations
    on biometric systems and high-risk AI applications set a high standard for AI
    governance globally. Let’s briefly review the existing landscape of AI regulations:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: UNESCO AI ethics recommendations
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: The [Recommendation on the Ethics of Artificial Intelligence](https://oreil.ly/Qr8eM),
    adopted in November 2021 by UNESCO’s 193 Member States, provides a framework for
    ethical AI development. It outlines core values and principles for the ethical
    development and deployment of AI, including respect for human rights, inclusion
    and diversity, fairness and non-discrimination, transparency and explainability,
    accountability, safety and security, and sustainability. The recommendation aims
    to influence ethical AI practices globally and includes innovative tools and methodologies
    to translate ethical principles into practice.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: US executive order on trustworthy AI
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: The [Executive Order on the Safe, Secure, and Trustworthy Development and Use
    of Artificial Intelligence](https://oreil.ly/zRfHi), issued in October 2023, emphasizes
    the potential benefits and risks of AI. It highlights the importance of responsible
    AI governance to address societal issues and prevent negative consequences such
    as fraud, bias, and threats to national security. The order stresses the need
    for collaboration across government, industry, academia, and civil society to
    ensure the safe and responsible development and application of AI.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: China generative AI services law
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: The [Interim Measures for the Administration of Generative Artificial Intelligence
    Services](https://oreil.ly/Axc-r), jointly issued by the Cyberspace Administration
    of China (CAC) and six other government agencies in July 2023, signal China’s
    proactive approach to regulating generative AI services (including models and
    related technologies that produce text, graphics, audio, and video). The regulation
    applies to all entities offering these services to the general Chinese population.
    The Interim Measures recognize the potential for foreign investment while also
    promoting innovation and research. Future artificial intelligence laws are expected
    to expand the regulation’s scope beyond generative AI. Given the potential penalties
    or shutdowns for noncompliant services operating in China, it is essential to
    ensure compliance.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: National Institute of Standards and Technology (NIST) AI Risk Management Framework
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'The NIST [Artificial Intelligence Risk Management Framework: Generative Artificial
    Intelligence Profile](https://oreil.ly/sISXu) released in July 2024 identifies
    12 key risks unique to or exacerbated by GenAI technologies that organizations
    should take care to identify and mitigate. These include risks related to chemical,
    biological, radiological, or nuclear information, dangerous or violent recommendations,
    data privacy, environmental impacts, information integrity and security, and intellectual
    property. The framework proposes over 400 actions organizations can take to manage
    these risks.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Institute of Electrical and Electronics Engineers Standards Association (IEEE
    SA) recommended practice
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: The IEEE’s [Recommended Practice for Organizational Governance of Artificial
    Intelligence](https://oreil.ly/Pd2qj) outlines governance criteria for AI development
    and use within organizations, including safety, transparency, accountability,
    responsibility, and bias minimization. It also provides steps for implementation,
    auditing, training, and compliance. The document aims to promote responsible AI
    practices by integrating internal governance structures with external AI governance
    instruments.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: These laws and frameworks collectively aim to regulate AI in a way that promotes
    innovation while ensuring safety, transparency, and respect for human rights,
    similar to the objectives of the EU AI Act.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The EU AI Act focuses on promoting the development and use of human-centered
    and trustworthy artificial intelligence, to safeguard health, safety, fundamental
    rights, democracy, the rule of law, and environmental protection within the European
    Union. In this first chapter, we examined the concept of trustworthiness in AI,
    which is based on three pillars: lawfulness, ethics, and robustness. I outlined
    the seven key requirements for trustworthy AI systems, which include human agency
    and oversight; technical robustness and safety; privacy and data governance; transparency;
    diversity, non-discrimination, and fairness; societal and environmental well-being;
    and accountability.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Act requires knowledge of its scope, who it applies to, and
    the established timeline for AI system compliance. The EU AI Act is a comprehensive
    risk-based legal framework that establishes rules for the development, distribution,
    and use of AI systems and GPAI models. It applies to businesses and individuals
    both within and outside the European Union, including providers, importers, distributors,
    authorized representatives, deployers, and users of AI systems.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: The Act classifies AI systems into four categories based on risk and establishes
    corresponding obligations. It prohibits AI systems that pose unacceptable risks,
    imposes strict requirements on high-risk systems, and mandates transparency and
    labeling for limited-risk systems. The Act also lays out a specific set of obligations
    for GPAI models, with additional rules for models deemed to pose systemic risk.
    These regulations apply not only to the AI systems themselves but to all organizations
    involved in their development and deployment across the AI value chain.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Classifying the AI systems in use in your organization and identifying the corresponding
    compliance requirements will help determine the scope of required data and AI
    governance measures, as well as the necessary AI engineering practices. In the
    next chapter, I’ll discuss the CRISP-ML(Q) structured AI system development process
    and MLOps, which provides technical and organizational best practices for AI system
    operationalization.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ali, Sajid, Tamer Abuhmed, Shaker El-Sappagh, Khan Muhammad, José M. Alonso-Moral,
    Roberto Confalonieri, Riccardo Guidotti, et al. “Explainable Artificial Intelligence
    (XAI): What We Know and What Is Left to Attain Trustworthy Artificial Intelligence.”
    *Information Fusion* 99 (November 2023): 101805\. [*https://oreil.ly/WERsh*](https://oreil.ly/WERsh).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'Braiek, Houssem Ben, and Foutse Khomh. “Machine Learning Robustness: A Primer.”
    arXiv preprint arXiv:2404.00897, May 2024\. [*https://oreil.ly/6iJoo*](https://oreil.ly/6iJoo).'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Caton, Simon, and Christian Haas. “Fairness in Machine Learning: A Survey.”
    *ACM Computing Surveys* 56, no. 7 (2024): 166\. [*https://oreil.ly/ykP9I*](https://oreil.ly/ykP9I).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: High-Level Expert Group on AI (AI HLEG). *The Assessment List for Trustworthy
    Artificial Intelligence (ALTAI) for Self-Assessment*. European Commission, July
    2020\. [*https://oreil.ly/QNzCW*](https://oreil.ly/QNzCW).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Li, Bo, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and
    Bowen Zhou. “Trustworthy AI: From Principles to Practices.” *ACM Computing Surveys*
    55, no. 9 (2023): 177\. [*https://oreil.ly/sAQ99*](https://oreil.ly/sAQ99).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Malhotra, Tanya. “Top Artificial Intelligence (AI) Governance Laws and Frameworks.”
    *Marktechpost*, May 2, 2024\. [*https://oreil.ly/OvYAI*](https://oreil.ly/OvYAI).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'Monteith, Scott, Tasha Glenn, John R. Geddes, Peter C. Whybrow, Eric Achtyes,
    and Micheal Bauer. “Artificial Intelligence and Increasing Misinformation.” *The
    British Journal of Psychiatry* 224, no. 2 (2024): 33–35\. [*https://oreil.ly/KgesS*](https://oreil.ly/KgesS).'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Novelli, Claudio, Mariarosaria Taddeo, and Luciano Floridi. “Accountability
    in Artificial Intelligence: What It Is and How It Works.” *AI & SOCIETY* 39 (2023):
    1871–82. [*https://oreil.ly/yEJ1b*](https://oreil.ly/yEJ1b).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'Radclyffe, Charless, Mafalda Ribeiro, and Robert H. Wortham. “The Assessment
    List for Trustworthy Artificial Intelligence: A Review and Recommendations.” *Frontiers
    in Artificial Intelligence* 6 (2023): 1020592\. [*https://oreil.ly/bmSIN*](https://oreil.ly/bmSIN).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Reinsel, David, John Gantz, and John Rydning. *Data Age 2025: The Evolution
    of Data to Life-Critical*. Seagate, April 2017\. [*https://oreil.ly/PYxM2*](https://oreil.ly/PYxM2).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Repetto, Marco. “Fostering Robust AI: Understanding Its Importance and Navigating
    the EU Artificial Intelligence Act.” CertX, June 12, 2023\. [*https://oreil.ly/U2WUK*](https://oreil.ly/U2WUK).'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Schwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. “Green AI.” *Communications
    of the ACM* 63, no. 12 (2020): 54–63\. [*https://oreil.ly/S7AJT*](https://oreil.ly/S7AJT).'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Studer, Stefan, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin, Ludwig
    Winkler, Steven Peters, and Klaus-Robert Mueller. “Towards CRISP-ML(Q): A Machine
    Learning Process Model with Quality Assurance Methodology.” arXiv preprint arXiv:2003.05155,
    February 2021\. [*https://oreil.ly/srSRa*](https://oreil.ly/srSRa).'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Turri, Violet. “What Is Explainable AI?” *SEI Blog* (Carnegie Mellon University
    Software Engineering Institute), January 17, 2022\. [*https://oreil.ly/pciy_*](https://oreil.ly/pciy_).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: Turri, Violet. “什么是可解释人工智能？” *SEI 博客* (卡内基梅隆大学软件工程研究所), 2022年1月17日\. [*https://oreil.ly/pciy_*](https://oreil.ly/pciy_).
- en: 'Wu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani,
    Kiwan Maeng, Gloria Chang, et al. “Sustainable AI: Environmental Implications,
    Challenges and Opportunities.” arXiv preprint arXiv:2111.00364, January 2022\.
    [*https://oreil.ly/vplRJ*](https://oreil.ly/vplRJ).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Wu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani,
    Kiwan Maeng, Gloria Chang, 等人. “可持续人工智能：环境影响、挑战与机遇。” arXiv 预印本 arXiv:2111.00364,
    2022年1月\. [*https://oreil.ly/vplRJ*](https://oreil.ly/vplRJ).
- en: ^([1](ch01.html#id333-marker)) For further reading on this and other topics
    introduced in this chapter, see the list of references at the end of the chapter.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.html#id333-marker)) 关于本章介绍的其他主题的进一步阅读，请参阅章节末尾的参考文献列表。
