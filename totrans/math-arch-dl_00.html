<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="chap-overview">1 An overview of machine learning and deep learning</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">A first look at machine learning and deep learning</li>
<li class="co-summary-bullet">A simple machine learning model: The cat brain</li>
<li class="co-summary-bullet">Understanding deep neural networks</li>
</ul>
<p class="body"><a id="marker-1"/>Deep learning has transformed computer vision, natural language and speech processing in particular, and artificial intelligence in general. From a bag of semi-discordant tricks, none of which worked satisfactorily on real-life problems, artificial intelligence has become a formidable tool to solve real problems faced by industry, at scale. This is nothing short of a revolution going on under our very noses. To lead the curve of this revolution, it is imperative to understand the underlying principles and abstractions rather than simply memorizing the “how-to” steps of some hands-on guide. This is where mathematics comes in.</p>
<p class="body">In this first chapter, we present an overview of deep learning. This will require us to use some concepts explained in subsequent chapters. Don’t worry if there are some open questions at the end of this chapter: it is aimed at orienting your mind toward this difficult subject. As individual concepts become clearer in subsequent chapters, you should consider coming back and re-reading this chapter.</p>
<h2 class="fm-head" id="sec-paradigm-shift">1.1 A first look at machine/deep learning: A paradigm shift in computation</h2>
<p class="body"><a id="marker-2"/>Making decisions and/or predictions is a central requirement of life. Doing so essentially involves taking in a set of sensory or knowledge inputs and processing them to generate decisions or estimates.</p>
<p class="body">For instance, a cat’s brain is often trying to choose between the following options:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">run away</i> from the object in front of it</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">ignore</i> the object in front of it </p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">approach</i> the object in front of it and purr.</p>
</li>
</ul>
<p class="body">The cat’s brain makes that decision by processing sensory inputs like the perceived <i class="fm-italics">hardness</i> of the object in front of it, the perceived <i class="fm-italics">sharpness</i> of the object in front of it, and so on. This is an instance of a <i class="fm-italics">classification</i> problem, where the output is one of a set of possible classes.</p>
<p class="body">Some other examples of classification problems in life are as follows:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Buy</i> vs. <i class="fm-italics">hold</i> vs. <i class="fm-italics">sell</i> a certain stock, from inputs like the <i class="fm-italics">price history of this stock</i> and the <i class="fm-italics">change in price of the stock in recent times</i></p>
</li>
<li class="fm-list-bullet">
<p class="list">Object recognition (from an image):</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">Is this a car or a giraffe?</p>
</li>
<li class="fm-list-bullet">
<p class="list">Is this a human or a non-human?</p>
</li>
<li class="fm-list-bullet">
<p class="list">Is this an inanimate object or a living object?</p>
</li>
<li class="fm-list-bullet">
<p class="list">Face recognition—is this Tom or Dick or Mary or Einstein or Messi?</p>
</li>
</ul>
</li>
<li class="fm-list-bullet">
<p class="list">Action recognition from a video:</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">Is this person running or not running?</p>
</li>
<li class="fm-list-bullet">
<p class="list">Is this person picking something up or not?</p>
</li>
<li class="fm-list-bullet">
<p class="list">Is this person doing something violent or not?</p>
</li>
</ul>
</li>
<li class="fm-list-bullet">
<p class="list">Natural language processing (NLP) from digital documents:</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">Does this news article belong to the realm of politics or sports?</p>
</li>
<li class="fm-list-bullet">
<p class="list">Does this query phrase match a particular article in the archive?</p>
</li>
</ul>
</li>
</ul>
<p class="body">Sometimes life requires a <i class="fm-italics">quantitative</i> estimation instead of a classification. A lion’s brain needs to estimate how far to jump so as to land on top of its prey, by processing inputs like</p>
<p class="body">Another instance of quantitative estimation is estimating a house’s price based on inputs like current income of the house’s owner, crime statistics for the neighborhood, and so on. Machines that make such quantitative estimators are called <i class="fm-italics">regressors</i>.</p>
<p class="body">Here are some other examples of quantitative estimations required in daily life:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Object localization from an image: identifying the rectangle bounding the location of an object</p>
</li>
<li class="fm-list-bullet">
<p class="list">Stock price prediction from historical stock prices and other world events</p>
</li>
<li class="fm-list-bullet">
<p class="list">Similarity score between a pair of documents</p>
</li>
</ul>
<p class="body">Sometimes a classification output can be generated from a quantitative estimate. For instance, the cat brain described earlier can combine the inputs (hardness, sharpness, and so on) to generate a quantitative threat score. If that threat score is high, the cat runs away. If the threat score is near zero, the cat ignores the object in front of it. If the threat score is negative, the cat approaches the object and purrs.</p>
<p class="body">Many of these examples are shown in figure <a class="url" href="#fig-decisions-quantestimates">1.1</a>. In each instance, a machine—that is, a brain—transforms sensory or knowledge inputs into decisions or quantitative estimates. The goal of machine learning is to emulate that machine.</p>
<p class="body"><a id="marker-3"/>Note that machine learning has a long way to go before it can catch up with the human brain. The human brain can single-handedly deal with thousands, if not millions, of such problems. On the other hand, at its present state of development, machine learning can hardly create a single general-purpose machine that makes a wide variety of decisions and estimates. We are mostly trying to make separate machines to solve individual tasks (such as a stock picker or a car recognizer).</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="827" id="fig-decisions-quantestimates" src="../../OEBPS/Images/CH01_F01_Chaudhury.png" width="823"/></p>
<p class="figurecaption">Figure 1.1 Examples of decision making and quantitative estimations in life</p>
</div>
<p class="body">At this point, you may ask, “Wait: converting inputs to outputs—isn’t that exactly what computers have been doing for the last 30 or more ears? What is this paradigm shift I am hearing about?” The answer is that it <i class="fm-italics">is</i> a paradigm shift because we do not provide a step-by-step instruction set—that is, a program—to the machine to convert the input to output. Instead, we develop a mathematical model for the problem.</p>
<p class="body">Let’s illustrate the idea with an example. For the sake of simplicity and concreteness, we will consider a hypothetical cat brain that needs to make only one decision in life: whether to <i class="fm-italics">run away from the object in front of it</i> or <i class="fm-italics">ignore the object</i> or <i class="fm-italics">approach and purr</i>. This decision, then, is the output of the model we will discuss. And in this toy example, the decision is made based on only two quantitative inputs (aka features): the perceived hardness and sharpness of the object (as depicted in figure <a class="url" href="#fig-decisions-quantestimates">1.1</a>). We do <i class="fm-italics">not</i> provide any step-by-step instructions such as “if sharpness greater than some threshold, then run away.” Instead, we try to identify a <i class="fm-italics">parameterized</i> function that takes the input and converts it to the desired decision or estimate. The simplest such function is a <i class="fm-italics">weighted sum of inputs</i>:<a id="marker-4"/></p>
<p class="fm-equation"><span class="math"><i class="fm-italics">y</i>(<i class="fm-italics">hardness</i>, <i class="fm-italics">sharpness</i>) = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> × <i class="fm-italics">hardness</i> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> × <i class="fm-italics">sharpness</i> + <i class="fm-italics">b</i></span></p>
<p class="body">The weights <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span> and the bias <i class="timesitalic">b</i> are the parameters of the function. The output <i class="timesitalic">y</i> can be interpreted as a threat score. If the threat score exceeds a threshold, the cat runs away. If it is close to <span class="math">0</span>, the cat ignores the object. If the threat score is negative, the cat approaches and purrs. For more complex tasks, we will use more sophisticated functions.</p>
<p class="body">Note that the weights are not known at first; we need to estimate them. This is done through a process called <i class="fm-italics">model training</i>.</p>
<p class="body">Overall, solving a problem via machine learning has the following stages:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">We design a parameterized model function (e.g., weighted sum) with unknown parameters (weights). This constitutes the <i class="fm-italics">model architecture</i>. Choosing the right model architecture is where the expertise of the machine learning engineer comes into play.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Then we estimate the weights via model training.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Once the weights are estimated, we have a complete <i class="fm-italics">model</i>. This model can take arbitrary inputs not necessarily seen before and generate outputs. The process in which a trained model processes an arbitrary real-life input and emits an output is called <i class="fm-italics">inferencing</i>.</p>
</li>
</ul>
<p class="body">In the most popular variety of machine learning, called <i class="fm-italics">supervised learning</i>, we prepare the training data before we commence training. Training data comprises <i class="fm-italics">example input items, each with its corresponding desired output</i>. <a class="url" href="#fn1" id="fnref1"><sup class="fm-superscript">1</sup></a> Training data is often created manually: a human goes over every single input item and produces the desired output (aka target output). This is usually the most arduous part of doing machine learning.</p>
<p class="body">For instance, in our hypothetical cat brain example, some possible training data items are as follows</p>
<p class="fm-equation"><span class="math">input: <i class="fm-italics">hardness</i> = 0.01, <i class="fm-italics">sharpness</i> = 0.02 → threat = —0.90 → <i class="fm-italics">decision</i>: “approach and purr”</span></p>
<p class="fm-equation"><span class="math">input: <i class="fm-italics">hardness</i> = 0.50, <i class="fm-italics">sharpness</i> = 0.60 → threat = 0.01   → <i class="fm-italics">decision</i>: “ignore”</span></p>
<p class="fm-equation"><span class="math">input: <i class="fm-italics">hardness</i> = 0.99, <i class="fm-italics">sharpness</i> = 0.97 → threat = 0.90   → <i class="fm-italics">decision</i>: “run away”</span></p><!--<div class="Equation"><p class="FM-Equation"><span class="times">$$\begin{aligned}
\begin{array}{l@{\hskip0.25em}c@{\hskip0.25em}l@{\hskip0.25em}c@{\hskip0.25em}l}
\displaystyle\text{input:}\left(\textit{hardness\kern0.4pt}=0.01,\textit{sharpness\kern0.4pt}= 0.02\right) &amp;\displaystyle\rightarrow
&amp;\displaystyle\text{threat} = -0.90 &amp;\displaystyle\rightarrow
&amp;\displaystyle \textit{decision:} \text{ ``approach and purr''}\\[0.5pt]
%\end{array}
%\end{align*}
%\begin{align*}
%\begin{array}{l@{\hskip0.25em}c@{\hskip0.25em}l@{\hskip0.25em}c@{\hskip0.25em}l}
\displaystyle \text{input:}\left(\textit{hardness\kern0.4pt}= 0.50,\textit{sharpness\kern0.4pt}=0.60\right) &amp;\displaystyle
\rightarrow
&amp;\displaystyle \text{threat} = 0.01 &amp;\displaystyle \rightarrow
&amp;\displaystyle \textit{decision:} \text{ ``ignore''}\\[0.5pt]
\displaystyle \text{input:}\left(\textit{hardness\kern0.4pt}= 0.99,\textit{sharpness\kern0.4pt}=0.97\right) &amp;\displaystyle
\rightarrow
&amp;\displaystyle \text{threat} = 0.90 &amp;\displaystyle \rightarrow
&amp;\displaystyle \textit{decision:} \text{ ``run away''}
\end{array}\end{aligned}$$</span></p>
</div>-->
<p class="body">where the input values of hardness and sharpness are assumed to lie between <span class="math">0</span> and <span class="math">1</span>.</p>
<p class="body"><a id="marker-5"/>What exactly happens during training? Answer: we iteratively process the input training data items. For each input item, we know the desired aka target) output. On each iteration, we adjust the model weight values in a way that the output of the model function on that specific input item gets at least a little closer to the corresponding target output. For instance, suppose at a given iteration, the weight values are <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub> = 20</span> and <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub> = 10</span>, and <span class="math"><i class="fm-italics">b</i> = 50</span>. On the input <span class="math">(<i class="fm-italics">hardness</i> = 0.01, <i class="fm-italics">sharpness</i> = 0.02)</span>, we get an output threat score <span class="math"><i class="fm-italics">y</i> = 50.3</span>, which is quite different from the desired <span class="math"><i class="fm-italics">y</i> = −0.9</span>. We will adjust the weights: for instance, reducing the bias so <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub> = 20</span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub> = 10</span>, and <span class="math"><i class="fm-italics">b</i> = 40</span>. The corresponding threat score <span class="math"><i class="fm-italics">y</i> = 40.3</span> is still nowhere near the desired value, but it has moved closer. After we do this on many training data items, the weights will start approaching their ideal values. Note that how to identify the adjustments to the weight values is not discussed here; it requires somewhat deeper math and will be discussed later.</p>
<p class="body">As stated earlier, this process of iteratively tuning weights is called <i class="fm-italics">training</i> or <i class="fm-italics">learning</i>. At the beginning of learning, the weights have random values, so the machine outputs often do not match desired outputs. But with time, more training iterations happen, and the machine “learns” to generate the correct output. That is when the model is ready for deployment in the real world. Given arbitrary input, the model will (hopefully) emit something close to the desired output during inferencing.</p>
<p class="body">Come to think of it, that is probably how living brains work. They contain equivalents of mathematical models for various tasks. Here, the weights are the strengths of the connections (aka synapses) between the different neurons in the brain. In the beginning, the parameters are untuned; the brain repeatedly makes mistakes. For example, a baby’s brain often makes mistakes in identifying edible objects—anybody who has had a child will know what we are talking about. But each example tunes the parameters (eating green and white rectangular things with a $ sign on them invites much scolding—should not eat them in the future, etc.). Eventually, this machine tunes its parameters to yield better results.</p>
<p class="body">One subtle point should be noted here. During training, the machine is tuning its parameters so that it produces the desired outcome—<i class="fm-italics">on the training data input only</i>. Of course, it sees only a small fraction of all possible inputs during training—we are <i class="fm-italics">not</i> building a lookup table from known inputs to known outputs. Hence, when this machine is released in the world, it mostly runs on input data it has never seen before. What guarantee do we have that it will generate the right outcome on never-before-seen data? Frankly, there is no guarantee. Only, in most real-life problems, the inputs are not really random. They have a pattern. Hopefully, the machine will see enough during training to capture that pattern. Then its output on unseen input will be close to the desired value. The closer the distribution of the training data is to real life, the more likely that becomes.</p>
<h2 class="fm-head" id="sec-func_approx_view">1.2 A function approximation view of machine learning:Models and their training</h2>
<p class="body"><a id="marker-6">A</a>s stated in section <a class="url" href="#sec-paradigm-shift">1.1</a>, to create a brain-like machine that makes classifications or estimations, we have to find a mathematical function (model) that transforms inputs into corresponding desired outputs. Sadly, however, in typical real-life situations, we do not know that transformation function. For instance, we do not know the function that takes in past prices, world events, and so on and estimates the future price of a stock—something that stops us from building a stock price estimator and getting rich. All we have is the training data—a set of inputs on which the output is known. How do we proceed, then? Answer: we will try to model the unknown function. This means we will create a function that will be a proxy or surrogate to the unknown function. Viewed this way, machine learning is nothing but function approximation—we are simply trying to approximate the unknown classification or estimation function.</p>
<p class="body">Let’s briefly recap the main ideas from the previous section. In machine learning, we try to solve problems that can be abstractly viewed as transforming a set of inputs to an output. The output is either a class or an estimated value. Since we do not know the true transformation function, we try to come up with a model function. We start by designing—using our physical understanding of the problem—a model function with tunable parameter values that can serve as a proxy for the true function. This is the <i class="fm-italics">model architecture</i>, and the tunable parameters are also known as <i class="fm-italics">weights</i>. The simplest model architecture is one where the output is a weighted sum of the input values. Determining the model architecture does not fully determine the model—we still need to determine the actual parameter values (weights). That is where <i class="fm-italics">training</i> comes in. During training, we find an optimal set of weights that transform the training inputs to outputs that match the corresponding training outputs as closely as possible. Then we deploy this machine in the world: its weights are estimated and the function is fully determined, so on any input, it simply applies the function and generates an output. This is called <i class="fm-italics">inferencing</i>. Of course, training inputs are only a fraction of all possible inputs, so there is no guarantee that inferencing will yield a desired result on all real inputs. The success of the model depends on the appropriateness of the chosen model architecture and the quality and quantity of training data.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Obtaining training data</p>
<p class="fm-sidebar-text">After mastering machine learning, the biggest struggle turns out to be the procurement of training data. When practitioners can afford it, it is common practice to use humans to hand-generate the outputs corresponding to the training data inputs (these target outputs are sometimes referred to as <i class="fm-italics">ground truth</i>). This process, known as <i class="fm-italics">human labeling</i> or <i class="fm-italics">human curation</i>, involves an army of human beings looking at a substantial number of training data inputs and producing the corresponding ground truth outputs. For some well-researched problems, we may be lucky enough to get training data on the internet; otherwise it becomes a daunting challenge. More on this later.</p>
</div>
<p class="body">Now, let’s study the process of model building with a concrete example: the cat brain machine shown in figure <a class="url" href="#fig-decisions-quantestimates">1.1</a>.</p>
<h2 class="fm-head" id="sec-cat_brain">1.3 A simple machine learning model: The cat brain</h2>
<p class="body"><a id="marker-7"/>For the sake of simplicity and concreteness, we will deal with a hypothetical cat that needs to make only one decision in life: whether to run away from the object in front of it, ignore it, or approach and purr. And it makes this decision based on only two quantitative inputs pertaining to the object in front of it (shown in figure <a class="url" href="#fig-decisions-quantestimates">1.1</a>).</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> This chapter is a lightweight overview of machine/deep learning. As such, it relies some on mathematical concepts that we will introduce later. You are encouraged to read this chapter now, nonetheless, and perhaps re-read it after digesting the chapters on vectors and matrices.</p>
<h3 class="fm-head1" id="input-features">1.3.1 Input features</h3>
<p class="body">The input features are <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub></span>, signifying <i class="fm-italics">hardness</i>, and <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">1</sub></span>, signifying <i class="fm-italics">sharpness</i>. Without loss of generality, we can <i class="fm-italics">normalize</i> the inputs. This is a pretty popular trick whereby the input values ranging between a minimum possible value <i class="timesitalic">v<sub class="fm-subscript">min</sub></i> and a maximum possible value <i class="timesitalic">v<sub class="fm-subscript">max</sub></i> are transformed to values between <span class="math">0</span> and <span class="math">1</span>. To transform an arbitrary input value <i class="timesitalic">v</i> to a normalized value <i class="timesitalic">v<sub class="fm-subscript">norm</sub></i>, we use the formula</p><!--<p class="Body"><span class="times">$$v_{norm}=\frac{\left(v - v_{min}\right)}{\left(v_{max} - v_{min}\right)}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_01-01.png" width="170"/></p>
</div>
<p class="fm-equation-caption">Equation 1.1 <span class="calibre" id="eq-normalization"/></p>
<p class="body">In mathematical parlance, transformation via equation <a class="url" href="#eq-normalization">1.1</a>, <span class="math"><i class="fm-italics">v</i> <span class="cambria">∈</span> [<i class="fm-italics">v<sub class="fm-subscript">min</sub></i>, <i class="fm-italics">v<sub class="fm-subscript">max</sub></i>] → <i class="fm-italics">v<sub class="fm-subscript">norm</sub></i> <span class="cambria">∈</span> [0,1]</span> maps the values <i class="timesitalic">v</i> from the input domain <span class="math">[<i class="fm-italics">v<sub class="fm-subscript">min</sub></i>, <i class="fm-italics">v<sub class="fm-subscript">max</sub></i>]</span> to the output values <i class="timesitalic">v<sub class="fm-subscript">norm</sub></i> in the range <span class="math">[0,1]</span>.</p>
<p class="body">A two-element vector <!--<span class="times">$\vec{x} =
\begin{bmatrix}
x_{0}\\x_{1}
\end{bmatrix} \in \left[0, 1\right]^{2}$</span>--><span class="infigure"><img alt="" class="calibre5" height="76" src="../../OEBPS/Images/eq_01-01-a.png" width="143"/></span> represents a single input instance succinctly.</p>
<h3 class="fm-head1" id="output-decisions">1.3.2 Output decisions</h3>
<p class="body">The final output is multiclass and can take one of three possible values: <i class="fm-italics">0</i>, implying running away from the object in front of the cat; <i class="fm-italics">1</i>, implying ignoring the object; and <i class="fm-italics">2</i>, implying approaching the object and purring. It is possible in machine learning to compute the class directly. However, in this example, we will have our model estimate a <i class="fm-italics">threat score</i>. It is interpreted as follows: threat high positive = run away, threat near zero = ignore, and threat high negative = approach and purr (negative threat is attractive).</p>
<p class="body">We can make a final multiclass run/ignore/approach decision based on threat score by comparing the threat score <i class="timesitalic">y</i> against a threshold <i class="timesitalic">δ</i>, as follows:</p><!--<p class="Body"><span class="times">$$y
\begin{cases}
&amp; &gt; \delta \rightarrow\text{\;\;\;high threat, run away}\\
&amp; &gt;= -\delta \text{\;\;and\;\;} &lt;= \delta
\rightarrow\text{\;\;\;threat close to zero, ignore}\\
&amp; &lt; -\delta \rightarrow\text{\;\;\;negative threat, approach and purr}
\end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="110" src="../../OEBPS/Images/eq_01-02.png" width="485"/></p>
</div>
<p class="fm-equation-caption">Equation 1.2 <span class="calibre" id="eq-threat-thresholding"/></p>
<h3 class="fm-head1" id="model-estimation">1.3.3 Model estimation</h3>
<p class="body"><a id="marker-8"/>Now for the all-important step: we need to estimate the function that transforms the input vector to the output. With slight abuse of terms, we will denote this function as well as the output by <i class="timesitalic">y</i>. In mathematical notation, we want to estimate <span class="math"><i class="timesitalic">y</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>.</p>
<p class="body">Of course, we do not know the ideal function. We will try to estimate this unknown function from the training data. This is accomplished in two steps:</p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Model architecture selection</i>—Designing a parameterized function that we expect is a good proxy or surrogate for the unknown ideal function</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Training</i>—Estimating the parameters of that chosen function such that the outputs on training inputs match corresponding outputs as closely as possible</p>
</li>
</ol>
<h3 class="fm-head1" id="model-architecture-selection">1.3.4 Model architecture selection</h3>
<p class="body">This is the step where various machine learning approaches differ from one another. In this toy cat brain example, we will use the simplest possible model. Our model has three parameters, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">b</i></span>. They can be represented compactly with a single two-element vector <!--<p class="Body"><span class="times">$\vec{w}=
\begin{bmatrix} w_{0}\\w_{1}
\end{bmatrix}
\in \mathbb{R}^{2}$</span></p>--><span class="infigure"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_01-02-a2.png" width="114"/></span> and a constant bias <span class="math"><i class="fm-italics">b</i> <span class="cambria">∈</span> ℝ</span> (here, <span class="math">ℝ</span> denotes the set of all real numbers, <span class="math">ℝ<sup class="fm-superscript">2</sup></span> denotes the set of 2D vectors with both elements real, and so on). It emits the threat score, <i class="timesitalic">y</i>, which is computed as <!--<p class="Body"><span class="times">$$y\left(x_{0}, x_{1}\right) = w_{0} x_{0} + w_{1} x_{1} + b =
\begin{bmatrix} w_{0} &amp;w_{1}
\end{bmatrix}
\begin{bmatrix}
x_{0}\\x_{1}
\end{bmatrix} + b
=\vec{w}^{T}\vec{x}+ b$$</span></p>--></p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="76" src="../../OEBPS/Images/eq_01-03.png" width="468"/></p>
</div>
<p class="fm-equation-caption">Equation 1.3 <span class="calibre" id="eq-linear-predictor"/></p>
<p class="body">Note that <i class="timesitalic">b</i> is a slightly special parameter. It is a constant that does not get multiplied by any of the inputs. It is common practice in machine learning to refer to it as <i class="fm-italics">bias</i>; the other parameters are multiplied by inputs as weights.</p>
<h3 class="fm-head1" id="model-training">1.3.5 Model training</h3>
<p class="body">Once the model architecture is chosen, we know the exact parametric function we are going to use to model the unknown function <i class="timesitalic">y</i><span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> that transforms inputs to outputs. We still need to estimate the function’s parameters. Thus, we have a function with unknown parameters, and the parameters are to be estimated from a set of inputs with known outputs (training data). We will choose the parameters so that the outputs on the training data inputs match the corresponding outputs as closely as possible.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Iterative training</p>
<p class="fm-sidebar-text">This problem has been studied by mathematicians and is known as a <i class="fm-italics">function-fitting</i> problem in mathematics. What changed with the advent of machine learning, however, is the sheer scale. In machine learning, we deal with training data comprising millions and millions of items. This altered the philosophy of the solution. Mathematicians use a <i class="fm-italics">closed-form solution</i>, where the parameters are estimated by directly solving equations involving <i class="fm-italics">all</i> the training data items together. In machine learning, we go for iterative solutions, dealing with a <i class="fm-italics">few</i> training data items (or perhaps only one) at a time. In the iterative solution, there is no need to hold all the training data in the computer’s memory. We simply load small portions of it at a time and deal with only that portion. We will exemplify this with our cat brain example.</p>
</div>
<p class="body">Concretely, the goal of the training process is to estimate the parameters <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span>, <i class="timesitalic">b</i> or, equivalently, the vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> along with constant <i class="timesitalic">b</i> from equation <a class="url" href="../Text/01.xhtml#eq-linear-predictor">1.3</a> in such a way that the output <i class="timesitalic">y</i><span class="math">(<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">x</i><sub class="fm-subscript">1</sub>)</span> on the training data input <span class="math">(<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">x</i><sub class="fm-subscript">1</sub>)</span> matches the corresponding known training data outputs (aka ground truth [GT]) as much as possible.<a id="marker-9"/></p>
<p class="body">Let the training data consist of <span class="math"><i class="fm-italics">N</i> + 1</span> inputs <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(0)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>, ⋯ <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">N</i>)</sup></span>. Here, each <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> is a <span class="math">2 × 1</span> vector denoting a single training data input instance. The corresponding desired threat values (outputs) are <span class="math"><i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">gt</sub></i><sup class="fm-superscript">(0)</sup>, <i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">gt</sub></i><sup class="fm-superscript">(1)</sup>, ⋯ <i class="timesitalic">y<sub class="fm-subscript">gt</sub></i><sup class="fm-superscript">(<i class="fm-italics1">N</i>)</sup></span>, say (here, the subscript <i class="timesitalic">gt</i> denotes ground truth). Equivalently, we can say that the training data consists of <span class="math"><i class="fm-italics">N</i> + 1</span> <span class="math">(input, output)</span> pairs:</p>
<p class="fm-equation"><span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(0)</sup>, <i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">gt</sub></i><sup class="fm-superscript">(0)</sup>), (<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>, <i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">gt</sub></i><sup class="fm-superscript">(1)</sup>)⋯(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">N</i>)</sup>, <i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">gt</sub></i><sup class="fm-superscript">(<i class="fm-italics1">N</i>)</sup>)</span></p>
<p class="body">Suppose <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> denotes the (as-yet-unknown) optimal parameters for the model. Then, given an arbitrary input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, the machine will estimate a threat value of <span class="math"><i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">predicted</sub></i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i></span>. On the <i class="timesitalic">i<sub class="fm-subscript">th</sub></i> training data pair, <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>, <i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">gt</sub></i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)</span> the machine will estimate</p>
<p class="fm-equation"><span class="math"><i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">predicted</sub></i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> + <i class="fm-italics">b</i></span></p>
<p class="body">while the desired output is <span class="math"><i class="timesitalic">y<sub class="fm-subscript">gt</sub></i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>. Thus the squared error (aka loss) made by the machine on the <i class="timesitalic">i<sub class="fm-subscript">th</sub></i> training data instance is <a class="url" href="#fn2" id="fnref2"><sup class="fm-superscript">2</sup></a></p>
<p class="fm-equation"><span class="math"><i class="fm-italics">e<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">2</sup> = (<i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">predicted</sub></i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>−<i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">gt</sub></i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)<sup class="fm-superscript">2</sup></span></p>
<p class="body">The overall loss on the entire training data set is obtained by adding the loss from each individual training data instance:</p><!--<p class="Body"><span class="times">$$E^{2} = \sum_{i=0}^{i=N} e_{i}^{2} =
\sum_{i=0}^{i=N} \left(y_{predicted}^{\left(i\right)} - _{gt}^{\left(i\right)}\right)^{2}
  = \sum_{i=0}^{i=N} \left(\vec{w}^{T}\vec{x}_{i} + b - _{gt}^{\left(i\right)}\right)^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_01-03-a.png" width="470"/></p>
</div>
<p class="body">The goal of training is to find the set of model parameters (aka weights), <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, that minimizes the total error <i class="timesitalic">E</i>. Exactly how we do this will be described later.</p>
<p class="body">In most cases, it is not possible to come up with a closed-form solution for the optimal <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span>. Instead, we take an iterative approach depicted in algorithm <a class="url" href="../Text/01.xhtml#alg-supervised_training">1.1</a>.</p>
<div class="calibre3">
<p class="fm-algorithm-caption" id="alg-supervised_training">Algorithm 1.1 Training a supervised model</p><!--<p class="algorithm-body"><code class="FM-Code-in-Text"> Initialize parameters \vec{w}, b with random values \triangleright iterate while error not small enough instances \triangleright details provided in section [sec-grad] after gradients are introducedAdjust \vec{w}, b so that E^{2} is reduced \triangleright remember the final parameter values as optimal \vec{w}_{*} \leftarrow \vec{w}, b_{*} \leftarrow b </code></p>-->
<p class="algorithm-body-a">Initialize parameters <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> with random values</p>
<p class="algorithm-body-a"><span class="segoe">⊳</span> iterate while error not small enough</p>
<p class="algorithm-body-a"><b class="fm-bold">while</b> <span class="math">(<i class="fm-italics">E</i><sub class="fm-subscript">2</sub> = Σ<sub class="fm-subscript"><i class="fm-italics1">i</i> = 0</sub><sup class="fm-superscript">i=<i class="fm-italics1">N</i></sup> (<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">i</sub> ¸ <i class="fm-italics">b</i> — <i class="fm-italics">y<sub class="fm-subscript">gt</sub></i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)<sup class="fm-superscript">2</sup> &gt; <i class="fm-italics">threshold</i>)</span> <b class="fm-bold">do</b></p>
<p class="algorithm-body-b"><span class="segoe">⊳</span> iterate over all training data instances</p>
<p class="algorithm-body-b"><b class="fm-bold">for</b> <span class="cambria">∀</span><span class="math"><i class="fm-italics"><sub class="fm-subscript">i</sub></i> <span class="cambria">∈</span> 2 [0, <i class="fm-italics">N</i>]</span> <b class="fm-bold">do</b></p>
<p class="algorithm-body-c"><span class="segoe">⊳</span> details provided in section 3.3 after gradients are introduced</p>
<p class="algorithm-body-c">Adjust <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="calibre11">b</i></span> so that <span class="math"><i class="calibre11">E</i><sup class="fm-superscript">2</sup></span> is reduced</p>
<p class="algorithm-body-b"><b class="fm-bold">end</b> <b class="fm-bold">for</b></p>
<p class="algorithm-body-a"><b class="fm-bold">end</b> <b class="fm-bold">while</b></p>
<p class="algorithm-body-a"><span class="segoe">⊳</span> remember the final parameter values as optimal</p>
<p class="algorithm-body-a"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><sub class="fm-subscript">*</sub>← <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i><sub class="fm-subscript">*</sub>← <i class="fm-italics">b</i></span></p>
</div>
<p class="body"><a id="marker-10"/>In this algorithm, we start with random parameter values and keep tuning the parameters so the total error goes down at least a little. We keep doing this until the error becomes sufficiently small.</p>
<p class="body">In a purely mathematical sense, we continue the iterations until the error is minimal. But in practice, we often stop when the results are accurate enough for the problem being solved. It is worth re-emphasizing that <i class="fm-italics">error</i> here refers only to error on training data.</p>
<h3 class="fm-head1" id="inferencing">1.3.6 Inferencing</h3>
<p class="body">Finally, a trained machine (with optimal parameters <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><span class="math"><sub class="fm-subscript">*</sub>, <i class="fm-italics">b</i><sub class="fm-subscript">*</sub></span> is deployed in the world. It will receive new inputs <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> and will infer <i class="timesitalic">y</i><span class="math"><i class="fm-italics"><sub class="fm-subscript">predicted</sub></i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><sub class="fm-subscript">*</sub><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> <span class="math">+ <i class="fm-italics">b</i><sub class="fm-subscript">*</sub></span></span>. Classification will happen by thresholding <span class="math"><i class="timesitalic">y</i><i class="fm-italics"><sub class="fm-subscript">predicted</sub></i></span>, as shown in equation <a class="url" href="#eq-threat-thresholding">1.2</a>.</p>
<h2 class="fm-head" id="sec-geom-view-ml">1.4 Geometrical view of machine learning</h2>
<p class="body">Each input to the cat brain model is an array of two numbers: <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub></span> (signifying hardness of the object), <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">1</sub></span> signifying sharpness of the object) or, equivalently, a <span class="math">2 × 1</span> vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. A good mental picture is to think of the input as a point in a high-dimensional space. The input space is often called the <i class="fm-italics">feature space</i>—a space where all the characteristic features to be examined by the model are represented. The feature space dimension is two in this case, but in real-life problems it will be in the hundreds or thousands or more. The exact dimensionality of the input changes from problem to problem, but the intuition that it is a point remains.</p>
<p class="body">The output <i class="timesitalic">y</i> should also be viewed as a point in another high-dimensional space. In this toy problem, the dimensionality of the output space is one, but in real problems, it will be higher. Typically, however, the number of output dimensions is much smaller than the number of input dimensions.</p>
<p class="body">Geometrically speaking, a machine learning model essentially maps a point in the feature space to a point in the output space. It is expected that the classification or estimation job to be performed by the model is easier in the output space than in the feature space. In particular, <i class="fm-italics">for a classification job, input points belonging to separate classes are expected to map to separate clusters in output space</i>.</p>
<p class="body">Let’s continue with our example cat brain model to illustrate the idea. As stated earlier, our feature space is 2D, with two coordinate axes <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub></span> signifying hardness and <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span> signifying sharpness.<a class="url" href="#fn3" id="fnref3"><sup class="fm-superscript">3</sup></a> Individual points in this 2D space are denoted by coordinate values <span class="math">(<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">x</i><sub class="fm-subscript">1</sub>)</span> in lowercase (see figure <a class="url" href="#fig-geometrical_view">1.2</a>). As shown in the diagram, a good way to model the threat score is to measure the distance from line <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">1</sub> = 1</span>.<a id="marker-11"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="636" id="fig-geometrical_view" src="../../OEBPS/Images/CH01_F02_Chaudhury.png" width="729"/></p>
<p class="figurecaption">Figure 1.2 2D input point space for the cat brain model. The bottom-left corner shows objects with low hardness and low sharpness objects (–), while the top-right corner shows objects with high hardness and high sharpness (+). Intermediate values are near the diagonal ($).</p>
</div>
<p class="body">From coordinate geometry, in a 2D space with coordinate axes <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub></span> and <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span>, the signed distance of a point <span class="math">(<i class="fm-italics">a</i>, <i class="fm-italics">b</i>)</span> from the line <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">1</sub> = 1</span> is <span class="math"><i class="fm-italics">y</i> = (<i class="fm-italics">a</i>+<i class="fm-italics">b</i>–1)/√2</span>. Examining the sign of <i class="timesitalic">y</i>, we can determine which side of the separator line the input point belongs to. In the simple situation depicted in figure <a class="url" href="#fig-geometrical_view">1.2</a>, observation tells us that the threat score can be proxied by the signed distance, <i class="timesitalic">y</i>, from the diagonal line <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">1</sub> – 1 = 0</span>. We can make the run/ignore/approach decision by thresholding <i class="timesitalic">y</i>. Values close to zero imply ignore, positive values imply run away, and negative values imply approach and purr. From high school geometry, the distance of an arbitrary input point <span class="math">(<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>=<i class="fm-italics">a</i>, <i class="fm-italics">x</i><sub class="fm-subscript">1</sub>=<i class="fm-italics">b</i>)</span> from line <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">1</sub> – 1 = 0</span> is <span class="math">(<i class="fm-italics">a</i>+<i class="fm-italics">b</i>–1)/√2</span>. Thus, the function <span class="math"><i class="fm-italics">y</i>(<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">x</i><sub class="fm-subscript">1</sub>) = (<i class="fm-italics">x</i><sub class="fm-subscript">0</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">1</sub>–1)/√2</span> is a possible model for the cat brain threat estimator function. Training should converge to <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub> = 1/√2</span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub> = 1/√2</span> and <span class="math"><i class="fm-italics">b</i> = –1/√2</span>.</p>
<p class="body">Thus, our simplified cat brain threat score model is</p><!--<p class="Body"><span class="times">$$y\left(x_{0}, x_{1}\right) =
\frac{1}{\sqrt{2}}x_{0} + \frac{1}{\sqrt{2}}x_{1} -
\frac{1}{\sqrt{2}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="62" src="../../OEBPS/Images/eq_01-04.png" width="266"/></p>
</div>
<p class="fm-equation-caption">Equation 1.4 <span class="calibre" id="eq-cat-brain-model"/></p>
<p class="body"><a id="marker-12"/>It maps the 2D input points, signifying the hardness and sharpness of the object in front of the cat, to a 1D value corresponding to the signed distance from a separator line. This distance, physically interpretable as a threat score, makes it possible to separate the classes (negative threat, neutral, positive threat) via thresholding, as shown in equation <a class="url" href="#eq-threat-thresholding">1.2</a>. The separate classes form distinct clusters in the output space, depicted by +, –, and $ signs in the output space. Low values of inputs produce negative threats (the cat will approach and purr): for example, <span class="math"><i class="fm-italics">y</i>(0, 0) = –1/√2</span>. High values of inputs produce high threats (the cat will run away): for example, <span class="math"><i class="fm-italics">y</i>(1, 1) = 1/√2</span>. Medium values of inputs produce near-zero threats (the cat will ignore the object): for example, <span class="math"><i class="fm-italics">y</i>(0.5, 0.5) = 0</span>. Of course, because the problem is so simple, we could come up with the model parameters via simple observation. In real-life situations, this will need training.</p>
<p class="body">The geometric view holds in higher dimensions, too. In general, an <i class="timesitalic">n</i>-dimensional input vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is mapped to an <i class="timesitalic">m</i>-dimensional output vector (usually <span class="math"><i class="fm-italics">m</i> &lt; <i class="fm-italics">n</i></span>) in such a way that the problem becomes much simpler in the output space. An example with 3D feature space is shown in figure <a class="url" href="#fig-ml-as-mapping">1.3</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre12" height="543" id="fig-ml-as-mapping" src="../../OEBPS/Images/CH01_F03_Chaudhury.png" width="936"/></p>
<p class="figurecaption">Figure 1.3 A model maps the points from input (feature) space to an output space where it is easier to separate the classes. For instance, in this figure, input feature points belonging to two classes, red (+) and green (–) are distributed over the volume of a cylinder in a 3D feature space. The model unfurls the cylinder into a rectangle. The feature points are mapped onto a 2D planar output space where the two classes can be discriminated with a simple linear separator.<a id="marker-13"/></p>
</div>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="308" id="fig-non_linear_separator" src="../../OEBPS/Images/CH01_F04_Chaudhury.png" width="419"/></p>
<p class="figurecaption">Figure 1.4 The two classes (indicated by light and dark shades) cannot be separated by a line. A curved separator is needed. In 3D, this is equivalent to saying that no plane can separate the surfaces; a curved surface is necessary. In still higher-dimensional spaces, this is equivalent to saying that no hyperplane can separate the classes; a curved is needed.</p>
</div>
<h2 class="fm-head" id="sec-regression-vs-classification">1.5 Regression vs. classification in machine learning</h2>
<p class="body">As briefly outlined in section <a class="url" href="#sec-paradigm-shift">1.1</a>, there are two types of machine learning models: <i class="fm-italics">regressors</i> and <i class="fm-italics">classifiers</i>.</p>
<p class="body">In a <i class="fm-italics">regressor</i>, the model tries to emit a desired value given a specific input. For instance, the first stage (threat-score estimator) of the cat brain model in section <a class="url" href="../Text/01.xhtml#sec-cat_brain">1.3</a> is a regressor model.</p>
<p class="body">Classifiers, on the other hand, have a set of prespecified classes. Given a specific input, they try to emit the <i class="fm-italics">class</i> to which the input belongs. For instance, the full cat brain model has three classes: 1) run away, (2) ignore, and (3) approach and purr. Thus, it takes an input (hardness and sharpness values) and emits an output decision (aka class).</p>
<p class="body">In this example, we convert a regressor into a classifier by thresholding the output of the regressor (see equation <a class="url" href="#eq-threat-thresholding">1.2</a>). It is also possible to create models that directly output the class without having an intervening regressor.</p>
<h2 class="fm-head" id="sec-non-linearity">1.6 Linear vs. nonlinear models</h2>
<p class="body">In figure <a class="url" href="#fig-geometrical_view">1.2</a> we faced a rather simple situation where the classes could be separated by a line (a hyperplane in higher-dimensional surfaces). This does not happen often in real life. What if the points belonging to different classes are as shown in figure <a class="url" href="#fig-non_linear_separator">1.4</a>? In such cases, our model architecture should no longer be a simple weighted combination. It is a nonlinear function. For instance, check the curved separator in figure <a class="url" href="#fig-non_linear_separator">1.4</a>. Nonlinear models make sense from the function approximation point of view as well. Ultimately, our goal is to approximate very complex and highly nonlinear functions that model the classification or estimation processes demanded by life. Intuitively, it seems better to use <i class="fm-italics">nonlinear functions</i> to model them.</p>
<p class="body">A very popular nonlinear function in machine learning is the <i class="fm-italics">sigmoid</i> function, so named because it looks like the letter <i class="fm-italics">S</i>. The sigmoid function is typically symbolized by the Greek letter <i class="timesitalic">σ</i>. It is defined as</p><!--<p class="Body"><span class="times">$$\sigma\left(x\right) =\frac{1}{1+e^{-x}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="50" src="../../OEBPS/Images/eq_01-05.png" width="146"/></p>
</div>
<p class="fm-equation-caption">Equation 1.5 <span class="calibre" id="eq-sigmoid"/></p>
<p class="body"><a id="marker-14"/>The graph of the sigmoid function is shown in figure <a class="url" href="#fig-sigmoid">1.5</a>. Thus we can use the following popular model architecture (still kind of simple) that takes the sigmoid without parameters) of the weighted sum of the inputs:</p><!--<p class="Body"><span class="times"><span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_y.png" /></span> = <i class="fm-italics">σ</em>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_w.png" /></span><i class="fm-italics"><sup class="FM-Superscript">T</sup></em><span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_x.png" /></span> + <i class="fm-italics">b</em>)</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_01-06.png" width="146"/></p>
</div>
<p class="fm-equation-caption">Equation 1.6 <span class="calibre" id="eq-logistic-regression"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre13" height="500" id="fig-sigmoid" src="../../OEBPS/Images/CH01_F05_Chaudhury.jpg" width="750"/></p>
<p class="figurecaption">Figure 1.5 The sigmoid graph</p>
</div>
<p class="body">The sigmoid imparts the nonlinearity. This architecture can handle relatively more complex classification tasks than the weighted sum alone. In fact, equation <a class="url" href="#eq-logistic-regression">1.6</a> depicts the basic building block of a neural network.</p>
<h2 class="fm-head" id="sec-multi-layered-nn">1.7 Higher expressive power through multiple nonlinear layers: Deep neural networks</h2>
<p class="body">In section <a class="url" href="#sec-non-linearity">1.6</a> we stated that adding nonlinearity to the basic weighted sum yielded a model architecture that is able to handle more complex tasks. In machine learning parlance, the nonlinear model has more <i class="fm-italics">expressive power</i>.</p>
<p class="body">Now consider a real-life problem: say, building a dog recognizer. The input space comprises pixel locations and pixel colors (<span class="math"><i class="fm-italics">x</i>, <i class="timesitalic">y</i>, <i class="fm-italics">r</i>, <i class="fm-italics">g</i>, <i class="fm-italics">b</i></span>, where <span class="math"><i class="fm-italics">r</i>, <i class="fm-italics">g</i>, <i class="fm-italics">b</i></span> denote the red, green, and blue components of a pixel color). The input dimensionality is large (proportional to the number of pixels in the image). Figure <a class="url" href="#fig-dog_images">1.6</a> gives a small glimpse of the possible variations in background and foreground that a typical deep learning system (such as a dog image recognizer) has to deal with.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="430" id="fig-dog_images" src="../../OEBPS/Images/CH01_F06abcd_Chaudhury.jpg" width="625"/></p>
<p class="figurecaption">Figure 1.6 A glimpse into background and foreground variations that a typical deep learning system (here, a dog image recognizer) has to deal with</p>
</div>
<p class="body">We need a machine with really high expressive power here. How do we create such a machine in a principled way?</p>
<p class="body">Instead of generating the output from input in a single step, how about taking a cascaded approach? We will generate a set of intermediate or hidden outputs from the inputs, where each hidden output is essentially a single logistic regression unit. Then we add another layer that takes the output of the previous layer as input, and so on. Finally, we combine the outermost hidden layer outputs into the grand output.</p>
<p class="body"/>
<p class="body">We describe the system in the following equations. Note that we have added a superscript to the weights to identify the layer (layer 0 is closest to the input; layer <i class="timesitalic">L</i> is the last layer, furthest from the input). We have also made the subscripts twodimensional (so the weights for a given layer become a matrix). The first subscript identifies the destination node, and the second subscript identifies the source node (see figure 1.7).<a id="marker-15"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="518" id="fig-deep-neural-network" src="../../OEBPS/Images/CH01_F07_Chaudhury.png" width="999"/></p>
<p class="figurecaption">Figure 1.7 Multilayered neural network</p>
</div>
<p class="body">The astute reader may notice that the following equations do <i class="fm-italics">not</i> have an explicit bias term. That is because, for simplicity of notation, we have rolled it into the set of weights and assumed that one of the inputs (say, <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub> = 1</span>) and the corresponding weight (such as <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span>) is the bias.</p>
<p class="body">Layer 0: generates <span class="math"><i class="fm-italics">n</i><sub class="fm-subscript">0</sub></span> hidden outputs from <span class="math"><i class="fm-italics">n</i> + 1</span> inputs</p><!--<p class="Body"><span class="times">$$\begin{aligned} h_{0}^{\left(0\right)} &amp;= \sigma\left(w_{00}^{\left(0\right)}x_{0} + w_{01}^{\left(0\right)}x_{1} + \cdots w_{0n}^{\left(0\right)}x_{n}\right) \nonumber \\[4pt] h_{1}^{\left(0\right)} &amp;= \sigma\left(w_{10}^{\left(0\right)}x_{0} + w_{11}^{\left(0\right)}x_{1} + \cdots w_{1n}^{\left(0\right)}x_{n}\right) \nonumber \\[2pt]
\vdots &amp; \nonumber \\[2pt] h_{n_{0}}^{\left(0\right)} &amp;=
\sigma\left(w_{n_{0}0}^{\left(0\right)}x_{0} + w_{n_{0}1}^{\left(0\right)}x_{1} + \cdots w_{n_{0}n}^{\left(0\right)}x_{n}\right)
\label{eq-multi-layered-nn-layer0}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="187" src="../../OEBPS/Images/eq_01-07.png" width="461"/></p>
</div>
<p class="fm-equation-caption">Equation 1.7 <span class="calibre" id="eq-multi-layered-nn-layer0"/></p>
<p class="body">Layer 1: generates <span class="math"><i class="fm-italics">n</i><sub class="fm-subscript">1</sub></span> hidden outputs from <span class="math"><i class="fm-italics">n</i><sub class="fm-subscript">0</sub></span> hidden outputs from layer<a id="marker-16"/></p><!--<p class="Body"><span class="times">0</span> <span class="times">$$\begin{aligned} h_{0}^{\left(1\right)} &amp;=
\sigma\left(w_{00}^{\left(1\right)}h_{0}^{\left(0\right)} + w_{01}^{\left(1\right)}h_{1}^{\left(0\right)} + \cdots w_{0n_{0}}^{\left(1\right)}h_{n_{0}}^{\left(0\right)}\right) \nonumber
\\[4pt] h_{1}^{\left(1\right)} &amp;=
\sigma\left(w_{10}^{\left(1\right)}h_{0}^{\left(0\right)} + w_{11}^{\left(1\right)}h_{1}^{\left(0\right)} + \cdots w_{1n_{0}}^{\left(1\right)}h_{n_{0}}^{\left(0\right)}\right) \nonumber
\\[2pt]
\vdots &amp; \nonumber \\[2pt] h_{n_{1}}^{\left(1\right)} &amp;=
\sigma\left(w_{n_{1}0}^{\left(1\right)}h_{0}^{\left(0\right)} + w_{n_{1}1}^{\left(1\right)}h_{1}^{\left(0\right)} + \cdots w_{n_{1}n_{0}}^{\left(1\right)}h_{n_{0}}^{\left(0\right)}\right)
\label{eq-multi-layered-nn-layer1}\end{aligned}$$</span> <span class="times">⋯</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="222" src="../../OEBPS/Images/eq_01-08.png" width="461"/></p>
</div>
<p class="fm-equation-caption">Equation 1.8 <span class="calibre" id="eq-multi-layered-nn-layer1"/></p>
<p class="body">Final layer (<i class="timesitalic">L</i>): generates <span class="times"><i class="fm-italics">m</i> + 1</span> visible outputs from <span class="times"><i class="fm-italics">n</i><sub class="fm-subscript"><i class="fm-italics">L</i> − 1</sub></span> previous layer hidden outputs</p><!--<span class="times">$$\begin{aligned} h_{0}^{\left(L\right)} &amp;=
\sigma\left(w_{00}^{\left(L\right)}h_{0}^{\left(L-1\right)} + w_{01}^{\left(L\right)}h_{1}^{\left(L-1\right)} + \cdots w_{0n_{L-1}}^{\left(L\right)}h_{n_{L-1}}^{\left(L-1\right)}\right)
\nonumber\\
%\end{eqnarray}
%\begin{eqnarray} h_{1}^{\left(L\right)} &amp;=
\sigma\left(w_{10}^{\left(L\right)}h_{0}^{\left(L-1\right)} + w_{11}^{\left(L\right)}h_{1}^{\left(L-1\right)} + \cdots w_{1n_{L-1}}^{\left(L\right)}h_{n_{L-1}}^{\left(L-1\right)}\right)
\nonumber \\
\vdots &amp; \nonumber \\ h_{m}^{\left(L\right)} &amp;=
\sigma\left(w_{m0}^{\left(L\right)}h_{0}^{\left(L-1\right)} + w_{m1}^{\left(L\right)}h_{1}^{\left(L-1\right)} + \cdots w_{mn_{L-1}}^{\left(L\right)}h_{n_{L-1}}^{\left(L-1\right)}\right)
\label{eq-multi-layered-nn-layerL}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="173" src="../../OEBPS/Images/eq_01-09.png" width="461"/></p>
</div>
<p class="fm-equation-caption">Equation 1.9 <span class="calibre" id="eq-multi-layered-nn-layerL"/></p>
<p class="body">These equations are shown in figure <a class="url" href="#fig-deep-neural-network">1.7</a>. The machine depicted in figure <a class="url" href="#fig-deep-neural-network">1.7</a> can be incredibly powerful, with huge expressive power. We can adjust its expressive power systematically to fit the problem at hand. It then is a neural network. We will devote the rest of the book to studying this.</p>
<h2 class="fm-head" id="summary">Summary</h2>
<p class="body">In this chapter, we gave an overview of machine learning, leading all the way up to deep learning. The ideas were illustrated with a toy cat brain example. Some mathematical notions (e.g., vectors) were used in this chapter without proper introduction, and you are encouraged to revisit this chapter after vectors and matrices have been introduced.</p>
<p class="body">We would like to leave you with the following mental pictures from this chapter:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Machine learning is a fundamentally different paradigm of computing. In traditional computing, we provide a step-by-step instruction sequence to the computer, telling it what to do. In machine learning, we build a mathematical model that tries to approximate the unknown function that generates a classification or estimation from inputs.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The mathematical nature of the model function is stipulated from the physical nature and complexity of the classification or estimation task. Models have parameters. Parameter values are estimated from training data—inputs with known outputs. The parameter values are optimized so that the model output is as close as possible to training outputs on training inputs.</p>
</li>
<li class="fm-list-bullet">
<p class="list">An alternative geometric view of a machine is a transformation that maps points in the multidimensional input space to a point in the output space.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The more complex the classification/estimation task, the more complex the approximating function. In machine learning parlance, complex tasks need machines with greater expressive power. Higher expressive power comes from nonlinearity (e.g., the sigmoid function; see equation <a class="url" href="../Text/01.xhtml#eq-sigmoid">1.5</a>) and a layered combination of simpler machines. This takes us to deep learning, which is nothing but a multilayered nonlinear machine.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Complex model functions are often built by combining simpler basis functions.</p>
</li>
</ul>
<p class="body"><a id="marker-17"/>Tighten your seat belts: the fun is about to get more intense.</p>
<hr class="calibre14"/>
<p class="fm-footnote" id="fn1"><sup class="footnotenumber">1</sup>  If you have some experience with machine learning, you will realize that we are talking about “supervised” learning here. There are also machines that do not need known outputs to learn—so-called “unsupervised” machines—and we will talk about them later. <a class="url" href="#fnref1">↩</a></p>
<p class="fm-footnote" id="fn2"><sup class="footnotenumber">2</sup>  In this context, note that it is a common practice to square the error/loss to make it sign independent. If we desire an output of, say, <span class="math">10</span>, we are equally happy/unhappy if the output is <span class="math">9.5</span> or <span class="math">10.5</span>. Thus, an error of <span class="math">+ 5</span> or <span class="math">−5</span> is effectively the same; hence we make the error sign independent. <a class="url" href="#fnref2">↩</a></p>
<p class="fm-footnote" id="fn3"><sup class="footnotenumber">3</sup>  We use <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span> as coordinate symbols instead of the more familiar <i class="timesitalic">X</i>, <i class="timesitalic">Y</i> so as not to run out of symbols when going to higher-dimensional spaces. <a class="url" href="#fnref3">↩</a></p>
<p class="fm-footnote" id="fn4"><sup class="footnotenumber">4</sup>  In mathematics, vectors can have an infinite number of elements. Such vectors cannot be expressed as arrays—but we will mostly ignore them in this book. <a class="url" href="02.xhtml#fnref4">↩</a></p>
</div></body></html>