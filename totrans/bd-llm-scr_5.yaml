- en: 5 Pretraining on Unlabeled Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 在未标记数据上进行预训练
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容涵盖
- en: Computing the training and validation set losses to assess the quality of LLM-generated
    text during training
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算训练和验证集损失，以评估训练过程中 LLM 生成文本的质量
- en: Implementing a training function and pretraining the LLM
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施训练函数并对 LLM 进行预训练
- en: Saving and loading model weights to continue training an LLM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存和加载模型权重以继续训练 LLM
- en: Loading pretrained weights from OpenAI
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 OpenAI 加载预训练权重
- en: In the previous chapters, we implemented the data sampling, attention mechanism
    and coded the LLM architecture. The core focus of this chapter is to implement
    a training function and pretrain the LLM, as illustrated in Figure 5.1.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们实现了数据采样、注意力机制并编码了 LLM 架构。本章的核心重点是实现训练函数并对 LLM 进行预训练，如图 5.1 所示。
- en: Figure 5.1 A mental model of the three main stages of coding an LLM, pretraining
    the LLM on a general text dataset and finetuning it on a labeled dataset. This
    chapter focuses on pretraining the LLM, which includes implementing the training
    code, evaluating the performance, and saving and loading model weights.
  id: totrans-7
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.1 LLM 编码的三个主要阶段的思维模型，分别是对一般文本数据集的 LLM 进行预训练，以及在标记数据集上进行微调。本章专注于对 LLM 进行预训练，包括实施训练代码、评估性能以及保存和加载模型权重。
- en: '![](images/05__image001.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image001.png)'
- en: As illustrated in Figure 5.1, we will also learn about basic model evaluation
    techniques to measure the quality of the generated text, which is a requirement
    for optimizing the LLM during the training process. Moreover, we will discuss
    how to load pretrained weights, giving our LLM a solid starting point for finetuning
    in the upcoming chapters.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 5.1 所示，我们还将学习基本的模型评估技术，以测量生成文本的质量，这是在训练过程中优化 LLM 的必要条件。此外，我们将讨论如何加载预训练权重，为我们的
    LLM 在接下来的章节中微调提供坚实的起点。
- en: Weight parameters
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 权重参数
- en: In the context of LLMs and other deep learning models, *weights* refer to the
    trainable parameters that the learning process adjusts. These weights are also
    known as *weight parameters* or simply *parameters*. In frameworks like PyTorch,
    these weights are stored in linear layers, for example, which we used to implement
    the multi-head attention module in chapter 3 and the `GPTModel` in chapter 4\.
    After initializing a layer (`new_layer = torch.nn.Linear(...)`), we can access
    its weights through the `.weight` attribute, `new_layer.weight`. Additionally,
    for convenience, PyTorch allows direct access to all a model's trainable parameters,
    including weights and biases, through the method `model.parameters()`, which we
    will use later when implementing the model training.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 和其他深度学习模型的背景下，*权重* 是指学习过程调整的可训练参数。这些权重也称为 *权重参数* 或简单地称为 *参数*。在 PyTorch
    等框架中，这些权重存储在线性层中，例如，我们在第三章中实现的多头注意力模块和第四章中的 `GPTModel`。在初始化一个层（`new_layer = torch.nn.Linear(...)`）后，我们可以通过
    `.weight` 属性访问其权重，即 `new_layer.weight`。此外，为了方便起见，PyTorch 允许通过 `model.parameters()`
    方法直接访问模型的所有可训练参数，包括权重和偏置，我们将在后续实现模型训练时使用该方法。
- en: 5.1 Evaluating generative text models
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 评估生成文本模型
- en: We begin this chapter by setting up the LLM for text generation based on code
    from the previous chapter and discuss basic ways to evaluate the quality of the
    generated text in this section. The content we cover in this section and the remainder
    of this chapter is outlined in Figure 5.2.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们根据前一章的代码设置 LLM 以进行文本生成，并在本节中讨论评估生成文本质量的基本方法。本节和本章其余部分涵盖的内容在图 5.2 中概述。
- en: Figure 5.2 An overview of the topics covered in this chapter. We begin by recapping
    the text generation from the previous chapter and implementing basic model evaluation
    techniques that we can use during the pretraining stage.
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.2 本章涵盖主题的概述。我们首先回顾前一章的文本生成，并实施基本的模型评估技术，这些技术可在预训练阶段使用。
- en: '![](images/05__image003.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image003.png)'
- en: As shown in Figure 5.2, the next subsection recaps the text generation we set
    up at the end of the previous chapter before we dive into the text evaluation
    and calculation of the training and validation losses in the subsequent subsections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 5.2 所示，下一小节回顾了我们在前一章结束时设置的文本生成内容，然后我们将深入探讨文本评估以及后续小节中训练和验证损失的计算。
- en: 5.1.1 Using GPT to generate text
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 使用 GPT 生成文本
- en: 'In this section, we set up the LLM and briefly recap the text generation process
    we implemented in chapter 4\. We begin by initializing the GPT model that we will
    evaluate and train in this chapter, using the `GPTModel` class and `GPT_CONFIG_124M`
    dictionary from chapter 4:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们设置 LLM，并简要回顾我们在第 4 章中实施的文本生成过程。我们首先初始化将在本章评估和训练的 GPT 模型，使用第 4 章中的 `GPTModel`
    类和 `GPT_CONFIG_124M` 字典：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Considering the `GPT_CONFIG_124M` dictionary, the only adjustment we have made
    compared to the previous chapter is reducing the context length (`context_length`)
    to 256 tokens. This modification reduces the computational demands of training
    the model, making it possible to carry out the training on a standard laptop computer.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 `GPT_CONFIG_124M` 字典，与上一章相比，我们所做的唯一调整是将上下文长度 (`context_length`) 减少到 256
    个 token。这一修改减少了训练模型的计算需求，使得在标准笔记本电脑上进行训练成为可能。
- en: Originally, the GPT-2 model with 124 million parameters was configured to handle
    up to 1,024 tokens. After the training process, at the end of this chapter, we
    will update the context size setting and load pretrained weights to work with
    a model configured for a 1,024-token context length.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，具有 1.24 亿参数的 GPT-2 模型被配置为处理最多 1,024 个 token。在训练过程结束时，本章将更新上下文大小设置并加载预训练权重，以处理一个配置为
    1,024 token 上下文长度的模型。
- en: Using the `GPTmodel` instance, we adopt the `generate_text_simple` function
    introduced in the previous chapter and introduce two handy functions, `text_to_token_ids`
    and `token_ids_to_text`. These functions facilitate the conversion between text
    and token representations, a technique we will utilize throughout this chapter.
    To provide a clearer understanding, Figure 5.3 illustrates this process before
    we dive into the code.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `GPTmodel` 实例，我们采用了上一章介绍的 `generate_text_simple` 函数，并引入了两个便捷函数 `text_to_token_ids`
    和 `token_ids_to_text`。这些函数简化了文本与 token 表示之间的转换，这一技术我们将在本章中反复使用。为了提供更清晰的理解，图 5.3
    在我们深入代码之前展示了这一过程。
- en: Figure 5.3 Generating text involves encoding text into token IDs that the LLM
    processes into logit vectors. The logit vectors are then converted back into token
    IDs, detokenized into a text representation.
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.3 生成文本涉及将文本编码为 LLM 处理的 token ID，进而生成 logit 向量。这些 logit 向量随后被转换回 token ID，最后解码为文本表示。
- en: '![](images/05__image005.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image005.png)'
- en: Figure 5.3 illustrates a three-step text generation process using a GPT model.
    First, the tokenizer converts input text into a series of token IDs, as discussed
    in chapter 2\. Second, the model receives these token IDs and generates corresponding
    logits, which are vectors representing the probability distribution for each token
    in the vocabulary, as discussed in chapter 4\. Third, these logits are converted
    back into token IDs, which the tokenizer decodes into human-readable text, completing
    the cycle from textual input to textual output.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 展示了使用 GPT 模型的三步文本生成过程。首先，分词器将输入文本转换为一系列的 token ID，如第 2 章所讨论的。其次，模型接收这些
    token ID 并生成相应的 logits，logits 是表示词汇表中每个 token 概率分布的向量，如第 4 章所讨论的。最后，这些 logits
    被转换回 token ID，分词器将其解码为人类可读的文本，完成从文本输入到文本输出的循环。
- en: 'In code, we implement the text generation process as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们实现文本生成过程如下：
- en: Listing 5.1 Utility functions for text to token ID conversion
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.1 文本到 token ID 转换的工具函数
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Using the preceding code, the `model` generates the following text:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，`model` 生成了以下文本：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Based on the output, it's clear the model isn't yet producing coherent text
    because it hasn't undergone training. To define what makes text "coherent" or
    "high quality," we have to implement a numerical method to evaluate the generated
    content. This approach will enable us to monitor and enhance the model's performance
    throughout its training process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输出，很明显模型尚未生成连贯的文本，因为它还没有经过训练。为了定义什么使文本“连贯”或“高质量”，我们必须实施一种数值方法来评估生成的内容。这种方法将使我们能够在整个训练过程中监控和提升模型的表现。
- en: The following section introduces how we calculate a *loss metric* for the generated
    outputs. This loss serves as a progress and success indicator of the training
    progress. Furthermore, in subsequent chapters on finetuning LLMs, we will review
    additional methodologies for assessing model quality.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分介绍如何为生成的输出计算一个 *损失指标*。这个损失作为训练进展的指示器，显示训练的成功与否。此外，在后续关于微调 LLM 的章节中，我们将回顾评估模型质量的其他方法。
- en: 5.1.2 Calculating the text generation loss
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 计算文本生成损失
- en: This section explores techniques for numerically assessing text quality generated
    during training by calculating a so-called text generation loss. We go over this
    topic step-by-step with a practical example to make the concepts clear and applicable,
    beginning with a short recap of how the data is loaded from chapter 2 and how
    the text is generated via the `generate_text_simple` function from chapter 4.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨通过计算所谓的文本生成损失来对训练期间生成的文本质量进行数值评估的技术。我们将逐步深入这一主题，并用一个实际示例来使概念清晰易懂，首先回顾一下第2章中数据是如何加载的，以及如何通过第4章中的`generate_text_simple`函数生成文本。
- en: Figure 5.4 illustrates the overall flow from input text to LLM-generated text
    using a five-step procedure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4展示了从输入文本到LLM生成文本的整体流程，采用了五个步骤的过程。
- en: Figure 5.4 For each of the 3 input tokens, shown on the left, we compute a vector
    containing probability scores corresponding to each token in the vocabulary. The
    index position of the highest probability score in each vector represents the
    most likely next token ID. These token IDs associated with the highest probability
    scores are selected and mapped back into a text that represents the text generated
    by the model.
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4显示了左侧的3个输入标记，我们计算了一个向量，包含与词汇中每个标记对应的概率分数。每个向量中最高概率分数的索引位置代表最可能的下一个标记ID。这些与最高概率分数相关的标记ID被选中并映射回文本，表示模型生成的文本。
- en: '![](images/05__image007.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image007.png)'
- en: The text generation process in Figure 5.4 outlines what the `generate_text_simple`
    function from chapter 4 does internally. We need to perform these same initial
    steps before we can compute a loss that measures the generated text quality later
    in this section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4中的文本生成过程概述了第4章的`generate_text_simple`函数的内部工作。我们需要执行这些相同的初始步骤，然后才能计算损失，以衡量本节后面生成文本的质量。
- en: Figure 5.4 outlines the text generation process with a small 7-token vocabulary
    to fit this image on a single page. However, our `GPTModel` works with a much
    larger vocabulary consisting of 50,257 words; hence, the token IDs in the following
    codes will range from 0 to 50,256 rather than 0 to 6.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4概述了文本生成过程，使用了一个小的7个标记的词汇，以便将该图放在一页上。然而，我们的`GPTModel`使用的词汇量更大，包含50,257个单词；因此，以下代码中的标记ID范围将是从0到50,256，而不是0到6。
- en: 'Also, Figure 5.4 only shows a single text example (`"every effort moves"`)
    for simplicity. In the following hands-on code example that implements the steps
    in Figure 5.4, we will work with two input examples (`"every effort moves"` and
    `"I really like"`) as inputs for the GPT model:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图5.4仅显示了一个文本示例(`"every effort moves"`)，以简化说明。在接下来的动手代码示例中，我们将使用两个输入示例(`"every
    effort moves"`和`"I really like"`)，作为GPT模型的输入：
- en: 'Consider the two input examples, which which have already been mapped to token
    IDs, corresponding to step 1 in Figure 5.4:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这两个输入示例，这些示例已经映射到标记ID，对应于图5.4中的步骤1：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Matching these inputs, the `targets` contain the token IDs we aim for the model
    to produce:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与这些输入相匹配，`targets`包含我们希望模型生成的标记ID：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that the targets are the inputs but shifted one position forward, a concept
    we covered chapter 2 during the implementation of the data loader. This shifting
    strategy is crucial for teaching the model to predict the next token in a sequence.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，targets是输入，但向前移动一个位置，这是我们在第2章实施数据加载器时讨论的概念。这种位移策略对于教模型预测序列中的下一个标记至关重要。
- en: 'When we feed the `inputs` into the model to calculate logit vectors for the
    two input examples, each comprising three tokens, and apply the softmax function
    to transform these logit values into probability scores, which corresponds to
    step 2 in Figure 5.4:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将`inputs`输入模型以计算两个输入示例的logit向量，每个示例包含三个标记，并应用softmax函数将这些logit值转化为概率分数时，对应于图5.4中的步骤2：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The resulting tensor dimension of the probability score (`probas`) tensor is
    as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分数(`probas`)张量的最终维度如下：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first number, 2, corresponds to the two examples (rows) in the `inputs`,
    also known as batch size. The second number, 3, corresponds to the number of tokens
    in each input (row). Finally, the last number corresponds to the embedding dimensionality,
    which is determined by the vocabulary size, as discussed in previous chapters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数字2对应于`inputs`中的两个示例（行），也称为批量大小。第二个数字3对应于每个输入（行）中的标记数量。最后一个数字对应于嵌入维度，由词汇大小决定，具体内容在前面的章节中讨论过。
- en: Following the conversion from logits to probabilities via the softmax function,
    the `generate_text_simple` function from chapter 4 then converts the resulting
    probability scores back into text, as illustrated in steps 3-5 in Figure 5.4.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 softmax 函数将 logits 转换为概率后，第 4 章的 `generate_text_simple` 函数将结果概率分数转换回文本，如图
    5.4 的步骤 3-5 所示。
- en: 'We can implement steps 3 and 4 by applying the argmax function to the probability
    scores to obtain the corresponding token IDs:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将 argmax 函数应用于概率分数来实施步骤 3 和 4，以获得相应的令牌 ID：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Given that we have 2 input batches, each containing 3 tokens, applying the
    argmax function to the probability scores (step 3 in Figure 5.4) yields 2 sets
    of outputs, each with 3 predicted token IDs:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们有 2 个输入批次，每个批次包含 3 个令牌，对概率分数应用 argmax 函数（图 5.4 的步骤 3）将产生 2 组输出，每组包含 3 个预测的令牌
    ID：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When we decode these tokens, we find that these output tokens are quite different
    from the target tokens we want the model to generate:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们解码这些令牌时，会发现这些输出令牌与我们希望模型生成的目标令牌大相径庭：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The model produces random text that is different from the target text because
    it has not been trained yet. We now get to the part where we evaluate the performance
    of the model's generated text numerically via a so-called loss as illustrated
    in Figure 5.4\. Not only is this useful for measuring the quality of the generated
    text, but it's also a building block for implementing the training function later,
    which we use to update the model's weight to improve the generated text.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 模型生成的文本与目标文本不同，因为它尚未经过训练。我们现在进入评估模型生成文本性能的部分，采用一种所谓的损失进行数值评估，如图 5.4 所示。这不仅对测量生成文本的质量有用，而且也是稍后实现训练函数的基础，训练函数用于更新模型的权重，从而改进生成文本。
- en: Figure 5.5 We now implement the text evaluation function in the remainder of
    this section. In the next section, we apply this evaluation function to the entire
    dataset we use for model training.
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.5 我们现在在本节其余部分实施文本评估函数。在下一节中，我们将此评估函数应用于用于模型训练的整个数据集。
- en: '![](images/05__image009.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image009.png)'
- en: Part of the text evaluation process that we implement in the remainder of this
    section, as shown in Figure 5.5, is to measure "how far" the generated tokens
    are from the correct predictions (targets). The training function we implement
    later in this chapter will use this information to adjust the model weights to
    generate text that is more similar to (or ideally matches) the target text.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节其余部分实施的文本评估过程的一部分，如图 5.5 所示，是测量生成的令牌与正确预测（目标）之间的“距离”。本章稍后实施的训练函数将利用这些信息调整模型权重，以生成更接近（或理想情况下匹配）目标文本的文本。
- en: 'The model training aims to increase the softmax probability in the index positions
    corresponding to the correct target token IDs, as illustrated in Figure 5.6\.
    This softmax probability is also used in the evaluation metric we are implementing
    in the remainder of this section to numerically assess the model''s generated
    outputs: the higher the probability in the correct positions, the better.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练的目的是提高与正确目标令牌 ID 对应的索引位置的 softmax 概率，如图 5.6 所示。这一 softmax 概率也用于我们在本节其余部分实施的评估指标，以数值评估模型生成的输出：正确位置的概率越高，结果越好。
- en: Figure 5.6 Before training, the model produces random next-token probability
    vectors. The goal of model training is to ensure that the probability values corresponding
    to the highlighted target token IDs are maximized.
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.6 在训练之前，模型生成随机的下一个令牌概率向量。模型训练的目标是确保与高亮的目标令牌 ID 对应的概率值被最大化。
- en: '![](images/05__image011.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image011.png)'
- en: Remember that Figure 5.6 displays the softmax probabilities for a compact 7-token
    vocabulary to fit everything into a single figure. This implies that the starting
    random values will hover around 1/7, which equals approximately 0.14.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，图 5.6 显示了一个紧凑的 7 令牌词汇的 softmax 概率，以便将所有内容放入一个单一的图中。这意味着初始随机值将围绕 1/7 波动，约等于
    0.14。
- en: However, the vocabulary we are using for our GPT-2 model has 50,257 tokens,
    so most of the initial probabilities will hover around 0.00002 via 1/50,257.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们用于 GPT-2 模型的词汇包含 50,257 个令牌，因此大多数初始概率将通过 1/50,257 徘徊在 0.00002 附近。
- en: 'For each of the two input texts, we can print the initial softmax probability
    scores corresponding to the target tokens via the following code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个输入文本，我们可以通过以下代码打印与目标令牌对应的初始 softmax 概率分数：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The 3 target token ID probabilities for each batch are as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 每个批次的3个目标令牌ID概率如下：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The goal of training an LLM is to maximize these values, aiming to get them
    as close to a probability of 1\. This way, we ensure the LLM consistently picks
    the target token—essentially the next word in the sentence—as the next token it
    generates.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型语言模型(LLM)的目标是最大化这些值，力求将它们接近概率1。这样，我们确保LLM始终选择目标令牌——实际上是句子中的下一个单词——作为其生成的下一个令牌。
- en: Backpropagation
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 反向传播
- en: How do we maximize the softmax probability values corresponding to the target
    tokens? The big picture is that we update the model weights so that the model
    outputs higher values for the respective token IDs we want to generate. The weight
    update is done via a process called *backpropagation*, a standard technique for
    training deep neural networks (see sections A.3 to A.7 in Appendix A for more
    details about backpropagation and model training).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何最大化与目标令牌对应的softmax概率值？整体思路是更新模型权重，以使模型为我们希望生成的相应令牌ID输出更高的值。权重更新通过一个称为*反向传播*的过程完成，这是一种训练深度神经网络的标准技术（有关反向传播和模型训练的更多细节，请参见附录A的A.3到A.7节）。
- en: Backpropagation requires a loss function, which calculates the difference between
    the model's predicted output (here, the probabilities corresponding to the target
    token IDs) and the actual desired output. This loss function measures how far
    off the model's predictions are from the target values.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播需要一个损失函数，该函数计算模型预测输出（此处为与目标令牌ID对应的概率）和实际期望输出之间的差异。这个损失函数衡量模型预测与目标值之间的偏差。
- en: In the remainder of this section, we calculate the loss for the probability
    scores of the two example batches, `target_probas_1` and `target_probas_2`. The
    main steps are illustrated in Figure 5.7.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们计算两个示例批次`target_probas_1`和`target_probas_2`的概率分数损失。主要步骤在图5.7中说明。
- en: Figure 5.7 Calculating the loss involves several steps. Steps 1 to 3 calculate
    the token probabilities corresponding to the target tensors. These probabilities
    are then transformed via a logarithm and averaged in steps 4-6.
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7 计算损失涉及几个步骤。步骤1到3计算与目标张量对应的令牌概率。这些概率随后通过对数进行转换，并在步骤4-6中取平均。
- en: '![](images/05__image013.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image013.png)'
- en: 'Since we already applied steps 1-3 listed in Figure 5.7 to obtain `target_probas_1`
    and `target_probas_2`, we proceed with step 4, applying the *logarithm* to the
    probability scores:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经应用了图5.7中列出的步骤1-3以获得`target_probas_1`和`target_probas_2`，我们继续进行步骤4，将*对数*应用于概率分数：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in the following values:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下值：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Working with logarithms of probability scores is more manageable in mathematical
    optimization than handling the scores directly. This topic is outside the scope
    of this book, but I've detailed it further in a lecture, which is linked in the
    reference section in appendix B.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学优化中，处理概率分数的对数比直接处理分数更容易。这一主题超出了本书的范围，但我在讲座中详细讨论过，链接在附录B的参考部分。
- en: 'Next, we combine these log probabilities into a single score by computing the
    average (step 5 in Figure 5.7):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过计算平均值将这些对数概率合并为一个单一得分（图5.7中的步骤5）：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting average log probability score is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的平均对数概率得分如下：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The goal is to get the average log probability as close to 0 as possible by
    updating the model's weights as part of the training process, which we will implement
    later in section 5.2.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是通过更新模型的权重，使平均对数概率尽可能接近0，这将在5.2节中实现。
- en: 'However, in deep learning, the common practice isn''t to push the average log
    probability up to 0 but rather to bring the negative average log probability down
    to 0\. The negative average log probability is simply the average log probability
    multiplied by -1, which corresponds to step 6 in Figure 5.7:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在深度学习中，通常的做法不是将平均对数概率推高至0，而是将负的平均对数概率降低至0。负的平均对数概率仅是平均对数概率乘以-1，这对应于图5.7中的步骤6：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This prints `tensor(-10.7722)`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这打印出`tensor(-10.7722)`。
- en: The term for this negative value, -10.7722 turning into 10.7722, is known as
    the *cross entropy* loss in deep learning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个负值-10.7722转变为10.7722的术语，在深度学习中称为*交叉熵*损失。
- en: PyTorch comes in handy here, as it already has a built-in `cross_entropy` function
    that takes care of all these 6 steps in Figure 5.7 for us.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch在这里派上用场，因为它已经内置了一个`cross_entropy`函数，可以为我们处理图5.7中的所有这6个步骤。
- en: Cross entropy loss
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: At its core, the cross entropy loss is a popular measure in machine learning
    and deep learning that measures the difference between two probability distributions--typically,
    the true distribution of labels (here, tokens in a dataset) and the predicted
    distribution from a model (for instance, the token probabilities generated by
    an LLM).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，交叉熵损失是机器学习和深度学习中一种流行的度量，用于测量两个概率分布之间的差异——通常是真实标签的分布（这里是数据集中的标记）与模型的预测分布（例如，LLM生成的标记概率）。
- en: In the context of machine learning and specifically in frameworks like PyTorch,
    the `cross_entropy` function computes this measure for discrete outcomes, which
    is similar to the negative average log probability of the target tokens given
    the model's generated token probabilities, making the terms cross entropy and
    negative average log probability related and often used interchangeably in practice.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，特别是在像PyTorch这样的框架中，`cross_entropy`函数为离散结果计算该度量，这类似于给定模型生成的标记概率的目标标记的负平均对数概率，使得交叉熵和负平均对数概率相关并在实践中常常可以互换使用。
- en: 'Before we apply the cross entropy function, let''s briefly recall the shape
    of the logits and target tensors:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用交叉熵函数之前，让我们简要回顾一下logits和目标张量的形状：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The resulting shapes are as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结果形状如下：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As we can see, the `logits` tensor has three dimensions: batch size, number
    of tokens, and vocabulary size. The `targets` tensor has two dimensions: batch
    size and number of tokens.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`logits`张量有三个维度：批量大小、标记数量和词汇大小。`targets`张量有两个维度：批量大小和标记数量。
- en: 'For the cross `entropy_loss` function in PyTorch, we want to flatten these
    tensors by combining them over the batch dimension:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PyTorch中的交叉`entropy_loss`函数，我们希望通过在批量维度上组合来展平这些张量：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The resulting tensor dimensions are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 结果张量维度如下：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Remember that the `targets` are the token IDs we want the LLM to generate, and
    the `logits` contain the unscaled model outputs before they enter the softmax
    function to obtain the probability scores.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`targets`是我们希望LLM生成的标记ID，而`logits`包含进入softmax函数之前未缩放的模型输出，以获得概率分数。
- en: 'Previously, we applied the softmax function, selected the probability scores
    corresponding to the target IDs, and computed the negative average log probabilities.
    PyTorch''s `cross_entropy` function will take care of all these steps for us:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们应用了softmax函数，选择了与目标ID对应的概率分数，并计算了负平均对数概率。PyTorch的`cross_entropy`函数会为我们处理所有这些步骤：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The resulting loss is the same that we obtained previously when applying the
    individual steps shown in Figure 5.7 manually:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 结果损失与我们之前在手动应用图5.7中显示的各个步骤时获得的损失相同：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Perplexity
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 困惑度
- en: '*Perplexity* is a measure often used alongside cross entropy loss to evaluate
    the performance of models in tasks like language modeling. It can provide a more
    interpretable way to understand the uncertainty of a model in predicting the next
    token in a sequence.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*困惑度*是一个常用于评估模型在语言建模等任务中性能的度量。它可以提供更易于理解的方式来理解模型在预测序列中下一个标记时的不确定性。'
- en: Perplexity measures how well the probability distribution predicted by the model
    matches the actual distribution of the words in the dataset. Similar to the loss,
    a lower perplexity indicates that the model predictions are closer to the actual
    distribution.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度衡量模型预测的概率分布与数据集中单词的实际分布之间的匹配程度。与损失类似，较低的困惑度表明模型预测更接近实际分布。
- en: Perplexity can be calculated as `perplexity = torch.exp(loss)`, which returns
    `tensor(47678.8633)` when applied to the previously calculated loss.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度可以计算为`perplexity = torch.exp(loss)`，当应用于先前计算的损失时返回`tensor(47678.8633)`。
- en: Perplexity is often considered more interpretable than the raw loss value because
    it signifies the effective vocabulary size about which the model is uncertain
    at each step. In the given example, this would translate to the model being unsure
    about which among 47,678 words or tokens in the vocabulary to generate as the
    next token.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度通常被认为比原始损失值更易于解释，因为它表示模型在每个步骤中对有效词汇大小的不确定性。在给定的示例中，这意味着模型对词汇中47,678个单词或标记中的哪个生成下一个标记感到不确定。
- en: In this section, we calculated the loss for two small text inputs for illustration
    purposes. In the next section, we apply the loss computation to the entire training
    and validation sets.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们为两个小文本输入计算了损失，以便进行说明。在下一节中，我们将损失计算应用于整个训练和验证集。
- en: 5.1.3 Calculating the training and validation set losses
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 计算训练和验证集损失
- en: In this section, we first prepare the training and validation datasets that
    we will use to train the LLM later in this chapter. Then, we calculate the cross
    entropy for the training and validation sets, as illustrated in Figure 5.8, which
    is an important component of the model training process.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先准备将用于在本章后面训练LLM的训练和验证数据集。然后，我们计算训练和验证集的交叉熵，如图5.8所示，这是模型训练过程中的一个重要组成部分。
- en: Figure 5.8 After computing the cross entropy loss in the previous section, we
    now apply this loss computation to the entire text dataset that we will use for
    model training.
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.8 在上一节计算交叉熵损失后，我们现在将该损失计算应用于我们将用于模型训练的整个文本数据集。
- en: '![](images/05__image015.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image015.png)'
- en: To compute the loss on the training and validation datasets as illustrated in
    Figure 5.8, we use a very small text dataset, the "The Verdict" short story by
    Edith Wharton, which we have already worked with in chapter 2\. By selecting a
    text from the public domain, we circumvent any concerns related to usage rights.
    Additionally, the reason why we use such a small dataset is that it allows for
    the execution of code examples on a standard laptop computer in a matter of minutes,
    even without a high-end GPU, which is particularly advantageous for educational
    purposes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算训练和验证数据集上的损失，如图5.8所示，我们使用了一个非常小的文本数据集，即艾迪丝·华顿的短篇小说《判决》，我们在第二章中已经使用过。通过选择公共领域的文本，我们避免了与使用权相关的任何担忧。此外，使用如此小的数据集的原因在于，它使得在标准笔记本电脑上运行代码示例只需几分钟，甚至不需要高端GPU，这对于教育目的特别有利。
- en: Interested readers can also use the supplementary code of this book to prepare
    a larger-scale dataset consisting of more than 60,000 public domain books from
    Project Gutenberg and train an LLM on these (see appendix D for details).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者还可以使用本书的补充代码，准备一个由超过60,000本公共领域书籍（来自古腾堡计划）组成的大规模数据集，并在这些书籍上训练一个LLM（详见附录D）。
- en: The cost of pretraining LLMs
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 预训练LLM的成本
- en: To put the scale of our project into perspective, consider the training of the
    7 billion parameter Llama 2 model, a relatively popular openly available LLM.
    This model required 184,320 GPU hours on expensive A100 GPUs, processing 2 trillion
    tokens. At the time of writing, running an 8xA100 cloud server on AWS costs around
    $30 per hour. A rough estimate puts the total training cost of such an LLM at
    around $690,000 (calculated as 184,320 hours divided by 8, then multiplied by
    $30).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的项目规模更具可比性，考虑一下7亿参数的Llama 2模型的训练，这是一个相对流行的公开可用LLM。该模型在昂贵的A100 GPU上需要184,320
    GPU小时，处理2万亿个标记。在撰写本文时，在AWS上运行一个8个A100的云服务器大约需要每小时30美元。粗略估算，该LLM的总训练成本大约为69万美元（计算方式为184,320小时除以8，然后乘以30美元）。
- en: 'The following code loads the "The Verdict" short story we used in chapter 2:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载了我们在第二章中使用的《判决》短篇小说：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After loading the dataset, we can check the number of characters and tokens
    in the dataset:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集后，我们可以检查数据集中字符和标记的数量：
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: With just 5,145 tokens, the text might seem too small to train an LLM, but as
    mentioned earlier, it's for educational purposes so that we can run the code in
    minutes instead of weeks. Plus, we will be loading pretrained weights from OpenAI
    into our `GPTModel` code at the end of this chapter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用5,145个标记，文本看起来可能太小而无法训练LLM，但如前所述，这是出于教育目的，以便我们能够在几分钟内运行代码，而不是几周。此外，在本章结束时，我们将从OpenAI加载预训练权重到我们的`GPTModel`代码中。
- en: Next, we divide the dataset into a training and a validation set and use the
    data loaders from chapter 2 to prepare the batches for LLM training. This process
    is visualized in Figure 5.9.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据集划分为训练集和验证集，并使用第二章中的数据加载器准备LLM训练的批次。此过程在图5.9中进行了可视化。
- en: Figure 5.9 When preparing the data loaders, we split the input text into training
    and validation set portions. Then, we tokenize the text (only shown for the training
    set portion for simplicity) and divide the tokenized text into chunks of a user-specified
    length (here 6). Finally, we shuffle the rows and organize the chunked text into
    batches (here, batch size 2), which we can use for model training.
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.9 在准备数据加载器时，我们将输入文本拆分为训练集和验证集部分。然后，我们对文本进行标记化（为了简单起见，仅对训练集部分显示），并将标记化的文本划分为用户指定长度（这里为6）的块。最后，我们对行进行随机打乱，并将分块的文本组织成批次（这里，批次大小为2），以便用于模型训练。
- en: '![](images/05__image017.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image017.png)'
- en: For visualization purposes, Figure 5.9 uses a `max_length=6` due to spatial
    constraints. However, for the actual data loaders we are implementing, we set
    the `max_length` equal to the 256-token context length that the LLM supports so
    that the LLM sees longer texts during training.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化，图5.9使用了`max_length=6`，这是由于空间限制。然而，对于我们正在实现的实际数据加载器，我们将`max_length`设置为LLM支持的256个标记上下文长度，以便LLM在训练期间看到更长的文本。
- en: Training with variable lengths
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用可变长度进行训练
- en: We are training the model with training data presented in similarly-sized chunks
    for simplicity and efficiency. However, in practice, it can also be beneficial
    to train an LLM with variable-length inputs to help the LLM to better generalize
    across different types of inputs when it is being used.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用相似大小的块呈现训练数据来训练模型，以简化和提高效率。然而，实际上，以可变长度的输入训练LLM也可以是有益的，这有助于LLM在使用时更好地进行不同类型输入的泛化。
- en: 'To implement the data splitting and loading visualized in Figure 5.9, we first
    define a `train_ratio` to use 90% of the data for training and the remaining 10%
    as validation data for model evaluation during training:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现图5.9中可视化的数据分割和加载，我们首先定义一个`train_ratio`，使用90%的数据进行训练，剩余的10%作为训练期间模型评估的验证数据：
- en: '[PRE26]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Using the `train_data` and `val_data` subsets, we can now create the respective
    data loader reusing the `create_dataloader_v1` code from chapter 2:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`train_data`和`val_data`子集，我们现在可以创建相应的数据加载器，重用第2章中的`create_dataloader_v1`代码：
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We used a relatively small batch size in the preceding code to reduce the computational
    resource demand because we were working with a very small dataset. In practice,
    training LLMs with batch sizes of 1,024 or larger is not uncommon.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了相对较小的批次大小，以减少计算资源的需求，因为我们使用的是非常小的数据集。实际上，以1,024或更大的批次大小训练LLM并不罕见。
- en: 'As an optional check, we can iterate through the data loaders to ensure that
    they were created correctly:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 作为可选检查，我们可以遍历数据加载器，以确保它们被正确创建：
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We should see the following outputs:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下输出：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Based on the preceding code output, we have 9 training set batches with 2 samples
    and 256 tokens each. Since we allocated only 10% of the data for validation, there
    is only one validation batch consisting of 2 input examples.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码输出，我们有9个训练集批次，每个批次有2个样本和256个标记。由于我们只分配了10%的数据用于验证，因此只有一个验证批次，由2个输入示例组成。
- en: As expected, the input data (`x`) and target data (`y`) have the same shape
    (the batch size times the number of tokens in each batch) since the targets are
    the inputs shifted by one position, as discussed in chapter 2.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，输入数据（`x`）和目标数据（`y`）具有相同的形状（批次大小乘以每个批次中的标记数），因为目标是输入向前移动一个位置，如第2章所讨论的。
- en: 'Next, we implement a utility function to calculate the cross entropy loss of
    a given batch returned via the training and validation loader:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现一个工具函数，以计算通过训练和验证加载器返回的给定批次的交叉熵损失：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can now use this `calc_loss_batch` utility function, which computes the
    loss for a single batch, to implement the following `calc_loss_loader` function
    that computes the loss over all the batches sampled by a given data loader:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用这个`calc_loss_batch`工具函数，它计算单个批次的损失，来实现以下`calc_loss_loader`函数，该函数计算通过给定数据加载器抽样的所有批次的损失：
- en: Listing 5.2 Function to compute the training and validation loss
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.2 计算训练和验证损失的函数
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: By default, the `calc_loss_batch` function iterates over all batches in a given
    data loader, accumulates the loss in the `total_loss` variable, and then computes
    and averages the loss over the total number of batches. Alternatively, we can
    specify a smaller number of batches via `num_batches` to speed up the evaluation
    during model training.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`calc_loss_batch` 函数遍历给定数据加载器中的所有批次，累积损失到 `total_loss` 变量中，然后计算并平均所有批次的损失。或者，我们可以通过
    `num_batches` 指定较小的批次数，以加快模型训练期间的评估速度。
- en: 'Let''s now see this `calc_loss_batch` function in action, applying it to the
    training and validation set loaders:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看 `calc_loss_batch` 函数的实际应用，将其应用于训练和验证集加载器：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The resulting loss values are as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的损失值如下：
- en: '[PRE33]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The loss values are relatively high because the model has not yet been trained.
    For comparison, the loss approaches 0 if the model learns to generate the next
    tokens as they appear in the training and validation sets.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 损失值相对较高，因为模型尚未训练。相比之下，如果模型学会生成训练集和验证集中出现的下一个标记，损失会接近 0。
- en: Now that we have a way to measure the quality of the generated text, in the
    next section, we train the LLM to reduce this loss so that it becomes better at
    generating text, as illustrated in Figure 5.10.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了衡量生成文本质量的方法，在下一节中，我们训练 LLM 以降低此损失，从而使其在生成文本时表现更好，如图 5.10 所示。
- en: Figure 5.10 We have recapped the text generation process and implemented basic
    model evaluation techniques to compute the training and validation set losses.
    Next, we will go to the training functions and pretrain the LLM.
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.10 我们回顾了文本生成过程，并实现了基本模型评估技术以计算训练和验证集的损失。接下来，我们将进入训练函数并对 LLM 进行预训练。
- en: '![](images/05__image019.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image019.png)'
- en: As shown in Figure 5.10, the next section focuses on pretraining the LLM. After
    model training, we implement alternative text generation strategies and save and
    load pretrained model weights.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 5.10 所示，下一节专注于对 LLM 的预训练。在模型训练完成后，我们实现替代文本生成策略，并保存和加载预训练模型权重。
- en: 5.2 Training an LLM
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 训练 LLM
- en: In this section, we finally implement the code for pretraining the LLM, our
    `GPTModel`. For this, we focus on a straightforward training loop, as illustrated
    in Figure 5.11, to keep the code concise and readable. However, interested readers
    can learn about more advanced techniques, including l*earning rate warmup*, *cosine
    annealing*, and *gradient clipping*, in *Appendix D, Adding Bells and Whistles
    to the Training Loop.*
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们最终实现了对 LLM，即我们的 `GPTModel` 的预训练代码。为此，我们专注于一个简单的训练循环，如图 5.11 所示，以保持代码简洁易读。然而，感兴趣的读者可以在
    *附录 D, 为训练循环增添附加功能* 中学习更高级的技术，包括 *学习率预热*、*余弦退火* 和 *梯度裁剪*。
- en: Figure 5.11 A typical training loop for training deep neural networks in PyTorch
    consists of several steps, iterating over the batches in the training set for
    several epochs. In each loop, we calculate the loss for each training set batch
    to determine loss gradients, which we use to update the model weights so that
    the training set loss is minimized.
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.11 一个典型的训练循环用于在 PyTorch 中训练深度神经网络，包含多个步骤，遍历训练集中的批次进行多个纪元。在每个循环中，我们计算每个训练集批次的损失，以确定损失梯度，利用这些梯度更新模型权重，从而最小化训练集损失。
- en: '![](images/05__image021.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image021.png)'
- en: The flowchart in Figure 5.11 depicts a typical PyTorch neural network training
    workflow, which we use for training an LLM. It outlines eight steps, starting
    with iterating over each epoch, processing batches, resetting and calculating
    gradients, updating weights, and concluding with monitoring steps like printing
    losses and generating text samples. If you are relatively new to training deep
    neural networks with PyTorch and any of these steps are unfamiliar, consider reading
    sections A.5 to A.8 in *Appendix A, Introduction to PyTorch*.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 的流程图描绘了一个典型的 PyTorch 神经网络训练工作流程，我们用于训练 LLM。它概述了八个步骤，从遍历每个纪元、处理批次、重置和计算梯度、更新权重，到最后监控步骤，如打印损失和生成文本样本。如果你对使用
    PyTorch 训练深度神经网络相对陌生，且这些步骤中有不熟悉的，考虑阅读 *附录 A, PyTorch 介绍* 中的 A.5 到 A.8 节。
- en: 'In code, we can implement this training flow via the following `train_model_simple`
    function:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以通过以下 `train_model_simple` 函数实现这个训练流程：
- en: Listing 5.3 The main function for pretraining LLMs
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.3 LLM 预训练的主要功能
- en: '[PRE34]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Note that the `train_model_simple` function we just created uses two functions
    we have not defined yet: `evaluate_model` and `generate_and_print_sample`.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们刚刚创建的`train_model_simple`函数使用了两个尚未定义的函数：`evaluate_model`和`generate_and_print_sample`。
- en: The `evaluate_model` function corresponds to step 7 in Figure 5.11\. It prints
    the training and validation set losses after each model update so we can evaluate
    whether the training improves the model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_model`函数对应于图5.11中的第7步。它在每次模型更新后打印训练集和验证集的损失，以便我们评估训练是否改善了模型。'
- en: 'More specifically, the `evaluate_model` function calculates the loss over the
    training and validation set while ensuring the model is in evaluation mode with
    gradient tracking and dropout disabled when calculating the loss over the training
    and validation sets:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，`evaluate_model`函数计算训练集和验证集的损失，同时确保模型处于评估模式，在计算训练和验证集的损失时禁用梯度跟踪和丢弃。
- en: '[PRE35]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Similar to `evaluate_model`, the `generate_and_print_sample` function is a
    convenience function that we use to track whether the model improves during the
    training. In particular, the `generate_and_print_sample` function takes a text
    snippet (`start_context`) as input, converts it into token IDs, and feeds it to
    the LLM to generate a text sample using the `generate_text_simple` function we
    used earlier:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与`evaluate_model`类似，`generate_and_print_sample`函数是一个便捷函数，我们用它来跟踪模型在训练过程中是否有所改善。具体来说，`generate_and_print_sample`函数将一个文本片段（`start_context`）作为输入，将其转换为令牌ID，并将其输入LLM，以使用我们之前使用的`generate_text_simple`函数生成文本示例：
- en: '[PRE36]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: While the `evaluate_model` function gives us a numeric estimate of the model's
    training progress, this `generate_and_print_sampl`e text function provides a concrete
    text example generated by the model to judge its capabilities during training.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_model`函数为我们提供模型训练进度的数值估计，而`generate_and_print_sample`文本函数则提供由模型生成的具体文本示例，以便在训练过程中判断其能力。'
- en: AdamW
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: AdamW
- en: '*Adam* optimizers are a popular choice for training deep neural networks. However,
    in our training loop, we opt for the *AdamW* optimizer. AdamW is a variant of
    Adam that improves the weight decay approach, which aims to minimize model complexity
    and prevent overfitting by penalizing larger weights. This adjustment allows AdamW
    to achieve more effective regularization and better generalization and is thus
    frequently used in the training of LLMs.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*Adam*优化器是训练深度神经网络的热门选择。然而，在我们的训练循环中，我们选择了*AdamW*优化器。AdamW是Adam的一个变体，改进了权重衰减的方法，旨在通过惩罚较大的权重来最小化模型复杂度并防止过拟合。这一调整使得AdamW能够实现更有效的正则化和更好的泛化，因此在LLM的训练中经常使用。'
- en: Let's see this all in action by training a GPTModel instance for 10 epochs using
    an AdamW optimizer and the `train_model_simple` function we defined earlier.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过训练一个GPTModel实例进行10个周期，使用AdamW优化器和我们之前定义的`train_model_simple`函数，来看看这一切是如何运作的。
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Executing the `training_model_simple` function starts the training process,
    which takes about 5 minutes on a MacBook Air or a similar laptop to complete.
    The output printed during this execution is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 执行`training_model_simple`函数启动训练过程，在MacBook Air或类似笔记本电脑上大约需要5分钟完成。执行过程中打印的输出如下：
- en: '[PRE38]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As we can see, based on the results printed during the training, the training
    loss improves drastically, starting with a value of 9.558 and converging to 0.762\.
    The language skills of the model have improved quite a lot. In the beginning,
    the model is only able to append commas to the start context (`"Every effort moves
    you,,,,,,,,,,,,"`) or repeat the word `"and"`. At the end of the training, it
    can generate grammatically correct text.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，基于训练期间打印的结果，训练损失显著改善，从9.558的值开始收敛到0.762。模型的语言能力有了很大提升。在最开始，模型只能在起始上下文中添加逗号（`"Every
    effort moves you,,,,,,,,,,,,"`）或重复单词`"and"`。在训练结束时，它能够生成语法正确的文本。
- en: Similar to the training set loss, we can see that the validation loss starts
    high (9.856) and decreases during the training. However, it never becomes as small
    as the training set loss and remains at 6.372 after the 10th epoch.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练集损失类似，我们可以看到验证损失起初较高（9.856），在训练过程中逐渐下降。然而，它从未降至与训练集损失一样低，并且在第10个周期后保持在6.372。
- en: 'Before discussing the validation loss in more detail, let''s create a simple
    plot that shows the training and validation set losses side by side:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在更详细讨论验证损失之前，让我们创建一个简单的图表，展示训练集和验证集的损失并排比较：
- en: '[PRE39]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The resulting training and validation loss plot is shown in Figure 5.12.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 结果训练和验证损失图如图5.12所示。
- en: Figure 5.12 At the beginning of the training, we observe that both the training
    and validation set losses sharply decrease, which is a sign that the model is
    learning. However, the training set loss continues to decrease past the second
    epoch, whereas the validation loss stagnates. This is a sign that the model is
    still learning, but it's overfitting to the training set past epoch 2.
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.12 在训练开始时，我们观察到训练集和验证集的损失急剧下降，这表明模型正在学习。然而，训练集的损失在第二轮后继续下降，而验证损失则停滞不前。这表明模型仍在学习，但在第2轮后对训练集过拟合。
- en: '![](images/05__image023.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image023.png)'
- en: As Figure 5.12 shows, both the training and validation losses start to improve
    for the first epoch. However, the losses start to diverge past the second epoch.
    This divergence and the fact that the validation loss is much larger than the
    training loss indicate that the model is overfitting to the training data. We
    can confirm that the model memorizes the training data verbatim by searching for
    the generated text snippets, such as `"quite insensible to the irony"` in the
    `"The Verdict"` text file.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如图5.12所示，训练和验证损失在第一轮开始改善。然而，损失在第二轮后开始发散。这种发散以及验证损失远大于训练损失表明模型对训练数据过拟合。我们可以通过搜索生成的文本片段，例如在《裁决》文本文件中查找`"quite
    insensible to the irony"`，来确认模型逐字记住了训练数据。
- en: This memorization is expected since we are working with a very, very small training
    dataset and training the model for multiple epochs. Usually, it's common to train
    a model on a much, much larger dataset for only one epoch.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这种记忆是预期的，因为我们正在处理一个非常非常小的训练数据集，并且对模型进行了多轮训练。通常，训练模型时会在一个更大得多的数据集上只进行一轮训练。
- en: As mentioned earlier, interested readers can try to train the model on 60,000
    public domain books from Project Gutenberg, where this overfitting does not occur;
    see appendix B for details.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，有兴趣的读者可以尝试在来自古腾堡计划的60,000本公共领域书籍上训练模型，在那里不会发生过拟合；有关详细信息，请参见附录B。
- en: In the upcoming section, as shown in Figure 5.13, we explore sampling methods
    employed by LLMs to mitigate memorization effects, resulting in more novel generated
    text.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，如图5.13所示，我们探索LLM采用的采样方法，以缓解记忆效应，从而生成更新颖的文本。
- en: Figure 5.13 Our model can generate coherent text after implementing the training
    function. However, it often memorizes passages from the training set verbatim.
    The following section covers strategies to generate more diverse output texts.
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.13 我们的模型在实现训练功能后能够生成连贯的文本。然而，它通常逐字记住训练集中的段落。下一节将介绍生成更多样化输出文本的策略。
- en: '![](images/05__image025.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image025.png)'
- en: As illustrated in Figure 5.13, the next section will cover text generation strategies
    for LLM to reduce training data memorization and increase the originality of the
    LLM-generated text before we cover weight loading and saving and loading pretrained
    weights from OpenAI's GPT model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如图5.13所示，下一节将介绍LLM的文本生成策略，以减少训练数据的记忆，并增加LLM生成文本的原创性，然后我们将讨论权重加载和从OpenAI的GPT模型中保存和加载预训练权重。
- en: 5.3 Decoding strategies to control randomness
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 控制随机性的解码策略
- en: In this section, we will cover text generation strategies (also called decoding
    strategies) to generate more original text. First, we briefly revisit the `generate_text_simple`
    function from the previous chapter that we used inside the `generate_and_print_sample`
    earlier in this chapter. Then, we will cover two techniques, *temperature scaling*,
    and *top-k sampling*, to improve this function.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍文本生成策略（也称为解码策略），以生成更原创的文本。首先，我们简要回顾一下上一章中使用的`generate_text_simple`函数，该函数在本章的`generate_and_print_sample`中被使用。然后，我们将介绍两种技术，*温度缩放*和*top-k采样*，以改进该函数。
- en: 'We begin by transferring the model back from the GPU to the CPU since inference
    with a relatively small model does not require a GPU. Also, after training, we
    put the model into evaluation model to turn off random components such as dropout:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将模型从GPU转移回CPU，因为使用相对较小的模型进行推理并不需要GPU。此外，在训练后，我们将模型放入评估模式，以关闭诸如dropout之类的随机组件：
- en: '[PRE40]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we plug the `GPTModel` instance (`model`) into the `generate_text_simple`
    function, which uses the LLM to generate one token at a time:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`GPTModel`实例（`model`）插入`generate_text_simple`函数，该函数使用LLM逐个生成标记：
- en: '[PRE41]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The generated text is as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本如下：
- en: '[PRE42]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As explained earlier in section 5.1.2, the generated token is selected at each
    generation step corresponding to the largest probability score among all tokens
    in the vocabulary.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如第5.1.2节所述，生成的标记在每次生成步骤中选择，依据词汇表中所有标记的最大概率分数。
- en: This means that the LLM will always generate the same outputs even if we run
    the `generate_text_simple` function above multiple times on the same start context
    (`"Every effort moves you"`).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着即使我们在相同的起始上下文（`"Every effort moves you"`）上多次运行上面的`generate_text_simple`函数，LLM仍会生成相同的输出。
- en: 'The following subsections introduce two concepts to control the randomness
    and diversity of the generated text: temperature scaling and top-k sampling.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节介绍了两个控制生成文本的随机性和多样性的概念：温度缩放和top-k采样。
- en: 5.3.1 Temperature scaling
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 温度缩放
- en: This section introduces temperature scaling, a technique that adds a probabilistic
    selection process to the next-token generation task.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了温度缩放，这是一种将概率选择过程添加到下一个标记生成任务中的技术。
- en: Previously, inside the `generate_text_simple` function, we always sampled the
    token with the highest probability as the next token using `torch.argmax`, also
    known as *greedy decoding*. To generate text with more variety, we can replace
    the argmax with a function that samples from a probability distribution (here,
    the probability scores the LLM generates for each vocabulary entry at each token
    generation step).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，在`generate_text_simple`函数内，我们总是使用`torch.argmax`采样概率最高的标记作为下一个标记，这也称为*贪婪解码*。为了生成更多样化的文本，我们可以用一个从概率分布中采样的函数替换argmax（在这里，概率分数是LLM在每个标记生成步骤为每个词汇条目生成的）。
- en: 'To illustrate the probabilistic sampling with a concrete example, let''s briefly
    discuss the next-token generation process using a very small vocabulary for illustration
    purposes:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过具体示例说明概率采样，我们来简要讨论使用非常小的词汇表进行的下一个标记生成过程：
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, assume the LLM is given the start context `"every effort moves you"`
    and generates the following next-token logits:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，假设LLM给出的起始上下文为`"every effort moves you"`，并生成以下下一个标记的logits：
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As discussed in the previous chapter, Inside the `generate_text_simple`, we
    convert the logits into probabilities via the softmax function and obtain the
    token ID corresponding the generated token via the argmax function, which we can
    then map back into text via the inverse vocabulary:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，在`generate_text_simple`内部，我们通过softmax函数将logits转换为概率，并通过argmax函数获得与生成标记对应的标记ID，然后可以通过逆词汇表映射回文本：
- en: '[PRE45]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Since the largest logit value, and correspondingly the largest softmax probability
    score, is in the fourth position (index position 3 since Python uses 0-indexing),
    the generated word is "`forward"`.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最大的logit值以及相应的最大softmax概率分数位于第四个位置（索引位置3，因为Python使用0索引），生成的单词是`"forward"`。
- en: 'To implement a probabilistic sampling process, we can now replace the argmax
    with the `multinomial` function in PyTorch:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现概率采样过程，我们现在可以将argmax替换为PyTorch中的`multinomial`函数：
- en: '[PRE46]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The printed output is `"forward"` just like before. What happened? The `multinomial`
    function samples the next token proportional to its probability score. In other
    words, `"forward"` is still the most likely token and will be selected by `multinomial`
    most of the time but not all the time. To illustrate this, let''s implement a
    function that repeats this sampling 1000 times:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的输出仍是`"forward"`，这和之前一样。发生了什么？`multinomial`函数根据其概率分数采样下一个标记。换句话说，`"forward"`仍然是最可能的标记，并且大多数情况下会被`multinomial`选择，但并不是每次。为了说明这一点，我们实现一个函数，将此采样重复1000次：
- en: '[PRE47]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As we can see based on the output, the word `"forward"` is sampled most of the
    time (582 out of 1000 times), but other tokens such as `"closer"`, `"inches"`,
    and `"toward"` will also be sampled some of the time. This means that if we replaced
    the `argmax` function with the `multinomial` function inside the `generate_and_print_sample`
    function, the LLM would sometimes generate texts such as "`every effort moves
    you toward`", "`every effort moves you inches`", and "`every effort moves you
    closer"` instead of "`every effort moves you forward`".
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输出，我们可以看到单词`"forward"`大多数情况下被采样（1000次中582次），但其他标记如`"closer"`、`"inches"`和`"toward"`也会在某些情况下被采样。这意味着如果我们在`generate_and_print_sample`函数内用`multinomial`函数替换`argmax`函数，LLM有时会生成文本如`"every
    effort moves you toward"`、`"every effort moves you inches"`和`"every effort moves
    you closer"`，而不是`"every effort moves you forward"`。
- en: 'We can further control the distribution and selection process via a concept
    called temperature scaling, where *temperature scaling* is just a fancy description
    for dividing the logits by a number greater than 0:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个叫做温度缩放的概念进一步控制分布和选择过程，其中 *温度缩放* 只是一个花哨的描述，用于将 logits 除以一个大于 0 的数字：
- en: '[PRE48]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Temperatures greater than 1 result in more uniformly distributed token probabilities,
    and Temperatures smaller than 1 will result in more confident (sharper or more
    peaky) distributions. Let''s illustrate this by plotting the original probabilities
    alongside probabilities scaled with different temperature values:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 大于 1 的温度会导致标记概率更均匀分布，而小于 1 的温度则会导致更自信（更尖锐或更高峰）的分布。我们通过绘制原始概率与不同温度值缩放后的概率进行说明：
- en: '[PRE49]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The resulting plot is shown in Figure 5.14.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如图 5.14 所示。
- en: Figure 5.14 A temperature of 1 represents the unscaled probability scores for
    each token in the vocabulary. Decreasing the temperature to 0.1 sharpens the distribution,
    so the most likely token (here "forward") will have an even higher probability
    score. Vice versa, increasing the temperature to 5 makes the distribution more
    uniform.
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.14 温度为 1 时表示词汇表中每个标记的未缩放概率分数。将温度降低到 0.1 会使分布更尖锐，因此最可能的标记（此处为 `"forward"`）的概率分数会更高。反之，温度增加到
    5 会使分布更均匀。
- en: '![](images/05__image027.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image027.png)'
- en: A temperature of 1 divides the logits by 1 before passing them to the softmax
    function to compute the probability scores. In other words, using a temperature
    of 1 is the same as not using any temperature scaling. In this case, the tokens
    are selected with a probability equal to the original softmax probability scores
    via the `multinomial` sampling function in PyTorch.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 温度为 1 时，将 logits 除以 1 后再传递给 softmax 函数以计算概率分数。换句话说，使用温度为 1 等同于不使用任何温度缩放。在这种情况下，标记的选择概率等于通过
    PyTorch 中的 `multinomial` 采样函数计算的原始 softmax 概率分数。
- en: For example, for the temperature setting 1, the token corresponding to "forward"
    would be selected with about 60% of the time, as we can see in Figure 5.14.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于温度设置为 1 时，与 `"forward"` 对应的标记大约有 60% 的时间被选择，正如我们在图 5.14 中所看到的。
- en: 'Also, as we can see in Figure 5.14, applying very small temperatures, such
    as 0.1, will result in sharper distributions such that the behavior of the `multinomial`
    function selects the most likely token (here: `"forward"`) almost 100% of the
    time, approaching the behavior of the argmax function. Vice versa, a temperature
    of 5 results in a more uniform distribution where other tokens are selected more
    often. This can add more variety to the generated texts but also more often results
    in nonsensical text. For example, using the temperature of 5 results in texts
    such as `"every effort moves you pizza"` about 4% of the time.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如我们在图 5.14 中所看到的，应用非常小的温度，例如 0.1，将导致更尖锐的分布，使得 `multinomial` 函数几乎 100% 地选择最可能的标记（此处为：`"forward"`），接近
    argmax 函数的行为。反之，温度为 5 时，分布更加均匀，其他标记被选择的频率更高。这可以为生成的文本增加更多的多样性，但也更常导致无意义的文本。例如，使用温度
    5 时，生成类似于 `"every effort moves you pizza"` 的文本约 4% 的时间。
- en: Exercise 5.1
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 5.1
- en: Use the `print_sampled_tokens` function to print the sampling frequencies of
    the softmax probabilities scaled with the temperatures shown in Figure 5.13\.
    How often is the word `"pizza"` sampled in each case? Can you think of a faster
    and more accurate way to determine how often the word `"pizza"` is sampled?
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `print_sampled_tokens` 函数打印与图 5.13 中显示的温度缩放后的 softmax 概率的采样频率。每种情况下，单词 `"pizza"`
    被采样的频率是多少？你能想到一种更快速、更准确的方法来确定单词 `"pizza"` 被采样的频率吗？
- en: 5.3.2 Top-k sampling
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 Top-k 采样
- en: In the previous section, we implemented a probabilistic sampling approach coupled
    with temperature scaling to increase the diversity of the outputs. We saw that
    higher temperature values result in more uniformly distributed next-token probabilities,
    which result in more diverse outputs as it reduces the likelihood of the model
    repeatedly selecting the most probable token. This method allows for exploring
    less likely but potentially more interesting and creative paths in the generation
    process. However, One downside of this approach is that it sometimes leads to
    grammatically incorrect or completely nonsensical outputs such as `"every effort
    moves you pizza".`
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们实施了一种与温度缩放结合的概率采样方法，以增加输出的多样性。我们看到更高的温度值会导致下一个令牌的概率更均匀分布，从而产生更丰富的输出，因为它减少了模型重复选择最可能令牌的可能性。这种方法允许在生成过程中探索不太可能但潜在更有趣和创造性的路径。然而，这种方法的一个缺点是，有时会导致语法错误或完全无意义的输出，例如
    `"every effort moves you pizza"`。
- en: In this section, we introduce another concept called *top-k sampling*, which,
    when combined with probabilistic sampling and temperature scaling, can improve
    the text generation results.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍另一个概念，称为 *top-k 采样*，它与概率采样和温度缩放结合时，可以改善文本生成结果。
- en: In top-k sampling, we can restrict the sampled tokens to the top-k most likely
    tokens and exclude all other tokens from the selection process by masking their
    probability scores, as illustrated in Figure 5.15.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在 top-k 采样中，我们可以将采样的令牌限制为最可能的 top-k 令牌，并通过屏蔽它们的概率得分排除所有其他令牌的选择过程，如图 5.15 所示。
- en: Figure 5.15 Using top-k sampling with k=3, we focus on the 3 tokens associated
    with the highest logits and mask out all other tokens with negative infinity (-inf)
    before applying the softmax function. This results in a probability distribution
    with a probability value 0 assigned to all non-top-k tokens.
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.15 使用 k=3 的 top-k 采样，我们专注于与最高 logits 相关的 3 个令牌，并在应用 softmax 函数之前，将所有其他令牌屏蔽为负无穷（-inf）。这导致概率分布中所有非
    top-k 令牌的概率值为 0。
- en: '![](images/05__image029.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image029.png)'
- en: The approach outlined in Figure 5.15 replaces all non-selected logits with negative
    infinity value (`-inf`), such that when computing the softmax values, the probability
    scores of the non-top-k tokens are 0, and the remaining probabilities sum up to
    1\. (Careful readers may remember this masking trick from the causal attention
    module we implemented in chapter 3 in section 3.5.1 *Applying a causal attention
    mask*.)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 中概述的方法将所有未选择的 logits 替换为负无穷值（`-inf`），以便在计算 softmax 值时，非 top-k 令牌的概率得分为
    0，剩余的概率总和为 1。（细心的读者可能还记得我们在第 3 章第 3.5.1 节 *应用因果注意力掩码* 中实现的因果注意力模块的屏蔽技巧。）
- en: 'In code, we can implement the top-k procedure outlined in Figure 5.15 as follows,
    starting with the selection of the tokens with the largest logit values:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以按照图 5.15 中概述的方式实现 top-k 程序，从选择 logits 值最大的令牌开始：
- en: '[PRE50]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The logits values and token IDs of the top 3 tokens, in descending order, are
    as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 前 3 个令牌的 logits 值和令牌 ID，按降序排列，如下所示：
- en: '[PRE51]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Subsequently, we apply PyTorch's `where` function to set the logit values of
    tokens that are below the lowest logit value within our top-3 selection to negative
    infinity (`-inf`).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们应用 PyTorch 的 `where` 函数，将我们 top-3 选择中低于最低 logits 值的令牌的 logits 值设置为负无穷（`-inf`）。
- en: '[PRE52]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The resulting logits for the next token in the 9-token vocabulary are as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 9 令牌词汇中下一个令牌的结果 logits 如下：
- en: '[PRE53]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Lastly, let''s apply the softmax function to turn these into next-token probabilities:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们应用 softmax 函数将这些转化为下一个令牌的概率：
- en: '[PRE54]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'As we can see, the result of this top-3 approach are 3 non-zero probability
    scores:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这种 top-3 方法的结果是 3 个非零概率得分：
- en: '[PRE55]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We can now apply the temperature scaling and multinomial function for probabilistic
    sampling introduced in the previous section to select the next token among these
    3 non-zero probability scores to generate the next token. We do this in the next
    section by modifying the text generation function.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以应用温度缩放和多项式函数，这些在前一节中介绍过，用于从这 3 个非零概率得分中选择下一个令牌。我们将在下一节通过修改文本生成函数来实现这一点。
- en: 5.3.3 Modifying the text generation function
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 修改文本生成函数
- en: 'The previous two subsections introduced two concepts to increase the diversity
    of LLM-generated text: temperature sampling and top-k sampling. In this section,
    we combine and add these concepts to modify the `generate_simple` function we
    used to generate text via the LLM earlier, creating a new `generate` function:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个小节介绍了两个概念，以增加LLM生成文本的多样性：温度采样和top-k采样。在本节中，我们将结合并添加这些概念，以修改之前用于通过LLM生成文本的`generate_simple`函数，创建一个新的`generate`函数：
- en: Listing 5.4 A modified text generation function with more diversity
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.4 一个修改过的具有更多多样性的文本生成函数
- en: '[PRE56]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s now see this new `generate` function in action:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这个新的`generate`函数的实际效果：
- en: '[PRE57]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The generated text is as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本如下：
- en: '[PRE58]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: As we can see, the generated text is very different from the one we previously
    generated via the `generate_simple` function at the beginning of section 5.3 (`"Every
    effort moves you know," was one of the axioms he laid...!"`), which was a memorized
    passage from the training set.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，生成的文本与我们在5.3节开头通过`generate_simple`函数生成的文本非常不同（`"Every effort moves you
    know," was one of the axioms he laid...!"`），那是一段来自训练集的记忆段落。
- en: Exercise 5.2
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.2
- en: Play around with different temperatures and top-k settings. Based on your observations,
    can you think of applications where lower temperature and top-k settings are desired?
    Vice versa, can you think of applications where higher temperature and top-k settings
    are preferred? (It's recommended to also revisit this exercise at the end of the
    chapter after loading the pretrained weights from OpenAI.)
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 玩弄不同的温度和top-k设置。根据你的观察，你能想到在哪些应用中需要较低的温度和top-k设置吗？反之，你能想到在哪些应用中更喜欢较高的温度和top-k设置吗？（建议在本章末尾加载OpenAI的预训练权重后重新审视这个练习。）
- en: Exercise 5.3
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.3
- en: What are the different combinations of settings for the `generate` function
    to force deterministic behavior, that is, disabling the random sampling such that
    it always produces the same outputs similar to the `generate_simple` function?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate`函数的不同设置组合是什么，以强制确定性行为，即禁用随机采样，以便它总是产生与`generate_simple`函数类似的相同输出？'
- en: So far, we covered how to pretrain LLMs and use them to generate text. The last
    two sections of this chapter will discuss how we save and load the trained LLM
    and how we load pretrained weights from OpenAI.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了如何预训练LLM并使用它们生成文本。本章的最后两节将讨论我们如何保存和加载训练好的LLM，以及如何从OpenAI加载预训练权重。
- en: 5.4 Loading and saving model weights in PyTorch
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 在PyTorch中加载和保存模型权重
- en: In this chapter, we have discussed how to numerically evaluate the training
    progress and pretrain an LLM from scratch. Even though both the LLM and dataset
    were relatively small, this exercise showed that pretraining LLMs is computationally
    expensive. Thus, it is important to be able to save the LLM so that we don't have
    to rerun the training every time we want to use it in a new session.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何定量评估训练进度以及从头预训练LLM。尽管LLM和数据集相对较小，但这个练习表明预训练LLM的计算开销很大。因此，能够保存LLM是很重要的，这样我们在每次想在新会话中使用它时就不必重新运行训练。
- en: As illustrated in the chapter overview in Figure 5.16, we cover how to save
    and load a pretrained model in this section. Then, in the upcoming section, we
    will load a more capable pretrained GPT model from OpenAI into our `GPTModel`
    instance.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如图5.16的章节概述所示，本节将介绍如何保存和加载预训练模型。接下来，在即将到来的章节中，我们将从OpenAI加载一个更强大的预训练GPT模型到我们的`GPTModel`实例中。
- en: Figure 5.16 After training and inspecting the model, it is often helpful to
    save the model so that we can use or continue training it later, which is the
    topic of this section before we load the pretrained model weights from OpenAI
    in the final section of this chapter.
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.16 在训练和检查模型后，保存模型通常是有帮助的，这样我们可以在稍后使用或继续训练它，这就是本节的主题，随后在本章最后一节加载OpenAI的预训练模型权重。
- en: '![](images/05__image031.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image031.png)'
- en: 'Fortunately, saving a PyTorch model is relatively straightforward. The recommended
    way is to save a model''s so-called `state_dict`, a dictionary mapping each layer
    to its parameters, using the `torch.save` function as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，保存PyTorch模型相对简单。推荐的方法是使用`torch.save`函数保存模型的所谓`state_dict`，这是一个将每个层映射到其参数的字典，如下所示：
- en: '[PRE59]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: In the preceding code, `"model.pth"` is the filename where the `state_dict`
    is saved. The `.pth` extension is a convention for PyTorch files, though we could
    technically use any file extension.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，`"model.pth"`是保存`state_dict`的文件名。虽然我们可以技术上使用任何文件扩展名，但`.pth`扩展名是 PyTorch
    文件的惯例。
- en: 'Then, after saving the model weights via the `state_dict`, we can load the
    model weights into a new `GPTModel` model instance as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在通过`state_dict`保存模型权重后，我们可以将模型权重加载到新的`GPTModel`模型实例中，如下所示：
- en: '[PRE60]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: As discussed in chapter 4, dropout helps prevent the model from overfitting
    to the training data by randomly "dropping out" of a layer's neurons during training.
    However, during inference, we don't want to randomly drop out any of the information
    the network has learned. Using `model.eval()` switches the model to evaluation
    mode for inference, disabling the dropout layers of the `model`.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 4 章所述，dropout 通过在训练期间随机“丢弃”一层的神经元来帮助防止模型过拟合训练数据。然而，在推断过程中，我们不想随机丢弃网络已经学习到的任何信息。使用`model.eval()`将模型切换到推断的评估模式，禁用`model`的
    dropout 层。
- en: If we plan to continue pretraining a model later, for example, using the `train_model_simple`
    function we defined earlier in this chapter, saving the optimizer state is also
    recommended.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计划稍后继续对模型进行预训练，例如使用本章之前定义的`train_model_simple`函数，建议同时保存优化器状态。
- en: 'Adaptive optimizers such as AdamW store additional parameters for each model
    weight. AdamW uses historical data to adjust learning rates for each model parameter
    dynamically. Without it, the optimizer resets, and the model may learn suboptimally
    or even fail to converge properly, which means that it will lose the ability to
    generate coherent text. . Using `torch.save`, we can save both the model and optimizer
    `state_dict` contents as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应优化器，如 AdamW，为每个模型权重存储额外的参数。AdamW 使用历史数据动态调整每个模型参数的学习率。如果没有它，优化器会重置，模型可能学习不佳，甚至无法正确收敛，这意味着它将失去生成连贯文本的能力。使用`torch.save`，我们可以按如下方式保存模型和优化器的`state_dict`内容：
- en: '[PRE61]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Then, we can restore the model and optimizer states as follows by first loading
    the saved data via `torch.load` and then using the `load_state_dict` method:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过首先使用`torch.load`加载保存的数据，然后使用`load_state_dict`方法来恢复模型和优化器状态：
- en: '[PRE62]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Exercise 5.4
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 5.4
- en: After saving the weights, load the model and optimizer in a new Python session
    or Jupyter notebook file and continue pretraining it for 1 more epoch using the
    `train_model_simple` function.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在保存权重后，在一个新的 Python 会话或 Jupyter notebook 文件中加载模型和优化器，并使用`train_model_simple`函数继续预训练
    1 个周期。
- en: 5.5 Loading pretrained weights from OpenAI
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 从 OpenAI 加载预训练权重
- en: Previously, for educational purposes, we trained a small GPT-2 model using a
    limited dataset comprising a short-story book. This approach allowed us to focus
    on the fundamentals without the need for extensive time and computational resources.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 此前，为了教育目的，我们使用一个有限的数据集训练了一个小型的 GPT-2 模型，该数据集包括一本短篇小说。这种方法使我们能够专注于基本知识，而无需大量时间和计算资源。
- en: Fortunately, OpenAI openly shared the weights of their GPT-2 models, thus eliminating
    the need to invest tens to hundreds of thousands of dollars in retraining the
    model on a large corpus ourselves.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，OpenAI 公开分享了他们的 GPT-2 模型的权重，从而消除了我们在大型语料库上重新训练模型所需投入数万到数十万美元的需求。
- en: In the remainder of this section, we load these weights into our GPTModel class
    and use the model for text generation. Here, *weights* refer to the weight parameters
    that are stored in the `.weight` attributes of PyTorch's `Linear` and `Embedding`
    layers, for example. We accessed them earlier via `model.parameters()` when training
    the model.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将这些权重加载到我们的 GPTModel 类中，并使用该模型进行文本生成。在这里，*权重*是指存储在 PyTorch 的`Linear`和`Embedding`层的`.weight`属性中的权重参数。例如，我们在训练模型时通过`model.parameters()`访问了它们。
- en: In the next chapters, we will reuse these pretrained weights to finetune the
    model for a text classification task and follow instructions similar to ChatGPT.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将重用这些预训练权重，以微调模型用于文本分类任务，并遵循与 ChatGPT 类似的指令。
- en: Note that OpenAI originally saved the GPT-2 weights via TensorFlow, which we
    have to install to load the weights in Python. Moreover, the following code will
    use a progress bar tool called `tqdm` to track the download process, which we
    also have to install.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，OpenAI 最初通过 TensorFlow 保存了 GPT-2 权重，因此我们必须安装它以在 Python 中加载权重。此外，以下代码将使用一个名为`tqdm`的进度条工具来跟踪下载过程，我们也必须安装它。
- en: 'You can install these libraries by executing the following command in your
    terminal:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在终端中执行以下命令来安装这些库：
- en: '[PRE63]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The download code is relatively long, mostly boilerplate, and not very interesting.
    Hence, instead of devoting precious space in this chapter to discussing Python
    code for fetching files from the internet, we download the `gpt_download.py` Python
    module directly from this chapter''s online repository:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 下载代码相对较长，大多是样板代码，并不是很有趣。因此，我们不打算在本章中花费宝贵的空间讨论用于从互联网获取文件的 Python 代码，而是直接从本章的在线库中下载
    `gpt_download.py` Python 模块：
- en: '[PRE64]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Next, after downloading this file to the local directory of your Python session,
    readers are encouraged to briefly inspect the contents of this file to ensure
    that it was saved correctly and contains valid Python code.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在将此文件下载到您的 Python 会话的本地目录后，建议读者简要检查此文件的内容，以确保它已正确保存并包含有效的 Python 代码。
- en: 'We can now import the `download_and_load_gpt2` function from the `gpt_download.py`
    file as follows, which will load the GPT-2 architecture settings (`settings`)
    and weight parameters (`params`) into our Python session:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以按如下方式从 `gpt_download.py` 文件中导入 `download_and_load_gpt2` 函数，这将把 GPT-2 架构设置
    (`settings`) 和权重参数 (`params`) 加载到我们的 Python 会话中：
- en: '[PRE65]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Executing the proceeding codel downloads the following 7 files associated with
    the 124M parameter GPT-2 model:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 执行下面的代码将下载与 124M 参数 GPT-2 模型相关的 7 个文件：
- en: '[PRE66]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Updated download instructions
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更新的下载说明
- en: If the download code does not work for you, it could be due to intermittent
    internet connection, server issues, or changes in how OpenAI shares the weights
    of the open-source GPT-2 model. In this case, please visit this chapter's online
    code repository at [https://github.com/rasbt/LLMs-from-scratch](rasbt.html) for
    alternative and updated instructions, and please reach out via the Manning Forum
    for further questions.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如果下载代码对您无效，可能是由于间歇性的互联网连接、服务器问题或 OpenAI 分享开源 GPT-2 模型权重方式的变化。在这种情况下，请访问本章的在线代码库
    [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)
    以获取替代和更新的说明，并请通过 Manning Forum 联系以获取进一步的问题。
- en: 'After the execution of the previous code has been completed, let''s inspect
    the contents of `settings` and `params`:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码执行完成后，让我们检查 `settings` 和 `params` 的内容：
- en: '[PRE67]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The contents are as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 内容如下：
- en: '[PRE68]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Both `settings` and `params` are Python dictionaries. The `settings` dictionary
    stores the LLM architecture settings similarly to our manually defined `GPT_CONFIG_124M`
    settings. The `params` dictionary contains the actual weight tensors. Note that
    we only printed the dictionary keys because printing the weight contents would
    take up too much screen space, however, we can inspect these weight tensors by
    printing the whole dictionary via `print(params)` or by selecting individual tensors
    via the respective dictionary keys, for example, the embedding layer weights:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`settings` 和 `params` 都是 Python 字典。`settings` 字典存储与我们手动定义的 `GPT_CONFIG_124M`
    设置类似的 LLM 架构设置。`params` 字典包含实际的权重张量。请注意，我们只打印了字典的键，因为打印权重内容会占用太多屏幕空间，然而，我们可以通过
    `print(params)` 打印整个字典来检查这些权重张量，或者通过各自的字典键选择单个张量，例如，嵌入层权重：'
- en: '[PRE69]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The weights of the token embedding layer are as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌嵌入层的权重如下：
- en: '[PRE70]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We downloaded and loaded the weights of the smallest GPT-2 model via the `download_and_load_gpt2(model_size="124M",
    ...)` setting. However, note that OpenAI also shares the weights of larger models:
    `"355M"`, `"774M"`, and `"1558M"`. The overall architecture of these differently-sized
    GPT models is the same, as illustrated in Figure 5.17.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 `download_and_load_gpt2(model_size="124M", ...)` 设置下载并加载了最小的 GPT-2 模型的权重。但是，请注意，OpenAI
    还分享了更大模型的权重：“355M”、“774M”和“1558M”。这些不同大小的 GPT 模型的整体架构是相同的，如图 5.17 所示。
- en: Figure 5.17 GPT-2 LLMs come in several different model sizes, ranging from 124
    million to 1,558 million parameters. The core architecture is the same, with the
    only difference being the embedding sizes and the number of times individual components
    like the attention heads and transformer blocks are repeated.
  id: totrans-314
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.17 GPT-2 LLM 有几种不同的模型大小，从 1.24 亿到 15.58 亿参数。核心架构是相同的，唯一的区别在于嵌入大小以及注意力头和变压器块等各个组件的重复次数。
- en: '![](images/05__image033.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](images/05__image033.png)'
- en: As illustrated in Figure 5.17, the overall architecture of the differently-sized
    GPT-2 models remains the same, except that different architectural elements are
    repeated different numbers of times, and the embedding size differs. The remaining
    code in this chapter is also compatible with these larger models.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 5.17 所示，不同大小的 GPT-2 模型的整体架构保持不变，除了不同的架构元素重复的次数不同，嵌入大小也不同。本章剩余的代码也与这些更大的模型兼容。
- en: After loading the GPT-2 model weights into Python, we still need to transfer
    them from the `settings` and `params` dictionaries into our `GPTModel` instance.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 GPT-2 模型权重加载到 Python 中后，我们仍需将它们从 `settings` 和 `params` 字典转移到我们的 `GPTModel`
    实例中。
- en: 'First, we create a dictionary that lists the differences between the different
    GPT model sizes, as explained in Figure 5.17:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个字典，列出不同 GPT 模型大小之间的差异，如图 5.17 所示：
- en: '[PRE71]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Suppose we are interested in loading the smallest model, `"gpt2-small (124M)"`.
    We can use the corresponding settings from the `model_configs` table able to update
    our full-length `GPT_CONFIG_124M` we defined and used earlier throughout the chapter
    as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有兴趣加载最小的模型，`"gpt2-small (124M)"`。我们可以使用 `model_configs` 表中的相应设置来更新我们之前在整个章节中定义和使用的完整
    `GPT_CONFIG_124M`，具体如下：
- en: '[PRE72]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Careful readers may remember that we used a 256-token length earlier, but the
    original GPT-2 models from OpenAI were trained with a 1,024-token length, so we
    have to update the `NEW_CONFIG` accordingly:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 细心的读者可能还记得我们之前使用了 256-token 的长度，但 OpenAI 的原始 GPT-2 模型是使用 1,024-token 的长度训练的，因此我们需要相应地更新
    `NEW_CONFIG`：
- en: '[PRE73]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Also, OpenAI used bias vectors in the multi-head attention module''s linear
    layers to implement the query, key, and value matrix computations. Bias vectors
    are not commonly used in LLMs anymore as they don''t improve the modeling performance
    and are thus unnecessary. However, since we are working with pretrained weights,
    we need to match the settings for consistency and enable these bias vectors:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，OpenAI 在多头注意力模块的线性层中使用了偏置向量，以实现查询、键和值矩阵的计算。偏置向量在大型语言模型中不再常用，因为它们不会提高建模性能，因此是多余的。然而，由于我们正在使用预训练权重，我们需要匹配设置以保持一致，并启用这些偏置向量：
- en: '[PRE74]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We can now use the updated `NEW_CONFIG` dictionary to initialize a new `GPTModel`
    instance:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用更新后的 `NEW_CONFIG` 字典来初始化一个新的 `GPTModel` 实例：
- en: '[PRE75]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: By default, the `GPTModel` instance is initialized with random weights for pretraining.
    The last step to using OpenAI's model weights is to override these random weights
    with the weights we loaded into the `params` dictionary.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`GPTModel` 实例初始化时具有随机权重用于预训练。使用 OpenAI 模型权重的最后一步是用我们加载到 `params` 字典中的权重覆盖这些随机权重。
- en: 'For this, we will first define a small `assign` utility function that checks
    whether two tensors or arrays (`left` and `right`) have the same dimensions or
    shape and returns the right tensor as trainable PyTorch parameters:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将首先定义一个小型的 `assign` 工具函数，该函数检查两个张量或数组（`left` 和 `right`）是否具有相同的维度或形状，并返回可训练的
    PyTorch 参数作为右侧张量：
- en: '[PRE76]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Next, we define a `load_weights_into_gpt` function that loads the weights from
    the `params` dictionary into a `GPTModel` instance `gpt`:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个 `load_weights_into_gpt` 函数，该函数将权重从 `params` 字典加载到 `GPTModel` 实例 `gpt`
    中：
- en: Listing 5.5 Loading OpenAI weights into our GPT model code
  id: totrans-332
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.5 将 OpenAI 权重加载到我们的 GPT 模型代码中
- en: '[PRE77]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: In the `load_weights_into_gpt` function, we carefully match the weights from
    OpenAI's implementation with our `GPTModel` implementation. To pick a specific
    example, OpenAI stored the weight tensor for the output projection layer for the
    first transformer block as `params["blocks"][0]["attn"]["c_proj"]["w"]`. In our
    implementation, this weight tensor corresponds to `gpt.trf_blocks[b].att.out_proj.weight`,
    where `gpt` is a `GPTModel` instance.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `load_weights_into_gpt` 函数中，我们仔细匹配 OpenAI 实现中的权重与我们的 `GPTModel` 实现。以一个具体示例为例，OpenAI
    将第一个变换块的输出投影层的权重张量存储为 `params["blocks"][0]["attn"]["c_proj"]["w"]`。在我们的实现中，这个权重张量对应于
    `gpt.trf_blocks[b].att.out_proj.weight`，其中 `gpt` 是一个 `GPTModel` 实例。
- en: Developing the `load_weights_into_gpt` function took a lot of guesswork since
    OpenAI used a slightly different naming convention from ours. However, the `assign`
    function would alert us if we try to match two tensors with different dimensions.
    Also, if we made a mistake in this function, we would notice this as the resulting
    GPT model would be unable to produce coherent text.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 开发`load_weights_into_gpt`函数耗费了很多猜测，因为OpenAI使用了与我们稍有不同的命名约定。然而，`assign`函数会在我们尝试匹配两个不同维度的张量时提醒我们。此外，如果我们在此函数中出错，最终生成的GPT模型将无法生成连贯的文本。
- en: 'Let''s not try the `load_weights_into_gpt` out in practice and load the OpenAI
    model weights into our `GPTModel` instance `gpt`:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们不在实践中尝试`load_weights_into_gpt`，并将OpenAI模型权重加载到我们的`GPTModel`实例`gpt`中：
- en: '[PRE78]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'If the model is loaded correctly, we can now use it to generate new text using
    our previous `generate` function:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型加载正确，我们现在可以使用之前的`generate`函数生成新文本：
- en: '[PRE79]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The resulting text is as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本如下：
- en: '[PRE80]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: We can be confident that we loaded the model weights correctly because the model
    can produce coherent text. A tiny mistake in this process would cause the model
    to fail.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以确信我们正确加载了模型权重，因为该模型能够生成连贯的文本。这个过程中的一个小错误会导致模型失败。
- en: In the following chapters, we will work further with this pretrained model and
    fine-tune it to classify text and follow instructions.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将进一步使用这个预训练模型，并对其进行微调以进行文本分类和遵循指令。
- en: Exercise 5.5
  id: totrans-344
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.5
- en: Calculate the training and validation set losses of the GPTModel with the pretrained
    weights from OpenAI on the "The Verdict" dataset.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 使用来自OpenAI的预训练权重在“The Verdict”数据集上计算GPTModel的训练和验证集损失。
- en: Exercise 5.6
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.6
- en: Readers are encouraged to experiment with GPT-2 models of different sizes, for
    example, the largest 1558M parameter model and compare the generated text to the
    124M model we loaded in this chapter.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励读者尝试不同大小的GPT-2模型，例如，最大的1558M参数模型，并将生成的文本与我们在本章加载的124M模型进行比较。
- en: 5.6 Summary
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 摘要
- en: When LLMs generate text, they output one token at a time.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当LLM生成文本时，它们一次输出一个标记。
- en: By default, the next token is generated by converting the model outputs into
    probability scores and selecting the token from the vocabulary that corresponds
    to the highest probability score, which is known as "greedy decoding."
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，下一个标记是通过将模型输出转换为概率分数，并从词汇中选择与最高概率分数对应的标记生成的，这被称为“贪婪解码”。
- en: Using probabilistic sampling and temperature scaling, we can influence the diversity
    and coherence of the generated text.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用概率采样和温度缩放，我们可以影响生成文本的多样性和连贯性。
- en: Training and validation set losses can be used to gauge the quality of text
    generated by LLM during training.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证集损失可以用来评估LLM在训练期间生成文本的质量。
- en: Pretraining an LLM involves changing its weights to minimize the training loss.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练LLM涉及更改其权重以最小化训练损失。
- en: The training loop for LLMs itself is a standard procedure in deep learning,
    using a conventional cross entropy loss and AdamW optimizer.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的训练循环本身是深度学习中的标准程序，使用常规的交叉熵损失和AdamW优化器。
- en: Pretraining an LLM on a large text corpus is time- and resource-intensive so
    we can load openly available weights from OpenAI as an alternative to pretraining
    the model on a large dataset ourselves.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大型文本语料库上预训练LLM是耗时且资源密集的，因此我们可以加载OpenAI提供的公开可用权重，作为自己在大型数据集上预训练模型的替代方案。
