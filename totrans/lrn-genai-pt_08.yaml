- en: 7 Image generation with variational autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 使用变分自编码器进行图像生成
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Autoencoders vs. variational autoencoders
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器与变分自编码器比较
- en: Building and training an Autoencoder toreconstruct handwritten digits
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练自编码器以重构手写数字
- en: Building and training a variational autoencoder to generate human face images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练变分自编码器以生成人脸图像
- en: Performing encoding arithmetic and interpolation with a trained variational
    autoencoder
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练好的变分自编码器进行编码算术和插值
- en: 'So far, you have learned how to generate shapes, numbers, and images, all by
    using generative adversarial networks (GANs). In this chapter, you’ll learn to
    create images by using another generative model: variational autoencoders (VAEs).
    You’ll also learn the practical uses of VAEs by performing encoding arithmetic
    and encoding interpolation.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了如何使用生成对抗网络（GANs）生成形状、数字和图像。在本章中，你将学习如何使用另一种生成模型：变分自编码器（VAEs）来创建图像。你还将通过执行编码算术和编码插值来了解VAEs的实际应用。
- en: 'To know how VAEs work, we first need to understand autoencoders (AEs). AEs
    have a dual-component structure: an encoder and a decoder. The encoder compresses
    the data into an abstract representation in a lower-dimensional space (the latent
    space), and the decoder decompresses the encoded information and reconstructs
    the data. The primary goal of an AE is to learn a compressed representation of
    the input data, focusing on minimizing the reconstruction error—the difference
    between the original input and its reconstruction (at the pixel level, as we have
    seen in chapter 6 when calculating cycle consistency loss). The encoder-decoder
    architecture is a cornerstone in various generative models, including Transformers,
    which you’ll explore in detail in the latter half of this book. For example, in
    chapter 9, you’ll build a Transformer for machine language translation: the encoder
    converts an English phrase into an abstract representation while the decoder constructs
    the French translation based on the compressed representation generated by the
    encoder. Text-to-image Transformers like DALL-E 2 and Imagen also utilize an AE
    architecture in their design. This involves first encoding an image into a compact,
    low-dimensional probability distribution. Then, they decode from this distribution.
    Of course, what constitutes an encoder and a decoder is different in different
    models.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解变分自编码器（VAEs）是如何工作的，我们首先需要理解自编码器（AEs）。AEs具有双组件结构：一个编码器和一个解码器。编码器将数据压缩成一个低维空间（潜在空间）中的抽象表示，而解码器则将编码信息解压缩并重构数据。AE的主要目标是学习输入数据的压缩表示，重点是最小化重构误差——原始输入与其重构（在像素级别，正如我们在第6章计算循环一致性损失时所看到的）之间的差异。编码器-解码器架构是各种生成模型的基础，包括Transformers，你将在本书的后半部分详细探索。例如，在第9章，你将构建一个用于机器语言翻译的Transformer：编码器将英语短语转换为抽象表示，而解码器则根据编码器生成的压缩表示构建法语翻译。像DALL-E
    2和Imagen这样的文本到图像Transformer也在其设计中使用了AE架构。这涉及到首先将图像编码成一个紧凑的、低维的概率分布。然后，它们从这个分布中进行解码。当然，不同模型中编码器和解码器的构成是不同的。
- en: Your first project in this chapter involves constructing and training an AE
    from scratch to generate handwritten digits. You’ll use 60,000 grayscale images
    of handwritten digits (0 to 9), each with a size of 28 × 28 = 784 pixels, as the
    training data. The encoder in the AE compresses each image into a deterministic
    vector representation with only 20 values. The decoder in the AE reconstructs
    the image with the aim of minimizing the difference between the original image
    and the reconstructed image. This is achieved by minimizing the mean absolute
    error between the two images at the pixel level. The end result is an AE capable
    of generating handwritten digits almost identical to those in the training set.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一个项目涉及从头开始构建和训练一个AE以生成手写数字。你将使用60,000个手写数字（0到9）的灰度图像作为训练数据，每个图像的大小为28 ×
    28 = 784像素。AE中的编码器将每个图像压缩成一个只有20个值的确定性向量表示。AE中的解码器重构图像，目的是最小化原始图像与重构图像之间的差异。这是通过最小化两个图像在像素级别的平均绝对误差来实现的。最终结果是能够生成与训练集几乎相同的手写数字的AE。
- en: 'While AEs are good at replicating the input data, they often falter in generating
    new samples that are not present in the training set. More importantly, AEs are
    not good at input interpolation: they often fail to generate intermediate representations
    between two input data points. This leads us to VAEs. VAEs differ from AEs in
    two critical ways. First, while an AE encodes each input into a specific point
    in the latent space, a VAE encodes it into a probability distribution within this
    space. Second, an AE focuses solely on minimizing the reconstruction error, whereas
    a VAE learns the parameters of the probability distribution for latent variables,
    minimizing a loss function that includes both reconstruction loss and a regularization
    term, the Kullback–Liebler (KL) divergence.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 AEs 在复制输入数据方面做得很好，但它们在生成训练集中不存在的新的样本时往往表现不佳。更重要的是，AEs 在输入插值方面并不擅长：它们常常无法生成两个输入数据点之间的中间表示。这让我们转向了
    VAEs。VAEs 与 AEs 在两个关键方面有所不同。首先，虽然 AE 将每个输入编码为潜在空间中的一个特定点，VAE 则将其编码为该空间内的一个概率分布。其次，AE
    仅关注最小化重建误差，而 VAE 学习潜在变量的概率分布参数，最小化一个包含重建损失和正则化项（库尔巴克-利布勒（KL）散度）的损失函数。
- en: The KL-divergence encourages the latent space to approximate a certain distribution
    (a normal distribution in our example) and ensures that the latent variables don’t
    just memorize the training data but rather capture the underlying distribution.
    It helps in achieving a well-structured latent space where similar data points
    are mapped closely together, making the space continuous and interpretable. As
    a result, we can manipulate the encodings to achieve new outcomes, which makes
    encoding arithmetic and input interpolation possible in VAEs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: KL-散度鼓励潜在空间逼近某种分布（在我们的例子中是正态分布），并确保潜在变量不仅记住训练数据，而且捕获潜在的分布。它有助于实现一个结构良好的潜在空间，其中相似的数据点被紧密映射在一起，使空间连续且可解释。因此，我们可以操作编码以实现新的结果，这使得在
    VAEs 中进行编码算术和输入插值成为可能。
- en: In the second project in this chapter, you’ll build and train a VAE from the
    ground up to generate human face images. Here, your training set comprises eyeglasses
    images that you downloaded in chapter 5\. The VAE’s encoder compresses an image
    of size 3 × 256 × 256 = 196,608 pixels into a 100-value probabilistic vector,
    each following a normal distribution. The decoder then reconstructs the image
    based on this probabilistic vector. The trained VAE can not only replicate human
    faces from the training set but also generate novel ones.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二个项目中，你将从头开始构建和训练一个 VAE，以生成人脸图像。在这里，你的训练集包括你在第 5 章下载的眼镜图像。VAE 的编码器将一个 3
    × 256 × 256 = 196,608 像素的图像压缩成一个 100 个值的概率向量，每个向量遵循正态分布。然后解码器根据这个概率向量重建图像。训练好的
    VAE 不仅可以从训练集中复制人脸，还可以生成新的图像。
- en: You’ll learn how to conduct encoding arithmetic and input interpolation in VAEs.
    You’ll manipulate the encoded representations (latent vectors) of different inputs
    to achieve specific outcomes (i.e., with or without certain characteristics in
    images) when decoded. The latent vectors control different characteristics in
    the decoded images such as gender, whether there are eyeglasses in an image, and
    so on. For example, you can first obtain the latent vectors for men with glasses
    (z1), women with glasses (z2), and women without glasses (z3). You then calculate
    a new latent vector, z4 = z1 – z2 + z3. Since both z1 and z2 lead to eyeglasses
    in images when decoded, z1 – z2 cancels out the eyeglasses feature in the resulting
    image. Similarly, since both z2 and z3 lead to a female face, z3 – z2 cancels
    out the female feature in the resulting image. Therefore, if you decode z4 = z1
    – z2 + z3 with the trained VAE, you’ll get an image of a man without glasses.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习如何在 VAEs 中进行编码算术和输入插值。你将操作不同输入的编码表示（潜在向量），在解码时达到特定的结果（例如，图像中是否有某些特征）。潜在向量控制解码图像中的不同特征，如性别、图像中是否有眼镜等。例如，你可以首先获得戴眼镜的男性（z1）、戴眼镜的女性（z2）和没戴眼镜的女性（z3）的潜在向量。然后计算一个新的潜在向量，z4
    = z1 – z2 + z3。由于 z1 和 z2 在解码时都会导致图像中出现眼镜，z1 – z2 取消了结果图像中的眼镜特征。同样，由于 z2 和 z3
    都会导致女性面孔，z3 – z2 取消了结果图像中的女性特征。因此，如果你使用训练好的 VAE 解码 z4 = z1 – z2 + z3，你会得到一个不带眼镜的男性图像。
- en: You’ll also create a series of images transitioning from a woman with glasses
    to a woman without glasses by varying the weight assigned to the latent vectors
    z1 and z2. These exercises exemplify the versatility and creative potential of
    VAEs in the field of generative models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将通过改变分配给潜在向量 z1 和 z2 的权重，创建一系列从戴眼镜的女士到不戴眼镜的女士过渡的图像。这些练习展示了 VAEs 在生成模型领域的多功能性和创意潜力。
- en: Compared to GANs, which we studied in the last few chapters, AEs and VAEs have
    a simple architecture and are easy to construct. Further, AEs and VAEs are generally
    easier and more stable to train relative to GANs. However, images generated by
    AEs and VAEs tend to be blurrier compared to those generated by GANs. GANs excel
    in generating high-quality, realistic images but suffer from training difficulties
    and resource intensiveness. The choice between GANs and VAEs largely depends on
    the specific requirements of the task at hand, including the desired quality of
    the output, computational resources available, and the importance of having a
    stable training process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在前几章中学习的 GANs 相比，AEs 和 VAEs 具有简单的架构，易于构建。此外，AEs 和 VAEs 通常比 GANs 更容易、更稳定地训练。然而，AEs
    和 VAEs 生成的图像通常比 GANs 生成的图像更模糊。GANs 在生成高质量、逼真的图像方面表现出色，但训练困难且资源密集。GANs 和 VAEs 之间的选择很大程度上取决于手头任务的特定要求，包括所需的输出质量、可用的计算资源以及稳定训练过程的重要性。
- en: 'VAEs have a wide range of practical applications in the real world. Consider,
    for instance, that you run an eyewear store and have successfully marketed a new
    style of men’s glasses online. Now, you wish to target the female market with
    the same style but lack images of women wearing these glasses, and you face high
    costs for a professional photo shoot. Here’s where VAEs come into play: you can
    combine existing images of men wearing the glasses with pictures of both men and
    women without glasses. This way, you can create realistic images of women sporting
    the same eyewear style, as illustrated in figure 7.1, through encoding arithmetic,
    a technique you’ll learn in this chapter.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 在现实世界中具有广泛的应用。例如，假设你经营一家眼镜店，并成功在线推广了一种新款男士眼镜。现在，你希望针对女性市场推广同样的款式，但缺乏女士佩戴这些眼镜的图片，而且专业摄影的成本很高。这时，VAEs
    就派上用场了：你可以将男士佩戴眼镜的现有图片与男士和女士不戴眼镜的图片结合起来。这样，你可以通过编码算术（你将在本章中学习的技术）创建女士佩戴相同眼镜风格的逼真图像，如图
    7.1 所示。
- en: '![](../../OEBPS/Images/CH07_F01_Liu.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH07_F01_Liu.png)'
- en: Figure 7.1 Generating images of women with glasses by performing encoding arithmetic
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 通过执行编码算术生成戴眼镜女士的图像
- en: In another scenario, suppose your store offers eyeglasses with dark and light
    frames, both of which are popular. You want to introduce a middle option with
    frames of an intermediate shade. With VAEs, through a method called encoding interpolation,
    you can effortlessly generate a smooth transition series of images, as shown in
    figure 7.2\. These images would vary from dark to light-framed glasses, offering
    customers a visual spectrum of choices.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种场景中，假设你的商店提供深色和浅色框架的眼镜，这两种框架都很受欢迎。你想要引入一个带有中等色调框架的中等选项。使用 VAEs，通过一种称为编码插值的方法，你可以轻松地生成一系列平滑过渡的图像，如图
    7.2 所示。这些图像将从深色框架眼镜到浅色框架眼镜变化，为顾客提供视觉选择范围。
- en: '![](../../OEBPS/Images/CH07_F02_Liu.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH07_F02_Liu.png)'
- en: Figure 7.2 Generating a series of images that transition from glasses with dark
    frames to those with light frames
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 从深色框架眼镜生成到浅色框架眼镜的图像系列
- en: The use of VAEs is not limited to eyeglasses; it extends to virtually any product
    category, be it clothing, furniture, or food. The technology provides a creative
    and cost-effective solution for visualizing and marketing a wide range of products.
    Furthermore, although image generation is a prominent example, VAEs can be applied
    to many other types of data, including music and text. Their versatility opens
    up endless possibilities in terms of practical use!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 的应用不仅限于眼镜；它几乎涵盖了任何产品类别，无论是服装、家具还是食品。这项技术为可视化营销各种产品提供了一种创造性和成本效益的解决方案。此外，尽管图像生成是一个突出的例子，但
    VAEs 可以应用于许多其他类型的数据，包括音乐和文本。它们的通用性在实用方面开辟了无限的可能性！
- en: 7.1 An overview of AEs
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 AEs 概述
- en: This section discusses what an AE is and its basic structure. For you to have
    a deep understanding of the inner workings of AEs, you’ll build and train an AE
    to generate handwritten digits as your first project in this chapter. This section
    provides an overview of an AE’s architecture and a blueprint for completing the
    first project.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了自动编码器是什么以及其基本结构。为了让你深入理解自动编码器的内部工作原理，你将在本章的第一个项目中构建和训练一个自动编码器来生成手写数字。本节概述了自动编码器的架构以及完成第一个项目的蓝图。
- en: 7.1.1 What is an AE?
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 什么是自动编码器？
- en: 'AEs are a type of neural network used in unsupervised learning that are particularly
    effective for tasks like image generation, compression, and denoising. An AE consists
    of two main parts: an encoder and a decoder. The encoder compresses the input
    into a lower-dimensional representation (latent space), and the decoder reconstructs
    the input from this representation.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是用于无监督学习的一种神经网络，特别适用于图像生成、压缩和去噪等任务。自动编码器由两个主要部分组成：编码器和解码器。编码器将输入压缩成低维表示（潜在空间），解码器则从这个表示中重建输入。
- en: The compressed representation, or latent space, captures the most important
    features of the input data. In image generation, this space encodes crucial aspects
    of the images that the network has been trained on. AEs are useful for their efficiency
    in learning data representations and their ability to work with unlabeled data,
    making them suitable for tasks like dimensionality reduction and feature learning.
    One challenge with AEs is the risk of losing information in the encoding process,
    which can lead to less accurate reconstructions. Using deeper architectures with
    multiple hidden layers can help in learning more complex and abstract representations,
    potentially mitigating information loss in AEs. Also, training AEs to generate
    high-quality images can be computationally intensive and requires large datasets.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩表示或潜在空间捕捉了输入数据的最重要特征。在图像生成中，这个空间编码了网络训练过的图像的关键方面。自动编码器（AEs）因其学习数据表示的高效性和处理未标记数据的能力而非常有用，这使得它们适用于降维和特征学习等任务。自动编码器的一个挑战是编码过程中可能会丢失信息，这可能导致重建不够准确。使用具有多个隐藏层的更深层架构可以帮助学习更复杂和抽象的表示，从而可能减轻自动编码器中的信息丢失。此外，训练自动编码器生成高质量图像可能计算量很大，需要大量数据集。
- en: As we mentioned in chapter 1, the best way to learn something is to create it
    from scratch. To that end, you’ll learn to create an AE to generate handwritten
    digits in the first project in this chapter. The next subsection provides a blueprint
    for how to do that.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第1章中提到的，学习某物的最佳方式是从零开始创建它。为此，你将在本章的第一个项目中学习如何创建一个自动编码器来生成手写数字。下一小节将提供一个如何做到这一点的蓝图。
- en: 7.1.2 Steps in building and training an AE
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 构建和训练自动编码器的步骤
- en: Imagine that you must build and train an AE from the ground up to generate grayscale
    images of handwritten digits so that you acquire the skills needed to use AEs
    for more complicated tasks such as color image generation or dimensionality reduction.
    How should you go about this task?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你必须从头开始构建和训练一个自动编码器来生成手写数字的灰度图像，以便你获得使用自动编码器进行更复杂任务（如彩色图像生成或降维）所需的技能。你应该如何进行这项任务？
- en: Figure 7.3 provides a diagram of the architecture of an AE and the steps involved
    in training an AE to generate handwritten digits.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3展示了自动编码器的架构以及训练自动编码器生成手写数字的步骤。
- en: '![](../../OEBPS/Images/CH07_F03_Liu.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH07_F03_Liu.png)'
- en: Figure 7.3 The architecture of an AE and the steps to train one to generate
    handwritten digits. An AE consists of an encoder (middle left) and a decoder (middle
    right). In each iteration of training, images of handwritten digits are fed to
    the encoder (step 1). The encoder compresses the images to deterministic points
    in the latent space (step 2). The decoder takes the encoded vectors (step 3) from
    the latent space and reconstructs the images (step 4). The AE adjusts its parameters
    to minimize the reconstruction loss, the difference between the originals and
    the reconstructions (step 5).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 自动编码器的架构以及训练其生成手写数字的步骤。自动编码器包括一个编码器（中间左侧）和一个解码器（中间右侧）。在训练的每个迭代中，将手写数字的图像输入到编码器（步骤1）。编码器将图像压缩到潜在空间中的确定性点（步骤2）。解码器从潜在空间中获取编码向量（步骤3），并重建图像（步骤4）。自动编码器调整其参数以最小化重建损失，即原始图像与重建图像之间的差异（步骤5）。
- en: 'As you can see from the figure, the AE has two main parts: an encoder (middle
    left) that compresses images of handwritten digits into vectors in the latent
    space and a decoder (middle right) that reconstructs these images based on the
    encoded vectors. Both the encoder and decoder are deep neural networks that can
    potentially include different types of layers such as dense layers, convolutional
    layers, transposed convolutional layers, and so on. Since our example involves
    grayscale images of handwritten digits, we’ll use only dense layers. However,
    AEs can also be used to generate higher-resolution color images; for those tasks,
    convolutional neural networks (CNNs) are usually included in encoders and decoders.
    Whether to use CNNs in AEs depends on the resolution of the images you want to
    generate.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如从图中所见，自动编码器（AE）有两个主要部分：一个编码器（中间左侧），它将手写数字图像压缩为潜在空间中的向量，以及一个解码器（中间右侧），它根据编码向量重建这些图像。编码器和解码器都是深度神经网络，可能包含不同类型的层，如密集层、卷积层、转置卷积层等。由于我们的例子涉及手写数字的灰度图像，我们将仅使用密集层。然而，自动编码器也可以用来生成更高分辨率的彩色图像；对于这些任务，卷积神经网络（CNNs）通常包含在编码器和解码器中。是否在自动编码器中使用CNN取决于你想要生成的图像分辨率。
- en: 'When an AE is built, the parameters in it are randomly initialized. We need
    to obtain a training set to train the model: PyTorch provides 60,000 grayscale
    images of handwritten digits, evenly distributed among the 10 digits 0 to 9\.
    The left side of figure 7.3 shows three examples, and they are images of digits
    0, 1, and 9, respectively. In the first step in the training loop, we feed images
    in the training set to the encoder. The encoder compresses the images to 20-value
    vectors in the latent space (step 2). There is nothing magical about the number
    20\. If you use 25-value vectors in the latent space, you’ll get similar results.
    We then feed the vector representations to the decoder (step 3) and ask it to
    reconstruct the images (step 4). We calculate the reconstruct loss, which is the
    mean squared error, over all the pixels, between the original image and the reconstructed
    image. We then propagate this loss back through the network to update the parameters
    in the encoder and decoder to minimize the reconstruction loss (step 5) so that
    in the next iteration, the AE can reconstruct images closer to the original ones.
    This process is repeated for many epochs over the dataset.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建自动编码器时，其中的参数是随机初始化的。我们需要获得一个训练集来训练模型：PyTorch提供了60,000张手写数字的灰度图像，这些图像在0到9这10个数字之间均匀分布。图7.3的左侧展示了三个例子，分别是数字0、1和9的图像。在训练循环的第一步中，我们将训练集中的图像输入到编码器中。编码器将图像压缩为潜在空间中的20值向量（步骤2）。数字20并没有什么神奇之处。如果你在潜在空间中使用25值向量，你将得到相似的结果。然后，我们将向量表示传递给解码器（步骤3），并要求它重建图像（步骤4）。我们计算重建损失，这是原始图像和重建图像之间所有像素的平均平方误差。然后，我们将这个损失反向传播通过网络，以更新编码器和解码器中的参数，以最小化重建损失（步骤5），这样在下一个迭代中，自动编码器可以重建更接近原始图像的图像。这个过程在数据集上重复许多个epoch。
- en: 'After the model is trained, you’ll feed unseen images of handwritten digits
    to the encoder and obtain encodings. You then feed the encodings to the decoder
    to obtain reconstructed images. You’ll notice that the reconstructed images look
    almost identical to the originals. The right side of figure 7.3 shows three examples
    of reconstructed images: they do look similar to the corresponding originals on
    the left side of the figure.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练完成后，你将向编码器提供未见过的手写数字图像，并获取编码。然后，你将编码传递给解码器以获取重建的图像。你会发现重建的图像几乎与原始图像完全相同。图7.3的右侧展示了三个重建图像的例子：它们看起来与图左侧对应的原始图像非常相似。
- en: 7.2 Building and training an AE to generate digits
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 构建和训练自动编码器以生成数字
- en: Now that you have a blueprint to build and train an AE to generate handwritten
    digits, let’s dive into the project and implement the steps outlined in the last
    section.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了构建和训练自动编码器以生成手写数字的蓝图，让我们深入到这个项目中，并实现上一节中概述的步骤。
- en: Specifically, in this section, you’ll learn first how to obtain a training set
    and a test set of images of handwritten digits. You’ll then build an encoder and
    decoder with dense layers. You’ll train the AE with the training dataset and use
    the trained encoder to encode images in the test set. Finally, you’ll learn to
    use the trained decoder to reconstruct images and compare them to the originals.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本节中，您将首先学习如何获取手写数字图像的训练集和测试集。然后，您将使用密集层构建编码器和解码器。您将使用训练数据集训练自动编码器，并使用训练好的编码器对测试集中的图像进行编码。最后，您将学习如何使用训练好的解码器重建图像，并将它们与原始图像进行比较。
- en: 7.2.1 Gathering handwritten digits
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 收集手写数字
- en: You can download grayscale images of handwritten images using the *datasets*
    package in the Torchvision library, similar to how you downloaded images of clothing
    items in chapter 2\.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Torchvision库中的`datasets`包下载手写数字的灰度图像，类似于您在第二章中下载服装物品图像的方式。
- en: 'First, let’s download a training set and a test set:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们下载一个训练集和一个测试集：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Downloads handwritten digits by using the MNIST() class in torchvision.datasets
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用`torchvision.datasets`中的`MNIST()`类下载手写数字
- en: ② The train=True argument means you download the training set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ② `train=True`参数表示您下载训练集。
- en: ③ The train=False argument means you download the test set.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ③ `train=False`参数表示您下载测试集。
- en: Instead of using the `FashionMNIST()` class as we did in chapter 2, we use the
    `MNIST()` class here. The `train` argument in the class tells PyTorch whether
    to download the training set (when the argument is set to `True`) or the test
    set (when the argument is set to `False`). Before transformation, the image pixels
    are integers ranging from 0 to 255\. The `ToTensor()` class in the preceding code
    block converts them to PyTorch float tensors with values between 0 to 1\. There
    are 60,000 images in the training set and 10,000 in the test set, evenly distributed
    among 10 digits, 0 to 9, in each set.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与第二章中使用的`FashionMNIST()`类不同，我们在这里使用`MNIST()`类。类中的`train`参数告诉PyTorch是下载训练集（当参数设置为`True`时）还是测试集（当参数设置为`False`时）。在转换之前，图像像素是介于0到255之间的整数。前面代码块中的`ToTensor()`类将它们转换为介于0到1之间的PyTorch浮点张量。训练集中有60,000张图像，测试集中有10,000张图像，每个集合中均匀分布着10个数字，从0到9。
- en: 'We’ll create batches of data for training and testing, with 32 images in each
    batch:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为训练和测试创建数据批次，每个批次包含32张图像：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have the data ready, we’ll build and train an AE next.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了准备好的数据，我们将接下来构建和训练一个自动编码器。
- en: 7.2.2 Building and training an AE
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 构建和训练自动编码器
- en: 'An AE consists of two parts: the encoder and the decoder. We’ll define an `AE()`
    class, as shown in the following listing, to represent the AE.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器由两部分组成：编码器和解码器。我们将定义一个`AE()`类，如下面的列表所示，来表示自动编码器。
- en: Listing 7.1 Creating an AE to generate handwritten digits
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1 创建一个用于生成手写数字的自动编码器
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① The input to the AE has 28 × 28 = 784 values in it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ① 自动编码器的输入有28×28=784个值。
- en: ② The latent variable (encoding) has 20 values in it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ② 潜在变量（编码）中有20个值。
- en: ③ The encoder compresses images to latent variables.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 编码器将图像压缩为潜在变量。
- en: ④ The decoder reconstructs the images based on encodings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 解码器根据编码重建图像。
- en: ⑤ The encoder and decoder form the AE.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 编码器和解码器构成了自动编码器。
- en: 'The input size is 784 because the grayscale images of handwritten digits have
    a size of 28 by 28 pixels. We flatten the images to 1D tensors and feed them to
    the AE. The images first go through the encoder: they are compressed into encodings
    in a lower dimensional space. Each image is now represented by a 20-value latent
    variable. The decoder reconstructs the images based on the latent variables. The
    output from the AE has two tensors: `out`, the reconstructed images, and `mu`,
    latent variables (i.e., encodings).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小为784，因为手写数字的灰度图像大小为28×28像素。我们将图像展平为1D张量，并将其输入到自动编码器（AE）中。图像首先通过编码器：它们被压缩到低维空间中的编码。现在每个图像都由一个20值的潜在变量表示。解码器根据潜在变量重建图像。自动编码器（AE）的输出包含两个张量：`out`，重建的图像，和`mu`，潜在变量（即编码）。
- en: 'Next, we instantiate the `AE()` class we defined earlier to create an AE. We
    also use the Adam optimizer during training, as we did in previous chapters:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实例化之前定义的`AE()`类来创建一个自动编码器。我们还在训练过程中使用了Adam优化器，就像之前章节中做的那样：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We define a function `plot_digits()` to visually inspect the reconstructed handwritten
    digits after each epoch of training, as shown in the following listing.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个名为`plot_digits()`的函数，用于在训练的每个epoch后可视检查重建的手写数字，如下面的列表所示。
- en: Listing 7.2 The `plot_digits`() function to inspect reconstructed images
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 检查重建图像的 `plot_digits()` 函数
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Collects a sample image of each digit in the test set
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ① 收集测试集中每个数字的样本图像
- en: ② Feeds the image to the AE to obtain a reconstructed image
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将图像输入到自动编码器以获得重建图像
- en: ③ Collects the reconstructed image of each original image
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 收集每个原始图像的重建图像
- en: ④ Compares the originals to the reconstructed digits visually
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 视觉比较原始图像与重建数字
- en: We first collect 10 sample images, one representing a different digit, and place
    them in a list, `originals`. We feed the images to the AE to obtain the reconstructed
    images. Finally, we plot both the originals and the reconstructed images so that
    we can compare them and assess the performance of the AE periodically.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先收集 10 张样本图像，每张代表一个不同的数字，并将它们放入一个列表 `originals` 中。我们将图像输入到自动编码器以获得重建图像。最后，我们绘制原始图像和重建图像，以便我们可以比较它们并定期评估自动编码器的性能。
- en: 'Before training starts, we call the function `plot_digits()` to visualize the
    output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始之前，我们调用 `plot_digits()` 函数来可视化输出：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You’ll see the output as shown in figure 7.4.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到如图 7.4 所示的输出。
- en: '![](../../OEBPS/Images/CH07_F04_Liu.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH07_F04_Liu.png)'
- en: Figure 7.4 Comparing reconstructed images by the AE with the originals before
    training starts. The top row shows 10 original images of handwritten digits in
    the test set. The bottom row shows the reconstructed images by the AE before training.
    The reconstructions are nothing more than pure noise.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 在训练开始之前，自动编码器重建图像与原始图像的比较。第一行显示了测试集中 10 个原始的手写数字图像。第二行显示了自动编码器在训练之前的重建图像。这些重建不过是纯粹的噪声。
- en: Though we could divide our data into training and validation sets and train
    the model until no further improvements are seen on the validation set (as we
    have done in chapter 2), our primary aim here is to grasp how AEs work, not necessarily
    to achieve the best parameter tuning. Therefore, we’ll train the AE for 10 epochs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以将我们的数据分为训练集和验证集，并训练模型直到在验证集上不再看到进一步的改进（就像我们在第 2 章中所做的那样），但我们的主要目标是掌握自动编码器的工作原理，而不一定是实现最佳参数调整。因此，我们将自动编码器训练
    10 个周期。
- en: Listing 7.3 Training the AE to generate handwritten digits
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.3 训练自动编码器生成手写数字
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Iterates through batches in the training set
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历训练集中的批次
- en: ② Uses the AE to reconstruct images
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用自动编码器重建图像
- en: ③ Calculates reconstruct loss as measured by mean squared error
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 通过均方误差计算重建损失
- en: ④ Visually inspects the performance of the AE
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 视觉检查自动编码器的性能
- en: In each epoch of training, we iterate through all batches of data in the training
    set. We feed the original images to the AE to obtain the reconstructed images.
    We then calculate the reconstruction loss, which is the mean squared error between
    the original images and the reconstructed images. Specifically, the reconstruction
    loss is obtained by first calculating the difference between the two images, pixel
    by pixel, squaring the values and averaging the squared difference. We adjust
    the model parameters to minimize the reconstruction loss, utilizing the Adam optimizer,
    which is a variation of the gradient descent method.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练周期中，我们遍历训练集中的所有数据批次。我们将原始图像输入到自动编码器以获得重建图像。然后我们计算重建损失，即原始图像和重建图像之间的均方误差。具体来说，重建损失是通过首先计算两个图像之间的差异，逐像素平方这些值并平均平方差异来获得的。我们调整模型参数以最小化重建损失，利用
    Adam 优化器，这是一种梯度下降方法的变体。
- en: 'The model takes about 2 minutes to train if you are using GPU training. Alternatively,
    you can download the trained model from my website: [https://mng.bz/YV6K](https://mng.bz/YV6K).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 GPU 训练，模型大约需要 2 分钟来训练。或者，你可以从我的网站下载训练好的模型：[https://mng.bz/YV6K](https://mng.bz/YV6K)。
- en: 7.2.3 Saving and using the trained AE
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 保存和使用训练好的自动编码器
- en: 'We’ll save the model in the local folder on your computer:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模型保存在您计算机上的本地文件夹中：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To use it to reconstruct an image of handwritten digits, we load up the model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用它来重建手写数字的图像，我们加载模型：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can use it to generate handwritten digits by calling the `plot_digits()`
    function we defined earlier:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用我们之前定义的 `plot_digits()` 函数来使用它生成手写数字：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output is shown in figure 7.5.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如图 7.5 所示。
- en: '![](../../OEBPS/Images/CH07_F05_Liu.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH07_F05_Liu.png)'
- en: Figure 7.5 Comparing reconstructed images by the trained AE with the originals.
    The top row shows 10 original images of handwritten digits in the test set. The
    bottom row shows the reconstructed images by the trained AE. The reconstructed
    images look similar to the original ones.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 比较训练 AE 重建的图像与原始图像。第一行显示了测试集中 10 个原始的手写数字图像。第二行显示了训练 AE 重建的图像。重建的图像看起来与原始图像相似。
- en: The reconstructed handwritten digits do resemble the original ones, although
    the reconstruction is not perfect. Some information gets lost during the encoding–decoding
    process. However, compared to GANs, AEs are easy to construct and take less time
    to train. Further, the encoder–decoder architecture is employed by many generative
    models. This project will help your understanding of later chapters, especially
    when we explore Transformers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重建的手写数字与原始数字相似，尽管重建并不完美。在编码-解码过程中，一些信息丢失了。然而，与 GANs 相比，AEs 更容易构建，训练时间也更短。此外，编码器-解码器架构被许多生成模型采用。这个项目将帮助您理解后面的章节，尤其是在我们探索
    Transformers 时。
- en: 7.3 What are VAEs?
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 什么是 VAEs？
- en: 'While AEs are good at reconstructing original images, they fail at generating
    novel images that are unseen in the training set. Further, AEs tend not to map
    similar inputs to nearby points in the latent space. As a result, the latent space
    associated with an AE is neither continuous nor easily interpretable. For example,
    you cannot interpolate two input data points to generate meaningful intermediate
    representations. For these reasons, we’ll study an improvement in AEs: VAEs.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 AEs 在重建原始图像方面很擅长，但它们在生成训练集中未见过的创新图像方面却失败了。此外，AEs 往往不会将相似输入映射到潜在空间中的邻近点。因此，与
    AE 相关的潜在空间既不连续也不容易解释。例如，您无法插值两个输入数据点以生成有意义的中间表示。由于这些原因，我们将研究 AE 的改进：VAEs。
- en: In this section, you’ll first learn the key differences between AEs and VAEs
    and why these differences lead to the ability of the latter to generate realistic
    images that are unseen in the training set. You’ll then learn the steps involved
    in training VAEs in general and training one to generate high-resolution human
    face images in particular.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将首先了解 AEs 和 VAEs 之间的关键区别以及这些区别为何导致后者能够生成训练集中未见过的逼真图像。然后，您将学习训练 VAEs 的一般步骤，特别是训练一个用于生成高分辨率人脸图像的
    VAE。
- en: 7.3.1 Differences between AEs and VAEs
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 AEs 和 VAEs 之间的区别
- en: 'VAEs were first proposed by Diederik Kingma and Max Welling in 2013.^([1](#footnote-000))
    They are a variant of AEs. Like an AE, a VAE also has two main parts: an encoder
    and a decoder.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 首先由 Diederik Kingma 和 Max Welling 在 2013 年提出。[1](#footnote-000) 它是 AE 的一个变体。与
    AE 一样，VAE 也包含两个主要部分：编码器和解码器。
- en: However, there are two key differences between AEs and VAEs. First, the latent
    space in an AE is deterministic. Each input is mapped to a fixed point in the
    latent space. In contrast, the latent space in a VAE is probabilistic. Instead
    of encoding an input as a single vector in the latent space, a VAE encodes an
    input as a distribution over possible values. In our second project, for example,
    we’ll encode a color image into a 100-value probabilistic vector. Additionally,
    we’ll assume that each element in this vector adheres to an independent normal
    distribution. Since defining a normal distribution requires just the mean (*μ*)
    and standard deviation (*σ*), each element in our 100-element probabilistic vector
    will be characterized by these two parameters. To reconstruct the image, we sample
    a vector from this distribution and decode it. The uniqueness of VAEs is highlighted
    by the fact that each sampling from the distribution results in a slightly varied
    output.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，AEs 和 VAEs 之间有两个关键区别。首先，AE 中的潜在空间是确定性的。每个输入都被映射到潜在空间中的一个固定点。相比之下，VAE 中的潜在空间是概率性的。VAE
    不是将输入编码为潜在空间中的一个单一向量，而是将输入编码为可能值的分布。例如，在我们的第二个项目中，我们将编码一个彩色图像到一个 100 个值的概率向量。此外，我们假设这个向量中的每个元素都遵循独立的高斯分布。由于定义高斯分布只需要均值（*μ*）和标准差（*σ*），我们
    100 个元素的概率向量中的每个元素将由这两个参数来表征。为了重建图像，我们从这个分布中采样一个向量并将其解码。VAEs 的独特性体现在每次从分布中采样都会产生略微不同的输出。
- en: In statistical terms, the encoder in a VAE is trying to learn the true distribution
    of the training data *x*, *p*(*x*|*Θ*), where *Θ* is the parameters defining the
    distribution. For tractability, we usually assume that the distribution of the
    latent variable is normal. Because we only need the mean, *μ*, and standard deviation,
    *σ*, to define a normal distribution, we can rewrite the true distribution as
    *p*(*x*|*Θ*) = *p*(*x*|*μ*, *σ*). The decoder in the VAE generates a sample based
    on the distribution learned by the encoder. That is, the decoder generates an
    instance probabilistically from the distribution *p*(*x*|*μ*, *σ*).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计术语中，VAE 中的编码器试图学习训练数据 *x* 的真实分布 *p*(*x*|*Θ*)，其中 *Θ* 是定义分布的参数。为了便于处理，我们通常假设潜在变量的分布是正态的。因为我们只需要均值
    *μ* 和标准差 *σ* 来定义一个正态分布，我们可以将真实分布重写为 *p*(*x*|*Θ*) = *p*(*x*|*μ*, *σ*)。VAE 中的解码器根据编码器学习的分布生成一个样本。也就是说，解码器从分布
    *p*(*x*|*μ*, *σ*) 中以概率生成一个实例。
- en: 'The second key difference between AEs and VAEs lies in the loss function. When
    training an AE, we minimize the reconstruction loss so that the reconstructed
    images are as close to the originals as possible. In contrast, in VAEs, the loss
    function consists of two parts: the reconstruction loss and the KL divergence.
    KL divergence is a measure of how one probability distribution diverges from a
    second, expected probability distribution. In VAEs, KL divergence is used to regularize
    the encoder by penalizing deviations of the learned distribution (the encoder’s
    output) from a prior distribution (a standard normal distribution). This encourages
    the encoder to learn meaningful and generalizable latent representations. By penalizing
    distributions that are too far from the prior, KL divergence helps to avoid overfitting.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: AEs 和 VAEs 之间的第二个关键区别在于损失函数。当训练一个 AE 时，我们最小化重建损失，以便重建的图像尽可能接近原始图像。相比之下，在 VAEs
    中，损失函数由两部分组成：重建损失和 KL 散度。KL 散度是衡量一个概率分布如何偏离第二个预期概率分布的度量。在 VAEs 中，KL 散度用于通过惩罚学习到的分布（编码器的输出）与先验分布（标准正态分布）的偏差来正则化编码器。这鼓励编码器学习有意义的和可推广的潜在表示。通过惩罚偏离先验太远的分布，KL
    散度有助于避免过拟合。
- en: 'The KL divergence is calculated as follows in our setting since we assume a
    normal distribution (the formula is different if a nonnormal distribution is assumed):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的设置中，KL 散度是这样计算的，因为我们假设了一个正态分布（如果假设非正态分布，公式则不同）：
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/CH07_F06_Liu_EQ01.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH07_F06_Liu_EQ01.png)'
- en: '| (7.1) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| (7.1) |'
- en: The summation is taken over all 100 dimensions of the latent space. When the
    encoder compresses the images into standard normal distributions in the latent
    space, such that *μ*=0 and *σ*=1, the KL divergence becomes 0\. In any other scenario,
    the value exceeds 0\. Thus, the KL divergence is minimized when the encoder successfully
    compresses the images into standard normal distributions within the latent space.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 求和是在潜在空间的 100 个维度上进行的。当编码器将图像压缩到潜在空间中的标准正态分布时，使得 *μ*=0 和 *σ*=1，KL 散度变为 0。在任何其他情况下，值都超过
    0。因此，当编码器成功将图像压缩到潜在空间中的标准正态分布时，KL 散度被最小化。
- en: 7.3.2 The blueprint to train a VAE to generate human face images
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 训练 VAE 生成人脸图像的蓝图
- en: In the second project in this chapter, you’ll build and train a VAE from scratch
    to generate color images of human faces. The trained model can generate images
    that are unseen in the training set. Further, you can interpolate inputs to generate
    novel images that are intermediate representations between two input data points.
    The following is a blueprint for this second project.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二个项目中，你将从头开始构建和训练一个 VAE，以生成人脸彩色图像。训练好的模型可以生成训练集中未见过的图像。此外，你可以插值输入以生成介于两个输入数据点之间的新颖图像，这些图像是两个输入数据点之间的中间表示。以下是这个第二个项目的蓝图。
- en: Figure 7.6 provides a diagram of the architecture of a VAE and the steps in
    training a VAE to generate human face images.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 提供了 VAE 架构的图示以及训练 VAE 生成人脸图像的步骤。
- en: '![](../../OEBPS/Images/CH07_F06_Liu.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH07_F06_Liu.png)'
- en: Figure 7.6 The architecture of a VAE and the steps to train one to generate
    human face images. A VAE consists of an encoder (middle upper left) and a decoder
    (middle bottom right). In each iteration of training, human face images are fed
    to the encoder (step 1). The encoder compresses the images to probabilistic points
    in the latent space (step 2; since we assume normal distributions, each probability
    point is characterized by a vector of means and a vector of standard deviations).
    We then sample encodings from the distribution and present them to the decoder.
    The decoder takes sampled encodings (step 3) and reconstructs images (step 4).
    The VAE adjusts its parameters to minimize the sum of reconstruction loss and
    the KL divergence. The KL divergence measures the difference between the encoder’s
    output and a standard normal distribution.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6展示了VAE（变分自编码器）的架构以及训练其生成人脸图像的步骤。VAE由一个编码器（中间左上角）和一个解码器（中间右下角）组成。在训练的每一次迭代中，人脸图像被输入到编码器（步骤1）。编码器将图像压缩到潜在空间中的概率点（步骤2；由于我们假设正态分布，每个概率点由均值向量和标准差向量表征）。然后，我们从分布中采样编码并展示给解码器。解码器接收采样的编码（步骤3）并重建图像（步骤4）。VAE调整其参数以最小化重建损失和KL散度的总和。KL散度衡量编码器输出与标准正态分布之间的差异。
- en: 'Figure 7.6 shows that a VAE also has two parts: an encoder (middle top left)
    and a decoder (middle bottom right). Since the second project involves high-resolution
    color images, we’ll use CNNs to create the VAE. As we discussed in chapter 4,
    high-resolution color images contain many more pixels than low-resolution grayscale
    images. If we use fully connected (dense) layers only, the number of parameters
    in the model is too large, making learning slow and ineffective. CNNs require
    fewer parameters than fully connected networks of similar size, leading to faster
    and more effective learning.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6显示VAE也有两个部分：一个编码器（中间左上角）和一个解码器（中间右下角）。由于第二个项目涉及高分辨率彩色图像，我们将使用CNN（卷积神经网络）来创建VAE。正如我们在第4章中讨论的，高分辨率彩色图像包含比低分辨率灰度图像更多的像素。如果我们只使用全连接（密集）层，模型中的参数数量太大，使得学习过程缓慢且无效。与类似大小的全连接网络相比，CNN需要的参数更少，从而实现更快、更有效的学习。
- en: Once the VAE is created, you’ll use the eyeglasses dataset that you downloaded
    in chapter 5 to train the model. The left side of figure 7.6 shows three examples
    of the original human face images in the training set. In the first step in the
    training loop, we feed images in the training set, with a size of 3 × 256 × 256
    = 196,608 pixels, to the encoder. The encoder compresses the images to 100-value
    probabilistic vectors in the latent space (step 2; vectors of means and standard
    deviations due to the assumption of normal distribution). We then sample from
    the distribution and feed the sampled vector representations to the decoder (step
    3) and ask it to reconstruct the images (step 4). We calculate the total loss
    as the sum of the reconstruction loss at the pixel level and the KL divergence
    as specified in equation 7.1\. We propagate this loss back through the network
    to update the parameters in the encoder and decoder to minimize the total loss
    (step 5). The total loss encourages the VAE to encode the inputs into more meaningful
    and generalizable latent representations and to reconstruct images closer to the
    originals.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了VAE，你将使用在第5章中下载的眼镜数据集来训练模型。图7.6的左侧展示了训练集中三个原始人脸图像的例子。在训练循环的第一步中，我们将大小为3
    × 256 × 256 = 196,608像素的图像输入到编码器。编码器将图像压缩到潜在空间中的100值概率向量（步骤2；由于假设正态分布，向量为均值和标准差向量）。然后，我们从分布中采样并输入采样的向量表示到解码器（步骤3），并要求其重建图像（步骤4）。我们计算总损失为像素级重建损失和方程7.1中指定的KL散度的总和。我们将这个损失反向传播通过网络以更新编码器和解码器中的参数，以最小化总损失（步骤5）。总损失鼓励VAE将输入编码成更有意义和可推广的潜在表示，并重建更接近原始图像的图像。
- en: 'After the model is trained, you’ll feed human face images to the encoder and
    obtain encodings. You then feed the encodings to the decoder to obtain reconstructed
    images. You’ll notice that the reconstructed images look close to the originals.
    The right side of figure 7.6 shows three examples of reconstructed images: they
    look similar to the corresponding originals on the left side of the figure, though
    not perfectly.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，你将向编码器输入人脸图像并获取编码。然后，你将编码输入到解码器以获取重建图像。你会发现重建的图像与原始图像非常接近。图7.6的右侧展示了三个重建图像的例子：它们与图左侧对应的原始图像相似，尽管并不完全一样。
- en: More importantly, you can discard the encoder and randomly draw encodings from
    the latent space and feed them to the trained decoder in VAE to generate novel
    human face images that are unseen in the training set. Further, you can manipulate
    the encoded representations of different inputs to achieve specific outcomes when
    decoded. You can also create a series of images transitioning from one instance
    to another by varying the weight assigned to any two encodings.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 A VAE to generate human face images
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section creates and trains a VAE from scratch to generate human face images
    by following the steps outlined in the last section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Compared to what we have done to build and train AEs, our approach for the second
    project incorporates several modifications. Firstly, we plan to use CNNs in both
    the encoders and decoders of VAEs, particularly because high-resolution color
    images possess a greater number of pixels. Relying solely on fully connected (dense)
    layers would result in an excessively large number of parameters, leading to slow
    and inefficient learning. Second, as part of our process to compress images into
    vectors that follow a normal distribution in the latent space, we will generate
    both a mean vector and a standard deviation vector during the encoding of each
    image. This differs from the fixed value vector used in AEs. From the encoded
    normal distribution, we’ll then sample to obtain encodings, which are subsequently
    decoded to produce images. Notably, each reconstructed image will vary slightly
    every time we sample from this distribution, which gives rise to VAEs’ ability
    to generate novel images.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.1 Building a VAE
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you recall, the eyeglasses dataset that you downloaded in chapter 5 is saved
    in the folder /files/glasses/ on your computer after some labels are manually
    corrected. We’ll resize the images to 256 by 256 pixels with values between 0
    and 1\. We then create a batch iterator with 16 images in each batch:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Resizes images to 256 by 256 pixels
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts images to tensors with values between 0 and 1
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: ③ Loads images from the folder and apply the transformations
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: ④ Places the data in a batch iterator
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll create a VAE that includes convolutional and transposed convolutional
    layers. We first define an `Encoder()` class as follows.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.4 The encoder in the VAE
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① The dimension of the latent space is 100.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: ② The mean of the distribution of the encodings
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: ③ The standard deviation of the encodings
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: ④ The encoded vector representation
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder network consists of several convolutional layers, which extract
    the spatial features of the input images. The encoder compresses the inputs into
    vector representations, `z`, which are normally distributed with means, `mu`,
    and standard deviations, `std`. The output from the encoder consists of three
    tensors: `mu`, `std`, and `z`. While the `mu` and `std` are the mean and standard
    deviation of the probabilistic vector, respectively, `z` is an instance sampled
    from this distribution.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the input image, with a size of (3, 256, 256), first goes through
    a Conv2d layer with a stride value of 2\. As we explained in chapter 4, this means
    the filter skips two pixels each time it moves on the input image, which leads
    to downsampling of the image. The output has a size of (8, 128, 128). It then
    goes through two more Conv2d layers, and the size becomes (32, 31, 31). It is
    flattened and passed through linear layers to obtain values of `mu` and `std`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，输入图像，大小为（3, 256, 256），首先通过一个步长值为2的Conv2d层。正如我们在第4章中解释的，这意味着滤波器每次在输入图像上移动时跳过两个像素，这导致图像下采样。输出的大小为（8,
    128, 128）。然后它通过两个更多的Conv2d层，大小变为（32, 31, 31）。它被展平并通过线性层获得`mu`和`std`的值。
- en: We define a `Decoder()` class to represent the decoder in the VAE.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个`Decoder()`类来表示VAE中的解码器。
- en: Listing 7.5 The decoder in the VAE
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5 VAE中的解码器
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Encodings first go through two dense layers.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ① 编码首先通过两个密集层。
- en: ② Reshapes encodings into multidimensional objects so we can perform transposed
    convolutional operations on them
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将编码重新塑造成多维对象，以便我们可以对它们执行转置卷积操作
- en: ③ Passes the encodings through three transposed convolutional lay
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将编码通过三个转置卷积层
- en: ④ Squeezes the output to values between 0 and 1, the same as the values in the
    input images
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将输出挤压到0到1之间的值，与输入图像中的值相同
- en: 'The decoder is a mirror image of the encoder: instead of performing convolutional
    operations, it performs transposed convolutional operations on the encodings to
    generate feature maps. It gradually converts encodings in the latent space back
    into high-resolution color images.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器是编码器的镜像：它不是执行卷积操作，而是在编码上执行转置卷积操作以生成特征图。它逐渐将潜在空间中的编码转换回高分辨率彩色图像。
- en: Specifically, the encoding first goes through two linear layers. It’s then unflattened
    to a shape (32, 31, 31), mirroring the size of the image after the last Conv2d
    layer in the encoder. It then goes through three ConvTranspose2d layers, mirroring
    the Conv2d layers in the encoder. The output from the decoder has a shape of (3,
    256, 256), the same as that of the training image.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，编码首先通过两个线性层。然后它被反展平到一个形状（32, 31, 31），与编码器中最后一个Conv2d层之后的图像大小相匹配。然后它通过三个ConvTranspose2d层，与编码器中的Conv2d层相匹配。解码器的输出形状为（3,
    256, 256），与训练图像相同。
- en: 'We’ll combine the encoder with the decoder to create a VAE:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编码器与解码器结合起来创建一个VAE：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Creates an encoder by instantiating the Encoder() class
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ① 通过实例化Encoder()类创建一个编码器
- en: ② Creates a decoder by instantiating the Decoder() class
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ② 通过实例化Decoder()类创建一个解码器
- en: ③ Passes the input through the encoder to obtain the encoding
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将输入通过编码器以获取编码
- en: ④ The output of the VAE is the mean and standard deviation of the encodings,
    as well as the reconstructed images.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ④ VAE的输出是编码的均值和标准差，以及重建的图像。
- en: 'The VAE consists of an encoder and a decoder, as defined by the `Encoder()`
    and `Decoder()` classes. When we pass images through the VAE, the output consists
    of three tensors: the mean and standard deviation of the encodings and the reconstructed
    images.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: VAE由一个编码器和一个解码器组成，由`Encoder()`和`Decoder()`类定义。当我们通过VAE传递图像时，输出包括三个张量：编码和重建图像的均值和标准差。
- en: 'Next, we create a VAE by instantiating the `VAE()` class and define the optimizer
    for the model:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过实例化`VAE()`类创建一个VAE并定义模型的优化器：
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We’ll manually calculate the reconstruction loss and the KL-divergence loss
    during training. Therefore, we don't define a loss function here.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在训练期间手动计算重建损失和KL散度损失。因此，我们在此处不定义损失函数。
- en: 7.4.2 Training the VAE
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 训练VAE
- en: To train the model, we first define a `train_epoch()` function to train the
    model for one epoch.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们首先定义一个`train_epoch()`函数来训练模型一个epoch。
- en: Listing 7.6 Defining the `train_epoch()` function
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6 定义`train_epoch()`函数
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Obtains the reconstructed images
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ① 获取重建的图像
- en: ② Calculates the reconstruction loss
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ② 计算重建损失
- en: ③ Calculates the KL divergence
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算KL散度
- en: ④ Sum of the reconstruction loss and the KL divergence.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 重建损失和KL散度的总和。
- en: We iterate through all batches in the training set. We pass images through the
    VAE to obtain reconstructed images. The total loss is the sum of the reconstruction
    loss and the KL divergence. The model parameters are adjusted in each iteration
    to minimize the total loss.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历训练集中的所有批次。我们通过VAE传递图像以获取重建图像。总损失是重建损失和KL散度的总和。在每个迭代中调整模型参数以最小化总损失。
- en: 'We also define a `plot_epoch()` function to visually inspect the generated
    images by the VAE:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个 `plot_epoch()` 函数，用于直观检查 VAE 生成的图像：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A well-trained VAE can map similar inputs to nearby points in the latent space,
    leading to a more continuous and interpretable latent space. As a result, we can
    randomly draw vectors from the latent space, and the VAE can decode the vectors
    into meaningful outputs. Therefore, in the previous function `plot_epoch()`, we
    randomly draw 18 vectors from the latent space and use them to generate 18 images
    after each epoch of training. We plot them in a 3 × 6 grid and visually inspect
    them to see how the VAE is performing during the training process.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一个训练良好的 VAE 可以将相似的输入映射到潜在空间中的邻近点，从而得到一个更连续和可解释的潜在空间。因此，我们可以从潜在空间中随机抽取向量，VAE
    可以将这些向量解码成有意义的输出。因此，在前面的 `plot_epoch()` 函数中，我们随机从潜在空间中抽取 18 个向量，并在每个训练周期后使用它们生成
    18 张图像。我们将它们绘制在一个 3 × 6 的网格中，并直观地检查它们以了解 VAE 在训练过程中的表现。
- en: 'Next, we train the VAE for 10 epochs:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练 VAE 模型 10 个周期：
- en: '[PRE17]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This training takes about half an hour if you use GPU training or several hours
    otherwise. The trained model weights are saved on your computer. Alternatively,
    you can download the trained weights from my website: [https://mng.bz/GNRR](https://mng.bz/GNRR).
    Make sure you unzip the file after downloading.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用 GPU 训练，这个过程大约需要半小时，否则需要几个小时。训练好的模型权重已保存在您的计算机上。或者，您可以从我的网站上下载训练好的权重：[https://mng.bz/GNRR](https://mng.bz/GNRR)。请确保下载后解压文件。
- en: 7.4.3 Generating images with the trained VAE
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 使用训练好的 VAE 生成图像
- en: 'Now that the VAE is trained, we can use it to generate images. We first load
    the weights of the trained model that we saved in the local folder:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，VAE 已经训练好了，我们可以用它来生成图像。我们首先加载保存在本地文件夹中的训练好的模型权重：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We then check the VAE’s ability to reconstruct images and see how closely they
    resemble the originals:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们检查 VAE 重建图像的能力，并观察它们与原始图像的相似程度：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you run the previous code block, you’ll see an output similar to figure 7.7.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行前面的代码块，您将看到类似于图 7.7 的输出。
- en: '![](../../OEBPS/Images/CH07_F07_Liu.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH07_F07_Liu.png)'
- en: Figure 7.7 Comparing the reconstructed images by a trained VAE with the originals.
    The first and the third rows are the original images. We feed them to the trained
    VAE to obtain the reconstructed images, which are shown below the original images.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 比较训练好的 VAE 重建的图像与原始图像。第一行和第三行是原始图像。我们将它们输入到训练好的 VAE 模型中，以获得重建的图像，这些图像显示在原始图像下方。
- en: 'The original images are shown in the first and third rows, while the reconstructed
    images are shown below the originals. The reconstructed images resemble the originals,
    as shown in figure 7.7\. However, some information gets lost during the reconstruction
    process: they don’t look as realistic as the originals.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像显示在第一行和第三行，而重建的图像显示在原始图像下方。重建的图像与原始图像相似，如图 7.7 所示。然而，在重建过程中会丢失一些信息：它们看起来没有原始图像那么真实。
- en: 'Next, we test the VAE’s ability to generate novel images that are unseen in
    the training set, by calling the plot_epoch() function we defined before:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过调用之前定义的 `plot_epoch()` 函数来测试 VAE 生成训练集中未见过的创新图像的能力：
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The function randomly draws 18 vectors from the latent space and passes them
    to the trained VAE to generate 18 images. The output is shown in figure 7.8.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 函数从潜在空间随机抽取 18 个向量，并将它们传递给训练好的 VAE 模型以生成 18 张图像。输出结果如图 7.8 所示。
- en: '![](../../OEBPS/Images/CH07_F08_Liu.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH07_F08_Liu.png)'
- en: Figure 7.8 Novel images generated by the trained VAE. We randomly draw vector
    representations in the latent space and feed them to the decoder in the trained
    VAE. The decoded images are shown in this figure. Since the vector representations
    are randomly drawn, the images don’t correspond to any originals in the training
    set.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 由训练好的 VAE 生成的创新图像。我们在潜在空间中随机抽取向量表示，并将它们输入到训练好的 VAE 模型的解码器中。解码后的图像显示在本图中。由于向量表示是随机抽取的，因此这些图像与训练集中的任何原始图像都不对应。
- en: 'These images are not present in the training set: the encodings are randomly
    drawn from the latent space, not the encoded vectors after passing images in the
    training set through the encoder. This is because the latent space in VAEs is
    continuous and interpretable. New and unseen encodings in the latent space can
    be meaningfully decoded into images that resemble but differ from those in the
    training set.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.4 Encoding arithmetic with the trained VAE
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: VAEs include a regularization term (KL divergence) in their loss function, which
    encourages the latent space to approximate a normal distribution. This regularization
    ensures that the latent variables don’t just memorize the training data but rather
    capture the underlying distribution. It helps to achieve a well-structured latent
    space where similar data points are mapped closely together, making the space
    continuous and interpretable. As a result, we can manipulate the encodings to
    achieve new outcomes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'To make results reproducible, I encourage you to download the trained weights
    from my website ([https://mng.bz/GNRR](https://mng.bz/GNRR)) and use the same
    code blocks for the rest of the chapter. As we explained in the introduction,
    encoding arithmetic allows us to generate images with certain features. To illustrate
    how encoding arithmetic works in VAEs, let’s first hand-collect three images in
    each of the following four groups: men with glasses, men without glasses, women
    with glasses, and women without glasses.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.7 Collecting images with different characteristics
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ① Displays 25 images with eyeglasses
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: ② Selects three images of men with glasses
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: ③ Selects three images of women with glasses
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: ④ Displays 25 images without eyeglasses
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Selects three images of men without glasses
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Selects three images of women without glasses
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: We select three images in each group instead of just one so that we can calculate
    the average of multiple encodings in the same group when performing encoding arithmetic
    later. VAEs are designed to learn the distribution of the input data in the latent
    space. By averaging multiple encodings, we effectively smooth out the representation
    in this space. This helps us find an average representation that captures common
    features among different samples within a group.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Next we feed the three images of men with glasses to the trained VAE to obtain
    their encodings in the latent space. We then calculate the average encoding for
    the three images and use it to obtain a reconstructed image of a man with glasses.
    We then repeat this for the other three groups.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.8 Encoding and decoding images in four different groups
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ① Creates a batch of images of men with glasses
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: ② Obtains the average encoding for men with glasses
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: ③ Decodes the average encoding for men with glasses
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: ④ Obtains the average encodings for the other three groups
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Decodes the average encodings for the other three groups
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'The average encodings for the four groups are `men_g_encoding`, `women_g_encoding`,
    `men_ng_encoding`, and `women_ng_encoding`, respectively, where `g` stands for
    glasses and `ng` for no glasses. The decoded images for the four groups are `men_g_recon`,
    `women_g_recon`, `men_ng_recon`, and `women_ng_recon`, respectively. We plot the
    four images:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You’ll see the output as shown in figure 7.9.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F09_Liu.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9 Decoded images based on average encodings. We first obtain three
    images in each of the following four groups: men with glasses, women with glasses,
    men without glasses, and women without glasses. We feed the 12 images to the encoder
    in the trained VAE to obtain their encodings in the latent space. We then calculate
    the average encoding of the three images in each group. The four average encodings
    are fed to the decoder in the trained VAE to obtain four images and they are shown
    in this figure.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: The four decoded images are shown in figure 7.9\. They are the composite images
    representing the four groups. Notice that they are different from any of the original
    12 images. At the same time, they preserve the defining characteristics of each
    group.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s manipulate the encodings to create a new encoding and then use the
    trained decoder in the VAE to decode the new encoding and see what happens. For
    example, we can subtract the average encoding of women with glasses from the average
    encoding of men with glasses and add the average encoding of women without glasses.
    We then feed the result to the decoder and see the output.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.9 An example of encoding arithmetic
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ① Defines z as the encoding of men with glasses – women with glasses + women
    without glasses
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: ② Decodes z to generate an image
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: ③ Displays the four images
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: ④ Displays a title on top of the images
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: If you run the code block in listing 7.9, you’ll see an output as shown in figure
    7.10.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F10_Liu.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10 An example of encoding arithmetic with the trained VAE. We first
    obtain the average encodings for the following three groups: men with glasses
    (z1), women with glasses (z2), and women without glasses (z3). We define a new
    encoding z = z1 – z2 + z3. We then feed z to the decoder in the trained VAE and
    obtain the decoded image, as shown at the far right of this figure.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The first three images in figure 7.10 are the composite images representing
    the three input groups. The output image, at the far right, is an image of a man
    without glasses.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Since both `men_g_encoding` and `women_g_encoding` lead to eyeglasses in images
    when decoded, `men_g_encoding` – `women_g_encoding` cancels out eyeglasses features
    in the resulting image. Similarly, since both `women_ng_encoding` and `women_g_encoding`
    lead to a female face, `women_ng_encoding` – `women_g_encoding` cancels out female
    features in the resulting image. Therefore, if you decode `men_g_encoding` + `women_g_encoding`
    –`women_ng_encoding` with the trained VAE, you’ll get an image of a man without
    glasses. The encoding arithmetic in this example shows that an encoding for men
    without glasses can be obtained by manipulating the average encodings in the other
    three groups.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.1
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following encoding arithmetics by modifying code listing 7.9:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Subtract the average encoding of men without glasses from the average encoding
    of men with glasses and add the average encoding of women without glasses. Feed
    the result to the decoder and see what happens.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract the average encoding of women without glasses from the average encoding
    of men without glasses and add the average encoding of women with glasses. Feed
    the result to the decoder and see what happens.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Subtract the average encoding of men without glasses from the average encoding
    of women without glasses and add the average encoding of men with glasses. Feed
    the result to the decoder and see what happens. Make sure you modify the image
    titles to reflect the changes. The solutions are provided in the book’s GitHub
    repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further, we can interpolate any two encodings in the latent space by assigning
    different weights to them and creating a new encoding. We can then decode the
    new encoding and create a composite image as a result. By choosing different weights,
    we can create a series of intermediate images that transition from one image to
    another.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the encodings of women with and without glasses as an example. We’ll
    define a new encoding `z` as `w*women_ng_encoding+(1-w)*women_g_encoding`, where
    `w` is the weight we put on `women_ng_encoding`. We’ll change the value of `w`
    from 0 to 1 with an increment of 0.2 in each step. We then decode them and display
    the resulting six images.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.10 Interpolating two encodings to create a series of images
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ① Iterates through six different values of w
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: ② Interpolates between two encoding
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: ③ Decodes the interpolated encoding
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: ④ Displays the six resulting images
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: After running the code in listing 7.10, you’ll see an output as shown in figure
    7.11.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F11_Liu.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 Interpolating encodings to create a series of intermediate images.
    We first obtain the average encodings for women with glasses (`women_g_encoding`)
    and women without glasses (`women_ng_encoding`). The interpolated encoding z is
    defined as `w*women_ng_encoding+(1-w)*women_g_encoding`, where w is the weight
    on `women_ng_encoding`. We change the value of w from 0 to 1 with an increment
    of 0.2 to create six interpolated encodings. We then decode them and display the
    resulting six images in the figure.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in figure 7.11, as you move from left to right, the image gradually
    transitions from a woman with glasses to a woman without glasses. This shows that
    the encodings in the latent space are continuous, meaningful, and interpolatable.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.2
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify listing 7.10 to create a series of intermediate images by using the
    following pairs of encodings: (i) `men_ng_encoding` and `men_g_encoding`; (ii)
    `men_ng_encoding` and `women_ng_encoding`; (iii) `men_g_encoding` and `women_g_encoding`.
    The solutions are provided in the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting in the next chapter, you’ll embark on a journey in natural language
    processing. This will enable you to generate another form of content: text. However,
    many tools you have used so far will be used again in later chapters, such as
    deep neural networks and the encoder-decoder architecture.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AEs have a dual-component structure: an encoder and a decoder. The encoder
    compresses the data into an abstract representation in a lower-dimensional space
    (the latent space), and the decoder decompresses the encoded information and reconstructs
    the data.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VAEs also consist of an encoder and a decoder. They differ from AEs in two critical
    ways. First, while an AE encodes each input into a specific point in the latent
    space, a VAE encodes it into a probability distribution within this space. Second,
    an AE focuses solely on minimizing the reconstruction error, whereas a VAE learns
    the parameters of the probability distribution for latent variables, minimizing
    a loss function that includes both reconstruction loss and a regularization term,
    the KL divergence.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The KL divergence in the loss function when training VAEs ensures the distribution
    for latent variables resembles a normal distribution. This encourages the encoder
    to learn continuous, meaningful, and generalizable latent representations.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A well-trained VAE can map similar inputs to nearby points in the latent space,
    leading to a more continuous and interpretable latent space. As a result, VAEs
    can decode random vectors in the latent space into meaningful outputs, leading
    to images that are unseen in the training set.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latent space in a VAE is continuous and interpretable, different from that
    in an AE. As a result, we can manipulate the encodings to achieve new outcomes.
    We can also create a series of intermediate images transitioning from one instance
    to another by varying weights on two encodings in the latent space.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](#footnote-000-backlink))  Diederik P Kingma and Max Welling, 2013, “Auto-Encoding
    Variational Bayes.” [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
