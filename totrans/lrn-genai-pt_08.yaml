- en: 7 Image generation with variational autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders vs. variational autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and training an Autoencoder toreconstruct handwritten digits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and training a variational autoencoder to generate human face images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing encoding arithmetic and interpolation with a trained variational
    autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, you have learned how to generate shapes, numbers, and images, all by
    using generative adversarial networks (GANs). In this chapter, you’ll learn to
    create images by using another generative model: variational autoencoders (VAEs).
    You’ll also learn the practical uses of VAEs by performing encoding arithmetic
    and encoding interpolation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To know how VAEs work, we first need to understand autoencoders (AEs). AEs
    have a dual-component structure: an encoder and a decoder. The encoder compresses
    the data into an abstract representation in a lower-dimensional space (the latent
    space), and the decoder decompresses the encoded information and reconstructs
    the data. The primary goal of an AE is to learn a compressed representation of
    the input data, focusing on minimizing the reconstruction error—the difference
    between the original input and its reconstruction (at the pixel level, as we have
    seen in chapter 6 when calculating cycle consistency loss). The encoder-decoder
    architecture is a cornerstone in various generative models, including Transformers,
    which you’ll explore in detail in the latter half of this book. For example, in
    chapter 9, you’ll build a Transformer for machine language translation: the encoder
    converts an English phrase into an abstract representation while the decoder constructs
    the French translation based on the compressed representation generated by the
    encoder. Text-to-image Transformers like DALL-E 2 and Imagen also utilize an AE
    architecture in their design. This involves first encoding an image into a compact,
    low-dimensional probability distribution. Then, they decode from this distribution.
    Of course, what constitutes an encoder and a decoder is different in different
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: Your first project in this chapter involves constructing and training an AE
    from scratch to generate handwritten digits. You’ll use 60,000 grayscale images
    of handwritten digits (0 to 9), each with a size of 28 × 28 = 784 pixels, as the
    training data. The encoder in the AE compresses each image into a deterministic
    vector representation with only 20 values. The decoder in the AE reconstructs
    the image with the aim of minimizing the difference between the original image
    and the reconstructed image. This is achieved by minimizing the mean absolute
    error between the two images at the pixel level. The end result is an AE capable
    of generating handwritten digits almost identical to those in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'While AEs are good at replicating the input data, they often falter in generating
    new samples that are not present in the training set. More importantly, AEs are
    not good at input interpolation: they often fail to generate intermediate representations
    between two input data points. This leads us to VAEs. VAEs differ from AEs in
    two critical ways. First, while an AE encodes each input into a specific point
    in the latent space, a VAE encodes it into a probability distribution within this
    space. Second, an AE focuses solely on minimizing the reconstruction error, whereas
    a VAE learns the parameters of the probability distribution for latent variables,
    minimizing a loss function that includes both reconstruction loss and a regularization
    term, the Kullback–Liebler (KL) divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: The KL-divergence encourages the latent space to approximate a certain distribution
    (a normal distribution in our example) and ensures that the latent variables don’t
    just memorize the training data but rather capture the underlying distribution.
    It helps in achieving a well-structured latent space where similar data points
    are mapped closely together, making the space continuous and interpretable. As
    a result, we can manipulate the encodings to achieve new outcomes, which makes
    encoding arithmetic and input interpolation possible in VAEs.
  prefs: []
  type: TYPE_NORMAL
- en: In the second project in this chapter, you’ll build and train a VAE from the
    ground up to generate human face images. Here, your training set comprises eyeglasses
    images that you downloaded in chapter 5\. The VAE’s encoder compresses an image
    of size 3 × 256 × 256 = 196,608 pixels into a 100-value probabilistic vector,
    each following a normal distribution. The decoder then reconstructs the image
    based on this probabilistic vector. The trained VAE can not only replicate human
    faces from the training set but also generate novel ones.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll learn how to conduct encoding arithmetic and input interpolation in VAEs.
    You’ll manipulate the encoded representations (latent vectors) of different inputs
    to achieve specific outcomes (i.e., with or without certain characteristics in
    images) when decoded. The latent vectors control different characteristics in
    the decoded images such as gender, whether there are eyeglasses in an image, and
    so on. For example, you can first obtain the latent vectors for men with glasses
    (z1), women with glasses (z2), and women without glasses (z3). You then calculate
    a new latent vector, z4 = z1 – z2 + z3. Since both z1 and z2 lead to eyeglasses
    in images when decoded, z1 – z2 cancels out the eyeglasses feature in the resulting
    image. Similarly, since both z2 and z3 lead to a female face, z3 – z2 cancels
    out the female feature in the resulting image. Therefore, if you decode z4 = z1
    – z2 + z3 with the trained VAE, you’ll get an image of a man without glasses.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll also create a series of images transitioning from a woman with glasses
    to a woman without glasses by varying the weight assigned to the latent vectors
    z1 and z2. These exercises exemplify the versatility and creative potential of
    VAEs in the field of generative models.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to GANs, which we studied in the last few chapters, AEs and VAEs have
    a simple architecture and are easy to construct. Further, AEs and VAEs are generally
    easier and more stable to train relative to GANs. However, images generated by
    AEs and VAEs tend to be blurrier compared to those generated by GANs. GANs excel
    in generating high-quality, realistic images but suffer from training difficulties
    and resource intensiveness. The choice between GANs and VAEs largely depends on
    the specific requirements of the task at hand, including the desired quality of
    the output, computational resources available, and the importance of having a
    stable training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'VAEs have a wide range of practical applications in the real world. Consider,
    for instance, that you run an eyewear store and have successfully marketed a new
    style of men’s glasses online. Now, you wish to target the female market with
    the same style but lack images of women wearing these glasses, and you face high
    costs for a professional photo shoot. Here’s where VAEs come into play: you can
    combine existing images of men wearing the glasses with pictures of both men and
    women without glasses. This way, you can create realistic images of women sporting
    the same eyewear style, as illustrated in figure 7.1, through encoding arithmetic,
    a technique you’ll learn in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 Generating images of women with glasses by performing encoding arithmetic
  prefs: []
  type: TYPE_NORMAL
- en: In another scenario, suppose your store offers eyeglasses with dark and light
    frames, both of which are popular. You want to introduce a middle option with
    frames of an intermediate shade. With VAEs, through a method called encoding interpolation,
    you can effortlessly generate a smooth transition series of images, as shown in
    figure 7.2\. These images would vary from dark to light-framed glasses, offering
    customers a visual spectrum of choices.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 Generating a series of images that transition from glasses with dark
    frames to those with light frames
  prefs: []
  type: TYPE_NORMAL
- en: The use of VAEs is not limited to eyeglasses; it extends to virtually any product
    category, be it clothing, furniture, or food. The technology provides a creative
    and cost-effective solution for visualizing and marketing a wide range of products.
    Furthermore, although image generation is a prominent example, VAEs can be applied
    to many other types of data, including music and text. Their versatility opens
    up endless possibilities in terms of practical use!
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 An overview of AEs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section discusses what an AE is and its basic structure. For you to have
    a deep understanding of the inner workings of AEs, you’ll build and train an AE
    to generate handwritten digits as your first project in this chapter. This section
    provides an overview of an AE’s architecture and a blueprint for completing the
    first project.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1 What is an AE?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'AEs are a type of neural network used in unsupervised learning that are particularly
    effective for tasks like image generation, compression, and denoising. An AE consists
    of two main parts: an encoder and a decoder. The encoder compresses the input
    into a lower-dimensional representation (latent space), and the decoder reconstructs
    the input from this representation.'
  prefs: []
  type: TYPE_NORMAL
- en: The compressed representation, or latent space, captures the most important
    features of the input data. In image generation, this space encodes crucial aspects
    of the images that the network has been trained on. AEs are useful for their efficiency
    in learning data representations and their ability to work with unlabeled data,
    making them suitable for tasks like dimensionality reduction and feature learning.
    One challenge with AEs is the risk of losing information in the encoding process,
    which can lead to less accurate reconstructions. Using deeper architectures with
    multiple hidden layers can help in learning more complex and abstract representations,
    potentially mitigating information loss in AEs. Also, training AEs to generate
    high-quality images can be computationally intensive and requires large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned in chapter 1, the best way to learn something is to create it
    from scratch. To that end, you’ll learn to create an AE to generate handwritten
    digits in the first project in this chapter. The next subsection provides a blueprint
    for how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2 Steps in building and training an AE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine that you must build and train an AE from the ground up to generate grayscale
    images of handwritten digits so that you acquire the skills needed to use AEs
    for more complicated tasks such as color image generation or dimensionality reduction.
    How should you go about this task?
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 provides a diagram of the architecture of an AE and the steps involved
    in training an AE to generate handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F03_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 The architecture of an AE and the steps to train one to generate
    handwritten digits. An AE consists of an encoder (middle left) and a decoder (middle
    right). In each iteration of training, images of handwritten digits are fed to
    the encoder (step 1). The encoder compresses the images to deterministic points
    in the latent space (step 2). The decoder takes the encoded vectors (step 3) from
    the latent space and reconstructs the images (step 4). The AE adjusts its parameters
    to minimize the reconstruction loss, the difference between the originals and
    the reconstructions (step 5).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the figure, the AE has two main parts: an encoder (middle
    left) that compresses images of handwritten digits into vectors in the latent
    space and a decoder (middle right) that reconstructs these images based on the
    encoded vectors. Both the encoder and decoder are deep neural networks that can
    potentially include different types of layers such as dense layers, convolutional
    layers, transposed convolutional layers, and so on. Since our example involves
    grayscale images of handwritten digits, we’ll use only dense layers. However,
    AEs can also be used to generate higher-resolution color images; for those tasks,
    convolutional neural networks (CNNs) are usually included in encoders and decoders.
    Whether to use CNNs in AEs depends on the resolution of the images you want to
    generate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When an AE is built, the parameters in it are randomly initialized. We need
    to obtain a training set to train the model: PyTorch provides 60,000 grayscale
    images of handwritten digits, evenly distributed among the 10 digits 0 to 9\.
    The left side of figure 7.3 shows three examples, and they are images of digits
    0, 1, and 9, respectively. In the first step in the training loop, we feed images
    in the training set to the encoder. The encoder compresses the images to 20-value
    vectors in the latent space (step 2). There is nothing magical about the number
    20\. If you use 25-value vectors in the latent space, you’ll get similar results.
    We then feed the vector representations to the decoder (step 3) and ask it to
    reconstruct the images (step 4). We calculate the reconstruct loss, which is the
    mean squared error, over all the pixels, between the original image and the reconstructed
    image. We then propagate this loss back through the network to update the parameters
    in the encoder and decoder to minimize the reconstruction loss (step 5) so that
    in the next iteration, the AE can reconstruct images closer to the original ones.
    This process is repeated for many epochs over the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the model is trained, you’ll feed unseen images of handwritten digits
    to the encoder and obtain encodings. You then feed the encodings to the decoder
    to obtain reconstructed images. You’ll notice that the reconstructed images look
    almost identical to the originals. The right side of figure 7.3 shows three examples
    of reconstructed images: they do look similar to the corresponding originals on
    the left side of the figure.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Building and training an AE to generate digits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have a blueprint to build and train an AE to generate handwritten
    digits, let’s dive into the project and implement the steps outlined in the last
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, in this section, you’ll learn first how to obtain a training set
    and a test set of images of handwritten digits. You’ll then build an encoder and
    decoder with dense layers. You’ll train the AE with the training dataset and use
    the trained encoder to encode images in the test set. Finally, you’ll learn to
    use the trained decoder to reconstruct images and compare them to the originals.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Gathering handwritten digits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can download grayscale images of handwritten images using the *datasets*
    package in the Torchvision library, similar to how you downloaded images of clothing
    items in chapter 2\.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s download a training set and a test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Downloads handwritten digits by using the MNIST() class in torchvision.datasets
  prefs: []
  type: TYPE_NORMAL
- en: ② The train=True argument means you download the training set.
  prefs: []
  type: TYPE_NORMAL
- en: ③ The train=False argument means you download the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using the `FashionMNIST()` class as we did in chapter 2, we use the
    `MNIST()` class here. The `train` argument in the class tells PyTorch whether
    to download the training set (when the argument is set to `True`) or the test
    set (when the argument is set to `False`). Before transformation, the image pixels
    are integers ranging from 0 to 255\. The `ToTensor()` class in the preceding code
    block converts them to PyTorch float tensors with values between 0 to 1\. There
    are 60,000 images in the training set and 10,000 in the test set, evenly distributed
    among 10 digits, 0 to 9, in each set.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll create batches of data for training and testing, with 32 images in each
    batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the data ready, we’ll build and train an AE next.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2 Building and training an AE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An AE consists of two parts: the encoder and the decoder. We’ll define an `AE()`
    class, as shown in the following listing, to represent the AE.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.1 Creating an AE to generate handwritten digits
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① The input to the AE has 28 × 28 = 784 values in it.
  prefs: []
  type: TYPE_NORMAL
- en: ② The latent variable (encoding) has 20 values in it.
  prefs: []
  type: TYPE_NORMAL
- en: ③ The encoder compresses images to latent variables.
  prefs: []
  type: TYPE_NORMAL
- en: ④ The decoder reconstructs the images based on encodings.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The encoder and decoder form the AE.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input size is 784 because the grayscale images of handwritten digits have
    a size of 28 by 28 pixels. We flatten the images to 1D tensors and feed them to
    the AE. The images first go through the encoder: they are compressed into encodings
    in a lower dimensional space. Each image is now represented by a 20-value latent
    variable. The decoder reconstructs the images based on the latent variables. The
    output from the AE has two tensors: `out`, the reconstructed images, and `mu`,
    latent variables (i.e., encodings).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we instantiate the `AE()` class we defined earlier to create an AE. We
    also use the Adam optimizer during training, as we did in previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We define a function `plot_digits()` to visually inspect the reconstructed handwritten
    digits after each epoch of training, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.2 The `plot_digits`() function to inspect reconstructed images
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① Collects a sample image of each digit in the test set
  prefs: []
  type: TYPE_NORMAL
- en: ② Feeds the image to the AE to obtain a reconstructed image
  prefs: []
  type: TYPE_NORMAL
- en: ③ Collects the reconstructed image of each original image
  prefs: []
  type: TYPE_NORMAL
- en: ④ Compares the originals to the reconstructed digits visually
  prefs: []
  type: TYPE_NORMAL
- en: We first collect 10 sample images, one representing a different digit, and place
    them in a list, `originals`. We feed the images to the AE to obtain the reconstructed
    images. Finally, we plot both the originals and the reconstructed images so that
    we can compare them and assess the performance of the AE periodically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before training starts, we call the function `plot_digits()` to visualize the
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You’ll see the output as shown in figure 7.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F04_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 Comparing reconstructed images by the AE with the originals before
    training starts. The top row shows 10 original images of handwritten digits in
    the test set. The bottom row shows the reconstructed images by the AE before training.
    The reconstructions are nothing more than pure noise.
  prefs: []
  type: TYPE_NORMAL
- en: Though we could divide our data into training and validation sets and train
    the model until no further improvements are seen on the validation set (as we
    have done in chapter 2), our primary aim here is to grasp how AEs work, not necessarily
    to achieve the best parameter tuning. Therefore, we’ll train the AE for 10 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.3 Training the AE to generate handwritten digits
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through batches in the training set
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses the AE to reconstruct images
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates reconstruct loss as measured by mean squared error
  prefs: []
  type: TYPE_NORMAL
- en: ④ Visually inspects the performance of the AE
  prefs: []
  type: TYPE_NORMAL
- en: In each epoch of training, we iterate through all batches of data in the training
    set. We feed the original images to the AE to obtain the reconstructed images.
    We then calculate the reconstruction loss, which is the mean squared error between
    the original images and the reconstructed images. Specifically, the reconstruction
    loss is obtained by first calculating the difference between the two images, pixel
    by pixel, squaring the values and averaging the squared difference. We adjust
    the model parameters to minimize the reconstruction loss, utilizing the Adam optimizer,
    which is a variation of the gradient descent method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model takes about 2 minutes to train if you are using GPU training. Alternatively,
    you can download the trained model from my website: [https://mng.bz/YV6K](https://mng.bz/YV6K).'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.3 Saving and using the trained AE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll save the model in the local folder on your computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To use it to reconstruct an image of handwritten digits, we load up the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use it to generate handwritten digits by calling the `plot_digits()`
    function we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output is shown in figure 7.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F05_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 Comparing reconstructed images by the trained AE with the originals.
    The top row shows 10 original images of handwritten digits in the test set. The
    bottom row shows the reconstructed images by the trained AE. The reconstructed
    images look similar to the original ones.
  prefs: []
  type: TYPE_NORMAL
- en: The reconstructed handwritten digits do resemble the original ones, although
    the reconstruction is not perfect. Some information gets lost during the encoding–decoding
    process. However, compared to GANs, AEs are easy to construct and take less time
    to train. Further, the encoder–decoder architecture is employed by many generative
    models. This project will help your understanding of later chapters, especially
    when we explore Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 What are VAEs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While AEs are good at reconstructing original images, they fail at generating
    novel images that are unseen in the training set. Further, AEs tend not to map
    similar inputs to nearby points in the latent space. As a result, the latent space
    associated with an AE is neither continuous nor easily interpretable. For example,
    you cannot interpolate two input data points to generate meaningful intermediate
    representations. For these reasons, we’ll study an improvement in AEs: VAEs.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’ll first learn the key differences between AEs and VAEs
    and why these differences lead to the ability of the latter to generate realistic
    images that are unseen in the training set. You’ll then learn the steps involved
    in training VAEs in general and training one to generate high-resolution human
    face images in particular.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 Differences between AEs and VAEs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'VAEs were first proposed by Diederik Kingma and Max Welling in 2013.^([1](#footnote-000))
    They are a variant of AEs. Like an AE, a VAE also has two main parts: an encoder
    and a decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: However, there are two key differences between AEs and VAEs. First, the latent
    space in an AE is deterministic. Each input is mapped to a fixed point in the
    latent space. In contrast, the latent space in a VAE is probabilistic. Instead
    of encoding an input as a single vector in the latent space, a VAE encodes an
    input as a distribution over possible values. In our second project, for example,
    we’ll encode a color image into a 100-value probabilistic vector. Additionally,
    we’ll assume that each element in this vector adheres to an independent normal
    distribution. Since defining a normal distribution requires just the mean (*μ*)
    and standard deviation (*σ*), each element in our 100-element probabilistic vector
    will be characterized by these two parameters. To reconstruct the image, we sample
    a vector from this distribution and decode it. The uniqueness of VAEs is highlighted
    by the fact that each sampling from the distribution results in a slightly varied
    output.
  prefs: []
  type: TYPE_NORMAL
- en: In statistical terms, the encoder in a VAE is trying to learn the true distribution
    of the training data *x*, *p*(*x*|*Θ*), where *Θ* is the parameters defining the
    distribution. For tractability, we usually assume that the distribution of the
    latent variable is normal. Because we only need the mean, *μ*, and standard deviation,
    *σ*, to define a normal distribution, we can rewrite the true distribution as
    *p*(*x*|*Θ*) = *p*(*x*|*μ*, *σ*). The decoder in the VAE generates a sample based
    on the distribution learned by the encoder. That is, the decoder generates an
    instance probabilistically from the distribution *p*(*x*|*μ*, *σ*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second key difference between AEs and VAEs lies in the loss function. When
    training an AE, we minimize the reconstruction loss so that the reconstructed
    images are as close to the originals as possible. In contrast, in VAEs, the loss
    function consists of two parts: the reconstruction loss and the KL divergence.
    KL divergence is a measure of how one probability distribution diverges from a
    second, expected probability distribution. In VAEs, KL divergence is used to regularize
    the encoder by penalizing deviations of the learned distribution (the encoder’s
    output) from a prior distribution (a standard normal distribution). This encourages
    the encoder to learn meaningful and generalizable latent representations. By penalizing
    distributions that are too far from the prior, KL divergence helps to avoid overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The KL divergence is calculated as follows in our setting since we assume a
    normal distribution (the formula is different if a nonnormal distribution is assumed):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F06_Liu_EQ01.png)'
  prefs: []
  type: TYPE_IMG
- en: '| (7.1) |'
  prefs: []
  type: TYPE_TB
- en: The summation is taken over all 100 dimensions of the latent space. When the
    encoder compresses the images into standard normal distributions in the latent
    space, such that *μ*=0 and *σ*=1, the KL divergence becomes 0\. In any other scenario,
    the value exceeds 0\. Thus, the KL divergence is minimized when the encoder successfully
    compresses the images into standard normal distributions within the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 The blueprint to train a VAE to generate human face images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the second project in this chapter, you’ll build and train a VAE from scratch
    to generate color images of human faces. The trained model can generate images
    that are unseen in the training set. Further, you can interpolate inputs to generate
    novel images that are intermediate representations between two input data points.
    The following is a blueprint for this second project.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 provides a diagram of the architecture of a VAE and the steps in
    training a VAE to generate human face images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F06_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 The architecture of a VAE and the steps to train one to generate
    human face images. A VAE consists of an encoder (middle upper left) and a decoder
    (middle bottom right). In each iteration of training, human face images are fed
    to the encoder (step 1). The encoder compresses the images to probabilistic points
    in the latent space (step 2; since we assume normal distributions, each probability
    point is characterized by a vector of means and a vector of standard deviations).
    We then sample encodings from the distribution and present them to the decoder.
    The decoder takes sampled encodings (step 3) and reconstructs images (step 4).
    The VAE adjusts its parameters to minimize the sum of reconstruction loss and
    the KL divergence. The KL divergence measures the difference between the encoder’s
    output and a standard normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.6 shows that a VAE also has two parts: an encoder (middle top left)
    and a decoder (middle bottom right). Since the second project involves high-resolution
    color images, we’ll use CNNs to create the VAE. As we discussed in chapter 4,
    high-resolution color images contain many more pixels than low-resolution grayscale
    images. If we use fully connected (dense) layers only, the number of parameters
    in the model is too large, making learning slow and ineffective. CNNs require
    fewer parameters than fully connected networks of similar size, leading to faster
    and more effective learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the VAE is created, you’ll use the eyeglasses dataset that you downloaded
    in chapter 5 to train the model. The left side of figure 7.6 shows three examples
    of the original human face images in the training set. In the first step in the
    training loop, we feed images in the training set, with a size of 3 × 256 × 256
    = 196,608 pixels, to the encoder. The encoder compresses the images to 100-value
    probabilistic vectors in the latent space (step 2; vectors of means and standard
    deviations due to the assumption of normal distribution). We then sample from
    the distribution and feed the sampled vector representations to the decoder (step
    3) and ask it to reconstruct the images (step 4). We calculate the total loss
    as the sum of the reconstruction loss at the pixel level and the KL divergence
    as specified in equation 7.1\. We propagate this loss back through the network
    to update the parameters in the encoder and decoder to minimize the total loss
    (step 5). The total loss encourages the VAE to encode the inputs into more meaningful
    and generalizable latent representations and to reconstruct images closer to the
    originals.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the model is trained, you’ll feed human face images to the encoder and
    obtain encodings. You then feed the encodings to the decoder to obtain reconstructed
    images. You’ll notice that the reconstructed images look close to the originals.
    The right side of figure 7.6 shows three examples of reconstructed images: they
    look similar to the corresponding originals on the left side of the figure, though
    not perfectly.'
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, you can discard the encoder and randomly draw encodings from
    the latent space and feed them to the trained decoder in VAE to generate novel
    human face images that are unseen in the training set. Further, you can manipulate
    the encoded representations of different inputs to achieve specific outcomes when
    decoded. You can also create a series of images transitioning from one instance
    to another by varying the weight assigned to any two encodings.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 A VAE to generate human face images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section creates and trains a VAE from scratch to generate human face images
    by following the steps outlined in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to what we have done to build and train AEs, our approach for the second
    project incorporates several modifications. Firstly, we plan to use CNNs in both
    the encoders and decoders of VAEs, particularly because high-resolution color
    images possess a greater number of pixels. Relying solely on fully connected (dense)
    layers would result in an excessively large number of parameters, leading to slow
    and inefficient learning. Second, as part of our process to compress images into
    vectors that follow a normal distribution in the latent space, we will generate
    both a mean vector and a standard deviation vector during the encoding of each
    image. This differs from the fixed value vector used in AEs. From the encoded
    normal distribution, we’ll then sample to obtain encodings, which are subsequently
    decoded to produce images. Notably, each reconstructed image will vary slightly
    every time we sample from this distribution, which gives rise to VAEs’ ability
    to generate novel images.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.1 Building a VAE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you recall, the eyeglasses dataset that you downloaded in chapter 5 is saved
    in the folder /files/glasses/ on your computer after some labels are manually
    corrected. We’ll resize the images to 256 by 256 pixels with values between 0
    and 1\. We then create a batch iterator with 16 images in each batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ① Resizes images to 256 by 256 pixels
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts images to tensors with values between 0 and 1
  prefs: []
  type: TYPE_NORMAL
- en: ③ Loads images from the folder and apply the transformations
  prefs: []
  type: TYPE_NORMAL
- en: ④ Places the data in a batch iterator
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll create a VAE that includes convolutional and transposed convolutional
    layers. We first define an `Encoder()` class as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.4 The encoder in the VAE
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① The dimension of the latent space is 100.
  prefs: []
  type: TYPE_NORMAL
- en: ② The mean of the distribution of the encodings
  prefs: []
  type: TYPE_NORMAL
- en: ③ The standard deviation of the encodings
  prefs: []
  type: TYPE_NORMAL
- en: ④ The encoded vector representation
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder network consists of several convolutional layers, which extract
    the spatial features of the input images. The encoder compresses the inputs into
    vector representations, `z`, which are normally distributed with means, `mu`,
    and standard deviations, `std`. The output from the encoder consists of three
    tensors: `mu`, `std`, and `z`. While the `mu` and `std` are the mean and standard
    deviation of the probabilistic vector, respectively, `z` is an instance sampled
    from this distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the input image, with a size of (3, 256, 256), first goes through
    a Conv2d layer with a stride value of 2\. As we explained in chapter 4, this means
    the filter skips two pixels each time it moves on the input image, which leads
    to downsampling of the image. The output has a size of (8, 128, 128). It then
    goes through two more Conv2d layers, and the size becomes (32, 31, 31). It is
    flattened and passed through linear layers to obtain values of `mu` and `std`.
  prefs: []
  type: TYPE_NORMAL
- en: We define a `Decoder()` class to represent the decoder in the VAE.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.5 The decoder in the VAE
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① Encodings first go through two dense layers.
  prefs: []
  type: TYPE_NORMAL
- en: ② Reshapes encodings into multidimensional objects so we can perform transposed
    convolutional operations on them
  prefs: []
  type: TYPE_NORMAL
- en: ③ Passes the encodings through three transposed convolutional lay
  prefs: []
  type: TYPE_NORMAL
- en: ④ Squeezes the output to values between 0 and 1, the same as the values in the
    input images
  prefs: []
  type: TYPE_NORMAL
- en: 'The decoder is a mirror image of the encoder: instead of performing convolutional
    operations, it performs transposed convolutional operations on the encodings to
    generate feature maps. It gradually converts encodings in the latent space back
    into high-resolution color images.'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the encoding first goes through two linear layers. It’s then unflattened
    to a shape (32, 31, 31), mirroring the size of the image after the last Conv2d
    layer in the encoder. It then goes through three ConvTranspose2d layers, mirroring
    the Conv2d layers in the encoder. The output from the decoder has a shape of (3,
    256, 256), the same as that of the training image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll combine the encoder with the decoder to create a VAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates an encoder by instantiating the Encoder() class
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a decoder by instantiating the Decoder() class
  prefs: []
  type: TYPE_NORMAL
- en: ③ Passes the input through the encoder to obtain the encoding
  prefs: []
  type: TYPE_NORMAL
- en: ④ The output of the VAE is the mean and standard deviation of the encodings,
    as well as the reconstructed images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The VAE consists of an encoder and a decoder, as defined by the `Encoder()`
    and `Decoder()` classes. When we pass images through the VAE, the output consists
    of three tensors: the mean and standard deviation of the encodings and the reconstructed
    images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a VAE by instantiating the `VAE()` class and define the optimizer
    for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We’ll manually calculate the reconstruction loss and the KL-divergence loss
    during training. Therefore, we don't define a loss function here.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.2 Training the VAE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train the model, we first define a `train_epoch()` function to train the
    model for one epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.6 Defining the `train_epoch()` function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: ① Obtains the reconstructed images
  prefs: []
  type: TYPE_NORMAL
- en: ② Calculates the reconstruction loss
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates the KL divergence
  prefs: []
  type: TYPE_NORMAL
- en: ④ Sum of the reconstruction loss and the KL divergence.
  prefs: []
  type: TYPE_NORMAL
- en: We iterate through all batches in the training set. We pass images through the
    VAE to obtain reconstructed images. The total loss is the sum of the reconstruction
    loss and the KL divergence. The model parameters are adjusted in each iteration
    to minimize the total loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also define a `plot_epoch()` function to visually inspect the generated
    images by the VAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: A well-trained VAE can map similar inputs to nearby points in the latent space,
    leading to a more continuous and interpretable latent space. As a result, we can
    randomly draw vectors from the latent space, and the VAE can decode the vectors
    into meaningful outputs. Therefore, in the previous function `plot_epoch()`, we
    randomly draw 18 vectors from the latent space and use them to generate 18 images
    after each epoch of training. We plot them in a 3 × 6 grid and visually inspect
    them to see how the VAE is performing during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we train the VAE for 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This training takes about half an hour if you use GPU training or several hours
    otherwise. The trained model weights are saved on your computer. Alternatively,
    you can download the trained weights from my website: [https://mng.bz/GNRR](https://mng.bz/GNRR).
    Make sure you unzip the file after downloading.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.3 Generating images with the trained VAE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that the VAE is trained, we can use it to generate images. We first load
    the weights of the trained model that we saved in the local folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We then check the VAE’s ability to reconstruct images and see how closely they
    resemble the originals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If you run the previous code block, you’ll see an output similar to figure 7.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F07_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 Comparing the reconstructed images by a trained VAE with the originals.
    The first and the third rows are the original images. We feed them to the trained
    VAE to obtain the reconstructed images, which are shown below the original images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original images are shown in the first and third rows, while the reconstructed
    images are shown below the originals. The reconstructed images resemble the originals,
    as shown in figure 7.7\. However, some information gets lost during the reconstruction
    process: they don’t look as realistic as the originals.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we test the VAE’s ability to generate novel images that are unseen in
    the training set, by calling the plot_epoch() function we defined before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The function randomly draws 18 vectors from the latent space and passes them
    to the trained VAE to generate 18 images. The output is shown in figure 7.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F08_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 Novel images generated by the trained VAE. We randomly draw vector
    representations in the latent space and feed them to the decoder in the trained
    VAE. The decoded images are shown in this figure. Since the vector representations
    are randomly drawn, the images don’t correspond to any originals in the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'These images are not present in the training set: the encodings are randomly
    drawn from the latent space, not the encoded vectors after passing images in the
    training set through the encoder. This is because the latent space in VAEs is
    continuous and interpretable. New and unseen encodings in the latent space can
    be meaningfully decoded into images that resemble but differ from those in the
    training set.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.4 Encoding arithmetic with the trained VAE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: VAEs include a regularization term (KL divergence) in their loss function, which
    encourages the latent space to approximate a normal distribution. This regularization
    ensures that the latent variables don’t just memorize the training data but rather
    capture the underlying distribution. It helps to achieve a well-structured latent
    space where similar data points are mapped closely together, making the space
    continuous and interpretable. As a result, we can manipulate the encodings to
    achieve new outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make results reproducible, I encourage you to download the trained weights
    from my website ([https://mng.bz/GNRR](https://mng.bz/GNRR)) and use the same
    code blocks for the rest of the chapter. As we explained in the introduction,
    encoding arithmetic allows us to generate images with certain features. To illustrate
    how encoding arithmetic works in VAEs, let’s first hand-collect three images in
    each of the following four groups: men with glasses, men without glasses, women
    with glasses, and women without glasses.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.7 Collecting images with different characteristics
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: ① Displays 25 images with eyeglasses
  prefs: []
  type: TYPE_NORMAL
- en: ② Selects three images of men with glasses
  prefs: []
  type: TYPE_NORMAL
- en: ③ Selects three images of women with glasses
  prefs: []
  type: TYPE_NORMAL
- en: ④ Displays 25 images without eyeglasses
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Selects three images of men without glasses
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Selects three images of women without glasses
  prefs: []
  type: TYPE_NORMAL
- en: We select three images in each group instead of just one so that we can calculate
    the average of multiple encodings in the same group when performing encoding arithmetic
    later. VAEs are designed to learn the distribution of the input data in the latent
    space. By averaging multiple encodings, we effectively smooth out the representation
    in this space. This helps us find an average representation that captures common
    features among different samples within a group.
  prefs: []
  type: TYPE_NORMAL
- en: Next we feed the three images of men with glasses to the trained VAE to obtain
    their encodings in the latent space. We then calculate the average encoding for
    the three images and use it to obtain a reconstructed image of a man with glasses.
    We then repeat this for the other three groups.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.8 Encoding and decoding images in four different groups
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a batch of images of men with glasses
  prefs: []
  type: TYPE_NORMAL
- en: ② Obtains the average encoding for men with glasses
  prefs: []
  type: TYPE_NORMAL
- en: ③ Decodes the average encoding for men with glasses
  prefs: []
  type: TYPE_NORMAL
- en: ④ Obtains the average encodings for the other three groups
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Decodes the average encodings for the other three groups
  prefs: []
  type: TYPE_NORMAL
- en: 'The average encodings for the four groups are `men_g_encoding`, `women_g_encoding`,
    `men_ng_encoding`, and `women_ng_encoding`, respectively, where `g` stands for
    glasses and `ng` for no glasses. The decoded images for the four groups are `men_g_recon`,
    `women_g_recon`, `men_ng_recon`, and `women_ng_recon`, respectively. We plot the
    four images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You’ll see the output as shown in figure 7.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F09_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9 Decoded images based on average encodings. We first obtain three
    images in each of the following four groups: men with glasses, women with glasses,
    men without glasses, and women without glasses. We feed the 12 images to the encoder
    in the trained VAE to obtain their encodings in the latent space. We then calculate
    the average encoding of the three images in each group. The four average encodings
    are fed to the decoder in the trained VAE to obtain four images and they are shown
    in this figure.'
  prefs: []
  type: TYPE_NORMAL
- en: The four decoded images are shown in figure 7.9\. They are the composite images
    representing the four groups. Notice that they are different from any of the original
    12 images. At the same time, they preserve the defining characteristics of each
    group.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s manipulate the encodings to create a new encoding and then use the
    trained decoder in the VAE to decode the new encoding and see what happens. For
    example, we can subtract the average encoding of women with glasses from the average
    encoding of men with glasses and add the average encoding of women without glasses.
    We then feed the result to the decoder and see the output.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.9 An example of encoding arithmetic
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines z as the encoding of men with glasses – women with glasses + women
    without glasses
  prefs: []
  type: TYPE_NORMAL
- en: ② Decodes z to generate an image
  prefs: []
  type: TYPE_NORMAL
- en: ③ Displays the four images
  prefs: []
  type: TYPE_NORMAL
- en: ④ Displays a title on top of the images
  prefs: []
  type: TYPE_NORMAL
- en: If you run the code block in listing 7.9, you’ll see an output as shown in figure
    7.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F10_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10 An example of encoding arithmetic with the trained VAE. We first
    obtain the average encodings for the following three groups: men with glasses
    (z1), women with glasses (z2), and women without glasses (z3). We define a new
    encoding z = z1 – z2 + z3. We then feed z to the decoder in the trained VAE and
    obtain the decoded image, as shown at the far right of this figure.'
  prefs: []
  type: TYPE_NORMAL
- en: The first three images in figure 7.10 are the composite images representing
    the three input groups. The output image, at the far right, is an image of a man
    without glasses.
  prefs: []
  type: TYPE_NORMAL
- en: Since both `men_g_encoding` and `women_g_encoding` lead to eyeglasses in images
    when decoded, `men_g_encoding` – `women_g_encoding` cancels out eyeglasses features
    in the resulting image. Similarly, since both `women_ng_encoding` and `women_g_encoding`
    lead to a female face, `women_ng_encoding` – `women_g_encoding` cancels out female
    features in the resulting image. Therefore, if you decode `men_g_encoding` + `women_g_encoding`
    –`women_ng_encoding` with the trained VAE, you’ll get an image of a man without
    glasses. The encoding arithmetic in this example shows that an encoding for men
    without glasses can be obtained by manipulating the average encodings in the other
    three groups.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.1
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following encoding arithmetics by modifying code listing 7.9:'
  prefs: []
  type: TYPE_NORMAL
- en: Subtract the average encoding of men without glasses from the average encoding
    of men with glasses and add the average encoding of women without glasses. Feed
    the result to the decoder and see what happens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract the average encoding of women without glasses from the average encoding
    of men without glasses and add the average encoding of women with glasses. Feed
    the result to the decoder and see what happens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Subtract the average encoding of men without glasses from the average encoding
    of women without glasses and add the average encoding of men with glasses. Feed
    the result to the decoder and see what happens. Make sure you modify the image
    titles to reflect the changes. The solutions are provided in the book’s GitHub
    repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further, we can interpolate any two encodings in the latent space by assigning
    different weights to them and creating a new encoding. We can then decode the
    new encoding and create a composite image as a result. By choosing different weights,
    we can create a series of intermediate images that transition from one image to
    another.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the encodings of women with and without glasses as an example. We’ll
    define a new encoding `z` as `w*women_ng_encoding+(1-w)*women_g_encoding`, where
    `w` is the weight we put on `women_ng_encoding`. We’ll change the value of `w`
    from 0 to 1 with an increment of 0.2 in each step. We then decode them and display
    the resulting six images.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.10 Interpolating two encodings to create a series of images
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through six different values of w
  prefs: []
  type: TYPE_NORMAL
- en: ② Interpolates between two encoding
  prefs: []
  type: TYPE_NORMAL
- en: ③ Decodes the interpolated encoding
  prefs: []
  type: TYPE_NORMAL
- en: ④ Displays the six resulting images
  prefs: []
  type: TYPE_NORMAL
- en: After running the code in listing 7.10, you’ll see an output as shown in figure
    7.11.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F11_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 Interpolating encodings to create a series of intermediate images.
    We first obtain the average encodings for women with glasses (`women_g_encoding`)
    and women without glasses (`women_ng_encoding`). The interpolated encoding z is
    defined as `w*women_ng_encoding+(1-w)*women_g_encoding`, where w is the weight
    on `women_ng_encoding`. We change the value of w from 0 to 1 with an increment
    of 0.2 to create six interpolated encodings. We then decode them and display the
    resulting six images in the figure.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in figure 7.11, as you move from left to right, the image gradually
    transitions from a woman with glasses to a woman without glasses. This shows that
    the encodings in the latent space are continuous, meaningful, and interpolatable.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.2
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify listing 7.10 to create a series of intermediate images by using the
    following pairs of encodings: (i) `men_ng_encoding` and `men_g_encoding`; (ii)
    `men_ng_encoding` and `women_ng_encoding`; (iii) `men_g_encoding` and `women_g_encoding`.
    The solutions are provided in the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting in the next chapter, you’ll embark on a journey in natural language
    processing. This will enable you to generate another form of content: text. However,
    many tools you have used so far will be used again in later chapters, such as
    deep neural networks and the encoder-decoder architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AEs have a dual-component structure: an encoder and a decoder. The encoder
    compresses the data into an abstract representation in a lower-dimensional space
    (the latent space), and the decoder decompresses the encoded information and reconstructs
    the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VAEs also consist of an encoder and a decoder. They differ from AEs in two critical
    ways. First, while an AE encodes each input into a specific point in the latent
    space, a VAE encodes it into a probability distribution within this space. Second,
    an AE focuses solely on minimizing the reconstruction error, whereas a VAE learns
    the parameters of the probability distribution for latent variables, minimizing
    a loss function that includes both reconstruction loss and a regularization term,
    the KL divergence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The KL divergence in the loss function when training VAEs ensures the distribution
    for latent variables resembles a normal distribution. This encourages the encoder
    to learn continuous, meaningful, and generalizable latent representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A well-trained VAE can map similar inputs to nearby points in the latent space,
    leading to a more continuous and interpretable latent space. As a result, VAEs
    can decode random vectors in the latent space into meaningful outputs, leading
    to images that are unseen in the training set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latent space in a VAE is continuous and interpretable, different from that
    in an AE. As a result, we can manipulate the encodings to achieve new outcomes.
    We can also create a series of intermediate images transitioning from one instance
    to another by varying weights on two encodings in the latent space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](#footnote-000-backlink))  Diederik P Kingma and Max Welling, 2013, “Auto-Encoding
    Variational Bayes.” [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114).
  prefs: []
  type: TYPE_NORMAL
