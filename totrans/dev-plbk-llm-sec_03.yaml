- en: Chapter 3\. Architectures and Trust Boundaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike traditional web applications that rely on predefined algorithms and static
    databases, LLMs utilize massive neural networks to generate dynamic, context-aware
    responses. This seismic shift brings a unique set of security challenges, different
    from those seen in traditional web applications. While researchers have meticulously
    studied web applications and their vulnerabilities, the field of LLM security
    is still relatively nascent.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter aims to bridge this knowledge gap by dissecting the fundamental
    elements that set LLMs apart. We’ll start by exploring the building blocks of
    AI, neural networks, and how they relate to large language models. Then, we dive
    into the groundbreaking architecture that powers most LLMs today—the transformer
    model. Following this, we look into the various LLM-powered applications, such
    as chatbots and copilots.
  prefs: []
  type: TYPE_NORMAL
- en: However, in addition to understanding the technology, security professionals
    must be aware of the new kinds of *trust boundaries* unique to LLMs—boundaries
    that demarcate areas of varying trustworthiness within an application. These include
    user prompts, uploaded content, training and test data, databases, plug-ins, and
    other boundary systems that we’ll detail later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI, Neural Networks, and Large Language Models: What’s the Difference?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Artificial intelligence, neural network, and LLM are terms often used interchangeably,
    but they represent different facets of a broader landscape of machine learning
    and computational intelligence. Let’s break down the differences to understand
    their unique roles in technology and security:'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence (AI)
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence*, at its core, is a multidisciplinary field aimed
    at creating systems capable of performing tasks that would ordinarily require
    human intelligence. These tasks include problem-solving, perception, and language
    understanding. AI encompasses a wide range of technologies and methodologies,
    from rule-based systems to machine learning algorithms, serving as an umbrella
    term for multiple approaches to achieving artificial intelligence. It’s worth
    noting that the very definition of AI has been a moving target over the past few
    decades and continues to evolve as technology advances.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs: []
  type: TYPE_NORMAL
- en: '*Neural networks* are one type of AI technology inspired by the human brain’s
    architecture. They are computational models designed to recognize patterns and
    make decisions based on the data they process. Neural networks can be simple,
    with a minimal number of layers (shallow neural networks), or highly complex,
    with multiple interconnected layers (deep neural networks). They are the backbone
    of many modern AI applications, including image recognition, natural language
    processing, and autonomous vehicles.'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs)
  prefs: []
  type: TYPE_NORMAL
- en: '*LLMs* represent a specific type of neural network. LLMs usually employ advanced
    forms of neural networks, such as transformer models, to analyze and produce text
    based on the training data their developers feed them. What sets them apart is
    their massive scale and specialization in handling linguistic tasks, which range
    from simple text completion to complex question answering and summarization.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding these distinctions is crucial for security professionals. Each
    layer—from broad AI technologies to specialized LLMs—introduces vulnerabilities
    and requires unique security measures. As we analyze the complexities of LLMs,
    recognizing their position in the broader AI landscape will be critical to discussing
    effectively safeguarding them. The rest of this book is centered on that discussion.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Transformer Revolution: Origins, Impact, and the LLM Connection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The transformer architecture is a pivotal milestone in the evolution of artificial
    intelligence, profoundly impacting the AI landscape and, by extension, LLMs. Let’s
    unravel the story of the transformer revolution—where it came from, when it happened,
    and the seismic shifts it brought to AI and LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Origins of the Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The transformer architecture was introduced in the groundbreaking research
    paper [“Attention Is All You Need”](https://oreil.ly/lRNoH) by Ashish Vaswani
    et al., published in 2017\. This paper proposed a novel approach to natural language
    processing (NLP) tasks, departing from the traditional models that relied heavily
    on recurrent neural networks (RNNs) and convolutional neural networks (CNNs).
    The transformer introduced a key innovation: the self-attention mechanism. This
    mechanism allowed the model to weigh the importance of different words in a sentence,
    enabling it to understand context more effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Before the emergence of transformers, the world of neural networks was replete
    with promise but often struggled to deliver on the lofty expectations. Traditional
    architectures like RNNs and CNNs enabled advanced AI capabilities but grappled
    with inherent limitations. These limitations stemmed from their inability to capture
    and utilize context effectively, particularly in natural language understanding.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs, while suitable for sequential data, faced challenges maintaining context
    over long sequences. They exhibited a form of “short-term memory,” which made
    them less adept at grasping intricate relationships and dependencies within lengthy
    texts or conversations. On the other hand, CNNs, renowned for their prowess in
    image recognition, needed help to extend their effectiveness to sequential data
    like language, where understanding context across words and sentences was paramount.
  prefs: []
  type: TYPE_NORMAL
- en: This shortcoming in contextual understanding was the Achilles’ heel of traditional
    neural networks. They could only glimpse small portions of a text at a time, rendering
    them incapable of comprehending the broader narrative or nuances. It was akin
    to trying to understand a novel by reading only a few random sentences from its
    pages. The result was a gap between the promise of AI and its practical application,
    particularly in natural language understanding. It was this gap that the transformer
    architecture would bridge, unleashing a wave of progress and redefining the landscape
    of AI-driven language models.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer Architecture’s Impact on AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Introducing the transformer architecture wasn’t just a milestone for natural
    language processing; it marked a paradigm shift across multiple domains within
    the AI landscape. While researchers initially used the transformer architecture
    to solve problems related to understanding and generating text, researchers and
    engineers quickly found that its capabilities extended far beyond that. Here are
    some areas where transformer architectures have made a considerable impact:'
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing (NLP)
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the first and most immediate impact was in NLP. Transformer models
    are now the backbone for various language tasks such as translation, summarization,
    question-answering, and sentiment analysis. They have set new performance benchmarks,
    sometimes surpassing human-level capabilities in specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the transformer architecture also has applications in computer
    vision. While CNNs have been the gold standard for image-related tasks, transformer-based
    models like vision transformer (ViT) demonstrate competitive, if not superior,
    performance in tasks like image classification, object detection, and segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition
  prefs: []
  type: TYPE_NORMAL
- en: The flexibility of transformer architectures has also made them a good fit for
    speech recognition. Combined with specialized models like the conformer, which
    fuses convolutional layers with transformer layers, they have set new standards
    for understanding spoken language.
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous systems and self-driving cars
  prefs: []
  type: TYPE_NORMAL
- en: One of the most intriguing applications of transformers is autonomous systems,
    including self-driving cars. These vehicles require a high contextual understanding
    to navigate the world safely. Transformer models are at the heart of self-driving
    models from companies like Tesla.
  prefs: []
  type: TYPE_NORMAL
- en: Health care
  prefs: []
  type: TYPE_NORMAL
- en: In health care, transformer models are aiding in tasks ranging from drug discovery
    to the analysis of medical images. Their ability to sift through and interpret
    large amounts of data can speed up research and potentially lead to more accurate
    diagnoses.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the rise of the transformer architecture has been a tide that lifted
    all boats, revolutionizing not just one but multiple fields within AI. However,
    this versatility also brings unique security challenges across these various applications.
    As we look more deeply into LLM security, we’ll explore how the ubiquitous nature
    of transformer architectures necessitates a multifaceted approach to safeguarding
    AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Types of LLM-Based Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two common types of LLM-based applications are chatbots and copilots. Let’s
    briefly look at each to help you understand the breadth of applications in which
    developers use LLMs and give you context for understanding various architectural
    choices as you study further.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chatbots* are computer programs that can simulate conversations with humans,
    and they often power customer service applications, where they can answer questions
    and support customers. Chatbots also excel at entertainment applications like
    playing games or telling stories. Tay from [Chapter 1](ch01.html#chatbots_breaking_bad)
    is an example of an entertainment chatbot. Here are some more examples of LLM-based
    chatbots:'
  prefs: []
  type: TYPE_NORMAL
- en: Sephora uses a chatbot to help customers find the right products for their skin
    type and needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: H&M uses a chatbot to help customers find clothes and accessories that match
    their style.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domino’s Pizza uses a chatbot to allow customers to order pizza via X (Twitter)
    or Facebook Messenger.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fandango uses a chatbot to help customers find movie times and theaters nearby.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JetBlue Airways uses a chatbot to answer customer questions about flights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amtrak uses a chatbot to help customers book tickets, check train status, and
    get answers to their questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Golden State Warriors use a chatbot to help fans purchase tickets, learn
    about upcoming games, and get news about the team.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Copilots* are AI systems that can assist humans with writing, coding, and
    research tasks. They can help users to generate ideas, identify errors, and improve
    their work. Copilots are still under development, but they have the potential
    to revolutionize the way we work and learn. Specific examples of LLM-based copilots
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Grammarly and ProWritingAid help users improve their writing by identifying
    and correcting grammatical errors, suggesting style improvements, and providing
    feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub Copilot, Google Gemini Code Assist, and AWS CodeWhisperer help programmers
    write code faster and more efficiently. They can generate code suggestions, translate
    between programming languages, and help to identify and debug errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copilot for Microsoft 365 and Gemini for Google Workspace are AI-powered tools
    integrated into their respective office suites that help users to be more productive
    and creative in their work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A chatbot like ChatGPT can read and review a text block and then provide suggestions
    to improve it. However, the experience of using a copilot like Grammarly to do
    that is dramatically different and generally superior for that type of focused
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarities between chatbots and copilots:'
  prefs: []
  type: TYPE_NORMAL
- en: Both chatbots and copilots are LLM-based applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both chatbots and copilots generate text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both chatbots and copilots assist humans with tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Differences between chatbots and copilots:'
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots simulate conversation with humans, while copilots assist humans with
    specific tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatbots often power customer service applications, while copilots assist in
    writing, coding, and research applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatbots are typically more interactive than copilots, while copilots focus
    more on completing tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep these concepts in mind as we dig into the details of LLM architectures.
    Both application types share similar components, but you may make different decisions
    on implementing pieces based on the differing security considerations.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Application Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developers often consider LLMs standalone entities capable of impressive text
    generation and comprehension feats. However, in practice, an LLM is rarely isolated;
    it is a cog in the intricate machinery that constitutes an intelligent application.
    These applications are complex systems comprising multiple interconnected components,
    each playing a vital role in the overall functionality and performance of the
    application. Whether a conversational agent, an automated content generator, or
    a copilot for code, an LLM usually interacts with various elements such as users,
    databases, APIs, web pages, and even other machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the architecture of such composite systems is not just a matter
    of technical proficiency; it is crucial for effective security planning. The way
    these components interact introduces multiple trust and data flow layers, defining
    new security boundaries far removed from traditional web application security
    models. For instance, user inputs may not just be simple text fields but could
    include voice commands, images, or real-time collaborative editing. Similarly,
    an LLM’s outputs could be fed into other systems for further processing, introducing
    vulnerabilities and risks.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, the holistic view of an LLM-based application goes beyond securing
    the language model itself. It demands a comprehensive approach that considers
    the security of the entire architecture, from data ingestion and storage to model
    serving and user interaction. Only by understanding these intricacies can one
    formulate an effective strategy to safeguard an application against the myriad
    vulnerabilities such complex systems inherently possess.
  prefs: []
  type: TYPE_NORMAL
- en: As we dig deeper into the subject in this chapter, we’ll dissect the various
    components that typically make up an LLM application, examine their roles, and
    explore the unique security challenges each presents. This understanding will
    be the foundation for a robust, multilayered approach to securing your LLM-based
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-1](#fig_1_typical_llm_application_dataflow_architecture) shows a
    highly simplified diagram to illustrate the components, relationships, and data
    flows in an application using an LLM. Subsequent chapters will expand on these
    areas.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dpls_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Typical LLM application data-flow architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Trust Boundaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In application security, a *trust boundary* serves as an invisible, yet crucial,
    demarcation line that separates different components or entities based on their
    level of trustworthiness. These boundaries delineate areas where data or control
    flow changes from one level of trust to another—such as transitioning from user-controlled
    input to internal processing or moving from a secure internal database to a public-facing
    API. These boundaries act as checkpoints where developers should rigorously apply
    security measures like authentication, authorization, and data validation to prevent
    vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Understanding trust boundaries is critical to threat modeling. Properly defining
    and recognizing these boundaries can be the difference between a secure system
    and one vulnerable to threats.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-2](#fig_2_llm_application_architecture_with_trust_boundaries) adds
    the trust boundaries to our architecture diagram.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dpls_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. LLM application architecture with trust boundaries
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'These boundaries, as depicted in the diagram, serve as gateways through which
    the LLM interfaces with diverse components—public data from the web, structured
    databases, spontaneous user interactions, or internally sourced training sets.
    Each delineated boundary highlights considerations we must make when considering
    data that flows into and out of the LLM. Here’s a quick summary; we’ll dive more
    deeply in the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: User interactions
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to consider safeguarding the model from potential adversarial or
    misleading inputs that users or systems might introduce. You’ll also need to worry
    about toxic, inaccurate, or sensitive data being output from the model and passed
    back to the user.
  prefs: []
  type: TYPE_NORMAL
- en: In-the-wild training data
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are often trained on massive amounts of internet data. You need to consider
    this data untrusted and watch out for potential toxicity, bias, and adversarial
    data poisoning, which we’ll cover in [Chapter 7](ch07.html#trust_no_one).
  prefs: []
  type: TYPE_NORMAL
- en: Internal test and training data
  prefs: []
  type: TYPE_NORMAL
- en: You may use internally curated data to fine-tune your model, which can significantly
    increase accuracy. But you must be wary of ingesting and exposing sensitive, confidential,
    or personally identifiable information. We’ll discuss this more in [Chapter 5](ch05.html#can_your_llm_know_too_much).
  prefs: []
  type: TYPE_NORMAL
- en: External services
  prefs: []
  type: TYPE_NORMAL
- en: You must actively control how the LLM interfaces with connected services, like
    databases or APIs, from unauthorized interactions or data leaks. We’ll cover this
    more in [Chapter 7](ch07.html#trust_no_one).
  prefs: []
  type: TYPE_NORMAL
- en: Public data access
  prefs: []
  type: TYPE_NORMAL
- en: Pulling data live from the web can be a powerful way to augment your application’s
    capabilities. However, you’ll need to consider this data untrusted and watch for
    issues like indirect prompt injection, which we’ll cover in [Chapter 4](ch04.html#prompt_injection).
  prefs: []
  type: TYPE_NORMAL
- en: Each point is a potential avenue of vulnerability, susceptible to exploitation
    if overlooked. In the evolving landscape of LLM applications, securing these trust
    boundaries is not just best practice—it’s essential to prevent unauthorized data
    access, mitigate data tampering, and avert system breaches. Recognizing these
    boundaries and their implications is the cornerstone of a resilient LLM security
    architecture. Now, let’s go into more detail on each area to ensure you have enough
    context to dive into the following chapters that detail the risk areas and mitigations.
  prefs: []
  type: TYPE_NORMAL
- en: The Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The language model serves as the intellectual core of any LLM application, taking
    in data, generating responses, and driving interactions. Depending on the architecture
    and requirements, you may interact with the language model through a public API
    hosted by a third-party service or run a privately hosted model. For example,
    you can download versions of Meta’s powerful Llama model from GitHub or Hugging
    Face and run it locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Public APIs: The convenience and the risks'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Utilizing a public API to access a language model offers convenience and lower
    up-front costs. Third parties manage and update these models, reducing your organization’s
    resource burden. However, the trade-off often comes in the form of higher risk
    of data exposure. When making a request to a third-party model, the data crosses
    a trust boundary, exiting your secure network and entering an external system.
    This process exposes you to risks around data confidentiality and, depending on
    the third party’s security measures, could make you vulnerable to data breaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Privately hosted models: More control, different risks'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Opting for a privately hosted model gives you more control over your data, allowing
    you to manage trust boundaries more tightly. It also allows you to customize or
    fine-tune the model according to your needs. However, running a privately hosted
    model brings challenges, such as maintenance, updates, and ensuring that the model
    doesn’t contain vulnerabilities—essentially exposing you to potential supply chain
    risks. If you use an open source model, it becomes crucial to ensure its provenance
    and integrity to avoid embedded vulnerabilities or biases.
  prefs: []
  type: TYPE_NORMAL
- en: Risk considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s look at some security considerations that depend on your choice of model
    and where it is deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive data exposure
  prefs: []
  type: TYPE_NORMAL
- en: Public APIs may increase the risk of exposing sensitive information, while privately
    hosted models offer better control but require robust internal security measures.
  prefs: []
  type: TYPE_NORMAL
- en: Supply chain risk
  prefs: []
  type: TYPE_NORMAL
- en: The origins of your model, whether it’s a well-vetted public service or an open
    source download, are crucial. A compromised model can introduce vulnerabilities
    into your application, effectively acting as a back door for attacks. We’ll explore
    this more in [Chapter 9](ch09.html#find_the_weakest_link).
  prefs: []
  type: TYPE_NORMAL
- en: By carefully considering the model’s hosting environment, you can better assess
    the trade-offs and risks associated with sensitive data exposure and supply chain
    vulnerabilities. These considerations will guide you in establishing appropriate
    trust boundaries and security protocols tailored to your chosen model’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: User Interaction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While *user**input* might suggest a one-way flow of information from the user
    into the application, the reality is often more nuanced. In the context of LLM
    applications, *user interaction* encapsulates both receiving input from the user
    and providing output back to the user. This bidirectional interaction is fundamental
    for creating an engaging and practical user experience, but also introduces a
    more complicated security landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts are a vital element of user interaction. They are not merely requests
    for information but serve as a guide to how the user interacts with the LLM. A
    well-crafted prompt can direct the model to provide valuable and accurate information,
    while an ambiguous or poorly constructed one can lead to unclear or even misleading
    outputs. As a result, the management of prompts becomes a critical aspect of application
    security. For example, a carefully crafted prompt from a malicious user could
    trick the model into divulging information it shouldn’t or cause the model to
    generate harmful content. Returning to [Chapter 1](ch01.html#chatbots_breaking_bad),
    Tay fell victim to this when prompts from her 4chan hackers helped lead her astray.
  prefs: []
  type: TYPE_NORMAL
- en: Given the importance of this bidirectional interaction, securing both inputs
    and outputs is crucial. On the input side, input validation, sanitation, and rate
    limiting measures are vital in mitigating vulnerabilities like injection attacks.
    On the output side, ensuring that the model’s responses are appropriately filtered
    and that your application does not leak sensitive information is equally vital.
    The nature of LLMs makes this even more challenging than it is with traditional
    applications, and we’ll discuss more techniques related to this later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: This interactive layer with the user creates a critical trust boundary in the
    application architecture. Any data crossing this boundary, whether going in or
    out, should be carefully managed to avoid security risks. Additional layers of
    protection include using encryption for sensitive outputs and employing real-time
    monitoring to flag potentially harmful or sensitive data flows. We’ll discuss
    this more thoroughly in [Chapter 7](ch07.html#trust_no_one).
  prefs: []
  type: TYPE_NORMAL
- en: Training Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training data is the bedrock upon which LLMs build their understanding and capabilities.
    Whether used for initial training or subsequent fine-tuning, the nature and source
    of this data have significant implications for both the model’s performance and
    security posture. One crucial distinction is whether the data is internally sourced
    or culled from public or external sources (“in the wild”).
  prefs: []
  type: TYPE_NORMAL
- en: Data generated or curated within an organization usually undergoes a more rigorous
    vetting than publicly sourced data. It is often aligned with the application’s
    specific requirements or use cases, making it generally more reliable and relevant.
    The controlled environment also allows for better implementation of security measures
    like encryption, access controls, and auditing. However, this data may contain
    sensitive or proprietary information, and the trust boundary here is closely tied
    to internal security protocols. A breach at this level could have serious ramifications,
    including data leakage or the corruption of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data sourced from public repositories or “the wild” introduces different challenges.
    While this data can offer diversity and scale, its reliability and safety are
    often not guaranteed. Such data could include misleading information, biases,
    or malicious inputs to compromise the model. The trust boundary here is more porous
    and extends to the external entities that generate or host this data: rigorous
    filtering, validation, and continuous monitoring become essential to mitigate
    risks and vulnerabilities. As we saw in [Chapter 1](ch01.html#chatbots_breaking_bad),
    Tay was digesting user prompts directly as training data. In this way, remnants
    of toxic prompts became part of her knowledge base, and then she began to spill
    poisonous output. Accepting unfiltered, untrusted user input into your training
    dataset is the simplest example of a failure to manage this critical security
    boundary.'
  prefs: []
  type: TYPE_NORMAL
- en: For either internally sourced or public data, the concept of trust boundaries
    is critical. For internally sourced data, the boundary is often within the organization’s
    controlled environment, making it easier to enforce security measures. On the
    other hand, using external data effectively extends your trust boundary to include
    those external sources, which may not adhere to your security standards. Using
    external data for training necessitates additional layers of validation and security
    checks to ensure that unvetted data doesn’t compromise the integrity or security
    of the LLM application.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the origins of your training data, the associated trust boundaries,
    and their respective security implications is crucial for safeguarding your LLM
    application. Comprehensive data governance policies must be in place to manage
    the lifecycle of your training data, regardless of its source.
  prefs: []
  type: TYPE_NORMAL
- en: Access to Live External Data Sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Live external data sources bring an added dimension to the capabilities of LLM
    applications by enabling them to provide real-time information, context, or even
    third-party integrations. While access to live external data enhances the user
    experience and functional range, it introduces a new layer of complexity to the
    application’s security landscape.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of this, as of the writing of this chapter, OpenAI’s ChatGPT does
    not have direct access to the live web and is thus limited only to facts in its
    older training data. On the other hand, Google’s Bard (now called Gemini) does
    have access to live internet data for this test. Because of this, while the GPT-4
    model is doubtlessly superior in reasoning capability, it fails at many basic
    tasks where Bard succeeds. [Figure 3-3](#fig_3_chatgpt_with_gtp_4_fails_to_answer_a_simple_questi)
    shows an interaction with ChatGPT. [Figure 3-4](#fig_4_bard_s_direct_access_to_internet_feeds_gives_it_an)
    shows the same interaction with Bard.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dpls_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. ChatGPT with GTP-4 fails to answer a simple question due to limited
    access to external data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/dpls_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Bard’s direct access to internet feeds gives it an advantage
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While accessing outside data sources such as websites, APIs, or third-party
    databases has advantages, it exposes the application to potential risks. The risks
    of ingesting untrusted external data sources can range from consuming false or
    harmful information from compromised websites to becoming a conduit for security
    threats like malware or unauthorized data access. The untrusted nature of these
    data sources makes them inherently less controllable than internal resources,
    thereby adding an additional layer of uncertainty and risk.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of trust boundaries becomes especially pertinent when accessing
    public internet data. Unlike internal services, where you can uniformly apply
    security measures, external sources may adhere to security standards different
    from those of your organization. This differential in trust necessitates additional
    layers of validation, security checks, and monitoring to ensure that data crossing
    this boundary doesn’t compromise the system.
  prefs: []
  type: TYPE_NORMAL
- en: Access to Internal Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Internal services like databases and internal APIs often serve as the backend
    support structure for LLM applications. They may house critical data from user
    profiles and logs to configuration settings and even vast data in SQL or vector
    databases. As a component that often interfaces with various other internal and
    external elements of the system, internal services represent a critical point
    in the application’s architecture, both functionally and from a security perspective.
  prefs: []
  type: TYPE_NORMAL
- en: These services often function within an organization’s controlled environment,
    enabling uniform application of security policies. However, just because these
    services are internal, you mustn’t fall victim to a false sense of security. They
    are still vulnerable to various threats, such as unauthorized access, data leaks,
    and internal threats from within the organization.
  prefs: []
  type: TYPE_NORMAL
- en: Internal services such as databases, proprietary APIs, and backend systems often
    constitute the operational backbone for LLM applications. These resources typically
    reside within the organization’s secure network, providing trust and control that
    is harder to achieve with external services. However, this internal nature can
    paradoxically elevate the security risks involved, primarily if these services
    house the organization’s “crown jewels” of sensitive or valuable data.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Securing LLM applications is an endeavor fraught with complexities, intricacies,
    and challenges that are significantly different from those of traditional web
    applications. This chapter has aimed to lay down the foundational knowledge required
    to navigate this complex landscape, focusing on three critical areas: distinguishing
    between artificial intelligence, neural networks, and large language models; understanding
    the pivotal role of transformer architectures; and diving deep into LLM application
    architecture, particularly the concept of trust boundaries. Knowing what sets
    LLMs apart helps us tailor our security strategies more effectively, going beyond
    general AI or machine learning frameworks.'
  prefs: []
  type: TYPE_NORMAL
