- en: Chapter 6\. Autonomous Agents with Memory and Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter dives deeper into the importance of chain-of-thought reasoning
    and the ability of large language models (LLMs) to reason through complex problems
    as agents. By breaking down complex problems into smaller, more manageable components,
    LLMs can provide more thorough and effective solutions. You will also learn about
    the components that make up autonomous agents, such as inputs, goal or reward
    functions, and available actions.
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-Thought
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability of AI to reason through complex problems is essential for creating
    effective, reliable, and user-friendly applications.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chain-of-thought reasoning* (CoT) is a method of guiding LLMs through a series
    of steps or logical connections to reach a conclusion or solve a problem. This
    approach is particularly useful for tasks that require a deeper understanding
    of context or multiple factors to consider.'
  prefs: []
  type: TYPE_NORMAL
- en: '[CoT](https://oreil.ly/fAeLo) is asking an LLM to *think* through complex problems,
    breaking them down into smaller, more manageable components. This allows the LLM
    to focus on each part individually, ensuring a more thorough understanding of
    the issue at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, chain-of-thought reasoning might involve:'
  prefs: []
  type: TYPE_NORMAL
- en: Asking an LLM to provide explanations for its decisions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning multiple steps before deciding on a final answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, you’ll explore examples of both ineffective and effective
    chain-of-thought reasoning. We will also discuss various techniques for building
    effective chain-of-thought reasoning and how they can be integrated into AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine that a user wants the AI to generate a comprehensive marketing
    plan for promoting a new software product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this example, GPT-4 doesn’t use chain-of-thought reasoning, and it does not
    address the specific aspects of the marketing plan. The LLM generates a generic
    list of marketing strategies that could apply to any product, rather than focusing
    on the unique characteristics of the new software product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now GPT-4 specifically addresses the unique characteristics of the new software
    product, demonstrating effective chain-of-thought reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Give Direction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take note of the phrase *step-by-step*, a critical element in CoT. By incorporating
    this phrase into your prompt, you’re asking the LLM to reason through the steps
    that are required to generate a highly effective software product.
  prefs: []
  type: TYPE_NORMAL
- en: Also, by providing a $20,000 budget and the type of software, GPT-4 is able
    to provide a much more relevant and contextualized response.
  prefs: []
  type: TYPE_NORMAL
- en: Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI models have given rise to an *agent-based architecture*. Conceptually,
    an agent acts, perceives, and makes decisions within a specified environment to
    achieve predefined objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Agents can take various actions such as executing a Python function; afterward,
    the agent will observe what happens and will decide on whether it is finished
    or what action to take next.
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent will continously loop through a series of actions and observations
    until there are no further actions, as you can see in the following pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The behavior of the agent is governed by three principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs: []
  type: TYPE_NORMAL
- en: These are the sensory stimuli or data points the agent receives from its environment.
    Inputs can be diverse, ranging from visual (like images) and auditory (like audio
    files) to thermal signals and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: Goal or reward function
  prefs: []
  type: TYPE_NORMAL
- en: This represents the guiding principle for an agent’s actions. In goal-based
    frameworks, the agent is tasked with reaching a specific end state. In a reward-based
    setting, the agent is driven to maximize cumulative rewards over time, often in
    dynamic environments.
  prefs: []
  type: TYPE_NORMAL
- en: Available actions
  prefs: []
  type: TYPE_NORMAL
- en: The *action space* is the range of permissible actions an agent [can undertake
    at any given moment](https://oreil.ly/5AVfM). The breadth and nature of this space
    are contingent upon the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain these concepts further, consider a self-driving car:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs: []
  type: TYPE_NORMAL
- en: The car’s sensors, such as cameras, LIDAR, and ultrasonic sensors, provide a
    continuous stream of data about the environment. This can include information
    about nearby vehicles, pedestrians, road conditions, and traffic signals.
  prefs: []
  type: TYPE_NORMAL
- en: Goal or reward function
  prefs: []
  type: TYPE_NORMAL
- en: The primary goal for a self-driving car is safe and efficient navigation from
    point A to point B. If we were to use a reward-based system, the car might receive
    positive rewards for maintaining a safe distance from other objects, adhering
    to speed limits, and following traffic rules. Conversely, it could receive negative
    rewards for risky behaviors, like hard braking or veering off the lane. Tesla
    specifically uses miles driven without an intervention as their reward function.
  prefs: []
  type: TYPE_NORMAL
- en: Available actions
  prefs: []
  type: TYPE_NORMAL
- en: The car’s action space includes accelerating, decelerating, turning, changing
    lanes, and more. Each action is chosen based on the current input data and the
    objective defined by the goal or reward function.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find that agents in systems like self-driving cars rely on foundational
    principles like inputs, goal/reward functions, and available actions. However,
    when delving into the realm of LLMs like GPT, there’s a bespoke set of dynamics
    that cater specifically to their unique nature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how they align with your needs:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs: []
  type: TYPE_NORMAL
- en: 'For LLMs, the gateway is primarily through text. But that doesn’t restrain
    the wealth of information you can use. Whether you’re dealing with thermal readings,
    musical notations, or intricate data structures, your challenge lies in molding
    these into textual representations suitable for an LLM. Think about videos: while
    raw footage might seem incompatible, video text transcriptions allow an LLM to
    extract insights for you.'
  prefs: []
  type: TYPE_NORMAL
- en: Harnessing goal-driven directives
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs primarily use goals defined within your text prompts. By creating effective
    prompts with objectives, you’re not just accessing the LLM’s vast knowledge; you’re
    effectively charting its reasoning path. Think of it as laying down a blueprint:
    your specific prompt instructs the model, guiding it to dissect your overarching
    objective into a systematic sequence of steps.'
  prefs: []
  type: TYPE_NORMAL
- en: Crafting action through functional tools
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are not limited to mere text generation; there’s so much more you can achieve.
    By integrating *ready-made tools* or *custom-developed tools*, you can equip LLMs
    to undertake diverse tasks, from API calls to database engagements or even orchestrating
    external systems. Tools can be written in any programming language, and by adding
    more tools you are effectively *expanding the action space* of what an LLM can
    achieve.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also different components that are directly applicable to LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs: []
  type: TYPE_NORMAL
- en: It’s ideal to store state between agent steps; this is particularly useful for
    chatbots, where remembering the previous chat history provides a better user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Agent planning/execution strategies
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways to achieve a high-level goal, of which a mixture of
    planning and executing is essential.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can use different types of retrieval methods. Semantic similarity within
    vector databases is the most common, but there are others such as including custom
    information from a SQL database into prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive deeper into the shared and different components and explore the implementation
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Reason and Act (ReAct)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many agent frameworks that ultimately aim to improve LLM responses
    toward a goal. The original framework was *ReAct*, which is an improved version
    of CoT, allowing an LLM to create observations after taking actions via tools.
    These observations are then turned into *thoughts* about what would be the *right
    tool* to use within the next step ([Figure 6-1](#fig-6-1)). The LLM continues
    to reason until either a `'Final Answer'` string value is present or a maximum
    number of iterations has taken place.
  prefs: []
  type: TYPE_NORMAL
- en: '![The ReAct Framework](assets/pega_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. The ReAct framework
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The [ReAct](https://oreil.ly/ssdnL) framework uses a mixture of task decomposition,
    a thought loop, and multiple tools to solve questions. Let’s explore the thought
    loop within ReAct:'
  prefs: []
  type: TYPE_NORMAL
- en: Observe the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpret the environment with a thought.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide on an action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Act on the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1–4 until you find a solution or you’ve done too many iterations
    (the solution is “I’ve found the answer”).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can easily create a ReAct-style prompt by using the preceding thought loop
    while also providing the LLM with several inputs such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{question}`: The query that you want answered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`{tools}`: These refer to functions that can be used to accomplish a step within
    the overall task. It is common practice to include a list of tools where each
    tool is a Python function, a name, and a description of the function and its purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a prompt that implements the ReAct pattern with prompt variables
    wrapped in `{}` characters such as `{question}`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a breakdown of the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The introduction of the prompt clearly establishes the LLM’s purpose: `You
    will attempt to solve the problem of finding the answer to a question.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The problem-solving approach is then outlined: `Use chain-of-thought reasoning
    to solve through the problem, using the following pattern:`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The steps in the chain-of-thought reasoning are then laid out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The LLM starts by observing the original question and subsequently formulates
    an observation about it: `original_question: original_problem_text`, `observation:
    observation_text`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on this observation, the AI should formulate a thought that signifies
    a step in the reasoning process: `thought: thought_text`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Having established a thought, it then decides on an action using one of the
    available tools: `action: tool_name`, `action_input: tool_input`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM is then reminded not to make assumptions about what a tool might return,
    and it should explicitly outline its intended action and the corresponding input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`You have access to the following tools: {tools}` communicates to the LLM what
    tools it has available for solving the problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The actual problem that the LLM must solve is then introduced: `original_​problem:
    {question}`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, instructions are provided on how the LLM should respond based on the
    results of its actions. It can either continue with new observations, actions,
    and inputs or, if a solution is found, provide the final answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The prompt outlines a systematic problem-solving process in which the LLM observes
    a problem, thinks about it, decides on an action, and repeats this process until
    a solution is discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Reason and Act Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you’re aware of ReAct, it’s important to create a simple Python implementation
    that replicates what LangChain does automatically, allowing you to build the intuition
    about what’s truly happening between the LLM responses.
  prefs: []
  type: TYPE_NORMAL
- en: To keep it simple, this example will not implement looping and will assume that
    the output can be obtained from a single tool call.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a basic ReAct implementation, you’ll implement the following:'
  prefs: []
  type: TYPE_NORMAL
- en: At every thought, you need to extract the tool that the LLM wants to use. Therefore,
    you’ll extract the last `action` and `action_input`. The `action` represents the
    tool name, while the `action_input` consists of the values of the function arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check whether the LLM thinks that it has found the final answer, in which case
    the thought loop has ended.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can use regular expressions to extract the `action` and `action_input`
    values from the LLM response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break down the regular expression to extract the `action`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`action_pattern = re.compile(r"(?i)action\s*:\s*([^\n]+)", re.MULTILINE)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(?i)`: This is called an *inline flag* and makes the regex pattern case-insensitive.
    It means that the pattern will match “action,” “Action,” “ACTION,” or any other
    combination of uppercase and lowercase letters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`action`: This part of the pattern matches the word *action* literally. Due
    to the case-insensitive flag, it will match any capitalization of the word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`\s*`: This part of the pattern matches zero or more whitespace characters
    (spaces, tabs, etc.). The `\*` means *zero or more*, and `\s` is the regex shorthand
    for a whitespace character.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:` This part of the pattern matches the colon character literally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`\s*`: This is the same as the previous `\s\*` part, matching zero or more
    whitespace characters after the colon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`+([^\n]++)`: This pattern is a capturing group, denoted by the parentheses.
    It matches one or more characters that are *not a newline character*. The `^`
    inside the square brackets `[]` negates the character class, and `\n` represents
    the newline character. The `+` means *one or more*. The text matched by this group
    will be extracted when using the `findall()` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`re.MULTILINE`: This is a flag passed to `re.compile()` function. It tells
    the regex engine that the input text may have multiple lines, so the pattern should
    be applied line by line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In regular expressions, square brackets `[]` are used to define a character
    class, which is a set of characters that you want to match. For example, `[abc]`
    would match any single character that is either `'a'`, `'b'`, or `'c'`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you add a caret `^` at the beginning of the character class, it negates
    the character class, meaning it will match any character that is *not in the character
    class*. In other words, it inverts the set of characters you want to match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, when we use `[^abc]`, it will match any single character that is *not* `'a'`,
    `'b'`, or `'c'`. In the regex pattern `+([^\n]++)`, the character class is `[^n]`,
    which means it will match any character that is *not* a newline character (`\n`).
    The `+` after the negated character class means that the pattern should match
    one or more characters that are not newlines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using the negated character class `[^n]` in the capturing group, we ensure
    that the regex engine captures text up to the end of the line without including
    the newline character itself. This is useful when we want to extract the text
    after the word *action* or *action input* up to the end of the line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, this regular expression pattern matches the word *action* (case-insensitive)
    followed by optional whitespace, a colon, and optional whitespace again, and then
    captures any text up to the end of the line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only difference between these two regex patterns is the literal text they
    are looking for at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '`action_pattern` looks for the word `"action".`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`action_input_pattern` looks for the word `"action_input".`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can now abstract the regex into a Python function that will always find
    the last `action` and `action_input`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To determine and extract whether the LLM has discovered the final answer, you
    can also use regular expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLMs do not always respond in the intended way, so your application needs to
    be able to handle regex parsing errors. Several approaches include using an LLM
    to fix the previous LLM response or making another new LLM request with the previous
    state.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now combine all of the components; here is a step-by-step explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the `ChatOpenAI` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Adding a `stop` sequence forces an LLM to stop generating new tokens after encounting
    the phrase `"tool_result:"`. This helps by stopping hallucinations for tool usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the available tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the base prompt template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the model output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the last `action`, `action_input`, and call the relevant function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the tool details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the current prompt with the tool result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the model output for the current prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the model output for the current prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding steps provide a very simple ReAct implementation. In this case,
    the LLM decided to use the `search_on_google` tool with `"Jason Derulo current
    relationship status"` as the `action_input`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LangChain agents will automatically do all of the preceding steps in a concise
    manner, as well as provide multiple tool usage (through looping) and handling
    for tool failures when an agent can’t parse the `action` or `action_input`.
  prefs: []
  type: TYPE_NORMAL
- en: Before exploring LangChain agents and what they have to offer, it’s vital that
    you learn *tools* and how to create and use them.
  prefs: []
  type: TYPE_NORMAL
- en: Using Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As large language models such as GPT-4 can only generate text, providing tools
    that can perform other actions such as interacting with a database or reading/writing
    files provides an effective method to increase an LLM’s capabilities. A *tool*
    is simply a predefined function that permits the agent to take a specific action.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common part of an agent’s prompt will likely include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Most tools are written as functions within a programming language. As you explore
    LangChain, you’ll find that it offers three different approaches to tool creation/usage:'
  prefs: []
  type: TYPE_NORMAL
- en: Create your own custom tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use preexisting tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leverage `AgentToolkits`, which are multiple tools bundled together to accomplish
    a specific task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start by creating a custom tool that checks the length of a given string
    using LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Following the import of necessary modules, you initialize a `ChatOpenAI` chat
    model. Then create a function called `count_characters_in_string` that computes
    the length of any given string. This function is encapsulated within a `Tool`
    object, providing a descriptive name and explanation for its role.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, you utilize `create_react_agent` to initialize your agent, combining
    the defined `Tool`, the `ChatOpenAI` model, and a react prompt pulled from the
    LangChain hub. This sets up a comprehensive interactive agent.
  prefs: []
  type: TYPE_NORMAL
- en: With `AgentExecutor`, the agent is equipped with the tools and verbose output
    is enabled, allowing for detailed logging.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `agent_executor.invoke(...)` is executed with a query about the character
    count in “supercalifragilisticexpialidocious.” The agent utilizes the defined
    tool to calculate and return the precise character count in the word.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Example 6-1](#ex_agent_custom_tool_character), you can see that the agent
    decided to use the `Action` called `Characters` `in a text string` with an `Action
    Input`: `''supercalifragilisticexpialidocious''`. This pattern is extremely familiar
    to the simplistic ReAct implementation that you previously made.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-1\. A single tool, agent output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Give Direction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Writing expressive names for your Python functions and tool descriptions will
    increase an LLM’s ability to effectively choose the right tools.
  prefs: []
  type: TYPE_NORMAL
- en: Using LLMs as an API (OpenAI Functions)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in [Chapter 4](ch04.html#advanced_text_04), OpenAI [released more
    fine-tuned LLMs](https://oreil.ly/hYTus) tailored toward function calling. This
    is important because it offers an alternative against the standard ReAct pattern
    for tool use. It’s similar to ReAct in that you’re still utilizing an LLM as a
    *reasoning engine.*
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Figure 6-2](#fig-6-2), function calling allows an LLM to easily
    transform a user’s input into a weather API call.
  prefs: []
  type: TYPE_NORMAL
- en: '![Function calling flow using OpenAI functions](assets/pega_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Function calling flow using OpenAI functions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LangChain allows users to effortlessly switch between different agent types
    including ReAct, OpenAI functions, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to [Table 6-1](#table-6-1) for a comprehensive comparison of the different
    agent types.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Comparison of agent types
  prefs: []
  type: TYPE_NORMAL
- en: '| Agent type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI Functions | Works with fine-tuned models like gpt-3.5-turbo-0613 and
    gpt-4-0613 for function calling. It intelligently outputs JSON objects for the
    function calls. Best for open source models and providers adopting this format.
    Note: deprecated in favor of OpenAI Tools. |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI Tools | Enhanced version for newer models, capable of invoking one
    or more functions. It intelligently outputs JSON objects for these function calls,
    optimizing the response efficiency and reducing response times in some architectures.
    |'
  prefs: []
  type: TYPE_TB
- en: '| XML Agent | Ideal for language models like Anthropic’s Claude, which excel
    in XML reasoning/writing. Best used with regular LLMs (not chat models) and unstructured
    tools accepting single string inputs. |'
  prefs: []
  type: TYPE_TB
- en: '| JSON Chat Agent | Tailored for language models skilled in JSON formatting.
    This agent uses JSON to format its outputs, supporting chat models for scenarios
    requiring JSON outputs. |'
  prefs: []
  type: TYPE_TB
- en: '| Structured Chat | Capable of using multi-input tools, this agent is designed
    for complex tasks requiring structured inputs and responses. |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct | Implements ReAct logic, using tools like Tavily’s Search for interactions
    with a document store or search tools. |'
  prefs: []
  type: TYPE_TB
- en: '| Self-Ask with Search | Utilizes the Intermediate Answer tool for factual
    question resolution, following the self-ask with search methodology. Best for
    scenarios requiring quick and accurate factual answers. |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s use prepackaged tools such as a `Calculator` to answer math questions
    using OpenAI function calling from the LangChain documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: After initiating the necessary libraries, you’ll use `ChatOpenAI`, setting the
    `temperature` parameter to 0 for deterministic outputs. By using `hub.pull("...")`,
    you can easily download prompts that have been saved on LangChainHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'This model is then coupled with a tool named `Calculator` that leverages the
    capabilities of `LLMMathChain` to compute math queries. The OpenAI functions agent
    then decides to use the `Calculator` tool to compute `5 + 5` and returns `Answer:
    10`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following on, you can equip an agent with multiple tools, enhancing its versatility.
    To test this, let’s add an extra `Tool` object to our agent that allows it to
    perform a fake Google search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: When executed, the agent will first invoke the `google_search` function and
    then proceed to the `llm_math_chain.run` function. By mixing both custom and prepackaged
    tools, you significantly increase the flexibility of your agents.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Depending upon how many tools you provide, an LLM will either restrict or increase
    its ability to solve different user queries. Also, if you add too many tools,
    the LLM may become confused about what tools to use at every step while solving
    the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are several recommended tools that you might want to explore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Google search](https://oreil.ly/TjrnF)'
  prefs: []
  type: TYPE_NORMAL
- en: Enables an LLM to perform web searches, which provides timely and relevant context.
  prefs: []
  type: TYPE_NORMAL
- en: '[File system tools](https://oreil.ly/5tAB0)'
  prefs: []
  type: TYPE_NORMAL
- en: Essential for managing files, whether it involves reading, writing, or reorganizing
    them. Your LLM can interact with the file system more efficiently with them.
  prefs: []
  type: TYPE_NORMAL
- en: '[Requests](https://oreil.ly/vZjm1)'
  prefs: []
  type: TYPE_NORMAL
- en: A pragmatic tool that makes an LLM capable of executing HTTP requests for create,
    read, update, and delete (CRUD) functionality.
  prefs: []
  type: TYPE_NORMAL
- en: '[Twilio](https://oreil.ly/ECS4r)'
  prefs: []
  type: TYPE_NORMAL
- en: Enhance the functionality of your LLM by allowing it to send SMS messages or
    WhatsApp messages through Twilio.
  prefs: []
  type: TYPE_NORMAL
- en: Divide Labor and Evaluate Quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using tools, make sure you divide the tasks appropriately. For example,
    entrust Twilio with communication services, while assigning requests for HTTP-related
    tasks. Additionally, it is crucial to consistently evaluate the performance and
    quality of the tasks performed by each tool.
  prefs: []
  type: TYPE_NORMAL
- en: Different tools may be called more or less frequently, which will influence
    your LLM agent’s performance. Monitoring tool usage will offer insights into your
    agent’s overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing OpenAI Functions and ReAct
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both OpenAI functions and the ReAct framework bring unique capabilities to the
    table for executing tasks with generative AI models. Understanding the differences
    between them can help you determine which is better suited for your specific use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI functions operate in a straightforward manner. In this setup, the LLM
    decides at runtime whether to execute a function. This is beneficial when integrated
    into a conversational agent, as it provides several features including:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime decision making
  prefs: []
  type: TYPE_NORMAL
- en: The LLM autonomously makes the decision on whether a function(s) should be executed
    or not in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Single tool execution
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI functions are ideal for tasks requiring a single tool execution.
  prefs: []
  type: TYPE_NORMAL
- en: Ease of implementation
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI functions can be easily merged with conversational agents.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel function calling
  prefs: []
  type: TYPE_NORMAL
- en: For single task executions requiring multiple parses, OpenAI functions offer
    parallel function calling to invoke several functions within the same API request.
  prefs: []
  type: TYPE_NORMAL
- en: Use Cases for OpenAI Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your task entails a definitive action such as a simple search or data extraction,
    OpenAI functions are an ideal choice.
  prefs: []
  type: TYPE_NORMAL
- en: ReAct
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you require executions involving multiple sequential tool usage and deeper
    introspection of previous actions, ReAct comes into play. Compared to function
    calling, ReAct is designed to go through many *thought loops* to accomplish a
    higher-level goal, making it suitable for queries with multiple intents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite ReAct’s compatibility with `conversational-react` as an agent, it doesn’t
    yet offer the same level of stability as function calling and often favors toward
    using tools over simply responding with text. Nevertheless, if your task requires
    successive executions, ReAct’s ability to generate many thought loops and decide
    on a single tool at a time demonstrates several distinct features including:'
  prefs: []
  type: TYPE_NORMAL
- en: Iterative thought process
  prefs: []
  type: TYPE_NORMAL
- en: ReAct allows agents to generate numerous thought loops for complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-intent handling
  prefs: []
  type: TYPE_NORMAL
- en: ReAct handles queries with multiple intents effectively, thus making it suitable
    for complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple tool execution
  prefs: []
  type: TYPE_NORMAL
- en: Ideal for tasks requiring multiple tool executions sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: Use Cases for ReAct
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re working on a project that requires introspection of previous actions
    or uses multiple functions in succession such as saving an interview and then
    sending it in an email, ReAct is the best choice.
  prefs: []
  type: TYPE_NORMAL
- en: To aid decision making, see a comprehensive comparison in [Table 6-2](#table-6-2).
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-2\. A feature comparison between OpenAI functions and ReAct
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | OpenAI functions | ReAct |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Runtime decision making | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Single tool execution | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Ease of implementation | ✓ | x |'
  prefs: []
  type: TYPE_TB
- en: '| Parallel function calling | ✓ | x |'
  prefs: []
  type: TYPE_TB
- en: '| Iterative thought process | x | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-intent handling | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Sequential tool execution | x | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Customizable prompt | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: Give Direction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When interacting with different AI frameworks, it’s crucial to understand that
    each framework has its strengths and trade-offs. Each framework will provide a
    unique form of direction to your LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Agent Toolkits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*[Agent toolkits](https://oreil.ly/_v6dm)* are a LangChain integration that
    provides multiple tools and chains together, allowing you to quickly automate
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install some more packages by typing `**pip install langchain_experimental
    pandas tabulate langchain-community pymongo --upgrade**` on your terminal. Popular
    agent toolkits include:'
  prefs: []
  type: TYPE_NORMAL
- en: CSV Agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gmail Toolkit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI Agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python Agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSON Agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pandas DataFrame Agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CSV Agent uses a Pandas DataFrame Agent and `python_repl_ast` tool to investigate
    a *.csv* file. You can ask it to quantify the data, identify column names, or
    create a correlation matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Jupyter Notebook or Python file in *content/chapter_6* of the
    [shared repository](https://oreil.ly/x6FHn), then you will need to import `create_csv_agent`,
    `ChatOpenAI`, and `AgentType`. The `create_csv_agent` function requires an LLM,
    dataset `file path`, and `agent_type`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s even possible for you to interact with a SQL database via a SQLDatabase
    agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]sql\nINSERT INTO Users (FirstName, LastName, Email,'
  prefs: []
  type: TYPE_NORMAL
- en: DateJoined)\nVALUES (\'John\', \'Doe\', \'john.doe@email.com\',
  prefs: []
  type: TYPE_NORMAL
- en: \'2023-05-01\'), \n(\'Mary\', \'Johnson\', \'mary.johnson@email.com\',
  prefs: []
  type: TYPE_NORMAL
- en: \'2023-05-02\'),\n (\'Peter\', \'Smith\', \'peter.smith@email.com\',
  prefs: []
  type: TYPE_NORMAL
- en: \'2023-05-03\'),\n (\'Paul\', \'Brown\', \'paul.brown@email.com\',
  prefs: []
  type: TYPE_NORMAL
- en: \'2023-05-04\'),\n (\'Jane\', \'Davis\', \'jane.davis@email.com\',
  prefs: []
  type: TYPE_NORMAL
- en: \'2023-05-05\');\n[PRE28]
  prefs: []
  type: TYPE_NORMAL
- en: First, the `agent_executor` inspects the SQL database to understand the database
    schema, and then the agent writes and executes a SQL statement that successfully
    adds five users into the SQL table.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing Standard Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s worth considering how to customize LangChain agents. Key function arguments
    can include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`prefix` and `suffix` are the prompt templates that are inserted directly into
    the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_iterations` and `max_execution_time` provide you with a way to limit API
    and compute costs in case an agent becomes stuck in an endless loop:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s update the previously created `agent_executor` so that the agent can
    perform more SQL statements. The `SQL_PREFIX` is directly inserted into the `create_sql_agent`
    function as the `prefix`. Additionally, you’ll insert the recommended `user_sql`
    from the previous agent that wouldn’t directly run `INSERT`, `UPDATE`, or `EDIT`
    commands; however, the new agent will happily execute CRUD (create, read, update,
    delete) operations against the SQLite database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Custom Agents in LCEL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s very easy to create a custom agent using LCEL; let’s create a chat model
    with one tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you’ll set up the prompt with a system message, user message, and a `MessagesPlaceholder`,
    which allows the agent to store its intermediate steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Before creating an agent, you’ll need to bind the tools directly to the LLM
    for function calling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a step-by-step walk-through of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Importing tool conversion function
  prefs: []
  type: TYPE_NORMAL
- en: You begin by importing `convert_to_openai_tool`. This allows you to convert
    Python function tools into a JSON schema, making them compatible with OpenAI’s
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Binding tools to your language model (LLM)
  prefs: []
  type: TYPE_NORMAL
- en: Next, you bind the tools to your LLM. By iterating over each tool in your `tools`
    list and converting them with `convert_to_openai_tool`, you effectively create
    `llm_with_tools`. This equips your LLM with the functionalities of the defined
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Importing agent formatting and parsing functions
  prefs: []
  type: TYPE_NORMAL
- en: Here, you import `format_to_openai_tool_messages` and `OpenAIToolsAgentOutputParser`.
    These format the agent’s scratchpad and parse the output from your LLM bound with
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Setting up your agent chain
  prefs: []
  type: TYPE_NORMAL
- en: In this final and crucial step, you set up the agent chain.
  prefs: []
  type: TYPE_NORMAL
- en: You take the lead by processing the user’s input directly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You then strategically format intermediate steps into OpenAI function messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `llm_with_tools` will then be called.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OpenAIToolsAgentOutputParser` is used to parse the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, let’s create and use the `AgentExecutor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The LCEL agent uses the `.invoke(...)` function and correctly identifies that
    there are eight letters within the word *software*.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and Using Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When interacting with LLMs, understanding the role and importance of memory
    is paramount. It’s not just about how these models recall information but also
    about the strategic interplay between long-term (LTM) and short-term memory (STM).
  prefs: []
  type: TYPE_NORMAL
- en: Long-Term Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Think of long-term memory as the library of an LLM. It’s the vast, curated collection
    of data, storing everything from text to conceptual frameworks. This knowledge
    pool aids the model in comprehending and generating responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications include:'
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases
  prefs: []
  type: TYPE_NORMAL
- en: These databases can store unstructured text data, providing the model with a
    reference point when generating content. By indexing and categorizing this data,
    LLMs can swiftly retrieve relevant information via *similarity distance metrics*.
  prefs: []
  type: TYPE_NORMAL
- en: Self-reflection
  prefs: []
  type: TYPE_NORMAL
- en: Advanced applications include an LLM that introspects, records, and stores thoughts.
    Imagine an LLM that meticulously observes user patterns on a book review platform
    and catalogs these as deep insights. Over time, it pinpoints preferences, such
    as favored genres and writing styles. These insights are stored and accessed using
    retrieval. When users seek book recommendations, the LLM, *powered by the retrieved
    context*, provides bespoke suggestions aligned with their tastes.
  prefs: []
  type: TYPE_NORMAL
- en: Custom retrievers
  prefs: []
  type: TYPE_NORMAL
- en: Creating specific retrieval functions can significantly boost an LLM’s efficiency.
    Drawing parallels with human memory systems, these functions can prioritize data
    based on its relevance, the elapsed time since the last memory, and its utility
    in achieving a particular objective.
  prefs: []
  type: TYPE_NORMAL
- en: Short-Term Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Short-term memory in LLMs is akin to a temporary workspace. Here, recent interactions,
    active tasks, or ongoing conversations are kept at the forefront to ensure continuity
    and context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications include:'
  prefs: []
  type: TYPE_NORMAL
- en: Conversational histories
  prefs: []
  type: TYPE_NORMAL
- en: For chatbots, tracking conversational history is essential. It allows the bot
    to maintain context over multiple exchanges, preventing redundant queries and
    ensuring the conversation flows naturally.
  prefs: []
  type: TYPE_NORMAL
- en: Repetition avoidance
  prefs: []
  type: TYPE_NORMAL
- en: STM proves invaluable when similar or identical queries are posed by users.
    By referencing its short-term recall, the model can provide consistent answers
    or diversify its responses, based on the application’s requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Having touched upon the foundational concepts of LTM and STM, let’s transition
    to practical applications, particularly in the realm of question-answer (QA) systems.
  prefs: []
  type: TYPE_NORMAL
- en: Short-Term Memory in QA Conversation Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine Eva, a virtual customer support agent for an e-commerce platform. A
    user might have several interlinked queries:'
  prefs: []
  type: TYPE_NORMAL
- en: 'User: “How long is the return policy for electronics?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eva: “The return policy for electronics is 30 days.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'User: “What about for clothing items?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eva, leveraging STM: “For clothing items, it’s 45 days. Would you like to know
    about any other categories?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that by utilizing short term memory (STM), Eva seamlessly continues the
    conversation, anticipating potential follow-up questions. This fluidity is only
    possible due to the effective deployment of short-term memory, allowing the agent
    to perceive conversations not as isolated QAs but as a cohesive interaction.
  prefs: []
  type: TYPE_NORMAL
- en: For developers and prompt engineers, understanding and harnessing this can significantly
    elevate the user experience, fostering engagements that are meaningful, efficient,
    and humanlike.
  prefs: []
  type: TYPE_NORMAL
- en: Memory in LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LangChain provides easy techniques for adding memory to LLMs. As shown in [Figure 6-3](#fig-6-3),
    every memory system in a chain is tasked with two fundamental operations: reading
    and storing.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s pivotal to understand that each chain has innate steps that demand particular
    inputs. While a user provides some of this data, the chain can also source other
    pieces of information from its memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory within LangChain](assets/pega_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Memory within LangChain
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In every operation of the chain, there are two crucial interactions with its
    memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '*After collecting the initial user data but before executing*, the chain retrieves
    information from its memory, adding to the user’s input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*After the chain has completed but before returning the answer*, a chain will
    write the inputs and outputs of the current run to memory so that they can be
    referred to in future runs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two pivotal choices you’ll need to make when creating a memory system:'
  prefs: []
  type: TYPE_NORMAL
- en: The method of storing state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approach to querying the memory state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preserving the State
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beneath the surface, the foundational memory of generative AI models is structured
    as a sequence of chat messages. These messages can be stored in temporary in-memory
    lists or anchored in a more durable database. For those leaning toward long-term
    storage, there’s a wide range of [database integrations available](https://oreil.ly/ECD_n),
    streamlining the process and saving you from the hassle of manual integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'With five to six lines of code, you can easily integrate a `MongoDBChatMessageHistory`
    that’s unique based on a `session_id` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Querying the State
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A basic memory framework might merely relay the latest messages with every interaction.
    A slightly more nuanced setup might distill a crisp synopsis of the last set of
    messages. An even more advanced setup would discern specific entities from dialogue
    and relay only data about those entities highlighted in the ongoing session.
  prefs: []
  type: TYPE_NORMAL
- en: Different applications require varying demands on memory querying. LangChain’s
    memory toolkit will help you to create simplistic memory infrastructures while
    empowering you to architect bespoke systems when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: ConversationBufferMemory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are various types of memory within LangChain, and one of the most popular
    is ConversationBufferMemory. This allows you to store multiple chat messages with
    no restriction on chat history size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing `ConversationBufferMemory`, and you can then add context
    with the `save_context` function. The `load_memory_variables` function returns
    a Python dictionary containing the `Human` and `AI` messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also return the LangChain schema messages, i.e., `SystemMessage`, `AIMessage`
    or `HumanMessage`, by adding `return_messages=True` to `ConversationBufferMemory`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s add memory directly to a chain in LCEL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Notice the `MessagesPlaceholder` has a `variable_name` of `"history"`. This
    is aligned with the `memory` key within `ConversationBufferMemory`, allowing the
    previous chat history to be directly formatted into the `ChatPromptTemplate`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting up the LCEL chain, let’s invoke it and save the messages to the
    `memory` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The memory has two messages, a `HumanMessage` and an `AIMessage`; both are
    saved to memory by using the `save_context` function. Let’s test whether the LCEL
    chain is able to use previous context to answer new questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The LCEL chain is now able to use previous messages to answer new queries!
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, you can easily add memory to an agent by adding a `MessagesPlaceHolder`
    to the `ChatPromptTemplate` and adding memory to the `AgentExecutor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: You can view the full implementation within this [Jupyter Notebook](https://oreil.ly/LXQNy).
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging this memory, your agent delivers a more context-aware and fluid
    conversational experience, negating the need for additional tools to recall past
    interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'ConversationBufferMemory doesn’t have a buffer limit, but different memory
    types such as ConversationSummaryBufferMemory allow you specify a maximum token
    limit, after which the conversation is summarized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By default, memory is stored locally within the Python process. This approach
    is inherently transient and limited by the session or process lifespan. For applications
    requiring continuity over time and the ability to learn from historical data,
    a shift to database-backed memory becomes essential.
  prefs: []
  type: TYPE_NORMAL
- en: There are several integrations available for [database-backed memory](https://oreil.ly/nTBox),
    which transition the memory usage from a short-term, session-specific context
    to a more robust, long-term storage solution.
  prefs: []
  type: TYPE_NORMAL
- en: Other Popular Memory Types in LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While ConversationBufferMemory is a well-known memory type, it has limitations
    such as context length limits, potential lack of relevance, and lack of summarization.
    To address these issues, LangChain offers several other memory types.
  prefs: []
  type: TYPE_NORMAL
- en: ConversationBufferWindowMemory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This type maintains a sliding window of the most recent interactions, ensuring
    the buffer doesn’t grow excessively large. Features include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Keeps only the last `K` interactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can return history as either a string or a list of messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: ConversationSummaryMemory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This one condenses and summarizes the conversation over time and is ideal for
    longer conversations where verbatim message history would be token-expensive.
    Features include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Summarizes conversation on the fly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can return history as a summary string or a list of system messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows direct prediction of new summaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be initialized with existing messages or summaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: ConversationSummaryBufferMemory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a hybrid memory that maintains a buffer of recent interactions but also
    compiles older interactions into a summary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Features include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Uses token length to determine when to flush interactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can return history as a summary with recent interactions or a list of messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows direct prediction of new summaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: ConversationTokenBufferMemory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This one keeps a buffer of recent interactions using token length to determine
    when to flush interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Features include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Uses token length for flushing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can return history as a string or a list of messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: You’ve learned about the importance of memory in LangChain. Also, you now understand
    how to build and customize a memory system using LangChain’s memory toolkit, including
    methods of storing state and querying memory; you’ve seen examples on integrating
    MongoDBChatMessageHistory and utilizing the versatile ConversationBufferMemory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s summarize the different memory types available in LangChain and when
    they might be particularly useful:'
  prefs: []
  type: TYPE_NORMAL
- en: ConversationBufferWindowMemory
  prefs: []
  type: TYPE_NORMAL
- en: This memory type maintains the most recent interactions, thus proving useful
    in cases where the context of the conversation is essential without letting the
    buffer grow extensively large.
  prefs: []
  type: TYPE_NORMAL
- en: ConversationSummaryMemory
  prefs: []
  type: TYPE_NORMAL
- en: Ideal for extended conversations, this memory type provides summarized versions
    of the conversation, saving valuable token space.
  prefs: []
  type: TYPE_NORMAL
- en: ConversationSummaryBufferMemory
  prefs: []
  type: TYPE_NORMAL
- en: Convenient for situations where you not only want to maintain a record of recent
    interactions but also want to compile older interactions into a summary, thereby
    offering a hybrid approach.
  prefs: []
  type: TYPE_NORMAL
- en: ConversationTokenBufferMemory
  prefs: []
  type: TYPE_NORMAL
- en: This memory type is useful when defining a specific token length is vital and
    a buffer of recent interactions needs to be maintained. It determines when to
    flush interactions based on token length.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the different memory options available can help you choose the
    most suitable one for your exact needs, depending on the situation.
  prefs: []
  type: TYPE_NORMAL
- en: Give Direction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even as you’re determining which memory type to use, remember to direct the
    AI model appropriately. For instance, with ConversationBufferWindowMemory, you
    would need to specify the number of recent interactions (`K`) you want to keep.
    Be clear about your requirements for optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Functions Agent with Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dive deeper into agents with a comprehensive example available on [GitHub](https://oreil.ly/jyLab).
    In this example, you’ll uncover how OpenAI integrates several essential components:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory management using chat messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use tools such as API requests and file saving that can handle multiple function
    parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate a custom `SystemMessage` to guide and define the agent’s behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To illustrate, consider how a Python function’s docstring is utilized to provide
    a tool’s description:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`StructuredTool.from_function()` will create a LangChain tool that’s capable
    of accepting multiple function arguments.'
  prefs: []
  type: TYPE_NORMAL
- en: Give Direction and Specify Format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The docstring within the Python function showcases a designated format guiding
    the LLM on the content to use for the `raw_interview_text` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the `return` statement emphasizes instructing the LLM to inform
    the user that the interview has been stored. This ensures the agent returns a
    more conversational response post-tool execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'To further demonstrate prompt engineering techniques, let’s examine another
    Python code snippet from the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `args_schema` is used within the `SummarizeFileFromURL` class.
    This attribute leverages the `ArgumentType` class, ensuring that the tool’s arguments
    are validated before execution. Specifically, it enforces that a valid URL string
    be provided and that the `file_type` argument should be either `"pdf"` or `"txt"`.
  prefs: []
  type: TYPE_NORMAL
- en: By adding validation checks, you can guarantee that the agent processes functional
    arguments correctly, which, in turn, enhances the overall reliability and efficiency
    of tool execution.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Agent Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You now know about ReAct and OpenAI functions, but there are several other agent
    frameworks. Two other popular frameworks include *plan and execute agents* and
    *tree of thoughts*.
  prefs: []
  type: TYPE_NORMAL
- en: Plan-and-Execute Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rather than have the LLM do the task planning and tool execution, you can separate
    this into two separate modules. Each module can be handled separately by an individual
    LLM that has access to the objective, current tasks, and completed tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Two popular versions of the plan-and-execute framework include [BabyAGI](https://oreil.ly/xeijG)
    and [AutoGPT](https://oreil.ly/M4z8K).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-4](#fig-6-4) showcases BabyAGI’s agent setup, which is designed to
    merge OpenAI LLMs with vector databases such as Chroma/Weaviate to create a robust,
    adaptive task management system.'
  prefs: []
  type: TYPE_NORMAL
- en: In a continuous loop, the agent starts by fetching a task and passes it to the
    `execution_agent`, which taps into OpenAI to perform the task based on contextual
    data. After this, the outcomes are enriched and archived in Chroma/Weaviate.
  prefs: []
  type: TYPE_NORMAL
- en: The `task_creation_agent` then steps in, utilizing OpenAI to discern new tasks
    from the objective and results of the prior task. These tasks are presented as
    a list of dictionaries, giving structure to the resultant tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The `prioritization_agent` then interacts with OpenAI to rearrange the task
    list, ensuring alignment with the main objective. The synergy of these agents
    ensures that the system is always evolving, continuously generating and prioritizing
    tasks in an informed manner. Integrating [Chroma](https://oreil.ly/9R3pU) or [Weaviate](https://oreil.ly/2wu-y)
    plays a crucial role by offering a reservoir of contextual data, ensuring that
    tasks are always aligned with their predefined goals.
  prefs: []
  type: TYPE_NORMAL
- en: '![BabyAGI](assets/pega_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. BabyAGI’s agent architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The [plan-and-execute agent type](https://oreil.ly/8vYF5) does exist within
    LangChain, though it’s still experimental.
  prefs: []
  type: TYPE_NORMAL
- en: Tree of Thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the application of language models in problem-solving expands across diverse
    tasks, their inference method remains bound to token-level, linear processing.
    This approach, while effective in many contexts, is limited when faced with tasks
    that need advanced strategic foresight or where the initial decisions are crucial.
    The [Tree of Thoughts (ToT) framework](https://oreil.ly/1rYDI) is a novel way
    to harness language models that goes beyond the conventional chain-of-thought
    prompting technique ([Figure 6-5](#fig-6-5)).
  prefs: []
  type: TYPE_NORMAL
- en: The central premise of ToT is to enable exploration across coherent text chunks,
    termed *thoughts*. These thoughts represent stages in problem-solving, facilitating
    the language model to undertake a more deliberate decision-making process. Instead
    of sticking to one reasoning path, the model can explore various reasoning trajectories,
    self-assessing its decisions at each step. The framework is designed to allow
    for forward planning, revisiting past decisions, and making overarching choices.
  prefs: []
  type: TYPE_NORMAL
- en: '![Tree of Thoughts](assets/pega_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Tree of Thoughts (ToT)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Evidence of its success comes from experimental results on tasks requiring intricate
    planning or searching capabilities. In a game like *game of 24*, the traditional
    GPT-4, when prompted using chain-of-thought, managed a 4% success rate. In contrast,
    the ToT approach skyrocketed this figure to an impressive 74%. This paradigm shift
    isn’t limited to games. The ToT method also showed promise in areas like creative
    writing and mini crosswords, underscoring its versatility.
  prefs: []
  type: TYPE_NORMAL
- en: Complementing the theory is a [LangChain implementation](https://oreil.ly/fub1z),
    which gives a glimpse into how ToT can be actualized. A sudoku puzzle serves as
    the illustrative example, with the main aim to replace wildcard characters (*)
    with numbers, while adhering to sudoku rules.
  prefs: []
  type: TYPE_NORMAL
- en: ToT is not just a new method; it’s a paradigm shift in how we envision language
    model inference. By providing models the capacity to think, backtrack, and strategize,
    ToT is redefining the boundaries of AI problem-solving.
  prefs: []
  type: TYPE_NORMAL
- en: If you consider ToT as a strategy for commanding LLMs, LangChain callbacks can
    be viewed as tools to diagnose and ensure the smooth operation of these strategies.
    Let’s dive into how you can harness this feature effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LangChain’s [*callbacks*](https://oreil.ly/8EhXl) empower you to seamlessly
    monitor and pinpoint issues within your application. Until now, you’ve encountered
    the parameter `verbose=True` in `AgentExecutor` chains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This parameter logs useful outputs for debugging purposes, but what if you’re
    keen on tracking specific events? Enter callbacks, your go-to solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `BaseCallbackHandler` class acts as a foundation for monitoring and responding
    to various events during the execution of your generative AI models. Each method
    in this class corresponds to specific stages like the start, end, or even errors
    during the model’s runtime. For instance, the `on_llm_start` gets triggered when
    an LLM begins its operation. Similarly, methods like `on_chain_error` and `on_tool_end`
    react to errors in chains or after using a tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Each callback can be scoped to either the class or individual requests.
  prefs: []
  type: TYPE_NORMAL
- en: Global (Constructor) Callbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When defining callbacks within a constructor, like `AgentExecutor(callbacks=[handler],
    tags=[''a-tag''])`, they are activated for every call made on that instance. These
    callbacks are limited to that specific instance. To illustrate, when a handler
    is passed to an `LLMChain` during its creation, it won’t interact with any children
    chains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The tags you include, such as `'a-tag'`, can be tremendously useful in tracing
    and sorting the outputs of your generative AI setup. Especially in large projects
    with numerous chains, utilizing tags can significantly streamline your workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Request-Specific Callbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On the other hand, callbacks can be defined within the `invoke()` method. For
    instance, a request to an `LLMChain` might subsequently trigger another `LLMChain`
    request, and the same handler would be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The Verbose Argument
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common utility, the `verbose` argument, is accessible for most API objects.
    When you use `AgentExecutor(verbose=True)`, it’s the same as integrating a `ConsoleCallbackHandler`
    into the callbacks argument of the object and its descendants. It acts as a useful
    debugging tool by logging every event directly to your console.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use Which?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Constructor callbacks
  prefs: []
  type: TYPE_NORMAL
- en: Ideal for overarching tasks like logging or monitoring across an entire chain.
    If tracking all interactions within agents is your goal, attach the handler during
    its initiation.
  prefs: []
  type: TYPE_NORMAL
- en: Request callbacks
  prefs: []
  type: TYPE_NORMAL
- en: Tailored for specific use cases like streaming, where outputs from a single
    request are relayed to dedicated endpoints, say a websocket. So, for a scenario
    where the output from a singular request needs to be streamed to a websocket,
    the handler should be linked to the `invoke()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Verbose arguments
  prefs: []
  type: TYPE_NORMAL
- en: Useful for debugging and local LLM development, but it can generate a large
    number of logs.
  prefs: []
  type: TYPE_NORMAL
- en: Token Counting with LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain provides an effective method for token counting during your interactions
    with generative AI models.
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to set up the necessary modules; import the `asyncio` module and the
    relevant functions from the LangChain package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, employ the `get_openai_callback` context manager to make a request and
    count the tokens used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: After executing this code, `total_tokens` will store the number of tokens used
    for your request.
  prefs: []
  type: TYPE_NORMAL
- en: 'When making multiple requests within the context manager, you can verify that
    the total tokens counted are accurate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: As you can observe, making the same request twice often results in `cb.total_tokens`
    being twice the value of `total_tokens`.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain supports concurrent runs, letting you execute multiple requests at
    the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '`cb` provides a detailed breakdown of your interaction with the AI model, offering
    key metrics that are pivotal for prompt engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cb.successful_requests` tracks the number of requests that have been executed
    successfully. It’s a direct indicator of how many API requests were effectively
    processed without encountering errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With `cb.total_cost`, you get a transparent view of the cost associated with
    your requests. This can be a crucial metric for budgeting and managing expenses
    when working extensively with the AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cb.total_tokens` denotes the cumulative number of tokens used in both the
    prompt and the completion. This provides a holistic view of token consumption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cb.prompt_tokens` gives insight into how many tokens were used in the prompts
    you provided. This can guide you in optimizing your prompts to be concise yet
    effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cb.completion_tokens` highlights the number of tokens taken up by the AI’s
    response. This can be beneficial when analyzing the verbosity or depth of the
    AI’s answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the concept of chain-of-thought reasoning
    and its importance in autonomous agents. You discovered how LLMs can break down
    complex problems into smaller components to provide effective solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you explored the agent-based architecture in generative AI models
    and gained valuable insights into memory integration and advanced agent frameworks.
    You investigated several agent frameworks such as ReAct and OpenAI function calling
    and learned that these frameworks enhance LLM model responses by utilizing external
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](ch07.html#intro_image_07), you’ll be introduced to image generation
    using generative AI. You will learn the history of generative AI image models,
    including the strengths and weaknesses of each vendor.
  prefs: []
  type: TYPE_NORMAL
