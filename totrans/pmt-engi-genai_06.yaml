- en: Chapter 6\. Autonomous Agents with Memory and Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter dives deeper into the importance of chain-of-thought reasoning
    and the ability of large language models (LLMs) to reason through complex problems
    as agents. By breaking down complex problems into smaller, more manageable components,
    LLMs can provide more thorough and effective solutions. You will also learn about
    the components that make up autonomous agents, such as inputs, goal or reward
    functions, and available actions.
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-Thought
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability of AI to reason through complex problems is essential for creating
    effective, reliable, and user-friendly applications.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chain-of-thought reasoning* (CoT) is a method of guiding LLMs through a series
    of steps or logical connections to reach a conclusion or solve a problem. This
    approach is particularly useful for tasks that require a deeper understanding
    of context or multiple factors to consider.'
  prefs: []
  type: TYPE_NORMAL
- en: '[CoT](https://oreil.ly/fAeLo) is asking an LLM to *think* through complex problems,
    breaking them down into smaller, more manageable components. This allows the LLM
    to focus on each part individually, ensuring a more thorough understanding of
    the issue at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, chain-of-thought reasoning might involve:'
  prefs: []
  type: TYPE_NORMAL
- en: Asking an LLM to provide explanations for its decisions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning multiple steps before deciding on a final answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, you’ll explore examples of both ineffective and effective
    chain-of-thought reasoning. We will also discuss various techniques for building
    effective chain-of-thought reasoning and how they can be integrated into AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine that a user wants the AI to generate a comprehensive marketing
    plan for promoting a new software product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this example, GPT-4 doesn’t use chain-of-thought reasoning, and it does not
    address the specific aspects of the marketing plan. The LLM generates a generic
    list of marketing strategies that could apply to any product, rather than focusing
    on the unique characteristics of the new software product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now GPT-4 specifically addresses the unique characteristics of the new software
    product, demonstrating effective chain-of-thought reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Give Direction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take note of the phrase *step-by-step*, a critical element in CoT. By incorporating
    this phrase into your prompt, you’re asking the LLM to reason through the steps
    that are required to generate a highly effective software product.
  prefs: []
  type: TYPE_NORMAL
- en: Also, by providing a $20,000 budget and the type of software, GPT-4 is able
    to provide a much more relevant and contextualized response.
  prefs: []
  type: TYPE_NORMAL
- en: Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI models have given rise to an *agent-based architecture*. Conceptually,
    an agent acts, perceives, and makes decisions within a specified environment to
    achieve predefined objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Agents can take various actions such as executing a Python function; afterward,
    the agent will observe what happens and will decide on whether it is finished
    or what action to take next.
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent will continously loop through a series of actions and observations
    until there are no further actions, as you can see in the following pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The behavior of the agent is governed by three principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs: []
  type: TYPE_NORMAL
- en: These are the sensory stimuli or data points the agent receives from its environment.
    Inputs can be diverse, ranging from visual (like images) and auditory (like audio
    files) to thermal signals and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: Goal or reward function
  prefs: []
  type: TYPE_NORMAL
- en: This represents the guiding principle for an agent’s actions. In goal-based
    frameworks, the agent is tasked with reaching a specific end state. In a reward-based
    setting, the agent is driven to maximize cumulative rewards over time, often in
    dynamic environments.
  prefs: []
  type: TYPE_NORMAL
- en: Available actions
  prefs: []
  type: TYPE_NORMAL
- en: The *action space* is the range of permissible actions an agent [can undertake
    at any given moment](https://oreil.ly/5AVfM). The breadth and nature of this space
    are contingent upon the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain these concepts further, consider a self-driving car:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs: []
  type: TYPE_NORMAL
- en: The car’s sensors, such as cameras, LIDAR, and ultrasonic sensors, provide a
    continuous stream of data about the environment. This can include information
    about nearby vehicles, pedestrians, road conditions, and traffic signals.
  prefs: []
  type: TYPE_NORMAL
- en: Goal or reward function
  prefs: []
  type: TYPE_NORMAL
- en: The primary goal for a self-driving car is safe and efficient navigation from
    point A to point B. If we were to use a reward-based system, the car might receive
    positive rewards for maintaining a safe distance from other objects, adhering
    to speed limits, and following traffic rules. Conversely, it could receive negative
    rewards for risky behaviors, like hard braking or veering off the lane. Tesla
    specifically uses miles driven without an intervention as their reward function.
  prefs: []
  type: TYPE_NORMAL
- en: Available actions
  prefs: []
  type: TYPE_NORMAL
- en: The car’s action space includes accelerating, decelerating, turning, changing
    lanes, and more. Each action is chosen based on the current input data and the
    objective defined by the goal or reward function.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find that agents in systems like self-driving cars rely on foundational
    principles like inputs, goal/reward functions, and available actions. However,
    when delving into the realm of LLMs like GPT, there’s a bespoke set of dynamics
    that cater specifically to their unique nature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how they align with your needs:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs: []
  type: TYPE_NORMAL
- en: 'For LLMs, the gateway is primarily through text. But that doesn’t restrain
    the wealth of information you can use. Whether you’re dealing with thermal readings,
    musical notations, or intricate data structures, your challenge lies in molding
    these into textual representations suitable for an LLM. Think about videos: while
    raw footage might seem incompatible, video text transcriptions allow an LLM to
    extract insights for you.'
  prefs: []
  type: TYPE_NORMAL
- en: Harnessing goal-driven directives
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs primarily use goals defined within your text prompts. By creating effective
    prompts with objectives, you’re not just accessing the LLM’s vast knowledge; you’re
    effectively charting its reasoning path. Think of it as laying down a blueprint:
    your specific prompt instructs the model, guiding it to dissect your overarching
    objective into a systematic sequence of steps.'
  prefs: []
  type: TYPE_NORMAL
- en: Crafting action through functional tools
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are not limited to mere text generation; there’s so much more you can achieve.
    By integrating *ready-made tools* or *custom-developed tools*, you can equip LLMs
    to undertake diverse tasks, from API calls to database engagements or even orchestrating
    external systems. Tools can be written in any programming language, and by adding
    more tools you are effectively *expanding the action space* of what an LLM can
    achieve.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also different components that are directly applicable to LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs: []
  type: TYPE_NORMAL
- en: It’s ideal to store state between agent steps; this is particularly useful for
    chatbots, where remembering the previous chat history provides a better user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Agent planning/execution strategies
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways to achieve a high-level goal, of which a mixture of
    planning and executing is essential.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can use different types of retrieval methods. Semantic similarity within
    vector databases is the most common, but there are others such as including custom
    information from a SQL database into prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive deeper into the shared and different components and explore the implementation
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Reason and Act (ReAct)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many agent frameworks that ultimately aim to improve LLM responses
    toward a goal. The original framework was *ReAct*, which is an improved version
    of CoT, allowing an LLM to create observations after taking actions via tools.
    These observations are then turned into *thoughts* about what would be the *right
    tool* to use within the next step ([Figure 6-1](#fig-6-1)). The LLM continues
    to reason until either a `'Final Answer'` string value is present or a maximum
    number of iterations has taken place.
  prefs: []
  type: TYPE_NORMAL
- en: '![The ReAct Framework](assets/pega_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. The ReAct framework
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The [ReAct](https://oreil.ly/ssdnL) framework uses a mixture of task decomposition,
    a thought loop, and multiple tools to solve questions. Let’s explore the thought
    loop within ReAct:'
  prefs: []
  type: TYPE_NORMAL
- en: Observe the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpret the environment with a thought.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide on an action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Act on the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1–4 until you find a solution or you’ve done too many iterations
    (the solution is “I’ve found the answer”).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can easily create a ReAct-style prompt by using the preceding thought loop
    while also providing the LLM with several inputs such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{question}`: The query that you want answered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`{tools}`: These refer to functions that can be used to accomplish a step within
    the overall task. It is common practice to include a list of tools where each
    tool is a Python function, a name, and a description of the function and its purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a prompt that implements the ReAct pattern with prompt variables
    wrapped in `{}` characters such as `{question}`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a breakdown of the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The introduction of the prompt clearly establishes the LLM’s purpose: `You
    will attempt to solve the problem of finding the answer to a question.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The problem-solving approach is then outlined: `Use chain-of-thought reasoning
    to solve through the problem, using the following pattern:`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The steps in the chain-of-thought reasoning are then laid out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The LLM starts by observing the original question and subsequently formulates
    an observation about it: `original_question: original_problem_text`, `observation:
    observation_text`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on this observation, the AI should formulate a thought that signifies
    a step in the reasoning process: `thought: thought_text`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Having established a thought, it then decides on an action using one of the
    available tools: `action: tool_name`, `action_input: tool_input`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM is then reminded not to make assumptions about what a tool might return,
    and it should explicitly outline its intended action and the corresponding input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`You have access to the following tools: {tools}` communicates to the LLM what
    tools it has available for solving the problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The actual problem that the LLM must solve is then introduced: `original_​problem:
    {question}`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, instructions are provided on how the LLM should respond based on the
    results of its actions. It can either continue with new observations, actions,
    and inputs or, if a solution is found, provide the final answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The prompt outlines a systematic problem-solving process in which the LLM observes
    a problem, thinks about it, decides on an action, and repeats this process until
    a solution is discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Reason and Act Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you’re aware of ReAct, it’s important to create a simple Python implementation
    that replicates what LangChain does automatically, allowing you to build the intuition
    about what’s truly happening between the LLM responses.
  prefs: []
  type: TYPE_NORMAL
- en: To keep it simple, this example will not implement looping and will assume that
    the output can be obtained from a single tool call.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a basic ReAct implementation, you’ll implement the following:'
  prefs: []
  type: TYPE_NORMAL
- en: At every thought, you need to extract the tool that the LLM wants to use. Therefore,
    you’ll extract the last `action` and `action_input`. The `action` represents the
    tool name, while the `action_input` consists of the values of the function arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check whether the LLM thinks that it has found the final answer, in which case
    the thought loop has ended.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can use regular expressions to extract the `action` and `action_input`
    values from the LLM response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break down the regular expression to extract the `action`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`action_pattern = re.compile(r"(?i)action\s*:\s*([^\n]+)", re.MULTILINE)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(?i)`: This is called an *inline flag* and makes the regex pattern case-insensitive.
    It means that the pattern will match “action,” “Action,” “ACTION,” or any other
    combination of uppercase and lowercase letters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`action`: This part of the pattern matches the word *action* literally. Due
    to the case-insensitive flag, it will match any capitalization of the word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`\s*`: This part of the pattern matches zero or more whitespace characters
    (spaces, tabs, etc.). The `\*` means *zero or more*, and `\s` is the regex shorthand
    for a whitespace character.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:` This part of the pattern matches the colon character literally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`\s*`: This is the same as the previous `\s\*` part, matching zero or more
    whitespace characters after the colon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`+([^\n]++)`: This pattern is a capturing group, denoted by the parentheses.
    It matches one or more characters that are *not a newline character*. The `^`
    inside the square brackets `[]` negates the character class, and `\n` represents
    the newline character. The `+` means *one or more*. The text matched by this group
    will be extracted when using the `findall()` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`re.MULTILINE`: This is a flag passed to `re.compile()` function. It tells
    the regex engine that the input text may have multiple lines, so the pattern should
    be applied line by line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In regular expressions, square brackets `[]` are used to define a character
    class, which is a set of characters that you want to match. For example, `[abc]`
    would match any single character that is either `'a'`, `'b'`, or `'c'`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you add a caret `^` at the beginning of the character class, it negates
    the character class, meaning it will match any character that is *not in the character
    class*. In other words, it inverts the set of characters you want to match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, when we use `[^abc]`, it will match any single character that is *not* `'a'`,
    `'b'`, or `'c'`. In the regex pattern `+([^\n]++)`, the character class is `[^n]`,
    which means it will match any character that is *not* a newline character (`\n`).
    The `+` after the negated character class means that the pattern should match
    one or more characters that are not newlines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using the negated character class `[^n]` in the capturing group, we ensure
    that the regex engine captures text up to the end of the line without including
    the newline character itself. This is useful when we want to extract the text
    after the word *action* or *action input* up to the end of the line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, this regular expression pattern matches the word *action* (case-insensitive)
    followed by optional whitespace, a colon, and optional whitespace again, and then
    captures any text up to the end of the line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only difference between these two regex patterns is the literal text they
    are looking for at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '`action_pattern` looks for the word `"action".`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`action_input_pattern` looks for the word `"action_input".`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can now abstract the regex into a Python function that will always find
    the last `action` and `action_input`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To determine and extract whether the LLM has discovered the final answer, you
    can also use regular expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLMs do not always respond in the intended way, so your application needs to
    be able to handle regex parsing errors. Several approaches include using an LLM
    to fix the previous LLM response or making another new LLM request with the previous
    state.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now combine all of the components; here is a step-by-step explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the `ChatOpenAI` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Adding a `stop` sequence forces an LLM to stop generating new tokens after encounting
    the phrase `"tool_result:"`. This helps by stopping hallucinations for tool usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the available tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the base prompt template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]` Generate the model output:    [PRE14]    Extract the last `action`,
    `action_input`, and call the relevant function:    [PRE15]    Print the tool details:    [PRE16]
    `tool_result:` `{``tool_result``}``"""` `)` [PRE17]   [PRE18] [PRE19]`py [PRE20]
    [PRE21] You are looking to accomplish: {goal} You have access to the following
    {tools} [PRE22] # Import necessary classes and functions: from langchain.agents
    import AgentExecutor, create_react_agent from langchain import hub from langchain_openai
    import ChatOpenAI from langchain.tools import Tool  # Defining the LLM to use:
    model = ChatOpenAI()  # Function to count the number of characters in a string:
    def count_characters_in_string(string):     return len(string)  # Create a list
    of tools: # Currently, only one tool is defined that counts characters in a text
    string. tools = [     Tool.from_function(         func=count_characters_in_string,         name="Count
    Characters in a text string",         description="Count the number of characters
    in a text string",     ) ]  # Download a react prompt! prompt = hub.pull("hwchase17/react")  #
    Construct the ReAct agent: agent = create_react_agent(model, tools, prompt)  #
    Initialize an agent with the defined tools and # Create an agent executor by passing
    in the agent and tools: agent_executor = AgentExecutor(agent=agent, tools=tools,
    verbose=True)  # Invoke the agent with a query to count the characters in the
    given word: agent_executor.invoke({"input": ''''''How many characters are in the
    word "supercalifragilisticexpialidocious"?''''''})  # ''There are 34 characters
    in the word "supercalifragilisticexpialidocious".'' [PRE23] Entering new AgentExecutor
    change... I should count the number of characters in the word "supercalifragilisticexpiladocious".
    Action: Count Characters in a text string Action Input: "supercalifragilisticexpiladocious"
    Observation: 34 Thought: I now know the final answer Final Answer: There are 34
    characters in the word "supercalifragilisticexpiladocious". [PRE24]` [PRE25]``
    [PRE26] # Import necessary modules and functions from the langchain package: from
    langchain.chains import (     LLMMathChain, ) from langchain import hub from langchain.agents
    import create_openai_functions_agent, Tool, AgentExecutor from langchain_openai.chat_models
    import ChatOpenAI  # Initialize the ChatOpenAI with temperature set to 0: model
    = ChatOpenAI(temperature=0)  # Create a LLMMathChain instance using the ChatOpenAI
    model: llm_math_chain = LLMMathChain.from_llm(llm=model, verbose=True)  # Download
    the prompt from the hub: prompt = hub.pull("hwchase17/openai-functions-agent")  tools
    = [     Tool(         name="Calculator",         func=llm_math_chain.run, # run
    the LLMMathChain         description="useful for when you need to answer questions
    about math",         return_direct=True,     ), ]  # Create an agent using the
    ChatOpenAI model and the tools: agent = create_openai_functions_agent(llm=model,
    tools=tools, prompt=prompt) agent_executor = AgentExecutor(agent=agent, tools=tools,
    verbose=True)  result = agent_executor.invoke({"input": "What is 5 + 5?"}) print(result)
    # {''input'': ''What is 5 + 5?'', ''output'': ''Answer: 10''} [PRE27] def google_search(query:
    str) -> str:     return "James Phoenix is 31 years old."  # List of tools that
    the agent can use: tools = [     Tool(         # The LLMMathChain tool for math
    calculations.         func=llm_math_chain.run,         name="Calculator",         description="useful
    for when you need to answer questions about math",     ),     Tool(         #
    Tool for counting characters in a string.         func=google_search,         name="google_search",         description="useful
    for when you need to find out about someones age.",     ), ]   # Create an agent
    using the ChatOpenAI model and the tools: agent = create_openai_functions_agent(llm=model,
    tools=tools, prompt=prompt) agent_executor = AgentExecutor(agent=agent, tools=tools,
    verbose=True)  # Asking the agent to run a task and store its result: result =
    agent_executor.invoke(     {         "input": """Task: Google search for James
    Phoenix''s age.  Then square it."""} ) print(result) # {''input'': "...", ''output'':
    ''James Phoenix is 31 years old. # Squaring his age, we get 961.''} [PRE28] #
    Importing the relevant packages: from langchain.agents.agent_types import AgentType
    from langchain_experimental.agents.agent_toolkits import create_csv_agent from
    langchain_openai.chat_models import ChatOpenAI  # Creating a CSV Agent: agent
    = create_csv_agent(     ChatOpenAI(temperature=0),     "data/heart_disease_uci.csv",     verbose=True,     agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    )  agent.invoke("How many rows of data are in the file?") # ''920''  agent.invoke("What
    are the columns within the dataset?") # "''id'', ''age'', ''sex'', ''dataset'',
    ''cp'', ''trestbps'', ''chol'', ''fbs'', # ''restecg'', ''thalch'', ''exang'',
    ''oldpeak'', ''slope'', ''ca'', ''thal'', ''num''"  agent.invoke("Create a correlation
    matrix for the data and save it to a file.") # "The correlation matrix has been
    saved to a file named # ''correlation_matrix.csv''." [PRE29] from langchain.agents
    import create_sql_agent from langchain_community.agent_toolkits import SQLDatabaseToolkit
    from langchain.sql_database import SQLDatabase from langchain.agents.agent_types
    import AgentType from langchain_openai.chat_models import ChatOpenAI  db = SQLDatabase.from_uri("sqlite:///./data/demo.db")
    toolkit = SQLDatabaseToolkit(db=db, llm=ChatOpenAI(temperature=0))  # Creating
    an agent executor: agent_executor = create_sql_agent(     llm=ChatOpenAI(temperature=0),     toolkit=toolkit,     verbose=True,     agent_type=AgentType.OPENAI_FUNCTIONS,
    )  # Identifying all of the tables: agent_executor.invoke("Identify all of the
    tables") # ''The database contains the following tables:\n1\. Orders\n2\. Products\n3\.
    Users'' [PRE30] user_sql = agent_executor.invoke(     ''''''Add 5 new users to
    the database. Their names are:  John, Mary, Peter, Paul, and Jane.'''''' ) ''''''Based
    on the schema of the "Users" table, I can see that the relevant columns for adding
    new users are "FirstName", "LastName", "Email", and "DateJoined". I will now run
    the SQL query to add the new users.\n\n[PRE31]\n\nPlease note that I have added
    the new users with the specified names and email addresses. The "DateJoined" column
    is set to the respective dates mentioned.'''''' [PRE32] # This the function signature
    for demonstration purposes and is not executable. def create_sql_agent(     llm:
    BaseLanguageModel,     toolkit: SQLDatabaseToolkit,     agent_type: Any | None
    = None,     callback_manager: BaseCallbackManager | None = None,     prefix: str
    = SQL_PREFIX,     suffix: str | None = None,     format_instructions: str | None
    = None,     input_variables: List[str] | None = None,     top_k: int = 10,     max_iterations:
    int | None = 15,     max_execution_time: float | None = None,     early_stopping_method:
    str = "force",     verbose: bool = False,     agent_executor_kwargs: Dict[str,
    Any] | None = None,     extra_tools: Sequence[BaseTool] = (),     **kwargs: Any
    ) -> AgentExecutor [PRE33] SQL_PREFIX = """You are an agent designed to interact
    with a SQL database. Given an input question, create a syntactically correct {dialect}
    query to run, then look at the results of the query and return the answer. Unless
    the user specifies a specific number of examples they wish to obtain always limit
    your query to at most {top_k} results. You can order the results by a relevant
    column to return the most interesting examples in the database. Never query for
    all the columns from a specific table, only ask for the relevant columns given
    the question. You have access to tools for interacting with the database. Only
    use the below tools. Only use the information returned by the below tools to construct
    your final answer. You MUST double-check your query before executing it. If you
    get an error while executing a query, rewrite the query and try again. If the
    question does not seem related to the database, just return "I don''t know" as
    the answer. """  agent_executor = create_sql_agent(     llm=ChatOpenAI(temperature=0),     toolkit=toolkit,     verbose=True,     agent_type=AgentType.OPENAI_FUNCTIONS,     prefix=SQL_PREFIX,
    )  agent_executor.invoke(user_sql) # ''...sql\nINSERT INTO Users (FirstName, LastName,
    Email, # DateJoined)\nVALUES (...)...''  # Testing that Peter was inserted into
    the database: agent_executor.invoke("Do we have a Peter in the database?") ''''''Yes,
    we have a Peter in the database. Their details are as follows:\n- First Name:
    Peter...'''''' [PRE34] from langchain_openai import ChatOpenAI from langchain_core.tools
    import tool  # 1\. Create the model: llm = ChatOpenAI(temperature=0)  @tool def
    get_word_length(word: str) -> int:     """Returns the length of a word."""     return
    len(word)  # 2\. Create the tools: tools = [get_word_length] [PRE35] from langchain_core.prompts
    import ChatPromptTemplate, MessagesPlaceholder  # 3\. Create the Prompt: prompt
    = ChatPromptTemplate.from_messages(     [         (             "system",             """You
    are very powerful assistant, but don''t know current events  and aren''t good
    at calculating word length.""",         ),         ("user", "{input}"),         #
    This is where the agent will write/read its messages from         MessagesPlaceholder(variable_name="agent_scratchpad"),     ]
    ) [PRE36] from langchain_core.utils.function_calling import convert_to_openai_tool
    from langchain.agents.format_scratchpad.openai_tools import (     format_to_openai_tool_messages,
    )  # 4\. Formats the python function tools into JSON schema and binds # them to
    the model: llm_with_tools = llm.bind_tools(tools=[convert_to_openai_tool(t) for
    t in tools])  from langchain.agents.output_parsers.openai_tools \ import OpenAIToolsAgentOutputParser   #
    5\. Setting up the agent chain: agent = (     {         "input": lambda x: x["input"],         "agent_scratchpad":
    lambda x: format_to_openai_tool_messages(             x["intermediate_steps"]         ),     }     |
    prompt     | llm_with_tools     | OpenAIToolsAgentOutputParser() ) [PRE37] from
    langchain.agents import AgentExecutor  agent_executor = AgentExecutor(agent=agent,
    tools=tools, verbose=True) agent_executor.invoke({"input": "How many letters in
    the word Software?"}) #{''input'': ''How many letters in the word Software?'',
    # ''output'': ''There are 8 letters in the word "Software".''} [PRE38] # Provide
    the connection string to connect to the MongoDB database. connection_string =
    "mongodb://mongo_user:password123@mongo:27017"  chat_message_history = MongoDBChatMessageHistory(     session_id="test_session",     connection_string=connection_string,     database_name="my_db",     collection_name="chat_histories",
    )  chat_message_history.add_user_message("I love programming!!") chat_message_history.add_ai_message("What
    do you like about it?")  chat_message_history.messages # [HumanMessage(content=''I
    love programming!!'', # AIMessage(content=''What do you like about it?'') [PRE39]
    from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory()
    memory.save_context({"input": "hi"}, {"output": "whats up"}) memory.load_memory_variables({})
    # {''history'': ''Human: hi\nAI: whats up''} [PRE40] memory = ConversationBufferMemory(return_messages=True)
    memory.save_context({"input": "hi"}, {"output": "whats up"}) memory.load_memory_variables({})
    # {''history'': [HumanMessage(content=''hi''), # AIMessage(content=''whats up'')]}
    [PRE41] # Using within a chain: from langchain.memory import ConversationBufferMemory
    from langchain_openai.chat_models import ChatOpenAI from langchain_core.prompts
    import ChatPromptTemplate, MessagesPlaceholder from langchain_core.output_parsers
    import StrOutputParser from langchain_core.runnables import RunnableLambda from
    operator import itemgetter  memory = ConversationBufferMemory(return_messages=True)  model
    = ChatOpenAI(temperature=0) prompt = ChatPromptTemplate.from_messages(     [         ("system",
    "Act as a chatbot that helps users with their queries."),         # The history
    of the conversation         MessagesPlaceholder(variable_name="history"),         ("human",
    "{input}"),     ] ) chain = (     {         "input": lambda x: x["input"],         "history":
    RunnableLambda(memory.load_memory_variables) | \         itemgetter("history"),     }     |
    prompt     | model     | StrOutputParser() ) [PRE42] inputs = {"input": "Hi my
    name is James!"} result = chain.invoke(inputs) memory.save_context(inputs, {"outputs":
    result}) print(memory.load_memory_variables({}))  # {''history'': [HumanMessage(content=''Hi
    my name is James!''), # AIMessage(content=''Hello James! How can I assist you
    today?'')]} [PRE43] inputs = {"input": "What is my name?"} second_result = chain.invoke(inputs)
    print(second_result) # Your name is James. [PRE44] prompt = ChatPromptTemplate.from_messages(     [         (             "system",             """You
    are a very powerful assistant, but don''t know current events and aren''t good
    at calculating word length.""",         ),         # This is where the agent will
    write/read its messages from         MessagesPlaceholder(variable_name="agent_scratchpad"),         MessagesPlaceholder(variable_name="history"),         ("user",
    "{input}"),     ] )  # ... The rest of the code remains the same as before ...  #
    Create an agent executor by passing in the agent, tools, and memory: memory =
    ConversationBufferMemory(return_messages=True) agent_executor = AgentExecutor(agent=agent,
    tools=tools, verbose=True, memory=memory) [PRE45] from langchain.memory import
    ConversationBufferMemory  memory = ConversationBufferMemory() memory.save_context({"input":
    "hi"}, {"output": "whats up"}) memory.load_memory_variables({}) # {''history'':
    ''Human: hi\nAI: whats up''} [PRE46] from langchain.memory import ConversationBufferWindowMemory  memory
    = ConversationBufferWindowMemory(k=1) memory.save_context({"input": "hi"}, {"output":
    "whats up"}) memory.save_context({"input": "not much you"}, {"output": "not much"})
    # Returns: {''history'': ''Human: not much you\nAI: not much''} memory.load_memory_variables({})
    [PRE47] from langchain.memory import ConversationSummaryMemory, ChatMessageHistory
    from langchain_openai import OpenAI  memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))
    memory.save_context({"input": "hi"}, {"output": "whats up"}) memory.load_memory_variables({})
    # Returns: {''history'': ''\nThe human greets the AI, to which the AI responds.''}
    [PRE48] from langchain.memory import ConversationSummaryBufferMemory from langchain_openai.chat_models
    import ChatOpenAI  memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(),
    max_token_limit=10) memory.save_context({"input": "hi"}, {"output": "whats up"})
    memory.load_memory_variables({}) # Returns: {''history'': ''System: \nThe human
    says "hi", and the AI responds with # "whats up".\nHuman: not much you\nAI: not
    much''} [PRE49] from langchain.memory import ConversationTokenBufferMemory from
    langchain_openai.chat_models import ChatOpenAI  memory = ConversationTokenBufferMemory(llm=ChatOpenAI(),
    max_token_limit=50) memory.save_context({"input": "hi"}, {"output": "whats up"})
    memory.load_memory_variables({}) # Returns: {''history'': ''Human: not much you\nAI:
    not much''} [PRE50] from langchain.tools import StructuredTool  def save_interview(raw_interview_text:
    str):     """Tool to save the interview. You must pass the entire interview and  conversation
    in here. The interview will then be saved to a local file.  Remember to include
    all of the previous chat messages. Include all of  the messages with the user
    and the AI, here is a good response:  AI: some text  Human: some text  ...  ---  """     #
    Save to local file:     with open("interview.txt", "w") as f:         f.write(raw_interview_text)     return
    f''''''Interview saved! Content: {raw_interview_text}. File:  interview.txt. You
    must tell the user that the interview is saved.''''''  save_interview = StructuredTool.from_function(save_interview)
    [PRE51] from pydantic.v1 import BaseModel from typing import Union, Literal, Type
    from langchain_core.tools import BaseTool  class ArgumentType(BaseModel):     url:
    str     file_type: Union[Literal["pdf"], Literal["txt"]]  class SummarizeFileFromURL(BaseTool):     name
    = "SummarizeFileFromURL"     description = "Summarize a file from a URL."     args_schema:
    Type[ArgumentType] = ArgumentType [PRE52] AgentExecutor(.., verbose=True) [PRE53]
    class BaseCallbackHandler:     """Base callback handler that can be used to handle
    callbacks from  langchain."""      def on_llm_start(         self, serialized:
    Dict[str, Any], prompts: List[str],         **kwargs: Any     ) -> Any:         """Run
    when LLM starts running."""      def on_chat_model_start(         self, serialized:
    Dict[str, Any],         messages: List[List[BaseMessage]], **kwargs: Any     )
    -> Any:         """Run when Chat Model starts running."""      def on_llm_new_token(self,
    token: str, **kwargs: Any) -> Any:         """Run on new LLM token. Only available
    when streaming is enabled."""      def on_llm_end(self, response: LLMResult, **kwargs:
    Any) -> Any:         """Run when LLM ends running."""      def on_llm_error(         self,
    error: Union[Exception, KeyboardInterrupt], **kwargs: Any     ) -> Any:         """Run
    when LLM errors."""      def on_chain_start(         self, serialized: Dict[str,
    Any], inputs: Dict[str, Any],         **kwargs: Any     ) -> Any:         """Run
    when chain starts running."""      def on_chain_end(self, outputs: Dict[str, Any],
    **kwargs: Any) -> Any:         """Run when chain ends running."""      def on_chain_error(         self,
    error: Union[Exception, KeyboardInterrupt], **kwargs: Any     ) -> Any:         """Run
    when chain errors."""      def on_tool_start(         self, serialized: Dict[str,
    Any], input_str: str, **kwargs: Any     ) -> Any:         """Run when tool starts
    running."""      def on_tool_end(self, output: str, **kwargs: Any) -> Any:         """Run
    when tool ends running."""      def on_tool_error(         self, error: Union[Exception,
    KeyboardInterrupt], **kwargs: Any     ) -> Any:         """Run when tool errors."""      def
    on_text(self, text: str, **kwargs: Any) -> Any:         """Run on arbitrary text."""      def
    on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:         """Run
    on agent action."""      def on_agent_finish(self, finish: AgentFinish, **kwargs:
    Any) -> Any:         """Run on agent end.""" [PRE54] from langchain.agents import
    AgentExecutor from langchain.callbacks import StdOutCallbackHandler  agent_executor
    = AgentExecutor(     agent=agent,     tools=tools,     verbose=True,     callbacks=[StdOutCallbackHandler()],     tags=[''a-tag''])  agent_executor.invoke({"input":
    "How many letters in the word Software?"}) [PRE55] from langchain.callbacks import
    StdOutCallbackHandler from langchain.chains import LLMChain from langchain_openai
    import OpenAI from langchain_core.prompts import PromptTemplate  handler = StdOutCallbackHandler()
    llm = OpenAI() prompt = PromptTemplate.from_template("What is 1 + {number} = ")
    chain = LLMChain(llm=llm, prompt=prompt) chain.invoke({"number": 2}, {"callbacks":
    [handler]}) [PRE56] import asyncio from langchain.callbacks import get_openai_callback
    from langchain_core.messages import SystemMessage from langchain_openai.chat_models
    import ChatOpenAI model = ChatOpenAI() [PRE57] with get_openai_callback() as cb:     model.invoke([SystemMessage(content="My
    name is James")]) total_tokens = cb.total_tokens print(total_tokens) # 25 assert
    total_tokens > 0 [PRE58] with get_openai_callback() as cb:     model.invoke([SystemMessage(content="My
    name is James")])     model.invoke([SystemMessage(content="My name is James")])
    assert cb.total_tokens > 0 print(cb.total_tokens) # 50 [PRE59] # Async callbacks:
    with get_openai_callback() as cb:     await asyncio.gather(         model.agenerate(             [                 [SystemMessage(content="Is
    the meaning of life 42?")],                 [SystemMessage(content="Is the meaning
    of life 42?")],             ],         )     ) print(cb.__dict__) # {''successful_requests'':
    2, ''total_cost'': 0.000455, # ''total_tokens'': 235, ''prompt_tokens'': 30, #
    ''completion_tokens'': 205} [PRE60]` `````'
  prefs: []
  type: TYPE_NORMAL
