- en: Chapter 6\. Autonomous Agents with Memory and Tools
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章：具有记忆和工具的自主代理
- en: This chapter dives deeper into the importance of chain-of-thought reasoning
    and the ability of large language models (LLMs) to reason through complex problems
    as agents. By breaking down complex problems into smaller, more manageable components,
    LLMs can provide more thorough and effective solutions. You will also learn about
    the components that make up autonomous agents, such as inputs, goal or reward
    functions, and available actions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了思维链推理的重要性以及大型语言模型（LLMs）作为代理通过复杂问题进行推理的能力。通过将复杂问题分解成更小、更易于管理的组件，LLMs可以提供更全面和有效的解决方案。你还将了解构成自主代理的组件，例如输入、目标或奖励函数以及可用的动作。
- en: Chain-of-Thought
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维链
- en: The ability of AI to reason through complex problems is essential for creating
    effective, reliable, and user-friendly applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: AI通过复杂问题进行推理的能力对于创建有效、可靠和用户友好的应用程序至关重要。
- en: '*Chain-of-thought reasoning* (CoT) is a method of guiding LLMs through a series
    of steps or logical connections to reach a conclusion or solve a problem. This
    approach is particularly useful for tasks that require a deeper understanding
    of context or multiple factors to consider.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*思维链推理*（CoT）是一种引导LLM通过一系列步骤或逻辑连接来得出结论或解决问题的方法。这种方法对于需要更深入理解上下文或考虑多个因素的任务特别有用。'
- en: '[CoT](https://oreil.ly/fAeLo) is asking an LLM to *think* through complex problems,
    breaking them down into smaller, more manageable components. This allows the LLM
    to focus on each part individually, ensuring a more thorough understanding of
    the issue at hand.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[思维链](https://oreil.ly/fAeLo)要求LLM通过复杂问题进行思考，将它们分解成更小、更易于管理的组件。这允许LLM单独关注每个部分，确保对当前问题的更深入理解。'
- en: 'In practice, chain-of-thought reasoning might involve:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，思维链推理可能涉及：
- en: Asking an LLM to provide explanations for its decisions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求LLM为其决策提供解释
- en: Planning multiple steps before deciding on a final answer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在做出最终决定之前规划多个步骤
- en: In the following sections, you’ll explore examples of both ineffective and effective
    chain-of-thought reasoning. We will also discuss various techniques for building
    effective chain-of-thought reasoning and how they can be integrated into AI applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将探索无效和有效思维链推理的例子。我们还将讨论构建有效思维链推理的各种技术以及它们如何集成到AI应用中。
- en: Let’s imagine that a user wants the AI to generate a comprehensive marketing
    plan for promoting a new software product.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，一个用户希望AI生成一个全面的市场推广计划来推广一款新的软件产品。
- en: 'Input:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Output:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this example, GPT-4 doesn’t use chain-of-thought reasoning, and it does not
    address the specific aspects of the marketing plan. The LLM generates a generic
    list of marketing strategies that could apply to any product, rather than focusing
    on the unique characteristics of the new software product.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，GPT-4没有使用思维链推理，也没有针对市场推广计划的特定方面。LLM生成了一份通用的营销策略列表，这些策略可以适用于任何产品，而不是专注于新软件产品的独特特性。
- en: 'Input:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Output:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now GPT-4 specifically addresses the unique characteristics of the new software
    product, demonstrating effective chain-of-thought reasoning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，GPT-4专门针对新软件产品的独特特性，展示了有效的思维链推理。
- en: Give Direction
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 给予指导
- en: Take note of the phrase *step-by-step*, a critical element in CoT. By incorporating
    this phrase into your prompt, you’re asking the LLM to reason through the steps
    that are required to generate a highly effective software product.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到“逐步”这个短语，这是思维链中的一个关键要素。通过将这个短语纳入你的提示，你是在要求LLM通过生成高度有效的软件产品所需的步骤进行推理。
- en: Also, by providing a $20,000 budget and the type of software, GPT-4 is able
    to provide a much more relevant and contextualized response.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过提供20,000美元的预算和软件类型，GPT-4能够提供更加相关和具体化的响应。
- en: Agents
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理
- en: Generative AI models have given rise to an *agent-based architecture*. Conceptually,
    an agent acts, perceives, and makes decisions within a specified environment to
    achieve predefined objectives.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI模型催生了基于代理的架构。从概念上讲，代理在指定环境中行动、感知并做出决策以实现预定义的目标。
- en: Agents can take various actions such as executing a Python function; afterward,
    the agent will observe what happens and will decide on whether it is finished
    or what action to take next.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以采取各种行动，例如执行Python函数；之后，代理将观察发生了什么，并决定是否完成或采取下一步行动。
- en: 'The agent will continously loop through a series of actions and observations
    until there are no further actions, as you can see in the following pseudocode:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 代理将不断循环一系列动作和观察，直到没有更多的动作，如下面的伪代码所示：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The behavior of the agent is governed by three principal components:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的行为由三个主要组成部分控制：
- en: Inputs
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: These are the sensory stimuli or data points the agent receives from its environment.
    Inputs can be diverse, ranging from visual (like images) and auditory (like audio
    files) to thermal signals and beyond.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是代理从其环境中接收的感官刺激或数据点。输入可以多种多样，从视觉（如图像）和听觉（如音频文件）到热信号等。
- en: Goal or reward function
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目标或奖励函数
- en: This represents the guiding principle for an agent’s actions. In goal-based
    frameworks, the agent is tasked with reaching a specific end state. In a reward-based
    setting, the agent is driven to maximize cumulative rewards over time, often in
    dynamic environments.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了一个代理行为的指导原则。在基于目标的框架中，代理被要求达到一个特定的最终状态。在基于奖励的设置中，代理被驱动去最大化随时间累积的奖励，通常是在动态环境中。
- en: Available actions
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的动作
- en: The *action space* is the range of permissible actions an agent [can undertake
    at any given moment](https://oreil.ly/5AVfM). The breadth and nature of this space
    are contingent upon the task at hand.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*动作空间*是代理在任何给定时刻可以采取的允许动作的范围。[可以采取的动作](https://oreil.ly/5AVfM)的广度和性质取决于手头的任务。'
- en: 'To explain these concepts further, consider a self-driving car:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步解释这些概念，考虑一下自动驾驶汽车：
- en: Inputs
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: The car’s sensors, such as cameras, LIDAR, and ultrasonic sensors, provide a
    continuous stream of data about the environment. This can include information
    about nearby vehicles, pedestrians, road conditions, and traffic signals.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车的传感器，如摄像头、激光雷达和超声波传感器，提供有关环境的连续数据流。这可能包括关于附近车辆、行人、道路状况和交通信号的信息。
- en: Goal or reward function
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 目标或奖励函数
- en: The primary goal for a self-driving car is safe and efficient navigation from
    point A to point B. If we were to use a reward-based system, the car might receive
    positive rewards for maintaining a safe distance from other objects, adhering
    to speed limits, and following traffic rules. Conversely, it could receive negative
    rewards for risky behaviors, like hard braking or veering off the lane. Tesla
    specifically uses miles driven without an intervention as their reward function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车的主要目标是安全且高效地从A点到B点的导航。如果我们使用基于奖励的系统，汽车可能会因为保持与其他物体的安全距离、遵守速度限制和遵守交通规则而获得积极奖励。相反，它可能会因为危险行为（如紧急制动或偏离车道）而收到负面奖励。特斯拉特别使用无需干预行驶的英里数作为他们的奖励函数。
- en: Available actions
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的动作
- en: The car’s action space includes accelerating, decelerating, turning, changing
    lanes, and more. Each action is chosen based on the current input data and the
    objective defined by the goal or reward function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车的动作空间包括加速、减速、转弯、换车道等。每个动作都是根据当前输入数据和由目标或奖励函数定义的目标来选择的。
- en: You’ll find that agents in systems like self-driving cars rely on foundational
    principles like inputs, goal/reward functions, and available actions. However,
    when delving into the realm of LLMs like GPT, there’s a bespoke set of dynamics
    that cater specifically to their unique nature.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，在自动驾驶汽车等系统中，代理依赖于基础原则，如输入、目标/奖励函数和可用的动作。然而，当深入研究GPT等LLM领域时，存在一套专门的动力机制，专门针对它们的独特性质。
- en: 'Here’s how they align with your needs:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它们如何与您的需求相匹配：
- en: Inputs
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: 'For LLMs, the gateway is primarily through text. But that doesn’t restrain
    the wealth of information you can use. Whether you’re dealing with thermal readings,
    musical notations, or intricate data structures, your challenge lies in molding
    these into textual representations suitable for an LLM. Think about videos: while
    raw footage might seem incompatible, video text transcriptions allow an LLM to
    extract insights for you.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLM来说，主要入口是通过文本。但这并不限制你可以使用的信息量。无论你是在处理温度读数、音乐符号，还是复杂的数据结构，你的挑战在于将这些内容塑造成适合LLM的文本表示。想想视频：虽然原始素材可能看起来不兼容，但视频文本转录允许LLM为你提取见解。
- en: Harnessing goal-driven directives
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 利用目标驱动的指令
- en: 'LLMs primarily use goals defined within your text prompts. By creating effective
    prompts with objectives, you’re not just accessing the LLM’s vast knowledge; you’re
    effectively charting its reasoning path. Think of it as laying down a blueprint:
    your specific prompt instructs the model, guiding it to dissect your overarching
    objective into a systematic sequence of steps.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs主要使用你文本提示中定义的目标。通过创建具有目标的有效提示，你不仅访问了LLM的广泛知识，而且实际上绘制了它的推理路径。把它想象成绘制蓝图：你的特定提示指导模型，引导它将你的总体目标分解成一系列系统的步骤。
- en: Crafting action through functional tools
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过功能性工具制定行动
- en: LLMs are not limited to mere text generation; there’s so much more you can achieve.
    By integrating *ready-made tools* or *custom-developed tools*, you can equip LLMs
    to undertake diverse tasks, from API calls to database engagements or even orchestrating
    external systems. Tools can be written in any programming language, and by adding
    more tools you are effectively *expanding the action space* of what an LLM can
    achieve.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs不仅限于简单的文本生成；还有更多你可以实现的事情。通过集成*现成的工具*或*自定义开发的工具*，你可以装备LLMs执行各种任务，从API调用到数据库交互，甚至编排外部系统。工具可以用任何编程语言编写，通过添加更多工具，你实际上是在*扩展LLM可以实现的行为空间*。
- en: 'There are also different components that are directly applicable to LLMs:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些组件可以直接应用于LLMs：
- en: Memory
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆
- en: It’s ideal to store state between agent steps; this is particularly useful for
    chatbots, where remembering the previous chat history provides a better user experience.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理步骤之间存储状态是理想的；这在聊天机器人中尤其有用，记住之前的聊天历史可以提供更好的用户体验。
- en: Agent planning/execution strategies
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 代理规划/执行策略
- en: There are multiple ways to achieve a high-level goal, of which a mixture of
    planning and executing is essential.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实现高级目标有多种方式，其中规划和执行的结合是至关重要的。
- en: Retrieval
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 检索
- en: LLMs can use different types of retrieval methods. Semantic similarity within
    vector databases is the most common, but there are others such as including custom
    information from a SQL database into prompts.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs可以使用不同类型的检索方法。在向量数据库中的语义相似性是最常见的，但还有其他方法，例如将来自SQL数据库的自定义信息包含到提示中。
- en: Let’s dive deeper into the shared and different components and explore the implementation
    details.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨共享和不同的组件，并探索实现细节。
- en: Reason and Act (ReAct)
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和行动（ReAct）
- en: There are many agent frameworks that ultimately aim to improve LLM responses
    toward a goal. The original framework was *ReAct*, which is an improved version
    of CoT, allowing an LLM to create observations after taking actions via tools.
    These observations are then turned into *thoughts* about what would be the *right
    tool* to use within the next step ([Figure 6-1](#fig-6-1)). The LLM continues
    to reason until either a `'Final Answer'` string value is present or a maximum
    number of iterations has taken place.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多代理框架最终旨在提高LLMs向目标响应的能力。原始框架是*ReAct*，它是CoT的改进版本，允许LLM在通过工具采取行动后创建观察。然后，这些观察被转化为关于下一步应该使用什么*正确工具*的*思考*（[图6-1](#fig-6-1)）。LLM会继续推理，直到出现一个`'Final
    Answer'`字符串值或者达到最大迭代次数。
- en: '![The ReAct Framework](assets/pega_0601.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![ReAct框架](assets/pega_0601.png)'
- en: Figure 6-1\. The ReAct framework
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. ReAct框架
- en: 'The [ReAct](https://oreil.ly/ssdnL) framework uses a mixture of task decomposition,
    a thought loop, and multiple tools to solve questions. Let’s explore the thought
    loop within ReAct:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[ReAct](https://oreil.ly/ssdnL)框架结合了任务分解、思考循环和多个工具来解决疑问。让我们探索ReAct中的思考循环：'
- en: Observe the environment.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察环境。
- en: Interpret the environment with a thought.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用思考来解释环境。
- en: Decide on an action.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定一个行动。
- en: Act on the environment.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在环境中采取行动。
- en: Repeat steps 1–4 until you find a solution or you’ve done too many iterations
    (the solution is “I’ve found the answer”).
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1-4，直到找到解决方案或迭代次数过多（解决方案是“我已经找到了答案”）。
- en: 'You can easily create a ReAct-style prompt by using the preceding thought loop
    while also providing the LLM with several inputs such as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用前面的思考循环，同时向LLM提供多个输入，如：
- en: '`{question}`: The query that you want answered.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{question}`: 你想要回答的查询。'
- en: '`{tools}`: These refer to functions that can be used to accomplish a step within
    the overall task. It is common practice to include a list of tools where each
    tool is a Python function, a name, and a description of the function and its purpose.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{tools}`：这些指的是可以用于完成整体任务中某个步骤的函数。通常的做法是包括一个工具列表，其中每个工具都是一个 Python 函数、一个名称以及该函数及其目的的描述。'
- en: 'The following is a prompt that implements the ReAct pattern with prompt variables
    wrapped in `{}` characters such as `{question}`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个实现 ReAct 模式且提示变量用 `{}` 字符括起来的提示：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is a breakdown of the prompt:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是提示的分解：
- en: 'The introduction of the prompt clearly establishes the LLM’s purpose: `You
    will attempt to solve the problem of finding the answer to a question.`'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示的引入清楚地确立了 LLM 的目的：`您将尝试解决找到问题答案的问题。`
- en: 'The problem-solving approach is then outlined: `Use chain-of-thought reasoning
    to solve through the problem, using the following pattern:`'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后概述了解决问题的方法：`使用链式思维推理通过以下模式解决问题：`
- en: 'The steps in the chain-of-thought reasoning are then laid out:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 链式思维推理的步骤随后被列出：
- en: 'The LLM starts by observing the original question and subsequently formulates
    an observation about it: `original_question: original_problem_text`, `observation:
    observation_text`.'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LLM 首先观察原始问题，然后对其形成观察：`original_question: original_problem_text`，`observation:
    observation_text`。'
- en: 'Based on this observation, the AI should formulate a thought that signifies
    a step in the reasoning process: `thought: thought_text`.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '基于此观察，AI 应该制定一个表示推理过程中一步的思想：`thought: thought_text`。'
- en: 'Having established a thought, it then decides on an action using one of the
    available tools: `action: tool_name`, `action_input: tool_input`.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在建立了一个思想之后，它接着决定使用可用的工具之一采取行动：`action: tool_name`，`action_input: tool_input`。'
- en: The LLM is then reminded not to make assumptions about what a tool might return,
    and it should explicitly outline its intended action and the corresponding input.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后提醒 LLM 不要对工具可能返回的内容做出假设，并且它应该明确说明其预期的行动和相应的输入。
- en: '`You have access to the following tools: {tools}` communicates to the LLM what
    tools it has available for solving the problem.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`您可以使用以下工具：{tools}` 通知 LLM 它可用于解决问题的工具。'
- en: 'The actual problem that the LLM must solve is then introduced: `original_​problem:
    {question}`.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '然后介绍了 LLM 必须解决的真正问题：`original_问题: {question}`。'
- en: Finally, instructions are provided on how the LLM should respond based on the
    results of its actions. It can either continue with new observations, actions,
    and inputs or, if a solution is found, provide the final answer.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，根据其行动的结果，提供了 LLM 应如何响应的说明。它可以选择继续进行新的观察、行动和输入，或者如果找到了解决方案，提供最终答案。
- en: The prompt outlines a systematic problem-solving process in which the LLM observes
    a problem, thinks about it, decides on an action, and repeats this process until
    a solution is discovered.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 提示概述了一个系统化的问题解决过程，其中 LLM 观察一个问题，思考它，决定采取的行动，并重复此过程，直到找到解决方案。
- en: Reason and Act Implementation
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理由与行动实施
- en: Now that you’re aware of ReAct, it’s important to create a simple Python implementation
    that replicates what LangChain does automatically, allowing you to build the intuition
    about what’s truly happening between the LLM responses.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经了解了 ReAct，创建一个简单的 Python 实现来复制 LangChain 自动执行的操作非常重要，这样您就可以构建关于 LLM 响应之间真正发生的事情的直觉。
- en: To keep it simple, this example will not implement looping and will assume that
    the output can be obtained from a single tool call.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，此示例将不会实现循环，并假设输出可以从单个工具调用中获得。
- en: 'To create a basic ReAct implementation, you’ll implement the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个基本的 ReAct 实现，您将实现以下内容：
- en: At every thought, you need to extract the tool that the LLM wants to use. Therefore,
    you’ll extract the last `action` and `action_input`. The `action` represents the
    tool name, while the `action_input` consists of the values of the function arguments.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个思考过程中，您需要提取 LLM 想要使用的工具。因此，您将提取最后一个 `action` 和 `action_input`。`action` 代表工具名称，而
    `action_input` 包含函数参数的值。
- en: Check whether the LLM thinks that it has found the final answer, in which case
    the thought loop has ended.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 LLM 是否认为它已经找到了最终答案，如果是这样，则思想循环结束。
- en: 'You can use regular expressions to extract the `action` and `action_input`
    values from the LLM response:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用正则表达式从 LLM 响应中提取 `action` 和 `action_input` 值：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s break down the regular expression to extract the `action`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个正则表达式以提取 `action`：
- en: '`action_pattern = re.compile(r"(?i)action\s*:\s*([^\n]+)", re.MULTILINE)`'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`action_pattern = re.compile(r"(?i)action\s*:\s*([^\n]+)", re.MULTILINE)`'
- en: '`(?i)`: This is called an *inline flag* and makes the regex pattern case-insensitive.
    It means that the pattern will match “action,” “Action,” “ACTION,” or any other
    combination of uppercase and lowercase letters.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(?i)`: 这被称为 *内联标志*，使得正则表达式模式不区分大小写。这意味着模式将匹配“action”、“Action”、“ACTION”或任何其他大小写组合。'
- en: '`action`: This part of the pattern matches the word *action* literally. Due
    to the case-insensitive flag, it will match any capitalization of the word.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`action`: 模式的一部分匹配单词 *action* 字面意义。由于不区分大小写的标志，它将匹配该单词的任何大小写形式。'
- en: '`\s*`: This part of the pattern matches zero or more whitespace characters
    (spaces, tabs, etc.). The `\*` means *zero or more*, and `\s` is the regex shorthand
    for a whitespace character.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`\s*`: 模式的一部分匹配零个或多个空白字符（空格、制表符等）。`\*` 表示 *零个或多个*，`\s` 是正则表达式的空白字符简写。'
- en: '`:` This part of the pattern matches the colon character literally.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`:` 模式的一部分匹配字面意义上的冒号。'
- en: '`\s*`: This is the same as the previous `\s\*` part, matching zero or more
    whitespace characters after the colon.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`\s*`: 这与之前的 `\s\*` 部分相同，匹配冒号后面的零个或多个空白字符。'
- en: '`+([^\n]++)`: This pattern is a capturing group, denoted by the parentheses.
    It matches one or more characters that are *not a newline character*. The `^`
    inside the square brackets `[]` negates the character class, and `\n` represents
    the newline character. The `+` means *one or more*. The text matched by this group
    will be extracted when using the `findall()` function.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`+([^\n]++)`: 这个模式是一个捕获组，由括号表示。它匹配一个或多个不是换行符的字符。方括号 `[]` 内的 `^` 否定字符类，`\n`
    表示换行符。`+` 表示 *一个或多个*。该组匹配的文本将在使用 `findall()` 函数时被提取。'
- en: '`re.MULTILINE`: This is a flag passed to `re.compile()` function. It tells
    the regex engine that the input text may have multiple lines, so the pattern should
    be applied line by line.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`re.MULTILINE`: 这是传递给 `re.compile()` 函数的标志。它告诉正则表达式引擎输入文本可能有多个行，因此模式应该逐行应用。'
- en: In regular expressions, square brackets `[]` are used to define a character
    class, which is a set of characters that you want to match. For example, `[abc]`
    would match any single character that is either `'a'`, `'b'`, or `'c'`.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在正则表达式中，方括号 `[]` 用于定义字符类，这是一组你想要匹配的字符。例如，`[abc]` 将匹配任何单个字符，该字符是 `'a'`、`'b'`
    或 `'c'`。
- en: When you add a caret `^` at the beginning of the character class, it negates
    the character class, meaning it will match any character that is *not in the character
    class*. In other words, it inverts the set of characters you want to match.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你在字符类开始处添加一个撇号 `^` 时，它否定字符类，这意味着它将匹配不在字符类中的任何字符。换句话说，它反转了你想要匹配的字符集。
- en: So, when we use `[^abc]`, it will match any single character that is *not* `'a'`,
    `'b'`, or `'c'`. In the regex pattern `+([^\n]++)`, the character class is `[^n]`,
    which means it will match any character that is *not* a newline character (`\n`).
    The `+` after the negated character class means that the pattern should match
    one or more characters that are not newlines.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，当我们使用 `[^abc]` 时，它将匹配任何不是 `'a'`、`'b'` 或 `'c'` 的单个字符。在正则表达式模式 `+([^\n]++)`
    中，字符类是 `[^n]`，这意味着它将匹配任何不是换行符 (`\n`) 的字符。字符类否定后的 `+` 表示模式应该匹配一个或多个不是换行符的字符。
- en: By using the negated character class `[^n]` in the capturing group, we ensure
    that the regex engine captures text up to the end of the line without including
    the newline character itself. This is useful when we want to extract the text
    after the word *action* or *action input* up to the end of the line.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在捕获组中使用否定字符类 `[^n]`，我们确保正则表达式引擎捕获直到行尾的文本，而不包括换行符本身。这在我们要提取单词 *action* 或 *action
    input* 后直到行尾的文本时很有用。
- en: Overall, this regular expression pattern matches the word *action* (case-insensitive)
    followed by optional whitespace, a colon, and optional whitespace again, and then
    captures any text up to the end of the line.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这个正则表达式模式匹配单词 *action*（不区分大小写）后面跟可选的空白，一个冒号，再次跟可选的空白，然后捕获任何直到行尾的文本。
- en: 'The only difference between these two regex patterns is the literal text they
    are looking for at the beginning:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个正则表达式模式之间的唯一区别是它们在开始处寻找的文本：
- en: '`action_pattern` looks for the word `"action".`'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`action_pattern` 寻找单词 `"action"`。'
- en: '`action_input_pattern` looks for the word `"action_input".`'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`action_input_pattern` 查找单词 `"action_input"`.'
- en: 'You can now abstract the regex into a Python function that will always find
    the last `action` and `action_input`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以将正则表达式抽象成一个 Python 函数，该函数将始终找到最后一个 `action` 和 `action_input`：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To determine and extract whether the LLM has discovered the final answer, you
    can also use regular expressions:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定和提取语言模型是否发现了最终答案，您也可以使用正则表达式：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Warning
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: LLMs do not always respond in the intended way, so your application needs to
    be able to handle regex parsing errors. Several approaches include using an LLM
    to fix the previous LLM response or making another new LLM request with the previous
    state.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型并不总是以预期的方式响应，因此您的应用程序需要能够处理正则表达式解析错误。几种方法包括使用语言模型来修复前一个语言模型的响应或使用前一个状态发出另一个新的语言模型请求。
- en: 'You can now combine all of the components; here is a step-by-step explanation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以组合所有组件；以下是一个逐步解释：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Initialize the `ChatOpenAI` instance:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `ChatOpenAI` 实例：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Adding a `stop` sequence forces an LLM to stop generating new tokens after encounting
    the phrase `"tool_result:"`. This helps by stopping hallucinations for tool usage.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个 `stop` 序列会迫使语言模型在遇到短语 `"tool_result:"` 后停止生成新的标记。这有助于通过停止工具使用时的幻觉。
- en: 'Define the available tools:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 定义可用的工具：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Set the base prompt template:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 设置基本提示模板：
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]` Generate the model output:    [PRE14]    Extract the last `action`,
    `action_input`, and call the relevant function:    [PRE15]    Print the tool details:    [PRE16]
    `tool_result:` `{``tool_result``}``"""` `)` [PRE17]   [PRE18] [PRE19]`py [PRE20]
    [PRE21] You are looking to accomplish: {goal} You have access to the following
    {tools} [PRE22] # Import necessary classes and functions: from langchain.agents
    import AgentExecutor, create_react_agent from langchain import hub from langchain_openai
    import ChatOpenAI from langchain.tools import Tool  # Defining the LLM to use:
    model = ChatOpenAI()  # Function to count the number of characters in a string:
    def count_characters_in_string(string):     return len(string)  # Create a list
    of tools: # Currently, only one tool is defined that counts characters in a text
    string. tools = [     Tool.from_function(         func=count_characters_in_string,         name="Count
    Characters in a text string",         description="Count the number of characters
    in a text string",     ) ]  # Download a react prompt! prompt = hub.pull("hwchase17/react")  #
    Construct the ReAct agent: agent = create_react_agent(model, tools, prompt)  #
    Initialize an agent with the defined tools and # Create an agent executor by passing
    in the agent and tools: agent_executor = AgentExecutor(agent=agent, tools=tools,
    verbose=True)  # Invoke the agent with a query to count the characters in the
    given word: agent_executor.invoke({"input": ''''''How many characters are in the
    word "supercalifragilisticexpialidocious"?''''''})  # ''There are 34 characters
    in the word "supercalifragilisticexpialidocious".'' [PRE23] Entering new AgentExecutor
    change... I should count the number of characters in the word "supercalifragilisticexpiladocious".
    Action: Count Characters in a text string Action Input: "supercalifragilisticexpiladocious"
    Observation: 34 Thought: I now know the final answer Final Answer: There are 34
    characters in the word "supercalifragilisticexpiladocious". [PRE24]` [PRE25]``
    [PRE26] # Import necessary modules and functions from the langchain package: from
    langchain.chains import (     LLMMathChain, ) from langchain import hub from langchain.agents
    import create_openai_functions_agent, Tool, AgentExecutor from langchain_openai.chat_models
    import ChatOpenAI  # Initialize the ChatOpenAI with temperature set to 0: model
    = ChatOpenAI(temperature=0)  # Create a LLMMathChain instance using the ChatOpenAI
    model: llm_math_chain = LLMMathChain.from_llm(llm=model, verbose=True)  # Download
    the prompt from the hub: prompt = hub.pull("hwchase17/openai-functions-agent")  tools
    = [     Tool(         name="Calculator",         func=llm_math_chain.run, # run
    the LLMMathChain         description="useful for when you need to answer questions
    about math",         return_direct=True,     ), ]  # Create an agent using the
    ChatOpenAI model and the tools: agent = create_openai_functions_agent(llm=model,
    tools=tools, prompt=prompt) agent_executor = AgentExecutor(agent=agent, tools=tools,
    verbose=True)  result = agent_executor.invoke({"input": "What is 5 + 5?"}) print(result)
    # {''input'': ''What is 5 + 5?'', ''output'': ''Answer: 10''} [PRE27] def google_search(query:
    str) -> str:     return "James Phoenix is 31 years old."  # List of tools that
    the agent can use: tools = [     Tool(         # The LLMMathChain tool for math
    calculations.         func=llm_math_chain.run,         name="Calculator",         description="useful
    for when you need to answer questions about math",     ),     Tool(         #
    Tool for counting characters in a string.         func=google_search,         name="google_search",         description="useful
    for when you need to find out about someones age.",     ), ]   # Create an agent
    using the ChatOpenAI model and the tools: agent = create_openai_functions_agent(llm=model,
    tools=tools, prompt=prompt) agent_executor = AgentExecutor(agent=agent, tools=tools,
    verbose=True)  # Asking the agent to run a task and store its result: result =
    agent_executor.invoke(     {         "input": """Task: Google search for James
    Phoenix''s age.  Then square it."""} ) print(result) # {''input'': "...", ''output'':
    ''James Phoenix is 31 years old. # Squaring his age, we get 961.''} [PRE28] #
    Importing the relevant packages: from langchain.agents.agent_types import AgentType
    from langchain_experimental.agents.agent_toolkits import create_csv_agent from
    langchain_openai.chat_models import ChatOpenAI  # Creating a CSV Agent: agent
    = create_csv_agent(     ChatOpenAI(temperature=0),     "data/heart_disease_uci.csv",     verbose=True,     agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    )  agent.invoke("How many rows of data are in the file?") # ''920''  agent.invoke("What
    are the columns within the dataset?") # "''id'', ''age'', ''sex'', ''dataset'',
    ''cp'', ''trestbps'', ''chol'', ''fbs'', # ''restecg'', ''thalch'', ''exang'',
    ''oldpeak'', ''slope'', ''ca'', ''thal'', ''num''"  agent.invoke("Create a correlation
    matrix for the data and save it to a file.") # "The correlation matrix has been
    saved to a file named # ''correlation_matrix.csv''." [PRE29] from langchain.agents
    import create_sql_agent from langchain_community.agent_toolkits import SQLDatabaseToolkit
    from langchain.sql_database import SQLDatabase from langchain.agents.agent_types
    import AgentType from langchain_openai.chat_models import ChatOpenAI  db = SQLDatabase.from_uri("sqlite:///./data/demo.db")
    toolkit = SQLDatabaseToolkit(db=db, llm=ChatOpenAI(temperature=0))  # Creating
    an agent executor: agent_executor = create_sql_agent(     llm=ChatOpenAI(temperature=0),     toolkit=toolkit,     verbose=True,     agent_type=AgentType.OPENAI_FUNCTIONS,
    )  # Identifying all of the tables: agent_executor.invoke("Identify all of the
    tables") # ''The database contains the following tables:\n1\. Orders\n2\. Products\n3\.
    Users'' [PRE30] user_sql = agent_executor.invoke(     ''''''Add 5 new users to
    the database. Their names are:  John, Mary, Peter, Paul, and Jane.'''''' ) ''''''Based
    on the schema of the "Users" table, I can see that the relevant columns for adding
    new users are "FirstName", "LastName", "Email", and "DateJoined". I will now run
    the SQL query to add the new users.\n\n[PRE31]\n\nPlease note that I have added
    the new users with the specified names and email addresses. The "DateJoined" column
    is set to the respective dates mentioned.'''''' [PRE32] # This the function signature
    for demonstration purposes and is not executable. def create_sql_agent(     llm:
    BaseLanguageModel,     toolkit: SQLDatabaseToolkit,     agent_type: Any | None
    = None,     callback_manager: BaseCallbackManager | None = None,     prefix: str
    = SQL_PREFIX,     suffix: str | None = None,     format_instructions: str | None
    = None,     input_variables: List[str] | None = None,     top_k: int = 10,     max_iterations:
    int | None = 15,     max_execution_time: float | None = None,     early_stopping_method:
    str = "force",     verbose: bool = False,     agent_executor_kwargs: Dict[str,
    Any] | None = None,     extra_tools: Sequence[BaseTool] = (),     **kwargs: Any
    ) -> AgentExecutor [PRE33] SQL_PREFIX = """You are an agent designed to interact
    with a SQL database. Given an input question, create a syntactically correct {dialect}
    query to run, then look at the results of the query and return the answer. Unless
    the user specifies a specific number of examples they wish to obtain always limit
    your query to at most {top_k} results. You can order the results by a relevant
    column to return the most interesting examples in the database. Never query for
    all the columns from a specific table, only ask for the relevant columns given
    the question. You have access to tools for interacting with the database. Only
    use the below tools. Only use the information returned by the below tools to construct
    your final answer. You MUST double-check your query before executing it. If you
    get an error while executing a query, rewrite the query and try again. If the
    question does not seem related to the database, just return "I don''t know" as
    the answer. """  agent_executor = create_sql_agent(     llm=ChatOpenAI(temperature=0),     toolkit=toolkit,     verbose=True,     agent_type=AgentType.OPENAI_FUNCTIONS,     prefix=SQL_PREFIX,
    )  agent_executor.invoke(user_sql) # ''...sql\nINSERT INTO Users (FirstName, LastName,
    Email, # DateJoined)\nVALUES (...)...''  # Testing that Peter was inserted into
    the database: agent_executor.invoke("Do we have a Peter in the database?") ''''''Yes,
    we have a Peter in the database. Their details are as follows:\n- First Name:
    Peter...'''''' [PRE34] from langchain_openai import ChatOpenAI from langchain_core.tools
    import tool  # 1\. Create the model: llm = ChatOpenAI(temperature=0)  @tool def
    get_word_length(word: str) -> int:     """Returns the length of a word."""     return
    len(word)  # 2\. Create the tools: tools = [get_word_length] [PRE35] from langchain_core.prompts
    import ChatPromptTemplate, MessagesPlaceholder  # 3\. Create the Prompt: prompt
    = ChatPromptTemplate.from_messages(     [         (             "system",             """You
    are very powerful assistant, but don''t know current events  and aren''t good
    at calculating word length.""",         ),         ("user", "{input}"),         #
    This is where the agent will write/read its messages from         MessagesPlaceholder(variable_name="agent_scratchpad"),     ]
    ) [PRE36] from langchain_core.utils.function_calling import convert_to_openai_tool
    from langchain.agents.format_scratchpad.openai_tools import (     format_to_openai_tool_messages,
    )  # 4\. Formats the python function tools into JSON schema and binds # them to
    the model: llm_with_tools = llm.bind_tools(tools=[convert_to_openai_tool(t) for
    t in tools])  from langchain.agents.output_parsers.openai_tools \ import OpenAIToolsAgentOutputParser   #
    5\. Setting up the agent chain: agent = (     {         "input": lambda x: x["input"],         "agent_scratchpad":
    lambda x: format_to_openai_tool_messages(             x["intermediate_steps"]         ),     }     |
    prompt     | llm_with_tools     | OpenAIToolsAgentOutputParser() ) [PRE37] from
    langchain.agents import AgentExecutor  agent_executor = AgentExecutor(agent=agent,
    tools=tools, verbose=True) agent_executor.invoke({"input": "How many letters in
    the word Software?"}) #{''input'': ''How many letters in the word Software?'',
    # ''output'': ''There are 8 letters in the word "Software".''} [PRE38] # Provide
    the connection string to connect to the MongoDB database. connection_string =
    "mongodb://mongo_user:password123@mongo:27017"  chat_message_history = MongoDBChatMessageHistory(     session_id="test_session",     connection_string=connection_string,     database_name="my_db",     collection_name="chat_histories",
    )  chat_message_history.add_user_message("I love programming!!") chat_message_history.add_ai_message("What
    do you like about it?")  chat_message_history.messages # [HumanMessage(content=''I
    love programming!!'', # AIMessage(content=''What do you like about it?'') [PRE39]
    from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory()
    memory.save_context({"input": "hi"}, {"output": "whats up"}) memory.load_memory_variables({})
    # {''history'': ''Human: hi\nAI: whats up''} [PRE40] memory = ConversationBufferMemory(return_messages=True)
    memory.save_context({"input": "hi"}, {"output": "whats up"}) memory.load_memory_variables({})
    # {''history'': [HumanMessage(content=''hi''), # AIMessage(content=''whats up'')]}
    [PRE41] # Using within a chain: from langchain.memory import ConversationBufferMemory
    from langchain_openai.chat_models import ChatOpenAI from langchain_core.prompts
    import ChatPromptTemplate, MessagesPlaceholder from langchain_core.output_parsers
    import StrOutputParser from langchain_core.runnables import RunnableLambda from
    operator import itemgetter  memory = ConversationBufferMemory(return_messages=True)  model
    = ChatOpenAI(temperature=0) prompt = ChatPromptTemplate.from_messages(     [         ("system",
    "Act as a chatbot that helps users with their queries."),         # The history
    of the conversation         MessagesPlaceholder(variable_name="history"),         ("human",
    "{input}"),     ] ) chain = (     {         "input": lambda x: x["input"],         "history":
    RunnableLambda(memory.load_memory_variables) | \         itemgetter("history"),     }     |
    prompt     | model     | StrOutputParser() ) [PRE42] inputs = {"input": "Hi my
    name is James!"} result = chain.invoke(inputs) memory.save_context(inputs, {"outputs":
    result}) print(memory.load_memory_variables({}))  # {''history'': [HumanMessage(content=''Hi
    my name is James!''), # AIMessage(content=''Hello James! How can I assist you
    today?'')]} [PRE43] inputs = {"input": "What is my name?"} second_result = chain.invoke(inputs)
    print(second_result) # Your name is James. [PRE44] prompt = ChatPromptTemplate.from_messages(     [         (             "system",             """You
    are a very powerful assistant, but don''t know current events and aren''t good
    at calculating word length.""",         ),         # This is where the agent will
    write/read its messages from         MessagesPlaceholder(variable_name="agent_scratchpad"),         MessagesPlaceholder(variable_name="history"),         ("user",
    "{input}"),     ] )  # ... The rest of the code remains the same as before ...  #
    Create an agent executor by passing in the agent, tools, and memory: memory =
    ConversationBufferMemory(return_messages=True) agent_executor = AgentExecutor(agent=agent,
    tools=tools, verbose=True, memory=memory) [PRE45] from langchain.memory import
    ConversationBufferMemory  memory = ConversationBufferMemory() memory.save_context({"input":
    "hi"}, {"output": "whats up"}) memory.load_memory_variables({}) # {''history'':
    ''Human: hi\nAI: whats up''} [PRE46] from langchain.memory import ConversationBufferWindowMemory  memory
    = ConversationBufferWindowMemory(k=1) memory.save_context({"input": "hi"}, {"output":
    "whats up"}) memory.save_context({"input": "not much you"}, {"output": "not much"})
    # Returns: {''history'': ''Human: not much you\nAI: not much''} memory.load_memory_variables({})
    [PRE47] from langchain.memory import ConversationSummaryMemory, ChatMessageHistory
    from langchain_openai import OpenAI  memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))
    memory.save_context({"input": "hi"}, {"output": "whats up"}) memory.load_memory_variables({})
    # Returns: {''history'': ''\nThe human greets the AI, to which the AI responds.''}
    [PRE48] from langchain.memory import ConversationSummaryBufferMemory from langchain_openai.chat_models
    import ChatOpenAI  memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(),
    max_token_limit=10) memory.save_context({"input": "hi"}, {"output": "whats up"})
    memory.load_memory_variables({}) # Returns: {''history'': ''System: \nThe human
    says "hi", and the AI responds with # "whats up".\nHuman: not much you\nAI: not
    much''} [PRE49] from langchain.memory import ConversationTokenBufferMemory from
    langchain_openai.chat_models import ChatOpenAI  memory = ConversationTokenBufferMemory(llm=ChatOpenAI(),
    max_token_limit=50) memory.save_context({"input": "hi"}, {"output": "whats up"})
    memory.load_memory_variables({}) # Returns: {''history'': ''Human: not much you\nAI:
    not much''} [PRE50] from langchain.tools import StructuredTool  def save_interview(raw_interview_text:
    str):     """Tool to save the interview. You must pass the entire interview and  conversation
    in here. The interview will then be saved to a local file.  Remember to include
    all of the previous chat messages. Include all of  the messages with the user
    and the AI, here is a good response:  AI: some text  Human: some text  ...  ---  """     #
    Save to local file:     with open("interview.txt", "w") as f:         f.write(raw_interview_text)     return
    f''''''Interview saved! Content: {raw_interview_text}. File:  interview.txt. You
    must tell the user that the interview is saved.''''''  save_interview = StructuredTool.from_function(save_interview)
    [PRE51] from pydantic.v1 import BaseModel from typing import Union, Literal, Type
    from langchain_core.tools import BaseTool  class ArgumentType(BaseModel):     url:
    str     file_type: Union[Literal["pdf"], Literal["txt"]]  class SummarizeFileFromURL(BaseTool):     name
    = "SummarizeFileFromURL"     description = "Summarize a file from a URL."     args_schema:
    Type[ArgumentType] = ArgumentType [PRE52] AgentExecutor(.., verbose=True) [PRE53]
    class BaseCallbackHandler:     """Base callback handler that can be used to handle
    callbacks from  langchain."""      def on_llm_start(         self, serialized:
    Dict[str, Any], prompts: List[str],         **kwargs: Any     ) -> Any:         """Run
    when LLM starts running."""      def on_chat_model_start(         self, serialized:
    Dict[str, Any],         messages: List[List[BaseMessage]], **kwargs: Any     )
    -> Any:         """Run when Chat Model starts running."""      def on_llm_new_token(self,
    token: str, **kwargs: Any) -> Any:         """Run on new LLM token. Only available
    when streaming is enabled."""      def on_llm_end(self, response: LLMResult, **kwargs:
    Any) -> Any:         """Run when LLM ends running."""      def on_llm_error(         self,
    error: Union[Exception, KeyboardInterrupt], **kwargs: Any     ) -> Any:         """Run
    when LLM errors."""      def on_chain_start(         self, serialized: Dict[str,
    Any], inputs: Dict[str, Any],         **kwargs: Any     ) -> Any:         """Run
    when chain starts running."""      def on_chain_end(self, outputs: Dict[str, Any],
    **kwargs: Any) -> Any:         """Run when chain ends running."""      def on_chain_error(         self,
    error: Union[Exception, KeyboardInterrupt], **kwargs: Any     ) -> Any:         """Run
    when chain errors."""      def on_tool_start(         self, serialized: Dict[str,
    Any], input_str: str, **kwargs: Any     ) -> Any:         """Run when tool starts
    running."""      def on_tool_end(self, output: str, **kwargs: Any) -> Any:         """Run
    when tool ends running."""      def on_tool_error(         self, error: Union[Exception,
    KeyboardInterrupt], **kwargs: Any     ) -> Any:         """Run when tool errors."""      def
    on_text(self, text: str, **kwargs: Any) -> Any:         """Run on arbitrary text."""      def
    on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:         """Run
    on agent action."""      def on_agent_finish(self, finish: AgentFinish, **kwargs:
    Any) -> Any:         """Run on agent end.""" [PRE54] from langchain.agents import
    AgentExecutor from langchain.callbacks import StdOutCallbackHandler  agent_executor
    = AgentExecutor(     agent=agent,     tools=tools,     verbose=True,     callbacks=[StdOutCallbackHandler()],     tags=[''a-tag''])  agent_executor.invoke({"input":
    "How many letters in the word Software?"}) [PRE55] from langchain.callbacks import
    StdOutCallbackHandler from langchain.chains import LLMChain from langchain_openai
    import OpenAI from langchain_core.prompts import PromptTemplate  handler = StdOutCallbackHandler()
    llm = OpenAI() prompt = PromptTemplate.from_template("What is 1 + {number} = ")
    chain = LLMChain(llm=llm, prompt=prompt) chain.invoke({"number": 2}, {"callbacks":
    [handler]}) [PRE56] import asyncio from langchain.callbacks import get_openai_callback
    from langchain_core.messages import SystemMessage from langchain_openai.chat_models
    import ChatOpenAI model = ChatOpenAI() [PRE57] with get_openai_callback() as cb:     model.invoke([SystemMessage(content="My
    name is James")]) total_tokens = cb.total_tokens print(total_tokens) # 25 assert
    total_tokens > 0 [PRE58] with get_openai_callback() as cb:     model.invoke([SystemMessage(content="My
    name is James")])     model.invoke([SystemMessage(content="My name is James")])
    assert cb.total_tokens > 0 print(cb.total_tokens) # 50 [PRE59] # Async callbacks:
    with get_openai_callback() as cb:     await asyncio.gather(         model.agenerate(             [                 [SystemMessage(content="Is
    the meaning of life 42?")],                 [SystemMessage(content="Is the meaning
    of life 42?")],             ],         )     ) print(cb.__dict__) # {''successful_requests'':
    2, ''total_cost'': 0.000455, # ''total_tokens'': 235, ''prompt_tokens'': 30, #
    ''completion_tokens'': 205} [PRE60]` `````'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
