- en: Chapter 18\. CNN Interpretation with CAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how to build up pretty much anything from scratch, let’s use
    that knowledge to create entirely new (and very useful!) functionality: the *class
    activation map*. It gives a us some insight into why a CNN made the predictions
    it did.'
  prefs: []
  type: TYPE_NORMAL
- en: In the process, we’ll learn about one handy feature of PyTorch we haven’t seen
    before, the *hook*, and we’ll apply many of the concepts introduced in the rest
    of the book. If you want to really test out your understanding of the material
    in this book, after you’ve finished this chapter, try putting it aside and re-creating
    the ideas here yourself from scratch (no peeking!).
  prefs: []
  type: TYPE_NORMAL
- en: CAM and Hooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *class activation map* (CAM) was introduced by Bolei Zhou et al. in [“Learning
    Deep Features for Discriminative Localization”](https://oreil.ly/5hik3). It uses
    the output of the last convolutional layer (just before the average pooling layer)
    together with the predictions to give us a heatmap visualization of why the model
    made its decision. This is a useful tool for interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: More precisely, at each position of our final convolutional layer, we have as
    many filters as in the last linear layer. We can therefore compute the dot product
    of those activations with the final weights to get, for each location on our feature
    map, the score of the feature that was used to make a decision.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to need a way to get access to the activations inside the model
    while it’s training. In PyTorch, this can be done with a *hook*. Hooks are PyTorch’s
    equivalent of fastai’s callbacks. However, rather than allowing you to inject
    code into the training loop like a fastai `Learner` callback, hooks allow you
    to inject code into the forward and backward calculations themselves. We can attach
    a hook to any layer of the model, and it will be executed when we compute the
    outputs (forward hook) or during backpropagation (backward hook). A forward hook
    is a function that takes three things—a module, its input, and its output—and
    it can perform any behavior you want. (fastai also provides a handy `HookCallback`
    that we won’t cover here, but take a look at the fastai docs; it makes working
    with hooks a little easier.)
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, we’ll use the same cats and dogs model we trained in [Chapter 1](ch01.xhtml#chapter_intro):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | error_rate | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.141987 | 0.018823 | 0.007442 | 00:16 |'
  prefs: []
  type: TYPE_TB
- en: '| epoch | train_loss | valid_loss | error_rate | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.050934 | 0.015366 | 0.006766 | 00:21 |'
  prefs: []
  type: TYPE_TB
- en: 'To start, we’ll grab a cat picture and a batch of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For CAM, we want to store the activations of the last convolutional layer.
    We put our hook function in a class so it has a state that we can access later,
    and just store a copy of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then instantiate a `Hook` and attach it to the layer we want, which
    is the last layer of the CNN body:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can grab a batch and feed it through our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can access our stored activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also double-check our predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We know `0` (for `False`) is “dog,” because the classes are automatically sorted
    in fastai, but we can still double-check by looking at `dls.vocab`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So, our model is very confident this was a picture of a cat.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do the dot product of our weight matrix (2 by number of activations) with
    the activations (batch size by activations by rows by cols), we use a custom `einsum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For each image in our batch, and for each class, we get a 7×7 feature map that
    tells us where the activations were higher and where they were lower. This will
    let us see which areas of the pictures influenced the model’s decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, we can find out which areas made the model decide this animal
    was a cat (note that we need to `decode` the input `x` since it’s been normalized
    by the `DataLoader`, and we need to cast to `TensorImage` since at the time this
    book is written, PyTorch does not maintain types when indexing—this may be fixed
    by the time you are reading this):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_18in01.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the areas in bright yellow correspond to high activations, and
    the areas in purple to low activations. In this case, we can see the head and
    the front paw were the two main areas that made the model decide it was a picture
    of a cat.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’re done with your hook, you should remove it as otherwise it might
    leak some memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: That’s why it’s usually a good idea to have the `Hook` class be a *context manager*,
    registering the hook when you enter it and removing it when you exit. A context
    manager is a Python construct that calls `__enter__` when the object is created
    in a `with` clause, and `__exit__` at the end of the `with` clause. For instance,
    this is how Python handles the `with open(...) as f:` construct that you’ll often
    see for opening files without requiring an explicit `close(f)` at the end.
  prefs: []
  type: TYPE_NORMAL
- en: If we define `Hook` as follows
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'we can safely use it this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: fastai provides this `Hook` class for you, as well as some other handy classes
    to make working with hooks easier.
  prefs: []
  type: TYPE_NORMAL
- en: This method is useful, but works for only the last layer. *Gradient CAM* is
    a variant that addresses this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient CAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The method we just saw lets us compute only a heatmap with the last activations,
    since once we have our features, we have to multiply them by the last weight matrix.
    This won’t work for inner layers in the network. A variant introduced in the 2016
    paper [“Grad-CAM: Why Did You Say That?”](https://oreil.ly/4krXE) by Ramprasaath
    R. Selvaraju et al. uses the gradients of the final activation for the desired
    class. If you remember a little bit about the backward pass, the gradients of
    the output of the last layer with respect to the input of that layer are equal
    to the layer weights, since it is a linear layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With deeper layers, we still want the gradients, but they won’t just be equal
    to the weights anymore. We have to calculate them. The gradients of every layer
    are calculated for us by PyTorch during the backward pass, but they’re not stored
    (except for tensors where `requires_grad` is `True`). We can, however, register
    a hook on the backward pass, which PyTorch will give the gradients to as a parameter,
    so we can store them there. For this, we will use a `HookBwd` class that works
    like `Hook`, but intercepts and stores gradients instead of activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then for the class index `1` (for `True`, which is “cat”), we intercept the
    features of the last convolutional layer, as before, and compute the gradients
    of the output activations of our class. We can’t just call `output.backward`,
    because gradients make sense only with respect to a scalar (which is normally
    our loss), and `output` is a rank-2 tensor. But if we pick a single image (we’ll
    use `0`) and a single class (we’ll use `1`), we *can* calculate the gradients
    of any weight or activation we like, with respect to that single value, using
    `output[0,cls].backward`. Our hook intercepts the gradients that we’ll use as
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The weights for Grad-CAM are given by the average of our gradients across the
    feature map. Then it’s exactly the same as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_18in02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The novelty with Grad-CAM is that we can use it on any layer. For example,
    here we use it on the output of the second-to-last ResNet group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can now view the activation map for this layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_18in03.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model interpretation is an area of active research, and we just scraped the
    surface of what is possible in this brief chapter. Class activation maps give
    us insight into why a model predicted a certain result by showing the areas of
    the images that were most responsible for a given prediction. This can help us
    analyze false positives and figure out what kind of data is missing in our training
    to avoid them.
  prefs: []
  type: TYPE_NORMAL
- en: Questionnaire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a hook in PyTorch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which layer does CAM use the outputs of?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does CAM require a hook?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at the source code of the `ActivationStats` class and see how it uses hooks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a hook that stores the activations of a given layer in a model (without
    peeking, if possible).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we call `eval` before getting the activations? Why do we use `no_grad`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `torch.einsum` to compute the “dog” or “cat” score of each of the locations
    in the last activation of the body of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you check which order the categories are in (i.e., the correspondence
    of index→category)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are we using `decode` when displaying the input image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a context manager? What special methods need to be defined to create
    one?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why can’t we use plain CAM for the inner layers of a network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need to register a hook on the backward pass in order to do Grad-CAM?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why can’t we call `output.backward` when `output` is a rank-2 tensor of output
    activations per image per class?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Try removing `keepdim` and see what happens. Look up this parameter in the PyTorch
    docs. Why do we need it in this notebook?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a notebook like this one, but for NLP, and use it to find which words
    in a movie review are most significant in assessing the sentiment of a particular
    movie review.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
