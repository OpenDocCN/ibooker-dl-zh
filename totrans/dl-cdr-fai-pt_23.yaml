- en: Chapter 18\. CNN Interpretation with CAM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章。使用CAM解释CNN
- en: 'Now that we know how to build up pretty much anything from scratch, let’s use
    that knowledge to create entirely new (and very useful!) functionality: the *class
    activation map*. It gives a us some insight into why a CNN made the predictions
    it did.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何从头开始构建几乎任何东西，让我们利用这些知识来创建全新（并非常有用！）的功能：*类激活图*。它让我们对 CNN 为何做出预测有一些见解。
- en: In the process, we’ll learn about one handy feature of PyTorch we haven’t seen
    before, the *hook*, and we’ll apply many of the concepts introduced in the rest
    of the book. If you want to really test out your understanding of the material
    in this book, after you’ve finished this chapter, try putting it aside and re-creating
    the ideas here yourself from scratch (no peeking!).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们将学习到 PyTorch 中一个我们之前没有见过的方便功能，*hook*，并且我们将应用本书中介绍的许多概念。如果你想真正测试你对本书材料的理解，完成本章后，尝试将其放在一边，从头开始重新创建这里的想法（不要偷看！）。
- en: CAM and Hooks
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CAM 和 Hooks
- en: The *class activation map* (CAM) was introduced by Bolei Zhou et al. in [“Learning
    Deep Features for Discriminative Localization”](https://oreil.ly/5hik3). It uses
    the output of the last convolutional layer (just before the average pooling layer)
    together with the predictions to give us a heatmap visualization of why the model
    made its decision. This is a useful tool for interpretation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*类激活图*（CAM）是由周博磊等人在[“学习用于区分定位的深度特征”](https://oreil.ly/5hik3)中引入的。它使用最后一个卷积层的输出（就在平均池化层之前）以及预测结果，为我们提供一个热图可视化，解释模型为何做出决定。这是一个有用的解释工具。'
- en: More precisely, at each position of our final convolutional layer, we have as
    many filters as in the last linear layer. We can therefore compute the dot product
    of those activations with the final weights to get, for each location on our feature
    map, the score of the feature that was used to make a decision.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，在我们最终卷积层的每个位置，我们有与最后一个线性层中一样多的滤波器。因此，我们可以计算这些激活与最终权重的点积，以便为我们特征图上的每个位置得到用于做出决定的特征的分数。
- en: We’re going to need a way to get access to the activations inside the model
    while it’s training. In PyTorch, this can be done with a *hook*. Hooks are PyTorch’s
    equivalent of fastai’s callbacks. However, rather than allowing you to inject
    code into the training loop like a fastai `Learner` callback, hooks allow you
    to inject code into the forward and backward calculations themselves. We can attach
    a hook to any layer of the model, and it will be executed when we compute the
    outputs (forward hook) or during backpropagation (backward hook). A forward hook
    is a function that takes three things—a module, its input, and its output—and
    it can perform any behavior you want. (fastai also provides a handy `HookCallback`
    that we won’t cover here, but take a look at the fastai docs; it makes working
    with hooks a little easier.)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，我们需要一种方法来访问模型内部的激活。在 PyTorch 中，可以通过 *hook* 来实现。Hook 是 PyTorch 的等价于 fastai
    的回调。然而，与允许您像 fastai 的 `Learner` 回调一样将代码注入训练循环不同，hook 允许您将代码注入前向和反向计算本身。我们可以将 hook
    附加到模型的任何层，并且在计算输出（前向 hook）或反向传播（后向 hook）时执行。前向 hook 是一个接受三个参数的函数——一个模块，它的输入和输出——它可以执行任何您想要的行为。（fastai
    还提供了一个方便的 `HookCallback`，我们这里不涉及，但看看 fastai 文档；它使使用 hook 更容易一些。）
- en: 'To illustrate, we’ll use the same cats and dogs model we trained in [Chapter 1](ch01.xhtml#chapter_intro):'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我们将使用我们在[第1章](ch01.xhtml#chapter_intro)中训练的相同的猫狗模型：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '| epoch | train_loss | valid_loss | error_rate | time |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 轮次 | 训练损失 | 验证损失 | 错误率 | 时间 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.141987 | 0.018823 | 0.007442 | 00:16 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.141987 | 0.018823 | 0.007442 | 00:16 |'
- en: '| epoch | train_loss | valid_loss | error_rate | time |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 轮次 | 训练损失 | 验证损失 | 错误率 | 时间 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.050934 | 0.015366 | 0.006766 | 00:21 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.050934 | 0.015366 | 0.006766 | 00:21 |'
- en: 'To start, we’ll grab a cat picture and a batch of data:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将获取一张猫的图片和一批数据：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For CAM, we want to store the activations of the last convolutional layer.
    We put our hook function in a class so it has a state that we can access later,
    and just store a copy of the output:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CAM，我们想要存储最后一个卷积层的激活。我们将我们的 hook 函数放在一个类中，这样它就有一个我们稍后可以访问的状态，并且只存储输出的副本：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can then instantiate a `Hook` and attach it to the layer we want, which
    is the last layer of the CNN body:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以实例化一个 `Hook` 并将其附加到我们想要的层，即 CNN 主体的最后一层：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we can grab a batch and feed it through our model:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以获取一个批次并将其通过我们的模型：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And we can access our stored activations:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以访问我们存储的激活：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s also double-check our predictions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次双重检查我们的预测：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We know `0` (for `False`) is “dog,” because the classes are automatically sorted
    in fastai, but we can still double-check by looking at `dls.vocab`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 `0`（对于 `False`）是“狗”，因为在 fastai 中类别会自动排序，但我们仍然可以通过查看 `dls.vocab` 来进行双重检查：
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So, our model is very confident this was a picture of a cat.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的模型非常确信这是一张猫的图片。
- en: 'To do the dot product of our weight matrix (2 by number of activations) with
    the activations (batch size by activations by rows by cols), we use a custom `einsum`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对我们的权重矩阵（2乘以激活数量）与激活（批次大小乘以激活乘以行乘以列）进行点积，我们使用自定义的 `einsum`：
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For each image in our batch, and for each class, we get a 7×7 feature map that
    tells us where the activations were higher and where they were lower. This will
    let us see which areas of the pictures influenced the model’s decision.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们批次中的每个图像，对于每个类别，我们得到一个 7×7 的特征图，告诉我们激活较高和较低的位置。这将让我们看到哪些图片区域影响了模型的决策。
- en: 'For instance, we can find out which areas made the model decide this animal
    was a cat (note that we need to `decode` the input `x` since it’s been normalized
    by the `DataLoader`, and we need to cast to `TensorImage` since at the time this
    book is written, PyTorch does not maintain types when indexing—this may be fixed
    by the time you are reading this):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以找出哪些区域使模型决定这个动物是一只猫（请注意，由于`DataLoader`对输入`x`进行了归一化，我们需要`decode`，并且由于在撰写本书时，PyTorch在索引时不保留类型，我们需要转换为`TensorImage`——这可能在您阅读本文时已经修复）：
- en: '[PRE14]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](Images/dlcf_18in01.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_18in01.png)'
- en: In this case, the areas in bright yellow correspond to high activations, and
    the areas in purple to low activations. In this case, we can see the head and
    the front paw were the two main areas that made the model decide it was a picture
    of a cat.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，明亮黄色的区域对应于高激活，紫色区域对应于低激活。在这种情况下，我们可以看到头部和前爪是使模型决定这是一张猫的图片的两个主要区域。
- en: 'Once you’re done with your hook, you should remove it as otherwise it might
    leak some memory:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 完成钩子后，应该将其删除，否则可能会泄漏一些内存：
- en: '[PRE15]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That’s why it’s usually a good idea to have the `Hook` class be a *context manager*,
    registering the hook when you enter it and removing it when you exit. A context
    manager is a Python construct that calls `__enter__` when the object is created
    in a `with` clause, and `__exit__` at the end of the `with` clause. For instance,
    this is how Python handles the `with open(...) as f:` construct that you’ll often
    see for opening files without requiring an explicit `close(f)` at the end.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么将`Hook`类作为*上下文管理器*通常是一个好主意，当您进入时注册钩子，当您退出时删除它。上下文管理器是一个Python构造，在`with`子句中创建对象时调用`__enter__`，在`with`子句结束时调用`__exit__`。例如，这就是Python处理`with
    open(...) as f:`构造的方式，您经常会看到用于打开文件而不需要在最后显式调用`close(f)`。
- en: If we define `Hook` as follows
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将`Hook`定义如下
- en: '[PRE16]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'we can safely use it this way:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以安全地这样使用它：
- en: '[PRE17]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: fastai provides this `Hook` class for you, as well as some other handy classes
    to make working with hooks easier.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: fastai为您提供了这个`Hook`类，以及一些其他方便的类，使使用钩子更容易。
- en: This method is useful, but works for only the last layer. *Gradient CAM* is
    a variant that addresses this problem.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法很有用，但仅适用于最后一层。*梯度CAM*是一个解决这个问题的变体。
- en: Gradient CAM
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度CAM
- en: 'The method we just saw lets us compute only a heatmap with the last activations,
    since once we have our features, we have to multiply them by the last weight matrix.
    This won’t work for inner layers in the network. A variant introduced in the 2016
    paper [“Grad-CAM: Why Did You Say That?”](https://oreil.ly/4krXE) by Ramprasaath
    R. Selvaraju et al. uses the gradients of the final activation for the desired
    class. If you remember a little bit about the backward pass, the gradients of
    the output of the last layer with respect to the input of that layer are equal
    to the layer weights, since it is a linear layer.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '我们刚刚看到的方法让我们只能计算最后激活的热图，因为一旦我们有了我们的特征，我们必须将它们乘以最后的权重矩阵。这对网络中的内部层不起作用。2016年的一篇论文[“Grad-CAM:
    Why Did You Say That?”](https://oreil.ly/4krXE)由Ramprasaath R. Selvaraju等人介绍了一种变体，使用所需类的最终激活的梯度。如果您还记得一点关于反向传播的知识，最后一层输出的梯度与该层输入的梯度相对应，因为它是一个线性层。'
- en: 'With deeper layers, we still want the gradients, but they won’t just be equal
    to the weights anymore. We have to calculate them. The gradients of every layer
    are calculated for us by PyTorch during the backward pass, but they’re not stored
    (except for tensors where `requires_grad` is `True`). We can, however, register
    a hook on the backward pass, which PyTorch will give the gradients to as a parameter,
    so we can store them there. For this, we will use a `HookBwd` class that works
    like `Hook`, but intercepts and stores gradients instead of activations:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更深的层，我们仍然希望梯度，但它们不再等于权重。我们必须计算它们。PyTorch在反向传播期间为我们计算每一层的梯度，但它们不会被存储（除了`requires_grad`为`True`的张量）。然而，我们可以在反向传播上注册一个钩子，PyTorch将把梯度作为参数传递给它，因此我们可以在那里存储它们。为此，我们将使用一个`HookBwd`类，它的工作方式类似于`Hook`，但是拦截并存储梯度而不是激活：
- en: '[PRE18]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then for the class index `1` (for `True`, which is “cat”), we intercept the
    features of the last convolutional layer, as before, and compute the gradients
    of the output activations of our class. We can’t just call `output.backward`,
    because gradients make sense only with respect to a scalar (which is normally
    our loss), and `output` is a rank-2 tensor. But if we pick a single image (we’ll
    use `0`) and a single class (we’ll use `1`), we *can* calculate the gradients
    of any weight or activation we like, with respect to that single value, using
    `output[0,cls].backward`. Our hook intercepts the gradients that we’ll use as
    weights:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对于类索引`1`（对于`True`，即“猫”），我们拦截最后一个卷积层的特征，如前所述，计算我们类的输出激活的梯度。我们不能简单地调用`output.backward`，因为梯度只对标量有意义（通常是我们的损失），而`output`是一个秩为2的张量。但是，如果我们选择单个图像（我们将使用`0`）和单个类（我们将使用`1`），我们*可以*计算我们喜欢的任何权重或激活的梯度，与该单个值相关，使用`output[0,cls].backward`。我们的钩子拦截了我们将用作权重的梯度：
- en: '[PRE19]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The weights for Grad-CAM are given by the average of our gradients across the
    feature map. Then it’s exactly the same as before:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Grad-CAM的权重由特征图上的梯度平均值给出。然后就像以前一样：
- en: '[PRE20]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](Images/dlcf_18in02.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_18in02.png)'
- en: 'The novelty with Grad-CAM is that we can use it on any layer. For example,
    here we use it on the output of the second-to-last ResNet group:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Grad-CAM的新颖之处在于我们可以在任何层上使用它。例如，在这里我们将其用于倒数第二个ResNet组的输出：
- en: '[PRE22]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And we can now view the activation map for this layer:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以查看此层的激活图：
- en: '[PRE24]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](Images/dlcf_18in03.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_18in03.png)'
- en: Conclusion
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Model interpretation is an area of active research, and we just scraped the
    surface of what is possible in this brief chapter. Class activation maps give
    us insight into why a model predicted a certain result by showing the areas of
    the images that were most responsible for a given prediction. This can help us
    analyze false positives and figure out what kind of data is missing in our training
    to avoid them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模型解释是一个活跃研究领域，我们只是在这一简短章节中探讨了可能性的一部分。类激活图让我们了解模型为什么预测了某个结果，它展示了图像中对于给定预测最负责的区域。这可以帮助我们分析假阳性，并找出在我们的训练中缺少了哪种数据以避免它们。
- en: Questionnaire
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问卷调查
- en: What is a hook in PyTorch?
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyTorch中的hook是什么？
- en: Which layer does CAM use the outputs of?
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CAM使用哪个层的输出？
- en: Why does CAM require a hook?
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么CAM需要一个hook？
- en: Look at the source code of the `ActivationStats` class and see how it uses hooks.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看`ActivationStats`类的源代码，看看它如何使用hooks。
- en: Write a hook that stores the activations of a given layer in a model (without
    peeking, if possible).
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个hook，用于存储模型中给定层的激活（如果可能的话，不要偷看）。
- en: Why do we call `eval` before getting the activations? Why do we use `no_grad`?
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们在获取激活之前要调用`eval`？为什么我们要使用`no_grad`？
- en: Use `torch.einsum` to compute the “dog” or “cat” score of each of the locations
    in the last activation of the body of the model.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`torch.einsum`来计算模型主体最后激活的每个位置的“狗”或“猫”得分。
- en: How do you check which order the categories are in (i.e., the correspondence
    of index→category)?
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何检查类别的顺序（即索引→类别的对应关系）？
- en: Why are we using `decode` when displaying the input image?
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们在显示输入图像时使用`decode`？
- en: What is a context manager? What special methods need to be defined to create
    one?
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是上下文管理器？需要定义哪些特殊方法来创建一个？
- en: Why can’t we use plain CAM for the inner layers of a network?
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们不能对网络的内部层使用普通的CAM？
- en: Why do we need to register a hook on the backward pass in order to do Grad-CAM?
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了执行Grad-CAM，为什么我们需要在反向传播中注册一个hook？
- en: Why can’t we call `output.backward` when `output` is a rank-2 tensor of output
    activations per image per class?
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当`output`是每个图像每个类别的输出激活的秩为2的张量时，为什么我们不能调用`output.backward`？
- en: Further Research
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究
- en: Try removing `keepdim` and see what happens. Look up this parameter in the PyTorch
    docs. Why do we need it in this notebook?
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试移除`keepdim`，看看会发生什么。查阅PyTorch文档中的这个参数。为什么我们在这个笔记本中需要它？
- en: Create a notebook like this one, but for NLP, and use it to find which words
    in a movie review are most significant in assessing the sentiment of a particular
    movie review.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个类似这个的笔记本，但用于NLP，并用它来找出电影评论中哪些词对于评估特定电影评论的情感最重要。
