- en: Chapter 7\. Monitoring the Training Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, you learned how to launch the model training process. In
    this chapter, we’ll cover the process itself.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve used fairly straightforward examples in this book to help you grasp each
    concept. When you’re running a real training process in TensorFlow, however, things
    can be more complicated. When problems arise, for example, you need to think about
    how to determine whether your model is *overfitting* the training data. (Overfitting
    occurs when the model learns and memorizes the training data and the noise in
    training data so well that it negatively affects its ability to learn new data.)
    If it is, you’ll need to set up cross validation. If not, you can take steps to
    prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other questions that often arise during the training process include:'
  prefs: []
  type: TYPE_NORMAL
- en: How often should I save the model during the training process?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should I determine which epoch gives the best model before overfitting occurs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I track model performance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can I stop training if the model is not improving or is overfitting?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a way to visualize the model training process?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow provides a very easy way to address these questions: callback functions.
    In this chapter, you will learn how to make quick use of callback functions as
    you monitor the training process. The first half of the chapter discusses `ModelCheckpoint`
    and `EarlyStopping`, while the second half focuses on TensorBoard and shows you
    several techniques for invoking TensorBoard and using it for visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: Callback Objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A TensorFlow *callback object* is an object that can perform a group of built-in
    functions provided by `tf.keras`. When certain events occur during training, a
    callback object will execute specific code or functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using callbacks is optional, so you don’t need to implement any callback objects
    to train a model. We’ll be looking at three of the most frequently used classes:
    `ModelCheckpoint`, `EarlyStopping`, and TensorBoard.^([1](ch07.xhtml#ch06fn01))'
  prefs: []
  type: TYPE_NORMAL
- en: ModelCheckpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `ModelCheckpoint` class enables you to save your model regularly throughout
    the training process. By default, at the end of each training epoch, model weights
    and biases are finalized and saved as a weight file. Typically, when you launch
    a training process, the model learns from the training data in that epoch and
    updates the weights and biases, which are saved in a directory you specify before
    beginning the training process. However, sometimes you’ll want to save the model
    only if it has improved from the previous epoch, so that the last saved model
    is always the best model. To do this, you can use the `ModelCheckpoint` class.
    In this section, you are going to see how to leverage this class in your model
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try it out using the CIFAR-10 image classification dataset that we used
    in [Chapter 6](ch06.xhtml#model_creation_styles). As usual, we start by importing
    the necessary libraries, and then we read the CIFAR-10 data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'First, normalize the pixel values in the images to be in a range of 0 to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The image labels in this dataset consist of integers. Verify this using a NumPy
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows the values to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can map these integers to the plain-text labels. The [labels here](https://oreil.ly/hYg5R)
    (provided by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton) are in alphabetical
    order. Thus, `airplane` maps to a value of 0 in `train_labels`, while `truck`
    maps to 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Since there is a separate partition for `test_images`, extract the first 500
    images from `test_images` to use for cross validation and name it `validation_images`.
    You’ll use the remaining images for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use your compute resources more efficiently, convert the `test_images` images
    and labels from their native NumPy array format to dataset format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing these commands, you should have all of the images in three
    datasets: a training dataset (`train_dataset`), a validation dataset (`validation_dataset`),
    and a testing dataset (`test_dataset`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It would be nice to know the sizes of these datasets. To find the sample size
    of a TensorFlow dataset, convert it to a list, and then find the length of the
    list using the `len` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can expect these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, shuffle and batch the three datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Notice that `train_dataset` will be split into multiple batches. Each batch
    will contain `TRAIN_BATCH_SIZE` samples (in this case, 128). Each training batch
    is fed to the model during the training process to enable incremental updates
    to weights and biases. There is no need to create multiple batches for validation
    and testing. These will be used as one batch, but only for the purposes of logging
    metrics and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, specify how often to update weights and validate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code means that after the model has seen the number of batches
    of training data specified by `STEPS_PER_EPOCH`, it’s time to test the model with
    the validation dataset (used as one batch).
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, you’ll first define the model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, compile the model to make sure it’s set up properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, name the folders to which TensorFlow should save the model at each checkpoint.
    Usually, you will rerun the training routine multiple times, and you may find
    it tedious to create a unique folder name each time. A simple and frequently used
    approach is to append a timestamp to the model name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command yields a name such as *myCIFAR10-20210123-212138*. You
    can use this name for the checkpoint directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command specifies the directory path to be *./myCIFAR10-20210123-212138/ckpt-{epoch}*.
    This directory is located one level below your current directory. *{epoch}* will
    be encoded with the epoch number during training. Now define the `myCheckPoint`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here you specified the file path where TensorFlow will save the model at each
    epoch. You also set it up to monitor validation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you launch the training process with a callback, the callback will expect
    a Python list. So, let’s put the `myCheckPoint` object into a Python list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now launch the training process. This command assigns the entire model training
    history to the object `hist`, which is a Python dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can view the cross-validation accuracy from the first epoch of training
    to the last using the command `hist[''val_accuracy'']`. The display should look
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, cross-validation accuracy improved for a number of epochs, then
    gradually degraded. This degradation is a typical sign of overfitting. The best
    model here is the one with the highest validation accuracy (the highest value
    in the array). To determine its position (or index) in the array, use this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember to add 1 to `max_index` because the epoch starts at 1, not 0 (unlike
    a NumPy array index). The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, take a look at the checkpoint directories by running the following Linux
    command in your Jupyter Notebook cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You will see the contents of this directory (shown in [Figure 7-1](#model_saved_at_each_checkpoint)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Model saved at each checkpoint](Images/t2pr_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Model saved at each checkpoint
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can rerun this command and specify a specific directory to see the model
    built by a particular epoch (as shown in [Figure 7-2](#model_files_saved_at_checkpoint_eight)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![Model files saved at checkpoint 8](Images/t2pr_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Model files saved at checkpoint 8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So far, you have seen how to use `CheckPoint` to save the model at each epoch.
    If you wish to save only the best model, specify `save_best_only = True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then put `bestCheckPoint` in a callback list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, you can launch the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this training, rather than saving all checkpoints, `bestCallbacks` causes
    the model to be saved only if it has a better validation accuracy than the previous
    epoch. The `save_best_only` option lets you save checkpoints after the first epoch
    *only* if there is an incremental improvement to the model metric of your choice
    (specified with `monitor`), so that the last checkpoint saved is the best model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To look at what you’ve saved, run the following command in a Jupyter Notebook
    cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The saved models with incremental improvement in validation accuracy are shown
    in [Figure 7-3](#models_saved_with_save_best_only_set_to).
  prefs: []
  type: TYPE_NORMAL
- en: '![Models saved with save_best_only set to True](Images/t2pr_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. Models saved with save_best_only set to True
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The model from the first epoch is always saved. In the third epoch, the model
    shows improvement in validation accuracy, so the third checkpoint model is saved.
    The training continues. Validation accuracy improves in the ninth epoch, so the
    ninth checkpoint model is the last directory that is saved. The training continues
    through the 12th epoch without any further incremental improvement in validation
    accuracy. This means that the ninth checkpoint directory contains the model with
    the best validation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you’re familiar with `ModelCheckpoint`, let’s examine another callback
    object: `EarlyStopping`.'
  prefs: []
  type: TYPE_NORMAL
- en: EarlyStopping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `EarlyStopping` callback object enables you to stop the training process
    before it reaches the final epoch. Usually, you’d do this to save training time
    if the model isn’t improving.
  prefs: []
  type: TYPE_NORMAL
- en: The object lets you specify a model metric—for example, validation accuracy—to
    monitor through all epochs. If the specified metric does not improve after a certain
    number of epochs, the training will stop.
  prefs: []
  type: TYPE_NORMAL
- en: 'To define an `EarlyStopping` object, use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this case, you are monitoring validation accuracy at each epoch. You set
    the `patience` parameter to 4, which means that if validation accuracy does not
    improve within four epochs, the training stops.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To learn more ways to customize early stopping, see the [TensorFlow 2 documentation](https://oreil.ly/UDpaA).
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement early stopping with a `ModelCheckpoint` object in a callback,
    you need to put it into a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The training process is the same, but you designate `callbacks=myCallbacks`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Once you launch the preceding training command, the output should resemble [Figure 7-4](#early_stop_during_training).
  prefs: []
  type: TYPE_NORMAL
- en: '![Early stop during training](Images/t2pr_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Early stop during training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the training shown in [Figure 7-4](#early_stop_during_training), the best
    validation accuracy appears in epoch 15, with a value of 0.7220\. After four more
    epochs, validation accuracy has not improved beyond that value, and so the training
    stops after epoch 19.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the `ModelCheckpoint` class lets you set a condition or cadence to save
    the model during training, the `EarlyStopping` class lets you terminate training
    early if the model is showing no improvement to the metric of your choice. Together,
    these classes are specified in a Python list, and this list is passed into the
    training routine as a callback.
  prefs: []
  type: TYPE_NORMAL
- en: Many other functions are available for monitoring training progress (see [tf.keras.callbacks.Callback](https://oreil.ly/1nIE6)
    and the [Keras Callbacks API](https://oreil.ly/BeJBW)), but `ModelCheckpoint`
    and `EarlyStopping` are two of the most frequently used ones.
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of this chapter will dive deep into the popular callback class
    known as `TensorBoard`, which provides a visual representation of your training
    progress and results.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you wish to visualize your model and training process, TensorBoard is the
    tool for you. TensorBoard provides a visual representation of how your model parameters
    and metrics evolve from the beginning to the end of your training process. It’s
    frequently used for tracking model accuracy over training epochs. It can also
    let you see how weights and biases evolve in each model layer. And just like `ModelCheckpoint`
    and `EarlyStopping`, TensorBoard is applied to the training process via the callbacks
    module. You create an object that represents a `Tensorboard`, then pass that object
    as a member of a callback list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try building a model that classifies CIFAR-10 images. As usual, start
    by importing libraries, loading the CIFAR-10 images, and normalizing pixel values
    to a range of 0 to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Define your plain-text labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now convert the images to a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then determine the data size for training, validation, and test partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Your results should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can shuffle and batch the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'And then specify parameters for setting up a cadence to update model weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`STEPS_PER_EPOCH` is an integer, rounded down from the division between `train_dataset_size`
    and `TRAIN_BATCH_SIZE`. (The double forward slashes indicate division and rounding
    down to the nearest integer.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll reuse the model architecture we built in [“ModelCheckpoint”](#modelcheckpoint):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Notice that this time, each layer has a name. Designating a name for each layer
    helps you know which layer you are inspecting. This is not required, but it’s
    a good practice for visualization in TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now compile the model to make sure the model architecture is valid, and designate
    a loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Setting a model name will be helpful later, when TensorBoard lets you pick
    a model (or models) and inspect the visualization of its training results. You
    can append a timestamp to the model name as we did when using `ModelCheckpoint`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `MODEL_NAME` is `myCIFAR10-20210124-135804`. Yours will be
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, set up the checkpoint directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '*./myCIFAR10-20210124-135804/ckpt-{epoch}* is the name of this checkpoint directory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model checkpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Next you will define a TensorBoard, and then we’ll take a closer look at this
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The first argument specified here is `log_dir`. It is the path where you want
    to save the training logs. As indicated, it is in a directory below the current
    level, named *tensorboardlogs*, which is followed by a subdirectory named `MODEL_NAME`.
    As training progresses, your logs will be generated and stored here so that TensorBoard
    can parse them for visualization.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `write_graph` is set to True, so that the model graph will be
    visualized. Another parameter, `write_images`, is also set to True. This ensures
    that model weights will be written in the log, so you can visualize how they change
    throughout training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, `histogram_freq` is set to 1\. This tells TensorBoard when to create
    a visualization by epoch: 1 means a visualization is created for each epoch. For
    more parameters, see the TensorBoard [documentation](https://oreil.ly/k1Pd2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you have two callback objects to set up: `myCheckPoint` and `myTensorBoard`.
    To put both in a Python list, you only have to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then pass your `myCallbacks` list into the training routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Once the training process is done, there are three ways to invoke TensorBoard.
    You can run it from the next cell in your Jupyter Notebook on your own computer,
    from a command terminal on your own computer, or in Google Colab. We’ll look at
    each of these options in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Invoking TensorBoard by Local Jupyter Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you choose to use your Jupyter Notebook, run the following command in the
    next cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Notice that in this case, when you specify the path to find the training logs,
    the argument is `logdir`, not `log_dir` as it was when you defined `myTensorBoard`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you run the preceding command, you’ll see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, TensorBoard is running at your current compute instance (localhost)
    on port number 6006.
  prefs: []
  type: TYPE_NORMAL
- en: Now open a browser and navigate to *http://localhost:6006*, and you will see
    TensorBoard running, as shown in [Figure 7-5](#tensorboard_visualization).
  prefs: []
  type: TYPE_NORMAL
- en: '![TensorBoard visualization](Images/t2pr_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. TensorBoard visualization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, through each epoch, accuracy and loss are traced in graphs.
    Training data is shown in lighter gray and validation data is shown in a darker
    gray.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of using a Jupyter Notebook cell is convenience. A drawback
    is that the cell that runs the `!tensorboard` command will remain active, and
    you won’t be able to use this notebook until you stop TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Invoking TensorBoard by Local Command Terminal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Your second option is to launch TensorBoard from a command terminal in your
    local environment. As shown in [Figure 7-6](#tensorboard_invoked_from_the_command_ter),
    the equivalent command is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![TensorBoard invoked from the command terminal](Images/t2pr_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. TensorBoard invoked from the command terminal
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember that `logdir` is the directory path to the training logs created by
    the training callback API. The command in the preceding code uses relative path
    notation; you may use a full path if you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is exactly the same as seen in the Jupyter Notebook: the URL (*http://localhost:6006*).
    Open this URL with a browser to display TensorBoard.'
  prefs: []
  type: TYPE_NORMAL
- en: Invoking TensorBoard by Colab Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now for our third option. If you are using a Google Colab notebook for this
    exercise, then invoking TensorBoard will be a little different from what you have
    seen so far. You won’t be able to open a browser on your local computer to point
    to the Colab notebook, because it will be running in Google’s cloud environment.
    You’ll thus need to install a TensorBoard notebook extension. This may be done
    in the first cell, when you import all the libraries. Just add this command and
    run it in the first Colab cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is done, whenever you are ready to invoke TensorBoard (such as after
    training is complete), use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: You will see the output running inside your Colab notebook, looking as shown
    in [Figure 7-5](#tensorboard_visualization).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Model Overfitting Using TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you use TensorBoard as a callback for model training, you will get graphs
    of model accuracy and loss from the first epoch to the last.
  prefs: []
  type: TYPE_NORMAL
- en: For example, with our CIFAR-10 image classification model, you’ll see output
    like that shown in [Figure 7-5](#tensorboard_visualization). In that particular
    training run, while both training and validation accuracy are increasing and losses
    are decreasing, both trends start to flatten out, indicating that further training
    epochs are likely to deliver only marginal gains.
  prefs: []
  type: TYPE_NORMAL
- en: Note also that in this run, validation accuracy is lower than training accuracy,
    while validation loss is higher than training loss. This makes sense, because
    the model performs better with training data than when tested with cross-validation
    data.
  prefs: []
  type: TYPE_NORMAL
- en: You may also get graphs in the Scalars tab of TensorBoard, like those shown
    in [Figure 7-7](#model_overfitting_shown_in_tensorboard).
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 7-7](#model_overfitting_shown_in_tensorboard), the darker lines indicate
    validation metrics, while lighter gray lines indicate training metrics. Model
    accuracy in the training data is much higher than in the cross-validation data,
    and the loss in the training data is much lower than in the cross-validation data.
  prefs: []
  type: TYPE_NORMAL
- en: You may also notice that cross-validation accuracy peaked at epoch 10, with
    a value slightly above 0.7\. After that, validation data accuracy started to decrease,
    while loss started to increase. This is a clear sign of model overfitting. What
    these graphs are telling you is that after epoch 10, your model started to memorize
    training data patterns. That doesn’t help when it encounters new, previously unseen
    data (like the cross-validation images). In fact, the model’s performance in cross
    validation (accuracy and loss) will start to worsen.
  prefs: []
  type: TYPE_NORMAL
- en: '![Model overfitting shown in TensorBoard](Images/t2pr_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. Model overfitting shown in TensorBoard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you inspect these graphs, you’ll know which epoch delivered the best model
    of the training process. You’ll also be well informed about when the model starts
    to overfit and memorize its training data.
  prefs: []
  type: TYPE_NORMAL
- en: If your model still has room for improvement, like the one in [Figure 7-5](#tensorboard_visualization),
    you may decide to increase your training epochs and keep looking for the best
    model before the overfitting pattern starts to appear (see [Figure 7-7](#model_overfitting_shown_in_tensorboard)).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Learning Process Using TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another cool feature in TensorBoard is the histogram of weights and bias distribution.
    These are shown across each epoch as the result of training. By visualizing how
    these parameters are distributed and how their distribution changes over time,
    you gain insights into the impact of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how to use TensorBoard to inspect model weights and bias distributions.
    This information will be in TensorBoard’s Histogram tab ([Figure 7-8](#weights_and_bias_histogram_in_tensorboar)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Weights and bias histogram in TensorBoard](Images/t2pr_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. Weights and bias histogram in TensorBoard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On the left is the panel with all the models trained. Notice there are two models
    selected. On the right are their weights (denoted as `kernel_0`) and bias distributions
    across each epoch of training. Each row of figures represents a particular layer
    in the model. The first layer is named `conv_1`, which is what you named this
    layer back when you set up the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine these figures a bit more closely. We’ll start with the conv_1
    layer shown in [Figure 7-9](#bias_distribution_in_conv_one_layer_thro).
  prefs: []
  type: TYPE_NORMAL
- en: '![Bias distribution in conv_1 layer through training](Images/t2pr_0709.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. Bias distribution in conv_1 layer through training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In both models, the distribution of bias values in the `conv_1` layer definitely
    changed from the first epoch (background) to the last epoch (foreground). The
    boxes indicate that as training progresses, a certain distribution pattern of
    bias starts to emerge in all the nodes of this layer. The new values are away
    from zero, or the center of the overall distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also take a look at the distribution of weights. This time, let’s just
    focus on one model and one layer: conv_3\. This is shown in [Figure 7-10](#weight_distribution_in_conv_three_layer).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Weight distribution in conv_3 layer through training](Images/t2pr_0710.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-10\. Weight distribution in conv_3 layer through training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What’s worth noting is that the distribution became broader and flatter as training
    progressed. This can be seen from the peak count of the histogram from the first
    to the last epochs, from 1.22e+4 to 7.0e+3\. This means that the histogram gradually
    becomes broader, and that there are more weights being updated with values away
    from zero (the center of the histogram).
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorBoard, you can examine different layers and combinations of model
    training runs to see how they are impacted by the training process or by changes
    in model architecture. This is why TensorBoard is frequently used to visually
    inspect the model training process.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you saw some of the most popular methods for tracking the
    model training process. This chapter presented the concept of model checkpointing
    and offered two important ways to help you manage how to save models during training:
    either save the model at every epoch, or only save the model when an incremental
    improvement in model metrics is validated. You also learned that model accuracy
    in cross validation determines when the model starts to overfit the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: Another important tool you learned about in this chapter is TensorBoard, which
    can be used to visualize the training process. TensorBoard presents visual images
    of basic metrics (accuracy and loss) trends through training epochs. It also lets
    you inspect the weight and bias distributions of each layer throughout the training.
    All these techniques are easy to implement in the training routine via callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will see how to implement distributed training in TensorFlow,
    which takes advantage of performant compute units such as GPUs to provide shorter
    training times.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.xhtml#ch06fn01-marker)) Two other common and useful functions not
    covered here are [LearningRateScheduler](https://oreil.ly/CyuGs) and [CSVLogger](https://oreil.ly/vmeaY).
  prefs: []
  type: TYPE_NORMAL
