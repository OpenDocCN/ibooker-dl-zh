<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. The 70% Problem: AI-Assisted Workflows That Actually Work"><div class="chapter" id="ch03_the_70_problem_ai_assisted_workflows_that_actual_1752630043200933">
<h1><span class="label">Chapter 3. </span>The 70% Problem: AI-Assisted <span class="keep-together">Workflows That Actually Work</span></h1>

<p>AI-based coding tools are astonishingly good at certain tasks.<sup><a data-type="noteref" id="id486-marker" href="ch03.html#id486">1</a></sup> They excel<a contenteditable="false" data-primary="70% problem" data-primary-sortas="seventy" data-type="indexterm" id="ix_sevnty"/> at producing boilerplate, writing routine functions, and getting projects <em>most of the way</em> to completion. <a contenteditable="false" data-primary="AI-based coding tools" data-secondary="tasks they excel at" data-type="indexterm" id="id487"/>In fact, many developers find that an AI assistant can implement an initial solution that covers roughly 70% of the requirements.</p>

<p>Peter Yang perfectly captured what I’ve been observing in the field<a href="https://oreil.ly/i9qwq"> in a post on X</a>:</p>

<blockquote>
<p>Honest reflections from coding with AI so far as a non-engineer:</p>

<p>It can get you 70% of the way there, but that last 30% is frustrating. It keeps taking one step forward and two steps backward with new bugs, issues, etc.</p>

<p>If I knew how the code worked I could probably fix it myself. But since I don’t, I question if I’m actually learning that much.</p>
</blockquote>

<p>Nonengineers using AI for coding find themselves hitting a frustrating wall. They can get 70% of the way there surprisingly quickly, but that final 30% becomes an exercise in diminishing returns.</p>

<p>This “70% problem” reveals something crucial about the current state of AI-assisted development. The initial progress feels magical: you can describe what you want, and AI tools like v0 or Bolt will generate a working prototype that looks impressive. But then reality sets in.</p>

<p class="pagebreak-before less_space">The 70% is often the straightforward, patterned part of the work—the kind of code that follows well-trod paths or common frameworks. As one <a href="https://oreil.ly/Ff3Ts"><em>Hacker News</em> commenter observed</a>, AI is superb at handling the “accidental complexity” of software (the repetitive, mechanical stuff), while the “essential complexity”⁠—understanding and managing the inherent complexity of a problem—remains on human shoulders. In Fred Brooks’s classic terms, AI tackles the incidental but not the intrinsic difficulties of development.</p>

<p>Where do these tools struggle? <a contenteditable="false" data-primary="AI-based coding tools" data-secondary="situations in which they struggle" data-type="indexterm" id="id488"/>Experienced engineers consistently report a “last mile” gap. AI can generate a plausible solution, but the final 30%—covering edge cases, refining the architecture, and ensuring maintainability—“needs serious human expertise.”</p>

<p>For example, an AI might give you a function that technically works for the basic scenario, but it won’t automatically account for unusual inputs, race conditions, performance constraints, or future requirements unless explicitly told. AI can get you most of the way there, but that final crucial 30% (edge cases, keeping things maintainable, and solid architecture) needs serious human expertise.</p>

<p>Moreover, AI <a contenteditable="false" data-primary="AI-based coding tools" data-secondary="generating convincing but incorrect output" data-type="indexterm" id="id489"/>has a known tendency to generate convincing but incorrect output. It may introduce subtle bugs or “hallucinate” nonexistent functions and libraries. <a href="https://oreil.ly/hjv8f">Steve Yegge wryly likens</a> today’s LLMs to “wildly productive junior developers”—incredibly fast and enthusiastic but “potentially whacked out on mind-altering drugs,” prone to concocting crazy or unworkable approaches.</p>

<p>In <a href="https://oreil.ly/yPMPO">Yegge’s words</a>, an LLM can spew out code that looks polished at first glance, yet if a less-experienced developer naively says, “Looks good to me!” and runs with it, hilarity (or disaster) ensues in the following weeks. The AI doesn’t truly <em>understand</em> the problem; it stitches together patterns that <em>usually</em> make sense. Only a human can discern whether a seemingly fine solution hides long-term landmines. <a href="https://oreil.ly/sLzFY">Simon Willison echoed this</a> after seeing an AI propose a bewitchingly clever design that <em>only a senior engineer with deep understanding of the problem</em> could recognize as flawed. The lesson: AI’s confidence far exceeds its reliability.<a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="confidence far exceeding its reliability" data-type="indexterm" id="id490"/></p>

<p>Crucially, current AIs <a href="https://oreil.ly/HkwVF">do not create fundamentally new abstractions or strategies beyond their training data</a>. <a contenteditable="false" data-primary="AI-based coding tools" data-secondary="incapable of producing new abstractions or strategies" data-type="indexterm" id="id491"/>They won’t invent a novel algorithm or an innovative architecture for you—they remix what’s known. They also won’t take responsibility for decisions. As one engineer noted, “AIs don’t have ‘better ideas’ than what their training data contains. They don’t take responsibility for their work.”</p>

<p>All of this means that creative and analytical thinking—deciding <em>what</em> to build, <em>how</em> to structure it, and <em>why</em>—firmly remains a human domain. <a contenteditable="false" data-primary="human oversight of AI-assisted coding" data-type="indexterm" id="id492"/>In summary, AI is a force multiplier for developers, handling the repetitive 70% and giving us a “turbo boost” in productivity. But it is <em>not</em> a silver bullet that can replace human judgment. The remaining 30% of software engineering—the hard parts—still requires skills that only trained, thoughtful developers can bring. Those are the durable skills to focus on, and <a data-type="xref" href="ch04.html#ch04_beyond_the_70_maximizing_human_contribution_1752630043401362">Chapter 4</a> is dedicated to them. As one<a href="https://oreil.ly/QXYsj"> discussion</a> put it: “AI is a powerful tool, but it’s not a magic bullet.…Human judgment and good software engineering practices are still essential.”</p>

<section data-type="sect1" data-pdf-bookmark="How Developers Are Actually Using AI"><div class="sect1" id="ch03_how_developers_are_actually_using_ai_1752630043201065">
<h1>How Developers Are Actually Using AI</h1>

<p>I’ve observed <a contenteditable="false" data-primary="70% problem" data-primary-sortas="seventy" data-startref="ix_sevnty" data-type="indexterm" id="id493"/>two distinct patterns <a contenteditable="false" data-primary="developers" data-secondary="how they're actually using AI" data-type="indexterm" id="ix_devAI"/>in how teams are leveraging AI for development. Let’s call them the “bootstrappers” and the “iterators.” Both are helping engineers (and even nontechnical users) reduce the gap from idea to execution (or MVP).</p>

<p>First, there are the <em>bootstrappers</em>, who are generally taking a new project from zero to MVP. Tools like Bolt, v0, and screenshot-to-code AI are revolutionizing how these teams bootstrap new projects. These teams typically:</p>

<ul>
	<li>
	<p>Start with a design or rough concept</p>
	</li>
	<li>
	<p>Use AI to generate a complete initial codebase</p>
	</li>
	<li>
	<p>Get a working prototype in hours or days instead of weeks</p>
	</li>
	<li>
	<p>Focus on rapid validation and iteration</p>
	</li>
</ul>

<p>The results can be impressive. I recently watched a solo developer use Bolt to turn a Figma design into a working web app in next to no time. It wasn’t production-ready, but it was good enough to get very initial user feedback.</p>

<p>The second camp, the <em>iterators</em>, uses tools like Cursor, Cline, Copilot, and Windsurf for their daily development workflow. This is less flashy but potentially more transformative. These developers are:</p>

<ul>
	<li>
	<p>Using AI for code completion and suggestions</p>
	</li>
	<li>
	<p>Leveraging AI for complex refactoring tasks</p>
	</li>
	<li>
	<p>Generating tests and documentation</p>
	</li>
	<li>
	<p>Using AI as a “pair programmer” for problem solving</p>
	</li>
</ul>

<p>But here’s the catch: while both approaches can dramatically accelerate development, they come with hidden costs that aren’t immediately obvious.</p>

<p class="fix_tracking">When you watch a senior engineer work with AI tools like Cursor or Copilot, it looks like magic. They can scaffold entire features in minutes, complete with tests and documentation. But watch carefully, and you’ll notice something crucial: they’re not just accepting what the AI suggests. They’re constantly refactoring the generated code into smaller, focused modules. They’re adding comprehensive error handling and edge-case handling the AI missed, strengthening its type definitions and interfaces, and questioning its architectural decisions. In other words, they’re applying years of hard-won engineering wisdom to shape and constrain the AI’s output. The AI is accelerating their implementation, but their expertise is what keeps the code <span class="keep-together">maintainable</span>.</p>

<section data-type="sect2" data-pdf-bookmark="Common Failure Patterns"><div class="sect2" id="ch03_common_failure_patterns_1752630043201131">
<h2>Common Failure Patterns</h2>

<p>Junior engineers often miss these crucial steps.<a contenteditable="false" data-primary="developers" data-secondary="how they're actually using AI" data-tertiary="common failure patterns" data-type="indexterm" id="ix_devAIfail"/><a contenteditable="false" data-primary="failure patterns with use of AI-assisted coding" data-type="indexterm" id="id494"/>  They accept the AI’s output more readily, leading to what I call “house of cards code”—it looks complete but collapses under real-world pressure.</p>

<section data-type="sect3" data-pdf-bookmark="Two steps back"><div class="sect3" id="ch03_two_steps_back_1752630043201184">
<h3>Two steps back</h3>

<p>What typically happens next <a contenteditable="false" data-primary="failure patterns with use of AI-assisted coding" data-secondary="two steps back pattern" data-type="indexterm" id="id495"/>follows a predictable antipattern I call<a contenteditable="false" data-primary="two steps back antipattern" data-type="indexterm" id="id496"/> the “two steps back” pattern (shown in <a data-type="xref" href="#ch03_figure_1_1752630043194318">Figure 3-1</a>):</p>

<ul class="two-col">
	<li>
	<p>You try to fix a small bug.</p>
	</li>
	<li>
	<p>The AI suggests a change that seems <span class="keep-together">reasonable</span>.</p>
	</li>
	<li>
	<p>This fix breaks something else.</p>
	</li>
	<li>
	<p>You ask AI to fix the new issue.</p>
	</li>
	<li>
	<p>This creates two more problems.</p>
	</li>
	<li>
	<p>Rinse and repeat.</p>
	</li>
</ul>

<figure><div id="ch03_figure_1_1752630043194318" class="figure"><img src="assets/bevc_0301.png" width="972" height="645"/>
<h6><span class="label">Figure 3-1. </span>The “two steps back” antipattern.</h6>
</div></figure>

<p>This cycle is particularly painful for nonengineers because they lack the mental models to understand what’s actually going wrong. When an experienced developer encounters a bug, they can reason about potential causes and solutions based on years of pattern recognition. Without this background, you’re essentially playing whack-a-mole with code you don’t fully understand.<a contenteditable="false" data-primary="knowledge paradox" data-type="indexterm" id="id497"/> This is the “knowledge paradox” I mentioned back in this book’s preface: senior engineers and developers use AI to accelerate what they already know how to do, while juniors try to use it to learn <em>what</em> to do.</p>

<p>This cycle is particularly painful for nonengineers using AI in a “bootstrapper” pattern, because they lack the mental models needed to address these issues building their MVP. However, even experienced “iterators” can fall into this whack-a-mole trap if they overly rely on AI suggestions without deep validation.</p>

<p>There’s a deeper issue here: the <a contenteditable="false" data-primary="dependence on AI coding tools, problems caused by" data-type="indexterm" id="id498"/>very thing that makes AI coding tools accessible to nonengineers—their ability to handle complexity on your behalf—can actually impede learning. <a contenteditable="false" data-primary="debugging" data-secondary="lack of skill development in caused by AI coding" data-type="indexterm" id="id499"/>When code just “appears” without you understanding the underlying principles, you don’t develop debugging skills. You miss learning fundamental patterns. You can’t reason about architectural decisions, and so you struggle to maintain and evolve the code. This creates a dependency where you need to keep going back to the AI model to fix issues rather than developing the expertise to handle them yourself.<a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="autonomous AI coding agents" data-type="indexterm" id="id500"/><a contenteditable="false" data-primary="autonomous AI coding agents" data-type="indexterm" id="id501"/></p>

<p>This dependency risk enters a new dimension with the emergence of autonomous AI coding agents—a topic I explore in depth in <a data-type="xref" href="ch10.html#ch10_autonomous_background_coding_agents_1752630045087844">Chapter 10</a>.  Unlike current tools that suggest code snippets, these agents represent a fundamental shift in how software can be developed. As I write this, we’re witnessing the early deployment of systems that can independently plan, execute, and iterate on entire development tasks with minimal human oversight.</p>

<p>This evolution from assistive to autonomous AI introduces profound questions about developer expertise and control. When an AI system can handle complete development workflows, from initial implementation through testing and deployment, the risk of skill atrophy becomes acute. Developers who rely heavily on these agents without maintaining their foundational knowledge may find themselves unable to effectively audit, guide, or intervene when the AI’s decisions diverge from intended outcomes.</p>

<p>The challenge compounds when we consider how these autonomous systems make cascading decisions throughout a project. Each individual choice might appear reasonable in isolation, yet the cumulative effect could steer development in unintended directions. Without the expertise to recognize and correct these trajectory shifts early, teams risk building increasingly complex systems on foundations they don’t fully understand.</p>

<p>As we’ll examine more thoroughly later, the advent of autonomous coding agents doesn’t diminish the importance of software engineering fundamentals—it amplifies it. The more powerful our AI tools become, the more critical it is that we maintain the expertise to remain architects of our systems rather than mere operators. Only through deep understanding of software principles can we ensure these remarkable tools enhance our capabilities rather than erode them.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="The demo-quality trap"><div class="sect3" id="ch03_the_demo_quality_trap_1752630043201232">
<h3>The demo-quality trap</h3>

<p>It’s becoming a pattern: teams use AI to rapidly build impressive demos.<a contenteditable="false" data-primary="failure patterns with use of AI-assisted coding" data-secondary="demo-quality trap" data-type="indexterm" id="id502"/><a contenteditable="false" data-primary="demo-quality trap" data-type="indexterm" id="id503"/> The happy path works beautifully. <a contenteditable="false" data-primary="quality" data-secondary="demo-quality trap with AI coding" data-type="indexterm" id="id504"/>Investors and social networks are wowed. But when real users start clicking around? That’s when things fall apart.</p>

<p>I’ve seen this firsthand: error messages that make no sense to normal users, edge cases that crash the application, confusing UI states that never got cleaned up, accessibility completely overlooked, and performance issues on slower devices. These aren’t just low-priority bugs—they’re the difference between software people tolerate and software people love.</p>

<p>Creating truly self-serve software—the kind where users never need to contact support—requires a different mindset, one that’s all about the lost art of polish. You need to be obsessing over error messages; testing on slow connections and with real, nontechnical users; making features discoverable; and handling every edge case gracefully. This kind of attention to detail (perhaps) can’t be AI-generated. It comes from empathy, experience, and caring deeply about craft.<a contenteditable="false" data-primary="developers" data-secondary="how they're actually using AI" data-startref="ix_devAIfail" data-tertiary="common failure patterns" data-type="indexterm" id="id505"/></p>
</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="What Actually Works: Practical Workflow Patterns"><div class="sect2" id="ch03_what_actually_works_practical_workflow_patterns_1752630043201287">
<h2>What Actually Works: Practical Workflow Patterns</h2>

<p>Before we dive into coding in <a data-type="xref" href="part02.html#part02">Part II</a> of this book, we need to talk about modern development practices and how AI-assisted coding fits within a team workflow.<a contenteditable="false" data-primary="developers" data-secondary="how they're actually using AI" data-tertiary="practical workflow patterns, what actually works" data-type="indexterm" id="id506"/> Software development is more than writing code, after all—it’s a whole workflow that includes planning, collaboration, testing, deployment, and maintenance. And vibe coding isn’t a standalone novelty—it can be woven into agile methodologies and DevOps practices, augmenting the team’s productivity while preserving quality and reliability.<a contenteditable="false" data-primary="DevOps" data-secondary="vibe coding woven into" data-type="indexterm" id="id507"/></p>

<p>In this section, we’ll explore how team members can collectively use vibe-coding tools without stepping on each other’s toes, how to balance AI suggestions with human insight, and how continuous integration/continuous delivery (CI/CD) pipelines can incorporate AI or adapt to AI-generated code. I’ll also touch on important considerations like version-control strategies.</p>

<p>After observing dozens of teams, here are three patterns I’ve seen work consistently in both solo and team workflows:</p>

<dl>
	<dt>AI as first drafter</dt>
	<dd><p>The AI model generates the initial code and developers then refine, refactor, and test it<a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="as first drafter" data-secondary-sortas="first" data-type="indexterm" id="id508"/><a contenteditable="false" data-primary="first drafter, AI as" data-type="indexterm" id="id509"/></p>
	</dd>
	<dt>AI as pair programmer</dt>
	<dd><p>Developer and AI are in constant conversation, with tight feedback loops, frequent <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="as pair programmer" data-secondary-sortas="pair" data-type="indexterm" id="id510"/>code review, and<a contenteditable="false" data-primary="pair programmer, AI as" data-type="indexterm" id="id511"/> minimal context provided<a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="as validator" data-secondary-sortas="validator" data-type="indexterm" id="id512"/><a contenteditable="false" data-primary="validator, AI as" data-type="indexterm" id="id513"/></p>
	</dd>
	<dt>AI as validator</dt>
	<dd><p>Developers still write the initial code and then use AI to validate, test, and improve it (see <a data-type="xref" href="#ch03_figure_2_1752630043194352">Figure 3-2</a>)</p>
	</dd>
</dl>

<figure><div id="ch03_figure_2_1752630043194352" class="figure"><img src="assets/bevc_0302.png" width="736" height="757"/>
<h6><span class="label">Figure 3-2. </span>AI validation workflow: developers write initial code; AI systems analyze for bugs and security issues, then suggest improvements; and developers review and apply recommended changes.</h6>
</div></figure>

<p>In this section, I’ll walk you through each pattern in turn, discussing workflows and tips for success.</p>

<section data-type="sect3" data-pdf-bookmark="AI as first drafter"><div class="sect3" id="ch03_ai_as_first_drafter_1752630043201340">
<h3>AI as first drafter</h3>

<p>It’s important to ensure everyone on the team is on the same page before you ask your AI model to draft any code.<a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="as first drafter" data-secondary-sortas="first" data-type="indexterm" id="id514"/><a contenteditable="false" data-primary="first drafter, AI as" data-type="indexterm" id="id515"/> Communication is key so that developers don’t ask their AI assistants to do redundant tasks or generate conflicting implementations.</p>

<p>In daily stand-ups (a staple of agile workflows), it’s worth discussing not just what you’re working on but also whether you plan to use AI for certain tasks. For example, two developers might be working on different features that both involve a utility function for date formatting.<a contenteditable="false" data-primary="date formatting, using AI for" data-type="indexterm" id="id516"/> If both ask the AI to create a <code>formatDate</code> helper, you might end up with two similar functions. Coordinating up front (“I’ll generate a date utility we can both use”) can prevent duplication.</p>

<p>Teams that successfully integrate AI tools often start by agreeing on coding standards and prompting practices. For example, the team might decide on a consistent style (linting rules, project conventions) and even feed those guidelines into their AI tools (some assistants allow providing style preferences or example code to steer outputs). As <a href="https://oreil.ly/FeEN_">noted in Codacy’s blog</a>, by familiarizing the AI with the team’s coding standards, you get generated code that is more uniform and easier for everyone to work with. On a practical level, this could mean having a section in your project README for “AI Usage Tips,” where you note things like “We use functional components only” or “Prefer using Fetch API over Axios,” which developers can keep in mind when prompting AI.</p>

<p>Another practice is to use your tools’ <em>collaboration features</em>, if available. <a contenteditable="false" data-primary="collaboration features of tools, using" data-type="indexterm" id="id517"/>Some AI-assisted IDEs allow users to share their AI sessions or at least the prompts they use. If Developer A got a great result with a prompt for a complex component, sharing that prompt with Developer B (perhaps via the issue tracker or a team chat) can save time and ensure consistency.</p>

<p>As for using version control, the fundamentals remain—with a twist. <a contenteditable="false" data-primary="version control" data-secondary="more essential with AI-generated code" data-type="indexterm" id="id518"/>Using Git (or another version control system) is nonnegotiable in modern development, and that doesn’t change with vibe coding. In fact, version control becomes even more crucial when AI is generating code rapidly. Commits act as the safety net to catch AI missteps; if an AI-generated change breaks something, you can revert to a previous <span class="keep-together">commit</span>.</p>

<p>One strategy is to commit more frequently when using AI assistance. Each time the AI produces a significant chunk of code (like generating a feature or doing some major refactoring) that you accept, consider making a commit with a clear message. Frequent commits ensure that if you need to bisect issues or undo a portion of AI-introduced code, the history is granular enough.</p>

<p>Also, try to isolate different AI-introduced changes. If you let the AI make many changes across different areas and commit them all together, it’s harder to disentangle if something goes wrong. For example, if you use an agent to optimize performance and it also tweaks some UI texts, commit those separately. (Your two commit messages might be “Optimize list rendering performance [AI-assisted]” and “Update UI copy for workout completion message [AI-assisted]”). Descriptive commit messages are important; some teams even tag commits that had heavy AI involvement, just for traceability. It’s not about blame but about understanding the origin of code—a commit tagged with “[AI]” might signal to a reviewer that the code could use an extra thorough review for edge cases.</p>

<p>Essentially, the team should treat AI usage as a normal part of the development conversation: share experiences, successful techniques, and warnings about what not to do (like “Copilot suggests using an outdated library for X, so be careful with that”).</p>

<p>Review and refinement are crucial to this pattern. Developers should manually review and refactor the code for modularity, add comprehensive error handling, write thorough tests, and document key decisions as they refine the code. The next chapter goes into detail about these processes.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="AI as pair programmer"><div class="sect3" id="ch03_ai_as_pair_programmer_1752630043201387">
<h3>AI as pair programmer</h3>

<p>Traditional pair programming involves two humans collaborating at one workstation.<a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="as pair programmer" data-secondary-sortas="pair" data-type="indexterm" id="id519"/><a contenteditable="false" data-primary="pair programmer, AI as" data-type="indexterm" id="id520"/><a contenteditable="false" data-primary="human-AI pair programming" data-type="indexterm" id="id521"/> With the advent of AI, a hybrid approach has emerged: one human developer <span class="keep-together">working</span> alongside an AI assistant. This setup can be particularly effective, offering a blend of human intuition and machine efficiency.</p>

<p>In a human-AI pairing, the developer interacts with the AI to generate code suggestions while also reviewing and refining the output. This dynamic allows the human to leverage the AI’s speed in handling repetitive tasks, such as writing boilerplate code or generating test cases, while maintaining oversight to ensure code quality and relevance.</p>

<p>For instance, when integrating a new library, a developer might prompt the AI to draft the initial integration code. The developer then reviews the AI’s suggestions, cross-referencing with official documentation to verify accuracy. This process not only accelerates development but also facilitates knowledge acquisition, as the developer engages deeply with both the AI’s output and the library’s intricacies. </p>

<p>Let’s compare this to traditional<a contenteditable="false" data-primary="human-human pair programming" data-secondary="versus human-AI pair programming" data-secondary-sortas="human-AI" data-type="indexterm" id="id522"/><a contenteditable="false" data-primary="pair programming" data-secondary="human-human versus human-AI" data-type="indexterm" id="id523"/> human-human pair programming:</p>

<ul>
	<li>
	<p><em>Human-AI pairing</em> offers rapid <a contenteditable="false" data-primary="human-AI pair programming" data-secondary="versus human-human pair programming" data-seealso="human-human" data-type="indexterm" id="id524"/>code generation and can handle mundane tasks efficiently. It’s particularly beneficial for solo developers or when team resources are limited.</p>
	</li>
	<li>
	<p><em>Human-human pairing</em> excels in complex problem-solving scenarios, where nuanced understanding and collaborative brainstorming are essential. It fosters shared ownership and collective code comprehension.</p>
	</li>
</ul>

<p>Both approaches have their merits, and your choice between them can be guided by the project’s complexity, resource availability, and the specific goals of the development process.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Best practices for AI pair programming"><div class="sect3" id="ch03_best_practices_for_ai_pair_programming_1752630043201451">
<h3>Best practices for AI pair programming</h3>

<p class="fix_tracking">To maximize the<a contenteditable="false" data-primary="pair programmer, AI as" data-secondary="best practices for AI pair programming" data-type="indexterm" id="id525"/> benefits <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="as pair programmer" data-secondary-sortas="pair" data-tertiary="best practices for" data-type="indexterm" id="id526"/>of AI-assisted development, consider the following practices:</p>

<dl>
	<dt>Initiate new AI sessions for distinct tasks</dt>
	<dd>
	<p>This helps maintain context clarity and ensures the AI’s suggestions are relevant to the specific task at hand.</p>
	</dd>
	<dt>Keep prompts focused and concise</dt>
	<dd>
	<p>Providing clear and specific instructions enhances the quality of the AI’s output.</p>
	</dd>
	<dt>Review and commit changes frequently</dt>
	<dd>
	<p>Regularly integrating and testing AI-generated code helps catch issues early and maintains project momentum.</p>
	</dd>
	<dt>Maintain tight feedback loops</dt>
	<dd>
	<p>Continuously assess the AI’s contributions, providing corrections or refinements as needed to guide its learning and improve future suggestions.</p>
	</dd>
</dl>
</div></section>

<section data-type="sect3" data-pdf-bookmark="AI as validator"><div class="sect3" id="ch03_ai_as_validator_1752630043201500">
<h3>AI as validator</h3>

<p>Beyond code generation, AI can serve as a valuable validator, assisting in code review and quality assurance. <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="as validator" data-secondary-sortas="validator" data-type="indexterm" id="id527"/><a contenteditable="false" data-primary="validator, AI as" data-type="indexterm" id="id528"/>AI tools can analyze code for potential bugs, security vulnerabilities, and adherence to best practices. For example, platforms like DeepCode and Snyk’s AI-powered code checker can identify issues such as missing input sanitization or insecure configurations, providing actionable insights directly within the development environment.<a contenteditable="false" data-primary="testing" data-secondary="AI tools assisting in" data-type="indexterm" id="id529"/> Platforms such as Qodo and TestGPT can automatically generate test cases, ensuring broader coverage and reducing manual effort. And many AI tools can assist in monitoring application performance, detecting anomalies that might indicate underlying issues.</p>

<p>By integrating AI validators into the development workflow, teams can enhance code quality, reduce the likelihood of defects, and ensure compliance with security standards. This proactive approach to validation complements human oversight, leading to more robust and reliable software. <a contenteditable="false" data-primary="quality assurance" data-secondary="AI tools enhancing efficiency and effectiveness of" data-type="indexterm" id="id530"/>These tools enhance the efficiency and effectiveness of the quality assurance (QA) process by handling repetitive and time-consuming tasks, allowing human testers to focus on more complex and nuanced aspects of QA.</p>

<p>Incorporating AI into the development process, whether as a pair programmer or validator, offers opportunities to enhance productivity and code quality. By thoughtfully integrating these tools, developers can harness the strengths of both human and artificial intelligence.</p>

<p>To maximize the benefits<a contenteditable="false" data-primary="quality assurance" data-secondary="maximizing benefits of AI and human capabilities in" data-type="indexterm" id="id531"/> of both AI and human capabilities in QA, I recommend a few best practices:</p>

<ul>
	<li>
	<p>Use AI for initial assessments and preliminary scans to identify obvious issues.</p>
	</li>
	<li>
	<p>Prioritize human review for critical areas, such as complex functionalities, user experience, and AI limitations.</p>
	</li>
	<li>
	<p>Foster an environment of continuous collaboration, where AI tools and human testers work in tandem, with ongoing feedback loops to improve both AI performance and human decision making.<a contenteditable="false" data-primary="developers" data-secondary="how they're actually using AI" data-startref="ix_devAI" data-type="indexterm" id="id532"/></p>
	</li>
</ul>
</div></section>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="The Golden Rules of Vibe Coding"><div class="sect1" id="ch03_the_golden_rules_of_vibe_coding_1752630043201549">
<h1>The Golden Rules of Vibe Coding</h1>

<p>While vibe coding offers unprecedented speed and creative <a contenteditable="false" data-primary="vibe coding" data-secondary="golden rules of" data-type="indexterm" id="ix_vbcdrule"/>freedom in software development, its very flexibility demands a structured approach to ensure consistent quality and team cohesion. The rapid, intuitive nature of AI-assisted development can quickly lead to chaos without clear guidelines that balance creative exploration with engineering discipline.</p>

<p>These golden rules emerged from collective experience across teams who have successfully integrated vibe coding into their workflows. They represent hard-won insights about where AI excels, where it stumbles, and how human judgment remains essential throughout the process. Rather than constraining creativity, these principles create a framework within which teams can confidently experiment while maintaining the standards necessary for production-ready software.</p>

<p>The rules address three critical dimensions of vibe coding: the interaction between human and AI, the integration of AI-generated code into existing systems, and the cultivation of team practices that support sustainable AI-assisted development. By following these guidelines, teams can harness the transformative power of vibe coding while avoiding common pitfalls that lead to technical debt, security vulnerabilities, or unmaintainable codebases:</p>

<dl>
	<dt>Be specific and clear about what you want</dt>
	<dd>
	<p>Clearly articulate your requirements, tasks, and outcomes when interacting with AI. Precise prompts yield precise results.</p>
	</dd>
	<dt>Always validate AI output against your intent</dt>
	<dd>
	<p>AI-generated code must always be checked against your original goal. Verify functionality, logic, and relevance before accepting.</p>
	</dd>
	<dt>Treat AI as a junior developer (with supervision)</dt>
	<dd>
	<p>Consider AI outputs as drafts that require your careful oversight. Provide feedback, refine, and ensure quality and correctness.</p>
	</dd>
	<dt>Use AI to expand your capabilities, not replace your thinking</dt>
	<dd>
	<p>Leverage AI to automate routine or complex tasks, but always remain actively engaged in problem solving and decision making.</p>
	</dd>
	<dt>Coordinate up front among the team before generating code</dt>
	<dd>
	<p>Align with your team on AI usage standards, code expectations, and practices before starting AI-driven development.</p>
	</dd>
	<dt>Treat AI usage as a normal part of the development conversation</dt>
	<dd>
	<p>Regularly discuss AI experiences, techniques, successes, and pitfalls with your team. Normalize AI as another tool for collective improvement.</p>
	</dd>
	<dt>Isolate AI changes in Git by doing separate commits</dt>
	<dd>
	<p>Clearly identify and separate AI-generated changes within version control to simplify reviews, rollbacks, and tracking.</p>
	</dd>
	<dt>Ensure that all code, whether human or AI-written, undergoes code review</dt>
	<dd>
	<p>Maintain consistent standards by subjecting all contributions to the same rigorous review processes, enhancing code quality and team understanding.</p>
	</dd>
	<dt>Don’t merge code you don’t understand</dt>
	<dd>
	<p class="fix_tracking">Never integrate AI-generated code unless you thoroughly comprehend its functionality and implications. Understanding is critical to maintainability and <span class="keep-together">security</span>.</p>
	</dd>
	<dt>Prioritize documentation, comments, and ADRs</dt>
	<dd>
	<p>Clearly document the rationale, functionality, and context for AI-generated code. Good documentation ensures long-term clarity and reduces future technical debt.</p>
	</dd>
	<dt>Share and reuse effective prompts</dt>
	<dd>
	<p>Document prompts that lead to high-quality AI outputs. Maintain a repository of proven prompts to streamline future interactions and enhance consistency.</p>
	</dd>
	<dt>Regularly reflect and iterate</dt>
	<dd>
	<p>Periodically review and refine your AI development workflow. Use insights from past experiences to continuously enhance your team’s approach.</p>
	</dd>
</dl>

<p>By adhering to these golden rules, your team can harness AI effectively, enhancing productivity while maintaining clarity, quality, and control.<a contenteditable="false" data-primary="vibe coding" data-secondary="golden rules of" data-startref="ix_vbcdrule" data-type="indexterm" id="id533"/></p>
</div></section>
<section data-typpe="sect1" data-pdf-bookmark="0. Summary and Next Steps"><div class="section" id="id243">
<h1>Summary and Next Steps</h1>
 <p>The 70% problem defines the current state of AI-assisted development: these tools excel at generating boilerplate and routine functions but struggle with the final 30% that includes edge cases, architectural decisions, and production readiness. We’ve identified two main usage patterns—bootstrappers who rapidly build MVPs, and iterators who integrate AI into daily workflows—along with common failure patterns like the “two steps back” antipattern and the “demo-quality trap” where impressive prototypes fail under real-world pressure.</p>
 <p>Three proven workflow patterns have emerged: AI as first drafter (generate then refine), AI as pair programmer (continuous collaboration), and AI as validator (human-written code with AI analysis). The golden rules of vibe coding provide essential guardrails, emphasizing clear communication, thorough validation, team coordination, and the nonnegotiable requirement to understand all code before merging it.</p>
 <p>Individual developers should choose one workflow pattern to experiment with systematically while implementing the golden rules in daily practice. Focus on developing the durable skills covered in <a data-type="xref" href="ch04.html#ch04_beyond_the_70_maximizing_human_contribution_1752630043401362">Chapter 4</a>: system design, debugging, and architecture—rather than competing with AI on code generation.
</p>
 <p>Teams need to establish standards for AI usage, create shared repositories of effective prompts, and integrate AI considerations into existing agile practices. Regular knowledge sharing about successes and pitfalls will help teams avoid common traps while maximizing AI’s benefits.</p>
 <p>As autonomous AI coding agents emerge, the human role will shift toward architectural oversight and strategic decision making. The next chapter explores how to maximize this irreplaceable human contribution, helping engineers at every level thrive as partners to increasingly capable AI systems rather than competitors.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id486"><sup><a href="ch03.html#id486-marker">1</a></sup> This chapter is based on an essay originally published on my Substack newsletter. See Addy Osmani, <a href="https://oreil.ly/aRKIJ">“The 70% Problem: Hard Truths About AI-Assisted Coding”</a>, <em>Elevate with Addy Osmani</em>, December 4, 2024.</p></div></div></section></div></div></body></html>