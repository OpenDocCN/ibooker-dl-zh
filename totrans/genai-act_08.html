<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">7</span> </span> <span class="chapter-title-text">Retrieval-augmented generation: The secret weapon </span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Concepts of retrieval-augmented generation</li> 
    <li class="readable-text" id="p3">Benefits of the RAG architecture in conjunction with large language models</li> 
    <li class="readable-text" id="p4">Understanding the role of vector databases and indexes in implementing RAG</li> 
    <li class="readable-text" id="p5">Basics of vector search and understanding the distance functions</li> 
    <li class="readable-text" id="p6">Challenges in RAG implementation and potential solutions</li> 
    <li class="readable-text" id="p7">Different methods of chunking text for RAG</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>As we have seen, large language models (LLMs) are very powerful and help us achieve things that were not possible until very recently. Interestingly, LLMs capture the world’s knowledge and are available to anyone at the end of an API, anywhere in the world.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>However, LLMs have a knowledge constraint: their understanding and knowledge extend up to their last training cut-off; after that date, they do not have any new information. Consequently, LLMs cannot utilize the latest information. In addition, the training corpus of LLMs does not contain any private nonpublic knowledge. Therefore, LLMs cannot operate and answer specific and proprietary questions to enterprises.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>One practical way to solve this problem is by using a pattern called retrieval-augmented generation (RAG). This chapter will explore using RAG to enhance LLMs with your data. You will learn what RAG is, why it is useful for enterprise applications, and how to implement it using vector databases and indexes. Finally, the chapter will discuss some chunking strategies to optimize the relevance and efficiency of RAG. </p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>In this chapter, we will start by understanding RAG. In the next chapter, we will build on that by combining all the concepts for an end-to-end sample.</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_117"><span class="num-string">7.1</span> What is RAG?</h2> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>RAG is a method that combines additional data with a language model’s input to improve its output without altering the initial prompt. This supplemental data can come from an organization’s database or an external, updated source. The language model then processes the merged information to include factual data from the knowledge base in its response. This technique is particularly useful when the latest data and its integration into your information are required.</p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>In technical terms, RAG merges a pretrained language model and an external knowledge index to enhance language generation. Facebook AI Research first introduced RAG in a study titled “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks” [1]. It demonstrated that RAG models can achieve state-of-the-art results on various knowledge-intensive tasks in natural language processing (NLP), such as open-domain question answering, fact verification, and natural language inference. It also proved that RAG models can generate more precise, diverse, and factual language than a leading language model that doesn’t use additional data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p15"> 
   <p>The RAG model combines the powers of a dense passage retriever and a sequence-to-sequence model to generate informative answers based on a large corpus of text. It was designed to improve question-answering systems, fact verification, and question-generation tasks by integrating information retrieval with generative language models. </p> 
  </div> 
  <div class="readable-text intended-text" id="p16"> 
   <p>Figure 7.1 shows an overview of the RAG pattern and the overall approach. At a high level, there are two components: the retriever and the generator. As the name<span class="aframe-location"/> suggests, the retriever is responsible for retrieving the information, and the generator is the LLM, used to generate the text.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p17">  
   <img alt="figure" src="../Images/CH07_F01_Bahree.png" width="594" height="297"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.1</span> RAG architecture overview</h5>
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>Foundational models, notably LLMs such as the OpenAI GPT series, possess immense potential but do have drawbacks. These models, while powerful, suffer from a static knowledge base, meaning they are unaware of events or developments posttraining, causing them to become outdated over time. They are also heavily influenced by their training data, and any bias, misinformation, or imbalance in this data can taint the model’s output. Furthermore, LLMs lack a genuine understanding of the content, often generating text based solely on patterns observed during training without comprehension. This can be problematic in corporate scenarios with specific policies and rules. Finally, these models can create plausible yet factually incorrect information, which can propagate misinformation without a reliable verification method.</p> 
  </div> 
  <div class="readable-text intended-text" id="p19"> 
   <p>RAG helps improve the quality of responses by drawing on these external sources of knowledge to supplement the LLM’s internal information. This is especially helpful in addressing the static knowledge of LLMs where they cannot provide accurate generations for events or facts that happened after their training cutoff dates.</p> 
  </div> 
  <div class="readable-text intended-text" id="p20"> 
   <p>RAG is an essential component of working with LLMs, along with prompt engineering. By accessing a broader variety of information, RAG can produce more accurate and informative answers. It ensures that the model relies on the most up-to-date, dependable facts and that users can see its sources, ensuring that its statements can be verified for correctness and ultimately trusted.</p> 
  </div> 
  <div class="readable-text" id="p21"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_118"><span class="num-string">7.2</span> RAG benefits</h2> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>While RAG is still in its early stages of development, it holds the potential to transform the landscape of text generation models. RAG can be harnessed to produce more comprehensive, varied, and factual text generation models for many applications. This section delves into the myriad of benefits that enterprises can gain.</p> 
  </div> 
  <div class="readable-text intended-text" id="p23"> 
   <p>RAG’s ability to draw data from external resources in real-time is a game changer for sectors that require up-to-the-minute data, such as finance, healthcare, or news. Whether tracking market dynamics, updating healthcare records, or breaking news, RAG guarantees the inclusion of the latest information. This ensures that the output is consistently relevant and current.</p> 
  </div> 
  <div class="readable-text intended-text" id="p24"> 
   <p>Compared to traditional ML techniques, RAG offers a cost-effective alternative for businesses. Traditional techniques may necessitate retraining a model each time new data is added. However, with RAG, businesses only need to update the external dataset, saving time and costs related to model training and data processing.</p> 
  </div> 
  <div class="readable-text intended-text" id="p25"> 
   <p>RAG proves particularly useful when responses need to cite data or display source references. It can anchor the generated data in the source material and even provide citations. This is of immense value in academic, legal, or professional scenarios where precise sourcing of information is required.</p> 
  </div> 
  <div class="readable-text intended-text" id="p26"> 
   <p>RAG’s versatility extends to the types of data it can process, accommodating structured and unstructured data in various formats. This adaptability allows RAG to be utilized in diverse applications, from analyzing intricate datasets to processing and generating multimedia content.</p> 
  </div> 
  <div class="readable-text intended-text" id="p27"> 
   <p>Implementing RAG enhances customer interactions and facilitates improved decision-making. In customer service or chatbot applications, RAG can retrieve detailed information from databases or FAQs, which results in more accurate and constructive responses. Furthermore, RAG can combine insights from large datasets with language model generation in decision support systems to offer comprehensive and informed recommendations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>RAG’s scalability and performance are exceptional, enabling businesses to utilize vast external datasets without overburdening the language model. This allows generating outputs based on a wide array of information without compromising the model’s performance or efficiency.</p> 
  </div> 
  <div class="readable-text intended-text" id="p29"> 
   <p>RAG also allows customizing of external datasets based on a business domain. For instance, a pharmaceutical company could maintain a dataset solely for new drug research, allowing RAG to offer domain-specific responses. From a research and development perspective, sectors such as biotechnology or technology can greatly benefit from RAG’s ability to retrieve relevant literature or data insights, speeding up the innovation process.</p> 
  </div> 
  <div class="readable-text intended-text" id="p30"> 
   <p>RAG offers a dynamic, efficient, and versatile solution for integrating external datasets into language models. This feature results in more accurate, relevant, and current information in automated systems, enhancing efficiency, customer satisfaction, and decision-making.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p31"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">What is data grounding?</h5> 
   </div> 
   <div class="readable-text" id="p32"> 
    <p>Grounding your data means connecting LLMs with external information sources. Grounding can be done using various methods; however, RAG is a common one. Usually, these external data sources are chosen based on the use case needs, enhancing the quality and dependability of the generated output. Grounding can make the generated output better by giving LLMs information that is use-case specific, relevant, and not included in the LLM’s training data. This way, the LLMs can use the data from external sources as context and generate more precise and relevant answers for the user.</p> 
   </div> 
   <div class="readable-text" id="p33"> 
    <p>Some of the benefits of grounding are</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p34"> It can help the LLMs produce more factual and reliable output, as it reduces the risk of hallucination, which is when the LLMs invent false or misleading information in their output. </li> 
    <li class="readable-text" id="p35"> It can help the LLMs produce more diverse and representative output, allowing them to access information from various sources and perspectives and avoid biases or errors in their internal knowledge. </li> 
    <li class="readable-text" id="p36"> It can help the LLMs produce more customized and personalized output, enabling them to adapt to the user’s preferences, needs, and goals and provide tailored solutions or suggestions. </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>RAG models can utilize the vast amount of information stored in text corpora to enrich their outputs with relevant facts and details. They can also handle open-domain questions and tasks that require reasoning and inference beyond the scope of LLMs. Let’s explore the RAG architecture in more detail.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_119"><span class="num-string">7.3</span> RAG architecture</h2> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>It was outlined earlier that the RAG architecture consists of two main components: the retriever and the LLM. The retriever extracts data from different enterprise systems, as illustrated in figure 7.2 [1]. These components can be adapted and adjusted based on the application and task at hand, and together, they give the RAG model a lot of flexibility and strength.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>The retriever can access information from private knowledge sources and search engines. This is the mechanism behind Bing Chat, which helps provide more current information. This retriever does more than search—it filters out only the relevant information, which becomes the context for the generative model.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p41">  
   <img alt="figure" src="../Images/CH07_F02_Bahree.png" width="1015" height="294"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.2</span> Overview of RAG for knowledge-intensive NLP tasks [1]</h5>
  </div> 
  <div class="readable-text" id="p42"> 
   <p>The RAG pattern combines information retrieval and text generation to enhance language model outputs. The query encoder initially encodes an input question or statement into a vector. This vector, <code>q(x)</code>, is then utilized by a nonparametric retriever to sift through a precompiled document index, seeking documents relevant to the query.</p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>The retriever employs maximum inner product search (MIPS), which identifies documents with the highest similarity to the query vector. These documents are pre-encoded into vectors, represented as <code>d(z)</code>, in the document index.</p> 
  </div> 
  <div class="readable-text intended-text" id="p44"> 
   <p>The generator (i.e., the LLM) utilizes the information from the retrieved documents to produce human-like text. This architecture component is responsible for answering questions, verifying facts, or generating new questions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p45"> 
   <p>The final process is marginalization, where instead of relying on a single document to generate a response, the RAG model considers all pertinent documents. It calculates the overall probability of each possible answer by summing up the probabilities based on each retrieved document, which ensures a more comprehensive and contextual awareness by integrating a wide array of retrieved information into the final text generation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p46"> 
   <p>The other key component is the LLM, which takes the context from the retrieval model and generates a natural language output. The generative model also provides feedback to the retrieval model to improve its accuracy over time. This is done using prompt engineering, as we saw in the previous chapter.</p> 
  </div> 
  <div class="readable-text" id="p47"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_120"><span class="num-string">7.4</span> Retriever system</h2> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>The retriever is essentially the component that searches various knowledge sources, as shown in figure 7.2. Its main purpose is to search through the corpus of information and find the relevant information that can be used. The retrieved information is then provided to the generator model, which uses it to generate its output.</p> 
  </div> 
  <div class="readable-text intended-text" id="p49"> 
   <p>Two main types of retriever systems are used in RAG: sparse and dense. Sparse retrievers are traditional retrieval systems that use traditional search techniques, such as term frequency-inverse document frequency (TF-IDF), to match queries to documents. Dense retrievers are newer retrieval systems that use machine learning to encode queries and documents into vectors and then match queries to documents based on the similarity of their vectors.</p> 
  </div> 
  <div class="readable-text intended-text" id="p50"> 
   <p>Choosing the right type of retrieval system in a RAG architecture (sparse or dense) is critical because it fundamentally affects the model’s performance and applicability. Sparse retrievers, such as those using TF-IDF, are fast and efficient, using inverted indexes to match queries with documents based on keyword overlap. This makes them suitable for large-scale, keyword-dependent search tasks with limited computational resources. However, they might struggle with the subtleties of language, such as synonyms and nuanced phrasing.</p> 
  </div> 
  <div class="readable-text intended-text" id="p51"> 
   <p>In contrast, dense retrievers utilize machine learning techniques to encode queries and documents into vectors, capturing deeper semantic relationships beyond mere keyword matching. This allows them to handle more complex queries and understand context better, which is particularly beneficial for queries with ambiguous or specialized language. While dense retrievers often yield more relevant and contextually appropriate documents, they are more computationally intensive and require substantial amounts of training data, making them resource-heavy both in the training phase and during inference. </p> 
  </div> 
  <div class="readable-text intended-text" id="p52"> 
   <p>The choice between sparse and dense retrievers should be guided by the task’s specific needs, considering the nature of the queries, domain specificity, resource availability, and the necessity of nuanced language understanding.</p> 
  </div> 
  <div class="readable-text intended-text" id="p53"> 
   <p>The choice of retriever affects the balance between computational efficiency and depth of understanding. Despite their computational costs, dense retrievers are often preferred for tasks requiring a nuanced understanding of language. However, sparse retrievers may still be viable for applications where speed and efficiency are paramount, or where queries are expected to match document text closely. The best retriever for a given application will depend on its specific requirements and the resources available for implementing and maintaining the system.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p54"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">What are BM25, TF-IDF, and DPR?</h5> 
   </div> 
   <div class="readable-text" id="p55"> 
    <p>BM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is one of the most widely used ranking functions in information retrieval. BM25 considers many factors, including the term frequency (TF) of the query terms in the document, the inverse document frequency (IDF) of the query terms, and the length of the document.</p> 
   </div> 
   <div class="readable-text" id="p56"> 
    <p>TF-IDF is a statistical measure used to evaluate how important a word is to a document in a collection of documents. The TF-IDF value increases proportionally to the number of times a word appears in a document. It decreases proportionally to the number of documents in the collection that contain the word. TF-IDF is often used in information retrieval and text mining to rank documents based on their relevance to a given query.</p> 
   </div> 
   <div class="readable-text" id="p57"> 
    <p>DPR is a neural network model that retrieves relevant passages from a large text corpus. It is trained on a massive dataset of text and code and learns to embed passages and queries into a dense vector space. DPR can retrieve passages semantically, similar to the query, by calculating the cosine similarity between the passage and query vectors.</p> 
   </div> 
   <div class="readable-text" id="p58"> 
    <p>BM25 and TF-IDF are statistical measures of a document’s relevance to a given query. However, BM25 considers additional factors, such as the length of the document and the saturation of term frequency. DPR can be used to improve the performance of BM25 and TF-IDF ranking functions.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>At a high level, we need to follow the process outlined in figure 7.3 to harness the power of LLMs on our data. The source pulled by the retriever would need to be split into smaller sizes. This is required to make the information more manageable and conform to the context windows of the LLMs. Next, we must create embeddings of these smaller chunks and link them to the source as metadata. Finally, these embeddings and associated metadata should be persisted in a data store.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p60">  
   <img alt="figure" src="../Images/CH07_F03_Bahree.png" width="758" height="213"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.3</span> Custom data on LLMs</h5>
  </div> 
  <div class="readable-text" id="p61"> 
   <p>For RAG to be efficient and scalable, the retriever component must quickly fetch the most relevant documents from potentially billions of candidates. We need two components to help address this challenge: a vector database and an index. A vector database is a system that stores and provides access to structured or unstructured data (e.g., text or images) alongside its vector embeddings, which are the data’s numerical representation. A vector index is a data structure that enables efficient and fast lookup of nearest neighbors in the high-dimensional vector space.</p> 
  </div> 
  <div class="readable-text intended-text" id="p62"> 
   <p>Without efficient vector databases and indexes, the retrieval step would become a bottleneck, making the entire RAG system slow and impractical. Using these tools, relevant documents can be retrieved in real time, allowing the generator component to produce answers quickly and making the system usable for applications such as open-domain question answering. Let’s explore both in more detail.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_121"><span class="num-string">7.5</span> Understanding vector databases</h2> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>Vector databases enable enterprises to manage, secure, and scale embeddings in a production environment. For many enterprises, vector databases for semantic search use cases solve the performance and security requirements needed for production systems.</p> 
  </div> 
  <div class="readable-text intended-text" id="p65"> 
   <p>A vector database is specifically designed to operate on embedding vectors. As the popularity of LLMs and generative AI has grown recently, so has the use of embeddings to encode unstructured data. Vector databases have emerged as an effective solution for enterprises to deliver and scale these use cases.</p> 
  </div> 
  <div class="readable-text intended-text" id="p66"> 
   <p>Vector databases are specialized databases that store data as high-dimensional vectors and their original content. They offer the capabilities of both vector indexes and traditional databases, such as optimized storage, scalability, flexibility, and query language support. They allow users to find and retrieve similar or relevant data based on their semantic or contextual meaning.</p> 
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>Given the vast number of documents in large corpora, brute-force comparison of a query vector with every document vector is computationally prohibitive. The solution is vector search, which comprises indexes and databases that allow efficient storage and near-neighbor lookups in high-dimensional spaces. Figure 7.4 shows a typical pipeline of incorporating a vector database when implementing a RAG pattern with LLMs.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p68">  
   <img alt="figure" src="../Images/CH07_F04_Bahree.png" width="726" height="106"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.4</span> Typical pipeline for vector database</h5>
  </div> 
  <div class="readable-text" id="p69"> 
   <p>Vector databases can help RAG models quickly find the most similar documents or passages to a given query and use them as additional context for the LLM. Depending on the trade-off between speed and accuracy, vector databases can also support various retrieval strategies, such as exact, approximate, or hybrid methods. Having a vector database is a good start, but finding the most similar documents or passages can only happen with a vector index.</p> 
  </div> 
  <div class="readable-text" id="p70"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_122"><span class="num-string">7.5.1</span> What is a vector index?</h3> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>A vector index is a data structure in a vector database designed to enhance the efficiency of processing, and it is particularly suited for the high-dimensional vector data encountered with LLMs. Its function is to streamline the search and retrieval processes within the database. By implementing a vector index, the system is capable of conducting quick similarity searches, identifying vectors that closely match or are most similar to a given input vector. Essentially, vector indexes are designed to enable rapid and precise similarity search, facilitating the recovery of vector embeddings.</p> 
  </div> 
  <div class="readable-text intended-text" id="p72"> 
   <p>They organize the vectors using various techniques, such as hashing, clustering, or tree-based methods, to make finding the most similar ones easy based on their distance or similarity metrics. For example, FAISS (Facebook AI Similarity Search) is a popular vector index that efficiently handles billions of vectors.</p> 
  </div> 
  <div class="readable-text intended-text" id="p73"> 
   <p>To create vector indexes for your embeddings, there are many options, such as exact or approximate nearest neighbor algorithms (e.g., HNSW or IVF), different distance metrics (e.g., cosine or Euclidean), or various compression techniques (e.g., quantization or pruning). Your index method depends on balancing speed, accuracy, and memory consumption. We can use different mathematical methods to compare how similar two vector embeddings are—these are useful when searching and matching different embeddings. Let’s see what vector search means and how we can apply different mathematical functions when searching.</p> 
  </div> 
  <div class="readable-text" id="p74"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_123"><span class="num-string">7.5.2</span> Vector search </h3> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>A vector search is a query operation that finds the vectors most similar to a given query vector based on a similarity metric. In a RAG pattern for LLMs, a vector index stores the documents’ embeddings or passages that the LLM can retrieve as context for generating responses. A vector search is used to find the most relevant documents or passages to the query based on the similarity between the query vector and the document vectors in the index.</p> 
  </div> 
  <div class="readable-text intended-text" id="p76"> 
   <p>Similarity measures are mathematical methods that compare two vectors and compute a distance value between them. This distance value indicates how dissimilar or similar the two vectors are in terms of their semantic meaning.</p> 
  </div> 
  <div class="readable-text intended-text" id="p77"> 
   <p>The distance can be based on multiple criteria, such as the length of the line segment between two points, the angle between two directions, or the number of mismatched elements in two arrays. Similarity measures are useful for machine learning tasks involving grouping or classifying data objects, especially for vector or semantic search.</p> 
  </div> 
  <div class="readable-text intended-text" id="p78"> 
   <p>For example, if we want to find words similar to “puppy,” we can generate a vector embedding for this word and look for other words with close vector embeddings, such as “dog” (figure 7.5).</p> 
  </div> 
  <div class="browsable-container figure-container" id="p79">  
   <img alt="figure" src="../Images/CH07_F05_Bahree.png" width="900" height="834"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.5</span> Vector search</h5>
  </div> 
  <div class="readable-text" id="p80"> 
   <p>We should choose the similarity measure that best suits the data and query needs of<strong><span class="aframe-location"/></strong> the use case. We must use a similarity measure to perform a vector search, a mathematical method for calculating the distance between two vectors. The smaller the distance, the more similar the vectors are. Some popular enterprise-ready services, such as Azure AI Search, support several similarity measures. Some of the more common similarity searches are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p81"> <em>Cosine similarity</em><em> </em>—This measure calculates the cosine of the angle between two vectors. It ranges from –1 to 1, where 1 means identical vectors and –1 means opposite vectors. Cosine similarity is commonly used for normalized embedding spaces. </li> 
   <li class="readable-text" id="p82"> <em>Squared Euclidean or L2-squared distance</em><em> </em>—It calculates the straight-line distance between two vectors. It ranges from 0 to infinity [0, ∞], where 0 means identical vectors, and larger values mean more dissimilar vectors. Squared Euclidean distance is also known as the L2 norm. </li> 
   <li class="readable-text" id="p83"> <em>Dot product</em><em> </em>—This measure calculates the product of the magnitudes of two vectors and the cosine of the angle between them. It ranges from –infinity to infinity [–, ∞], where 0 means orthogonal vectors and larger values mean more similar vectors. The dot product is equivalent to cosine similarity for normalized embedding spaces but is more efficient. </li> 
   <li class="readable-text" id="p84"> <em>Hamming distance</em><em> </em>—This calculates the number of differences between vectors at each dimension. </li> 
   <li class="readable-text" id="p85"> <em>Manhattan or L1 distance</em><em> </em>—This measures the sum of the absolute differences between the coordinates of two vectors. It ranges from 0 to infinity [0, ∞], where 0 means identical vectors and larger values mean vectors mean the opposite, that is, dissimilar vectors. </li> 
  </ul> 
  <div class="readable-text" id="p86"> 
   <p>Figure 7.6 shows the different similarity measures. It is important to use the same metric on which the underlying foundational model has been trained. For example, in the case of the OpenAI GPT class of models, the distance function is cosine similarity.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p87">  
   <img alt="figure" src="../Images/CH07_F06_Bahree.png" width="745" height="738"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.6</span> Different distant functions</h5>
  </div> 
  <div class="readable-text print-book-callout" id="p88"> 
   <p><span class="print-book-callout-head">Note </span> OpenAI embeddings are normalized to length 1, meaning each vector’s magnitude equals 1. Therefore, if we use OpenAI embeddings normalized to length 1, we can choose either cosine similarity or Euclidean distance as our distance function, and we will get the same results for vector search. However, cosine similarity might be slightly faster to compute because it only involves a dot product operation.</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>Choosing the right distance measure depends on the specific use case, the nature of the data, and the desired outcomes. Table 7.1 gives a brief overview of when to use each measure.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p90"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.1</span> Choosing the right distance measure</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Measure 
       </div></th> 
      <th> 
       <div>
         When to use 
       </div></th> 
      <th> 
       <div>
         Advantage 
       </div></th> 
      <th> 
       <div>
         Disadvantage 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Cosine similarity <br/></td> 
      <td>  Ideal for text and document similarity, where the magnitude of the vectors is not as important as the orientation; common in NLP tasks <br/></td> 
      <td>  Effective in high-dimensional spaces and for normalized vectors; ignores the magnitude of vectors, focusing on orientation, making it suitable for comparing documents of different lengths <br/></td> 
      <td>  It is not effective if the magnitude of vectors is important. <br/></td> 
     </tr> 
     <tr> 
      <td>  Squared Euclidean (L2) <br/></td> 
      <td>  Suitable for geometric or spatial data, like in image processing or when clustering multi-dimensional numerical data <br/></td> 
      <td>  It reflects the actual distance between points in a Euclidean space, making it intuitive and suitable for spatial datasets. <br/></td> 
      <td>  It can be sensitive to the scale of the data. High dimensions can lead to the curse of dimensionality. <br/></td> 
     </tr> 
     <tr> 
      <td>  Dot product <br/></td> 
      <td>  Efficient for high-volume, high-dimensional data, such as user preferences in recommendation systems <br/></td> 
      <td>  Computationally efficient, especially for sparse vectors. It is good for cases where the magnitude of vectors matters. <br/></td> 
      <td>  Interpretation is less intuitive than cosine similarity and can be sensitive to vector magnitudes. <br/></td> 
     </tr> 
     <tr> 
      <td>  Hamming distance <br/></td> 
      <td>  Best for comparing binary or categorical data, such as genetic sequences or error detection in data transmission <br/></td> 
      <td>  Simple and effective for datasets with discrete attributes <br/></td> 
      <td>  It only applies to strings of equal length and doesn’t consider the magnitude of differences. <br/></td> 
     </tr> 
     <tr> 
      <td>  Manhattan (L1) distance <br/></td> 
      <td>  Useful in grid-like pathfinding (e.g., urban road layouts) and in cases where differences in individual dimensions are important <br/></td> 
      <td>  It is more sensitive to differences in individual dimensions than L2 distance; robust to outliers. <br/></td> 
      <td>  It may not reflect the true distance in nongrid-like spaces or high-dimensional data. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p91"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_124"><span class="num-string">7.6</span> RAG challenges</h2> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>Enterprises considering implementing RAG systems face several hurdles that need careful consideration. First and foremost, ensuring effective scalability with increasing data volumes is critical. As data grows, so does the complexity and size of the retrieval index. Managing this growth becomes challenging, necessitating more powerful computational resources. Specifically, dense retrieval systems, which are resource-intensive in terms of computation and storage, require careful balancing to ensure scalability. Additionally, maintaining an efficient and fast retrieval index becomes crucial as the volume of documents increases. Parallelizing requests, managing retry mechanisms, and deploying appropriate infrastructure are essential for achieving scalable RAG systems.</p> 
  </div> 
  <div class="readable-text intended-text" id="p93"> 
   <p>Ensuring the quality and relevance of the indexed data is another significant concern. The utility of the RAG system is contingent upon the quality of its data; outdated or irrelevant information will lead to subpar responses —the principle of garbage-in-garbage-out still very much holds. This underscores the need for meticulous curation and regular updates of the document index to align with the enterprise's evolving requirements.</p> 
  </div> 
  <div class="readable-text intended-text" id="p94"> 
   <p>Once deployed, RAG systems introduce an additional layer of complexity in integrating existing workflows, requiring ongoing maintenance to ensure consistent performance. RAG systems need to be seamlessly incorporated into an enterprise's existing technical landscape. This process often involves navigating complex data governance problems and ensuring system interoperability.</p> 
  </div> 
  <div class="readable-text intended-text" id="p95"> 
   <p>RAG systems involve complex encoding and querying of dense vectors in real-time, which can cause delays and affect response times. For applications that need fast answers, such latency may not meet user expectations for promptness. In addition, the complicated nature of RAG models makes identifying the cause of errors difficult. Finding and fixing errors effectively is important, whether they happen during retrieval or generation. Moreover, once deployed, RAG systems introduce extra complexity when integrating with existing workflows. Ensuring smooth integration into an enterprise’s technical landscape involves dealing with data governance problems and system compatibility.</p> 
  </div> 
  <div class="readable-text intended-text" id="p96"> 
   <p>From a socio-technical perspective, ensuring that RAG systems are fair and unbiased is imperative. The risk of perpetuating existing biases from training data is real and can have far-reaching implications, requiring rigorous oversight and mitigation strategies. In addition, privacy and security are also key, especially if the indexed data includes confidential information, necessitating stringent compliance with data protection regulations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p97"> 
   <p>Chunking is a key problem that needs to be addressed by RAG implementations. Chunking is splitting a long text into smaller segments that an LLM can handle more easily. It can help lower the model’s computational and memory demands and enhance the quality and relevance of the output text. Chunking can also help the model concentrate on the most crucial parts of the text and avoid unimportant or repetitive parts. The difficulties with chunking are huge; we will discuss them in detail in the following sections.</p> 
  </div> 
  <div class="readable-text intended-text" id="p98"> 
   <p>Enterprises need to be aware of these challenges and weigh them against the benefits that RAG systems can bring, such as improved accuracy and contextual relevance in natural language processing tasks. When implementing an RAG-based solution, they must consider the trade-offs regarding costs, resources, and potential risks.</p> 
  </div> 
  <div class="readable-text" id="p99"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_125"><span class="num-string">7.7</span> Overcoming challenges for chunking</h2> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>Today, enterprises face many challenges when implementing RAG at a production scale. As mentioned before, chunking is the process of dividing a long sequence of text into smaller, more manageable pieces. This is necessary for LLMs, which have limited processing capacity. RAG models typically use a chunking algorithm to divide the input text into smaller chunks, which the LLM processes. The LLM generates a response for each chunk, and the responses are then concatenated to form the final output.</p> 
  </div> 
  <div class="readable-text intended-text" id="p101"> 
   <p>Chunking, however, can be challenging for RAG models for the following reasons:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p102"> Chunks may not be aligned with the natural boundaries of the text. This can lead to the LLM generating grammatically incorrect or semantically incoherent responses. </li> 
   <li class="readable-text" id="p103"> Chunks may vary in length and complexity. This can make it difficult for the LLM to generate responses consistent in quality. </li> 
   <li class="readable-text" id="p104"> Chunks may contain multiple intents. This can make it difficult for the LLM to identify the correct intent and generate the appropriate response. </li> 
  </ul> 
  <div class="readable-text" id="p105"> 
   <p>We start by understanding a strategy for chunking.</p> 
  </div> 
  <div class="readable-text" id="p106"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_126"><span class="num-string">7.7.1</span> Chunking strategies</h3> 
  </div> 
  <div class="readable-text" id="p107"> 
   <p>One downside of search is that we can only put so much information in the context window. If we use OpenAI models as a measure, depending on the model, we can only use a finite set of information that can be passed, as shown in table 7.2. In practical terms, this length is even shorter, given that we need space for the generation. This is where chunking becomes key.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p108"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.2</span> OpenAI model context length</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Open AI model 
       </div></th> 
      <th> 
       <div>
         Maximum length (token size) 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  GPT-3.5 Turbo <br/></td> 
      <td>  4K tokens; approx. 5 pages <br/></td> 
     </tr> 
     <tr> 
      <td>  GPT-4 <br/></td> 
      <td>  8K tokens; approx. 10 pages <br/></td> 
     </tr> 
     <tr> 
      <td>  GPT-4 32K <br/></td> 
      <td>  32K tokens; approx. 40 pages <br/></td> 
     </tr> 
     <tr> 
      <td>  GPT-4 Turbo, GPT-4o <br/></td> 
      <td>  128K tokens; approx. 300 pages <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>Chunking means breaking down big documents or text passages into smaller, more digestible parts or chunks. This is done to make the retrieval process faster and better, especially when working with huge collections of texts; the main reason is also the context window constraint of the LLMs. Chunking is useful for RAG for a few reasons:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p110"> <em>Granularity</em><em> </em>—When querying a large corpus for relevant information, searching at the granularity of smaller chunks might lead to more precise retrievals than searching entire documents. This can enhance the overall quality of the answers generated by RAG. </li> 
   <li class="readable-text" id="p111"> <em>Efficiency</em><em> </em>—Dealing with smaller chunks can make the retrieval process more efficient, especially when using dense retrievers that embed each chunk into a high-dimensional vector space. </li> 
   <li class="readable-text" id="p112"> <em>Flexibility</em><em> </em>—Chunking allows the system to match varying lengths of relevant information to a given query, offering more flexibility in what is considered relevant. </li> 
  </ul> 
  <div class="readable-text" id="p113"> 
   <p>When considering the chunking strategy, we need to consider it holistically and see how the resulting searches capture the essence of the user’s query. If a chunk is too large or small, it could lead to inaccurate results. As a simple rule, if a chunk makes sense to us as humans without additional information, an LLM could also understand it.</p> 
  </div> 
  <div class="readable-text intended-text" id="p114"> 
   <p>For conversational use cases, as the turn-by-turn back and forth happens and the dialogue gets longer, it is important to evaluate how much of the previous conversation is needed for the next turn in the ongoing context. Adding bigger chunks could affect relevancy, and we also will get up to the limitations of the context windows of the LLM. </p> 
  </div> 
  <div class="readable-text intended-text" id="p115"> 
   <p>Let’s use a customer service chatbot scenario with a service provider—a mobile phone provider, as an example. The conversation might start with one topic, a question on activating a new phone, and can turn to other topics such as details about plans, add-on products, coverage details, billing, payment methods, etc. In this example, as the conversation turns from one topic to the other, in many cases, we don’t need all the previous history and dialogue, and it can be either discarded or trimmed. Of course, in some cases, we would only want the needed details for the context of the ongoing conversation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p116"> 
   <p>The length of the information being chunked depends on the use case and the user’s expected behavior. For example, if we chunk a paragraph, we get a vector representation that captures more meaning from the content. This differs from sentence-level embedding, where the vector representation reflects more of the sentence’s meaning. This would lead to comparisons of other sentences and be more limited than the previous paragraph-based approach.</p> 
  </div> 
  <div class="readable-text" id="p117"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_127"><span class="num-string">7.7.2</span> Factors affecting chunking strategies</h3> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>Before we get into the different chunking approaches, a few additional things to factor in from a chunking strategy perspective will help us balance higher accuracy with keeping within acceptable performance and cost thresholds:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p119"> <em>Nature of the content</em><em> </em>—The nature of the content that’s being indexed affects the chunking strategy. For example, shorter content, such as tweets, might require different chunking than longer content like books or reports. Shorter content may be chunked together, while longer content, such as documents, reports, and similar, may need to be broken down into smaller parts for efficient processing. </li> 
   <li class="readable-text" id="p120"> <em>LLM and the associated embedding model</em><em> </em>—The LLM and the associated embedding model can also affect the chunking strategy. For instance, some models may be more efficient at processing smaller chunks or, given their architecture, a chunk of a certain size, while others may handle larger chunks better. Knowing which LLM and associated embedding model we will use is important. For example, when using OpenAI, we should consider the <code>text-embedding-ada-002</code> embedding with a size of 256 or 512 tokens. </li> 
   <li class="readable-text" id="p121"> <em>Query complexity</em><em> </em>—The length and complexity of the user query can affect the chunking approach. More complex queries might necessitate more intricate chunking strategies to match the query with the relevant data. It is important to remember that LLMs are not search engines and should not be used as such. The query complexity is multidimensional, both in terms of length and complexity, and might involve breaking the query into smaller subqueries that target different aspects of the original query before bringing everything back to the answer. For example, the query “What is the capital of the UK?” is very specific and straightforward. In contrast, the query “What are the economic implications of the rise of AI in various industries in the United States?” is multifaceted. It requires a deeper understanding of the technology (AI, in this example) and the geographic and industry details to infer the meaning of implications. </li> 
   <li class="readable-text" id="p122"> <em>Integration into the application</em><em> </em>—Understanding how the output (query result) is used within the application can also influence the chunking strategy. For example, the limitations of the LLM and the context window might dictate how the data should be chunked to achieve the best results. This also factors in other data and metadata that the application might need. </li> 
   <li class="readable-text" id="p123"> <em>Preprocessing data</em><em> </em>—Preprocessing the data will help increase the quality of the generation and help us determine a possible good size. Preprocessing would include cleaning up extra noise or using other AI techniques to extract information, including data cleaning, data transformation from one format to another, feature normalization if required, tokenization, removing common stop words (such as “is,” “the,” “and”), and so forth. </li> 
   <li class="readable-text" id="p124"> <em>Evaluating and comparing different chunk sizes</em><em> </em>—It’s crucial to evaluate and compare the effects of different chunk sizes on both the quality and performance of the process. This can be especially important in enterprise settings where varying chunk sizes might be used based on the nature of the content, and a balance may need to be struck between accuracy and performance. This evaluation would include both quality and performance. </li> 
  </ul> 
  <div class="readable-text" id="p125"> 
   <p>One can take a few approaches when thinking about chunking information, as outlined in table 7.3. It’s worth noting that the ideal chunking strategy might vary based on the corpus, the nature of the queries, and the application’s specific requirements. Experimentation might be needed to find the most effective approach for a particular RAG implementation.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p126"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.3</span> Chunking approaches</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Chunking approach 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Fixed length <br/></td> 
      <td>  Divide documents into chunks of a fixed number of words or tokens. This is straightforward but may sometimes split information that ideally should be kept together. <br/></td> 
     </tr> 
     <tr> 
      <td>  Sliding window <br/></td> 
      <td>  Use a fixed-sized sliding window with or without overlapping data. This can ensure that important boundaries within the text are not missed, but it can also lead to redundancy if there’s significant overlap. <br/></td> 
     </tr> 
     <tr> 
      <td>  Punctuation based <br/></td> 
      <td>  Divide the text based on punctuation, such as paragraphs or sections. This is less arbitrary than fixed-length chunking and often preserves the semantic integrity of the content. However, it can result in variable chunk sizes. <br/></td> 
     </tr> 
     <tr> 
      <td>  Topic or section breaks <br/></td> 
      <td>  In structured documents such as Wikipedia articles, natural breaks like sections or subsections can be used to define chunks. This method ensures that the content within a chunk is semantically coherent. <br/></td> 
     </tr> 
     <tr> 
      <td>  Adaptive <br/></td> 
      <td>  Use algorithms or models that adaptively determine the best way to chunk documents based on their content. This can be more complex but might yield semantically cohesive chunks. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p127"> 
   <p>Depending on the size and structure of the text, there are different ways to chunk it for RAG. Some of the common methods are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p128"> <em>Sentence splitting</em><em> </em>—As the name suggests, sentence boundaries are used to split the text, which is useful to ensure that each chunk contains whole sentences, preserving the context and meaning. </li> 
   <li class="readable-text" id="p129"> <em>Fixed-length splitting</em><em> </em>—Here, text into is divided into fixed-length chunks. This can sometimes result in sentences being cut off in the middle. </li> 
   <li class="readable-text" id="p130"> <em>Token-based splitting</em><em> </em>—Splitting the text based on a fixed number of tokens (e.g., words). This is more fine-grained than sentence splitting but can still result in sentences being cut off. </li> 
   <li class="readable-text" id="p131"> <em>Semantic chunking</em><em> </em>—Using natural language processing (NLP) tools to identify coherent segments in the text. For instance, splitting a text based on topics or paragraphs. </li> 
   <li class="readable-text" id="p132"> <em>Hierarchical chunking</em><em> </em>—Dividing text into hierarchical sections, such as chapters, sections, and subsections. </li> 
  </ul> 
  <div class="readable-text" id="p133"> 
   <p>To illustrate how different chunking approaches (fixed length and a semantic NLP) might affect the outcomes, we use an example of the UK Constitution from Wikipedia [2] as our input text. We can see the outcome in figure 7.7 when we apply a fixed-length chunking approach. The text is broken up into chunks of a fixed size, and in this simple example, we see that some information is cut off and some context is missing.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p134">  
   <img alt="figure" src="../Images/CH07_F07_Bahree.png" width="1100" height="713"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.7</span> Fixed-length chunking approach</h5>
  </div> 
  <div class="readable-text" id="p135"> 
   <p>The text of the UK’s constitution appears in figure 7.8, using an NLP-based chunking approach. Because NLP comprehends the text and context, it splits it at the proper level with the correct tokens to maintain the sense and accuracy.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p136">  
   <img alt="figure" src="../Images/CH07_F08_Bahree.png" width="1100" height="712"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.8</span> NLP-based chunking approach</h5>
  </div> 
  <div class="readable-text" id="p137"> 
   <p>These chunking strategies are useful and important for any provider or LLM we choose. In the following sections, we will see how to apply these strategies. We will begin with Sentence Splitter, a text splitter that splits the text based on a new line.</p> 
  </div> 
  <div class="readable-text" id="p138"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_128"><span class="num-string">7.7.3</span> Handling unknown complexities</h3> 
  </div> 
  <div class="readable-text" id="p139"> 
   <p>Sometimes, we don’t know the complexities and length of the user queries in advance. In such cases, RAG implementations that can deal with unknown lengths and complexities of user queries can be challenging. Here are several strategies to determine the chunking approach in such scenarios:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p140"> <em>Adaptive chunking</em><em> </em>—Implement an adaptive chunking mechanism that automatically adjusts the size of chunks based on the query length and complexity. Smaller chunks can be used for shorter, simpler queries, while larger chunks might be needed to capture the necessary context for longer, more complex queries. </li> 
   <li class="readable-text" id="p141"> <em>Preprocessing heuristics</em><em> </em>—Use heuristics to analyze the query before chunking. These heuristics could estimate the complexity by looking at factors like the number of unique words, the presence of specialized terminology, or the syntactic structure. Based on this estimation, the chunking mechanism can adapt the size of the chunks. </li> 
   <li class="readable-text" id="p142"> <em>Dynamic retrieval window</em><em> </em>—Implement a dynamic retrieval window that expands or contracts based on the query. If the initial retrieval results are unsatisfactory, the window can be adjusted to include more or fewer documents or to change the granularity of the chunking. </li> 
   <li class="readable-text" id="p143"> <em>Overlapping chunks</em><em> </em>—Create overlapping chunks to ensure that no critical information is lost at the boundaries of chunks. This approach can help maintain context when queries span multiple chunks. Depending on the use case, this can also overpower the other information, which isn’t something one should do by default. </li> 
   <li class="readable-text" id="p144"> <em>ML approaches</em><em> </em>—Use traditional ML models to predict the optimal chunk size based on the query characteristics. The model can be trained on a dataset of queries and optimal chunk sizes determined by performance on a validation set. </li> 
   <li class="readable-text" id="p145"> <em>Fallback strategies</em><em> </em>—Have fallback strategies in place for when the initial chunking does not yield good results. This can involve re-querying with different chunk sizes or using different chunking strategies if the initial response does not meet certain confidence thresholds. </li> 
   <li class="readable-text" id="p146"> <em>Feedback loop</em><em> </em>—Implement a feedback loop where user interactions can help adjust the chunking. If a user indicates an unsatisfactory response, the system could automatically try different chunking strategies to improve the response. </li> 
   <li class="readable-text" id="p147"> <em>Hybrid approaches</em><em> </em>—Combine several of the preceding strategies to handle various queries. For example, adaptive chunking with a fallback strategy that continuously employs user feedback can improve the chunking mechanism. </li> 
  </ul> 
  <div class="readable-text" id="p148"> 
   <p>In practice, the optimal solution would combine these strategies for a specific use case, and trial and error are needed to enhance performance. Moreover, making the system's components flexible can enable changes and upgrades to the chunking mechanism as more information is collected about the kinds of queries users enter.</p> 
  </div> 
  <div class="readable-text" id="p149"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_129"><span class="num-string">7.7.4</span> Chunking sentences</h3> 
  </div> 
  <div class="readable-text" id="p150"> 
   <p>A sentence-based splitter is a method that splits the text into chunks based on sentence boundaries, such as periods, question marks, or exclamation points. This method can preserve the meaning and coherence of the text, as each chunk contains one or more complete sentences.</p> 
  </div> 
  <div class="readable-text intended-text" id="p151"> 
   <p>Listing 7.1 shows a simple implementation: an incoming text is split into sentences using regular expressions. The function splits the input text at every occurrence of a period (.), exclamation mark (!), or question mark (?). These characters are typically used to denote the end of a sentence in English. The result is a list of strings, each being a sentence from the original text.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p152"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.1</span> Split sentence function</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def split_sentences(text):
    sentences = re.split('[.!?]', text)  <span class="aframe-location"/> #1
    sentences = [sentence.strip() for sentence in sentences if sentence]
    return sentences</pre> 
    <div class="code-annotations-overlay-container">
     #1 Splits the sentence at every occurrence of these characters
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p153"> 
   <p>Another way to implement the same thing is using a sentence-based splitter, such as the <code>textwrap</code> library in Python. This function, <code>wrap()</code>, can split a string into a list of strings based on a given width. We can pass additional parameters to ensure that words don’t get split mid-sentence.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p154"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.2</span> Splitting sentences using <code>textwrap</code></h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def split_sentences_by_textwrap(text):
    max_chunk_size = 2048           <span class="aframe-location"/> #1

    chunks = textwrap.wrap(text,    <span class="aframe-location"/> #2
        width=max_chunk_size,
        break_long_words=False,
        break_on_hyphens=False)

    return chunks</pre> 
    <div class="code-annotations-overlay-container">
     #1 Sets the maximum chunk size
     <br/>#2 Splits the text into chunks
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p155"> 
   <p>It is important to point out that both the <code>textwrap.wrap()</code>and <code>re.split()</code> functions serve different purposes, and their efficiency, speed, and accuracy depend on the specific use case.</p> 
  </div> 
  <div class="readable-text intended-text" id="p156"> 
   <p>The original purpose of the <code>textwrap</code> library is for display purposes and to help format and wrap strings where we want control over the maximum line length. It’s efficient and fast for its intended use case. However, it’s not designed to split text into sentences, so if you use it for that purpose, it may not be accurate. For example, it could split a sentence in the middle if the sentence is longer than the specified width.</p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>The <code>split()</code> function in regular expressions divides a string where the pattern matches. It can split a text into sentences well when used with a pattern such as '[.!?]'. It’s also quick and effective for what it does. However, it doesn’t consider line length or word boundaries, so if you need to limit the length of each chunk, <code>re.split()</code> would not be the best option.</p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>In terms of speed, both functions are quite fast and should perform well for most typical use cases. The speed could become a problem for very large strings, but in most cases, the difference would not be noticeable. Regarding accuracy, if we need to split the text into sentences, <code>re.split()</code> would be more accurate. If you need to wrap text to a certain line length, <code>textwrap.wrap()</code> would be more accurate.</p> 
  </div> 
  <div class="readable-text intended-text" id="p159"> 
   <p>Both functions are quite efficient, as they are part of Python’s standard library and are implemented in C. The efficiency would also depend on the size and complexity of the input string.</p> 
  </div> 
  <div class="readable-text" id="p160"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_130"><span class="num-string">7.7.5</span> Chunking using natural language processing</h3> 
  </div> 
  <div class="readable-text" id="p161"> 
   <p>As outlined in the earlier example, we can use a natural language processing (NLP) approach to split the text into chunks; these chunks can be based on linguistic features, such as clauses, phrases, or entities. Compared to the sentence splitter methods outlined earlier, this method can capture the meaning and context of the text, but it may require more computational resources and domain knowledge. Let’s see some examples using two of the most common NLP libraries available today—the Natural Language Toolkit (NLTK) and spaCy.</p> 
  </div> 
  <div class="readable-text" id="p162"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Using the NLTK </h4> 
  </div> 
  <div class="readable-text" id="p163"> 
   <p>The NLTK is one of the most well-known libraries for natural language processing and text analytics. It provides easy-to-use interfaces to many corpora and lexical resources. Furthermore, it includes a suite of text-processing libraries for classification, tokenization, stemming, tagging, parsing, and more. NTLK can be installed in many ways; in the case of conda, we can use the following: <code>conda install -c anaconda nltk</code>. For pip, we can use <code>pip install nltk</code>. Before we can use NLTK, we need to install the NLTK data, which can be done using the NLTK’s data downloader. A simple way to do this is to run a Python interpreter using administrator privileges and run the following commands. More details can be found at <a href="https://www.nltk.org/data.html">https://www.nltk.org/data.html</a>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p164"> 
   <div class="code-area-container"> 
    <pre class="code-area">&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.download()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p165"> 
   <p>The following listing shows how to implement NLTK using the <code>sent_tokenize()</code> function to split the text into sentences.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p166"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.3</span> Chunking text using NLP</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def split_sentences_by_nltk(text):
 chunks = []
  for sentence in nltk.sent_tokenize(text):
    chunks.append(sentence)

  return chunks</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p167"> 
   <p>The <code>sent_tokenize()</code> function uses an instance of <code>PunktSentenceTokenizer</code>, an unsupervised ML-based tokenizer that comes pretrained and is ready for sentence splitting. If the text is very large, you might consider using a generator expression instead of a list comprehension for memory efficiency. </p> 
  </div> 
  <div class="readable-text intended-text" id="p168"> 
   <p>The next listing shows how the previous function could be rewritten as a generative function.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p169"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.4</span> Chunking using NLP: Generative function</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def split_sentences_by_nltk(text):
    for sentence in nltk.sent_tokenize(text):
        yield sentence</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p170"> 
   <p>The NLTK can be quite advantageous when it comes to chunking. It can detect sentence boundaries and split on those lines, and it is also effective for splitting texts into individual sentences, which can be useful for chunking large texts, while ensuring that sentences are not broken in the middle.</p> 
  </div> 
  <div class="readable-text intended-text" id="p171"> 
   <p>From an enterprise perspective, it’s worth noting that while NLTK is comprehensive and suitable for research and educational purposes, it might not always be the most efficient in terms of speed. Other libraries such as spaCy might be more suitable for production-level applications, especially when processing vast amounts of text.</p> 
  </div> 
  <div class="readable-text" id="p172"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Using spaCy</h4> 
  </div> 
  <div class="readable-text" id="p173"> 
   <p>spaCy is a free, open source NLP library for Python that provides a wide range of NLP tasks, including sentence segmentation, named entity recognition, part-of-speech tagging, and dependency parsing. It is also good for chunking text and grouping words into meaningful units, such as noun phrases, verb phrases, and prepositional phrases. </p> 
  </div> 
  <div class="readable-text intended-text" id="p174"> 
   <p>spaCy is a good choice for RAG implementations, as it is efficient and fast, especially when processing large amounts of text in real-time. It is accurate and reliable and can be customized depending on the specific needs. For example, spaCy can be used to chunk text using different linguistic theories, such as phrase structure grammar and dependency grammar.</p> 
  </div> 
  <div class="readable-text intended-text" id="p175"> 
   <p>Before we can use spaCy, we need to install the packages and download the appropriate pretrained language model for spaCy to use. If using conda, we can install spaCy using <code>conda install -c conda-forge spacy</code>. If we are using pip, then we can use the following: <code>pip</code> <code>install</code> <code>spacy</code>. In the example, we download the small general-purpose English language model called <code>en_core_web_sm</code> using the following command: <code>python -m spacy download en_core_web_sm</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p176"> 
   <p>spaCy offers additional models for different purposes and languages. In addition to the small English model, medium and large models are available—<code>en_core_web_md</code> and <code>en_core_web_lg</code>, respectively, for more comprehensive word vectors. The larger the model, the longer it will take to process. Choosing a model involves more than just the size; one must factor in accuracy, languages, and domain. More details on the pre-trained models can be found at <a href="https://spacy.io/usage/models/">https://spacy.io/usage/models/</a>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p177"> 
   <p>The following listing shows how we can use spaCy for chunking. In this example, we factor in token counts for the LLMs context windows, and we also have the option to overlap text between chunks to allow for context continuity.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p178"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.5</span> Sentence chunking using spaCy</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def split_sentences_by_spacy(text, max_tokens, overlap=0, model="en_core_web_sm"):
    nlp = spacy.load(model)   <span class="aframe-location"/> #1

    doc = nlp(text)           <span class="aframe-location"/> #2
    sentences = [sent.text for sent in doc.sents]

    tokens_lengths = [count_tokens(sent) for sent in sentences]  <span class="aframe-location"/> #3

    chunks = []
    start_idx = 0

    while start_idx &lt; len(sentences):
        current_chunk = []
        current_token_count = 0
        for idx in range(start_idx, len(sentences)):
            if current_token_count + tokens_lengths[idx] &gt; max_tokens:
                break
            current_chunk.append(sentences[idx])
            current_token_count += tokens_lengths[idx]

        chunks.append(" ".join(current_chunk))

        if overlap &gt;= len(current_chunk):  <span class="aframe-location"/> #4
            start_idx += 1
        else:
            start_idx += len(current_chunk) - overlap

    return chunks</pre> 
    <div class="code-annotations-overlay-container">
     #1 Loads the spaCy model
     <br/>#2 Tokenizes the text into sentences using spaCy
     <br/>#3 Tokenizes sentences and accumulates tokens
     <br/>#4 Sliding window adjustment
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p179"> 
   <p>These techniques have different benefits and computing characteristics. Let’s try them all and compare their performance, duration, and effect. For example, we use Azure OpenAI and the FIFA 2023 Women’s World Cup as data [3]. This happened in 2023, and at the time of this publication, the LLMs lack this knowledge, as it is beyond the training cut-off.</p> 
  </div> 
  <div class="readable-text intended-text" id="p180"> 
   <p>For this example, we save the Wikipedia page for the FIFA 2023 Women’s World Cup as a raw text field. This file is not processed, and the resulting file is messy enough to reflect many real-world problems enterprises would face.</p> 
  </div> 
  <div class="readable-text intended-text" id="p181"> 
   <p>In this example, as shown in listing 7.6, we run through the four different chunking techniques using the same file and outline the time it takes for each technique to execute, the number of chunks created, and the tokens used. We also use GPT3 to create a summary of the text read.</p> 
  </div> 
  <div class="readable-text intended-text" id="p182"> 
   <p>We begin by loading the stored text file named women_ fifa_worldcup_2023.txt. We apply four different chunking techniques separately and then use the same GPT mode to summarize them. We first chunk using a basic sentence chunking method and process those. Then, we process the same file using <code>textwrap</code>, NLTK, and spaCy. We record some simple telemetry at each run and show all of these at the end, along with the summary.</p> 
  </div> 
  <div class="readable-text intended-text" id="p183"> 
   <p>Note that several helper functions, such as <code>get_embedding()</code>, <code>count_tokens()</code>, and so forth, have been used earlier in the book—we do not call those out again for brevity. The complete code samples are in the GitHub code repository accompanying the book (<a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p184"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.6</span> Sentence-chunking comparison </h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import nltk
import spacy
...

GPT_MODEL = "gpt-35-turbo"

def generate_summaries(chunks):
    summaries = []                             <span class="aframe-location"/> #1
    # loop through each chunk
    for chunk in tqdm(chunks):
        prompt = f"Summarize the following text in one
        <span class="">↪</span>sentence:\n{chunk}\nSummary:"
        response = openai.Completion.create(    <span class="aframe-location"/> #2
            engine=GPT_MODEL,
            prompt=prompt,
            max_tokens=800,
            temperature=0.7        )
        summary = response.choices[0].text
        summaries.append(summary)
        sleep(1)                                <span class="aframe-location"/> #3

    # return the list of summaries
    return summaries

def process_chunks(sentences):
    sentence_embeddings = []
    total_token_count = 0

    for i, sentence in enumerate(tqdm(sentences)):
        total_token_count += count_tokens( 
              <span class="">↪</span>sentence, "cl100k_base") <span class="aframe-location"/> #4
        embedding = get_embedding(sentence)
        sentence_embeddings.append([sentence, embedding])

    print("\tNumber of sentence embeddings:", len(sentence_embeddings))
    print("\tTotal number of tokens:", total_token_count)

    return sentence_embeddings

TEXT_FILE = f"data/women_fifa_worldcup_2023.txt"     <span class="aframe-location"/> #5

with open(TEXT_FILE, "r") as f:
    text = f.read()

print("1. Simple sentence chunking ...")
sentences = split_sentences(text)

process_chunks(sentences)

print("="*20)
# ===================================

#Reset variables
summaries = []
sentences = []
sentence_embeddings = []
total_token_count = 0
chunks = []

print("2. Sentence chunking using textwrap ...")
chunks = split_sentences_by_textwrap(text)  <span class="aframe-location"/> #6
process_chunks(chunks)

# ===================================

#Reset variables
...

print("3. Sentence chunking using NLTK ...")
chunks = split_sentences_by_nltk(text)       <span class="aframe-location"/> #7
process_chunks(chunks)

# ===================================

#Reset variables
...

print("4. Sentence chunking using spaCy ...")
chunks = split_sentences_by_spacy(text, 
               <span class="">↪</span>max_tokens=2000, overlap=0)  <span class="aframe-location"/> #8
process_chunks(chunks)

# ===================================
summaries = generate_summaries(chunks)         <span class="aframe-location"/> #9
print("Summaries generated by OpenAI API:")
print(summaries)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Empty list to store the summaries
     <br/>#2 Completion to generate a summary for the chunk
     <br/>#3 Rate limiting
     <br/>#4 Counts tokens in the sentence
     <br/>#5 File that we want to chunk
     <br/>#6 Chunks text using textwrap
     <br/>#7 Chunks text using NLTK
     <br/>#8 Chunks text using spaCy
     <br/>#9 Generates summaries for each chunk using OpenAI API
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p185"> 
   <p>Table 7.4 shows the output when we run this, with the time duration in seconds. As expected, the time it takes to process the same input text differs greatly depending on the technique used.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p186"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.4</span> Sentence-chunking comparison</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Chunking method 
       </div></th> 
      <th> 
       <div>
         Embeddings count 
       </div></th> 
      <th> 
       <div>
         Tokens count 
       </div></th> 
      <th> 
       <div>
         Execution time (secs) 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Simple <br/></td> 
      <td>  120 <br/></td> 
      <td>  5815 <br/></td> 
      <td>  16.96 <br/></td> 
     </tr> 
     <tr> 
      <td>  Using <code>textwrap</code> <br/></td> 
      <td>  12 <br/></td> 
      <td>  5933 <br/></td> 
      <td>  1.66 <br/></td> 
     </tr> 
     <tr> 
      <td>  Using <code>NLTK</code> <br/></td> 
      <td>  105 <br/></td> 
      <td>  5909 <br/></td> 
      <td>  13.31 <br/></td> 
     </tr> 
     <tr> 
      <td>  Using <code>spaCy</code> <br/></td> 
      <td>  4 <br/></td> 
      <td>  5876 <br/></td> 
      <td>  5.8 <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p187"> 
   <p>The following is the summary generated by the LLM using the spaCy chunks; these summaries are concise and informative, which is what we intended:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p188"> 
   <div class="code-area-container"> 
    <pre class="code-area">Summaries generated by OpenAI API:
[" The FIFA Women's World Cup is an international association football competition contested by the senior women's national teams of members of FIFA, and has been held every four years since 1991; the most successful team is the United States, with four titles,..."]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p189"> 
   <p>This example shows that the <code>textwrap</code> approach is the quickest, taking 1.66 seconds; this does not imply that the <code>textwrap</code> approach is always the most suitable and the one we should adopt. We have to evaluate this for each situation, depending on the kind of information and the use case involved. Let’s explore the decision factors required to select the best strategy for chunking.</p> 
  </div> 
  <div class="readable-text" id="p190"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Choosing the right strategy</h4> 
  </div> 
  <div class="readable-text" id="p191"> 
   <p>Whether to use an NLP-based chunking strategy or a fixed-length chunking approach depends on the specific requirements and constraints of the task at hand. Table 7.5 outlines some of the decision factors.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p192"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.5</span> Chunking decision factors</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Decision factor 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Task requirements <br/></td> 
      <td>  If the task requires understanding the nuances of language, such as answering questions that depend on context or generating coherent text, NLP-based chunking is preferable. <br/></td> 
     </tr> 
     <tr> 
      <td>  Performance <br/></td> 
      <td>  If maintaining the context isn’t critical, and there are performance constraints, fixed-length chunking could be the better choice. <br/></td> 
     </tr> 
     <tr> 
      <td>  Resource availability <br/></td> 
      <td>  Fixed-length chunking is less resource-intensive for projects with limited computational resources and is easier to scale. <br/></td> 
     </tr> 
     <tr> 
      <td>  Data characteristics <br/></td> 
      <td>  NLP-based chunking can use those boundaries for text with clear linguistic demarcations (such as well-structured documents). In contrast, fixed-length chunking might be more practical if the text is poorly structured or if the boundaries are unclear. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p193"> 
   <p>One might begin with a fixed-length method because it is easy and then switch to an NLP-based method when more complexity is required. Some advanced systems might even employ both, using fixed-length chunking to deal with large amounts of text quickly and then using NLP-based chunking for the smaller, more controllable chunks to improve the context and meaning. Let’s change topics and see how we can chunk other documents, such as PDFs.</p> 
  </div> 
  <div class="readable-text" id="p194"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_131"><span class="num-string">7.8</span> Chunking PDFs</h2> 
  </div> 
  <div class="readable-text" id="p195"> 
   <p>At a high level, chunking PDFs is quite similar to chunking sentences. There are different options for PDFs that are not too complex and have basic tables or images. A simple method to start is to use the PyPDF2 library. PyPDF2 is an open source Python PDF library that can perform various operations on PDF pages, such as splitting, merging, cropping, and transforming. It can also extract text, custom data, passwords, and metadata from PDFs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p196"> 
   <p>Listing 7.7 shows how to use it. We can install PyPDF2 using the following command in conda: <code>conda</code> <code>install</code> <code>-c</code> <code>conda-forge</code> <code>pypdf2</code>, or if using pip, then <code>pip install pypdf2</code>. This output is text that can be chunked and processed like any other text previously discussed. This library doesn’t handle images; if any images are in PDF, those will be ignored. Note that the following listing only shows the relevant section for brevity; the book’s GitHub repository has the complete code.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p197"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.7</span> Extracting text from PDF</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import PyPDF2
def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        print("Number of PDF pages:", len(reader.pages))
        text = ""
        for  page in reader.pages:
            page_text = page.extract_text()
            text += page_text
            #print(page_text)
    return text</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p198"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Handling tables and images in PDF</h4> 
  </div> 
  <div class="readable-text" id="p199"> 
   <p>While handling text in the last example seems quite straightforward, PDFs can add a lot of complexity. The accuracy of text extraction depends on the PDF itself, given that not all PDFs encode text in a manner that is easily extractable. The following listing shows one example of how to process images and tables for chunking from PDFs, but overall, this will be challenging.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p200"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.8</span> Example of how to extract tables and images</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from PIL import Image
import tabula
from pdfminer.high_level import extract_pages
from PyPDF2 import PdfReader
...

# Define the PDF file path
pdf_file = f"data/test.pdf"
output_folder = f"data/temp/"

text = ''                 <span class="aframe-location"/> #1

def process_element(element, iw):
    global text
    if isinstance(element, LTTextBox):
        text += element.get_text()
    elif isinstance(element, LTImage):
        bmp_file = iw.export_image(element)           <span class="aframe-location"/> #2
        bmp_file = os.path.join(iw.outdir, bmp_file)
       <span class="aframe-location"/> img = Image.open(bmp_file)       #3
        png_file = bmp_file.rsplit('.', 1)[0] + '.png' <span class="aframe-location"/> #4
        img.save(png_file)
    if isinstance(element, LTFigure):
        for child in element:
            process_element(child, iw)

iw = ImageWriter(output_folder)            <span class="aframe-location"/> #5

page = next(extract_pages(pdf_file))   <span class="aframe-location"/> #6

for element in page:
    process_element(element, iw)

with open(output_folder + 'text.txt', 'w', encoding='utf-8') as f:
    f.write(text)

tables = tabula.read_pdf(pdf_file, pages='all') <span class="aframe-location"/> #7

for i, table in enumerate(tables):             <span class="aframe-location"/> #8
    table.to_csv(f'{output_folder}table_{i}.csv', index=False)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates an empty string to store the text
     <br/>#2 Exports the image as a BMP file
     <br/>#3 Opens the BMP file with PIL
     <br/>#4 Converts the image to PNG and saves
     <br/>#5 Creates an ImageWriter object to save the images
     <br/>#6 Gets the first page from the PDF file
     <br/>#7 Reads the tables
     <br/>#8 Saves each table into a separate file
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p201"> 
   <p>Images and tables are difficult to parse and handle in PDFs, especially consistently and predictably. One way enterprises can solve this problem is by daisy-chaining other ML models instead of just trying to parse documents. Microsoft’s Azure Document Intelligence is a service that allows enterprises to implement this. </p> 
  </div> 
  <div class="readable-text" id="p202"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Azure Document Intelligence</h4> 
  </div> 
  <div class="readable-text" id="p203"> 
   <p>Azure AI Document Intelligence is a cloud service that employs advanced ML models to auto-extract text, key values, tables, and structures from documents, converting them into actionable data. It offers three types of ML models for document analysis: prebuilt models for common scenarios (e.g., IDs, receipts), custom models trained on your data, and document analysis models for structured content extraction.</p> 
  </div> 
  <div class="readable-text intended-text" id="p204"> 
   <p>Unlike many Python PDF-handling packages, it supports multiple document formats, handles complex layouts, handwritten text, and objects, and allows for custom ML model creation. Integration is simple via a REST API to extract data from documents such as PDFs and use an RAG pattern for summarization or answer generation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p205"> 
   <p>Listing 7.9 shows how to analyze a document using a prebuilt layout model. We start the analysis of a sample document using the prebuilt layout model and iterate the result. It detects the text and tables for each page, including understanding rows and columns. Please note that Document Intelligence isn't available on conda yet, so we'll use pip to install it: <code>pip install azure-ai-formrecognizer</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p206"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.9</span> Azure AI Document Intelligence prebuilt layout operations</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from azure.core.credentials import AzureKeyCredential
from azure.ai.formrecognizer import DocumentAnalysisClient

endpoint = "YOUR_FORM_RECOGNIZER_ENDPOINT"
key = "YOUR_FORM_RECOGNIZER_KEY"

# sample document
pdf_file = f"YOUR_PDF_FILE"

document_analysis_client = DocumentAnalysisClient(
    endpoint=endpoint, credential=AzureKeyCredential(key)
)

poller = poller = document_analysis_client.begin_analyze_document_from_url(
    "prebuilt-layout", pdf_file)   <span class="aframe-location"/> #1
result = poller.result()<span class="aframe-location"/>             #2

for page in result.pages:             <span class="aframe-location"/> #3
    for line_idx, line in enumerate(page.lines):
        print(
         "...Line # {} has text content '{}'".format(
        line_idx,
        line.content.encode("utf-8")  <span class="aframe-location"/> #4
        )
    )

    for selection_mark in page.selection_marks:
        print("...Selection mark is '{}'".format(
         selection_mark.state,      <span class="aframe-location"/> #5
         )
    )

for table_idx, table in enumerate(result.tables):
    print(
        "Table # {} has {} rows and {} columns".format(
        table_idx, table.row_count, table.column_count
        )
    )

    for cell in the table.cells:                 <span class="aframe-location"/> #6
        print(
            "...Cell[{}][{}] has content '{}'".format(
            cell.row_index,
            cell.column_index,
            cell.content.encode("utf-8"),
            )
        )</pre> 
    <div class="code-annotations-overlay-container">
     #1 Calls the API to analyze a PDF using a pre-build layout
     <br/>#2 Gets the results from the analysis
     <br/>#3 Iterates through all the pages in the PDF
     <br/>#4 Extracts the text on each line on the page
     <br/>#5 Examines whether there is a Selection mark
     <br/>#6 Parses tables found in the PDF
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p207"> 
   <p>More details on Azure Document Intelligence can be found at <a href="https://mng.bz/6YNA">https://mng.bz/6YNA</a>.</p> 
  </div> 
  <div class="readable-text" id="p208"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_132">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p209"> LLMs don’t have any up-to-date information past their training cut-off, and they don’t know private and proprietary information. Retrieval-augmented generation (RAG) is the technique that helps address these limitations. </li> 
   <li class="readable-text" id="p210"> RAG is a powerful technique that provides up-to-date and grounded information, leading to improved LLM responses. It also helps ground data and can improve the generation of LLMs regarding quality, diversity, and customization. </li> 
   <li class="readable-text" id="p211"> Vector searches powered by vector indexes and databases are pivotal in making the retriever component of RAG implementations efficient and scalable. They enable real-time, large-scale semantic search, essential for applications that require rapid access to vast amounts of information. </li> 
   <li class="readable-text" id="p212"> RAG must deal with various challenges, with chunking being one of the most critical. Due to LLM content window limitations, we need to chunk a large corpus of data and employ various techniques for splitting information—fixed length, sliding window, punctuation based, sections based, or adaptive. Each has advantages and challenges and needs to be considered in the context of the use case, shape, and data type. </li> 
   <li class="readable-text" id="p213"> Understanding and parsing PDFs into chunks is difficult, especially if they contain images and tables. Daisy-chaining other ML models, such as Azure Document Intelligence, can help simplify this. </li> 
   <li class="readable-text" id="p214"> Combined with prompt engineering, RAG helps address requirements such as dynamic information access, cost efficiency, grounding, citation, scalability, and customization in various enterprise scenarios and contexts. </li> 
  </ul>
 </div></div></body></html>