<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">12 Blending gradient boosting and deep learning<a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/><a id="idTextAnchor006"/><a id="idTextAnchor007"/><a id="idTextAnchor008"/></h1>

  <p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="marker-442"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">A review of the end-to-end gradient boosting example from chapter 7</li>

    <li class="co-summary-bullet">A comparison of the results of the gradient boosting example from chapter 7 with a deep learning solution for the same problem</li>

    <li class="co-summary-bullet">The result of ensembling a gradient boosted model with a deep learning model</li>
  </ul>

  <p class="body">In chapter 7, we did an in-depth exploration of an end-to-end example of using gradient boosting. We explored a dataset of Airbnb listings for Tokyo, we engineered features suitable for a pricing regression task, and then we created a baseline model trained on this dataset to predict prices. Finally, applying the techniques we had learned in the book up to that point, we optimized an XGBoost model trained on this dataset and examined some approaches to explain the behavior of the model.</p>

  <p class="body">In this chapter, we evaluate if using deep learning would have led to different results and performance on the same problem, determine what approach works best, and discover how to use and integrate the strengths and weaknesses of each method. To do this, we begin this chapter by revisiting the gradient boosting approach to the Airbnb Tokyo problem from chapter 7. Next, we review some of the approaches we could take to apply deep learning to the same problem and share our chosen deep learning approach to this problem. Finally, we compare the two solutions and determine what we can learn from such a comparison when it comes to deciding whether to use gradient boosting or deep learning to tackle a tabular data problem, whether it’s a regression or classification problem. In addition to comparing the solutions according to core performance (such as how well each solution makes predictions and their inference time), we explore how the two solutions compare according to more business-oriented metrics, such as the cost of maintenance, clarity to business stakeholders, and stability postdeployment.</p>

  <p class="body">This chapter pulls together themes that you have seen across this book, incorporating what we learned in chapter 7 about XGBoost and what we learned about how to approach tabular problems with deep learning in chapters 1 and 8.</p>

  <p class="body">In chapter 1, we reviewed some of the papers that compare tabular data applications of classical machine learning with applications of deep learning. In this chapter, we will see that the results of our comparison of XGBoost with deep learning align with the observations of one of the papers we cited in chapter 1. In chapter 8, we assessed a variety of deep learning approaches to working with tabular data. Now we will use what we learned there to help us select a deep learning approach that has the best chance of being competitive with the performance of XGBoost.</p>

  <p class="body">By combining the two main threads that we have explored in this book (classical machine learning approaches and deep learning approaches to solving tabular data problems), this chapter provides a summary of what we have covered so far in this book and guidance to set you up for success in your application of machine learning to tabular data. The code shown in the chapter is available at <a class="url" href="https://mng.bz/vKPp">https://mng.bz/vKPp</a><a id="idTextAnchor009"/>.</p>

  <div class="calibre9"/>

  <div class="calibre10"/><h2 class="fm-head" id="heading_id_3">12.1 Review of the gradient boosting solution from chapter 7</h2>

  <p class="body"><a id="marker-443"/>In chapter 7, we assembled a dataset for Airbnb listings in Tokyo, analyzed the key characteristics of this dataset, and created an XGBoost model to predict the price of a listing. Starting at the Inside Airbnb Network website (<a class="url" href="http://insideairbnb.com/">http://insideairbnb.com/</a>), we downloaded the following files related to the city of Tokyo:<a id="idIndexMarker002"/><a id="idIndexMarker003"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">listings.csv</code>, which contains the summary listings and other information about the Airbnb accommodations in Tokyo</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">calendar.csv.gz</code>, a zipped file containing <code class="fm-code-in-text">calendar.csv</code>, a dataset containing occupancy and price information for a given year for each listing</p>
    </li>
  </ul>

  <p class="body">Recall that the dataset in <code class="fm-code-in-text">listings.csv</code> has the following columns<a id="idTextAnchor010"/>:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">id</code>—This is a unique identifier for each listing on Airbnb. It is an <code class="fm-code-in-text">int64</code> data type, meaning it is a numerical ID representation. In other tables, it can be referred to as <code class="fm-code-in-text">listing_id</code>.<a id="idIndexMarker004"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">name</code>—The description of the Airbnb listing. It is of the <code class="fm-code-in-text">object</code> data type, which typically represents a string or text.<a id="idIndexMarker005"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">host_id</code>—This is a unique identifier for each host on Airbnb. It is an <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker006"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">host_name</code>—The name of the host who owns the listing. It is of the <code class="fm-code-in-text">object</code> data type.<a id="marker-444"/><a id="idIndexMarker007"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">neighbourhood_group</code>—This field represents the broader area or region the neighborhood belongs to. It is stored as a <code class="fm-code-in-text">float64</code> data type, but it is important to note that using a float data type to represent groups or categories is uncommon. In this case, the presence of float values indicates that the data for this field is entirely made up of missing values.<a id="idIndexMarker008"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">neighbourhood</code>—The specific neighborhood where the listing is located. It is of the <code class="fm-code-in-text">object</code> data type.<a id="idIndexMarker009"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">latitude</code>—The latitude coordinates of the listing’s location. It is of the <code class="fm-code-in-text">float64</code> data type.<a id="idIndexMarker010"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">longitude</code>—The longitude coordinates of the listing’s location. It is of the <code class="fm-code-in-text">float64</code> data type.<a id="idIndexMarker011"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">room_type</code>—The type of room or accommodation offered in the listing (e.g., entire home/apartment, private room, shared room). It is of the <code class="fm-code-in-text">object</code> data type.<a id="idIndexMarker012"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">price</code>—The price per night to rent the listing. It is of the <code class="fm-code-in-text">int64</code> data type, representing an integer price value.<a id="idIndexMarker013"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">minimum_nights</code>—The minimum number of nights that is required for booking the listing. It is of the <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker014"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">number_of_reviews</code>—The total number of reviews received by the listing. It is of the <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker015"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">last_review</code>—The date of the last review received by the listing. It is of the <code class="fm-code-in-text">object</code> data type, which could represent date and time information, but it might require further parsing to be used effectively.<a id="idIndexMarker016"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">reviews_per_month</code>—The average number of reviews per month for the listing. It is of the <code class="fm-code-in-text">float64</code> data type.<a id="idIndexMarker017"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">calculated_host_listings_count</code>—The total number of listings the host has on Airbnb. It is of the <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker018"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">availability_365</code>—The number of days the listing is available for booking in a year (out of 365 days). It is of the <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker019"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">number_of_reviews_ltm</code>—The number of reviews received in the last 12 months. It is of the <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker020"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">license</code>—The license number or information related to the listing. It is of the <code class="fm-code-in-text">object</code> data type, which typically represents a string or text.<a id="idIndexMarker021"/></p>
    </li>
  </ul>

  <p class="body">The goal of the model we created in chapter 7 was to predict the price of a new listing. This is actually a more challenging problem for a deep learning model than the Airbnb NYC problem that we tackled in chapter 8, as we can see from the comp<a id="idTextAnchor011"/>arison in table 12.1.</p>

  <p class="fm-table-caption">Table 12.1 Airbnb NYC problem vs Airbnb Tokyo problem</p>

  <table border="1" class="contenttable-1-table" id="table001" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="20%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th"/>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Airbnb NYC</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Airbnb Tokyo</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Rows in dataset</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">48,000</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">10,000</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Columns in dataset</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">18</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">31</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Target</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Classification: predict whether the price is over or under the median price</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Regression: predict price</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body"><a id="marker-445"/>In fact, the Tokyo Airbnb dataset has fewer than 25% of the records of the NYC Airbnb dataset and over double the number of columns as the NYC Airbnb dataset. With fewer data points, you may need to rely more on domain expertise (hence the role of feature engineering). Having more columns implies there’s more risk of overfitting during training, and in any case, you have to deal with more complex relationships between features and the target variable itself.</p>

  <p class="body">Generally speaking, classical machine learning approaches can get better results on small datasets than deep learning. Techniques such as data augmentation can mitigate this downside of deep learning, but deep learning approaches will struggle with a dataset with fewer than tens of thousands of rows. Research on why deep learning requires more data is not complete, but the large number of parameters in deep learning architectures and the need for at least a certain amount of data for the model to generalize is identified as one of the reasons why deep learning struggles with problems having smaller datasets [see, for instance, “The Computational Limits of Deep Learning” by Thompson et al. (<a class="url" href="https://arxiv.org/pdf/2007.05558.pdf">https://arxiv.org/pdf/2007.05558.pdf</a>), which also argues how computational efficiency is necessary for the approach to progress].</p>

  <p class="body">The smaller number of rows in the Tokyo Airbnb dataset hence certainly presents a challenge for the successful implementation of a deep learning solution. In addition, the larger number of columns in the dataset necessitates more extensive preprocessing and feature engineering to ensure that the resulting features are relevant and usable for modeling purposes. Since we have followed the same data preparation steps as in chapter 7, we should not have to worry about having to deal with more columns for this particular application, but it is worth keeping in mind for other datasets that more columns equate to coming up with effective strategies for handling missing values, dealing with multicollinearity (as we discussed in chapter 2), and selecting the most informative features. In addition to requiring more data preparation, having more columns opens the possibility of having redundant or noisy features that could reduce the effectiveness of a deep learning solution on this dataset, although deep learning models, given their capability to capture complex relationships between features and the target variable, tend to be more robust to noise in the data.</p>

  <p class="body">Further, the problem we are tackling with the Tokyo Airbnb dataset is a regression (predicting the price of a listing) as opposed to a binary classification (predicting whether a given listing has a price above or below the median price) as we attempted to solve with the NYC Airbnb dataset, which has different business implications. If the goal is to provide a solution that returns business benefits, our solution should predict correctly as often as possible. With a binary classification problem, the solution is discrete (one class or the other), and the likelihood of the solution making a prediction that is correct from a business perspective is higher than it appears for a regression problem, where the business expectation is for the model to predict a price that closely matches the actual price but the outputs are continuous values that can be significantly different from the expected values. In short, a binary classification problem looks easier (from the point of view of satisfying the business need)—since it has a clear threshold for correctness—than a regression problem.</p>

  <p class="body">In chapter 7, after completing a set of transformations on the Airbnb Tokyo dataset, we end up with the following set of features for the dataset:<a id="marker-446"/></p>
  <pre class="programlisting">array(['onehot_encoding__room_type_Entire home/apt',
       'onehot_encoding__room_type_Hotel room',
       'onehot_encoding__room_type_Private room',
       'onehot_encoding__room_type_Shared room',
       'ordinal_encoding__neighbourhood_more_than_30',
       'ordinal_encoding__type_of_accommodation',
       'target_encoding__coordinates', 'numeric__minimum_nights',
       'numeric__number_of_reviews', 'numeric__days_since_last_review',
       'numeric__reviews_per_month',
       'numeric__calculated_host_listings_count',
       'numeric__availability_365', 'numeric__score',
       'numeric__number_of_reviews_ltm',
       'numeric__number_of_reviews_ltm_ratio',
       'numeric__number_of_bedrooms', 'numeric__number_of_beds',
       'numeric__number_of_baths', 'numeric__imperial_palace_distance',
       'numeric__nearest_convenience_store',
       'numeric__nearest_train_station', 'numeric__nearest_airport',
       'numeric__nearest_bus_station', 'numeric__nearest_subway',
       'binary__is_new', 'binary__is_studio', 'binary__has_shared_bath',
       'binary__has_half_bath'], dtype=object)</pre>

  <p class="body">All these features are numeric, and we also properly dealt with any missing values to be able to work with a linear model baseline. In fact, we started by creating a linear regression model to act as a baseline and give us a measuring stick against which to compare further improvements to the XGBoost model.</p>

  <p class="body">Preparing your data for linear regression or logistic regression (depending on whether it’s a regression or classification problem) automatically prepares your model for being processed by a neural network as well. However, while this is convenient, it might miss out on some specific preparations suitable only for neural networks. For instance, categorical features are commonly dealt with using one-hot-encoding in a linear model, whereas with a neural network, you can employ an encoding layer to convert categorical values into numeric ones directly during training.</p>

  <p class="body">After getting the baseline results for linear regression, we conducted a set of optimizations on the XGBoost model. We use that optimized XGBoost code as the basis for the gradient boosted solution to the Tokyo Airbnb problem, which we will use for the comparison with deep learning in this chapter. Listing 12.1 shows the XGBoost code that we will use to compare with a deep learning solution. This version of the XGBoost code is very close to the final XGBoost code used in chapter 7. The hyperparameters match the best hyperparameters from the notebook used in chapter 7; <a class="url" href="https://mng.bz/4a6R">https://mng.bz/4a6R</a>). In the version of the code used in this chapter, the predictions are saved in the <code class="fm-code-in-text">xgb_oof_preds</code> array so that they can be further processed or used in conjunction with the predictions we are going to obtain from a deep learning model.<a id="marker-447"/></p>

  <p class="fm-code-listing-caption"><a id="idTextAnchor012"/>Listing 12.1 Code for training the final XGBoost model</p>
  <pre class="programlisting">xgb_params =  {'booster': 'gbtree',                          <span class="fm-combinumeral">①</span>
               'objective': 'reg:tweedie', 
               'n_estimators': 932, 
               'learning_rate': 0.08588055025922144, 
               'subsample': 0.9566295202123205, 
               'colsample_bytree': 0.6730567082779646, 
               'max_depth': 7, 
               'min_child_weight': 6, 
               'reg_lambda': 6.643211493348415e-06, 
               'reg_alpha': 7.024597970671363e-05, 
               'tweedie_variance_power': 1.6727891016980427}
 
 
from sklearn.metrics import r2_score  
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error              <span class="fm-combinumeral">②</span>
from XGBoost import XGBRegressor
import numpy as np
 
xgb = XGBRegressor(**xgb_params)                             <span class="fm-combinumeral">③</span>
 
cv_splits = cv.split(X, y=neighbourhood_more_than_30)        <span class="fm-combinumeral">④</span>
 
r2_scores = []
rmse_scores = []
mae_scores = []
xgb_oof_preds = np.zeros(len(X))
 
for train_index, test_index in cv_splits:                    <span class="fm-combinumeral">⑤</span>
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
 
    xgb.fit(X_train, y_train)
    y_pred = xgb.predict(X_test)                             <span class="fm-combinumeral">⑥</span>
    xgb_oof_preds[test_index] = y_pred
 
    r2_scores.append(r2_score(y_test, y_pred))               <span class="fm-combinumeral">⑦</span>
    rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))
    mae_scores.append(mean_absolute_error(y_test, y_pred))
 
print(f"Mean cv R-squared: {np.mean(r2_scores):.3f}")        <span class="fm-combinumeral">⑧</span>
print(f"Mean cv RMSE: {np.mean(rmse_scores):.3f}")
Print(f"Mean cv MAE: {np.mean(mae_scores):.3f}")</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Sets hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Imports required libraries</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Sets up an XGBoost regressor with the specified hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Defines cross-validation splits based on the neighbourhood_more_than_30 feature</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Generates cross-validation splits based on the neighbourhood_more_than_30 feature</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Performs cross-validated predictions</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Calculates R-squared, root mean squared error, and mean absolute error evaluation metrics to assess the model’s performance</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Prints mean values for R-squared, root mean squared error, and mean absolute error</p>

  <p class="body">The optimizations performed on the XGBoost solution produce results that are significantly better than the linear regression baseline, as summarized in table 12.2.</p>

  <p class="fm-table-caption"><a id="idTextAnchor013"/>Table 12.2 Summary of the results from chapter 7</p>

  <table border="1" class="contenttable-1-table" id="table002" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="20%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Metric</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Linear regression baseline</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Optimized XGBoost</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">R-squared</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.320</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.729</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Root mean squared error</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">17197.323</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">10853.661</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Mean absolute error</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">12568.371</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">6611.609</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">As a further check, you can verify that for all three of the metrics we tracked in chapter 7—that is, R-squared, root mean squared error (RMSE), and mean absolute error (MAE)—the optimized XGBoost model is definitely always an improvement on the linear regression baseline. Now, using the XGBoost model as our reference, we will explore what kind of results we can get with the same dataset using a deep learning model in the remainder of this chapter.<a id="idIndexMarker022"/><a id="idTextAnchor014"/><a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="idIndexMarker025"/><a id="marker-448"/></p>

  <h2 class="fm-head" id="heading_id_4">12.2 Selecting a deep learning solution</h2>

  <p class="body">In chapter 8, we reviewed a set of different deep learning stacks for working with tabular data, including Keras, fastai, and other libraries specifically designed for tabular data, like TabNet. If we now want to compare the XGBoost solution to the Tokyo Airbnb problem from chapter 7, which deep learning approach should we use?<a id="idIndexMarker026"/></p>

  <p class="body">As a reminder, we shared a comparison of the different deep learning approaches using the NYC Airbnb dataset, which is shown again in table 1<a id="idTextAnchor015"/>2.3.</p>

  <p class="fm-table-caption">Table 12.3 Comparison of deep learning options</p>

  <table border="1" class="contenttable-1-table" id="table003" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="10%"/>
      <col class="contenttable-0-col" span="1" width="30%"/>
      <col class="contenttable-0-col" span="1" width="30%"/>
      <col class="contenttable-0-col" span="1" width="30%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th"/>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Keras</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">fastai</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Tabular data library (e.g., TabNet)</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Pro</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Model details are transparent.</p>

          <p class="fm-table-body">A large community using the framework means that it’s easy to find solutions to common problems.</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">The framework includes explicit support for tabular data models, which means the code will be more compact. It also sets intelligent defaults so we can quickly reach reasonable results.</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">A bespoke library explicitly created to handle tabular datasets</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Con</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">No built-in support for tabular data</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">If we run into a problem, we could be on our own getting a resolution because the community is smaller than the Keras one.</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Up to now, no library has emerged as an obvious choice; the fragmented community and inconsistent maintenance of some libraries make it a challenge to get the basic code to run reliably.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body"><a id="marker-449"/>The differences between the dataset we used in chapter 8 and the dataset we will use now for the comparison between XGBoost and deep learning—that is, a dataset that was previously both much larger (with four times as many rows as the Tokyo Airbnb dataset) and much simpler (with less than half as many columns as the Tokyo Airbnb dataset)—for our comparison with XGBoost do not significantly weight in favor of any of the proposed solutions. In fact, Keras and fastai are general-purpose deep learning frameworks and are not specifically designed for small or complex datasets. TabNet’s design gives it an edge with high-dimensional data, but when applied to smaller datasets, the advantage over Keras or fastai turns out to be less significant.</p>

  <p class="body">What actually weighs more in our choice is the fact that we want a fair comparison between XGBoost and a deep learning approach. As you saw in chapter 7, XGBoost shines with its ease of use, and it gets good results without a lot of tweaking. If we decide on a complex deep learning model that takes a long time to set up, tweak, and optimize, it wouldn’t turn into a fair comparison for the deep learning model.</p>

  <p class="body">Given this, which deep learning framework should we choose? We will pass over TabNet because of the complexity of getting it to run reliably. That leaves us with a choice between Keras and fastai. As mentioned in table 12.3, Keras is indeed popular in production and boasts a larger community. However, fastai aligns better with our goals. Recall that in chapter 8 we noted that fastai is built for tabular data like ours, and it comes with smart defaults. This means you can get decent results quickly, without spending ages on optimizations. fastai handles a lot of the nitty-gritty stuff and details for you behind the scenes. As you’ll see later, choosing fastai for this problem paid off. For the moment, we believe it provides a strong deep learning solution for the Tokyo Airbnb problem without much hassle, ready in a few steps to take on the XGBoost model from chapter 7.</p>

  <h2 class="fm-head" id="heading_id_5">12.3 <a id="idTextAnchor016"/>Selected deep learning solution to the Tokyo Airbnb problem</h2>

  <p class="body">So far in this chapter we have reviewed the XGBoost solution to the Tokyo Airbnb problem, reviewed options for a deep learning solution to compare it with, and selected fastai as the deep learning framework to use for the comparison with XGBoost. In this section, we’ll go through the details of the fastai solution for the Tokyo Airbnb problem.<a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="marker-450"/></p>

  <p class="body">Listing 12.2 shows the core of the fastai model that we used to compare with the XGBoost solution. This code trains a fastai regression model on the Tokyo Airbnb dataset using the <code class="fm-code-in-text">TabularPandas</code> function (<a class="url" href="https://mng.bz/QDR6">https://mng.bz/QDR6</a>), which is a wrapper providing all the necessary transformations under the hood<a id="idTextAnchor017"/>.<a id="idIndexMarker029"/></p>

  <p class="fm-code-listing-caption">Listing 12.2 fastai model for the Tokyo Airbnb problem</p>
  <pre class="programlisting">from fastai.tabular.all import *
 
procs = [FillMissing, Normalize, Categorify]
cat_vars = [
    col for col in airbnb_tokyo.columns 
    if "onehot_encoding__" in col
    or ordinal_encoding__" in col 
    or "binary__" in col
]
cont_vars = [
    col for col in airbnb_tokyo.columns 
    if "numeric__" in col 
    or "target_encoding__" in col
]
dep_var = 'target'
 
cv_splits = cv.split(X, y=neighbourhood_more_than_30)
 
r2_scores = []
rmse_scores = []
mae_scores = []
dnn_oof_preds = np.zeros(len(X))
 
for k, (train_index, test_index) in enumerate(cv_splits):
    X_train = airbnb_tokyo.set_index("listing_id").iloc[train_index].copy()
    X_test = airbnb_tokyo.set_index("listing_id").iloc[test_index].copy()
    y_test = airbnb_tokyo["target"].iloc[test_index].copy()
 
    tab = TabularPandas(
        X_train, procs, cat_vars, cont_vars, 
        dep_var, y_block=RegressionBlock(),                       <span class="fm-combinumeral">①</span>
        splits=RandomSplitter(
             valid_pct=0.2, seed=0)(range_of(X_train)),
        inplace=True, 
        reduce_memory=True
    )
 
    dls = tab.dataloaders(bs=128)                                 <span class="fm-combinumeral">②</span>
    y_range = torch.tensor([0, X_train['target'].max() * 1.2])
    tc = tabular_config(ps=[0.001, 0.01], embed_p=0.04, y_range=y_range)
    learn = tabular_learner(dls, layers=[1000,500],               <span class="fm-combinumeral">③</span>
                            metrics=mae,
                            config=tc,
                            loss_func=L1LossFlat())
    with learn.no_bar(), learn.no_logging():
        lr = learn.lr_find(show_plot=False)
        learn.fit_one_cycle(80, lr.valley)                        <span class="fm-combinumeral">④</span>
 
    dl = learn.dls.test_dl(X_test)
    y_pred = (
        learn.get_preds(dl=dl)[0]
        .numpy()
        .ravel()
    )                                                             <span class="fm-combinumeral">⑤</span>
    dnn_oof_preds[test_index] = y_pred
 
    r2_scores.append(r2_score(y_test, y_pred))                    <span class="fm-combinumeral">⑥</span>
    rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))
    mae_scores.append(mean_absolute_error(y_test, y_pred))
    print(f"CV Fold {k} MAE: {mae_scores[-1]:.3f}")
 
print(f"\nMean cv R-squared: {np.mean(r2_scores):.3f}")
print(f"Mean cv RMSE: {np.mean(rmse_scores):.3f}")
print(f"Mean cv MAE: {np.mean(mae_scores):.3f}")</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines a fastai TabularPandas object</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines a dataloaders object based on the TabularPandas object</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Defines a tabular_learner object based on the dataloaders object</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Trains the model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Gets predictions from the model on the test set</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Saves the metrics</p>

  <p class="body">As you can see from the code in listing 12.2, the solution for fastai is straightforward. First, we define the preprocessing steps (<code class="fm-code-in-text">procs</code>), such as filling missing values, normalization, and categorization. Then we separate categorical and continuous variables from the dataset and choose the dependent variable (<code class="fm-code-in-text">dep_var</code>). After that, we iterate through a stratified k-fold cross-validation in the same manner as we did for the XGBoost solution. <a id="idIndexMarker030"/><a id="marker-451"/><a id="idIndexMarker031"/></p>

  <p class="body">During the iterations, the training data is preprocessed using <code class="fm-code-in-text">TabularPandas</code>, specifying categorical and continuous variables, the target variable, and data splits. DataLoader objects (<code class="fm-code-in-text">dls</code>) are created for training and validation batches. After defining a neural network model (<code class="fm-code-in-text">tabular_learner</code>) consisting of two layers, the first with 1,000 neurons and the following with 500 nodes, and their dropout rates (using <code class="fm-code-in-text">tabular_config</code> and setting a higher dropout for the last layer), we train the model using the learning rate found by <code class="fm-code-in-text">lr_find</code> and using the one cycle (<code class="fm-code-in-text">fit_one_cycle</code>) procedure.<a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/><a id="idIndexMarker036"/><a id="idIndexMarker037"/></p>

  <p class="body">By combining the <code class="fm-code-in-text">lr_find</code> and <code class="fm-code-in-text">fit_one_cycle</code> procedures, we automatically tune the learning rate parameter for the best results on the type of data we are processing, thus achieving a straightforward solution without much tweaking and experimentation. The procedure <code class="fm-code-in-text">lr_find</code> (<a class="url" href="https://mng.bz/Xxq9">https://mng.bz/Xxq9</a>) explores a range of learning rates on a sample of the data, stopping when the learning rate is so high that the learning diverges. While the procedure takes some time, it is relatively fast and returns the value of the learning parameter that is two-thirds of the way along the section of the loss curve where the loss is decreasing. We use that value as the upper boundary of another procedure, the <code class="fm-code-in-text">fit_one_cycle</code> (<a class="url" href="https://mng.bz/yWZp">https://mng.bz/yWZp</a>), which is a training method where the learning rate is not fixed or constantly decreasing but rather oscillates between a minimum and a maximum value. The oscillations allow for the network to not get stuck in a local minima, and overall the resulting network performs better than using other approaches, especially when dealing with tabular data. Both methods have been developed by Leslie Smith in a series of papers:<a id="idIndexMarker038"/><a id="idIndexMarker039"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">“Cyclical Learning Rates for Training Neural Networks” (<a class="url" href="https://arxiv.org/abs/1506.01186">https://arxiv.org/abs/1506.01186</a>)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">“Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates” (<a class="url" href="https://arxiv.org/abs/1708.07120">https://arxiv.org/abs/1708.07120</a>)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">“A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch Size, Momentum, and Weight Decay” (<a class="url" href="https://arxiv.org/abs/1803.09820">https://arxiv.org/abs/1803.09820</a>)</p>
    </li>
  </ul>

  <p class="body">To our knowledge, the implementation by fastai of these methods is the most efficient and high-performing available in the open-source community.</p>

  <p class="body">Proceeding with the code, for each pass through the <code class="fm-code-in-text">for</code> loop, the predictions for the current fold and the R-squared, RMSE, and MAE evaluations are saved. The mean values for all the metrics are printed after the loop so we can get an idea of the overall values for the fastai solution. Note that we will recalculate these values as we go through the ensembling process when we compare the predictions and actual y values for an ensemble that is 100% fas<a id="idTextAnchor018"/>tai results.<a id="idIndexMarker040"/><a id="idIndexMarker041"/></p>

  <h2 class="fm-head" id="heading_id_6">12.4 Comparing the XGBoost and fastai solutions to the Tokyo Airbnb problem</h2>

  <p class="body">Now that we have a deep learning solution for the Tokyo Airbnb problem, we can compare its results against the XGBoost solution. By comparing the metrics that we collected for both solutions (R-squared, RMSE, and MAE), we can get an idea of how effective each solution is at solving the Tokyo Airbnb problem. Table 12.4 contains a summary of the results of bo<a id="idTextAnchor019"/>th approaches.<a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="marker-452"/></p>

  <p class="fm-table-caption">Table 12.4 Comparison of results from XGBoost and fastai models</p>

  <table border="1" class="contenttable-1-table" id="table004" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="20%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Metric</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">XGBoost</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Fastai</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">R-squared</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.599</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">0.572</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">RMSE</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">10783.027</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">11719.387</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">MAE</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">6531.102</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">7152.143</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">Table 12.4 demonstrates that XGBoost provides better results than fastai for all three error metrics—in fact, the R-squared value for XGBoost is significantly higher, and its RMSE and MAE values are about 8% to 9% lower.</p>

  <p class="body">In addition to the basic comparison of error metrics in table 12.4, we can visualize some differences relative to how the two approaches deal with the problem. For example, we can directly compare XGBoost and fastai predictions by examining each predicted data point in the Tokyo Airbnb test set. In figure 12.1, the x-axis represents the prediction from XGBoost, while the y-axis represents the prediction from fastai for each respective data point. The figure shows the relationship between XGBoost and fastai predictions.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F01_Ryan2.png"/></p>

    <p class="figurecaption">F<a id="idTextAnchor020"/>igure 12.1 Scatterplot of predictions for XGBoost and fastai</p>
  </div>

  <p class="body"><a id="marker-453"/>On the diagonal of the chart, the solid trend line coincides fairly well with the dashed diagonal line, showing that, overall, there is not a huge variance in the predictions between XGBoost and fastai. The solid trend line, showing the smoothed regression line (a technique called LOWESS [LOcally WEighted Scatterplot Smoothing]) between the predictions from XGBoost and fastai, doesn’t deviate significantly in comparison from the dashed line, confirming that, even if the algorithms tend to disagree in their predictions, there is no systematic over- or underestimation from one of the two in respect of the other.<a id="idIndexMarker044"/></p>

  <p class="body">In addition, we can also try to explore how their predictions relate, using the value of 70,000 on the x-axis as a pivot, since we can spot two distinct clusters of predictions. We observe that the average fastai prediction is around 81,300 against a XGBoost average at 81,550 for XGBoost predictions over 70,000 and around 23,050 against 22,500 for XGBoost predictions below 70,000.</p>

  <p class="fm-code-listing-caption"><a id="idTextAnchor021"/>Listing 12.3 Average fastai predictions for XGBoost predictions</p>
  <pre class="programlisting">predictions = pd.DataFrame(
    {'xgb': xgb_oof_preds, 'fastai': dnn_oof_preds}
)                                                             <span class="fm-combinumeral">①</span>
 
avg_fastai_over_70000 = predictions.loc[
    predictions['xgb'] &gt; 70000, 'fastai'
].mean()                                                      <span class="fm-combinumeral">②</span>
avg_xgb_over_70000 = predictions.loc[
    predictions['xgb'] &gt; 70000, 'xgb'
].mean()
print(f"Average prediction values when xgb &gt; 70000:",
      f"fastai:{avg_fastai_over_70000:0.2f}",
      f"xgb:{avg_xgb_over_70000:0.2f}")
 
 
avg_fastai_under_70000 = predictions.loc[predictions['xgb'] &lt;= 70000, 'fastai'].mean()
avg_xgb_under_70000 = predictions.loc[predictions['xgb'] &lt;= 70000, 'xgb'].mean()
print(f"Average prediction values when xgb &lt;= 70000: fastai:{avg_fastai_under_70000:0.2f}    
      xgb:{avg_xgb_under_70000:0.2f}")</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Gets a pandas DataFrame with a column each for XGBoost and fastai predictions</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Computes mean statistics for XGBoost and fastai predictions depending on XGBoost predicted value</p>

  <p class="body"><a id="marker-454"/>The differences between the two models are minimal; on average, fastai and XGBoost predictions tend to be aligned. Fastai tends to overestimate for lower predicted pricing levels and slightly underestimate for higher predicted pricing levels.</p>

  <p class="body">Returning to figure 12.1, now that we have examined the chart comparing the predictions, let’s look at a chart comparing the error for XGBoost with the error for fastai. For each data point in the Tokyo Airbnb test set, the plot in figure 12.2 shows the error for that data point (the absolute value of the difference between the prediction and the actual value), with the x value being the XGBoost error and the y value being the fastai error.</p>

  <p class="body">Figure 12.2 shows that there is a large cluster of data points where the error for both XGBoost and fastai is lower than 20,000. The overall LOWESS line, in red, shows that the error for XGBoost is overall lower than for fastai most of the time.</p>

  <p class="body">As shown by the aggregate error metrics and the plots for predictions and errors, the XGBoost model displays better performance than the fastai model. However, given the spread of the predictions between the two models, we believe that there is an opportunity to do even better than XGBoost alone by ensembling the two models because there is a strong hint that, apart from the differences in performance, the two models operate differently in their predictions by capturing different data patterns and characteristics. In the next section, we will find out whether ensembling improves the results.<a id="idIndexMarker045"/><a id="idIndexMarker046"/><a id="idTextAnchor022"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F02_Ryan2.png"/></p>

    <p class="figurecaption">F<a id="idTextAnchor023"/>igure 12.2 Scatterplot of errors for XGBoost and fastai</p>
  </div>

  <h2 class="fm-head" id="heading_id_7">12.5 Ensembling the two solutions to the Tokyo Airbnb problem</h2>

  <p class="body">Now that we have established the performance of the XGBoost and fastai solutions to the Tokyo Airbnb problem in isolation, we will look at ensembling the two solutions to see whether a combination of the two approaches provides any improvements.<a id="idIndexMarker047"/><a id="idIndexMarker048"/><a id="marker-455"/></p>

  <p class="body">The following listing shows the loop where we ensemble the results of the two model<a id="idTextAnchor024"/>s.</p>

  <p class="fm-code-listing-caption">Listing 12.4 Code to ensemble the two models</p>
  <pre class="programlisting">blend_list = [
    [1., 0.], [0., 1.], [0.25,0.75],
    [0.75,0.25],[.5, .5]
]                                                            <span class="fm-combinumeral">①</span>
for a, b in blend_list:
    print(f"XGBoost weight={a}, DNN weight={b}")
    blended_oof_preds = (
        xgb_oof_preds * a + dnn_oof_preds * b
    )                                                        <span class="fm-combinumeral">②</span>
    r2 = r2_score(blended_oof_preds, y)                      <span class="fm-combinumeral">③</span>
    rmse = np.sqrt(mean_squared_error(blended_oof_preds, y))
    mae = mean_absolute_error(blended_oof_preds, y)
    print(f"blended result for R-squared: {r2:.3f}")
    print(f"blended result for RMSE: {rmse:.3f}")
    print(f"blended result for MAE: {mae:.3f}\n")</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Ensembles ratios to iterate through</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Generates predictions that are blended according to the ensembling ratios</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Gets R-squared, RMSE, and MAE for the blended predictions</p>

  <p class="body">The code in listing 12.4 combines results from XGBoost and fastai according to the blending values in <code class="fm-code-in-text">blend_list</code>. Note that these blending values are not optimized to find the absolute optimum—we are simply using a set of fixed blending values to get a general sense of the effect of blending results. Also, note that we are evaluating the results using out-of-fold predictions. Nevertheless, by combining the predictions from XGBoost and fastai according to the proportions specified by <code class="fm-code-in-text">blend_list</code>, we can see the effect of ensembling the two approaches across a range of values. <a id="idIndexMarker049"/><a id="marker-456"/></p>

  <p class="body">The output of the blending code for a typical run is</p>
  <pre class="programlisting">XGBoost weight=1.0, DNN weight=0.0
blended result for R-squared: 0.599
blended result for RMSE: 10783.027
blended result for MAE: 6531.102
 
XGBoost weight=0.75, DNN weight=0.25
blended result for R-squared: 0.619
blended result for RMSE: 10507.904
blended result for MAE: 6366.257
 
XGBoost weight=0.5, DNN weight=0.5
blended result for R-squared: 0.625
blended result for RMSE: 10527.024
blended result for MAE: 6384.576
 
XGBoost weight=0.25, DNN weight=0.75
blended result for R-squared: 0.618
blended result for RMSE: 10838.831
blended result for MAE: 6566.663
 
XGBoost weight=0.0, DNN weight=1.0
blended result for R-squared: 0.599
blended result for RMSE: 11419.374
blended result for MAE: 6959.540</pre>

  <p class="body">These figures may be a bit difficult to interpret, so let’s see what they look like in the form of a chart. Figure 12.3 shows the results for R-squared, RMSE, and MAE for a range of blending between the XGBoost and fastai models.<a id="idTextAnchor025"/></p>

  <p class="body">As figure 12.3 shows, we get the optimal results for R-squared evaluation when we use a 50/50 blend of the predictions from the XGBoost and fastai models. For error-based measures, RMSE and MAE, we obtain a better result weighting more XGBoost than fastai; however, if we were to use a 50/50 blend, we would obtain only slightly worse scores. The worst results are when we use 100% of prediction from fastai, as we would expect from the results we got from looking at each model in isolation, but it is interesting to notice that using only XGBoost is always worse than blending it with the other solution using a 75/25 or 50/50 share.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F03_Ryan2.png"/></p>

    <p class="figurecaption">Figure 12.3 Results of blending XGBoost and fastai models</p>
  </div>

  <p class="body">Ensembling the XGBoost and fastai models gets better results than using either model in isolation. As we will see in the next section, our observations from ensembling are consistent with the results shared in an important research paper that compared classical machine learning approaches and deep learning on tabular data problem<a id="idTextAnchor026"/>s.<a id="idIndexMarker050"/><a id="marker-457"/><a id="idIndexMarker051"/></p>

  <h2 class="fm-head" id="heading_id_8">12.6 Overall comparison of gradient boosting and deep learning</h2>

  <p class="body">In chapter 1, we introduced the controversy over whether deep learning is needed to solve problems involving tabular data. We cited academic papers that support both sides of the argument—those that advocate for deep learning approaches and those that maintain that classical machine learning approaches, in particular gradient boosting, consistently outperform deep learning. One of the papers that we mentioned in chapter 1 is worth revisiting here: “Tabular Data: Deep Learning Is Not All You Need,” by Ravid Shwartz-Ziv and Amitai Armon (<a class="url" href="https://arxiv.org/abs/2106.03253">https://arxiv.org/abs/2106.03253</a>). In the Discussion and Conclusions section of this paper, the authors make the following statement:<a id="idIndexMarker052"/><a id="idIndexMarker053"/></p>

  <p class="quote">In our analysis, the deep models were weaker on datasets that did not appear in their original papers, and they were weaker than XGBoost, the baseline model. Therefore, we proposed using an ensemble of these deep models with XGBoost. This ensemble performed better on these datasets than any individual model and the 'non-deep’ classical ensemble.</p>

  <p class="body">Their observation is not a hard and fast rule, because, in our experience, we encountered situations where using a gradient boosting solution or a deep learning one alone resulted in the best performances. However, in many of the situations we faced, we can confirm that simply averaging solutions resulted in better predictions. We are strongly convinced that this happened because the two algorithms have two different ways of optimizing predictions. Gradient boosting is based on decision trees, which are a form of analogy search because, as an algorithm, trees split your dataset into parts where the features tend to be similar in values between themselves and map to similar target outputs. The gradient part of the algorithm intelligently ensembles multiple trees together for a better prediction, although it doesn’t change the basic way in which decision trees behave. Deep learning, on the other hand, is purely based on the principle of differentiation and nonlinear transformations, where the algorithm looks for the best weights to combine the transformed inputs.</p>

  <p class="body">These two different approaches result in quite different estimations whose errors tend to partially cancel each other out because they are strongly uncorrelated, in a similar fashion to what actually happens in a random forest algorithm when you average the results of uncorrelated decision trees. This conclusion matches our experience comparing the results of gradient boosting and deep learning on the Tokyo Airbnb problem. The ensemble of the XGBoost model with the fastai model produced the best results, as shown in figure<a id="idTextAnchor027"/> 12.3.<a id="marker-458"/></p>

  <h2 class="fm-head" id="heading_id_9">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The XGBoost solution to the Tokyo Airbnb problem shown in chapter 7 provides a baseline that we can use to assess the efficacy of deep learning to solve the same problem.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Using the XGBoost solution from chapter 7 as a starting point, we can create a deep learning solution for the Tokyo Airbnb problem. The fastai library provides a compact and relatively well-performing deep learning solution.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">By blending the predictions of the XGBoost and fastai models across a range of ratios, from 100% XGBoost to 100% fastai, we can see the effect of ensembling the models. We get the optimal results with a 50/50 ensemble of the two models.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">This result matches the recommendation from research focused on scrutinizing the claims of the efficacy of deep learning on tabular data.</p>
    </li>
  </ul>
</body></html>