["```py\nimport numpy as np\n\nX = [...]  # create a small 3D dataset\nX_centered = X - X.mean(axis=0)\nU, s, Vt = np.linalg.svd(X_centered)\nc1 = Vt[0]\nc2 = Vt[1]\n```", "```py\nW2 = Vt[:2].T\nX2D = X_centered @ W2\n```", "```py\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX2D = pca.fit_transform(X)\n```", "```py\n>>> pca.explained_variance_ratio_\narray([0.82279334, 0.10821224])\n```", "```py\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', as_frame=False)\nX_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\nX_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n\npca = PCA()\npca.fit(X_train)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95) + 1  # d equals 154\n```", "```py\npca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(X_train)\n```", "```py\n>>> pca.n_components_\nnp.int64(154)\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import make_pipeline\n\nclf = make_pipeline(PCA(random_state=42),\n                    RandomForestClassifier(random_state=42))\nparam_distrib = {\n    \"pca__n_components\": np.arange(10, 80),\n    \"randomforestclassifier__n_estimators\": np.arange(50, 500)\n}\nrnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3,\n                                random_state=42)\nrnd_search.fit(X_train[:1000], y_train[:1000])\n```", "```py\n>>> print(rnd_search.best_params_)\n{'randomforestclassifier__n_estimators': np.int64(475),\n 'pca__n_components': np.int64(57)}\n```", "```py\nX_recovered = pca.inverse_transform(X_reduced)\n```", "```py\nrnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\nX_reduced = rnd_pca.fit_transform(X_train)\n```", "```py\nfrom sklearn.decomposition import IncrementalPCA\n\nn_batches = 100\ninc_pca = IncrementalPCA(n_components=154)\nfor X_batch in np.array_split(X_train, n_batches):\n    inc_pca.partial_fit(X_batch)\n\nX_reduced = inc_pca.transform(X_train)\n```", "```py\nfilename = \"my_mnist.mmap\"\nX_mmap = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)\nX_mmap[:] = X_train  # could be a loop instead, saving the data chunk by chunk\nX_mmap.flush()\n```", "```py\nX_mmap = np.memmap(filename, dtype=\"float32\", mode=\"readonly\").reshape(-1, 784)\nbatch_size = X_mmap.shape[0] // n_batches\ninc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\ninc_pca.fit(X_mmap)\n```", "```py\n>>> from sklearn.random_projection import johnson_lindenstrauss_min_dim\n>>> m, ε = 5_000, 0.1\n>>> d = johnson_lindenstrauss_min_dim(m, eps=ε)\n>>> d\n7300\n```", "```py\nn = 20_000\nrng = np.random.default_rng(seed=42)\nP = rng.standard_normal((d, n)) / np.sqrt(d)  # std dev = sqrt(variance)\n\nX = rng.standard_normal((m, n))  # generate a fake dataset\nX_reduced = X @ P.T\n```", "```py\nfrom sklearn.random_projection import GaussianRandomProjection\n\ngaussian_rnd_proj = GaussianRandomProjection(eps=ε, random_state=42)\nX_reduced = gaussian_rnd_proj.fit_transform(X)  # same result as above\n```", "```py\ncomponents_pinv = np.linalg.pinv(gaussian_rnd_proj.components_)\nX_recovered = X_reduced @ components_pinv.T\n```", "```py\nfrom sklearn.datasets import make_swiss_roll\nfrom sklearn.manifold import LocallyLinearEmbedding\n\nX_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\nX_unrolled = lle.fit_transform(X_swiss)\n```"]