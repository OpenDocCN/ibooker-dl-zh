- en: 'Chapter 11\. Magic Wand: Building an Application'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, our example applications have worked with data that human beings can
    easily comprehend. We have entire areas of our brain devoted to understanding
    speech and vision, so it’s not difficult for us to interpret visual or audio data
    and form an idea of what’s going on.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of data, however, is not so easily understood. Machines and their sensors
    generate huge streams of information that don’t map easily onto our human senses.
    Even when represented visually, it can be difficult for our brains to grasp the
    trends and patterns within the data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the two graphs presented in [Figure 11-1](#jogging) and [Figure 11-2](#walking_downstairs)
    show sensor data captured by mobile phones placed in the front pockets of people
    doing exercise. The sensor in question is an *accelerometer*, which measures acceleration
    in three dimensions (we’ll talk more about these later). The graph in [Figure 11-1](#jogging)
    shows accelerometer data for a person who is jogging, whereas the graph in [Figure 11-2](#walking_downstairs)
    shows data for the same person walking down stairs.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it’s tough to distinguish between the two activities, even though
    the data represents a simple and relatable activity. Imagine trying to distinguish
    between the operating states of a complex industrial machine, which might have
    hundreds of sensors measuring all sorts of obscure properties.
  prefs: []
  type: TYPE_NORMAL
- en: It’s often possible to write handcrafted algorithms that can make sense of this
    type of data. For example, an expert in human gait might recognize the telltale
    signs of walking up stairs, and be able to express this knowledge as a function
    in code. This type of function is called a *heuristic*, and it’s commonly used
    in all sorts of applications, from industrial automation to medical devices.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of accelerometer data from someone jogging](Images/timl_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. Graph showing data for a person who is jogging [(MotionSense dataset)](https://oreil.ly/ZUPV5)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![A graph of accelerometer data from someone walking upstairs](Images/timl_1102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. Graph showing data for a person who is walking down stairs [(MotionSense
    dataset)](https://oreil.ly/ZUPV5)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To create a heuristic, you need two things. The first is domain knowledge. A
    heuristic algorithm expresses human knowledge and understanding, so to write one,
    you need to already understand what the data means. To understand this, imagine
    a heuristic that determines whether a person has a fever based on their body temperature.
    Whoever created it must have had knowledge of the temperature changes that indicate
    a fever.
  prefs: []
  type: TYPE_NORMAL
- en: The second requirement for building a heuristic is programming and mathematical
    expertise. Although it’s fairly easy to determine whether someone’s temperature
    is too high, other problems can be far more complex. Discerning a system’s state
    based on complex patterns in multiple streams of data might require knowledge
    of some advanced techniques, like statistical analysis or signal processing. For
    example, imagine creating a heuristic to distinguish between walking and running
    based on accelerometer data. To build this, you might need to know how to mathematically
    filter the accelerometer data to get an estimate of step frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Heuristics can be extremely useful, but the fact that they require domain knowledge
    and programming expertise means that they can be a challenge to build. First,
    domain knowledge is not always available. For example, a small company might not
    have the resources to conduct the basic research necessary to know what indicates
    one state versus another. Similarly, even given domain knowledge, not everyone
    has the expertise required to design and implement the heuristic algorithm in
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning gives us an opportunity to shortcut these requirements. A model
    trained on labeled data can learn to recognize the signals that indicate one class
    or another, meaning there’s less need for deep domain knowledge. For example,
    a model can learn the human temperature fluctuations that indicate a fever without
    ever being told which specific temperatures are important—all it needs is temperature
    data labelled with “fever” or “nonfever.” In addition, the engineering skills
    required to work with machine learning are arguably easier to acquire than those
    that might be required to implement a sophisticated heuristic.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of having to design a heuristic algorithm from scratch, a machine learning
    developer can find a suitable model architecture, collect and label a dataset,
    and iteratively create a model through training and evaluation. Domain knowledge
    is still extremely helpful, but it might no longer be a prerequisite to getting
    something working. And in some cases, the resulting model can actually be more
    accurate than the best handcoded algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, a recent paper^([1](ch11.xhtml#idm46473555451544)) showed how a simple
    convolutional neural network is able to detect congestive heart failure in a patient
    from a single heartbeat *with 100% accuracy*. This is better performance than
    any previous diagnostic technique. The paper is a fascinating read, even if you
    don’t understand every detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'By training a deep learning model to understand complex data and embedding
    it in a microcontroller program, we can create smart sensors that are able to
    understand the complexities of their environments and tell us, at a high level,
    what is going on. This has huge implications across dozens of fields. Here are
    just a few potential applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Environmental monitoring in remote places with poor connectivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated industrial processes that adjust to problems in real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robots that react to complex external stimuli
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disease diagnosis without the need for medical professionals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computer interfaces that understand physical movement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we build a project in the final category: a digital “magic
    wand,” which can be waved by its owner to cast a variety of spells. As its input,
    it takes complex, multidimensional sensor data that would be inscrutable to a
    human. Its output will be a simple classification that alerts us if one of several
    classes of movements has recently occurred. We’ll look at how deep learning can
    transform strange numerical data into meaningful information—to magical effect.'
  prefs: []
  type: TYPE_NORMAL
- en: What We’re Building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our “magic wand” can be used to cast several types of spells. To do so, the
    wielder need only wave the wand in one of three gestures, named “wing,” “ring,”
    and “slope,” as shown in [Figure 11-3](#gesture_spells).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagrams of the three magic wand gestures](Images/timl_1103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-3\. The three magic wand gestures
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The wand will react to each spell by lighting an LED. In case the magic of electric
    light is not sufficiently exciting, it will also output information to its serial
    port, which can be used to control an attached computer.
  prefs: []
  type: TYPE_NORMAL
- en: To understand physical gestures, the magic wand application uses a device’s
    accelerometer to collect information about its motion through space. An accelerometer
    measures the degree of acceleration that it is currently experiencing. For example,
    imagine that we’ve attached an accelerometer to a car that has stopped at a red
    light and is about to drive away.
  prefs: []
  type: TYPE_NORMAL
- en: When the light turns green, the car starts moving forward, increasing in speed
    until it reaches the speed limit. During this period, the accelerometer will output
    a value that indicates the car’s rate of acceleration. After the car has reached
    a steady speed, it is no longer accelerating, so the accelerometer will output
    zero.
  prefs: []
  type: TYPE_NORMAL
- en: The SparkFun Edge and Arduino Nano 33 BLE Sense boards are both equipped with
    three-axis accelerometers contained within components that are soldered to each
    board. These measure acceleration in three directions, which means they can be
    used to track the motion of the device in 3D space. To construct our magic wand,
    we’ll attach the microcontroller board to the end of a stick so it can be waved
    in a sorcerous manner. We’ll then feed the accelerometer’s output into a deep
    learning model, which will perform classification to tell us whether a known gesture
    was made.
  prefs: []
  type: TYPE_NORMAL
- en: 'We provide instructions on deploying this application to the following microcontroller
    platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Arduino Nano 33 BLE Sense](https://oreil.ly/6qlMD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SparkFun Edge](https://oreil.ly/-hoL-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the [ST Microelectronics STM32F746G Discovery kit](https://oreil.ly/SSsVJ)
    doesn’t include an accelerometer (and is too big to attach to the end of a magic
    wand), we won’t be featuring it here.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TensorFlow Lite regularly adds support for new devices, so if the device you’d
    like to use isn’t listed here, it’s worth checking the example’s [*README.md*](https://oreil.ly/dkZfA).
    You can also check there for updated deployment instructions if you run into trouble.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll look at the structure of our application and learn
    more about how its model works.
  prefs: []
  type: TYPE_NORMAL
- en: Application Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our application will again follow the now-familiar pattern of obtaining input,
    running inference, processing the output, and using the resulting information
    to make things happen.
  prefs: []
  type: TYPE_NORMAL
- en: A three-axis accelerometer outputs three values representing the amount of acceleration
    on the device’s x, y, and z-axes. The accelerometer on the SparkFun Edge board
    can do this 25 times per second (a rate of 25 Hz). Our model takes these values
    directly as its input, meaning we won’t need to do any preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: After data has been captured and inference has been run, our application will
    determine whether a valid gesture was detected, print some output to the terminal,
    and light an LED.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Our Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our gesture-detecting model is a convolutional neural network, weighing in at
    around 20 KB, that accepts raw accelerometer values as its input. It takes in
    128 sets of *x*, *y*, and *z* values at once, which at a rate of 25 Hz adds up
    to a little more than five seconds’ worth of data. Each value is a 32-bit floating-point
    number that indicates the amount of acceleration in that direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model was trained on four gestures performed by numerous people. It outputs
    probability scores for four classes: one representing each gesture (“wing,” “ring,”
    and “slope”), and one representing no recognized gesture. The probability scores
    sum to 1, with a score above 0.8 being considered confident.'
  prefs: []
  type: TYPE_NORMAL
- en: Because we’ll be running multiple inferences per second, we’ll need to make
    sure a single errant inference while a gesture is performed doesn’t skew our results.
    Our mechanism for doing this will be to consider a gesture as being detected only
    after it has been confirmed by a certain number of inferences. Given that each
    gesture takes a different amount of time to perform, the number of required inferences
    is different for each gesture, with the optimal numbers being determined through
    experimentation. Likewise, inference runs at varying rates on different devices,
    so these thresholds are also set per device.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 12](ch12.xhtml#chapter_magic_wand_training), we’ll explore how to
    train a model on our own gesture data and dig deeper into how the model works.
    Until then, let’s continue walking through our application.
  prefs: []
  type: TYPE_NORMAL
- en: All the Moving Parts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 11-4](#gesture_application_architecture) shows the structure of our
    magic wand application.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it’s almost as simple as our person detection application. Our
    model accepts raw accelerometer data, meaning we don’t need to do any preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code’s six main parts follow a similar structure as in our person detection
    example. Let’s walk through them in turn:'
  prefs: []
  type: TYPE_NORMAL
- en: Main loop
  prefs: []
  type: TYPE_NORMAL
- en: Our application runs in a continuous loop. Since its model is small and simple
    and there’s no preprocessing required, we’ll be able to run multiple inferences
    per second.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerometer handler
  prefs: []
  type: TYPE_NORMAL
- en: This component captures data from the accelerometer and writes it to the model’s
    input tensor. It uses a buffer to hold data.
  prefs: []
  type: TYPE_NORMAL
- en: TF Lite interpreter
  prefs: []
  type: TYPE_NORMAL
- en: The interpreter runs the TensorFlow Lite model, as in our earlier examples.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs: []
  type: TYPE_NORMAL
- en: The model is included as a data array and run by the interpreter. It’s nice
    and small, weighing in at only 19.5 KB.
  prefs: []
  type: TYPE_NORMAL
- en: Gesture predictor
  prefs: []
  type: TYPE_NORMAL
- en: This component takes the model’s output and decides whether a gesture has been
    detected, based on thresholds for both probability and the number of consecutive
    positive predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Output handler
  prefs: []
  type: TYPE_NORMAL
- en: The output handler lights LEDs and prints output to the serial port depending
    on which gesture was recognized.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of the components of our magic wand application](Images/timl_1104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-4\. The components of our magic wand application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Walking Through the Tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the application’s tests in the [GitHub repository](https://oreil.ly/h4iYb):'
  prefs: []
  type: TYPE_NORMAL
- en: '[*magic_wand_test.cc*](https://oreil.ly/X0AJP)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to run inference on a sample of accelerometer data
  prefs: []
  type: TYPE_NORMAL
- en: '[*accelerometer_handler_test.cc*](https://oreil.ly/MwM7g)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to use the accelerometer handler to obtain fresh data
  prefs: []
  type: TYPE_NORMAL
- en: '[*gesture_predictor_test.cc*](https://oreil.ly/cGbim)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to use the gesture predictor to interpret the results of inference
  prefs: []
  type: TYPE_NORMAL
- en: '[*output_handler_test.cc*](https://oreil.ly/MYwUW)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to use the output handler to show results of inference
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by walking through *magic_wand_test.cc*, which will show us the
    end-to-end process of inference with our model.
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We step through the basic flow in *magic_wand_test.cc*.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we list the ops our model will need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The test itself begins (as usual) by setting up everything required for inference
    and grabbing a pointer to the model’s input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We then inspect the input tensor to ensure that it’s the expected shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Our input’s shape is `(1, 128, 3, 1)`. The first dimension is just a wrapper
    around the second, which holds 128 three-axis accelerometer readings. Each reading
    has three values, one for each axis, and each value is wrapped within a single-element
    tensor. The inputs are all 32-bit floating-point values.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we’ve confirmed the input shape, we write some data to the input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The constant `g_circle_micro_f9643d42_nohash_4_data` is defined in *circle_micro_features_data.cc*;
    it contains an array of floating-point values representing one person’s attempt
    at performing a circle gesture. In the `for` loop, we step through this data and
    write each value into the input. We write only as many `float` values as the input
    tensor can hold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we run inference in the familiar manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterward, we investigate our output tensor to ensure that it’s the shape we
    expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'It should have two dimensions: a single-element wrapper, and a set of four
    values that indicate our four probabilities (“wing,” “ring,” “slope,” and unknown).
    Each of these will be a 32-bit floating-point number.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then test our data to make sure the inference result is what we expect.
    We passed in data for a circle gesture, so we expect the “ring” score to be the
    highest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We then repeat this entire process for the “slope” gesture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! We’ve seen how we can run inference on raw accelerometer data.
    Like the previous example, the fact that we can avoid preprocessing keeps things
    nice and simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this test, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The Accelerometer Handler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our next test shows the interface for the accelerometer handler. This component’s
    task is to populate the input tensor with accelerometer data for each inference.
  prefs: []
  type: TYPE_NORMAL
- en: Because both of these things depend on how the device’s accelerometer works,
    a different accelerometer handler implementation is provided for each individual
    device. We’ll walk through these implementations later on, but for now, the tests
    located in [*accelerometer_handler_test.cc*](https://oreil.ly/MwM7g) will show
    us how the handler should be called.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first test is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `SetupAccelerometer()` function performs the one-time setup that needs to
    happen in order to obtain values from the accelerometer. The test shows how the
    function should be called (with a pointer to an `ErrorReporter`) and that it returns
    a `TfLiteStatus` indicating that setup was successful.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next test shows how the accelerometer handler is used to fill the input
    tensor with data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: First, we prepare a `float` array named `input` to simulate the model’s input
    tensor. Because there are 128 three-axis readings, it has a total size of 384
    bytes (128 * 3). We initialize every value in the array to `0.0`.
  prefs: []
  type: TYPE_NORMAL
- en: We then call `ReadAccelerometer()`. We provide an `ErrorReporter` instance,
    the array to which we want data to be written (`input`), and the total amount
    of data that we want to obtain (384 bytes). The final argument is a Boolean flag
    that instructs `ReadAccelerometer()` whether to clear the buffer before reading
    more data, which needs to be done after a gesture has been successfully recognized.
  prefs: []
  type: TYPE_NORMAL
- en: When called, the `ReadAccelerometer()` function attempts to write 384 bytes
    of data to the array passed to it. If the accelerometer has only just started
    collecting data, the full 384 bytes might not yet be available. In this case,
    the function will do nothing and return a value of `false`. We can use this to
    avoid running inference if no data is available.
  prefs: []
  type: TYPE_NORMAL
- en: The dummy implementation of the accelerometer handler, located in [*accelerometer_handler.cc*](https://oreil.ly/MwM7g),
    simulates another reading being available every time it is called. By calling
    it 127 additional times we ensure it will have accrued enough data to start returning
    `true`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run these tests, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The Gesture Predictor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After inference has occurred, our output tensor will be filled with probabilities
    that indicate to us which gesture, if any, was made. However, because machine
    learning is not an exact science, there’s a chance that any single inference might
    result in a false positive.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the impact of false positives, we can stipulate that for a gesture
    to be recognized, it must have been detected in at least a certain number of consecutive
    inferences. Given that we run inference multiple times per second, we can quickly
    determine whether a result is valid. This is the job of the gesture predictor.
  prefs: []
  type: TYPE_NORMAL
- en: 'It defines a single function, `PredictGesture()`, which takes the model’s output
    tensor as its input. To determine whether a gesture has been detected, the function
    does two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Checks whether the gesture’s probability meets a minimum threshold
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Checks whether the gesture has been consistently detected over a certain number
    of inferences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The minimum number of inferences required varies per gesture because some take
    longer to perform than others. It also varies per device, given that faster devices
    are able to run inference more frequently. The default values, tuned for the SparkFun
    Edge board, are located in [*constants.cc*](https://oreil.ly/ktGgw):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The values are defined in the same order as the gestures appear in the model’s
    output tensor. Other platforms, such as Arduino, have device-specific versions
    of this file that contain values tuned to their own performance.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through the code in [*gesture_predictor.cc*](https://oreil.ly/f3I6U)
    to see how these are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define some variables that are used to keep track of the last gesture
    seen and how many of the same gesture have been recorded in a row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the `PredictGesture()` function and determine whether any of
    the gesture categories had a probability of greater than 0.8 in the most recent
    inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We use `this_predict` to store the index of the gesture that was predicted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variable `continuous_count` is used to track how many times the most recently
    spotted gesture has been predicted in a row. If none of the gesture categories
    meet the probability threshold of 0.8, we reset any ongoing detection process
    by setting `continuous_count` to `0`, and `last_predict` to `3` (the index of
    the “unknown” category), indicating that the most recent result was no known gesture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, if the most recent prediction aligns with the previous one, we increment
    `continuous_count`. Otherwise, we reset it to `0`. We also store the most recent
    prediction in `last_predict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next section of `PredictGesture()`, we use `should_continuous_count`
    to check whether the current gesture has met its threshold yet. If it hasn’t,
    we return a `3`, indicating an unknown gesture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If we get past this point, it means that we’ve confirmed a valid gesture. In
    this case, we reset all of our variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The function ends by returning the current prediction. This will be passed by
    our main loop into the output handler, which displays the result to the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gesture predictor’s tests are located in [*gesture_predictor_test.cc*](https://oreil.ly/5BZzt).
    The first test demonstrates a successful prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `PredictGesture()` function is fed a set of probabilities that strongly
    indicate that the first category should be matched. However, until it has been
    called with these probabilities `threshold` number of times, it returns a `3`,
    signifying an “unknown” result. After it has been called `threshold` number of
    times, it returns a positive prediction for category `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next test shows what happens if a consecutive run of high probabilities
    for one category is interrupted by a high probability for a different category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we feed in a set of consecutive high probabilities for category
    `0`, but not a sufficient number to meet the threshold. We then change the probabilities
    so that category `2` is the highest, which results in a category `3` prediction,
    signifying an “unknown” gesture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final test shows how `PredictGesture()` ignores probabilities that are
    below its threshold. In a loop, we feed in exactly the correct number of predictions
    to meet category `0`’s threshold. However, although category `0` has the highest
    probability, its value is 0.7, which is below `PredictGesture()`’s internal threshold
    of 0.8\. This results in a category `3` “unknown” prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To run these tests, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The Output Handler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The output handler is very simple; it just takes the class index returned by
    `PredictGesture()` and displays the results to the user. Its test, in [*output_handler_test.cc*](https://oreil.ly/QWkeL),
    shows its interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this test, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Detecting Gestures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All of these components come together in [*main_functions.cc*](https://oreil.ly/ggNtD),
    which contains the core logic of our program. First it sets up the usual variables,
    along with some extras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `input_length` variable stores the length of the model’s input tensor, and
    the `should_clear_buffer` variable is a flag that indicates whether the accelerometer
    handler’s buffer should be cleared the next time it runs. Clearing the buffer
    is done after a successful detection result in order to provide a clean slate
    for subsequent inferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the `setup()` function does all of the usual housekeeping so that we’re
    ready to run inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The more interesting stuff happens in the `loop()` function, which is still
    very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: First, we attempt to read some values from the accelerometer. After the attempt,
    we set `should_clear_buffer` to `false` to ensure that we stop trying to clear
    it for the time being.
  prefs: []
  type: TYPE_NORMAL
- en: If obtaining new data was unsuccessful, `ReadAccelerometer()` will return a
    `false` value, and we’ll return from the `loop()` function so that we can try
    again the next time it is called.
  prefs: []
  type: TYPE_NORMAL
- en: If the value returned by `ReadAccelerometer()` is `true`, we’ll run inference
    on our freshly populated input tensor. We pass the result into `PredictGesture()`,
    which gives us the index of which gesture was detected. If the index is less than
    `3`, the gesture was valid, so we set the `should_clear_buffer` flag in order
    to clear the buffer next time `ReadAccelerometer()` is called. We then call `HandleOutput()`
    to report any results to the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over in *main.cc*, the `main()` function kicks off our program, runs `setup()`,
    and calls the `loop()` function in a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'And that’s it! To build the program on your development computer, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to run the program, enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The program won’t produce any output, because there isn’t any accelerometer
    data available, but you can confirm that it builds and runs.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we walk through the code for each platform that captures accelerometer
    data and produces an output. We also show how to deploy and run the application.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Microcontrollers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’ll deploy our code to two devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Arduino Nano 33 BLE Sense](https://oreil.ly/6qlMD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SparkFun Edge](https://oreil.ly/-hoL-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin with the Arduino implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Arduino
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Arduino Nano 33 BLE Sense has a three-axis accelerometer as well as Bluetooth
    support, and is small and lightweight—ideal for building a magic wand.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through the Arduino-specific implementations of some of the application’s
    key files.
  prefs: []
  type: TYPE_NORMAL
- en: Arduino constants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The constant `kConsecutiveInferenceThresholds` is redefined in the file [*arduino/constants.cc*](https://oreil.ly/5bBt0):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned earlier in the chapter, this constant stores the number of consecutive
    positive inferences required for each gesture to be considered detected. The number
    depends on how many inferences are run per second, which varies per device. Because
    the default numbers were calibrated for the SparkFun Edge, the Arduino implementation
    needs its own set of numbers. You can modify these thresholds to make inference
    more difficult or easier to trigger, but setting them too low will result in false
    positives.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing accelerometer data on Arduino
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Arduino accelerometer handler is located in [*arduino/accelerometer_handler.cc*](https://oreil.ly/jV_Qm).
    It has the task of capturing data from the accelerometer and writing it to the
    model’s input buffer.
  prefs: []
  type: TYPE_NORMAL
- en: The model we are using was trained using data from the SparkFun Edge board.
    The Edge’s accelerometer provides a set of readings at a rate of 25 Hz, or 25
    times per second. To work correctly, it needs to be fed data that is captured
    at the same rate. As it turns out, the accelerometer on the Arduino Nano 33 BLE
    Sense board returns measurements at a rate of 119 Hz. This means that in addition
    to capturing data, we need to *downsample* it to suit our model.
  prefs: []
  type: TYPE_NORMAL
- en: Although it sounds very technical, downsampling is actually pretty easy. To
    reduce the sample rate of a signal, we can just throw away some of the data. We
    look at how this works in the following code.
  prefs: []
  type: TYPE_NORMAL
- en: 'First the implementation includes its own header file, along with some others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The file *Arduino.h* provides access to some basic features of the Arduino platform.
    The file *Arduino_LSM9DS1.h* is part of the [Arduino_LSM9DS1](https://oreil.ly/eb3Zs)
    library, which we’ll be using to communicate with the board’s accelerometer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we set up some variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: These include a buffer we’ll be filling with our data, `save_data`, along with
    some variables for tracking our current position in the buffer and whether we
    have enough data to start running inference. The most interesting two variables,
    `sample_every_n` and `sample_skip_counter`, are used in the downsampling process.
    We’ll look at this more closely in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next in the file, the `SetupAccelerometer()` function is called by the program’s
    main loop to get the board ready to capture data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Because we’ll be outputting a message to indicate that everything is ready to
    go, the first thing it does is make sure that the device’s serial port is ready.
    It then switches on the *inertial measurement unit* (IMU), which is the electronic
    component that contains the accelerometer. The `IMU` object comes from the Arduino_LSM9DS1
    library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to start thinking about downsampling. We first query the IMU
    library to determine the board’s sample rate. When we have that number, we divide
    it by our target sample rate, which is defined in `kTargetHz` as part of [*constants.h*](https://oreil.ly/rQaSw):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Our target rate is 25 Hz, and the board’s sample rate is 119 Hz; thus, the
    result of our division is 4.76\. This lets us know how many of the 119 Hz samples
    we need to keep in order to attain the target sample rate of 25 Hz: 1 sample in
    every 4.76.'
  prefs: []
  type: TYPE_NORMAL
- en: Because keeping a fractional number of samples is difficult, we use the `roundf()`
    function to round to the nearest number, 5\. To downsample our signal, then, we
    need to keep one in every five measurements. This will result in an effective
    sample rate of 23.8 Hz, which is a close enough approximation that our model should
    work well. We store this value in the `sample_every_n` variable for use later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve established the parameters of our downsampling, we give the
    user a message to inform them that the application is ready to go and then return
    from the `SetupAccelerometer()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we define `ReadAccelerometer()`. This function is tasked with capturing
    new data and writing it to the model’s output tensor. It begins with some code
    that is used to clear its internal buffer after a gesture has been successfully
    recognized, cleaning the slate for any subsequent gestures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the IMU library to check for available data in a loop. If there’s
    data available, we read it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The accelerometer on the Arduino Nano 33 BLE Sense board is equipped with something
    called a [*FIFO buffer*](https://oreil.ly/kFEa0). This is a special memory buffer,
    located on the accelerometer itself, which holds the most recent 32 measurements.
    Because it’s part of the accelerometer hardware, the FIFO buffer continues to
    accrue measurements even while our application code is running. If it weren’t
    for the FIFO buffer, we might lose a lot of data, meaning we wouldn’t have an
    accurate record of the gestures being made.
  prefs: []
  type: TYPE_NORMAL
- en: When we call `IMU.accelerationAvailable()`, we are querying the accelerometer
    to see whether new data is available in its FIFO buffer. Using our loop, we continue
    to read all the data from the buffer until there is none remaining.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up, we implement our super-simple downsampling algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Our approach is to keep one in every *n* samples, where *n* is the number stored
    in `sample_every_n`. To do this, we maintain a counter, `sample_skip_counter`,
    which lets us know how many samples have been read since the last one we kept.
    For every measurement we read, we check whether it is the *n*th. If it isn’t,
    we `continue` the loop without writing the data anywhere, effectively throwing
    it away. This simple process leads to our data being downsampled.
  prefs: []
  type: TYPE_NORMAL
- en: 'If execution gets further than this point, we’re planning on keeping the data.
    To do this, we write it to consecutive positions in our `save_data` buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Our model accepts accelerometer measurements in the order *x*, *y*, *z*. You’ll
    notice here that we’re writing the *y* value to the buffer before the *x*. This
    is because our model was trained on data captured on the SparkFun Edge board,
    whose accelerometer has its axes pointing in different physical directions to
    the one on the Arduino. This difference means that the SparkFun Edge’s x-axis
    is equivalent to the Arduino’s y-axis, and vice versa. By swapping these axes’
    data in our code, we can make sure our model is being fed data that it can understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final few lines of our loop do some housework, setting some state variables
    that are used in our loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We reset our downsampling counter, make sure we don’t run off the end of our
    sample buffer, and set a flag to indicate that new data has been saved.
  prefs: []
  type: TYPE_NORMAL
- en: 'After grabbing this new data, we do some more checks. This time, we’re making
    sure that we have sufficient data to perform an inference. If not, or if new data
    was not captured this time around, we return from the function without doing anything:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: By returning `false` when there’s no new data, we make sure the calling function
    knows not to bother running inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we got this far, we’ve obtained some new data. We copy the appropriate amount
    of data, including our new samples, to the input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! We’ve populated the input tensor and are ready to run inference.
    After inference has been run, the results are passed into the gesture predictor,
    which determines whether a valid gesture has been spotted. The result is passed
    into the output handler, which we walk through next.
  prefs: []
  type: TYPE_NORMAL
- en: Responding to gestures on Arduino
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The output handler is defined in [*arduino/output_handler.cc*](https://oreil.ly/kdVLW).
    It’s nice and simple: all it does is log information to the serial port depending
    on which gesture was detected, and toggle the board’s LED each time inference
    is run.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first time the function runs, the LED is configured for output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the LED is toggled on and off with each inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print some beautiful ASCII art depending on which gesture was matched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: It’s difficult to read now, but you’ll be rewarded with the output’s full glory
    when you deploy the application to your board.
  prefs: []
  type: TYPE_NORMAL
- en: Running the example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To deploy this example, here’s what we’ll need:'
  prefs: []
  type: TYPE_NORMAL
- en: An Arduino Nano 33 BLE Sense board
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A micro-USB cable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Arduino IDE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There’s always a chance that the build process might have changed since this
    book was written, so check [*README.md*](https://oreil.ly/Zkd3x) for the latest
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The projects in this book are available as example code in the TensorFlow Lite
    Arduino library. If you haven’t already installed the library, open the Arduino
    IDE and select Manage Libraries from the Tools menu. In the window that appears,
    search for and install the library named TensorFlowLite. You should be able to
    use the latest version, but if you run into issues, the version that was tested
    with this book is 1.14-ALPHA.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also install the library from a *.zip* file, which you can either [download](https://oreil.ly/blgB8)
    from the TensorFlow Lite team or generate yourself using the TensorFlow Lite for
    Microcontrollers Makefile. If you’d prefer to do the latter, see [Appendix A](app01.xhtml#appendix_arduino_library_zip).
  prefs: []
  type: TYPE_NORMAL
- en: After you’ve installed the library, the `magic_wand` example will show up in
    the File menu under Examples→Arduino_TensorFlowLite, as shown in [Figure 11-5](#arduino_examples_magic_wand).
  prefs: []
  type: TYPE_NORMAL
- en: Click “magic_wand” to load the example. It will appear as a new window, with
    a tab for each of the source files. The file in the first tab, *magic_wand*, is
    equivalent to the *main_functions.cc* we walked through earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[“Running the Example”](ch06.xhtml#hello_world_running_the_example) already
    explained the structure of the Arduino example, so we won’t cover it again here.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the ''Examples'' menu](Images/timl_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-5\. The Examples menu
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition to the TensorFlow library, we also need to install and patch the
    Arduino_LSM9DS1 library. By default, the library doesn’t enable the FIFO buffer
    that is required by the example, so we have to make some modifications to its
    code.
  prefs: []
  type: TYPE_NORMAL
- en: In the Arduino IDE, select Tools→Manage Libraries and then search for Arduino_LSM9DS1\.
    To ensure the following instructions work, you must install version 1.0.0 of the
    driver.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s possible that the driver might have been fixed by the time you are reading
    this chapter. You can find the latest deployment instructions in [*README.md*](https://oreil.ly/pk61J).
  prefs: []
  type: TYPE_NORMAL
- en: The driver will be installed to your *Arduino/libraries* directory, in the subdirectory
    *Arduino_LSM9DS1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the *Arduino_LSM9DS1/src/LSM9DS1.cpp* driver source file and then go to
    the function named `LSM9DS1Class::begin()`. Insert the following lines at the
    end of the function, immediately before the `return 1` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, locate the function named `LSM9DS1Class::accelerationAvailable()`. You
    will see the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Comment out those lines and then replace them with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Save the file. Patching is now complete!
  prefs: []
  type: TYPE_NORMAL
- en: To run the example, plug in your Arduino device via USB. On the Tools menu,
    make sure that the correct device type is selected from the Board drop-down list,
    as shown in [Figure 11-6](#arduino_board_dropdown_3).
  prefs: []
  type: TYPE_NORMAL
- en: If your device’s name doesn’t appear in the list, you’ll need to install its
    support package. To do this, click Boards Manager and then, in the window that
    appears, search for your device and install the latest version of the corresponding
    support package.
  prefs: []
  type: TYPE_NORMAL
- en: Next, make sure the device’s port is selected in the Port drop-down, also in
    the Tools menu, as demonstrated in [Figure 11-7](#arduino_port_dropdown_3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the ''Board'' dropdown](Images/timl_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-6\. The Board drop-down list
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Screenshot of the ''Port'' dropdown](Images/timl_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-7\. The Port drop-down list
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, click the upload button in the Arduino window (highlighted in white
    in [Figure 11-8](#arduino_upload_button_3)) to compile and upload the code to
    your Arduino device.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the upload button](Images/timl_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-8\. The upload button
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After the upload has successfully completed, you should see the LED on your
    Arduino board begin to flash.
  prefs: []
  type: TYPE_NORMAL
- en: 'To try some gestures, select Serial Monitor in the Tools menu. You should initially
    see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: You can now try to make some gestures. Hold the board up with one hand, with
    the components facing up and the USB adapter facing toward the left, as shown
    in [Figure 11-9](#gesture_holding_arduino).
  prefs: []
  type: TYPE_NORMAL
- en: '![Photo of a hand holding an Arduino Nano 33 BLE Sense board](Images/timl_1109.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-9\. How to hold the board while performing gestures
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 11-10](#gesture_spells_2) presents a diagram showing how to perform
    each gesture. Because the model was trained on data collected when the board was
    attached to a wand, you might need a few tries to get them to work.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagrams of the three magic wand gestures](Images/timl_1103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-10\. The three magic wand gestures
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The easiest one to start with is “wing.” You should move your hand quickly
    enough that it takes around one second to perform the gesture. If you’re successful,
    you should see the following output, and the red LED should illuminate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations, you’ve cast your first magic spell using the Arduino!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At this point, you might choose to be creative and attach the board to the tip
    of a magic wand, at the point furthest from your hand. Any stick, ruler, or other
    household item with a length of around a foot (30 cm) should work well.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure the device is attached firmly, and in the same orientation, with the
    components facing up and the USB adapter facing toward the left. And pick a rigid
    wand, not a flexible one; any wobbling will affect the accelerometer readings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, try the “ring” gesture, by tracing a clockwise circle with your hand
    (or the tip of your wand). Again, aim to take around a second to perform the gesture.
    You should see the following appear, as if by magic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'For the final gesture, trace the corner of a triangle in the air. It’s best
    described by its ASCII art demonstration, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Like any good magic spells, you might have to practice these a bit before you
    can perform them perfectly each time. You can see video demonstrations of the
    gestures in [*README.md*](https://oreil.ly/O1LqD).
  prefs: []
  type: TYPE_NORMAL
- en: Making your own changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you deployed the basic application, try playing around and making some
    changes to the code. Just edit the files in the Arduino IDE and save them, and
    then repeat the previous instructions to deploy your modified code to the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few things you could try:'
  prefs: []
  type: TYPE_NORMAL
- en: Experiment with the threshold values in [*arduino/constants.cc*](https://oreil.ly/H49iS)
    to make the gestures easier or more difficult to perform (at the cost of more
    false positives or negatives).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a program on your computer that lets you perform tasks using physical
    gestures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extend the program to transmit detection results via Bluetooth. There are examples
    showing how to do this included with the [ArduinoBLE library](https://oreil.ly/xW4SN),
    which you can download via the Arduino IDE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkFun Edge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SparkFun Edge features a three-axis accelerometer, a battery mount, and
    Bluetooth support. This makes it perfect for a magic wand because it can operate
    wirelessly.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing accelerometer data on SparkFun Edge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code that captures accelerometer data is located in [*sparkfun_edge/accelerometer_handler.cc*](https://oreil.ly/yZi0v).
    A lot of it is device-specific, but we’ll skip over the implementation details
    and focus on the important stuff.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step involved with capturing accelerometer data is configuring the
    hardware. The `SetupAccelerometer()` function kicks this off by setting various
    low-level parameters required by the accelerometer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: You’ll notice a call to a function named `initAccelerometer()`. This is defined
    in the [SparkFun Edge BSP’s accelerometer example](https://oreil.ly/JC0b6), which
    is pulled down as a dependency when our project is built. It performs various
    tasks to switch on and configure the board’s accelerometer.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the accelerometer is running, we enable its [FIFO buffer](https://oreil.ly/kFEa0).
    This is a special memory buffer, located on the accelerometer itself, which holds
    the last 32 datapoints. By enabling it, we’re able to continue collecting accelerometer
    measurements even while our application code is busy running inference. The remainder
    of the function sets up the buffer and logs errors if anything goes wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: When we’re done with initialization, we can call the `ReadAccelerometer()` function
    to get the latest data. This will happen between every inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, if the `reset_buffer` argument is `true`, `ReadAccelerometer()` performs
    a reset of its data buffer. This is done after a valid gesture has been detected
    in order to provide a clean slate for further gestures. As part of this process,
    we use `am_util_delay_ms()` to make our code wait for 10 ms. Without this delay,
    the code often hangs when reading new data (as of this writing, the cause was
    unclear, but the TensorFlow open source project welcomes pull requests if you
    determine a better fix):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'After resetting the main buffer, `ReadAccelerometer()` checks whether there
    is any new data available in the accelerometer’s FIFO buffer. If there’s nothing
    available yet, we just return from the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Our application’s main loop will continue calling, meaning as soon as there’s
    data available, we can move past this point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of the function loops through the new data and stores it in another,
    larger buffer. First we set up a special struct of type `axis3bit16_t`, designed
    to hold accelerometer data. We then call `lis2dh12_acceleration_raw_get()` to
    fill it with the next available measurement. This function will return zero if
    it fails, at which point we display an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'If the measurement was obtained successfully, we convert it into milli-Gs,
    the unit of measurement expected by the model, and then write it into `save_data[]`,
    which is an array we’re using as a buffer to store values that we’ll use for inference.
    The values for each axis of the accelerometer are stored consecutively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Our `save_data[]` array can store 200 sets of three-axis values, so we set our
    `begin_index` counter back to 0 when it reaches 600.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve now incorporated all of the new data into our `save_data[]` buffer. Next,
    we check whether we have enough data to make a prediction. When testing the model,
    it was discovered that around a third of our total buffer size is the bare minimum
    amount of data that results in a reliable prediction; therefore, if we have at
    least this much data, we set the `pending_initial_data` flag to `false` (it defaults
    to `true`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, if there is still insufficient data to run an inference, we return `false`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'If we got this far, there’s sufficient data in the buffer to run an inference.
    The final part of the function copies the requested data from the buffer into
    the `input` argument, which is a pointer to the model’s input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The variable `length` is an argument passed into `ReadAccelerometer()` that
    determines how much data should be copied. Because our model takes 128 three-axis
    readings as its input, the code in *main_functions.cc* calls `ReadAccelerometer()`
    with a `length` of 384 (128 * 3).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, our input tensor is filled with fresh accelerometer data. Inference
    will be run, the results will be interpreted by the gesture predictor, and the
    result will be passed to the output handler to display to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Responding to gestures on SparkFun Edge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The output handler, located in [*sparkfun_edge/output_handler.cc*](https://oreil.ly/ix1o1),
    is very simple. The first time it runs, we configure the LEDs for output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we toggle the yellow LED with each inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we check which gesture was detected. For each individual gesture,
    we light an LED, clear all the others, and output some beautiful ASCII art via
    the serial port. Here’s the code that handles the “wing” gesture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'On a serial port monitor, the output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: A different serial output and LED are used for each gesture.
  prefs: []
  type: TYPE_NORMAL
- en: Running the example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve now seen how the SparkFun Edge code works. Next, let’s get it running
    on our hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There’s always a chance that the build process might have changed since this
    book was written, so check [*README.md*](https://oreil.ly/Ts6MT) for the latest
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build and deploy our code, we’ll need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A SparkFun Edge board with the [Himax HM01B0 breakout](https://oreil.ly/f23oa)
    attached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A USB programmer (we recommend the SparkFun Serial Basic Breakout, which is
    available in [micro-B USB](https://oreil.ly/KKfyI) and [USB-C](https://oreil.ly/ztUrB)
    variants)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A matching USB cable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3 and some dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re unsure whether you have the correct version of Python installed, [“Running
    the Example”](ch06.xhtml#running_hello_world_sparkfun_edge) has instructions on
    how to check.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a terminal window, clone the TensorFlow repository, and then change into
    its directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’re going to build the binary and run some commands that get it ready
    for downloading to the device. To avoid some typing, you can copy and paste these
    commands from [*README.md*](https://oreil.ly/MQmWw).
  prefs: []
  type: TYPE_NORMAL
- en: Build the binary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following command downloads all the required dependencies and then compiles
    a binary for the SparkFun Edge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The binary will be created as a *.bin* file, in the following location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'To check that the file exists, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: If you run that command, you should see `Binary was successfully created` printed
    to the console.
  prefs: []
  type: TYPE_NORMAL
- en: If you see `Binary is missing`, there was a problem with the build process.
    If so, it’s likely that there are some clues to what went wrong in the output
    of the `make` command.
  prefs: []
  type: TYPE_NORMAL
- en: Sign the binary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The binary must be signed with cryptographic keys to be deployed to the device.
    Let’s run some commands that will sign the binary so that it can be flashed to
    the SparkFun Edge. The scripts used here come from the Ambiq SDK, which is downloaded
    when the Makefile is run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following command to set up some dummy cryptographic keys you can
    use for development:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, run the following command to create a signed binary. Substitute `python3`
    with `python` if necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates the file *main_nonsecure_ota.bin*. Now, run this command to create
    a final version of the file that you can use to flash your device with the script
    you will use in the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: You should now have a file called *main_nonsecure_wire.bin* in the directory
    where you ran the commands. This is the file you’ll be flashing to the device.
  prefs: []
  type: TYPE_NORMAL
- en: Flash the binary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The SparkFun Edge stores the program it is currently running in its 1 megabyte
    of flash memory. If you want the board to run a new program, you need to send
    it to the board, which will store it in flash memory, overwriting any program
    that was previously saved. This process is called flashing.
  prefs: []
  type: TYPE_NORMAL
- en: Attach the programmer to the board
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To download new programs to the board, you’ll use the SparkFun USB-C Serial
    Basic serial programmer. This device allows your computer to communicate with
    the microcontroller via USB.
  prefs: []
  type: TYPE_NORMAL
- en: 'To attach this device to your board, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: On the side of the SparkFun Edge, locate the six-pin header.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plug the SparkFun USB-C Serial Basic into these pins, ensuring that the pins
    labeled BLK and GRN on each device are lined up correctly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can see the correct arrangement in [Figure 11-11](#sparkfun_edge_serial_basic_4).
  prefs: []
  type: TYPE_NORMAL
- en: '![A photo showing how the SparkFun Edge and USB-C Serial Basic should be connected](Images/timl_0613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-11\. Connecting the SparkFun Edge and USB-C Serial Basic (image courtesy
    of SparkFun)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Attach the programmer to your computer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, connect the board to your computer via USB. To program the board, you
    need to determine the name that your computer gives the device. The best way of
    doing this is to list all the computer’s devices before and after attaching it
    and then look to see which device is new.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some people have reported issues with their operating system’s default drivers
    for the programmer, so we strongly recommend installing the [driver](https://oreil.ly/vLavS)
    before you continue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before attaching the device via USB, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output a list of attached devices that looks something like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, connect the programmer to your computer’s USB port and run the command
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an extra item in the output, as in the example that follows.
    Your new item might have a different name. This new item is the name of the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: This name will be used to refer to the device. However, it can change depending
    on which USB port the programmer is attached to, so if you disconnect the board
    from your computer and then reattach it you might need to look up its name again.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some users have reported two devices appearing in the list. If you see two devices,
    the correct one to use begins with the letters “wch”; for example, “/dev/wchusbserial-14410.”
  prefs: []
  type: TYPE_NORMAL
- en: 'After you’ve identified the device name, put it in a shell variable for later
    use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: This is a variable that you can use when running commands that require the device
    name, later in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Run the script to flash your board
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To flash the board, you need to put it into a special “bootloader” state that
    prepares it to receive the new binary. You can then run a script to send the binary
    to the board.
  prefs: []
  type: TYPE_NORMAL
- en: 'First create an environment variable to specify the baud rate, which is the
    speed at which data will be sent to the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Now paste the command that follows into your terminal—but *do not press Enter
    yet*!. The `${DEVICENAME}` and `${BAUD_RATE}` in the command will be replaced
    with the values you set in the previous sections. Remember to substitute `python3`
    with `python` if necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Next you’ll reset the board into its bootloader state and flash the board.
  prefs: []
  type: TYPE_NORMAL
- en: On the board, locate the buttons marked `RST` and `14`, as shown in [Figure 11-12](#sparkfun_edge_buttons_4).
  prefs: []
  type: TYPE_NORMAL
- en: '![A photo showing the SparkFun Edge''s buttons](Images/timl_0614.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-12\. The SparkFun Edge’s buttons
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that your board is connected to the programmer, and the entire thing
    is connected to your computer via USB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the board, press and hold the button marked `14`. *Continue holding it.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While still holding the button marked `14`, press the button marked `RST` to
    reset the board.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press Enter on your computer to run the script. *Continue holding button `14`.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should now see something like the following appearing on your screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Keep holding button `14` until you see `Sending Data Packet of length 8180`.
    You can release the button after seeing this (but it’s okay if you keep holding
    it).
  prefs: []
  type: TYPE_NORMAL
- en: 'The program will continue to print lines on the terminal. Eventually, you’ll
    see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: This indicates a successful flashing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the program output ends with an error, check whether `Sending Reset Command.`
    was printed. If so, flashing was likely successful despite the error. Otherwise,
    flashing might have failed. Try running through these steps again (you can skip
    over setting the environment variables).
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Program
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Start by pressing the `RST` button to make sure the program is running. When
    the program is running, the yellow LED will toggle on and off, once for each inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, use the following command to start printing the serial output of the
    device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'You should initially see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: You can now try to make some gestures. Hold the board up with one hand, with
    the components facing up and the USB adapter facing toward the left, as shown
    in [Figure 11-13](#gesture_holding_edge).
  prefs: []
  type: TYPE_NORMAL
- en: '![Photo of a hand holding a SparkFun edge board](Images/timl_1113.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-13\. How to hold the board while performing gestures
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 11-14](#gesture_spells_3) presents a diagram showing how to perform
    each gesture. Because the model was trained on data collected when the board was
    attached to a wand, you might need a few tries to get them to work.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagrams of the three magic wand gestures](Images/timl_1103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-14\. The three magic wand gestures
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The easiest one to start with is “wing.” You should move your hand quickly
    enough that it takes around one second to perform the gesture. If you’re successful,
    the red LED should illuminate, and you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations, you’ve cast your first magic spell using the SparkFun Edge!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At this point, you might choose to be creative and attach the board to the tip
    of a magic wand, at the point furthest from your hand. Any stick, ruler, or other
    household item with a length of around a foot (30 cm) should work well.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure the device is attached firmly, and in the same orientation, with the
    components facing up and the USB adapter facing toward the left. And pick a rigid
    wand, not a flexible one because any wobbling will affect the accelerometer readings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next try the “ring” gesture, by tracing a clockwise circle with your hand (or
    the tip of your wand). Again, aim to take around a second to perform the gesture.
    You should see the following appear, as if by magic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'For the final gesture, trace the corner of a triangle in the air. It’s best
    described by its ASCII art demonstration, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Like any good magic spells, you might have to practice these a bit before you
    can perform them perfectly each time. You can see video demonstrations of the
    gestures in [*README.md*](https://oreil.ly/ilGJY).
  prefs: []
  type: TYPE_NORMAL
- en: Making your own changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve deployed the basic application, try playing around and making
    some changes. You can find the application’s code in the *tensorflow/lite/micro/examples/magic_wand*
    folder. Just edit and save, and then repeat the previous instructions to deploy
    your modified code to the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few things you could try:'
  prefs: []
  type: TYPE_NORMAL
- en: Experiment with the threshold values in [*constants.cc*](https://oreil.ly/s5bdg)
    to make the gestures easier or more difficult to perform (at the cost of more
    false positives or negatives).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a program on your computer that lets you perform tasks using physical
    gestures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extend the program to transmit detection results via Bluetooth. There’s an example
    of how to do this in the [Ambiq SDK](https://oreil.ly/Bci3a), in *AmbiqSuite-Rel2.0.0/boards/apollo3_evb/examples/uart_ble_bridge*.
    When the magic wand application is built, the SDK is downloaded to *tensorflow/tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw a fun example of how obscure sensor data can be interpreted
    by an embedded machine learning application into a much more useful form. By seeing
    the patterns in noise, embedded machine learning models allow devices to understand
    the world around them and alert us to events, even when the raw data might be
    difficult for a human to digest.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 12](ch12.xhtml#chapter_magic_wand_training), we explore how our
    magic wand model works and learn how to collect data and train our own magic spells.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch11.xhtml#idm46473555451544-marker)) Mihaela Porumb et al., “A convolutional
    neural network approach to detect congestive heart failure.” *Biomedical Signal
    Processing and Control* (Jan 2020). [*https://oreil.ly/4HBFt*](https://oreil.ly/4HBFt)
  prefs: []
  type: TYPE_NORMAL
