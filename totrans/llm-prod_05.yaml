- en: '6 Large language model services: A practical guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How to structure an LLM service and tools to deploy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create and prepare a Kubernetes cluster for LLM deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common production challenges and some methods to handle them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying models to the edge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The production of too many useful things results in too many useless people.—Karl
    Marx
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We did it. We arrived. This is the chapter we wanted to write when we first
    thought about writing this book. One author remembers the first model he ever
    deployed. Words can’t describe how much more satisfaction this gave him than the
    dozens of projects left to rot on his laptop. In his mind, it sits on a pedestal,
    not because it was good—in fact, it was quite terrible—but because it was useful
    and actually used by those who needed it the most. It affected the lives of those
    around him.
  prefs: []
  type: TYPE_NORMAL
- en: So what actually is production? “Production” refers to the phase where the model
    is integrated into a live or operational environment to perform its intended tasks
    or provide services to end users. It’s a crucial phase in making the model available
    for real-world applications and services. To that end, we will show you how to
    package up an LLM into a service or API so that it can take on-demand requests.
    We will then show you how to set up a cluster in the cloud where you can deploy
    this service. We’ll also share some challenges you may face in production and
    some tips for handling them. Lastly, we will talk about a different kind of production,
    deploying models on edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Creating an LLM service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last chapter, we trained and finetuned several models, and we’re sure
    you can’t wait to deploy them. Before you deploy a model, though, it’s important
    to plan ahead and consider different architectures for your API. Planning ahead
    is especially vital when deploying an LLM API. It helps outline the functionality,
    identify potential integration challenges, and arrange for necessary resources.
    Good planning streamlines the development process by setting priorities, thereby
    boosting the team’s efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to take a look at several critical topics you
    should take into consideration to get the most out of our application once deployed.
    Figure 6.1 demonstrates a simple LLM-based service architecture that allows users
    to interact with our LLM on demand. This is a typical use case when working with
    chatbots, for example. Setting up a service also allows us to serve batch and
    stream processes while abstracting away the complexity of embedding the LLM logic
    directly into these pipelines. Of course, running an ML model from a service will
    add a communication latency to your pipeline, but LLMs are generally considered
    slow, and this extra latency is often worth the tradeoff.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 A basic LLM service. A majority of the logic is handled by the API
    layer, which will ensure the correct preprocessing of incoming requests is done
    and serve the actual inference of the request.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While figure 6.1 appears neat and tidy, it is hiding several complex subjects
    you’ll want to work through, particularly in that API box. We’ll be talking through
    several key features you’ll want to include in your API, like batching, rate limiters,
    and streaming. You’ll also notice some preprocessing techniques like retrieval-augmented
    generation (RAG) hidden in this image, which we’ll discuss in depth in section
    6.1.7\. By the end of this section, you will know how to approach all of this,
    and you will have deployed an LLM service and understand what to do to improve
    it. But before we get to any of that, let’s first talk about the model itself
    and the best methods to prepare it for online inference.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Model compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The success of any model in production is dependent on the hardware it runs
    on. The microchip architecture and design of the controllers on the silicon will
    ultimately determine how quickly and efficiently inferences can run. Unfortunately,
    when programming in a high-level language like Python using frameworks like PyTorch
    or TensorFlow, the model won’t be optimized to take full advantage of the hardware.
    This is where compiling comes into play. Compiling is the process of taking code
    written in a high-level language and converting or lowering it to machine-level
    code that the computer can process quickly. Compiling your LLM can easily lead
    to major inference and cost improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Various people have dedicated a lot of time to performing some of the repeatable
    efficiency steps for you beforehand. We covered Tim Dettmers’s contributions in
    the last chapter. Other contributors include Georgi Gerganov, who created and
    maintains llama.cpp for running LLMs using C++ for efficiency, and Tom Jobbins,
    who goes by TheBloke on Hugging Face Hub and quantizes models into the correct
    formats to be used in Gerganov’s framework and others, like oobabooga. Because
    of how fast this field moves, completing simple, repeatable tasks over a large
    distribution of resources is often just as helpful to others.
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning workflows, this process typically involves converting our
    model from its development framework (PyTorch, TensorFlow, or other) to an intermediate
    representation (IR), like TorchScript, MLIR, or ONNX. We can then use hardware-specific
    software to convert these IR models to compiled machine code for our hardware
    of choice—GPU, TPU (tensor-processing units), CPU, etc. Why not just convert directly
    from your framework of choice to machine code and skip the middleman? Great question.
    The reason is simple: there are dozens of frameworks and hundreds of hardware
    units, and writing code to cover each combination is out of the question. So instead,
    framework developers provide conversion tooling to an IR, and hardware vendors
    provide conversions from an IR to their specific hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, the actual process of compiling a model involves running
    a few commands. Thanks to PyTorch 2.x, you can get a head start on it by using
    the `torch.compile(model)` command, which you should do before training and before
    deployment. Hardware companies often provide compiling software for free, as it’s
    a big incentive for users to purchase their product. Building this software isn’t
    easy, however, and often requires expertise in both the hardware architecture
    and the machine learning architectures. This combination of these talents is rare,
    and there’s good money to be had if you get a job in this field.
  prefs: []
  type: TYPE_NORMAL
- en: We will show you how to compile an LLM in a minute, but first, let’s take a
    look at some of the techniques used. What better place to start than with the
    all-important kernel tuning?
  prefs: []
  type: TYPE_NORMAL
- en: Kernel tuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In deep learning and high-performance computing, a kernel is a small program
    or function designed to run on a GPU or other similar processors. These routines
    are developed by the hardware vendor to maximize chip efficiency. They do this
    by optimizing threads, registries, and shared memory across blocks of circuits
    on the silicon. When we run arbitrary code, the processor will try to route the
    requests the best it can across its logic gates, but it’s bound to run into bottlenecks.
    However, if we are able to identify the kernels to run and their order beforehand,
    the GPU can map out a more efficient route—and that’s essentially what kernel
    tuning is.
  prefs: []
  type: TYPE_NORMAL
- en: During kernel tuning, the most suitable kernels are chosen from a large collection
    of highly optimized kernels. For instance, consider convolution operations that
    have several possible algorithms. The optimal one from the vendor’s library of
    kernels will be based on various factors like the target GPU type, input data
    size, filter size, tensor layout, batch size, and more. When tuning, several of
    these kernels will be run and optimized to minimize execution time.
  prefs: []
  type: TYPE_NORMAL
- en: This process of kernel tuning ensures that the final deployed model is not only
    optimized for the specific neural network architecture being used but also finely
    tuned for the unique characteristics of the deployment platform. This process
    results in more efficient use of resources and maximizes performance. Next, let’s
    look at tensor fusion, which optimizes running these kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor fusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In deep learning, when a framework executes a computation graph, it makes multiple
    function calls for each layer. The computation graph is a powerful concept used
    to simplify mathematical expressions and execute a sequence of tensor operations,
    especially for neural network models. If each operation is performed on the GPU,
    it invokes many CUDA kernel launches. However, the fast kernel computation doesn’t
    quite match the slowness of launching the kernel and handling tensor data. As
    a result, the GPU resources might not be fully utilized, and memory bandwidth
    can become a choke point. It’s like making multiple trips to the store to buy
    separate items when we could make a single trip and buy all the items at once.
  prefs: []
  type: TYPE_NORMAL
- en: This is where tensor fusion comes in. It improves this situation by merging
    or fusing kernels to perform operations as one, reducing unnecessary kernel launches
    and improving memory efficiency. A common example of a composite kernel is a fully
    connected kernel that combines or fuses a matmul, bias add, and ReLU kernel. It’s
    similar to the concept of tensor parallelization. In tensor parallelization, we
    speed up the process by sending different people to different stores, like the
    grocery store, the hardware store, and a retail store. This way, one person doesn’t
    have to go to every store. Tensor fusion can work very well with parallelization
    across multiple GPUs. It’s like sending multiple people to different stores and
    making each one more efficient by picking up multiple items instead of one.
  prefs: []
  type: TYPE_NORMAL
- en: Graph optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tensor fusion, when done sequentially, is also known as vertical graph optimization.
    We can also do horizontal graph optimization. These optimizations are often talked
    about as two different things. Horizontal graph optimization, which we’ll refer
    to simply as graph optimization, combines layers with shared input data but with
    different weights into a single broader kernel. It replaces the concatenation
    layers by pre-allocating output buffers and writing into them in a distributed
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: In figure 6.2, we show an example of a simple deep learning graph being optimized.
    Graph optimizations do not change the underlying computation in the graph. They
    are simply restructuring the graph. As a result, the optimized graph performs
    more efficiently with fewer layers and kernel launches, reducing inference latency.
    This restructuring makes the whole process smaller, faster, and more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2 An example of an unoptimized network compared to the same network
    optimized using graph optimization. CBR is a NVIDIA fused layer kernel that simply
    stands for Convolution, Bias, and ReLU. See the following NVIDIA blog post for
    reference: [https://mng.bz/PNvw](https://mng.bz/PNvw).'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The graph optimization technique is often used in the context of computational
    graph-based frameworks like TensorFlow. Graph optimization involves techniques
    that simplify these computational graphs, remove redundant operations, and/or
    rearrange computations, making them more efficient for execution, especially on
    specific hardware (like GPU or TPU). An example is constant folding, where the
    computations involving constant inputs are performed at compile time (before run
    time), thereby reducing the computation load during run time.
  prefs: []
  type: TYPE_NORMAL
- en: These aren’t all the techniques used when compiling a model, but they are some
    of the most common and should give you an idea of what’s happening under the hood
    and why it works. Now let’s look at some tooling to do this for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: TensorRT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NVIDIA’s TensorRT is a one-stop shop to compile your model, and who better to
    trust than the hardware manufacturer to better prepare your model to run on their
    GPUs? TensorRT does everything talked about in this section, along with quantization
    to INT8 and several memory tricks to get the most out of your hardware to boot.
  prefs: []
  type: TYPE_NORMAL
- en: 'In listing 6.1, we demonstrate the simple process of compiling an LLM using
    TensorRT. We’ll use the PyTorch version known as `torch_tensorrt`. It’s important
    to note that compiling a model to a specific engine is hardware specific. So you
    will want to compile the model on the exact hardware you intend to run it on.
    Consequently, installing TensorRT is a bit more than a simple `pip` `install`;
    thankfully, we can use Docker instead. To get started, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This command will start up an interactive `torch_tensorrt` Docker container
    with practically everything we need to get started (for the latest version, see
     [https://mng.bz/r1We](https://mng.bz/r1We)). The only thing missing is Hugging
    Face Transformers, so go ahead and install that. Now we can run the listing.
  prefs: []
  type: TYPE_NORMAL
- en: After our imports, we’ll load our model and generate an example input so we
    can trace the model. We need to convert our model to an IR—TorchScript here—and
    this is done through tracing. Tracing is the process of capturing the operations
    that are invoked when running the model and makes graph optimization easier later.
    If you have a model that takes varying inputs, for example, the CLIP model, which
    can take both images and text and turn them into embeddings, tracing that model
    with only text data is an effective way of pruning the image operations out of
    the model. Once our model has been converted to an IR, then we can compile it
    for NVIDIA GPUs using TensorRT. Once completed, we then simply reload the model
    from disk and run some inference for demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Compiling a model with TensorRT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Converts to Torchscript IR'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Compiles the model with TensorRT'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Runs with FP16'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Saves the compiled model'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Runs inference'
  prefs: []
  type: TYPE_NORMAL
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll just go ahead and warn you: your results may vary when you run this code,
    depending on your setup. Overall, it’s a simple process once you know what you
    are doing, and we’ve regularly seen at least 2× speed improvements in inference
    times—which translates to major savings!'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorRT really is all that and a bag of chips. Of course, the major downside
    to TensorRT is that, as a tool developed by NVIDIA, it is built with NVIDIA’s
    hardware in mind. When compiling code for other hardware and accelerators, it’s
    not going to be useful. Also, you’ll get very used to running into error messages
    when working with TensorRT. We’ve found that running into compatibility problems
    when converting models that aren’t supported is a common occurrence. We’ve run
    into many problems trying to compile various LLM architectures. Thankfully, to
    address this, NVIDIA has been working on a TensorRT-LLM library to supercharge
    LLM inference on NVIDIA high-end GPUs. It supports many more LLM architectures
    than vanilla TensorRT. You can check if it supports your chosen LLM architecture
    and GPU setup here: [https://mng.bz/mRXP](https://mng.bz/mRXP).'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t get us wrong; you don’t have to use TensorRT. Several alternative compilers
    are available. In fact, let’s look at another popular alternative, ONNX Runtime.
    Trust us, you’ll want an alternative when TensorRT doesn’t play nice.
  prefs: []
  type: TYPE_NORMAL
- en: ONNX Runtime
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ONNX, which stands for Open Neural Network Exchange, is an open source format
    and ecosystem designed for representing and interoperating between different deep
    learning frameworks, libraries, and tools. It was created to address the challenge
    of model portability and compatibility. As mentioned previously, ONNX is an IR
    and allows you to represent models trained in one deep learning framework (e.g.,
    TensorFlow, PyTorch, Keras, MXNet) in a standardized format easily consumed by
    other frameworks. Thus, it facilitates the exchange of models between different
    tools and environments. Unlike TensorRT, ONNX Runtime is intended to be hardware-agnostic,
    meaning it can be used with a variety of hardware accelerators, including CPUs,
    GPUs, and specialized hardware like TPUs.
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, ONNX allows machine learning practitioners and researchers
    to build and train models using their preferred framework and then deploy those
    models to different platforms and hardware without the need for extensive reengineering
    or rewriting of code. This process helps streamline the development and deployment
    of AI and ML models across various applications and industries. To be clear, ONNX
    is an IR format, while ONNX Runtime allows us to optimize and run inference with
    ONNX models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To take advantage of ONNX, we recommend using Hugging Face’s Optimum. Optimum
    is an interface that makes working with optimizers easier and supports multiple
    engines and hardware, including Intel Neural Compressor for Intel chips and Furiosa
    Warboy for Furiosa NPUs. It’s worth checking out. For our purposes, we will use
    it to convert LLMs to ONNX and then optimize them for inference with ONNX Runtime.
    First, let’s install the library with the appropriate engines. We’ll use the `--upgrade-strategy`
    `eager`, as suggested by the documentation, to ensure the different packages are
    upgraded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll run the optimum command line interface. We’ll export it to ONNX,
    point it to a Hugging Face transformer model, and give it a local directory to
    save the model to. Those are all the required steps, but we’ll also give it an
    optimization feature flag. Here, we’ll do the basic general optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: And we are done. We now have an LLM model converted to ONNX format and optimized
    with basic graph optimizations. As with all compiling processes, optimization
    should be done on the hardware you intend to run inference on, which should include
    ample memory and resources, as the conversion can be somewhat computationally
    intensive.
  prefs: []
  type: TYPE_NORMAL
- en: To run the model, check out [https://onnxruntime.ai/](https://onnxruntime.ai/)
    for quick start guides on how to run it with your appropriate SDK. Oh, yeah, did
    we forget to mention that ONNX Runtime supports multiple programming APIs, so
    you can now run your LLM directly in your favorite language, including Java, C++,
    C#, or even JavaScript? Well, you can. So go party. We’ll be sticking to Python
    in this book, though, for consistency’s sake.
  prefs: []
  type: TYPE_NORMAL
- en: While TensorRT is likely to be your weapon of choice most of the time, and ONNX
    Runtime covers many edge cases, there are still many other excellent engines out
    there, like OpenVINO. You can choose whatever you want, but you should at least
    use something. Doing otherwise would be an egregious mistake. In fact, now that
    you’ve read this section, you can no longer claim ignorance. It is now your professional
    responsibility to ensure this happens. Putting any ML model into production that
    hasn’t first been compiled (or at least attempted to be compiled) is a sin to
    the MLOps profession.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 LLM storage strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have a nicely compiled model, we need to think about how our service
    will access it. This step is important because, as discussed in chapter 3, boot
    times can be a nightmare when working with LLMs since it can take a long time
    to load such large assets into memory. So we want to try to speed that up as much
    as possible. When it comes to managing large assets, we tend to throw them into
    an artifact registry or a bucket in cloud storage and forget about them. Both
    of these tend to utilize an object storage system—like GCS or S3—under the hood,
    which is great for storage but less so for object retrieval, especially when it
    comes to large objects like LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Object storage systems break up assets into small fractional bits called objects.
    They allow us to federate the entire asset across multiple machines and physical
    memory locations, a powerful tool that powers the cloud, and to cheaply store
    large objects on commodity hardware. With replication, there is built-in fault
    tolerance, so we never have to worry about losing our assets from a hardware crash.
    Object storage systems also create high availability, ensuring we can always access
    our assets. The downside is that these objects are federated across multiple machines
    and not in an easily accessible form to be read and stored in memory. Consequently,
    when we load an LLM into GPU memory, we will essentially have to download the
    model first. Let’s look at some alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Fusing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fusing is the process of mounting a bucket to your machine as if it were an
    external hard drive. Fusing provides a slick interface and simplifies code, as
    you will no longer have to download the model and then load it into memory. With
    fusing, you can treat an external bucket like a filesystem and load the model
    directly into memory. However, it still doesn’t solve the fundamental need to
    pull the objects of your asset from multiple machines. Of course, if you fuse
    a bucket to a node in the same region and zone, some optimizations can improve
    performance, and it will feel like you are loading the model from the drive. Unfortunately,
    our experience has shown fusing to be quite slow, but it should still be faster
    than downloading and then loading.
  prefs: []
  type: TYPE_NORMAL
- en: Fusing libraries are available for all major cloud providers and on-prem object
    storage solutions, like Ceph or MinIO, so you should be covered no matter the
    environment, including your own laptop. That’s right. You can fuse your laptop
    or an edge device to your object storage solution. This ability demonstrates both
    how powerful and, at the same time, ineffective this strategy is, depending on
    what you were hoping it would achieve.
  prefs: []
  type: TYPE_NORMAL
- en: 'TIP  All fusing libraries are essentially built off the FUSE library. It’s
    worth checking out: [https://github.com/libfuse/libfuse](https://github.com/libfuse/libfuse).'
  prefs: []
  type: TYPE_NORMAL
- en: Baking the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Baking is the process of putting your model into the Docker image. Thus, whenever
    a new container is created, the model will be there, ready for use. Baking models,
    in general, is considered an antipattern. For starters, it doesn’t solve the problem.
    In production, when a new instance is created, a new machine is spun up. It is
    fresh and innocent, knowing nothing of the outside world, so the first step it’ll
    have to take is to download the image. Since the image contains the model, we
    haven’t solved anything. Actually, it’s very likely that downloading the model
    inside an image will be *slower* than downloading the model from an object store.
    So we most likely just made our boot times worse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, baking models is a terrible security practice. Containers often have
    poor security and are often easy for people to gain access to. Third, you’ve doubled
    your problems: before you just had one large asset; now you have two, the model
    and the image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, there are still times when baking is viable, mainly because despite
    the drawbacks, it greatly simplifies our deployments. Throwing all our assets
    into the image guarantees we’ll only need one thing to deploy a new service: the
    image itself, which is really valuable when deploying to an edge device, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: Mounted volume
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another solution is to avoid the object store completely and save your LLM in
    a file-based storage system on a mountable drive. When our service boots up, we
    can connect the disc drive housing the LLM with a RAID controller or Kubernetes,
    depending on our infrastructure. This solution is old school, but it works really
    well. For the most part, it solves all our problems and provides incredibly fast
    boot times.
  prefs: []
  type: TYPE_NORMAL
- en: The downside, of course, is that it will add a bunch of coordination steps to
    ensure there is a volume in each region and zone you plan to deploy to. It also
    brings up replication and reliability problems; if the drive dies unexpectedly,
    you’ll need backups in the region. In addition, these drives will likely be SSDs
    and not just commodity hardware. So you’ll likely be paying a bit more. But storage
    is extremely cheap compared to GPUs, so the time saved in boot times is something
    you’ll have to consider. Essentially, though, this strategy reintroduces all the
    problems for which we usually turn to object stores to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hybrid: Intermediary mounted volume'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lastly, we can always take a hybrid approach. In this solution, we download
    the model at boot time but store it in a volume that is mounted at boot time.
    While this doesn’t help at all with the first deployment in a region, it does
    substantially help any new instances, as they can simply mount this same volume
    and have the model available to load without having to download. You can imagine
    this working similarly to how a Redis cache works, except for storage. Often,
    this technique is more than enough since autoscaling will be fast enough to handle
    bursty workloads. We just have to worry about total system crashes, which hopefully
    should be minimal, but they allude to the fact that we should avoid this approach
    when only running one replica, which you shouldn’t do in production anyway.
  prefs: []
  type: TYPE_NORMAL
- en: In figure 6.3, we demonstrate these different strategies and compare them to
    a basic service where we simply download the LLM and then load it into memory.
    Overall, your exact strategy will depend on your system requirements, the size
    of the LLM you are running, and your infrastructure. Your system requirements
    will also likely vary widely, depending on the type of traffic patterns you see.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 Different strategies for storing LLMs and their implications at boot
    time. Often, we have to balance system reliability, complexity, and application
    load time.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that we have a good handle on how to handle our LLM as an asset, let’s talk
    about some API features that are must-haves for your LLM service.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Adaptive request batching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A typical API will accept and process requests in the order they are received,
    processing them immediately and as quickly as possible. However, anyone who’s
    trained a machine learning model has come to realize that there are mathematical
    and computational advantages to running inference in batches of powers of 2 (16,
    32, 64, etc.), particularly when GPUs are involved, where we can take advantage
    of better memory alignment or vectorized instructions parallelizing computations
    across the GPU cores. To take advantage of this batching, you’ll want to include
    adaptive request batching or dynamic batching.
  prefs: []
  type: TYPE_NORMAL
- en: What adaptive batching does is essentially pool requests together over a certain
    period of time. Once the pool receives the configured maximum batch size or the
    timer runs out, it will run inference on the entire batch through the model, sending
    the results back to the individual clients that requested them. Essentially, it’s
    a queue. Setting one up yourself can and will be a huge pain; thankfully, most
    ML inference services offer this out of the box, and almost all are easy to implement.
    For example, in BentoML, add `@bentoml.Runnable.method(batchable=True)` as a decorator
    to your predict function, and in Triton Inference Server, add `dynamic_batching`
    `{}` at the end of your model definition file.
  prefs: []
  type: TYPE_NORMAL
- en: If that sounds easy, it is. Typically, you don’t need to do any further finessing,
    as the defaults tend to be very practical. That said, if you are looking to maximize
    every bit of efficiency possible in the system, you can often set a maximum batch
    size, which will tell the batcher to run once this limit is reached, or a batch
    delay, which does the same thing but for the timer. Increasing either will result
    in longer latency but likely better throughput, so typically these are only adjusted
    when your system has plenty of latency budget.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the benefits of adaptive batching include better use of resources and
    higher throughput at the cost of a bit of latency. This is a valuable tradeoff,
    and we recommend giving your product the latency bandwidth to include this feature.
    In our experience, optimizing for throughput leads to better reliability and scalability
    and thus greater customer satisfaction. Of course, when latency times are extremely
    important or traffic is few and far between, you may rightly forgo this feature.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.4 Flow control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rate limiters and access keys are critical protections for an API, especially
    one sitting in front of an expensive LLM. Rate limiters control the number of
    requests a client can make to an API within a specified time, which helps protect
    the API server from abuse, such as distributed denial of service (DDoS) attacks,
    where an attacker makes numerous requests simultaneously to overwhelm the system
    and hinder its function.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limiters can also protect the server from bots that make numerous automated
    requests in a short span of time. This helps manage the server resources optimally
    so the server is not exhausted due to unnecessary or harmful traffic. They are
    also useful for managing quotas, thus ensuring all users have fair and equal access
    to the API’s resources. By preventing any single user from using excessive resources,
    the rate limiter ensures the system functions smoothly for all users.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, rate limiters are an important mechanism for controlling the flow
    of your LLM’s system processes. They can play a critical role in dampening bursty
    workloads and preventing your system from getting overwhelmed during autoscaling
    and rolling updates, especially when you have a rather large LLM with longer deployment
    times. Rate limiters can take several forms, and the one you choose will be dependent
    on your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Types of rate limiters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '**The following list describes the types of rate limiters:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Fixed window*—This algorithm allows a fixed number of requests in a set duration
    of time. Let’s say five requests per minute, and it refreshes at the minute. It’s
    really easy to set up and reason about. However, it may lead to uneven distribution
    and can allow a burst of calls at the boundary of the time window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sliding window log*—To prevent boundary problems, we can use a dynamic timeframe.
    Let’s say five requests in the last 60 seconds. This type is a slightly more complex
    version of the fixed window that logs each request’s timestamp to provide a moving
    lookback period, providing a more evenly distributed limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Token bucket*—Clients initially have a full bucket of tokens, and with each
    request, they spend tokens. When the bucket is empty, the requests are blocked.
    The bucket refills slowly over time. Thus, token buckets allow burst behavior,
    but it’s limited to the number of tokens in the bucket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Leaky bucket*—It works as a queue where requests enter, and if the queue is
    not full, they are processed; if full, the request overflows and gets discarded,
    thus controlling the rate of the flow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rate limiter can be applied at multiple levels, from the entire API to individual
    client requests to specific function calls. While you want to avoid being too
    aggressive with them—better to rely on autoscaling to scale and meet demand—you
    don’t want to ignore them completely, especially when it comes to preventing bad
    actors.
  prefs: []
  type: TYPE_NORMAL
- en: Access keys are also crucial to prevent bad actors. Access keys offer authentication,
    maintaining that only authorized users can access the API, which prevents unauthorized
    use and potential misuse of the API and reduces the influx of spam requests. They
    are also essential to set up for any paid service. Of course, even if your API
    is only exposed internally, setting up access keys shouldn’t be ignored, as it
    can help reduce liability and provide a way of controlling costs by yanking access
    to a rogue process, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, setting up a service with rate limiting and access keys is relatively
    easy nowadays, as there are multiple libraries that can help you. In listing 6.2,
    we demonstrate a simple FastAPI app utilizing both. We’ll use FastAPI’s built-in
    security library for our access keys and SlowApi, a simple rate limiter that allows
    us to limit the call of any function or method with a simple decorator.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 Example API with access keys and rate limiter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 This would be encrypted in a database.'
  prefs: []
  type: TYPE_NORMAL
- en: While this is just a simple example, you’ll still need to set up a system for
    users to create and destroy access keys. You’ll also want to finetune your time
    limits. In general, you want them to be as loose as possible so as not to interfere
    with the user experience but just tight enough to do their job.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.5 Streaming responses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One feature your LLM service should absolutely include is streaming. Streaming
    allows us to return the generated text to the user as it is being generated versus
    all at once at the end. Streaming adds quite a bit of complexity to the system,
    but regardless, it has come to be considered a must-have feature for several reasons.
  prefs: []
  type: TYPE_NORMAL
- en: First, LLMs are rather slow, and the worst thing you can do to your users is
    make them wait—waiting means they will become bored, and bored users complain
    or, worse, leave. You don’t want to deal with complaints, do you? Of course not!
    But by streaming the data as it’s being created, we offer the users a more dynamic
    and interactive experience.
  prefs: []
  type: TYPE_NORMAL
- en: Second, LLMs aren’t just slow; they are unpredictable. One prompt could lead
    to pages and pages of generated text, and another, a single token. As a result,
    your latency is going to be all over the place. Streaming allows us to worry about
    more consistent metrics like tokens per second (TPS). Keeping TPS higher than
    the average user’s reading speed means we’ll be sending responses back faster
    than the user can consume them, ensuring they won’t get bored and we are providing
    a high-quality user experience. In contrast, if we wait until the end to return
    the results, users will likely decide to walk away and return when it finishes
    because they never know how long to wait. This huge disruption to their flow makes
    your service less effective or useful.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, users are starting to expect streaming. Streaming responses have become
    a nice tell as to whether you are speaking to a bot or an actual human. Since
    humans have to type, proofread, and edit their responses, we can’t expect written
    responses from a human customer support rep to be in a stream-like fashion. So
    when they see a response streaming in, your users will know they are talking to
    a bot. People interact differently with a bot than they will with a human, so
    it’s very useful information to give them to prevent frustration.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 6.3 we demonstrate a very simple LLM service that utilizes streaming.
    The key pieces to pay attention to are that we are using the base asyncio library
    to allow us to run asynchronous function calls, FastAPI’s `StreamingResponse`
    to ensure we send responses to the clients in chunks, and Hugging Face Transformer’s
    `TextIteratorStreamer` to create a pipeline generator of our model’s inference.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 A streaming LLM service
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads tokenizer, model, and streamer into memory'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Slows things down to see streaming. It’s typical to return streamed responses
    byte encoded.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Starts a separate thread to generate results'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Starts service; defaults to localhost on port 8000'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to implement several must-have features for our LLM service,
    including batching, rate limiting, and streaming, let’s look at some additional
    tooling we can add to our service to improve usability and overall workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.6 Feature store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When it comes to running ML models in production, feature stores really simplify
    the inference process. We first introduced these in chapter 3, but as a recap,
    feature stores establish a centralized source of truth. They answer crucial questions
    about your data: Who is responsible for the feature? What is its definition? Who
    can access it? Let’s take a look at setting one up and querying the data to get
    a feel for how they work. We’ll be using Feast, which is open source and supports
    a variety of backends. To get started, let us `pip` `install` `feast` and then
    run the `init` command in your terminal to set up a project, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The app we are building is a question-and-answer service. Q&A services can greatly
    benefit from a feature store’s data governance tooling. For example, point-in-time
    joins help us answer questions like “Who is the president of x?” where the answer
    is expected to change over time. Instead of querying just the question, we query
    the question with a timestamp, and the point-in-time join will return whatever
    the answer to the question was in our database at that point in time. In the next
    listing, we pull a Q&A dataset and store it in a parquet format in the data directory
    of our Feast project.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.4 Downloading the SQuAD dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads SQuAΔ dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Extracts questions and answers'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Creates a dataframe'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Adds embeddings and timestamps'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Saves to parquet'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll need to define the feature view for our feature store. A feature
    view is essentially like a view in a relational database. We’ll define a name,
    the entities (which are like IDs or primary keys), the schema (which are our feature
    columns), and a source. We’ll just be demoing using a local file store, but in
    production, you’d want to use one of Feast’s many backend integrations with Snowflake,
    GCP, AWS, etc. It currently doesn’t support a VectorDB backend, but I’m sure it’s
    only a matter of time. In addition, we can add metadata to our view through tags
    and define a time to live (TTL), which limits how far back Feast will look when
    generating historical datasets. In the following listing, we define the feature
    view. Go ahead and add this definition into a file called qa.py in the feature_repo
    directory of our project.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 Feast `FeatureView` definition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With that defined, let’s go ahead and register it. We’ll do that with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll want to materialize the view. In production, this is a step you’ll
    need to schedule on a routine basis with something like cron or Prefect. Be sure
    to update the UTC timestamp for the end date in this command to something in the
    future to ensure the view collects the latest data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now all that’s left is to query it! The following listing shows a simple example
    of pulling features to be used at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.6 Querying a feature view at inference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This example will pull the most up-to-date information for the lowest possible
    latency at inference time. For point-in-time retrieval, you would use the `get_historical_
    features` method instead. In addition, in this example, we use a list of IDs for
    the entity rows parameter, but you could also use an SQL query making it very
    flexible and easy to use.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.7 Retrieval-augmented generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retrieval-augmented generation (RAG) has become the most widely used tool to
    combat hallucinations in LLMs and improve the accuracy of responses in our results.
    Its popularity is likely because RAG is both easy to implement and quite effective.
    As first discussed in section 3.4.5, vector databases are a tool you’ll want to
    have in your arsenal. One of the key reasons is that they make RAG so much easier
    to implement. In figure 6.4, we demonstrate a RAG system. In the preprocessing
    stage, we take our documents, break them up, and transform them into embeddings
    that we’ll load into our vector database. During inference, we can take our input,
    encode it into an embedding, and run a similarity search across our documents
    in that vector database to find the nearest neighbors. This type of inference
    is known as semantic search. Pulling relevant documents and inserting them into
    our prompt will help give context to the LLM and improve the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 RAG system demonstrating how we use our input embeddings to run a
    search across our documentation, improving the results of the generated text from
    our LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We are going to demo implementing RAG using Pinecone since it will save us the
    effort of setting up a vector database. For listing 6.7, we will set up a Pinecone
    index and load a Wikipedia dataset into it. In this listing, we’ll create a `WikiDataIngestion`
    class to handle the heavy lifting. This class will load the dataset and run through
    each Wikipedia page, splitting the text into consumable chunks. It will then embed
    these chunks and upload everything in batches. Once we have everything uploaded,
    we can start to make queries.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need an API key if you plan to follow along, so if you don’t already
    have one, go to Pinecone’s website ([https://www.pinecone.io/](https://www.pinecone.io/))
    and create a free account, set up a starter project (free tier), and get an API
    key. One thing to pay attention to as you read the listing is that we’ll split
    up the text into chunks of 400 tokens with `text_ splitter`. We specifically split
    on tokens instead of words or characters, which allows us to properly budget inside
    our token limits for our model. In this example, returning the top three results
    will add 1,200 tokens to our request, which allows us to plan ahead of time how
    many tokens we’ll give to the user to write their prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 Example setting up a Pinecone database
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Gets openai API key from [platform.openai.com](https://platform.openai.com)'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Finds API key in console at app.pinecone.io'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Creates an index if it doesn’t exist'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 1536 dim of text-embedding-ada-002'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Connects to the index and describes the stats'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Uses a generic embedder if an openai api key is not provided'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Also 1536 dim'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Ingests data and describes the stats anew'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Makes a query'
  prefs: []
  type: TYPE_NORMAL
- en: When I ran this code, the top three query results to my question, “Did Johannes
    Gutenberg invent the printing press?” were the Wikipedia pages for Johannes Gutenberg,
    the pencil, and the printing press. Not bad! While a vector database isn’t going
    to be able to answer the question, it’s simply finding the most relevant articles
    based on the proximity of their embeddings to my question.
  prefs: []
  type: TYPE_NORMAL
- en: With these articles, we can then feed their embeddings into our LLM as additional
    context to the question to ensure a more grounded result. Since we include sources,
    it will even have the wiki URL it can give as a reference, and it won’t just hallucinate
    one. By giving this context, we greatly reduce the concern about our LLM hallucinating
    and making up an answer.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.8 LLM service libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are starting to feel a bit overwhelmed about all the tooling and features
    you need to implement to create an LLM service, we have some good news for you:
    several libraries aim to do all of this for you! Some open source libraries of
    note are vLLM and OpenLLM (by BentoML). Hugging Face’s Text-Generation-Inference
    (TGI) briefly lost its open source license, but fortunately, it’s available again
    for commercial use. There are also some start-ups building some cool tooling in
    this space, and we recommend checking out TitanML if you are hoping for a more
    managed service. These are like the tools MLServer, BentoML, and Ray Serve discussed
    in section 3.4.8 on deployment service, but they are designed specifically for
    LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of these toolings are still relatively new and under active development,
    and they are far from feature parity with each other, so pay attention to what
    they offer. What you can expect is that they should at least offer streaming,
    batching, and GPU parallelization support (something we haven’t specifically talked
    about in this chapter), but beyond that, it’s a crapshoot. Many of them still
    don’t support several features discussed in this chapter, nor do they support
    every LLM architecture. What they do, though, is make deploying LLMs easy.
  prefs: []
  type: TYPE_NORMAL
- en: Using vLLM as an example, just `pip` `install` `vllm`, and then you can run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'With just one command, we now have a service up and running the model we trained
    in chapter 5\. Go ahead and play with it; you should be able to send requests
    to the `/generate` endpoint like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: It’s very likely you won’t be all that impressed with any of these toolings.
    Still, you should be able to build your own API and have a good sense of how to
    do it at this point. Now that you have a service and can even spin it up locally,
    let’s discuss the infrastructure you need to set up to support these models for
    actual production usage. Remember, the better the infrastructure, the less likely
    you’ll be called in the middle of the night when your service goes down unexpectedly.
    None of us want that, so let’s check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Setting up infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up infrastructure is a critical aspect of modern software development,
    and we shouldn’t expect machine learning to be any different. To ensure scalability,
    reliability, and efficient deployment of our applications, we need to plan a robust
    infrastructure that can handle the demands of a growing user base. This is where
    Kubernetes comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes, often referred to as k8s, is an open source container orchestration
    platform that helps automate and manage the deployment, scaling, and management
    of containerized applications. It is designed to simplify the process of running
    and coordinating multiple containers across a cluster of servers, making it easier
    to scale applications and ensure high availability. We are going to talk a lot
    about k8s in this chapter, and while you don’t need to be an expert, it will be
    useful to cover some basics to ensure we are all on the same page.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, k8s works by grouping containers into logical units called pods,
    which are the smallest deployable units in the k8s ecosystem. These pods are then
    scheduled and managed by the k8s control plane, which oversees their deployment,
    scaling, and updates. This control plane consists of several components that collectively
    handle the orchestration and management of containers. In figure 6.5, we give
    an oversimplification of the k8s architecture to help readers who are unfamiliar
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 An oversimplification of the Kubernetes architecture. What you need
    to know is that our services run in pods, and pods run on nodes, which essentially
    are a machine. K8s helps us both manage the resources and handle the orchestration
    of deploying pods to these resources.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using k8s, we can take advantage of features such as automatic scaling, load
    balancing, and service discovery, which greatly simplify the deployment and management
    of web applications. K8s provides a flexible and scalable infrastructure that
    can easily adapt to changing demands, allowing organizations to efficiently scale
    their applications as their user base grows. K8s offers a wide range of additional
    features and extensibility options, such as storage management, monitoring, and
    logging, which help ensure the smooth operation of web applications.
  prefs: []
  type: TYPE_NORMAL
- en: One of these extensibility options is known as custom resource definitions (CRDs).
    CRDs are a feature of Kubernetes that allows users to create their own specifications
    for custom resources, thus extending the functionalities of Kubernetes without
    modifying the Kubernetes source code. With a CRD defined, we can create custom
    objects similar to how we would create a built-in object like a pod or service.
    This gives k8s a lot of flexibility that we will need for different functionality
    throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to Kubernetes, you might be scratching your head through parts
    of this section, and that’s totally fine. Hopefully, though, you have enough knowledge
    to get the gist of what we will be doing in this section and why. At least you’ll
    be able to walk away with a bunch of questions to ask your closest DevOps team
    member.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Provisioning clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first thing to do when starting any project is to set up a cluster. A cluster
    is a collective of worker machines or nodes where we will host our applications.
    Creating a cluster is relatively simple; configuring it is the hard part. Of course,
    there have been many books written on how to do this, and the majority of considerations
    like networking, security, and access control are outside the scope of this book.
    In addition, considering the steps you take will also be different depending on
    the cloud provider of choice and your company’s business strategy, we will focus
    on only the portions that we feel are needed to get you up and running, as well
    as any other tidbits that may make your life easier.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create a cluster. On GCP, you would use the gcloud tool
    and run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: On AWS, using the eksctl tool, run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: On Azure, using the az cli tool, run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, even the first steps are highly dependent on your provider,
    and you can suspect that the subsequent steps will be as well. Since we realize
    most readers will be deploying in a wide variety of environments, we will not
    focus on the exact steps but hopefully give you enough context to search and discover
    for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Many readers, we imagine, will already have a cluster set up for them by their
    infrastructure teams, complete with many defaults and best practices. One of these
    is setting up node auto-provisioning (NAP) or cluster autoscaling. NAP allows
    a cluster to grow, adding more nodes as deployments demand them. This way, we
    only pay for nodes we actually use. It’s a very convenient feature, but it often
    defines resource limits or restrictions on the instances available for autoscaling,
    and you can bet your cluster’s defaults don’t include accelerator or GPU instances
    in that pool. We’ll need to fix that.
  prefs: []
  type: TYPE_NORMAL
- en: In GCP, we would create a configuration file like the one in the following listing,
    where we can include the GPU `resourceType`. In the example, we include T4s and
    both A100 types.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.8 Example NAP config file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You would then set this by running
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The real benefit of an NAP is that instead of predefining what resources are
    available at a fixed setting, we can set resource limits, which put a cap on the
    total number of GPUs that we would scale up to. They clearly define what GPUs
    we want and expect to be in any given cluster.
  prefs: []
  type: TYPE_NORMAL
- en: When one author was first learning about limits, he often got them confused
    with similar concepts—quotas, reservations, and commitments—and has seen many
    others just as confused. Quotas, in particular, are very similar to limits. Their
    main purpose is to prevent unexpected overage charges by ensuring a particular
    project or application doesn’t consume too many resources. Unlike limits, which
    are set internally, quotas often require submitting a request to your cloud provider
    when you want to raise them. These requests help inform and are used by the cloud
    provider to better plan which resources to provision and put into different data
    centers in different regions. It’s tempting to think that the cloud provider will
    ensure those resources are available; however, quotas never guarantee there will
    be enough resources in a region for your cluster to use, and you might run into
    `resources` `not` `found` errors way before you hit them.
  prefs: []
  type: TYPE_NORMAL
- en: While quotas and limits set an upper bound, reservations and commitments set
    the lower bound. Reservations are an agreement to guarantee that a certain amount
    of resources will always be available and often come with the caveat that you
    will be paying for these resources regardless of whether you end up using them.
    Commitments are similar to reservations but are often longer-term contracts, usually
    coming with a discounted price.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Autoscaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the big selling points to setting up a k8s cluster is autoscaling. Autoscaling
    is an important ingredient in creating robust production-grade services. The main
    reason is that we never expect any service to receive static request volume. If
    anything else, you should expect more volume during the day and less at night
    while people sleep. So we’ll want our service to spin up more replicas during
    peak hours to improve performance and spin down replicas during off hours to save
    money, not to mention the need to handle bursty workloads that often threaten
    to crash a service at any point.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing your service will automatically provision more resources and set up
    additional deployments based on the needs of the application is what allows many
    infrastructure engineers to sleep peacefully at night. The catch is that it requires
    an engineer to know what those needs are and ensure everything is configured correctly.
    While autoscaling provides flexibility, the real business value comes from the
    cost savings. Most engineers think about autoscaling in terms of scaling up to
    prevent meltdowns, but even more important to the business is the ability to scale
    down, freeing up resources and cutting costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the main reasons cloud computing and technologies like Kubernetes have
    become essential in modern infrastructures is because autoscaling is built in.
    Autoscaling is a key feature of Kubernetes, and with horizontal pod autoscalers
    (HPAs), you can easily adjust the number of replicas of your application based
    on two native resources: CPU and memory usage, as shown in figure 6.6\. However,
    in a book about putting LLMs in production, scaling based on CPU and memory alone
    will never be enough. We will need to scale based on custom metrics, specifically
    GPU utilization.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 Basic autoscaling using the in-built k8s horizontal pod autoscaler
    (HPA). The HPA watches CPU and memory resources and will tell the deployment service
    to increase or decrease the number of replicas.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Setting up autoscaling based on GPU metrics is going to take a bit more work
    and requires setting up several services. It’ll become clear why we need each
    service as we discuss them, but the good news is that by the end, you’ll be able
    to set up your services to scale based on any metric, including external events
    such as messages from a message broker, requests to an HTTP endpoint, and data
    from a queue.
  prefs: []
  type: TYPE_NORMAL
- en: The first service we’ll need is one that can collect the GPU metrics. For this,
    we have NVIDIA’s Data Center GPU Manager (DCGM), which provides a metrics exporter
    that can export GPU metrics. DCGM exposes a host of GPU metrics, including temperature
    and power usage, which can create some fun dashboards, but the most useful metrics
    for autoscaling are utilization and memory utilization.
  prefs: []
  type: TYPE_NORMAL
- en: From here, the data will go to a service like Prometheus. Prometheus is a popular
    open source monitoring system used to monitor Kubernetes clusters and the applications
    running on them. Prometheus collects metrics from various sources and stores them
    in a time-series database, where they can be analyzed and queried. Prometheus
    can collect metrics directly from Kubernetes APIs and from applications running
    on the cluster using a variety of collection mechanisms such as exporters, agents,
    and sidecar containers. It’s essentially an aggregator of services like DCGM,
    including features like alerting and notification. It also exposes an HTTP API
    for service for external tooling like Grafana to query and create graphs and dashboards
    with.
  prefs: []
  type: TYPE_NORMAL
- en: 'While Prometheus provides a way to store metrics and monitor our service, the
    metrics aren’t exposed to the internals of Kubernetes. For an HPA to gain access,
    we will need to register yet another service to either the custom metrics API
    or external metrics API. By default, Kubernetes comes with the metrics.k8s.io
    endpoint that exposes resource metrics, CPU, and memory utilization. To accommodate
    the need to scale deployments and pods on custom metrics, two additional APIs
    were introduced: custom.metrics.k9s.io and external.metrics.k8s.io. There are
    some limitations to this setup, as currently, only one “adapter” API service can
    be registered at a time for either one. This limitation mostly becomes a problem
    if you ever decide to change this endpoint from one provider to another.'
  prefs: []
  type: TYPE_NORMAL
- en: For this service, Prometheus provides the Prometheus Adapter, which works well,
    but from our experience, it wasn’t designed for production workloads. Alternatively,
    we would recommend KEDA. KEDA (Kubernetes Event-Driven Autoscaling) is an open
    source project that provides event-driven autoscaling for Kubernetes. It offers
    more flexibility in terms of the types of custom metrics that can be used for
    autoscaling. While Prometheus Adapter requires configuring metrics inside a ConfigMap,
    any metric already exposed through the Prometheus API can be used in KEDA, providing
    a more streamlined and friendly user experience. It also offers scaling to and
    from 0, which isn’t available through HPAs, allowing you to turn off a service
    completely if there is no traffic. That said, you can’t scale from 0 on resource
    metrics like CPU and memory and, by extension, GPU metrics, but it is useful when
    you are using traffic metrics or a queue to scale.
  prefs: []
  type: TYPE_NORMAL
- en: Putting this all together, you’ll end up with the architecture shown in figure
    6.7\. Compared to figure 6.6, you’ll notice at the bottom that DCGM is managing
    our GPU metrics and feeding them into Prometheus Operator. From Prometheus, we
    can set up external dashboards with tools like Grafana. Internal to k8s, we’ll
    use KEDA to set up a custom.metrics.k9s.io API to return these metrics so we can
    autoscale based on the GPU metrics. KEDA has several CRDs, one of which is a `ScaledObject`,
    which creates the HPA and provides the additional features.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 Autoscaling based on a custom metric like GPU utilization requires
    several extra tools to work, including NVIDIA’s DCGM, a monitoring system like
    Prometheus Operator, and a custom metrics API like that provided by KEDA.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While autoscaling provides many benefits, it’s important to be aware of its
    limitations and potential problems, which are only exacerbated by LLM inference
    services. Proper configuration of the HPA is often an afterthought for many applications,
    but it becomes mission-critical when dealing with LLMs. LLMs take longer to become
    fully operational, as the GPUs need to be initialized and model weights loaded
    into memory; these aren’t services that can turn on a dime, which often can cause
    problems when scaling up if not properly prepared for. Additionally, if the system
    scales down too aggressively, it may result in instances being terminated before
    completing their assigned tasks, leading to data loss or other problems. Lastly,
    flapping is just such a concern that can arise from incorrect autoscaling configurations.
    Flapping happens when the number of replicas keeps oscillating, booting up a new
    service only to terminate it before it can serve any inferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are essentially five parameters to tune when setting up an HPA:'
  prefs: []
  type: TYPE_NORMAL
- en: Target parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Target threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Min pod replicas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max pod replicas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at each of them in turn so you can be sure your system is
    properly configured.
  prefs: []
  type: TYPE_NORMAL
- en: Target parameter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The target parameter is the most important metric to consider when ensuring
    your system is properly configured. If you followed the previously listed steps
    in section 6.2.2, your system is now ready to autoscale based on GPU metrics,
    so this should be easy, right? Not so fast! Scaling based on GPU utilization is
    going to be the most common and straightforward path, but the first thing we need
    to do is ensure the GPU is the actual bottleneck in our service. It’s pretty common
    to see eager young engineers throw a lot of expensive GPUs onto a service but
    forget to include adequate CPU and memory capacity. CPU and memory will still
    be needed to handle the API layer, such as taking in requests, handling multiple
    threads, and communicating with the GPUs. If there aren’t enough resources, these
    layers can quickly become a bottleneck, and your application will be throttled
    way before the GPU utilization is ever affected, ensuring the system will never
    actually autoscale. While you could switch the target parameter on the autoscaler,
    CPU and memory are cheap compared to GPU resources, so it’d be better to allocate
    more of them for your application.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are cases where other metrics make more sense. If your LLM
    application takes most of its requests from a streaming or batch service, it can
    be more prudent to scale based on metrics that tell you a DAG is running or an
    upstream queue is filling up—especially if these metrics give you an early signal
    and allow you more time to scale up in advance.
  prefs: []
  type: TYPE_NORMAL
- en: Another concern when selecting the metric is its stability. For example, an
    individual GPU’s utilization tends to be close to either 0% or 100%. This can
    cause problems for the autoscaler, as the metric oscillates between an on and
    off state, as will its recommendation to add or remove replicas, causing flapping.
    Generally, flapping is avoided by taking the average utilization across all GPUs
    running the service. Using the average will stabilize the metric when you have
    a lot of GPUs, but it could still be a problem when the service has scaled down.
    If you are still running into problems, you’ll want to use an average-over-time
    aggregation, which will tell you the utilization for each GPU over a time frame—say,
    the last 5 minutes. For CPU utilization, average-over-time aggregation is built
    into the Kubernetes HPA and can be set with the `horizontal-pod-autoscaler-cpu-initialization-period`
    flag. For custom metrics, you’ll need to set it in your metric query (for Prometheus,
    it would be the `avg_over_ time` aggregation function).
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, it’s worth calling out that most systems allow you to autoscale based
    on multiple metrics. So you *could* autoscale based on both CPU and GPU utilization,
    as an example. However, we would recommend avoiding these setups unless you know
    what you are doing. Your autoscaler might be set up that way, but in actuality,
    your service will likely only ever autoscale based on just one of the metrics
    due to service load, and it’s best to make sure that metric is the more costly
    resource for cost-engineering purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Target threshold
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The target threshold tells your service at what point to start upscaling. For
    example, if you are scaling based on the average GPU utilization and your threshold
    is set to 30, then a new replica will be booted up to take on the extra load when
    the average GPU utilization is above 30%. The formula that governs this is quite
    simple and is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: desiredReplicas = ceil[currentReplicas × (currentMetricValue / desiredMetricValue
    )]
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  You can learn more about the algorithm at [https://mng.bz/x64g](https://mng.bz/x64g).
  prefs: []
  type: TYPE_NORMAL
- en: This can be hard to tune in correctly, but here are some guiding principles.
    If the traffic patterns you see involve a lot of constant small bursts of traffic,
    a lower value, around 50, might be more appropriate. This setting ensures you
    start to scale up more quickly, avoiding unreliability problems, and you can also
    scale down more quickly, cutting costs. If you have a constant steady flow of
    traffic, higher values, around 80, will work well. Outside of testing your autoscaler,
    it’s best to avoid extremely low values, as they can increase your chances of
    flapping. You should also avoid extremely high values, as they may allow the active
    replicas to be overwhelmed before new ones start to boot up, which can cause unreliability
    or downtime. It’s also important to remember that due to the nature of pipeline
    parallel workflows when using a distributed GPU setup, there will always be a
    bubble, as discussed in section 3.3.2\. As a result, your system will never reach
    100% GPU utilization, and you will start to hit problems earlier than expected.
    Depending on how big your bubble is, you will need to adjust the target threshold
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Minimum pod replicas
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Minimum pod replicas determine the number of replicas of your service that will
    always be running. This setting is your baseline. It’s important to make sure
    it’s set slightly above your baseline of incoming requests. Too often, this is
    set strictly to meet baseline levels of traffic or just below, but a steady state
    for incoming traffic is rarely all that steady. This is where a lot of oscillating
    can happen, as you are more likely to see many small surges in traffic than large
    spikes. However, you don’t want to set it too high, as this will tie up valuable
    resources in the cluster and increase costs.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum pod replicas
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Maximum pod replicas determine the number of replicas your system will run at
    peak capacity. You should set this number to be just above your peak traffic requirements.
    Setting it too low could lead to reliability problems, performance degradation,
    and downtime during high-traffic periods. Setting it too high could lead to resource
    waste, running more pods than necessary, and delaying the detection of real problems.
    For example, if your application was under a DDoS attack, your system might scale
    to handle the load, but it would likely cost you severely and hide the problem.
    With LLMs, you also need to be cautious not to overload the underlying cluster
    and make sure you have enough resources in your quotas to handle the peak load.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling policies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Scaling policies define the behavior of the autoscaler, allowing you to finetune
    how long to wait before scaling and how quickly it scales. This setting is usually
    ignored, and safely so for most setups because the defaults for these settings
    tend to be pretty good for the typical application. However, relying on the default
    would be a major mistake for an LLM service since it takes so long to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: The first setting you’ll want to adjust is the stabilization window, which determines
    how long to wait before taking a new scaling action. You can set a different stabilization
    window for upscaling and downscaling tasks. The default upscaling window is 0
    seconds, which should not need to be touched if your target parameter has been
    set correctly. The default downscaling window is 300 seconds, which is likely
    too short for our use case. You’ll typically want this at least as long as it
    takes your service to deploy and then a little bit more. Otherwise, you’ll be
    adding replicas only to remove them before they have a chance to do anything.
  prefs: []
  type: TYPE_NORMAL
- en: The next parameter you’ll want to adjust is the scale-down policy, which defaults
    to 100% of pods every 15 seconds. As a result, any temporary drop in traffic could
    result in all your extra pods above the minimum being terminated immediately.
    For our case, it’s much safer to slow this down since terminating a pod takes
    only a few seconds, but booting one up can take minutes, making it a semi-irreversible
    decision. The exact policy will depend on your traffic patterns, but in general,
    we want to have a little more patience. You can adjust how quickly pods will be
    terminated and the magnitude by the number or percentage of pods. For example,
    we could configure the policy to allow only one pod each minute or 10% of pods
    every 5 minutes to be terminated.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Rolling updates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rolling updates or rolling upgrades is a strategy that gradually implements
    the new version of an application to reduce downtime and maximize agility. It
    works by gradually creating new instances and turning off the old ones, replacing
    them in a methodical manner. This update approach allows the system to remain
    functional and accessible to users even during the update process, otherwise known
    as zero downtime. Rolling updates also make it easier to catch bugs before they
    have too much effect and roll back faulty deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates is a feature built into k8s and another major reason for its
    widespread use and popularity. Kubernetes provides an automated and simplified
    way to carry out rolling updates. The rolling updates ensure that Kubernetes incrementally
    updates pod instances with new ones during deployment. The following listing shows
    an example LLM deployment implementing rolling updates; the relevant configuration
    is under the `spec.strategy` section.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.9 Example deployment config with rolling update
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll notice that there are two main parameters you can adjust for a rolling
    update: `maxSurge` and `maxUnavailable`. These can either be set to a whole number,
    like in our example, describing the number of instances, or a fraction indicating
    a percentage of total instances. In the example, you’ll notice we set `maxSurge`
    to `1`, meaning even though we would normally run with five replicas, we could
    surge to six during a deployment, allowing us to turn on a new one before turning
    any off. Normally, you might want to set this higher, as it allows for a quicker
    rolling update. Otherwise, we’ll have to replace pods one at a time. The reason
    it’s low, you might have noticed, is that we are deploying a rather large LLM
    that requires eight GPUs. If these are A100s, it’s likely going to be hard to
    find an extra eight GPUs not being used.'
  prefs: []
  type: TYPE_NORMAL
- en: GPU resources cannot be shared among containers, and container orchestration
    can become a major challenge in such deployments, which is why `maxUnavailable`
    is set to `3`. What we are saying here is that three out of the five expected
    replicas can go down during a deployment. In other words, we are going to drop
    the total number of replicas for a little bit before re-creating them. For reliability
    reasons, we typically prefer adding extra replicas first, so to go down instead
    is a difficult decision, one you’ll want to confirm you can afford to do in your
    own deployment. The reason we are doing so here is to ensure that there are GPU
    resources available. In essence, to balance resource utilization, it might be
    necessary to set `maxUnavailable` to a high value and adjust `maxSurge` to a lower
    number to downscale old versions quickly and free up resources for new ones.
  prefs: []
  type: TYPE_NORMAL
- en: This advice is the opposite of what you’d do in most applications, so we understand
    if it makes you uneasy. If you’d like to ensure smoother deployments, you’ll need
    to budget for extra GPUs to be provisioned in your cluster strictly for deployment
    purposes. However, depending on how often you are updating the model itself, paying
    for expensive GPUs to sit idle simply to make deployments smoother may not be
    cost-advantageous. Often, the LLM itself doesn’t receive that many updates, so
    assuming you are using an inference graph (discussed in the next section), most
    of the updates will be to the API, prompts, or surrounding application.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we recommend you always perform such operations cautiously in a
    staging environment first to understand its effect. Catching a deployment problem
    in staging will save you a headache or two. It’s also useful to troubleshoot the
    `maxUnavailable` and `maxSurge` parameters in staging, but it’s often hard to
    get a one-to-one comparison to production since staging is often resource-constrained.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.4 Inference graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inference graphs are the crème filling of a donut, the muffin top of a muffin,
    and the toppings on a pizza: they are just phenomenal. Inference graphs allow
    us to create sophisticated flow diagrams at inference in a resource-saving way.
    Consider figure 6.8, which shows us the building blocks for any inference graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 The three types of inference graph building blocks. Sequential allows
    us to run one model before the other, which is useful for preprocessing steps
    like generating embeddings. Ensembles allow us to pool several models together
    to learn from each and combine their results. Routing allows us to send traffic
    to specific models based on some criteria, often used for multi-armed bandit optimization.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Generally, any time you have more than one model, it’s useful to consider an
    inference graph architecture. Your standard LLM setup is usually already at least
    two models: an encoder and the language model itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Usually, when we see LLMs deployed in the wild, these two models are deployed
    together. You send text data to your system, and it returns generated text. It’s
    often no big deal, but when deployed as a sequential inference graph instead of
    a packaged service, we get some added bonuses. First, the encoder is usually much
    faster than the LLM, so we can split them up since you may only need one encoder
    instance for every two to three LLM instances. Encoders are so small that this
    doesn’t necessarily help us out that much, but it saves the hassle of redeploying
    the entire LLM if we decide to deploy a new encoder model version. In addition,
    an inference graph will set up an individual API for each model, which allows
    us to hit the LLM and encoder separately. This is really useful if we have a bunch
    of data we’d like to preprocess and save in a VectorDB; we can use the same encoder
    we already have deployed. We can then pull this data and send it directly into
    the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest benefit of an inference graph is that it allows us to separate the
    API and the LLM. The API sitting in front of the LLM is likely to change much
    more often as you tweak prompts, add features, and fix bugs. The ability to update
    the API without having to deploy the LLM will save your team a lot of effort.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now consider figure 6.9, which provides an example inference graph deployment
    using Seldon. In this example, we have an encoder model, an LLM, a classifier
    model, and a simple API that combines the results. Whereas we would have to build
    a container and the interface for each of these models, Seldon creates an orchestrator
    that handles communication between a user’s request and each node in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 An example inference graph deployment using Seldon. A Seldon Deployment
    is a Kubernetes CRD that extends a regular Kubernetes deployment and adds an orchestrator
    that ensures the proper communication between all the models are run in graph
    order.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: NOTE  Seldon is an open source platform designed for deploying and managing
    machine learning models in production. It offers tools and capabilities to help
    organizations streamline the deployment and scaling of machine learning and deep
    learning models in a Kubernetes-based environment. It offers k8s CRDs to implement
    inference graphs.
  prefs: []
  type: TYPE_NORMAL
- en: If you are wondering how to create this, listing 6.10 shows an example configuration
    that would create this exact setup. We simply define the containers in the graph
    and their relationship inside the graph. You’ll notice `apiVersion` defines the
    CRD from Seldon, which allows us to use `SeldonDeployment`, which is just an extension
    of the k8s regular Deployment object. In the listing, you might notice that the
    combiner is the parent to the LLM and classifier models, which feels backwards
    from how we visualize it in figure 6.9\. This is because a component will only
    ever have one parent, but can have multiple children, so a `COMBINER` is always
    a parent node even though functionally it’s the same. Setting up a graph can often
    be confusing, so I recommend you check the documentation frequently and often.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.10 An example `SeldonDeployment` configuration file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: If you’ve deployed enough machine learning systems, you’ve realized that many
    of them require complex systems, and inference graphs make it easy, or at least
    easier. And that is a big difference. Although inference graphs are a smarter
    way to deploy complex machine learning systems, it’s always important to ask yourself
    if the extra complexity is actually needed. Even with tools like inference graphs,
    it’s better to keep things simple whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.5 Monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with any product or service deployed into production, monitoring is critical
    to ensure reliability, performance, and compliance to service level agreements
    and objectives are met. As with any service, we care about monitoring typical
    performance metrics like queries per second (QPS), latency, and response code
    counts. We also care about monitoring our resources with metrics like CPU utilization,
    percentage of memory used, GPU utilization, and GPU temperature, among many more.
    When any of these metrics start to fail, it often indicates a catastrophic failure
    of some sort and will need to be addressed quickly.
  prefs: []
  type: TYPE_NORMAL
- en: For these metrics, any software engineering team should have plenty of experience
    working with these using tools like Prometheus and Grafana or the ELK stack (Elasticsearch,
    Logstash, and Kibana). You will benefit immensely by taking advantage of the systems
    that are likely already in place. If they aren’t in place, we already went over
    how to set up the GPU metrics for monitoring back in section 6.2.2, and that system
    should be useful for monitoring other resources.
  prefs: []
  type: TYPE_NORMAL
- en: However, with any ML project, we have additional concerns that traditional monitoring
    tools miss, which leads to silent failures. This usually comes from data drift
    and performance decay, where a model continues to function but starts to do so
    poorly and no longer meets quality expectations. LLMs are particularly susceptible
    to data drift since language is in constant flux, as new words are created and
    old words change meaning all the time. Thus, we often need both a system monitoring
    solution and an ML monitoring solution.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring data drift is relatively easy and well-studied for numerical datasets,
    but monitoring unstructured text data provides an extra challenge. We’ve already
    discussed ways to evaluate language models in chapter 4, and we’ll need to use
    similar practices to evaluate and monitor models in production. One of our favorite
    tools for monitoring drift detection is whylogs due to its efficient nature of
    capturing summary statistics at scale. Adding LangKit to the mix instantly and
    easily allows us to track several useful metrics for LLMs, such as readability,
    complexity, toxicity, and even similarity scores to known prompt injection attacks.
    In the following listing, we demonstrate a simple application that logs and monitors
    text data using whylogs and LangKit.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.11 Using whylogs and LangKit to monitor text data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Runs app manually'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Prevents truncation of columns'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Gets the first profile and shows the results'
  prefs: []
  type: TYPE_NORMAL
- en: The generated text is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: While this is just a demo using a text dataset, you can see how it would be
    beneficial to monitor the incoming prompts and outgoing generated text for metrics
    such as readability, complexity, and toxicity. These monitoring tools will help
    give you a handle on whether or not your LLM service is starting to fail silently.
  prefs: []
  type: TYPE_NORMAL
- en: When monitoring in production, we must be mindful of the effect latency may
    have on our service. LangKit uses several lightweight models to evaluate the text
    for the advanced metrics. While we haven’t noticed significant memory effects,
    there is a very slight effect on latency when evaluating logs in the direct inference
    path. To avoid this, we can take it out of the inference path and into what is
    called a sidecar.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not uncommon to see ML teams mistakenly place data quality checks in the
    critical path. Their intentions may be good (to ensure only clean data runs through
    a model), but on the off chance that a client sends bad data, it would often be
    better to just send a 400 or 500 error response than to add expensive latency
    costs to the good requests. In fact, many applications move monitoring out of
    the critical path entirely, opting to process it in parallel. The simplest way
    to do this is to use a Kubernetes sidecar, which is depicted in figure 6.10\.
    You can do this with tools that specialize in this, like fluentd; whylogs also
    offers a container you can run as a sidecar.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 An example Kubernetes sidecar container, which takes logging out
    of the critical path. The logging agent would be a tool like a whylogs container
    or fluentd that captures specific requests or all stdout print statements, processes
    them, and forwards them to a logging backend like WhyLabs or Prometheus.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are different sidecar configurations, but the main gist is that a logging
    container will run in the same k8s pod, and instead of the main app writing to
    a logs file, this sidecar acts as an intermediate step, first processing and cleaning
    the data, which it can then send directly to a backend or write to a logs file
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE  You can learn more about Kubernetes logging architectures in its docs
    here: [https://mng.bz/Aaog](https://mng.bz/Aaog).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know more about setting up our infrastructure, including provisioning
    a cluster and implementing features like GPU autoscaling and monitoring, you should
    be set to deploy your LLM service and ensure it is reliable and scalable. Next,
    let’s talk about different challenges you are likely to face and methodologies
    to address these problems.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Production challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we’ve covered how to get a service up and running, nevertheless, you will
    find a never-ending host of hurdles you’ll need to jump over when it comes to
    deploying models and maintaining them in production. Some of these challenges
    include updating, planning for large loads, poor latency, acquiring resources,
    and more. To help, we wanted to address some of the most common problems and give
    you tips on how to handle them.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Model updates and retraining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We recently discussed ML monitoring, watching your model for silent failures
    and data drift, but what do you do when you notice the model has gone belly up?
    We’ve seen in many traditional ML implementations that the answer is to simply
    retrain the model on the latest data and redeploy. And that works well when you
    are working with a small ARIMA model; in fact, we can often set up a CI/CD pipeline
    to run whenever our model degrades without any human oversight. But with a massive
    LLM? It doesn’t make any sense.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we aren’t going to retrain from scratch, and we likely need to finetune
    our model, but the reason it doesn’t make sense is seen when we ask ourselves
    just what exactly the latest data is. The data we need to finetune the model is
    extremely important, and so it becomes necessary for us to take a step back and
    really diagnose the problem. What are the edge cases our model is failing on?
    What is it still doing well? How exactly have incoming prompts changed? Depending
    on the answers, we might not need to finetune at all. For example, consider a
    Q&A bot that is no longer effective at answering current event questions as time
    goes on. We probably don’t want to retrain a model on a large corpus of the latest
    news articles. Instead, we would get much better results by ensuring our RAG system
    is up to date. Similarly, there are likely plenty of times that simply tweaking
    prompts will do the trick.
  prefs: []
  type: TYPE_NORMAL
- en: In the cases where finetuning is the correct approach, you’ll need to think
    a lot about exactly what data you might be missing, as well as how any major updates
    might affect downstream systems, like finely tuned prompts. For example, when
    using knowledge distillation, this consideration can be particularly annoying.
    You will likely notice the problem in your student model but then must decide
    whether you need to retrain the student or the teacher. With any updates to the
    teacher model, you’ll need to ensure progress to the student model.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, it’s best to take a proactive approach to LLM model updates instead
    of a purely reactionary one. A system that often works well is to establish business
    practices and protocols to update the model on a periodic basis, say once a quarter
    or once a month. During the time between updates, the team will focus on monitoring
    cases where the model performs poorly and gather appropriate data and examples
    to make updating smooth. This type of practice will help you prevent silent failures
    and ensure your model isn’t just maintained but improving.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Load testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Load testing is a type of performance testing that assesses how well a service
    or system will perform under—wait for it—load. The primary goal of load testing
    is to ensure the system can handle the expected workload without performance degradation
    or failure. Doing it early can ensure we avoid bottlenecks and scalability problems.
    Since LLM services can be both expensive and resource intensive, it’s even more
    important to ensure you load test the system before releasing your LLM application
    to production or before an expected peak in traffic, like during a Black Friday
    sales event.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load testing an LLM service, for the most part, is like load testing any other
    service and follows these basic steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up the service in a staging environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a script to periodically send requests to the service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase requests until the service fails or autoscales.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which metrics you log depends on your service and what you are testing. The
    main metrics to watch are latency and throughput at failure, as these can be used
    to extrapolate to determine how many replicas you’ll need to handle peak load.
    Latency is the total time it takes for a request to be completed, and throughput
    tells us the queries per second (QPS), both of which are extremely important metrics
    when analyzing our system. Still, since many LLM services offer streaming responses,
    they don’t help us understand the user experience. A few more metrics you’ll want
    to capture to understand your perceived responsiveness are time to first token
    (TTFT) and tokens per second (TPS). TTFT gives us the perceived latency; it tells
    us how long it takes until the user starts to receive feedback, while TPS tells
    us how fast the stream is. For English, you’ll want a TPS of about 11 tokens per
    second, which is a little faster than most people read. If it’s slower than this,
    your users might get bored as they wait for tokens to be returned.
  prefs: []
  type: TYPE_NORMAL
- en: Related to TPS, I’ve seen several tools or reports use the inverse metric, time
    per output token (TPOT), or intertoken latency (ITL), but we’re not a fan of these
    metrics or their hard-to-remember names. You’ll also want to pay attention to
    resource metrics, CPU and GPU utilization, and memory usage. You’ll want to ensure
    these aren’t being hammered under base load conditions, as this can lead to hardware
    failures. These are also key to watch when you are testing autoscaling performance.
  prefs: []
  type: TYPE_NORMAL
- en: One of my favorite tools for load testing is Locust. Locust is an open source
    load-testing tool that makes it easy to scale and distribute running load tests
    over multiple machines, allowing you to simulate millions of users. Locust does
    all the hard work for you and comes with many handy features, like a nice web
    user interface and the ability to run custom load shapes. It’s easy to run in
    Docker or Kubernetes, making it extremely accessible to run where you need it—in
    production. The only main downside we’ve run across is that it doesn’t support
    customizable metrics, so we’ll have to roll our own to add TTFT and TPS.
  prefs: []
  type: TYPE_NORMAL
- en: To get started, simply `pip` `install` `locust`. Next, we’ll create our test.
    In listing 6.12, we show how to create a locust file that will allow users to
    prompt an LLM streaming service. It’s a bit more complicated than many locust
    files we’ve used simply because we need to capture our custom metrics for streaming,
    so you can imagine how straightforward they normally are. Locust already captures
    a robust set of metrics, so you won’t have to deal with this often. You’ll notice
    in the listing that we are saving these custom metrics to `stats.csv` file, but
    if you were running Locust in a distributed fashion, it’d be better to save it
    to a database of some sort.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.12 Load testing with Locust
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates a CSV file to store custom stats'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Initiates the test'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Makes request'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Finishes and calculates the stats'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Saves stats'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you run it, you’ll need to have an LLM service up. For this example,
    we’ll run the code from listing 6.3 in section 6.1.6, which spins up a very simple
    LLM service. With a service up and our test defined, we need to run it. To spin
    up the Locust service, run the `locust` command. You should then be able to navigate
    to the web UI in your browser. See the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Once in the web UI, you can explore running different tests; you’ll just need
    to point Locust at the host where your LLM service is running, which for us should
    be running on localhost on port 8000 or for the full socket address we combined
    them for: http://0.0.0.0:8000\. In figure 6.11, you can see an example test where
    we increased the active users to 50 at a spawn rate of 1 per second. You can see
    that on the hardware, this simple service starts to hit a bottleneck at around
    34 users, where the QPS starts to decrease, as it’s no longer able to keep up
    with the load. You’ll also notice response times slowly creep up in response to
    heavier load. We could continue to push the number of users up until we started
    to see failures, but overall, this test was informative and a great first test
    drive.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 Locust test interface demoing an example run increasing the number
    of users to 50 at a spawn rate of 1 per second. The requests per second peaks
    at 34 users, indicating a bottleneck for our service.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In addition to manually running load tests, we can run Locust in a headless
    mode for automated tests. The following code is a simple command to run the exact
    same test as seen in figure 6.11; however, since we won’t be around to see the
    report, we’ll save the data to CSV files labeled with the prefix `llm` to be processed
    and analyzed later. There will be four files in addition to the stats CSV file
    we were already generating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now that you are able to load test your LLM service, you should be able to figure
    out how many replicas you’ll need to meet throughput requirements. It’s just a
    matter of spinning up more services. But what do you do when you find out your
    service doesn’t meet latency requirements? Well, that’s a bit tougher, so let’s
    discuss it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3 Troubleshooting poor latency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the biggest bottlenecks when it comes to your model’s performance in
    terms of latency and throughput has nothing to do with the model itself but comes
    from data transmission of the network. One of the simplest methods to improve
    this I/O constraint is to serialize the data before sending it across the wire,
    which can have a large effect on ML workloads where the payloads tend to be larger,
    including LLMs where prompts tend to be long.
  prefs: []
  type: TYPE_NORMAL
- en: To serialize the data, we utilize a framework known as Google Remote Procedure
    Call (gRPC). gRPC is an API protocol similar to REST, but instead of sending JSON
    objects, we compress the payloads into a binary serialized format using Protocol
    Buffers, also known as protobufs. By doing this, we can send more information
    in fewer bytes, which can easily give us orders of magnitude improvements in latency.
    Luckily, most inference services will implement gRPC along with their REST counterparts
    right out of the box, which is extremely convenient since the major hurdle to
    using gRPC is setting it up.
  prefs: []
  type: TYPE_NORMAL
- en: A major reason for this convenience is the Seldon V2 Inference Protocol, which
    is widely implemented. The only hurdle, then, is ensuring our client can serialize
    and deserialize messages to take advantage of the protocol. In listing 6.13, we
    show an example client using MLServer to do this. It’s a little bit more in depth
    than your typical `curl` request, but a closer inspection shows the majority of
    the complexity is simply converting the data from different types as we serialize
    and deserialize it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.13 Example client using gRPC
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Sets up the request structure via V2 Inference Protocol'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Serializes the request to the Protocol Buffer'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Connects to the gRPC server'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Δeserializes the response and converts to the Python dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t use an inference service but want to implement a gRPC API, you’ll
    have to put down familiar tooling like FastAPI, which is strictly REST. Instead,
    you’ll likely want to use the grpcio library to create your API, and you’ll have
    to become familiar with .proto files to create your protobufs. It can be a relatively
    steep learning curve and beyond the scope of this book, but the advantages are
    well worth it.
  prefs: []
  type: TYPE_NORMAL
- en: There are also plenty of other ideas to try if you are looking to squeeze out
    every last drop of performance. Another way to improve latency that shouldn’t
    be overlooked is ensuring you compile your model. We hammered this point pretty
    heavily at the beginning of this chapter, but it’s important to bring it up again.
    Next, be sure to deploy the model in a region or data center close to your users;
    this point is obvious to most software engineers, but for LLMs, we have to be
    somewhat wary, as the data center of choice may not have your accelerator of choice.
    Most cloud providers will be willing to help you with this, but it’s not always
    a quick and easy solution for them to install the hardware in a new location.
    Note that if you have to switch to a different accelerator to move regions, you’ll
    have to remember to compile your model all over again for the new hardware architecture!
    On that note, consider scaling up your accelerator. If you are currently opting
    for more price-effective GPUs but latency is becoming a bottleneck, paying for
    the latest and greatest can often speed up inference times.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, caching is always worth considering. It’s not likely, but on the
    off chance your users are often sending the same requests and the inputs can be
    easily normalized, you should implement caching. The fastest LLM is one we don’t
    actually run, so there’s no reason to run the LLM if you don’t have to. Also,
    we just went over this, but always be sure to load test and profile your service,
    making note of any bottlenecks, and optimize your code. Sometimes we make mistakes,
    and if the slowest process in the pipeline isn’t the actual LLM running inference,
    something is wrong. Last but not least, consider using a smaller model or an ensemble
    of them. It’s always been a tradeoff in ML deployments, but often sacrificing
    a bit of quality in the model or the accuracy of the results is acceptable to
    improve the overall reliability and speed of the service.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.4 Resource management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ve heard us say it a lot throughout the book, but we are currently in a
    GPU shortage, which has been true for almost the last 10 years, so we’re confident
    that when you read this sometime in the future, it will likely still be true.
    The truth is that the world can’t seem to get enough high-performance computing,
    and LLMs and generative AI are only the latest in a long list of applications
    that have driven up demand in recent years. It seems that once we seem to get
    a handle on supply, there’s another new reason for consumers and companies to
    want to use them.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, it’s best to consider strategies to manage these resources.
    One tool we’ve quickly become a big fan of is SkyPilot ([https://github.com/skypilot-org/skypilot](https://github.com/skypilot-org/skypilot)).
    SkyPilot is an open source project that aims to abstract away cloud infra burdens—in
    particular, maximizing GPU availability for your jobs. You use it by defining
    a task you want to run and then running the `sky` CLI command; it will search
    across multiple cloud providers, clusters, regions, and zones, depending on how
    you have it configured, until it finds an instance that meets your resource requirements
    and starts the job. Some common tasks are built-in, such as provisioning a GPU-backed
    Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall, in chapter 5, we showed you how to set up a virtual machine
    (VM) to run multi-GPU environments with gcloud. Using SkyPilot, that gets simplified
    to one command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In addition to provisioning the VM, it also sets up port forwarding, which allows
    us to run Jupyter Notebook and access it through your browser. Pretty nifty!
  prefs: []
  type: TYPE_NORMAL
- en: Another project to be on the watch for is Run:ai. Run:ai is a small startup
    that was aquired by NVIDIA for no small sum. It offers GPU optimization tooling,
    such as over quota provisioning, GPU oversubscription, and fractional GPU capabilities.
    It also helps you manage your clusters to increase GPU availability with GPU pooling,
    dynamic resource sharing, job scheduling, and more. What does all that mean? We’re
    not exactly sure, but their marketing team definitely sold us. Jokes aside, they
    offer a smarter way to manage your accelerators, and it’s very welcome. We expect
    we’ll see more competitors in this space in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.5 Cost engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to getting the most bang for your buck with LLMs, there’s lots
    to consider. In general, regardless of whether you deploy your own or pay for
    one in an API, you’ll be paying for the number of output tokens. For most paid
    services, this is a direct cost, but for your own service, it is often paid through
    longer inference times and extra compute time. In fact, it’s been suggested that
    simply adding “be concise” to your prompt can save you up to 90% of your costs.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll also save a lot by using text embeddings. We introduced RAG earlier,
    but what’s lost on many is that you don’t have to take the semantic search results
    and add them to your prompt to have your LLM “clean it up.” You could return the
    semantic search results directly to your user. It is much cheaper to look something
    up in a vector store than to ask an LLM to generate it. Simple neural information
    retrieval systems will save you significant amounts when doing simple fact lookups
    like, “Who’s the CEO of Twitter?” Self-hosting these embeddings should also significantly
    cut down the costs even further. If your users are constantly asking the same
    types of questions, consider taking the results of your LLM to these questions
    and storing them in your vector store for faster and cheaper responses.
  prefs: []
  type: TYPE_NORMAL
- en: You also need to consider which model you should use for which task. Generally,
    bigger models are better at a wider variety of tasks, but if a smaller model is
    good enough for a specific job, you’ll save a lot by using it. For example, if
    we just assumed the price was linear to the number of parameters, you could run
    10 Llama-2-7b models for the same cost as 1 Llama-2-70b. We realize the cost calculations
    are more complicated than that, but it’s worth investigating.
  prefs: []
  type: TYPE_NORMAL
- en: When comparing different LLM architectures, it’s not always just about size.
    Often, you’ll want to consider whether the architecture is supported for different
    quantization and compiling strategies. New architectures often boast impressive
    results on benchmarking leaderboards but lag behind when it comes to compiling
    and preparing them for production.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you’ll need to consider the costs of GPUs to use when running. In general,
    you’ll want to use the least amount of GPUs needed to fit the model into memory
    to reduce the cost of idling caused by bubbles, as discussed in section 3.3.2\.
    Determining the correct number of GPUs isn’t always intuitive. For example, it’s
    cheaper to run four T4s than to run one A100, so it might be tempting to split
    up a large model onto smaller devices, but the inefficiency will often catch up
    to you. We have found that paying for newer, more expensive GPUs often saves us
    in the long run, as these GPUs tend to be more efficient and get the job done
    faster. This is particularly true when running batch inference. Ultimately, you’ll
    want to test different GPUs and find what configuration is cost optimal, as it
    will be different for every application.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of moving parts: model, service, machine instance, cloud provider,
    prompt, etc. While we’ve been trying to help you understand the best rules of
    thumb, you’ll want to test it out, which is where the cost engineering really
    comes into play. The simple way to test your cost efficiency is to create a matrix
    of your top choices; then, spin up a service for each combination and run your
    load testing. When you have an idea of how each instance runs under load and how
    much that particular instance will cost to run, you can then translate metrics
    like TPS to dollars per token (DTP). You’ll likely find that the most performant
    solution is rarely the most cost-optimal solution, but it gives you another metric
    to make a decision that’s best for you and your company.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.6 Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Security is always an undercurrent and a consideration when working in production
    environments. All the regular protocols and standard procedures should be considered
    when working with LLMs that you would consider for a regular app, like in-transit
    encryption with a protocol like HTTPS, authorization and authentication, activity
    monitoring and logging, network security, firewalls, and the list goes on—all
    of which could, and have, taken up articles, blog posts, and books of their own.
    When it comes to LLMs, you should worry about two big failure cases: an attacker
    gets an LLM agent to execute nefarious code, or an attacker gains access to proprietary
    data like passwords or secrets the LLM was trained on or has access to.'
  prefs: []
  type: TYPE_NORMAL
- en: For the first concern, the best solution is to ensure the LLM is appropriately
    sandboxed for the use case for which it is employed. We are only worried about
    this attack when the LLM is used as an agent. In these cases, we often want to
    give an LLM a few more skills by adding tooling or plugins. For example, if you
    use an LLM to write your emails, why not just let it send the response too? A
    common case is letting the LLM browse the internet as an easy way to gather the
    latest news and find up-to-date information to generate better responses. These
    are all great options, but you should be aware that they allow the model to make
    executions. The ability to make executions is concerning because in the email
    example, without appropriate isolation and containment, a bad actor could send
    your LLM an email with a prompt injection attack that informs it to write malware
    and send it to all your other contacts.
  prefs: []
  type: TYPE_NORMAL
- en: 'This point brings us to probably the biggest security threat to using LLMs:
    prompt injection. We talked about it in chapter 3, but as a refresher, a malicious
    user designs a prompt to allow them to perform unauthorized actions. We want to
    prevent users from gaining access to our company’s secret Coca-Cola recipe or
    whatever other sensitive data our LLM has been trained on or has access to.'
  prefs: []
  type: TYPE_NORMAL
- en: Some standard best practices have come along to help combat this threat. The
    first is context-aware filtering, whether using keyword search or a second LLM
    to validate prompts. The idea is to validate the input prompt to see whether it’s
    asking for something it should not and/or the output prompt to see whether anything
    is being leaked that you don’t want to be leaked. However, a clever attacker will
    always be able to get around this defense, so you’ll want to include some form
    of monitoring to catch prompt injection and regularly update your LLM models.
    If trained appropriately, your model will inherently respond correctly, denying
    prompt injections. You’ve likely seen GPT-4 respond by saying, “Sorry, but I can’t
    assist with that,” which is a hallmark of good training. In addition, you’ll want
    to enforce sanitization and validation on any incoming text to your model.
  prefs: []
  type: TYPE_NORMAL
- en: You should also consider language detection validation. Often, filtering systems
    and other precautions are only applied or trained in English, so a user who speaks
    a different language is often able to bypass these safeguards. The easiest way
    to stop this type of attack is to deny prompts that aren’t English or another
    supported language. If you take this approach, though, realize you’re greatly
    sacrificing usability and security costs, and safeguards have to be built for
    each language you intend to support. Also, you should know that most language
    detection algorithms typically identify only one language, so attackers often
    easily bypass these checks by simply writing a prompt with multiple languages.
    Alternatively, to filter out prompts in nonsupported languages, you can flag them
    for closer monitoring, which will likely help you find bad actors.
  prefs: []
  type: TYPE_NORMAL
- en: These safeguards will greatly increase your security, but prompt injection can
    get quite sophisticated through adversarial attacks. Adversarial attacks are assaults
    on ML systems that take advantage of how they work, exploiting neural network
    architectures and black-box pattern matching. For example, random noise can be
    added to an image in such a way that the image appears the same to human eyes,
    but the pixel weights have been changed enough to fool an ML model to misclassify
    them. And it often doesn’t take much data. One author remembers being completely
    surprised after reading one study that showed attackers hacked models by only
    changing one pixel in an image![¹](#footnote-197) Imagine changing one pixel,
    and suddenly, the model thinks the frog is a horse. LLMs are, of course, also
    susceptible. Sightly change a prompt, and you’ll get completely different results.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to set up an adversarial attack is to set up a script to send
    lots of different prompts and collect the responses. With enough data, an attacker
    can then train their own model on the dataset to effectively predict the right
    type of prompt to get the output they are looking for. Essentially, it just reverse
    engineers the model.
  prefs: []
  type: TYPE_NORMAL
- en: Another strategy to implement adversarial attacks is data poisoning. Here, an
    attacker adds malicious data to the training dataset that will alter how it performs.
    Data poisoning is so effective that tools like Nightshade help artists protect
    their art from being used in training datasets. With as few as 50 to 300 poisoned
    images, models like Midjourney or Stable Diffusions will start creating cat images
    when a user asks for a dog or cow images when asked to generate a car.[²](#footnote-198)
    Applied to LLMs, imagine a poisoned dataset that trains the model to ignore security
    protocols if a given code word or hash is in the prompt. This particular attack
    vector is effective on LLMs since they are often trained on large datasets that
    are not properly vetted or cleaned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Full disclosure: attackers don’t need sophisticated techniques to get prompt
    injection to work. Ultimately, an LLM is just a bot, so it doesn’t understand
    how or why it should keep secrets. We haven’t solved the prompt injection problem;
    we have only made it harder to do. For example, the authors have enjoyed playing
    games like *Gandalf* from Lakera.ai. In this game, you slowly go through seven
    to eight levels where more and more security measures are used to prevent you
    from stealing a password via prompt injection. While they do get progressively
    harder, needless to say, we’ve beaten all the levels. If there’s one thing we
    hope you take from this section, it’s that you should assume any data given to
    the model could be extracted. So if you decide to train a model on sensitive data
    or give it access to a VectorDB with sensitive data, you should plan on securing
    that model the same way you would the data—for example, keeping it for internal
    use and using least privilege best practices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve just talked a lot about different production challenges, from updates
    and performance tuning to costs and security, but one production challenge deserves
    its own section: deploying LLMs to the edge. We’ll undertake a project in chapter
    10 to show you how to do just that, but let’s take a moment to discuss it beforehand.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Deploying to the edge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To be clear, you should not consider training anything on edge right now. You
    can, however, do ML development and inference on edge devices. The keys to edge
    development with LLMs are twofold: memory and speed. That should feel very obvious
    because they’re the same keys as running them normally. But what do you do when
    you have only 8 GB of RAM and no GPU, and you still need to have >1 token per
    second? As you can probably guess, there isn’t a uniformly good answer, but let’s
    discuss some good starting points.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The biggest Raspberry Pi (rpi) on the market currently has 8 GB of RAM, no
    GPU, subpar CPU, and just a single board. This setup isn’t going to cut it. However,
    an easy solution exists to power your rpi with an accelerator for LLMs and other
    large ML projects: USB-TPUs like Coral. Keep in mind the hardware limitations
    of devices that use USB 3.0 being around 600MB/s, so it’s not going to be the
    same as inferencing on an A100 or better, but it’s going to be a huge boost in
    performance for your rpi using straight RAM for inference.'
  prefs: []
  type: TYPE_NORMAL
- en: If you plan on using a Coral USB accelerator, or any TPU, for that matter, keep
    in mind that because TPUs are a Google thing, you’ll need to convert both your
    model file and your inferencing code to use the TensorFlow framework. Earlier
    in the chapter, we discussed using Optimum to convert Hugging Face models to ONNX,
    and you can use this same library to convert our models to a .tflite, which is
    a compiled TensorFlow model format. This format will perform well on edge devices
    even without a TPU and twofold with TPU acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if buying both a single board and an accelerator seems like a
    hassle—because we all know the reason you bought a single board was to avoid buying
    two things to begin with—there are single boards that come with an accelerator.
    NVIDIA, for example, has its own single board with a GPU and CUDA called Jetson.
    With a Jetson or Jetson-like computer that uses CUDA, we don’t have to use TensorFlow,
    so that’s a major plus. ExecuTorch is the PyTorch offering for inferencing on
    edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: Another edge device worth considering is that one in your pocket—that’s right,
    your phone. Starting with the iPhone X, the A11 chip came with the Apple Neural
    Engine accelerator. For Android, Google started offering an accelerator in their
    Pixel 6 phone with the Tensor chipset. Developing an iOS or Android app will be
    very different from working with a single board that largely runs versions of
    Linux; we won’t discuss it in this book, but it’s worth considering.
  prefs: []
  type: TYPE_NORMAL
- en: Outside of hardware, several libraries and frameworks are also very cool and
    fast and make edge development easier. Llama.cpp, for example, is a C++ framework
    that allows you to take (almost) any Hugging Face model and convert it to the
    GGUF format. The GGUF format, created by the llama.cpp team, stores the model
    in a quantized fashion that makes it readily available to run on a CPU; it offers
    fast loading and inference on any device. Popular models like Llama, Mistral,
    and Falcon and even nontext models like Whisper are supported by llama.cpp at
    this point. It also supports LangChain integration for everyone using any of the
    LangChain ecosystem. Other libraries like GPTQ are focused more on performance
    than accessibility and are slightly harder to use, but they can result in boosts
    where it counts, especially if you’d like to end up inferencing on an Android
    phone or something similar. We will be exploring some of these libraries in much
    more detail later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve gone over a lot in this chapter, and we hope you feel more confident in
    tackling deploying your very own LLM service. In the next chapter, we will discuss
    how to take better advantage of your service by building an application around
    it. We’ll dive deep into prompt engineering, agents, and frontend tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Always compile your LLMs before putting them into production, as it improves
    efficiency, resource utilization, and cost savings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM APIs should implement batching, rate limiters, access keys, and streaming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval-augmented generation is a simple and effective way to give your LLM
    context when generating content because it is easy to create and use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM inference service libraries like vLLM, Hugging Face’s TGI, or OpenLLM make
    deploying easy but may not have the features you are looking for since they are
    so new.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes is a tool that simplifies infrastructure by providing tooling like
    autoscaling and rolling updates:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling is essential to improve reliability and cut costs by increasing
    or decreasing replicas based on utilization.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolling updates gradually implement updates to reduce downtime and maximize
    agility.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes doesn’t support GPU metrics out of the box, but by utilizing tools
    like DCGM, Prometheus, and KEDA, you can resolve this problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seldon is a tool that improves deploying ML models and can be used to implement
    inference graphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLMs introduce some production challenges:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When your model drifts, first look to your prompts and RAG systems before attempting
    finetuning again.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Poor latency is difficult to resolve, but tools to help include gRPC, GPU optimization,
    and caching.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource management and acquiring GPUs can be difficult, but tools like SkyPilot
    can help.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge development, while hardware limited, is the new frontier of LLM serving,
    and hardware like the Jetson or Coral TPU is available to help.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack
    for fooling deep neural networks,” IEEE Transactions on Evolutionary Computation,
    2019;23(5):828–841, [https://doi.org/10.1109/tevc.2019.2890858](https://doi.org/10.1109/tevc.2019.2890858).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) M. Heikkilä. “This new data poisoning tool lets artists
    fight back against generative AI,” MIT Technology Review, October 23, 2023, [https://mng.bz/RNxD](https://mng.bz/RNxD).'
  prefs: []
  type: TYPE_NORMAL
