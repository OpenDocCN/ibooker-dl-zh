- en: '6 Large language model services: A practical guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 大型语言模型服务：实用指南
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How to structure an LLM service and tools to deploy
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建LLM服务和部署工具
- en: How to create and prepare a Kubernetes cluster for LLM deployment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何创建和准备Kubernetes集群以部署LLM
- en: Common production challenges and some methods to handle them
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的生产挑战和一些处理方法
- en: Deploying models to the edge
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型部署到边缘
- en: The production of too many useful things results in too many useless people.—Karl
    Marx
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 生产出过多的有用事物会导致无用的人太多。——卡尔·马克思
- en: We did it. We arrived. This is the chapter we wanted to write when we first
    thought about writing this book. One author remembers the first model he ever
    deployed. Words can’t describe how much more satisfaction this gave him than the
    dozens of projects left to rot on his laptop. In his mind, it sits on a pedestal,
    not because it was good—in fact, it was quite terrible—but because it was useful
    and actually used by those who needed it the most. It affected the lives of those
    around him.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做到了。我们到达了。这是我们最初考虑写这本书时想要写的章节。一位作者记得他第一次部署的模型。言语无法描述这给他带来的满足感比那些在他笔记本电脑上腐烂的数十个项目要多多少。在他心中，它坐落在祭坛上，不是因为它是好的——事实上，它相当糟糕——而是因为它是有用的，并且被那些最需要它的人实际使用。它影响了周围人的生活。
- en: So what actually is production? “Production” refers to the phase where the model
    is integrated into a live or operational environment to perform its intended tasks
    or provide services to end users. It’s a crucial phase in making the model available
    for real-world applications and services. To that end, we will show you how to
    package up an LLM into a service or API so that it can take on-demand requests.
    We will then show you how to set up a cluster in the cloud where you can deploy
    this service. We’ll also share some challenges you may face in production and
    some tips for handling them. Lastly, we will talk about a different kind of production,
    deploying models on edge devices.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，实际上“生产”是什么意思呢？“生产”是指模型集成到实时或运营环境以执行其预期任务或向最终用户提供服务的阶段。这是使模型可用于现实世界应用和服务的关键阶段。为此，我们将向您展示如何将LLM打包成服务或API，以便它可以接受按需请求。然后我们将向您展示如何在云中设置一个集群，您可以在那里部署此服务。我们还将分享您在生产中可能遇到的挑战和一些处理它们的技巧。最后，我们将讨论另一种类型的生产，即在边缘设备上部署模型。
- en: 6.1 Creating an LLM service
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 创建LLM服务
- en: In the last chapter, we trained and finetuned several models, and we’re sure
    you can’t wait to deploy them. Before you deploy a model, though, it’s important
    to plan ahead and consider different architectures for your API. Planning ahead
    is especially vital when deploying an LLM API. It helps outline the functionality,
    identify potential integration challenges, and arrange for necessary resources.
    Good planning streamlines the development process by setting priorities, thereby
    boosting the team’s efficiency.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们训练和微调了几个模型，我们确信您迫不及待地想要部署它们。然而，在部署模型之前，提前规划和考虑您API的不同架构非常重要。在部署LLM API时，提前规划尤其至关重要。它有助于概述功能、识别潜在的集成挑战，并为必要的资源做出安排。良好的规划通过设定优先级简化了开发过程，从而提高了团队的效率。
- en: In this section, we are going to take a look at several critical topics you
    should take into consideration to get the most out of our application once deployed.
    Figure 6.1 demonstrates a simple LLM-based service architecture that allows users
    to interact with our LLM on demand. This is a typical use case when working with
    chatbots, for example. Setting up a service also allows us to serve batch and
    stream processes while abstracting away the complexity of embedding the LLM logic
    directly into these pipelines. Of course, running an ML model from a service will
    add a communication latency to your pipeline, but LLMs are generally considered
    slow, and this extra latency is often worth the tradeoff.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一些关键主题，这些主题对于您在部署我们的应用程序后充分利用它至关重要。图6.1展示了一个基于简单LLM的服务架构，允许用户按需与我们的LLM交互。例如，当与聊天机器人一起工作时，这是一个典型的用例。设置服务还允许我们在抽象出将LLM逻辑直接嵌入这些管道的复杂性时，同时处理批处理和流处理。当然，从服务中运行ML模型会给您的管道增加通信延迟，但LLMs通常被认为运行速度较慢，这种额外的延迟通常值得权衡。
- en: '![figure](../Images/6-1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-1.png)'
- en: Figure 6.1 A basic LLM service. A majority of the logic is handled by the API
    layer, which will ensure the correct preprocessing of incoming requests is done
    and serve the actual inference of the request.
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1 基本的LLM服务。大部分逻辑由API层处理，这将确保对传入请求的正确预处理并服务于请求的实际推理。
- en: While figure 6.1 appears neat and tidy, it is hiding several complex subjects
    you’ll want to work through, particularly in that API box. We’ll be talking through
    several key features you’ll want to include in your API, like batching, rate limiters,
    and streaming. You’ll also notice some preprocessing techniques like retrieval-augmented
    generation (RAG) hidden in this image, which we’ll discuss in depth in section
    6.1.7\. By the end of this section, you will know how to approach all of this,
    and you will have deployed an LLM service and understand what to do to improve
    it. But before we get to any of that, let’s first talk about the model itself
    and the best methods to prepare it for online inference.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图6.1看起来整洁有序，但它隐藏了几个你想要解决的复杂主题，尤其是在那个API框中。我们将讨论你想要包含在API中的几个关键特性，如批处理、速率限制器和流式传输。你也会注意到一些预处理技术，如检索增强生成（RAG），隐藏在这张图片中，我们将在第6.1.7节中深入讨论。到本节结束时，你将知道如何处理所有这些，你将已经部署了LLM服务并了解如何改进它。但在我们到达任何这些内容之前，让我们首先谈谈模型本身以及最佳方法来准备它进行在线推理。
- en: 6.1.1 Model compilation
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 模型编译
- en: The success of any model in production is dependent on the hardware it runs
    on. The microchip architecture and design of the controllers on the silicon will
    ultimately determine how quickly and efficiently inferences can run. Unfortunately,
    when programming in a high-level language like Python using frameworks like PyTorch
    or TensorFlow, the model won’t be optimized to take full advantage of the hardware.
    This is where compiling comes into play. Compiling is the process of taking code
    written in a high-level language and converting or lowering it to machine-level
    code that the computer can process quickly. Compiling your LLM can easily lead
    to major inference and cost improvements.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 任何模型在生产中的成功都取决于其运行的硬件。微芯片架构和硅片上控制器的设计最终将决定推理的快速性和效率。不幸的是，当使用Python等高级语言以及PyTorch或TensorFlow等框架进行编程时，模型不会优化以充分利用硬件。这就是编译发挥作用的地方。编译是将用高级语言编写的代码转换为计算机可以快速处理的机器级代码的过程。编译你的LLM可以轻松地带来推理和成本的重大改进。
- en: Various people have dedicated a lot of time to performing some of the repeatable
    efficiency steps for you beforehand. We covered Tim Dettmers’s contributions in
    the last chapter. Other contributors include Georgi Gerganov, who created and
    maintains llama.cpp for running LLMs using C++ for efficiency, and Tom Jobbins,
    who goes by TheBloke on Hugging Face Hub and quantizes models into the correct
    formats to be used in Gerganov’s framework and others, like oobabooga. Because
    of how fast this field moves, completing simple, repeatable tasks over a large
    distribution of resources is often just as helpful to others.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人已经投入了大量时间来为你事先执行一些可重复的效率步骤。我们在上一章中介绍了Tim Dettmers的贡献。其他贡献者包括Georgi Gerganov，他创建了并维护了lama.cpp，用于使用C++运行LLM以提高效率，以及Tom
    Jobbins，他在Hugging Face Hub上以TheBloke为名，将模型量化为正确的格式，以便在Gerganov的框架和其他框架（如oobabooga）中使用。由于这个领域发展迅速，在大量资源上完成简单的可重复任务对他人也很有帮助。
- en: 'In machine learning workflows, this process typically involves converting our
    model from its development framework (PyTorch, TensorFlow, or other) to an intermediate
    representation (IR), like TorchScript, MLIR, or ONNX. We can then use hardware-specific
    software to convert these IR models to compiled machine code for our hardware
    of choice—GPU, TPU (tensor-processing units), CPU, etc. Why not just convert directly
    from your framework of choice to machine code and skip the middleman? Great question.
    The reason is simple: there are dozens of frameworks and hundreds of hardware
    units, and writing code to cover each combination is out of the question. So instead,
    framework developers provide conversion tooling to an IR, and hardware vendors
    provide conversions from an IR to their specific hardware.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习工作流程中，这个过程通常涉及将我们的模型从其开发框架（PyTorch、TensorFlow或其他）转换为中间表示（IR），如TorchScript、MLIR或ONNX。然后我们可以使用特定于硬件的软件将这些IR模型转换为针对我们选择的硬件（GPU、TPU（张量处理单元）、CPU等）的编译机器代码。为什么不直接从您选择的框架转换为机器代码，而跳过中间环节呢？这是一个很好的问题。原因很简单：有数十个框架和数百个硬件单元，编写代码来覆盖每一种组合是不可能的。因此，框架开发者提供了将代码转换为IR的工具，而硬件供应商提供了从IR到其特定硬件的转换。
- en: For the most part, the actual process of compiling a model involves running
    a few commands. Thanks to PyTorch 2.x, you can get a head start on it by using
    the `torch.compile(model)` command, which you should do before training and before
    deployment. Hardware companies often provide compiling software for free, as it’s
    a big incentive for users to purchase their product. Building this software isn’t
    easy, however, and often requires expertise in both the hardware architecture
    and the machine learning architectures. This combination of these talents is rare,
    and there’s good money to be had if you get a job in this field.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数情况，编译模型的实际过程涉及运行几个命令。多亏了PyTorch 2.x，您可以通过使用`torch.compile(model)`命令来提前开始，您应该在训练和部署之前执行此操作。硬件公司通常会免费提供编译软件，因为这可以激励用户购买他们的产品。然而，构建这种软件并不容易，通常需要硬件架构和机器学习架构的专家知识。这种才能的结合是罕见的，如果您在这个领域找到工作，可以赚得丰厚的收入。
- en: We will show you how to compile an LLM in a minute, but first, let’s take a
    look at some of the techniques used. What better place to start than with the
    all-important kernel tuning?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一分钟内向您展示如何编译一个LLM，但首先，让我们看看一些使用的技术。从至关重要的内核调整开始不是更好吗？
- en: Kernel tuning
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内核调整
- en: In deep learning and high-performance computing, a kernel is a small program
    or function designed to run on a GPU or other similar processors. These routines
    are developed by the hardware vendor to maximize chip efficiency. They do this
    by optimizing threads, registries, and shared memory across blocks of circuits
    on the silicon. When we run arbitrary code, the processor will try to route the
    requests the best it can across its logic gates, but it’s bound to run into bottlenecks.
    However, if we are able to identify the kernels to run and their order beforehand,
    the GPU can map out a more efficient route—and that’s essentially what kernel
    tuning is.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习和高性能计算中，内核是一个小型程序或函数，设计用于在GPU或其他类似处理器上运行。这些例程由硬件供应商开发，以最大化芯片效率。他们通过优化硅片上电路块的线程、寄存器和共享内存来实现这一点。当我们运行任意代码时，处理器将尽力在其逻辑门之间路由请求，但难免会遇到瓶颈。然而，如果我们能够事先识别出要运行的内核及其顺序，GPU可以规划出一条更高效的路径——这正是内核调整的本质。
- en: During kernel tuning, the most suitable kernels are chosen from a large collection
    of highly optimized kernels. For instance, consider convolution operations that
    have several possible algorithms. The optimal one from the vendor’s library of
    kernels will be based on various factors like the target GPU type, input data
    size, filter size, tensor layout, batch size, and more. When tuning, several of
    these kernels will be run and optimized to minimize execution time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核调整过程中，从大量高度优化的内核中选出最合适的内核。例如，考虑具有几种可能算法的卷积操作。从供应商的内核库中选择的最佳内核将基于各种因素，如目标GPU类型、输入数据大小、过滤器大小、张量布局、批量大小等等。在调整时，将运行并优化这些内核中的几个，以最小化执行时间。
- en: This process of kernel tuning ensures that the final deployed model is not only
    optimized for the specific neural network architecture being used but also finely
    tuned for the unique characteristics of the deployment platform. This process
    results in more efficient use of resources and maximizes performance. Next, let’s
    look at tensor fusion, which optimizes running these kernels.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内核调优的过程确保最终部署的模型不仅针对所使用的特定神经网络架构进行了优化，而且针对部署平台的独特特性进行了精细调优。这个过程导致资源使用更加高效，并最大化性能。接下来，让我们看看张量融合，它优化了运行这些内核的过程。
- en: Tensor fusion
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量融合
- en: In deep learning, when a framework executes a computation graph, it makes multiple
    function calls for each layer. The computation graph is a powerful concept used
    to simplify mathematical expressions and execute a sequence of tensor operations,
    especially for neural network models. If each operation is performed on the GPU,
    it invokes many CUDA kernel launches. However, the fast kernel computation doesn’t
    quite match the slowness of launching the kernel and handling tensor data. As
    a result, the GPU resources might not be fully utilized, and memory bandwidth
    can become a choke point. It’s like making multiple trips to the store to buy
    separate items when we could make a single trip and buy all the items at once.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，当框架执行计算图时，它为每个层调用多个函数。计算图是一个强大的概念，用于简化数学表达式并执行一系列张量操作，特别是对于神经网络模型。如果每个操作都在GPU上执行，它将引发许多CUDA内核启动。然而，快速的内核计算并不完全匹配内核启动和处理张量数据的缓慢速度。因此，GPU资源可能没有得到充分利用，内存带宽可能成为瓶颈。这就像当我们可以一次性购买所有物品时，我们却要多次去商店购买单独的物品一样。
- en: This is where tensor fusion comes in. It improves this situation by merging
    or fusing kernels to perform operations as one, reducing unnecessary kernel launches
    and improving memory efficiency. A common example of a composite kernel is a fully
    connected kernel that combines or fuses a matmul, bias add, and ReLU kernel. It’s
    similar to the concept of tensor parallelization. In tensor parallelization, we
    speed up the process by sending different people to different stores, like the
    grocery store, the hardware store, and a retail store. This way, one person doesn’t
    have to go to every store. Tensor fusion can work very well with parallelization
    across multiple GPUs. It’s like sending multiple people to different stores and
    making each one more efficient by picking up multiple items instead of one.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是张量融合发挥作用的地方。通过合并或融合内核以单个操作执行操作，它改善了这种情况，减少了不必要的内核启动并提高了内存效率。一个常见的复合内核示例是全连接内核，它结合或融合了矩阵乘法、偏置添加和ReLU内核。这与张量并行化的概念类似。在张量并行化中，我们通过将不同的人派往不同的存储库（如杂货店、五金店和零售店）来加速过程。这样，一个人不必去每个商店。张量融合可以与多个GPU的并行化很好地工作。这就像派多个人去不同的商店，通过一次拿多个物品而不是一个来提高每个人的效率。
- en: Graph optimization
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图优化
- en: Tensor fusion, when done sequentially, is also known as vertical graph optimization.
    We can also do horizontal graph optimization. These optimizations are often talked
    about as two different things. Horizontal graph optimization, which we’ll refer
    to simply as graph optimization, combines layers with shared input data but with
    different weights into a single broader kernel. It replaces the concatenation
    layers by pre-allocating output buffers and writing into them in a distributed
    manner.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 张量融合，当按顺序执行时，也被称为垂直图优化。我们还可以进行水平图优化。这些优化通常被谈论为两件不同的事情。我们将简单地称之为图优化，水平图优化将具有不同权重但共享输入数据的层合并成一个更广泛的内核。它通过预分配输出缓冲区并以分布式方式写入来替换连接层。
- en: In figure 6.2, we show an example of a simple deep learning graph being optimized.
    Graph optimizations do not change the underlying computation in the graph. They
    are simply restructuring the graph. As a result, the optimized graph performs
    more efficiently with fewer layers and kernel launches, reducing inference latency.
    This restructuring makes the whole process smaller, faster, and more efficient.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.2中，我们展示了一个简单的深度学习图优化示例。图优化不会改变图中的底层计算。它们只是重新构建图。因此，优化后的图以更少的层和内核启动运行得更加高效，减少了推理延迟。这种重构使整个过程更小、更快、更高效。
- en: '![figure](../Images/6-2.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-2.png)'
- en: 'Figure 6.2 An example of an unoptimized network compared to the same network
    optimized using graph optimization. CBR is a NVIDIA fused layer kernel that simply
    stands for Convolution, Bias, and ReLU. See the following NVIDIA blog post for
    reference: [https://mng.bz/PNvw](https://mng.bz/PNvw).'
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.2 是一个未优化的网络与使用图优化优化后的相同网络的示例。CBR 是一个 NVIDIA 熔合层内核，它简单地代表卷积、偏置和 ReLU。请参阅以下
    NVIDIA 博客文章以获取参考：[https://mng.bz/PNvw](https://mng.bz/PNvw)。
- en: The graph optimization technique is often used in the context of computational
    graph-based frameworks like TensorFlow. Graph optimization involves techniques
    that simplify these computational graphs, remove redundant operations, and/or
    rearrange computations, making them more efficient for execution, especially on
    specific hardware (like GPU or TPU). An example is constant folding, where the
    computations involving constant inputs are performed at compile time (before run
    time), thereby reducing the computation load during run time.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图优化技术常用于基于计算图的框架，如 TensorFlow 的上下文中。图优化涉及简化这些计算图、移除冗余操作和/或重新排列计算的技术，使它们在执行时更加高效，尤其是在特定的硬件（如
    GPU 或 TPU）上。一个例子是常量折叠，其中涉及常量输入的计算在编译时（在运行时之前）执行，从而减少了运行时的计算负载。
- en: These aren’t all the techniques used when compiling a model, but they are some
    of the most common and should give you an idea of what’s happening under the hood
    and why it works. Now let’s look at some tooling to do this for LLMs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些并不是编译模型时使用的所有技术，但它们是最常见的，应该能让你了解底层发生了什么以及为什么它有效。现在让我们看看一些用于 LLM 的工具。
- en: TensorRT
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TensorRT
- en: NVIDIA’s TensorRT is a one-stop shop to compile your model, and who better to
    trust than the hardware manufacturer to better prepare your model to run on their
    GPUs? TensorRT does everything talked about in this section, along with quantization
    to INT8 and several memory tricks to get the most out of your hardware to boot.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 的 TensorRT 是一个一站式商店来编译你的模型，而且谁比硬件制造商更值得信赖来更好地准备你的模型在他们的 GPU 上运行？TensorRT
    做了本节中提到的所有事情，包括量化到 INT8 和几个内存技巧，以充分利用你的硬件。
- en: 'In listing 6.1, we demonstrate the simple process of compiling an LLM using
    TensorRT. We’ll use the PyTorch version known as `torch_tensorrt`. It’s important
    to note that compiling a model to a specific engine is hardware specific. So you
    will want to compile the model on the exact hardware you intend to run it on.
    Consequently, installing TensorRT is a bit more than a simple `pip` `install`;
    thankfully, we can use Docker instead. To get started, run the following command:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 6.1 中，我们展示了使用 TensorRT 编译 LLM 的简单过程。我们将使用名为 `torch_tensorrt` 的 PyTorch 版本。需要注意的是，将模型编译到特定引擎是硬件特定的。因此，你将想要在打算运行的硬件上编译模型。因此，安装
    TensorRT 不仅仅是简单的 `pip` `install`；幸运的是，我们可以使用 Docker。要开始，请运行以下命令：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This command will start up an interactive `torch_tensorrt` Docker container
    with practically everything we need to get started (for the latest version, see
     [https://mng.bz/r1We](https://mng.bz/r1We)). The only thing missing is Hugging
    Face Transformers, so go ahead and install that. Now we can run the listing.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将启动一个交互式的 `torch_tensorrt` Docker 容器，其中包含我们开始所需的所有内容（关于最新版本，请参阅 [https://mng.bz/r1We](https://mng.bz/r1We)）。唯一缺少的是
    Hugging Face Transformers，所以请继续安装它。现在我们可以运行列表了。
- en: After our imports, we’ll load our model and generate an example input so we
    can trace the model. We need to convert our model to an IR—TorchScript here—and
    this is done through tracing. Tracing is the process of capturing the operations
    that are invoked when running the model and makes graph optimization easier later.
    If you have a model that takes varying inputs, for example, the CLIP model, which
    can take both images and text and turn them into embeddings, tracing that model
    with only text data is an effective way of pruning the image operations out of
    the model. Once our model has been converted to an IR, then we can compile it
    for NVIDIA GPUs using TensorRT. Once completed, we then simply reload the model
    from disk and run some inference for demonstration.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入之后，我们将加载我们的模型并生成一个示例输入，以便我们可以跟踪模型。我们需要将我们的模型转换为 IR（TorchScript），这是通过跟踪完成的。跟踪是捕捉运行模型时调用的操作的过程，使得后续的图优化更加容易。例如，如果你有一个接受可变输入的模型，比如
    CLIP 模型，它可以接受图像和文本并将它们转换为嵌入，使用仅文本数据跟踪该模型是一种有效地从模型中剪除图像操作的有效方法。一旦我们的模型被转换为 IR，我们就可以使用
    TensorRT 为 NVIDIA GPU 编译它。一旦完成，我们只需从磁盘重新加载模型并运行一些推理以进行演示。
- en: Listing 6.1 Compiling a model with TensorRT
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.1 使用 TensorRT 编译模型
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Converts to Torchscript IR'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 转换为 Torchscript IR'
- en: '#2 Compiles the model with TensorRT'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用 TensorRT 编译模型'
- en: '#3 Runs with FP16'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用 FP16 运行'
- en: '#4 Saves the compiled model'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 保存编译后的模型'
- en: '#5 Runs inference'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 运行推理'
- en: The output is
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We’ll just go ahead and warn you: your results may vary when you run this code,
    depending on your setup. Overall, it’s a simple process once you know what you
    are doing, and we’ve regularly seen at least 2× speed improvements in inference
    times—which translates to major savings!'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们就直接警告你：当你运行这段代码时，根据你的设置，结果可能会有所不同。总的来说，一旦你知道你在做什么，这是一个简单的过程，我们经常看到至少 2 倍的推理时间提升——这相当于巨大的节省！
- en: 'TensorRT really is all that and a bag of chips. Of course, the major downside
    to TensorRT is that, as a tool developed by NVIDIA, it is built with NVIDIA’s
    hardware in mind. When compiling code for other hardware and accelerators, it’s
    not going to be useful. Also, you’ll get very used to running into error messages
    when working with TensorRT. We’ve found that running into compatibility problems
    when converting models that aren’t supported is a common occurrence. We’ve run
    into many problems trying to compile various LLM architectures. Thankfully, to
    address this, NVIDIA has been working on a TensorRT-LLM library to supercharge
    LLM inference on NVIDIA high-end GPUs. It supports many more LLM architectures
    than vanilla TensorRT. You can check if it supports your chosen LLM architecture
    and GPU setup here: [https://mng.bz/mRXP](https://mng.bz/mRXP).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: TensorRT 确实是应有尽有，还附带一袋芯片。当然，TensorRT 的主要缺点是，作为一个由 NVIDIA 开发的工具，它是针对 NVIDIA 的硬件构建的。当为其他硬件和加速器编译代码时，它将不会很有用。此外，你会在使用
    TensorRT 时非常习惯于遇到错误信息。我们发现，在转换不受支持的模型时遇到兼容性问题是一个常见现象。我们在尝试编译各种 LLM 架构时遇到了许多问题。幸运的是，为了解决这个问题，NVIDIA
    一直在开发一个 TensorRT-LLM 库，以加速在 NVIDIA 高端 GPU 上的 LLM 推理。它支持的 LLM 架构比普通的 TensorRT 多得多。你可以在这里检查它是否支持你选择的
    LLM 架构和 GPU 设置：[https://mng.bz/mRXP](https://mng.bz/mRXP)。
- en: Don’t get us wrong; you don’t have to use TensorRT. Several alternative compilers
    are available. In fact, let’s look at another popular alternative, ONNX Runtime.
    Trust us, you’ll want an alternative when TensorRT doesn’t play nice.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要误解我们；你不必使用 TensorRT。有几种替代的编译器可供选择。实际上，让我们看看另一个流行的替代品，ONNX Runtime。相信我们，当
    TensorRT 不配合时，你会想要一个替代品。
- en: ONNX Runtime
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ONNX Runtime
- en: ONNX, which stands for Open Neural Network Exchange, is an open source format
    and ecosystem designed for representing and interoperating between different deep
    learning frameworks, libraries, and tools. It was created to address the challenge
    of model portability and compatibility. As mentioned previously, ONNX is an IR
    and allows you to represent models trained in one deep learning framework (e.g.,
    TensorFlow, PyTorch, Keras, MXNet) in a standardized format easily consumed by
    other frameworks. Thus, it facilitates the exchange of models between different
    tools and environments. Unlike TensorRT, ONNX Runtime is intended to be hardware-agnostic,
    meaning it can be used with a variety of hardware accelerators, including CPUs,
    GPUs, and specialized hardware like TPUs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX，即开放神经网络交换，是一个开源格式和生态系统，旨在表示不同深度学习框架、库和工具之间的互操作性。它是为了解决模型可移植性和兼容性挑战而创建的。如前所述，ONNX
    是一个 IR，允许你以标准化的格式表示在一个深度学习框架（例如，TensorFlow、PyTorch、Keras、MXNet）中训练的模型，以便其他框架轻松消费。因此，它促进了不同工具和环境之间模型的交换。与
    TensorRT 不同，ONNX Runtime 的目的是硬件无关，这意味着它可以与各种硬件加速器一起使用，包括 CPU、GPU 和专门的硬件如 TPUs。
- en: In practical terms, ONNX allows machine learning practitioners and researchers
    to build and train models using their preferred framework and then deploy those
    models to different platforms and hardware without the need for extensive reengineering
    or rewriting of code. This process helps streamline the development and deployment
    of AI and ML models across various applications and industries. To be clear, ONNX
    is an IR format, while ONNX Runtime allows us to optimize and run inference with
    ONNX models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，ONNX 允许机器学习实践者和研究人员使用他们首选的框架构建和训练模型，然后将这些模型部署到不同的平台和硬件上，而无需进行大量的重新工程或代码重写。这个过程有助于简化
    AI 和 ML 模型在各种应用和行业中的开发和部署。明确来说，ONNX 是一个 IR 格式，而 ONNX Runtime 允许我们优化和运行 ONNX 模型的推理。
- en: 'To take advantage of ONNX, we recommend using Hugging Face’s Optimum. Optimum
    is an interface that makes working with optimizers easier and supports multiple
    engines and hardware, including Intel Neural Compressor for Intel chips and Furiosa
    Warboy for Furiosa NPUs. It’s worth checking out. For our purposes, we will use
    it to convert LLMs to ONNX and then optimize them for inference with ONNX Runtime.
    First, let’s install the library with the appropriate engines. We’ll use the `--upgrade-strategy`
    `eager`, as suggested by the documentation, to ensure the different packages are
    upgraded:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要利用ONNX，我们建议使用Hugging Face的Optimum。Optimum是一个使与优化器一起工作变得更容易的接口，支持多个引擎和硬件，包括Intel
    Neural Compressor用于Intel芯片和Furiosa Warboy用于Furiosa NPUs。值得一看。为了我们的目的，我们将使用它将LLM转换为ONNX，然后使用ONNX
    Runtime对其进行推理优化。首先，让我们使用适当的引擎安装库。我们将使用文档中建议的`--upgrade-strategy` `eager`，以确保不同的包得到升级：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we’ll run the optimum command line interface. We’ll export it to ONNX,
    point it to a Hugging Face transformer model, and give it a local directory to
    save the model to. Those are all the required steps, but we’ll also give it an
    optimization feature flag. Here, we’ll do the basic general optimizations:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将运行最优的命令行界面。我们将将其导出为ONNX格式，指向Hugging Face的transformer模型，并给它一个本地目录来保存模型。这些都是所需的步骤，但我们还会给它一个优化功能标志。在这里，我们将进行基本的通用优化：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And we are done. We now have an LLM model converted to ONNX format and optimized
    with basic graph optimizations. As with all compiling processes, optimization
    should be done on the hardware you intend to run inference on, which should include
    ample memory and resources, as the conversion can be somewhat computationally
    intensive.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了。现在我们有一个LLM模型被转换为ONNX格式，并经过基本的图优化。与所有编译过程一样，优化应该在打算运行推理的硬件上进行，这应该包括充足的内存和资源，因为转换可能需要相当的计算量。
- en: To run the model, check out [https://onnxruntime.ai/](https://onnxruntime.ai/)
    for quick start guides on how to run it with your appropriate SDK. Oh, yeah, did
    we forget to mention that ONNX Runtime supports multiple programming APIs, so
    you can now run your LLM directly in your favorite language, including Java, C++,
    C#, or even JavaScript? Well, you can. So go party. We’ll be sticking to Python
    in this book, though, for consistency’s sake.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行模型，请查看[https://onnxruntime.ai/](https://onnxruntime.ai/)获取如何使用适当的SDK运行它的快速入门指南。哦，对了，我们忘记提到ONNX
    Runtime支持多个编程API了吗？所以你现在可以直接在你的首选语言中运行你的LLM，包括Java、C++、C#甚至是JavaScript？是的，你可以。不过，在这本书中，我们将坚持使用Python，以保持一致性。
- en: While TensorRT is likely to be your weapon of choice most of the time, and ONNX
    Runtime covers many edge cases, there are still many other excellent engines out
    there, like OpenVINO. You can choose whatever you want, but you should at least
    use something. Doing otherwise would be an egregious mistake. In fact, now that
    you’ve read this section, you can no longer claim ignorance. It is now your professional
    responsibility to ensure this happens. Putting any ML model into production that
    hasn’t first been compiled (or at least attempted to be compiled) is a sin to
    the MLOps profession.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然TensorRT可能是你大多数时候的首选武器，而ONNX Runtime覆盖了许多边缘情况，但仍然有许多其他优秀的引擎，比如OpenVINO。你可以选择你想要的任何东西，但至少你应该使用一些。否则，那将是一个严重的错误。实际上，现在你已经阅读了这一节，你不能再声称自己无知。现在，确保这一点发生是你的专业责任。将任何未首先编译（或至少尝试编译）的ML模型投入生产，对于MLOps职业来说是一种罪过。
- en: 6.1.2 LLM storage strategies
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 LLM存储策略
- en: Now that we have a nicely compiled model, we need to think about how our service
    will access it. This step is important because, as discussed in chapter 3, boot
    times can be a nightmare when working with LLMs since it can take a long time
    to load such large assets into memory. So we want to try to speed that up as much
    as possible. When it comes to managing large assets, we tend to throw them into
    an artifact registry or a bucket in cloud storage and forget about them. Both
    of these tend to utilize an object storage system—like GCS or S3—under the hood,
    which is great for storage but less so for object retrieval, especially when it
    comes to large objects like LLMs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个编译得很好的模型，我们需要考虑我们的服务如何访问它。这一步很重要，因为，如第3章所述，当与LLM一起工作时，启动时间可能是一个噩梦，因为将如此大的资产加载到内存中可能需要很长时间。因此，我们希望尽可能加快这一过程。当涉及到管理大型资产时，我们倾向于将它们扔进一个工件注册表或云存储中的桶里，然后忘记它们。这两个都倾向于使用底层的对象存储系统——如GCS或S3——这对于存储来说很棒，但在对象检索方面就不那么好了，尤其是对于LLM这样的大型对象。
- en: Object storage systems break up assets into small fractional bits called objects.
    They allow us to federate the entire asset across multiple machines and physical
    memory locations, a powerful tool that powers the cloud, and to cheaply store
    large objects on commodity hardware. With replication, there is built-in fault
    tolerance, so we never have to worry about losing our assets from a hardware crash.
    Object storage systems also create high availability, ensuring we can always access
    our assets. The downside is that these objects are federated across multiple machines
    and not in an easily accessible form to be read and stored in memory. Consequently,
    when we load an LLM into GPU memory, we will essentially have to download the
    model first. Let’s look at some alternatives.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储系统将资产分解成称为对象的小分数位。它们允许我们将整个资产跨多台机器和物理内存位置进行联邦化，这是一个强大的工具，为云服务提供动力，并且可以在通用硬件上以低成本存储大型对象。通过复制，有内置的错误容错功能，所以我们永远不必担心由于硬件故障而丢失资产。对象存储系统还创建高可用性，确保我们始终可以访问我们的资产。缺点是这些对象被联邦化到多台机器上，而不是以易于访问的形式读取和存储在内存中。因此，当我们将LLM加载到GPU内存时，我们实际上首先必须下载模型。让我们看看一些替代方案。
- en: Fusing
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 融合
- en: Fusing is the process of mounting a bucket to your machine as if it were an
    external hard drive. Fusing provides a slick interface and simplifies code, as
    you will no longer have to download the model and then load it into memory. With
    fusing, you can treat an external bucket like a filesystem and load the model
    directly into memory. However, it still doesn’t solve the fundamental need to
    pull the objects of your asset from multiple machines. Of course, if you fuse
    a bucket to a node in the same region and zone, some optimizations can improve
    performance, and it will feel like you are loading the model from the drive. Unfortunately,
    our experience has shown fusing to be quite slow, but it should still be faster
    than downloading and then loading.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 融合是将存储桶挂载到您的机器上的过程，就像它是一个外部硬盘驱动器一样。融合提供了一个流畅的界面，简化了代码，因为您将不再需要下载模型并将其加载到内存中。通过融合，您可以像文件系统一样处理外部存储桶，并将模型直接加载到内存中。然而，它仍然无法解决从多台机器中提取资产对象的基本需求。当然，如果您将存储桶融合到同一区域和区域的节点上，一些优化可以提高性能，并且感觉就像是从驱动器中加载模型一样。不幸的是，我们的经验表明融合相当慢，但它应该仍然比下载然后加载要快。
- en: Fusing libraries are available for all major cloud providers and on-prem object
    storage solutions, like Ceph or MinIO, so you should be covered no matter the
    environment, including your own laptop. That’s right. You can fuse your laptop
    or an edge device to your object storage solution. This ability demonstrates both
    how powerful and, at the same time, ineffective this strategy is, depending on
    what you were hoping it would achieve.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 融合库适用于所有主要的云服务提供商和本地对象存储解决方案，如Ceph或MinIO，因此无论在何种环境下，包括您自己的笔记本电脑，都应该得到覆盖。没错。您可以将笔记本电脑或边缘设备与对象存储解决方案融合。这种能力展示了该策略的强大之处，同时，根据您希望它实现的目标，它也可能是不够有效的。
- en: 'TIP  All fusing libraries are essentially built off the FUSE library. It’s
    worth checking out: [https://github.com/libfuse/libfuse](https://github.com/libfuse/libfuse).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：所有融合库实际上都是基于FUSE库构建的。值得一看：[https://github.com/libfuse/libfuse](https://github.com/libfuse/libfuse)。
- en: Baking the model
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 烘焙模型
- en: Baking is the process of putting your model into the Docker image. Thus, whenever
    a new container is created, the model will be there, ready for use. Baking models,
    in general, is considered an antipattern. For starters, it doesn’t solve the problem.
    In production, when a new instance is created, a new machine is spun up. It is
    fresh and innocent, knowing nothing of the outside world, so the first step it’ll
    have to take is to download the image. Since the image contains the model, we
    haven’t solved anything. Actually, it’s very likely that downloading the model
    inside an image will be *slower* than downloading the model from an object store.
    So we most likely just made our boot times worse.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 烘焙是将模型放入Docker镜像的过程。因此，每当创建一个新的容器时，模型就会在那里，准备好使用。通常，烘焙模型被认为是一种反模式。首先，它没有解决问题。在生产中，当创建一个新的实例时，会启动一台新机器。它是新鲜无辜的，对外部世界一无所知，所以它必须采取的第一步是下载镜像。由于镜像包含模型，所以我们并没有解决问题。实际上，在镜像内下载模型很可能比从对象存储中下载模型要慢。所以，我们很可能只是使启动时间变得更长。
- en: 'Second, baking models is a terrible security practice. Containers often have
    poor security and are often easy for people to gain access to. Third, you’ve doubled
    your problems: before you just had one large asset; now you have two, the model
    and the image.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，烘焙模型是一种糟糕的安全实践。容器通常安全性较差，并且人们很容易获得访问权限。第三，你的问题翻倍了：在你之前只有一个大型资产；现在你有两个，模型和镜像。
- en: 'That said, there are still times when baking is viable, mainly because despite
    the drawbacks, it greatly simplifies our deployments. Throwing all our assets
    into the image guarantees we’ll only need one thing to deploy a new service: the
    image itself, which is really valuable when deploying to an edge device, for example.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，仍然有一些时候烘焙是可行的，主要是因为尽管有缺点，但它极大地简化了我们的部署。将所有资产放入镜像中可以保证我们只需要一件东西来部署新的服务：镜像本身，这在部署到边缘设备时非常有价值。
- en: Mounted volume
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 挂载卷
- en: Another solution is to avoid the object store completely and save your LLM in
    a file-based storage system on a mountable drive. When our service boots up, we
    can connect the disc drive housing the LLM with a RAID controller or Kubernetes,
    depending on our infrastructure. This solution is old school, but it works really
    well. For the most part, it solves all our problems and provides incredibly fast
    boot times.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决方案是完全避免对象存储，并将你的LLM保存在可挂载驱动器上的基于文件的存储系统中。当我们的服务启动时，我们可以使用RAID控制器或Kubernetes（取决于我们的基础设施）连接包含LLM的磁盘驱动器。这个解决方案是老式的，但它确实非常有效。在大多数情况下，它解决了我们所有的问题，并提供了极快的启动时间。
- en: The downside, of course, is that it will add a bunch of coordination steps to
    ensure there is a volume in each region and zone you plan to deploy to. It also
    brings up replication and reliability problems; if the drive dies unexpectedly,
    you’ll need backups in the region. In addition, these drives will likely be SSDs
    and not just commodity hardware. So you’ll likely be paying a bit more. But storage
    is extremely cheap compared to GPUs, so the time saved in boot times is something
    you’ll have to consider. Essentially, though, this strategy reintroduces all the
    problems for which we usually turn to object stores to begin with.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，缺点是它将增加许多协调步骤以确保在每个你计划部署的区域和区域中都有一个卷。它还引发了复制和可靠性问题；如果驱动器意外损坏，你将需要在该区域进行备份。此外，这些驱动器很可能是SSD而不是普通硬件。所以你可能会多付一些钱。但是，与GPU相比，存储非常便宜，所以节省的启动时间是你要考虑的。本质上，这种策略重新引入了我们最初转向对象存储的所有问题。
- en: 'Hybrid: Intermediary mounted volume'
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混合：启动时挂载的中间件卷
- en: Lastly, we can always take a hybrid approach. In this solution, we download
    the model at boot time but store it in a volume that is mounted at boot time.
    While this doesn’t help at all with the first deployment in a region, it does
    substantially help any new instances, as they can simply mount this same volume
    and have the model available to load without having to download. You can imagine
    this working similarly to how a Redis cache works, except for storage. Often,
    this technique is more than enough since autoscaling will be fast enough to handle
    bursty workloads. We just have to worry about total system crashes, which hopefully
    should be minimal, but they allude to the fact that we should avoid this approach
    when only running one replica, which you shouldn’t do in production anyway.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们总是可以采取混合方法。在这个解决方案中，我们在启动时下载模型，但将其存储在启动时挂载的卷中。虽然这并不能帮助解决该地区第一次部署的问题，但它确实大大帮助了任何新的实例，因为它们可以简单地挂载这个相同的卷，并能够加载模型而无需下载。你可以想象这和Redis缓存的工作方式类似，只是不涉及存储。通常，这种技术已经足够，因为自动扩展将足够快以处理突发的工作负载。我们只需要担心整个系统的崩溃，希望这种情况会是最小的，但这也暗示我们应该避免在只运行一个副本时采用这种方法，因为在生产环境中你本来就不应该这样做。
- en: In figure 6.3, we demonstrate these different strategies and compare them to
    a basic service where we simply download the LLM and then load it into memory.
    Overall, your exact strategy will depend on your system requirements, the size
    of the LLM you are running, and your infrastructure. Your system requirements
    will also likely vary widely, depending on the type of traffic patterns you see.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.3中，我们展示了这些不同的策略，并将它们与一个基本服务进行了比较，在这个服务中，我们只是下载LLM并将其加载到内存中。总的来说，你的具体策略将取决于你的系统需求、你运行的LLM的大小以及你的基础设施。你的系统需求也可能因你看到的流量模式而大相径庭。
- en: '![figure](../Images/6-3.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图6-3](../Images/6-3.png)'
- en: Figure 6.3 Different strategies for storing LLMs and their implications at boot
    time. Often, we have to balance system reliability, complexity, and application
    load time.
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3 存储LLMs的不同策略及其在启动时的含义。通常，我们必须在系统可靠性、复杂性和应用程序加载时间之间取得平衡。
- en: Now that we have a good handle on how to handle our LLM as an asset, let’s talk
    about some API features that are must-haves for your LLM service.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经很好地掌握了如何将我们的LLM视为一项资产，那么让我们来谈谈一些对于您的LLM服务来说是必备的API功能。
- en: 6.1.3 Adaptive request batching
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 自适应请求分批
- en: A typical API will accept and process requests in the order they are received,
    processing them immediately and as quickly as possible. However, anyone who’s
    trained a machine learning model has come to realize that there are mathematical
    and computational advantages to running inference in batches of powers of 2 (16,
    32, 64, etc.), particularly when GPUs are involved, where we can take advantage
    of better memory alignment or vectorized instructions parallelizing computations
    across the GPU cores. To take advantage of this batching, you’ll want to include
    adaptive request batching or dynamic batching.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的API将按接收到的顺序接受和处理请求，立即尽可能快地处理它们。然而，任何训练过机器学习模型的人都会意识到，在2的幂次（16、32、64等）批次中运行推理具有数学和计算上的优势，尤其是在涉及GPU的情况下，我们可以利用更好的内存对齐或向量化指令并行化GPU核心的计算。为了利用这种分批，您可能希望包括自适应请求分批或动态分批。
- en: What adaptive batching does is essentially pool requests together over a certain
    period of time. Once the pool receives the configured maximum batch size or the
    timer runs out, it will run inference on the entire batch through the model, sending
    the results back to the individual clients that requested them. Essentially, it’s
    a queue. Setting one up yourself can and will be a huge pain; thankfully, most
    ML inference services offer this out of the box, and almost all are easy to implement.
    For example, in BentoML, add `@bentoml.Runnable.method(batchable=True)` as a decorator
    to your predict function, and in Triton Inference Server, add `dynamic_batching`
    `{}` at the end of your model definition file.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应分批所做的是在一段时间内将请求汇集在一起。一旦池收到配置的最大批次大小或计时器耗尽，它将通过模型对整个批次进行推理，并将结果发送回请求它们的单个客户端。本质上，它是一个队列。自己设置一个可能会非常痛苦；幸运的是，大多数ML推理服务都提供这项功能，而且几乎都很容易实现。例如，在BentoML中，将`@bentoml.Runnable.method(batchable=True)`作为装饰器添加到您的预测函数中，在Triton推理服务器中，在模型定义文件末尾添加`dynamic_batching`
    `{}`。
- en: If that sounds easy, it is. Typically, you don’t need to do any further finessing,
    as the defaults tend to be very practical. That said, if you are looking to maximize
    every bit of efficiency possible in the system, you can often set a maximum batch
    size, which will tell the batcher to run once this limit is reached, or a batch
    delay, which does the same thing but for the timer. Increasing either will result
    in longer latency but likely better throughput, so typically these are only adjusted
    when your system has plenty of latency budget.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果听起来很简单，那是因为它确实很简单。通常，您不需要进行任何进一步的调整，因为默认设置往往非常实用。话虽如此，如果您希望最大限度地提高系统中的每一分效率，您通常可以设置一个最大批次大小，这将告诉分批器一旦达到这个限制就运行一次，或者设置一个批次延迟，这会对计时器做同样的事情。增加任何一个都会导致更长的延迟，但可能带来更好的吞吐量，因此通常只有在您的系统有足够的延迟预算时才会调整这些设置。
- en: Overall, the benefits of adaptive batching include better use of resources and
    higher throughput at the cost of a bit of latency. This is a valuable tradeoff,
    and we recommend giving your product the latency bandwidth to include this feature.
    In our experience, optimizing for throughput leads to better reliability and scalability
    and thus greater customer satisfaction. Of course, when latency times are extremely
    important or traffic is few and far between, you may rightly forgo this feature.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，自适应分批的好处包括更有效地使用资源并提高吞吐量，但代价是略微增加延迟。这是一个有价值的权衡，我们建议为您的产品提供足够的延迟带宽以包含此功能。根据我们的经验，优化吞吐量会导致更好的可靠性和可扩展性，从而带来更高的客户满意度。当然，当延迟时间非常重要或流量稀少时，您可能正确地放弃此功能。
- en: 6.1.4 Flow control
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.4 流控制
- en: Rate limiters and access keys are critical protections for an API, especially
    one sitting in front of an expensive LLM. Rate limiters control the number of
    requests a client can make to an API within a specified time, which helps protect
    the API server from abuse, such as distributed denial of service (DDoS) attacks,
    where an attacker makes numerous requests simultaneously to overwhelm the system
    and hinder its function.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 速率限制器和访问密钥是API的关键保护措施，尤其是对于那些位于昂贵LLM前面的API。速率限制器控制客户端在指定时间内可以向API发出的请求数量，这有助于保护API服务器免受滥用，例如分布式拒绝服务（DDoS）攻击，攻击者会同时发出大量请求以压倒系统并阻碍其功能。
- en: Rate limiters can also protect the server from bots that make numerous automated
    requests in a short span of time. This helps manage the server resources optimally
    so the server is not exhausted due to unnecessary or harmful traffic. They are
    also useful for managing quotas, thus ensuring all users have fair and equal access
    to the API’s resources. By preventing any single user from using excessive resources,
    the rate limiter ensures the system functions smoothly for all users.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 速率限制器还可以保护服务器免受短时间内进行大量自动化请求的机器人攻击。这有助于优化服务器资源管理，使服务器不会因为不必要的或有害的流量而耗尽。它们在管理配额方面也非常有用，从而确保所有用户都能公平且平等地访问API资源。通过防止任何单个用户过度使用资源，速率限制器确保系统对所有用户都能平稳运行。
- en: All in all, rate limiters are an important mechanism for controlling the flow
    of your LLM’s system processes. They can play a critical role in dampening bursty
    workloads and preventing your system from getting overwhelmed during autoscaling
    and rolling updates, especially when you have a rather large LLM with longer deployment
    times. Rate limiters can take several forms, and the one you choose will be dependent
    on your use case.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，速率限制器是控制你LLM系统流程流的重要机制。它们在减轻突发工作负载和防止系统在自动扩展和滚动更新期间被压垮方面可以发挥关键作用，尤其是在你有一个相当大的LLM且部署时间较长的情况下。速率限制器可以采取多种形式，你选择哪一种将取决于你的用例。
- en: Types of rate limiters
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 速率限制器的类型
- en: '**The following list describes the types of rate limiters:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**以下列表描述了速率限制器的类型：**'
- en: '*Fixed window*—This algorithm allows a fixed number of requests in a set duration
    of time. Let’s say five requests per minute, and it refreshes at the minute. It’s
    really easy to set up and reason about. However, it may lead to uneven distribution
    and can allow a burst of calls at the boundary of the time window.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*固定窗口*—该算法允许在固定的时间段内进行固定数量的请求。比如说，每分钟五次请求，并且每分钟刷新一次。它设置起来非常简单，也很容易理解。然而，它可能会导致分布不均，并且可能在时间窗口的边界允许突发调用。'
- en: '*Sliding window log*—To prevent boundary problems, we can use a dynamic timeframe.
    Let’s say five requests in the last 60 seconds. This type is a slightly more complex
    version of the fixed window that logs each request’s timestamp to provide a moving
    lookback period, providing a more evenly distributed limit.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*滑动窗口日志*—为了防止边界问题，我们可以使用一个动态的时间框架。比如说，过去60秒内有五次请求。这种类型是固定窗口的一个稍微复杂一些的版本，它会记录每个请求的时间戳，提供一个移动的回溯期，从而提供一个更均匀分布的限制。'
- en: '*Token bucket*—Clients initially have a full bucket of tokens, and with each
    request, they spend tokens. When the bucket is empty, the requests are blocked.
    The bucket refills slowly over time. Thus, token buckets allow burst behavior,
    but it’s limited to the number of tokens in the bucket.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*令牌桶*—客户端最初有一个满的令牌桶，并且每次请求都会消耗令牌。当桶为空时，请求会被阻塞。桶会随着时间的推移缓慢地重新填充。因此，令牌桶允许突发行为，但限制在桶中令牌的数量。'
- en: '*Leaky bucket*—It works as a queue where requests enter, and if the queue is
    not full, they are processed; if full, the request overflows and gets discarded,
    thus controlling the rate of the flow.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*漏桶*—它作为一个队列工作，请求进入，如果队列不满，则处理请求；如果满了，请求会溢出并被丢弃，从而控制流量的速率。'
- en: A rate limiter can be applied at multiple levels, from the entire API to individual
    client requests to specific function calls. While you want to avoid being too
    aggressive with them—better to rely on autoscaling to scale and meet demand—you
    don’t want to ignore them completely, especially when it comes to preventing bad
    actors.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 速率限制器可以在多个级别上应用，从整个API到单个客户端请求，再到特定的函数调用。虽然你希望避免对它们过于激进——最好依靠自动扩展来扩展和满足需求——但你也不希望完全忽略它们，尤其是在防止不良行为者方面。
- en: Access keys are also crucial to prevent bad actors. Access keys offer authentication,
    maintaining that only authorized users can access the API, which prevents unauthorized
    use and potential misuse of the API and reduces the influx of spam requests. They
    are also essential to set up for any paid service. Of course, even if your API
    is only exposed internally, setting up access keys shouldn’t be ignored, as it
    can help reduce liability and provide a way of controlling costs by yanking access
    to a rogue process, for example.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 访问密钥对于防止恶意行为也至关重要。访问密钥提供身份验证，确保只有授权用户才能访问API，从而防止未经授权的使用和API的潜在滥用，并减少垃圾请求的涌入。对于任何付费服务，它们也是必不可少的。当然，即使你的API仅面向内部使用，设置访问密钥也不应被忽视，因为它可以帮助减少责任，例如通过撤销恶意进程的访问来控制成本。
- en: Thankfully, setting up a service with rate limiting and access keys is relatively
    easy nowadays, as there are multiple libraries that can help you. In listing 6.2,
    we demonstrate a simple FastAPI app utilizing both. We’ll use FastAPI’s built-in
    security library for our access keys and SlowApi, a simple rate limiter that allows
    us to limit the call of any function or method with a simple decorator.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，如今设置带有速率限制和访问密钥的服务相对容易，因为有多个库可以帮助你。在第6.2列表中，我们展示了使用这两个功能的简单FastAPI应用。我们将使用FastAPI内置的安全库来处理访问密钥，以及SlowApi，一个简单的速率限制器，它允许我们通过简单的装饰器来限制任何函数或方法的调用。
- en: Listing 6.2 Example API with access keys and rate limiter
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第6.2列表 示例API带有访问密钥和速率限制器
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 This would be encrypted in a database.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 这将在数据库中加密。'
- en: While this is just a simple example, you’ll still need to set up a system for
    users to create and destroy access keys. You’ll also want to finetune your time
    limits. In general, you want them to be as loose as possible so as not to interfere
    with the user experience but just tight enough to do their job.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这只是一个简单的例子，但你仍然需要为用户设置一个系统来创建和销毁访问密钥。你还想微调你的时间限制。一般来说，你希望它们尽可能宽松，以免干扰用户体验，但又要足够紧，以完成它们的工作。
- en: 6.1.5 Streaming responses
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.5 流式响应
- en: One feature your LLM service should absolutely include is streaming. Streaming
    allows us to return the generated text to the user as it is being generated versus
    all at once at the end. Streaming adds quite a bit of complexity to the system,
    but regardless, it has come to be considered a must-have feature for several reasons.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你的LLM服务应该绝对包含的一个功能是流式处理。流式处理允许我们在文本生成时将其返回给用户，而不是在最后一次性返回。流式处理给系统增加了相当多的复杂性，但无论如何，它已经成为几个原因的必备功能。
- en: First, LLMs are rather slow, and the worst thing you can do to your users is
    make them wait—waiting means they will become bored, and bored users complain
    or, worse, leave. You don’t want to deal with complaints, do you? Of course not!
    But by streaming the data as it’s being created, we offer the users a more dynamic
    and interactive experience.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，LLMs相当慢，而你对你用户能做的最糟糕的事情就是让他们等待——等待意味着他们会变得无聊，无聊的用户会抱怨，更糟糕的是，他们会离开。你不想处理投诉，对吧？当然不！但是通过在数据创建时流式传输数据，我们为用户提供了一个更动态和互动的体验。
- en: Second, LLMs aren’t just slow; they are unpredictable. One prompt could lead
    to pages and pages of generated text, and another, a single token. As a result,
    your latency is going to be all over the place. Streaming allows us to worry about
    more consistent metrics like tokens per second (TPS). Keeping TPS higher than
    the average user’s reading speed means we’ll be sending responses back faster
    than the user can consume them, ensuring they won’t get bored and we are providing
    a high-quality user experience. In contrast, if we wait until the end to return
    the results, users will likely decide to walk away and return when it finishes
    because they never know how long to wait. This huge disruption to their flow makes
    your service less effective or useful.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，LLMs不仅速度慢，而且不可预测。一个提示可能导致生成数页的文本，而另一个提示可能只生成一个标记。因此，你的延迟将会到处都是。流式处理允许我们关注更一致的指标，如每秒标记数（TPS）。保持TPS高于平均用户的阅读速度意味着我们将比用户消费得更快地发送响应，确保他们不会感到无聊，并且我们提供了高质量的用户体验。相比之下，如果我们等到最后才返回结果，用户可能会决定离开，直到它完成才回来，因为他们不知道要等待多长时间。这种对他们的流程的巨大干扰使得你的服务更无效或无用。
- en: Lastly, users are starting to expect streaming. Streaming responses have become
    a nice tell as to whether you are speaking to a bot or an actual human. Since
    humans have to type, proofread, and edit their responses, we can’t expect written
    responses from a human customer support rep to be in a stream-like fashion. So
    when they see a response streaming in, your users will know they are talking to
    a bot. People interact differently with a bot than they will with a human, so
    it’s very useful information to give them to prevent frustration.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，用户开始期待流式传输。流式响应已成为判断你是否在与机器人还是真人交谈的一个很好的指标。由于人类需要键入、校对和编辑他们的响应，我们无法期望来自人类客户支持代表的书面响应以流式传输的形式出现。因此，当用户看到响应流式传输进来时，他们会知道他们正在与机器人交谈。人们与机器人的互动方式与与人类的互动方式不同，因此这是非常有用的信息，可以防止用户感到沮丧。
- en: In listing 6.3 we demonstrate a very simple LLM service that utilizes streaming.
    The key pieces to pay attention to are that we are using the base asyncio library
    to allow us to run asynchronous function calls, FastAPI’s `StreamingResponse`
    to ensure we send responses to the clients in chunks, and Hugging Face Transformer’s
    `TextIteratorStreamer` to create a pipeline generator of our model’s inference.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 6.3 中，我们展示了一个非常简单的利用流式传输的 LLM 服务。需要注意的关键点是，我们使用 base asyncio 库来允许我们运行异步函数调用，FastAPI
    的 `StreamingResponse` 确保我们以块的形式向客户端发送响应，以及 Hugging Face Transformer 的 `TextIteratorStreamer`
    来创建我们模型推理的管道生成器。
- en: Listing 6.3 A streaming LLM service
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.3 流式 LLM 服务
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Loads tokenizer, model, and streamer into memory'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将分词器、模型和流式传输器加载到内存中'
- en: '#2 Slows things down to see streaming. It’s typical to return streamed responses
    byte encoded.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 减慢速度以查看流式传输。通常返回的字节编码流式响应。'
- en: '#3 Starts a separate thread to generate results'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 启动一个单独的线程来生成结果'
- en: '#4 Starts service; defaults to localhost on port 8000'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 启动服务；默认在端口 8000 的 localhost 上'
- en: Now that we know how to implement several must-have features for our LLM service,
    including batching, rate limiting, and streaming, let’s look at some additional
    tooling we can add to our service to improve usability and overall workflow.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了如何实现我们 LLM 服务的一些必备功能，包括批处理、速率限制和流式传输，让我们看看我们可以添加到我们的服务中的一些额外的工具，以改善可用性和整体工作流程。
- en: 6.1.6 Feature store
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.6 特征存储
- en: 'When it comes to running ML models in production, feature stores really simplify
    the inference process. We first introduced these in chapter 3, but as a recap,
    feature stores establish a centralized source of truth. They answer crucial questions
    about your data: Who is responsible for the feature? What is its definition? Who
    can access it? Let’s take a look at setting one up and querying the data to get
    a feel for how they work. We’ll be using Feast, which is open source and supports
    a variety of backends. To get started, let us `pip` `install` `feast` and then
    run the `init` command in your terminal to set up a project, like so:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到在生产环境中运行 ML 模型时，特征存储可以极大地简化推理过程。我们首次在第三章介绍了这些内容，但作为一个回顾，特征存储建立了一个中心化的真实来源。它们回答关于你的数据的关键问题：谁负责这个特征？它的定义是什么？谁可以访问它？让我们看看如何设置一个并查询数据以了解它们的工作方式。我们将使用
    Feast，它是一个开源项目，支持多种后端。要开始，让我们使用 `pip` `install` `feast` 并然后在您的终端中运行 `init` 命令来设置一个项目，如下所示：
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The app we are building is a question-and-answer service. Q&A services can greatly
    benefit from a feature store’s data governance tooling. For example, point-in-time
    joins help us answer questions like “Who is the president of x?” where the answer
    is expected to change over time. Instead of querying just the question, we query
    the question with a timestamp, and the point-in-time join will return whatever
    the answer to the question was in our database at that point in time. In the next
    listing, we pull a Q&A dataset and store it in a parquet format in the data directory
    of our Feast project.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建的应用程序是一个问答服务。问答服务可以从特征存储的数据治理工具中受益良多。例如，点时间连接帮助我们回答像“x 的总统是谁？”这样的问题，答案可能会随时间而改变。我们不仅查询问题，还查询带有时戳的问题，点时间连接将返回在那个时间点我们数据库中问题的答案。在下一个列表中，我们拉取一个问答数据集并将其存储在我们的
    Feast 项目的数据目录中的 parquet 格式中。
- en: Listing 6.4 Downloading the SQuAD dataset
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.4 下载 SQuAD 数据集
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Loads SQuAΔ dataset'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载 SQuAΔ 数据集'
- en: '#2 Extracts questions and answers'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 提取问题和答案'
- en: '#3 Creates a dataframe'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 创建一个数据框'
- en: '#4 Adds embeddings and timestamps'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 添加嵌入和时间戳'
- en: '#5 Saves to parquet'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 保存到 parquet 格式'
- en: Next, we’ll need to define the feature view for our feature store. A feature
    view is essentially like a view in a relational database. We’ll define a name,
    the entities (which are like IDs or primary keys), the schema (which are our feature
    columns), and a source. We’ll just be demoing using a local file store, but in
    production, you’d want to use one of Feast’s many backend integrations with Snowflake,
    GCP, AWS, etc. It currently doesn’t support a VectorDB backend, but I’m sure it’s
    only a matter of time. In addition, we can add metadata to our view through tags
    and define a time to live (TTL), which limits how far back Feast will look when
    generating historical datasets. In the following listing, we define the feature
    view. Go ahead and add this definition into a file called qa.py in the feature_repo
    directory of our project.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为我们的特征存储定义特征视图。特征视图本质上类似于关系数据库中的视图。我们将定义一个名称、实体（类似于ID或主键）、模式（即我们的特征列）和来源。我们只是演示使用本地文件存储，但在生产中，您希望使用Feast与Snowflake、GCP、AWS等许多后端集成之一。它目前不支持VectorDB后端，但我相信这只是时间问题。此外，我们可以通过标签添加元数据到我们的视图，并定义一个生存时间（TTL），这限制了Feast在生成历史数据集时可以回溯多远。在以下列表中，我们定义了特征视图。请将此定义添加到我们项目中的feature_repo目录下的名为qa.py的文件中。
- en: Listing 6.5 Feast `FeatureView` definition
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.5 Feast `FeatureView` 定义
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With that defined, let’s go ahead and register it. We’ll do that with
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好之后，让我们继续注册它。我们将使用
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we’ll want to materialize the view. In production, this is a step you’ll
    need to schedule on a routine basis with something like cron or Prefect. Be sure
    to update the UTC timestamp for the end date in this command to something in the
    future to ensure the view collects the latest data:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们希望将视图实体化。在生产中，这是一个您需要定期使用cron或Prefect之类的工具安排的步骤。确保将此命令中结束日期的UTC时间戳更新为未来的某个时间，以确保视图收集最新的数据：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now all that’s left is to query it! The following listing shows a simple example
    of pulling features to be used at inference time.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在只剩下查询它了！以下列表展示了在推理时提取要使用的特征的简单示例。
- en: Listing 6.6 Querying a feature view at inference
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.6 在推理时查询特征视图
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This example will pull the most up-to-date information for the lowest possible
    latency at inference time. For point-in-time retrieval, you would use the `get_historical_
    features` method instead. In addition, in this example, we use a list of IDs for
    the entity rows parameter, but you could also use an SQL query making it very
    flexible and easy to use.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例将在推理时获取最新信息，以实现最低可能的延迟。对于点时间检索，您将使用`get_historical_features`方法。此外，在此示例中，我们使用实体行参数的ID列表，但您也可以使用SQL查询，使其非常灵活且易于使用。
- en: 6.1.7 Retrieval-augmented generation
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.7 增强检索生成
- en: Retrieval-augmented generation (RAG) has become the most widely used tool to
    combat hallucinations in LLMs and improve the accuracy of responses in our results.
    Its popularity is likely because RAG is both easy to implement and quite effective.
    As first discussed in section 3.4.5, vector databases are a tool you’ll want to
    have in your arsenal. One of the key reasons is that they make RAG so much easier
    to implement. In figure 6.4, we demonstrate a RAG system. In the preprocessing
    stage, we take our documents, break them up, and transform them into embeddings
    that we’ll load into our vector database. During inference, we can take our input,
    encode it into an embedding, and run a similarity search across our documents
    in that vector database to find the nearest neighbors. This type of inference
    is known as semantic search. Pulling relevant documents and inserting them into
    our prompt will help give context to the LLM and improve the results.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 增强检索生成（RAG）已成为对抗LLM中的幻觉并提高我们结果中响应准确性的最广泛使用的工具。其流行可能是因为RAG既易于实现又非常有效。如第3.4.5节首次讨论的，向量数据库是你希望拥有的工具之一。其中一个关键原因是它们使RAG的实现变得容易得多。在图6.4中，我们展示了一个RAG系统。在预处理阶段，我们将我们的文档拆分，并将它们转换成嵌入，然后我们将它们加载到我们的向量数据库中。在推理期间，我们可以将我们的输入编码成嵌入，并在该向量数据库中运行相似度搜索以找到最近的邻居。这种推理被称为语义搜索。提取相关文档并将它们插入到我们的提示中，将有助于为LLM提供上下文并提高结果。
- en: '![figure](../Images/6-4.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-4.png)'
- en: Figure 6.4 RAG system demonstrating how we use our input embeddings to run a
    search across our documentation, improving the results of the generated text from
    our LLM
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4 展示了RAG系统如何使用我们的输入嵌入在文档中运行搜索，从而提高我们LLM生成的文本结果
- en: We are going to demo implementing RAG using Pinecone since it will save us the
    effort of setting up a vector database. For listing 6.7, we will set up a Pinecone
    index and load a Wikipedia dataset into it. In this listing, we’ll create a `WikiDataIngestion`
    class to handle the heavy lifting. This class will load the dataset and run through
    each Wikipedia page, splitting the text into consumable chunks. It will then embed
    these chunks and upload everything in batches. Once we have everything uploaded,
    we can start to make queries.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将演示使用Pinecone实现RAG，因为它将节省我们设置向量数据库的精力。对于列表6.7，我们将设置一个Pinecone索引并将Wikipedia数据集加载到其中。在这个列表中，我们将创建一个`WikiDataIngestion`类来处理繁重的工作。这个类将加载数据集并遍历每个维基百科页面，将文本分割成可消费的块。然后它将嵌入这些块并将所有内容批量上传。一旦我们上传了所有内容，我们就可以开始进行查询。
- en: You’ll need an API key if you plan to follow along, so if you don’t already
    have one, go to Pinecone’s website ([https://www.pinecone.io/](https://www.pinecone.io/))
    and create a free account, set up a starter project (free tier), and get an API
    key. One thing to pay attention to as you read the listing is that we’ll split
    up the text into chunks of 400 tokens with `text_ splitter`. We specifically split
    on tokens instead of words or characters, which allows us to properly budget inside
    our token limits for our model. In this example, returning the top three results
    will add 1,200 tokens to our request, which allows us to plan ahead of time how
    many tokens we’ll give to the user to write their prompt.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算跟随操作，你需要一个API密钥，所以如果你还没有，请访问Pinecone的网站([https://www.pinecone.io/](https://www.pinecone.io/))并创建一个免费账户，设置一个启动项目（免费层），并获取一个API密钥。在阅读列表时需要注意的一点是，我们将使用`text_splitter`将文本分割成400个标记的块。我们特别按标记分割而不是按单词或字符，这允许我们在模型标记限制内正确预算。在这个例子中，返回前三个结果将使我们的请求增加1,200个标记，这允许我们提前计划我们将给用户多少标记来编写他们的提示。
- en: Listing 6.7 Example setting up a Pinecone database
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.7 Pinecone数据库设置示例
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Gets openai API key from [platform.openai.com](https://platform.openai.com)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从[platform.openai.com](https://platform.openai.com)获取openai API密钥'
- en: '#2 Finds API key in console at app.pinecone.io'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在app.pinecone.io控制台中查找API密钥'
- en: '#3 Creates an index if it doesn’t exist'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 如果不存在则创建索引'
- en: '#4 1536 dim of text-embedding-ada-002'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 文本嵌入-ada-002的1536维'
- en: '#5 Connects to the index and describes the stats'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 连接到索引并描述统计数据'
- en: '#6 Uses a generic embedder if an openai api key is not provided'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 如果未提供openai api密钥，则使用通用嵌入器'
- en: '#7 Also 1536 dim'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 也为1536维'
- en: '#8 Ingests data and describes the stats anew'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 读取数据并重新描述统计数据'
- en: '#9 Makes a query'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 执行查询'
- en: When I ran this code, the top three query results to my question, “Did Johannes
    Gutenberg invent the printing press?” were the Wikipedia pages for Johannes Gutenberg,
    the pencil, and the printing press. Not bad! While a vector database isn’t going
    to be able to answer the question, it’s simply finding the most relevant articles
    based on the proximity of their embeddings to my question.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行这段代码时，针对我的问题“约翰内斯·古腾堡发明了印刷机吗？”的前三个查询结果分别是约翰内斯·古腾堡的维基百科页面、铅笔和印刷机。还不错！虽然向量数据库无法回答这个问题，但它只是根据它们的嵌入与我问题的接近程度找到最相关的文章。
- en: With these articles, we can then feed their embeddings into our LLM as additional
    context to the question to ensure a more grounded result. Since we include sources,
    it will even have the wiki URL it can give as a reference, and it won’t just hallucinate
    one. By giving this context, we greatly reduce the concern about our LLM hallucinating
    and making up an answer.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些文章，我们可以将它们的嵌入输入到我们的LLM中，作为问题的额外上下文，以确保结果更加可靠。由于我们包括了来源，它甚至可以提供作为参考的wiki
    URL，而不会只是凭空想象。通过提供这个上下文，我们大大减少了我们的LLM凭空想象并编造答案的担忧。
- en: 6.1.8 LLM service libraries
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.8 LLM服务库
- en: 'If you are starting to feel a bit overwhelmed about all the tooling and features
    you need to implement to create an LLM service, we have some good news for you:
    several libraries aim to do all of this for you! Some open source libraries of
    note are vLLM and OpenLLM (by BentoML). Hugging Face’s Text-Generation-Inference
    (TGI) briefly lost its open source license, but fortunately, it’s available again
    for commercial use. There are also some start-ups building some cool tooling in
    this space, and we recommend checking out TitanML if you are hoping for a more
    managed service. These are like the tools MLServer, BentoML, and Ray Serve discussed
    in section 3.4.8 on deployment service, but they are designed specifically for
    LLMs.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你开始觉得需要实现一个LLM服务所需的所有工具和功能让你有些不知所措，我们有一些好消息要告诉你：有几个库旨在为你完成所有这些工作！一些值得注意的开源库包括vLLM和BentoML的OpenLLM。Hugging
    Face的Text-Generation-Inference（TGI）短暂地失去了开源许可，但幸运的是，它现在又可用于商业用途。还有一些初创公司在这个领域构建了一些酷炫的工具，如果你希望获得更托管的服务，我们建议你检查一下TitanML。这些工具类似于第3.4.8节中讨论的MLServer、BentoML和Ray
    Serve，但它们是为LLM专门设计的。
- en: Most of these toolings are still relatively new and under active development,
    and they are far from feature parity with each other, so pay attention to what
    they offer. What you can expect is that they should at least offer streaming,
    batching, and GPU parallelization support (something we haven’t specifically talked
    about in this chapter), but beyond that, it’s a crapshoot. Many of them still
    don’t support several features discussed in this chapter, nor do they support
    every LLM architecture. What they do, though, is make deploying LLMs easy.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些工具仍然相对较新，处于积极开发中，它们之间的功能对等性还远未实现，所以请注意它们提供的内容。你可以期待的是，它们至少应该提供流式传输、批处理和GPU并行化支持（这是我们本章没有具体讨论的内容），但除此之外，就很难说了。许多它们仍然不支持本章讨论的几个功能，也不支持每个LLM架构。尽管如此，它们确实使得部署LLM变得容易。
- en: Using vLLM as an example, just `pip` `install` `vllm`, and then you can run
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以vLLM为例，只需执行`pip install vllm`命令，然后你就可以运行
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With just one command, we now have a service up and running the model we trained
    in chapter 5\. Go ahead and play with it; you should be able to send requests
    to the `/generate` endpoint like so:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 只需一条命令，我们现在就有一个服务正在运行我们在第5章训练的模型。大胆地尝试一下；你应该能够像这样向`/generate`端点发送请求：
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It’s very likely you won’t be all that impressed with any of these toolings.
    Still, you should be able to build your own API and have a good sense of how to
    do it at this point. Now that you have a service and can even spin it up locally,
    let’s discuss the infrastructure you need to set up to support these models for
    actual production usage. Remember, the better the infrastructure, the less likely
    you’ll be called in the middle of the night when your service goes down unexpectedly.
    None of us want that, so let’s check it out.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能你对这些工具不会特别印象深刻。尽管如此，你现在应该能够构建自己的API，并且对如何做到这一点有一个很好的理解。现在你有一个服务，甚至可以在本地启动它，让我们讨论你需要建立以支持这些模型实际生产使用的架构。记住，基础设施越好，你半夜服务意外中断时被叫醒的可能性就越小。我们都不希望那样，所以让我们来看看。
- en: 6.2 Setting up infrastructure
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 建立基础设施
- en: Setting up infrastructure is a critical aspect of modern software development,
    and we shouldn’t expect machine learning to be any different. To ensure scalability,
    reliability, and efficient deployment of our applications, we need to plan a robust
    infrastructure that can handle the demands of a growing user base. This is where
    Kubernetes comes into play.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 建立基础设施是现代软件开发的一个关键方面，我们不应该期望机器学习有任何不同。为了确保可扩展性、可靠性和应用的高效部署，我们需要规划一个能够应对不断增长的用户群需求的强大基础设施。这正是Kubernetes发挥作用的地方。
- en: Kubernetes, often referred to as k8s, is an open source container orchestration
    platform that helps automate and manage the deployment, scaling, and management
    of containerized applications. It is designed to simplify the process of running
    and coordinating multiple containers across a cluster of servers, making it easier
    to scale applications and ensure high availability. We are going to talk a lot
    about k8s in this chapter, and while you don’t need to be an expert, it will be
    useful to cover some basics to ensure we are all on the same page.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes，通常被称为k8s，是一个开源的容器编排平台，它帮助自动化和管理容器化应用程序的部署、扩展和管理。它旨在简化在服务器集群中运行和协调多个容器的过程，使扩展应用程序和确保高可用性变得更加容易。在本章中，我们将大量讨论k8s，虽然你不需要成为专家，但了解一些基础知识将有助于我们保持一致。
- en: At its core, k8s works by grouping containers into logical units called pods,
    which are the smallest deployable units in the k8s ecosystem. These pods are then
    scheduled and managed by the k8s control plane, which oversees their deployment,
    scaling, and updates. This control plane consists of several components that collectively
    handle the orchestration and management of containers. In figure 6.5, we give
    an oversimplification of the k8s architecture to help readers who are unfamiliar
    with it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，k8s通过将容器分组到称为Pod的逻辑单元中来工作，这些Pod是k8s生态系统中最小的可部署单元。然后，这些Pod由k8s控制平面进行调度和管理，该平面负责它们的部署、扩展和更新。这个控制平面由几个组件组成，共同处理容器的编排和管理。在图6.5中，我们给出了k8s架构的过度简化，以帮助不熟悉它的读者。
- en: '![figure](../Images/6-5.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/6-5.png)'
- en: Figure 6.5 An oversimplification of the Kubernetes architecture. What you need
    to know is that our services run in pods, and pods run on nodes, which essentially
    are a machine. K8s helps us both manage the resources and handle the orchestration
    of deploying pods to these resources.
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5 Kubernetes架构的过度简化。你需要知道的是，我们的服务运行在Pods中，而Pods运行在节点上，本质上是一个机器。K8s帮助我们管理资源，并处理将Pods部署到这些资源上的编排。
- en: Using k8s, we can take advantage of features such as automatic scaling, load
    balancing, and service discovery, which greatly simplify the deployment and management
    of web applications. K8s provides a flexible and scalable infrastructure that
    can easily adapt to changing demands, allowing organizations to efficiently scale
    their applications as their user base grows. K8s offers a wide range of additional
    features and extensibility options, such as storage management, monitoring, and
    logging, which help ensure the smooth operation of web applications.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k8s，我们可以利用自动扩展、负载均衡和服务发现等特性，这些特性极大地简化了Web应用程序的部署和管理。K8s提供了一个灵活且可扩展的基础设施，可以轻松适应不断变化的需求，使组织能够随着用户基础的扩大而高效地扩展其应用程序。K8s提供了一系列额外的功能和可扩展选项，如存储管理、监控和日志记录，这些有助于确保Web应用程序的平稳运行。
- en: One of these extensibility options is known as custom resource definitions (CRDs).
    CRDs are a feature of Kubernetes that allows users to create their own specifications
    for custom resources, thus extending the functionalities of Kubernetes without
    modifying the Kubernetes source code. With a CRD defined, we can create custom
    objects similar to how we would create a built-in object like a pod or service.
    This gives k8s a lot of flexibility that we will need for different functionality
    throughout this chapter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可扩展选项之一被称为自定义资源定义（CRDs）。CRDs是Kubernetes的一个特性，允许用户为自定义资源创建自己的规范，从而在不修改Kubernetes源代码的情况下扩展Kubernetes的功能。定义了CRD之后，我们可以创建类似于Pod或服务这样的内置对象的自定义对象。这为k8s提供了大量的灵活性，我们将在本章的不同功能中需要它。
- en: If you are new to Kubernetes, you might be scratching your head through parts
    of this section, and that’s totally fine. Hopefully, though, you have enough knowledge
    to get the gist of what we will be doing in this section and why. At least you’ll
    be able to walk away with a bunch of questions to ask your closest DevOps team
    member.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚开始接触Kubernetes，你可能会对本节的部分内容感到困惑，这是完全可以理解的。不过，希望你有足够的知识来把握本节将要做什么以及为什么这么做的大致内容。至少，你将能够带着一串问题离开，这些问题可以向你的最亲密的DevOps团队成员提出。
- en: 6.2.1 Provisioning clusters
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 部署集群
- en: The first thing to do when starting any project is to set up a cluster. A cluster
    is a collective of worker machines or nodes where we will host our applications.
    Creating a cluster is relatively simple; configuring it is the hard part. Of course,
    there have been many books written on how to do this, and the majority of considerations
    like networking, security, and access control are outside the scope of this book.
    In addition, considering the steps you take will also be different depending on
    the cloud provider of choice and your company’s business strategy, we will focus
    on only the portions that we feel are needed to get you up and running, as well
    as any other tidbits that may make your life easier.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 开始任何项目时首先要做的事情是设置一个集群。集群是由工作机器或节点组成的集合，我们将在这里托管我们的应用程序。创建一个集群相对简单；配置它是难点。当然，已经有许多书籍讲述了如何做这件事，但关于网络、安全和访问控制等大多数考虑因素都不在本书的范围之内。此外，您采取的步骤也会根据您选择的云提供商和公司的业务策略而有所不同，因此我们将只关注我们认为对您启动项目所必需的部分，以及任何可能使您生活更轻松的小贴士。
- en: The first step is to create a cluster. On GCP, you would use the gcloud tool
    and run
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一个集群。在GCP上，您将使用gcloud工具并运行
- en: '[PRE16]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: On AWS, using the eksctl tool, run
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS上，使用eksctl工具，运行
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: On Azure, using the az cli tool, run
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure上，使用az cli工具，运行
- en: '[PRE18]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, even the first steps are highly dependent on your provider,
    and you can suspect that the subsequent steps will be as well. Since we realize
    most readers will be deploying in a wide variety of environments, we will not
    focus on the exact steps but hopefully give you enough context to search and discover
    for yourself.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，即使是第一步也非常依赖于您的提供商，您可能会怀疑后续步骤也是如此。由于我们意识到大多数读者将在各种环境中部署，我们将不会关注具体的步骤，但希望给您足够的背景知识，以便您自己搜索和发现。
- en: Many readers, we imagine, will already have a cluster set up for them by their
    infrastructure teams, complete with many defaults and best practices. One of these
    is setting up node auto-provisioning (NAP) or cluster autoscaling. NAP allows
    a cluster to grow, adding more nodes as deployments demand them. This way, we
    only pay for nodes we actually use. It’s a very convenient feature, but it often
    defines resource limits or restrictions on the instances available for autoscaling,
    and you can bet your cluster’s defaults don’t include accelerator or GPU instances
    in that pool. We’ll need to fix that.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想象许多读者可能已经由他们的基础设施团队为他们设置了一个集群，其中包含许多默认值和最佳实践。其中之一是设置节点自动配置（NAP）或集群自动扩展。NAP允许集群增长，根据部署需求添加更多节点。这样，我们只为实际使用的节点付费。这是一个非常方便的功能，但它通常定义了资源限制或对自动扩展实例的可用性进行限制，您可以确信集群的默认设置不包括加速器或GPU实例。我们需要解决这个问题。
- en: In GCP, we would create a configuration file like the one in the following listing,
    where we can include the GPU `resourceType`. In the example, we include T4s and
    both A100 types.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCP中，我们会创建一个类似于以下列表中的配置文件，其中我们可以包括GPU `resourceType`。在示例中，我们包括了T4s和两种A100类型。
- en: Listing 6.8 Example NAP config file
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.8 示例NAP配置文件
- en: '[PRE19]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You would then set this by running
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以通过运行以下命令来设置
- en: '[PRE20]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The real benefit of an NAP is that instead of predefining what resources are
    available at a fixed setting, we can set resource limits, which put a cap on the
    total number of GPUs that we would scale up to. They clearly define what GPUs
    we want and expect to be in any given cluster.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: NAP的实际好处是，我们不是预先定义在固定设置下可用的资源，而是可以设置资源限制，这将对我们可以扩展到的总GPU数量设置上限。它们清楚地定义了我们想要并在任何给定集群中期望的GPU。
- en: When one author was first learning about limits, he often got them confused
    with similar concepts—quotas, reservations, and commitments—and has seen many
    others just as confused. Quotas, in particular, are very similar to limits. Their
    main purpose is to prevent unexpected overage charges by ensuring a particular
    project or application doesn’t consume too many resources. Unlike limits, which
    are set internally, quotas often require submitting a request to your cloud provider
    when you want to raise them. These requests help inform and are used by the cloud
    provider to better plan which resources to provision and put into different data
    centers in different regions. It’s tempting to think that the cloud provider will
    ensure those resources are available; however, quotas never guarantee there will
    be enough resources in a region for your cluster to use, and you might run into
    `resources` `not` `found` errors way before you hit them.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当一位作者刚开始学习极限概念时，他经常将它们与类似的概念混淆——配额、保留和承诺，并看到许多其他人也是如此。特别是配额，与极限非常相似。它们的主要目的是通过确保特定的项目或应用不会消耗过多的资源来防止意外超量费用。与内部设定的限制不同，提高配额通常需要向云服务提供商提交请求。这些请求有助于通知云服务提供商，并用于更好地规划分配哪些资源以及将它们放置在不同地区的不同数据中心。可能会觉得云服务提供商将确保这些资源可用；然而，配额永远不会保证某个地区有足够的资源供您的集群使用，您可能会在达到限制之前就遇到`资源未找到`的错误。
- en: While quotas and limits set an upper bound, reservations and commitments set
    the lower bound. Reservations are an agreement to guarantee that a certain amount
    of resources will always be available and often come with the caveat that you
    will be paying for these resources regardless of whether you end up using them.
    Commitments are similar to reservations but are often longer-term contracts, usually
    coming with a discounted price.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然配额和限制设定了上限，但保留和承诺设定了下限。保留是一种保证一定数量的资源始终可用的协议，通常附带条件，即无论您是否最终使用这些资源，您都将为这些资源付费。承诺与保留类似，但通常是更长期限的合同，通常带有折扣价格。
- en: 6.2.2 Autoscaling
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 自动扩展
- en: One of the big selling points to setting up a k8s cluster is autoscaling. Autoscaling
    is an important ingredient in creating robust production-grade services. The main
    reason is that we never expect any service to receive static request volume. If
    anything else, you should expect more volume during the day and less at night
    while people sleep. So we’ll want our service to spin up more replicas during
    peak hours to improve performance and spin down replicas during off hours to save
    money, not to mention the need to handle bursty workloads that often threaten
    to crash a service at any point.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 设置k8s集群的一个主要卖点就是自动扩展。自动扩展是创建健壮的生产级服务的重要成分。主要原因是我们从不期望任何服务接收静态请求量。如果不是其他原因，您应该预计在白天会有更多的流量，而在夜间人们睡觉时流量会减少。因此，我们希望在高峰时段启动更多副本以提高性能，在非高峰时段关闭副本以节省金钱，更不用说处理那些经常在任何时候威胁到服务崩溃的突发工作负载了。
- en: Knowing your service will automatically provision more resources and set up
    additional deployments based on the needs of the application is what allows many
    infrastructure engineers to sleep peacefully at night. The catch is that it requires
    an engineer to know what those needs are and ensure everything is configured correctly.
    While autoscaling provides flexibility, the real business value comes from the
    cost savings. Most engineers think about autoscaling in terms of scaling up to
    prevent meltdowns, but even more important to the business is the ability to scale
    down, freeing up resources and cutting costs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 知道您的服务将根据应用程序的需求自动分配更多资源并设置额外的部署，这使得许多基础设施工程师能够安心入睡。问题是这需要工程师了解这些需求并确保一切配置正确。虽然自动扩展提供了灵活性，但真正的商业价值来自成本节约。大多数工程师认为自动扩展是为了向上扩展以防止崩溃，但对企业来说，更重要的是能够向下扩展，释放资源并降低成本。
- en: 'One of the main reasons cloud computing and technologies like Kubernetes have
    become essential in modern infrastructures is because autoscaling is built in.
    Autoscaling is a key feature of Kubernetes, and with horizontal pod autoscalers
    (HPAs), you can easily adjust the number of replicas of your application based
    on two native resources: CPU and memory usage, as shown in figure 6.6\. However,
    in a book about putting LLMs in production, scaling based on CPU and memory alone
    will never be enough. We will need to scale based on custom metrics, specifically
    GPU utilization.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算和像Kubernetes这样的技术成为现代基础设施中不可或缺的原因之一是因为自动扩展是内置的。自动扩展是Kubernetes的一个关键特性，通过水平自动扩展器（HPA），你可以根据两个原生资源：CPU和内存使用情况轻松调整应用程序副本的数量，如图6.6所示。然而，在关于将LLMs投入生产的书籍中，仅基于CPU和内存的扩展将永远不够。我们需要根据自定义指标进行扩展，特别是GPU利用率。
- en: '![figure](../Images/6-6.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-6.png)'
- en: Figure 6.6 Basic autoscaling using the in-built k8s horizontal pod autoscaler
    (HPA). The HPA watches CPU and memory resources and will tell the deployment service
    to increase or decrease the number of replicas.
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6 使用内置的k8s水平自动扩展器（HPA）的基本自动扩展。HPA监视CPU和内存资源，并将通知部署服务增加或减少副本数量。
- en: Setting up autoscaling based on GPU metrics is going to take a bit more work
    and requires setting up several services. It’ll become clear why we need each
    service as we discuss them, but the good news is that by the end, you’ll be able
    to set up your services to scale based on any metric, including external events
    such as messages from a message broker, requests to an HTTP endpoint, and data
    from a queue.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 基于GPU指标设置自动扩展需要更多的工作，并需要设置几个服务。当我们讨论它们时，将清楚为什么我们需要每个服务，但好消息是，到最后，你将能够设置你的服务以基于任何指标进行扩展，包括来自消息代理的消息、对HTTP端点的请求以及来自队列的数据。
- en: The first service we’ll need is one that can collect the GPU metrics. For this,
    we have NVIDIA’s Data Center GPU Manager (DCGM), which provides a metrics exporter
    that can export GPU metrics. DCGM exposes a host of GPU metrics, including temperature
    and power usage, which can create some fun dashboards, but the most useful metrics
    for autoscaling are utilization and memory utilization.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要的服务是能够收集GPU指标的服务。为此，我们使用了NVIDIA的数据中心GPU管理器（DCGM），它提供了一个可以导出GPU指标的指标导出器。DCGM公开了大量GPU指标，包括温度和功耗，可以创建一些有趣的仪表板，但最有助于自动扩展的指标是利用率和内存利用率。
- en: From here, the data will go to a service like Prometheus. Prometheus is a popular
    open source monitoring system used to monitor Kubernetes clusters and the applications
    running on them. Prometheus collects metrics from various sources and stores them
    in a time-series database, where they can be analyzed and queried. Prometheus
    can collect metrics directly from Kubernetes APIs and from applications running
    on the cluster using a variety of collection mechanisms such as exporters, agents,
    and sidecar containers. It’s essentially an aggregator of services like DCGM,
    including features like alerting and notification. It also exposes an HTTP API
    for service for external tooling like Grafana to query and create graphs and dashboards
    with.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，数据将流向像Prometheus这样的服务。Prometheus是一个流行的开源监控系统，用于监控Kubernetes集群及其上运行的应用程序。Prometheus从各种来源收集指标，并将它们存储在时序数据库中，以便进行分析和查询。Prometheus可以直接从Kubernetes
    API以及集群上运行的应用程序使用各种收集机制（如导出器、代理和边车容器）收集指标。它本质上是一个像DCGM这样的服务的聚合器，包括警报和通知等功能。它还提供了一个HTTP
    API，供Grafana等外部工具查询和创建图表和仪表板。
- en: 'While Prometheus provides a way to store metrics and monitor our service, the
    metrics aren’t exposed to the internals of Kubernetes. For an HPA to gain access,
    we will need to register yet another service to either the custom metrics API
    or external metrics API. By default, Kubernetes comes with the metrics.k8s.io
    endpoint that exposes resource metrics, CPU, and memory utilization. To accommodate
    the need to scale deployments and pods on custom metrics, two additional APIs
    were introduced: custom.metrics.k9s.io and external.metrics.k8s.io. There are
    some limitations to this setup, as currently, only one “adapter” API service can
    be registered at a time for either one. This limitation mostly becomes a problem
    if you ever decide to change this endpoint from one provider to another.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Prometheus 提供了一种存储指标和监控我们服务的方法，但这些指标并没有暴露给 Kubernetes 的内部。为了使 HPA 能够访问，我们需要将另一个服务注册到自定义指标
    API 或外部指标 API。默认情况下，Kubernetes 随带 metrics.k8s.io 端点，该端点公开资源指标、CPU 和内存利用率。为了满足在自定义指标上缩放部署和
    pod 的需求，引入了两个额外的 API：custom.metrics.k9s.io 和 external.metrics.k8s.io。这种设置有一些限制，因为目前，一次只能为其中一个注册一个“适配器”API
    服务。如果决定将此端点从一家提供商更改为另一家，这种限制主要成为一个问题。
- en: For this service, Prometheus provides the Prometheus Adapter, which works well,
    but from our experience, it wasn’t designed for production workloads. Alternatively,
    we would recommend KEDA. KEDA (Kubernetes Event-Driven Autoscaling) is an open
    source project that provides event-driven autoscaling for Kubernetes. It offers
    more flexibility in terms of the types of custom metrics that can be used for
    autoscaling. While Prometheus Adapter requires configuring metrics inside a ConfigMap,
    any metric already exposed through the Prometheus API can be used in KEDA, providing
    a more streamlined and friendly user experience. It also offers scaling to and
    from 0, which isn’t available through HPAs, allowing you to turn off a service
    completely if there is no traffic. That said, you can’t scale from 0 on resource
    metrics like CPU and memory and, by extension, GPU metrics, but it is useful when
    you are using traffic metrics or a queue to scale.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这项服务，Prometheus 提供了 Prometheus 适配器，它运行良好，但根据我们的经验，它并不是为生产工作负载设计的。作为替代方案，我们推荐使用
    KEDA。KEDA（Kubernetes 事件驱动自动缩放）是一个开源项目，为 Kubernetes 提供事件驱动自动缩放功能。它在自动缩放中使用的自定义指标类型方面提供了更多灵活性。虽然
    Prometheus 适配器需要在 ConfigMap 内配置指标，但任何通过 Prometheus API 已公开的指标都可以在 KEDA 中使用，从而提供更流畅和友好的用户体验。它还提供了从
    0 到 0 的缩放功能，这是通过 HPAs 无法实现的，允许您在没有流量时完全关闭服务。但话虽如此，您无法从 0 缩放资源指标，如 CPU 和内存，以及由此扩展的
    GPU 指标，但在使用流量指标或队列进行缩放时很有用。
- en: Putting this all together, you’ll end up with the architecture shown in figure
    6.7\. Compared to figure 6.6, you’ll notice at the bottom that DCGM is managing
    our GPU metrics and feeding them into Prometheus Operator. From Prometheus, we
    can set up external dashboards with tools like Grafana. Internal to k8s, we’ll
    use KEDA to set up a custom.metrics.k9s.io API to return these metrics so we can
    autoscale based on the GPU metrics. KEDA has several CRDs, one of which is a `ScaledObject`,
    which creates the HPA and provides the additional features.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，您将得到图 6.7 所示的架构。与图 6.6 相比，您会注意到底部 DCGM 正在管理我们的 GPU 指标并将它们输入 Prometheus
    Operator。从 Prometheus，我们可以使用像 Grafana 这样的工具设置外部仪表板。在 k8s 内部，我们将使用 KEDA 来设置一个 custom.metrics.k9s.io
    API，以便返回这些指标，这样我们就可以根据 GPU 指标进行自动缩放。KEDA 有几个 CRDs，其中之一是 `ScaledObject`，它创建 HPA
    并提供额外的功能。
- en: '![figure](../Images/6-7.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-7.png)'
- en: Figure 6.7 Autoscaling based on a custom metric like GPU utilization requires
    several extra tools to work, including NVIDIA’s DCGM, a monitoring system like
    Prometheus Operator, and a custom metrics API like that provided by KEDA.
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.7 基于自定义指标（如 GPU 利用率）进行自动缩放需要几个额外的工具才能工作，包括 NVIDIA 的 DCGM、一个监控系统（如 Prometheus
    Operator）以及像 KEDA 提供的自定义指标 API。
- en: While autoscaling provides many benefits, it’s important to be aware of its
    limitations and potential problems, which are only exacerbated by LLM inference
    services. Proper configuration of the HPA is often an afterthought for many applications,
    but it becomes mission-critical when dealing with LLMs. LLMs take longer to become
    fully operational, as the GPUs need to be initialized and model weights loaded
    into memory; these aren’t services that can turn on a dime, which often can cause
    problems when scaling up if not properly prepared for. Additionally, if the system
    scales down too aggressively, it may result in instances being terminated before
    completing their assigned tasks, leading to data loss or other problems. Lastly,
    flapping is just such a concern that can arise from incorrect autoscaling configurations.
    Flapping happens when the number of replicas keeps oscillating, booting up a new
    service only to terminate it before it can serve any inferences.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自动扩展提供了许多好处，但了解其局限性和潜在问题很重要，这些问题在LLM推理服务中只会加剧。对于许多应用程序来说，HPA的正确配置通常是一个事后考虑的问题，但处理LLM时它变得至关重要。LLM需要更长的时间才能完全运行，因为GPU需要初始化，模型权重需要加载到内存中；这些不是可以瞬间开启的服务，如果没有正确准备，在扩展时可能会引起问题。此外，如果系统过于激进地缩小规模，可能会导致实例在完成分配的任务之前被终止，从而导致数据丢失或其他问题。最后，由于错误的自动扩展配置，可能会出现摆动现象。摆动发生在副本数量不断振荡时，启动一个新的服务，但又在它能够提供服务之前将其终止。
- en: 'There are essentially five parameters to tune when setting up an HPA:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置HPA时，实际上有五个参数需要调整：
- en: Target parameter
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标参数
- en: Target threshold
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标阈值
- en: Min pod replicas
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小Pod副本数
- en: Max pod replicas
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大Pod副本数
- en: Scaling policies
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展策略
- en: Let’s take a look at each of them in turn so you can be sure your system is
    properly configured.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一查看它们，以确保你的系统配置正确。
- en: Target parameter
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 目标参数
- en: The target parameter is the most important metric to consider when ensuring
    your system is properly configured. If you followed the previously listed steps
    in section 6.2.2, your system is now ready to autoscale based on GPU metrics,
    so this should be easy, right? Not so fast! Scaling based on GPU utilization is
    going to be the most common and straightforward path, but the first thing we need
    to do is ensure the GPU is the actual bottleneck in our service. It’s pretty common
    to see eager young engineers throw a lot of expensive GPUs onto a service but
    forget to include adequate CPU and memory capacity. CPU and memory will still
    be needed to handle the API layer, such as taking in requests, handling multiple
    threads, and communicating with the GPUs. If there aren’t enough resources, these
    layers can quickly become a bottleneck, and your application will be throttled
    way before the GPU utilization is ever affected, ensuring the system will never
    actually autoscale. While you could switch the target parameter on the autoscaler,
    CPU and memory are cheap compared to GPU resources, so it’d be better to allocate
    more of them for your application.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 目标参数是在确保系统配置正确时需要考虑的最重要指标。如果你已经按照第6.2.2节中列出的步骤操作，那么你的系统现在应该已经准备好根据GPU指标进行自动扩展，所以这应该很简单，对吧？但别急！基于GPU利用率的扩展将是最常见的直接路径，但我们需要做的第一件事是确保GPU确实是我们的服务中的实际瓶颈。我们经常看到一些急切的年轻工程师在服务上投入大量昂贵的GPU，但忘记包括足够的CPU和内存容量。CPU和内存仍然需要处理API层，例如接收请求、处理多个线程以及与GPU通信。如果没有足够的资源，这些层可能会迅速成为瓶颈，你的应用程序在GPU利用率受到影响之前就会被限制，从而确保系统永远不会真正进行自动扩展。虽然你可以在自动扩展器上切换目标参数，但与GPU资源相比，CPU和内存很便宜，所以为你的应用程序分配更多的它们会更好。
- en: In addition, there are cases where other metrics make more sense. If your LLM
    application takes most of its requests from a streaming or batch service, it can
    be more prudent to scale based on metrics that tell you a DAG is running or an
    upstream queue is filling up—especially if these metrics give you an early signal
    and allow you more time to scale up in advance.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有其他情况下其他指标更有意义。如果你的LLM应用程序的大部分请求来自流式或批量服务，那么基于告诉你DAG正在运行或上游队列正在填满的指标进行扩展可能更为谨慎——特别是如果这些指标给你提供了早期信号，并允许你提前更多时间进行扩展。
- en: Another concern when selecting the metric is its stability. For example, an
    individual GPU’s utilization tends to be close to either 0% or 100%. This can
    cause problems for the autoscaler, as the metric oscillates between an on and
    off state, as will its recommendation to add or remove replicas, causing flapping.
    Generally, flapping is avoided by taking the average utilization across all GPUs
    running the service. Using the average will stabilize the metric when you have
    a lot of GPUs, but it could still be a problem when the service has scaled down.
    If you are still running into problems, you’ll want to use an average-over-time
    aggregation, which will tell you the utilization for each GPU over a time frame—say,
    the last 5 minutes. For CPU utilization, average-over-time aggregation is built
    into the Kubernetes HPA and can be set with the `horizontal-pod-autoscaler-cpu-initialization-period`
    flag. For custom metrics, you’ll need to set it in your metric query (for Prometheus,
    it would be the `avg_over_ time` aggregation function).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择指标时，另一个需要考虑的问题是它的稳定性。例如，单个GPU的利用率往往接近0%或100%。这可能会给自动扩展器带来问题，因为指标在开启和关闭状态之间波动，其推荐添加或删除副本也会如此，从而导致摆动。通常，通过取所有运行服务的GPU的平均利用率来避免摆动。当您有很多GPU时，使用平均值将稳定指标，但当服务缩小时，这仍然可能是一个问题。如果您仍然遇到问题，您可能需要使用时间聚合的平均值，这将告诉您每个GPU在一段时间内的利用率——比如说，过去5分钟。对于CPU利用率，时间聚合的平均值是Kubernetes
    HPA内置的，可以通过`horizontal-pod-autoscaler-cpu-initialization-period`标志来设置。对于自定义指标，您需要在您的指标查询中设置它（对于Prometheus，将是`avg_over_time`聚合函数）。
- en: Lastly, it’s worth calling out that most systems allow you to autoscale based
    on multiple metrics. So you *could* autoscale based on both CPU and GPU utilization,
    as an example. However, we would recommend avoiding these setups unless you know
    what you are doing. Your autoscaler might be set up that way, but in actuality,
    your service will likely only ever autoscale based on just one of the metrics
    due to service load, and it’s best to make sure that metric is the more costly
    resource for cost-engineering purposes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得指出的是，大多数系统允许您根据多个指标进行自动扩展。例如，您可以根据CPU和GPU利用率进行自动扩展。然而，除非您知道自己在做什么，否则我们建议避免这些设置。您的自动扩展器可能就是这样设置的，但实际上，由于服务负载，您的服务可能只会根据一个指标进行自动扩展，并且最好确保这个指标是成本工程目的上更昂贵的资源。
- en: Target threshold
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 目标阈值
- en: 'The target threshold tells your service at what point to start upscaling. For
    example, if you are scaling based on the average GPU utilization and your threshold
    is set to 30, then a new replica will be booted up to take on the extra load when
    the average GPU utilization is above 30%. The formula that governs this is quite
    simple and is as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 目标阈值告诉您的服务在何时开始扩容。例如，如果您是基于平均GPU利用率进行扩展，并且阈值设置为30，那么当平均GPU利用率超过30%时，将启动一个新的副本来承担额外的负载。管理这一点的公式相当简单，如下所示：
- en: desiredReplicas = ceil[currentReplicas × (currentMetricValue / desiredMetricValue
    )]
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: desiredReplicas = ceil[currentReplicas × (currentMetricValue / desiredMetricValue
    )]
- en: NOTE  You can learn more about the algorithm at [https://mng.bz/x64g](https://mng.bz/x64g).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可以在[https://mng.bz/x64g](https://mng.bz/x64g)了解更多关于该算法的信息。
- en: This can be hard to tune in correctly, but here are some guiding principles.
    If the traffic patterns you see involve a lot of constant small bursts of traffic,
    a lower value, around 50, might be more appropriate. This setting ensures you
    start to scale up more quickly, avoiding unreliability problems, and you can also
    scale down more quickly, cutting costs. If you have a constant steady flow of
    traffic, higher values, around 80, will work well. Outside of testing your autoscaler,
    it’s best to avoid extremely low values, as they can increase your chances of
    flapping. You should also avoid extremely high values, as they may allow the active
    replicas to be overwhelmed before new ones start to boot up, which can cause unreliability
    or downtime. It’s also important to remember that due to the nature of pipeline
    parallel workflows when using a distributed GPU setup, there will always be a
    bubble, as discussed in section 3.3.2\. As a result, your system will never reach
    100% GPU utilization, and you will start to hit problems earlier than expected.
    Depending on how big your bubble is, you will need to adjust the target threshold
    accordingly.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能很难正确调整，但以下是一些指导原则。如果你看到的流量模式涉及大量恒定的微小流量突发，那么大约50的较低值可能更合适。这个设置确保你开始更快地扩展，避免不可靠的问题，你也可以更快地缩小规模，降低成本。如果你有一个恒定的稳定流量，那么大约80的较高值将工作得很好。在测试自动扩展器之外，最好避免极端的低值，因为它们可能会增加你的振荡机会。你还应该避免极端的高值，因为它们可能会在新的副本开始启动之前，让活动副本被压垮，这可能导致不可靠或停机。还重要的是要记住，由于使用分布式GPU设置时管道并行工作流程的性质，总会有一个气泡，如第3.3.2节所述。因此，你的系统永远不会达到100%的GPU利用率，你可能会比预期更早遇到问题。根据你的气泡有多大，你需要相应地调整目标阈值。
- en: Minimum pod replicas
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最小副本数量
- en: Minimum pod replicas determine the number of replicas of your service that will
    always be running. This setting is your baseline. It’s important to make sure
    it’s set slightly above your baseline of incoming requests. Too often, this is
    set strictly to meet baseline levels of traffic or just below, but a steady state
    for incoming traffic is rarely all that steady. This is where a lot of oscillating
    can happen, as you are more likely to see many small surges in traffic than large
    spikes. However, you don’t want to set it too high, as this will tie up valuable
    resources in the cluster and increase costs.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 最小副本数量决定了你的服务将始终运行的副本数量。这个设置是你的基线。确保它略高于你的入站请求基线是很重要的。这种情况经常被严格设定以满足基线流量水平或略低于基线，但入站流量的稳定状态很少真的那么稳定。这就是为什么可能会发生很多振荡，因为你更有可能看到许多小的流量激增而不是大的峰值。然而，你不想设置得太高，因为这会将宝贵的资源绑定在集群中并增加成本。
- en: Maximum pod replicas
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最大副本数量
- en: Maximum pod replicas determine the number of replicas your system will run at
    peak capacity. You should set this number to be just above your peak traffic requirements.
    Setting it too low could lead to reliability problems, performance degradation,
    and downtime during high-traffic periods. Setting it too high could lead to resource
    waste, running more pods than necessary, and delaying the detection of real problems.
    For example, if your application was under a DDoS attack, your system might scale
    to handle the load, but it would likely cost you severely and hide the problem.
    With LLMs, you also need to be cautious not to overload the underlying cluster
    and make sure you have enough resources in your quotas to handle the peak load.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最大副本数量决定了系统在峰值容量下将运行的副本数量。你应该将这个数字设置得略高于你的峰值流量需求。设置得太低可能会导致可靠性问题、性能下降和高流量期间的停机。设置得太高可能会导致资源浪费、运行不必要的更多副本，并延迟发现真实问题。例如，如果你的应用程序遭受了DDoS攻击，你的系统可能会扩展以处理负载，但这可能会让你付出高昂的代价并隐藏问题。在使用LLM时，你还需要小心不要过度负载底层集群，并确保你有足够的配额资源来处理峰值负载。
- en: Scaling policies
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缩放策略
- en: Scaling policies define the behavior of the autoscaler, allowing you to finetune
    how long to wait before scaling and how quickly it scales. This setting is usually
    ignored, and safely so for most setups because the defaults for these settings
    tend to be pretty good for the typical application. However, relying on the default
    would be a major mistake for an LLM service since it takes so long to deploy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放策略定义了自动扩展器的行为，允许你微调在缩放之前等待多长时间以及缩放的速度。这个设置通常被忽略，并且对于大多数设置来说，这是安全的，因为这些设置的默认值通常对典型应用程序来说相当不错。然而，对于LLM服务来说，依赖默认设置将是一个严重的错误，因为它的部署需要很长时间。
- en: The first setting you’ll want to adjust is the stabilization window, which determines
    how long to wait before taking a new scaling action. You can set a different stabilization
    window for upscaling and downscaling tasks. The default upscaling window is 0
    seconds, which should not need to be touched if your target parameter has been
    set correctly. The default downscaling window is 300 seconds, which is likely
    too short for our use case. You’ll typically want this at least as long as it
    takes your service to deploy and then a little bit more. Otherwise, you’ll be
    adding replicas only to remove them before they have a chance to do anything.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先想要调整的设置是稳定窗口，它决定了在采取新的缩放操作之前需要等待多长时间。你可以为升级和降级任务设置不同的稳定窗口。默认的升级窗口是0秒，如果你的目标参数已经设置正确，那么通常不需要调整。默认的降级窗口是300秒，对于我们的用例可能太短了。你通常希望这个时间至少与你的服务部署所需的时间一样长，然后再稍微长一点。否则，你只会添加副本，然后在它们有机会做任何事情之前就移除它们。
- en: The next parameter you’ll want to adjust is the scale-down policy, which defaults
    to 100% of pods every 15 seconds. As a result, any temporary drop in traffic could
    result in all your extra pods above the minimum being terminated immediately.
    For our case, it’s much safer to slow this down since terminating a pod takes
    only a few seconds, but booting one up can take minutes, making it a semi-irreversible
    decision. The exact policy will depend on your traffic patterns, but in general,
    we want to have a little more patience. You can adjust how quickly pods will be
    terminated and the magnitude by the number or percentage of pods. For example,
    we could configure the policy to allow only one pod each minute or 10% of pods
    every 5 minutes to be terminated.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 你接下来想要调整的参数是缩放策略，默认设置为每15秒将pods缩放到100%。因此，任何暂时的流量下降都可能导致所有超出最低需求的额外pods立即被终止。对于我们的情况，将其放缓要安全得多，因为终止一个pods只需要几秒钟，但启动一个可能需要几分钟，这使得它成为一个半不可逆的决定。确切的策略将取决于你的流量模式，但一般来说，我们希望有更多的耐心。你可以通过调整pods将被终止的速度和数量或百分比来调整。例如，我们可以配置策略，每分钟只允许一个pods被终止，或者每5分钟只允许10%的pods被终止。
- en: 6.2.3 Rolling updates
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 滚动更新
- en: Rolling updates or rolling upgrades is a strategy that gradually implements
    the new version of an application to reduce downtime and maximize agility. It
    works by gradually creating new instances and turning off the old ones, replacing
    them in a methodical manner. This update approach allows the system to remain
    functional and accessible to users even during the update process, otherwise known
    as zero downtime. Rolling updates also make it easier to catch bugs before they
    have too much effect and roll back faulty deployments.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新或滚动升级是一种策略，它逐步实施应用程序的新版本以减少停机时间并最大化敏捷性。它通过逐步创建新实例并关闭旧实例，以有组织的方式替换它们来实现。这种更新方法允许系统在更新过程中保持功能性和对用户的可访问性，也称为零停机时间。滚动更新还使得在它们产生太大影响之前更容易捕捉到错误，并回滚有缺陷的部署。
- en: Rolling updates is a feature built into k8s and another major reason for its
    widespread use and popularity. Kubernetes provides an automated and simplified
    way to carry out rolling updates. The rolling updates ensure that Kubernetes incrementally
    updates pod instances with new ones during deployment. The following listing shows
    an example LLM deployment implementing rolling updates; the relevant configuration
    is under the `spec.strategy` section.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新是k8s内置的功能，也是其广泛使用和受欢迎的另一个主要原因。Kubernetes提供了一个自动化和简化的方式来执行滚动更新。滚动更新确保在部署过程中Kubernetes以增量方式使用新实例更新pod实例。以下列表显示了一个实现滚动更新的LLM部署示例；相关的配置位于`spec.strategy`部分。
- en: Listing 6.9 Example deployment config with rolling update
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.9 带有滚动更新的示例部署配置
- en: '[PRE21]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You’ll notice that there are two main parameters you can adjust for a rolling
    update: `maxSurge` and `maxUnavailable`. These can either be set to a whole number,
    like in our example, describing the number of instances, or a fraction indicating
    a percentage of total instances. In the example, you’ll notice we set `maxSurge`
    to `1`, meaning even though we would normally run with five replicas, we could
    surge to six during a deployment, allowing us to turn on a new one before turning
    any off. Normally, you might want to set this higher, as it allows for a quicker
    rolling update. Otherwise, we’ll have to replace pods one at a time. The reason
    it’s low, you might have noticed, is that we are deploying a rather large LLM
    that requires eight GPUs. If these are A100s, it’s likely going to be hard to
    find an extra eight GPUs not being used.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，对于滚动更新，你可以调整两个主要参数：`maxSurge`和`maxUnavailable`。这些参数可以是整数，就像我们的例子中那样，描述实例的数量，也可以是表示总实例百分比的分数。在例子中，你会注意到我们将`maxSurge`设置为`1`，这意味着尽管我们通常运行五个副本，但在部署期间我们可以增加到六个，这样我们就可以在关闭任何副本之前开启一个新的副本。通常，你可能希望将其设置得更高，因为这允许更快的滚动更新。否则，我们不得不逐个替换Pod。你可能已经注意到，它的原因很低，是因为我们正在部署一个相当大的LLM，需要八个GPU。如果这些是A100s，那么找到额外八个未使用的GPU可能很困难。
- en: GPU resources cannot be shared among containers, and container orchestration
    can become a major challenge in such deployments, which is why `maxUnavailable`
    is set to `3`. What we are saying here is that three out of the five expected
    replicas can go down during a deployment. In other words, we are going to drop
    the total number of replicas for a little bit before re-creating them. For reliability
    reasons, we typically prefer adding extra replicas first, so to go down instead
    is a difficult decision, one you’ll want to confirm you can afford to do in your
    own deployment. The reason we are doing so here is to ensure that there are GPU
    resources available. In essence, to balance resource utilization, it might be
    necessary to set `maxUnavailable` to a high value and adjust `maxSurge` to a lower
    number to downscale old versions quickly and free up resources for new ones.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: GPU资源不能在容器之间共享，在这样部署中容器编排可能成为一个主要挑战，这就是为什么`maxUnavailable`被设置为`3`的原因。我们在这里说的是，在部署期间，五个预期的副本中有三个可以下线。换句话说，我们将在重新创建它们之前稍微减少副本的总数。出于可靠性的原因，我们通常更喜欢首先添加额外的副本，因此下线是一个艰难的决定，你将想要确认你可以在自己的部署中承担这个决定。我们之所以这样做，是为了确保有GPU资源可用。本质上，为了平衡资源利用率，可能有必要将`maxUnavailable`设置得较高，并将`maxSurge`调整到较低数值，以便快速缩减旧版本并为新版本释放资源。
- en: This advice is the opposite of what you’d do in most applications, so we understand
    if it makes you uneasy. If you’d like to ensure smoother deployments, you’ll need
    to budget for extra GPUs to be provisioned in your cluster strictly for deployment
    purposes. However, depending on how often you are updating the model itself, paying
    for expensive GPUs to sit idle simply to make deployments smoother may not be
    cost-advantageous. Often, the LLM itself doesn’t receive that many updates, so
    assuming you are using an inference graph (discussed in the next section), most
    of the updates will be to the API, prompts, or surrounding application.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这些建议与你在大多数应用程序中采取的做法相反，所以我们理解这可能会让你感到不安。如果你想确保部署更加顺畅，你将需要在你的集群中为部署目的预留额外的GPU。然而，根据你更新模型本身的频率，仅仅为了使部署更加顺畅而支付昂贵的GPU闲置费用可能并不划算。通常，LLM本身并不需要那么多的更新，所以假设你正在使用推理图（下一节讨论），大多数更新将是针对API、提示或周围应用程序的。
- en: In addition, we recommend you always perform such operations cautiously in a
    staging environment first to understand its effect. Catching a deployment problem
    in staging will save you a headache or two. It’s also useful to troubleshoot the
    `maxUnavailable` and `maxSurge` parameters in staging, but it’s often hard to
    get a one-to-one comparison to production since staging is often resource-constrained.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们建议你首先在一个预演环境中谨慎地执行此类操作，以了解其效果。在预演中捕捉到部署问题可以为你节省不少麻烦。在预演中调试`maxUnavailable`和`maxSurge`参数也是有用的，但由于预演通常资源受限，因此很难与生产环境进行一对一的比较。
- en: 6.2.4 Inference graphs
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.4 推理图
- en: 'Inference graphs are the crème filling of a donut, the muffin top of a muffin,
    and the toppings on a pizza: they are just phenomenal. Inference graphs allow
    us to create sophisticated flow diagrams at inference in a resource-saving way.
    Consider figure 6.8, which shows us the building blocks for any inference graph.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 推理图就像是甜甜圈的奶油馅、松饼的顶部或是披萨上的配料：它们真的是太棒了。推理图允许我们以节省资源的方式在推理时创建复杂的流程图。考虑图6.8，它展示了任何推理图的构建块。
- en: '![figure](../Images/6-8.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-8.png)'
- en: Figure 6.8 The three types of inference graph building blocks. Sequential allows
    us to run one model before the other, which is useful for preprocessing steps
    like generating embeddings. Ensembles allow us to pool several models together
    to learn from each and combine their results. Routing allows us to send traffic
    to specific models based on some criteria, often used for multi-armed bandit optimization.
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8展示了推理图构建块的三个类型。顺序型允许我们在其他模型之前运行一个模型，这对于预处理步骤，如生成嵌入很有用。集成型允许我们将多个模型组合在一起，从每个模型中学习并合并它们的结果。路由型允许我们根据某些标准将流量发送到特定的模型，通常用于多臂老虎机优化。
- en: 'Generally, any time you have more than one model, it’s useful to consider an
    inference graph architecture. Your standard LLM setup is usually already at least
    two models: an encoder and the language model itself.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，当你拥有多个模型时，考虑一个推理图架构是有用的。你的标准LLM设置通常已经至少包含两个模型：一个编码器以及语言模型本身。
- en: Usually, when we see LLMs deployed in the wild, these two models are deployed
    together. You send text data to your system, and it returns generated text. It’s
    often no big deal, but when deployed as a sequential inference graph instead of
    a packaged service, we get some added bonuses. First, the encoder is usually much
    faster than the LLM, so we can split them up since you may only need one encoder
    instance for every two to three LLM instances. Encoders are so small that this
    doesn’t necessarily help us out that much, but it saves the hassle of redeploying
    the entire LLM if we decide to deploy a new encoder model version. In addition,
    an inference graph will set up an individual API for each model, which allows
    us to hit the LLM and encoder separately. This is really useful if we have a bunch
    of data we’d like to preprocess and save in a VectorDB; we can use the same encoder
    we already have deployed. We can then pull this data and send it directly into
    the LLM.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们看到LLM在野外部署时，这两个模型通常是一起部署的。你将文本数据发送到你的系统，它返回生成的文本。这通常不是什么大问题，但当我们将其作为顺序推理图而不是打包服务部署时，我们会得到一些额外的优势。首先，编码器通常比LLM快得多，因此我们可以将它们分开，因为你可能只需要为每两个到三个LLM实例提供一个编码器实例。编码器如此小巧，这并不一定给我们带来太多帮助，但它可以节省我们重新部署整个LLM的麻烦。此外，推理图将为每个模型设置一个单独的API，这允许我们分别击中LLM和编码器。如果我们有一堆想要预处理并保存到VectorDB中的数据，这非常有用；我们可以使用已经部署的相同编码器。然后我们可以拉取这些数据并将其直接发送到LLM。
- en: The biggest benefit of an inference graph is that it allows us to separate the
    API and the LLM. The API sitting in front of the LLM is likely to change much
    more often as you tweak prompts, add features, and fix bugs. The ability to update
    the API without having to deploy the LLM will save your team a lot of effort.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 推理图的最大好处是它允许我们将API和LLM分开。位于LLM前面的API可能会随着你调整提示、添加功能和修复错误而频繁更改。无需部署LLM即可更新API的能力将为你的团队节省大量精力。
- en: Let’s now consider figure 6.9, which provides an example inference graph deployment
    using Seldon. In this example, we have an encoder model, an LLM, a classifier
    model, and a simple API that combines the results. Whereas we would have to build
    a container and the interface for each of these models, Seldon creates an orchestrator
    that handles communication between a user’s request and each node in the graph.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑图6.9，它提供了一个使用Seldon的示例推理图部署。在这个例子中，我们有一个编码器模型、一个LLM、一个分类器模型以及一个简单的API，它结合了这些结果。而如果我们必须为每个这些模型构建容器和接口，Seldon则创建了一个调度器，它处理用户请求与图中每个节点之间的通信。
- en: '![figure](../Images/6-9.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-9.png)'
- en: Figure 6.9 An example inference graph deployment using Seldon. A Seldon Deployment
    is a Kubernetes CRD that extends a regular Kubernetes deployment and adds an orchestrator
    that ensures the proper communication between all the models are run in graph
    order.
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.9展示了使用Seldon的示例推理图部署。Seldon部署是一个Kubernetes CRD，它扩展了常规的Kubernetes部署并添加了一个调度器，确保所有在图中运行的模型以正确的顺序进行通信。
- en: NOTE  Seldon is an open source platform designed for deploying and managing
    machine learning models in production. It offers tools and capabilities to help
    organizations streamline the deployment and scaling of machine learning and deep
    learning models in a Kubernetes-based environment. It offers k8s CRDs to implement
    inference graphs.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Seldon是一个开源平台，旨在用于在生产环境中部署和管理机器学习模型。它提供工具和能力，帮助组织在基于Kubernetes的环境中简化机器学习和深度学习模型的部署和扩展。它提供k8s
    CRDs以实现推理图。
- en: If you are wondering how to create this, listing 6.10 shows an example configuration
    that would create this exact setup. We simply define the containers in the graph
    and their relationship inside the graph. You’ll notice `apiVersion` defines the
    CRD from Seldon, which allows us to use `SeldonDeployment`, which is just an extension
    of the k8s regular Deployment object. In the listing, you might notice that the
    combiner is the parent to the LLM and classifier models, which feels backwards
    from how we visualize it in figure 6.9\. This is because a component will only
    ever have one parent, but can have multiple children, so a `COMBINER` is always
    a parent node even though functionally it’s the same. Setting up a graph can often
    be confusing, so I recommend you check the documentation frequently and often.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道如何创建这样的配置，列表6.10展示了创建此确切设置的示例配置。我们只需在图中定义容器及其在图中的关系。你会注意到`apiVersion`定义了来自Seldon的CRD，这允许我们使用`SeldonDeployment`，它只是k8s常规Deployment对象的扩展。在列表中，你可能注意到组合器是LLM和分类器模型的父节点，这与我们在图6.9中可视化的方式相反。这是因为一个组件将只有一个父节点，但可以有多个子节点，所以`COMBINER`始终是一个父节点，尽管在功能上它是相同的。设置图可能会很复杂，所以我建议你经常查阅文档。
- en: Listing 6.10 An example `SeldonDeployment` configuration file
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.10 一个示例`SeldonDeployment`配置文件
- en: '[PRE22]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If you’ve deployed enough machine learning systems, you’ve realized that many
    of them require complex systems, and inference graphs make it easy, or at least
    easier. And that is a big difference. Although inference graphs are a smarter
    way to deploy complex machine learning systems, it’s always important to ask yourself
    if the extra complexity is actually needed. Even with tools like inference graphs,
    it’s better to keep things simple whenever possible.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你部署了足够的机器学习系统，你就会意识到其中许多需要复杂的系统，推理图使得这变得简单，或者至少更容易。这是一个很大的区别。尽管推理图是部署复杂机器学习系统的一种更智能的方式，但始终重要的是要问自己额外的复杂性是否真的需要。即使有推理图这样的工具，在可能的情况下保持简单总是更好的。
- en: 6.2.5 Monitoring
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.5 监控
- en: As with any product or service deployed into production, monitoring is critical
    to ensure reliability, performance, and compliance to service level agreements
    and objectives are met. As with any service, we care about monitoring typical
    performance metrics like queries per second (QPS), latency, and response code
    counts. We also care about monitoring our resources with metrics like CPU utilization,
    percentage of memory used, GPU utilization, and GPU temperature, among many more.
    When any of these metrics start to fail, it often indicates a catastrophic failure
    of some sort and will need to be addressed quickly.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何部署到生产中的产品或服务一样，监控对于确保可靠性、性能以及满足服务级别协议和目标至关重要。就像任何服务一样，我们关心监控典型的性能指标，如每秒查询数（QPS）、延迟和响应代码计数。我们也关心使用CPU利用率、内存使用百分比、GPU利用率和GPU温度等指标来监控我们的资源，等等。当任何这些指标开始失败时，通常表明某种灾难性的故障，并且需要迅速解决。
- en: For these metrics, any software engineering team should have plenty of experience
    working with these using tools like Prometheus and Grafana or the ELK stack (Elasticsearch,
    Logstash, and Kibana). You will benefit immensely by taking advantage of the systems
    that are likely already in place. If they aren’t in place, we already went over
    how to set up the GPU metrics for monitoring back in section 6.2.2, and that system
    should be useful for monitoring other resources.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些指标，任何软件工程团队都应该有丰富的使用Prometheus和Grafana或ELK堆栈（Elasticsearch、Logstash和Kibana）等工具与之工作的经验。通过利用可能已经存在的系统，你将获得巨大的益处。如果这些系统尚未建立，我们已经在第6.2.2节中讨论了如何设置GPU指标以进行监控，并且该系统应该适用于监控其他资源。
- en: However, with any ML project, we have additional concerns that traditional monitoring
    tools miss, which leads to silent failures. This usually comes from data drift
    and performance decay, where a model continues to function but starts to do so
    poorly and no longer meets quality expectations. LLMs are particularly susceptible
    to data drift since language is in constant flux, as new words are created and
    old words change meaning all the time. Thus, we often need both a system monitoring
    solution and an ML monitoring solution.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于任何机器学习项目，我们还有传统监控工具未能涵盖的额外担忧，这会导致无声故障。这通常来自数据漂移和性能衰减，即模型继续运行，但开始表现不佳，不再满足质量预期。由于语言始终处于变化之中，新词不断出现，旧词的含义不断改变，LLM尤其容易受到数据漂移的影响。因此，我们通常需要一个系统监控解决方案和一个机器学习监控解决方案。
- en: Monitoring data drift is relatively easy and well-studied for numerical datasets,
    but monitoring unstructured text data provides an extra challenge. We’ve already
    discussed ways to evaluate language models in chapter 4, and we’ll need to use
    similar practices to evaluate and monitor models in production. One of our favorite
    tools for monitoring drift detection is whylogs due to its efficient nature of
    capturing summary statistics at scale. Adding LangKit to the mix instantly and
    easily allows us to track several useful metrics for LLMs, such as readability,
    complexity, toxicity, and even similarity scores to known prompt injection attacks.
    In the following listing, we demonstrate a simple application that logs and monitors
    text data using whylogs and LangKit.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值数据集，监控数据漂移相对容易且研究得很好，但对于非结构化文本数据的监控则提供额外的挑战。我们在第4章中已经讨论了评估语言模型的方法，我们将需要使用类似的方法来评估和监控生产中的模型。我们最喜欢的用于监控漂移检测的工具是whylogs，因为它在捕获大规模汇总统计方面的效率很高。将LangKit添加到其中可以立即且轻松地跟踪LLM的几个有用指标，例如可读性、复杂性、毒性和甚至与已知提示注入攻击的相似度得分。在下面的列表中，我们展示了使用whylogs和LangKit记录和监控文本数据的一个简单应用。
- en: Listing 6.11 Using whylogs and LangKit to monitor text data
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.11 使用whylogs和LangKit监控文本数据
- en: '[PRE23]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#1 Runs app manually'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 手动运行应用'
- en: '#2 Prevents truncation of columns'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 防止列截断'
- en: '#3 Gets the first profile and shows the results'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 获取第一个配置文件并显示结果'
- en: The generated text is
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本是
- en: '[PRE24]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: While this is just a demo using a text dataset, you can see how it would be
    beneficial to monitor the incoming prompts and outgoing generated text for metrics
    such as readability, complexity, and toxicity. These monitoring tools will help
    give you a handle on whether or not your LLM service is starting to fail silently.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这只是一个使用文本数据集的演示，但您可以看到监控输入提示和输出生成的文本（如可读性、复杂性和毒性等指标）将是有益的。这些监控工具将帮助您了解您的LLM服务是否开始出现无声故障。
- en: When monitoring in production, we must be mindful of the effect latency may
    have on our service. LangKit uses several lightweight models to evaluate the text
    for the advanced metrics. While we haven’t noticed significant memory effects,
    there is a very slight effect on latency when evaluating logs in the direct inference
    path. To avoid this, we can take it out of the inference path and into what is
    called a sidecar.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中监控时，我们必须注意延迟可能对我们服务的影响。LangKit使用几个轻量级模型来评估文本的高级指标。虽然我们没有注意到显著的内存影响，但在直接推理路径中评估日志时，对延迟有轻微的影响。为了避免这种情况，我们可以将其从推理路径中移除，进入所谓的sidecar。
- en: It’s not uncommon to see ML teams mistakenly place data quality checks in the
    critical path. Their intentions may be good (to ensure only clean data runs through
    a model), but on the off chance that a client sends bad data, it would often be
    better to just send a 400 or 500 error response than to add expensive latency
    costs to the good requests. In fact, many applications move monitoring out of
    the critical path entirely, opting to process it in parallel. The simplest way
    to do this is to use a Kubernetes sidecar, which is depicted in figure 6.10\.
    You can do this with tools that specialize in this, like fluentd; whylogs also
    offers a container you can run as a sidecar.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习团队中，错误地将数据质量检查放在关键路径上并不少见。他们的意图可能是好的（确保只有干净的数据通过模型），但在客户发送不良数据的情况下，发送400或500错误响应通常比向良好请求添加昂贵的延迟成本要好。事实上，许多应用程序将监控完全从关键路径中移除，选择并行处理。实现这一点的最简单方法是使用Kubernetes
    sidecar，如图6.10所示。您可以使用专门从事此工作的工具来完成此操作，例如fluentd；whylogs也提供可以运行的容器作为sidecar。
- en: '![figure](../Images/6-10.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-10.png)'
- en: Figure 6.10 An example Kubernetes sidecar container, which takes logging out
    of the critical path. The logging agent would be a tool like a whylogs container
    or fluentd that captures specific requests or all stdout print statements, processes
    them, and forwards them to a logging backend like WhyLabs or Prometheus.
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.10 一个示例 Kubernetes 侧边容器，它将日志从关键路径中移除。日志代理可能是一个像 whylogs 容器或 fluentd 这样的工具，它捕获特定的请求或所有
    stdout 打印语句，处理它们，并将它们转发到日志后端，如 WhyLabs 或 Prometheus。
- en: There are different sidecar configurations, but the main gist is that a logging
    container will run in the same k8s pod, and instead of the main app writing to
    a logs file, this sidecar acts as an intermediate step, first processing and cleaning
    the data, which it can then send directly to a backend or write to a logs file
    itself.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的侧边容器配置，但主要思想是日志容器将在同一个 k8s pod 中运行，而不是主应用程序直接写入日志文件，这个侧边容器充当一个中间步骤，首先处理和清理数据，然后可以直接发送到后端或将其写入日志文件。
- en: 'NOTE  You can learn more about Kubernetes logging architectures in its docs
    here: [https://mng.bz/Aaog](https://mng.bz/Aaog).'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可以在其文档中了解更多关于 Kubernetes 日志架构的信息：[https://mng.bz/Aaog](https://mng.bz/Aaog)。
- en: Now that we know more about setting up our infrastructure, including provisioning
    a cluster and implementing features like GPU autoscaling and monitoring, you should
    be set to deploy your LLM service and ensure it is reliable and scalable. Next,
    let’s talk about different challenges you are likely to face and methodologies
    to address these problems.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经更多地了解了设置我们的基础设施，包括提供集群和实施如 GPU 自动扩展和监控等功能，您应该准备好部署您的 LLM 服务并确保它是可靠和可扩展的。接下来，让我们谈谈您可能会遇到的不同挑战以及解决这些问题的方法。
- en: 6.3 Production challenges
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 生产挑战
- en: While we’ve covered how to get a service up and running, nevertheless, you will
    find a never-ending host of hurdles you’ll need to jump over when it comes to
    deploying models and maintaining them in production. Some of these challenges
    include updating, planning for large loads, poor latency, acquiring resources,
    and more. To help, we wanted to address some of the most common problems and give
    you tips on how to handle them.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经介绍了如何启动一个服务，但无论如何，你会在部署模型并维护它们在生产中的过程中遇到无数需要跨越的障碍。这些挑战包括更新、规划大量负载、低延迟、获取资源等等。为了帮助您，我们希望解决一些最常见的问题，并为您提供处理它们的技巧。
- en: 6.3.1 Model updates and retraining
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 模型更新和重新训练
- en: We recently discussed ML monitoring, watching your model for silent failures
    and data drift, but what do you do when you notice the model has gone belly up?
    We’ve seen in many traditional ML implementations that the answer is to simply
    retrain the model on the latest data and redeploy. And that works well when you
    are working with a small ARIMA model; in fact, we can often set up a CI/CD pipeline
    to run whenever our model degrades without any human oversight. But with a massive
    LLM? It doesn’t make any sense.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近讨论了机器学习监控，监控模型的无声故障和数据漂移，但当你注意到模型已经崩溃时，你该怎么办？我们在许多传统的机器学习实现中看到，答案是简单地使用最新数据重新训练模型并重新部署。当你处理一个小型的
    ARIMA 模型时，这很有效；事实上，我们通常可以设置一个 CI/CD 管道，在模型退化时自动运行，而不需要人工监督。但是，对于大规模的 LLM 来说，这并没有任何意义。
- en: Of course, we aren’t going to retrain from scratch, and we likely need to finetune
    our model, but the reason it doesn’t make sense is seen when we ask ourselves
    just what exactly the latest data is. The data we need to finetune the model is
    extremely important, and so it becomes necessary for us to take a step back and
    really diagnose the problem. What are the edge cases our model is failing on?
    What is it still doing well? How exactly have incoming prompts changed? Depending
    on the answers, we might not need to finetune at all. For example, consider a
    Q&A bot that is no longer effective at answering current event questions as time
    goes on. We probably don’t want to retrain a model on a large corpus of the latest
    news articles. Instead, we would get much better results by ensuring our RAG system
    is up to date. Similarly, there are likely plenty of times that simply tweaking
    prompts will do the trick.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不会从头开始重新训练，我们可能需要微调我们的模型，但当我们问自己最新数据究竟是什么时，不合理的理由就显现出来了。我们需要微调模型的数据非常重要，因此我们有必要退一步真正诊断问题。我们的模型在哪些边缘案例上失败了？它还在哪些方面做得很好？新来的提示具体是如何变化的？根据答案，我们可能根本不需要微调。例如，考虑一个随着时间的推移在回答当前事件问题方面不再有效的问答机器人。我们可能不想在大量最新的新闻文章语料库上重新训练模型。相反，通过确保我们的RAG系统是最新的，我们可能会得到更好的结果。同样，可能有很多时候，仅仅调整提示就能解决问题。
- en: In the cases where finetuning is the correct approach, you’ll need to think
    a lot about exactly what data you might be missing, as well as how any major updates
    might affect downstream systems, like finely tuned prompts. For example, when
    using knowledge distillation, this consideration can be particularly annoying.
    You will likely notice the problem in your student model but then must decide
    whether you need to retrain the student or the teacher. With any updates to the
    teacher model, you’ll need to ensure progress to the student model.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要微调的情况下，您需要仔细思考可能遗漏的确切数据，以及任何重大更新可能对下游系统（如精细调整的提示）产生的影响。例如，当使用知识蒸馏时，这种考虑可能会特别令人烦恼。您可能会在学生模型中注意到问题，但随后必须决定是否需要重新训练学生模型或教师模型。对于教师模型的任何更新，您都需要确保学生模型有进步。
- en: Overall, it’s best to take a proactive approach to LLM model updates instead
    of a purely reactionary one. A system that often works well is to establish business
    practices and protocols to update the model on a periodic basis, say once a quarter
    or once a month. During the time between updates, the team will focus on monitoring
    cases where the model performs poorly and gather appropriate data and examples
    to make updating smooth. This type of practice will help you prevent silent failures
    and ensure your model isn’t just maintained but improving.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，最好采取积极主动的方法来更新LLM模型，而不是仅仅采取反应式的方法。一个经常有效的方法是建立业务实践和协议，定期更新模型，比如每季度或每月更新一次。在更新之间，团队将专注于监控模型表现不佳的案例，并收集适当的数据和示例，以便更新过程顺利。这种做法将帮助您防止无声故障，并确保您的模型不仅得到维护，而且还在不断改进。
- en: 6.3.2 Load testing
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 负载测试
- en: Load testing is a type of performance testing that assesses how well a service
    or system will perform under—wait for it—load. The primary goal of load testing
    is to ensure the system can handle the expected workload without performance degradation
    or failure. Doing it early can ensure we avoid bottlenecks and scalability problems.
    Since LLM services can be both expensive and resource intensive, it’s even more
    important to ensure you load test the system before releasing your LLM application
    to production or before an expected peak in traffic, like during a Black Friday
    sales event.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 负载测试是一种性能测试，评估服务或系统在——等一下——负载下的表现。负载测试的主要目标是确保系统在性能下降或失败的情况下能够处理预期的负载。尽早进行测试可以确保我们避免瓶颈和可扩展性问题。由于LLM服务可能既昂贵又资源密集，因此在将LLM应用程序发布到生产环境之前，或者在预期的流量高峰期（如黑色星期五销售活动期间）之前进行负载测试就更加重要了。
- en: 'Load testing an LLM service, for the most part, is like load testing any other
    service and follows these basic steps:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLM服务的负载测试，大部分类似于对其他服务的负载测试，并遵循以下基本步骤：
- en: Set up the service in a staging environment.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预演环境中设置服务。
- en: Run a script to periodically send requests to the service.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行一个脚本，定期向服务发送请求。
- en: Increase requests until the service fails or autoscales.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加请求，直到服务失败或自动扩展。
- en: Log metrics.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录指标。
- en: Analyze results.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析结果。
- en: Which metrics you log depends on your service and what you are testing. The
    main metrics to watch are latency and throughput at failure, as these can be used
    to extrapolate to determine how many replicas you’ll need to handle peak load.
    Latency is the total time it takes for a request to be completed, and throughput
    tells us the queries per second (QPS), both of which are extremely important metrics
    when analyzing our system. Still, since many LLM services offer streaming responses,
    they don’t help us understand the user experience. A few more metrics you’ll want
    to capture to understand your perceived responsiveness are time to first token
    (TTFT) and tokens per second (TPS). TTFT gives us the perceived latency; it tells
    us how long it takes until the user starts to receive feedback, while TPS tells
    us how fast the stream is. For English, you’ll want a TPS of about 11 tokens per
    second, which is a little faster than most people read. If it’s slower than this,
    your users might get bored as they wait for tokens to be returned.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 你记录哪些指标取决于你的服务和你要测试的内容。主要需要关注的指标是失败时的延迟和吞吐量，因为这些可以用来外推以确定你需要多少副本来处理峰值负载。延迟是请求完成所需的总时间，吞吐量告诉我们每秒的查询数（QPS），这两个指标在分析我们的系统时都非常重要。然而，由于许多LLM服务提供流式响应，它们并不能帮助我们了解用户体验。为了理解你的感知响应速度，你还需要捕获一些其他指标，比如首次标记时间（TTFT）和每秒标记数（TPS）。TTFT给我们提供了感知延迟；它告诉我们用户开始收到反馈需要多长时间，而TPS告诉我们流的速度。对于英语，你希望每秒大约有11个标记，这比大多数人阅读的速度要快。如果它比这个慢，当用户等待标记返回时，他们可能会感到无聊。
- en: Related to TPS, I’ve seen several tools or reports use the inverse metric, time
    per output token (TPOT), or intertoken latency (ITL), but we’re not a fan of these
    metrics or their hard-to-remember names. You’ll also want to pay attention to
    resource metrics, CPU and GPU utilization, and memory usage. You’ll want to ensure
    these aren’t being hammered under base load conditions, as this can lead to hardware
    failures. These are also key to watch when you are testing autoscaling performance.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 与TPS相关，我见过一些工具或报告使用逆指标，即每输出标记时间（TPOT）或标记间延迟（ITL），但我们并不喜欢这些指标或它们难以记住的名称。你还需要注意资源指标，如CPU和GPU利用率以及内存使用情况。你想要确保这些指标在基本负载条件下不会被过度使用，因为这可能导致硬件故障。这些也是你在测试自动扩展性能时需要关注的关键指标。
- en: One of my favorite tools for load testing is Locust. Locust is an open source
    load-testing tool that makes it easy to scale and distribute running load tests
    over multiple machines, allowing you to simulate millions of users. Locust does
    all the hard work for you and comes with many handy features, like a nice web
    user interface and the ability to run custom load shapes. It’s easy to run in
    Docker or Kubernetes, making it extremely accessible to run where you need it—in
    production. The only main downside we’ve run across is that it doesn’t support
    customizable metrics, so we’ll have to roll our own to add TTFT and TPS.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢的负载测试工具之一是Locust。Locust是一个开源的负载测试工具，它使得在多台机器上扩展和分发运行负载测试变得容易，允许你模拟数百万用户。Locust为你做了所有艰苦的工作，并附带了许多实用的功能，如友好的Web用户界面和运行自定义负载形状的能力。它在Docker或Kubernetes上运行非常容易，这使得它在需要的地方运行变得极其方便——在生产环境中。我们遇到的主要缺点是它不支持可定制的指标，因此我们不得不自己编写代码来添加TTFT和TPS。
- en: To get started, simply `pip` `install` `locust`. Next, we’ll create our test.
    In listing 6.12, we show how to create a locust file that will allow users to
    prompt an LLM streaming service. It’s a bit more complicated than many locust
    files we’ve used simply because we need to capture our custom metrics for streaming,
    so you can imagine how straightforward they normally are. Locust already captures
    a robust set of metrics, so you won’t have to deal with this often. You’ll notice
    in the listing that we are saving these custom metrics to `stats.csv` file, but
    if you were running Locust in a distributed fashion, it’d be better to save it
    to a database of some sort.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，只需`pip` `install` `locust`。接下来，我们将创建我们的测试。在列表6.12中，我们展示了如何创建一个locust文件，该文件将允许用户提示LLM流式服务。它比我们使用的许多locust文件都要复杂，因为我们需要捕获我们的自定义指标以进行流式处理，所以你可以想象它们通常是多么简单。Locust已经捕获了一组强大的指标，所以你不必经常处理这个问题。你会在列表中注意到，我们将这些自定义指标保存到`stats.csv`文件中，但如果你以分布式方式运行Locust，最好将其保存到某种数据库中。
- en: Listing 6.12 Load testing with Locust
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.12 使用Locust进行负载测试
- en: '[PRE25]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 Creates a CSV file to store custom stats'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建CSV文件以存储自定义统计信息'
- en: '#2 Initiates the test'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 启动测试'
- en: '#3 Makes request'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 发送请求'
- en: '#4 Finishes and calculates the stats'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 完成并计算统计数据'
- en: '#5 Saves stats'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 保存统计数据'
- en: 'Before you run it, you’ll need to have an LLM service up. For this example,
    we’ll run the code from listing 6.3 in section 6.1.6, which spins up a very simple
    LLM service. With a service up and our test defined, we need to run it. To spin
    up the Locust service, run the `locust` command. You should then be able to navigate
    to the web UI in your browser. See the following example:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行之前，您需要有一个LLM服务正在运行。在这个例子中，我们将运行6.1.6节中的列表6.3中的代码，该代码启动一个非常简单的LLM服务。服务启动并且测试定义好之后，我们需要运行它。要启动Locust服务，请运行`locust`命令。然后您应该能够在浏览器中导航到Web界面。请参见以下示例：
- en: '[PRE26]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once in the web UI, you can explore running different tests; you’ll just need
    to point Locust at the host where your LLM service is running, which for us should
    be running on localhost on port 8000 or for the full socket address we combined
    them for: http://0.0.0.0:8000\. In figure 6.11, you can see an example test where
    we increased the active users to 50 at a spawn rate of 1 per second. You can see
    that on the hardware, this simple service starts to hit a bottleneck at around
    34 users, where the QPS starts to decrease, as it’s no longer able to keep up
    with the load. You’ll also notice response times slowly creep up in response to
    heavier load. We could continue to push the number of users up until we started
    to see failures, but overall, this test was informative and a great first test
    drive.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦进入Web界面，您可以探索运行不同的测试；您只需将Locust指向运行LLM服务的宿主机，对于我们来说，应该在本地主机上的端口8000运行，或者对于完整的套接字地址，我们将它们组合起来：http://0.0.0.0:8000。在图6.11中，您可以看到一个示例测试，我们以每秒1个的速度将活跃用户增加到50。您可以看到，在硬件上，这个简单的服务在约34个用户时开始遇到瓶颈，此时QPS开始下降，因为它已经无法跟上负载。您还会注意到，在更重的负载下，响应时间会逐渐上升。我们可以继续增加用户数量，直到我们看到失败，但总体来说，这个测试很有信息量，是一次很好的首次测试体验。
- en: '![figure](../Images/6-11.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/6-11.png)'
- en: Figure 6.11 Locust test interface demoing an example run increasing the number
    of users to 50 at a spawn rate of 1 per second. The requests per second peaks
    at 34 users, indicating a bottleneck for our service.
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.11 Locust测试界面演示了一个示例运行，以每秒1个的速度将用户数量增加到50。每秒请求数在34个用户时达到峰值，表明我们的服务存在瓶颈。
- en: 'In addition to manually running load tests, we can run Locust in a headless
    mode for automated tests. The following code is a simple command to run the exact
    same test as seen in figure 6.11; however, since we won’t be around to see the
    report, we’ll save the data to CSV files labeled with the prefix `llm` to be processed
    and analyzed later. There will be four files in addition to the stats CSV file
    we were already generating:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 除了手动运行负载测试外，我们还可以以无头模式运行Locust进行自动化测试。以下代码是一个简单的命令，用于运行与图6.11中看到的完全相同的测试；然而，由于我们不会在旁边看到报告，我们将数据保存到以`llm`为前缀的CSV文件中，以便稍后处理和分析。除了我们之前生成的统计数据CSV文件外，还将有四个文件：
- en: '[PRE27]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now that you are able to load test your LLM service, you should be able to figure
    out how many replicas you’ll need to meet throughput requirements. It’s just a
    matter of spinning up more services. But what do you do when you find out your
    service doesn’t meet latency requirements? Well, that’s a bit tougher, so let’s
    discuss it in the next section.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您能够对LLM服务进行负载测试，您应该能够确定需要多少个副本以满足吞吐量要求。这只是启动更多服务的问题。但是，当您发现服务无法满足延迟要求时，您该怎么办？嗯，这有点困难，所以让我们在下一节中讨论它。
- en: 6.3.3 Troubleshooting poor latency
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 故障排除：低延迟问题
- en: One of the biggest bottlenecks when it comes to your model’s performance in
    terms of latency and throughput has nothing to do with the model itself but comes
    from data transmission of the network. One of the simplest methods to improve
    this I/O constraint is to serialize the data before sending it across the wire,
    which can have a large effect on ML workloads where the payloads tend to be larger,
    including LLMs where prompts tend to be long.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到模型在延迟和吞吐量方面的性能时，最大的瓶颈之一与模型本身无关，而是来自网络的传输数据。改善这种I/O约束的最简单方法之一是在发送到网络之前序列化数据，这对ML工作负载有很大的影响，其中有效载荷往往较大，包括LLM，其中提示往往较长。
- en: To serialize the data, we utilize a framework known as Google Remote Procedure
    Call (gRPC). gRPC is an API protocol similar to REST, but instead of sending JSON
    objects, we compress the payloads into a binary serialized format using Protocol
    Buffers, also known as protobufs. By doing this, we can send more information
    in fewer bytes, which can easily give us orders of magnitude improvements in latency.
    Luckily, most inference services will implement gRPC along with their REST counterparts
    right out of the box, which is extremely convenient since the major hurdle to
    using gRPC is setting it up.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了序列化数据，我们利用一个名为 Google Remote Procedure Call (gRPC) 的框架。gRPC 是一个类似于 REST 的
    API 协议，但与发送 JSON 对象不同，我们使用 Protocol Buffers（也称为 protobufs）将有效载荷压缩成二进制序列化格式。通过这种方式，我们可以在更少的字节中发送更多信息，这可以轻易地给我们带来数量级的延迟改进。幸运的是，大多数推理服务都会直接实现
    gRPC 以及它们的 REST 对应版本，这非常方便，因为使用 gRPC 的主要障碍是设置它。
- en: A major reason for this convenience is the Seldon V2 Inference Protocol, which
    is widely implemented. The only hurdle, then, is ensuring our client can serialize
    and deserialize messages to take advantage of the protocol. In listing 6.13, we
    show an example client using MLServer to do this. It’s a little bit more in depth
    than your typical `curl` request, but a closer inspection shows the majority of
    the complexity is simply converting the data from different types as we serialize
    and deserialize it.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这种便利性的一个主要原因是 Seldon V2 推理协议，它得到了广泛的应用。那么，唯一的障碍就是确保我们的客户端可以序列化和反序列化消息以利用该协议。在列表
    6.13 中，我们展示了使用 MLServer 实现此功能的示例客户端。它比典型的 `curl` 请求要深入一些，但仔细检查会发现，大部分的复杂性仅仅是我们在序列化和反序列化数据时将数据从不同类型转换过来。
- en: Listing 6.13 Example client using gRPC
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.13 使用 gRPC 的示例客户端
- en: '[PRE28]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#1 Sets up the request structure via V2 Inference Protocol'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 通过 V2 推理协议设置请求结构'
- en: '#2 Serializes the request to the Protocol Buffer'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将请求序列化为 Protocol Buffer'
- en: '#3 Connects to the gRPC server'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 连接到 gRPC 服务器'
- en: '#4 Δeserializes the response and converts to the Python dictionary'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 反序列化响应并将其转换为 Python 字典'
- en: If you don’t use an inference service but want to implement a gRPC API, you’ll
    have to put down familiar tooling like FastAPI, which is strictly REST. Instead,
    you’ll likely want to use the grpcio library to create your API, and you’ll have
    to become familiar with .proto files to create your protobufs. It can be a relatively
    steep learning curve and beyond the scope of this book, but the advantages are
    well worth it.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想使用推理服务但想实现 gRPC API，你将不得不放下像 FastAPI 这样的熟悉工具，因为它是严格遵循 REST 的。相反，你可能会想使用
    grpcio 库来创建你的 API，并且你需要熟悉 .proto 文件来创建你的 protobufs。这可能是一个相对陡峭的学习曲线，并且超出了本书的范围，但优点是值得的。
- en: There are also plenty of other ideas to try if you are looking to squeeze out
    every last drop of performance. Another way to improve latency that shouldn’t
    be overlooked is ensuring you compile your model. We hammered this point pretty
    heavily at the beginning of this chapter, but it’s important to bring it up again.
    Next, be sure to deploy the model in a region or data center close to your users;
    this point is obvious to most software engineers, but for LLMs, we have to be
    somewhat wary, as the data center of choice may not have your accelerator of choice.
    Most cloud providers will be willing to help you with this, but it’s not always
    a quick and easy solution for them to install the hardware in a new location.
    Note that if you have to switch to a different accelerator to move regions, you’ll
    have to remember to compile your model all over again for the new hardware architecture!
    On that note, consider scaling up your accelerator. If you are currently opting
    for more price-effective GPUs but latency is becoming a bottleneck, paying for
    the latest and greatest can often speed up inference times.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找尽可能榨取性能的最后一滴，还有很多其他想法可以尝试。另一种提高延迟的方法是确保你编译了你的模型。我们在本章的开头就强调了这一点，但再次提出这一点是很重要的。接下来，确保将模型部署在靠近你的用户的区域或数据中心；这对大多数软件工程师来说是很明显的，但对于
    LLMs 来说，我们必须保持一定的警惕，因为我们选择的数据中心可能没有你选择的加速器。大多数云服务提供商都愿意帮助你，但对他们来说，在新的位置安装硬件并不总是快速和容易的解决方案。请注意，如果你必须切换到不同的加速器以移动区域，你必须记得为新硬件架构重新编译你的模型！关于这一点，考虑扩展你的加速器。如果你目前选择更具价格效益的
    GPU，但延迟正在成为瓶颈，支付最新和最好的设备通常可以加快推理时间。
- en: In addition, caching is always worth considering. It’s not likely, but on the
    off chance your users are often sending the same requests and the inputs can be
    easily normalized, you should implement caching. The fastest LLM is one we don’t
    actually run, so there’s no reason to run the LLM if you don’t have to. Also,
    we just went over this, but always be sure to load test and profile your service,
    making note of any bottlenecks, and optimize your code. Sometimes we make mistakes,
    and if the slowest process in the pipeline isn’t the actual LLM running inference,
    something is wrong. Last but not least, consider using a smaller model or an ensemble
    of them. It’s always been a tradeoff in ML deployments, but often sacrificing
    a bit of quality in the model or the accuracy of the results is acceptable to
    improve the overall reliability and speed of the service.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，缓存始终值得考虑。虽然可能性不大，但如果您的用户经常发送相同的请求并且输入可以轻松归一化，那么您应该实现缓存。最快的 LLM 是我们实际上没有运行的，所以如果您不需要运行
    LLM，就没有必要运行它。此外，我们刚刚讨论过这一点，但请务必确保加载测试和配置您的服务，注意任何瓶颈，并优化您的代码。有时我们会犯错误，如果管道中最慢的过程不是实际运行推理的
    LLM，那么就说明有问题。最后但同样重要的是，考虑使用更小的模型或它们的集合。在机器学习部署中，这始终是一个权衡，但通常在模型质量或结果准确性上做出一些牺牲，以提高服务的整体可靠性和速度是可以接受的。
- en: 6.3.4 Resource management
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 资源管理
- en: You’ve heard us say it a lot throughout the book, but we are currently in a
    GPU shortage, which has been true for almost the last 10 years, so we’re confident
    that when you read this sometime in the future, it will likely still be true.
    The truth is that the world can’t seem to get enough high-performance computing,
    and LLMs and generative AI are only the latest in a long list of applications
    that have driven up demand in recent years. It seems that once we seem to get
    a handle on supply, there’s another new reason for consumers and companies to
    want to use them.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 您在本书中已经多次听到我们说过，但我们目前正处于 GPU 缺货的状态，这几乎在过去 10 年中一直如此，所以我们有信心，当您在未来某个时候阅读这本书时，它可能仍然如此。事实是，世界似乎无法满足对高性能计算的需求，而
    LLM 和生成式 AI 只是近年来推动需求上升的众多应用中的最新应用。似乎一旦我们似乎掌握了供应，就会有另一个新的原因让消费者和公司想要使用它们。
- en: With this in mind, it’s best to consider strategies to manage these resources.
    One tool we’ve quickly become a big fan of is SkyPilot ([https://github.com/skypilot-org/skypilot](https://github.com/skypilot-org/skypilot)).
    SkyPilot is an open source project that aims to abstract away cloud infra burdens—in
    particular, maximizing GPU availability for your jobs. You use it by defining
    a task you want to run and then running the `sky` CLI command; it will search
    across multiple cloud providers, clusters, regions, and zones, depending on how
    you have it configured, until it finds an instance that meets your resource requirements
    and starts the job. Some common tasks are built-in, such as provisioning a GPU-backed
    Jupyter notebook.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，最好考虑管理这些资源的策略。我们很快成为其忠实粉丝的一个工具是 SkyPilot ([https://github.com/skypilot-org/skypilot](https://github.com/skypilot-org/skypilot))。SkyPilot
    是一个开源项目，旨在抽象化云基础设施的负担——特别是最大化您作业的 GPU 可用性。您通过定义要运行的任务，然后运行 `sky` CLI 命令来使用它；它将根据您的配置在多个云提供商、集群、区域和区域中进行搜索，直到找到满足您资源要求的实例并启动作业。一些常见任务已内置，例如配置一个基于
    GPU 的 Jupyter Notebook。
- en: 'If you recall, in chapter 5, we showed you how to set up a virtual machine
    (VM) to run multi-GPU environments with gcloud. Using SkyPilot, that gets simplified
    to one command:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得，在第 5 章中，我们向您展示了如何使用 gcloud 设置虚拟机（VM）以运行多 GPU 环境。使用 SkyPilot，这可以简化为一个命令：
- en: '[PRE29]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In addition to provisioning the VM, it also sets up port forwarding, which allows
    us to run Jupyter Notebook and access it through your browser. Pretty nifty!
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 除了配置虚拟机外，它还设置了端口转发，这使得我们能够运行 Jupyter Notebook 并通过您的浏览器访问它。非常巧妙！
- en: Another project to be on the watch for is Run:ai. Run:ai is a small startup
    that was aquired by NVIDIA for no small sum. It offers GPU optimization tooling,
    such as over quota provisioning, GPU oversubscription, and fractional GPU capabilities.
    It also helps you manage your clusters to increase GPU availability with GPU pooling,
    dynamic resource sharing, job scheduling, and more. What does all that mean? We’re
    not exactly sure, but their marketing team definitely sold us. Jokes aside, they
    offer a smarter way to manage your accelerators, and it’s very welcome. We expect
    we’ll see more competitors in this space in the future.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要关注的项目是Run:ai。Run:ai是一家小型初创公司，被NVIDIA以不菲的价格收购。它提供GPU优化工具，如超出配额的预配、GPU过度订阅和分数GPU能力。它还帮助你通过GPU池、动态资源共享、作业调度等方式管理你的集群，以增加GPU的可用性。这一切意味着什么？我们并不完全清楚，但他们的营销团队确实说服了我们。开个玩笑，他们提供了一种更智能地管理你的加速器的方法，这非常受欢迎。我们预计未来在这个领域将出现更多的竞争对手。
- en: 6.3.5 Cost engineering
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.5 成本工程
- en: When it comes to getting the most bang for your buck with LLMs, there’s lots
    to consider. In general, regardless of whether you deploy your own or pay for
    one in an API, you’ll be paying for the number of output tokens. For most paid
    services, this is a direct cost, but for your own service, it is often paid through
    longer inference times and extra compute time. In fact, it’s been suggested that
    simply adding “be concise” to your prompt can save you up to 90% of your costs.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到从LLM中获得最大价值时，有很多需要考虑的因素。一般来说，无论你是部署自己的还是通过API付费使用，你都将为输出令牌的数量付费。对于大多数付费服务，这是一个直接成本，但对你自己的服务来说，它通常是通过更长的推理时间和额外的计算时间来支付的。事实上，有人建议，只需在提示中添加“简洁”一词，就可以节省高达90%的成本。
- en: You’ll also save a lot by using text embeddings. We introduced RAG earlier,
    but what’s lost on many is that you don’t have to take the semantic search results
    and add them to your prompt to have your LLM “clean it up.” You could return the
    semantic search results directly to your user. It is much cheaper to look something
    up in a vector store than to ask an LLM to generate it. Simple neural information
    retrieval systems will save you significant amounts when doing simple fact lookups
    like, “Who’s the CEO of Twitter?” Self-hosting these embeddings should also significantly
    cut down the costs even further. If your users are constantly asking the same
    types of questions, consider taking the results of your LLM to these questions
    and storing them in your vector store for faster and cheaper responses.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本嵌入还可以节省很多。我们之前介绍了RAG，但许多人没有意识到，你不必将语义搜索结果添加到你的提示中，让LLM“清理”它们。你可以直接将语义搜索结果返回给用户。在向量存储中查找某物比要求LLM生成它要便宜得多。简单的神经网络信息检索系统在进行简单的事实查找时可以为你节省大量费用，例如，“Twitter的CEO是谁？”自行托管这些嵌入也应该可以进一步显著降低成本。如果你的用户不断提出相同类型的问题，考虑将LLM对这些问题的结果存储在你的向量存储中，以便更快、更便宜地响应。
- en: You also need to consider which model you should use for which task. Generally,
    bigger models are better at a wider variety of tasks, but if a smaller model is
    good enough for a specific job, you’ll save a lot by using it. For example, if
    we just assumed the price was linear to the number of parameters, you could run
    10 Llama-2-7b models for the same cost as 1 Llama-2-70b. We realize the cost calculations
    are more complicated than that, but it’s worth investigating.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要考虑为哪些任务使用哪种模型。一般来说，更大的模型在更广泛的任务上表现更好，但如果较小的模型足以完成特定工作，使用它就可以节省很多。例如，如果我们假设价格与参数数量成线性关系，你就可以用10个Llama-2-7b模型的成本运行1个Llama-2-70b模型。我们意识到成本计算比这更复杂，但值得调查。
- en: When comparing different LLM architectures, it’s not always just about size.
    Often, you’ll want to consider whether the architecture is supported for different
    quantization and compiling strategies. New architectures often boast impressive
    results on benchmarking leaderboards but lag behind when it comes to compiling
    and preparing them for production.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较不同的LLM架构时，并不仅仅是关于大小的问题。通常，你还需要考虑该架构是否支持不同的量化和编译策略。新的架构通常在基准测试排行榜上表现出令人印象深刻的结果，但在编译和准备它们用于生产时却落后。
- en: Next, you’ll need to consider the costs of GPUs to use when running. In general,
    you’ll want to use the least amount of GPUs needed to fit the model into memory
    to reduce the cost of idling caused by bubbles, as discussed in section 3.3.2\.
    Determining the correct number of GPUs isn’t always intuitive. For example, it’s
    cheaper to run four T4s than to run one A100, so it might be tempting to split
    up a large model onto smaller devices, but the inefficiency will often catch up
    to you. We have found that paying for newer, more expensive GPUs often saves us
    in the long run, as these GPUs tend to be more efficient and get the job done
    faster. This is particularly true when running batch inference. Ultimately, you’ll
    want to test different GPUs and find what configuration is cost optimal, as it
    will be different for every application.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要考虑运行时使用的GPU成本。一般来说，你希望使用最少的GPU数量来将模型放入内存，以减少由于气泡引起的闲置成本，正如第3.3.2节中讨论的那样。确定正确的GPU数量并不总是直观的。例如，运行四个T4比运行一个A100更便宜，所以可能会诱使你将大型模型分割到较小的设备上，但效率低下通常会追上你。我们发现，为较新、更昂贵的GPU付费通常从长远来看会节省我们费用，因为这些GPU通常更高效，并且能更快地完成任务。这在运行批量推理时尤其正确。最终，你将想要测试不同的GPU，并找到每个应用程序成本最优的配置，因为每个应用程序都不同。
- en: 'There are a lot of moving parts: model, service, machine instance, cloud provider,
    prompt, etc. While we’ve been trying to help you understand the best rules of
    thumb, you’ll want to test it out, which is where the cost engineering really
    comes into play. The simple way to test your cost efficiency is to create a matrix
    of your top choices; then, spin up a service for each combination and run your
    load testing. When you have an idea of how each instance runs under load and how
    much that particular instance will cost to run, you can then translate metrics
    like TPS to dollars per token (DTP). You’ll likely find that the most performant
    solution is rarely the most cost-optimal solution, but it gives you another metric
    to make a decision that’s best for you and your company.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多移动部件：模型、服务、机器实例、云提供商、提示等。虽然我们一直在尝试帮助你理解最佳的经验法则，但你仍需亲自测试，这正是成本工程真正发挥作用的地方。测试你的成本效率的简单方法是创建一个你首选选择的矩阵；然后，为每种组合启动一个服务并运行负载测试。当你了解每个实例在负载下的运行情况以及运行特定实例的成本时，你就可以将TPS等指标转换为每令牌美元（DTP）。你可能会发现，性能最佳解决方案很少是成本最优的解决方案，但它为你提供了一个额外的指标，以便做出对你和你的公司最有利的决策。
- en: 6.3.6 Security
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.6 安全性
- en: 'Security is always an undercurrent and a consideration when working in production
    environments. All the regular protocols and standard procedures should be considered
    when working with LLMs that you would consider for a regular app, like in-transit
    encryption with a protocol like HTTPS, authorization and authentication, activity
    monitoring and logging, network security, firewalls, and the list goes on—all
    of which could, and have, taken up articles, blog posts, and books of their own.
    When it comes to LLMs, you should worry about two big failure cases: an attacker
    gets an LLM agent to execute nefarious code, or an attacker gains access to proprietary
    data like passwords or secrets the LLM was trained on or has access to.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中工作，安全性始终是一个潜流和考虑因素。当与LLM（大型语言模型）一起工作时，应考虑所有常规协议和标准程序，就像在常规应用中一样，例如使用HTTPS等协议进行传输加密、授权和身份验证、活动监控和日志记录、网络安全、防火墙等等——所有这些都可以，并且已经占据了文章、博客文章和书籍。当涉及到LLM时，你应该担心两个主要失败案例：攻击者获取LLM代理以执行恶意代码，或者攻击者获得对专有数据的访问，如密码或LLM训练或访问的秘密。
- en: For the first concern, the best solution is to ensure the LLM is appropriately
    sandboxed for the use case for which it is employed. We are only worried about
    this attack when the LLM is used as an agent. In these cases, we often want to
    give an LLM a few more skills by adding tooling or plugins. For example, if you
    use an LLM to write your emails, why not just let it send the response too? A
    common case is letting the LLM browse the internet as an easy way to gather the
    latest news and find up-to-date information to generate better responses. These
    are all great options, but you should be aware that they allow the model to make
    executions. The ability to make executions is concerning because in the email
    example, without appropriate isolation and containment, a bad actor could send
    your LLM an email with a prompt injection attack that informs it to write malware
    and send it to all your other contacts.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个问题，最好的解决方案是确保LLM在它被使用的用例中得到了适当的沙箱化。我们只担心当LLM被用作代理时这种攻击。在这些情况下，我们经常希望通过添加工具或插件来给LLM赋予更多技能。例如，如果你使用LLM来撰写你的电子邮件，为什么不让它发送响应呢？一个常见的案例是让LLM浏览互联网，作为一种轻松获取最新新闻和查找最新信息以生成更好响应的方式。这些都是很好的选择，但你应该意识到，它们允许模型执行操作。能够执行操作的能力令人担忧，因为在电子邮件的例子中，如果没有适当的隔离和遏制，恶意行为者可以向你的LLM发送带有提示注入攻击的电子邮件，指示它编写恶意软件并将其发送给所有其他联系人。
- en: 'This point brings us to probably the biggest security threat to using LLMs:
    prompt injection. We talked about it in chapter 3, but as a refresher, a malicious
    user designs a prompt to allow them to perform unauthorized actions. We want to
    prevent users from gaining access to our company’s secret Coca-Cola recipe or
    whatever other sensitive data our LLM has been trained on or has access to.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这个观点带我们来到了使用LLM的最大安全威胁：提示注入。我们在第三章中讨论了它，但作为一个提醒，恶意用户设计了一个提示，允许他们执行未经授权的操作。我们希望阻止用户获取我们公司秘密的可口可乐配方或其他敏感数据，这些数据是我们LLM训练过的或可以访问的。
- en: Some standard best practices have come along to help combat this threat. The
    first is context-aware filtering, whether using keyword search or a second LLM
    to validate prompts. The idea is to validate the input prompt to see whether it’s
    asking for something it should not and/or the output prompt to see whether anything
    is being leaked that you don’t want to be leaked. However, a clever attacker will
    always be able to get around this defense, so you’ll want to include some form
    of monitoring to catch prompt injection and regularly update your LLM models.
    If trained appropriately, your model will inherently respond correctly, denying
    prompt injections. You’ve likely seen GPT-4 respond by saying, “Sorry, but I can’t
    assist with that,” which is a hallmark of good training. In addition, you’ll want
    to enforce sanitization and validation on any incoming text to your model.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 一些标准的最佳实践已经出现，以帮助对抗这种威胁。首先是上下文感知过滤，无论是使用关键词搜索还是使用第二个LLM来验证提示。其想法是验证输入提示，看看是否请求了不应该请求的内容，以及输出提示，看看是否有你不想泄露的信息被泄露。然而，一个聪明的攻击者总是会找到绕过这种防御的方法，所以你希望包括某种形式的监控来捕捉提示注入，并定期更新你的LLM模型。如果训练得当，你的模型将本能地正确响应，拒绝提示注入。你可能已经看到GPT-4通过说“抱歉，但我无法协助此事”来响应，这是良好训练的标志。此外，你希望对你的模型接收到的任何传入文本执行清理和验证。
- en: You should also consider language detection validation. Often, filtering systems
    and other precautions are only applied or trained in English, so a user who speaks
    a different language is often able to bypass these safeguards. The easiest way
    to stop this type of attack is to deny prompts that aren’t English or another
    supported language. If you take this approach, though, realize you’re greatly
    sacrificing usability and security costs, and safeguards have to be built for
    each language you intend to support. Also, you should know that most language
    detection algorithms typically identify only one language, so attackers often
    easily bypass these checks by simply writing a prompt with multiple languages.
    Alternatively, to filter out prompts in nonsupported languages, you can flag them
    for closer monitoring, which will likely help you find bad actors.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应该考虑语言检测验证。通常，过滤系统和其他预防措施仅在英语中应用或训练，因此说不同语言的用户通常能够绕过这些安全措施。阻止这种攻击的最简单方法就是拒绝非英语或其他受支持语言的提示。然而，如果您采取这种方法，请意识到您正在极大地牺牲可用性和安全成本，并且必须为每个打算支持的语言构建安全措施。此外，您应该知道，大多数语言检测算法通常只能识别一种语言，因此攻击者通常很容易通过简单地编写包含多种语言的提示来绕过这些检查。或者，为了过滤掉非受支持语言的提示，您可以将其标记为更密切的监控对象，这可能会帮助您找到不良行为者。
- en: These safeguards will greatly increase your security, but prompt injection can
    get quite sophisticated through adversarial attacks. Adversarial attacks are assaults
    on ML systems that take advantage of how they work, exploiting neural network
    architectures and black-box pattern matching. For example, random noise can be
    added to an image in such a way that the image appears the same to human eyes,
    but the pixel weights have been changed enough to fool an ML model to misclassify
    them. And it often doesn’t take much data. One author remembers being completely
    surprised after reading one study that showed attackers hacked models by only
    changing one pixel in an image![¹](#footnote-197) Imagine changing one pixel,
    and suddenly, the model thinks the frog is a horse. LLMs are, of course, also
    susceptible. Sightly change a prompt, and you’ll get completely different results.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这些安全措施将大大提高您的安全性，但通过对抗性攻击，注入攻击可能会变得相当复杂。对抗性攻击是对机器学习系统的攻击，利用了它们的工作方式，利用神经网络架构和黑盒模式匹配。例如，可以通过一种方式向图像中添加随机噪声，使得图像在人类眼中看起来相同，但像素权重已经改变得足够多，足以欺骗机器学习模型将其误分类。而且通常不需要太多数据。一位作者记得在阅读一项研究后感到完全惊讶，这项研究表明攻击者通过仅更改图像中的一个像素就黑入了模型![¹](#footnote-197)
    想象一下更改一个像素，突然，模型认为青蛙是马。当然，大型语言模型（LLMs）也容易受到攻击。稍微改变一下提示，你将得到完全不同的结果。
- en: The easiest way to set up an adversarial attack is to set up a script to send
    lots of different prompts and collect the responses. With enough data, an attacker
    can then train their own model on the dataset to effectively predict the right
    type of prompt to get the output they are looking for. Essentially, it just reverse
    engineers the model.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 设置对抗性攻击的最简单方法是设置一个脚本来发送大量不同的提示并收集响应。有了足够的数据，攻击者就可以在数据集上训练自己的模型，以有效地预测正确的提示类型以获得他们想要的输出。本质上，这只是在逆向工程模型。
- en: Another strategy to implement adversarial attacks is data poisoning. Here, an
    attacker adds malicious data to the training dataset that will alter how it performs.
    Data poisoning is so effective that tools like Nightshade help artists protect
    their art from being used in training datasets. With as few as 50 to 300 poisoned
    images, models like Midjourney or Stable Diffusions will start creating cat images
    when a user asks for a dog or cow images when asked to generate a car.[²](#footnote-198)
    Applied to LLMs, imagine a poisoned dataset that trains the model to ignore security
    protocols if a given code word or hash is in the prompt. This particular attack
    vector is effective on LLMs since they are often trained on large datasets that
    are not properly vetted or cleaned.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 实施对抗性攻击的另一种策略是数据中毒。在这里，攻击者向训练数据集中添加恶意数据，这将改变其性能。数据中毒的效果如此之好，以至于像Nightshade这样的工具帮助艺术家保护他们的艺术不被用于训练数据集。只需50到300张中毒图像，Midjourney或Stable
    Diffusions这样的模型就会在用户请求狗或牛的图像时开始创建猫的图像，当被要求生成汽车的图像时。[²](#footnote-198) 将其应用于LLMs，想象一下中毒数据集训练模型，如果提示中包含特定的代码词或哈希值，则忽略安全协议。这种特定的攻击向量对LLMs非常有效，因为它们通常是在未经适当审查或清理的大型数据集上训练的。
- en: 'Full disclosure: attackers don’t need sophisticated techniques to get prompt
    injection to work. Ultimately, an LLM is just a bot, so it doesn’t understand
    how or why it should keep secrets. We haven’t solved the prompt injection problem;
    we have only made it harder to do. For example, the authors have enjoyed playing
    games like *Gandalf* from Lakera.ai. In this game, you slowly go through seven
    to eight levels where more and more security measures are used to prevent you
    from stealing a password via prompt injection. While they do get progressively
    harder, needless to say, we’ve beaten all the levels. If there’s one thing we
    hope you take from this section, it’s that you should assume any data given to
    the model could be extracted. So if you decide to train a model on sensitive data
    or give it access to a VectorDB with sensitive data, you should plan on securing
    that model the same way you would the data—for example, keeping it for internal
    use and using least privilege best practices.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 完全坦白说：攻击者不需要复杂的技巧就能让提示注入工作。最终，LLM只是一个机器人，它不理解为什么或它应该如何保守秘密。我们没有解决提示注入问题；我们只是让它变得更难。例如，作者们喜欢玩Lakera.ai的*甘道夫*游戏。在这个游戏中，你慢慢地通过七个到八个级别，每个级别都会使用更多的安全措施来防止你通过提示注入窃取密码。虽然它们确实会越来越难，但不用说，我们已经通过了所有级别。如果你从这个章节中得到一点希望，那就是你应该假设模型接收到的任何数据都可能被提取。所以如果你决定在敏感数据上训练模型或给它访问包含敏感数据的VectorDB的权限，你应该像保护数据一样保护该模型——例如，将其保留为内部使用并使用最小权限最佳实践。
- en: 'We’ve just talked a lot about different production challenges, from updates
    and performance tuning to costs and security, but one production challenge deserves
    its own section: deploying LLMs to the edge. We’ll undertake a project in chapter
    10 to show you how to do just that, but let’s take a moment to discuss it beforehand.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚讨论了很多关于不同的生产挑战，从更新和性能调整到成本和安全，但有一个生产挑战值得单独成章：将LLM部署到边缘。在第10章中，我们将进行一个项目，向你展示如何做到这一点，但让我们先讨论一下。
- en: 6.4 Deploying to the edge
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 部署到边缘
- en: 'To be clear, you should not consider training anything on edge right now. You
    can, however, do ML development and inference on edge devices. The keys to edge
    development with LLMs are twofold: memory and speed. That should feel very obvious
    because they’re the same keys as running them normally. But what do you do when
    you have only 8 GB of RAM and no GPU, and you still need to have >1 token per
    second? As you can probably guess, there isn’t a uniformly good answer, but let’s
    discuss some good starting points.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确起见，你现在不应该考虑在边缘设备上训练任何东西。然而，你可以在边缘设备上进行机器学习开发和推理。使用大型语言模型（LLM）进行边缘开发的两个关键点是：内存和速度。这一点应该非常明显，因为它们与正常运行的相同关键点。但是，当你只有8GB的RAM和没有GPU，并且你仍然需要每秒处理超过1个token时，你该怎么办？正如你可能猜到的，并没有一个统一的解决方案，但让我们讨论一些好的起点。
- en: 'The biggest Raspberry Pi (rpi) on the market currently has 8 GB of RAM, no
    GPU, subpar CPU, and just a single board. This setup isn’t going to cut it. However,
    an easy solution exists to power your rpi with an accelerator for LLMs and other
    large ML projects: USB-TPUs like Coral. Keep in mind the hardware limitations
    of devices that use USB 3.0 being around 600MB/s, so it’s not going to be the
    same as inferencing on an A100 or better, but it’s going to be a huge boost in
    performance for your rpi using straight RAM for inference.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 目前市场上最大的树莓派（rpi）有8GB的RAM，没有GPU，CPU性能不佳，只有一个单板。这种配置是不够的。然而，有一个简单的解决方案可以为你的rpi提供LLM和其他大型机器学习项目的加速器：Coral这样的USB-TPU。记住，使用USB
    3.0的设备的硬件限制大约是600MB/s，所以它不会和A100或更好的推理一样，但它将为你的rpi使用直接RAM进行推理提供巨大的性能提升。
- en: If you plan on using a Coral USB accelerator, or any TPU, for that matter, keep
    in mind that because TPUs are a Google thing, you’ll need to convert both your
    model file and your inferencing code to use the TensorFlow framework. Earlier
    in the chapter, we discussed using Optimum to convert Hugging Face models to ONNX,
    and you can use this same library to convert our models to a .tflite, which is
    a compiled TensorFlow model format. This format will perform well on edge devices
    even without a TPU and twofold with TPU acceleration.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算使用Coral USB加速器，或者任何TPU，记住，因为TPU是谷歌的东西，你需要将你的模型文件和推理代码都转换为使用TensorFlow框架。在章节的早期，我们讨论了使用Optimum将Hugging
    Face模型转换为ONNX，你可以使用这个相同的库将我们的模型转换为.tflite，这是一个编译后的TensorFlow模型格式。这种格式在没有TPU的情况下也能在边缘设备上表现良好，并且在使用TPU加速时表现更佳。
- en: Alternatively, if buying both a single board and an accelerator seems like a
    hassle—because we all know the reason you bought a single board was to avoid buying
    two things to begin with—there are single boards that come with an accelerator.
    NVIDIA, for example, has its own single board with a GPU and CUDA called Jetson.
    With a Jetson or Jetson-like computer that uses CUDA, we don’t have to use TensorFlow,
    so that’s a major plus. ExecuTorch is the PyTorch offering for inferencing on
    edge devices.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你觉得同时购买一块单板和加速器很麻烦——因为我们都知道你购买单板的原因是为了避免一开始就购买两样东西——有一些单板自带加速器。例如，NVIDIA有自己的单板，带有GPU和CUDA，称为Jetson。使用CUDA的Jetson或类似计算机，我们不需要使用TensorFlow，这确实是一个很大的优点。ExecuTorch是PyTorch在边缘设备上进行推理的解决方案。
- en: Another edge device worth considering is that one in your pocket—that’s right,
    your phone. Starting with the iPhone X, the A11 chip came with the Apple Neural
    Engine accelerator. For Android, Google started offering an accelerator in their
    Pixel 6 phone with the Tensor chipset. Developing an iOS or Android app will be
    very different from working with a single board that largely runs versions of
    Linux; we won’t discuss it in this book, but it’s worth considering.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得考虑的边缘设备就是你口袋里的那个——没错，是你的手机。从iPhone X开始，A11芯片配备了Apple Neural Engine加速器。对于Android，Google开始在Pixel
    6手机中提供Tensor芯片组加速器。开发iOS或Android应用程序将与主要运行Linux版本的单板有很大不同；我们不会在本书中讨论它，但这是值得考虑的。
- en: Outside of hardware, several libraries and frameworks are also very cool and
    fast and make edge development easier. Llama.cpp, for example, is a C++ framework
    that allows you to take (almost) any Hugging Face model and convert it to the
    GGUF format. The GGUF format, created by the llama.cpp team, stores the model
    in a quantized fashion that makes it readily available to run on a CPU; it offers
    fast loading and inference on any device. Popular models like Llama, Mistral,
    and Falcon and even nontext models like Whisper are supported by llama.cpp at
    this point. It also supports LangChain integration for everyone using any of the
    LangChain ecosystem. Other libraries like GPTQ are focused more on performance
    than accessibility and are slightly harder to use, but they can result in boosts
    where it counts, especially if you’d like to end up inferencing on an Android
    phone or something similar. We will be exploring some of these libraries in much
    more detail later in the book.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 除了硬件之外，还有几个库和框架也非常酷且速度快，使边缘开发变得更容易。例如，Llama.cpp是一个C++框架，允许你将（几乎）任何Hugging Face模型转换为GGUF格式。由Llama.cpp团队创建的GGUF格式以量化方式存储模型，使其在CPU上运行变得容易；它为任何设备提供快速加载和推理。Llama、Mistral、Falcon等流行模型以及
    Whisper这样的非文本模型目前都由Llama.cpp支持。它还支持LangChain集成，为使用LangChain生态系统的每个人提供支持。其他如GPTQ之类的库更注重性能而非易用性，使用起来稍微困难一些，但它们可以在关键领域带来提升，特别是如果你希望最终在Android手机或类似设备上进行推理。我们将在本书的后面部分更详细地探讨一些这些库。
- en: We’ve gone over a lot in this chapter, and we hope you feel more confident in
    tackling deploying your very own LLM service. In the next chapter, we will discuss
    how to take better advantage of your service by building an application around
    it. We’ll dive deep into prompt engineering, agents, and frontend tooling.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经讨论了很多内容，我们希望你在部署你自己的LLM服务时更有信心。在下一章中，我们将讨论如何通过围绕它构建应用程序来更好地利用你的服务。我们将深入探讨提示工程、代理和前端工具。
- en: Summary
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Always compile your LLMs before putting them into production, as it improves
    efficiency, resource utilization, and cost savings.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将LLM投入生产之前，始终编译它们，因为这可以提高效率、资源利用率和节省成本。
- en: LLM APIs should implement batching, rate limiters, access keys, and streaming.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM API应该实现批处理、速率限制、访问密钥和流式传输。
- en: Retrieval-augmented generation is a simple and effective way to give your LLM
    context when generating content because it is easy to create and use.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索增强生成是一种简单而有效的方法，在生成内容时为你的LLM提供上下文，因为它易于创建和使用。
- en: LLM inference service libraries like vLLM, Hugging Face’s TGI, or OpenLLM make
    deploying easy but may not have the features you are looking for since they are
    so new.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于vLLM、Hugging Face的TGI或OpenLLM这样的LLM推理服务库使部署变得容易，但由于它们非常新，可能没有你需要的所有功能。
- en: 'Kubernetes is a tool that simplifies infrastructure by providing tooling like
    autoscaling and rolling updates:'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes是一个通过提供自动缩放和滚动更新等工具来简化基础设施的工具：
- en: Autoscaling is essential to improve reliability and cut costs by increasing
    or decreasing replicas based on utilization.
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动扩展对于通过根据利用率增加或减少副本来提高可靠性和降低成本至关重要。
- en: Rolling updates gradually implement updates to reduce downtime and maximize
    agility.
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动更新逐渐实施更新，以减少停机时间并最大化灵活性。
- en: Kubernetes doesn’t support GPU metrics out of the box, but by utilizing tools
    like DCGM, Prometheus, and KEDA, you can resolve this problem.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 默认不支持 GPU 指标，但通过利用 DCGM、Prometheus 和 KEDA 等工具，你可以解决这个问题。
- en: Seldon is a tool that improves deploying ML models and can be used to implement
    inference graphs.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seldon 是一个改进部署机器学习模型的工具，可以用来实现推理图。
- en: 'LLMs introduce some production challenges:'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs 引入了一些生产挑战：
- en: When your model drifts, first look to your prompts and RAG systems before attempting
    finetuning again.
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你的模型漂移时，在尝试再次微调之前，首先检查你的提示和 RAG 系统。
- en: Poor latency is difficult to resolve, but tools to help include gRPC, GPU optimization,
    and caching.
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较差的延迟难以解决，但可以帮助的工具包括 gRPC、GPU 优化和缓存。
- en: Resource management and acquiring GPUs can be difficult, but tools like SkyPilot
    can help.
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源管理和获取 GPU 可能很困难，但像 SkyPilot 这样的工具可以帮助。
- en: Edge development, while hardware limited, is the new frontier of LLM serving,
    and hardware like the Jetson or Coral TPU is available to help.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然硬件有限，但边缘开发是 LLM 服务的全新前沿，Jetson 或 Coral TPU 等硬件可用于帮助。
- en: '[[1]](#footnote-source-1) J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack
    for fooling deep neural networks,” IEEE Transactions on Evolutionary Computation,
    2019;23(5):828–841, [https://doi.org/10.1109/tevc.2019.2890858](https://doi.org/10.1109/tevc.2019.2890858).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#footnote-source-1) J. Su, D. V. Vargas, 和 K. Sakurai, “用于欺骗深度神经网络的单像素攻击，”
    IEEE 交易进化计算，2019;23(5):828–841, [https://doi.org/10.1109/tevc.2019.2890858](https://doi.org/10.1109/tevc.2019.2890858).'
- en: '[[2]](#footnote-source-2) M. Heikkilä. “This new data poisoning tool lets artists
    fight back against generative AI,” MIT Technology Review, October 23, 2023, [https://mng.bz/RNxD](https://mng.bz/RNxD).'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#footnote-source-2) M. Heikkilä. “这个新的数据中毒工具让艺术家能够反击生成式 AI，” MIT 科技评论，2023
    年 10 月 23 日，[https://mng.bz/RNxD](https://mng.bz/RNxD).'
