- en: Chapter 13\. Processing Sequences Using RNNs and CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting the future is something you do all the time, whether you are finishing
    a friend’s sentence or anticipating the smell of coffee at breakfast. In this
    chapter we will discuss recurrent neural networks (RNNs)—a class of nets that
    can predict the future (well, up to a point). RNNs can analyze time series data,
    such as the number of daily active users on your website, the hourly temperature
    in your city, your home’s daily power consumption, the trajectories of nearby
    cars, and more. Once an RNN learns past patterns in the data, it is able to use
    its knowledge to forecast the future, assuming, of course, that past patterns
    still hold in the future.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, RNNs can work on sequences of arbitrary lengths, rather than
    on fixed-sized inputs. For example, they can take sentences, documents, or audio
    samples as input, which makes them extremely useful for natural language processing
    applications such as automatic translation or speech-to-text.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will first go through the fundamental concepts underlying
    RNNs and how to train them using backpropagation through time. Then, we will use
    them to forecast a time series. Along the way, we will look at the popular autoregressive
    moving average (ARMA) family of models, often used to forecast time series, and
    use them as baselines to compare with our RNNs. After that, we’ll explore the
    two main difficulties that RNNs face:'
  prefs: []
  type: TYPE_NORMAL
- en: Unstable gradients (discussed in [Chapter 11](ch11.html#deep_chapter)), which
    can be alleviated using various techniques, including *recurrent dropout* and
    *recurrent layer normalization*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A (very) limited short-term memory, which can be extended using long short-term
    memory (LSTM) and gated recurrent unit (GRU) cells.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RNNs are not the only types of neural networks capable of handling sequential
    data. For small sequences, a regular dense network can do the trick, and for very
    long sequences, such as audio samples or text, convolutional neural networks can
    actually work quite well too. We will discuss both of these possibilities, and
    we will finish this chapter by implementing a WaveNet—a CNN architecture capable
    of handling sequences of tens of thousands of time steps. But we can do even better!
    In [Chapter 14](ch14.html#nlp_chapter), we will apply RNNs to natural language
    processing (NLP), and we will see how to boost them using attention mechanisms.
    Attention is at the core of transformers, which we will discover in [Chapter 15](ch15.html#transformer_chapter):
    they are now the state of the art for sequence processing, NLP, and even computer
    vision. But before we get there, let’s start with the simplest RNNs!'
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neurons and Layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to now we have focused on feedforward neural networks, where the activations
    flow only in one direction, from the input layer to the output layer. A recurrent
    neural network looks very much like a feedforward neural network, except it also
    has connections pointing backward.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the simplest possible RNN, composed of one neuron receiving inputs,
    producing an output, and sending that output back to itself (see [Figure 13-1](#simple_rnn_diagram),
    left). At each *time step* *t* (also called a *frame*), this *recurrent neuron*
    receives the inputs **x**[(*t*)] as well as its own output from the previous time
    step, *ŷ*[(*t*–1)]. Since there is no previous output at the first time step,
    it is generally set to zero. We can represent this tiny network against the time
    axis (see [Figure 13-1](#simple_rnn_diagram), right). This is called *unrolling
    the network through time* (it’s the same recurrent neuron represented once per
    time step).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of a recurrent neuron loop (left) and its unrolled representation
    over time (right), illustrating input and output flow across time steps.](assets/hmls_1301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. A recurrent neuron (left) unrolled through time (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can easily create a layer of recurrent neurons. At each time step *t*, every
    neuron receives both the input vector **x**[(*t*)] and the output vector from
    the previous time step **ŷ**[(*t*–1)], as shown in [Figure 13-2](#rnn_layer_diagram).
    Note that both the inputs and outputs are now vectors (when there was just a single
    neuron, the output was a scalar).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating a layer of recurrent neurons on the left and their unrolled
    representation through time on the right, showing input and output vectors at
    each time step.](assets/hmls_1302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. A layer of recurrent neurons (left) unrolled through time (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Each recurrent neuron has two sets of weights: one for the inputs **x**[(*t*)]
    and the other for the outputs of the previous time step, **ŷ**[(*t*–1)]. Let’s
    call these weight vectors **w**[*x*] and **w**[*ŷ*]. If we consider the whole
    recurrent layer instead of just one recurrent neuron, we can place all the weight
    vectors in two weight matrices: **W**[*x*] and **W**[*ŷ*].'
  prefs: []
  type: TYPE_NORMAL
- en: The output vector of the whole recurrent layer can then be computed pretty much
    as you might expect, as shown in [Equation 13-1](#rnn_output_equation), where
    **b** is the bias vector and *ϕ*(·) is the activation function (e.g., ReLU⁠^([1](ch13.html#id3062))).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 13-1\. Output of a recurrent layer for a single instance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $dollar-sign ModifyingAbove bold y With caret Subscript left-parenthesis t right-parenthesis
    Baseline equals phi left-parenthesis bold upper W Subscript x Baseline Superscript
    upper T Baseline bold x Subscript left-parenthesis t right-parenthesis Baseline
    plus bold upper W Subscript ModifyingAbove y With caret Baseline Superscript upper
    T Baseline ModifyingAbove bold y With caret Subscript left-parenthesis t minus
    1 right-parenthesis Baseline plus bold b right-parenthesis dollar-sign$
  prefs: []
  type: TYPE_NORMAL
- en: Just as with feedforward neural networks, we can compute a recurrent layer’s
    output in one shot for an entire mini-batch by placing all the inputs at time
    step *t* into an input matrix **X**[(*t*)] (see [Equation 13-2](#rnn_output_vectorized_equation)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 13-2\. Outputs of a layer of recurrent neurons for all instances in
    a mini-batch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>ϕ</mi> <mn>(</mn> <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub> <mo>+</mo> <msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub> <msub><mi
    mathvariant="bold">W</mi> <mi>ŷ</mi></msub> <mo>+</mo> <mi mathvariant="bold">b</mi>
    <mn>)</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mi>ϕ</mi>
    <mn>(</mn> <mn>[</mn> <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">Ŷ</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mn>]</mn> <mi mathvariant="bold">W</mi> <mo>+</mo> <mi mathvariant="bold">b</mi>
    <mn>)</mn> <mtext>with</mtext> <mi mathvariant="bold">W</mi> <mo>=</mo> <mo>[</mo>
    <mtable><mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub></mtd></mtr>
    <mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>ŷ</mi></msub></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ŷ**[(*t*)] is an *m* × *n*[neurons] matrix containing the layer’s outputs
    at time step *t* for each instance in the mini-batch (*m* is the number of instances
    in the mini-batch, and *n*[neurons] is the number of neurons).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**X**[(*t*)] is an *m* × *n*[inputs] matrix containing the inputs for all instances
    (*n*[inputs] is the number of input features).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**W**[*x*] is an *n*[inputs] × *n*[neurons] matrix containing the connection
    weights for the inputs of the current time step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**W**[*ŷ*] is an *n*[neurons] × *n*[neurons] matrix containing the connection
    weights for the outputs of the previous time step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b** is a vector of size *n*[neurons] containing each neuron’s bias term.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weight matrices **W**[*x*] and **W**[*ŷ*] are often concatenated vertically
    into a single weight matrix **W** of shape (*n*[inputs] + *n*[neurons]) × *n*[neurons]
    (see the second line of [Equation 13-2](#rnn_output_vectorized_equation)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The notation [**X**[(*t*)] **Ŷ**[(*t*–1)]] represents the horizontal concatenation
    of the matrices **X**[(*t*)] and **Ŷ**[(*t*–1)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that **Ŷ**[(*t*)] is a function of **X**[(*t*)] and **Ŷ**[(*t*–1)], which
    is a function of **X**[(*t*–1)] and **Ŷ**[(*t*–2)], which is a function of **X**[(*t*–2)]
    and **Ŷ**[(*t*–3)], and so on. This makes **Ŷ**[(*t*)] a function of all the inputs
    since time *t* = 0 (that is, **X**[(0)], **X**[(1)], …​, **X**[(*t*)]). At the
    first time step, *t* = 0, there are no previous outputs, so they are typically
    assumed to be all zeros.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The idea of introducing backward connections (i.e., loops) in artificial neural
    networks dates back to the very origins of ANNs, but the first modern RNN architecture
    was [proposed by Michael I. Jordan in 1986](https://homl.info/jordanrnn).⁠^([2](ch13.html#id3066))
    At each time step, his RNN would look at the inputs for that time step, plus its
    own outputs from the previous time step. This is called *output feedback*.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Cells
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the output of a recurrent neuron at time step *t* is a function of all
    the inputs from previous time steps, you could say it has a form of *memory*.
    A part of a neural network that preserves some state across time steps is called
    a *memory cell* (or simply a *cell*). A single recurrent neuron, or a layer of
    recurrent neurons, is a very basic cell, capable of learning only short patterns
    (typically about 10 steps long, but this varies depending on the task). Later
    in this chapter, we will look at some more complex and powerful types of cells
    capable of learning longer patterns (roughly 10 times longer, but again, this
    depends on the task).
  prefs: []
  type: TYPE_NORMAL
- en: 'A cell’s state at time step *t*, denoted **h**[(*t*)] (the “h” stands for “hidden”),
    is a function of some inputs at that time step and its state at the previous time
    step: **h**[(*t*)] = *f*(**x**[(*t*)], **h**[(*t*–1)]). Its output at time step
    *t*, denoted **ŷ**[(*t*)], is also a function of the previous state and the current
    inputs, and typically it’s just a function of the current state. In the case of
    the basic cells we have discussed so far, the output is just equal to the state,
    but in more complex cells this is not always the case, as shown in [Figure 13-3](#hidden_state_diagram).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating how a cell''s hidden state and output evolve over time,
    with feedback loops showing the interaction between inputs, hidden states, and
    outputs at different time steps.](assets/hmls_1303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-3\. A cell’s hidden state and its output may be different
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The first modern RNN that fed back the hidden state rather than the outputs
    was [proposed by Jeffrey Elman in 1990](https://homl.info/elmanrnn).⁠^([3](ch13.html#id3069))
    This is called *state feedback*, and it’s the most common approach today.
  prefs: []
  type: TYPE_NORMAL
- en: Input and Output Sequences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An RNN can simultaneously take a sequence of inputs and produce a sequence
    of outputs (see the top-left network in [Figure 13-4](#seq_to_seq_diagram)). This
    type of *sequence-to-sequence network* is useful to forecast time series, such
    as your home’s daily power consumption: you feed it the data over the last *N*
    days, and you train it to output the power consumption shifted by one day into
    the future (i.e., from *N* – 1 days ago to tomorrow).'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you could feed the network a sequence of inputs and ignore all
    outputs except for the last one (see the top-right network in [Figure 13-4](#seq_to_seq_diagram)).
    This is a *sequence-to-vector network*. For example, you could feed the network
    a sequence of words corresponding to a movie review, and the network would output
    a sentiment score (e.g., from 0 [hate] to 1 [love]).
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, you could feed the network the same input vector over and over again
    at each time step and let it output a sequence (see the bottom-left network of
    [Figure 13-4](#seq_to_seq_diagram)). This is a *vector-to-sequence network*. For
    example, the input could be an image (or the output of a CNN), and the output
    could be a caption for that image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating different neural network architectures: sequence-to-sequence,
    sequence-to-vector, vector-to-sequence, and encoder-decoder models.](assets/hmls_1304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-4\. Sequence-to-sequence (top left), sequence-to-vector (top right),
    vector-to-sequence (bottom left), and encoder-decoder (bottom right) networks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Lastly, you could have a sequence-to-vector network, called an *encoder*, followed
    by a vector-to-sequence network, called a *decoder* (see the bottom-right network
    of [Figure 13-4](#seq_to_seq_diagram)). For example, this could be used for translating
    a sentence from one language to another. You would feed the network a sentence
    in one language, the encoder would convert this sentence into a single vector
    representation, and then the decoder would decode this vector into a sentence
    in another language. This two-step model, called an [*encoder-decoder*](https://homl.info/seq2seq),⁠^([4](ch13.html#id3076))
    works much better than trying to translate on the fly with a single sequence-to-sequence
    RNN (like the one represented at the top left): the last words of a sentence can
    affect the first words of the translation, so you need to wait until you have
    seen the whole sentence before translating it. We will go through the implementation
    of an encoder-decoder in [Chapter 14](ch14.html#nlp_chapter) (as you will see,
    it is a bit more complex than what [Figure 13-4](#seq_to_seq_diagram) suggests).'
  prefs: []
  type: TYPE_NORMAL
- en: This versatility sounds promising, but how do you train a recurrent neural network?
  prefs: []
  type: TYPE_NORMAL
- en: Training RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To train an RNN, the trick is to unroll it through time (like we just did) and
    then use regular backpropagation (see [Figure 13-5](#bptt_diagram)). This strategy
    is called *backpropagation through time* (BPTT).
  prefs: []
  type: TYPE_NORMAL
- en: Just like in regular backpropagation, there is a first forward pass through
    the unrolled network (represented by the dashed arrows). Then the output sequence
    is evaluated using a loss function ℒ(**Y**[(0)], **Y**[(1)], …​, **Y**[(*T*)];
    **Ŷ**[(0)], **Ŷ**[(1)], …​, **Ŷ**[(*T*)]) (where **Y**[(*i*)] is the *i*^(th)
    target, **Ŷ**[(*i*)] is the *i*^(th) prediction, and *T* is the max time step).
    Note that this loss function may ignore some outputs. For example, in a sequence-to-vector
    RNN, all outputs are ignored except for the very last one. In [Figure 13-5](#bptt_diagram),
    the loss function is computed based on the last three outputs only. The gradients
    of that loss function are then propagated backward through the unrolled network
    (represented by the solid arrows). In this example, since the outputs **Ŷ**[(0)]
    and **Ŷ**[(1)] are not used to compute the loss, the gradients do not flow backward
    through them; they only flow through **Ŷ**[(2)], **Ŷ**[(3)], and **Ŷ**[(4)]. Moreover,
    since the same parameters **W** and **b** are used at each time step, their gradients
    will be tweaked multiple times during backprop. Once the backward phase is complete
    and all the gradients have been computed, BPTT can perform a gradient descent
    step to update the parameters (this is no different from regular backprop).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating backpropagation through time in an unrolled RNN, showing
    forward and backward passes with gradients flowing only through certain outputs
    for loss computation.](assets/hmls_1305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-5\. Backpropagation through time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fortunately, PyTorch takes care of all of this complexity for you, as you will
    see. But before we get there, let’s load a time series and start analyzing it
    using classical tools to better understand what we’re dealing with, and to get
    some baseline metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting a Time Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All right! Let’s pretend you’ve just been hired as a data scientist by Chicago’s
    Transit Authority. Your first task is to build a model capable of forecasting
    the number of passengers that will ride on bus and rail the next day. You have
    access to daily ridership data since 2001\. Let’s walk through how you would handle
    this. We’ll start by loading and cleaning up the data:^([5](ch13.html#id3084))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We load the CSV file, set short column names, sort the rows by date, remove
    the redundant `total` column, and drop duplicate rows. Now let’s check what the
    first few rows look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2][PRE3] On January 1st, 2001, 297,192 people boarded a bus in Chicago,
    and 126,455 boarded a train. The `day_type` column contains `W` for **W**eekdays,
    `A` for S**a**turdays, and `U` for S**u**ndays or holidays.    Now let’s plot
    the bus and rail ridership figures over a few months in 2019, to see what it looks
    like (see [Figure 13-6](#daily_ridership_plot)):    [PRE4]  ![Line graph showing
    daily bus and rail ridership in Chicago from March to May 2019, illustrating regular
    peaks and troughs over time.](assets/hmls_1306.png)  ###### Figure 13-6\. Daily
    ridership in Chicago    Note that Pandas includes both the start and end month
    in the range, so this plots the data from the 1st of March all the way up to the
    31st of May. This is a *time series*: data with values at different time steps,
    usually at regular intervals. More specifically, since there are multiple values
    per time step, this is called a *multivariate time series*. If we only looked
    at the `bus` column, it would be a *univariate time series*, with a single value
    per time step. Predicting future values (i.e., forecasting) is the most typical
    task when dealing with time series, and this is what we will focus on in this
    chapter. Other tasks include imputation (filling in missing past values), classification,
    anomaly detection, and more.    Looking at [Figure 13-6](#daily_ridership_plot),
    we can see that a similar pattern is clearly repeated every week. This is called
    a weekly *seasonality*. In fact, it’s so strong in this case that forecasting
    tomorrow’s ridership by just copying the values from a week earlier will yield
    reasonably good results. This is called *naive forecasting*: simply copying a
    past value to make our forecast. Naive forecasting is often a great baseline,
    and it can even be tricky to beat in some cases.    ###### Note    In general,
    naive forecasting means copying the latest known value (e.g., forecasting that
    tomorrow will be the same as today). However, in our case, copying the value from
    the previous week works better, due to the strong weekly seasonality.    To visualize
    these naive forecasts, let’s overlay the two time series (for bus and rail) as
    well as the same time series lagged by one week (i.e., shifted toward the right)
    using dotted lines. We’ll also plot the difference between the two (i.e., the
    value at time *t* minus the value at time *t* – 7); this is called *differencing*
    (see [Figure 13-7](#differencing_plot)):    [PRE5]  ![Two-panel plot showing the
    original and 7-day lagged time series for bus and rail (top), and the differences
    between them (bottom), demonstrating autocorrelation patterns.](assets/hmls_1307.png)  ######
    Figure 13-7\. Time series overlaid with 7-day lagged time series (top), and difference
    between *t* and *t* – 7 (bottom)    Not too bad! Notice how closely the lagged
    time series track the actual time series. When a time series is correlated with
    a lagged version of itself, we say that the time series is *autocorrelated*. As
    you can see, most of the differences are fairly small, except at the end of May.
    Maybe there was a holiday at that time? Let’s check the `day_type` column:    [PRE6]   [PRE7][PRE8]``py[PRE9]`py`
    [PRE10] [PRE11]`` [PRE12]` It looks like our `TimeSeriesDataset` class works fine!
    Now we can create a `DataLoader` for this tiny dataset, shuffling the windows
    and grouping them into batches of two:    [PRE13][PRE14]`` `>>>` `for` `X``,`
    `y` `in` `my_loader``:` [PRE15] `...` `` `X: tensor([[[0], [1], [2]], [[2], [3],
    [4]]])  y: tensor([[3], [5]])` `X: tensor([[[1], [2], [3]]])  y: tensor([[4]])`
    `` [PRE16]` [PRE17][PRE18]   [PRE19] [PRE20]`py [PRE21]py`` [PRE22]py` [PRE23]py
    [PRE24]py[PRE25][PRE26][PRE27][PRE28]py[PRE29]py`  [PRE30]'
  prefs: []
  type: TYPE_NORMAL
