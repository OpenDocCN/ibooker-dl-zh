- en: Chapter 13\. Processing Sequences Using RNNs and CNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章\. 使用RNNs和CNNs处理序列
- en: Predicting the future is something you do all the time, whether you are finishing
    a friend’s sentence or anticipating the smell of coffee at breakfast. In this
    chapter we will discuss recurrent neural networks (RNNs)—a class of nets that
    can predict the future (well, up to a point). RNNs can analyze time series data,
    such as the number of daily active users on your website, the hourly temperature
    in your city, your home’s daily power consumption, the trajectories of nearby
    cars, and more. Once an RNN learns past patterns in the data, it is able to use
    its knowledge to forecast the future, assuming, of course, that past patterns
    still hold in the future.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 预测未来是你在日常生活中经常做的事情，无论是完成朋友的句子还是期待早餐时咖啡的香气。在本章中，我们将讨论循环神经网络（RNNs）——一类能够预测未来的网络（好吧，至少在一定范围内）。RNNs可以分析时间序列数据，例如你网站上每日活跃用户数量、你城市的小时温度、你家的每日电力消耗、附近车辆的轨迹等等。一旦RNN学会了数据中的过去模式，它就能利用其知识来预测未来，当然，前提是过去模式在未来仍然成立。
- en: More generally, RNNs can work on sequences of arbitrary lengths, rather than
    on fixed-sized inputs. For example, they can take sentences, documents, or audio
    samples as input, which makes them extremely useful for natural language processing
    applications such as automatic translation or speech-to-text.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，RNNs可以在任意长度的序列上工作，而不是在固定大小的输入上。例如，它们可以接受句子、文档或音频样本作为输入，这使得它们在自然语言处理应用，如自动翻译或语音转文本中非常有用。
- en: 'In this chapter, we will first go through the fundamental concepts underlying
    RNNs and how to train them using backpropagation through time. Then, we will use
    them to forecast a time series. Along the way, we will look at the popular autoregressive
    moving average (ARMA) family of models, often used to forecast time series, and
    use them as baselines to compare with our RNNs. After that, we’ll explore the
    two main difficulties that RNNs face:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先将介绍RNNs的基本概念以及如何使用时间反向传播来训练它们。然后，我们将使用它们来预测时间序列。在这个过程中，我们将查看流行的自回归移动平均（ARMA）模型系列，这些模型通常用于预测时间序列，并将它们作为基准与我们的RNNs进行比较。之后，我们将探索RNNs面临的主要两个困难：
- en: Unstable gradients (discussed in [Chapter 11](ch11.html#deep_chapter)), which
    can be alleviated using various techniques, including *recurrent dropout* and
    *recurrent layer normalization*.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不稳定的梯度（在第11章[Chapter 11](ch11.html#deep_chapter)中讨论），可以使用各种技术来缓解，包括*循环dropout*和*循环层归一化*。
- en: A (very) limited short-term memory, which can be extended using long short-term
    memory (LSTM) and gated recurrent unit (GRU) cells.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （非常）有限的短期记忆，可以使用长短期记忆（LSTM）和门控循环单元（GRU）细胞来扩展。
- en: 'RNNs are not the only types of neural networks capable of handling sequential
    data. For small sequences, a regular dense network can do the trick, and for very
    long sequences, such as audio samples or text, convolutional neural networks can
    actually work quite well too. We will discuss both of these possibilities, and
    we will finish this chapter by implementing a WaveNet—a CNN architecture capable
    of handling sequences of tens of thousands of time steps. But we can do even better!
    In [Chapter 14](ch14.html#nlp_chapter), we will apply RNNs to natural language
    processing (NLP), and we will see how to boost them using attention mechanisms.
    Attention is at the core of transformers, which we will discover in [Chapter 15](ch15.html#transformer_chapter):
    they are now the state of the art for sequence processing, NLP, and even computer
    vision. But before we get there, let’s start with the simplest RNNs!'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs并不是唯一能够处理序列数据的神经网络类型。对于短序列，一个常规的密集网络就可以解决问题，而对于非常长的序列，例如音频样本或文本，卷积神经网络实际上也可以工作得相当好。我们将讨论这两种可能性，并在本章结束时实现一个WaveNet——一个能够处理数万个时间步序列的CNN架构。但我们可以做得更好！在第14章[Chapter 14](ch14.html#nlp_chapter)中，我们将应用RNNs到自然语言处理（NLP）中，并了解如何使用注意力机制来提升它们。注意力是transformers的核心，我们将在第15章[Chapter 15](ch15.html#transformer_chapter)中了解到：它们现在是序列处理、NLP甚至计算机视觉的尖端技术。但在我们到达那里之前，让我们从最简单的RNNs开始！
- en: Recurrent Neurons and Layers
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经元和层
- en: Up to now we have focused on feedforward neural networks, where the activations
    flow only in one direction, from the input layer to the output layer. A recurrent
    neural network looks very much like a feedforward neural network, except it also
    has connections pointing backward.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直关注的是前馈神经网络，其中激活只在一个方向上流动，从输入层到输出层。循环神经网络看起来非常像前馈神经网络，只不过它还有指向后方的连接。
- en: Let’s look at the simplest possible RNN, composed of one neuron receiving inputs,
    producing an output, and sending that output back to itself (see [Figure 13-1](#simple_rnn_diagram),
    left). At each *time step* *t* (also called a *frame*), this *recurrent neuron*
    receives the inputs **x**[(*t*)] as well as its own output from the previous time
    step, *ŷ*[(*t*–1)]. Since there is no previous output at the first time step,
    it is generally set to zero. We can represent this tiny network against the time
    axis (see [Figure 13-1](#simple_rnn_diagram), right). This is called *unrolling
    the network through time* (it’s the same recurrent neuron represented once per
    time step).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最简单的RNN，它由一个接收输入、产生输出并将该输出发送回自身的神经元组成（见图[图13-1](#simple_rnn_diagram)，左侧）。在每个时间步*t*（也称为*帧*），这个*循环神经元*接收输入**x**[(*t*)]以及前一个时间步的输出*ŷ*[(*t*–1)]。由于第一个时间步没有前一个输出，它通常被设置为零。我们可以将这个微小的网络与时间轴相对比（见图[图13-1](#simple_rnn_diagram)，右侧）。这被称为*通过时间展开网络*（它是在每个时间步表示一次的相同循环神经元）。
- en: '![Diagram of a recurrent neuron loop (left) and its unrolled representation
    over time (right), illustrating input and output flow across time steps.](assets/hmls_1301.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经元循环图（左侧）及其随时间展开的表示（右侧），展示了时间步之间的输入和输出流](assets/hmls_1301.png)'
- en: Figure 13-1\. A recurrent neuron (left) unrolled through time (right)
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-1\. 循环神经元（左侧）随时间展开（右侧）
- en: You can easily create a layer of recurrent neurons. At each time step *t*, every
    neuron receives both the input vector **x**[(*t*)] and the output vector from
    the previous time step **ŷ**[(*t*–1)], as shown in [Figure 13-2](#rnn_layer_diagram).
    Note that both the inputs and outputs are now vectors (when there was just a single
    neuron, the output was a scalar).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松创建一个循环神经元层。在每个时间步*t*，每个神经元都会接收到输入向量**x**[(*t*)]和前一个时间步的输出向量**ŷ**[(*t*–1)]，如图[图13-2](#rnn_layer_diagram)所示。请注意，现在输入和输出都是向量（当只有一个神经元时，输出是一个标量）。
- en: '![Diagram illustrating a layer of recurrent neurons on the left and their unrolled
    representation through time on the right, showing input and output vectors at
    each time step.](assets/hmls_1302.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图示左侧为展示输入和输出向量的每个时间步的循环神经元层，右侧为通过时间展开的表示](assets/hmls_1302.png)'
- en: Figure 13-2\. A layer of recurrent neurons (left) unrolled through time (right)
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-2\. 循环神经元层（左侧）随时间展开（右侧）
- en: 'Each recurrent neuron has two sets of weights: one for the inputs **x**[(*t*)]
    and the other for the outputs of the previous time step, **ŷ**[(*t*–1)]. Let’s
    call these weight vectors **w**[*x*] and **w**[*ŷ*]. If we consider the whole
    recurrent layer instead of just one recurrent neuron, we can place all the weight
    vectors in two weight matrices: **W**[*x*] and **W**[*ŷ*].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 每个循环神经元有两套权重：一套用于输入**x**[(*t*)]，另一套用于前一个时间步的输出**ŷ**[(*t*–1)]。让我们称这些权重向量为**w**[*x*]和**w**[*ŷ*]。如果我们考虑整个循环层而不是单个循环神经元，我们可以将所有权重向量放入两个权重矩阵中：**W**[*x*]和**W**[*ŷ*]。
- en: The output vector of the whole recurrent layer can then be computed pretty much
    as you might expect, as shown in [Equation 13-1](#rnn_output_equation), where
    **b** is the bias vector and *ϕ*(·) is the activation function (e.g., ReLU⁠^([1](ch13.html#id3062))).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 整个循环层的输出向量可以像你预期的那样计算，如[方程式13-1](#rnn_output_equation)所示，其中**b**是偏置向量，*ϕ*(·)是激活函数（例如，ReLU⁠^([1](ch13.html#id3062))）。
- en: Equation 13-1\. Output of a recurrent layer for a single instance
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式13-1\. 单个实例的循环层输出
- en: $dollar-sign ModifyingAbove bold y With caret Subscript left-parenthesis t right-parenthesis
    Baseline equals phi left-parenthesis bold upper W Subscript x Baseline Superscript
    upper T Baseline bold x Subscript left-parenthesis t right-parenthesis Baseline
    plus bold upper W Subscript ModifyingAbove y With caret Baseline Superscript upper
    T Baseline ModifyingAbove bold y With caret Subscript left-parenthesis t minus
    1 right-parenthesis Baseline plus bold b right-parenthesis dollar-sign$
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: $美元符号修改上标y带撇号下标左括号t右括号基线等于phi左括号上标W下标x基线上标T基线上标W下标修改上标y带撇号基线上标T基线修改上标y带撇号下标左括号t减1右括号基线加上上标b右括号美元符号$
- en: Just as with feedforward neural networks, we can compute a recurrent layer’s
    output in one shot for an entire mini-batch by placing all the inputs at time
    step *t* into an input matrix **X**[(*t*)] (see [Equation 13-2](#rnn_output_vectorized_equation)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前馈神经网络一样，我们可以通过将所有输入在时间步 *t* 放入一个输入矩阵 **X**[(*t*)]（见 [方程 13-2](#rnn_output_vectorized_equation)）中，一次性计算整个
    mini-batch 的循环层输出。
- en: Equation 13-2\. Outputs of a layer of recurrent neurons for all instances in
    a mini-batch
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 13-2\. mini-batch 中所有实例的循环神经元层输出
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>ϕ</mi> <mn>(</mn> <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub> <mo>+</mo> <msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub> <msub><mi
    mathvariant="bold">W</mi> <mi>ŷ</mi></msub> <mo>+</mo> <mi mathvariant="bold">b</mi>
    <mn>)</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mi>ϕ</mi>
    <mn>(</mn> <mn>[</mn> <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">Ŷ</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mn>]</mn> <mi mathvariant="bold">W</mi> <mo>+</mo> <mi mathvariant="bold">b</mi>
    <mn>)</mn> <mtext>with</mtext> <mi mathvariant="bold">W</mi> <mo>=</mo> <mo>[</mo>
    <mtable><mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub></mtd></mtr>
    <mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>ŷ</mi></msub></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>ϕ</mi> <mn>(</mn> <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub> <mo>+</mo> <msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub> <msub><mi
    mathvariant="bold">W</mi> <mi>ŷ</mi></msub> <mo>+</mo> <mi mathvariant="bold">b</mi>
    <mn>)</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mi>ϕ</mi>
    <mn>(</mn> <mn>[</mn> <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">Ŷ</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mn>]</mn> <mi mathvariant="bold">W</mi> <mo>+</mo> <mi mathvariant="bold">b</mi>
    <mn>)</mn> <mtext>with</mtext> <mi mathvariant="bold">W</mi> <mo>=</mo> <mo>[</mo>
    <mtable><mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub></mtd></mtr>
    <mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>ŷ</mi></msub></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
- en: 'In this equation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '**Ŷ**[(*t*)] is an *m* × *n*[neurons] matrix containing the layer’s outputs
    at time step *t* for each instance in the mini-batch (*m* is the number of instances
    in the mini-batch, and *n*[neurons] is the number of neurons).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ŷ**[(*t*)] 是一个 *m* × *n*[neurons] 的矩阵，包含 mini-batch 中每个实例在时间步 *t* 的层输出（*m*
    是 mini-batch 中的实例数量，*n*[neurons] 是神经元的数量）。'
- en: '**X**[(*t*)] is an *m* × *n*[inputs] matrix containing the inputs for all instances
    (*n*[inputs] is the number of input features).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X**[(*t*)] 是一个 *m* × *n*[inputs] 的矩阵，包含所有实例的输入（*n*[inputs] 是输入特征的数量）。'
- en: '**W**[*x*] is an *n*[inputs] × *n*[neurons] matrix containing the connection
    weights for the inputs of the current time step.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**[*x*] 是一个 *n*[inputs] × *n*[neurons] 的矩阵，包含当前时间步输入的连接权重。'
- en: '**W**[*ŷ*] is an *n*[neurons] × *n*[neurons] matrix containing the connection
    weights for the outputs of the previous time step.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**[*ŷ*] 是一个 *n*[neurons] × *n*[neurons] 的矩阵，包含前一时间步输出的连接权重。'
- en: '**b** is a vector of size *n*[neurons] containing each neuron’s bias term.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**b** 是一个大小为 *n*[neurons] 的向量，包含每个神经元的偏置项。'
- en: The weight matrices **W**[*x*] and **W**[*ŷ*] are often concatenated vertically
    into a single weight matrix **W** of shape (*n*[inputs] + *n*[neurons]) × *n*[neurons]
    (see the second line of [Equation 13-2](#rnn_output_vectorized_equation)).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重矩阵 **W**[*x*] 和 **W**[*ŷ*] 通常垂直连接成一个形状为 (*n*[inputs] + *n*[neurons]) × *n*[neurons]
    的单个权重矩阵 **W**（见 [方程 13-2](#rnn_output_vectorized_equation) 的第二行）。
- en: The notation [**X**[(*t*)] **Ŷ**[(*t*–1)]] represents the horizontal concatenation
    of the matrices **X**[(*t*)] and **Ŷ**[(*t*–1)].
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符号 [**X**[(*t*)] **Ŷ**[(*t*–1)]] 表示矩阵 **X**[(*t*)] 和 **Ŷ**[(*t*–1)] 的水平连接。
- en: Notice that **Ŷ**[(*t*)] is a function of **X**[(*t*)] and **Ŷ**[(*t*–1)], which
    is a function of **X**[(*t*–1)] and **Ŷ**[(*t*–2)], which is a function of **X**[(*t*–2)]
    and **Ŷ**[(*t*–3)], and so on. This makes **Ŷ**[(*t*)] a function of all the inputs
    since time *t* = 0 (that is, **X**[(0)], **X**[(1)], …​, **X**[(*t*)]). At the
    first time step, *t* = 0, there are no previous outputs, so they are typically
    assumed to be all zeros.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 **Ŷ**[(*t*)] 是 **X**[(*t*)] 和 **Ŷ**[(*t*–1)] 的函数，**Ŷ**[(*t*–1)] 是 **X**[(*t*–1)]
    和 **Ŷ**[(*t*–2)] 的函数，**Ŷ**[(*t*–2)] 是 **X**[(*t*–2)] 和 **Ŷ**[(*t*–3)] 的函数，依此类推。这使得
    **Ŷ**[(*t*)] 成为自时间 *t* = 0（即 **X**[(0)], **X**[(1)], …​, **X**[(*t*)]）以来所有输入的函数。在第一个时间步，*t*
    = 0，没有先前的输出，因此它们通常被假定为都是零。
- en: Note
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The idea of introducing backward connections (i.e., loops) in artificial neural
    networks dates back to the very origins of ANNs, but the first modern RNN architecture
    was [proposed by Michael I. Jordan in 1986](https://homl.info/jordanrnn).⁠^([2](ch13.html#id3066))
    At each time step, his RNN would look at the inputs for that time step, plus its
    own outputs from the previous time step. This is called *output feedback*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经网络中引入反向连接（即循环）的想法可以追溯到人工神经网络的最早期，但第一个现代循环神经网络（RNN）架构是在1986年由迈克尔·I·乔丹（Michael
    I. Jordan）提出的[1](https://homl.info/jordanrnn)。⁠^([2](ch13.html#id3066)) 在每个时间步，他的RNN会查看该时间步的输入，以及它从上一个时间步的输出。这被称为*输出反馈*。
- en: Memory Cells
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记忆单元
- en: Since the output of a recurrent neuron at time step *t* is a function of all
    the inputs from previous time steps, you could say it has a form of *memory*.
    A part of a neural network that preserves some state across time steps is called
    a *memory cell* (or simply a *cell*). A single recurrent neuron, or a layer of
    recurrent neurons, is a very basic cell, capable of learning only short patterns
    (typically about 10 steps long, but this varies depending on the task). Later
    in this chapter, we will look at some more complex and powerful types of cells
    capable of learning longer patterns (roughly 10 times longer, but again, this
    depends on the task).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在时间步 *t* 的循环神经元的输出是所有先前时间步输入的函数，因此可以说它具有一种*记忆*形式。一个在时间步之间保持某些状态的神经网络部分被称为*记忆单元*（或简单地称为*单元*）。单个循环神经元或循环神经元层是一个非常基本的单元，只能学习短模式（通常长度约为10步，但这个数字根据任务而变化）。在本章的后面部分，我们将探讨一些更复杂且功能强大的单元类型，这些单元能够学习更长的模式（大约长10倍，但同样，这取决于任务）。
- en: 'A cell’s state at time step *t*, denoted **h**[(*t*)] (the “h” stands for “hidden”),
    is a function of some inputs at that time step and its state at the previous time
    step: **h**[(*t*)] = *f*(**x**[(*t*)], **h**[(*t*–1)]). Its output at time step
    *t*, denoted **ŷ**[(*t*)], is also a function of the previous state and the current
    inputs, and typically it’s just a function of the current state. In the case of
    the basic cells we have discussed so far, the output is just equal to the state,
    but in more complex cells this is not always the case, as shown in [Figure 13-3](#hidden_state_diagram).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 单元在时间步 *t* 的状态，表示为 **h**[(*t*)]（“h”代表“隐藏”），是某些在该时间步的输入及其前一个时间步的状态的函数：**h**[(*t*)]
    = *f*(**x**[(*t*)], **h**[(*t*–1)]). 它在时间步 *t* 的输出，表示为 **ŷ**[(*t*)]，也是前一个状态和当前输入的函数，通常它只是当前状态的函数。在我们迄今为止讨论的基本单元中，输出等于状态，但在更复杂的单元中，情况并不总是如此，如图13-3所示。
- en: '![Diagram illustrating how a cell''s hidden state and output evolve over time,
    with feedback loops showing the interaction between inputs, hidden states, and
    outputs at different time steps.](assets/hmls_1303.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图示一个单元的隐藏状态和输出随时间演变的过程，其中反馈回路显示了不同时间步输入、隐藏状态和输出之间的交互。](assets/hmls_1303.png)'
- en: Figure 13-3\. A cell’s hidden state and its output may be different
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-3\. 单元的隐藏状态及其输出可能不同
- en: Note
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The first modern RNN that fed back the hidden state rather than the outputs
    was [proposed by Jeffrey Elman in 1990](https://homl.info/elmanrnn).⁠^([3](ch13.html#id3069))
    This is called *state feedback*, and it’s the most common approach today.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个现代RNN，它反馈的是隐藏状态而不是输出，是在1990年由杰弗里·埃尔曼（Jeffrey Elman）提出的[2](https://homl.info/elmanrnn)。⁠^([3](ch13.html#id3069))
    这被称为*状态反馈*，并且是今天最常见的方法。
- en: Input and Output Sequences
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入和输出序列
- en: 'An RNN can simultaneously take a sequence of inputs and produce a sequence
    of outputs (see the top-left network in [Figure 13-4](#seq_to_seq_diagram)). This
    type of *sequence-to-sequence network* is useful to forecast time series, such
    as your home’s daily power consumption: you feed it the data over the last *N*
    days, and you train it to output the power consumption shifted by one day into
    the future (i.e., from *N* – 1 days ago to tomorrow).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: RNN可以同时接受一系列输入并产生一系列输出（见[图13-4](#seq_to_seq_diagram)左上角的网络）。这种类型的*序列到序列网络*对于预测时间序列很有用，例如你家的每日电力消耗：你给它输入过去*N*天的数据，并训练它输出未来一天（即从*N*
    - 1天到明天）的电力消耗。
- en: Alternatively, you could feed the network a sequence of inputs and ignore all
    outputs except for the last one (see the top-right network in [Figure 13-4](#seq_to_seq_diagram)).
    This is a *sequence-to-vector network*. For example, you could feed the network
    a sequence of words corresponding to a movie review, and the network would output
    a sentiment score (e.g., from 0 [hate] to 1 [love]).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以给网络输入一系列输入，忽略除了最后一个之外的所有输出（见[图13-4](#seq_to_seq_diagram)右上角的网络）。这是一个*序列到向量网络*。例如，你可以给网络输入一系列与电影评论对应的单词，网络会输出一个情感分数（例如，从0[恨]到1[爱]）。
- en: Conversely, you could feed the network the same input vector over and over again
    at each time step and let it output a sequence (see the bottom-left network of
    [Figure 13-4](#seq_to_seq_diagram)). This is a *vector-to-sequence network*. For
    example, the input could be an image (or the output of a CNN), and the output
    could be a caption for that image.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你可以在每个时间步重复输入相同的输入向量，并让网络输出一个序列（见[图13-4](#seq_to_seq_diagram)左下角的网络）。这是一个*向量到序列网络*。例如，输入可以是图像（或CNN的输出），输出可以是该图像的标题。
- en: '![Diagram illustrating different neural network architectures: sequence-to-sequence,
    sequence-to-vector, vector-to-sequence, and encoder-decoder models.](assets/hmls_1304.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![不同神经网络架构的示意图：序列到序列、序列到向量、向量到序列和编码器-解码器模型](assets/hmls_1304.png)'
- en: Figure 13-4\. Sequence-to-sequence (top left), sequence-to-vector (top right),
    vector-to-sequence (bottom left), and encoder-decoder (bottom right) networks
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-4\. 序列到序列（左上角）、序列到向量（右上角）、向量到序列（左下角）和编码器-解码器（右下角）网络
- en: 'Lastly, you could have a sequence-to-vector network, called an *encoder*, followed
    by a vector-to-sequence network, called a *decoder* (see the bottom-right network
    of [Figure 13-4](#seq_to_seq_diagram)). For example, this could be used for translating
    a sentence from one language to another. You would feed the network a sentence
    in one language, the encoder would convert this sentence into a single vector
    representation, and then the decoder would decode this vector into a sentence
    in another language. This two-step model, called an [*encoder-decoder*](https://homl.info/seq2seq),⁠^([4](ch13.html#id3076))
    works much better than trying to translate on the fly with a single sequence-to-sequence
    RNN (like the one represented at the top left): the last words of a sentence can
    affect the first words of the translation, so you need to wait until you have
    seen the whole sentence before translating it. We will go through the implementation
    of an encoder-decoder in [Chapter 14](ch14.html#nlp_chapter) (as you will see,
    it is a bit more complex than what [Figure 13-4](#seq_to_seq_diagram) suggests).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以有一个序列到向量网络，称为*编码器*，后面跟着一个向量到序列网络，称为*解码器*（见[图13-4](#seq_to_seq_diagram)右下角的网络）。例如，这可以用于将一种语言的句子翻译成另一种语言。你会给网络输入一种语言的句子，编码器会将这个句子转换成一个单一的向量表示，然后解码器会将这个向量解码成另一种语言的句子。这个称为[*编码器-解码器*](https://homl.info/seq2seq)的两步模型，⁠^([4](ch13.html#id3076))比尝试使用单个序列到序列RNN（如左上角表示的）即时翻译要好得多：句子的最后几个词可能会影响翻译的第一个词，所以你需要等到看到整个句子后再进行翻译。我们将在[第14章](ch14.html#nlp_chapter)中介绍编码器-解码器的实现（正如你将看到的，它比[图13-4](#seq_to_seq_diagram)所暗示的要复杂一些）。
- en: This versatility sounds promising, but how do you train a recurrent neural network?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多功能性听起来很有希望，但你如何训练一个循环神经网络呢？
- en: Training RNNs
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练RNNs
- en: To train an RNN, the trick is to unroll it through time (like we just did) and
    then use regular backpropagation (see [Figure 13-5](#bptt_diagram)). This strategy
    is called *backpropagation through time* (BPTT).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个RNN，技巧是将其在时间上展开（就像我们刚才做的那样），然后使用常规的反向传播（见[图13-5](#bptt_diagram)）。这种策略被称为*时间反向传播*（BPTT）。
- en: Just like in regular backpropagation, there is a first forward pass through
    the unrolled network (represented by the dashed arrows). Then the output sequence
    is evaluated using a loss function ℒ(**Y**[(0)], **Y**[(1)], …​, **Y**[(*T*)];
    **Ŷ**[(0)], **Ŷ**[(1)], …​, **Ŷ**[(*T*)]) (where **Y**[(*i*)] is the *i*^(th)
    target, **Ŷ**[(*i*)] is the *i*^(th) prediction, and *T* is the max time step).
    Note that this loss function may ignore some outputs. For example, in a sequence-to-vector
    RNN, all outputs are ignored except for the very last one. In [Figure 13-5](#bptt_diagram),
    the loss function is computed based on the last three outputs only. The gradients
    of that loss function are then propagated backward through the unrolled network
    (represented by the solid arrows). In this example, since the outputs **Ŷ**[(0)]
    and **Ŷ**[(1)] are not used to compute the loss, the gradients do not flow backward
    through them; they only flow through **Ŷ**[(2)], **Ŷ**[(3)], and **Ŷ**[(4)]. Moreover,
    since the same parameters **W** and **b** are used at each time step, their gradients
    will be tweaked multiple times during backprop. Once the backward phase is complete
    and all the gradients have been computed, BPTT can perform a gradient descent
    step to update the parameters (this is no different from regular backprop).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如常规反向传播一样，有一个通过展开网络的第一次正向传递（由虚线箭头表示）。然后使用损失函数ℒ(**Y**[(0)], **Y**[(1)], …​,
    **Y**[(*T*)]; **Ŷ**[(0)], **Ŷ**[(1)], …​, **Ŷ**[(*T*)])评估输出序列（其中**Y**[(*i*)]是第*i*个目标，**Ŷ**[(*i*)]是第*i*个预测，*T*是最大时间步）。请注意，这个损失函数可能会忽略一些输出。例如，在序列到向量的RNN中，除了最后一个输出之外，所有输出都被忽略。在[图13-5](#bptt_diagram)中，损失函数仅基于最后三个输出进行计算。然后，该损失函数的梯度通过展开网络向后传播（由实线箭头表示）。在这个例子中，由于输出**Ŷ**[(0)]和**Ŷ**[(1)]没有用于计算损失，因此梯度不会通过它们向后流动；它们只通过**Ŷ**[(2)]，**Ŷ**[(3)]和**Ŷ**[(4)]流动。此外，由于每个时间步都使用相同的参数**W**和**b**，它们的梯度将在反向传播期间多次调整。一旦反向阶段完成并且所有梯度都已计算，BPTT可以执行梯度下降步骤以更新参数（这与常规反向传播没有不同）。
- en: '![Diagram illustrating backpropagation through time in an unrolled RNN, showing
    forward and backward passes with gradients flowing only through certain outputs
    for loss computation.](assets/hmls_1305.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![展示未展开的RNN中时间反向传播的图表，显示只有通过某些输出流动梯度以进行损失计算的正向和反向传递。](assets/hmls_1305.png)'
- en: Figure 13-5\. Backpropagation through time
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-5. 时间反向传播
- en: Fortunately, PyTorch takes care of all of this complexity for you, as you will
    see. But before we get there, let’s load a time series and start analyzing it
    using classical tools to better understand what we’re dealing with, and to get
    some baseline metrics.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，PyTorch为你处理了所有这些复杂性，你将会看到。但在我们到达那里之前，让我们加载一个时间序列，并使用经典工具开始分析它，以便更好地了解我们正在处理的内容，并获得一些基线指标。
- en: Forecasting a Time Series
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列预测
- en: All right! Let’s pretend you’ve just been hired as a data scientist by Chicago’s
    Transit Authority. Your first task is to build a model capable of forecasting
    the number of passengers that will ride on bus and rail the next day. You have
    access to daily ridership data since 2001\. Let’s walk through how you would handle
    this. We’ll start by loading and cleaning up the data:^([5](ch13.html#id3084))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！让我们假设你刚刚被芝加哥交通管理局雇佣为数据科学家。你的第一个任务是构建一个能够预测第二天公交和铁路乘客数量的模型。你自2001年以来就有每日客流量数据。让我们看看你会如何处理这个问题。我们将从加载数据并对其进行清理开始：^([5](ch13.html#id3084))
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We load the CSV file, set short column names, sort the rows by date, remove
    the redundant `total` column, and drop duplicate rows. Now let’s check what the
    first few rows look like:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载CSV文件，设置短列名，按日期排序行，删除冗余的`total`列，并删除重复的行。现在让我们看看前几行看起来像什么：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: On January 1st, 2001, 297,192 people boarded a bus in Chicago, and 126,455 boarded
    a train. The `day_type` column contains `W` for **W**eekdays, `A` for S**a**turdays,
    and `U` for S**u**ndays or holidays.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 2001年1月1日，有297,192人在芝加哥乘坐公交车，有126,455人乘坐火车。`day_type`列包含`W`代表**W**eekdays，`A`代表**S**a**turdays，`U`代表**S**u**ndays或节假日。
- en: 'Now let’s plot the bus and rail ridership figures over a few months in 2019,
    to see what it looks like (see [Figure 13-6](#daily_ridership_plot)):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制2019年几个月的公交和铁路客流量数据，看看它看起来像什么（见[图13-6](#daily_ridership_plot)）：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Line graph showing daily bus and rail ridership in Chicago from March to
    May 2019, illustrating regular peaks and troughs over time.](assets/hmls_1306.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![展示2019年3月至5月芝加哥每日公交和铁路客流量线图的图表，说明随着时间的推移出现的常规峰值和谷值。](assets/hmls_1306.png)'
- en: Figure 13-6\. Daily ridership in Chicago
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-6. 芝加哥每日客流量
- en: 'Note that Pandas includes both the start and end month in the range, so this
    plots the data from the 1st of March all the way up to the 31st of May. This is
    a *time series*: data with values at different time steps, usually at regular
    intervals. More specifically, since there are multiple values per time step, this
    is called a *multivariate time series*. If we only looked at the `bus` column,
    it would be a *univariate time series*, with a single value per time step. Predicting
    future values (i.e., forecasting) is the most typical task when dealing with time
    series, and this is what we will focus on in this chapter. Other tasks include
    imputation (filling in missing past values), classification, anomaly detection,
    and more.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Pandas包括范围中的起始和结束月份，因此这个图显示了从3月1日到5月31日的数据。这是一个*时间序列*：在不同时间步长上有值的序列，通常是以固定间隔。更具体地说，由于每个时间步长有多个值，这被称为*多元时间序列*。如果我们只看`bus`列，它将是一个*单变量时间序列*，每个时间步长只有一个值。预测未来值（即预测）是处理时间序列时最典型的任务，这也是本章我们将关注的重点。其他任务包括插补（填补过去缺失的值）、分类、异常检测等。
- en: 'Looking at [Figure 13-6](#daily_ridership_plot), we can see that a similar
    pattern is clearly repeated every week. This is called a weekly *seasonality*.
    In fact, it’s so strong in this case that forecasting tomorrow’s ridership by
    just copying the values from a week earlier will yield reasonably good results.
    This is called *naive forecasting*: simply copying a past value to make our forecast.
    Naive forecasting is often a great baseline, and it can even be tricky to beat
    in some cases.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 观察图13-6[图13-6](#daily_ridership_plot)，我们可以看到这种模式每周都会清晰地重复。这被称为*季节性*。实际上，在这个例子中，这种季节性非常强烈，仅通过复制一周前的值来预测明天的客流量就能得到相当好的结果。这被称为*简单预测*：简单地复制过去的一个值来做出我们的预测。简单预测通常是一个很好的基线，在某些情况下甚至可能很难超越。
- en: Note
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In general, naive forecasting means copying the latest known value (e.g., forecasting
    that tomorrow will be the same as today). However, in our case, copying the value
    from the previous week works better, due to the strong weekly seasonality.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，简单预测意味着复制最新的已知值（例如，预测明天将与今天相同）。然而，在我们的情况下，由于强烈的每周季节性，复制上周的值效果更好。
- en: 'To visualize these naive forecasts, let’s overlay the two time series (for
    bus and rail) as well as the same time series lagged by one week (i.e., shifted
    toward the right) using dotted lines. We’ll also plot the difference between the
    two (i.e., the value at time *t* minus the value at time *t* – 7); this is called
    *differencing* (see [Figure 13-7](#differencing_plot)):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这些简单预测，让我们用虚线叠加两个时间序列（公交车和铁路）以及相同时间序列滞后一周（即向右移动）的时间序列。我们还将绘制两者之间的差异（即时间*t*的值减去时间*t*–7的值）；这被称为*差分*（见[图13-7](#differencing_plot)）：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Two-panel plot showing the original and 7-day lagged time series for bus
    and rail (top), and the differences between them (bottom), demonstrating autocorrelation
    patterns.](assets/hmls_1307.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![两面板图显示了公交车和铁路的原始和7天滞后时间序列（顶部），以及它们之间的差异（底部），展示了自相关模式。](assets/hmls_1307.png)'
- en: Figure 13-7\. Time series overlaid with 7-day lagged time series (top), and
    difference between *t* and *t* – 7 (bottom)
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-7\. 时间序列与7天滞后时间序列叠加（顶部），以及*t*和*t*–7之间的差异（底部）
- en: 'Not too bad! Notice how closely the lagged time series track the actual time
    series. When a time series is correlated with a lagged version of itself, we say
    that the time series is *autocorrelated*. As you can see, most of the differences
    are fairly small, except at the end of May. Maybe there was a holiday at that
    time? Let’s check the `day_type` column:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 还不错！注意观察滞后时间序列如何紧密地跟踪实际时间序列。当一个时间序列与其自身的滞后版本相关时，我们称该时间序列为*自相关的*。正如你所见，大部分差异都相当小，除了五月底。那时可能有个假期？让我们检查一下`day_type`列：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Indeed, there was a long weekend back then: the Monday was the Memorial Day
    holiday. We could use this column to improve our forecasts, but for now let’s
    just measure the mean absolute error over the three-month period we’re arbitrarily
    focusing on—March, April, and May 2019—to get a rough idea:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，那时有一个长假：星期一是纪念日假期。我们可以使用这个列来改进我们的预测，但现在让我们只测量我们任意关注的三个月的平均绝对误差——2019年3月、4月和5月，以获得一个大致的概念：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our naive forecasts get a mean absolute error (MAE) of about 43,916 bus riders,
    and about 42,143 rail riders. It’s hard to tell at a glance how good or bad this
    is, so let’s put the forecast errors into perspective by dividing them by the
    target values:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的简单预测得到的平均绝对误差（MAE）大约为43,916名公交车乘客，以及大约42,143名铁路乘客。一眼看去很难判断这是好是坏，所以让我们通过将预测误差除以目标值来将这些误差置于适当的视角：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: What we just computed is called the *mean absolute percentage error* (MAPE).
    It looks like our naive forecasts give us a MAPE of roughly 8.3% for bus and 9.0%
    for rail. It’s interesting to note that the MAE for the rail forecasts looks slightly
    better than the MAE for the bus forecasts, while the opposite is true for the
    MAPE. That’s because the bus ridership is larger than the rail ridership, so naturally
    the forecast errors are also larger, but when we put the errors into perspective,
    it turns out that the bus forecasts are actually slightly better than the rail
    forecasts.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚计算的是所谓的*平均绝对百分比误差*（MAPE）。看起来我们的简单预测给出了公交车大约8.3%的MAPE，铁路大约9.0%。值得注意的是，铁路预测的MAE看起来略好于公交车预测的MAE，而MAPE的情况则相反。这是因为公交车乘客数量大于铁路乘客数量，所以预测误差自然也更大，但当我们将这些误差置于适当的视角时，结果竟然是公交车预测实际上略好于铁路预测。
- en: Tip
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The MAE, MAPE, and mean squared error (MSE) are among the most common metrics
    you can use to evaluate your forecasts. As always, choosing the right metric depends
    on the task. For example, if your project suffers quadratically more from large
    errors than from small ones, then the MSE may be preferable, as it strongly penalizes
    large errors.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: MAE（平均绝对误差）、MAPE（平均绝对百分比误差）和均方误差（MSE）是你可以用来评估预测的最常见指标之一。和往常一样，选择合适的指标取决于任务。例如，如果你的项目对大误差的惩罚比对小误差的惩罚更严重，那么MSE可能更可取，因为它对大误差的惩罚更重。
- en: 'Looking at the time series, there doesn’t appear to be any significant monthly
    seasonality, but let’s check whether there’s any yearly seasonality. We’ll look
    at the data from 2001 to 2019\. To reduce the risk of data snooping, we’ll ignore
    more recent data for now. Let’s also plot a 12-month rolling average for each
    series to visualize long-term trends (see [Figure 13-8](#long_term_ridership_plot)):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 观察时间序列，似乎没有明显的月度季节性，但让我们检查是否存在年度季节性。我们将查看2001年至2019年的数据。为了减少数据挖掘的风险，我们目前将忽略更近期的数据。让我们也为每个序列绘制12个月的滚动平均值，以可视化长期趋势（参见[图13-8](#long_term_ridership_plot)）：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Line graph showing monthly data from 2001 to 2019 of bus (blue) and rail
    (orange) services with marked yearly seasonality and long-term trends, including
    a 12-month rolling average.](assets/hmls_1308.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![展示2001年至2019年公交车（蓝色）和铁路（橙色）服务月度数据的折线图，标明了年度季节性和长期趋势，包括12个月的滚动平均值。](assets/hmls_1308.png)'
- en: Figure 13-8\. Yearly seasonality and long-term trends
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-8. 年度季节性和长期趋势
- en: 'Yep! There’s definitely some yearly seasonality as well, although it is noisier
    than the weekly seasonality, and more visible for the rail series than the bus
    series: we see peaks and troughs at roughly the same dates each year. Let’s check
    what we get if we plot the 12-month difference (see [Figure 13-9](#yearly_diff_plot)):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！确实存在一些年度季节性，尽管它比周季节性更嘈杂，并且对于铁路序列比公交车序列更明显：我们每年在相同日期看到峰值和谷值。让我们检查如果我们绘制12个月差异会得到什么（参见[图13-9](#yearly_diff_plot)）：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Line graph showing the 12-month difference in data trends for bus and rail
    from 2001 to 2019, highlighting fluctuations and removal of seasonality and trends.](assets/hmls_1309.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![展示2001年至2019年公交车和铁路数据趋势12个月差异的折线图，突出波动和季节性及趋势的去除。](assets/hmls_1309.png)'
- en: Figure 13-9\. The 12-month difference
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-9. 12个月差异
- en: 'Notice how differencing not only removed the yearly seasonality, but it also
    removed the long-term trends. For example, the linear downward trend present in
    the time series from 2016 to 2019 became a roughly constant negative value in
    the differenced time series. In fact, differencing is a common technique used
    to remove trend and seasonality from a time series: it’s easier to study a *stationary*
    time series, meaning one whose statistical properties remain the same over time,
    without any seasonality or trends. Once you’re able to make accurate forecasts
    on the differenced time series, it’s easy to turn them into forecasts for the
    actual time series by just adding back the past values that were previously subtracted.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到差分不仅消除了年度季节性，还消除了长期趋势。例如，2016 年至 2019 年时间序列中存在的线性下降趋势在差分时间序列中变成了一个大致恒定的负值。实际上，差分是常用的一种技术，用于从时间序列中去除趋势和季节性：研究一个
    *平稳* 时间序列，即其统计属性在时间上保持不变，没有任何季节性或趋势，更容易一些。一旦你能够对差分时间序列进行准确的预测，只需将之前减去的过去值加回，就可以很容易地将它们转换为实际时间序列的预测。
- en: You may be thinking that we’re only trying to predict tomorrow’s ridership,
    so the long-term patterns matter much less than the short-term ones. You’re right,
    but still, we may be able to improve performance slightly by taking long-term
    patterns into account. For example, daily bus ridership dropped by about 2,500
    in October 2017, which represents about 570 fewer passengers each week, so if
    we were at the end of October 2017, it would make sense to forecast tomorrow’s
    ridership by copying the value from last week, minus 570\. Accounting for the
    trend will make your forecasts a bit more accurate on average.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为我们只是在尝试预测明天的客流量，所以长期模式的重要性远不如短期模式。你说得对，但即便如此，我们仍然可能通过考虑长期模式来略微提高性能。例如，2017
    年 10 月的每日公交客流量下降了约 2,500 人，这相当于每周少 570 名乘客，所以如果我们是在 2017 年 10 月末，那么通过复制上周的值并减去
    570 来预测明天的客流量是有意义的。考虑趋势会使你的预测在平均上更准确。
- en: Now that you’re familiar with the ridership time series, as well as some of
    the most important concepts in time series analysis, including seasonality, trend,
    differencing, and moving averages, let’s take a quick look at a very popular family
    of statistical models that are commonly used to analyze time series.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了客流量时间序列，以及时间序列分析中的一些最重要的概念，包括季节性、趋势、差分和移动平均，让我们快速了解一下一个非常流行的统计模型族，这些模型通常用于分析时间序列。
- en: The ARMA Model Family
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ARMA 模型族
- en: 'We’ll start with the *autoregressive moving average* (ARMA) model, developed
    by Herman Wold in the 1930s: it computes its forecasts using a simple weighted
    sum of lagged values and corrects these forecasts by adding a moving average,
    very much like we just discussed. Specifically, the moving average component is
    computed using a weighted sum of the last few forecast errors. [Equation 13-3](#arma_equation)
    shows how the model makes its forecasts.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从由赫尔曼·沃尔德在 1930 年代开发的 *自回归移动平均*（ARMA）模型开始：它通过简单的加权滞后值之和来计算预测，并通过添加移动平均来校正这些预测，这与我们刚才讨论的非常相似。具体来说，移动平均成分是通过计算最后几个预测误差的加权平均来得到的。[方程式
    13-3](#arma_equation) 展示了模型是如何进行预测的。
- en: Equation 13-3\. Forecasting using an ARMA model
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 13-3\. 使用 ARMA 模型进行预测
- en: <mtable columnalign="left"><mtr><mtd><msub><mover><mi>y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msub><mi>α</mi><mi>i</mi></msub><msub><mi>y</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover><msub><mi>θ</mi><mi>i</mi></msub><msub><mi>ε</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd></mtr><mtr><mtd><mtext>with </mtext><msub><mi>ε</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><msub><mi>y</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>-</mo><msub><mover><mi>y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd></mtr></mtable>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable columnalign="left"><mtr><mtd><msub><mover><mi>y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msub><mi>α</mi><mi>i</mi></msub><msub><mi>y</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover><msub><mi>θ</mi><mi>i</mi></msub><msub><mi>ε</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd></mtr><mtr><mtd><mtext>with 
    </mtext><msub><mi>ε</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><msub><mi>y</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>-</mo><msub><mover><mi>y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd></mtr></mtable>
- en: 'In this equation:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*ŷ*[(*t*)] is the model’s forecast for time step *t*.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ŷ*[(*t*)] 是模型对时间步 *t* 的预测。'
- en: '*y*[(*t*)] is the time series’ value at time step *t*.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*[(*t*)] 是时间序列在时间步 *t* 的值。'
- en: 'The first sum is the weighted sum of the past *p* values of the time series,
    using the learned weights *α*[*i*]. The number *p* is a hyperparameter, and it
    determines how far back into the past the model should look. This sum is the *autoregressive*
    component of the model: it performs regression based on past values.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个求和是时间序列过去 *p* 个值的加权求和，使用学习到的权重 *α*[*i*]。数字 *p* 是一个超参数，它决定了模型应该回溯多远的历史。这个求和是模型的
    *自回归* 部分：它基于过去值进行回归。
- en: The second sum is the weighted sum over the past *q* forecast errors *ε*[(*t*)],
    using the learned weights *θ*[*i*]. The number *q* is a hyperparameter. This sum
    is the moving average component of the model.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个求和是过去 *q* 个预测误差 *ε*[(*t*)] 的加权求和，使用学习到的权重 *θ*[*i*]。数字 *q* 是一个超参数。这个求和是模型的
    *移动平均* 部分。
- en: 'Importantly, this model assumes that the time series is stationary. If it is
    not, then differencing may help. Using differencing over a single time step will
    produce an approximation of the derivative of the time series: indeed, it will
    give the slope of the series at each time step. This means that it will eliminate
    any linear trend, transforming it into a constant value. For example, if you apply
    one-step differencing to the series [3, 5, 7, 9, 11], you get the differenced
    series [2, 2, 2, 2].'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这个模型假设时间序列是平稳的。如果不是，那么差分可能有助于解决这个问题。对单个时间步进行差分将产生时间序列导数的近似值：实际上，它将给出每个时间步的序列斜率。这意味着它将消除任何线性趋势，将其转换为常数值。例如，如果你对一个序列
    [3, 5, 7, 9, 11] 应用单步差分，你得到差分序列 [2, 2, 2, 2]。
- en: If the original time series has a quadratic trend instead of a linear trend,
    then a single round of differencing will not be enough. For example, the series
    [1, 4, 9, 16, 25, 36] becomes [3, 5, 7, 9, 11] after one round of differencing,
    but if you run differencing for a second round, then you get [2, 2, 2, 2]. So,
    running two rounds of differencing will eliminate quadratic trends. More generally,
    running *d* consecutive rounds of differencing computes an approximation of the
    *d*^(th) order derivative of the time series, so it will eliminate polynomial
    trends up to degree *d*. This hyperparameter *d* is called the *order of integration*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果原始时间序列有一个二次趋势而不是线性趋势，那么单次差分将不足以解决问题。例如，序列 [1, 4, 9, 16, 25, 36] 在一次差分后变为 [3,
    5, 7, 9, 11]，但如果你进行第二次差分，那么你得到 [2, 2, 2, 2]。因此，进行两次差分将消除二次趋势。更一般地，进行 *d* 次连续差分计算时间序列的
    *d* 次导数的近似值，因此它将消除高达 *d* 次的多项式趋势。这个超参数 *d* 被称为 *积分阶数*。
- en: Differencing is the central contribution of the *autoregressive integrated moving
    average* (ARIMA) model, introduced in 1970 by George Box and Gwilym Jenkins in
    their book *Time Series Analysis* (Wiley). This model runs *d* rounds of differencing
    to make the time series more stationary, then it applies a regular ARMA model.
    When making forecasts, it uses this ARMA model, then it adds back the terms that
    were subtracted by differencing.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 差分是*自回归积分移动平均*（ARIMA）模型的核心贡献，由George Box和Gwilym Jenkins在他们的书《时间序列分析》（Wiley）中于1970年引入。该模型进行*d*轮差分，使时间序列更加平稳，然后应用常规的ARMA模型。在做出预测时，它使用这个ARMA模型，然后添加回由差分减去的项。
- en: 'One last member of the ARMA family is the *seasonal ARIMA* (SARIMA) model:
    it models the time series in the same way as ARIMA, but it additionally models
    a seasonal component for a given frequency (e.g., weekly), using the exact same
    ARIMA approach. It has a total of seven hyperparameters: the same *p*, *d*, and
    *q* hyperparameters as ARIMA; plus additional *P*, *D*, and *Q* hyperparameters
    to model the seasonal pattern; and lastly the period of the seasonal pattern,
    denoted *s*. The hyperparameters *P*, *D*, and *Q* are just like *p*, *d*, and
    *q*, but they are used to model the time series at *t* – *s*, *t* – 2*s*, *t* – 3*s*,
    etc.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ARMA家族的最后一个成员是*季节性ARIMA*（SARIMA）模型：它以与ARIMA相同的方式对时间序列进行建模，但还额外对给定频率（例如，每周）的季节性成分进行建模，使用完全相同的ARIMA方法。它总共有七个超参数：与ARIMA相同的*p*，*d*，和*q*超参数；以及额外的*p*，*d*，和*q*超参数来建模季节性模式；最后是季节性模式的周期，表示为*s*。超参数*p*，*d*，和*q*与*p*，*d*，和*q*类似，但它们用于在*t*
    – *s*，*t* – 2*s*，*t* – 3*s*等时间点建模时间序列。
- en: 'Let’s see how to fit a SARIMA model to the rail time series, and use it to
    make a forecast for tomorrow’s ridership. We’ll pretend today is the last day
    of May 2019, and we want to forecast the rail ridership for “tomorrow”, the 1st
    of June, 2019\. For this, we can use the `statsmodels` library, which contains
    many different statistical models, including the ARMA model and its variants,
    implemented by the `ARIMA` class:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将SARIMA模型拟合到铁路时间序列，并使用它来预测明天的客流量。我们将假设今天是2019年5月最后一天，我们想要预测2019年6月1日的铁路客流量。为此，我们可以使用包含许多不同统计模型（包括ARMA模型及其变体）的`statsmodels`库，这些模型通过`ARIMA`类实现：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this code example:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中：
- en: We start by importing the `ARIMA` class, then we take the rail ridership data
    from the start of 2019 up to “today”, and we use `asfreq("D")` to set the time
    series’ frequency to daily. This doesn’t change the data at all in this case,
    since it’s already daily, but without this the `ARIMA` class would have to guess
    the frequency, and it would display a warning.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先导入`ARIMA`类，然后从2019年初开始到“今天”获取铁路客流量数据，并使用`asfreq("D")`将时间序列的频率设置为每日。在这种情况下，这不会改变数据，因为它已经是每日的，但如果没有这个设置，`ARIMA`类将不得不猜测频率，并显示警告。
- en: 'Next, we create an `ARIMA` instance, passing it all the data until “today”,
    and we set the model hyperparameters: `order=(1, 0, 0)` means that *p* = 1, *d*
    = 0, and *q* = 0; and `seasonal_order=(0, 1, 1, 7)` means that *P* = 0, *D* =
    1, *Q* = 1, and *s* = 7. Notice that the `statsmodels` API differs a bit from
    Scikit-Learn’s API, since we pass the data to the model at construction time,
    instead of passing it to the `fit()` method.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`ARIMA`实例，传递从“今天”之前的所有数据，并设置模型超参数：`order=(1, 0, 0)`表示*p* = 1，*d* =
    0，和*q* = 0；而`seasonal_order=(0, 1, 1, 7)`表示*p* = 0，*d* = 1，*q* = 1，和*s* = 7。请注意，`statsmodels`
    API与Scikit-Learn的API略有不同，因为我们是在构造时将数据传递给模型，而不是在`fit()`方法中传递。
- en: Next, we fit the model, and we use it to make a forecast for “tomorrow”, the
    1st of June, 2019.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们拟合模型，并使用它来预测“明天”，即2019年6月1日。
- en: 'The forecast is 427,759 passengers, when in fact there were 379,044\. Yikes,
    we’re 12.9% off—that’s pretty bad. It’s actually slightly worse than naive forecasting,
    which forecasts 426,932, off by 12.6%. But perhaps we were just unlucky that day?
    To check this, we can run the same code in a loop to make forecasts for every
    day in March, April, and May, and compute the MAE over that period:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的客流量为427,759人，而实际上有379,044人。哎呀，我们偏离了12.9%——这相当糟糕。实际上比简单的预测还要糟糕，简单的预测为426,932，偏离了12.6%。但也许我们那天只是运气不好？为了检查这一点，我们可以循环运行相同的代码，为三月份、四月份和五月份的每一天做出预测，并计算该期间的MAE：
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Ah, that’s much better! The MAE is about 32,041, which is significantly lower
    than the MAE we got with naive forecasting (42,143). So although the model is
    not perfect, it still beats naive forecasting by a large margin, on average.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 哎，这好多了！平均绝对误差（MAE）大约是32,041，这比我们用朴素预测得到的MAE（42,143）低得多。所以尽管模型并不完美，但它仍然在平均意义上比朴素预测有大幅提升。
- en: 'At this point, you may be wondering how to pick good hyperparameters for the
    SARIMA model. There are several methods, but the simplest to understand and to
    get started with is the brute-force approach: just run a grid search. For each
    model you want to evaluate (i.e., each hyperparameter combination), you can run
    the preceding code example, changing only the hyperparameter values. Good *p*,
    *q*, *P*, and *Q* values are usually fairly small (typically 0 to 2, sometimes
    up to 5 or 6), and *d* and *D* are typically 0 or 1, sometimes 2\. As for *s*,
    it’s just the main seasonal pattern’s period: in our case it’s 7 since there’s
    a strong weekly seasonality. The model with the lowest MAE wins. Of course, you
    can replace the MAE with another metric if it better matches your business objective.
    And that’s it!'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能想知道如何为SARIMA模型选择良好的超参数。有几种方法，但最简单易懂且易于开始的方法是暴力搜索法：只需运行网格搜索。对于你想要评估的每个模型（即每个超参数组合），你可以运行前面的代码示例，只需更改超参数值。好的*p*，*q*，*P*和*Q*值通常相当小（通常是0到2，有时高达5或6），而*d*和*D*通常是0或1，有时是2。至于*s*，它只是主要季节性模式的周期：在我们的案例中是7，因为有强烈的每周季节性。MAE最低的模型获胜。当然，如果你有更好的指标来匹配你的业务目标，你也可以用其他指标替换MAE。就这样！
- en: Tip
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: There are other more principled approaches to selecting good hyperparameters,
    based on analyzing the *autocorrelation function* (ACF) and *partial autocorrelation
    function* (PACF),⁠^([6](ch13.html#id3107)) or minimizing the AIC or BIC metrics
    (introduced in [Chapter 8](ch08.html#unsupervised_learning_chapter)) to penalize
    models that use too many parameters and reduce the risk of overfitting the data,
    but grid search is a good place to start.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 选择良好的超参数还有其他更原则性的方法，基于分析*自相关函数*（ACF）和*偏自相关函数*（PACF），⁠^([6](ch13.html#id3107))
    或者最小化AIC或BIC指标（在第8章[8](ch08.html#unsupervised_learning_chapter)中介绍），以惩罚使用过多参数的模型并降低过拟合数据的风险，但网格搜索是一个不错的起点。
- en: Preparing the Data for Machine Learning Models
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为机器学习模型准备数据
- en: 'Now that we have two baselines, naive forecasting and SARIMA, let’s try to
    use the machine learning models we’ve covered so far to forecast this time series,
    starting with a basic linear model. Our goal will be to forecast tomorrow’s ridership
    based on the ridership of the past 8 weeks of data (56 days). The inputs to our
    model will therefore be sequences (usually a single sequence per day once the
    model is in production), each containing 56 values from time steps *t* – 55 to
    *t*. For each input sequence, the model will output a single value: the forecast
    for time step *t* + 1.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个基线，朴素预测和SARIMA，让我们尝试使用我们之前覆盖的机器学习模型来预测这个时间序列，从基本的线性模型开始。我们的目标将是基于过去8周的数据（56天）预测明天的乘客量。因此，我们模型的输入将是序列（一旦模型投入生产，通常每天只有一个序列），每个序列包含从时间步长*t* – 55到*t*的56个值。对于每个输入序列，模型将输出一个值：时间步长*t* + 1的预测值。
- en: 'But what will we use as training data? Well, here’s the trick: we will use
    every 56-day window from the past as training data, and the target for each window
    will be the value immediately following it. To do that, we need to create a custom
    dataset that will chop a given time series into all possible windows of a given
    length, each with its corresponding target:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们将使用什么作为训练数据呢？嗯，这里有个技巧：我们将使用过去每56天的窗口作为训练数据，每个窗口的目标将是紧随其后的值。为了做到这一点，我们需要创建一个自定义数据集，该数据集将把给定的时间序列切割成所有可能的长度为给定长度的窗口，每个窗口都有其对应的目标：
- en: '[PRE11]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s test this class by applying it to a simple time series containing the
    numbers 0 to 5\. We could represent this series in 1D using [0, 1, 2, 3, 4, 5],
    but the RNN modules expect each sequence to be 2D, with a shape of [*sequence
    length*, *dimensionality*]. For univariate time series, the dimensionality is
    simply 1, so we represent the time series as [[0], [1], [2], [3], [4], [5]]. In
    the following code below, the `TimeSeriesDataset` contains all the windows of
    length 3, each with its corresponding target (i.e., the first value after the
    window):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将这个简单的包含数字 0 到 5 的时间序列应用于这个类来测试它。我们可以使用 [0, 1, 2, 3, 4, 5] 来表示这个序列，但 RNN
    模块期望每个序列都是 2D 的，形状为 [*序列长度*, *维度*]。对于单变量时间序列，维度是 1，因此我们表示时间序列为 [[0], [1], [2],
    [3], [4], [5]]。在下面的代码中，`TimeSeriesDataset` 包含所有长度为 3 的窗口，每个窗口都有其对应的目标（即窗口后的第一个值）：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'It looks like our `TimeSeriesDataset` class works fine! Now we can create a
    `DataLoader` for this tiny dataset, shuffling the windows and grouping them into
    batches of two:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的 `TimeSeriesDataset` 类工作得很好！现在我们可以为这个小型数据集创建一个 `DataLoader`，随机打乱窗口并将它们分成每批两个：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first batch contains the windows [[0], [1], [2]] and [[2], [3], [4]], along
    with their respective targets [3] and [5]; and the second batch contains only
    one window [[1], [2], [3]], with its target [4]. Indeed, when the length of a
    dataset is not a multiple of the batch size, the last batch is shorter.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第一批包含窗口 [[0], [1], [2]] 和 [[2], [3], [4]]，以及它们各自的目标 [3] 和 [5]；第二批只包含一个窗口 [[1],
    [2], [3]]，其目标为 [4]。实际上，当数据集的长度不是批量大小的倍数时，最后一个批次会更短。
- en: 'OK, now that we have a way to convert a time series into a dataset that we
    can use to train ML models, let’s go ahead and prepare our ridership dataset.
    First, we need to split our data into a training period, a validation period,
    and a test period. We will focus on the rail ridership for now. We will convert
    the data to 32-bit float tensors, and scale them down by a factor of one million
    to ensure the values end up near the 0–1 range; this plays nicely with the default
    weight initialization and learning rate. In this code, we use `df[["rail"]]` instead
    of `df["rail"]` to ensure the resulting tensor has a shape of [*series length*,
    1] rather than [*series length*]:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们已经有了将时间序列转换为可用于训练 ML 模型的数据集的方法，让我们继续准备我们的客流量数据集。首先，我们需要将我们的数据分为训练期、验证期和测试期。我们目前将专注于铁路客流量。我们将数据转换为
    32 位浮点张量，并按一百万的比例缩小，以确保值最终接近 0–1 范围；这很好地与默认权重初始化和学习率相匹配。在这段代码中，我们使用 `df[["rail"]]`
    而不是 `df["rail"]`，以确保生成的张量形状为 [*序列长度*, 1] 而不是 [*序列长度*]：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When dealing with time series, you generally want to split across time. However,
    in some cases you may be able to split along other dimensions, which will give
    you a longer time period to train on. For example, if you have data about the
    financial health of 10,000 companies from 2001 to 2019, you might be able to split
    this data across the different companies. It’s very likely that many of these
    companies will be strongly correlated, though (e.g., whole economic sectors may
    go up or down jointly), and if you have correlated companies across the training
    set and the test set, your test set will not be as useful, as its measure of the
    generalization error will be optimistically biased.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理时间序列时，你通常希望按时间进行分割。然而，在某些情况下，你可能能够在其他维度上进行分割，这将为你提供更长的训练时间。例如，如果你有关于 2001
    年至 2019 年 10,000 家公司财务状况的数据，你可能能够将这些数据分割到不同的公司。然而，这些公司之间很可能存在强烈的关联（例如，整个经济部门可能共同上升或下降），如果你在训练集和测试集中有相关的公司，那么你的测试集将不那么有用，因为其泛化误差的度量将过于乐观地偏差。
- en: 'Next, let’s use our `TimeSeriesDataset` class to create datasets for training,
    validation, and testing, and also create the corresponding data loaders. Since
    gradient descent expects the instances in the training set to be independent and
    identically distributed (IID), as we saw in [Chapter 4](ch04.html#linear_models_chapter),
    we must set `shuffle` to `True`—this will shuffle the windows, but not their contents:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用我们的 `TimeSeriesDataset` 类来创建用于训练、验证和测试的数据集，并创建相应的数据加载器。由于梯度下降期望训练集中的实例是独立同分布的（IID），正如我们在第
    4 章[线性模型](ch04.html#linear_models_chapter)中看到的，我们必须将 `shuffle` 设置为 `True`——这将打乱窗口，但不会打乱它们的内部内容：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: And now we’re ready to build and train any regression model we want!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好构建和训练我们想要的任何回归模型了！
- en: Forecasting Using a Linear Model
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用线性模型进行预测
- en: 'Let’s try a basic linear model first. We will use the Huber loss, which usually
    works better than minimizing the MAE directly, as discussed in [Chapter 9](ch09.html#ann_chapter):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先尝试一个基本的线性模型。我们将使用Huber损失，正如在第9章中讨论的，这通常比直接最小化MAE效果更好：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that we must use an `nn.Flatten` layer before the `nn.Linear` layer, because
    the inputs have a shape of [*batch size*, *window length*, *dimensionality*],
    but the `nn.Linear` layer expects inputs of shape [*batch size*, *features*].
    If you train this model, you will see that it reaches a validation MAE of 37,726
    (your mileage may vary). That’s better than naive forecasting, but worse than
    the SARIMA model.^([7](ch13.html#id3125))
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们必须在`nn.Linear`层之前使用一个`nn.Flatten`层，因为输入的形状为[*批大小*，*窗口长度*，*维度性*]，而`nn.Linear`层期望输入的形状为[*批大小*，*特征*]。如果你训练这个模型，你会看到它达到了验证MAE为37,726（你的结果可能会有所不同）。这比简单的预测要好，但比SARIMA模型差。（^([7](ch13.html#id3125)）
- en: Can we do better with an RNN? Well, let’s see!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否用RNN做得更好？好吧，让我们看看！
- en: Forecasting Using a Simple RNN
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用简单RNN进行预测
- en: 'Let’s implement a simple RNN containing a single recurrent layer (see [Figure 13-2](#rnn_layer_diagram))
    plus a final `nn.Linear` layer that will take the last hidden state as input and
    output the model’s forecast:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个包含单个循环层的简单RNN（见[图13-2](#rnn_layer_diagram)）以及一个最终的`nn.Linear`层，该层将最后一个隐藏状态作为输入并输出模型的预测：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let’s go through this code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析这段代码：
- en: 'The constructor takes three arguments: the input size, the hidden size, and
    the output size. In our case, the input size is set to 1 when we create the model
    (on the very last line) since we are dealing with a univariate time series. The
    hidden size is the number of recurrent neurons. We set it to 32 in this example,
    but this is a hyperparameter you can tune. The output size is 1 since we’re only
    forecasting a single value.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数接受三个参数：输入大小、隐藏大小和输出大小。在我们的例子中，当创建模型时（在最后一行），输入大小被设置为1，因为我们处理的是一元时间序列。隐藏大小是循环神经元的数量。在这个例子中，我们将其设置为32，但这是一个你可以调整的超参数。输出大小为1，因为我们只预测一个值。
- en: 'The constructor first creates the memory cell, which will be used once per
    time step: it’s a sequential module composed of an `nn.Linear` layer and the tanh
    activation function. You can use another activation function here, but it’s common
    to use the tanh activation function because it tends to be more stable than other
    activation functions in RNNs.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数首先创建内存单元，它将在每个时间步使用一次：它是一个由`nn.Linear`层和tanh激活函数组成的顺序模块。你可以在这里使用另一个激活函数，但通常使用tanh激活函数，因为它在RNN中比其他激活函数更稳定。
- en: Next, we create an `nn.Linear` layer that will be used to take the last hidden
    state and produce the final output. This is needed because the hidden state has
    one dimension per recurrent neuron (32 in this example), while the target has
    just one dimension since we’re dealing with a univariate time series and we’re
    only trying to forecast a single future value. Moreover, the tanh activation function
    only outputs values between –1 and +1, while the values we need to forecast occasionally
    exceed +1.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`nn.Linear`层，它将被用来取最后一个隐藏状态并产生最终输出。这是必需的，因为隐藏状态有每个循环神经元的一个维度（在这个例子中是32），而目标只有一个维度，因为我们处理的是一元时间序列，我们只尝试预测一个未来的值。此外，tanh激活函数只输出介于-1和+1之间的值，而我们需要预测的值偶尔会超过+1。
- en: The `forward()` method will be passed input batches produced by our data loader,
    so each batch will have a shape of [*batch size*, *window length*, *dimensionality*],
    with *dimensionality* = 1.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()`方法将传递由我们的数据加载器产生的输入批次，因此每个批次将具有[*批大小*，*窗口长度*，*维度性*]的形状，其中*维度性* =
    1。'
- en: 'The hidden state `H` is initialized to zeros: for each input window, there’s
    one zero per recurrent neuron, so the hidden state’s shape is [*batch size*, *hidden_size*].'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏状态`H`被初始化为零：对于每个输入窗口，每个循环神经元有一个零，所以隐藏状态的形状是[*批大小*，*隐藏大小*]。
- en: Next, we iterate over each time step. For this, we must swap the first two dimensions
    of `X` using `permute(0, 1)`. As a result, the input tensor `X_t` at each time
    step has a shape of [*batch size*, *dimensionality*].
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们遍历每个时间步。为此，我们必须使用`permute(0, 1)`交换`X`的前两个维度。结果，每个时间步的输入张量`X_t`的形状为[*批大小*，*维度性*]。
- en: At each time step, we want to feed both the current inputs `X_t` and the hidden
    state `H` to the memory cell. For this, we must first concatenate `X_t` and `H`
    along the first dimension, resulting in a tensor `XH` of shape [*batch size*,
    *input_size* + *hidden size*]. Then we can pass `XH` to the memory cell to get
    the new hidden state.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个时间步，我们希望将当前的输入 `X_t` 和隐藏状态 `H` 同时输入到记忆单元中。为此，我们首先需要将 `X_t` 和 `H` 在第一个维度上连接起来，得到形状为
    [*批大小*，*输入大小* + *隐藏大小*] 的张量 `XH`。然后我们可以将 `XH` 传递给记忆单元以获取新的隐藏状态。
- en: After the loop, `H` represents the final hidden state. We pass it through the
    output `nn.Linear` layer, and we get our final prediction of shape [*batch size*,
    *output size*].
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环结束后，`H` 代表最终的隐藏状态。我们将其通过输出层 `nn.Linear`，得到形状为 [*批大小*，*输出大小*] 的最终预测。
- en: In short, this model initializes the hidden state to zeros, then it goes through
    each time step and applies the memory cell to both the current inputs and the
    last hidden state, which gives it the new hidden state. It repeats this process
    until the last time step, then it passes the last hidden state through a linear
    layer to get the actual forecasts. All of this is performed simultaneously for
    every sequence in the batch.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，该模型将隐藏状态初始化为零，然后通过每个时间步，将记忆单元应用于当前输入和最后一个隐藏状态，从而得到新的隐藏状态。它重复此过程，直到最后一个时间步，然后通过线性层传递最后一个隐藏状态以获取实际的预测。所有这些操作都是针对批次中的每个序列同时进行的。
- en: So that’s our first recurrent model! It’s a sequence-to-vector model. Since
    there’s a single output neuron in this case, the output vector for each input
    sequence has a size of 1.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是我们的第一个循环模型！它是一个序列到向量的模型。由于在这种情况下只有一个输出神经元，因此每个输入序列的输出向量大小为 1。
- en: Now if you move this model to the GPU, then train and evaluate it just like
    the previous one, you will find that its validation MAE reaches 30,659\. That’s
    the best model we’ve trained so far, and it even beats the SARIMA model; we’re
    doing pretty well!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果您将此模型移至 GPU，并像之前一样进行训练和评估，您会发现其验证 MAE 达到 30,659。这是我们迄今为止训练的最佳模型，甚至超过了 SARIMA
    模型；我们做得相当不错！
- en: Tip
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: We’ve only normalized the time series, without removing trend and seasonality,
    and yet the model still performs well. This is convenient, as it makes it possible
    to quickly search for promising models without worrying too much about preprocessing.
    However, to get the best performance, you may want to try making the time series
    more stationary, for example, using differencing.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只对时间序列进行了归一化，而没有去除趋势和季节性，但模型仍然表现良好。这很方便，因为它使得可以快速搜索有潜力的模型，而无需过多担心预处理。然而，为了获得最佳性能，你可能想尝试使时间序列更加平稳，例如，使用差分法。
- en: 'PyTorch comes with an `nn.RNN` module that can greatly simplify the implementation
    of our `SimpleRnnModel`. The following implementation is (almost) equivalent to
    the previous one:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 内置的 `nn.RNN` 模块可以极大地简化 `SimpleRnnModel` 的实现。以下实现（几乎）等同于之前的实现：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The code is much shorter than before. Let’s go through it:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 代码比之前短得多。让我们来看看它：
- en: In the constructor, we now create an `nn.RNN` module instead of building a memory
    cell. We specify the input size and the hidden size, just like we did earlier,
    and we also set `batch_first=True` because our input batches have the batch dimension
    first. If we didn’t set `batch_first=True`, the `nn.RNN` module would assume that
    the time dimension comes first (i.e., it would expect the input batches to have
    a shape of [*window length*, *batch size*, *dimensionality*] instead of [*batch
    size*, *window length*, *dimensionality*]).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构造函数中，我们现在创建一个 `nn.RNN` 模块而不是构建一个记忆单元。我们指定输入大小和隐藏大小，就像我们之前做的那样，并且我们还设置了 `batch_first=True`，因为我们的输入批次具有批维度优先。如果我们没有设置
    `batch_first=True`，则 `nn.RNN` 模块将假设时间维度首先（即，它期望输入批次具有形状为 [*窗口长度*，*批大小*，*维度*] 而不是
    [*批大小*，*窗口长度*，*维度*]）。
- en: The constructor also creates an output layer, exactly like in our previous implementation.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数还创建了一个输出层，这与我们之前的实现完全相同。
- en: 'In the `forward()` method, we pass the input batch directly to the `nn.RNN`
    module. This takes care of everything: internally, it initializes the hidden state
    with zeros, and it processes each time step using a simple memory cell based on
    a linear layer and an activation function (tanh by default), much like we did
    earlier.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `forward()` 方法中，我们直接将输入批次传递给 `nn.RNN` 模块。这处理了所有事情：内部，它使用零初始化隐藏状态，并使用基于线性层和激活函数（默认为
    tanh）的简单记忆单元处理每个时间步，就像我们之前做的那样。
- en: 'Note that the `nn.RNN` module returns two things:'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，`nn.RNN` 模块返回两件事：
- en: '`outputs` is a tensor containing the outputs of the top recurrent layer at
    every time step. Right now we have a single recurrent layer, but in the next section
    we will see that the `nn.RNN` module supports multiple recurrent layers. Since
    we are dealing with a simple RNN, the outputs are just the hidden states of the
    top recurrent layer at each time step. The `outputs` tensor has a shape of [*batch
    size*, *window length*, *hidden size*] (if we didn’t set `batch_first=True`, then
    the first two dimensions would be swapped).'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` 是一个张量，包含每个时间步长顶部循环层的输出。目前我们只有一个循环层，但在下一节中我们将看到 `nn.RNN` 模块支持多个循环层。由于我们处理的是一个简单的
    RNN，输出仅仅是每个时间步长顶部循环层的隐藏状态。`outputs` 张量的形状为 [*批大小*, *窗口长度*, *隐藏大小*]（如果我们没有设置 `batch_first=True`，则前两个维度会被交换）。'
- en: '`last_state` contains the hidden state of each recurrent layer after the very
    last time step. Its shape is [*number of layers*, *batch size*, *hidden size*].
    In our case, there’s a single recurrent layer, so the size of the first dimension
    is 1.'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_state` 包含每个循环层在最后一个时间步之后的隐藏状态。其形状为 [*层数*, *批大小*, *隐藏大小*]。在我们的例子中，有一个循环层，所以第一维的大小是
    1。'
- en: In the end, we take the last output (which is also the last state of the top
    recurrent layer) and we pass it through our `nn.Linear` output layer.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们取最后一个输出（也是顶部循环层的最后一个状态）并通过我们的 `nn.Linear` 输出层传递。
- en: If you train this model, you will get a similar result as before, but generally
    much faster because the `nn.RNN` module is well optimized. In particular, when
    using an Nvidia GPU, the `nn.RNN` module leverages the cuDNN library which provides
    highly optimized implementations of various neural net architectures, including
    several RNN architectures.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你训练这个模型，你将得到与之前相似的结果，但通常要快得多，因为 `nn.RNN` 模块得到了很好的优化。特别是，当使用 Nvidia GPU 时，`nn.RNN`
    模块利用 cuDNN 库，该库提供了各种神经网络架构的高度优化的实现，包括几个 RNN 架构。
- en: Note
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The `nn.RNN` module uses two bias parameters: one for the inputs, and the other
    for the hidden states. It just adds them up, so this really doesn’t improve the
    model at all, but this extra parameter is required by the cuDNN library. This
    explains why you won’t get exactly the same results as before.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.RNN` 模块使用两个偏置参数：一个用于输入，另一个用于隐藏状态。它只是将它们相加，所以这实际上并没有提高模型，但这个额外的参数是 cuDNN
    库所必需的。这就是为什么你不会得到与之前完全相同的结果。'
- en: Forecasting Using a Deep RNN
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用深度 RNN 进行预测
- en: It is quite common to stack multiple layers of cells, as shown in [Figure 13-10](#deep_rnn_diagram).
    This gives you a *deep RNN*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠多个层是很常见的，如图 [图 13-10](#deep_rnn_diagram) 所示。这给你一个 *深度 RNN*。
- en: '![Diagram showing a deep recurrent neural network (RNN) with stacked layers,
    unrolled through time to illustrate sequential data processing.](assets/hmls_1310.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![展示具有堆叠层、通过时间展开以说明序列数据处理过程的深度循环神经网络 (RNN) 图](assets/hmls_1310.png)'
- en: Figure 13-10\. A deep RNN (left) unrolled through time (right)
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-10\. 一个深度 RNN（左）通过时间展开（右）
- en: 'Implementing a deep RNN with PyTorch is straightforward: just set the `num_layers`
    argument to the desired number of recurrent layers when creating the `nn.RNN`
    module. For example, if you set `num_layers=3` when creating the `nn.RNN` module
    in the previous model’s constructor, you get a three-layer RNN (the rest of the
    code remains unchanged):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 实现深度 RNN 很简单：只需在创建 `nn.RNN` 模块时将 `num_layers` 参数设置为所需的循环层数。例如，如果你在先前模型的构造函数中创建
    `nn.RNN` 模块时设置 `num_layers=3`，你将得到一个三层 RNN（其余代码保持不变）：
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you train and evaluate this model, you will find that it reaches an MAE of
    29,273\. That’s our best model so far!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你训练和评估这个模型，你会发现它达到了 29,273 的 MAE。这是我们迄今为止最好的模型！
- en: Forecasting Multivariate Time Series
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多变量时间序列预测
- en: 'An important quality of neural networks is their flexibility: in particular,
    they can deal with multivariate time series with almost no change to their architecture.
    For example, let’s try to forecast the rail time series using both the rail and
    bus data as input. In fact, let’s also throw in the day type! Since we can always
    know in advance whether tomorrow is going to be a weekday, a weekend, or a holiday,
    we can shift the day type series one day into the future, so that the model is
    given tomorrow’s day type as input. For simplicity, we’ll do this processing using
    Pandas:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个重要特性是其灵活性：特别是，它们可以几乎不改变其架构来处理多元时间序列。例如，让我们尝试使用铁路和公交数据作为输入来预测铁路时间序列。实际上，让我们也加入天气类型！由于我们总是可以提前知道明天是工作日、周末还是假日，我们可以将天气类型序列提前一天，这样模型就可以得到明天的天气类型作为输入。为了简单起见，我们将使用Pandas进行此处理：
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now `df_mulvar` is a DataFrame with five columns: the rail and bus data, plus
    three columns containing the one-hot encoding of the next day’s type (recall that
    there are three possible day types, `W`, `A`, and `U`). Next, we can proceed much
    like we did earlier. First we split the data into three periods, scale it down
    by a factor of one million, and convert it to tensors:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`df_mulvar`是一个包含五个列的DataFrame：铁路和公交数据，以及包含下一天类型one-hot编码的三个列（回想一下，有三种可能的天气类型，`W`、`A`和`U`）。接下来，我们可以像之前一样进行操作。首先，我们将数据分为三个时期，将其按一百万的比例缩小，并将其转换为张量：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then we need to create the PyTorch datasets. If we used the `TimeSeriesDataset`
    for this, the targets would include the next day’s rail and bus ridership, as
    well as the one-hot encoding of the following day type. Since we only want to
    predict the rail ridership for now, we must tweak the `TimeSeriesDataset` to keep
    only the first value in the target, which is the rail ridership. One way to do
    this is to create a new `MulvarTimeSeriesDataset` class that extends the `TimeSeriesDataset`
    class and tweaks the `__getitem__()` method to filter the target:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要创建PyTorch数据集。如果我们使用`TimeSeriesDataset`，目标将包括第二天铁路和公交客流量，以及下一天类型的one-hot编码。由于我们目前只想预测铁路客流量，我们必须调整`TimeSeriesDataset`，只保留目标中的第一个值，即铁路客流量。一种方法是为`TimeSeriesDataset`创建一个新的`MulvarTimeSeriesDataset`类，该类扩展了`TimeSeriesDataset`类，并调整了`__getitem__()`方法以过滤目标：
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we can create the datasets and the data loaders, much like we did earlier:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建数据集和数据加载器，就像我们之前做的那样：
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If you look at the batches produced by the data loaders, you will find that
    the input shape is [32, 56, 5], and the target shape is [32, 1]. Perfect!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看数据加载器产生的批次，你会发现输入形状是[32, 56, 5]，目标形状是[32, 1]。完美！
- en: 'So we can finally create the RNN:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们最终可以创建RNN：
- en: '[PRE24]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Notice that this model is identical to the `univar_model` RNN we built earlier,
    except `input_size=5`: at each time step, the model now receives five inputs instead
    of one. This model actually reaches a validation MAE of 23,227\. Now we’re making
    big progress!'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到这个模型与之前我们构建的`univar_model` RNN完全相同，只是`input_size=5`：在每一个时间步，模型现在接收五个输入而不是一个。实际上，这个模型达到了验证集的MAE为23,227。现在我们取得了很大的进步！
- en: In fact, it’s not too hard to make the RNN forecast both the rail and bus ridership.
    You just need to return `target[:2]` instead of `target[:1]` in the `MulvarTimeSeriesDataset`
    class, and set `output_size=2` when creating the `SimpleRnnModel`, that’s all
    there is to it!
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，让RNN预测铁路和公交客流量并不太难。你只需要在`MulvarTimeSeriesDataset`类中返回`target[:2]`而不是`target[:1]`，并在创建`SimpleRnnModel`时设置`output_size=2`，这就是全部了！
- en: As we discussed in [Chapter 9](ch09.html#ann_chapter), using a single model
    for multiple related tasks often results in better performance than using a separate
    model for each task, since features learned for one task may be useful for the
    other tasks, and also because having to perform well across multiple tasks prevents
    the model from overfitting (it’s a form of regularization). However, it depends
    on the task, and in this particular case the multitask RNN that forecasts both
    the bus and the rail ridership doesn’t perform quite as well as dedicated models
    that forecast one or the other (using all five columns as input). Still, it reaches
    a validation MAE of 26,441 for rail and 26,178 for bus, which is still pretty
    good.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第9章](ch09.html#ann_chapter)中讨论的，使用单个模型执行多个相关任务通常比为每个任务使用单独的模型性能更好，因为为某个任务学习到的特征可能对其他任务也有用，并且因为需要在多个任务上表现良好，这可以防止模型过拟合（这是一种正则化形式）。然而，这取决于任务，在这个特定的情况下，预测公交和铁路客流量的多任务RNN并不像专门预测一个或另一个（使用所有五个列作为输入）的模型表现得那么好。尽管如此，它达到了铁路验证MAE为26,441，公交车为26,178，这仍然相当不错。
- en: Forecasting Several Time Steps Ahead
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测多个时间步长
- en: So far we have only predicted the value at the next time step, but we could
    just as easily have predicted the value several steps ahead by changing the targets
    appropriately (e.g., to predict the ridership 2 weeks from now, we could just
    change the targets to be the value 14 days ahead instead of 1 day ahead). But
    what if we want to predict the next 14 values with a single model?
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只预测了下一个时间步长的值，但我们可以通过适当地更改目标（例如，为了预测两周后的客流量，我们可以将目标更改为14天前的值而不是1天前的值）来预测多个步骤。但如果我们想使用单个模型预测下一个14个值怎么办？
- en: 'The first option is to take the `univar_model` RNN we trained earlier for the
    rail time series, make it predict the next value, and add that value to the inputs,
    acting as if the predicted value had actually occurred. We then use the model
    again to predict the following value, and so on, as in the following code:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种选择是使用我们之前为铁路时间序列训练的`univar_model` RNN，预测下一个值，并将该值添加到输入中，就像预测的值实际上已经发生一样。然后我们再次使用模型来预测下一个值，依此类推，如下面的代码所示：
- en: '[PRE25]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this code, we take the rail ridership of the first 56 days of the validation
    period, we add a batch dimension of size 1 using the `unsqueeze()` method (since
    our `univar_model` expects 3D inputs), then we move the tensor to the GPU. Now
    the shape of `X` is [1, 56, 1]. Then we repeatedly use the model to forecast the
    next value, and we append each forecast to the input series, along the time axis
    (`dim=1`). Since each prediction has a shape of [1, 1], we must use `unsqueeze()`
    again to add a batch dimension of size 1 before we can concatenate it to `X`.
    In the end, `X` has a shape of [1, 56 + 14, 1], and our final forecasts are the
    last 14 values of `X`. The resulting forecasts are plotted in [Figure 13-11](#forecast_ahead_plot).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们取验证期前56天的铁路客流量，使用`unsqueeze()`方法添加一个大小为1的批次维度（因为我们的`univar_model`期望3D输入），然后将张量移动到GPU上。现在`X`的形状是[1,
    56, 1]。然后我们反复使用模型来预测下一个值，并将每个预测值追加到输入序列中，沿着时间轴（`dim=1`）。由于每个预测的形状为[1, 1]，我们必须再次使用`unsqueeze()`来添加一个大小为1的批次维度，然后才能将其连接到`X`。最后，`X`的形状变为[1,
    56 + 14, 1]，我们的最终预测是`X`的最后14个值。生成的预测结果在[图13-11](#forecast_ahead_plot)中展示。
- en: Warning
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'If the model makes an error at one time step, then the forecasts for the following
    time steps are impacted as well: the errors tend to accumulate. So, it’s preferable
    to use this technique only for a small number of steps.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型在某个时间步长出现错误，那么后续时间步长的预测也会受到影响：错误往往会累积。因此，最好只使用这种技术进行少数几个步骤。
- en: '![Line graph comparing actual vs. predicted data points over time, highlighting
    forecasts for 14 steps ahead made individually, with potential error accumulation.](assets/hmls_1311.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![比较实际与预测数据点随时间变化的线图，突出显示逐个预测14个步骤的预测，并显示潜在的误差累积。](assets/hmls_1311.png)'
- en: Figure 13-11\. Forecasting 14 steps ahead, 1 step at a time
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-11. 逐个预测14个步骤的预测
- en: 'The second option is to train an RNN to predict the next 14 values in one shot.
    We can still use a sequence-to-vector model, but it will output 14 values instead
    of 1\. However, we first need to change the targets to be vectors containing the
    next 14 values. For this, we can create the following class:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择是训练一个RNN一次性预测接下来的14个值。我们仍然可以使用序列到向量的模型，但它将输出14个值而不是1个。然而，我们首先需要将目标更改为包含下一个14个值的向量。为此，我们可以创建以下类：
- en: '[PRE26]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Tip
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: I’ve hardcoded the number 14, but in a real project you should make this configurable
    (e.g., just like the `window_length`).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我硬编码了数字14，但在实际项目中你应该使其可配置（例如，就像`window_length`一样）。
- en: 'This class inherits from the `TimeSeriesDataset` class and tweaks its `__len__()`
    and `__getitem__()` methods. The target is now a tensor containing the next 14
    rail ridership values, rather than just the next value. We can once again create
    a training set, a validation set, and a test set, based on the multivariate time
    series we built earlier:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类继承自`TimeSeriesDataset`类，并调整了其`__len__()`和`__getitem__()`方法。现在目标是一个包含下一个14个铁路客流量值的张量，而不仅仅是下一个值。我们可以再次创建一个训练集、一个验证集和一个测试集，基于我们之前构建的多变量时间序列：
- en: '[PRE27]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Lastly, we can create a simple RNN, just like the `mulvar_model`, but with
    `output_size=14`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以创建一个简单的RNN，就像`mulvar_model`一样，但`output_size=14`：
- en: '[PRE28]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'After training this model, you can predict the next 14 values at once, like
    this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个模型后，你可以一次性预测下一个14个值，如下所示：
- en: '[PRE29]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This approach works quite well. Its forecasts for the next day are obviously
    better than its forecasts for 14 days into the future, but it doesn’t accumulate
    errors like the previous approach did. Now let’s see whether a sequence-to-sequence
    model can do even better.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法相当有效。它对下一天的预测显然比对未来14天的预测要好，但它不会像之前的方法那样累积误差。现在让我们看看序列到序列模型是否可以做得更好。
- en: Tip
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'You can combine both approaches to forecast many steps ahead: use a model that
    forecasts the next 14 days in one shot, then append the forecasts to the inputs
    and run the model again to get forecasts for the following 14 days, and so on.⁠^([8](ch13.html#id3135))'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这两种方法结合起来，预测很多步的未来：一次性使用一个模型来预测接下来的14天，然后将预测结果附加到输入上，再次运行模型以获取接下来14天的预测，依此类推。⁠^([8](ch13.html#id3135))
- en: Forecasting Using a Sequence-to-Sequence Model
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用序列到序列模型进行预测
- en: Instead of training the model to forecast the next 14 values only at the very
    last time step, we can train it to forecast the next 14 values at each and every
    time step. To be clear, at time step 0 the model will output a vector containing
    the forecasts for time steps 1 to 14, then at time step 1 the model will forecast
    time steps 2 to 15, and so on. In other words, the targets are sequences of consecutive
    windows, shifted by one time step at each time step. The target for each input
    window is not a vector anymore, but a sequence of the same length as the inputs,
    containing a 14-dimensional vector at each step. Given an input batch of shape
    [*batch size*, *window length*, *input size*], the output will have a shape of
    [*batch size*, *window length*, *output_size*]. This is no longer a sequence-to-vector
    RNN, it’s a sequence-to-sequence (or *seq2seq*) RNN.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是只在最后一个时间步训练模型来预测接下来的14个值，而是可以在每个时间步都训练它来预测接下来的14个值。为了清楚起见，在时间步0时，模型将输出一个包含从时间步1到14的预测的向量，然后在时间步1时，模型将预测时间步2到15，依此类推。换句话说，目标是连续窗口的序列，每个时间步向前移动一个时间步。对于每个输入窗口的目标不再是向量，而是一个与输入长度相同的序列，每个步骤包含一个14维向量。给定一个形状为[*batch
    size*, *window length*, *input size*]的输入批次，输出将具有形状[*batch size*, *window length*,
    *output_size*]。这不再是一个序列到向量的RNN，而是一个序列到序列（或*seq2seq*）RNN。
- en: Note
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'It may be surprising that the targets contain values that appear in the inputs
    (except for the last time step). Isn’t that cheating? Fortunately, not at all:
    at each time step, an RNN only knows about past time steps; it cannot look ahead.
    It is said to be a *causal* model.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会令人惊讶的是，目标中包含出现在输入中的值（除了最后一个时间步）。这不是作弊吗？幸运的是，根本不是：在每个时间步，RNN只知道过去的时间步；它不能向前看。它被称为*因果*模型。
- en: 'You may be wondering why we would want to train a seq2seq model when we’re
    really only interested in forecasting future values, which are output by our model
    at the very last time step. And you’re right: after training, you can actually
    ignore all outputs except for the very last time step. The main advantage of this
    technique is that the loss will contain a term for the output of the RNN at each
    and every time step, not just for the output at the last time step. This means
    there will be many more error gradients flowing through the model, and they won’t
    have to flow through time as much since they will come from the output of each
    time step, not just the last one. This can both stabilize training and speed up
    convergence. Moreover, since the model must make predictions at each time step,
    it will see input sequences of varying lengths, which can reduce the risk of overfitting
    the model to the specific window length used during training. Well, at least that’s
    the hope! Let’s give this technique a try on the rail ridership time series. As
    usual, we first need to prepare the dataset:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道，当我们真正只对预测未来值感兴趣时，为什么还要训练一个seq2seq模型，这些未来值是在模型的最后一个时间步输出的。你是对的：训练完成后，实际上可以忽略所有输出，除了最后一个时间步的输出。这种技术的主要优势是，损失将包含每个时间步RNN输出的项，而不仅仅是最后一个时间步的输出。这意味着将有更多的错误梯度流经模型，并且由于它们将来自每个时间步的输出，而不是仅仅来自最后一个，因此它们不需要像以前那样在时间上流动得那么多。这既可以稳定训练，又可以加快收敛速度。此外，由于模型必须在每个时间步进行预测，它将看到不同长度的输入序列，这可以降低模型过度拟合训练期间使用的特定窗口长度的风险。好吧，至少这是我们的希望！让我们尝试将这种技术应用于铁路客流量时间序列。像往常一样，我们首先需要准备数据集：
- en: '[PRE30]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Our new `Seq2SeqDataset` class inherits from the `ForecastAheadDataset` class
    and overrides the `__getitem__()` method: the input window is defined just like
    before, but the target is now a sequence of consecutive windows, shifted by one
    time step at each time step. The `unfold()` method is where the magic happens:
    it takes a tensor and produces sliding blocks from it. For this, it repeatedly
    slides along the given dimension by the given number of steps and extracts a block
    of the given size. For example:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新`Seq2SeqDataset`类继承自`ForecastAheadDataset`类，并重写了`__getitem__()`方法：输入窗口的定义和以前一样，但现在目标是每个时间步连续的窗口，每个时间步向前移动一个时间步。`unfold()`方法就是魔法所在：它从一个张量中产生滑动块。为此，它沿着给定的维度以给定的步数反复滑动，并提取给定大小的块。例如：
- en: '[PRE31]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once again we must create a training set, a validation set, and a test set,
    as well as the corresponding data loaders:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须再次创建一个训练集、一个验证集和一个测试集，以及相应的数据加载器：
- en: '[PRE32]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'And lastly, we can build the sequence-to-sequence model:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以构建序列到序列模型：
- en: '[PRE33]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We inherit from the `SimpleRnnModel` class, and we override the `forward()`
    method. Instead of applying the linear `self.output` layer only to the outputs
    of the last time step, as we did before, we now apply it to the outputs of every
    time step. It may surprise you that this works at all. So far, we have only applied
    `nn.Linear` layers to 2D inputs of shape [*batch size*, *features*], but here
    the `outputs` tensor has a shape of [*batch size*, *window length*, *hidden size*]:
    it’s 3D, not 2D! Luckily, this works fine as the `nn.Linear` layer will automatically
    be applied to each time step, so the model’s predictions will have a shape of
    [*batch size*, *window length*, *output size*]: just what we need.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`SimpleRnnModel`类继承，并重写了`forward()`方法。与之前只将线性`self.output`层应用于最后一个时间步的输出不同，我们现在将其应用于每个时间步的输出。这可能会让你感到惊讶，但到目前为止，我们只将`nn.Linear`层应用于形状为[*批大小*，*特征*]的2D输入，但这里的`outputs`张量形状为[*批大小*，*窗口长度*，*隐藏大小*]：它是3D的，而不是2D！幸运的是，这可以正常工作，因为`nn.Linear`层将自动应用于每个时间步，因此模型的预测将具有形状[*批大小*，*窗口长度*，*输出大小*]：这正是我们需要的。
- en: 'Under the hood, the `nn.Linear` layer relies on `torch.matmul()` for matrix
    multiplication. This function efficiently supports multiplying arrays of more
    than two dimensions. For example, you can multiply an array of shape [2, 3, 5,
    7] with an array of shape [2, 3, 7, 11]. Indeed, these two arrays can both be
    seen as 2 × 3 grids of matrices, and `torch.matmul()` simply multiplies the corresponding
    matrices in both grids. Since multiplying a 5 × 7 matrix with a 7 × 11 matrix
    produces a 5 × 11 matrix, the final result is a 2 × 3 grid of 5 × 11 matrices,
    represented as a tensor of shape [2, 3, 5, 11]. Broadcasting is also supported;
    for example, you can multiply an array of shape [10, 56, 32] with an array of
    shape [32, 14]: each of the ten 56 × 32 matrices in the first array will be multiplied
    by the same 32 × 14 matrix in the second array, and you will get a tensor of shape
    [10, 56, 14]. That’s what happens when you pass a 3D input to an `nn.Linear` layer.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，`nn.Linear`层依赖于`torch.matmul()`进行矩阵乘法。这个函数有效地支持乘以超过两个维度的数组。例如，你可以将形状为[2,
    3, 5, 7]的数组与形状为[2, 3, 7, 11]的数组相乘。实际上，这两个数组都可以看作是2×3的矩阵网格，而`torch.matmul()`只是简单地乘以两个网格中对应的矩阵。由于将5×7矩阵与7×11矩阵相乘会产生一个5×11矩阵，最终结果是一个2×3的5×11矩阵网格，表示为一个形状为[2,
    3, 5, 11]的张量。广播也被支持；例如，你可以将形状为[10, 56, 32]的数组与形状为[32, 14]的数组相乘：第一个数组中的十个56×32矩阵将分别与第二个数组中的相同的32×14矩阵相乘，你将得到一个形状为[10,
    56, 14]的张量。这就是当你将3D输入传递给`nn.Linear`层时发生的事情。
- en: Tip
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Another way to get the exact same result is to replace the `nn.Linear` output
    layer with an `nn.Conv1d` layer using a kernel size of one (i.e., `Conv1d(32,
    14, kernel_size=1)`). However, you would have to swap the last two dimensions
    of both the inputs and the outputs, treating the time dimension as a spatial dimension.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种得到相同结果的方法是将`nn.Linear`输出层替换为使用大小为1的核的`nn.Conv1d`层（即`Conv1d(32, 14, kernel_size=1)`）。然而，你必须交换输入和输出的最后两个维度，将时间维度视为空间维度。
- en: 'The training code is the same as usual. During training, all the model’s outputs
    are used, but after training, only the outputs of the very last time step matter,
    and the rest can be ignored (as mentioned earlier). For example, we can forecast
    the rail ridership for the next 14 days like this:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 训练代码与平常一样。在训练过程中，使用模型的所有输出，但在训练后，只有最后一个时间步的输出才是重要的，其余的可以忽略（如前所述）。例如，我们可以这样预测未来14天的铁路客流量：
- en: '[PRE34]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: If you evaluate this model’s forecasts for *t* + 1, you will find a validation
    MAE of 23,350, which is very good. Of course, the model is not as accurate for
    more distant forecasts. For example, the MAE for *t* + 14 is 35,315.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你评估这个模型对*t+1*的预测，你会发现验证MAE为23,350，这是一个非常好的结果。当然，对于更远的预测，模型的准确性并不高。例如，*t+14*的MAE为35,315。
- en: Simple RNNs can be quite good at forecasting time series or handling other kinds
    of sequences, but they do not perform as well on long time series or sequences.
    Let’s discuss why and see what we can do about it.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的RNN在预测时间序列或处理其他类型的序列方面可能相当出色，但它们在长时间序列或序列上的表现并不好。让我们讨论一下原因，并看看我们能做些什么。
- en: Handling Long Sequences
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理长序列
- en: 'To train an RNN on long sequences, we must run it over many time steps, making
    the unrolled RNN a very deep network. Just like any deep neural network, it may
    suffer from the unstable gradients problem, discussed in [Chapter 11](ch11.html#deep_chapter):
    it may take forever to train, or training may be unstable. Moreover, when an RNN
    processes a long sequence, it will gradually forget the first inputs in the sequence.
    Let’s look at both these problems, starting with the unstable gradients problem.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 要在长序列上训练一个RNN，我们必须运行它经过许多时间步，这使得展开的RNN成为一个非常深的网络。就像任何深度神经网络一样，它可能遭受不稳定梯度问题，这在[第11章](ch11.html#deep_chapter)中有讨论：训练可能永远无法完成，或者训练可能不稳定。此外，当RNN处理长序列时，它会逐渐忘记序列中的第一个输入。让我们看看这两个问题，先从不稳定梯度问题开始。
- en: Fighting the Unstable Gradients Problem
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应对不稳定梯度问题
- en: 'Many of the tricks we used in deep nets to alleviate the unstable gradients
    problem can also be used for RNNs: good parameter initialization, faster optimizers,
    dropout, and so on. However, nonsaturating activation functions (e.g., ReLU) may
    not help as much here. In fact, they may actually lead the RNN to be even more
    unstable during training. Why? Well, suppose gradient descent updates the weights
    in a way that increases the outputs slightly at the first time step. Because the
    same weights are used at every time step, the outputs at the second time step
    may also be slightly increased, and those at the third, and so on until the outputs
    explode—and a nonsaturating activation function does not prevent that.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在深度网络中使用的许多技巧也可以用于RNNs来减轻不稳定梯度的問題：良好的参数初始化、更快的优化器、dropout等。然而，非饱和激活函数（例如ReLU）可能在这里帮助不大。实际上，它们可能实际上会导致RNN在训练期间变得更加不稳定。为什么？好吧，假设梯度下降以增加第一个时间步输出的方式更新权重。因为每个时间步都使用相同的权重，所以第二个时间步的输出也可能略微增加，第三步也是如此，直到输出爆炸——而非饱和激活函数不能阻止这一点。
- en: You can reduce this risk by using a smaller learning rate, or you can use a
    saturating activation function like the hyperbolic tangent (this explains why
    it’s the default).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用较小的学习率来降低这种风险，或者您可以使用饱和激活函数，如双曲正切函数（这也解释了为什么它是默认的）。
- en: In much the same way, the gradients themselves can explode. If you notice that
    training is unstable, you may want to monitor the size of the gradients and perhaps
    use gradient clipping.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，梯度本身也可能爆炸。如果您注意到训练不稳定，您可能想要监控梯度的尺寸，并可能使用梯度裁剪。
- en: Moreover, batch normalization cannot be used as efficiently with RNNs as with
    deep feedforward nets. In fact, you cannot use it between time steps, only between
    recurrent layers. To be more precise, it is technically possible to add a BN layer
    to a memory cell so that it will be applied at each time step (both on the inputs
    for that time step and on the hidden state from the previous step). However, this
    implies that the same BN layer will be used at each time step, with the same parameters,
    regardless of the actual scale and offset of the inputs and hidden state. In practice,
    this does not yield good results, as was demonstrated by César Laurent et al.
    in a [2015 paper](https://homl.info/rnnbn).⁠^([9](ch13.html#id3144)) The authors
    found that BN was slightly beneficial only when it was applied to the layer’s
    inputs, not to the hidden states. In other words, it was slightly better than
    nothing when applied between recurrent layers (i.e., vertically in [Figure 13-10](#deep_rnn_diagram)),
    but not within recurrent layers (i.e., horizontally).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与深度前馈网络相比，批归一化在RNNs中的使用效率较低。实际上，您不能在时间步之间使用它，只能在循环层之间使用。更准确地说，从技术上讲，可以在记忆单元中添加一个BN层，以便在每个时间步应用它（包括该时间步的输入和前一步的隐藏状态）。然而，这意味着相同的BN层将在每个时间步使用，使用相同的参数，而不管输入和隐藏状态的实际规模和偏移量。实际上，这并没有产生良好的结果，正如César
    Laurent等人2015年在一篇[论文](https://homl.info/rnnbn)中展示的那样。⁠^([9](ch13.html#id3144))
    作者发现，当BN应用于层的输入而不是隐藏状态时，它只有轻微的益处。换句话说，当应用于循环层之间（即[图13-10](#deep_rnn_diagram)中的垂直方向）时，它略好于无，但在循环层内部（即水平方向）则不是。
- en: 'Layer norm (introduced in [Chapter 11](ch11.html#deep_chapter)) tends to work
    a bit better than BN within recurrent layers. It is usually applied just before
    the activation function, at each time step. Sadly, PyTorch’s `nn.RNN` module does
    not support LN, so you have to implement the RNN’s loop manually (as we did earlier),
    and apply the `nn.LayerNorm` module at each iteration. This is not too hard, but
    you do lose the simplicity and speed of the `nn.RNN` module. For example, you
    can take the first version of our `SimpleRnnModel` class and add an `nn.LayerNorm`
    module to the memory cell, just before the tanh activation function:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化（在第11章中介绍）在循环层内通常比BN表现得更好。它通常在每个时间步之前应用于激活函数。遗憾的是，PyTorch的`nn.RNN`模块不支持LN，因此您必须手动实现RNN的循环（就像我们之前做的那样），并在每个迭代中应用`nn.LayerNorm`模块。这并不太难，但您确实失去了`nn.RNN`模块的简单性和速度。例如，您可以将我们的`SimpleRnnModel`类的第一个版本添加一个`nn.LayerNorm`模块到记忆单元中，就在tanh激活函数之前：
- en: '[PRE35]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Warning
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Layer norm does not always help; you just have to try it. In general, it works
    better in *gated RNNs* such as LSTM and GRU (discussed shortly). It is also more
    likely to help when the time series is preprocessed to remove any seasonality
    or trend.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化并不总是有帮助；你只需要尝试一下。一般来说，它在*门控RNN*如LSTM和GRU（稍后讨论）中表现更好。当时间序列经过预处理以去除任何季节性或趋势时，它更有可能有所帮助。
- en: 'Similarly, if you wish to apply dropout between each time step, you must write
    a custom RNN since the `nn.RNN` module does not support that. However, it does
    support adding a dropout layer after every recurrent layer: simply set the `dropout`
    hyperparameter to the desired dropout rate (it defaults to zero).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果你希望在每一步之间应用dropout，你必须编写一个自定义的RNN，因为`nn.RNN`模块不支持这一点。然而，它支持在每个循环层之后添加dropout层：只需将`dropout`超参数设置为所需的dropout率（默认为0）。
- en: With these techniques, you can alleviate the unstable gradients problem and
    train an RNN much more efficiently. Now let’s look at how to deal with the short-term
    memory problem.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些技术，你可以减轻梯度不稳定的问题，并更有效地训练RNN。现在让我们看看如何处理短期记忆问题。
- en: Tip
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When forecasting time series, it is often useful to have some error bars along
    with your predictions. For this, one approach is to use MC dropout (introduced
    in [Chapter 11](ch11.html#deep_chapter)).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 当预测时间序列时，在预测结果旁边有一些误差条通常很有用。为此，一种方法是在[第11章](ch11.html#deep_chapter)中介绍的多变量dropout（MC
    dropout）。
- en: Tackling the Short-Term Memory Problem
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决短期记忆问题
- en: 'Due to the transformations that the data goes through when traversing an RNN,
    some information is lost at each time step. After a while, the RNN’s state contains
    virtually no trace of the first inputs. This can be a showstopper. Imagine Dory
    the fish⁠^([10](ch13.html#id3155)) trying to translate a long sentence; by the
    time she’s finished reading it, she has no clue how it started. To tackle this
    problem, various types of cells with long-term memory have been introduced. They
    have proven so successful that the basic cells are not used much anymore. Let’s
    first look at the most popular of these long-term memory cells: the LSTM cell.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据在穿越RNN时经历的转换，每个时间步长都会丢失一些信息。过了一段时间后，RNN的状态几乎没有任何关于第一个输入的痕迹。这可能会成为致命的问题。想象一下多莉鱼⁠^([10](ch13.html#id3155))试图翻译一个长句子；当她读完的时候，她已经不知道它是如何开始的。为了解决这个问题，已经引入了各种具有长期记忆的单元。它们已经证明非常成功，以至于基本的单元已经很少使用了。让我们首先看看这些长期记忆单元中最受欢迎的：LSTM单元。
- en: LSTM cells
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM单元
- en: 'The *long short-term memory* (LSTM) cell was [proposed in 1997](https://homl.info/93)⁠^([11](ch13.html#id3156))
    by Sepp Hochreiter and Jürgen Schmidhuber and gradually improved over the years
    by several researchers, such as [Alex Graves](https://homl.info/graves), [Haşim
    Sak](https://homl.info/94),⁠^([12](ch13.html#id3157)) and [Wojciech Zaremba](https://homl.info/95).⁠^([13](ch13.html#id3158))
    You can simply use the `nn.LSTM` module instead of the `nn.RNN` module; it’s a
    drop-in replacement, and it usually performs much better: training converges faster,
    and the model detects longer-term patterns in the data.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*长短期记忆*（LSTM）单元是由Sepp Hochreiter和Jürgen Schmidhuber于1997年[提出](https://homl.info/93)⁠^([11](ch13.html#id3156))，并且经过多年多位研究者的逐步改进，例如[Alex
    Graves](https://homl.info/graves)，[Haşim Sak](https://homl.info/94)，⁠^([12](ch13.html#id3157))和[Wojciech
    Zaremba](https://homl.info/95).⁠^([13](ch13.html#id3158))。你可以简单地使用`nn.LSTM`模块来代替`nn.RNN`模块；这是一个即插即用的替换，并且通常表现更好：训练收敛更快，模型能够检测数据中的长期模式。'
- en: 'So how does this magic work? Well, the LSTM architecture is shown in [Figure 13-12](#lstm_cell_diagram).
    If you don’t look at what’s inside the box, the LSTM cell looks exactly like a
    regular cell, except that its state is split into two vectors: **h**[(*t*)] and
    **c**[(*t*)] (“c” stands for “cell”). You can think of **h**[(*t*)] as the short-term
    state and **c**[(*t*)] as the long-term state.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这种魔法是如何工作的呢？好吧，LSTM架构在[图13-12](#lstm_cell_diagram)中展示。如果你不查看盒子内部的内容，LSTM单元看起来就像一个普通的单元，只不过其状态被分成两个向量：**h**[(*t*)]和**c**[(*t*)]（“c”代表“cell”）。你可以将**h**[(*t*)]视为短期状态，将**c**[(*t*)]视为长期状态。
- en: '![Diagram of an LSTM cell showing the flow of information through forget, input,
    and output gates, illustrating how short-term and long-term states are updated.](assets/hmls_1312.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM单元的流程图，展示了通过遗忘、输入和输出门的信息流，说明了短期和长期状态是如何更新的。](assets/hmls_1312.png)'
- en: Figure 13-12\. An LSTM cell
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-12\. LSTM单元
- en: Now let’s open the box! The key idea is that the network can learn what to store
    in the long-term state, what to throw away, and what to read from it. As the long-term
    state **c**[(*t*–1)] traverses the network from left to right, you can see that
    it first goes through a *forget gate*, dropping some memories, and then it adds
    some new memories via the addition operation (which adds the memories that were
    selected by an *input gate*). The result **c**[(*t*)] is sent straight out without
    any further transformation. So at each time step, some memories are dropped and
    some memories are added. Moreover, after the addition operation, the long-term
    state is copied and passed through the tanh function, and the result is filtered
    by the *output gate*. This produces the short-term state **h**[(*t*)] (which is
    equal to the cell’s output for this time step, **y**[(*t*)]). Now let’s look at
    where new memories come from and how the gates work.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开盒子！关键思想是网络可以学习在长期状态中存储什么，丢弃什么，以及从中读取什么。当长期状态**c**[(*t*–1)]从左到右穿越网络时，你可以看到它首先通过一个*遗忘门*，丢弃一些记忆，然后通过加法操作添加一些新的记忆（这些记忆是通过*输入门*选择的）。结果**c**[(*t*)]直接输出，没有任何进一步的转换。所以，在每一个时间步，一些记忆被丢弃，一些记忆被添加。此外，在加法操作之后，长期状态被复制并通过tanh函数传递，结果通过*输出门*过滤。这产生了短期状态**h**[(*t*)]（它等于这个时间步的细胞输出**y**[(*t*)]）。现在让我们看看新的记忆从哪里来以及门是如何工作的。
- en: 'First, the current input vector **x**[(*t*)] and the previous short-term state
    **h**[(*t*–1)] are fed to four different fully connected layers. They all serve
    a different purpose:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当前输入向量**x**[(*t*)]和前一个短期状态**h**[(*t*–1)]被输入到四个不同的全连接层。它们各自有不同的作用：
- en: The main layer is the one that outputs **g**[(*t*)]. It has the usual role of
    analyzing the current inputs **x**[(*t*)] and the previous (short-term) state
    **h**[(*t*–1)]. In a simple RNN cell, there is nothing other than this layer,
    and its output goes straight out to **y**[(*t*)] and **h**[(*t*)]. But in an LSTM
    cell, this layer’s output does not go straight out; instead its most important
    parts are stored in the long-term state (and the rest is dropped).
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要层是输出**g**[(*t*)]的那一层。它具有分析当前输入**x**[(*t*)]和前一个（短期）状态**h**[(*t*–1)]的常规作用。在一个简单的RNN细胞中，除了这一层之外没有其他层，其输出直接输出到**y**[(*t*)]和**h**[(*t*)]。但在LSTM细胞中，这一层的输出不会直接输出；相反，其最重要的部分被存储在长期状态中（其余部分被丢弃）。
- en: 'The three other layers are *gate controllers*. Since they use the logistic
    activation function, the outputs range from 0 to 1\. As you can see, the gate
    controllers’ outputs are fed to element-wise multiplication operations: if they
    output 0s they close the gate, and if they output 1s they open it. Specifically:'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外三个层是*门控制器*。由于它们使用逻辑激活函数，输出范围在0到1之间。正如你所见，门控制器的输出被输入到逐元素乘法操作：如果它们输出0，则关闭门；如果它们输出1，则打开门。具体来说：
- en: The *forget gate* (controlled by **f**[(*t*)]) controls which parts of the long-term
    state should be erased.
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*遗忘门*（由**f**[(*t*)]控制）控制哪些长期状态的部分应该被删除。'
- en: The *input gate* (controlled by **i**[(*t*)]) controls which parts of **g**[(*t*)]
    should be added to the long-term state.
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入门*（由**i**[(*t*)]控制）控制哪些**g**[(*t*)]的部分应该添加到长期状态中。'
- en: Finally, the *output gate* (controlled by **o**[(*t*)]) controls which parts
    of the long-term state should be read and output at this time step, both to **h**[(*t*)]
    and to **y**[(*t*)].
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，*输出门*（由**o**[(*t*)]控制）控制在这个时间步应该读取和输出的长期状态的部分，既包括**h**[(*t*)]也包括**y**[(*t*)]。
- en: In short, an LSTM cell can learn to recognize an important input (that’s the
    role of the input gate), store it in the long-term state, preserve it for as long
    as it is needed (that’s the role of the forget gate), and extract it whenever
    it is needed (that’s the role of the output gate), all while being fully differentiable.
    This explains why these cells have been amazingly successful at capturing long-term
    patterns in time series, long texts, audio recordings, and more.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，LSTM细胞可以学习识别重要的输入（这是输入门的作用），将其存储在长期状态中，根据需要保留它（这是遗忘门的作用），并在需要时提取它（这是输出门的作用），同时保持完全可微。这解释了为什么这些细胞在捕捉时间序列、长文本、音频记录等方面的长期模式方面取得了惊人的成功。
- en: '[Equation 13-4](#lstm_equation) summarizes how to compute the cell’s long-term
    state, its short-term state, and its output at each time step for a single instance
    (the equations for a whole mini-batch are very similar).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 13-4](#lstm_equation) 总结了如何计算单个实例（整个小批次的方程式非常相似）的细胞长期状态、短期状态以及每个时间步的输出。'
- en: Equation 13-4\. LSTM computations
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 13-4\. LSTM 计算
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">i</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>i</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>i</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">f</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>f</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>f</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>f</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">o</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>o</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>o</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>o</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mo form="prefix">tanh</mo> <mo>(</mo>
    <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>g</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>g</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>g</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi mathvariant="bold">f</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">y</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>=</mo> <msub><mi mathvariant="bold">o</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <mo form="prefix">tanh</mo> <mrow><mo>(</mo> <msub><mi mathvariant="bold">c</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable>
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在此方程中：
- en: '**W**[*xi*], **W**[*xf*], **W**[*xo*], and **W**[*xg*] are the weight matrices
    of each of the four layers for their connection to the input vector **x**[(*t*)].'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**[*xi*]，**W**[*xf*]，**W**[*xo*] 和 **W**[*xg*] 是每个四层与输入向量 **x**[(*t*)] 连接的权重矩阵。'
- en: '**W**[*hi*], **W**[*hf*], **W**[*ho*], and **W**[*hg*] are the weight matrices
    of each of the four layers for their connection to the previous short-term state
    **h**[(*t*–1)].'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**[*hi*]，**W**[*hf*]，**W**[*ho*] 和 **W**[*hg*] 是每个四层与之前短期状态 **h**[(*t*–1)]
    连接的权重矩阵。'
- en: '**b**[*i*], **b**[*f*], **b**[*o*], and **b**[*g*] are the bias terms for each
    of the four layers.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**b**[*i*]，**b**[*f*]，**b**[*o*] 和 **b**[*g*] 是每个四层的偏置项。'
- en: Tip
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Try replacing `nn.RNN` with `nn.LSTM` in the previous models and see what performance
    you can reach on the ridership dataset (a bit of hyperparameter tuning may be
    required).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试将之前的模型中的 `nn.RNN` 替换为 `nn.LSTM`，并查看在乘客数据集上可以达到的性能（可能需要进行一些超参数调整）。
- en: 'Just like for simple RNNs, if you want to add layer normalization or dropout
    at each time step, you must implement the recurrent loop manually. One option
    is to use [Equation 13-4](#lstm_equation), but a simpler option is to use the
    `nn.LSTMCell` module, which runs a single time step. For example, here is a simple
    implementation:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 就像简单的 RNN 一样，如果你想在每个时间步添加层归一化或 dropout，你必须手动实现循环。一个选项是使用 [方程式 13-4](#lstm_equation)，但一个更简单的选项是使用
    `nn.LSTMCell` 模块，它运行单个时间步。例如，这里有一个简单的实现：
- en: '[PRE36]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This is very similar to the first implementation of our `SimpleRnnModel`, but
    we are now using an `nn.LSTMCell` at each time step, and the hidden state is now
    split in two parts: the short-term `H` and the long-term `C`.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们的 `SimpleRnnModel` 的第一次实现非常相似，但现在我们在每个时间步使用 `nn.LSTMCell`，并且隐藏状态现在分为两部分：短期
    `H` 和长期 `C`。
- en: There are several variants of the LSTM cell. One particularly popular variant
    is the GRU cell, which we will look at now.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 单元有几个变体。一个特别受欢迎的变体是 GRU 单元，我们现在将探讨它。
- en: GRU cells
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GRU 单元
- en: The *gated recurrent unit* (GRU) cell (see [Figure 13-13](#gru_cell_diagram))
    was proposed by Kyunghyun Cho et al. in a [2014 paper](https://homl.info/97)⁠^([14](ch13.html#id3164))
    that also introduced the encoder-decoder network we discussed earlier.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '*门控循环单元*（GRU）单元（见 [图 13-13](#gru_cell_diagram)）是由 Kyunghyun Cho 等人提出的，该论文于
    2014 年发表，也介绍了我们之前讨论过的编码器-解码器网络。[14](ch13.html#id3164)'
- en: '![Diagram of a GRU cell illustrating its components and flow of information
    including reset, update, and candidate activation functions.](assets/hmls_1313.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![GRU 单元图，展示了其组件和信息流，包括重置、更新和候选激活函数。](assets/hmls_1313.png)'
- en: Figure 13-13\. GRU cell
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-13\. GRU 单元
- en: 'The GRU cell is a simplified version of the LSTM cell, and it often performs
    just as well.⁠^([15](ch13.html#id3165)) These are the main simplifications:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 单元是 LSTM 单元的简化版本，并且通常表现良好。[15](ch13.html#id3165) 这些是主要的简化：
- en: Both state vectors are merged into a single vector **h**[(*t*)].
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个状态向量合并为一个单独的向量 **h**[(*t*)]。
- en: A single gate controller **z**[(*t*)] controls both the forget gate and the
    input gate. If the gate controller outputs a 1, the forget gate is open (= 1)
    and the input gate is closed (1 – 1 = 0). If it outputs a 0, the opposite happens.
    In other words, whenever a memory must be stored, the location where it will be
    stored is erased first.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个门控制器 **z**[(*t*)] 控制遗忘门和输入门。如果门控制器输出 1，则遗忘门打开（= 1）且输入门关闭（1 – 1 = 0）。如果它输出
    0，则发生相反的情况。换句话说，每当需要存储记忆时，首先会擦除它将被存储的位置。
- en: There is no output gate; the full state vector is output at every time step.
    However, there is a new gate controller **r**[(*t*)] that controls which part
    of the previous state will be shown to the main layer (**g**[(*t*)]).
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有输出门；在每一步都输出完整的状态向量。然而，有一个新的门控制器 **r**[(*t*)]，它控制将哪一部分的前一个状态展示给主层（**g**[(*t*)]）。
- en: '[Equation 13-5](#gru_equation) summarizes how to compute the cell’s state at
    each time step for a single instance.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 13-5](#gru_equation) 总结了如何计算单个实例在每个时间步的细胞状态。'
- en: Equation 13-5\. GRU computations
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 13-5\. GRU 计算
- en: $StartLayout 1st Row 1st Column Blank 2nd Column bold z Subscript left-parenthesis
    t right-parenthesis Baseline equals sigma left-parenthesis bold upper W Subscript
    x z Baseline Superscript upper T Baseline bold x Subscript left-parenthesis t
    right-parenthesis Baseline plus bold upper W Subscript h z Baseline Superscript
    upper T Baseline bold h Subscript left-parenthesis t minus 1 right-parenthesis
    Baseline plus bold b Subscript z Baseline right-parenthesis 2nd Row 1st Column
    Blank 2nd Column bold r Subscript left-parenthesis t right-parenthesis Baseline
    equals sigma left-parenthesis bold upper W Subscript x r Baseline Superscript
    upper T Baseline bold x Subscript left-parenthesis t right-parenthesis Baseline
    plus bold upper W Subscript h r Baseline Superscript upper T Baseline bold h Subscript
    left-parenthesis t minus 1 right-parenthesis Baseline plus bold b Subscript r
    Baseline right-parenthesis 3rd Row 1st Column Blank 2nd Column bold g Subscript
    left-parenthesis t right-parenthesis Baseline equals hyperbolic tangent left-parenthesis
    bold upper W Subscript x g Baseline Superscript upper T Baseline bold x Subscript
    left-parenthesis t right-parenthesis Baseline plus bold upper W Subscript h g
    Baseline Superscript upper T Baseline left-parenthesis r Subscript left-parenthesis
    t right-parenthesis Baseline circled-times bold h Subscript left-parenthesis t
    minus 1 right-parenthesis Baseline right-parenthesis plus bold b Subscript g Baseline
    right-parenthesis 4th Row 1st Column Blank 2nd Column bold h Subscript left-parenthesis
    t right-parenthesis Baseline equals bold z Subscript left-parenthesis t right-parenthesis
    Baseline circled-times bold h Subscript left-parenthesis t minus 1 right-parenthesis
    Baseline plus left-parenthesis 1 minus bold z Subscript left-parenthesis t right-parenthesis
    Baseline right-parenthesis circled-times bold g Subscript left-parenthesis t right-parenthesis
    EndLayout$
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: $StartLayout 1st Row 1st Column 空白 2nd Column 粗体 z 下标 左括号 t 右括号 基线 等于 sigma
    左括号 粗体 上标 W 下标 x z 基线 上标 T 基线 粗体 x 下标 t 右括号 基线 加上 粗体 上标 W 下标 h z 基线 上标 T 基线 粗体
    h 下标 t 减 1 右括号 基线 加上 粗体 b 下标 z 基线 右括号 2nd Row 1st Column 空白 2nd Column 粗体 r 下标
    t 右括号 基线 等于 sigma 左括号 粗体 上标 W 下标 x r 基线 上标 T 基线 粗体 x 下标 t 右括号 基线 加上 粗体 上标 W 下标
    h r 基线 上标 T 基线 粗体 h 下标 t 减 1 右括号 基线 加上 粗体 b 下标 r 基线 右括号 3rd Row 1st Column 空白
    2nd Column 粗体 g 下标 t 右括号 基线 等于 双曲正切 左括号 粗体 上标 W 下标 x g 基线 上标 T 基线 粗体 x 下标 t 右括号
    基线 加上 粗体 上标 W 下标 h g 基线 上标 T 基线 粗体 h 下标 t 减 1 右括号 基线 加上 粗体 b 下标 g 基线 右括号 4th Row
    1st Column 空白 2nd Column 粗体 h 下标 t 右括号 基线 等于 粗体 z 下标 t 右括号 基线 乘以 粗体 h 下标 t 减 1
    右括号 基线 加上 左括号 1 减去 粗体 z 下标 t 右括号 基线 乘以 粗体 g 下标 t 右括号 EndLayout$
- en: PyTorch provides an `nn.GRU` layer; using it is just a matter of replacing `nn.RNN`
    or `nn.LSTM` with `nn.GRU`. It also provides an `nn.GRUCell` in case you want
    to create a custom RNN based on a GRU cell (just replace `nn.LSTMCell` with `nn.GRUCell`
    in the previous example, and get rid of `C`).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了 `nn.GRU` 层；使用它只需将 `nn.RNN` 或 `nn.LSTM` 替换为 `nn.GRU`。如果你想要基于 GRU
    单元创建自定义 RNN，它还提供了一个 `nn.GRUCell`（在前面示例中将 `nn.LSTMCell` 替换为 `nn.GRUCell`，并去掉 `C`）。
- en: LSTM and GRU are one of the main reasons behind the success of RNNs. Yet while
    they can tackle much longer sequences than simple RNNs, they still have a fairly
    limited short-term memory, and they have a hard time learning long-term patterns
    in sequences of 100 time steps or more, such as audio samples, long time series,
    or long sentences. One way to solve this is to shorten the input sequences, for
    example, using 1D convolutional layers.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 和 GRU 是 RNN 成功的主要原因之一。尽管它们可以处理比简单 RNN 更长的序列，但它们在短期记忆方面仍然相当有限，并且在处理 100
    个时间步或更长的序列（如音频样本、长时间序列或长句子）时，学习长期模式比较困难。解决这一问题的方法之一是缩短输入序列，例如，使用一维卷积层。
- en: Using 1D convolutional layers to process sequences
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用一维卷积层处理序列
- en: In [Chapter 12](ch12.html#cnn_chapter), we saw that a 2D convolutional layer
    works by sliding several fairly small kernels (or filters) across an image, producing
    multiple 2D feature maps (one per kernel). Similarly, a 1D convolutional layer
    slides several kernels across a sequence, producing a 1D feature map per kernel.
    Each kernel will learn to detect a single very short sequential pattern (no longer
    than the kernel size). If you use 10 kernels, then the layer’s output will be
    composed of 10 1D sequences (all of the same length), or equivalently you can
    view this output as a single 10D sequence. This means that you can build a neural
    network composed of a mix of recurrent layers and 1D convolutional layers (or
    even 1D pooling layers). However, as mentioned earlier, you must swap the last
    two dimensions of the `nn.Conv1d` layer’s inputs and outputs, since the `nn.Conv1d`
    layer expects inputs of shape [*batch size*, *input features*, *sequence length*],
    and produces outputs of shape [*batch size*, *output features*, *sequence length*].
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第12章](ch12.html#cnn_chapter)中，我们了解到二维卷积层通过在图像上滑动几个相当小的核（或过滤器）来工作，从而产生多个二维特征图（每个核一个）。同样，一维卷积层在序列上滑动几个核，每个核产生一个一维特征图。每个核将学会检测一个非常短的序列模式（不超过核大小）。如果你使用10个核，那么层的输出将由10个一维序列组成（所有长度相同），或者等价地，你可以将这个输出视为一个10维序列。这意味着你可以构建一个由循环层和一维卷积层（甚至一维池化层）混合组成的神经网络。然而，如前所述，你必须交换`nn.Conv1d`层输入和输出的最后两个维度，因为`nn.Conv1d`层期望输入形状为[*批大小*，*输入特征*，*序列长度*]，并产生形状为[*批大小*，*输出特征*，*序列长度*]的输出。
- en: Warning
  id: totrans-290
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If you use a 1D convolutional layer with a stride of 1 and `"same"` padding,
    then the output sequence will have the same length as the input sequence. But
    if you use `"valid"` padding or a stride greater than 1, then the output sequence
    will be shorter than the input sequence, so make sure you adjust the targets accordingly.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用步长为1且填充为"same"的一维卷积层，那么输出序列的长度将与输入序列相同。但是，如果你使用"valid"填充或步长大于1，那么输出序列将比输入序列短，因此请确保相应地调整目标。
- en: 'For example, the following model is composed of a 1D convolutional layer, followed
    by a GRU layer, and lastly a linear output layer, all of which input and output
    batches of sequences (i.e., 3D tensors). The `nn.Conv1d` layer downsamples the
    input sequences by a factor of 2, using a stride of 2\. The kernel size is as
    large as the stride (larger, in fact), so all inputs will be used to compute the
    layer’s output, and therefore the model can learn to preserve the most useful
    information, dropping only the unimportant details. In the `forward()` method,
    we just chain the layers, but we permute the last two dimensions before and after
    the `nn.Conv1d` layer, and we ignore the hidden states returned by the `nn.GRU`
    layer:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下模型由一个一维卷积层、一个GRU层和一个线性输出层组成，所有这些层都输入和输出序列批次（即3D张量）。`nn.Conv1d`层通过步长为2的方式将输入序列下采样2倍。核大小与步长相同（实际上更大），因此所有输入都将用于计算层的输出，因此模型可以学会保留最有用的信息，丢弃不重要的细节。在`forward()`方法中，我们只是链式连接层，但在`nn.Conv1d`层前后交换最后两个维度，并且我们忽略`nn.GRU`层返回的隐藏状态：
- en: '[PRE37]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'By shortening the time series, the convolutional layer helps the `GRU` layer
    detect longer patterns, so we can afford to double the window length to 112 days.
    Note that we must also crop off the first three time steps from the targets: indeed,
    the kernel’s size is 4, so the first output of the convolutional layer will be
    based on the input time steps 0 to 3, therefore the first forecasts must be for
    time steps 4 to 17 (instead of time steps 1 to 14). Moreover, we must downsample
    the targets by a factor of 2 because of the stride. For all this, we need a new
    `Dataset` class, so let’s create a subclass of the `Seq2SeqDataset` class:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 通过缩短时间序列，卷积层有助于`GRU`层检测更长的模式，因此我们可以将窗口长度加倍到112天。请注意，我们还必须从目标中裁剪掉前三个时间步：实际上，核的大小是4，因此卷积层的第一个输出将基于输入时间步0到3，因此第一个预测必须针对时间步4到17（而不是时间步1到14）。此外，由于步长，我们必须将目标下采样2倍。为此，我们需要一个新的`Dataset`类，因此让我们创建`Seq2SeqDataset`类的子类：
- en: '[PRE38]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: And now the model can be trained as usual. We’ve successfully mixed convolutional
    layers and recurrent layers. But what if we used only 1D convolutional layers
    and dropped the recurrent layers entirely?
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型可以像往常一样进行训练。我们已经成功混合了卷积层和循环层。但是，如果我们只使用一维卷积层并完全丢弃循环层会怎样呢？
- en: WaveNet
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WaveNet
- en: 'In a [2016 paper](https://homl.info/wavenet),⁠^([16](ch13.html#id3177)) Aaron
    van den Oord and other DeepMind researchers introduced a novel architecture called
    *WaveNet*. They stacked 1D convolutional layers, doubling the dilation rate (how
    spread apart each neuron’s inputs are) at every layer: the first convolutional
    layer gets a glimpse of just two time steps at a time, while the next one sees
    four time steps (its receptive field is four time steps long), the next one sees
    eight time steps, and so on (see [Figure 13-14](#wavenet_diagram)). This way,
    the lower layers learn short-term patterns, while the higher layers learn long-term
    patterns. Thanks to the doubling dilation rate, the network can process extremely
    large sequences very efficiently.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在[2016年的一篇论文](https://homl.info/wavenet)⁠^([16](ch13.html#id3177))中，Aaron van
    den Oord和其他DeepMind研究人员介绍了一种名为*WaveNet*的新架构。他们堆叠了1D卷积层，每层将膨胀率（每个神经元的输入之间的距离）加倍：第一层卷积层一次只能看到两个时间步，而下一层可以看到四个时间步（其感受野长度为四个时间步），下一层可以看到八个时间步，以此类推（见[图13-14](#wavenet_diagram))。这样，较低层学习短期模式，而较高层学习长期模式。由于膨胀率加倍，网络可以非常高效地处理极长的序列。
- en: '![Diagram illustrating the WaveNet architecture with stacked 1D convolutional
    layers showing increasing dilation rates, demonstrating how the model processes
    time steps at varying scales for efficient audio generation.](assets/hmls_1314.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![展示具有堆叠的1D卷积层和递增膨胀率的WaveNet架构的示意图，说明该模型如何以不同规模处理时间步以高效生成音频。](assets/hmls_1314.png)'
- en: Figure 13-14\. WaveNet architecture
  id: totrans-300
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-14\. WaveNet架构
- en: The authors of the paper actually stacked 10 convolutional layers with dilation
    rates of 1, 2, 4, 8, …​, 256, 512, then they stacked another group of 10 identical
    layers (also with dilation rates 1, 2, 4, 8, …​, 256, 512), then again another
    identical group of 10 layers. They justified this architecture by pointing out
    that a single stack of 10 convolutional layers with these dilation rates will
    act like a super-efficient convolutional layer with a kernel of size 1,024 (except
    way faster, more powerful, and using significantly fewer parameters). They also
    left-padded the input sequences with a number of zeros equal to the dilation rate
    before every layer to preserve the same sequence length throughout the network.
    Padding on the left rather than on both sides is important, as it ensures that
    the convolutional layer does not peek into the future when making predictions.
    This makes it a causal model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者实际上堆叠了10个具有膨胀率1、2、4、8、…、256、512的卷积层，然后他们又堆叠了另一组10个相同的层（也具有膨胀率1、2、4、8、…、256、512），然后再次堆叠了另一个相同的10层组。他们通过指出，一个具有这些膨胀率的单个10层卷积层堆叠将像一个具有1,024个大小内核的超高效卷积层（除了更快、更强大，并且使用显著更少的参数）。他们还在每一层之前用与膨胀率相等的零填充输入序列，以在整个网络中保持相同的序列长度。在左侧而不是两侧进行填充很重要，因为它确保卷积层在做出预测时不会看到未来。这使得它成为一个因果模型。
- en: 'Let’s implement a simplified WaveNet to tackle the same sequences as earlier.⁠^([17](ch13.html#id3180))
    We will start by creating a custom `CausalConv1d` module that acts just like an
    `nn.Conv1d` module, except the inputs get padded on the left side by the appropriate
    amount to ensure the sequence preserves the same length:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个简化的WaveNet来处理之前相同的序列。⁠^([17](ch13.html#id3180)) 我们将首先创建一个自定义的`CausalConv1d`模块，它就像一个`nn.Conv1d`模块一样工作，除了输入在左侧通过适当的数量进行填充，以确保序列保持相同的长度：
- en: '[PRE39]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In this code, we inherit from the `nn.Conv1d` class and we override the `forward()`
    method. In it, we calculate the size of the left-padding we need, and we pad the
    sequences using the `pad()` function before calling the base class’s `forward()`
    method. The `pad()` function takes two arguments: the tensor to pad (`X`), and
    a tuple of ints that indicates how much to pad to the left and right in the last
    dimension (i.e., the time dimension).'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们继承自`nn.Conv1d`类，并重写了`forward()`方法。在其中，我们计算所需的左填充大小，并在调用基类的`forward()`方法之前使用`pad()`函数填充序列。`pad()`函数接受两个参数：要填充的张量（`X`），以及一个表示在最后一个维度（即时间维度）左右填充多少个整数的元组。
- en: 'Now we’re ready to build the WaveNet model itself:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好构建WaveNet模型本身：
- en: '[PRE40]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In the constructor, we create eight `CausalConv1d` layers with various dilation
    rates (1, 2, 4, 8, then again 1, 2, 4, 8), each followed by the ReLU activation
    function. We chain all these modules in an `nn.Sequential` module `self.convs`.
    We also create the output `nn.Linear` layer. In the forward method, we permute
    the last two dimensions of the inputs, as we did earlier, we then pass them through
    the convolutional layers, then we permute the last two dimensions back to their
    original order, and we pass the result through the output layer. Thanks to the
    causal padding, every convolutional layer outputs a sequence of the same length
    as its input sequence, so the targets we use during training can be the full 112-day
    sequences; no need to crop them or downsample them. Thus, we can train the model
    using the data loaders we built for the `Seq2SeqModel` (i.e., `seq_train_loader`
    and `seq_valid_loader`).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，我们创建了八个具有不同膨胀率的`CausalConv1d`层（1, 2, 4, 8，然后再次是1, 2, 4, 8），每个层后面都跟着ReLU激活函数。我们将所有这些模块链接在一个`nn.Sequential`模块`self.convs`中。我们还创建了输出`nn.Linear`层。在正向方法中，我们交换输入的最后两个维度，就像我们之前做的那样，然后通过卷积层传递它们，然后交换最后两个维度回到原始顺序，并通过输出层传递结果。多亏了因果填充，每个卷积层都输出一个与输入序列长度相同的序列，因此我们用于训练的目标可以是完整的112天序列；无需裁剪或下采样。因此，我们可以使用为`Seq2SeqModel`（即`seq_train_loader`和`seq_valid_loader`）构建的数据加载器来训练模型。
- en: The models we’ve discussed in this section offer similar performance for the
    ridership forecasting task, but they may vary significantly depending on the task
    and the amount of available data. In the WaveNet paper, the authors achieved state-of-the-art
    performance on various audio tasks (hence the name of the architecture), including
    text-to-speech tasks, producing very realistic voices across several languages.
    They also used the model to generate music, one audio sample at a time. This feat
    is all the more impressive when you realize that a single second of audio can
    contain tens of thousands of time steps—even LSTMs and GRUs cannot handle such
    long sequences.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中讨论的模型在乘客预测任务上提供了相似的性能，但它们可能因任务和可用数据量的不同而有很大差异。在WaveNet论文中，作者在各种音频任务上实现了最先进的性能（因此得名该架构），包括文本到语音任务，产生跨越多种语言的非常逼真的声音。他们还使用该模型一次生成一个音频样本来生成音乐。当你意识到一个音频秒可以包含数万个时间步时，这一成就就更加令人印象深刻——即使是LSTMs和GRUs也无法处理如此长的序列。
- en: Warning
  id: totrans-309
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If you evaluate our best Chicago ridership models on the test period, starting
    in 2020, you will find that they perform much worse than expected! Why is that?
    Well, that’s when the Covid-19 pandemic started, which greatly affected public
    transportation. As mentioned earlier, these models will only work well if the
    patterns they learned from the past continue in the future. In any case, before
    deploying a model to production, verify that it works well on recent data. And
    once it’s in production, make sure to monitor its performance regularly.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你评估我们最好的芝加哥乘客模型在2020年开始的测试期间，你会发现它们的性能远低于预期！为什么？嗯，那正是Covid-19大流行开始的时候，这对公共交通产生了巨大影响。如前所述，这些模型只有在它们从过去学到的模式在未来继续存在时才会表现良好。无论如何，在将模型部署到生产之前，请确保它在最近的数据上表现良好。一旦它进入生产，请确保定期监控其性能。
- en: With that, you can now tackle all sorts of time series! In [Chapter 14](ch14.html#nlp_chapter),
    we will continue to explore RNNs, and we will see how they can tackle various
    NLP tasks as well.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你现在可以处理各种时间序列！在[第14章](ch14.html#nlp_chapter)中，我们将继续探索RNN，并了解它们如何处理各种NLP任务。
- en: Exercises
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Can you think of a few applications for a sequence-to-sequence RNN? What about
    a sequence-to-vector RNN, and a vector-to-sequence RNN?
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能想到几个序列到序列RNN的应用吗？序列到向量RNN和向量到序列RNN呢？
- en: How many dimensions must the inputs of an RNN layer have? What does each dimension
    represent? What about its outputs?
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN层的输入必须有多少维？每个维度代表什么？它的输出呢？
- en: How can you build a deep sequence-to-sequence RNN in PyTorch?
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何在PyTorch中构建一个深度序列到序列RNN？
- en: Suppose you have a daily univariate time series, and you want to forecast the
    next seven days using an RNN. Which architecture should you use?
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你有一个每日的单变量时间序列，你想使用RNN预测接下来七天。你应该使用哪种架构？
- en: What are the main difficulties when training RNNs? How can you handle them?
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练RNN时主要有哪些困难？你如何处理它们？
- en: Can you sketch the LSTM cell’s architecture?
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能画出LSTM单元的架构吗？
- en: Why would you want to use 1D convolutional layers in an RNN?
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么你想要在一个RNN中使用1D卷积层？
- en: Which neural network architecture could you use to classify videos?
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用哪种神经网络架构来对视频进行分类？
- en: Try to tweak the `Seq2SeqModel` model to forecast both rail and bus ridership
    for the next 14 days. The model will now need to predict 28 values instead of
    14.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试调整`Seq2SeqModel`模型以预测未来14天内铁路和公交客流量。现在，模型需要预测28个值而不是14个。
- en: 'Download the [Bach chorales](https://homl.info/bach) dataset and unzip it.
    It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale
    is 100 to 640 time steps long, and each time step contains 4 integers, where each
    integer corresponds to a note’s index on a piano (except for the value 0, which
    means that no note is played). Train a model—recurrent, convolutional, or both—that
    can predict the next time step (four notes), given a sequence of time steps from
    a chorale. Then use this model to generate Bach-like music, one note at a time:
    you can do this by giving the model the start of a chorale and asking it to predict
    the next time step, then appending these time steps to the input sequence and
    asking the model for the next note, and so on. Also make sure to check out [Google’s
    Coconet model](https://homl.info/coconet), which was used for a nice Google doodle
    about Bach.'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载[Bach合唱曲](https://homl.info/bach)数据集并解压它。它由约翰·塞巴斯蒂安·巴赫创作的382首合唱曲组成。每首合唱曲长100到640个时间步，每个时间步包含4个整数，其中每个整数对应于钢琴上一个音符的索引（除了值0，表示没有音符演奏）。训练一个模型——循环的、卷积的或两者结合——可以预测合唱曲的下一个时间步（四个音符），给定一个合唱曲的时间步序列。然后使用这个模型一次生成一个音符的巴赫风格音乐：你可以通过给模型一个合唱曲的开始部分并要求它预测下一个时间步，然后将这些时间步添加到输入序列中并要求模型预测下一个音符，依此类推。同时，确保查看[Google的Coconet模型](https://homl.info/coconet)，该模型被用于一个关于巴赫的漂亮的Google涂鸦。
- en: 'Train a classification model for the [QuickDraw dataset](https://homl.info/quickdraw),
    which contains millions of sketches of various objects. Start by downloading the
    simplified data for a few classes (e.g., *ant.ndjson*, *axe.ndjson*, and *bat.ndjson*).
    Each NDJSON file contains one JSON object per line, which you can parse using
    Python’s `json.loads()` function. This will give you a list of sketches, where
    each sketch is represented as a Python dictionary. In each dictionary, the `"drawing"`
    entry contains a list of pen strokes. You can convert this list to a 3D float
    tensor where the dimensions are [*strokes*, *x coordinates*, *y coordinates*].
    Since an RNN takes a single sequence as input, you will need to concatenate all
    the strokes for each sketch into a single sequence. It’s best to add an extra
    feature to allow the RNN to know how far along each stroke it currently is (e.g.,
    from 0 to 1). In other words, the model will receive a sequence where each time
    step has three features: the *x* and *y* coordinates of the pen, and the progress
    ratio along the current stroke.'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个针对[QuickDraw数据集](https://homl.info/quickdraw)的分类模型，该数据集包含数百万个各种物体的草图。首先，下载几个类别的简化数据（例如，*ant.ndjson*、*axe.ndjson*和*bat.ndjson*）。每个NDJSON文件每行包含一个JSON对象，你可以使用Python的`json.loads()`函数来解析它。这将给你一个草图列表，其中每个草图都表示为一个Python字典。在每个字典中，`"drawing"`条目包含一系列笔触。你可以将这个列表转换为维度为[*笔触*、*x坐标*、*y坐标*]的3D浮点张量。由于RNN以单个序列作为输入，你需要将每个草图的笔触全部连接成一个单独的序列。最好添加一个额外的特征，以便RNN知道当前笔触的进度（例如，从0到1）。换句话说，模型将接收一个序列，其中每个时间步长有三个特征：笔的*x*和*y*坐标，以及当前笔触的进度比。
- en: 'Create a dataset containing short audio recordings of you saying “yes” or “no”,
    and train a binary classification RNN on it. For example, you could:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含你说“是”或“否”的短音频录音的数据集，并在其上训练一个二进制分类循环神经网络（RNN）。例如，你可以：
- en: Use an audio recording software such as Audacity to record yourself saying “yes”
    as many times as your patience allows, with short pauses between each word. Create
    a similar recording for the word “no”. Try to cover the various ways you might
    realistically pronounce these words in real life.
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Audacity等音频录制软件，尽可能多地录制你说“是”的声音，每个词之间有短暂的停顿。为“否”这个词也创建一个类似的录音。尽量涵盖你在现实生活中可能以各种方式发音的这些词。
- en: 'Load each WAV file using the `torchaudio.load()` function from the TorchAudio
    library. This will return a tensor containing the audio, as well as an integer
    indicating the number of samples per second. The audio tensor has a shape of [*channels*,
    *samples*]: one channel for mono, two for stereo. Convert stereo to mono by averaging
    over the channel dimension.'
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TorchAudio库中的`torchaudio.load()`函数加载每个WAV文件。这将返回一个包含音频的张量，以及一个表示每秒样本数的整数。音频张量的形状为[*通道*，*样本*]：单声道一个通道，立体声两个通道。通过在通道维度上平均来将立体声转换为单声道。
- en: Chop each recording into individual words by splitting at the silences. You
    can do this using the `torchaudio.transforms.Vad` transform (Voice Activity Detection).
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在静音处分割，将每段录音分割成单个单词。你可以使用`torchaudio.transforms.Vad`转换（语音活动检测）来完成此操作。
- en: Since the sequences are so long, it’s hard to directly train an RNN on them,
    so it helps to convert the audio to a spectrogram first. For this, you can use
    the `torchaudio.transforms.MelSpectrogram` transform, which is well suited for
    voice. The output is a dramatically shorter sequence, with many more channels.
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于序列如此之长，直接在它们上训练RNN很困难，因此首先将音频转换为频谱图很有帮助。为此，你可以使用`torchaudio.transforms.MelSpectrogram`转换，这对于语音来说非常适合。输出是一个显著更短的序列，具有更多的通道。
- en: Now try building and training a binary classification RNN on your yes/no dataset!
    Consider sharing your dataset and model with the world (e.g., via the Hugging
    Face Hub).
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在尝试在你的是/否数据集上构建和训练一个二进制分类RNN！考虑将你的数据集和模型与世界分享（例如，通过Hugging Face Hub）。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: ^([1](ch13.html#id3062-marker)) Note that many researchers prefer to use the
    hyperbolic tangent (tanh) activation function in RNNs rather than the ReLU activation
    function. For example, see Vu Pham et al.’s [2013 paper](https://homl.info/91)
    “Dropout Improves Recurrent Neural Networks for Handwriting Recognition”. ReLU-based
    RNNs are also possible, as shown in Quoc V. Le et al.’s [2015 paper](https://homl.info/92)
    “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units”.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch13.html#id3062-marker)) 注意，许多研究人员更喜欢在RNN中使用双曲正切（tanh）激活函数，而不是ReLU激活函数。例如，参见Vu
    Pham等人2013年的论文“Dropout Improves Recurrent Neural Networks for Handwriting Recognition”。基于ReLU的RNN也是可能的，如Quoc
    V. Le等人2015年的论文“A Simple Way to Initialize Recurrent Networks of Rectified Linear
    Units”所示。
- en: ^([2](ch13.html#id3066-marker)) Michael I. Jordan, “Attractor Dynamics and Parallelism
    in a Connectionist Sequential Machine”, *Proceedings of the Eighth Annual Conference
    of the Cognitive Science Society* (1986).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch13.html#id3066-marker)) Michael I. Jordan，“Attractor Dynamics and Parallelism
    in a Connectionist Sequential Machine”，*第八届认知科学学会年度会议论文集*（1986年）。
- en: ^([3](ch13.html#id3069-marker)) Jeffrey L. Elman, “Finding Structure in Time”,
    *Cognitive Science*, Volume 14, Issue 2 (1990).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch13.html#id3069-marker)) Jeffrey L. Elman，“Finding Structure in Time”，*认知科学*，第14卷，第2期（1990年）。
- en: '^([4](ch13.html#id3076-marker)) Nal Kalchbrenner and Phil Blunsom, “Recurrent
    Continuous Translation Models”, *Proceedings of the 2013 Conference on Empirical
    Methods in Natural Language Processing* (2013): 1700–1709.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch13.html#id3076-marker)) Nal Kalchbrenner和Phil Blunsom，“Recurrent Continuous
    Translation Models”，*2013年实证自然语言处理会议论文集*（2013年）：1700–1709。
- en: ^([5](ch13.html#id3084-marker)) The latest data from the Chicago Transit Authority
    is available at the [Chicago Data Portal](https://homl.info/ridership).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch13.html#id3084-marker)) 芝加哥交通管理局的最新数据可在[芝加哥数据门户](https://homl.info/ridership)找到。
- en: ^([6](ch13.html#id3107-marker)) For more details on the ACF-PACF approach, check
    out this very nice [post by Jason Brownlee](https://homl.info/arimatuning).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch13.html#id3107-marker)) 关于ACF-PACF方法的更多细节，请查看Jason Brownlee的这篇非常好的[帖子](https://homl.info/arimatuning)。
- en: ^([7](ch13.html#id3125-marker)) Note that the validation period starts on the
    1st of January 2019, so the first prediction is for the 26th of February 2019,
    eight weeks later. When we evaluated the baseline models, we used predictions
    starting on the 1st of March instead, but this should be close enough.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch13.html#id3125-marker)) 注意，验证期从2019年1月1日开始，因此第一个预测是在2019年2月26日，八周后。当我们评估基线模型时，我们使用了从2019年3月1日开始进行的预测，但这应该足够接近。
- en: ^([8](ch13.html#id3135-marker)) We cannot use the `ahead_model` for this because
    it needs both the rail and bus ridership as input, but it only forecasts the rail
    ridership.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch13.html#id3135-marker)) 我们不能使用`ahead_model`，因为它需要铁路和公交客流量作为输入，但它只预测铁路客流量。
- en: '^([9](ch13.html#id3144-marker)) César Laurent et al., “Batch Normalized Recurrent
    Neural Networks”, *Proceedings of the IEEE International Conference on Acoustics,
    Speech, and Signal Processing* (2016): 2657–2661.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch13.html#id3144-marker)) César Laurent 等人，“批量归一化循环神经网络”，*IEEE 国际声学、语音和信号处理会议论文集*
    (2016): 2657–2661.'
- en: ^([10](ch13.html#id3155-marker)) A character from the animated movies *Finding
    Nemo* and *Finding Dory* who has short-term memory loss.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch13.html#id3155-marker)) 来自动画电影 *海底总动员* 和 *海底奇缘* 的角色，患有短期记忆丧失。
- en: '^([11](ch13.html#id3156-marker)) Sepp Hochreiter and Jürgen Schmidhuber, “Long
    Short-Term Memory”, *Neural Computation* 9, no. 8 (1997): 1735–1780.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '^([11](ch13.html#id3156-marker)) Sepp Hochreiter 和 Jürgen Schmidhuber，“长短期记忆”，*神经计算*
    9, 第 8 期 (1997): 1735–1780.'
- en: ^([12](ch13.html#id3157-marker)) Haşim Sak et al., “Long Short-Term Memory Based
    Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition”,
    arXiv preprint arXiv:1402.1128 (2014).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch13.html#id3157-marker)) Haşim Sak 等人，“基于长短期记忆的循环神经网络架构用于大词汇量语音识别”，arXiv
    预印本 arXiv:1402.1128 (2014).
- en: ^([13](ch13.html#id3158-marker)) Wojciech Zaremba et al., “Recurrent Neural
    Network Regularization”, arXiv preprint arXiv:1409.2329 (2014).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch13.html#id3158-marker)) Wojciech Zaremba 等人，“循环神经网络正则化”，arXiv 预印本 arXiv:1409.2329
    (2014).
- en: '^([14](ch13.html#id3164-marker)) Kyunghyun Cho et al., “Learning Phrase Representations
    Using RNN Encoder-Decoder for Statistical Machine Translation”, *Proceedings of
    the 2014 Conference on Empirical Methods in Natural Language Processing* (2014):
    1724–1734.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '^([14](ch13.html#id3164-marker)) Kyunghyun Cho 等人，“使用 RNN 编码器-解码器学习短语表示”，*2014
    年自然语言处理实证方法会议论文集* (2014): 1724–1734.'
- en: '^([15](ch13.html#id3165-marker)) See Klaus Greff et al., [“LSTM: A Search Space
    Odyssey”](https://homl.info/98), *IEEE Transactions on Neural Networks and Learning
    Systems* 28, no. 10 (2017): 2222–2232\. This paper seems to show that all LSTM
    variants perform roughly the same.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '^([15](ch13.html#id3165-marker)) 参见 Klaus Greff 等人，“LSTM：一次搜索空间探险”，[“LSTM:
    A Search Space Odyssey”](https://homl.info/98)，*IEEE 交易杂志：神经网络与学习系统* 28, 第 10
    期 (2017): 2222–2232。这篇论文似乎表明所有 LSTM 变体表现大致相同。'
- en: '^([16](ch13.html#id3177-marker)) Aaron van den Oord et al., “WaveNet: A Generative
    Model for Raw Audio”, arXiv preprint arXiv:1609.03499 (2016).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch13.html#id3177-marker)) Aaron van den Oord 等人，“WaveNet：原始音频的生成模型”，arXiv
    预印本 arXiv:1609.03499 (2016).
- en: ^([17](ch13.html#id3180-marker)) The complete WaveNet uses a few more tricks,
    such as skip connections like in a ResNet, and *gated activation units* similar
    to those found in a GRU cell.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch13.html#id3180-marker)) 完整的 WaveNet 使用了一些额外的技巧，例如类似于 ResNet 中的跳跃连接，以及类似于在
    GRU 单元中发现的 *门控激活单元*。
