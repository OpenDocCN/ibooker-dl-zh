<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 12. Deep Computer Vision Using Convolutional Neural Networks"><div class="chapter" id="cnn_chapter">
<h1><span class="label">Chapter 12. </span>Deep Computer Vision Using <span class="keep-together">Convolutional Neural Networks</span></h1>


<p>Although IBM’s Deep Blue supercomputer beat the chess world champion Garry Kasparov back in 1996, it wasn’t until fairly recently that computers were able to reliably perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing spoken words. Why are these tasks so effortless to us humans? The answer lies in the fact that perception largely takes place outside the realm of our consciousness, within specialized visual, auditory, and other sensory modules in our brains. By the time sensory information reaches our consciousness, it is already adorned with high-level features; for example, when you look at a picture of a cute puppy, you cannot choose <em>not</em> to see the puppy, <em>not</em> to notice its cuteness. Nor can you explain <em>how</em> you recognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective experience: perception is not trivial at all, and to understand it we must look at how our sensory modules work.</p>

<p><em>Convolutional neural networks</em> (CNNs)<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" id="xi_convolutionalneuralnetworksCNNs12739_1"/> emerged from the study of the brain’s visual cortex, and they have been used in computer image recognition since the 1980s. Over the last 15 years, thanks to the increase in computational power, the amount of available training data, and the tricks presented in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a> for training deep nets, CNNs have managed to achieve superhuman performance on some complex visual tasks. They power image search services, self-driving cars, automatic video classification systems, and more. Moreover, CNNs are not restricted to visual perception: they are also successful at many other tasks, such as voice recognition and natural language processing. However, we will focus on visual applications for now.</p>

<p>In this chapter we will explore where CNNs came from, what their building blocks look like, and how to implement them using PyTorch. Then we will discuss some of the best CNN architectures, as well as other visual tasks, including object detection (classifying multiple objects in an image and placing bounding boxes around them) and semantic segmentation (classifying each pixel according to the class of the object it belongs to).</p>






<section data-type="sect1" data-pdf-bookmark="The Architecture of the Visual Cortex"><div class="sect1" id="id213">
<h1>The Architecture of the Visual Cortex</h1>

<p>David H. Hubel and Torsten Wiesel performed a series of experiments<a data-type="indexterm" data-primary="visual cortex architecture" id="xi_visualcortexarchitecture121368_1"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="visual cortex architecture" id="xi_convolutionalneuralnetworksCNNsvisualcortexarchitecture121368_1"/> on cats<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="evolution of" id="id2733"/> in <a href="https://homl.info/71">1958</a>⁠<sup><a data-type="noteref" id="id2734-marker" href="ch12.html#id2734">1</a></sup> and <a href="https://homl.info/72">1959</a>⁠<sup><a data-type="noteref" id="id2735-marker" href="ch12.html#id2735">2</a></sup> (and a <a href="https://homl.info/73">few years later on monkeys</a>⁠<sup><a data-type="noteref" id="id2736-marker" href="ch12.html#id2736">3</a></sup>), giving crucial insights into the structure of the visual cortex (the authors received the Nobel Prize in Physiology or Medicine in 1981 for their work). In particular, they showed that many neurons in the visual cortex<a data-type="indexterm" data-primary="local receptive field" id="id2737"/> have a small <em>local receptive field</em>, meaning they react only to visual stimuli located in a limited region of the visual field (see <a data-type="xref" href="#cat_visual_cortex_diagram">Figure 12-1</a>, in which the local receptive fields of five neurons are represented by dashed circles). The receptive fields of different neurons may overlap, and together they tile the whole visual field.</p>

<figure class="width-85"><div id="cat_visual_cortex_diagram" class="figure">
<img src="assets/hmls_1201.png" alt="Diagram illustrating how biological neurons in the visual cortex respond to specific patterns within small receptive fields and integrate this information to recognize complex shapes like a house." width="1315" height="402"/>
<h6><span class="label">Figure 12-1. </span>Biological neurons in the visual cortex respond to specific patterns in small regions of the visual field called receptive fields; as the visual signal makes its way through consecutive brain modules, neurons respond to more complex patterns in larger receptive fields</h6>
</div></figure>

<p>Moreover, the authors showed that some neurons react only to images of horizontal lines, while others react only to lines with different orientations (two neurons may have the same receptive field but react to different line orientations). They also noticed that some neurons have larger receptive fields, and they react to more complex patterns that are combinations of the lower-level patterns. These observations led to the idea that the higher-level neurons are based on the outputs of neighboring lower-level neurons (in <a data-type="xref" href="#cat_visual_cortex_diagram">Figure 12-1</a>, notice that each neuron is connected only to nearby neurons from the previous layer). This powerful architecture is able to detect all sorts of complex patterns in any area of the visual field.</p>

<p>These studies of the visual cortex inspired the <a href="https://homl.info/74">neocognitron</a>,⁠<sup><a data-type="noteref" id="id2738-marker" href="ch12.html#id2738">4</a></sup> introduced in 1980, which gradually evolved into what we now call convolutional neural networks. An important milestone was a <a href="https://homl.info/75">1998 paper</a>⁠<sup><a data-type="noteref" id="id2739-marker" href="ch12.html#id2739">5</a></sup> by Yann LeCun et al. that introduced the famous <em>LeNet-5</em> architecture<a data-type="indexterm" data-primary="LeNet-5" id="id2740"/>, which became widely used by banks to recognize handwritten digits on checks. This architecture has some building blocks that you already know, such as fully connected layers and sigmoid activation functions, but it also introduces two new building blocks: <em>convolutional layers</em> and <em>pooling layers</em>. Let’s look at them now.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Why not simply use a deep neural network with fully connected layers<a data-type="indexterm" data-primary="fully connected layers" id="id2741"/> for image recognition tasks? Unfortunately, although this works fine for small images (e.g., Fashion MNIST), it breaks down for larger images because of the huge number of parameters it requires. For example, a 100 × 100–pixel image has 10,000 pixels, and if the first layer has just 1,000 neurons (which already severely restricts the amount of information transmitted to the next layer), this means a total of 10 million connections. And that’s just the first layer. CNNs solve this problem using partially connected layers and weight sharing<a data-type="indexterm" data-startref="xi_visualcortexarchitecture121368_1" id="id2742"/><a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsvisualcortexarchitecture121368_1" id="id2743"/>.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Convolutional Layers"><div class="sect1" id="id214">
<h1>Convolutional Layers</h1>

<p>The most important building block<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="convolutional layers" id="xi_convolutionalneuralnetworksCNNsconvolutionallayers122734_1"/> of a CNN is the <em>convolutional layer</em>:⁠<sup><a data-type="noteref" id="id2744-marker" href="ch12.html#id2744">6</a></sup> neurons in the first convolutional layer are not connected to every single pixel in the input image (like they were in the layers discussed in previous chapters), but only to pixels in their receptive fields (see <a data-type="xref" href="#cnn_layers_diagram">Figure 12-2</a>). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them into larger higher-level features in the next hidden layer, and so on. This hierarchical structure is well-suited to deal with composite objects, which are common in real-world images: this is one of the reasons why CNNs work so well for image recognition.</p>

<figure class="smallerseventy"><div id="cnn_layers_diagram" class="figure">
<img src="assets/hmls_1202.png" alt="Diagram illustrating the structure of CNN layers, highlighting the local receptive fields connecting the first convolutional layer to the second through the input." width="835" height="550"/>
<h6><span class="label">Figure 12-2. </span>CNN layers with rectangular local receptive fields</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>All the multilayer neural networks we’ve looked at so <a data-type="indexterm" data-primary="torch" data-secondary="mean()" id="id2745"/>far had layers composed of a long line of neurons, and we had to flatten input images to 1D<a data-type="indexterm" data-primary="1D convolutional layers" data-primary-sortas="one d convolutional layers" id="id2746"/> before feeding them to the neural network. In a CNN each layer is represented in 2D<a data-type="indexterm" data-primary="2D convolutional layers" data-primary-sortas="two d convolutional layers" id="id2747"/>, which makes it easier to match neurons with their corresponding inputs.</p>
</div>

<p>A neuron located in row <em>i</em>, column <em>j</em> of a given layer is connected to the outputs of the neurons in the previous layer<a data-type="indexterm" data-primary="padding options, convolutional layer" id="id2748"/> located in rows <em>i</em> to <em>i</em> + <em>f</em><sub><em>h</em></sub> – 1, columns <em>j</em> to <em>j</em> + <em>f</em><sub><em>w</em></sub> – 1, where <em>f</em><sub><em>h</em></sub> and <em>f</em><sub><em>w</em></sub> are the height and width of the receptive field (see <a data-type="xref" href="#slide_and_padding_diagram">Figure 12-3</a>). In order for a layer to have the same height and width as the previous layer, it is common to add zeros<a data-type="indexterm" data-primary="zero padding" id="id2749"/> around the inputs, as shown in the diagram. This is called <em>zero</em> <span class="keep-together"><em>padding</em></span>.</p>

<p>It is also possible to connect a large input layer to a much smaller layer by spacing out the receptive fields<a data-type="indexterm" data-primary="strides" id="id2750"/>, as shown in <a data-type="xref" href="#stride_diagram">Figure 12-4</a>. This dramatically reduces the model’s computational complexity. The horizontal or vertical step size from one receptive field to the next is called the <em>stride</em>. In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 × 4 layer, using 3 × 3 receptive fields and a stride of 2. In this example the stride is the same in both directions, which is generally the case (although there are exceptions). A neuron located in row <em>i</em>, column <em>j</em> in the upper layer is connected to the outputs of the neurons in the previous layer located in rows <em>i</em> × <em>s</em><sub><em>h</em></sub> to <em>i</em> × <em>s</em><sub><em>h</em></sub> + <em>f</em><sub><em>h</em></sub> – 1, columns <em>j</em> × <em>s</em><sub><em>w</em></sub> to 
<span class="keep-together"><em>j</em> × <em>s</em><sub><em>w</em></sub> + <em>f</em><sub><em>w</em></sub> – 1,</span> where <em>s</em><sub><em>h</em></sub> and <em>s</em><sub><em>w</em></sub> are the vertical and horizontal strides.</p>

<figure class="width-75"><div id="slide_and_padding_diagram" class="figure">
<img src="assets/hmls_1203.png" alt="Diagram illustrating the connection between a 5 × 7 input layer and a 3 × 4 layer using 3 × 3 receptive fields with a stride of 2, including zero padding." width="852" height="651"/>
<h6><span class="label">Figure 12-3. </span>Connections between layers and zero padding</h6>
</div></figure>

<figure class="width-75"><div id="stride_diagram" class="figure">
<img src="assets/hmls_1204.png" alt="Diagram illustrating the concept of reducing dimensionality with a stride of 2 on a grid, showing overlapping operations with different colored boxes and lines." width="809" height="547"/>
<h6><span class="label">Figure 12-4. </span>Reducing dimensionality using a stride of 2</h6>
</div></figure>








<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Filters"><div class="sect2" id="id215">
<h2>Filters</h2>

<p>A neuron’s weights<a data-type="indexterm" data-primary="weights" data-secondary="convolutional layers" id="id2751"/> can be represented as a small image the size of the receptive field. For example, <a data-type="xref" href="#filters_diagram">Figure 12-5</a> shows two possible sets of weights, called <em>filters</em> (or <em>convolution kernels</em>, or just <em>kernels</em>)<a data-type="indexterm" data-primary="convolution kernels (kernels)" id="id2752"/><a data-type="indexterm" data-primary="filters, convolutional layers" id="xi_filtersconvolutionallayers1252220_1"/><a data-type="indexterm" data-primary="kernels, convolution" id="id2753"/>. The first one is represented as a black square with a vertical white line in the middle (it’s a 7 × 7 matrix full of 0s except for the central column, which is full of 1s); neurons using these weights will ignore everything in their receptive field except for the central vertical line (since all inputs will be multiplied by 0, except for the ones in the central vertical line). The second filter is a black square with a horizontal white line in the middle. Neurons using these weights will ignore everything in their receptive field except for the central horizontal line.</p>

<figure class="width-90"><div id="filters_diagram" class="figure">
<img src="assets/hmls_1205.png" alt="Diagram showing the input image processed by vertical and horizontal filters to produce two feature maps, each highlighting different line orientations." width="1955" height="1227"/>
<h6><span class="label">Figure 12-5. </span>Applying two different filters to get two feature maps</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In deep learning, we often build a single model that takes the raw inputs and produces the final outputs. This is called <em>end-to-end learning</em>. In contrast, classical vision systems would usually divide the system into a sequence of specialized modules.</p>
</div>

<p>Now if all neurons in a layer use the same vertical line filter (and the same bias term), and you feed the network the input image shown in <a data-type="xref" href="#filters_diagram">Figure 12-5</a> (the bottom image), the layer will output the top-left image. Notice that the vertical white lines get enhanced while the rest gets blurred. Similarly, the upper-right image is what you get if all neurons use the same horizontal line filter; notice that the horizontal white lines get enhanced while the rest is blurred out. Thus, a layer full of neurons using the same filter outputs a <em>feature map</em>, which highlights the areas in an image that activate the filter the most. But don’t worry, you won’t have to define the filters manually: instead, during training the convolutional layer will automatically learn the most useful filters for its task, and the layers above will learn to combine them into more complex patterns<a data-type="indexterm" data-startref="xi_filtersconvolutionallayers1252220_1" id="id2754"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Stacking Multiple Feature Maps"><div class="sect2" id="id216">
<h2>Stacking Multiple Feature Maps</h2>

<p>Up to now, for simplicity<a data-type="indexterm" data-primary="feature maps" id="xi_featuremaps126326_1"/>, I have represented each convolutional layer as a 2D<a data-type="indexterm" data-primary="2D convolutional layers" data-primary-sortas="two d convolutional layers" id="id2755"/> layer, but in reality a convolutional layer has multiple filters (you decide how many) and it outputs one feature map per filter, so the output is more accurately represented in 3D<a data-type="indexterm" data-primary="3D convolutional layers" data-primary-sortas="three d convolutional layers" id="id2756"/> (see <a data-type="xref" href="#cnn_layers_volume_diagram">Figure 12-6</a>).</p>

<figure class="width-85"><div id="cnn_layers_volume_diagram" class="figure">
<img src="assets/hmls_1206.png" alt="Diagram illustrating two convolutional layers with multiple filters processing a color image with three RGB channels, producing one feature map per filter." width="1427" height="1266"/>
<h6><span class="label">Figure 12-6. </span>Two convolutional layers with multiple filters each (kernels), processing a color image with three color channels; each convolutional layer outputs one feature map per filter</h6>
</div></figure>

<p>There is one neuron per pixel in each feature map, and all neurons within a given feature map share the same parameters (i.e., the same kernel and bias term). Neurons in different feature maps use different parameters. A neuron’s receptive field is the same as described earlier, but it extends across all the feature maps of the previous layer. In short, a convolutional layer simultaneously applies multiple trainable filters to its inputs, making it capable of detecting multiple features anywhere in its inputs.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The fact that all neurons in a feature map share the same parameters dramatically reduces the number of parameters in the model. Once the CNN has learned to recognize a pattern in one location, it can recognize it in any other location. In contrast, once a fully connected neural network has learned to recognize a pattern in one location, it can only recognize it in that particular location.</p>
</div>

<p>Input images are also composed of multiple sublayers<a data-type="indexterm" data-primary="color channels" id="id2757"/>: one per <em>color channel</em>. As mentioned in <a data-type="xref" href="ch08.html#unsupervised_learning_chapter">Chapter 8</a>, there are typically three: red, green, and blue (RGB). Grayscale images have just one <span class="keep-together">channel</span>, but some images may have many more—for example, satellite images that capture extra light frequencies (such as infrared).</p>

<p>Specifically, a neuron located in row <em>i</em>, column <em>j</em> of the feature map <em>k</em> in a given convolutional layer <em>l</em> is connected to the outputs of the neurons in the previous layer 
<span class="keep-together"><em>l</em> – 1,</span> located in rows <em>i</em> × <em>s</em><sub><em>h</em></sub> to <em>i</em> × <em>s</em><sub><em>h</em></sub> + <em>f</em><sub><em>h</em></sub> – 1 and columns <em>j</em> × <em>s</em><sub><em>w</em></sub> to <em>j</em> × <em>s</em><sub><em>w</em></sub> + <em>f</em><sub><em>w</em></sub> – 1, across all feature maps (in layer <em>l</em> – <em>1</em>). Note that, within a layer, all neurons located in the same row <em>i</em> and column <em>j</em> but in different feature maps are connected to the outputs of the exact same neurons in the previous layer.</p>

<p><a data-type="xref" href="#convolutional_layer_equation">Equation 12-1</a> summarizes the preceding explanations in one big mathematical equation: it shows how to compute the output of a given neuron in a convolutional layer. It is a bit ugly due to all the different indices, but all it does is calculate the weighted sum of all the inputs, plus the bias term.</p>
<div data-type="equation" id="convolutional_layer_equation">
<h5><span class="label">Equation 12-1. </span>Computing the output of a neuron in a convolutional layer</h5>
<math display="block">
  <mrow>
    <msub><mi>z</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow> </msub>
    <mo>=</mo>
    <msub><mi>b</mi> <mi>k</mi> </msub>
    <mo>+</mo>
    <munderover><mo>∑</mo> <mrow><mi>u</mi><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <mi>h</mi> </msub><mo>-</mo><mn>1</mn></mrow> </munderover>
    <mspace width="0.166667em"/>
    <mspace width="0.166667em"/>
    <munderover><mo>∑</mo> <mrow><mi>v</mi><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <mi>w</mi> </msub><mo>-</mo><mn>1</mn></mrow> </munderover>
    <mspace width="0.166667em"/>
    <mspace width="0.166667em"/>
    <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>'</mo><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <msup><mi>n</mi> <mo>'</mo> </msup> </msub><mo>-</mo><mn>1</mn></mrow> </munderover>
    <mspace width="0.166667em"/>
    <mspace width="0.166667em"/>
    <msub><mi>x</mi> <mrow><msup><mi>i</mi> <mo>'</mo> </msup><mo lspace="0%" rspace="0%">,</mo><msup><mi>j</mi> <mo>'</mo> </msup><mo lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo> </msup></mrow> </msub>
    <mo>×</mo>
    <msub><mi>w</mi> <mrow><mi>u</mi><mo lspace="0%" rspace="0%">,</mo><mi>v</mi><mo lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo> </msup><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow> </msub>
    <mspace width="1.em"/>
    <mtext>with</mtext>
    <mspace width="4.pt"/>
    <mfenced separators="" open="{" close="">
      <mtable>
        <mtr>
          <mtd columnalign="left">
            <mrow>
              <mi>i</mi>
              <mo>'</mo>
              <mo>=</mo>
              <mi>i</mi>
              <mo>×</mo>
              <msub><mi>s</mi> <mi>h</mi> </msub>
              <mo>+</mo>
              <mi>u</mi>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd columnalign="left">
            <mrow>
              <mi>j</mi>
              <mo>'</mo>
              <mo>=</mo>
              <mi>j</mi>
              <mo>×</mo>
              <msub><mi>s</mi> <mi>w</mi> </msub>
              <mo>+</mo>
              <mi>v</mi>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</math>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>z</em><sub><em>i</em>,</sub> <sub><em>j</em>,</sub> <sub><em>k</em></sub> is the output of the neuron located in row <em>i</em>, column <em>j</em> in feature map <em>k</em> of the convolutional layer (layer <em>l</em>).</p>
</li>
<li>
<p>As explained earlier, <em>s</em><sub><em>h</em></sub> and <em>s</em><sub><em>w</em></sub> are the vertical and horizontal strides,  <em>f</em><sub><em>h</em></sub> and <em>f</em><sub><em>w</em></sub> are the height and width of the receptive field, and <em>f</em><sub><em>n</em>′</sub> is the number of feature maps in the previous layer (layer <em>l</em> – 1).</p>
</li>
<li>
<p><em>x</em><sub><em>i</em>′,</sub> <sub><em>j</em>′,</sub> <sub><em>k</em>′</sub> is the output of the neuron located in layer <em>l</em> – 1, row <em>i</em>′, column <em>j</em>′, feature map <em>k</em>′ (or channel <em>k</em>′ if the previous layer is the input layer).</p>
</li>
<li>
<p><em>b</em><sub><em>k</em></sub> is the bias term for feature map <em>k</em> (in layer <em>l</em>). You can think of it as a knob that tweaks the overall brightness of the feature map <em>k</em>.</p>
</li>
<li>
<p><em>w</em><sub><em>u</em>,</sub> <sub><em>v</em>,</sub> <sub><em>k</em>′,</sub> <sub><em>k</em></sub> is the connection weight between any neuron in feature map <em>k</em> of the layer <em>l</em> and its input located at row <em>u</em>, column <em>v</em> (relative to the neuron’s receptive field), and feature map <em>k</em>′.</p>
</li>
</ul>

<p>Let’s see how to create and use a convolutional layer using PyTorch<a data-type="indexterm" data-startref="xi_featuremaps126326_1" id="id2758"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Implementing Convolutional Layers with PyTorch"><div class="sect2" id="id217">
<h2>Implementing Convolutional Layers with PyTorch</h2>

<p>First, let’s load a couple of sample images using Scikit-Learn’s <code translate="no">load_sample_images()</code> function<a data-type="indexterm" data-primary="sklearn" data-secondary="load_sample_images()" id="id2759"/>. The first image represents the tower of buddhist incense in China, while the second one represents a beautiful <em>Dahlia pinnata</em> flower. These images are represented as a Python<a data-type="indexterm" data-primary="PyTorch" data-secondary="convolutional layer implementation" id="xi_PyTorchconvolutionallayerimplementation12152275_1"/><a data-type="indexterm" data-primary="images, classifying and generating" data-secondary="transfer learning pretrained model" id="xi_imagesclassifyingandgeneratingtransferlearningpretrainedmodel12152275_1"/> list of NumPy unsigned byte arrays, so let’s stack these images into a single NumPy array<a data-type="indexterm" data-primary="NumPy arrays" id="id2760"/>, then convert it to a 32-bit float tensor, and rescale the pixel values from 0–255 to 0–1:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_sample_images</code>

<code class="n">sample_images</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">stack</code><code class="p">(</code><code class="n">load_sample_images</code><code class="p">()[</code><code class="s2">"images"</code><code class="p">])</code>
<code class="n">sample_images</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">sample_images</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code> <code class="o">/</code> <code class="mi">255</code></pre>

<p>Let’s look at this tensor’s shape:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">sample_images</code><code class="o">.</code><code class="n">shape</code>
<code class="go">torch.Size([2, 427, 640, 3])</code></pre>

<p>We have two images, both are 427 pixels high and 640 pixels wide, and they have three color channels: red, green, and blue. As we saw in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>, PyTorch expects the channel dimension to be just <em>before</em> the height and width dimensions, not after, so we need to permute the dimensions using the <code translate="no">permute()</code> method<a data-type="indexterm" data-primary="torch" data-secondary="permute()" id="id2761"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">sample_images_permuted</code> <code class="o">=</code> <code class="n">sample_images</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">sample_images_permuted</code><code class="o">.</code><code class="n">shape</code>
<code class="go">torch.Size([2, 3, 427, 640])</code></pre>

<p>Let’s also use TorchVision’s <code translate="no">CenterCrop</code> class<a data-type="indexterm" data-primary="torchvision" data-secondary="T.CenterCrop" id="id2762"/> to center-crop the images:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">torchvision</code>
<code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">torchvision.transforms.v2</code> <code class="k">as</code> <code class="nn">T</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">cropped_images</code> <code class="o">=</code> <code class="n">T</code><code class="o">.</code><code class="n">CenterCrop</code><code class="p">((</code><code class="mi">70</code><code class="p">,</code> <code class="mi">120</code><code class="p">))(</code><code class="n">sample_images_permuted</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">cropped_images</code><code class="o">.</code><code class="n">shape</code>
<code class="go">torch.Size([2, 3, 70, 120])</code></pre>

<p>Now let’s create a 2D<a data-type="indexterm" data-primary="2D convolutional layers" data-primary-sortas="two d convolutional layers" id="id2763"/> convolutional layer and feed it these cropped images to see what comes out. For this, PyTorch provides the <code translate="no">nn.Conv2d</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.Conv2d" id="xi_torchnnConv2d12192147_1"/>. Under the hood, this layer relies on the <code translate="no">torch.nn.((("torch", "F.conv2d()")))functional.conv2d()</code> function. Let’s create a convolutional layer with 32 filters, each of size 7 × 7 (using <code translate="no">kernel_size=7</code>, which is equivalent to using <code translate="no">kernel_size=(7 , 7)</code>), and apply this layer to our small batch of two images:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">conv_layer</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="n">in_channels</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">out_channels</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">7</code><code class="p">)</code>
<code class="n">fmaps</code> <code class="o">=</code> <code class="n">conv_layer</code><code class="p">(</code><code class="n">cropped_images</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>When we talk about a 2D convolutional layer, “2D” refers to the number<a data-type="indexterm" data-primary="spatial dimensions" id="id2764"/> of <em>spatial</em> dimensions (height and width), but as you can see, the layer takes 4D inputs: as we saw, the two additional dimensions are the batch size (first dimension) and the channels (second dimension).</p>
</div>

<p>Now let’s look at the output’s shape:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">fmaps</code><code class="o">.</code><code class="n">shape</code>
<code class="go">torch.Size([2, 32, 64, 114])</code></pre>

<p>The output shape is similar to the input shape, with two main differences. First, there are 32 channels instead of 3. This is because we set <code translate="no">out_channels=32</code>, so we get 32 output feature maps<a data-type="indexterm" data-primary="feature maps" id="id2765"/>: instead of the intensity of red, green, and blue at each location, we now have the intensity of each feature at each location. Second, the height and width have both shrunk by 6 pixels. This is due to the fact that the <code translate="no">nn.Conv2d</code> layer does not use any zero-padding by default, which means that we lose a few pixels on the sides of the output feature maps, depending on the size of the filters. In this case, since the kernel size is 7, we lose 6 pixels horizontally and 6 pixels vertically (i.e., 3 pixels on each side).</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>By default, the <code translate="no">padding</code> hyperparameter is set to 0, which means that padding is turned off. Oddly, this is also called <em>valid padding</em> since every neuron’s receptive field lies strictly within <em>valid</em> positions inside the input (it does not go out of bounds). You can actually set<a data-type="indexterm" data-primary="“valid” padding, computer vision" data-primary-sortas="valid padding, computer vision" id="id2766"/> <code translate="no">padding="valid"</code>, which is equivalent to <code translate="no">padding=0</code>. It’s not a PyTorch naming quirk: everyone uses this confusing nomenclature.</p>
</div>

<p>If instead we set <code translate="no">padding="same"</code>, then the inputs are padded<a data-type="indexterm" data-primary="“same” padding" data-primary-sortas="same padding" id="id2767"/> with enough zeros on all sides to ensure that the output feature maps end up with the <em>same</em> size as the inputs (hence the name of this option):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">conv_layer</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="n">in_channels</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">out_channels</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">7</code><code class="p">,</code>
<code class="gp">... </code>                       <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">)</code>
<code class="gp">...</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">fmaps</code> <code class="o">=</code> <code class="n">conv_layer</code><code class="p">(</code><code class="n">cropped_images</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">fmaps</code><code class="o">.</code><code class="n">shape</code>
<code class="go">torch.Size([2, 32, 70, 120])</code></pre>

<p>These two padding options are illustrated in <a data-type="xref" href="#padding_options_stride_1_diagram">Figure 12-7</a>. For simplicity, only the horizontal dimension is shown here, but of course the same logic applies to the vertical dimension as well.</p>

<figure><div id="padding_options_stride_1_diagram" class="figure">
<img src="assets/hmls_1207.png" alt="Diagram comparing two padding options for a convolution with `stride=1` and `kernel_size=7`, illustrating &quot;valid&quot; and &quot;same&quot; padding concepts." width="1434" height="534"/>
<h6><span class="label">Figure 12-7. </span>Two different padding options, with <code translate="no">stride=1</code> and <code translate="no">kernel_size=7</code></h6>
</div></figure>

<p>If the stride is greater than 1 (in any direction), then the output size will be much smaller than the input size. For example, assuming the input size is 70 × 120, then if you set <code translate="no">stride=2</code> (or equivalently <code translate="no">stride=(2, 2)</code>), <code translate="no">padding=3</code>, and <code translate="no">kernel_size=7</code>, then the output feature maps will be 35 × 60: halved both vertically and horizontally. You could set a very large padding value to make the output size identical to the input size, but that’s almost certainly a bad idea since it would drown your image in a sea of zeros (for this reason, PyTorch raises an exception if you set <code translate="no">padding="same"</code> along with a <code translate="no">stride</code> greater than 1). <a data-type="xref" href="#padding_options_stride_2_diagram">Figure 12-8</a> illustrates <code translate="no">stride=2</code>, with <code translate="no">kernel_size=7</code> and <code translate="no">padding</code> set to 0 or 3.</p>

<figure><div id="padding_options_stride_2_diagram" class="figure">
<img src="assets/hmls_1208.png" alt="Diagram illustrating two padding options with `stride=2` and `kernel_size=7`: one with &quot;valid&quot; padding and ignored values, and one with &quot;same&quot; padding using zero padding." width="1434" height="417"/>
<h6><span class="label">Figure 12-8. </span>Two different padding options, with <code translate="no">stride=2</code> and <code translate="no">kernel_size=7</code>: the output size is much smaller</h6>
</div></figure>

<p class="pagebreak-before">Now let’s look at the layer’s parameters (which were denoted as <em>w</em><sub><em>u</em>,</sub> <sub><em>v</em>,</sub> <sub><em>k</em>′,</sub> <sub><em>k</em></sub> and <em>b</em><sub><em>k</em></sub> in <a data-type="xref" href="#convolutional_layer_equation">Equation 12-1</a>). Just like an <code translate="no">nn.Linear</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.Linear" id="id2768"/>, an <code translate="no">nn.Conv2d</code> layer holds all the layer’s parameters, including the kernels and biases, which are accessible via the <code translate="no">weight</code> and <code translate="no">bias</code> attributes<a data-type="indexterm" data-primary="weights" data-secondary="convolutional layers" id="id2769"/><a data-type="indexterm" data-primary="bias" data-secondary="convolutional layers" id="id2770"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="biases of" id="id2771"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">conv_layer</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">shape</code>
<code class="go">torch.Size([32, 3, 7, 7])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">conv_layer</code><code class="o">.</code><code class="n">bias</code><code class="o">.</code><code class="n">shape</code>
<code class="go">torch.Size([32])</code></pre>

<p>The <code translate="no">weight</code> tensor is 4D, and its shape is [<em>output_channels</em>, <em>input_channels</em>, <em>kernel_height</em>, <em>kernel_width</em>]. The <code translate="no">bias</code> tensor is 1D, with shape [<em>output_channels</em>]. The number of output channels is equal to the number of output feature maps, which is also equal to the number of filters. Most importantly, note that the height and width of the input images do not appear in the kernel’s shape: this is because all the neurons in the output feature maps share the same weights, as explained earlier. This means that you can feed images of any size to this layer, as long as they are at least as large as the kernels, and if they have the right number of channels (three in this case).</p>

<p>It’s important to add an activation function<a data-type="indexterm" data-primary="activation functions" data-secondary="for Conv2d layer" data-secondary-sortas="Conv2d layer" id="id2772"/> after each convolutional layer. This is for the same reason as for <code translate="no">nn.Linear</code> layers: a convolutional layer performs a linear operation, so if you stacked multiple convolutional layers without any activation functions, they would all be equivalent to a single convolutional layer, and they wouldn’t be able to learn anything really complex.</p>

<p>Both the <code translate="no">weight</code> and <code translate="no">bias</code> parameters are initialized randomly, using a uniform distribution similar to the one used by the <code translate="no">nn.Linear</code> layer, between <math alttext="minus StartFraction 1 Over StartRoot k EndRoot EndFraction">
  <mrow>
    <mo>-</mo>
    <mfrac><mn>1</mn> <msqrt><mi>k</mi></msqrt></mfrac>
  </mrow>
</math> and <math alttext="plus StartFraction 1 Over StartRoot k EndRoot EndFraction">
  <mrow>
    <mo>+</mo>
    <mfrac><mn>1</mn> <msqrt><mi>k</mi></msqrt></mfrac>
  </mrow>
</math>, where <em>k</em> is the fan<sub>in</sub>. In <code translate="no">nn.Conv2d</code>, <em>k</em> = <em>f</em><sub>h</sub> × <em>f</em><sub>w</sub> × <em>f</em><sub>n’</sub>, where <em>f</em><sub>h</sub> and <em>f</em><sub>w</sub> are the height and width of the kernel, and <em>f</em><sub>n’</sub> is the number of input channels. As we saw in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>, you will generally want to reinitialize the weights depending on the activation function you use. For example, you should apply He initialization<a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="He initialization with" id="id2773"/> whenever you use the ReLU activation function<a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="in CNN architectures" data-secondary-sortas="CNN architectures" id="id2774"/>. As for the biases, they can just be reinitialized to zero.</p>

<p>As you can see, convolutional layers have quite a few hyperparameters<a data-type="indexterm" data-primary="hyperparameters" data-secondary="convolutional layers" id="id2775"/>: the number of filters (<code translate="no">out_channels</code>), the kernel size, the type of padding, the strides, and the activation function. As always, you can use cross-validation to find the right hyperparameter values, but this is very time-consuming. We will discuss common CNN architectures later in this chapter to give you some idea of which hyperparameter values work best in practice<a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsconvolutionallayers122734_1" id="id2776"/><a data-type="indexterm" data-startref="xi_PyTorchconvolutionallayerimplementation12152275_1" id="id2777"/><a data-type="indexterm" data-startref="xi_torchnnConv2d12192147_1" id="id2778"/><a data-type="indexterm" data-startref="xi_imagesclassifyingandgeneratingtransferlearningpretrainedmodel12152275_1" id="id2779"/>.</p>

<p>Now, let’s look at the second common building block of CNNs: the <em>pooling layer</em>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Pooling Layers"><div class="sect1" id="id218">
<h1>Pooling Layers</h1>

<p>Once you understand how convolutional layers work, the pooling layers<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="pooling layers" id="xi_convolutionalneuralnetworksCNNspoolinglayers1226370_1"/><a data-type="indexterm" data-primary="pooling layers" id="xi_poolinglayers1226370_1"/> are quite easy to grasp. Their goal<a data-type="indexterm" data-primary="subsampling, pooling layer" id="id2780"/> is to <em>subsample</em> (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters (thereby limiting the risk of overfitting).</p>

<p>Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights or biases; all it does is aggregate the inputs using an aggregation function such as the max or mean. <a data-type="xref" href="#max_pooling_diagram">Figure 12-9</a> shows a <em>max pooling layer</em>, which is the most common type of pooling layer<a data-type="indexterm" data-primary="max pooling layer" id="xi_maxpoolinglayer12265521_1"/>. In this example<a data-type="indexterm" data-primary="pooling kernel" id="id2781"/><a data-type="indexterm" data-primary="kernels, convolution" id="id2782"/>, we use a 2 × 2 <em>pooling kernel</em>,⁠<sup><a data-type="noteref" id="id2783-marker" href="ch12.html#id2783">7</a></sup> with a stride of 2 and no padding. Only the max input value in each receptive field makes it to the next layer, while the other inputs are dropped. For example, in the lower-left receptive field in <a data-type="xref" href="#max_pooling_diagram">Figure 12-9</a>, the input values are 1, 5, 3, and 2, so only the max value, 5, is propagated to the next layer. Because of the stride of 2, the output image has half the height and half the width of the input image (rounded down since we use no padding).</p>

<figure class="smallereighty"><div id="max_pooling_diagram" class="figure">
<img src="assets/hmls_1209.png" alt="Diagram illustrating a max pooling layer using a 2x2 kernel with a stride of 2, showing how the maximum value from each receptive field is selected, reducing the output size." width="1437" height="689"/>
<h6><span class="label">Figure 12-9. </span>Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A pooling layer typically works on every input channel independently, so the output depth (i.e., the number of channels) is the same as the input depth.</p>
</div>

<p>Other than reducing computations, memory usage, and the number of parameters, a max pooling layer also introduces some level<a data-type="indexterm" data-primary="invariance, max pooling layer" id="id2784"/> of <em>invariance</em> to small translations, as shown in <a data-type="xref" href="#pooling_invariance_diagram">Figure 12-10</a>. Here we assume that the bright pixels have a lower value than dark pixels, and we consider three images (A, B, C) going through a max pooling layer with a 2 × 2 kernel and stride 2. Images B and C are the same as image A, but shifted by one and two pixels to the right. As you can see, the outputs of the max pooling layer for images A and B are identical. This is what translation invariance means. For image C, the output is different: it is shifted one pixel to the right (but there is still 50% invariance). By inserting a max pooling layer every few layers in a CNN, it is possible to get some level of translation invariance at a larger scale. Moreover, max pooling offers a small amount of rotational invariance and a slight scale invariance. Such invariance (even if it is limited) can be useful in cases where the prediction should not depend on these details, such as in classification tasks.</p>

<figure class="width-85"><div id="pooling_invariance_diagram" class="figure">
<img src="assets/hmls_1210.png" alt="Diagram illustrating how max pooling leads to invariance to small translations, showing reduced output size despite shifts in input grid positions." width="1385" height="864"/>
<h6><span class="label">Figure 12-10. </span>Invariance to small translations</h6>
</div></figure>

<p class="pagebreak-before">However, max pooling has some downsides too. It’s obviously very destructive: even with a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both directions (so its area will be four times smaller), thereby dropping 75% of the input values. And in some applications, invariance is not desirable. Take semantic segmentation (the task of classifying each pixel in an image according to the object that pixel belongs to, which we’ll explore later in this chapter): obviously, if the input image is translated by one pixel to the right, the output should also be translated by one pixel to the right. The goal in this case is <em>equivariance</em>, not invariance<a data-type="indexterm" data-primary="equivariance" id="id2785"/>: a small change to the inputs should lead to a corresponding small change in the output.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Implementing Pooling Layers with PyTorch"><div class="sect1" id="id219">
<h1>Implementing Pooling Layers with PyTorch</h1>

<p>The following code creates an <code translate="no">nn.MaxPool2d</code> layer, using a 2 × 2 kernel. The strides default to the kernel size<a data-type="indexterm" data-primary="PyTorch" data-secondary="pool layer implementation" id="xi_PyTorchpoollayerimplementation12284112_1"/>, so this layer uses a stride of 2 (horizontally and vertically). By default, it uses <code translate="no">padding=0</code> (i.e., “valid” padding):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">max_pool</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="n">kernel_size</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code></pre>

<p>To create<a data-type="indexterm" data-primary="average pooling layer" id="id2786"/> an <em>average pooling layer</em>, just<a data-type="indexterm" data-primary="torch" data-secondary="nn.AvgPool2d" id="id2787"/><a data-type="indexterm" data-primary="torch" data-secondary="nn.MaxPool2d" id="id2788"/> use <code translate="no">nn.AvgPool2d</code>, instead of <code translate="no">nn.MaxPool2d</code>. As you might expect, it works exactly like a max pooling layer, except it computes the mean rather than the max. Average pooling layers used to be very popular, but people mostly use max pooling layers now, as they generally perform better. This may seem surprising, since computing the mean generally loses less information than computing the max. But on the other hand, max pooling preserves only the strongest features, getting rid of all the meaningless ones, so the next layers get a cleaner signal to work with. Moreover, max pooling offers stronger translation invariance than average pooling, and it requires slightly less compute.</p>

<p>Note that max pooling and average pooling can also be performed along the depth dimension instead of the spatial dimensions, although it’s not as common. This can allow the CNN to learn to be invariant to various features. For example, it could learn multiple filters, each detecting a different rotation of the same pattern (such as handwritten digits; see <a data-type="xref" href="#depth_wise_pooling_diagram">Figure 12-11</a>), and the depthwise max pooling layer would ensure that the output is the same regardless of the rotation. The CNN could similarly learn to be invariant to anything: thickness, brightness, skew, color, and so on.</p>

<figure class="width-75"><div id="depth_wise_pooling_diagram" class="figure">
<img src="assets/hmls_1211.png" alt="Diagram illustrating depthwise max pooling in a CNN, showcasing how it helps achieve rotational invariance by selecting maximum features across learned filters." width="1328" height="1022"/>
<h6><span class="label">Figure 12-11. </span>Depthwise max pooling can help the CNN learn to be invariant (to rotation in this case)</h6>
</div></figure>

<p>PyTorch does not include a depthwise max pooling layer, but we can implement a custom module based on the <code translate="no">torch.F.max_pool1d()</code> function<a data-type="indexterm" data-primary="torch" data-secondary="nn.functional.max_pool1d()" id="id2789"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch.nn.functional</code> <code class="k">as</code> <code class="nn">F</code>

<code class="k">class</code> <code class="nc">DepthPool</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">kernel_size</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code> <code class="o">=</code> <code class="n">kernel_size</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">stride</code> <code class="o">=</code> <code class="n">stride</code> <code class="k">if</code> <code class="n">stride</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code> <code class="k">else</code> <code class="n">kernel_size</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">padding</code> <code class="o">=</code> <code class="n">padding</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="n">batch</code><code class="p">,</code> <code class="n">channels</code><code class="p">,</code> <code class="n">height</code><code class="p">,</code> <code class="n">width</code> <code class="o">=</code> <code class="n">inputs</code><code class="o">.</code><code class="n">shape</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">inputs</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="n">batch</code><code class="p">,</code> <code class="n">channels</code><code class="p">,</code> <code class="n">height</code> <code class="o">*</code> <code class="n">width</code><code class="p">)</code>  <code class="c1"># merge spatial dims</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">Z</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>  <code class="c1"># switch spatial and channels dims</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">max_pool1d</code><code class="p">(</code><code class="n">Z</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">stride</code><code class="p">,</code>
                         <code class="n">padding</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">padding</code><code class="p">)</code>  <code class="c1"># compute max pool</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">Z</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>  <code class="c1"># switch back spatial and channels dims</code>
        <code class="k">return</code> <code class="n">Z</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="n">batch</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">height</code><code class="p">,</code> <code class="n">width</code><code class="p">)</code>  <code class="c1"># unmerge spatial dims</code></pre>

<p>For example, suppose the input batch contains two 70 × 120 images, each with 32 channels (i.e., the inputs have a shape of <code translate="no">[2, 32, 70, 120]</code>), and we use <code translate="no">kernel_size=4</code>, and the default <code translate="no">stride</code> (equal to <code translate="no">kernel_size</code>) and <code translate="no">padding=0</code>:</p>

<ul>
<li>
<p>The <code translate="no">forward()</code> method<a data-type="indexterm" data-primary="torch" data-secondary="forward()" id="id2790"/> starts by merging the spatial dimensions, which gives us a tensor of shape <code translate="no">[2, 32, 8400]</code> (since 70 × 120 = 8,400).</p>
</li>
<li>
<p>It then permutes the last two dimensions, so we get a shape of <code translate="no">[2, 8400, 32]</code>.</p>
</li>
<li>
<p>Next, it uses the <code translate="no">max_pool1d()</code> function to compute the max pool along the last dimension, which corresponds to our original 32 channels. Since <code translate="no">kernel_size</code> and <code translate="no">stride</code> are both equal to 4, and we don’t use any padding, the size of the last dimension gets divided by 4, so the resulting shape is <code translate="no">[2, 8400, 8]</code>.</p>
</li>
<li>
<p>The function then permutes the last two dimensions again, giving us a shape of <code translate="no">[2, 8, 8400]</code>.</p>
</li>
<li>
<p>Lastly, it separates the spatial dimensions to get the final shape of <code translate="no">[2, 8, 50, 100]</code>. You can verify that the output is exactly what we were after<a data-type="indexterm" data-startref="xi_maxpoolinglayer12265521_1" id="id2791"/>.</p>
</li>
</ul>

<p>One last type of pooling layer that you will often see in modern architectures<a data-type="indexterm" data-primary="global average pooling layer" id="id2792"/> is the <em>global average pooling layer</em>. It works very differently: all it does is compute the mean of each entire feature map. Therefore it outputs a single number per feature map and per instance. Although this is of course extremely destructive (most of the information in the feature map is lost), it can be useful just before the output layer, as you will see later in this chapter.</p>

<p>To create such a layer, one option is to use a regular <code translate="no">nn.AvgPool2d</code> layer and set its kernel size to the same size as the inputs. However, this is not very convenient since it requires knowing the exact dimensions of the inputs ahead of time. A simpler solution is to use the <code translate="no">nn.AdaptiveAvgPool2d</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.AdaptiveAvgPool2d" id="id2793"/>, which lets you specify the desired spatial dimensions of the output: it automatically adapts the kernel size (with an equal stride) to get the desired result, adding a bit of padding if needed. If we set the output size to 1, we get a global average pooling layer:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">global_avg_pool</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">AdaptiveAvgPool2d</code><code class="p">(</code><code class="n">output_size</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">output</code> <code class="o">=</code> <code class="n">global_avg_pool</code><code class="p">(</code><code class="n">cropped_images</code><code class="p">)</code></pre>

<p>Alternatively, you could just use the <code translate="no">torch.mean()</code> function to get the same output:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">output</code> <code class="o">=</code> <code class="n">cropped_images</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="n">keepdim</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>Now you know all the building blocks to create convolutional neural networks. Let’s see how to assemble them<a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNspoolinglayers1226370_1" id="id2794"/><a data-type="indexterm" data-startref="xi_poolinglayers1226370_1" id="id2795"/><a data-type="indexterm" data-startref="xi_PyTorchpoollayerimplementation12284112_1" id="id2796"/>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="CNN Architectures"><div class="sect1" id="id220">
<h1>CNN Architectures</h1>

<p>Typical CNN architectures<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="architectures" id="xi_convolutionalneuralnetworksCNNsarchitectures1235126_1"/> stack a few convolutional layers (each one generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e., with more feature maps), thanks to the convolutional layers (see <a data-type="xref" href="#cnn_architecture_diagram">Figure 12-12</a>). At the top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that outputs estimated class probabilities).</p>

<figure><div id="cnn_architecture_diagram" class="figure">
<img src="assets/hmls_1212.png" alt="Diagram illustrating a typical CNN architecture with layers for convolution, pooling, and fully connected operations." width="1437" height="370"/>
<h6><span class="label">Figure 12-12. </span>Typical CNN architecture</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>Instead of using a convolutional layer with a 5 × 5 kernel<a data-type="indexterm" data-primary="kernels, convolution" id="id2797"/>, it is generally preferable to stack two layers with 3 × 3 kernels: it will use fewer parameters and require fewer computations, and it will usually perform better. One exception is for the first convolutional layer: it can typically have a large kernel (e.g., 5 × 5 or 7 × 7), usually with a stride of 2 or more. This reduces the spatial dimension of the image without losing too much information, and since the input image only has three channels in general, it will not be too costly.</p>
</div>

<p>Here is how you can implement a basic CNN to tackle the Fashion MNIST dataset<a data-type="indexterm" data-primary="Fashion MNIST dataset" data-secondary="classification CNNs" id="xi_FashionMNISTdatasetCNNimplementation1235978_1"/> (introduced in <a data-type="xref" href="ch09.html#ann_chapter">Chapter 9</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">functools</code> <code class="kn">import</code> <code class="n">partial</code>

<code class="n">DefaultConv2d</code> <code class="o">=</code> <code class="n">partial</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">DefaultConv2d</code><code class="p">(</code><code class="n">in_channels</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">out_channels</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">7</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="n">kernel_size</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>
    <code class="n">DefaultConv2d</code><code class="p">(</code><code class="n">in_channels</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> <code class="n">out_channels</code><code class="o">=</code><code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">DefaultConv2d</code><code class="p">(</code><code class="n">in_channels</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code> <code class="n">out_channels</code><code class="o">=</code><code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="n">kernel_size</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>
    <code class="n">DefaultConv2d</code><code class="p">(</code><code class="n">in_channels</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code> <code class="n">out_channels</code><code class="o">=</code><code class="mi">256</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">DefaultConv2d</code><code class="p">(</code><code class="n">in_channels</code><code class="o">=</code><code class="mi">256</code><code class="p">,</code> <code class="n">out_channels</code><code class="o">=</code><code class="mi">256</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="n">kernel_size</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">in_features</code><code class="o">=</code><code class="mi">2304</code><code class="p">,</code> <code class="n">out_features</code><code class="o">=</code><code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.5</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">in_features</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code> <code class="n">out_features</code><code class="o">=</code><code class="mi">64</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.5</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">in_features</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> <code class="n">out_features</code><code class="o">=</code><code class="mi">10</code><code class="p">),</code>
<code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>Let’s go through this code:</p>

<ul>
<li>
<p>We use the <code translate="no">functools.partial()</code> function<a data-type="indexterm" data-primary="functools.partial()" id="id2798"/> (introduced in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>) to define <code translate="no">DefaultConv2d</code>, which acts just like <code translate="no">nn.Conv2d</code> but with different default arguments: a small kernel size of 3, and <code translate="no">"same"</code> padding. This avoids having to repeat these arguments throughout the model.</p>
</li>
<li>
<p>Next, we create the <code translate="no">nn.Sequential</code> model<a data-type="indexterm" data-primary="torch" data-secondary="nn.Sequential" id="id2799"/>. Its first layer is a <code translate="no">DefaultConv2d</code> with 64 fairly large filters (7 × 7). It uses the default stride of 1 because the input images are not very large. It also uses <code translate="no">in_channels=1</code> because the Fashion MNIST images have a single color channel (i.e., grayscale). Each convolutional layer is followed by the ReLU activation function<a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="in CNN architectures" data-secondary-sortas="CNN architectures" id="id2800"/>.</p>
</li>
<li>
<p>We then add a max pooling layer with a kernel size of 2, so it divides each spatial dimension by a factor of 2 (rounded down if needed).</p>
</li>
<li>
<p>Then we repeat the same structure twice: two convolutional layers followed by a max pooling layer. For larger images, we could repeat this structure several more times. The number of repetitions is a hyperparameter you can tune.</p>
</li>
<li>
<p>Note that the number of filters doubles as we climb up the CNN toward the output layer (it is initially 64, then 128, then 256). It makes sense for it to grow, since the number of low-level features is often fairly low (e.g., small circles, horizontal lines), but there are many different ways to combine them into higher-level features. It is a common practice to double the number of filters after each pooling layer: since a pooling layer divides each spatial dimension by a factor of 2, we can afford to double the number of feature maps<a data-type="indexterm" data-primary="feature maps" id="xi_featuremaps12390544_1"/> in the next layer without fear of exploding the number of parameters, memory usage, or computational load.</p>
</li>
<li>
<p>Next is the fully connected network, composed of two hidden dense layers (<code translate="no">nn.Linear</code>) with the ReLU activation function, plus a dense output layer. Since it’s a classification task with 10 classes, the output layer has 10 units. As we did in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>, we leave out the softmax activation function, so the model will output logits rather than probabilities, and we must use the <code translate="no">nn.CrossEntropyLoss</code> to train the model<a data-type="indexterm" data-primary="torch" data-secondary="nn.CrossEntropyLoss" id="id2801"/>. Note that we must flatten the inputs just before the first dense layer, since it expects a 1D<a data-type="indexterm" data-primary="1D convolutional layers" data-primary-sortas="one d convolutional layers" id="id2802"/> array of features for each instance. We also add two dropout layers, with a dropout rate of 50% each, to reduce overfitting.</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>The first <code translate="no">nn.Linear</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.Linear" id="id2803"/> has 2,304 input features: where did this number come from? Well the Fashion MNIST images are 28 × 28 pixels, but the pooling layers shrink them to 14 × 14, then 7 × 7, and finally 3 × 3. Just before the first <code translate="no">nn.Linear</code> layer, there are 256 feature maps, so we end up with 256 × 3 × 3 = 2,304 input features. Figuring out the number of features can sometimes be a bit difficult, but one trick is to set <code translate="no">in_features</code> to some arbitrary value (say, 999), and let training crash. The correct number of features appears in the error message: “RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x2304 and 999x128)”. Another option is to use <code translate="no">nn.LazyLinear</code> instead of <code translate="no">nn.Linear</code>: it’s just like the <code translate="no">nn.Linear</code> layer, except it only creates the weights matrix the first time it gets called: it can then automatically set the number of input features to the correct value. Other layers—such as convolutional layers and batch-norm layers—also have lazy variants.</p>
</div>

<p>If you train this model on the Fashion MNIST training set<a data-type="indexterm" data-startref="xi_FashionMNISTdatasetCNNimplementation1235978_1" id="id2804"/>, it should reach close to 92% accuracy on the test set (you can use the <code translate="no">train()</code> and <code translate="no">evaluate_tm()</code> functions we defined in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>). It’s not state of the art, but it is pretty good, and better than what we achieved with dense networks in <a data-type="xref" href="ch09.html#ann_chapter">Chapter 9</a>.</p>

<p>Over the years, variants of this fundamental architecture have been developed, leading to amazing advances in the field. A good measure of this progress is the error rate in competitions such as the ILSVRC <a href="https://image-net.org">ImageNet challenge</a>. In this competition, the error rate for image classification fell from over 26% to less than 2.3% in just 6 years. More precisely, this was the <em>top-five error rate</em>, which is the ratio of test images for which the system’s five most confident predictions did <em>not</em> include the correct answer. The images are fairly large (e.g., 256 pixels high) and there are 1,000 classes, some of which are really subtle (try distinguishing 120 dog breeds!). Looking at the evolution of the winning entries is a good way to understand how CNNs work, and how research in deep learning progresses.</p>

<p>We will first look at the classical LeNet-5 architecture (1998), then several winners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), ResNet (2015), and SENet (2017). We will also discuss a few more architectures, including VGGNet, Xception, ResNeXt, DenseNet, MobileNet, CSPNet, EfficientNet, and ConvNeXt (and we will discuss vision transformers in <a data-type="xref" href="ch16.html#vit_chapter">Chapter 16</a>).</p>








<section data-type="sect2" data-pdf-bookmark="LeNet-5"><div class="sect2" id="id221">
<h2>LeNet-5</h2>

<p>The <a href="https://homl.info/lenet5">LeNet-5 architecture</a>⁠<sup><a data-type="noteref" id="id2805-marker" href="ch12.html#id2805">8</a></sup> is perhaps the most widely known CNN architecture<a data-type="indexterm" data-primary="LeNet-5" id="id2806"/>. As mentioned earlier, it was created by Yann LeCun in 1998 and has been widely used for handwritten digit recognition (MNIST). It is composed of the layers shown in <a data-type="xref" href="#lenet_5_architecture">Table 12-1</a>.</p>
<table id="lenet_5_architecture">
<caption><span class="label">Table 12-1. </span>LeNet-5 architecture</caption>
<thead>
<tr>
<th>Layer</th>
<th>Type</th>
<th>Maps</th>
<th>Size</th>
<th>Kernel size</th>
<th>Stride</th>
<th>Activation</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Out</p></td>
<td><p>Fully connected</p></td>
<td><p>–</p></td>
<td><p>10</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>RBF</p></td>
</tr>
<tr>
<td><p>F6</p></td>
<td><p>Fully connected</p></td>
<td><p>–</p></td>
<td><p>84</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>C5</p></td>
<td><p>Convolution</p></td>
<td><p>120</p></td>
<td><p>1 × 1</p></td>
<td><p>5 × 5</p></td>
<td><p>1</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>S4</p></td>
<td><p>Avg pooling</p></td>
<td><p>16</p></td>
<td><p>5 × 5</p></td>
<td><p>2 × 2</p></td>
<td><p>2</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>C3</p></td>
<td><p>Convolution</p></td>
<td><p>16</p></td>
<td><p>10 × 10</p></td>
<td><p>5 × 5</p></td>
<td><p>1</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>S2</p></td>
<td><p>Avg pooling</p></td>
<td><p>6</p></td>
<td><p>14 × 14</p></td>
<td><p>2 × 2</p></td>
<td><p>2</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>C1</p></td>
<td><p>Convolution</p></td>
<td><p>6</p></td>
<td><p>28 × 28</p></td>
<td><p>5 × 5</p></td>
<td><p>1</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>In</p></td>
<td><p>Input</p></td>
<td><p>1</p></td>
<td><p>32 × 32</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
</tr>
</tbody>
</table>

<p>As you can see, this looks pretty similar to our Fashion MNIST model: a stack of convolutional layers and pooling layers, followed by a dense network. Perhaps the main difference with more modern classification CNNs is the activation functions<a data-type="indexterm" data-primary="activation functions" data-secondary="LeNet-5" id="id2807"/>: today, we would use ReLU instead of tanh, and softmax instead of RBF (introduced in <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a>). There were several other minor differences that don’t really matter much, but in case you are interested, they are listed in this chapter’s notebook at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>. Yann LeCun’s <a href="http://yann.lecun.com/exdb/lenet">website</a> also features great demos of LeNet-5 classifying digits.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="AlexNet"><div class="sect2" id="id222">
<h2>AlexNet</h2>

<p>The <a href="https://homl.info/80">AlexNet CNN architecture</a>⁠<sup><a data-type="noteref" id="id2808-marker" href="ch12.html#id2808">9</a></sup> won the 2012 ILSVRC challenge by a large margin: it achieved a top-five error rate of 17%, while the second best competitor achieved only 26%! AlexNet was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and Geoffrey Hinton. It is similar to LeNet-5<a data-type="indexterm" data-primary="AlexNet" id="xi_AlexNet12422533_1"/>, only much larger and deeper, and it was the first to stack convolutional layers directly on top of one another, instead of stacking a pooling layer on top of each convolutional layer. <a data-type="xref" href="#alexnet_architecture">Table 12-2</a> presents this architecture.</p>
<table id="alexnet_architecture">
<caption><span class="label">Table 12-2. </span>AlexNet architecture</caption>
<thead>
<tr>
<th>Layer</th>
<th>Type</th>
<th>Maps</th>
<th>Size</th>
<th>Kernel size</th>
<th>Stride</th>
<th>Padding</th>
<th>Activation</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Out</p></td>
<td><p>Fully connected</p></td>
<td><p>–</p></td>
<td><p>1,000</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>Softmax</p></td>
</tr>
<tr>
<td><p>F10</p></td>
<td><p>Fully connected</p></td>
<td><p>–</p></td>
<td><p>4,096</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>F9</p></td>
<td><p>Fully connected</p></td>
<td><p>–</p></td>
<td><p>4,096</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>S8</p></td>
<td><p>Max pooling</p></td>
<td><p>256</p></td>
<td><p>6 × 6</p></td>
<td><p>3 × 3</p></td>
<td><p>2</p></td>
<td><p><code translate="no">valid</code></p></td>
<td><p>–</p></td>
</tr>
<tr>
<td><p>C7</p></td>
<td><p>Convolution</p></td>
<td><p>256</p></td>
<td><p>13 × 13</p></td>
<td><p>3 × 3</p></td>
<td><p>1</p></td>
<td><p><code translate="no">same</code></p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>C6</p></td>
<td><p>Convolution</p></td>
<td><p>384</p></td>
<td><p>13 × 13</p></td>
<td><p>3 × 3</p></td>
<td><p>1</p></td>
<td><p><code translate="no">same</code></p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>C5</p></td>
<td><p>Convolution</p></td>
<td><p>384</p></td>
<td><p>13 × 13</p></td>
<td><p>3 × 3</p></td>
<td><p>1</p></td>
<td><p><code translate="no">same</code></p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>S4</p></td>
<td><p>Max pooling</p></td>
<td><p>256</p></td>
<td><p>13 × 13</p></td>
<td><p>3 × 3</p></td>
<td><p>2</p></td>
<td><p><code translate="no">valid</code></p></td>
<td><p>–</p></td>
</tr>
<tr>
<td><p>C3</p></td>
<td><p>Convolution</p></td>
<td><p>256</p></td>
<td><p>27 × 27</p></td>
<td><p>5 × 5</p></td>
<td><p>1</p></td>
<td><p><code translate="no">same</code></p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>S2</p></td>
<td><p>Max pooling</p></td>
<td><p>96</p></td>
<td><p>27 × 27</p></td>
<td><p>3 × 3</p></td>
<td><p>2</p></td>
<td><p><code translate="no">valid</code></p></td>
<td><p>–</p></td>
</tr>
<tr>
<td><p>C1</p></td>
<td><p>Convolution</p></td>
<td><p>96</p></td>
<td><p>55 × 55</p></td>
<td><p>11 × 11</p></td>
<td><p>4</p></td>
<td><p><code translate="no">valid</code></p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>In</p></td>
<td><p>Input</p></td>
<td><p>3 (RGB)</p></td>
<td><p>227 × 227</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
</tr>
</tbody>
</table>

<p>To reduce overfitting, the authors used two regularization techniques. First, they applied dropout (introduced in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>) with a 50% dropout rate during training to the outputs of layers F9 and F10. Second, they performed data augmentation by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id2809">
<h1>Data Augmentation</h1>
<p>Data augmentation<a data-type="indexterm" data-primary="data augmentation" id="xi_dataaugmentation1244718_1"/> artificially increases the size of the training set<a data-type="indexterm" data-primary="training set expansion" id="xi_trainingsetexpansion1244770_1"/> by generating many realistic variants of each training instance. This reduces overfitting, making this a regularization technique. The generated instances should be as realistic as possible: ideally, given an image from the augmented training set, a human should not be able to tell whether it was augmented or not. Simply adding white noise will not help; the modifications should be learnable (white noise is not).</p>

<p>For example, you can slightly shift, rotate, and resize every picture in the training set by various amounts and add the resulting pictures to the training set (see <a data-type="xref" href="#data_augmentation_diagram">Figure 12-13</a>). To do this, you can use tools available in <code translate="no">torchvision.transforms.v2</code> (e.g., <code translate="no">RandomCrop</code>, <code translate="no">RandomRotation</code>, etc.). This forces the model to be more tolerant to variations in the position, orientation, and size of the objects in the pictures. You can similarly use transforms to tweak the colors, and contrasts to simulate many different lighting conditions. In general, you can also flip the pictures horizontally (except for text and other asymmetrical objects). By combining these transformations (using <code translate="no">Compose</code>), you can greatly increase your training set size.</p>

<figure class="width-80"><div id="data_augmentation_diagram" class="figure">
<img src="assets/hmls_1213.png" alt="Diagram illustrating data augmentation by showing variations of a mushroom image generated through transformations like rotation and cropping." width="1439" height="887"/>
<h6><span class="label">Figure 12-13. </span>Generating new training instances from existing ones</h6>
</div></figure>

<p>Data augmentation is also useful when you have an unbalanced dataset: you can use it to generate more samples of the less frequent classes. This is called the <em>synthetic minority oversampling technique</em>, or SMOTE<a data-type="indexterm" data-primary="synthetic minority oversampling technique (SMOTE)" id="id2810"/><a data-type="indexterm" data-primary="SMOTE (synthetic minority oversampling technique)" id="id2811"/> for short.</p>

<p>Lastly, although data augmentation is typically used only during training, one exception is <em>test-time augmentation</em> (TTA)<a data-type="indexterm" data-primary="test-time augmentation (TTA)" id="id2812"/><a data-type="indexterm" data-primary="TTA (test-time augmentation)" id="id2813"/>: this technique involves augmenting the test data and combining the predictions to boost accuracy. For example, if three augmented versions of an image are classified as a bus, while seven are classified as a truck, then it’s probably a truck<a data-type="indexterm" data-startref="xi_dataaugmentation1244718_1" id="id2814"/><a data-type="indexterm" data-startref="xi_trainingsetexpansion1244770_1" id="id2815"/>.</p>
</div></aside>

<p>AlexNet also used a regularization technique<a data-type="indexterm" data-primary="local response normalization (LRN)" id="id2816"/><a data-type="indexterm" data-primary="LRN (local response normalization)" id="id2817"/><a data-type="indexterm" data-primary="regularization" data-secondary="LRN" id="id2818"/> called <em>local response normalization</em> (LRN): the most strongly activated neurons inhibit other neurons located at the same position in neighboring feature maps. Such competitive activation has been observed in biological neurons. This encourages different feature maps to specialize, pushing them apart and forcing them to explore a wider range of features, ultimately improving generalization. However, this technique was mostly superseded by simpler and more efficient regularization techniques, especially batch normalization.</p>

<p>A variant of AlexNet<a data-type="indexterm" data-primary="ZFNet" id="id2819"/> called <a href="https://homl.info/zfnet"><em>ZFNet</em></a>⁠<sup><a data-type="noteref" id="id2820-marker" href="ch12.html#id2820">10</a></sup> was developed by Matthew Zeiler and Rob Fergus and won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked hyperparameters (number of feature maps, kernel size, stride, etc.)<a data-type="indexterm" data-startref="xi_AlexNet12422533_1" id="id2821"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="GoogLeNet"><div class="sect2" id="id223">
<h2>GoogLeNet</h2>

<p>The <a href="https://homl.info/81">GoogLeNet architecture</a> was developed by Christian 
<span class="keep-together">Szegedy</span> et al. from Google Research,⁠<sup><a data-type="noteref" id="id2822-marker" href="ch12.html#id2822">11</a></sup> and it won the ILSVRC 2014 challenge by pushing the top-five error rate below 7%. This great performance<a data-type="indexterm" data-primary="GoogLeNet" id="xi_GoogLeNet12465385_1"/> came in large part from the fact that the network was much deeper than previous CNNs (as you’ll see in 
<span class="keep-together"><a data-type="xref" href="#googlenet_diagram">Figure 12-15</a>).</span> This was made possible by subnetworks<a data-type="indexterm" data-primary="inception module" id="xi_inceptionmodule12465550_1"/> called <em>inception modules</em>,⁠<sup><a data-type="noteref" id="id2823-marker" href="ch12.html#id2823">12</a></sup> which allow GoogLeNet to use parameters much more efficiently than previous architectures: GoogLeNet actually has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million).</p>

<p><a data-type="xref" href="#inception_module_diagram">Figure 12-14</a> shows the architecture of an inception module. The notation “3 × 3 + 1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and <code translate="no">"same"</code> padding<a data-type="indexterm" data-primary="“same” padding" data-primary-sortas="same padding" id="id2824"/>. The input signal is first fed to four different layers in parallel. All convolutional layers use the ReLU activation function<a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="GoogLeNet" id="xi_ReLUrectifiedlinearunitsGoogLeNet12467304_1"/>. Note that the top convolutional layers use different kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different scales. Also note that every single layer uses a stride of 1 and <code translate="no">"same"</code> padding (even the max pooling layer), so their outputs all have the same height and width as their inputs. This makes it possible to concatenate all the outputs along the depth dimension<a data-type="indexterm" data-primary="depth concatenation layer" id="id2825"/><a data-type="indexterm" data-primary="concatenation layer" id="id2826"/> in the final <em>depth concatenation layer</em> (i.e., it concatenates the multiple feature maps output by each of the upper four convolutional layers). It can be implemented using the <code translate="no">torch.cat()</code> function<a data-type="indexterm" data-primary="torch" data-secondary="cat()" id="id2827"/>, with <code translate="no">dim=1</code>.</p>

<figure class="width-75"><div id="inception_module_diagram" class="figure">
<img src="assets/hmls_1214.png" alt="Diagram illustrating the architecture of an inception module, showing parallel convolutional layers with various kernel sizes and a depth concatenation layer for combining outputs." width="1143" height="718"/>
<h6><span class="label">Figure 12-14. </span>Inception module</h6>
</div></figure>

<p>You may wonder why inception modules have convolutional layers with 1 × 1 kernels<a data-type="indexterm" data-primary="convolution kernels (kernels)" id="id2828"/><a data-type="indexterm" data-primary="kernels, convolution" id="id2829"/>. Surely these layers cannot capture any features because they look at only one pixel at a time, right? In fact, these layers serve three purposes:</p>

<ul>
<li>
<p>Although they cannot capture spatial patterns, they can capture patterns along the depth dimension (i.e., across channels).</p>
</li>
<li>
<p>They are configured to output fewer feature maps than their inputs, so they serve<a data-type="indexterm" data-primary="bottleneck layers" id="id2830"/> as <em>bottleneck layers</em>, meaning they reduce dimensionality. This cuts the computational cost and the number of parameters, speeding up training and improving generalization.</p>
</li>
<li>
<p>Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like a single powerful convolutional layer, capable of capturing more complex patterns. A convolutional layer is equivalent to sweeping a dense layer across the image (at each location, it only looks at a small receptive field), and these pairs of convolutional layers are equivalent to sweeping two-layer neural networks across the image.</p>
</li>
</ul>

<p>In short, you can think of the whole inception module as a convolutional layer on steroids, able to output feature maps that capture complex patterns at various scales.</p>

<p>Now let’s look at the architecture of the GoogLeNet CNN (see <a data-type="xref" href="#googlenet_diagram">Figure 12-15</a>). The number of feature maps output by each convolutional layer and each pooling layer is shown before the kernel size. The architecture is so deep that it has to be represented in three columns, but GoogLeNet is actually one tall stack, including nine inception modules (the boxes with the spinning tops). The six numbers in the inception modules represent the number of feature maps output by each convolutional layer in the module (in the same order as in <a data-type="xref" href="#inception_module_diagram">Figure 12-14</a>). Note that all the convolutional layers use the ReLU activation function<a data-type="indexterm" data-startref="xi_ReLUrectifiedlinearunitsGoogLeNet12467304_1" id="id2831"/>.</p>

<figure class="width-65"><div id="googlenet_diagram" class="figure">
<img src="assets/hmls_1215.png" alt="Diagram of GoogLeNet architecture showing the layers and inception modules, detailing the number of feature maps and operations like convolution, pooling, and normalization." width="925" height="1122"/>
<h6><span class="label">Figure 12-15. </span>GoogLeNet architecture</h6>
</div></figure>

<p>Let’s go through this network:</p>

<ul>
<li>
<p>The first two layers divide the image’s height and width by 4 (so its area is divided by 16), to reduce the computational load. The first layer uses a large kernel size, 7 × 7, so that much of the information is preserved.</p>
</li>
<li>
<p>Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier).</p>
</li>
<li>
<p>Two convolutional layers follow, where the first acts like a bottleneck layer. As mentioned, you can think of this pair as a single smarter convolutional layer.</p>
</li>
<li>
<p>Again, a local response normalization layer ensures that the previous layers capture a wide variety of patterns.</p>
</li>
<li>
<p>Next, a max pooling layer reduces the image height and width by 2, again to speed up computations.</p>
</li>
<li>
<p>Then comes<a data-type="indexterm" data-primary="backbone, model" id="id2832"/> the CNN’s <em>backbone</em>: a tall stack of nine inception modules, interleaved with a couple of max pooling layers to reduce dimensionality and speed up the net.</p>
</li>
<li>
<p>Next, the global average pooling layer<a data-type="indexterm" data-primary="global average pooling layer" id="id2833"/> outputs the mean of each feature map: this drops any remaining spatial information, which is fine because there is not much spatial information left at that point. Indeed, GoogLeNet input images are typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each dividing the height and width by 2, the feature maps are down to 7 × 7. Moreover, this is a classification task, not localization, so it doesn’t matter where the object is. Thanks to the dimensionality reduction brought by this layer, there is no need to have several fully connected layers at the top of the CNN (like in AlexNet), and this considerably reduces the number of parameters in the network and limits the risk of overfitting.</p>
</li>
<li>
<p>The last layers are self-explanatory: dropout for regularization, then a fully connected layer<a data-type="indexterm" data-primary="fully connected layers" id="id2834"/> with 1,000 units (since there are 1,000 classes) and a softmax activation function to output estimated class probabilities<a data-type="indexterm" data-startref="xi_featuremaps12390544_1" id="id2835"/>.</p>
</li>
</ul>

<p>The original GoogLeNet architecture included two auxiliary classifiers plugged on top of the third and sixth inception modules. They were both composed of one average pooling layer, one convolutional layer, two fully connected layers, and a softmax activation layer. During training, their loss (scaled down by 70%) was added to the overall loss. The goal was to fight the vanishing gradients problem and regularize the network, but it was later shown that their effect was relatively minor.</p>

<p>Several variants of the GoogLeNet architecture were later proposed by Google researchers, including Inception-v3 and Inception-v4, using slightly different inception modules to reach even better performance<a data-type="indexterm" data-startref="xi_GoogLeNet12465385_1" id="id2836"/><a data-type="indexterm" data-startref="xi_inceptionmodule12465550_1" id="id2837"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="ResNet"><div class="sect2" id="id224">
<h2>ResNet</h2>

<p>Kaiming He et al. won the ILSVRC 2015 challenge<a data-type="indexterm" data-primary="residual learning" id="id2838"/> using a <a href="https://homl.info/82">Residual Network (ResNet)</a>⁠<sup><a data-type="noteref" id="id2839-marker" href="ch12.html#id2839">13</a></sup> that delivered an astounding top-five error rate<a data-type="indexterm" data-primary="residual network (ResNet)" id="xi_residualnetworkResNet12505274_1"/> under 3.6%. The winning variant used an extremely deep CNN composed of 152 layers (other variants had 34, 50, and 101 layers). It confirmed the general trend: computer vision models were getting deeper and deeper, with fewer and fewer parameters. The key to being able to train such a deep network<a data-type="indexterm" data-primary="skip (shortcut) connections" id="id2840"/> is to use <em>skip connections</em> (also called <em>shortcut connections</em>): the signal feeding into a layer is also added to the output of a layer located higher up the stack. Let’s see why this is useful.</p>

<p>When training a neural network, the goal is to make it model a target function <em>h</em>(<strong>x</strong>). If you add the input <strong>x</strong> to the output of the network (i.e., you add a skip connection), then the network will be forced to model <em>f</em>(<strong>x</strong>) = <em>h</em>(<strong>x</strong>) – <strong>x</strong> rather than <em>h</em>(<strong>x</strong>). This is called <em>residual learning</em> (see <a data-type="xref" href="#residual_learning_diagram">Figure 12-16</a>).</p>

<figure class="width-85"><div id="residual_learning_diagram" class="figure">
<img src="assets/hmls_1216.png" alt="Diagram comparing a basic neural network structure with a residual network that includes a skip connection, illustrating residual learning where output models the difference between the target function and input." width="1110" height="561"/>
<h6><span class="label">Figure 12-16. </span>Residual learning</h6>
</div></figure>

<p>When you initialize a neural network, its weights are close to zero, so a regular network just outputs values close to zero when training starts. But if you add a skip connection, the resulting network outputs a copy of its inputs; in other words, it acts as the identity function at the start of training. If the target function is fairly close to the identity function (which is often the case), this will speed up training considerably.</p>

<p>Moreover, if you add many skip connections, the network can start making progress even if several layers have not started learning yet (see <a data-type="xref" href="#deep_residual_network_diagram">Figure 12-17</a>). Thanks to skip connections, the signal can easily make its way across the whole network. The deep residual network can be seen as a stack<a data-type="indexterm" data-primary="residual units" id="id2841"/> of <em>residual units</em> (RUs), where each residual unit is a small neural network with a skip connection.</p>

<p>Now let’s look at ResNet’s architecture (see <a data-type="xref" href="#resnet_diagram">Figure 12-18</a>). It is surprisingly simple. It starts and ends exactly like GoogLeNet (except without a dropout layer), and in between is just a very deep stack of residual units. Each residual unit is composed of two convolutional layers (and no pooling layer!), with batch normalization (BN) and ReLU activation<a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="ResNet" id="id2842"/>, using 3 × 3 kernels and preserving spatial dimensions (stride 1, <code translate="no">"same"</code> padding).</p>

<figure class="width-70"><div id="deep_residual_network_diagram" class="figure">
<img src="assets/hmls_1217.png" alt="Diagram comparing a regular deep neural network with a deep residual network, highlighting how skip connections in residual units help bypass layers that block backpropagation and aid in learning." width="1033" height="924"/>
<h6><span class="label">Figure 12-17. </span>Regular deep neural network (left) and deep residual network (right)</h6>
</div></figure>

<figure class="width-80"><div id="resnet_diagram" class="figure">
<img src="assets/hmls_1218.png" alt="Diagram of ResNet architecture illustrating the deep stack of residual units, each composed of two convolutional layers with batch normalization and ReLU activation, demonstrating the flow and connections between layers." width="1290" height="1022"/>
<h6><span class="label">Figure 12-18. </span>ResNet architecture</h6>
</div></figure>

<p>Note that the number of feature maps is doubled every few residual units, at the same time as their height and width are halved (using a convolutional layer with stride 2). When this happens, the inputs cannot be added directly to the outputs of the residual unit because they don’t have the same shape (for example, this problem affects the skip connection represented by the dashed arrow in <a data-type="xref" href="#resnet_diagram">Figure 12-18</a>). To solve this problem, the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right number of output feature maps (see <a data-type="xref" href="#resize_skip_connection_diagram">Figure 12-19</a>).</p>

<figure class="smallersixtyfive"><div id="resize_skip_connection_diagram" class="figure">
<img src="assets/hmls_1219.png" alt="Diagram illustrating a skip connection in a residual network, showing how inputs are adjusted via a 1x1 convolution when changing feature map size and depth." width="954" height="502"/>
<h6><span class="label">Figure 12-19. </span>Skip connection when changing feature map size and depth</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>During training, for each mini-batch, you can skip a random set of residual units. This <a href="https://homl.info/sdepth"><em>stochastic depth</em> technique</a>⁠<sup><a data-type="noteref" id="id2843-marker" href="ch12.html#id2843">14</a></sup> speeds up training<a data-type="indexterm" data-primary="stochastic depth technique" id="id2844"/> considerably without compromising accuracy. You can implement it using the <code>torchvision.ops.​sto⁠chastic_depth()</code> function<a data-type="indexterm" data-primary="torchvision" data-secondary="ops.stochastic_depth()" id="id2845"/>.</p>
</div>

<p>Different variations of the architecture exist, with different numbers of layers. ResNet-34<a data-type="indexterm" data-primary="ResNet-34" id="id2846"/> is a ResNet with 34 layers (only counting the convolutional layers and the fully connected layer)⁠<sup><a data-type="noteref" id="id2847-marker" href="ch12.html#id2847">15</a></sup> containing 3 RUs that output 64 feature maps, 4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will implement this architecture later in this chapter.</p>

<p class="pagebreak-before">ResNets deeper than that, such as ResNet-152<a data-type="indexterm" data-primary="ResNet-152" data-primary-sortas="ResNetZ" id="id2848"/>, use slightly different residual units. Instead of two 3 × 3 convolutional layers with, say, 256 feature maps, they use three convolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4 times less), which acts as a bottleneck layer<a data-type="indexterm" data-primary="bottleneck layers" id="id2849"/> (as discussed already), then a 3 × 3 layer with 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature maps (4 times 64) that restores the original depth. ResNet-152 contains 3 such RUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024 maps, and finally 3 RUs with 2,048 maps.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Google’s <a href="https://homl.info/84">Inception-v4 architecture</a>⁠<sup><a data-type="noteref" id="id2850-marker" href="ch12.html#id2850">16</a></sup> merged the ideas of GoogLeNet and ResNet and achieved a top-five error rate of close to 3% on ImageNet classification<a data-type="indexterm" data-startref="xi_residualnetworkResNet12505274_1" id="id2851"/>.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Xception"><div class="sect2" id="id225">
<h2>Xception</h2>

<p>Another variant of the GoogLeNet<a data-type="indexterm" data-primary="GoogLeNet" id="id2852"/> architecture<a data-type="indexterm" data-primary="Xception (Extreme Inception)" id="xi_XceptionExtremeInception1254646_1"/> is worth noting: <a href="https://homl.info/xception">Xception</a>⁠<sup><a data-type="noteref" id="id2853-marker" href="ch12.html#id2853">17</a></sup> (which stands for <em>Extreme Inception</em>) was proposed in 2016 by François Chollet (the author of the deep learning framework Keras), and it significantly outperformed Inception-v3 on a huge vision task (350 million images and 17,000 classes). Just like Inception-v4, it merges the ideas of GoogLeNet and ResNet, but it replaces the inception modules<a data-type="indexterm" data-primary="inception module" id="id2854"/> with a special type of layer<a data-type="indexterm" data-primary="separable convolution layer" id="xi_separableconvolutionlayer12546614_1"/> called<a data-type="indexterm" data-primary="depthwise separable convolution layer" id="id2855"/> a <em>depthwise separable convolution layer</em> (or <em>separable convolution layer</em> for short⁠<sup><a data-type="noteref" id="id2856-marker" href="ch12.html#id2856">18</a></sup>). These layers had been used before in some CNN architectures, but they were not as central as in the Xception architecture. While a regular convolutional layer uses filters<a data-type="indexterm" data-primary="filters, convolutional layers" data-secondary="Xception" id="xi_filtersconvolutionallayersXception125461023_1"/> that try to simultaneously capture spatial patterns (e.g., an oval) and cross-channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer makes the strong assumption that spatial patterns and cross-channel patterns can be modeled separately (see <a data-type="xref" href="#separable_convolution_diagram">Figure 12-20</a>). Thus, it is composed of two parts: the first part applies a single spatial filter to each input feature map, then the second part looks exclusively for cross-channel patterns—it is just a regular convolutional layer with 1 × 1 filters.</p>

<figure><div id="separable_convolution_diagram" class="figure">
<img src="assets/hmls_1220.png" alt="Diagram illustrating a depthwise separable convolutional layer, showing the separation of spatial-only filters applied per input channel and a regular convolutional layer with depthwise-only filters." width="1441" height="911"/>
<h6><span class="label">Figure 12-20. </span>Depthwise separable convolutional layer</h6>
</div></figure>

<p>Since separable convolutional layers only have one spatial filter per input channel, you should avoid using them after layers that have too few channels, such as the input layer (granted, that’s what <a data-type="xref" href="#separable_convolution_diagram">Figure 12-20</a> represents, but it is just for illustration purposes). For this reason, the Xception architecture starts with 2 regular convolutional layers, but then the rest of the architecture uses only separable convolutions (34 in all), plus a few max pooling layers and the usual final layers (a global average pooling layer<a data-type="indexterm" data-primary="global average pooling layer" id="id2857"/><a data-type="indexterm" data-primary="dense layers" id="id2858"/> and a dense output layer).</p>

<p>You might wonder why Xception is considered a variant of GoogLeNet, since it contains no inception modules at all. Well, as discussed earlier, an inception module contains convolutional layers with 1 × 1 filters: these look exclusively for cross-channel patterns. However, the convolutional layers that sit on top of them are regular convolutional layers that look both for spatial and cross-channel patterns. So you can think of an inception module as an intermediate between a regular convolutional layer (which considers spatial patterns and cross-channel patterns jointly) and a separable convolutional layer (which considers them separately). In practice, it seems that separable convolutional layers often perform better.</p>

<p class="pagebreak-before">PyTorch does not include a <code translate="no">SeparableConv2d</code> module<a data-type="indexterm" data-primary="SeparableConv2d module" id="id2859"/>, but it’s fairly straightforward to implement your own:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">SeparableConv2d</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">in_channels</code><code class="p">,</code> <code class="n">out_channels</code><code class="p">,</code> <code class="n">kernel_size</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
                 <code class="n">padding</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">depthwise_conv</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code>
            <code class="n">in_channels</code><code class="p">,</code> <code class="n">in_channels</code><code class="p">,</code> <code class="n">kernel_size</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="n">stride</code><code class="p">,</code>
            <code class="n">padding</code><code class="o">=</code><code class="n">padding</code><code class="p">,</code> <code class="n">groups</code><code class="o">=</code><code class="n">in_channels</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">pointwise_conv</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code>
            <code class="n">in_channels</code><code class="p">,</code> <code class="n">out_channels</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">pointwise_conv</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">depthwise_conv</code><code class="p">(</code><code class="n">inputs</code><code class="p">))</code></pre>

<p>Notice the <code translate="no">groups</code> argument on the seventh line: it lets you split the input channels into the given number of independent groups, each with its own filters (note that <code translate="no">in_channels</code> and <code translate="no">out_channels</code> need to be divisible by <code translate="no">groups</code>). By default <code translate="no">groups=1</code>, giving you a normal convolutional layer, but if you set both <code translate="no">groups=in_channels</code> and <code translate="no">out_channels=in_channels</code>, you get a depthwise convolutional layer, with one filter per input channel. That’s the first layer in the separable convolutional layer. The second is a regular convolutional layer, except we set its kernel size and stride to 1. And that’s it<a data-type="indexterm" data-startref="xi_filtersconvolutionallayersXception125461023_1" id="id2860"/>!</p>
<div data-type="tip"><h6>Tip</h6>
<p>Separable convolutional layers use fewer parameters, less memory, and fewer computations than regular convolutional layers, and they often perform better. Consider using them by default, except after layers with few channels (such as the input channel)<a data-type="indexterm" data-startref="xi_separableconvolutionlayer12546614_1" id="id2861"/><a data-type="indexterm" data-startref="xi_XceptionExtremeInception1254646_1" id="id2862"/>.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="SENet"><div class="sect2" id="id226">
<h2>SENet</h2>

<p>The winning architecture<a data-type="indexterm" data-primary="SENet" id="xi_SENet1257925_1"/> in the ILSVRC 2017 challenge was the <a href="https://homl.info/senet">Squeeze-and-Excitation Network (SENet)</a>.⁠<sup><a data-type="noteref" id="id2863-marker" href="ch12.html#id2863">19</a></sup> This architecture extends existing architectures such as inception networks and ResNets, and boosts their performance. This allowed SENet to win the competition with an astonishing 2.25% top-five error rate! The extended versions of inception networks and ResNets are called <em>SE-Inception</em> and <em>SE-ResNet</em>, respectively. The boost comes from the fact that a SENet adds a small neural network, called an <em>SE block</em>, to every inception module<a data-type="indexterm" data-primary="inception module" id="id2864"/> or residual unit in the original architecture, as shown in <a data-type="xref" href="#senet_diagram">Figure 12-21</a>.</p>

<figure class="smallersixty"><div id="senet_diagram" class="figure">
<img src="assets/hmls_1221.png" alt="Diagram illustrating the integration of SE blocks into Inception modules and Residual units to enhance neural network performance." width="909" height="632"/>
<h6><span class="label">Figure 12-21. </span>SE-Inception module (left) and SE-ResNet unit (right)</h6>
</div></figure>

<p>An SE block analyzes the output of the unit it is attached to, focusing exclusively on the depth dimension (it does not look for any spatial pattern), and it learns which features are usually most active together. It then uses this information to recalibrate the feature maps, as shown in <a data-type="xref" href="#recalibration_diagram">Figure 12-22</a>. For example, an SE block may learn that mouths, noses, and eyes usually appear together in pictures: if you see a mouth and a nose, you should expect to see eyes as well. So, if the block sees a strong activation<a data-type="indexterm" data-primary="activation functions" data-secondary="SENet" id="xi_activationfunctionsSENet12586529_1"/> in the mouth and nose feature maps, but only mild activation in the eye feature map, it will boost the eye feature map (more accurately, it will reduce irrelevant feature maps). If the eyes were somewhat confused with something else, this feature map recalibration will help resolve the ambiguity.</p>

<figure><div id="recalibration_diagram" class="figure">
<img src="assets/hmls_1222.png" alt="Diagram illustrating how an SE block recalibrates feature maps by analyzing and adjusting their activation levels for improved representation." width="1014" height="481"/>
<h6><span class="label">Figure 12-22. </span>An SE block performs feature map recalibration</h6>
</div></figure>

<p>An SE block is composed of just three layers: a global average pooling layer, a hidden dense layer using the ReLU activation function, and a dense output layer using the sigmoid activation function (see <a data-type="xref" href="#seblock_diagram">Figure 12-23</a>).</p>

<figure class="width-35"><div id="seblock_diagram" class="figure">
<img src="assets/hmls_1223.png" alt="Diagram of an SE block architecture showing a global average pool layer followed by two dense layers with ReLU and sigmoid activation functions." width="529" height="451"/>
<h6><span class="label">Figure 12-23. </span>SE block architecture</h6>
</div></figure>

<p>As earlier, the global average pooling layer<a data-type="indexterm" data-primary="global average pooling layer" id="id2865"/> computes the mean activation for each feature map: for example, if its input contains 256 feature maps, it will output 256 <span class="keep-together">numbers</span> representing the overall level of response for each filter<a data-type="indexterm" data-primary="filters, convolutional layers" data-secondary="SENet" id="id2866"/>. The next layer is where the “squeeze” happens: this layer has significantly fewer than 256 neurons—typically 16 times fewer than the number of feature maps (e.g., 16 neurons)—so the 256 numbers get compressed into a small vector (e.g., 16 dimensions). This is a low-dimensional vector representation (i.e., an embedding) of the distribution of feature responses. This bottleneck step forces the SE block to learn a general representation of the feature combinations (we will see this principle in action again when we discuss autoencoders in <a data-type="xref" href="ch18.html#autoencoders_chapter">Chapter 18</a>). Finally, the output layer takes the embedding and outputs a recalibration vector containing one number per feature map (e.g., 256), each between 0 and 1. The feature maps are then multiplied by this recalibration vector, so irrelevant features (with a low recalibration score) get scaled down while relevant features (with a recalibration score close to 1) are left alone<a data-type="indexterm" data-startref="xi_SENet1257925_1" id="id2867"/><a data-type="indexterm" data-startref="xi_activationfunctionsSENet12586529_1" id="id2868"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Other Noteworthy Architectures"><div class="sect2" id="id227">
<h2>Other Noteworthy Architectures</h2>

<p>There are many other CNN architectures to explore. Here’s a brief overview of some of the most noteworthy:</p>
<dl>
<dt><a href="https://homl.info/vggnet">VGGNet</a>⁠<sup><a data-type="noteref" id="id2869-marker" href="ch12.html#id2869">20</a></sup></dt>
<dd>
<p>VGGNet<a data-type="indexterm" data-primary="VGGNet" id="id2870"/><a data-type="indexterm" data-primary="Visual Geometry Group (VGG)" id="id2871"/> was the runner-up in the ILSVRC 2014 challenge. Karen Simonyan and Andrew Zisserman, from the Visual Geometry Group (VGG) research lab at Oxford University, developed a very simple and classical architecture; it had 2 or 3 convolutional layers and a pooling layer, then again 2 or 3 convolutional layers and a pooling layer, and so on (reaching a total of 16 or 19 convolutional layers, depending on the VGG variant), plus a final dense network with 2 hidden layers and the output layer. It used small 3 × 3 filters, but it had many of them.</p>
</dd>
<dt><a href="https://homl.info/resnext">ResNeXt</a>⁠<sup><a data-type="noteref" id="id2872-marker" href="ch12.html#id2872">21</a></sup></dt>
<dd>
<p>ResNeXt<a data-type="indexterm" data-primary="ResNeXt" id="id2873"/> improves the residual units in ResNet. Whereas the residual units in the best ResNet models just contain 3 convolutional layers each, the ResNeXt residual units are composed of many parallel stacks (e.g., 32 stacks), with 3 convolutional layers each. However, the first two layers in each stack only use a few filters (e.g., just four), so the overall number of parameters remains the same as in ResNet. Then the outputs of all the stacks are added together, and the result is passed to the next residual unit (along with the skip connection).</p>
</dd>
<dt><a href="https://homl.info/densenet">DenseNet</a>⁠<sup><a data-type="noteref" id="id2874-marker" href="ch12.html#id2874">22</a></sup></dt>
<dd>
<p>A DenseNet<a data-type="indexterm" data-primary="DenseNet" id="id2875"/> is composed of several dense blocks, each made up of a few densely connected convolutional layers. This architecture achieved excellent accuracy while using comparatively few parameters. What does “densely connected” mean? The output of each layer is fed as input to every layer after it within the same block. For example, layer four in a block takes as input the depthwise concatenation of the outputs of layers one, two, and three in that block. Dense blocks are separated by a few transition layers.</p>
</dd>
<dt><a href="https://homl.info/mobilenet">MobileNet</a>⁠<sup><a data-type="noteref" id="id2876-marker" href="ch12.html#id2876">23</a></sup></dt>
<dd>
<p>MobileNets<a data-type="indexterm" data-primary="MobileNets" id="id2877"/> are streamlined models designed to be lightweight and fast, making them popular in mobile and web applications. They are based on depthwise separable convolutional layers<a data-type="indexterm" data-primary="depthwise separable convolution layer" id="id2878"/>, like Xception. The authors proposed several variants, trading a bit of accuracy for faster and smaller models. Several other CNN architectures are available for mobile devices, such as SqueezeNet, ShuffleNet, or MNasNet.</p>
</dd>
<dt><a href="https://homl.info/cspnet">CSPNet</a>⁠<sup><a data-type="noteref" id="id2879-marker" href="ch12.html#id2879">24</a></sup></dt>
<dd>
<p>A Cross Stage Partial Network (CSPNet)<a data-type="indexterm" data-primary="Cross Stage Partial Network (CSPNet)" id="id2880"/> is similar to a DenseNet, but part of each dense block’s input is concatenated directly to that block’s output, without going through the block.</p>
</dd>
<dt><a href="https://homl.info/efficientnet">EfficientNet</a>⁠<sup><a data-type="noteref" id="id2881-marker" href="ch12.html#id2881">25</a></sup></dt>
<dd>
<p>EfficientNet<a data-type="indexterm" data-primary="EfficientNet" id="id2882"/> is arguably the most important model in this list. The authors proposed a method to scale any CNN efficiently by jointly increasing the depth (number of layers), width (number of filters per layer), and resolution (size of the input image) in a principled way. This is called <em>compound scaling</em>. They used neural architecture search to find a good architecture for a scaled-down version of ImageNet (with smaller and fewer images), and then used compound scaling<a data-type="indexterm" data-primary="compound scaling" id="id2883"/> to create larger and larger versions of this architecture. When EfficientNet models came out, they vastly outperformed all existing models, across all compute budgets, and they remain among the best models out there today. The authors published a follow-up paper in 2021, introducing EfficientNetV2, which improved training time and parameter efficiency even further.</p>
</dd>
<dt><a href="https://homl.info/convnext">ConvNeXt</a>⁠<sup><a data-type="noteref" id="id2884-marker" href="ch12.html#id2884">26</a></sup></dt>
<dd>
<p>ConvNeXt<a data-type="indexterm" data-primary="ConvNeXt" id="id2885"/> is quite similar to ResNet, but with a number of tweaks inspired from the most successful vision transformer architectures (see <a data-type="xref" href="ch16.html#vit_chapter">Chapter 16</a>), such as using large kernels (e.g., 7 × 7 instead of 3 × 3), using fewer activation functions and normalization layers in each residual unit, and more.</p>
</dd>
</dl>

<p>Understanding EfficientNet’s compound scaling method is helpful to gain a deeper understanding of CNNs, especially if you ever need to scale a CNN architecture. It is based on a logarithmic measure of the compute budget, denoted <em>ϕ</em>: if your compute budget doubles, then <em>ϕ</em> increases by 1. In other words, the number of floating-point operations available for training is proportional to 2<sup><em>ϕ</em></sup>. Your CNN architecture’s depth, width, and resolution should scale as <em>α</em><sup><em>ϕ</em></sup>, <em>β</em><sup><em>ϕ</em></sup>, and <em>γ</em><sup><em>ϕ</em></sup>, respectively. The factors <em>α</em>, <em>β</em>, and <em>γ</em> must be greater than 1, and <em>αβ</em><sup>2</sup><em>γ</em><sup>2</sup> should be close to 2. The optimal values for these factors depend on the CNN’s architecture. To find the optimal values for the EfficientNet architecture, the authors started with a small baseline model (EfficientNetB0), fixed <em>ϕ</em> = 1, and simply ran a grid search: they found α = 1.2, β = 1.1, and γ = 1.1. They then used these factors to create several larger architectures, named EfficientNetB1 to EfficientNetB7, for increasing values of <em>ϕ</em>.</p>

<p>I hope you enjoyed this deep dive into the main CNN architectures! But how do you choose the right one?</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Choosing the Right CNN Architecture"><div class="sect2" id="id228">
<h2>Choosing the Right CNN Architecture</h2>

<p>As you might expect, the best architecture depends on what matters most for your project: Accuracy? Model size (e.g., for deployment to a mobile device)? Inference speed? Energy consumption? <a data-type="xref" href="#model_summary_table">Table 12-3</a> lists some of the pretrained classification models currently available in TorchVision (you’ll see how to use them later in this chapter). You can find the full list at <a href="https://pytorch.org/vision/stable/models" class="bare"><em class="hyperlink">https://pytorch.org/vision/stable/models</em></a> (including models for other computer vision tasks). The table shows each model’s top-1 and top-5 accuracy on the ImageNet dataset, its number of parameters (in millions), and how much compute it requires for each image (measured in GFLOPs: a Giga-FLOP is one billion floating-point operations). As you can see, larger models are generally more accurate, but not always; for example, the small variant of EfficientNet v2 outperforms Inception v3 both in size and accuracy (but not in compute)<a data-type="indexterm" data-primary="TorchVision" id="id2886"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="TorchVision pretrained models" id="id2887"/>.</p>
<table id="model_summary_table">
<caption><span class="label">Table 12-3. </span>Some of the pretrained models available in TorchVision, sorted by size</caption>
<thead>
<tr>
<th>Class name</th>
<th>Top-1 acc</th>
<th>Top-5 acc</th>
<th>Params</th>
<th>GFLOPs</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>MobileNet v3 small</p></td>
<td><p>67.7%</p></td>
<td><p>87.4%</p></td>
<td><p>2.5M</p></td>
<td><p>0.1</p></td>
</tr>
<tr>
<td><p>EfficientNet B0</p></td>
<td><p>77.7%</p></td>
<td><p>93.5%</p></td>
<td><p>5.3M</p></td>
<td><p>0.4</p></td>
</tr>
<tr>
<td><p>GoogLeNet</p></td>
<td><p>69.8%</p></td>
<td><p>89.5%</p></td>
<td><p>6.6M</p></td>
<td><p>1.5</p></td>
</tr>
<tr>
<td><p>DenseNet 121</p></td>
<td><p>74.4%</p></td>
<td><p>92.0%</p></td>
<td><p>8.0M</p></td>
<td><p>2.8</p></td>
</tr>
<tr>
<td><p>EfficientNet v2 small</p></td>
<td><p>84.2%</p></td>
<td><p>96.9%</p></td>
<td><p>21.5M</p></td>
<td><p>8.4</p></td>
</tr>
<tr>
<td><p>ResNet 34</p></td>
<td><p>73.3%</p></td>
<td><p>91.4%</p></td>
<td><p>21.8M</p></td>
<td><p>3.7</p></td>
</tr>
<tr>
<td><p>Inception V3</p></td>
<td><p>77.3%</p></td>
<td><p>93.5%</p></td>
<td><p>27.2M</p></td>
<td><p>5.7</p></td>
</tr>
<tr>
<td><p>ConvNeXt Tiny</p></td>
<td><p>82.6%</p></td>
<td><p>96.1%</p></td>
<td><p>28.6M</p></td>
<td><p>4.5</p></td>
</tr>
<tr>
<td><p>DenseNet 161</p></td>
<td><p>77.1%</p></td>
<td><p>93.6%</p></td>
<td><p>28.7M</p></td>
<td><p>7.7</p></td>
</tr>
<tr>
<td><p>ResNet 152</p></td>
<td><p>82.3%</p></td>
<td><p>96.0%</p></td>
<td><p>60.2M</p></td>
<td><p>11.5</p></td>
</tr>
<tr>
<td><p>AlexNet</p></td>
<td><p>56.5%</p></td>
<td><p>79.1%</p></td>
<td><p>61.1M</p></td>
<td><p>0.7</p></td>
</tr>
<tr>
<td><p>EfficientNet B7</p></td>
<td><p>84.1%</p></td>
<td><p>96.9%</p></td>
<td><p>66.3M</p></td>
<td><p>37.8</p></td>
</tr>
<tr>
<td><p>ResNeXt 101 32x8D</p></td>
<td><p>82.8%</p></td>
<td><p>96.2%</p></td>
<td><p>88.8M</p></td>
<td><p>16.4</p></td>
</tr>
<tr>
<td><p>EfficientNet v2 large</p></td>
<td><p>85.8%</p></td>
<td><p>97.8%</p></td>
<td><p>118.5M</p></td>
<td><p>56.1</p></td>
</tr>
<tr>
<td><p>VGG 11 with BN</p></td>
<td><p>70.4%</p></td>
<td><p>89.8%</p></td>
<td><p>132.9M</p></td>
<td><p>7.6</p></td>
</tr>
<tr>
<td><p>ConvNeXt Large</p></td>
<td><p>84.4%</p></td>
<td><p>97.0%</p></td>
<td><p>197.8M</p></td>
<td><p>34.4</p></td>
</tr>
</tbody>
</table>

<p>The smaller models will run on any GPU, but what about a large model, such as 
<span class="keep-together">ConvNeXt</span> Large? Since each parameter is represented as a 32-bit float (4 bytes), you might think you just need 800 MB of RAM to run a 200M parameter model, but you actually need <em>much</em> more, typically 5 GB per image at inference time (depending on the image size), and even more at training time. Let’s see why.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="GPU RAM Requirements: Inference Versus Training"><div class="sect2" id="id229">
<h2>GPU RAM Requirements: Inference Versus Training</h2>

<p>CNNs need a <em>lot</em> of RAM<a data-type="indexterm" data-primary="memory requirements, convolutional layers" id="id2888"/><a data-type="indexterm" data-primary="graphical processing units (GPUs)" data-secondary="RAM requirements for CNNs" id="xi_graphicalprocessingunitsGPUsRAMrequirementsforCNNs1265225_1"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="GPU RAM requirements" id="xi_convolutionalneuralnetworksCNNsGPURAMrequirements1265225_1"/>. For example, consider a single convolutional layer with 200 5 × 5 filters, stride 1 and <code translate="no">"same"</code> padding, processing a 150 × 100 RGB image (3 channels):</p>

<ul>
<li>
<p>The number of parameters is (5 × 5 × 3 + 1) × 200 = 15,200 (the + 1 corresponds to the bias terms). That’s not much: to produce the same size outputs, a fully connected layer would need 200 × 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs. It would have 200 × 150 × 100 × (150 × 100 × 3 + 1) ≈ 135 billion parameters!</p>
</li>
<li>
<p>However, each of the 200 feature maps contains 150 × 100 neurons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 = 75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully connected layer, but still quite computationally intensive.</p>
</li>
<li>
<p>Importantly, the convolutional layer’s output will occupy 200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM, assuming we’re using 32-bit floats.⁠<sup><a data-type="noteref" id="id2889-marker" href="ch12.html#id2889">27</a></sup> And that’s just for one instance—if a training batch contains 100 instances, then this single convolutional layer will use up 1.2 GB of RAM!</p>
</li>
</ul>

<p>During inference<a data-type="indexterm" data-primary="inference" id="id2890"/> (i.e., when making a prediction for a new instance) the RAM occupied by one layer can be released as soon as the next layer has been computed, so you only need as much RAM as required by two consecutive layers. But during training everything computed during the forward pass needs to be preserved for the backward pass, so the amount of RAM needed is (at least) the total amount of RAM required by all layers. You can easily run out of GPU RAM.</p>

<p>If training crashes because of an out-of-memory error, you can try reducing the batch size. To still get some of the benefits of large batches, you can accumulate the gradients after each batch, and only update the model weights every few batches. Alternatively, you can try reducing dimensionality using a stride, removing a few layers, using 16-bit floats instead of 32-bit floats, distributing the CNN across multiple devices, or offloading the most memory-hungry modules to the CPU (using <code translate="no">module.to("cpu")</code>).</p>

<p>Yet another option is to trade more compute in exchange for a lower memory usage. For example, instead of saving all of the activations during the forward pass, you can save some of them, called <em>activation checkpoints</em>, then during the backward pass<a data-type="indexterm" data-primary="checkpoints" id="id2891"/><a data-type="indexterm" data-primary="activation checkpoints" id="id2892"/><a data-type="indexterm" data-primary="gradient checkpoints" id="id2893"/>, you can recompute the missing activations as needed by running a partial forward pass starting from the previous checkpoint.</p>

<p>To implement activation checkpointing (also called <em>gradient checkpointing</em>) in PyTorch, you can use the <code translate="no">torch.utils.checkpoint.checkpoint()</code> function<a data-type="indexterm" data-primary="torch" data-secondary="utils.checkpoint.checkpoint()" id="id2894"/>: instead of calling a module <code translate="no">z = foo(x)</code>, you can call it using <code translate="no">z = checkpoint(foo, x)</code>. During inference, it will make no difference, but during training this module’s activations will no longer be saved during the forward pass, and <code translate="no">foo(x)</code> will be recomputed during the backward pass when needed. This approach is fairly simple to implement, and it doesn’t require any changes to your model architecture.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The forward pass needs to produce the same result if you call it twice with the same inputs, or else the gradients will be incorrect. This means that custom modules must respect a few constraints, such as avoiding in-place ops or using controlled states for random number generation: please see the <code translate="no">checkpoint()</code> function’s documentation for more details.</p>
</div>

<p>That said, if you’re OK with tweaking your model architecture, then there’s a much more efficient solution you can use to exchange compute for memory: reversible residual networks<a data-type="indexterm" data-startref="xi_graphicalprocessingunitsGPUsRAMrequirementsforCNNs1265225_1" id="id2895"/><a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsGPURAMrequirements1265225_1" id="id2896"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Reversible Residual Networks (RevNets)"><div class="sect2" id="id230">
<h2>Reversible Residual Networks (RevNets)</h2>

<p><a href="https://homl.info/revnet">RevNets</a> were proposed by Aidan Gomez et al. in 2017:⁠<sup><a data-type="noteref" id="id2897-marker" href="ch12.html#id2897">28</a></sup> they typically only increase compute<a data-type="indexterm" data-primary="Reversible Residual Networks (RevNets)" id="id2898"/> by about 33% and actually don’t require you to save any activations<a data-type="indexterm" data-primary="activations, RevNets" id="id2899"/> at all during the forward pass! Here’s how they work:</p>

<ul>
<li>
<p>Each layer, called<a data-type="indexterm" data-primary="reversible layer" id="id2900"/> a <em>reversible layer</em>, takes two inputs of equal sizes, <strong>x</strong><sub>1</sub> and <strong>x</strong><sub>2</sub>, and computes two outputs: <strong>y</strong><sub>1</sub> = <strong>x</strong><sub>1</sub> + f(<strong>x</strong><sub>2</sub>) and <strong>y</strong><sub>2</sub> = g(<strong>y</strong><sub>1</sub>) + <strong>x</strong><sub>2</sub>, where f and g can be any functions, as long as the output size equals the input size, and as long as they always produce the same output for a given input. For example, f and g can be identical modules composed of a few convolutional layers with stride 1 and <code translate="no">"same"</code> padding (each convolutional layer comes with its own batch-norm and ReLU activation).</p>
</li>
<li>
<p>During backpropagation, the inputs of each reversible layer can be recomputed from the outputs whenever needed, using: <strong>x</strong><sub>2</sub> = <strong>y</strong><sub>2</sub> – g(<strong>y</strong><sub>1</sub>) and <strong>x</strong><sub>1</sub> = <strong>y</strong><sub>1</sub> – f(<strong>x</strong><sub>2</sub>) (you can easily verify that these two equalities follow directly from the first two). No need to store any activations during the forward pass: brilliant!</p>
</li>
</ul>

<p>Since f and g must output the same shape as the input, reversible layers cannot contain convolutional layers with a stride greater than 1, or with <code translate="no">"valid"</code> padding. You can still use such layers in your CNN, but the RevNet trick won’t be applicable to them, so you will have to save their activations during the forward pass; luckily, a CNN usually requires only a handful of such layers. This includes the very first layer, which reduces the spatial dimensions and increases the number of channels: the result can be split in two equal parts along the channel dimension and fed to the first reversible layer.</p>

<p>RevNets aren’t limited to CNNs. In fact, they are at the heart of an influential Transformer architecture named Reformer (see <a data-type="xref" href="ch17.html#speedup_chapter">Chapter 17</a>).</p>

<p>OK, it’s now time to get our hands dirty! Let’s implement one of the most popular CNN architectures from scratch using PyTorch<a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsarchitectures1235126_1" id="id2901"/>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Implementing a ResNet-34 CNN Using PyTorch"><div class="sect1" id="id231">
<h1>Implementing a ResNet-34 CNN Using PyTorch</h1>

<p>Most CNN architectures described so far can be implemented pretty naturally using PyTorch (although generally you would load a pretrained network instead, as you will see). To illustrate the process, let’s implement a ResNet-34<a data-type="indexterm" data-primary="ResNet-34" id="xi_ResNet3412679228_1"/><a data-type="indexterm" data-primary="PyTorch" data-secondary="ResNet-34 CNN with" id="xi_PyTorchResNet34CNNwith12679228_1"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="ResNet-34 with PyTorch" id="xi_convolutionalneuralnetworksCNNsResNet34withPyTorch12679228_1"/> from scratch with PyTorch. First, we’ll create a <code translate="no">ResidualUnit</code> layer:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ResidualUnit</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">in_channels</code><code class="p">,</code> <code class="n">out_channels</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="n">DefaultConv2d</code> <code class="o">=</code> <code class="n">partial</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">bias</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">main_layers</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">DefaultConv2d</code><code class="p">(</code><code class="n">in_channels</code><code class="p">,</code> <code class="n">out_channels</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="n">stride</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm2d</code><code class="p">(</code><code class="n">out_channels</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">DefaultConv2d</code><code class="p">(</code><code class="n">out_channels</code><code class="p">,</code> <code class="n">out_channels</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm2d</code><code class="p">(</code><code class="n">out_channels</code><code class="p">),</code>
        <code class="p">)</code>
        <code class="k">if</code> <code class="n">stride</code> <code class="o">&gt;</code> <code class="mi">1</code><code class="p">:</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">skip_connection</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
                <code class="n">DefaultConv2d</code><code class="p">(</code><code class="n">in_channels</code><code class="p">,</code> <code class="n">out_channels</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
                              <code class="n">stride</code><code class="o">=</code><code class="n">stride</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">0</code><code class="p">),</code>
                <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm2d</code><code class="p">(</code><code class="n">out_channels</code><code class="p">),</code>
            <code class="p">)</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">skip_connection</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Identity</code><code class="p">()</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">F</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">main_layers</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">skip_connection</code><code class="p">(</code><code class="n">inputs</code><code class="p">))</code></pre>

<p>As you can see, this code matches <a data-type="xref" href="#resize_skip_connection_diagram">Figure 12-19</a> pretty closely. In the constructor, we create all the layers we need: the main layers are the ones on the righthand side of the figure, and the skip connection corresponds to the layers on the left when the stride is greater than 1, or an <code translate="no">nn.Identity</code> module when the stride is 1—the <code translate="no">nn.Identity</code> module does nothing at all, it just returns its inputs. Then in the <code translate="no">forward()</code> method, we make the inputs go through both the main layers and the skip connection, then we add both outputs and apply the activation function.</p>

<p>Next, let’s build our <code translate="no">ResNet34</code> module! Now that we have our <code translate="no">ResidualUnit</code> module, the whole ResNet-34 architecture becomes one big stack of modules, so we can base our <code translate="no">ResNet34</code> class on a single <code translate="no">nn.Sequential</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.Sequential" id="id2902"/>. The code closely matches <a data-type="xref" href="#resnet_diagram">Figure 12-18</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="less_space pagebreak-before"><code class="k">class</code> <code class="nc">ResNet34</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="n">layers</code> <code class="o">=</code> <code class="p">[</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="n">in_channels</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">out_channels</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">7</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
                      <code class="n">padding</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">bias</code><code class="o">=</code><code class="kc">False</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm2d</code><code class="p">(</code><code class="n">num_features</code><code class="o">=</code><code class="mi">64</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code>
        <code class="p">]</code>
        <code class="n">prev_filters</code> <code class="o">=</code> <code class="mi">64</code>
        <code class="k">for</code> <code class="n">filters</code> <code class="ow">in</code> <code class="p">[</code><code class="mi">64</code><code class="p">]</code> <code class="o">*</code> <code class="mi">3</code> <code class="o">+</code> <code class="p">[</code><code class="mi">128</code><code class="p">]</code> <code class="o">*</code> <code class="mi">4</code> <code class="o">+</code> <code class="p">[</code><code class="mi">256</code><code class="p">]</code> <code class="o">*</code> <code class="mi">6</code> <code class="o">+</code> <code class="p">[</code><code class="mi">512</code><code class="p">]</code> <code class="o">*</code> <code class="mi">3</code><code class="p">:</code>
            <code class="n">stride</code> <code class="o">=</code> <code class="mi">1</code> <code class="k">if</code> <code class="n">filters</code> <code class="o">==</code> <code class="n">prev_filters</code> <code class="k">else</code> <code class="mi">2</code>
            <code class="n">layers</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">ResidualUnit</code><code class="p">(</code><code class="n">prev_filters</code><code class="p">,</code> <code class="n">filters</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="n">stride</code><code class="p">))</code>
            <code class="n">prev_filters</code> <code class="o">=</code> <code class="n">filters</code>
        <code class="n">layers</code> <code class="o">+=</code> <code class="p">[</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">AdaptiveAvgPool2d</code><code class="p">(</code><code class="n">output_size</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">LazyLinear</code><code class="p">(</code><code class="mi">10</code><code class="p">),</code>
        <code class="p">]</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">resnet</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="o">*</code><code class="n">layers</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">resnet</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code></pre>

<p>The only tricky part in this code is the loop that adds the <code translate="no">ResidualUnit</code> layers to the list of layers: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs have 128 filters<a data-type="indexterm" data-primary="filters, convolutional layers" data-secondary="ResNet34" id="id2903"/>, and so on. At each iteration, we must set the stride to 1 when the number of filters is the same as in the previous RU, or else we set it to 2; then we append the <code translate="no">ResidualUnit</code> to the list, and finally we update <code translate="no">prev_filters</code>.</p>

<p>And that’s it, you could now train this model on ImageNet or any other dataset of 224 × 224 images. It is amazing that in just 45 lines of code, we can build the model that won the ILSVRC 2015 challenge! This demonstrates both the elegance of the ResNet model and the expressiveness of PyTorch (and Python). Implementing the other CNN architectures we discussed would take more time, but it wouldn’t be much harder. However, TorchVision comes with several of these architectures built in, so why not use them instead<a data-type="indexterm" data-startref="xi_ResNet3412679228_1" id="id2904"/><a data-type="indexterm" data-startref="xi_PyTorchResNet34CNNwith12679228_1" id="id2905"/><a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsResNet34withPyTorch12679228_1" id="id2906"/>?</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Using TorchVision’s Pretrained Models"><div class="sect1" id="id232">
<h1>Using TorchVision’s Pretrained Models</h1>

<p>In general, you won’t have to implement standard models<a data-type="indexterm" data-primary="PyTorch" data-secondary="pretrained CNN models" id="xi_PyTorchpretrainedCNNmodels1274556_1"/><a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="CNNs (TorchVision)" id="xi_pretrainingandpretrainedlayersCNNsTorchVision1274556_1"/><a data-type="indexterm" data-primary="TorchVision" id="xi_TorchVisionpretrainedmodelsforCNNs1274556_1"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="TorchVision pretrained models" id="xi_convolutionalneuralnetworksCNNsTorchVisionpretrainedmodels1274556_1"/> like GoogLeNet, ResNet, or ConvNeXt manually, since pretrained networks are readily available with a couple lines of code using TorchVision.</p>
<div data-type="tip"><h6>Tip</h6>
<p>TIMM<a data-type="indexterm" data-primary="TIMM" id="id2907"/> is another very popular library built on PyTorch: it provides a collection of pretrained image classification models, as well as many related tools such as data loaders, data augmentation utilities, optimizers, schedulers, and more. Hugging Face’s Hub is also a great place to get all sorts of pretrained models (see <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>).</p>
</div>

<p>For example, you can load a ConvNeXt<a data-type="indexterm" data-primary="ConvNeXt" id="xi_ConvNeXt1274937_1"/> model pretrained on ImageNet with the following code. There are several variants of the ConvNeXt model—tiny, small, base, and large—and this code loads the base variant:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">weights</code> <code class="o">=</code> <code class="n">torchvision</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">ConvNeXt_Base_Weights</code><code class="o">.</code><code class="n">IMAGENET1K_V1</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">torchvision</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">convnext_base</code><code class="p">(</code><code class="n">weights</code><code class="o">=</code><code class="n">weights</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>That’s all! This code automatically downloads the weights<a data-type="indexterm" data-primary="weights" data-secondary="downloading from Torch Hub" id="id2908"/> (338 MB) from the <em>Torch Hub</em>, an online repository<a data-type="indexterm" data-primary="Torch Hub" id="id2909"/> of pretrained models. The weights are saved and cached for future use (e.g., in <code translate="no">~/.cache/torch/hub</code>; run <code translate="no">torch.hub.get_dir()</code> to find the exact path on your system). Some models have newer weights versions (e.g., <code translate="no">IMAGENET1K_V2</code>) or other weight variants. For the full list of available models<a data-type="indexterm" data-primary="torchvision" data-secondary="models.list_models()" id="id2910"/>, run <code translate="no">torchvision.models.list_models()</code>. To find the list of pretrained weights available for a given model, such as <code translate="no">convnext_base</code>, run <code translate="no">list(torchvision.models.get_model_weights("convnext_base"))</code>. Alternatively, visit <a href="https://pytorch.org/vision/main/models" class="bare"><em class="hyperlink">https://pytorch.org/vision/main/models</em></a>.</p>

<p>Let’s use this model to classify the two sample images we loaded earlier. Before we can do this, we must first ensure that the images are preprocessed exactly as the model expects. In particular, they must have the right size. A ConvNeXt model expects 224 × 224 pixel images (other models may expect other sizes, such as 299 × 299). Since our sample images are 427 × 640 pixels, we need to resize them. We could do this using TorchVision’s <code translate="no">CenterCrop</code> and/or <code translate="no">Resize</code> transform, but it’s much easier and safer to use the transforms<a data-type="indexterm" data-primary="Transformers library" data-secondary="weights.transforms()" id="id2911"/><a data-type="indexterm" data-primary="torchvision" data-secondary="weights.transforms()" id="id2912"/> returned by <code translate="no">weights.transforms()</code>, as they are specifically designed for this particular pretrained model:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">transforms</code> <code class="o">=</code> <code class="n">weights</code><code class="o">.</code><code class="n">transforms</code><code class="p">()</code>
<code class="n">preprocessed_images</code> <code class="o">=</code> <code class="n">transforms</code><code class="p">(</code><code class="n">sample_images_permuted</code><code class="p">)</code></pre>

<p>Importantly, these transforms also normalize the pixel intensities just like during training. In this case, the transforms standardize the pixel intensities separately for each color channel, using ImageNet’s means and standard deviations for each channel (we will see how to do this manually later in this chapter).</p>

<p>Next we can move the images to the GPU and pass them to the model. As always, remember to switch the model to evaluation mode before making predictions—the model is in training mode by default—and also turn off autograd:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">y_logits</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">preprocessed_images</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">))</code></pre>

<p>The result is a 2 × 1,000 tensor containing the class logits for each image (recall that ImageNet has 1,000 classes). As we did in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>, we can<a data-type="indexterm" data-primary="torch" data-secondary="argmax()" id="id2913"/> use <code translate="no">torch.argmax()</code> to get the predicted class for each image (i.e., the class with the maximum logit):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">y_logits</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code>
<code class="go">tensor([698, 985], device='cuda:0')</code></pre>

<p>So far, so good, but what exactly do these classes represent? Well you could find the ImageNet class names online, but once again it’s simpler and safer to get the class names directly from the <code translate="no">weights</code> object. Indeed, its <code translate="no">meta</code> attribute is a dictionary containing metadata about the pretrained model, including the class names:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">class_names</code> <code class="o">=</code> <code class="n">weights</code><code class="o">.</code><code class="n">meta</code><code class="p">[</code><code class="s2">"categories"</code><code class="p">]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="p">[</code><code class="n">class_names</code><code class="p">[</code><code class="n">class_id</code><code class="p">]</code> <code class="k">for</code> <code class="n">class_id</code> <code class="ow">in</code> <code class="n">y_pred</code><code class="p">]</code>
<code class="go">['palace', 'daisy']</code></pre>

<p>There you have it: the first image is classified as a palace, and the second as a daisy. Since the ImageNet dataset does not have classes for Chinese towers or dahlia flowers, a palace and a daisy are reasonable substitutes (the tower is part of the Summer Palace in Beijing). Let’s look at the top-three predictions using <code translate="no">topk()</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_top3_logits</code><code class="p">,</code> <code class="n">y_top3_class_ids</code> <code class="o">=</code> <code class="n">y_logits</code><code class="o">.</code><code class="n">topk</code><code class="p">(</code><code class="n">k</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="p">[[</code><code class="n">class_names</code><code class="p">[</code><code class="n">class_id</code><code class="p">]</code> <code class="k">for</code> <code class="n">class_id</code> <code class="ow">in</code> <code class="n">top3</code><code class="p">]</code> <code class="k">for</code> <code class="n">top3</code> <code class="ow">in</code> <code class="n">y_top3_class_ids</code><code class="p">]</code>
<code class="go">[['palace', 'monastery', 'lakeside'], ['daisy', 'pot', 'ant']]</code></pre>

<p>Let’s look at the estimated probabilities for each of these classes:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_top3_logits</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="go">tensor([[0.8618, 0.1185, 0.0197],</code>
<code class="go">        [0.8106, 0.0964, 0.0930]], device='cuda:0')</code></pre>

<p>As you can see, TorchVision makes it easy to download and use pretrained models, and it works quite well out of the box for ImageNet classes<a data-type="indexterm" data-startref="xi_ConvNeXt1274937_1" id="id2914"/>. But what if you need to classify images into classes that don’t belong to the ImageNet dataset, such as various flower species? In that case, you may still benefit from the pretrained models by using them to perform transfer learning<a data-type="indexterm" data-startref="xi_PyTorchpretrainedCNNmodels1274556_1" id="id2915"/><a data-type="indexterm" data-startref="xi_pretrainingandpretrainedlayersCNNsTorchVision1274556_1" id="id2916"/><a data-type="indexterm" data-startref="xi_TorchVisionpretrainedmodelsforCNNs1274556_1" id="id2917"/><a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsTorchVisionpretrainedmodels1274556_1" id="id2918"/>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Pretrained Models for Transfer Learning"><div class="sect1" id="id233">
<h1>Pretrained Models for Transfer Learning</h1>

<p>If you want to build an image classifier<a data-type="indexterm" data-primary="transfer learning" id="xi_transferlearning1281741_1"/><a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="transfer learning models" id="xi_pretrainingandpretrainedlayerstransferlearningmodels1281741_1"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="transfer learning pretrained models" id="xi_convolutionalneuralnetworksCNNstransferlearningpretrainedmodels1281741_1"/> but you do not have enough data to train it from scratch, then it is often a good idea to reuse the lower layers of a pretrained model, as we discussed in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>.  In this section we will reuse the ConvNeXt model we loaded earlier—which was pretrained on ImageNet—and after replacing its classification head<a data-type="indexterm" data-primary="Flowers102 dataset" id="id2919"/>, we will fine-tune it on the <a href="https://homl.info/flowers102"><em>102 Category Flower Dataset</em></a>⁠<sup><a data-type="noteref" id="id2920-marker" href="ch12.html#id2920">29</a></sup> (Flowers102 for short). This dataset only contains 10 images per class, and there are 102 classes in total (as the name indicates), so if you try to train a model from scratch, you will really struggle to get high accuracy. However, it’s quite easy to get over 90% accuracy using a good pretrained model. Let’s see how. First, let’s download the dataset using Torchvision:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">DefaultFlowers102</code> <code class="o">=</code> <code class="n">partial</code><code class="p">(</code><code class="n">torchvision</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">Flowers102</code><code class="p">,</code> <code class="n">root</code><code class="o">=</code><code class="s2">"datasets"</code><code class="p">,</code>
                            <code class="n">transform</code><code class="o">=</code><code class="n">weights</code><code class="o">.</code><code class="n">transforms</code><code class="p">(),</code> <code class="n">download</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">train_set</code> <code class="o">=</code> <code class="n">DefaultFlowers102</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"train"</code><code class="p">)</code>
<code class="n">valid_set</code> <code class="o">=</code> <code class="n">DefaultFlowers102</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"val"</code><code class="p">)</code>
<code class="n">test_set</code> <code class="o">=</code> <code class="n">DefaultFlowers102</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"test"</code><code class="p">)</code></pre>

<p>This code<a data-type="indexterm" data-primary="torch" data-secondary="partial()" id="id2921"/> uses <code translate="no">partial()</code> to avoid repeating the same arguments three times. We also set <code translate="no">transform=weights.transforms()</code> to preprocess the images immediately when they are loaded. The Flowers102 dataset comes with three predefined splits, for training, validation, and testing. The first two have 10 images per class, but surprisingly the test set has many more (it has a variable number of images per class, between 20 and 238). In a real project, you would normally use most of your data for training rather than for testing, but this dataset was designed for computer vision research, and the authors purposely restricted the training set and the validation set.</p>

<p>We then create the data loaders, as usual:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code>

<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">valid_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">valid_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">)</code>
<code class="n">test_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">test_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">)</code></pre>

<p>Many TorchVision datasets conveniently contain the class names in the <code translate="no">classes</code> attribute, but sadly not this dataset.<sup><a data-type="noteref" id="id2922-marker" href="ch12.html#id2922">30</a></sup> If you prefer to see lovely names like “tiger lily”, “monkshood”, or “snapdragon” rather than boring class IDs, then you need to manually define the list of class names:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">class_names</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'pink primrose'</code><code class="p">,</code> <code class="o">...</code><code class="p">,</code> <code class="s1">'trumpet creeper'</code><code class="p">,</code> <code class="s1">'blackberry lily'</code><code class="p">]</code></pre>

<p>Now let’s adapt our pretrained ConvNeXt-base model to this dataset. Since it was pretrained on ImageNet, which has 1,000 classes, the model’s head (i.e., its upper layers) was designed to output 1,000 logits. But we only have 102 classes, so we must chop the model’s head off and replace it with a smaller one. But how can we find it? Well let’s use the model’s <code translate="no">named_children()</code> method<a data-type="indexterm" data-primary="torch" data-secondary="named_children()" id="id2923"/> to find the name of its submodules:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="p">[</code><code class="n">name</code> <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">child</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">named_children</code><code class="p">()]</code>
<code class="go">['features', 'avgpool', 'classifier']</code></pre>

<p>The <code translate="no">features</code> module is the main part of the model, which includes all layers except for the global average pooling layer (<code translate="no">avgpool</code>) and the model’s head (<code translate="no">classifier</code>). Let’s look more closely at the head:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">model</code><code class="o">.</code><code class="n">classifier</code>
<code class="go">Sequential(</code>
<code class="go">  (0): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)</code>
<code class="go">  (1): Flatten(start_dim=1, end_dim=-1)</code>
<code class="go">  (2): Linear(in_features=1024, out_features=1000, bias=True)</code>
<code class="go">)</code></pre>

<p>As you can see, it’s an <code translate="no">nn.Sequential</code> module composed of a layer normalization layer, an <code translate="no">nn.Flatten</code> layer, and an <code translate="no">nn.Linear</code> layer with 1,024 inputs and 1,000 outputs. This <code translate="no">nn.Linear</code> layer is the output layer, and it’s the one we need to replace. We must only change the number of outputs:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">n_classes</code> <code class="o">=</code> <code class="mi">102</code>  <code class="c1"># len(class_names) == 102</code>
<code class="n">model</code><code class="o">.</code><code class="n">classifier</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1024</code><code class="p">,</code> <code class="n">n_classes</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>As explained in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>, it’s usually a good idea to freeze the weights of the pretrained layers, at least at the beginning of training. We can do this by freezing every single parameter in the model, and then unfreezing only the parameters of the head:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">param</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">():</code>
    <code class="n">param</code><code class="o">.</code><code class="n">requires_grad</code> <code class="o">=</code> <code class="kc">False</code>

<code class="k">for</code> <code class="n">param</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">classifier</code><code class="o">.</code><code class="n">parameters</code><code class="p">():</code>
    <code class="n">param</code><code class="o">.</code><code class="n">requires_grad</code> <code class="o">=</code> <code class="kc">True</code></pre>

<p>Next, you can train this model for a few epochs, and you will already reach about 90% accuracy just by training the new head, without even fine-tuning the pretrained layers. After that, you can unfreeze the whole model, lower the learning rate—typically by a factor of 10—and continue training the model. Give this a try, and see what accuracy you can reach!</p>

<p>To reach an even higher accuracy, it’s usually a good idea to perform some data augmentation<a data-type="indexterm" data-primary="data augmentation" id="id2924"/> on the training images. For this, you can try randomly flipping the training images horizontally, randomly rotating them by a small angle, randomly resizing and cropping them, and randomly tweaking their colors. This must all be done before running the ImageNet normalization step, which you can implement using a <code translate="no">Normalize</code> transform<a data-type="indexterm" data-primary="Normalize transform, transfer learning" id="id2925"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torchvision.transforms.v2</code> <code class="k">as</code> <code class="nn">T</code>

<code class="n">transforms</code> <code class="o">=</code> <code class="n">T</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code>
    <code class="n">T</code><code class="o">.</code><code class="n">RandomHorizontalFlip</code><code class="p">(</code><code class="n">p</code><code class="o">=</code><code class="mf">0.5</code><code class="p">),</code>
    <code class="n">T</code><code class="o">.</code><code class="n">RandomRotation</code><code class="p">(</code><code class="n">degrees</code><code class="o">=</code><code class="mi">30</code><code class="p">),</code>
    <code class="n">T</code><code class="o">.</code><code class="n">RandomResizedCrop</code><code class="p">(</code><code class="n">size</code><code class="o">=</code><code class="p">(</code><code class="mi">224</code><code class="p">,</code> <code class="mi">224</code><code class="p">),</code> <code class="n">scale</code><code class="o">=</code><code class="p">(</code><code class="mf">0.8</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">)),</code>
    <code class="n">T</code><code class="o">.</code><code class="n">ColorJitter</code><code class="p">(</code><code class="n">brightness</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">contrast</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">saturation</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">hue</code><code class="o">=</code><code class="mf">0.1</code><code class="p">),</code>
    <code class="n">T</code><code class="o">.</code><code class="n">ToImage</code><code class="p">(),</code>
    <code class="n">T</code><code class="o">.</code><code class="n">ToDtype</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">,</code> <code class="n">scale</code><code class="o">=</code><code class="kc">True</code><code class="p">),</code>
    <code class="n">T</code><code class="o">.</code><code class="n">Normalize</code><code class="p">(</code><code class="n">mean</code><code class="o">=</code><code class="p">[</code><code class="mf">0.485</code><code class="p">,</code> <code class="mf">0.456</code><code class="p">,</code> <code class="mf">0.406</code><code class="p">],</code> <code class="n">std</code><code class="o">=</code><code class="p">[</code><code class="mf">0.229</code><code class="p">,</code> <code class="mf">0.224</code><code class="p">,</code> <code class="mf">0.225</code><code class="p">]),</code>
<code class="p">])</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>TorchVision comes with an <code translate="no">AutoAugment</code> transform<a data-type="indexterm" data-primary="AutoAugment transform, TorchVision" id="id2926"/> which applies multiple augmentation operations optimized for ImageNet. It generalizes well to many other image datasets, and it also offers predefined settings for two other datasets: CIFAR10 and the street view house numbers (SVHN) dataset.</p>
</div>

<p>Here are some more ideas to continue to improve your model’s accuracy:</p>

<ul>
<li>
<p>Try other pretrained models.</p>
</li>
<li>
<p>Extend the training set: find more flower images and label them.</p>
</li>
<li>
<p>Create an ensemble of models, and combine their predictions.</p>
</li>
<li>
<p>Analyze failure cases, and see whether they share specific characteristics, such as similar texture or color. You can then try to tweak image preprocessing to address these issues.</p>
</li>
<li>
<p>Use a learning schedule such as performance scheduling.</p>
</li>
<li>
<p>Unfreeze the layers gradually, starting from the top. Alternatively, you can<a data-type="indexterm" data-primary="differential learning rates" id="id2927"/> use <em>differential learning rates</em>: apply a smaller learning rate to lower layers, and a larger learning rate to upper layers. You can do this by using parameter groups (see <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>).</p>
</li>
<li>
<p>Explore different optimizers and fine-tune their hyperparameters.</p>
</li>
<li>
<p>Try different regularization techniques.</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>It’s worth spending time looking for models that were pretrained on similar images. For example, if you’re dealing with satellite images, aerial images, or even raster data such as digital elevation models (DEM), then models pretrained on ImageNet won’t help much. Instead, check out Microsoft’s <em>TorchGeo</em> library<a data-type="indexterm" data-primary="TorchGeo library from Microsoft" id="id2928"/>, which is similar to TorchVision but for geospatial data. For medical images, check out Project MONAI. For agricultural images, check out AgML. And so on.</p>
</div>

<p>With that, you can start training amazing image classifiers on your own images and classes! But there’s more to computer vision than just classification. For example, what if you also want to know <em>where</em> the flower is in a picture? Let’s look at this now<a data-type="indexterm" data-startref="xi_transferlearning1281741_1" id="id2929"/><a data-type="indexterm" data-startref="xi_pretrainingandpretrainedlayerstransferlearningmodels1281741_1" id="id2930"/><a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNstransferlearningpretrainedmodels1281741_1" id="id2931"/>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Classification and Localization"><div class="sect1" id="id234">
<h1>Classification and Localization</h1>

<p>Localizing an object in a picture can be expressed as a regression task<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="classification and localization" id="xi_convolutionalneuralnetworksCNNsclassificationandlocalization1292472_1"/><a data-type="indexterm" data-primary="classification" data-secondary="CNNs" id="xi_classificationCNNs1292472_1"/><a data-type="indexterm" data-primary="localization, CNNs" id="xi_localizationCNNs1292472_1"/>, as discussed in <a data-type="xref" href="ch09.html#ann_chapter">Chapter 9</a>: to predict a bounding box<a data-type="indexterm" data-primary="bounding boxes, image identification" id="xi_boundingboxesimageidentification12924132_1"/> around the object, a common approach is to predict the location of the bounding box’s center, as well as its width and height (alternatively, you could predict the horizontal and vertical coordinates of the object’s upper-left and lower-right corners). This means we have four numbers to predict. It does not require much change to the ConvNeXt<a data-type="indexterm" data-primary="ConvNeXt" id="xi_ConvNeXt12924477_1"/> model; we just need to add a second dense output layer with four units (e.g., on top of the global average pooling layer). Here’s a <code translate="no">FlowerLocator</code> model that adds a localization head to a given base model, such as our ConvNeXt model:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">FlowerLocator</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">base_model</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">base_model</code> <code class="o">=</code> <code class="n">base_model</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">localization_head</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">base_model</code><code class="o">.</code><code class="n">classifier</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code><code class="o">.</code><code class="n">in_features</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">features</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">base_model</code><code class="o">.</code><code class="n">features</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">pool</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">base_model</code><code class="o">.</code><code class="n">avgpool</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>
        <code class="n">logits</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">base_model</code><code class="o">.</code><code class="n">classifier</code><code class="p">(</code><code class="n">pool</code><code class="p">)</code>
        <code class="n">bbox</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">localization_head</code><code class="p">(</code><code class="n">pool</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">logits</code><code class="p">,</code> <code class="n">bbox</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">locator_model</code> <code class="o">=</code> <code class="n">FlowerLocator</code><code class="p">(</code><code class="n">model</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>This locator model has two heads: the first outputs class logits, while the second outputs the bounding box. The localization head has the same number of inputs as the <code translate="no">nn.Linear</code> layer of the classification head, but it outputs just four numbers. The <code translate="no">forward()</code> method<a data-type="indexterm" data-primary="torch" data-secondary="forward()" id="id2932"/> takes a batch of preprocessed images as input and outputs both the predicted class logits (102 per image) and the predicted bounding boxes (1 per image). After training this model, you can use it as follows:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">preproc_images</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># a batch of preprocessed images</code>
<code class="n">y_pred_logits</code><code class="p">,</code> <code class="n">y_pred_bbox</code> <code class="o">=</code> <code class="n">locator_model</code><code class="p">(</code><code class="n">preprocessed_images</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">))</code></pre>

<p>But how can we train this model? Well, we saw how to train a model with two or more outputs in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>, and this one is no different: in this case, we can use the <code translate="no">nn.CrossEntropyLoss</code> for the classification head<a data-type="indexterm" data-primary="torch" data-secondary="nn.CrossEntropyLoss" id="id2933"/><a data-type="indexterm" data-primary="torch" data-secondary="nn.MSELoss" id="id2934"/>, and the <code translate="no">nn.MSELoss</code> for the localization head. The final loss can just be a weighted sum of the two. Voilà, that’s all there is to it.</p>

<p>Hey, not so fast! We have a problem: the Flowers102 dataset<a data-type="indexterm" data-primary="Flowers102 dataset" id="id2935"/> does not include any bounding boxes, so we need to add them ourselves. This is often one of the hardest and most costly parts of a machine learning project: labeling<a data-type="indexterm" data-primary="labels" data-secondary="image classification" id="id2936"/><a data-type="indexterm" data-primary="images, classifying and generating" data-secondary="labels" id="id2937"/> and annotating the data. It’s a good idea to spend time looking for the right tools. To annotate images with bounding boxes, you may want to use an open source labeling tool like Label Studio, OpenLabeler, ImgLab, Labelme, VoTT, or VGG Image Annotator, or perhaps a commercial tool like LabelBox, Supervisely, Roboflow, or RectLabel. Many of these are now AI assisted, greatly speeding up the annotation task. You may also want to consider crowdsourcing platforms such as Amazon Mechanical Turk if you have a very large number of images to annotate. However, it is quite a lot of work to set up a crowdsourcing platform, prepare the form to be sent to the workers, supervise them, and ensure that the quality of the bounding boxes they produce is good, so make sure it is worth the effort. If there are just a few hundred or a even a couple of thousand images to label, and you don’t plan to do this frequently, it may be preferable to do it yourself: with the right tools, it will only take a few days, and you’ll also gain a better understanding of your dataset and task.</p>

<p>You can then create a custom dataset (see <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>) where each entry contains an image, a label, and a bounding box. TorchVision conveniently includes a <code translate="no">BoundingBoxes</code> class<a data-type="indexterm" data-primary="torchvision" data-secondary="BoundingBoxes class" id="id2938"/> that represents a list of bounding boxes. For example, the following code creates a bounding box for the largest flower in the first image of the Flowers102 training set (for now we only consider one bounding box per image, but we’ll discuss multiple bounding boxes per image later in this chapter):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torchvision.tv_tensors</code>

<code class="n">bbox</code> <code class="o">=</code> <code class="n">torchvision</code><code class="o">.</code><code class="n">tv_tensors</code><code class="o">.</code><code class="n">BoundingBoxes</code><code class="p">(</code>
    <code class="p">[[</code><code class="mi">377</code><code class="p">,</code> <code class="mi">199</code><code class="p">,</code> <code class="mi">248</code><code class="p">,</code> <code class="mi">262</code><code class="p">]],</code>  <code class="c1"># center x=377, center y=199, width=248, height=262</code>
    <code class="nb">format</code><code class="o">=</code><code class="s2">"CXCYWH"</code><code class="p">,</code>  <code class="c1"># other possible formats: "XYXY" and "XYWH"</code>
    <code class="n">canvas_size</code><code class="o">=</code><code class="p">(</code><code class="mi">500</code><code class="p">,</code> <code class="mi">754</code><code class="p">)</code>  <code class="c1"># raw image size before preprocessing</code>
<code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>To visualize bounding boxes, use the <code>torchvi⁠sion.utils.draw_​bounding_boxes()</code> function<a data-type="indexterm" data-primary="torchvision" data-secondary="utils.draw_bounding_boxes()" id="id2939"/>. You will first need to convert the bounding boxes to the XYXY format<a data-type="indexterm" data-primary="torchvision" data-secondary="T.ops.box_convert()" id="id2940"/> using <code>torchvi⁠sion.​ops.box_convert()</code>.</p>
</div>

<p>The <code translate="no">BoundingBoxes</code> class is a subclass of <code translate="no">TVTensor</code>, which is a subclass<a data-type="indexterm" data-primary="torchvision" data-secondary="TVTensor class" id="id2941"/> of <code>torch.​Ten⁠sor</code>, so you can treat bounding boxes exactly like regular tensors, with extra features. Most importantly, you can transform bounding boxes using TorchVision’s transforms API v2. For example, let’s use the transform we defined earlier to preprocess this bounding box:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">transform</code><code class="p">(</code><code class="n">bbox</code><code class="p">)</code>
<code class="go">BoundingBoxes([[ 90,  91, 120, 154]], format=BoundingBoxFormat.CXCYWH,</code>
<code class="go">              canvas_size=(224, 224), clamping_mode=soft)</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Resizing and cropping a bounding box works as expected, but rotation is special: the bounding box can’t be rotated since it doesn’t have any rotation parameter, so instead it is resized to fit the rotated box (<em>not</em> the rotated object). As a result, it may end up being a bit too large for the object.</p>
</div>

<p>You can pass a nested data structure to a transform and the output will have the same structure, except with all the images and bounding boxes transformed. For example, the following code transforms the first flower image in the training set and its bounding box, leaving the label unchanged. In this example, the input and output are both 2-tuples containing an image and a dictionary composed of a label and a bounding box, but you could use any other data structure:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">first_image</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># load the first training image without any preprocessing</code>
<code class="n">preproc_image</code><code class="p">,</code> <code class="n">preproc_target</code> <code class="o">=</code> <code class="n">transform</code><code class="p">(</code>
    <code class="p">(</code><code class="n">first_image</code><code class="p">,</code> <code class="p">{</code><code class="s2">"label"</code><code class="p">:</code> <code class="mi">0</code><code class="p">,</code> <code class="s2">"bbox"</code><code class="p">:</code> <code class="n">bbox</code><code class="p">})</code>
<code class="p">)</code>
<code class="n">preproc_bbox</code> <code class="o">=</code> <code class="n">preproc_target</code><code class="p">[</code><code class="s2">"bbox"</code><code class="p">]</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>When using the MSE<a data-type="indexterm" data-primary="MSE (mean squared error)" id="id2942"/><a data-type="indexterm" data-primary="mean squared error (MSE)" id="id2943"/>, a 10-pixel error for a large bounding box will be penalized just as much as a 10-pixel error for a small bounding box. To avoid this, you can use a custom loss function that computes the square root of the width and height—for both the target and the prediction—before computing the MSE.</p>
</div>

<p>The MSE is simple and often works fairly well to train the model, but it is not a great metric to evaluate how well the model can predict bounding boxes. The most common metric for this<a data-type="indexterm" data-primary="intersection over union (IoU) metric" id="xi_intersectionoverunionIoUmetric12999186_1"/><a data-type="indexterm" data-primary="Jaccard index metric" id="id2944"/> is the <em>intersection over union</em> (IoU, also known as the <em>Jaccard index</em>): it is the area of overlap between the target bounding box T and the predicted bounding box P, divided by the area of their union P ∪ T  (see <a data-type="xref" href="#iou_diagram">Figure 12-24</a>). In short, IoU = |P ∩ T| / |P ∪ T|, where |<em>x</em>| is the area of <em>x</em>. The IoU ranges from 0 (no overlap) to 1 (perfect overlap). It is implemented by the <code translate="no">torchvision.ops.box_iou()</code> function<a data-type="indexterm" data-primary="torchvision" data-secondary="ops.box_iou()" id="id2945"/>.</p>

<p>The IoU is not great for training because it is equal to zero whenever P and T have no overlap, regardless of the distance between them or their shapes: in this case the gradient is also equal to zero and therefore gradient descent cannot make any progress. Luckily, it’s possible to fix this flaw by incorporating extra information. For example, the <em>Generalized IoU</em> (GIoU)<a data-type="indexterm" data-primary="Generalized IoU (GIoU)" id="id2946"/><a data-type="indexterm" data-primary="GIoU (Generalized IoU)" id="id2947"/>, introduced in a <a href="https://homl.info/giou">2019 paper</a> by H. Rezatofighi et al.,⁠<sup><a data-type="noteref" id="id2948-marker" href="ch12.html#id2948">31</a></sup> considers the smallest box S that contains both P and T, and it subtracts from the IoU the ratio of S that is not covered by P or T. In short, GIoU = IoU – |S – (P ∪ T)| / |S|. This means that the GIoU gets smaller as P and T get further apart, which gives gradient descent something to play with so it can pull P closer to T. Since we want to maximize the GIoU, the GIoU loss is equal to 1 – GIoU. This loss quickly became popular, and it is implemented by the <code translate="no">torchvision.ops.generalized_box_iou_loss()</code> function<a data-type="indexterm" data-primary="torchvision" data-secondary="ops.generalized_box_iou_loss()" id="id2949"/>.</p>

<figure><div id="iou_diagram" class="figure">
<img src="assets/hmls_1224.png" alt="Diagram illustrating the intersection over union (IoU) metric for bounding boxes, showing overlapping and union areas around a flower." width="1972" height="1015"/>
<h6><span class="label">Figure 12-24. </span>IoU metric for bounding boxes</h6>
</div></figure>

<p>Another important variant of the IoU is the <em>Complete IoU</em> (CIoU)<a data-type="indexterm" data-primary="Complete IoU (CIoU)" id="id2950"/><a data-type="indexterm" data-primary="CIoU (Complete IoU)" id="id2951"/>, introduced in a <a href="https://homl.info/ciou">2020 paper</a> by Z. Zheng et al.⁠<sup><a data-type="noteref" id="id2952-marker" href="ch12.html#id2952">32</a></sup> It considers three geometric factors: the IoU (the more overlap, the better), the distance between the centers of P and T (the closer, the better), normalized by the length of the diagonal of S, and the similarity between the aspect ratios of P and T (the closer, the better). The loss is 
<span class="keep-together">1 – CIoU,</span> and it is implemented by the <code translate="no">torchvision.ops.complete_box_iou_loss()</code> function<a data-type="indexterm" data-primary="torchvision" data-secondary="ops.complete_box_iou_loss()" id="id2953"/>. It generally performs better than the MSE or the GIoU, converging faster and leading to more accurate bounding boxes, so it is becoming the default loss for localization<a data-type="indexterm" data-startref="xi_intersectionoverunionIoUmetric12999186_1" id="id2954"/>.</p>

<p>Classifying and localizing a single object is nice, but what if the images contain multiple objects (as is often the case in the flowers dataset)<a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsclassificationandlocalization1292472_1" id="id2955"/><a data-type="indexterm" data-startref="xi_classificationCNNs1292472_1" id="id2956"/><a data-type="indexterm" data-startref="xi_localizationCNNs1292472_1" id="id2957"/><a data-type="indexterm" data-startref="xi_ConvNeXt12924477_1" id="id2958"/>?</p>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Object Detection"><div class="sect1" id="id235">
<h1>Object Detection</h1>

<p>The task of classifying and localizing multiple objects in an image<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="object detection" id="xi_convolutionalneuralnetworksCNNsobjectdetection12101268_1"/><a data-type="indexterm" data-primary="object detection" id="xi_objectdetection12101268_1"/><a data-type="indexterm" data-primary="dense prediction, transformers for" data-secondary="object detection" id="xi_densepredictiontransformersforobjectdetection12101268_1"/> is called <em>object detection</em>. Until a few years ago, a common approach was to take a CNN that was trained to classify and locate a single object roughly centered in the image, then slide this CNN across the image and make predictions at each step. The CNN was generally trained to predict not only class probabilities and a bounding box<a data-type="indexterm" data-primary="objectness score" id="id2959"/>, but also an <em>objectness score</em>: this is the estimated probability that the image does indeed contain an object centered near the middle. This is a binary classification output; it can be produced by a dense output layer with a single unit, using the sigmoid activation function<a data-type="indexterm" data-primary="sigmoid activation function" data-secondary="object detection" id="id2960"/> and trained using the binary cross-entropy loss.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Instead of an objectness score, a “no-object” class was sometimes added, but in general this did not work as well. The questions “Is an object present?” and “What type of object is it?” are best answered separately.</p>
</div>

<p>This sliding-CNN approach is illustrated in <a data-type="xref" href="#sliding_cnn_diagram">Figure 12-25</a>. In this example, the image was chopped into a 5 × 7 grid, and we see a CNN—the thick black rectangle—sliding across all 3 × 3 regions and making predictions at each step.</p>

<figure class="width-90"><div id="sliding_cnn_diagram" class="figure">
<img src="assets/hmls_1225.png" alt="Diagram illustrating a sliding CNN approach on a grid over an image of pink roses, with colored rectangles indicating regions where predictions are made." width="1240" height="900"/>
<h6><span class="label">Figure 12-25. </span>Detecting multiple objects by sliding a CNN across the image</h6>
</div></figure>

<p class="pagebreak-before">In this figure, the CNN has already made predictions for three of these 3 × 3 regions:</p>

<ul>
<li>
<p>When looking at the top-left 3 × 3 region (centered on the red-shaded grid cell located in the second row and second column), it detected the leftmost rose. Notice that the predicted bounding box exceeds the boundary of this 3 × 3 region. That’s absolutely fine: even though the CNN could not see the bottom part of the rose, it was able to make a reasonable guess as to where it might be. It also predicted class probabilities, giving a high probability to the “rose” class. Lastly, it predicted a fairly high objectness score, since the center of the bounding box lies within the central grid cell (in this figure, the objectness score is represented by the thickness of the bounding box).</p>
</li>
<li>
<p>When looking at the next 3 × 3 region, one grid cell to the right (centered on the shaded blue square), it did not detect any flower centered in that region, so it predicted a very low objectness score; therefore, the predicted bounding box and class probabilities can safely be ignored. You can see that the predicted bounding box was no good anyway.</p>
</li>
<li>
<p>Finally, when looking at the next 3 × 3 region, again one grid cell to the right (centered on the shaded green cell), it detected the rose at the top, although not perfectly. This rose is not well centered within this region, so the predicted objectness score was not very high.</p>
</li>
</ul>

<p>You can imagine how sliding the CNN across the whole image would give you a total of 15 predicted bounding boxes, organized in a 3 × 5 grid, with each bounding box accompanied by its estimated class probabilities and objectness score. Since objects can have varying sizes, you may then want to slide the CNN again across 2 × 2 and 
<span class="keep-together">4 × 4 regions as well</span>, to capture smaller and larger objects.</p>

<p>This technique is fairly straightforward, but as you can see it will often detect the same object multiple times, at slightly different positions. Some post-processing is needed to get rid of all the unnecessary bounding boxes. A common approach for this is called <em>non-max suppression</em> (NMS)<a data-type="indexterm" data-primary="non-max suppression (NMS), bounding boxes" id="id2961"/>. Here’s how it works:</p>
<ol>
<li>
<p>First, get rid of all the bounding boxes for which the objectness score is below some threshold; since the CNN believes there’s no object at that location, the bounding box is useless.</p>
</li>
<li>
<p>Find the remaining bounding box with the highest objectness score, and get rid of all the other remaining bounding boxes that overlap a lot with it (e.g., with an IoU greater than 60%). For example, in <a data-type="xref" href="#sliding_cnn_diagram">Figure 12-25</a>, the bounding box with the max objectness score is the thick bounding box over the leftmost rose. The other bounding box that touches this same rose overlaps a lot with the max bounding box, so we will get rid of it (although in this example it would already have been removed in the previous step).</p>
</li>
<li>
<p>Repeat step 2 until there are no more bounding boxes to get rid of.</p>
</li>

</ol>

<p>This simple approach to object detection works pretty well, but it requires running the CNN many times (15 times in this example), so it is quite slow. Fortunately, there is a much faster way to slide a CNN across an image: using a <em>fully convolutional network</em> (FCN).</p>








<section data-type="sect2" data-pdf-bookmark="Fully Convolutional Networks"><div class="sect2" id="id236">
<h2>Fully Convolutional Networks</h2>

<p>The idea of FCNs<a data-type="indexterm" data-primary="fully convolutional networks (FCNs)" id="xi_fullyconvolutionalnetworksFCNs12104117_1"/><a data-type="indexterm" data-primary="FCNs (fully convolutional networks)" id="xi_FCNsfullyconvolutionalnetworks12104117_1"/> was first introduced in a <a href="https://homl.info/fcn">2015 paper</a>⁠<sup><a data-type="noteref" id="id2962-marker" href="ch12.html#id2962">33</a></sup> by Jonathan Long et al., for semantic segmentation<a data-type="indexterm" data-primary="semantic segmentation" id="id2963"/><a data-type="indexterm" data-primary="images, classifying and generating" data-secondary="segmentation" id="id2964"/><a data-type="indexterm" data-primary="dense prediction, transformers for" data-secondary="semantic segmentation" id="id2965"/> (the task of classifying every pixel in an image according to the class of the object it belongs to). The authors pointed out that you could replace the dense layers at the top of a CNN with convolutional layers. To understand this, let’s look at an example: suppose a dense layer with 200 neurons sits on top of a convolutional layer that outputs 100 feature maps<a data-type="indexterm" data-primary="feature maps" id="xi_featuremaps121041686_1"/>, each of size 7 × 7 (this is the feature map size, not the kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activations from the convolutional layer (plus a bias term). Now let’s see what happens if we replace the dense layer with a convolutional layer using 200 filters, each of size 7 × 7, and with <code translate="no">"valid"</code> padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel is exactly the size of the input feature maps and we are using <code translate="no">"valid"</code> padding). In other words, it will output 200 numbers, just like the dense layer did; and if you look closely at the computations performed by a convolutional layer, you will notice that these numbers will be precisely the same as those the dense layer produced. The only difference is that the dense layer’s output was a tensor of shape [<em>batch size</em>, 200], while the convolutional layer will output a tensor of shape [<em>batch size</em>, 200, 1, 1].</p>
<div data-type="tip"><h6>Tip</h6>
<p>To convert a dense layer to a convolutional layer<a data-type="indexterm" data-primary="dense layers" id="id2966"/>, the number of filters in the convolutional layer must be equal to the number of units in the dense layer, the filter size must be equal to the size of the input feature maps, and you must use <code translate="no">"valid"</code> padding. The stride may be set to 1 or more, as we will see shortly.</p>
</div>

<p>Why is this important? Well, while a dense layer expects a specific input size (since it has one weight per input feature), a convolutional layer will happily process images of any size⁠<sup><a data-type="noteref" id="id2967-marker" href="ch12.html#id2967">34</a></sup> (however, it does expect its inputs to have a specific number of channels, since each kernel contains a different set of weights for each input channel). Since an FCN contains only convolutional layers (and pooling layers, which have the same property), it can be trained and executed on images of any size!</p>

<p>For example, suppose we’d already trained a CNN for flower classification and localization, with an extra head for objectness. It was trained on 224 × 224 images, and it outputs 107 values per image:</p>

<ul>
<li>
<p>The classification head outputs 102 class logits (one per class), trained using the <code translate="no">nn.CrossEntropyLoss</code>.</p>
</li>
<li>
<p>The objectness head outputs a single objectness logit, trained using the <code translate="no">nn.BCELoss</code>.</p>
</li>
<li>
<p>The localization head outputs four numbers describing the bounding box, trained using the CIoU loss.</p>
</li>
</ul>

<p>We can now convert the CNN’s dense layers (<code translate="no">nn.Linear</code>) to convolutional layers (<code translate="no">nn.Conv2d</code>). In fact, we don’t even need to retrain the model; we can just copy the weights from the dense layers to the convolutional layers! Alternatively, we could have converted the CNN into an FCN before training.</p>

<p>Now suppose the last convolutional layer before the output layer (also called the bottleneck layer)<a data-type="indexterm" data-primary="bottleneck layers" id="id2968"/> outputs 7 × 7 feature maps when the network is fed a 224 × 224 image (see the left side of <a data-type="xref" href="#fcn_diagram">Figure 12-26</a>). For example, this would be the case if the network contains 5 layers with stride 2 and <code translate="no">"same"</code> padding, so the spatial dimensions get divided by 2<sup>5</sup> = 32 overall. If we feed the FCN a 448 × 448 image (see the righthand side of <a data-type="xref" href="#fcn_diagram">Figure 12-26</a>), the bottleneck layer will now output 14 × 14 feature maps. Since the dense output layer was replaced by a convolutional layer using 107 filters of size 7 × 7, with <code translate="no">"valid"</code> padding and stride 1, the output will be composed of 107 feature maps, each of size 8 × 8 (since 14 – 7 + 1 = 8)<a data-type="indexterm" data-startref="xi_featuremaps121041686_1" id="id2969"/>.</p>

<p>In other words, the FCN will process the whole image only once, and it will output an 8 × 8 grid where each cell contains the predictions for one region of the image: 107 numbers representing 102 class probabilities, 1 objectness score, and 4 bounding box coordinates. It’s exactly like taking the original CNN and sliding it across the image using 8 steps per row and 8 steps per column. To visualize this, imagine chopping the original image into a 14 × 14 grid, then sliding a 7 × 7 window across this grid; there will be 8 × 8 = 64 possible locations for the window, hence 8 × 8 predictions<a data-type="indexterm" data-startref="xi_fullyconvolutionalnetworksFCNs12104117_1" id="id2970"/><a data-type="indexterm" data-startref="xi_FCNsfullyconvolutionalnetworks12104117_1" id="id2971"/>. However, the FCN approach is <em>much</em> more efficient, since the network only looks at the image once. In fact, <em>You Only Look Once</em> (YOLO) is the name of a very popular object detection architecture, which we’ll look at next.</p>

<figure class="width-70"><div id="fcn_diagram" class="figure">
<img src="assets/hmls_1226.png" alt="A diagram illustrating a fully convolutional network processing a small and a large image, showing the progression from CNN layers to feature maps and convolution outputs." width="1141" height="1240"/>
<h6><span class="label">Figure 12-26. </span>The same fully convolutional network processing a small image (left) and a large one (right)</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="You Only Look Once"><div class="sect2" id="id237">
<h2>You Only Look Once</h2>

<p>YOLO<a data-type="indexterm" data-primary="You Only Look Once (YOLO)" id="xi_YouOnlyLookOnceYOLO1210645_1"/> is a fast and accurate object detection architecture proposed by Joseph Redmon et al. in a <a href="https://homl.info/yolo">2015 paper</a>.⁠<sup><a data-type="noteref" id="id2972-marker" href="ch12.html#id2972">35</a></sup> It is so fast that it can run in real time on a video, as seen in Redmon’s <a href="https://homl.info/yolodemo2">demo</a>. YOLO’s architecture is quite similar to the one we just discussed, but with a few important differences:</p>

<ul>
<li>
<p>For each grid cell, YOLO only considers objects whose bounding box center lies within that cell. The bounding box coordinates are relative to that cell, where (0, 0) means the top-left corner of the cell and (1, 1) means the bottom right. However, the bounding box’s height and width may extend well beyond the cell.</p>
</li>
</ul>

<ul class="less_space pagebreak-before">
<li>
<p>It outputs two bounding boxes for each grid cell (instead of just one), which allows the model to handle cases where two objects are so close to each other that their bounding box centers lie within the same cell. Each bounding box also comes with its own objectness score.</p>
</li>
<li>
<p>YOLO also outputs a class probability distribution for each grid cell, predicting 20 class probabilities per grid cell since YOLO was trained on the PASCAL VOC dataset, which contains 20 classes. This produces a coarse <em>class probability map</em>. Note that the model predicts one class probability distribution per grid cell, not per bounding box. However, it’s possible to estimate class probabilities for each bounding box during post-processing by measuring how well each bounding box matches each class in the class probability map. For example, imagine a picture of a person standing in front of a car. There will be two bounding boxes: one large horizontal one for the car, and a smaller vertical one for the person. These bounding boxes may have their centers within the same grid cell. So how can we tell which class should be assigned to each bounding box? Well, the class probability map will contain a large region where the “car” class is dominant, and inside it there will be a smaller region where the “person” class is dominant. Hopefully, the car’s bounding box will roughly match the “car” region, while the person’s bounding box will roughly match the “person” region: this will allow the correct class to be assigned to each bounding box.</p>
</li>
</ul>

<p>YOLO was originally developed using Darknet, an open source deep learning framework initially developed in C by Joseph Redmon, but it was soon ported to PyTorch and other libraries. It has been continuously improved over the years, initially by Joseph Redmon et al. (YOLOv2, YOLOv3, and YOLO9000), then by various other teams since 2020. Each version brought some impressive improvements in speed and accuracy, using a variety of techniques; for example, YOLOv3 boosted accuracy in part<a data-type="indexterm" data-primary="anchor priors" id="id2973"/> thanks to <em>anchor priors</em>, exploiting the fact that some bounding box shapes are more likely than others, depending on the class (e.g., people tend to have vertical bounding boxes, while cars usually don’t). They also increased the number of bounding boxes per grid cell, they trained on different datasets with many more classes (up to 9,000 classes organized in a hierarchy in the case of YOLO9000), they added skip connections to recover some of the spatial resolution that is lost in the CNN (we will discuss this shortly when we look at semantic segmentation), and much more. There are many variants of these models too, such as scaled down “tiny” YOLOs, optimized to be trained on less powerful machines and which can run extremely fast (at over 1,000 frames per second!), but with a slightly lower <em>mean average precision</em> (mAP).</p>
<aside data-type="sidebar" epub:type="sidebar" class="less_space pagebreak-before"><div class="sidebar" id="id2974">
<h1>Mean Average Precision</h1>
<p>A very common metric used in object detection tasks is the mean average precision<a data-type="indexterm" data-primary="mean Average Precision (mAP)" id="id2975"/><a data-type="indexterm" data-primary="mAP (mean Average Precision)" id="id2976"/><a data-type="indexterm" data-primary="average precision (AP)" id="id2977"/><a data-type="indexterm" data-primary="AP (average precision)" id="id2978"/>. “Mean average” sounds a bit redundant, doesn’t it? To understand this metric, let’s go back to two classification metrics we discussed in <a data-type="xref" href="ch03.html#classification_chapter">Chapter 3</a>: precision and recall. Remember the trade-off: in general, the higher the recall, the lower the precision. You can visualize this in a precision/recall curve (see <a data-type="xref" href="ch03.html#precision_vs_recall_plot">Figure 3-6</a>). To summarize this curve into a single number, we could compute its area under the curve (AUC). But note that the precision/recall curve may contain a few sections where precision actually goes up when recall increases, especially at low recall values (you can see this at the top right of <a data-type="xref" href="ch03.html#precision_vs_recall_plot">Figure 3-6</a>). This is one of the motivations for the mAP metric.</p>

<p>Suppose the classifier has 90% precision at 10% recall, but 96% precision at 20% recall. There’s really no trade-off here: it simply makes more sense to use the classifier at 20% recall rather than at 10% recall, as you will get both higher recall and higher precision. So instead of looking at the precision <em>at</em> 10% recall, we should really be looking at the <em>maximum</em> precision that the classifier can offer with <em>at least</em> 10% recall. It would be 96%, not 90%. Therefore, one way to get a fair idea of the model’s performance is to compute the maximum precision you can get with at least 0% recall, then 10% recall, 20%, and so on up to 100%, and then calculate the mean of these maximum precisions. This is called the <em>average precision</em> (AP) metric. Now when there are more than two classes, we can compute the AP for each class, and then compute the mean AP (mAP). Conveniently, the TorchMetrics library implements all of this in the <code translate="no">MeanAveragePrecision</code> metric.</p>

<p>In an object detection system, there is an additional level of complexity: what if the system detected the correct class, but at the wrong location (i.e., the bounding box is completely off)? Surely we should not count this as a positive prediction. One approach is to define an IoU threshold: for example, we may consider that a prediction is correct only if the IoU is greater than, say, 0.5, and the predicted class is correct. The corresponding mAP is generally denoted mAP@0.5 (or mAP@50%, or sometimes just AP<sub>50</sub>). In some competitions (such as the PASCAL VOC challenge), this is what is done. In others (such as the COCO competition), the mAP is computed for different IoU thresholds (0.50, 0.55, 0.60, …​, 0.95), and the final metric is the mean of all these mAPs (denoted mAP@[.50:.95] or mAP@[.50:0.05:.95]). Yes, that’s a mean mean average.</p>
</div></aside>

<p>TorchVision does not include any YOLO model<a data-type="indexterm" data-primary="TorchVision" data-secondary="Ultralytics library" id="id2979"/><a data-type="indexterm" data-primary="Ultralytics library" id="id2980"/><a data-type="indexterm" data-primary="PyTorch" data-secondary="object detection models" id="xi_PyTorchobjectdetectionmodels12108144_1"/>, but you can use the Ultralytics library, which provides a simple API to download and use various pretrained YOLO models, based on PyTorch. These models were pretrained on the COCO dataset which contains over 330,000 images, including 200,000 images annotated for object detection with 80 different classes (person, car, truck, bicycle, ball, etc.). The Ultralytics library is not installed on Colab by default, so we must run <code translate="no">%pip install ultralytics</code>. Then we can download a YOLO model and use it. For example, here is how to use this library to download the YOLOv9 model (medium variant) and detect objects in a batch of images (the model accepts PIL images, NumPy arrays, and even URLs):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">ultralytics</code> <code class="kn">import</code> <code class="n">YOLO</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">YOLO</code><code class="p">(</code><code class="s1">'yolov9m.pt'</code><code class="p">)</code>  <code class="c1"># n=nano, s=small, m=medium, x=large</code>
<code class="n">images</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"https://homl.info/soccer.jpg"</code><code class="p">,</code> <code class="s2">"https://homl.info/traffic.jpg"</code><code class="p">]</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">images</code><code class="p">)</code></pre>

<p>The output is a list of <code translate="no">Results</code> objects which offers a handy <code translate="no">summary()</code> method<a data-type="indexterm" data-primary="Ultralytics library" id="id2981"/>. For example, here is how we can see the first detected object in the first image:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">results</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">summary</code><code class="p">()[</code><code class="mi">0</code><code class="p">]</code>
<code class="go">{'name': 'sports ball',</code>
<code class="go"> 'class': 32,</code>
<code class="go"> 'confidence': 0.96214,</code>
<code class="go"> 'box': {'x1': 245.35733, 'y1': 286.03003, 'x2': 300.62509, 'y2': 343.57184}}</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>The Ultralytics library also provides a simple API to train a YOLO model on other common object detection datasets, or on your own dataset. See <a href="https://docs.ultralytics.com/modes/train" class="bare"><em class="hyperlink">https://docs.ultralytics.com/modes/train</em></a> for more details.</p>
</div>

<p>Several other pretrained object detection models are available via TorchVision. You can use them just like the pretrained classification models (e.g., ConvNeXt), except that each image prediction is a represented as a dictionary containing two entries: <code translate="no">"labels"</code> (i.e., class IDs) and <code translate="no">"boxes"</code>. The available models are listed here (see the <a href="https://pytorch.org/vision/main/models">models page</a> for the full list of variants available):</p>
<dl>
<dt><a href="https://homl.info/fasterrcnn">Faster R-CNN</a>⁠<sup><a data-type="noteref" id="id2982-marker" href="ch12.html#id2982">36</a></sup></dt>
<dd>
<p>This model<a data-type="indexterm" data-primary="Faster R-CNN" id="id2983"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="Faster R-CNN" id="id2984"/> has two stages: the image first goes through a CNN, then the output is passed to a <em>region proposal network</em> (RPN)<a data-type="indexterm" data-primary="region proposal network (RPN)" id="id2985"/><a data-type="indexterm" data-primary="RPN (region proposal network)" id="id2986"/> that proposes bounding boxes that are most likely to contain an object; a classifier is then run for each bounding box, based on the cropped output of the CNN.</p>
</dd>
<dt><a href="https://homl.info/ssd">SSD</a>⁠<sup><a data-type="noteref" id="id2987-marker" href="ch12.html#id2987">37</a></sup></dt>
<dd>
<p>SSD<a data-type="indexterm" data-primary="SSD (single-stage detector)" id="id2988"/> is a single-stage detector (“look once”) similar to YOLO.</p>
</dd>
<dt><a href="https://homl.info/ssdlite">SSDlite</a>⁠<sup><a data-type="noteref" id="id2989-marker" href="ch12.html#id2989">38</a></sup></dt>
<dd>
<p>A lightweight version of SSD<a data-type="indexterm" data-primary="SSDlite" id="id2990"/>, well suited for mobile devices.</p>
</dd>
<dt><a href="https://homl.info/retinanet">RetinaNet</a>⁠<sup><a data-type="noteref" id="id2991-marker" href="ch12.html#id2991">39</a></sup></dt>
<dd>
<p>A single-stage detector<a data-type="indexterm" data-primary="RetinaNet" id="id2992"/> which introduced a variant of the cross-entropy loss<a data-type="indexterm" data-primary="torchvision" data-secondary="ops.sigmoid_focal_loss()" id="id2993"/><a data-type="indexterm" data-primary="focal loss" id="id2994"/> called<a data-type="indexterm" data-primary="FCOS" id="id2995"/> the <em>focal loss</em> (see <code translate="no">torchvision.ops.sigmoid_focal_loss()</code>). This loss gives more weight to difficult samples and thereby improves performance on small objects and less frequent classes.</p>
</dd>
<dt><a href="https://homl.info/fcos">FCOS</a>⁠<sup><a data-type="noteref" id="id2996-marker" href="ch12.html#id2996">40</a></sup></dt>
<dd>
<p>A single-stage fully convolutional net which directly predicts bounding boxes without relying on anchor boxes<a data-type="indexterm" data-startref="xi_PyTorchobjectdetectionmodels12108144_1" id="id2997"/>.</p>
</dd>
</dl>

<p>So far, we’ve only considered detecting objects in single images. But what about videos? Objects must not only be detected in each frame, they must also be tracked over time. Let’s take a quick look at object tracking now<a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsobjectdetection12101268_1" id="id2998"/><a data-type="indexterm" data-startref="xi_objectdetection12101268_1" id="id2999"/><a data-type="indexterm" data-startref="xi_YouOnlyLookOnceYOLO1210645_1" id="id3000"/><a data-type="indexterm" data-startref="xi_densepredictiontransformersforobjectdetection12101268_1" id="id3001"/>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Object Tracking"><div class="sect1" id="id238">
<h1>Object Tracking</h1>

<p>Object tracking<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="object tracking" id="xi_convolutionalneuralnetworksCNNsobjecttracking12112016_1"/><a data-type="indexterm" data-primary="object tracking, CNNs" id="xi_objecttrackingCNNs12112016_1"/><a data-type="indexterm" data-primary="videos, object tracking" id="xi_videosobjecttracking12112016_1"/> is a challenging task: objects move, they may grow or shrink as they get closer or further away, their appearance may change as they turn around or move to different lighting conditions or backgrounds, they may be temporarily occluded by other objects, and so on.</p>

<p>One of the most popular object tracking systems<a data-type="indexterm" data-primary="DeepSort" id="id3002"/> is <a href="https://homl.info/deepsort">DeepSORT</a>.⁠<sup><a data-type="noteref" id="id3003-marker" href="ch12.html#id3003">41</a></sup> It is based on a combination of classical algorithms and deep learning:</p>

<ul class="fix_tracking">
<li>
<p>It uses <em>Kalman filters</em> to estimate<a data-type="indexterm" data-primary="filters, Kalman" id="id3004"/><a data-type="indexterm" data-primary="Kalman filters" id="id3005"/> the most likely current position of an object given prior detections, and assuming that objects tend to move at a constant speed.</p>
</li>
<li>
<p>It uses a deep learning model to measure the resemblance between new detections and existing tracked objects.</p>
</li>
<li>
<p>Lastly, it uses<a data-type="indexterm" data-primary="Hungarian algorithm" id="id3006"/> the <em>Hungarian algorithm</em> to map new detections to existing tracked objects (or to new tracked objects). This algorithm efficiently finds the 
<span class="keep-together">combination</span> of mappings that minimizes the distance between the detections and the predicted positions of tracked objects, while also minimizing the appearance discrepancy.</p>
</li>
</ul>

<p>For example, imagine a red ball that just bounced off a blue ball traveling in the opposite direction. Based on the previous positions of the balls, the Kalman filter will predict that the balls will go through each other; indeed, it assumes that objects move at a constant speed, so it will not expect the bounce. If the Hungarian algorithm only considered positions, then it would happily map the new detections to the wrong balls, as if they had just gone through each other and swapped colors. But thanks to the resemblance measure, the Hungarian algorithm will notice the problem. Assuming the balls are not too similar, the algorithm will map the new detections to the correct balls.</p>

<p>The Ultralytics library<a data-type="indexterm" data-primary="Ultralytics library" id="id3007"/><a data-type="indexterm" data-primary="TorchVision" data-secondary="Ultralytics library" id="id3008"/> supports object tracking. It uses the <a href="https://homl.info/botsort">Bot-SORT algorithm</a> by default: this algorithm is very similar to DeepSORT but it’s faster and more accurate thanks to improvements such as camera-motion compensation and tweaks to the Kalman filter.⁠<sup><a data-type="noteref" id="id3009-marker" href="ch12.html#id3009">42</a></sup> For example, we can track objects in a video using the YOLOv9 model we created earlier by executing the following code. In this example, we also print the ID of each tracked object at every frame, and we save a copy of the video with annotations (its path is displayed at the end):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">my_video</code> <code class="o">=</code> <code class="s2">"https://homl.info/cars.mp4"</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">track</code><code class="p">(</code><code class="n">source</code><code class="o">=</code><code class="n">my_video</code><code class="p">,</code> <code class="n">stream</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">save</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="k">for</code> <code class="n">frame_results</code> <code class="ow">in</code> <code class="n">results</code><code class="p">:</code>
    <code class="n">summary</code> <code class="o">=</code> <code class="n">frame_results</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code>  <code class="c1"># similar summary as earlier + track id</code>
    <code class="n">track_ids</code> <code class="o">=</code> <code class="p">[</code><code class="n">obj</code><code class="p">[</code><code class="s2">"track_id"</code><code class="p">]</code> <code class="k">for</code> <code class="n">obj</code> <code class="ow">in</code> <code class="n">summary</code><code class="p">]</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Track ids:"</code><code class="p">,</code> <code class="n">track_ids</code><code class="p">)</code></pre>

<p>So far we have located objects using bounding boxes. This is often sufficient, but sometimes you need to locate objects with much more precision—for example, to remove the background behind a person during a videoconference call. Let’s see how to go down to the pixel level<a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsobjecttracking12112016_1" id="id3010"/><a data-type="indexterm" data-startref="xi_objecttrackingCNNs12112016_1" id="id3011"/><a data-type="indexterm" data-startref="xi_videosobjecttracking12112016_1" id="id3012"/>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Semantic Segmentation"><div class="sect1" id="id239">
<h1>Semantic Segmentation</h1>

<p>In <em>semantic segmentation</em>, each pixel<a data-type="indexterm" data-primary="semantic segmentation" id="xi_semanticsegmentation12114539_1"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="semantic segmentation" id="xi_convolutionalneuralnetworksCNNssemanticsegmentation12114539_1"/><a data-type="indexterm" data-primary="images, classifying and generating" data-secondary="segmentation" id="xi_imagesclassifyingandgeneratingsegmentation12114539_1"/><a data-type="indexterm" data-primary="dense prediction, transformers for" data-secondary="semantic segmentation" id="xi_densepredictiontransformersforsemanticsegmentation12114539_1"/> is classified according to the class of the object it belongs to (e.g., road, car, pedestrian, building, etc.), as shown in <a data-type="xref" href="#semantic_segmentation_diagram">Figure 12-27</a>. Note that different objects of the same class are <em>not</em> distinguished. For example, all the bicycles on the righthand side of the segmented image end up as one big lump of pixels. The main difficulty in this task is that when images go through a regular CNN, they gradually lose their spatial resolution (due to the layers with strides greater than 1); so, a regular CNN may end up knowing that there’s a person somewhere in the bottom left of the image, but it might not be much more precise than that.</p>

<figure><div id="semantic_segmentation_diagram" class="figure">
<img src="assets/hmls_1227.png" alt="A street scene is displayed with semantic segmentation applied, classifying areas into categories like sky, buildings, people, cars, bicycles, sidewalk, and road." width="2333" height="925"/>
<h6><span class="label">Figure 12-27. </span>Semantic segmentation</h6>
</div></figure>

<p>Just like for object detection, there are many different approaches to tackle this problem, some quite complex. However, a fairly simple solution was proposed in the 2015 paper by Jonathan Long et al., that I mentioned earlier, on fully convolutional networks. The authors start by taking a pretrained CNN and turning it into an FCN<a data-type="indexterm" data-primary="FCNs (fully convolutional networks)" id="id3013"/><a data-type="indexterm" data-primary="fully convolutional networks (FCNs)" id="id3014"/>. The CNN applies an overall stride of 32 to the input image (i.e., if you multiply all the strides), meaning the last layer outputs feature maps that are 32 times smaller than the input image. This is clearly too coarse, so they added<a data-type="indexterm" data-primary="upsampling layer" id="id3015"/> a single <em>upsampling layer</em> that multiplies the resolution by 32.</p>

<p>There are several solutions available for upsampling (increasing the size of an image), such as bilinear interpolation, but that only works reasonably well up to ×4 or ×8. Instead, they<a data-type="indexterm" data-primary="transposed convolutional layer" id="id3016"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="convolutional layers" id="xi_convolutionalneuralnetworksCNNsconvolutionallayers121153186_1"/> use a <em>transposed convolutional layer</em>:⁠<sup><a data-type="noteref" id="id3017-marker" href="ch12.html#id3017">43</a></sup> this is equivalent to first stretching the image by inserting empty rows and columns (full of zeros), then performing a regular convolution (see <a data-type="xref" href="#conv2d_transpose_diagram">Figure 12-28</a>). Alternatively, some people prefer to think of it as a regular convolutional layer that uses fractional strides (e.g., the stride is 1/2 in <a data-type="xref" href="#conv2d_transpose_diagram">Figure 12-28</a>). The transposed convolutional layer can be initialized to perform something close to linear interpolation, but since it is a trainable layer, it will learn to do better during training. In PyTorch, you can use the <code translate="no">nn.ConvTranspose2d</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.ConvTranspose2d" id="id3018"/>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In a transposed convolutional layer, the stride<a data-type="indexterm" data-primary="strides" id="id3019"/> defines how much the input will be stretched, not the size of the filter steps, so the larger the stride, the larger the output (unlike for convolutional layers or pooling layers).</p>
</div>

<figure class="width-55"><div id="conv2d_transpose_diagram" class="figure">
<img src="assets/hmls_1228.png" alt="Diagram illustrating upsampling using a transposed convolutional layer, showing a 2x3 input being expanded to a 5x7 output with stride 2 and kernel size 3." width="821" height="668"/>
<h6><span class="label">Figure 12-28. </span>Upsampling using a transposed convolutional layer</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id3020">
<h1>Other PyTorch Convolutional Layers</h1>
<p>PyTorch also offers a few other kinds of convolutional layers:</p>
<dl>
<dt><code translate="no">nn.Conv1d</code></dt>
<dd>
<p>A convolutional layer for 1D<a data-type="indexterm" data-primary="1D convolutional layers" data-primary-sortas="one d convolutional layers" id="id3021"/> inputs<a data-type="indexterm" data-primary="torch" data-secondary="nn.Conv1d" id="id3022"/>, such as time series or text (sequences of letters or words), as you will see in <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>.</p>
</dd>
<dt><code translate="no">nn.Conv3d</code></dt>
<dd>
<p>A convolutional<a data-type="indexterm" data-primary="torch" data-secondary="nn.Conv3d" id="id3023"/> layer for 3D inputs, such as 3D PET scans.</p>
</dd>
<dt>À-trous convolutional layer</dt>
<dd>
<p>Setting the <code translate="no">dilation</code> hyperparameter of any convolutional layer<a data-type="indexterm" data-primary="à-trous convolutional layer" data-primary-sortas="àtrous convolutional layer" id="id3024"/><a data-type="indexterm" data-primary="PyTorch" data-secondary="à-trous convolutional layer" id="id3025"/> to a value of 2 or more creates an <em>à-trous convolutional layer</em> (<em>à trous</em> is French for “with holes”). This is equivalent to using a regular convolutional layer with a filter dilated by inserting rows and columns of zeros (i.e., holes). For example, a 1 × 3 filter equal to <code translate="no">[[1,2,3]]</code> may be dilated<a data-type="indexterm" data-primary="dilation rate" id="id3026"/> with a <em>dilation rate</em> of 4, resulting<a data-type="indexterm" data-primary="dilated filter" id="id3027"/> in a <em>dilated filter</em> of <code translate="no">[[1, 0, 0, 0, 2, 0, 0, 0, 3]]</code>. This lets the convolutional layer have a larger receptive field at no computational cost and using no extra parameters.</p>
</dd>
</dl>
</div></aside>

<p>Using transposed convolutional layers for upsampling is OK, but still too imprecise. To do better, Long et al. added skip connections from lower layers: for example, they upsampled the output image by a factor of 2 (instead of 32), and they added the output of a lower layer that had this double resolution. Then they upsampled the result by a factor of 16, leading to a total upsampling factor of 32 (see <a data-type="xref" href="#skip_plus_upsample_diagram">Figure 12-29</a>). This recovered some of the spatial resolution that was lost in earlier pooling layers. In their best architecture, they used a second similar skip connection to recover even finer details from an even lower layer. In short, the output of the original CNN goes through the following extra steps: upsample ×2, add the output of a lower layer (of the appropriate scale), upsample ×2, add the output of an even lower layer, and finally upsample ×8. It is even possible to scale up beyond the size of the original image: this can be used to increase the resolution of an image<a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsconvolutionallayers121153186_1" id="id3028"/>, which is a technique<a data-type="indexterm" data-primary="super-resolution" id="id3029"/> called <em>super-resolution</em>.</p>

<figure class="smallerninety"><div id="skip_plus_upsample_diagram" class="figure">
<img src="assets/hmls_1229.png" alt="Diagram showing the use of skip connections and upsampling in convolutional neural networks to recover spatial resolution, with feature maps being downsampled and then upsampled by factors of 2 and 16." width="1406" height="390"/>
<h6><span class="label">Figure 12-29. </span>Skip layers recover some spatial resolution from lower layers</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>The FCN model is available in TorchVision, along with a couple other semantic segmentation models. See the notebook for a code example.</p>
</div>

<p><em>Instance segmentation</em> is similar to semantic segmentation<a data-type="indexterm" data-primary="instance segmentation" id="id3030"/>, but instead of merging all objects of the same class into one big lump, each object is distinguished from the others (e.g., it identifies each individual bicycle). For example the <em>Mask R-CNN</em> architecture<a data-type="indexterm" data-primary="Mask R-CNN" id="id3031"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="Mask R-CNN" id="id3032"/>, proposed in a <a href="https://homl.info/maskrcnn">2017 paper</a>⁠<sup><a data-type="noteref" id="id3033-marker" href="ch12.html#id3033">44</a></sup> by Kaiming He et al., extends the Faster R-CNN model<a data-type="indexterm" data-primary="Faster R-CNN" id="id3034"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="Faster R-CNN" id="id3035"/> by additionally producing a pixel mask for each bounding box. So not only do you get a bounding box around each object, with a set of estimated class probabilities, you also get a pixel mask that locates pixels in the bounding box that belong to the object. This model is available in TorchVision, pretrained on the COCO 2017 dataset<a data-type="indexterm" data-startref="xi_boundingboxesimageidentification12924132_1" id="id3036"/>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>TorchVision’s transforms API v2 can apply to masks and videos<a data-type="indexterm" data-primary="TorchVision" data-secondary="transforms applied to masks and videos" id="id3037"/>, just like it applies to bounding boxes, thanks to the <code translate="no">Video</code> and <code translate="no">Mask</code> TVTensors.</p>
</div>

<p>As you can see, the field of deep computer vision is vast and fast-paced, with all sorts of architectures popping up every year. If you want to try the latest and greatest models, check out the trending papers at <a href="https://huggingface.co/papers" class="bare"><em class="hyperlink">https://huggingface.co/papers</em></a>. Most of them used to be based on convolutional neural networks, but since 2020 another neural net architecture has entered the computer vision space: Transformers (which we will discuss in <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>). The progress made over the last 15 years has been astounding, and researchers are now focusing on harder and harder problems<a data-type="indexterm" data-primary="adversarial learning" id="id3038"/>, such as <em>adversarial learning</em> (which attempts to make the network more resistant to images designed to fool it), <em>explainability</em> (understanding<a data-type="indexterm" data-primary="explainability, attention mechanisms" id="id3039"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="explainability" id="id3040"/> why the network makes a specific classification)<a data-type="indexterm" data-primary="image generation" id="id3041"/>, realistic <em>image generation</em> (which we will come back to in <a data-type="xref" href="ch18.html#autoencoders_chapter">Chapter 18</a>), <em>single-shot learning</em> (a system that can recognize an object after it has seen it just once)<a data-type="indexterm" data-primary="single-shot learning" id="id3042"/>, predicting the next frames in a video, combining text and image tasks, and more.</p>

<p>Now on to the next chapter, where we will look at how to process sequential data such as time series using recurrent neural networks and convolutional neural networks<a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNs12739_1" id="id3043"/><a data-type="indexterm" data-startref="xi_semanticsegmentation12114539_1" id="id3044"/><a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNssemanticsegmentation12114539_1" id="id3045"/><a data-type="indexterm" data-startref="xi_imagesclassifyingandgeneratingsegmentation12114539_1" id="id3046"/><a data-type="indexterm" data-startref="xi_densepredictiontransformersforsemanticsegmentation12114539_1" id="id3047"/>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id736">
<h1>Exercises</h1>
<ol>
<li>
<p>What are the advantages of a CNN over a fully connected DNN for image classification?</p>
</li>
<li>
<p>Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and <code translate="no">"same"</code> padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels:</p>
<ol>
<li>
<p>What is the total number of parameters in the CNN?</p>
</li>
<li>
<p>If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance?</p>
</li>
<li>
<p>What about when training on a mini-batch of 50 images?</p>
</li>

</ol>
</li>
<li>
<p>If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?</p>
</li>
<li>
<p>Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?</p>
</li>
<li>
<p>Can you name the main innovations in AlexNet, as compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, Xception, EfficientNet, and ConvNeXt?</p>
</li>
<li>
<p>What is a fully convolutional network? How can you convert a dense layer into a convolutional layer?</p>
</li>
<li>
<p>What is the main technical difficulty of semantic segmentation?</p>
</li>
<li>
<p>Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.</p>
</li>
<li>
<p>Use transfer learning for large image classification, going through these steps:</p>
<ol>
<li>
<p>Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.). Alternatively, you can use an existing dataset, such as the one used in PyTorch’s <a href="https://homl.info/transfertuto">transfer learning for computer vision tutorial</a>.</p>
</li>
<li>
<p>Split it into a training set, a validation set, and a test set.</p>
</li>
<li>
<p>Build the input pipeline, apply the appropriate preprocessing operations, and optionally add data augmentation.</p>
</li>
<li>
<p>Fine-tune a pretrained model on this dataset.</p>
</li>

</ol>
</li>
<li>
<p>Go through PyTorch’s <a href="https://homl.info/detectiontuto">object detection fine-tuning tutorial</a>.</p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id2734"><sup><a href="ch12.html#id2734-marker">1</a></sup> David H. Hubel, “Single Unit Activity in Striate Cortex of Unrestrained Cats”, <em>The Journal of Physiology</em> 147 (1959): 226–238.</p><p data-type="footnote" id="id2735"><sup><a href="ch12.html#id2735-marker">2</a></sup> David H. Hubel and Torsten N. Wiesel, “Receptive Fields of Single Neurons in the Cat’s Striate Cortex”, <em>The Journal of Physiology</em> 148 (1959): 574–591.</p><p data-type="footnote" id="id2736"><sup><a href="ch12.html#id2736-marker">3</a></sup> David H. Hubel and Torsten N. Wiesel, “Receptive Fields and Functional Architecture of Monkey Striate Cortex”, <em>The Journal of Physiology</em> 195 (1968): 215–243.</p><p data-type="footnote" id="id2738"><sup><a href="ch12.html#id2738-marker">4</a></sup> Kunihiko Fukushima, “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position”, <em>Biological Cybernetics</em> 36 (1980): 193–202.</p><p data-type="footnote" id="id2739"><sup><a href="ch12.html#id2739-marker">5</a></sup> Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition”, <em>Proceedings of the IEEE</em> 86, no. 11 (1998): 2278–2324.</p><p data-type="footnote" id="id2744"><sup><a href="ch12.html#id2744-marker">6</a></sup> A convolution is a mathematical operation that slides one function over another and measures the integral of their pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform, and is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very similar to convolutions (see <a href="https://homl.info/76" class="bare"><em class="hyperlink">https://homl.info/76</em></a> for more details).</p><p data-type="footnote" id="id2783"><sup><a href="ch12.html#id2783-marker">7</a></sup> Other kernels we’ve discussed so far had weights, but pooling kernels do not: they are just stateless sliding windows.</p><p data-type="footnote" id="id2805"><sup><a href="ch12.html#id2805-marker">8</a></sup> Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition”, <em>Proceedings of the IEEE</em> 86, no. 11 (1998): 2278–2324.</p><p data-type="footnote" id="id2808"><sup><a href="ch12.html#id2808-marker">9</a></sup> Alex Krizhevsky et al., “ImageNet Classification with Deep Convolutional Neural Networks”, <em>Proceedings of the 25th International Conference on Neural Information Processing Systems</em> 1 (2012): 1097–1105.</p><p data-type="footnote" id="id2820"><sup><a href="ch12.html#id2820-marker">10</a></sup> Matthew D. Zeiler and Rob Fergus, “Visualizing and Understanding Convolutional Networks”, <em>Proceedings of the European Conference on Computer Vision</em> (2014): 818–833.</p><p data-type="footnote" id="id2822"><sup><a href="ch12.html#id2822-marker">11</a></sup> Christian Szegedy et al., “Going Deeper with Convolutions”, <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2015): 1–9.</p><p data-type="footnote" id="id2823"><sup><a href="ch12.html#id2823-marker">12</a></sup> In the 2010 movie <em>Inception</em>, the characters keep going deeper and deeper into multiple layers of dreams; hence the name of these modules.</p><p data-type="footnote" id="id2839"><sup><a href="ch12.html#id2839-marker">13</a></sup> Kaiming He et al., “Deep Residual Learning for Image Recognition”, arXiv preprint arXiv:1512:03385 (2015).</p><p data-type="footnote" id="id2843"><sup><a href="ch12.html#id2843-marker">14</a></sup> Gao Huang, Yu Sun, et al., “Deep Networks with Stochastic Depth”, arXiv preprint arXiv:1603.09382 (2016).</p><p data-type="footnote" id="id2847"><sup><a href="ch12.html#id2847-marker">15</a></sup> It is a common practice when describing a neural network to count only layers with parameters.</p><p data-type="footnote" id="id2850"><sup><a href="ch12.html#id2850-marker">16</a></sup> Christian Szegedy et al., “Inception–v4, Inception-ResNet and the Impact of Residual Connections on Learning”, arXiv preprint arXiv:1602.07261 (2016).</p><p data-type="footnote" id="id2853"><sup><a href="ch12.html#id2853-marker">17</a></sup> François Chollet, “Xception: Deep Learning with Depthwise Separable Convolutions”, arXiv preprint arXiv:1610.02357 (2016).</p><p data-type="footnote" id="id2856"><sup><a href="ch12.html#id2856-marker">18</a></sup> This name can sometimes be ambiguous, since spatially separable convolutions are often called “separable convolutions” as well.</p><p data-type="footnote" id="id2863"><sup><a href="ch12.html#id2863-marker">19</a></sup> Jie Hu et al., “Squeeze-and-Excitation Networks”, <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2018): 7132–7141.</p><p data-type="footnote" id="id2869"><sup><a href="ch12.html#id2869-marker">20</a></sup> Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, arXiv preprint arXiv:1409.1556 (2014).</p><p data-type="footnote" id="id2872"><sup><a href="ch12.html#id2872-marker">21</a></sup> Saining Xie et al., “Aggregated Residual Transformations for Deep Neural Networks”, arXiv preprint arXiv:1611.05431 (2016).</p><p data-type="footnote" id="id2874"><sup><a href="ch12.html#id2874-marker">22</a></sup> Gao Huang et al., “Densely Connected Convolutional Networks”, arXiv preprint arXiv:1608.06993 (2016).</p><p data-type="footnote" id="id2876"><sup><a href="ch12.html#id2876-marker">23</a></sup> Andrew G. Howard et al., “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications”, arXiv preprint arXiv:1704.04861 (2017).</p><p data-type="footnote" id="id2879"><sup><a href="ch12.html#id2879-marker">24</a></sup> Chien-Yao Wang et al., “CSPNet: A New Backbone That Can Enhance Learning Capability of CNN”, arXiv preprint arXiv:1911.11929 (2019).</p><p data-type="footnote" id="id2881"><sup><a href="ch12.html#id2881-marker">25</a></sup> Mingxing Tan and Quoc V. Le, “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”, arXiv preprint arXiv:1905.11946 (2019).</p><p data-type="footnote" id="id2884"><sup><a href="ch12.html#id2884-marker">26</a></sup> Zhuang Liu et al, “A ConvNet for the 2020s”, arXiv preprint arXiv:2201.03545 (2022).</p><p data-type="footnote" id="id2889"><sup><a href="ch12.html#id2889-marker">27</a></sup> In the international system of units (SI), 1 MB = 1,000 KB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits. And 1 MiB = 1,024 kiB = 1,024 × 1,024 bytes. So 12 MB ≈ 11.44 MiB.</p><p data-type="footnote" id="id2897"><sup><a href="ch12.html#id2897-marker">28</a></sup> Aidan Gomez et al., “The Reversible Residual Network: Backpropagation Without Storing Activations”, arXiv preprint arXiv:1707.04585 (2017).</p><p data-type="footnote" id="id2920"><sup><a href="ch12.html#id2920-marker">29</a></sup> M. Nilsback and A. Zisserman, “Automated Flower Classification over a Large Number of Classes”, <em>Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</em> (2008).</p><p data-type="footnote" id="id2922"><sup><a href="ch12.html#id2922-marker">30</a></sup> TorchVision PR #8838 might have fixed this by the time your read these lines.</p><p data-type="footnote" id="id2948"><sup><a href="ch12.html#id2948-marker">31</a></sup> H. Rezatofighi et al., “Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression”, arXiv preprint arXiv:1902.09630 (2019).</p><p data-type="footnote" id="id2952"><sup><a href="ch12.html#id2952-marker">32</a></sup> Z. Zheng et al., “Enhancing Geometric Factors in Model Learning and Inference for Object Detection and Instance Segmentation”, arXiv preprint arXiv:2005.03572 (2020).</p><p data-type="footnote" id="id2962"><sup><a href="ch12.html#id2962-marker">33</a></sup> Jonathan Long et al., “Fully Convolutional Networks for Semantic Segmentation”, <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2015): 3431–3440.</p><p data-type="footnote" id="id2967"><sup><a href="ch12.html#id2967-marker">34</a></sup> There is one small exception: a convolutional layer using <code translate="no">"valid"</code> padding will complain if the input size is smaller than the kernel size.</p><p data-type="footnote" id="id2972"><sup><a href="ch12.html#id2972-marker">35</a></sup> Joseph Redmon et al., “You Only Look Once: Unified, Real-Time Object Detection”, <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2016): 779–788.</p><p data-type="footnote" id="id2982"><sup><a href="ch12.html#id2982-marker">36</a></sup> Shaoqing Ren et al., “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, <em>Proceedings of e 28th International Conference on Neural Information Processing Systems</em> 1 (2015): 91–99.</p><p data-type="footnote" id="id2987"><sup><a href="ch12.html#id2987-marker">37</a></sup> Wei Liu et al., “SSD: Single Shot Multibox Detector”, <em>Proceedings of the 14th European Conference on Computer Vision</em> 1 (2016): 21–37.</p><p data-type="footnote" id="id2989"><sup><a href="ch12.html#id2989-marker">38</a></sup> Mark Sandler et al., “MobileNetV2: Inverted Residuals and Linear Bottlenecks”, arXiv preprint arXiv:1801.04381 (2018).</p><p data-type="footnote" id="id2991"><sup><a href="ch12.html#id2991-marker">39</a></sup> Tsung-Yi Lin et al., “Focal Loss for Dense Object Detection”, arXiv preprint arXiv:1708.02002 (2017).</p><p data-type="footnote" id="id2996"><sup><a href="ch12.html#id2996-marker">40</a></sup> Zhi Tian et al., “FCOS: Fully Convolutional One-Stage Object Detection”, arXiv preprint arXiv:1904.01355 (2019).</p><p data-type="footnote" id="id3003"><sup><a href="ch12.html#id3003-marker">41</a></sup> Nicolai Wojke et al., “Simple Online and Realtime Tracking with a Deep Association Metric”, arXiv preprint arXiv:1703.07402 (2017).</p><p data-type="footnote" id="id3009"><sup><a href="ch12.html#id3009-marker">42</a></sup> Nir Aharon et al., “BoT-SORT: Robust Associations Multi-Pedestrian Tracking”, arXiv preprint arXiv:2206.14651 (2022).</p><p data-type="footnote" id="id3017"><sup><a href="ch12.html#id3017-marker">43</a></sup> This type of layer is sometimes referred<a data-type="indexterm" data-primary="deconvolution layer" id="id3048"/> to as a <em>deconvolution layer</em>, but it does <em>not</em> perform what mathematicians call a deconvolution, so this name should be avoided.</p><p data-type="footnote" id="id3033"><sup><a href="ch12.html#id3033-marker">44</a></sup> Kaiming He et al., “Mask R-CNN”, arXiv preprint arXiv:1703.06870 (2017).</p></div></div></section></div></div></body></html>