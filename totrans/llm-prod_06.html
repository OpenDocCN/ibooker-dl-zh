<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">7</span></span> <span class="chapter-title-text">Prompt engineering: Becoming an LLM whisperer </span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">What a prompt is and how to make one</li>
<li class="readable-text" id="p3">Prompt engineering—more than just crafting a prompt</li>
<li class="readable-text" id="p4">Prompt engineering tooling available to make it all possible</li>
<li class="readable-text" id="p5">Advanced prompting techniques to answer the hardest questions</li>
</ul>
</div>
<div class="readable-text" id="p6">
<blockquote>
<div>
     Behold, we put bits in the horses' mouths, that they may obey us; and we turn about their whole body. 
     <div class="quote-cite">
       —James 3:3 
     </div>
</div>
</blockquote>
</div>
<div class="readable-text" id="p7">
<p>In the last chapter, we discussed in depth how to deploy large language models and, before that, how to train them. In this chapter, we are going to talk a bit about how to use them. We mentioned before that one of the biggest draws to LLMs is that you don’t need to train them on every individual task. LLMs, especially the largest ones, have a deeper understanding of language, allowing them to act as a general-purpose tool.</p>
</div>
<div class="readable-text intended-text" id="p8">
<p>Want to create a tutoring app that helps kids learn difficult concepts? What about a language translation app that helps bridge the gap between you and your in-laws? Need a cooking assistant to help you think up fun new recipes? With LLMs, you no longer have to start from scratch for every single use case; you can use the same model for each of these problems. It just becomes a matter of how you prompt your model. This is where prompt engineering, also called in-context learning, comes in. In this chapter, we are going to dive deep into the best ways to do that.</p>
</div>
<div class="readable-text" id="p9">
<h2 class="readable-text-h2" id="sigil_toc_id_125"><span class="num-string">7.1</span> Prompting your model</h2>
</div>
<div class="readable-text" id="p10">
<p>What exactly is a prompt? We’ve used this word throughout this book, so it feels a bit late to be diving into definitions, but it’s worth discussing because in literature, a prompt is taken to mean many different things. In general, though, the most basic definition is that a prompt is the input to a language model. At this most basic level, you have already done lots of prompting at this point in the book. However, prompting often means more than that; it comes with the connotation that it is meaningful or done with thought. Of course, we know this isn’t usually the case in production with actual users. When we are prompting, we are doing more than just “chatting with a bot”; we are crafting an input to get a desired output.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>LLMs have access to vast vocabularies, terabytes of training data, and billions upon billions of weights, meaning that the information you’re looking to get out of the model has a decent chance of being in there somewhere—just not always up near the surface (read “the middle of the standard deviation of probable responses”) where you need it to be. The goal is to create a prompt that will guide the model in activating the parameters in the part of the model that contains the correct information. In essence, prompting is instruction given after the fact, and as such, it is important within app development because it doesn’t require expensive retraining of the model.</p>
</div>
<div class="readable-text intended-text" id="p12">
<p>With this in mind, prompt engineering is the process of designing, templating, and refining a prompt and then implementing our learnings into code. Prompt engineering is how we create meaningful and consistent user experiences out of the chaos of LLM-generated outputs. And it’s no joke. As LLMs are becoming more common in application workflows, we have seen the rise of titles like Prompt Engineer and AI Engineer, each of which commands impressive salaries.</p>
</div>
<div class="readable-text" id="p13">
<h3 class="readable-text-h3" id="sigil_toc_id_126"><span class="num-string">7.1.1</span> Few-shot prompting</h3>
</div>
<div class="readable-text" id="p14">
<p>The most common form of prompt engineering is few-shot prompting because it’s both simple to do and extremely effective. Few-shot prompting entails giving a couple of examples of how you want the AI to act. Instead of searching for the tokens with the right distribution to get the response we want, we give the model several example distributions and ask it to mimic those. For example, if we wanted the model to do sentiment analysis defining reviews as positive or negative, we could give it a few examples before the input. Consider the following prompt: </p>
</div>
<div class="readable-text prompt" id="p15">
<p>Worked as advertised, 10/10: positive</p>
</div>
<div class="readable-text prompt" id="p16">
<p>It was broken on delivery: negative</p>
</div>
<div class="readable-text prompt" id="p17">
<p>Worth every penny spent: positive</p>
</div>
<div class="readable-text prompt" id="p18">
<p>Overly expensive for the quality: negative</p>
</div>
<div class="readable-text prompt" id="p19">
<p>If this is the best quality they can do, call me mr president: negative</p>
</div>
<div class="readable-text prompt" id="p20">
<p>&lt;Input data&gt;: </p>
</div>
<div class="readable-text" id="p21">
<p>Note, in this example, that we aren’t telling the model how to respond, but from the context, the LLM can figure out that it needs to respond with either the word <em>positive</em> or <em>negative</em>. In figure 7.1, we go ahead and plug the prompt in a model so you can see for yourself that it did indeed give a correct response in the expected format. Of course, there could be an array of acceptable responses, in which case giving instructions beforehand can help improve the results. To do this, we might append to our few-shot prompt with the following phrase, “Determine the sentiment of each review as one of the following: (positive, negative, neutral, strongly positive, strongly negative).” It’s also needed with most models; OpenAI includes language to restrict the output, such as “Please respond with only one option from the list with no explanation.” You might wonder why we’d suggest you say words like “please” to a model. The answer is pretty simple: in the training data, the highest-quality and most usefully structured human-to-human conversations follow certain conventions of politeness that you’re likely familiar with, like saying please and thank you. The same results can be achieved by using an excess of profanity and deep jargon on a topic because flouting those politeness conventions is another huge part of the training set, although that strategy isn’t as consistent, given that the companies training the models often “clean” their data of examples like that, regardless of their quality downstream. This type of prompting can be very useful when you need your response to be formatted in a certain way. If we need our response in JSON or XML, we could ask the model to return it in the format, but it will likely get the keys or typing wrong. We can easily fix that by showing the model several samples of expected results. Of course, prompting the model to return JSON will work, but JSON is a very opinionated data structure, and the model might hallucinate problems that are hard to catch, like using single instead of double quotes. We’ll go over tooling that can help with that later in the chapter.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p22">
<img alt="figure" height="335" src="../Images/7-1.png" width="1050"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.1</span> Few-shot prompting example</h5>
</div>
<div class="readable-text" id="p23">
<p>The one major downside to few-shot prompting is that examples can end up being quite long. For example, coding examples we might add and share can easily be thousands of tokens long, and that’s possible when defining a single function. Giving an example of an entire class, file, or project can easily push us out of our limits. Many models still have context limits restricted to 2K, 4K, or 8K. Since token limits are often restrictive, it can be difficult to balance adding another example or giving the user more space. Also, we often pay per token, so few-shot prompting can be much more expensive than other prompting techniques. As a result, many have turned to one-shot prompting to be more efficient and save money. </p>
</div>
<div class="readable-text" id="p24">
<h3 class="readable-text-h3" id="sigil_toc_id_127"><span class="num-string">7.1.2</span> One-shot prompting</h3>
</div>
<div class="readable-text" id="p25">
<p>One-shot learning is a machine learning concept where a model is expected to make accurate predictions given only a single example of each new class during training. In the context of LLMs and prompting, one-shot refers to situations where the model must understand and execute a task based on a single clear instruction or example in the prompt, often without seeing similar examples during training. It requires crafting the perfect example to get the expected results. </p>
</div>
<div class="readable-text intended-text" id="p26">
<p>Consider our previous sentiment analysis example; if you give a model only one positive example, you will likely bias the model to give only positive classifications—especially if the model has never seen such a problem before. So how can one-shot prompting ever be achieved? Thankfully, while this seems impossible at the outset, it’s quite achievable. After all, few-shot prompting is very effective but follows the law of diminishing returns. Each new example improves only marginally. The first example always does the heaviest lifting.</p>
</div>
<div class="readable-text intended-text" id="p27">
<p>LLMs can perform well on one-shot tasks due to the extensive pretraining they undergo on large and diverse datasets. During this pretraining, the models learn a wide array of language patterns, concepts, and structures, giving them a broad understanding of language and the world. When given a new one-shot task, they use this learned understanding to comprehend the task and generate a response, even if the exact task was not part of their training data. Here’s an example of a prompt attempting to coerce the model using one-shot prompting to respond to a word problem correctly:</p>
</div>
<div class="readable-text prompt" id="p28">
<p><span class="prompt-head"><span class="prompt-initials">CB</span> <b>User:</b></span> Answer this question. Think it through step by step, so that we know it’s correct: A dad and his son were in an accident and rushed to a hospital. The man’s son was in the operating room and the doctor said, “I can’t operate on you. You’re my son.” How is that possible?</p>
</div>
<div class="readable-text response" id="p29">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Assistant:</b></span> The son and the father were both injured, meaning that the only other person that the doctor could be and still have the same son is the father’s spouse, maybe a mother, or another father perhaps.</p>
</div>
<div class="readable-text prompt" id="p30">
<p><span class="prompt-head"><span class="prompt-initials">CB</span> <b>User:</b></span> Answer this question. Think it through step by step, so that we know it’s correct: How could it be possible to be your own grandpa?</p>
</div>
<div class="readable-text response" id="p31">
<p><strong><span class="prompt-head"><span class="prompt-initials">CB</span> Assistant   </span></strong></p>
</div>
<div class="readable-text" id="p32">
<p>In figure 7.2 we go ahead and plug this one-shot prompt into an LLM. Think about how you imagine it might answer. Given our example, do you think it will do well on the task?<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p33">
<img alt="figure" height="581" src="../Images/7-2.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.2</span> One-shot prompting example</h5>
</div>
<div class="readable-text" id="p34">
<p>The ability of LLMs to handle one-shot tasks improves as they are scaled up. As the model size increases and they are trained on more diverse and larger datasets, their capacity to generalize from their training to unseen one-shot tasks also improves. Nonetheless, it’s worth noting that while LLMs can perform impressively on one-shot tasks, they are not perfect, and their performance can still vary based on the complexity and specificity of the task. One-shot prompting generally needs much less data and only one example to craft, making it more accessible, faster to craft, and easier to experiment with. One-shot prompting has led researchers to push the boundaries even further.</p>
</div>
<div class="readable-text" id="p35">
<h3 class="readable-text-h3" id="sigil_toc_id_128"><span class="num-string">7.1.3</span> Zero-shot prompting</h3>
</div>
<div class="readable-text" id="p36">
<p>Having just explained few-shot and one-shot prompting, we’re sure you have already guessed what zero-shot prompting is. But since this is a book, let’s spell it out: zero-shot prompting is figuring out how to craft a prompt to get us the expected results without giving any examples. Zero-shot prompts often don’t perform as consistently as few-shot or one-shot prompts, but they have the advantage of being ubiquitous since we don’t need any examples or data. </p>
</div>
<div class="readable-text intended-text" id="p37">
<p>A common zero-shot prompt is a very simple template: </p>
</div>
<div class="readable-text prompt" id="p38">
<p>“Q: [User’s Prompt] A:.” </p>
</div>
<div class="readable-text" id="p39">
<p>With just a slight variation to the user’s prompt—adding it to a template that contains only two letters—we can get much better results by priming the model to answer the prompt as if it were a question—no examples necessary. </p>
</div>
<div class="readable-text intended-text" id="p40">
<p>Most zero-shot prompts take advantage of Chain of Thought (CoT). Wei et al.<a href="#footnote-103"><sup class="footnote-reference" id="footnote-source-1">1</sup></a> showed that by encouraging models to follow a step-by-step process, reasoning through multiple steps instead of jumping to conclusions, LLMs were more likely to answer math problems correctly—similar to how math teachers ask their students to show their work. Using few-shot prompting, the model was given several examples of reasoning through math problems. However, it was soon discovered that examples weren’t needed. You could elicit chain-of-thought behavior simply by asking the model to “think step by step.”<a href="#footnote-104"><sup class="footnote-reference" id="footnote-source-2">2</sup></a> </p>
</div>
<div class="readable-text intended-text" id="p41">
<p>By appending four magic words to the end of our prompts, “think step by step,” models transformed from dunces into puzzle-solving Olympiads. It was truly a marvel. Of course, it came with some problems. Thinking through multiple steps led to longer responses and a less ideal user experience. This was compounded later with the phrases “a more elegant solution” and “get this through your head ********,” which worked just as well but were less consistent if the domain was less common, with the last one achieving very concise and correct responses. We like to get straight to the point, after all, and we are used to computers answering our math problems extremely quickly. From our own experience, we’ve often noticed that when models are giving longer answers, they also don’t know when to stop, continuing to generate responses long after giving an answer. Later, we’ll show you how to solve this problem by creating stopping criteria with tools like LangChain or Guidance.</p>
</div>
<div class="readable-text intended-text" id="p42">
<p>There isn’t, of course, a perfect zero-shot prompt yet, and it’s a continuing part of research, although there likely never will be just one perfect prompt. We could, at most, get one perfect zero-shot prompt per model. Zhou et al. proposed an interesting strategy they termed “thread of thought.”<a href="#footnote-105"><sup class="footnote-reference" id="footnote-source-3">3</sup></a> Essentially, they figured they could do better than “think step by step” if they just used a few more words. So they generated 30 variations of the phrase and ran evaluations to determine which one worked best. From their work, they proposed that the prompt “Walk me through this context in manageable parts step by step, summarizing and analyzing as we go” would give better results when working with GPT-4. It’s hard to know if this prompt works equally well with other models, but their strategy is interesting nonetheless.</p>
</div>
<div class="readable-text intended-text" id="p43">
<p>Some other notable findings have left researchers flabbergasted that the approach worked; for example, offering an imaginary tip to a model will return better results. One X (formerly known as Twitter) user suggested the solution as a joke and was confused to find it worked, and the model offered more info relative to the size of the tip (for the original tipping test, see <a href="https://mng.bz/2gD9">https://mng.bz/2gD9</a>). <span class="aframe-location"/>Others later confirmed it helped with several other prompting principles.<a href="#footnote-106"><sup class="footnote-reference" id="footnote-source-4">4</sup></a> In addition, the authors have found strategies like telling the model you’ll lose your job if it doesn’t help you or even threatening to fire the model if it does a terrible job have elicited better results. Like the original “think step by step,” asking the model to “take a deep breath” can also ensure better outputs, particularly in math problems.<a href="#footnote-107"><sup class="footnote-reference" id="footnote-source-5">5</sup></a> It seems most strategies humans use, or use on other humans, to produce better work are fair game. Of course, the best trick will depend on which model you use and the underlying data it was trained on.</p>
</div>
<div class="readable-text" id="p44">
<h2 class="readable-text-h2" id="sigil_toc_id_129"><span class="num-string">7.2</span> Prompt engineering basics</h2>
</div>
<div class="readable-text" id="p45">
<p>We expect that most readers have probably done lots of prompting, but very few have done much of any prompt engineering yet. We’ve heard lots of jokes that prompt engineering isn’t a real discipline. We’ve also heard every other week that some library is “killing prompt engineering” by automatically prompting the model. One doubt about prompt engineering stems from how accessible prompting is to anyone who wants to try it and the lack of education needed to prompt effectively. All doubts about prompt engineering are the same doubts people express about linguistics as a discipline: “I’ve used language all my life; I know how it works.” So it makes sense that people similarly assume they know what language to use to effectively prompt an LLM. Anyone can learn effective strategies by simply playing with models or from purely online resources. In other words, it’s hard to believe that there is any real engineering going on when the majority of players are simply using the “guess and check” method. But this logic highlights a basic misunderstanding of what engineering is. There’s a big difference between getting a model to solve your problem once and getting it to solve every user’s problem every single time.</p>
</div>
<div class="readable-text intended-text" id="p46">
<p>There are several challenges with prompt engineering over regular prompting. For example, prompt engineering relies particularly on knowing the format the user expects the answer to be in. With prompting, you are the user, so you can keep trying until you see an answer you like; that doesn’t fly in prompt engineering.</p>
</div>
<div class="readable-text intended-text" id="p47">
<p>A bigger problem is that when building an application, your end users will have varying levels of knowledge of how to craft a prompt. Some may not have any skill and will struggle to get good responses, and others will have so much skill they will likely try to persuade your LLM to go off the rails you’ve set for it. Regardless, our goal is to build railings so that skilled users won’t be able to derail your application and unskilled users will have a smooth ride. A user’s skill in crafting a prompt shouldn’t be the determining factor of a successful experience.</p>
</div>
<div class="readable-text intended-text" id="p48">
<p>Another thing to call out is the decision process that you, as a product owner, must go through to get the model output to match the style you want. Should you finetune a new checkpoint, should you PEFT a LoRA, or can you achieve it through <span class="aframe-location"/>prompting? Unfortunately, due to the emergent nature of the behavior that we’re seeing with LLMs, there isn’t a good or at least definitive answer. Our recommendation at this point is to try prompt engineering first to see how good you can get without changing the model and then finetune from there as you see fit. I’ve seen some professional success using one base model and multiple LoRAs trained on different scenarios and styles of response combined with prompt engineering on the front, especially sanitizing and stylizing user input.</p>
</div>
<div class="readable-text intended-text" id="p49">
<p>Lastly, a good prompt engineer should be able to tell you rather quickly whether the solution you are trying to build can be done with prompt engineering at all. Even utilizing advanced techniques like retrieval-augmented generation (RAG), there are limitations on what you can do with prompt engineering alone. Knowing when you need to send a model back for additional finetuning is invaluable and can save your team from spinning their wheels for weeks without any progress.</p>
</div>
<div class="readable-text intended-text" id="p50">
<p>To get started, we’ll need to cover the basics about what makes up a prompt. In this section, we’ll discuss the different parts of a prompt, additional parameters that can be tuned in a query, and notes about paying attention to a model’s training data that you should be aware of. </p>
</div>
<div class="readable-text" id="p51">
<h3 class="readable-text-h3" id="sigil_toc_id_130"><span class="num-string">7.2.1</span> Anatomy of a prompt</h3>
</div>
<div class="readable-text" id="p52">
<p>To an engineer, a prompt is made up of a few elements, and identifying these elements makes it easier to create a framework to solve your use case and provide a better example for your users. Let’s say we are building an internal chatbot for our company to help answer HR-related questions based on internal documentation. One prompt we might expect from a user would be, “How much does the company match for our 401k?” This is the first element of a prompt, the input or user’s prompt. If you have only ever used LLM apps and have never built them, this is likely all you’ve ever seen. Generally, the input is gathered from a free-form text box, so it’s important to note that it can almost be anything. Often it will be awful, riddled with typos and mistakes, and not written in a manner to speak to a bot but to speak to another human.</p>
</div>
<div class="readable-text intended-text" id="p53">
<p>Let’s go ahead and pull back the curtain for a second and look at what the LLM likely saw based on that question with proper prompt engineering in place. </p>
</div>
<div class="readable-text prompt" id="p54">
<p><span class="prompt-head"><span class="prompt-initials">CB</span> <b>System:</b></span> You are a helpful assistant who knows about all company policies at XYZ company. Be courteous and keep conversations strictly related to the company. Offer links when available so users can look at the documentation themselves.</p>
</div>
<div class="readable-text prompt" id="p55">
<p><span class="prompt-head"><span class="prompt-initials">CB</span> <b>User:</b></span> How much does the company match for our 401k?</p>
</div>
<div class="readable-text prompt" id="p56">
<p><span class="prompt-head"><span class="prompt-initials">CB</span> <b>Context:</b></span> <code>&lt;Examples</code> <code>from</code> <code>a</code> <code>VectorDB</code> <code>search</code> <code>for</code> <code>closest</code> <code>document&gt;</code></p>
</div>
<div class="readable-text prompt" id="p57">
<p><span class="prompt-head"><span class="prompt-initials">CB</span> <b>System:</b></span> Strictly answer the user’s question, and only if it relates to company policies. If you don’t know the answer, simply say so. Be courteous and keep conversations strictly related to the company.</p>
</div>
<div class="readable-text response" id="p58">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/><b>Assistant:</b></span> Sure, I can help you with that! The company currently offers a 4% match to your 401k. You can find more details…</p>
</div>
<div class="readable-text" id="p59">
<p>This was a real example showcasing a situation in which the bot responded in the most probable way to the user’s satisfaction. Giving an LLM information in a structured format improves the model’s chance of responding correctly. So let’s break down what we are seeing. </p>
</div>
<div class="readable-text intended-text" id="p60">
<p>First, to improve results, we will often take the user’s prompt and inject it into an instruction set or template. One of the most basic templates and a great example is the Q&amp;A bot template which we showed earlier and which would have looked like this: “Q: How much does the company match for our 401k? A:”. Generally, in this section, though, instructions will be given to direct the model. It doesn’t have to be much, but often it will be much more detailed. For example, “Answer the following question and explain it as if the user was a five-year-old. Q: How much does the company match for our 401k? A:”.</p>
</div>
<div class="readable-text intended-text" id="p61">
<p>The next element is the context the model will need to respond appropriately. In our example, it’s very likely we haven’t finetuned a model to know XYZ’s company policies. What we need to do is give it to the model inside the prompt. In our example, we are likely doing this with RAG and where we would add the results from a semantic search.</p>
</div>
<div class="readable-text intended-text" id="p62">
<p>Context can be lots of different things and not just RAG search results. It could be the current time, weather information, current events, or even just the chat history. You will often also want to include some database lookup information about the user to provide a more personalized experience. All of this is information we might look up at the time of query, but context can often be static. For example, one of the most important pieces of information to include in the context is examples to help guide the model via few-shot or one-shot prompting. If your examples are static and not dynamic, they likely are hard-coded into the instruction template. The context often contains the answers to the users’ queries, and we are simply using the LLM to clean, summarize, and format an appropriate response. Ultimately, any pragmatics the model lacks will need to be given in the context.</p>
</div>
<div class="readable-text intended-text" id="p63">
<p>The last element is the system prompt. The system prompt is a prompt that will be appended and used on every request by every user. It is designed to give a consistent user experience. Generally, it’s where we would include role prompting or style prompting. Some examples of such role prompting or style prompting could include the following:</p>
</div>
<div class="readable-text prompt" id="p64">
<p>Take this paragraph and rephrase it to have a cheerful tone and be both informative and perky.</p>
</div>
<div class="readable-text prompt" id="p65">
<p>You are a wise old owl who helps adventurers on their quest.</p>
</div>
<div class="readable-text prompt" id="p66">
<p>In the form of a poem written by a pirate.</p>
</div>
<div class="readable-text" id="p67">
<p>The system prompt isn’t designed to be seen by end users, but obtaining the system prompt is often the goal of many prompt injection attacks—since knowing what it is (along with the model you are using) is essentially like stealing source code and allows the hacker to recreate your application. Of course, the system prompt itself is a great way to curb prompt injection and ensure your bot stays in character. Many great applications will include two system prompts, one at the front and one at the end, to avoid any “ignore previous instructions” type prompt injection attacks. It also helps keep the model focused on how we want it to behave since models tend to put more weight on what is said at the beginning and at the end. You may have noticed this in our previous example. Regardless, you shouldn’t keep any sensitive information in the system prompt.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p68">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Parts of the prompt </h5>
</div>
<div class="readable-text" id="p69">
<p>The following are the four parts of a prompt:</p>
</div>
<ul>
<li class="readable-text" id="p70"> <em>Input</em>—What the user wrote; can be anything </li>
<li class="readable-text" id="p71"> <em>Instruction</em>—The template used; often contains details and instructions to guide the model </li>
<li class="readable-text" id="p72"> <em>Context</em>—Pragmatics that the model needs to respond appropriately (e.g., examples, database lookups, RAG) </li>
<li class="readable-text" id="p73"> <em>System prompt</em>—A specific instruction given to the model on every request to enforce a certain user experience (e.g., talk like a pirate) </li>
</ul>
</div>
<div class="readable-text" id="p74">
<h3 class="readable-text-h3" id="sigil_toc_id_131"><span class="num-string">7.2.2</span> Prompting hyperparameters</h3>
</div>
<div class="readable-text" id="p75">
<p>Another aspect of prompt engineering you won’t see with simple prompting is prompt hyperparameter tuning. There are several hyperparameters in addition to the prompt you can set when making a query to increase or decrease the diversity of responses. Depending on your objective, the value of these parameters can greatly improve or even be a detriment to the query results for your users. It is important to note that being able to set these depends on the LLM API endpoint you are querying to be set up to accept them.</p>
</div>
<div class="readable-text intended-text" id="p76">
<p>First and foremost is temperature. The temperature parameter determines the level of randomness your model will account for when generating tokens. Setting it to zero will ensure the model will always respond exactly the same way when presented with identical prompts. This consistency is critical for jobs where we want our results to be predictable, but it can leave our models stuck in a rut. Setting it to a higher value will make it more creative. Setting it to negative will tell it to give you the opposite response to your prompt.</p>
</div>
<div class="readable-text intended-text" id="p77">
<p>To understand this parameter better, it might help to look closer at how a model determines the next token. Figure 7.3 shows an example of this process. Given the input, “I am a,” a language model will generate a vector of logits for each token in the model’s vocabulary. From here, we’ll apply a softmax, which will generate a list of probabilities for each token. These probabilities show the likelihood that each token will be chosen.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p78">
<img alt="figure" height="650" src="../Images/7-3.png" width="1088"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.3</span> A simple path of how the next word is chosen. Given an input, a model will generate a vector of logits for each token in the model’s vocabulary. Using the softmax algorithm, these logits will be transformed into probabilities. These probabilities will correspond to how often that token is likely to be chosen. Temperature is applied during the softmax algorithm.</h5>
</div>
<div class="readable-text" id="p79">
<p>Temperature is applied during the softmax algorithm. A higher temperature will flatten out the probability distribution, giving less weight to tokens with large logits and more weight to tokens with smaller logits. A lower temperature does the opposite. A temperature of zero is actually impossible since we can’t divide by zero. Instead, we run an argmax algorithm, ensuring we pick the token with the highest logit.</p>
</div>
<div class="readable-text intended-text" id="p80">
<p>The next parameter to consider is the number of beams applied to the model’s beam search. Beam search is a heuristic search algorithm that explores the graph of your model’s to-be-generated text probabilities, expanding the graph’s most optimistic nodes. It helps balance time and memory usage and improves the flow and quality of the response. It’s similar to the minimax algorithm in chess, except instead of deciding the next best move, we are deciding the next best word. Selecting a higher number of beams will create a larger search, improving results at the cost of latency.</p>
</div>
<div class="readable-text intended-text" id="p81">
<p>Top K is an interesting parameter. Assuming a temperature that isn’t zero, top K allows us to filter the potential next tokens by the K most probable options. Consequently, we eliminate less-probable words on the tail end of the distribution from ever being picked and avoid generating tokens that are more likely to be incoherent. So in our example from figure 7.3, if k = 3, then the only tokens we would choose are woman, man, or boy, filtering out the rest.</p>
</div>
<div class="readable-text intended-text" id="p82">
<p>Top P sets the threshold probability that the next token must reach to be selected. It’s similar to top K, but instead of considering the number of tokens, we are considering their distributions. A top P of 0.05 will only consider the next 5% most likely tokens and will lead to very rigid responses, while a top P of 0.95 will have greater flexibility but may turn out more gibberish. From our example in figure 7.3, if P = 0.5, only the tokens woman or man would be chosen since their probabilities 0.35 and 0.26 add up to greater than 0.5.</p>
</div>
<div class="readable-text intended-text" id="p83">
<p>Language models can often get caught in generation loops, repeating themselves in circles. To prevent this, we can add penalties. A frequency penalty adds a penalty for reusing a word if it was recently used. It is good to help increase the diversity of language. For example, if the model keeps on reusing the word “great,” increasing the frequency penalty will push the model to use more diverse words like “awesome,” “fantastic,” and “amazing” to avoid the penalty of reusing the word “great.” </p>
</div>
<div class="readable-text intended-text" id="p84">
<p>A presence penalty is similar to a frequency penalty in that we penalize repeated tokens, but a token that appears twice and a token that appears 100 times are penalized the same. Instead of just reducing overused words and phrases, we are aiming to reduce overused ideas and increase the likelihood of generating new topics.</p>
</div>
<div class="readable-text" id="p85">
<h3 class="readable-text-h3" id="sigil_toc_id_132"><span class="num-string">7.2.3</span> Scrounging the training data</h3>
</div>
<div class="readable-text" id="p86">
<p>The importance of prompt engineering for model performance has led to important discussions surrounding context windows and the efficacy of particular prompt structures, as LLMs responding quickly and accurately to the prompts has become a more widespread goal. In addition, a correlation has been drawn between cleaner examples and better responses from the model, emphasizing the need for better prompt engineering, even on the data side. While prompt engineering is often proposed as an alternative to finetuning, we’ve found the most success using both in conjunction to get two boosts in LLM performance as opposed to just one.</p>
</div>
<div class="readable-text intended-text" id="p87">
<p>Knowing the lingo and the choice of words used to generate the model will help you craft better responses. Let’s explain with a personal example. For the birthday of this author’s wife, I finetuned a text-to-image Stable Diffusions model to replicate her image so she could create fun pictures and custom avatars. I used the DreamBooth (see figure 7.4<span class="aframe-location"/>).<a href="#footnote-108"><sup class="footnote-reference" id="footnote-source-6">6</sup></a> The finetuning method requires defining a base class that can be used as a starting point. My first attempts were naive, and using the base class of “a person” or “a woman” was terrible. A base class of “Asian woman” returned pictures of older Asian women, often stylized in black and white or sepia. I then tried “young Asian woman,” but this created weird images of Asian faces being plastered onto young white women’s bodies.</p>
</div>
<div class="browsable-container figure-container" id="p88">
<img alt="figure" height="274" src="../Images/7-4.png" width="1012"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.4</span> Example of DreamBooth from Ruiz et al.<sup>7</sup> DreamBooth allows you to finetune an image model to replicate an object’s likeness based on only a few sample input images. Here, with only four example images of a puppy, Dreambooth can put that same dog in many new scenarios.</h5>
</div>
<div class="readable-text intended-text" id="p89">
<p>Giving up guessing, I went to the source, the LAION dataset (<a href="https://laion.ai/blog/laion-400-open-dataset/">https://laion.ai/blog/laion-400-open-dataset/</a>) the model was trained on. LAION comprises 400 million images scraped from the internet with their accompanying captions. It is a noncurated dataset quickly put together for research purposes (aka, it’s unclean with lots of duplicates, NSFW content, and poor captions). Searching the dataset, I discovered that there was not a single caption with the words “Asian woman.” Scrolling through, I quickly found that pictures of Asian women and models were identified with the words “Asian beauty.” Using these words as the base class, I was finally able to create great avatars for my wife.<strong><a href="#footnote-109"><sup class="footnote-reference" id="footnote-source-7">7</sup></a></strong></p>
</div>
<div class="readable-text intended-text" id="p90">
<p>There’s lots of social commentary that can be drawn from this example, much of it controversial, but the main point is that if you want to craft effective prompts, you have to know your data. If your model believes “woman” and “beauty” are two different things because of the training data, that is something you’ll need to know to engineer better prompts. This is why finetuning in conjunction with prompt engineering is powerful. You can set the seed with particular phrases and choice of words when finetuning and then use prompt engineering to help the model recall the information based on using those same phrases and choice of words.</p>
</div>
<div class="readable-text" id="p91">
<h2 class="readable-text-h2" id="sigil_toc_id_133"><span class="num-string">7.3</span> Prompt engineering tooling</h2>
</div>
<div class="readable-text" id="p92">
<p>If you are building any application that is more than just a wrapper around the LLM itself, you will want to do a bit of prompt engineering to inject function or personality into it. We’ve already gone over the basics of prompt engineering itself, but when building, it would be helpful to have some tools at your disposal to know how to make it all work. To that extent, let’s look at some of the most prominent tooling available and how to use them.</p>
</div>
<div class="readable-text" id="p93">
<h3 class="readable-text-h3" id="sigil_toc_id_134"><span class="num-string">7.3.1</span> LangChain</h3>
</div>
<div class="readable-text" id="p94">
<p>Anyone who’s built an LLM application before has probably spent some time working with LangChain. One of the most popular libraries, it’s known for extracting away all the complexity—and simplicity—of building a language application. It is known for its ease of creating language chains with what it calls the LangChain Expression Language (LCEL). </p>
</div>
<div class="readable-text intended-text" id="p95">
<p>LCEL makes it easy to build complex chains from basic components. In the next listing, we demonstrate creating a very simple chain that creates a prompt from a template, sends it to an LLM model, and then parses the results, turning it into a string.</p>
</div>
<div class="browsable-container listing-container" id="p96">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.1</span> Example of creating a basic LangChain chain </h5>
<div class="code-area-container">
<pre class="code-area">import os
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


prompt = ChatPromptTemplate.from_template("Tell me a story about {topic}")
model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=OPENAI_API_KEY)
output_parser = StrOutputParser()

chain = prompt | model | output_parser

chain.invoke("the printing press")</pre>
</div>
</div>
<div class="readable-text" id="p97">
<p>To be honest, using LangChain for something like this is a bit of overkill for what is essentially an f-string prompt, but it demonstrates what is happening under the hood. For the most part, you are likely going to use one of the many chains already created by the community. In the next chapter, we will explain how to create a RAG system with the RetrievalQA chain, but many more chains are available. For example, there are chains for generating and running SQL, interacting with APIs, and generating synthetic data. </p>
</div>
<div class="readable-text intended-text" id="p98">
<p>Once we have a chain, additional tools in the LangChain ecosystem help provide a more complete user experience. We can use LangServe to easily host it as an API. We can also use LangSmith, an in-depth logging tool that allows us to trace a chain invocation and see how the results change passing through each link in the chain.</p>
</div>
<div class="readable-text intended-text" id="p99">
<p>Chains don’t have to be linear like they are in this example. Several asynchronous components allow you to create a whole slew of complicated language processing logic. Ultimately, chains are just another type of data pipeline or DAG, except specialized for language models.</p>
</div>
<div class="readable-text" id="p100">
<h3 class="readable-text-h3" id="sigil_toc_id_135"><span class="num-string">7.3.2</span> Guidance</h3>
</div>
<div class="readable-text" id="p101">
<p>Guidance is an open source library from Microsoft that enforces programmatic responses. We’ve heard from several developers that the best engineering when working with LLMs is the good ol’ prompt-and-pray method. Generate a prompt, and pray that it works. Guidance seeks to solve that problem and has tooling to constrain the response space and set custom stopping tokens, as well as complex templating. After looking at dozens of LangChain projects, we believe Guidance is likely what most people are looking for when considering prompt engineering tooling.</p>
</div>
<div class="readable-text intended-text" id="p102">
<p>Guidance allows you to control the flow of generated responses. It’ll be easiest to show you what we mean. In listing 7.2, you’ll see several of the basic building blocks of Guidance where we can guide our LLM to respond in very specific ways—namely, loading a model with the guidance HF wrapper (models) and using the <code>gen</code> function to generate specific text and constraints like <code>select</code>.</p>
</div>
<div class="browsable-container listing-container" id="p103">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.2</span> Guidance basics</h5>
<div class="code-area-container">
<pre class="code-area">from guidance import models, gen, select

falcon = models.Transformers("tiiuae/falcon-rw-1b")   <span class="aframe-location"/> #1

lm = falcon + "Once upon a time, " + gen(max_tokens=10)    <span class="aframe-location"/> #2
print(lm)  # Once upon a time, there was a little girl who was very shy.

lm = (        
    falcon
    + "Write a sentence about the printing press. "
    + gen(stop=["\n", ".", "!"])                      <span class="aframe-location"/> #3
)
print(lm)                                                    <span class="aframe-location"/> #4
# The printing press was invented by Johannes Gutenberg in 1450

lm = falcon + "1, 2, 3," + gen(max_tokens=50, stop="11")     <span class="aframe-location"/> #5
print(lm)
# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,

lm = falcon + "I like the color " + select(["cyan", "grey", "purple"])  <span class="aframe-location"/> #6
print(lm)  # I like the color purple

lm = falcon + "Generate an email: " + gen(regex="\w+@\w+.com")    <span class="aframe-location"/> #7
print(lm)  # Generate an email: theoreticaly@gmail.com</pre>
<div class="code-annotations-overlay-container">
     #1 Loads a Hugging Face Transformers model
     <br/>#2 Sets a token limit that is an actual limit
     <br/>#3 Sets stopping tokens
     <br/>#4 Writes a sentence about the printing press
     <br/>#5 Combines multiple limits
     <br/>#6 Generates a specific response from a list
     <br/>#7 Uses regular expressions to ensure the response matches a pattern
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p104">
<p>With these basic building blocks that allow us to constrain the LLM’s response space, we are then able to create grammars. Grammars are a Guidance concept, and as the name implies, are language rules your model will have to follow. Grammars are composable and reusable and allow us to build neat applications quickly. In the next listing, we show you how to build simple parts of a speech application using guidance grammars. To create a grammar, we only need to create a function using the <code>@guidance</code> decorator.</p>
</div>
<div class="browsable-container listing-container" id="p105">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.3</span> Building a parts-of-speech app with Guidance </h5>
<div class="code-area-container">
<pre class="code-area">import guidance
from guidance import models, select

falcon = models.Transformers("tiiuae/falcon-rw-1b")    <span class="aframe-location"/> #1


@guidance(stateless=True)       <span class="aframe-location"/> #2
def parts_of_speech(lm):
    return lm + select(["Noun", "Verb", "Adjective", "Adverb", ""])
lm = (
    falcon
    + "The child plays with a red ball. Ball in the previous sentence is a "
    + parts_of_speech()
)
print(lm)  # Noun


@guidance(stateless=True)
def pos_constraint(lm, sentence):
    words = sentence.split()
    for word in words:
        lm += word + ": " + parts_of_speech() + "\n"
    return lm


@guidance(stateless=True)
def pos_instruct(lm, sentence):
    lm += f"""
    Tag each word with their parts of speech.
    Example:
    Input: The child plays with a red ball.
    Output:
    The: 
    child: Noun
    plays: Verb
    with: 
    a: 
    red: Adjective
    ball.: Noun
    ---
    Input: {sentence}
    Output:
    """
    return lm


sentence = "Good software makes the complex appear to be simple"
lm = falcon + pos_instruct(sentence) + pos_constraint(sentence)</pre>
<div class="code-annotations-overlay-container">
     #1 Loads a Hugging Face Transformers model
     <br/>#2 Creates functions to easily implement grammars
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p106">
<p>Even though we are using a small language model, we get the exact output we’d expect. We no longer need to prompt and pray. Granted, the results in the actual parts of speech prediction aren’t that great, but we could easily improve that by using a more powerful LLM or finetuning on more representative data:</p>
</div>
<div class="browsable-container listing-container" id="p107">
<div class="code-area-container">
<pre class="code-area">print(lm)</pre>
</div>
</div>
<div class="readable-text" id="p108">
<p>The generated text is</p>
</div>
<div class="browsable-container listing-container" id="p109">
<div class="code-area-container">
<pre class="code-area"># Input: Good software makes the complex appear to be simple
# Output:
# Good:
# software:
# makes: Verb
# the:
# complex: Adjective
# appear: Adjective
# to:
# be: Verb
# simple: Adjective</pre>
</div>
</div>
<div class="readable-text" id="p110">
<p>Guidance isn’t as popular as LangChain, and at least at the time of this writing, its documentation leaves a lot to be desired, so you might find it a bit harder to get started. However, it has a thriving community of its own with a strong core group of developers who continue to support it. We highly recommend checking it out.</p>
</div>
<div class="readable-text" id="p111">
<h3 class="readable-text-h3" id="sigil_toc_id_136"><span class="num-string">7.3.3</span> DSPy</h3>
</div>
<div class="readable-text" id="p112">
<p>Unlike other toolings mentioned, DSPy does not give you tools to create your own prompts; rather, it attempts to program prompting. DSPy, coming out of Stanford and heavily backed by many corporate sponsors, takes a unique approach by emphasizing tool augmentation, including retrieval, and is helpful if you would like to treat LLMs as deterministic and programmatic tools instead of emergent infinite syntax generators. </p>
</div>
<div class="readable-text intended-text" id="p113">
<p>Although this is not exactly what happens, you can think of DSPy as taking a similar logic to prompting that ONNX takes to saving models. Give it some dummy inputs, and it’ll compile a graph that can then infer prompts that work the best for your model and return the results you’re wanting. There’s a bit more work involved, though. You need to write validation logic and modules, essentially a workflow and unit tests, to check against. This effectively changes the dynamic from coming up with clever strings to something much closer to engineering software. Admittedly, it leaves open the question, “If you’re going to define everything programmatically anyway, why are you using an LLM?” Still, we’ve had good experiences with this and use it frequently.</p>
</div>
<div class="readable-text intended-text" id="p114">
<p>The steps to using DSPy effectively are as follows: </p>
</div>
<ol>
<li class="readable-text" id="p115"> Create a signature or a description of the task(s) along with input and output fields. </li>
<li class="readable-text" id="p116"> Create a predictor or generation style similar to chain of thought or retrieval. </li>
<li class="readable-text" id="p117"> Define the module or program. </li>
</ol>
<div class="readable-text" id="p118">
<p>Once these steps have been completed, you’ll compile the program. This will update the module based on the examples given before, similar to the training set. All of this will feel like machine learning for LLMs, with a training set (examples), a loss function (validation metric), and essentially an optimizer (teleprompter). </p>
</div>
<div class="readable-text intended-text" id="p119">
<p>In lieu of writing another listing for this chapter showcasing another tool, we decided to point you to an excellent notebook created by the StanfordNLP team introducing DSPy along with local LLMs and custom datasets: <a href="https://mng.bz/PNzg">https://mng.bz/PNzg</a> (it’s forked from here: <a href="http://mng.bz/Ea4r">http://mng.bz/Ea4r</a>). Once you have a chance to explore this example, we also recommend checking out the DSPy documentation, as it has many more excellent examples.</p>
</div>
<div class="readable-text" id="p120">
<h3 class="readable-text-h3" id="sigil_toc_id_137"><span class="num-string">7.3.4</span> Other tooling is available but …</h3>
</div>
<div class="readable-text" id="p121">
<p>Beyond the previously mentioned tools, a whole host of tools are out there. A couple to note are MiniChain and AutoChain. Both aim to be lightweight alternatives to LangChain, which are sorely needed, as many complain about LangChain’s bulkiness. Promptify is an interesting project that is a full-feature alternative to LangChain. To be honest, we could list a dozen more, but there likely isn’t much point. While many of these projects drew vibrant communities to them when they started, most have been dormant for months already, with only the rare GitHub contribution. </p>
</div>
<div class="readable-text intended-text" id="p122">
<p>It’s hard to say exactly why the interest in these projects faltered, but one obvious reason is that most of these projects lacked the sponsorship that LangChain, Guidance, and DSPy have. Many of these projects started as personal projects in the middle of big waves from the hype of ChatGPT’s success, but hype energy is never enough to build software that lasts. Without proper backing, most open source projects fail.</p>
</div>
<div class="readable-text intended-text" id="p123">
<p>We’ve probably painted too bleak a picture. As of the time of this writing, though, it’s still too early to tell, and this space is still a growing sector. There are still plenty of interesting tools we recommend checking out that we just don’t have space to include, like Haystack, Langflow, and Llama Index. Outlines is particularly of note as a similar project to Guidance, which is also awesome. We mostly want to point out that readers should be careful when picking tooling in this space because everything is still so new. If you find a tool you like, contribute.</p>
</div>
<div class="readable-text" id="p124">
<h2 class="readable-text-h2" id="sigil_toc_id_138"><span class="num-string">7.4</span> Advanced prompt engineering techniques</h2>
</div>
<div class="readable-text" id="p125">
<p>No matter how well designed your prompt is, there will be pragmatic context your model won’t have access to. For example, current events are a struggle. The model itself will only know about information up to its training date. Sure, we could feed that context in with RAG, as we’ve done so far, but that just shifts the burden to keeping our RAG system up to date. There’s another way. In this section, we will discuss giving models access to tools and what we can do with them once we do.</p>
</div>
<div class="readable-text" id="p126">
<h3 class="readable-text-h3" id="sigil_toc_id_139"><span class="num-string">7.4.1</span> Giving LLMs tools</h3>
</div>
<div class="readable-text" id="p127">
<p>What if instead of a complicated prompt engineering system, we instead give our model access to the internet? If it knows how to search the internet, it can always find up-to-date information. While we are at it, we can give it access to a calculator so we don’t have to waste CPU cycles having the LLM itself do basic math. We can give it access to a clock so it knows the current time and maybe even a weather app so it can tell the weather. The sky’s the limit! We just need to train the model on how to use tools, and that’s where Toolformers comes in.<a href="#footnote-110"><sup class="footnote-reference" id="footnote-source-8">8</sup></a></p>
</div>
<div class="readable-text intended-text" id="p128">
<p>Toolformers is a marvelously simple idea. Let’s train a model to know it can run API calls to different tools using tags like <code>&lt;API&gt;&lt;/API&gt;</code>. Then, at inference, when we see these tags, we can tell our interpreter to run those API calls. If that sounds familiar, it’s because Toolformers just trained a model to use string interpolation! String interpolation is the process of evaluating a string literal containing placeholders, which are replaced with the actual values at run time. For example, in Python, we could take the string literal <code>print(f'2+2</code> <code>=</code> <code>{2+2}')</code>, and once printed, we’d get <code>'2+2</code> <code>=</code> <code>4'</code>. The placeholder <code>{2+2}</code> was evaluated and executed as Python code, returning <code>4</code>. Schick et al. finetuned a GPT-J model to use five different tools: a question-answering database, a calculator, a Wikipedia search, a translator, and a calendar. With access to these tools, they were able to achieve impressive results, outperforming GPT-3 on many tasks.</p>
</div>
<div class="readable-text intended-text" id="p129">
<p>While Schick et al.’s work paved the way, the major downside to this approach is that we don’t want to finetune a model every time we create a new tool. However, as we’ve discussed in this chapter, we don’t have to. Instead, we can use clever prompt engineering to introduce new tools using LangChain or Guidance. In the next listing, we demonstrate how to create simple math tools with Guidance. Guidance takes care of the heavy lifting by stopping generation when it recognizes a tool being called, running the tool, and starting generation again.</p>
</div>
<div class="browsable-container listing-container" id="p130">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.4</span> Giving tools to our LLM models with Guidance</h5>
<div class="code-area-container">
<pre class="code-area">import guidance
from guidance import models, gen

falcon = models.Transformers("tiiuae/falcon-rw-1b")    <span class="aframe-location"/> #1


@guidance
def add(lm, input1, input2):
    lm += f" = {int(input1) + int(input2)}"
    return lm


@guidance
def subtract(lm, input1, input2):
    lm += f" = {int(input1) - int(input2)}"
    return lm


@guidance
def multiply(lm, input1, input2):
    lm += f" = {float(input1) * float(input2)}"
    return lm


@guidance
def divide(lm, input1, input2):
    lm += f" = {float(input1) / float(input2)}"
    return lm

lm = (
    falcon
    + """\
1 + 2 = add(1, 2) = 3
4 - 5 = subtract(4, 5) = -1
5 * 6 = multiply(5, 6) = 30
7 / 8 = divide(7, 8) = 0.875
Generate more examples of add, subtract, multiply, and divide
"""
)
lm += gen(max_tokens=15, tools=[add, subtract, multiply, divide])
print(lm)</pre>
<div class="code-annotations-overlay-container">
     #1 Loads a Hugging Face Transformers model
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p131">
<p>While a simple example, it’s easy to imagine building more advanced tooling. Regardless of whether you use LangChain or Guidance, there are a few things to keep in mind when building tools. First, you’ll need to instruct your model in the prompt on where and how to use the tools you give it. This can be more or less difficult, depending on how open-ended your function is. Second, your model matters in its ease of extendibility. Some models we’ve worked with would never use the tools we gave them or would even hallucinate other tools that didn’t exist. Lastly, be really careful with the inputs and error handling for tools you give an LLM. The ones we used previously in this chapter are terrible and likely to break in several ways. For example, an LLM could easily try to run <code>add(one,</code> <code>two)</code> or <code>add(1,</code> <code>2,</code> <code>3)</code>, both of which would throw errors and crash the system. With Guidance, to make this easier, we can enforce tool inputs by building grammars to ensure our model inputs are always correct.</p>
</div>
<div class="readable-text intended-text" id="p132">
<p>This discussion leads us to uncover some problems with LLMs using tools. First, we have to be careful what tools we give to an LLM since we never really know what input it will generate. Even if we ensure the tool doesn’t break, it may do something malicious we didn’t intend. Second, as you’ve probably gathered throughout this chapter, prompt engineering quickly grows our input and thus shrinks the token limit for our actual users; explaining tools and how to use them adds to that constraint. Often, this limitation reduces the number of tools we can give an LLM and, thus, its usefulness. Third, LLMs are still hit or miss as to whether they actually use a tool and can often end up using the wrong tool. For example, should the LLM use the web search tool or the weather tool to look up the 10-day forecast? This might not matter much to us as humans, but results can vary widely for a bot. Lastly, building tools can be difficult and error prone, as you need to build both a clean tool and an effective prompt.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p133">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">OpenAI’s plugins</h5>
</div>
<div class="readable-text" id="p134">
<p>Toolformers opened the gates to OpenAI’s Plugins concept (<a href="https://mng.bz/q0rE">https://mng.bz/q0rE</a>). Plugins allow third parties to easily integrate their tools into ChatGPT and provide a simple way for ChatGPT to call external APIs. Plugins were introduced relatively early in ChatGPT’s life, shortly after the Toolformers paper.<sup>a</sup> All a third party had to do was create an OpenAPI config file and an ai-plugin.json file and host both where the API existed. OpenAPI is a specification language for APIs that standardizes and defines your API to make it easy for others to consume. (If you haven’t heard of OpenAPI and have APIs that customers use, it’s a good practice to follow. You can learn more at <a href="https://www.openapis.org/">https://www.openapis.org/</a>.) Plenty of tools can help you generate that file easily enough. The ai-plugin file created the plugin. Here, you could define a name for the plugin, how authentication should happen, and descriptions to be used to prompt ChatGPT. From here, the plugin could be registered with OpenAI in ChatGPT’s interface, and after a review process, your plugin could be added and used by users as they interacted with ChatGPT.</p>
</div>
<div class="readable-text" id="p136">
<p>Despite an initial fervor, plugins never left Beta—beyond OpenAI’s own web browsing plugin—and appear to be abandoned. There are lots of reasons for this, but in a since-taken-down report, the main reason came from Sam Altman when he suggested, “A lot of people thought they wanted their apps to be inside ChatGPT, but what they really wanted was ChatGPT in their apps” (<a href="https://mng.bz/75Dg">https://mng.bz/75Dg</a>). As a result, there didn’t seem to be a product market fit for OpenAI’s plugins that would make the company money. But we think it’s too early to abandon the idea entirely.</p>
</div>
<div class="readable-text" id="p137">
<p>As more companies integrate LLM technology into their apps, they are likely going to want access to third-party tools. Suppose you are going camping for the first time and you ask an LLM shopping assistant for advice on what to buy. In that case, it’d be really nice if it thought first to ask where and when you were going camping and then could use that information to identify weather-appropriate gear. The LLM shopping assistant for a particular brand or store is likely to have access to loads of products, but access to weather reports in a random geolocation? Not so much. </p>
</div>
<div class="readable-text" id="p138">
<p>While you can always build these tools, wouldn’t it be great if they were already created for you, and you could simply go to some hub, download the ones you wanted, and plug them in? Unfortunately, this option doesn’t exist yet, at least not to the extent we describe it here. We have high expectations that a marketplace or hub of some kind will be created in the future, like OpenAI’s plugins, that can be used with any LLM model. LLMs are still a new technology, and the ecosystems to be built around them are still forthcoming; we believe this will be one of them.</p>
</div>
<div class="readable-text sidebar-footer" id="p139">
<p><span class="num-string"><sup>a</sup></span> <span class="aframe-location"/>T. Schick et al., “Toolformer: Language models can teach themselves to use tools,” February 2023. </p>
</div>
</div>
<div class="readable-text" id="p140">
<p>Once we give our LLMs access to tools, it opens the gates to lots of cool prompt engineering techniques. Probably the most famous is the ReAct method.</p>
</div>
<div class="readable-text" id="p141">
<h3 class="readable-text-h3" id="sigil_toc_id_140"><span class="num-string">7.4.2</span> ReAct</h3>
</div>
<div class="readable-text" id="p142">
<p>Reasoning and Acting (ReAct) is a few-shot framework for prompting that is meant to emulate how people reason and make decisions when learning new tasks.<a href="#footnote-111"><sup class="footnote-reference" id="footnote-source-9">9</sup></a> <span class="aframe-location"/>It involves a multistep process for the LLM, where a question is asked, the model determines an action, and then it observes and reasons upon the results of that action to determine subsequent actions.</p>
</div>
<div class="readable-text intended-text" id="p143">
<p>An example could look like this:</p>
</div>
<div class="readable-text prompt" id="p144">
<p><span class="prompt-head"><span class="prompt-initials">CB</span> <b>Question:</b></span> What is the airspeed velocity of an unladen African swallow compared to a European swallow with the same load?</p>
</div>
<div class="readable-text response" id="p145">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Thought 1:</b></span> I need to search for airspeed velocity of a European swallow so I can compare it with an African swallow.</p>
</div>
<div class="readable-text response" id="p146">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Action 1:</b></span> Search[European Swallow airspeed velocity]</p>
</div>
<div class="readable-text response" id="p147">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Observation 1:</b></span> We need to know the Strouhal number in order to determine airspeed. The bird's Strouhal number converges between 0.2 and 0.4.</p>
</div>
<div class="readable-text response" id="p148">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Thought 2:</b></span> The airspeed velocity is not directly mentioned, but the Strouhal number is clear. So I need to look up velocity compared to the Strouhal number.</p>
</div>
<div class="readable-text response" id="p149">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Action 2:</b></span> Search[Strouhal wingbeat calculation]</p>
</div>
<div class="readable-text response" id="p150">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Observation 2:</b></span> (Result 1/1) A Swallow’s Strouhal number translates to ~12 wingbeats per second.</p>
</div>
<div class="readable-text response" id="p151">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Thought 3:</b></span> An African swallow requires 12 wingbeats per second to maintain airspeed velocity, so I need to look up how fast that makes it.</p>
</div>
<div class="readable-text response" id="p152">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Action 3:</b></span> Lookup[African Swallow wingbeat per second]</p>
</div>
<div class="readable-text response" id="p153">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Observation 3:</b></span> Each wingbeat propels an African swallow 2.5 feet per flap, making it go about 40.2mph</p>
</div>
<div class="readable-text response" id="p154">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Thought 4:</b></span> I need to look up the European swallow to compare it.</p>
</div>
<div class="readable-text response" id="p155">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Action 4:</b></span> Search[European Swallow Wingbeat per second]</p>
</div>
<div class="readable-text response" id="p156">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Observation 4:</b></span> A European swallow is about half the size of an African swallow, making its velocity about 20.1</p>
</div>
<div class="readable-text response" id="p157">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Thought 5:</b></span> An African swallow has an airspeed velocity of 40.2, and a European swallow has an airspeed velocity of 20.1, making the comparison 2x.</p>
</div>
<div class="readable-text response" id="p158">
<p><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> <b>Action 5:</b></span> Finish[Two times the airspeed velocity]</p>
</div>
<div class="readable-text" id="p159">
<p>As you can see, the purpose of ReAct is to force the model to think before it acts. This isn’t much different from the other prompting methods we have discussed. The big difference is that we allow the model to take actions. In our example, this included a “Search” action, or essentially an ability to look up information on the internet as a human would. We just showed you how to do this in the last section. The model can take that new information and observe what it learns from its actions to produce a result. </p>
</div>
<div class="readable-text intended-text" id="p160">
<p>Let’s explore this further with an example. We will use LangChain, which will make creating a ReAct agent seem a lot easier than it actually is. Listing 7.5 shows how to utilize ReAct on an OpenAI model and LangChain. For our search engine, we will be utilizing serper.dev, as it integrates nicely with LangChain, and it offers a free tier you can sign up for. We will also need to use the calculator <code>"llm-math"</code>, which is one of the many tools in LangChain’s toolbelt.</p>
</div>
<div class="browsable-container listing-container" id="p161">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.5</span> Example ReAct with Langchain</h5>
<div class="code-area-container">
<pre class="code-area"> import os
from langchain.llms import OpenAI
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from dotenv import load_dotenv

load_dotenv()

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")    <span class="aframe-location"/> #1
os.environ["SERPER_API_KEY"] = os.getenv("SERPER_API_KEY")

llm = OpenAI(model_name="text-davinci-003", temperature=0)
tools = load_tools(["google-serper", "llm-math"], llm=llm)
agent = initialize_agent(
    tools, llm, agent="zero-shot-react-description", verbose=True
)

agent.run(
    "Who is Olivia Wilde's boyfriend? \
    What is his current age raised to the 0.23 power?"
)</pre>
<div class="code-annotations-overlay-container">
     #1 Loads API keys; you will need to obtain these if you haven't yet
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p162">
<p>The output is</p>
</div>
<div class="browsable-container listing-container" id="p163">
<div class="code-area-container">
<pre class="code-area"># &gt; Entering new AgentExecutor chain...
# I need to find out who Olivia Wilde's boyfriend is and then
# calculate his age raised to the 0.23 power.
# Action: Search
# Action Input: "Olivia Wilde boyfriend"
# Observation: Olivia Wilde started dating Harry Styles after ending
# her years-long engagement to Jason Sudeikis — see their relationship
# timeline.
# Thought: I need to find out Harry Styles' age.
# Action: Search
# Action Input: "Harry Styles age"
# Observation: 29 years
# Thought: I need to calculate 29 raised to the 0.23 power.
# Action: Calculator
# Action Input: 29^0.23
# Observation: Answer: 2.169459462491557

# Thought: I now know the final answer.
# Final Answer: Harry Styles, Olivia Wilde's boyfriend, is 29 years old
# and his age raised to the 0.23 power is 2.169459462491557.

# &gt; Finished chain.

# "Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age
# raised to the 0.23 power is 2.169459462491557."</pre>
</div>
</div>
<div class="readable-text" id="p164">
<p>Listing 7.5 shows how ReAct can be used with an LLM in conjunction with particular agent tools like <code>"google-serper"</code> and <code>"llm-math"</code> to help augment your prompts. Prompt engineering looks more like a full-time job now, not just “coming up with words,” huh?</p>
</div>
<div class="readable-text intended-text" id="p165">
<p>Knowing how to build tools and combine them to prompt LLMs to answer more in-depth questions is a growing field of study as well as an expanding part of the job market. To be perfectly honest, the rate of change in the prompt engineering field seems to drastically outpace most of the other topics we cover in this book. There’s a lot more to be discussed that we simply can’t cover in this book, so much so, in fact, that there are now entire books in and of themselves being written to this end. It was difficult to determine what would be valuable to our readers and what would be outdated quickly, but we think we’ve found a good balance and encourage you to look forward to researching more on the topic.</p>
</div>
<div class="readable-text intended-text" id="p166">
<p>Overall, we’ve learned a lot throughout this chapter—how to craft a prompt and how to implement prompting in an engineering fashion. In the next chapter, we will put all of this knowledge to good use when we build LLM applications users can interact with.</p>
</div>
<div class="readable-text" id="p167">
<h2 class="readable-text-h2" id="sigil_toc_id_141">Summary</h2>
</div>
<ul>
<li class="readable-text buletless-item" id="p168"> The most straightforward approach to prompting is to give a model examples of what you want it to do: 
    <ul>
<li> The more examples you can add to a prompt, the more accurate your results will be. </li>
<li> The fewer examples you need to add, the more general and all-purpose your prompt will be. </li>
</ul></li>
<li class="readable-text buletless-item" id="p169"> The four parts of a prompt are 
    <ul>
<li> <em>Input</em> —What the user writes </li>
<li> <em>Instruction</em> —The template with task-specific information encoded </li>
<li> <em>Context</em> —The information you add through RAG or other database lookups </li>
<li> <em>System</em> —The specific instructions given for every task; should be hidden from the user </li>
</ul></li>
<li class="readable-text" id="p170"> Knowing your training data will help you craft better prompts by choosing a word order that matches the training data. </li>
<li class="readable-text" id="p171"> LangChain is a popular tool that allows us to create chains or pipelines to utilize LLMs in an engineering fashion. </li>
<li class="readable-text" id="p172"> Guidance is a powerful tool that gives us more fine-grained control over the LLMs’ actual generated text. </li>
<li class="readable-text" id="p173"> Toolformers teach LLMs how to use tools, giving them the ability to accomplish previously impossible tasks. </li>
<li class="readable-text" id="p174"> ReAct is a few-shot framework for prompting that is meant to emulate how people reason and make decisions when learning new tasks. </li>
</ul>
<div class="readable-text footnote-readable-text" id="p175">
<p><a href="#footnote-source-1"><span class="footnote-definition" id="footnote-103">[1]</span></a> J. Wei et al., “Chain of thought prompting elicits reasoning in large language models,” January 2022, <a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p176">
<p><a href="#footnote-source-2"><span class="footnote-definition" id="footnote-104">[2]</span></a> T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large Language models are zero-shot reasoners,” May 2022, <a href="https://arxiv.org/abs/2205.11916">https://arxiv.org/abs/2205.11916</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p177">
<p><a href="#footnote-source-3"><span class="footnote-definition" id="footnote-105">[3]</span></a> Y. Zhou et al., “Thread of thought unraveling chaotic contexts,” November 15, 2023, <a href="https://arxiv.org/abs/2311.08734">https://arxiv.org/abs/2311.08734</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p178">
<p><a href="#footnote-source-4"><span class="footnote-definition" id="footnote-106">[4]</span></a> Sondos Mahmoud Bsharat, Aidar Myrzakhan, and Z. Shen, “Principled instructions are all you need for questioning LLaMA-1/2, GPT-3.5/4,” December 2023, <a href="https://doi.org/10.48550/arxiv.2312.16171">https://doi.org/10.48550/arxiv.2312.16171</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p179">
<p><a href="#footnote-source-5"><span class="footnote-definition" id="footnote-107">[5]</span></a> C. Yang et al., “Large language models as optimizers,” September 6, 2023, <a href="https://arxiv.org/abs/2309.03409">https://arxiv.org/abs/2309.03409</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p180">
<p><a href="#footnote-source-6"><span class="footnote-definition" id="footnote-108">[6]</span></a> N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman, “DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation,” August 2022, <a href="https://arxiv.org/abs/2208.12242">https://arxiv.org/abs/2208.12242</a></p>
</div>
<div class="readable-text footnote-readable-text" id="p181">
<p><a href="#footnote-source-7"><span class="footnote-definition" id="footnote-109">[7]</span></a> Ruiz et al., “DreamBooth.”</p>
</div>
<div class="readable-text footnote-readable-text" id="p182">
<p><a href="#footnote-source-8"><span class="footnote-definition" id="footnote-110">[8]</span></a> T. Schick et al., “Toolformer: Language models can teach themselves to use tools,” February 2023, <a href="https://arxiv.org/abs/2302.04761">https://arxiv.org/abs/2302.04761</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p183">
<p><a href="#footnote-source-9"><span class="footnote-definition" id="footnote-111">[9]</span></a> S. Yao et al., “ReAct: Synergizing reasoning and acting in language models,” March 10, 2023, <a href="https://arxiv.org/abs/2210.03629">https://arxiv.org/abs/2210.03629</a>.</p>
</div>
</div></body></html>