["```py\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\n```", "```py\n# Load the PDF file\nloader = PyPDFLoader(pdf_path)\ndocuments = loader.load()\n```", "```py\n# Split the documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len,\n    add_start_index=True,\n)\n```", "```py\ntexts = text_splitter.split_documents(documents)\n```", "```py\n# Initialize OpenAI embeddings\n# Make sure to set your OPENAI_API_KEY environment variable\nembeddings = OpenAIEmbeddings()\n\n# Create and persist the vector store\nvectorstore = Chroma.from_documents(\n    documents=texts,\n    embedding=embeddings,\n    persist_directory=persist_directory\n)\n```", "```py\n# Persist the vector store\nvectorstore.persist()\n```", "```py\ndef search_vectorstore(vectorstore, query, k=3):\n    results = vectorstore.similarity_search(query, k=k)\n    return results\n```", "```py\n# Path to your PDF file\npdf_path = \"space-cadets-2020-master.pdf\"\n\n# Create the vector store\nvectorstore = create_vectorstore(pdf_path)\n\n# Example search\nquery = \"Give me some details about Soo-Kyung Kim. \n         Where is she from, what does she like, tell me all about her?\"\nresults = search_vectorstore(vectorstore, query, 5)\n```", "```py\n“I think we are going to be good friends,” said Soo-Kyung. “I like how\nyou are straightforward. I am too, but that intimidates some people.”\n“So where are you from?”\n“I am from a small village called Sijungho,” continued Soo -\nKyung. “There’s not much to see there.”\n“Sounds Korean,” said Aisha. “You from South Korea?”\n“North Korea,” corrected Soo -Kyung. “I’ve never even been to\nSouth Korea.”\n\n```", "```py\ndef load_vectorstore(persist_directory=\"./chroma_db\"):\n    embeddings = OpenAIEmbeddings()\n\n    # Load existing vector store\n    vectorstore = Chroma(\n        persist_directory=persist_directory,\n        embedding_function=embeddings\n    )\n\n    return vectorstore\n```", "```py\ndef search_vectorstore(vectorstore, query, k=3):\n    results = vectorstore.similarity_search(query, k=k)\n    return results\n```", "```py\nquery = \"Please tell me all about Soo-Kyung Kim.\"\n```", "```py\n# Example query\nquery = \"Please tell me all about Soo-Kyung Kim.\"\n\n# Perform RAG query\nanswer, sources = rag_query(vectorstore, query, num_contexts=10)\n```", "```py\ndef rag_query(vectorstore, query, num_contexts=3):\n    # Retrieve relevant documents\n    relevant_docs = search_vectorstore(vectorstore, query, k=num_contexts)\n\n    # Combine context from retrieved documents\n    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n\n    # Generate response using Ollama\n    response = query_ollama(query, context)\n\n    return response, relevant_docs\n```", "```py\ndef query_ollama(prompt, context, model=\"llama3.1:latest\", temperature=0.7):\n\n    ollama_url = \"http://localhost:11434/api/chat\"\n```", "```py\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful AI assistant. \n                        Use the provided context to answer questions. \n                        If you cannot find the answer in the context, say so. \n                        Only use information from the provided context.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Context:\\n{context}\\n\\nQuestion: {prompt}\"\n        }\n    ]\n```", "```py\n    payload = {\n        \"model\": model,\n        \"messages\": messages,\n        \"stream\": False,\n        \"temperature\": temperature\n    }\n```", "```py\n    try:\n        response = requests.post(ollama_url, json=payload)\n        response.raise_for_status()\n        return response.json()[\"message\"][\"content\"]\n    except requests.exceptions.RequestException as e:\n        return f\"Error querying Ollama: {str(e)}\"\n```", "```py\nBased on the provided context, here's what can be gathered about Soo-Kyung Kim:\n\n1. She is from North Korea.\n2. She has been trained in various skills, including science, technology, \nmartial arts, languages, piloting, and strategy.\n3. Her family name \"Kim\" is significant, as it is the name of the ruling family \n   of the Democratic People's Republic of Korea (North Korea).\n4. Soo-Kyung's presence on the space academy may be related to her exceptional \n   abilities, but there is also a suggestion that she was chosen for other \n   reasons.\n…\n```", "```py\nfrom langchain_openai import ChatOpenAI\n```", "```py\nchat = ChatOpenAI(\n    model=model,\n    temperature=temperature\n)\n```", "```py\n# Create prompt template\nprompt_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful AI assistant. \n                Use the following context to answer questions. \"\n               \"Please provide as much detail as possible in a comprehensive \n               answer.\"),\n    (\"system\", \"Context:\\n{context}\"),\n    (\"user\", \"{question}\")\n])\n```", "```py\n# Format the prompt with the context and question\nformatted_prompt = prompt_template.format(\n    context=context,\n    question=prompt\n)\n\n```", "```py\n# Get the response\nresponse = chat.invoke(formatted_prompt)\nreturn response.content\n```"]