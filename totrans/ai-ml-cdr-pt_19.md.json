["```py\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\n```", "```py\n# Load the PDF file\nloader = PyPDFLoader(pdf_path)\ndocuments = loader.load()\n```", "```py\n# Split the documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len,\n    add_start_index=True,\n)\n```", "```py\ntexts = text_splitter.split_documents(documents)\n```", "```py\n# Initialize OpenAI embeddings\n# Make sure to set your OPENAI_API_KEY environment variable\nembeddings = OpenAIEmbeddings()\n\n# Create and persist the vector store\nvectorstore = Chroma.from_documents(\n    documents=texts,\n    embedding=embeddings,\n    persist_directory=persist_directory\n)\n```", "```py\n# Persist the vector store\nvectorstore.persist()\n```", "```py\ndef search_vectorstore(vectorstore, query, k=3):\n    results = vectorstore.similarity_search(query, k=k)\n    return results\n```", "```py\n# Path to your PDF file\npdf_path = \"space-cadets-2020-master.pdf\"\n\n# Create the vector store\nvectorstore = create_vectorstore(pdf_path)\n\n# Example search\nquery = \"Give me some details about Soo-Kyung Kim. \n         `Where` `is` `she` `from``,` `what` `does` `she` `like``,` `tell` `me` `all` `about` `her``?``\"` ```", "```py\n```", "```py`` ```", "```py “I think we are going to be good friends,” said Soo-Kyung. “I like how you are straightforward. I am too, but that intimidates some people.” “So where are you from?” “I am from a small village called Sijungho,” continued Soo - Kyung. “There’s not much to see there.” “Sounds Korean,” said Aisha. “You from South Korea?” “North Korea,” corrected Soo -Kyung. “I’ve never even been to South Korea.”   ```", "```py` ```", "```py```", "````py``` ````", "`````` # Using RAG Content with an LLM    Now that you’ve created a vector store and stored the book in it, let’s explore how you would read snippets back from the store, add them to a prompt, and get data back. We’ll use a local Ollama server to keep things simple. For more on Ollama, see [Chapter 17](ch17.html#ch17_serving_llms_with_ollama_1748550058915113).    First, let’s load the vector store that we created in the previous step:    ```py def load_vectorstore(persist_directory=\"./chroma_db\"):     embeddings = OpenAIEmbeddings()       # Load existing vector store     vectorstore = Chroma(         persist_directory=persist_directory,         embedding_function=embeddings     )       return vectorstore ```    You *must* use the same embeddings as those you used when you created the vector store. Otherwise, there will be a mismatch when you try to encode your prompt and search for stuff similar to it.    In this case, I’m using the OpenAIEmbeddings, but it’s entirely up to you how to approach this. There are many embeddings available in open source on Hugging Face, or you could use things like the GLoVE embeddings we explored in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888).    ChromaDB persisted the embeddings in an SQLite database at a specific directory. Make sure you embed that, and then all you have to do is pass this and your embedding function to Chroma to get a reference to your database.    To search the vector store, you’ll use the same code as earlier:    ```py def search_vectorstore(vectorstore, query, k=3):     results = vectorstore.similarity_search(query, k=k)     return results ```    Next, input a query. For example, input this:    ```py query = \"Please tell me all about Soo-Kyung Kim.\" ```    At this point, you have all the pieces you need to do a RAG query, which you can do like this:    ```py # Example query query = \"Please tell me all about Soo-Kyung Kim.\"   # Perform RAG query answer, sources = rag_query(vectorstore, query, num_contexts=10) ```    Here, you create a helper function that will pass the query and the vector store, and you also have a parameter with the number of items to find in the vector store. The app will return the answer (from the LLM) as well as a list of sources from the data that it used to augment the query.    Let’s explore this function in depth:    ```py def rag_query(vectorstore, query, num_contexts=3):     # Retrieve relevant documents     relevant_docs = search_vectorstore(vectorstore, query, k=num_contexts)       # Combine context from retrieved documents     context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])       # Generate response using Ollama     response = query_ollama(query, context)       return response, relevant_docs ```    You’ll start by searching the vector store with the code provided earlier. This will give you the decoded chunks from the datastore as strings, and you should call these `relevant_docs`.    You’ll then create the context string by joining the chunks together with some new line characters to separate them. It’s as simple as that.    Now, the query and the context will be used in a call to Ollama. Let’s see how that will work.    Start by defining the function:    ```py def query_ollama(prompt, context, model=\"llama3.1:latest\", temperature=0.7):       ollama_url = \"http://localhost:11434/api/chat\" ```    Here, you can set the function to accept the prompt and context. I’ve added a couple of optional parameters that, if they’re not set, will use the defaults. The first is the model. To get a list of available models on your server, you can just use “ollama list” from the command prompt. The `temperature` parameter indicates how deterministic your response will be: the smaller the number, the more deterministic the answer, and the higher the number, the more creative the answer. I set a default of 0.7, which gives some flexibility to the model to make it natural sounding while staying relevant. But when you use smaller models in Ollama (like `llama3.1`, as shown), it does make hallucination more likely.    You’ll also want to specify the `ollama_url` endpoint, as shown in [Chapter 17](ch17.html#ch17_serving_llms_with_ollama_1748550058915113).    Next, you create the messages that will be used to interact with the model.    The structure of conversations with a model typically looks like the one in [Figure 18-7](#ch18_figure_7_1748550073457720). The model will optionally be primed with a system message that gives it instructions on how to behave. It will then have an initial message that it emits to the user, like, “Welcome to the Chat. How can I help?” The user will then respond with a prompt asking the model to do something, to which the model will respond, and so on.  ![](assets/aiml_1807.png)  ###### Figure 18-7\\. Anatomy of a conversation with a model    The *memory* of the conversation will be a JSON document with each of the roles prefixed by a `role` value. The initial message will have the `system` role, the model messages will have the `model` role, and the user messages will have the `user` role.    So, for the simple RAG app we’re creating, we can create an instance of a conversation like this—passing the system message and the user message, which will be composed of the prompt and the context, like this:    ```py     messages = [         {             \"role\": \"system\",             \"content\": \"You are a helpful AI assistant.                          `Use` `the` `provided` `context` `to` `answer` `questions``.`                          `If` `you` `cannot` `find` `the` `answer` `in` `the` `context``,` `say` `so``.`                          `Only` `use` `information` `from` `the` `provided` `context``.``\"` ``` `},`         `{`             `\"role\"``:` `\"user\"``,`             `\"content\"``:` `f``\"Context:``\\n``{``context``}``\\n\\n``Question:` `{``prompt``}``\"`         `}`     `]` ```py ```   ```py``````", "``````py```` Depending on how you set up the system role, you’ll get very different behavior. In this case, I used a prompt that gets it to heavily focus on the provided context. You don’t *need* to do this, and by working with this prompt, you might get much better results.    Within the user role, this is just as simple as creating a string with `Context:` and `Question:` content that you paste the context and prompt into.    From this, you can now create a JSON payload to pass to Ollama that contains the desired model, the messages, the temperature, and the stream (which must be set to `False` if you want to get a single answer back):    ```py     payload = {         \"model\": model,         \"messages\": messages,         \"stream\": False,         \"temperature\": temperature     } ```    Note also that the desired model must be installed in Ollama or you’ll get an error, so see [Chapter 17](ch17.html#ch17_serving_llms_with_ollama_1748550058915113) for adding models to Ollama.    Then, you simply have to use an HTTP post to the Ollama URL, passing it the payload. When you get the response, you can query the returned message—where there’ll now be new content added by the model. This content will contain your answer!    ```py     try:         response = requests.post(ollama_url, json=payload)         response.raise_for_status()         return response.json()[\"message\"][\"content\"]     except requests.exceptions.RequestException as e:         return f\"Error querying Ollama: {str(e)}\" ```    In this case, I used Llama 3.1 and got some excellent answers. Here’s an example:    ```py Based on the provided context, here's what can be gathered about Soo-Kyung Kim: `1.` `She` `is` `from` `North` `Korea``.` `2.` `She` `has` `been` `trained` `in` `various` `skills``,` `including` `science``,` `technology``,`  `martial` `arts``,` `languages``,` `piloting``,` `and` `strategy``.` `3.` `Her` `family` `name` `\"Kim\"` `is` `significant``,` `as` `it` `is` `the` `name` `of` `the` `ruling` `family`     `of` `the` `Democratic` `People``'s Republic of Korea (North Korea).` ```` `4.` `Soo``-``Kyung``'s presence on the space academy may be related to her exceptional` ```py     `abilities``,` `but` `there` `is` `also` `a` `suggestion` `that` `she` `was` `chosen` `for` `other`     `reasons``.` `…` ``` ```py` ```   ```py``````", "```py```", "```py```", "```py from langchain_openai import ChatOpenAI ```", "```py chat = ChatOpenAI(     model=model,     temperature=temperature ) ```", "```py # Create prompt template prompt_template = ChatPromptTemplate.from_messages([     (\"system\", \"You are a helpful AI assistant.                  `Use` `the` `following` `context` `to` `answer` `questions``.` `\"` ```", "```py`                 `answer``.``\"),` ```", "```py ```", "```py`` ```", "```py```", "````` ```py`Then, you can make the formatted prompt with the details of the context and prompt:    ``` # Format the prompt with the context and question formatted_prompt = prompt_template.format(     context=context,     question=prompt )   ```py    Finally, you can invoke the GPT chat with the formatted prompt and get the response:    ``` # Get the response response = chat.invoke(formatted_prompt) return response.content ```py    Now, as long as you ensure that you have an `OPENAI_API_KEY` environment variable, as discussed earlier, you’re RAGging against GPT! Please pay attention to the pricing on OpenAI for using the available models.```` ```py`` `````", "``````py` ``````", "``````py``` ``````", "```` ```py````", "```py`  ```", "```py ```", "```py```", "``` ```", "`````````"]