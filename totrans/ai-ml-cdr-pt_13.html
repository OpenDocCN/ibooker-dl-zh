<html><head></head><body><section data-pdf-bookmark="Chapter 12. Concepts of Inference" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch12_concepts_of_inference_1748549754445405">
      <h1><span class="label">Chapter 12. </span>Concepts of Inference</h1>
      <p>In the previous chapters of this book, you focused on <em>training</em> models using PyTorch and on how to create models that manage images (aka Computer Vision), text content (aka NLP), and sequence modelling. For the rest of this book, you’ll cover a lot of content around <em>using trained models</em> to make predictions from new data (aka <em>inference</em>) and in particular using large generative models for text-to-text and text-to-image generative AI.</p>
      <p>But before you jump into that, it’s important for you to understand the underlying data transfer technology. We’ve touched on it a little in the training chapters, but as you go deeper into ML—in either training or inference—it’s important for you to be able to understand the underlying concepts of tensors. </p>
      <p>Ultimately, no matter what <a contenteditable="false" data-primary="inference" data-secondary="tensor data type into and out of model" data-type="indexterm" id="ch12tens"/><a contenteditable="false" data-primary="models" data-secondary="tensor data type into and out of" data-type="indexterm" id="ch12tens3"/><a contenteditable="false" data-primary="datasets" data-secondary="tensor data type into and out of models" data-type="indexterm" id="ch12tens4"/><a contenteditable="false" data-primary="tensors" data-secondary="tensor data type into and out of model" data-type="indexterm" id="ch12tens5"/>data type you have, you’ll convert it into tensors to pass it <em>into</em> the model. Similarly, no matter the data type in which you want to present answers from the model to your users, you’ll get them back as tensors as well!</p>
      <p>In many cases, you’ll have helper functions, such as the <em>transformers</em> that you’ll see in <a data-type="xref" href="ch15.html#ch15_transformers_and_transformers_1748549808974580">Chapter 15</a> (which covers LLMs) and the <em>diffusers</em> that you’ll see in <a data-type="xref" href="ch19.html#ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373">Chapter 19</a> (which handles image generation). And while you won’t be touching tensors with them, you’ll still be using them under the hood.</p>
      <section data-pdf-bookmark="Tensors" data-type="sect1"><div class="sect1" id="ch12_tensors_1748549754445648">
        <h1>Tensors</h1>
        <p>A <em>tenso</em>r is an array<a contenteditable="false" data-primary="tensors" data-type="indexterm" id="id1532"/><a contenteditable="false" data-primary="inference" data-secondary="tensor data type into and out of model" data-tertiary="about tensors" data-type="indexterm" id="id1533"/><a contenteditable="false" data-primary="deep learning" data-secondary="tensors for numerical data" data-type="indexterm" id="id1534"/> that can have any number of dimensions. Tensors are typically used to represent numerical data for deep-learning algorithms; they’re containers that can hold numbers in multiple dimensions.</p>
        <p>Tensors can be simple scalar values (in zero dimensions), vectors (in one dimension), matrices (in two dimensions), and beyond (in three dimensions or more). In PyTorch, they’re the fundamental data structure for all computation.</p>
        <p>Tensors are also the <a contenteditable="false" data-primary="Google TensorFlow" data-type="indexterm" id="id1535"/>source of the name <em>TensorFlow </em>for the alternative deep-learning framework from Google.</p>
        <p>Here are some examples of tensors in PyTorch, which were created using <code>torch.​ten⁠sor</code>:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>
 
<code class="c1"># Scalar (0D tensor)</code>
<code class="n">scalar</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>  <code class="c1"># Single number</code>
 
<code class="c1"># Vector (1D tensor)</code>
<code class="n">vector</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">])</code>  <code class="c1"># Array of numbers</code>
 
<code class="c1"># Matrix (2D tensor)</code>
<code class="n">matrix</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
                      <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">]])</code>  <code class="c1"># 2x3 grid of numbers</code>
 
<code class="c1"># 3D tensor</code>
<code class="n">cube</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]],</code>
                    <code class="p">[[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">],</code> <code class="p">[</code><code class="mi">7</code><code class="p">,</code> <code class="mi">8</code><code class="p">]]])</code>  <code class="c1"># 2x2x2 cube of numbers</code></pre>
        <p>What makes tensors so useful<a contenteditable="false" data-primary="tensors" data-secondary="flexibility to hold different value types" data-type="indexterm" id="id1536"/> for ML is this flexibility to hold different value types. Numbers can be 0D tensors, the embedding vectors representing text can be 1D, and images can be 3D, with dimensions for height, width, and pixel value. Plus, all of these can have an additional dimension added for batches. So, for example, a single image can be a 3D matrix, but 100 images instead of 100 3D matrices could be a single 4D tensor, with the fourth dimension being the index of the image!</p>
        <p>When you’re using <code>torch.tensor</code>, keep in mind<a contenteditable="false" data-primary="GPU (graphics processing unit)" data-secondary="torch.tensor optimized for" data-type="indexterm" id="id1537"/><a contenteditable="false" data-primary="torch.tensor optimized to run on GPUs" data-type="indexterm" id="id1538"/> that a lot of work and investment has been put into optimizing them to run on GPUs, which makes them extremely efficient for deep learning computation.<a contenteditable="false" data-primary="" data-startref="ch12tens" data-type="indexterm" id="id1539"/><a contenteditable="false" data-primary="" data-startref="ch12tens3" data-type="indexterm" id="id1540"/><a contenteditable="false" data-primary="" data-startref="ch12tens4" data-type="indexterm" id="id1541"/><a contenteditable="false" data-primary="" data-startref="ch12tens5" data-type="indexterm" id="id1542"/></p>
      </div></section>
      <section data-pdf-bookmark="Image Data" data-type="sect1"><div class="sect1" id="ch12_image_data_1748549754445721">
        <h1>Image Data</h1>
        <p>Images are typically stored in formats<a contenteditable="false" data-primary="inference" data-secondary="image data" data-type="indexterm" id="ch12img"/><a contenteditable="false" data-primary="image data" data-type="indexterm" id="ch12img2"/><a contenteditable="false" data-primary="datasets" data-secondary="image data" data-type="indexterm" id="ch12img3"/> like JPEG or PNG, which are optimized for <em>human</em> viewing as well as storage efficiency. Each dot (or pixel) in the image is usually composed of a number of values, with each value being the intensity of a color channel. Typically, an image will have 24 bits of data, with 8 bits assigned each to red, green, and blue channels. If you see 32 bits, then the additional 8 are for an alpha, or transparency, channel.</p>
        <p>So, for example, a green pixel might have values 0 on the red, blue, and alpha channels and 255 on the green channel. If it’s semi-transparent, it might still have a value of 0 on red and blue, 128 on alpha, and 255 on green.</p>
        <p>An image file is typically <em>compressed</em>, <a contenteditable="false" data-primary="image data" data-secondary="compressed" data-type="indexterm" id="id1543"/><a contenteditable="false" data-primary="datasets" data-secondary="image data" data-tertiary="compressed" data-type="indexterm" id="id1544"/><a contenteditable="false" data-primary="inference" data-secondary="image data" data-tertiary="compressed" data-type="indexterm" id="id1545"/>which means that mathematical transforms have been applied to it to avoid wasted data and make the image smaller. An image can also contain metadata, file headers, and more. Then, once the image is loaded into memory, it is usually uncompressed to the 32-bits-per-pixel previously described.</p>
        <p>ML models typically use values between –1 and 1, and not 0 to 255, as the native values of the image are stored. If we want to learn the details of an image,<a contenteditable="false" data-primary="image data" data-secondary="variations between pixel values" data-type="indexterm" id="id1546"/><a contenteditable="false" data-primary="datasets" data-secondary="image data" data-tertiary="variations between pixel values" data-type="indexterm" id="id1547"/><a contenteditable="false" data-primary="inference" data-secondary="image data" data-tertiary="variations between pixel values" data-type="indexterm" id="id1548"/> it’s good for us to standardize the values by focusing on the meaningful <em>variations</em> between the pixel intensities as opposed to just their values. So, one method is to standardize by figuring out how far the pixel value is from the mean and standard deviations. This gives a broader spectrum of values that can lead to smoother loss curves and more effective learning.</p>
        <p>In PyTorch, you achieve this with code like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">torchvision</code> <code class="kn">import</code> <code class="n">transforms</code>
<code class="kn">from</code> <code class="nn">PIL</code> <code class="kn">import</code> <code class="n">Image</code>
 
<code class="k">def</code> <code class="nf">prepare_image</code><code class="p">(</code><code class="n">image_path</code><code class="p">):</code>
    <code class="c1"># Load the image using PIL</code>
    <code class="n">raw_image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">image_path</code><code class="p">)</code>
 
    <code class="c1"># Define the transformations</code>
    <code class="n">preprocess</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code>
        <code class="n">transforms</code><code class="o">.</code><code class="n">Resize</code><code class="p">(</code><code class="mi">256</code><code class="p">),</code>
        <code class="n">transforms</code><code class="o">.</code><code class="n">CenterCrop</code><code class="p">(</code><code class="mi">224</code><code class="p">),</code>
        <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">(),</code>
        <code class="n">transforms</code><code class="o">.</code><code class="n">Normalize</code><code class="p">(</code>
            <code class="n">mean</code><code class="o">=</code><code class="p">[</code><code class="mf">0.485</code><code class="p">,</code> <code class="mf">0.456</code><code class="p">,</code> <code class="mf">0.406</code><code class="p">],</code>
            <code class="n">std</code><code class="o">=</code><code class="p">[</code><code class="mf">0.229</code><code class="p">,</code> <code class="mf">0.224</code><code class="p">,</code> <code class="mf">0.225</code><code class="p">]</code>
        <code class="p">)</code>
    <code class="p">])</code>
 
    <code class="c1"># Apply transformations</code>
    <code class="n">input_tensor</code> <code class="o">=</code> <code class="n">preprocess</code><code class="p">(</code><code class="n">raw_image</code><code class="p">)</code>
 
    <code class="c1"># Add batch dimension</code>
    <code class="n">input_batch</code> <code class="o">=</code> <code class="n">input_tensor</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">input_batch</code></pre>
        <p>This code uses a very popular Python library called Pillow, or just PIL. </p>
        <p>In this case, the <code>PIL Image.open</code> will read the image, decompress it into pixels, and then apply a set of transforms to those pixels. The transforms normalize the channels into ML-friendly values, as described earlier, and then convert them into tensors. </p>
        <p>The <code>input_tensor</code> is then a 3D matrix/tensor, but if we want to have a number of them in a single batch, we can <em>unsqueeze</em> this tensor to add a new dimension, making it a 4D tensor.</p>
        <p>You can see a single image in <a data-type="xref" href="#ch12_figure_1_1748549754439352">Figure 12-1</a> as a 3D tensor, with one dimension for each color depth.</p>
        <figure><div class="figure" id="ch12_figure_1_1748549754439352">
          <img src="assets/aiml_1201.png"/>
          <h6><span class="label">Figure 12-1. </span>Tensor representing a full-color image</h6>
        </div></figure>
        <p>And then, if there’s a batch of images, you can see it as a fourth dimension in <a data-type="xref" href="#ch12_figure_2_1748549754439405">Figure 12-2</a>.<a contenteditable="false" data-primary="" data-startref="ch12img" data-type="indexterm" id="id1549"/><a contenteditable="false" data-primary="" data-startref="ch12img2" data-type="indexterm" id="id1550"/><a contenteditable="false" data-primary="" data-startref="ch12img3" data-type="indexterm" id="id1551"/></p>
        <figure><div class="figure" id="ch12_figure_2_1748549754439405">
          <img src="assets/aiml_1202.png"/>
          <h6><span class="label">Figure 12-2. </span>Tensor representing a batch of colored images</h6>
        </div></figure>
      </div></section>
      <section data-pdf-bookmark="Text Data" data-type="sect1"><div class="sect1" id="ch12_text_data_1748549754445782">
        <h1>Text Data</h1>
        <p>Typically, text is stored in a string,<a contenteditable="false" data-primary="inference" data-secondary="text data" data-type="indexterm" id="ch12txt"/><a contenteditable="false" data-primary="text data" data-type="indexterm" id="ch12txt2"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="text data" data-type="indexterm" id="ch12txt3"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="text data" data-type="indexterm" id="ch12txt4"/><a contenteditable="false" data-primary="embeddings" data-secondary="text data" data-type="indexterm" id="ch12txt5"/><a contenteditable="false" data-primary="vectors for word meanings" data-secondary="text data" data-type="indexterm" id="ch12txt6"/><a contenteditable="false" data-primary="words" data-secondary="text data" data-type="indexterm" id="ch12txt7"/><a contenteditable="false" data-primary="tokenization" data-secondary="text data" data-type="indexterm" id="ch12txt8"/><a contenteditable="false" data-primary="sentences" data-secondary="text data" data-type="indexterm" id="ch12txt9"/> like “The cat sat on the mat,” but training a model or passing text like this to a pretrained model is unfeasible. Models, as you saw in earlier chapters, are trained on numeric data—and the best way to do that is to either <em>tokenize</em> the text, by turning words or subwords into numbers, or to <em>calculate embeddings</em> for the text, by turning them into vectors. And if you choose to calculate embeddings for the text, you can also encode sentiment about the text into the direction of the vector (see <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a>).</p>
        <p>So, as a simple example, let’s consider the following sentences:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">texts</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s2">"I love my dog"</code><code class="p">,</code>
    <code class="s2">"The manatee became a doctor"</code>
<code class="p">]</code></pre>
        <p>You can <em>tokenize </em>this text into a series of numbers by using a tokenizer. You can create your own tokenized series of numbers from the corpus, as you did in <a data-type="xref" href="ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759">Chapter 5</a>, or you can use an existing one, like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">BertTokenizer</code><code class="p">,</code> <code class="n">BertModel</code>
 
<code class="k">def</code> <code class="nf">text_to_embeddings</code><code class="p">(</code><code class="n">texts</code><code class="p">):</code>
    <code class="c1"># Load pretrained BERT tokenizer and model</code>
    <code class="n">tokenizer</code> <code class="o">=</code> <code class="n">BertTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s1">'bert-base-uncased'</code><code class="p">)</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">BertModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s1">'bert-base-uncased'</code><code class="p">)</code>
    <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>  <code class="c1"># Set to evaluation mode</code>
 
    <code class="c1"># Tokenize the input texts</code>
    <code class="n">encoded</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code>
        <code class="n">texts</code><code class="p">,</code>
        <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>      <code class="c1"># Pad shorter sequences to match longest</code>
        <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>   <code class="c1"># Truncate sequences that are too long</code>
        <code class="n">return_tensors</code><code class="o">=</code><code class="s1">'pt'</code>  <code class="c1"># Return PyTorch tensors</code>
    <code class="p">)</code>
 </pre>
        <p>You’ll learn a lot more about transformers, including how to use them for BERT, starting in <a data-type="xref" href="ch13.html#ch13_hosting_pytorch_models_for_serving_1748549772563124">Chapter 13</a>. </p>
        <p>The important line here is the last one, in which we ask the tokenizer to return tensors in PyTorch format. You can see the result of this in the output of the encodings, like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">Encodings</code><code class="p">:</code> 
<code class="n">tensor</code><code class="p">([[</code>  <code class="mi">101</code><code class="p">,</code>  <code class="mi">1045</code><code class="p">,</code>  <code class="mi">2293</code><code class="p">,</code>  <code class="mi">2026</code><code class="p">,</code>  <code class="mi">3899</code><code class="p">,</code>   <code class="mi">102</code><code class="p">,</code>     <code class="mi">0</code><code class="p">,</code>     <code class="mi">0</code><code class="p">],</code>
        <code class="p">[</code>  <code class="mi">101</code><code class="p">,</code>  <code class="mi">1996</code><code class="p">,</code> <code class="mi">24951</code><code class="p">,</code> <code class="mi">17389</code><code class="p">,</code>  <code class="mi">2150</code><code class="p">,</code>  <code class="mi">1037</code><code class="p">,</code>  <code class="mi">3460</code><code class="p">,</code>   <code class="mi">102</code><code class="p">]])</code></pre>
        <p>If you explore this closely, you’ll see that the two sentences have been turned into a series of numbers. The first sentence, which has four words, has six numbers, and the second, which has six words, has eight numbers. Each has the number 101 at the front and 102 at the end, which gives us the two extra tokens. These are special tokens the encoder has added to indicate the start and end of the sentence.</p>
        <p>A string can be represented as a 1D vector, but we have multiple strings here, so we can add a dimension to 1D to get 2D, and the second dimension gives us each string. So, value 0 in the second dimension is the first string, value 1 is the second string, etc.</p>
        <p>The BERT model can also create embeddings from the sentence by getting an embedding for each word in the sentence and summing them all up to get an overall set of values. In the base model version of BERT, each embedding vector is a 1D matrix with 768 values. The embedding for a sentence is the same. Multiple sentences, just like the previous encodings, will have a second dimension.</p>
        <p>Here’s the code:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate embeddings</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>  <code class="c1"># No need to calculate gradients</code>
    <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="o">**</code><code class="n">encoded</code><code class="p">)</code>
    <code class="n">embeddings</code> <code class="o">=</code> <code class="n">outputs</code><code class="o">.</code><code class="n">last_hidden_state</code>
 </pre>
        <p>Each embedding has 768 values, so I’m not going to print them all out, but for the embedding that represents the first sentence, you can see values like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">First</code> <code class="n">word</code> <code class="n">embedding</code> <code class="p">(</code><code class="n">first</code> <code class="mi">5</code> <code class="n">values</code><code class="p">):</code> 
        <code class="n">tensor</code><code class="p">([</code> <code class="mf">0.0401</code><code class="p">,</code>  <code class="mf">0.3046</code><code class="p">,</code>  <code class="mf">0.0669</code><code class="p">,</code> <code class="err">–</code><code class="mf">0.1975</code><code class="p">,</code> <code class="err">–</code><code class="mf">0.0103</code><code class="p">])</code></pre>
        <p>Don’t worry if you don’t fully understand this yet—we’ll be going into it in more detail starting in <a data-type="xref" href="ch13.html#ch13_hosting_pytorch_models_for_serving_1748549772563124">Chapter 13</a>. The important point here is that the idea of a <em>tensor</em> is that the very flexible matrix, which can have any number of dimensions, gives you a consistent input into a model. You don’t need to train models on different data types—they’ll always be tensor in, tensor out.<a contenteditable="false" data-primary="" data-startref="ch12txt" data-type="indexterm" id="id1552"/><a contenteditable="false" data-primary="" data-startref="ch12txt2" data-type="indexterm" id="id1553"/><a contenteditable="false" data-primary="" data-startref="ch12txt3" data-type="indexterm" id="id1554"/><a contenteditable="false" data-primary="" data-startref="ch12txt4" data-type="indexterm" id="id1555"/><a contenteditable="false" data-primary="" data-startref="ch12txt5" data-type="indexterm" id="id1556"/><a contenteditable="false" data-primary="" data-startref="ch12txt6" data-type="indexterm" id="id1557"/><a contenteditable="false" data-primary="" data-startref="ch12txt7" data-type="indexterm" id="id1558"/><a contenteditable="false" data-primary="" data-startref="ch12txt8" data-type="indexterm" id="id1559"/><a contenteditable="false" data-primary="" data-startref="ch12txt9" data-type="indexterm" id="id1560"/></p>
      </div></section>
      <section data-pdf-bookmark="Tensors Out of a Model" data-type="sect1"><div class="sect1" id="ch12_tensors_out_of_a_model_1748549754445845">
        <h1>Tensors Out of a Model</h1>
        <p>As noted earlier, the power<a contenteditable="false" data-primary="inference" data-secondary="tensor data type into and out of model" data-tertiary="out of a model" data-type="indexterm" id="ch12out"/><a contenteditable="false" data-primary="tensors" data-secondary="tensor data type into and out of model" data-tertiary="out of a model" data-type="indexterm" id="ch12out2"/><a contenteditable="false" data-primary="models" data-secondary="tensor data type into and out of" data-tertiary="out of a model" data-type="indexterm" id="ch12out3"/> of tensors is in their consistency—regardless of what type of data you pass <em>into</em> a model, when they’re tensors, you can be consistent in your coding interface. The same applies for tensors <em>out</em> of a model.</p>
        <p>So, for example, consider a dataset like <code>ImageNet</code> that contains 15 million images in over 21,000 classes. When you design a model to recognize images in this dataset, you’ll need over 21,000 output neurons, each of which gives you a percentage likelihood that the image is of the representative class. So, for example, if neuron 0 represents “goldfish,” the value coming out of it when you do inference will be the probability that the image contains a goldfish!</p>
        <p>So, instead of outputting the classification, the model will expose the values of <em>each</em> of its output neurons. These values are often called <em>logits</em>.</p>
        <p>This list of values is a 1D vector of values—so of course, a tensor is the appropriate data type. </p>
        <p>Here’s a simulated example, with a set of representative outputs from multiple images passed into the model and the list of class names:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># ImageNet class labels (simplified - just a few examples)</code>
<code class="n">class_names</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s1">'tench'</code><code class="p">,</code> <code class="s1">'goldfish'</code><code class="p">,</code> <code class="s1">'great white shark'</code><code class="p">,</code> <code class="s1">'tiger shark'</code><code class="p">,</code> <code class="s1">'hammerhead shark'</code><code class="p">,</code>
    <code class="s1">'electric ray'</code><code class="p">,</code> <code class="s1">'stingray'</code><code class="p">,</code> <code class="s1">'rooster'</code><code class="p">,</code> <code class="s1">'hen'</code><code class="p">,</code> <code class="s1">'ostrich'</code><code class="p">,</code> <code class="s1">'brambling'</code><code class="p">,</code>
    <code class="s1">'goldfinch'</code><code class="p">,</code> <code class="s1">'house finch'</code><code class="p">,</code> <code class="s1">'junco'</code><code class="p">,</code> <code class="s1">'indigo bunting'</code><code class="p">,</code> <code class="s1">'robin'</code><code class="p">,</code> <code class="s1">'bulbul'</code><code class="p">,</code>
    <code class="s1">'jay'</code><code class="p">,</code> <code class="s1">'magpie'</code><code class="p">,</code> <code class="s1">'chickadee'</code>
<code class="p">]</code>
 
<code class="c1"># Simulate model output for demonstration</code>
<code class="c1"># This would normally come from model(input_tensor)</code>
<code class="n">example_output</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code>
    <code class="p">[</code> <code class="mf">1.2</code><code class="p">,</code>  <code class="mf">4.5</code><code class="p">,</code> <code class="err">–</code><code class="mf">0.8</code><code class="p">,</code>  <code class="mf">2.1</code><code class="p">,</code>  <code class="mf">0.3</code><code class="p">,</code>  <code class="c1"># First image predictions</code>
     <code class="err">–</code><code class="mf">1.5</code><code class="p">,</code>  <code class="mf">0.9</code><code class="p">,</code>  <code class="mf">3.2</code><code class="p">,</code> <code class="err">–</code><code class="mf">0.4</code><code class="p">,</code>  <code class="mf">1.1</code><code class="p">,</code>
      <code class="mf">0.5</code><code class="p">,</code> <code class="err">–</code><code class="mf">0.2</code><code class="p">,</code>  <code class="mf">1.8</code><code class="p">,</code>  <code class="mf">0.7</code><code class="p">,</code> <code class="err">–</code><code class="mf">1.0</code><code class="p">,</code>
      <code class="mf">2.8</code><code class="p">,</code>  <code class="mf">1.6</code><code class="p">,</code> <code class="err">–</code><code class="mf">0.6</code><code class="p">,</code>  <code class="mf">0.4</code><code class="p">,</code>  <code class="mf">1.3</code><code class="p">],</code>
    <code class="p">[</code><code class="err">–</code><code class="mf">0.5</code><code class="p">,</code>  <code class="mf">5.2</code><code class="p">,</code>  <code class="mf">0.3</code><code class="p">,</code>  <code class="mf">1.4</code><code class="p">,</code> <code class="err">–</code><code class="mf">0.8</code><code class="p">,</code>  <code class="c1"># Second image predictions</code>
      <code class="mf">0.9</code><code class="p">,</code>  <code class="mf">1.2</code><code class="p">,</code>  <code class="mf">2.8</code><code class="p">,</code>  <code class="mf">0.6</code><code class="p">,</code>  <code class="mf">1.5</code><code class="p">,</code>
     <code class="err">–</code><code class="mf">1.1</code><code class="p">,</code>  <code class="mf">0.4</code><code class="p">,</code>  <code class="mf">2.1</code><code class="p">,</code>  <code class="mf">0.2</code><code class="p">,</code> <code class="err">–</code><code class="mf">0.7</code><code class="p">,</code>
      <code class="mf">1.9</code><code class="p">,</code>  <code class="mf">0.8</code><code class="p">,</code> <code class="err">–</code><code class="mf">0.3</code><code class="p">,</code>  <code class="mf">1.6</code><code class="p">,</code>  <code class="mf">0.5</code><code class="p">]</code>
<code class="p">])</code>
 </pre>
        <p>Note that because the output is <em>tensors</em>, we can use many of the functions built into PyTorch that are optimized for tensors to work with them.</p>
        <p>When dealing with the output values, we want to find the best ones and maybe limit them to a range. Softmax is perfect for that, when it converts the raw output into probabilities—and TopK is used to pick the best <em>k</em> values. Here’s an example in which we can use Softmax and TopK functions that manage tensors, regardless of their dimensionality:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">interpret_output</code><code class="p">(</code><code class="n">output_tensor</code><code class="p">,</code> <code class="n">top_k</code><code class="o">=</code><code class="mi">5</code><code class="p">):</code>
    <code class="c1"># Apply softmax to convert logits to probabilities</code>
    <code class="n">probabilities</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">output_tensor</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
 
    <code class="c1"># Get top k probabilities and class indices</code>
    <code class="n">top_probs</code><code class="p">,</code> <code class="n">top_indices</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">topk</code><code class="p">(</code><code class="n">probabilities</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="n">top_k</code><code class="p">)</code>
 
    <code class="c1"># Convert to numpy for easier handling</code>
    <code class="n">top_probs</code> <code class="o">=</code> <code class="n">top_probs</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
    <code class="n">top_indices</code> <code class="o">=</code> <code class="n">top_indices</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
 
    <code class="k">return</code> <code class="n">top_probs</code><code class="p">,</code> <code class="n">top_indices</code></pre>
        <p>The values can then be converted to NumPy so that other code—like printing out the values—will work more easily.</p>
        <p>We can see the output of this here, where Softmax and TopK were used to interpret the data and then print it out:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">Image</code> <code class="mi">1</code> <code class="n">Predictions</code><code class="p">:</code>
<code class="o">------------------------</code>
<code class="n">Raw</code> <code class="n">logits</code> <code class="p">(</code><code class="n">first</code> <code class="mi">5</code><code class="p">):</code> <code class="p">[</code><code class="mf">1.2000000476837158</code><code class="p">,</code> <code class="mf">4.5</code><code class="p">,</code> <code class="err">–</code><code class="mf">0.800000011920929</code><code class="p">,</code> 
                       <code class="mf">2.0999999046325684</code><code class="p">,</code> <code class="mf">0.30000001192092896</code><code class="p">]</code>
 
<code class="n">Top</code> <code class="mi">5</code> <code class="n">Predictions</code><code class="p">:</code>
<code class="mf">1.</code> <code class="n">goldfish</code><code class="p">:</code> <code class="mf">52.3</code><code class="o">%</code>
<code class="mf">2.</code> <code class="n">rooster</code><code class="p">:</code> <code class="mf">14.2</code><code class="o">%</code>
<code class="mf">3.</code> <code class="n">robin</code><code class="p">:</code> <code class="mf">9.5</code><code class="o">%</code>
<code class="mf">4.</code> <code class="n">tiger</code> <code class="n">shark</code><code class="p">:</code> <code class="mf">4.7</code><code class="o">%</code>
<code class="mf">5.</code> <code class="n">house</code> <code class="n">finch</code><code class="p">:</code> <code class="mf">3.5</code><code class="o">%</code></pre>
        <p>The full code for this can be found in the <a href="https://github.com/lmoroney/PyTorch-Book-FIles">GitHub repository</a> for this book.<a contenteditable="false" data-primary="" data-startref="ch12out" data-type="indexterm" id="id1561"/><a contenteditable="false" data-primary="" data-startref="ch12out2" data-type="indexterm" id="id1562"/><a contenteditable="false" data-primary="" data-startref="ch12out3" data-type="indexterm" id="id1563"/></p>
      </div></section>
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch12_summary_1748549754445901">
        <h1>Summary</h1>
        <p>In this chapter, you took a brief look at tensors and the underlying idea behind them—that they are a flexible data structure that can be used to represent the best way to put data <em>into</em> an ML model, regardless of what it represents, and even batch it. It also provides a consistent way to manage outputs from a model—in which values are typically emitted via neurons that are arranged in the output layer as a list. Thus, by being able to handle tensors, you can build a foundation for how data flows in <em>and</em> out of a model.</p>
        <p>With that, we’re now going to switch gears from model training to inference, in particular with generative AI, starting with getting models from registries and hubs.</p>
      </div></section>
    </div></section></body></html>