<html><head></head><body><section data-pdf-bookmark="Chapter 16. Using LLMs with Custom Data" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch16_using_llms_with_custom_data_1748550037719939">&#13;
      <h1><span class="label">Chapter 16. </span>Using LLMs with Custom Data</h1>&#13;
      <p>In <a data-type="xref" href="ch15.html#ch15_transformers_and_transformers_1748549808974580">Chapter 15</a>, we looked at Transformers and how their encoder, decoder, and encoder-decoder architectures work. The results of their revolutionizing NLP can’t be disputed! Then, we looked at transformers, which form the Python library from Hugging Face that’s designed to make it easier to use Transformers.</p>&#13;
      <p>Large Transformer-based models, which are trained on vast amounts of text, are very powerful, but they aren’t always ideal for specific tasks or domains. In this chapter, we’ll look at how you can use transformers and other APIs to adapt these models to your specific needs.</p>&#13;
      <p>Fine-tuning allows you to customize pretrained models with your specific data. You could use this approach to create a chatbot, improve classification accuracy, or develop text generation for a more specific domain. </p>&#13;
      <p>There are several techniques for doing this, including traditional fine-tuning and parameter-efficient tuning with methods like LoRA and parameter-efficient fine-tuning (PEFT). You can also get more out of your LLMs with retrieval-augmented generation (RAG), which we’ll explore in <a data-type="xref" href="ch18.html#ch18_introduction_to_rag_1748550073472936">Chapter 18</a>.</p>&#13;
      <p>In this chapter, we’ll explore some hands-on examples, starting with traditional fine-tuning.</p>&#13;
      <section data-pdf-bookmark="Fine-Tuning an LLM" data-type="sect1"><div class="sect1" id="ch16_fine_tuning_an_llm_1748550037720172">&#13;
        <h1>Fine-Tuning an LLM</h1>&#13;
        <p>Let’s take a look, step by step,<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="fine-tuning" data-type="indexterm" id="ch16ft"/><a contenteditable="false" data-primary="fine-tuning LLMs" data-type="indexterm" id="ch16ft2"/> at how to fine-tune an LLM like BERT. We’ll take the IMDb database and fine-tune the model on it to be better at detecting sentiment in movie reviews. There are a number of steps involved in doing this, so we’ll look at each one in detail.</p>&#13;
        <section data-pdf-bookmark="Setup and Dependencies" data-type="sect2"><div class="sect2" id="ch16_setup_and_dependencies_1748550037720238">&#13;
          <h2>Setup and Dependencies</h2>&#13;
          <p>We’ll start by setting up everything<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="fine-tuning" data-tertiary="setup and dependencies" data-type="indexterm" id="id1773"/><a contenteditable="false" data-primary="fine-tuning LLMs" data-secondary="setup and dependencies" data-type="indexterm" id="id1774"/> that we need to do fine-tuning with PyTorch. In addition to the basics, there are three new things that you’ll need to include:</p>&#13;
          <dl>&#13;
            <dt>Datasets</dt>&#13;
            <dd>&#13;
              <p>We covered datasets in <a data-type="xref" href="ch04.html#ch04_using_data_with_pytorch_1748548966496246">Chapter 4</a>. We’re going to use these to load the IMDb dataset and the built-in splits for training and testing.</p>&#13;
            </dd>&#13;
            <dt>Evaluate</dt>&#13;
            <dd>&#13;
              <p>This library provides metrics for measuring load performance.</p>&#13;
            </dd>&#13;
            <dt>transformers</dt>&#13;
            <dd>&#13;
              <p>As we covered in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797">14</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch15.html#ch15_transformers_and_transformers_1748549808974580">15</a>, the transformers Hugging Face library is designed to make using LLMs much easier. </p>&#13;
            </dd>&#13;
          </dl>&#13;
          <p>We’ll use some classes from the Hugging Face transformers library for this chapter’s fine-tuning exercise. These include the following:</p>&#13;
          <dl>&#13;
            <dt>AutoModelForSequenceClassification </dt>&#13;
            <dd>&#13;
              <p>This class loads pretrained models for classification tasks and adds a classification head to the top of the base model. This classification head is then optimized for the specific classification scenario you are fine-tuning for, instead of being a generic model. If we specify the checkpoint name, it will automatically handle the model architecture for us. So, to use the BERT model with a linear classifier layer, we’ll use <code>bert-base-uncased</code>.</p>&#13;
            </dd>&#13;
            <dt>AutoTokenizer </dt>&#13;
            <dd>&#13;
              <p>This class automatically initializes the appropriate tokenizer. This converts text to the appropriate tokens and adds the appropriate special tokens, padding, truncation, etc.</p>&#13;
            </dd>&#13;
            <dt>TrainingArguments </dt>&#13;
            <dd>&#13;
              <p>This class lets us configure the training settings and all the hyperparameters, as well as setting up things like the device to use.</p>&#13;
            </dd>&#13;
            <dt>Trainer </dt>&#13;
            <dd>&#13;
              <p>This class manages the training loop on your behalf, handling batching, optimization, loss, backpropagation, and everything you need to retrain the model.</p>&#13;
            </dd>&#13;
            <dt>DataCollatorWithPadding </dt>&#13;
            <dd>&#13;
              <p>The number of records in the dataset doesn’t always line up with the batch size. This class therefore efficiently batches examples to the appropriate batch sizes while also handling details like attention masks and other model-specific inputs.</p>&#13;
            </dd>&#13;
          </dl>&#13;
          <p class="pagebreak-before">We can see this in code here:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 1. Setup and Dependencies</code>&#13;
<code class="kn">import</code> <code class="nn">torch</code>&#13;
<code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">load_dataset</code>&#13;
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="p">(</code>&#13;
    <code class="n">AutoModelForSequenceClassification</code><code class="p">,</code>&#13;
    <code class="n">AutoTokenizer</code><code class="p">,</code>&#13;
    <code class="n">TrainingArguments</code><code class="p">,</code>&#13;
    <code class="n">Trainer</code><code class="p">,</code>&#13;
    <code class="n">DataCollatorWithPadding</code>&#13;
<code class="p">)</code>&#13;
<code class="kn">import</code> <code class="nn">evaluate</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code></pre>&#13;
          <p>Now that the dependencies are in place, we’ll load the data.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Loading and Examining the Data" data-type="sect2"><div class="sect2" id="ch16_loading_and_examining_the_data_1748550037720291">&#13;
          <h2>Loading and Examining the Data</h2>&#13;
          <p>Next up, let’s load our data<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="fine-tuning" data-tertiary="loading and examining data" data-type="indexterm" id="id1775"/><a contenteditable="false" data-primary="fine-tuning LLMs" data-secondary="loading and examining data" data-type="indexterm" id="id1776"/> by using the datasets API. We’ll also explore the test and training dataset sizes. You can use the following code:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 2. Load and Examine Data</code>&#13;
<code class="n">dataset</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"imdb"</code><code class="p">)</code>  <code class="c1"># Movie reviews for sentiment analysis</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Train size: </code><code class="si">{</code><code class="nb">len</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s1">'train'</code><code class="p">])</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Test size: </code><code class="si">{</code><code class="nb">len</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s1">'test'</code><code class="p">])</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre>&#13;
          <p>That will output the following:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">Train</code> <code class="n">size</code><code class="p">:</code> <code class="mi">25000</code>&#13;
<code class="n">Test</code> <code class="n">size</code><code class="p">:</code> <code class="mi">25000</code></pre>&#13;
          <p>The next step is to initialize the model and the tokenizer.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Initializing the Model and Tokenizer" data-type="sect2"><div class="sect2" id="ch16_initializing_the_model_and_tokenizer_1748550037720346">&#13;
          <h2>Initializing the Model and Tokenizer</h2>&#13;
          <p>We’ll use the <code>bert-base-uncased</code> model in this example,<a contenteditable="false" data-primary="fine-tuning LLMs" data-secondary="initializing model and tokenizer" data-type="indexterm" id="id1777"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="fine-tuning" data-tertiary="initializing model and tokenizer" data-type="indexterm" id="id1778"/> so we need to initialize it by using <code>AutoModelForSequenceClassification</code> and getting its associated tokenizer:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 3. Initialize Model and Tokenizer</code>&#13;
<code class="n">model_name</code> <code class="o">=</code> <code class="s2">"bert-base-uncased"</code>&#13;
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_name</code><code class="p">)</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">AutoModelForSequenceClassification</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>&#13;
    <code class="n">model_name</code><code class="p">,</code>&#13;
    <code class="n">num_labels</code><code class="o">=</code><code class="mi">2</code>&#13;
<code class="p">)</code>&#13;
<code class="n">device</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s2">"cuda"</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">()</code> <code class="k">else</code> <code class="s2">"cpu"</code><code class="p">)</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>&#13;
          <p>Note the <code>AutoModelForSequenceClassification</code> needs to be initialized with the number of labels that we want to classify for. This defines the new classification head with two labels. The IMDb database that we’ll be using has two labels for positive and negative sentiment, so we’ll retrain for that.</p>&#13;
          <p>At this point, it’s also a good idea to specify the device that the model will run on. Training with this model is computationally intensive, and if you’re using Colab, you’ll likely need a high-RAM GPU like an A100. Training with that will take a couple of minutes, but it can take many hours on a CPU!</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Preprocessing the Data" data-type="sect2"><div class="sect2" id="ch16_preprocessing_the_data_1748550037720395">&#13;
          <h2>Preprocessing the Data</h2>&#13;
          <p>Once we have the data, we want<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="fine-tuning" data-tertiary="preprocessing the data" data-type="indexterm" id="id1779"/><a contenteditable="false" data-primary="fine-tuning LLMs" data-secondary="preprocessing the data" data-type="indexterm" id="id1780"/> to preprocess it just to get what we need to train. The first step in this, of course, will be to tokenize the text, and the <code>preprocess</code> function here handles that, giving a sequence length of 512 characters with padding:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 4. Preprocess Data</code>&#13;
<code class="k">def</code> <code class="nf">preprocess_function</code><code class="p">(</code><code class="n">examples</code><code class="p">):</code>&#13;
   <code class="n">result</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code>&#13;
       <code class="n">examples</code><code class="p">[</code><code class="s2">"text"</code><code class="p">],</code>&#13;
       <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
       <code class="n">max_length</code><code class="o">=</code><code class="mi">512</code><code class="p">,</code>&#13;
       <code class="n">padding</code><code class="o">=</code><code class="kc">True</code>&#13;
   <code class="p">)</code>&#13;
   <code class="c1"># Trainer expects a column called labels, so copy over from label</code>&#13;
   <code class="n">result</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">]</code> <code class="o">=</code> <code class="n">examples</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]</code>&#13;
   <code class="k">return</code> <code class="n">result</code>&#13;
 &#13;
<code class="n">tokenized_dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">map</code><code class="p">(</code>&#13;
   <code class="n">preprocess_function</code><code class="p">,</code>&#13;
   <code class="n">batched</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
   <code class="n">remove_columns</code><code class="o">=</code><code class="n">dataset</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">column_names</code>&#13;
<code class="p">)</code></pre>&#13;
          <p>One important note here is that the original data came with columns for <code>text</code> denoting the review and <code>label</code> being 0 or 1 for negative or positive sentiment. However, we don’t <em>need</em> the <code>text</code> column to train the data, and the Hugging Face Trainer (which we will see in a moment) expects the column containing the label to be called <code>labels</code> (plural). Therefore, you’ll see that we remove all of the columns in the original dataset, and the tokenized dataset will have the tokenized data and a column called <code>labels</code> instead of <code>label</code>, with the original values copied over.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Collating the Data" data-type="sect2"><div class="sect2" id="ch16_collating_the_data_1748550037720440">&#13;
          <h2>Collating the Data</h2>&#13;
          <p>When we’re dealing with passing sequenced,<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="fine-tuning" data-tertiary="collating the data" data-type="indexterm" id="id1781"/><a contenteditable="false" data-primary="fine-tuning LLMs" data-secondary="collating the data" data-type="indexterm" id="id1782"/> tokenized data into a model in batches, there can be differences in batch or sequence size that need processing. In our case, we shouldn’t have to worry about the sequence size because we used a tokenizer that forces the length to be 512 (in the previous set). However, as part of the transformers library, the collator classes are still equipped to deal with it, and we’ll be using them to ensure consistent batch sizing. </p>&#13;
          <p>So ultimately, the role of the <code>DataCollatorWithPadding</code> class is to take multiple examples of different lengths, provide padding if and when necessary, convert the inputs into tensors, and create attention masks if necessary.</p>&#13;
          <p>In our case, we’re really only getting the conversion to tensors for input to the model, but it’s still good practice to use <code>DataCollatorWithPadding</code> if we want to change anything in the tokenization process later.</p>&#13;
          <p>Here’s the code:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 5. Create Data Collator</code>&#13;
<code class="n">data_collator</code> <code class="o">=</code> <code class="n">DataCollatorWithPadding</code><code class="p">(</code><code class="n">tokenizer</code><code class="o">=</code><code class="n">tokenizer</code><code class="p">)</code></pre>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Defining Metrics" data-type="sect2"><div class="sect2" id="ch16_defining_metrics_1748550037720483">&#13;
          <h2>Defining Metrics</h2>&#13;
          <p>Now, let’s define some metrics<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="fine-tuning" data-tertiary="defining metrics" data-type="indexterm" id="id1783"/><a contenteditable="false" data-primary="fine-tuning LLMs" data-secondary="defining metrics" data-type="indexterm" id="id1784"/> that we want to capture as we’re training the model. We’ll just do accuracy, where we compare the predicted value to the actual value. Here’s some simple code to achieve that:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 6. Define Metrics</code>&#13;
<code class="n">metric</code> <code class="o">=</code> <code class="n">evaluate</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"accuracy"</code><code class="p">)</code>&#13;
 &#13;
<code class="k">def</code> <code class="nf">compute_metrics</code><code class="p">(</code><code class="n">eval_pred</code><code class="p">):</code>&#13;
    <code class="n">predictions</code><code class="p">,</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">eval_pred</code>&#13;
    <code class="n">predictions</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">predictions</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">metric</code><code class="o">.</code><code class="n">compute</code><code class="p">(</code><code class="n">predictions</code><code class="o">=</code><code class="n">predictions</code><code class="p">,</code> <code class="n">references</code><code class="o">=</code><code class="n">labels</code><code class="p">)</code></pre>&#13;
          <p>It’s using <code>evaluate.load</code> from Hugging Face’s evaluate library, which provides a simple standardized interface that’s specifically designed for tasks like this one. It can handle the heavy lifting for us, instead of requiring us to roll our own metrics, and for an evaluate task, we simply pass it the set of predictions and the set of labels and have it do the computation. The evaluate library is prebuilt to handle a number of metrics, including f1, BLEU, and many others.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Configuring Training" data-type="sect2"><div class="sect2" id="ch16_configuring_training_1748550037720528">&#13;
          <h2>Configuring Training</h2>&#13;
          <p>Next up, we can configure <em>how</em> the model will retrain<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="fine-tuning" data-tertiary="configuring training" data-type="indexterm" id="id1785"/><a contenteditable="false" data-primary="fine-tuning LLMs" data-secondary="configuring training" data-type="indexterm" id="id1786"/> by using the <code>TrainingArguments</code> object. This offers a large variety of hyperparameters you can set—including those for the learning rate, weight decay, etc., as used by the <code>optimizer</code> and <code>loss</code> function. It’s designed to give you granular control over the learning process while abstracting away the complexity. </p>&#13;
          <p>Here’s the set that I used for fine-tuning with IMDb:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 7. Configure Training</code>&#13;
<code class="n">training_args</code> <code class="o">=</code> <code class="n">TrainingArguments</code><code class="p">(</code>&#13;
    <code class="n">output_dir</code><code class="o">=</code><code class="s2">"./results"</code><code class="p">,</code>&#13;
    <code class="n">learning_rate</code><code class="o">=</code><code class="mf">2e-5</code><code class="p">,</code>&#13;
    <code class="n">per_device_train_batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code>&#13;
    <code class="n">per_device_eval_batch_size</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code>&#13;
    <code class="n">num_train_epochs</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>&#13;
    <code class="n">weight_decay</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code>&#13;
    <code class="n">logging_dir</code><code class="o">=</code><code class="s1">'./logs'</code><code class="p">,</code>&#13;
    <code class="n">logging_steps</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>&#13;
    <code class="n">evaluation_strategy</code><code class="o">=</code><code class="s2">"epoch"</code><code class="p">,</code>&#13;
    <code class="n">save_strategy</code><code class="o">=</code><code class="s2">"epoch"</code><code class="p">,</code>&#13;
    <code class="n">load_best_model_at_end</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
    <code class="n">metric_for_best_model</code><code class="o">=</code><code class="s2">"accuracy"</code><code class="p">,</code>&#13;
    <code class="n">push_to_hub</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>&#13;
    <code class="n">gradient_accumulation_steps</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>&#13;
    <code class="n">gradient_checkpointing</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
    <code class="n">report_to</code><code class="o">=</code><code class="s2">"none"</code><code class="p">,</code>&#13;
    <code class="n">fp16</code><code class="o">=</code><code class="kc">True</code>&#13;
<code class="p">)</code></pre>&#13;
          <p>It’s important to note and tweak hyperparameters for different results. In addition to the aforementioned ones for the optimizer, you’ll want to consider the batch sizes. You can set different parameters for training or evaluation. </p>&#13;
          <p>One very useful parameter—in particular for training sessions that are longer than the three epochs here—is <code>load_best_model_at_end</code>. Instead of always using the final checkpoint, it will keep track of the best checkpoint according to the specified metric (in this case, accuracy) and will load that one when it’s done. And because I set the <code>evaluation</code> and <code>save</code> strategies to <code>epoch</code>, it will only do this at the end of an epoch.</p>&#13;
          <p>Note also the <code>report_to</code> parameter: the training uses <code>weights and biases</code> as the backend for reporting by default. I set <code>report_to</code> to <code>none</code> to turn off this reporting. If you want to keep it, you’ll need a Weights and Biases API key. You can get this very easily from the status window or by going to the <a href="https://oreil.ly/yMX1A">Weights and Biases website</a>. As you train, you’ll be asked to paste in this API key. Be sure to do so before you walk away, particularly if you are paying for compute units on Colab.</p>&#13;
          <p>There’s a wealth of parameters to experiment with, and being able to parameterize easily like this also allows you easily to do a neural architecture search with tools like <a href="https://oreil.ly/fDAhG">Ray Tune</a>.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Initializing the Trainer" data-type="sect2"><div class="sect2" id="ch16_initializing_the_trainer_1748550037720572">&#13;
          <h2>Initializing the Trainer</h2>&#13;
          <p>As with the training parameters,<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="fine-tuning" data-tertiary="initializing the trainer" data-type="indexterm" id="id1787"/><a contenteditable="false" data-primary="fine-tuning LLMs" data-secondary="initializing the trainer" data-type="indexterm" id="id1788"/> transformers give you a trainer class that you can use alongside them to encapsulate a full training cycle. </p>&#13;
          <p>You initialize it with the model, the training arguments, the data, the collator, and the metrics strategy that you’ve previously initialized. All the previous steps build up to this. Here’s the code you’ll need:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 8. Initialize Trainer</code>&#13;
<code class="n">trainer</code> <code class="o">=</code> <code class="n">Trainer</code><code class="p">(</code>&#13;
    <code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">,</code>&#13;
    <code class="n">args</code><code class="o">=</code><code class="n">training_args</code><code class="p">,</code>&#13;
    <code class="n">train_dataset</code><code class="o">=</code><code class="n">tokenized_dataset</code><code class="p">[</code><code class="s2">"train"</code><code class="p">],</code>&#13;
    <code class="n">eval_dataset</code><code class="o">=</code><code class="n">tokenized_dataset</code><code class="p">[</code><code class="s2">"test"</code><code class="p">],</code>&#13;
    <code class="n">tokenizer</code><code class="o">=</code><code class="n">tokenizer</code><code class="p">,</code>&#13;
    <code class="n">data_collator</code><code class="o">=</code><code class="n">data_collator</code><code class="p">,</code>&#13;
    <code class="n">compute_metrics</code><code class="o">=</code><code class="n">compute_metrics</code><code class="p">,</code>&#13;
<code class="p">)</code></pre>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Training and Evaluation" data-type="sect2"><div class="sect2" id="ch16_training_and_evaluation_1748550037720617">&#13;
          <h2>Training and Evaluation</h2>&#13;
          <p>With everything now set up,<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="fine-tuning" data-tertiary="training and evaluation" data-type="indexterm" id="id1789"/><a contenteditable="false" data-primary="fine-tuning LLMs" data-secondary="training and evaluation" data-type="indexterm" id="id1790"/> it becomes as simple as calling the <code>train()</code> method on the trainer to do the training and the <code>evaluate()</code> method to do the evaluation.</p>&#13;
          <p>Here’s the code:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 9. Train and Evaluate</code>&#13;
<code class="n">train_results</code> <code class="o">=</code> <code class="n">trainer</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Training results: </code><code class="si">{</code><code class="n">train_results</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>&#13;
 &#13;
<code class="n">eval_results</code> <code class="o">=</code> <code class="n">trainer</code><code class="o">.</code><code class="n">evaluate</code><code class="p">()</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Evaluation results: </code><code class="si">{</code><code class="n">eval_results</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre>&#13;
          <p>As an example, while you could train this model with the free tiers in Google Colab, your experience in timing might vary. With the CPU alone, it can take many hours. I trained this model with the T4 High Ram GPU, which costs 1.6 compute units per hour. The entire training process was about 50 minutes, but I’ll round that up to an hour to include all the downloading and setup. At the time of writing, a pro Colab subscription gets one hundred compute Units with the US$9.99 per month subscription. You could also choose the A100 GPU, which is much faster (training took me about 12 minutes with it) but also more expensive, at about 6.8 compute units per hour.</p>&#13;
          <p>After training, the results looked like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">Training</code> <code class="n">results</code><code class="p">:</code>&#13;
<code class="n">TrainOutput</code><code class="p">(</code><code class="n">global_step</code><code class="o">=</code><code class="mi">585</code><code class="p">,</code>&#13;
            <code class="n">training_loss</code><code class="o">=</code><code class="mf">0.18643947177463108</code><code class="p">,</code>&#13;
            <code class="n">metrics</code><code class="o">=</code><code class="p">{</code><code class="s1">'train_runtime'</code><code class="p">:</code> <code class="mf">597.9931</code><code class="p">,</code>&#13;
            <code class="s1">'train_samples_per_second'</code><code class="p">:</code> <code class="mf">125.42</code><code class="p">,</code>&#13;
            <code class="s1">'train_steps_per_second'</code><code class="p">:</code> <code class="mf">0.978</code><code class="p">,</code>&#13;
            <code class="s1">'total_flos'</code><code class="p">:</code> <code class="mf">1.968912649469952e+16</code><code class="p">,</code>&#13;
            <code class="s1">'train_loss'</code><code class="p">:</code> <code class="mf">0.18643947177463108</code><code class="p">,</code>&#13;
            <code class="s1">'epoch'</code><code class="p">:</code> <code class="mf">2.9923273657289</code><code class="p">})</code>&#13;
<code class="n">Evaluation</code> <code class="n">results</code><code class="p">:</code>&#13;
            <code class="p">{</code><code class="s1">'eval_loss'</code><code class="p">:</code> <code class="mf">0.18489666283130646</code><code class="p">,</code>&#13;
            <code class="s1">'eval_accuracy'</code><code class="p">:</code> <code class="mf">0.93596</code><code class="p">,</code>&#13;
            <code class="s1">'eval_runtime'</code><code class="p">:</code> <code class="mf">63.8406</code><code class="p">,</code>&#13;
            <code class="s1">'eval_samples_per_second'</code><code class="p">:</code> <code class="mf">391.601</code><code class="p">,</code>&#13;
            <code class="s1">'eval_steps_per_second'</code><code class="p">:</code> <code class="mf">48.95</code><code class="p">,</code>&#13;
            <code class="s1">'epoch'</code><code class="p">:</code> <code class="mf">2.9923273657289</code><code class="p">}</code></pre>&#13;
          <p>We can see quite high accuracy on the evaluation dataset (about 94%) after only three epochs, which is a good sign—but of course, there may be overfitting going on that would require a separate evaluation. But after about 12 minutes of work fine-tuning an LLM, we’re clearly moving in the right direction!</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Saving and Testing the Model" data-type="sect2"><div class="sect2" id="ch16_saving_and_testing_the_model_1748550037720661">&#13;
          <h2>Saving and Testing the Model</h2>&#13;
          <p>Once we’ve trained the model,<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="fine-tuning" data-tertiary="saving and testing the model" data-type="indexterm" id="id1791"/><a contenteditable="false" data-primary="fine-tuning LLMs" data-secondary="saving and testing the model" data-type="indexterm" id="id1792"/> it’s a good idea to save it out for future use, and the <code>trainer</code> object makes this easy:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 10. Save Model</code>&#13;
<code class="n">trainer</code><code class="o">.</code><code class="n">save_model</code><code class="p">(</code><code class="s2">"./final_model"</code><code class="p">)</code>&#13;
 </pre>&#13;
          <p>Once we’ve saved the model, we can start using it. To that end, let’s create a helper function that takes in the input text, tokenizes it, and then turns those tokens into a set of input vectors of keys and values (k, v):</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># 11. Example Usage</code>&#13;
<code class="k">def</code> <code class="nf">predict_sentiment</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>&#13;
    <code class="n">inputs</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code>&#13;
        <code class="n">text</code><code class="p">,</code>&#13;
        <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
        <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
        <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code>&#13;
    <code class="p">)</code>&#13;
    <code class="n">inputs</code> <code class="o">=</code> <code class="p">{</code><code class="n">k</code><code class="p">:</code> <code class="n">v</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">inputs</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code>&#13;
 </pre>&#13;
          <p>We can then use PyTorch in inference mode to get the outputs from those inputs and turn them into a set of predictions:</p>&#13;
          <pre data-code-language="python" data-type="programlisting">    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>&#13;
        <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="o">**</code><code class="n">inputs</code><code class="p">)</code>&#13;
        <code class="n">predictions</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">outputs</code><code class="o">.</code><code class="n">logits</code><code class="p">,</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>&#13;
 </pre>&#13;
          <p>The returned predictions will be a tensor with two dimensions. Neuron 0 is the probability that the prediction is negative, and neuron 1 is the probability that the prediction is positive. Therefore, we can look at the positive probability and return a sentiment and confidence with its values. We could also have done the same with the negative one; it’s purely arbitrary:</p>&#13;
          <pre data-code-language="python" data-type="programlisting">    <code class="n">positive_prob</code> <code class="o">=</code> <code class="n">predictions</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>&#13;
    <code class="k">return</code> <code class="p">{</code>&#13;
        <code class="s1">'sentiment'</code><code class="p">:</code> <code class="s1">'positive'</code> <code class="k">if</code> <code class="n">positive_prob</code> <code class="o">&gt;</code> <code class="mf">0.5</code> <code class="k">else</code> <code class="s1">'negative'</code><code class="p">,</code>&#13;
        <code class="s1">'confidence'</code><code class="p">:</code> <code class="n">positive_prob</code> <code class="k">if</code> <code class="n">positive_prob</code> <code class="o">&gt;</code> <code class="mf">0.5</code> <code class="k">else</code> <code class="mi">1</code> <code class="err">–</code> <code class="n">positive_prob</code>&#13;
    <code class="p">}</code></pre>&#13;
          <p class="pagebreak-before">We can now test the prediction with code like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Test prediction</code>&#13;
<code class="n">test_text</code> <code class="o">=</code> <code class="s2">"This movie was absolutely fantastic! The acting was superb."</code>&#13;
<code class="n">result</code> <code class="o">=</code> <code class="n">predict_sentiment</code><code class="p">(</code><code class="n">test_text</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Test prediction for '</code><code class="si">{</code><code class="n">test_text</code><code class="si">}</code><code class="s2">':"</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Sentiment: </code><code class="si">{</code><code class="n">result</code><code class="p">[</code><code class="s1">'sentiment'</code><code class="p">]</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Confidence: </code><code class="si">{</code><code class="n">result</code><code class="p">[</code><code class="s1">'confidence'</code><code class="p">]</code><code class="si">:</code><code class="s2">.2%</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre>&#13;
          <p>And the output would look something like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">Test</code> <code class="n">prediction</code> <code class="k">for</code> <code class="s1">'This movie was absolutely fantastic! </code><code class="w"/>&#13;
                     <code class="n">The</code> <code class="n">acting</code> <code class="n">was</code> <code class="n">superb</code><code class="o">.</code><code class="s1">':</code><code class="w"/>&#13;
<code class="n">Sentiment</code><code class="p">:</code> <code class="n">positive</code>&#13;
<code class="n">Confidence</code><code class="p">:</code> <code class="mf">99.16</code><code class="o">%</code></pre>&#13;
          <p>We can see that this statement is positive, with high confidence!</p>&#13;
          <p>In this process, you can see how, step by step, you can fine-tune an existing LLM on new data to turn it into a classification engine! In many circumstances, this may be overkill (and training your own model instead of fine-tuning an LLM may be quicker and cheaper), but it’s certainly worth evaluating this process. Sometimes, even untuned LLMs will work well for classification! In my experience, using the general artificial-understanding nature of LLMs will lead to the creation of far more effective classifiers with stronger results.<a contenteditable="false" data-primary="" data-startref="ch16ft" data-type="indexterm" id="id1793"/><a contenteditable="false" data-primary="" data-startref="ch16ft2" data-type="indexterm" id="id1794"/> </p>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Prompt-Tuning an LLM" data-type="sect1"><div class="sect1" id="ch16_prompt_tuning_an_llm_1748550037720718">&#13;
        <h1>Prompt-Tuning an LLM</h1>&#13;
        <p>A lightweight alternative to<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="prompt-tuning" data-type="indexterm" id="ch16prmpt"/><a contenteditable="false" data-primary="prompt-tuning LLMs" data-type="indexterm" id="ch16prmpt2"/><a contenteditable="false" data-primary="prompt-tuning LLMs" data-secondary="about prompt-tuning" data-type="indexterm" id="id1795"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="prompt-tuning" data-tertiary="about prompt-tuning" data-type="indexterm" id="id1796"/><a contenteditable="false" data-primary="prompt-tuning LLMs" data-secondary="soft prompts" data-type="indexterm" id="id1797"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="prompt-tuning" data-tertiary="soft prompts" data-type="indexterm" id="id1798"/><a contenteditable="false" data-primary="soft prompts" data-type="indexterm" id="id1799"/> fine-tuning is <em>prompt tuning,</em> in which you can adapt a model to specific tasks. With prompt tuning, you do this by prepending trainable <em>soft prompts</em> to each input instead of modifying the model weights. These soft prompts will then be optimized during training. </p>&#13;
        <p>These soft prompts are like learned instructions that guide the model’s behavior. Unlike discrete text prompts (such as <code>Classify the sentiment</code>), the idea of soft prompts is that they exist in the model’s embedding space as continuous vectors. So, for example, when processing “This movie was great,” the model would see “[V1][V2]…[V20]This movie was great.” In this case, [V1][V2]...[V20] are vectors that will help steer the model toward the desired classification.</p>&#13;
        <p>Ultimately, the advantage here is efficiency. So instead of fine-tuning a model, amending its weights for each task, and saving the entire model for reuse, you only need to save the soft prompt vectors. These are much smaller, and they can help you have a suite of fine-tunes that you can easily use to guide the model to a specific task without needing to manage multiple models.</p>&#13;
        <p>Prompt tuning like this can actually match or exceed the performance of full fine-tuning, particularly with larger models, and it’s significantly more efficient. </p>&#13;
        <p>Now, let’s explore how to prompt-tune the BART LLM with the IMDb dataset in direct comparison to the fine-tuning earlier in this chapter.</p>&#13;
        <section data-pdf-bookmark="Preparing the Data" data-type="sect2"><div class="sect2" id="ch16_preparing_the_data_1748550037720772">&#13;
          <h2>Preparing the Data</h2>&#13;
          <p>Let’s start by preparing our data,<a contenteditable="false" data-primary="prompt-tuning LLMs" data-secondary="preparing the data" data-type="indexterm" id="id1800"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="prompt-tuning" data-tertiary="preparing the data" data-type="indexterm" id="id1801"/> loading it from the IMDb dataset, and setting up the virtual tokens. Here’s the code you’ll need:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Data preparation</code>&#13;
<code class="n">dataset</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"imdb"</code><code class="p">)</code>&#13;
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"bert-base-uncased"</code><code class="p">)</code>&#13;
<code class="n">max_length</code> <code class="o">=</code> <code class="mi">512</code>&#13;
<code class="n">num_virtual_tokens</code> <code class="o">=</code> <code class="mi">20</code>&#13;
 &#13;
<code class="k">def</code> <code class="nf">tokenize_function</code><code class="p">(</code><code class="n">examples</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">tokenizer</code><code class="p">(</code>&#13;
        <code class="n">examples</code><code class="p">[</code><code class="s2">"text"</code><code class="p">],</code>&#13;
        <code class="n">padding</code><code class="o">=</code><code class="s2">"max_length"</code><code class="p">,</code>&#13;
        <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
        <code class="n">max_length</code><code class="o">=</code><code class="n">max_length</code> <code class="o">-</code> <code class="n">num_virtual_tokens</code>&#13;
    <code class="p">)</code></pre>&#13;
          <p>This will also tokenize our incoming examples so you should note that the maximum length of any example will now be reduced by the number of virtual tokens. So, for example, with BERT, we have a maximum length of 512, but if we’re going to have 20 virtual tokens, then the sequence maximum length will now be 492.</p>&#13;
          <p>We’ll now load a subset of the data and try with 5,000 examples, instead of 25,000. You can experiment with this number and trade off smaller amounts for faster training against larger amounts for better accuracy.</p>&#13;
          <p>First, we’ll create the indices that we want to take from the dataset for training, and we’ll test them. Think of these as pointers to the records we’re interested in. We’re randomly sampling here:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Use only 5000 examples for training</code>&#13;
<code class="n">train_size</code> <code class="o">=</code> <code class="mi">5000</code>&#13;
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="n">train_indices</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]),</code> <code class="n">train_size</code><code class="p">,</code> &#13;
                                                        <code class="n">replace</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>&#13;
<code class="n">test_indices</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s2">"test"</code><code class="p">]),</code> <code class="n">train_size</code><code class="p">,</code> <code class="n">replace</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>&#13;
 &#13;
<code class="n">tokenized_train</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">tokenize_function</code><code class="p">,</code> <code class="n">batched</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
<code class="n">tokenized_test</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s2">"test"</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">tokenize_function</code><code class="p">,</code> <code class="n">batched</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>&#13;
          <p>Then, the last two lines define the mapping function, which simply takes the values from the dataset we’re interested in and tokenizes them. We’ll see that in the next step.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Creating the Data Loaders" data-type="sect2"><div class="sect2" id="ch16_creating_the_data_loaders_1748550037720816">&#13;
          <h2>Creating the Data Loaders</h2>&#13;
          <p>Now that we have sets of <a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="prompt-tuning" data-tertiary="creating the data loaders" data-type="indexterm" id="id1802"/><a contenteditable="false" data-primary="prompt-tuning LLMs" data-secondary="creating the data loaders" data-type="indexterm" id="id1803"/>tokenized training and test data, we want to turn them into data loaders. We’ll do this by first selecting the raw examples from the underlying data that match the content in our indices:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Create subset for training</code>&#13;
<code class="n">tokenized_train</code> <code class="o">=</code> <code class="n">tokenized_train</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="n">train_indices</code><code class="p">)</code>&#13;
<code class="n">tokenized_test</code> <code class="o">=</code> <code class="n">tokenized_test</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="n">test_indices</code><code class="p">)</code>&#13;
 </pre>&#13;
          <p>Then, we’ll set the format of the data that we’re interested in. There may be many <span class="keep-together">columns in</span> a dataset, but you won’t use them all for training. In this case, we’ll <span class="keep-together">want the <code>input_ids</code>,</span> which are the tokenized versions of our input content; the <span class="keep-together"><code>attention_mask</code>,</span> which is a set of vectors that tells us which tokens in the <code>input_ids</code> we should be interested in (this has the effect of filtering out padding or other nonsemantic tokens); and the label:</p>&#13;
          <pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">tokenized_train</code><code class="o">.</code><code class="n">set_format</code><code class="p">(</code><code class="nb">type</code><code class="o">=</code><code class="s2">"torch"</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"input_ids"</code><code class="p">,</code> &#13;
                                                  <code class="s2">"attention_mask"</code><code class="p">,</code> <code class="s2">"label"</code><code class="p">])</code>&#13;
<code class="n">tokenized_test</code><code class="o">.</code><code class="n">set_format</code><code class="p">(</code><code class="nb">type</code><code class="o">=</code><code class="s2">"torch"</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"input_ids"</code><code class="p">,</code> &#13;
                                                 <code class="s2">"attention_mask"</code><code class="p">,</code> <code class="s2">"label"</code><code class="p">])</code>&#13;
 </pre>&#13;
          <p>Now, we can specify the DataLoader that takes these training and test sets. I have a large batch size here because I was testing on a 40Gb GRAM GPU in Colab. In your environment, you may need to adjust these:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">train_dataloader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">tokenized_train</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
<code class="n">eval_dataloader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">tokenized_test</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">128</code><code class="p">)</code></pre>&#13;
          <p>Now that the data is processed and loaded into DataLoaders, we can go to the next step: defining the model.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Defining the Model" data-type="sect2"><div class="sect2" id="ch16_defining_the_model_1748550037720863">&#13;
          <h2>Defining the Model</h2>&#13;
          <p>First, let’s see how to instantiate<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="prompt-tuning" data-tertiary="defining the model" data-type="indexterm" id="ch16def"/><a contenteditable="false" data-primary="prompt-tuning LLMs" data-secondary="defining the model" data-type="indexterm" id="ch16def2"/> the model, and then we can go back to the raw definition. Typically, in our code, once we’ve set up our DataLoaders, we’ll want to create an instance of the model. We’ll use code like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Define the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">PromptTuningBERT</code><code class="p">(</code><code class="n">num_virtual_tokens</code><code class="o">=</code><code class="n">num_virtual_tokens</code><code class="p">,</code> &#13;
                         <code class="n">max_length</code><code class="o">=</code><code class="n">max_length</code><code class="p">)</code>&#13;
 &#13;
<code class="n">device</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s1">'cuda'</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">()</code> <code class="k">else</code> <code class="s1">'cpu'</code><code class="p">)</code>&#13;
 &#13;
<code class="n">model</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>&#13;
          <p>This keeps it nice and simple, and we’ll encapsulate the underlying BERT in an override for a prompt-tuning version. Now, as nice as it would be for transformers to have one, they don’t, so we need to create this class for ourselves.</p>&#13;
          <p>As we would with any PyTorch class that defines a model, we’ll create it with an <code>__init__</code> method to set it up and a forward method that PyTorch’s training loop will call during the forward pass. So, let’s start with the <code>__init__ </code>method and the class definition:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">PromptTuningBERT</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">model_name</code><code class="o">=</code><code class="s2">"bert-base-uncased"</code><code class="p">,</code> &#13;
                       <code class="n">num_virtual_tokens</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> &#13;
                       <code class="n">max_length</code><code class="o">=</code><code class="mi">512</code><code class="p">):</code>&#13;
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">bert</code> <code class="o">=</code> <code class="n">AutoModelForSequenceClassification</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>&#13;
                        <code class="n">model_name</code><code class="p">,</code> &#13;
                        <code class="n">num_labels</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">requires_grad_</code><code class="p">(</code><code class="kc">False</code><code class="p">)</code>&#13;
 &#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">n_tokens</code> <code class="o">=</code> <code class="n">num_virtual_tokens</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">max_length</code> <code class="o">=</code> <code class="n">max_length</code> <code class="o">-</code> <code class="n">num_virtual_tokens</code>&#13;
 &#13;
        <code class="n">vocab_size</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">vocab_size</code>&#13;
        <code class="n">token_ids</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">,</code> <code class="p">(</code><code class="n">num_virtual_tokens</code><code class="p">,))</code>&#13;
        <code class="n">word_embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">embeddings</code><code class="o">.</code><code class="n">word_embeddings</code>&#13;
        <code class="n">prompt_embeddings</code> <code class="o">=</code> <code class="n">word_embeddings</code><code class="p">(</code><code class="n">token_ids</code><code class="p">)</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">prompt_embeddings</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Parameter</code><code class="p">(</code><code class="n">prompt_embeddings</code><code class="p">)</code></pre>&#13;
          <p>There’s a lot going on here, so let’s break it down little by little. First of all, I set the defaults for the <code>num_virtual_tokens</code> to 50 and the <code>max_length</code> default to 512. If you don’t specify your own defaults when you instantiate the class, you’ll get these values. In this case, the calling code sets them to 20 and 512, respectively, but you’re free to experiment.</p>&#13;
          <p>Next, the code sets up the transformers <code>AutoModelForSequenceClassification</code> class to get BERT:</p>&#13;
          <pre data-code-language="python" data-type="programlisting">        <code class="bp">self</code><code class="o">.</code><code class="n">bert</code> <code class="o">=</code> <code class="n">AutoModelForSequenceClassification</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>&#13;
                        <code class="n">model_name</code><code class="p">,</code> &#13;
                        <code class="n">num_labels</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code></pre>&#13;
          <p>As with fine-tuning for IMDb, we’re interested in training the model to recognize two labels, so they’re set up here. However, one difference from fine-tuning is that we’re not going to change any of the weights within the BERT model itself, so we set that we don’t want gradients and freeze it like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">requires_grad_</code><code class="p">(</code><code class="kc">False</code><code class="p">)</code></pre>&#13;
          <p>The secret sauce in generating the soft prompts that we’re going to use comes at the end of the init. We’ll create a vector to contain our number of virtual tokens, and I just initialized it with random tokens from the vocabulary. There are smarter things that we might do here to make training more efficient over time, but for the sake of simplicity, let’s go with this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">token_ids</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">,</code> <code class="p">(</code><code class="n">num_virtual_tokens</code><code class="p">,))</code></pre>&#13;
          <p>The pretrained BERT model in transformers comes with embeddings, so we can use them to turn our list of random tokens into embeddings:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">word_embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">embeddings</code><code class="o">.</code><code class="n">word_embeddings</code>&#13;
<code class="n">prompt_embeddings</code> <code class="o">=</code> <code class="n">word_embeddings</code><code class="p">(</code><code class="n">token_ids</code><code class="p">)</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code></pre>&#13;
          <p>Importantly, we should now specify that the <code>prompt_embeddings</code> are parameters of the neural network. This will be important later, when we define the optimizer. We recently specified that all of the BERT parameters were frozen, but <em>these</em> parameters are not part of that and thus are not frozen, so they will be tweaked by the optimizer during training:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="bp">self</code><code class="o">.</code><code class="n">prompt_embeddings</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Parameter</code><code class="p">(</code><code class="n">prompt_embeddings</code><code class="p">)</code></pre>&#13;
          <p>We have now initialized a subclassed version of the tunable BERT, specified that we don’t want to amend its gradients, and created a set of soft prompts that we will append to the examples as we’re training—and we’ll tweak only those soft prompts to soft-tune the two output neurons. </p>&#13;
          <p>Now, let’s look at the <code>forward</code> function that will be called during the forward pass at training time. Given that we’ve set up everything, this is pretty straightforward:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_ids</code><code class="p">,</code> <code class="n">attention_mask</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>&#13;
    <code class="n">batch_size</code> <code class="o">=</code> <code class="n">input_ids</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
    <code class="n">input_ids</code> <code class="o">=</code> <code class="n">input_ids</code><code class="p">[:,</code> <code class="p">:</code><code class="bp">self</code><code class="o">.</code><code class="n">max_length</code><code class="p">]</code>&#13;
    <code class="n">attention_mask</code> <code class="o">=</code> <code class="n">attention_mask</code><code class="p">[:,</code> <code class="p">:</code><code class="bp">self</code><code class="o">.</code><code class="n">max_length</code><code class="p">]</code>&#13;
 &#13;
    <code class="n">embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">embeddings</code><code class="o">.</code><code class="n">word_embeddings</code><code class="p">(</code><code class="n">input_ids</code><code class="p">)</code>&#13;
    <code class="n">prompt_embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">prompt_embeddings</code><code class="o">.</code><code class="n">expand</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="err">–</code><code class="mi">1</code><code class="p">)</code>&#13;
    <code class="n">inputs_embeds</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">([</code><code class="n">prompt_embeddings</code><code class="p">,</code> <code class="n">embeddings</code><code class="p">],</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
 &#13;
    <code class="n">prompt_attention_mask</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">n_tokens</code><code class="p">,</code> &#13;
                                       <code class="n">device</code><code class="o">=</code><code class="n">attention_mask</code><code class="o">.</code><code class="n">device</code><code class="p">)</code>&#13;
    <code class="n">attention_mask</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">([</code><code class="n">prompt_attention_mask</code><code class="p">,</code> <code class="n">attention_mask</code><code class="p">],</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
 &#13;
    <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="p">(</code>&#13;
        <code class="n">inputs_embeds</code><code class="o">=</code><code class="n">inputs_embeds</code><code class="p">,</code>&#13;
        <code class="n">attention_mask</code><code class="o">=</code><code class="n">attention_mask</code><code class="p">,</code>&#13;
        <code class="n">labels</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code>&#13;
        <code class="n">return_dict</code><code class="o">=</code><code class="kc">True</code>&#13;
    <code class="p">)</code></pre>&#13;
          <p>Let’s look at it step-by-step. During the forward pass in training, this function will be passed batches of data. Therefore, we need to understand what the size of this batch is and then extract the <code>input_ids</code> (the tokens for the values read from the dataset) and the attention mask for that particular ID:</p>&#13;
          <pre data-code-language="python" data-type="programlisting">    <code class="n">batch_size</code> <code class="o">=</code> <code class="n">input_ids</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
    <code class="n">input_ids</code> <code class="o">=</code> <code class="n">input_ids</code><code class="p">[:,</code> <code class="p">:</code><code class="bp">self</code><code class="o">.</code><code class="n">max_length</code><code class="p">]</code>&#13;
    <code class="n">attention_mask</code> <code class="o">=</code> <code class="n">attention_mask</code><code class="p">[:,</code> <code class="p">:</code><code class="bp">self</code><code class="o">.</code><code class="n">max_length</code><code class="p">]</code>&#13;
 </pre>&#13;
          <p>We’ll also need to convert the <code>input_ids</code> into embeddings:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">embeddings</code><code class="o">.</code><code class="n">word_embeddings</code><code class="p">(</code><code class="n">input_ids</code><code class="p">)</code></pre>&#13;
          <p>Our soft prompts are also tokenized sentences. Originally, they were initialized to random words, and we’ll see over time that they’ll adjust appropriately. But for this step, these tokens need to be converted to embeddings:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">prompt_embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">prompt_embeddings</code><code class="o">.</code><code class="n">expand</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="err">–</code><code class="mi">1</code><code class="p">)</code>&#13;
 </pre>&#13;
          <p>The <code>expand</code> method just adds the batch size to the prompt embeddings. When we defined the class, we didn’t know how large each batch coming in would be (and the code is written to let you tweak that based on the size of your available memory), so using <code>expand(batch_size, –1, –1)</code> turns the vector of prompt embeddings, which was of shape <code>[1, num_prompt_tokens, embedding_dimensions]</code>, into <code>[batch_size, num_prompt_tokens, embedding_dimensions]</code>.</p>&#13;
          <p>Our soft prompt tuning involved prepending the soft embeddings to the embeddings for the actual input data, so we do that with this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">inputs_embeds</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">([</code><code class="n">prompt_embeddings</code><code class="p">,</code> <code class="n">embeddings</code><code class="p">],</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>&#13;
          <p>BERT uses an <code>attention_mask</code> to filter out the tokens we don’t want to worry about at training or inference time, which are usually the padding tokens. But we want BERT to pay attention to all of the soft prompt tokens, so we’ll set the attention mask for them to be all 1s and then append that to the incoming attention mask(s) for the training data. Here’s the code:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">prompt_attention_mask</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">n_tokens</code><code class="p">,</code> &#13;
                                   <code class="n">device</code><code class="o">=</code><code class="n">attention_mask</code><code class="o">.</code><code class="n">device</code><code class="p">)</code>&#13;
 &#13;
<code class="n">attention_mask</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">([</code><code class="n">prompt_attention_mask</code><code class="p">,</code> <code class="n">attention_mask</code><code class="p">],</code> &#13;
                            <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
 </pre>&#13;
          <p>Now that we’ve done all our tuning, we need to pass the data to the model to have it optimize and calculate the loss:</p>&#13;
          <pre data-code-language="python" data-type="programlisting">    <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="p">(</code>&#13;
        <code class="n">inputs_embeds</code><code class="o">=</code><code class="n">inputs_embeds</code><code class="p">,</code>&#13;
        <code class="n">attention_mask</code><code class="o">=</code><code class="n">attention_mask</code><code class="p">,</code>&#13;
        <code class="n">labels</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code>&#13;
        <code class="n">return_dict</code><code class="o">=</code><code class="kc">True</code>&#13;
    <code class="p">)</code>&#13;
 </pre>&#13;
          <p>We will see how this data is used in the training loop, next.<a contenteditable="false" data-primary="" data-startref="ch16def" data-type="indexterm" id="id1804"/><a contenteditable="false" data-primary="" data-startref="ch16def2" data-type="indexterm" id="id1805"/></p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Training the Model" data-type="sect2"><div class="sect2" id="ch16_training_the_model_1748550037720911">&#13;
          <h2>Training the Model</h2>&#13;
          <p>The key to this training is that<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="prompt-tuning" data-tertiary="training the model" data-type="indexterm" id="ch16trn"/><a contenteditable="false" data-primary="prompt-tuning LLMs" data-secondary="training the model" data-type="indexterm" id="ch16trn2"/> we’re going to do a full, normal training loop but in a special circumstance. In this case, we previously froze <em>everything</em> in the BERT model, <em>except</em> for the soft prompts that we defined as model parameters. Therefore, say we define the optimizer like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">AdamW</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">1e-2</code><code class="p">)</code></pre>&#13;
          <p>In this case, we’re using standard code, telling it to tweak the model’s parameters. But the only ones that are available to tune are the soft prompts, so this should be quick!</p>&#13;
          <p>Note that the value for the learning rate is quite large. This helps the system learn quickly, but in a real system, you’d likely want the value to be smaller—or at least adjustable, starting large and then shrinking in later epochs.</p>&#13;
          <p>So now, let’s get into training. First, we’ll set up the training loop:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">num_epochs</code> <code class="o">=</code> <code class="mi">3</code>&#13;
 &#13;
<code class="c1"># Perform the training</code>&#13;
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_epochs</code><code class="p">):</code>&#13;
    <code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>&#13;
    <code class="n">total_train_loss</code> <code class="o">=</code> <code class="mi">0</code></pre>&#13;
          <section data-pdf-bookmark="Managing data batches" data-type="sect3"><div class="sect3" id="ch16_managing_data_batches_1748550037720965">&#13;
            <h3>Managing data batches</h3>&#13;
            <p>For each batch, we’ll get the columns (<code>input_ids</code> and <code>attention_masks</code>) as well as the labels and pass them to the model:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="k">for</code> <code class="n">batch</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="p">(</code><code class="n">train_dataloader</code><code class="p">,</code> &#13;
                  <code class="n">desc</code><code class="o">=</code><code class="sa">f</code><code class="s1">'Training Epoch </code><code class="si">{</code><code class="n">epoch</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s1">'</code><code class="p">):</code>&#13;
    <code class="n">batch</code> <code class="o">=</code> <code class="p">{</code><code class="n">k</code><code class="p">:</code> <code class="n">v</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">batch</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code>&#13;
    <code class="n">labels</code> <code class="o">=</code> <code class="n">batch</code><code class="o">.</code><code class="n">pop</code><code class="p">(</code><code class="s1">'label'</code><code class="p">)</code>&#13;
    <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="o">**</code><code class="n">batch</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="n">labels</code><code class="p">)</code></pre>&#13;
            <p>This looks a little different from ones earlier in this book, but it’s pretty much doing the same thing. The <code>tqdm</code> code just gives us a status bar because we’re training. We read the data batch by batch, but we want the data to be on the same device as the model. So, for example, if the model is running on a GPU, we want it to access data in the GPU’s memory. Therefore, this line will iterate through each column, reading the key and passing the value to the device:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="n">batch</code> <code class="o">=</code> <code class="p">{</code><code class="n">k</code><code class="p">:</code> <code class="n">v</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">batch</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code></pre>&#13;
            <p>It redefines the batch that was read in to ensure that the data is on the same device as the model. But we don’t want the labels to be in the batch because the model expects them to be fed in separately, so we remove them from the batch with the <code>pop()</code> method:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="n">labels</code> <code class="o">=</code> <code class="n">batch</code><code class="o">.</code><code class="n">pop</code><code class="p">(</code><code class="s1">'label'</code><code class="p">)</code></pre>&#13;
            <p>Now, we can use the shorthand of <code>**batch</code> to pass the set of input values (in this case, the <code>input_ids</code> and the <code>attention_mask</code>) to the forward method of the model and unpack the dictionary along with the labels, like this:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="o">**</code><code class="n">batch</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="n">labels</code><code class="p">)</code></pre>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Handling the loss" data-type="sect3"><div class="sect3" id="ch16_handling_the_loss_1748550037721011">&#13;
            <h3>Handling the loss</h3>&#13;
            <p>The forward pass sends the data to the model and gets the loss back. We use this to update our overall loss, and we can then backward pass:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="n">loss</code> <code class="o">=</code> <code class="n">outputs</code><code class="o">.</code><code class="n">loss</code>&#13;
<code class="n">total_train_loss</code> <code class="o">+=</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>&#13;
 &#13;
<code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code></pre>&#13;
            <p>With the gradients flowing back, the optimizer can now do its job.</p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Optimizing for loss" data-type="sect3"><div class="sect3" id="ch16_optimizing_for_loss_1748550037721055">&#13;
            <h3>Optimizing for loss</h3>&#13;
            <p>Remembering that the <code>model.parameters()</code> will only manage the <em>trainable unfrozen</em> parameters, we can now call the optimizer. I added something called <em>gradient clipping</em> here to make the training a little more efficient, but the rest is just calling the optimizer’s next step and then zeroing out the gradients so we can use them next time:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="n">clip_grad_norm_</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">max_grad_norm</code><code class="p">)</code>  <code class="c1"># Add here</code>&#13;
<code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>&#13;
<code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code></pre>&#13;
            <div data-type="note" epub:type="note"><h6>Note</h6>&#13;
              <p>The idea behind <em>gradient clipping</em> is that sometimes, during backpropagation, the gradients can be too large and the optimizer might take very large steps. This can lead to a problem called <em>exploding gradients</em>, in which the value changes hide the nuances of what might be learned. But clipping scales the gradients down if their values grow too large, and in a situation like this one, they may not even be necessary.<a contenteditable="false" data-primary="" data-startref="ch16trn" data-type="indexterm" id="id1806"/><a contenteditable="false" data-primary="" data-startref="ch16trn2" data-type="indexterm" id="id1807"/></p>&#13;
            </div>&#13;
          </div></section>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Evaluation During Training" data-type="sect2"><div class="sect2" id="ch16_evaluation_during_training_1748550037721099">&#13;
          <h2>Evaluation During Training</h2>&#13;
          <p>We also have a set of test data,<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="prompt-tuning" data-tertiary="evaluation during training" data-type="indexterm" id="id1808"/><a contenteditable="false" data-primary="prompt-tuning LLMs" data-secondary="evaluation during training" data-type="indexterm" id="id1809"/> so we can evaluate how the model performs during the training cycle. In each epoch, once the forward and backward passes are done and the model parameters are reset, we can switch the model into evaluation mode and then start passing all of the test data through it to get inference. We’ll also compare the results of the inference against the actual labels to calculate accuracy:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>&#13;
<code class="n">val_accuracy</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">total_val_loss</code> <code class="o">=</code> <code class="mi">0</code></pre>&#13;
          <p>Then, we’ll have similar code—but this time, it will be to read the eval batches, turn them into outputs with labels, and get predictions and loss values from the model:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>&#13;
    <code class="k">for</code> <code class="n">batch</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="p">(</code><code class="n">eval_dataloader</code><code class="p">,</code> <code class="n">desc</code><code class="o">=</code><code class="s1">'Validating'</code><code class="p">):</code>&#13;
        <code class="n">batch</code> <code class="o">=</code> <code class="p">{</code><code class="n">k</code><code class="p">:</code> <code class="n">v</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">batch</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code>&#13;
        <code class="n">labels</code> <code class="o">=</code> <code class="n">batch</code><code class="o">.</code><code class="n">pop</code><code class="p">(</code><code class="s1">'label'</code><code class="p">)</code>&#13;
 &#13;
        <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="o">**</code><code class="n">batch</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="n">labels</code><code class="p">)</code>&#13;
        <code class="n">total_val_loss</code> <code class="o">+=</code> <code class="n">outputs</code><code class="o">.</code><code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>&#13;
 &#13;
        <code class="n">predictions</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">outputs</code><code class="o">.</code><code class="n">logits</code><code class="p">,</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>&#13;
        <code class="n">val_accuracy</code><code class="o">.</code><code class="n">extend</code><code class="p">((</code><code class="n">predictions</code> <code class="o">==</code> <code class="n">labels</code><code class="p">)</code><code class="o">.</code><code class="n">cpu</code><code class="p">()</code><code class="o">.</code><code class="n">numpy</code><code class="p">())</code></pre>&#13;
          <p>Once we’ve calculated these values, then at the end of each epoch, we can report on them and on training loss. </p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Reporting Training Metrics" data-type="sect2"><div class="sect2" id="ch16_reporting_training_metrics_1748550037721144">&#13;
          <h2>Reporting Training Metrics</h2>&#13;
          <p>During training in each epoch,<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="prompt-tuning" data-tertiary="reporting training metrics" data-type="indexterm" id="id1810"/><a contenteditable="false" data-primary="prompt-tuning LLMs" data-secondary="reporting training metrics" data-type="indexterm" id="id1811"/> we calculated the training loss, so we can now get the average across all records. We can do the same thing with the validation loss and (of course) with the validation accuracy and then report on them:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">avg_train_loss</code> <code class="o">=</code> <code class="n">total_train_loss</code> <code class="o">/</code> <code class="nb">len</code><code class="p">(</code><code class="n">train_dataloader</code><code class="p">)</code>&#13;
<code class="n">avg_val_loss</code> <code class="o">=</code> <code class="n">total_val_loss</code> <code class="o">/</code> <code class="nb">len</code><code class="p">(</code><code class="n">eval_dataloader</code><code class="p">)</code>&#13;
<code class="n">val_accuracy</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">val_accuracy</code><code class="p">)</code>&#13;
 &#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Epoch </code><code class="si">{</code><code class="n">epoch</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s2">:"</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Average training loss: </code><code class="si">{</code><code class="n">avg_train_loss</code><code class="si">:</code><code class="s2">.4f</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Average validation loss: </code><code class="si">{</code><code class="n">avg_val_loss</code><code class="si">:</code><code class="s2">.4f</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Validation accuracy: </code><code class="si">{</code><code class="n">val_accuracy</code><code class="si">:</code><code class="s2">.4f</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre>&#13;
&#13;
<p>Running this training for three epochs gives us this:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">Training</code> <code class="n">Epoch</code> <code class="mi">1</code><code class="p">:</code> <code class="mi">100</code><code class="o">%|</code><code class="err">██████████</code><code class="o">|</code> <code class="mi">79</code><code class="o">/</code><code class="mi">79</code> <code class="p">[</code><code class="mi">01</code><code class="p">:</code><code class="mi">01</code><code class="o">&lt;</code><code class="mi">00</code><code class="p">:</code><code class="mi">00</code><code class="p">,</code> <code class="mf">1.28</code><code class="n">it</code><code class="o">/</code><code class="n">s</code><code class="p">]</code>&#13;
<code class="n">Validating</code><code class="p">:</code> <code class="mi">100</code><code class="o">%|</code><code class="err">██████████</code><code class="o">|</code> <code class="mi">40</code><code class="o">/</code><code class="mi">40</code> <code class="p">[</code><code class="mi">00</code><code class="p">:</code><code class="mi">27</code><code class="o">&lt;</code><code class="mi">00</code><code class="p">:</code><code class="mi">00</code><code class="p">,</code> <code class="mf">1.44</code><code class="n">it</code><code class="o">/</code><code class="n">s</code><code class="p">]</code>&#13;
&#13;
<code class="n">Epoch</code> <code class="mi">1</code><code class="p">:</code>&#13;
<code class="n">Average</code> <code class="n">training</code> <code class="n">loss</code><code class="p">:</code> <code class="mf">0.6559</code>&#13;
<code class="n">Average</code> <code class="n">validation</code> <code class="n">loss</code><code class="p">:</code> <code class="mf">0.6037</code>&#13;
<code class="n">Validation</code> <code class="n">accuracy</code><code class="p">:</code> <code class="mf">0.8036</code>&#13;
<code class="n">Training</code> <code class="n">Epoch</code> <code class="mi">2</code><code class="p">:</code> <code class="mi">100</code><code class="o">%|</code><code class="err">██████████</code><code class="o">|</code> <code class="mi">79</code><code class="o">/</code><code class="mi">79</code> <code class="p">[</code><code class="mi">01</code><code class="p">:</code><code class="mi">01</code><code class="o">&lt;</code><code class="mi">00</code><code class="p">:</code><code class="mi">00</code><code class="p">,</code> <code class="mf">1.28</code><code class="n">it</code><code class="o">/</code><code class="n">s</code><code class="p">]</code>&#13;
<code class="n">Validating</code><code class="p">:</code> <code class="mi">100</code><code class="o">%|</code><code class="err">██████████</code><code class="o">|</code> <code class="mi">40</code><code class="o">/</code><code class="mi">40</code> <code class="p">[</code><code class="mi">00</code><code class="p">:</code><code class="mi">27</code><code class="o">&lt;</code><code class="mi">00</code><code class="p">:</code><code class="mi">00</code><code class="p">,</code> <code class="mf">1.44</code><code class="n">it</code><code class="o">/</code><code class="n">s</code><code class="p">]</code>&#13;
 &#13;
<code class="n">Epoch</code> <code class="mi">2</code><code class="p">:</code>&#13;
<code class="n">Average</code> <code class="n">training</code> <code class="n">loss</code><code class="p">:</code> <code class="mf">0.6112</code>&#13;
<code class="n">Average</code> <code class="n">validation</code> <code class="n">loss</code><code class="p">:</code> <code class="mf">0.5854</code>&#13;
<code class="n">Validation</code> <code class="n">accuracy</code><code class="p">:</code> <code class="mf">0.8386</code>&#13;
<code class="n">Training</code> <code class="n">Epoch</code> <code class="mi">3</code><code class="p">:</code> <code class="mi">100</code><code class="o">%|</code><code class="err">██████████</code><code class="o">|</code> <code class="mi">79</code><code class="o">/</code><code class="mi">79</code> <code class="p">[</code><code class="mi">01</code><code class="p">:</code><code class="mi">01</code><code class="o">&lt;</code><code class="mi">00</code><code class="p">:</code><code class="mi">00</code><code class="p">,</code> <code class="mf">1.28</code><code class="n">it</code><code class="o">/</code><code class="n">s</code><code class="p">]</code>&#13;
<code class="n">Validating</code><code class="p">:</code> <code class="mi">100</code><code class="o">%|</code><code class="err">██████████</code><code class="o">|</code> <code class="mi">40</code><code class="o">/</code><code class="mi">40</code> <code class="p">[</code><code class="mi">00</code><code class="p">:</code><code class="mi">27</code><code class="o">&lt;</code><code class="mi">00</code><code class="p">:</code><code class="mi">00</code><code class="p">,</code> <code class="mf">1.44</code><code class="n">it</code><code class="o">/</code><code class="n">s</code><code class="p">]</code>&#13;
 &#13;
<code class="n">Epoch</code> <code class="mi">3</code><code class="p">:</code>&#13;
<code class="n">Average</code> <code class="n">training</code> <code class="n">loss</code><code class="p">:</code> <code class="mf">0.5799</code>&#13;
<code class="n">Average</code> <code class="n">validation</code> <code class="n">loss</code><code class="p">:</code> <code class="mf">0.5270</code>&#13;
<code class="n">Validation</code> <code class="n">accuracy</code><code class="p">:</code> <code class="mf">0.8736</code>&#13;
</pre>&#13;
&#13;
<p>This was done on an A100 in Colab with 40 Gb of GRAM, and as you can see, each epoch only took about 1 minute to train and 30 seconds to evaluate. </p>&#13;
          <p>By the end, the average training loss had dropped from about 0.65 to 0.58. The accuracy was 0.8736. So, it’s likely overfitting because we only trained for three epochs.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Saving the Prompt Embeddings" data-type="sect2"><div class="sect2" id="ch16_saving_the_prompt_embeddings_1748550037721189">&#13;
          <h2>Saving the Prompt Embeddings</h2>&#13;
          <p>What’s really nice about this approach<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="prompt-tuning" data-tertiary="saving prompt embeddings" data-type="indexterm" id="id1812"/><a contenteditable="false" data-primary="prompt-tuning LLMs" data-secondary="saving prompt embeddings" data-type="indexterm" id="id1813"/> is that you can simply save out the prompt embeddings when you’re done. You can also load them back in for inference later, as you’ll see in the next section:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">torch</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">prompt_embeddings</code><code class="p">,</code> <code class="s2">"imdb_prompt_embeddings.pt"</code><code class="p">)</code></pre>&#13;
          <p>What I find really cool about this is that this file is relatively small (61 K), and it doesn’t require you to amend the underlying model in any way. Thus, in an application, you could potentially have a number of these prompt-tuning files and hot-swap and replace them as needed so that you can have multiple models that you can orchestrate, which is the basis for an agentic solution.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Performing Inference with the Model" data-type="sect2"><div class="sect2" id="ch16_performing_inference_with_the_model_1748550037721234">&#13;
          <h2>Performing Inference with the Model</h2>&#13;
          <p>To perform inference with<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="prompt-tuning" data-tertiary="inference" data-type="indexterm" id="ch16inf"/><a contenteditable="false" data-primary="prompt-tuning LLMs" data-secondary="inference" data-type="indexterm" id="ch16inf2"/><a contenteditable="false" data-primary="inference" data-secondary="prompt-tuning LLMs" data-type="indexterm" id="ch16inf3"/> a prompt-tuned model, you’ll simply define the model with the soft prompts and then, instead of training them, load the pretrained soft prompts back from disk. We’ll explore that in this section. If you don’t want to train your own model, then <a href="https://github.com/lmoroney/PyTorch-Book-FIles">in the download</a>, I’ve provided soft prompts from a version of the model that was trained for 30 epochs instead of 3.</p>&#13;
          <p>For tidier encapsulation, I created a class that is similar to the one we used for training but that is just for inference. I call it <code>PromptTunedBERTInference</code>, and here’s its initializer:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">PromptTunedBERTInference</code><code class="p">:</code>&#13;
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">model_name</code><code class="o">=</code><code class="s2">"bert-base-uncased"</code><code class="p">,</code> &#13;
                       <code class="n">prompt_path</code><code class="o">=</code><code class="s2">"imdb_prompt_embeddings.pt"</code><code class="p">):</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_name</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">model</code> <code class="o">=</code>  &#13;
                  <code class="n">AutoModelForSequenceClassification</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>&#13;
                                            <code class="n">model_name</code><code class="p">,</code> <code class="n">num_labels</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">prompt_embeddings</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">prompt_path</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">device</code> <code class="o">=</code> &#13;
           <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s1">'cuda'</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">()</code> <code class="k">else</code> <code class="s1">'cpu'</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">device</code><code class="p">)</code></pre>&#13;
          <p>It’s very similar to the initializer for the trainable one, except for a couple of important points. The first is that because we’re <em>only</em> using it for inference, I’ve set it into eval mode:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code></pre>&#13;
          <p>The second is that we don’t need to train the embeddings and do all the associated plumbing—instead, we just load them from the specified path:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="bp">self</code><code class="o">.</code><code class="n">prompt_embeddings</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">prompt_path</code><code class="p">)</code></pre>&#13;
          <p>And that’s it! As you can see, it’s quite lightweight and pretty straightforward. It won’t have a <code>forward</code> function because we’re not training it, but let’s add a <code>predict</code> function that encapsulates doing inference with it.</p>&#13;
          <section data-pdf-bookmark="The predict function" data-type="sect3"><div class="sect3" id="ch16_the_predict_function_1748550037721283">&#13;
            <h3>The predict function</h3>&#13;
            <p>The job of the<code> predict</code> function is to take in the string(s) that we want to perform inference with, tokenize it (them), and then pass it to the model with the soft tokens prepended. Let’s take a look at the code, piece by piece.</p>&#13;
            <p>First, let’s define it and have it accept text that it will then tokenize:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">text</code><code class="p">):</code>&#13;
    <code class="n">inputs</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">tokenizer</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> &#13;
                        <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
                        <code class="n">max_length</code><code class="o">=</code><code class="mi">512</code><code class="o">-</code><code class="bp">self</code><code class="o">.</code><code class="n">prompt_embeddings</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
                        <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code>&#13;
    <code class="n">inputs</code> <code class="o">=</code> <code class="p">{</code><code class="n">k</code><code class="p">:</code> <code class="n">v</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">device</code><code class="p">)</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">inputs</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code></pre>&#13;
            <p>The text will be tokenized up to the maximum length, less the size of the soft prompt, and then each of the items in the input will be loaded into a dictionary. Note that the tokenizer will return multiple columns for the text—usually, the tokens and the attention mask—so we’ll follow this approach to turn them into a set of key-value pairs that are easy for us to work with later.</p>&#13;
            <p>Now that we have our inputs, it’s time to pass them to the model. We’ll start by putting <code>torch</code> into <code>no_grad()</code> mode because we’re not interested in training gradients. We’ll then get the embeddings for each of our tokens:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>&#13;
    <code class="n">embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">embeddings</code><code class="o">.</code><code class="n">word_embeddings</code><code class="p">(</code>&#13;
                                        <code class="n">inputs</code><code class="p">[</code><code class="s1">'input_ids'</code><code class="p">])</code>&#13;
 &#13;
    <code class="n">batch_size</code> <code class="o">=</code> <code class="n">embeddings</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
 &#13;
    <code class="n">prompt_embeds</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">prompt_embeddings</code><code class="o">.</code><code class="n">expand</code><code class="p">(</code>&#13;
                         <code class="n">batch_size</code><code class="p">,</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="err">–</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">device</code><code class="p">)</code>&#13;
 &#13;
    <code class="n">inputs_embeds</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">([</code><code class="n">prompt_embeds</code><code class="p">,</code> <code class="n">embeddings</code><code class="p">],</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>&#13;
            <p class="pagebreak-before less_space">We have an attention mask for the input that’s generated by the tokenizer, but we don’t have one for the soft prompt. So, let’s create one and then append it to the input attention mask:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="n">attention_mask</code> <code class="o">=</code> <code class="n">inputs</code><code class="p">[</code><code class="s1">'attention_mask'</code><code class="p">]</code>&#13;
<code class="n">prompt_attention</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">prompt_embeddings</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
                            <code class="n">device</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">device</code><code class="p">)</code>&#13;
<code class="n">attention_mask</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">([</code><code class="n">prompt_attention</code><code class="p">,</code> <code class="n">attention_mask</code><code class="p">],</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>&#13;
            <p>Now that we have everything in place, we can pass our data to the model to get our inferences back:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="n">outputs</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="p">(</code><code class="n">inputs_embeds</code><code class="o">=</code><code class="n">inputs_embeds</code><code class="p">,</code>&#13;
                   <code class="n">attention_mask</code><code class="o">=</code><code class="n">attention_mask</code><code class="p">)</code></pre>&#13;
            <p>The outputs will be the logits from the two neurons, one representing positive sentiment and the other negative. We can then Softmax these to get the prediction:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="n">probs</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">outputs</code><code class="o">.</code><code class="n">logits</code><code class="p">,</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>&#13;
<code class="k">return</code> <code class="p">{</code><code class="s2">"prediction"</code><code class="p">:</code> <code class="n">outputs</code><code class="o">.</code><code class="n">logits</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code>&#13;
       <code class="s2">"confidence"</code><code class="p">:</code> <code class="n">probs</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">values</code><code class="o">.</code><code class="n">item</code><code class="p">()}</code></pre>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Usage example" data-type="sect3"><div class="sect3" id="ch16_usage_example_1748550037721338">&#13;
            <h3>Usage example</h3>&#13;
            <p>Using this class for a prediction is then pretty straightforward. We create an instance of the class and pass a string to it to get results. The results will contain a prediction and a confidence value, which we can then output:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="c1"># Usage example</code>&#13;
<code class="k">if</code> <code class="vm">__name__</code> <code class="o">==</code> <code class="s2">"__main__"</code><code class="p">:</code>&#13;
    <code class="n">model</code> <code class="o">=</code> <code class="n">PromptTunedBERTInference</code><code class="p">()</code>&#13;
    <code class="n">result</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="s2">"This movie was great!"</code><code class="p">)</code>&#13;
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Prediction: </code><code class="si">{</code><code class="s1">'Positive'</code> &#13;
                          <code class="k">if</code> <code class="n">result</code><code class="p">[</code><code class="s1">'prediction'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">1</code> <code class="k">else</code> <code class="s1">'Negative'</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>&#13;
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Confidence: </code><code class="si">{</code><code class="n">result</code><code class="p">[</code><code class="s1">'confidence'</code><code class="p">]</code><code class="si">:</code><code class="s2">.2f</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>&#13;
 </pre>&#13;
            <p>One note you might see with prompt tuning is low confidence values that can lead to mis-predictions, especially with binary classifiers like this one. It’s good to explore your inference to make sure that it’s working well, and there are also techniques that you could explore to ensure that the logits are giving the values you want. These include setting the temperature of the Softmax, using more prompt tokens to give the model more capacity, and initializing the prompt tokens with sentiment-related words (instead of random tokens, like we did here).<a contenteditable="false" data-primary="" data-startref="ch16prmpt" data-type="indexterm" id="id1814"/><a contenteditable="false" data-primary="" data-startref="ch16prmpt2" data-type="indexterm" id="id1815"/><a contenteditable="false" data-primary="" data-startref="ch16inf" data-type="indexterm" id="id1816"/><a contenteditable="false" data-primary="" data-startref="ch16inf2" data-type="indexterm" id="id1817"/><a contenteditable="false" data-primary="" data-startref="ch16inf3" data-type="indexterm" id="id1818"/></p>&#13;
          </div></section>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section class="pagebreak-before less_space" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch16_summary_1748550037721383">&#13;
        <h1>Summary</h1>&#13;
        <p>In this chapter, we explored different methods for customizing LLMs with our own data. We looked at two main approaches: traditional fine-tuning and prompt tuning.</p>&#13;
        <p>Using the IMDb dataset, you saw how to fine-tune BERT for sentiment analysis and walked through all the steps—from data preparation, to model configuration, to training and evaluation. The model achieved an impressive 95% accuracy in sentiment classification in just a few epochs. </p>&#13;
        <p>However, fine-tuning may not be appropriate in all cases, and to that end, you explored a lightweight alternative called prompt tuning. Instead of modifying model weights, the idea here was to prepend trainable soft prompts to inputs, which are optimized during training. This approach provides significant advantages in that it can be much faster and it doesn’t change the underlying model. In this case, the tuned prompts could be saved (and they were only a few Kb) and then reloaded to program the model to perform the desired task. You then went through a full implementation, showing you how to create, train, and save these soft prompts, plus load them back to perform inference.</p>&#13;
        <p>In the next chapter, we’ll explore how you can serve LLMs, including customized ones. I’ll explain how to do this in your own data center by using Ollama, which is a powerful tool for handling the serving and management of LLMs. You’ll learn how to take models and turn them into services, and we’ll also explore how to set up Ollama and use it over HTTP to talk with models in your data center.</p>&#13;
      </div></section>&#13;
    </div></section></body></html>