- en: 11 Neural networks for image classification and object detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 用于图像分类和物体检测的神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using deeper neural networks for image classification and object detection
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更深的神经网络进行图像分类和物体检测
- en: Understanding convolutional neural networks and other deep neural network architectures
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解卷积神经网络和其他深度神经网络架构
- en: Correcting imbalances in neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纠正神经网络中的不平衡
- en: 'If a human is shown the image in figure [11.1](#fig-bird-plane-superman), they
    can instantly recognize the objects in it, categorizing them as a bird, a plane,
    and Superman. In image classification, we want to impart this capability to computers—the
    ability to recognize objects in an image and classify them into one or more known
    and predetermined categories. Apart from identifying the object categories, we
    can also identify the location of the objects in the image. An object’s location
    can be described by a *bounding box*: a rectangle whose sides are parallel to
    coordinate axes. A bounding box is typically specified by four parameters: [(*xtl*,
    *ytl*),(*xbr*, *ybr*)], where (*xtl*, *y**tl*) are the xy coordinates of the top-left
    corner and (*xbr*, *ybr*) are the *xy* coordinates of the bottom-right corner
    of the bounding'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果人类看到图 [11.1](#fig-bird-plane-superman) 中的图像，他们可以立即识别其中的物体，将它们分类为鸟类、飞机和超人。在图像分类中，我们希望赋予计算机这种能力——在图像中识别物体并将它们分类到一个或多个已知和预定的类别中。除了识别物体类别外，我们还可以识别图像中物体的位置。物体的位置可以用一个
    *边界框* 来描述：一个边与坐标轴平行的矩形。边界框通常由四个参数指定：[(*xtl*, *ytl*),(*xbr*, *ybr*)]，其中 (*xtl*,
    *y**tl*) 是边界框左上角的 xy 坐标，(*xbr*, *ybr*) 是边界框右下角的 xy 坐标。
- en: box. The problem of identifying and categorizing the objects present in the
    image is called *image classification*. If we also want to identify their location
    in the image, it is referred to as *object detection*. Image classification and
    object detection are some of the most fundamental problems in computer vision.
    While the human brain can both classify and localize objects in images almost
    intuitively, how do we train a machine to do this? Before deep learning, computer
    vision techniques involved hand-crafting image features (to encode color, edges,
    and shapes) and designing rules on top of these features to classify/localize
    objects. However, this is not a scalable approach because images are extremely
    complex and varied. Think of a simple object like an automobile. It can come in
    various sizes, shapes, and colors. It can be seen from afar or close (scales),
    from various viewpoints (perspectives), and on a cloudy day or a sunny day (lighting
    conditions). The car can be on a busy street or a mountain road backgrounds).
    It is nearly impossible to engineer features and rules that can handle all such
    variations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们还想识别它们在图像中的位置，这被称为 *物体检测*。图像分类和物体检测是计算机视觉中最基本的问题之一。虽然人脑几乎可以直观地对图像中的物体进行分类和定位，但我们如何训练机器来做这件事呢？在深度学习之前，计算机视觉技术涉及手动制作图像特征（以编码颜色、边缘和形状）并在这些特征之上设计规则来分类/定位物体。然而，这不是一个可扩展的方法，因为图像极其复杂且多样化。想想一个简单的物体，比如汽车。它可以有各种大小、形状和颜色。它可以从远处或近处（尺度）看到，从不同的视角（视角），在阴天或晴天（光照条件）。汽车可以停在繁忙的街道或山路上（背景）。几乎不可能设计出可以处理所有这些变化的特征和规则。
- en: '![](../../OEBPS/Images/CH11_F01_Chaudhury.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH11_F01_Chaudhury.jpg)'
- en: Figure 11.1 Is it a bird? Is it a plane? Is it Superman?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 是鸟吗？是飞机吗？是超人吗？
- en: 'Over the last 10 years, a new class of algorithms has emerged: convolutional
    neural networks (CNNs). They do not rely on hand-engineered features but instead
    *learn* the relevant features from *data*. These models have shown tremendous
    success in several computer vision tasks, achieving (and sometimes even surpassing)
    human-level accuracy. They are increasingly used in the industry for applications
    ranging from medical diagnostics to e-commerce to manufacturing. In this chapter,
    we detail some of the most popular deep neural network architectures used for
    image classification and object detection. We look at some of their salient features,
    take a deep dive into the architectural details to understand how and why they
    work, and apply them to real-world problems.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的 10 年中，一类新的算法出现了：卷积神经网络（CNNs）。它们不依赖于手工设计的特征，而是从 *数据* 中 *学习* 相关特征。这些模型在多个计算机视觉任务中取得了巨大的成功，实现了（有时甚至超过了）人类水平的准确性。它们在工业界的应用越来越广泛，从医疗诊断到电子商务再到制造业。在本章中，我们详细介绍了用于图像分类和目标检测的一些最流行的深度神经网络架构。我们探讨了它们的显著特征，深入研究了架构细节，以了解它们是如何以及为什么能工作的，并将它们应用于实际问题。
- en: NOTE Fully functional code for this chapter, executable via Jupyter Notebook,
    can be found at [http://mng.bz/vojq](http://mng.bz/vojq)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章的完整功能代码，可通过 Jupyter Notebook 执行，可在 [http://mng.bz/vojq](http://mng.bz/vojq)
    找到。
- en: '11.1 CNNs for image classification: LeNet'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 用于图像分类的 CNN：LeNet
- en: In chapter [10](../Text/10.xhtml#ch-conv2d3dtranspose), we discussed the convolution
    operation in 1D, 2D, and 3D scenarios. We also saw how to implement a single convolutional
    layer as part of a larger neural network. This section shows how a neural network
    with multiple convolutional layers can be used for image classification. (If needed,
    ou are encouraged to revisit chapter [10](../Text/10.xhtml#ch-conv2d3dtranspose).)
    For this purpose, let’s consider the MNIST data set, a large collection of handwritten
    digits (0 through 9). It contains a training set of 60,000 images and a test set
    of 10,000 images. Each image is 28 × 28 in size and contains a center crop of
    a single digit. Figure [11.2](#fig-mnist-sample) shows sample images from the
    MNIST data set.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [10](../Text/10.xhtml#ch-conv2d3dtranspose) 章中，我们讨论了 1D、2D 和 3D 场景下的卷积操作。我们还看到了如何将单个卷积层作为更大神经网络的一部分来实现。本节展示了如何使用具有多个卷积层的神经网络进行图像分类。（如果需要，鼓励您回顾第
    [10](../Text/10.xhtml#ch-conv2d3dtranspose) 章。）为此，让我们考虑 MNIST 数据集，这是一个包含大量手写数字（0
    到 9）的大集合。它包含 60,000 张图像的训练集和 10,000 张图像的测试集。每个图像大小为 28 × 28，包含单个数字的中心裁剪。图 [11.2](#fig-mnist-sample)
    显示了 MNIST 数据集的样本图像。
- en: '![](../../OEBPS/Images/CH11_F02_Chaudhury.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH11_F02_Chaudhury.jpg)'
- en: 'Figure 11.2 Sample images from the MNIST data set. (Source: “Gradient-based
    learning applied to document recognition”; [http://mng.bz/Wz0a.](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf))'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 MNIST 数据集的样本图像。（来源：“应用于文档识别的基于梯度的学习”；[http://mng.bz/Wz0a.](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf))
- en: 'We’d like to build a classifier that takes in a 28 × 28 image as input and
    emits a label from 0 to 9 based on the digit contained in the image. One of the
    most popular neural network architectures for this task is the LeNet, which was
    proposed by LeCun et al. in their 1998 paper, “Gradient-based learning applied
    to document recognition” ([http://mng.bz/Wz0a](http://mng.bz/Wz0a)). The LeNet
    architecture is illustrated in figure [11.3](#fig-lenet-architecture) (LeNet expects
    input images of size 32 × 32, so the 28 × 28 MNIST images are resized to 32 ×
    32 before being fed into the network):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望构建一个分类器，该分类器接受 28 × 28 的图像作为输入，并根据图像中包含的数字输出 0 到 9 的标签。为此任务最流行的神经网络架构之一是
    LeNet，它由 LeCun 等人在 1998 年发表的论文“应用于文档识别的基于梯度的学习”中提出（[http://mng.bz/Wz0a](http://mng.bz/Wz0a)）。LeNet
    架构如图 [11.3](#fig-lenet-architecture) 所示（LeNet 预期输入图像大小为 32 × 32，因此将 28 × 28 的
    MNIST 图像调整为 32 × 32 大小后再输入网络）：
- en: '![](../../OEBPS/Images/CH11_F03_Chaudhury.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH11_F03_Chaudhury.png)'
- en: 'Figure 11.3 LeNet. (Source: “Gradient-based learning applied to document recognition”;
    [http://mng.bz/Wz0a.](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf))'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 LeNet。（来源：“应用于文档识别的基于梯度的学习”；[http://mng.bz/Wz0a.](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf))
- en: It consists of three convolutional layers with 5 × 5 kernels convolved with
    a stride of 1\. The first convolution layer produces 6 feature maps of size 28
    × 28, the
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它由三个具有 5 × 5 内核的卷积层组成，步长为 1。第一个卷积层生成 6 个大小为 28 × 28 的特征图。
- en: second convolution layer produces 16 feature maps of size 10 × 10, and the third
    convolution layer produces 120 feature maps of size 1 × 1 which are flattened
    into a 120-dimensional vector)
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个卷积层产生16个大小为10 × 10的特征图，第三个卷积层产生120个大小为1 × 1的特征图，这些特征图被展平成一个120维的向量）
- en: The first two convolutional layers are followed by subsampling (aka pooling)
    layers, which perform a local averaging and subsampling of the feature map, thus
    reducing the resolution of the feature map and the sensitivity of the output to
    shifts and distortions in the input. A pooling kernel of size 2 × 2 is applied,
    reducing the feature map size to half its original size. Refer to section [10.7](../Text/10.xhtml#sec-pooling)
    for more about pooling.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前两个卷积层后面跟着下采样（也称为池化）层，这些层对特征图进行局部平均和下采样，从而降低特征图的分辨率和输出对输入中位移和畸变的敏感性。应用了一个大小为2
    × 2的池化核，将特征图大小减少到原来的一半。有关池化的更多信息，请参阅[10.7](../Text/10.xhtml#sec-pooling)节。
- en: Every feature map is followed by a tanh activation layer. This introduces nonlinearity
    into the network, increasing its expressive power because it can now model the
    output as a nonlinear combination of the inputs. If we did not have a nonlinear
    activation function, no matter how many layers we had, the neural network would
    still behave as a single-linear-layer network because the combination of multiple
    linear layers is just another linear layer. While the original LeNet paper used
    tanh as the activation function, several activation functions such as ReLU and
    sigmoid can also be used. ReLU is discussed in detail in section [11.2.1.1](#subsec-relu).
    Detailed discussions of sigmoid and tanh can be found in sections [8.1](../Text/08.xhtml#sec-sigmoid-etc)
    and [8.1.2](../Text/08.xhtml#sec-tanh).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个特征图后面都跟着一个tanh激活层。这向网络引入了非线性，增加了其表达能力，因为它现在可以将输出建模为输入的非线性组合。如果我们没有非线性激活函数，无论我们有多少层，神经网络仍然会像单一线性层网络一样表现，因为多个线性层的组合只是另一个线性层。虽然原始的LeNet论文使用了tanh作为激活函数，但也可以使用几个其他激活函数，如ReLU和sigmoid。ReLU在[11.2.1.1](#subsec-relu)节中进行了详细讨论。sigmoid和tanh的详细讨论可以在[8.1](../Text/08.xhtml#sec-sigmoid-etc)和[8.1.2](../Text/08.xhtml#sec-tanh)节中找到。
- en: The output feature map is passed through two fully connected (FC, aka linear)
    layers, which finally produce a 10-dimensional *logits* vector that represents
    the score for every class. The logits scores are converted into probabilities
    using the softmax layer.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出特征图通过两个全连接（FC，也称为线性）层，最终产生一个代表每个类得分的10维*logits*向量。使用softmax层将logits得分转换为概率。
- en: '`CrossEntropyLoss`, discussed in section [6.3](../Text/06.xhtml#sec-cross-entropy),
    is used to compute the difference between the predicted probabilities and the
    ground truth.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CrossEntropyLoss`，在[6.3](../Text/06.xhtml#sec-cross-entropy)节中讨论，用于计算预测概率和真实值之间的差异。'
- en: NOTE A *feature map* is a 2D array of points (that is, a grid) with a fixed-size
    vector associated with every point. An image is an example of a feature map, with
    each point being a pixel and the associated vector representing the pixel’s color.
    A convolution layer transforms an input feature map into an output feature map.
    The output feature map usually has smaller width and height but a longer per-point
    vector.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：*特征图*是一个与每个点关联固定大小向量的二维数组（即网格）。图像是特征图的一个例子，每个点是一个像素，关联的向量表示像素的颜色。卷积层将输入特征图转换为输出特征图。输出特征图通常具有更小的宽度和高度，但每个点的向量更长。
- en: The LeNet performs very well on the MNIST data set, achieving test accuracies
    greater than 99%. A PyTorch implementation of LeNet is presented next.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet在MNIST数据集上表现非常好，测试准确率超过99%。接下来将展示LeNet的PyTorch实现。
- en: 11.1.1 PyTorch- Implementing LeNet for image classification on MNIST
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 使用PyTorch在MNIST上实现LeNet进行图像分类
- en: NOTE Fully functional code for training the LeNet, executable via Jupyter Notebook,
    can be found at [http://mng.bz/q2gz](http://mng.bz/q2gz).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：用于训练LeNet的完整功能代码，可通过Jupyter Notebook执行，可以在[http://mng.bz/q2gz](http://mng.bz/q2gz)找到。
- en: Listing 11.1 PyTorch code for the LeNet
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.1 LeNet的PyTorch代码
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① 5 × 5 conv
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ① 5 × 5卷积
- en: ② Tanh activation
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ② Tanh激活
- en: ③ 2 × 2 average pooling
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 2 × 2 平均池化
- en: ④ First FC layer
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 第一个全连接层
- en: ⑤ Second FC layer
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 第二个全连接层
- en: '⑥ X.shape: N × 3 × 32 × 32\. N is the batch size.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '⑥ X.shape: N × 3 × 32 × 32。N是批处理大小。'
- en: '⑦ conv_out.shape: N × 120 × 1 × 1'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '⑦ conv_out.shape: N × 120 × 1 × 1'
- en: '⑧ logits.shape: N × 10'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '⑧ logits.shape: N × 10'
- en: ⑨ Computes the probabilities using softmax
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 使用softmax计算概率
- en: 11.2 Toward deeper neural networks
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 向更深层的神经网络迈进
- en: 'The LeNet model is not a very deep network since it has only three convolutional
    layers. While this is sufficient to achieve accurate results on a simple data
    set like MNIST, it doesn’t work well on real-world image classification problems
    since it does not have enough expressive power to model complex images. So, we
    typically go for much deeper neural networks with multiple convolutional layers.
    Adding more layers does the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet模型并不是一个非常深的网络，因为它只有三个卷积层。虽然这对于在像MNIST这样的简单数据集上实现准确的结果是足够的，但它并不适用于现实世界的图像分类问题，因为它没有足够的表达能力来模拟复杂图像。因此，我们通常选择具有多个卷积层的更深层的神经网络。增加更多层可以实现以下功能：
- en: '*Brings extra expressive power due to extra nonlinearity*—Since every layer
    brings with it a new set of learnable parameters and extra nonlinearity, a deeper
    network can model more complex relationships between input data elements. Lower
    layers typically learn simpler features of the object, like lines and edges, whereas
    higher layers learn more abstract features of the object, like shapes or sets
    of lines.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*由于额外的非线性而带来额外的表达能力*——由于每一层都带来一组新的可学习参数和额外的非线性，深层网络可以模拟输入数据元素之间更复杂的关系。底层通常学习物体的简单特征，如线条和边缘，而高层学习物体的更抽象特征，如形状或线条集合。'
- en: '*Achieves the same reach with fewer parameters*—Let’s examine this via an example.
    Consider two output feature maps, one produced by a single 5 × 5 convolution on
    the input and another produced by two 3 × 3 convolutions applied one after another
    in sequence on the input. Assume a stride of 1 and the same (zero) padding. Figure
    [11.4](#fig-receptive-field) illustrates this scenario. Consider a single grid
    point in the output feature map. In both cases, the output value of the grid point
    is derived from a 5 × 5 patch in the input. We say the indicated 5 × 5 input patch
    is the *receptive field* of the output grid points. Thus, in both cases, the output
    grid point is a digest of the same input: that is, it expresses the same information.
    However, in the deeper network, there are fewer parameters. The number of parameters
    in a single 5 × 5 filter is 25, whereas that in two 3 × 3 filters is 2 × 9 = 18
    (assuming a single channel input image). This is a 38% difference. Similarly,
    if we compare one 7 × 7 filter with three 3 × 3 filters, they have the same receptive
    field, but the 7 × 7 filter has 81% more parameters than the 3 × 3 filter.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*以更少的参数达到相同的范围*——让我们通过一个例子来检验这一点。考虑两个输出特征图，一个是由输入上的单个5 × 5卷积产生的，另一个是由输入上连续应用的两个3
    × 3卷积产生的。假设步长为1，相同的（零）填充。图[11.4](#fig-receptive-field)说明了这种情况。考虑输出特征图中的一个网格点。在两种情况下，网格点的输出值都是来自输入中的5
    × 5块。我们称这个指示的5 × 5输入块是输出网格点的*感受野*。因此，在两种情况下，输出网格点是对相同输入的摘要：即，它表达了相同的信息。然而，在更深层的网络中，参数更少。单个5
    × 5滤波器的参数数量是25，而两个3 × 3滤波器的参数数量是2 × 9 = 18（假设单通道输入图像）。这是一个38%的差异。同样，如果我们比较一个7
    × 7滤波器与三个3 × 3滤波器，它们具有相同的感觉野，但7 × 7滤波器的参数比3 × 3滤波器多81%。'
- en: '![](../../OEBPS/Images/CH11_F04_Chaudhury.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH11_F04_Chaudhury.png)'
- en: Figure 11.4 A single 5 × 5 convolution layer vs. two 3 × 3 convolution layers
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 单个5 × 5卷积层与两个3 × 3卷积层的比较
- en: Now, let’s look at some of the most popular deep convolutional networks used
    for image classification. The first deep network that reignited the deep learning
    revolution was *AlexNet*, which was published by Krizhevsky et al. in 2012\. It
    significantly outperformed all previous state-of-the-art algorithms on the ImageNet
    Large Scale Visual Recognition Challenge (ILSVRC), a complex data set with 1.3
    million images across 1,000 classes. Since AlexNet, several deep networks have
    improved on the previous state of the art, such as GoogleNet, VGG, and ResNet.
    In this chapter, we discuss the key concepts that make each of these networks
    work. For a detailed review of their architectures, training methodologies, and
    final results, you are encouraged to read the original papers linked in each section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一些用于图像分类的最流行的深度卷积网络。第一个重新点燃深度学习革命的是*AlexNet*，它由Krizhevsky等人于2012年发表。它在ImageNet大规模视觉识别挑战（ILSVRC）上显著优于所有之前的最佳算法，这是一个包含130万张图片的复杂数据集，跨越1000个类别。自从AlexNet以来，几个深度网络在之前的状态上取得了进步，如GoogleNet、VGG和ResNet。在本章中，我们讨论了使每个网络工作的关键概念。为了详细了解它们的架构、训练方法和最终结果，我们鼓励您阅读每个部分中链接的原始论文。
- en: 11.2.1 VGG (Visual Geometry Group) Net
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 VGG（视觉几何组）网络
- en: 'The VGG family of networks was created by the Visual Geometry Group from the
    University of Oxford ([https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf)).
    Their main contribution was a thorough evaluation of networks of increasing depth
    using an architecture with very small (3 × 3) convolution filters. They demonstrated
    that by using 3 × 3 convolutions and networks with 16–19 weight layers, they could
    outperform previous state-of-the-art results on the ILSVRC-2014 challenge. The
    VGG network had two main differences compared to prior works:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: VGG系列网络是由牛津大学视觉几何组创建的([https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf))。他们的主要贡献是对使用非常小的（3
    × 3）卷积核的深度网络进行了彻底评估。他们证明了通过使用3 × 3卷积和具有16-19个权重层的网络，他们可以在ILSVRC-2014挑战赛上超越之前的最先进结果。与之前的工作相比，VGG网络有两个主要区别：
- en: '*Use of smaller (3 × 3) convolution filters*—Prior networks often relied on
    larger kernels of size 7 × 7 or 11 × 11 in the first convolution layers. VGG instead
    only used 3 × 3 kernels throughout the network. As discussed in section [11.2](#sec-towards-deeper-neural-nets),
    three 3 × 3 filters have the same receptive field as a single 7 × 7 filter. So
    what does replacing the 7 × 7 filter with three smaller filters buy?'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用较小的（3 × 3）卷积核*——之前的网络通常在第一层卷积层使用7 × 7或11 × 11的大核。VGG则在整个网络中只使用3 × 3核。如第[11.2](#sec-towards-deeper-neural-nets)节所述，三个3
    × 3滤波器具有与单个7 × 7滤波器相同的感受野。那么用三个较小的滤波器替换7 × 7滤波器能带来什么好处？'
- en: More nonlinearity and hence more expressive power because we have a ReLU activation
    function applied at the end of every convolution layer
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于在每个卷积层末尾应用了ReLU激活函数，因此具有更多的非线性，从而具有更强的表达能力。
- en: Fewer parameters (49*C*² vs. 27*C*²), which means faster learning and more robustness
    to overfitting
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数更少（49*C*²比27*C*²），这意味着学习速度更快，对过拟合的鲁棒性更强。
- en: '*Removal of the local response normalization (LRN) layers*—LRN was first introduced
    in the AlexNet architecture. Its purpose was twofold: to bound the output of the
    ReLU layer, which is an unbounded function and can produce outputs as large as
    the training permits; and to encourage *lateral inhibition* wherein a neuron can
    suppress the activity of its neighbors (this in effect acts as a regularization).
    The VGG paper demonstrated that adding LRN layers did not improve accuracy, so
    VGG chose to remove them from its architecture.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*移除局部响应归一化（LRN）层*——LRN最初是在AlexNet架构中引入的。它的目的是双重的：限制ReLU层的输出，ReLU层是一个无界函数，其输出可以大到训练允许的程度；并鼓励*侧抑制*，其中神经元可以抑制其邻居的活动（这实际上起到了正则化的作用）。VGG论文表明，添加LRN层并没有提高准确率，因此VGG选择从其架构中移除它们。'
- en: 'The VGG family of networks comes in five different configurations, which mainly
    differ in the number of layers (VGG-11, VGG-13, VGG-16, and VGG-19). Regardless
    of the exact configuration, the VGG family of networks follows a common structure.
    Here, we discuss these commonalities (a detailed description of the differences
    can be found in the original paper):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: VGG系列网络有五种不同的配置，这些配置主要区别在于层数（VGG-11、VGG-13、VGG-16和VGG-19）。无论具体配置如何，VGG系列网络都遵循一个共同的结构。在这里，我们讨论这些共同点（详细差异描述可以在原始论文中找到）：
- en: All architectures work on 224 × 224 input images.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有架构都针对224 × 224输入图像。
- en: 'All architectures have five convolutional blocks (conv blocks):'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有架构都有五个卷积块（conv blocks）：
- en: Each block can have multiple convolution layers followed by a max pool layer
    at the end.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个块可以包含多个卷积层，并在末尾跟一个最大池化层。
- en: All individual convolution layers use 3 × 3 kernels with a stride of 1 and same
    padding. Therefore, they don’t change the spatial resolution of the output feature
    map.
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有单独的卷积层都使用3 × 3核，步长为1，填充为same，因此它们不会改变输出特征图的空间分辨率。
- en: All convolution layers within a single conv block have the same-sized output
    feature maps.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个卷积块内的所有卷积层都有相同大小的输出特征图。
- en: Each convolution layer is followed by a ReLU layer that adds nonlinearity.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个卷积层后面都跟着一个ReLU层，增加非线性。
- en: The max pool layer at the end of every conv block reduces the spatial resolution
    to half.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个卷积块的末尾都有一个最大池化层，将空间分辨率降低到一半。
- en: Since each conv block downsamples by 2, the input feature map is reduced 2⁵
    (32) times, resulting in an output feature map of size 7 × 7. Additionally, at
    each conv block, the number of feature maps is doubled.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于每个卷积块通过2倍下采样，输入特征图被减少了2⁵（32）次，从而得到7 × 7大小的输出特征图。此外，在每个卷积块中，特征图的数量翻倍。
- en: 'All architectures end with three FC layers:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有架构都以三个 FC 层结束：
- en: The first takes a 51,277-sized input and converts it into a 4,096-dimensional
    output.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个将 51,277 大小的输入转换为 4,096 维输出。
- en: The second takes the resulting 4,096-dimensional output and converts it into
    another 4,096-dimensional output.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个将得到的 4,096 维输出转换为另一个 4,096 维输出。
- en: The final takes the resulting 4,096-dimensional output and converts it into
    a *C*-dimensional output, where *C* stands for the number of classes. In the case
    of ImageNet classification, *C* is 1,000.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个将得到的 4,096 维输出转换为 *C* 维输出，其中 *C* 代表类别数。在 ImageNet 分类的情况下，*C* 是 1,000。
- en: The architecture diagram for VGG-11 is shown in figure [11.5](#fig-vgg-architecture).
    The column on the left represents the shape of the input tensor to each layer.
    The column on the right represents the shape of the output tensor from each layer.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: VGG-11 的架构图显示在图 [11.5](#fig-vgg-architecture) 中。左侧的列表示每个层的输入张量的形状。右侧的列表示每个层的输出张量的形状。
- en: '![](../../OEBPS/Images/CH11_F05_Chaudhury.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH11_F05_Chaudhury.png)'
- en: Figure 11.5 VGG-11 architecture diagram. All shapes are of the form N × C ×
    H × W, where N is the batch size, C is the number of channels, H is the height,
    and W is the width.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 VGG-11 架构图。所有形状都是 N × C × H × W 的形式，其中 N 是批量大小，C 是通道数，H 是高度，W 是宽度。
- en: ReLU nonlinearity
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 非线性
- en: 'As we’ve discussed previously, nonlinear layers give the deep neural network
    more expressive power to model complex mathematical functions. In chapter [8](../Text/08.xhtml#ch-training-neural-networks),
    we looked at two nonlinear functions: sigmoid and tanh. However, the VGG network
    (like AlexNet) consists of a different nonlinear layer called rectified linear
    unit (ReLU). To understand the rationale for this choice, let’s revisit the sigmoid
    function and look at some of its drawbacks.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，非线性层赋予了深度神经网络更多的表达能力来模拟复杂的数学函数。在第 [8](../Text/08.xhtml#ch-training-neural-networks)
    章节中，我们研究了两个非线性函数：sigmoid 和 tanh。然而，VGG 网络（如 AlexNet）由一个不同的非线性层称为修正线性单元（ReLU）组成。为了理解这种选择的合理性，让我们回顾一下
    sigmoid 函数并看看它的一些缺点。
- en: Figure [11.6](#fig-sigmoid1d-with-derivative-2) plots the sigmoid function along
    with its derivative. As the plot shows, the gradient (derivative) is maximum when
    the input is 0, and it quickly tapers down to 0 as the input increases/decreases.
    This is true for the tanh activation function as well. It means when the output
    of a neuron before the sigmoid layer) is either high or low, the gradient becomes
    small. While this may not be an issue in shallow networks, it becomes a problem
    in larger networks because the gradients can become too small for training to
    work effectively. Gradients of neural networks are calculated using backpropagation.
    By the chain rule, the derivatives of each layer are multiplied down the network,
    starting from the final layer and moving toward the initial layers. If the gradients
    at each layer are small, a small number multiplied by another small number is
    an even smaller number. Thus the gradients at the initial layers are very close
    to 0, making the training ineffective. This is known as the *vanishing gradient*
    problem.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [11.6](#fig-sigmoid1d-with-derivative-2) 绘制了 sigmoid 函数及其导数。如图所示，当输入为 0 时，梯度（导数）最大，随着输入的增加/减少，它迅速减小到
    0。这对于 tanh 激活函数也是正确的。这意味着当 sigmoid 层之前神经元的输出要么很高要么很低时，梯度变得很小。虽然这可能在浅层网络中不是问题，但在大型网络中会成为一个问题，因为梯度可能太小，无法有效地进行训练。神经网络的梯度是通过反向传播计算的。根据链式法则，每一层的导数沿着网络相乘，从最终层开始，移动到初始层。如果每一层的梯度都很小，一个小的数乘以另一个小的数会得到一个更小的数。因此，初始层的梯度非常接近
    0，使得训练无效。这被称为 *梯度消失* 问题。
- en: '![](../../OEBPS/Images/CH11_F06_Chaudhury.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../../OEBPS/Images/CH11_F06_Chaudhury.png)'
- en: Figure 11.6 Graph of a 1D sigmoid function (dotted curve) and its derivative
    (solid curve)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 1D sigmoid 函数（虚线曲线）及其导数（实线曲线）的图形
- en: The ReLU function addresses this problem. Figure [11.7](#fig-relu) shows a graph
    of the ReLU function. Its equation is given by
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 函数解决了这个问题。图 [11.7](#fig-relu) 展示了 ReLU 函数的图形。其方程如下
- en: '*ReLU*(*x*) = *max*(0, *x*)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*ReLU*(*x*) = *max*(0, *x*)'
- en: Equation 11.1
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 11.1
- en: The derivative of ReLU is 1 (constant) when x is greater than 0, and 0 everywhere
    else. Therefore, it doesn’t suffer from the vanishing gradient problem. Most deep
    networks today use ReLU as their activation function. The AlexNet paper demonstrated
    that using ReLU nonlinearity significantly speeds up training because it helps
    with faster convergence.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当 x 大于 0 时，ReLU 的导数是 1（常数），在其他地方都是 0。因此，它不会受到梯度消失问题的影响。今天的大多数深度网络都使用 ReLU 作为它们的激活函数。AlexNet
    论文表明，使用 ReLU 非线性可以显著加快训练速度，因为它有助于更快地收敛。
- en: '![](../../OEBPS/Images/CH11_F07_Chaudhury.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH11_F07_Chaudhury.png)'
- en: Figure 11.7 Graph of the ReLU function
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 ReLU 函数的图
- en: PyTorch- VGG
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch- VGG
- en: Now let’s see how to implement the VGG network in PyTorch. First, let’s implement
    a single conv block, which is the core component of the VGG net. This conv block
    will later be repeated multiple times to form the entire VGG network.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何在 PyTorch 中实现 VGG 网络。首先，让我们实现单个卷积块，这是 VGG 网络的核心组件。这个卷积块将被多次重复，以形成整个
    VGG 网络。
- en: NOTE Fully functional code for the VGG network, executable via Jupyter Notebook,
    can be found at [http://mng.bz/7WE4](http://mng.bz/7WE4).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：VGG 网络的完整功能代码，可通过 Jupyter Notebook 执行，可以在 [http://mng.bz/7WE4](http://mng.bz/7WE4)
    找到。
- en: Listing 11.2 PyTorch code for a convolutional block
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.2 卷积块的 PyTorch 代码
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① 3 × 3 conv
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ① 3 × 3 卷积
- en: ② ReLU nonlinearity
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ② ReLU 非线性
- en: ③ 2 × 2 max pooling
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 2 × 2 最大池化
- en: Next, let’s implement the convolutional backbone (conv backbone) builder, which
    allows us to create different VGG architectures via simple configuration changes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实现卷积骨干（conv backbone）构建器，它允许我们通过简单的配置更改创建不同的 VGG 架构。
- en: Listing 11.3 PyTorch code for the conv backbone
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.3 卷积骨干的 PyTorch 代码
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '① Cfg: [(in_channels, num_conv_layers, num_features),] The different VGG networks
    can be created without duplicating code by passing in the right cfg.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '① Cfg: [(in_channels, num_conv_layers, num_features),] 通过传递正确的 cfg，可以创建不同的
    VGG 网络，而无需重复代码。'
- en: ② Iterates over conv block configurations
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ② 遍历卷积块配置
- en: ③ Instantiates the conv block defined in listing [11.2](#code-conv-block)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 实例化列表 [11.2](#code-conv-block) 中定义的卷积块
- en: ④ There must be three input channels.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 必须有三个输入通道。
- en: ⑤ out_Features of the previous block should be equal to in_features of the current
    block.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 前一个块的 out_Features 应等于当前块的 in_features。
- en: The conv backbone is instantiated with a config that contains the list of configurations
    for each of the conv blocks. The config for VGG-11 contains fewer layers, whereas
    that for VGG-19 contains more layers. The output of the conv backbone is fed into
    the classifier, which consists of three FC layers. Together, the conv backbone
    and the classifier form the VGG module.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用包含每个卷积块配置列表的配置实例化卷积骨干。VGG-11 的配置包含较少的层，而 VGG-19 的配置包含更多的层。卷积骨干的输出被送入分类器，该分类器由三个全连接层组成。卷积骨干和分类器共同构成了
    VGG 模块。
- en: Listing 11.4 PyTorch code for the VGG network
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.4 VGG 网络的 PyTorch 代码
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Backbone network defined in listing [11.3](#code-vgg-backbone)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ① 列表 [11.3](#code-vgg-backbone) 中定义的骨干网络
- en: ② The classifier is made up of three linear Layers. The first two are followed
    by ReLU nonlinearity.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ② 分类器由三个线性层组成。前两个层后面跟着 ReLU 非线性。
- en: ③ Flattens the conv features before passing it to the classifier
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在传递给分类器之前将卷积特征展平
- en: A VGG-11 network can be instantiated as follows.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: VGG-11 网络可以如下实例化。
- en: Listing 11.5 PyTorch code instantiating a VGG network from a specific config
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.5 从特定配置实例化 VGG 网络的 PyTorch 代码
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Creates the cfg for VGG-11
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建 VGG-11 的 cfg
- en: ② Instantiates the conv backbone
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化卷积骨干
- en: ③ Instantiates the VGG network
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 实例化 VGG 网络
- en: 'While we have discussed how to implement VGG in PyTorch, we don’t do this in
    practice because the `torchvision` package already implements the VGG network,
    along with several other popular deep networks. It is recommended that you use
    the `torchvision` implementation, as shown here:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经讨论了如何在 PyTorch 中实现 VGG，但在实践中我们并不这样做，因为 `torchvision` 包已经实现了 VGG 网络，以及几个其他流行的深度网络。建议您使用此处所示的
    `torchvision` 实现：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '11.2.2 Inception: Network-in-network paradigm'
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.2 Inception：网络内网络范式
- en: 'Previously, we saw how increasing the depth of a neural network—that is, the
    number of layers—can improve accuracy because it increases the expressive power
    of the network. Alternatively, we could increase the width of the network—the
    number of units at each level—to improve accuracy. However, both these methods
    suffer from two main drawbacks. First, blindly increasing the size of the network
    can lead to overfitting, wherein the network memorizes certain patterns in the
    training data that don’t extend well to test data. And second, increased computation
    resources are required during both training and inference times. The Inception
    architecture, introduced by Szegedy et al. in their paper "Going deeper with convolutions"
    ([https://arxiv.org/pdf/1409.4842v1.pdf](https://arxiv.org/pdf/1409.4842v1.pdf)),
    aims to address both these drawbacks. The Inception architecture increases the
    network’s depth and width while keeping the computational budget constant. In
    this section, we examine the main idea behind the Inception architecture. While
    there have been several improvements to it Inception_v2, Inception_v3, Inception_ResNet,
    and so on), we discuss the original: Inception_v1.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们看到了如何通过增加神经网络的深度——即层数——来提高准确率，因为这样可以增加网络的表达能力。或者，我们也可以通过增加网络的宽度——即每一层的单元数量——来提高准确率。然而，这两种方法都存在两个主要缺点。首先，盲目地增加网络的大小可能导致过拟合，即网络会记住训练数据中某些模式，而这些模式在测试数据中并不适用。其次，在训练和推理过程中都需要更多的计算资源。Szegedy等人在其论文《通过卷积加深网络》中介绍了Inception架构([https://arxiv.org/pdf/1409.4842v1.pdf](https://arxiv.org/pdf/1409.4842v1.pdf))，旨在解决这两个问题。Inception架构在保持计算预算不变的情况下，增加了网络的深度和宽度。在本节中，我们将探讨Inception架构背后的主要思想。尽管Inception_v2、Inception_v3、Inception_ResNet等对其进行了多次改进，但我们讨论的是原始的：Inception_v1。
- en: 'Prior deep learning architectures typically stacked convolutional filters sequentially:
    each layer applied a set of convolutional filters of the same size and passed
    it to the subsequent layer. The kernel size of the filter at each layer depended
    on the architecture. But with such an architecture, how do we know we have chosen
    the right kernel size for each layer? If we are detecting a car, say, the fraction
    of the image area (that is, the number of pixels) occupied by the car is different
    in an image taken close up than in one taken from far away. We say the *scale*
    of the car object is different in the two images. Consequently, the number of
    pixels that must be digested to recognize the car will differ at different scales.
    A larger kernel is preferred for information at a larger scale, and vice versa.
    An architecture that is forced to choose one kernel size may not be optimal. The
    Inception module tackles this problem by having multiple kernels of different
    sizes at each level and taking weighted combinations of the outputs. The network
    can learn to weigh the appropriate kernel more than others. The naive implementation
    of the Inception module performs convolutions on the input using three kernel
    sizes: 1 × 1, 3 × 3, and 5 × 5. Max pooling is also performed, using a 3 × 3 kernel
    with stride 1 and padding 1 (for output and input to be the same size). The outputs
    are concatenated and sent into the next Inception module. See figure [11.8](#fig-inception-architecture)
    for details.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的深度学习架构通常按顺序堆叠卷积滤波器：每一层应用一组相同大小的卷积滤波器，并将其传递到下一层。每一层的滤波器核大小取决于架构。但是，在这种架构下，我们如何知道我们为每一层选择了正确的核大小？如果我们正在检测一辆车，比如说，车在图像中占据的图像区域（即像素数）在近距离拍摄的图像中与从远处拍摄的图像中是不同的。我们说车这个物体的*尺度*在这两个图像中是不同的。因此，在不同尺度上识别汽车所需的像素数将不同。较大的核更适合处理较大尺度的信息，反之亦然。被迫选择一个核大小的架构可能不是最优的。Inception模块通过在每一层使用多个不同大小的核来解决这个问题，并取输出结果的加权组合。网络可以学会更重视适当的核，而不是其他核。Inception模块的简单实现使用三种核大小在输入上进行卷积：1
    × 1、3 × 3和5 × 5。还执行了最大池化操作，使用3 × 3核，步长为1，填充为1（以保持输出和输入大小相同）。输出结果被连接起来，并送入下一个Inception模块。参见图[11.8](#fig-inception-architecture)以获取详细信息。
- en: '![](../../OEBPS/Images/CH11_F08_Chaudhury.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH11_F08_Chaudhury.png)'
- en: Figure 11.8 Inception_v1 architecture
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8 Inception_v1架构
- en: This naive Inception block has a major flaw. Using even a small number of 5
    × 5 filters can prohibitively increase the number of parameters. This becomes
    even more expensive when we add the pooling layer, where the number of output
    filters equals the number of filters in the previous stage. Thus, concatenating
    the output of the pooling layer with the outputs of convolutional layers would
    lead to an inevitable increase in the number of output features. To fix this,
    the Inception module uses 1 × 1 convolution layers before the 3 × 3 and 5 × 5
    filters to reduce the number of input channels. This drastically reduces the number
    of parameters of the 3 × 3 and 5 × 5 convs. While it may seem counterintuitive,
    1 × 1 convs are much cheaper than 3 × 3 and 5 × 5 convs. Additionally, 1 × 1 convolution
    is applied after pooling see figure [11.8](#fig-inception-architecture)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的 Inception 模块存在一个主要缺陷。使用少量 5 × 5 过滤器就会极大地增加参数数量。当我们添加池化层，其中输出过滤器的数量等于前一阶段的过滤器数量时，这变得更加昂贵。因此，将池化层的输出与卷积层的输出连接起来会导致输出特征数量的不可避免增加。为了解决这个问题，Inception
    模块在 3 × 3 和 5 × 5 过滤器之前使用 1 × 1 卷积层来减少输入通道数。这极大地减少了 3 × 3 和 5 × 5 卷积的参数数量。虽然这看起来可能有些反直觉，但
    1 × 1 卷积比 3 × 3 和 5 × 5 卷积便宜得多。此外，1 × 1 卷积在池化之后应用，见图 [11.8](#fig-inception-architecture)。
- en: A neural network architecture was built using the dimension-reduced Inception
    module and was popularly known as GoogLeNet. GoogLeNet has nine such Inception
    modules stacked linearly. It is 22 layers deep (27, including the pooling layers).
    It uses global average pooling at the end of the last Inception module. With such
    a deep network, there is always the vanishing gradient problem; to prevent the
    middle part of the network from “dying out,” the paper introduced two auxiliary
    classifiers. This is done by applying softmax to the output of two of the intermediate
    Inception modules and computing an auxiliary loss over the ground truth. The total
    loss function is a weighted sum of the auxiliary loss and the real loss. You are
    encouraged to read the original paper to understand the details.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用维度降低的 Inception 模块构建了一个神经网络架构，这广为人知为 GoogLeNet。GoogLeNet 有九个这样的 Inception
    模块线性堆叠。它有 22 层深（包括池化层为 27 层）。它在最后一个 Inception 模块之后使用全局平均池化。对于如此深的网络，总是存在梯度消失问题；为了防止网络中间部分“死亡”，论文中引入了两个辅助分类器。这是通过对两个中间
    Inception 模块的输出应用 softmax 并计算辅助损失在真实值上完成的。总损失函数是辅助损失和真实损失的加权总和。我们鼓励您阅读原始论文以了解细节。
- en: PyTorch- Inception block
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch- Inception 模块
- en: Let’s see how to implement an Inception block in PyTorch. We typically don’t
    do this in practice because end-to-end deep network architectures containing Inception
    blocks are already implemented in the `torchvision` package. However, we implement
    the Inception block from scratch to understand the details.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 PyTorch 中实现 Inception 模块。在实践中，我们通常不会这样做，因为包含 Inception 模块的端到端深度网络架构已经在
    `torchvision` 包中实现。然而，我们从头开始实现 Inception 模块以了解其细节。
- en: NOTE Fully functional code for the Inception block, executable via Jupyter Notebook,
    can be found at [http://mng.bz/mxn0](http://mng.bz/mxn0).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Inception 模块的全功能代码，可通过 Jupyter Notebook 执行，可在 [http://mng.bz/mxn0](http://mng.bz/mxn0)
    找到。
- en: Listing 11.6 PyTorch code for a naive Inception block
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.6 用于简单 Inception 模块的 PyTorch 代码
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① 1 × 1 branch
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ① 1 × 1 分支
- en: ② 3 × 3 branch
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ② 3 × 3 分支
- en: ③ 5 × 5 branch
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 5 × 5 分支
- en: ④ 3 × 3 pooling
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 3 × 3 池化
- en: ⑤ Concatenates the outputs of the parallel branches
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 连接并行分支的输出
- en: Listing 11.7 PyTorch code for a dimensionality reduced Inception block
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.7 用于维度降低 Inception 模块的 PyTorch 代码
- en: '[PRE7]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① 1 × 1 branch
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ① 1 × 1 分支
- en: ② 1 × 1 conv in the 3 × 3 branch
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ② 3 × 3 分支中的 1 × 1 卷积
- en: ③ 3 × 3 conv in the 3 × 3 branch
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 3 × 3 分支中的 3 × 3 卷积
- en: ④ 1 × 1 conv in the 5 × 5 branch
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 5 × 5 分支中的 1 × 1 卷积
- en: ⑤ 5 × 5 conv in the 5 × 5 branch
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 5 × 5 分支中的 5 × 5 卷积
- en: ⑥ Max pooling followed by a 1 × 1 conv
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 先进行最大池化，然后是 1 × 1 卷积
- en: ⑦ Concatenates the outputs of the parallel branches
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 连接并行分支的输出
- en: '11.2.3 ResNet: Why stacking layers to add depth does not scale'
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.3 ResNet：为什么堆叠层以增加深度并不总是可扩展的
- en: 'We start with a fundamental question: is learning better networks as easy as
    stacking multiple layers? Consider the graphs in figure [11.9](#fig-varying-depth-deep-net-error-rates).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个基本问题开始：学习更好的网络是否像堆叠多个层一样简单？考虑图 [11.9](#fig-varying-depth-deep-net-error-rates)
    中的图表。
- en: '![](../../OEBPS/Images/CH11_F09_Chaudhury.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH11_F09_Chaudhury.jpg)'
- en: 'Figure 11.9 Training error (left) and test error (right) on the CIFAR-10 data
    set with 20-layer and 56-layer networks. (Source: “Deep residual learning for
    image recognition”; [https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf).)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9展示了在CIFAR-10数据集上20层和56层网络的训练错误（左）和测试错误（右）。(来源：“用于图像识别的深度残差学习”；[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)。)
- en: 'This image from the ResNet paper “Deep residual learning for image recognition”
    ([https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf))
    shows the training and test error rates for two networks: a shallower network
    with 20 layers and a deeper network with 56 layers, on the CIFAR-10 data set.
    Surprisingly, the training and test errors are *higher* for the deeper (56-layer)
    network. This result is extremely counterintuitive because we expect deeper networks
    to have more expressive power and hence higher accuracies/lower error rates than
    their shallower counterparts. This phenomenon is referred to as the *degradation*
    problem: with the network depth increasing, the accuracy becomes saturated and
    degrades rapidly. We might attribute this to overfitting, but that is not the
    case because even the training errors are higher for the deeper network. Another
    cause could be vanishing/exploding gradients. However, the authors of the ResNet
    paper investigated the gradients at each layer and established that they are healthy
    (not vanishing/exploding). So, what causes the degradation problem, and how do
    we solve it?'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片来自ResNet论文“用于图像识别的深度残差学习”（[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)），展示了两个网络在CIFAR-10数据集上的训练和测试错误率：一个有20层的较浅网络和一个有56层的较深网络。令人惊讶的是，较深（56层）网络的训练和测试错误率更高。这个结果极其反直觉，因为我们预期更深层的网络应该具有更强的表达能力，从而比其较浅的对应网络有更高的准确率/更低的错误率。这种现象被称为*退化*问题：随着网络深度的增加，准确率变得饱和并迅速下降。我们可能会将其归因于过拟合，但事实并非如此，因为即使是较深网络的训练错误率也更高。另一个可能的原因是梯度消失/爆炸。然而，ResNet论文的作者调查了每一层的梯度，并确定它们是健康的（没有消失/爆炸）。那么，是什么导致了退化问题，我们该如何解决它？
- en: Let’s consider a shallower architecture with *n* layers and a deeper counterpart
    that adds more layers to it (*n* + *m* layers). The deeper architecture should
    be able to achieve no higher loss than the shallow architecture. Intuitively,
    a trivial solution is to learn the exact *n* layers of the shallow architecture
    and the identity function for the additional *m* layers. The fact that this doesn’t
    happen in practice indicates that the neural network layers have a hard time learning
    the identity function. Thus the paper proposes "shortcut/skip connections" that
    enable the layers to potentially learn the identity function easily. This “identity
    shortcut connection” is the core idea of ResNet. Let’s look at a mathematical
    analogy. Let *h*(*x*) be the function we are trying to model (learn) via a stack
    of layers (not necessarily the entire network). It is reasonable to expect that
    the function *g*(*x*) = *h*(*x*) − *x* is simpler than *h*(*x*) and hence easier
    to learn. But we already have *x* at the input. So if we learn *g*(*x*) and add
    *x* to it to obtain *h*(*x*), we have effectively modeled *h*(*x*) by learning
    the simpler *g*(*x*) function. The name *residual* comes from *g*(*x*) = *h*(*x*)
    − *x*. Figure [11.10](#fig-residual-block) shows this in detail.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个具有*n*层的较浅架构及其更深的对应架构，后者增加了更多层(*n* + *m*层)。较深的架构应该能够实现的损失不高于较浅架构。直观地说，一个简单的解决方案是学习较浅架构的精确*n*层和额外的*m*层的恒等函数。实际上这种情况并未发生表明神经网络层很难学习恒等函数。因此，论文提出了“快捷/跳过连接”，使得层能够轻松地学习恒等函数。这种“恒等快捷连接”是ResNet的核心思想。让我们看看一个数学类比。设*h*(*x*)是我们试图通过一层层（不一定是整个网络）来建模（学习）的函数。我们合理地预期函数*g*(*x*)
    = *h*(*x*) − *x*比*h*(*x*)简单，因此更容易学习。但我们已经在输入处有了*x*。所以如果我们学习*g*(*x*)并将其加到*x*上以获得*h*(*x*)，我们就有效地通过学习更简单的*g*(*x*)函数来建模*h*(*x*)。名称*residual*来自*g*(*x*)
    = *h*(*x*) − *x*。图[11.10](#fig-residual-block)详细展示了这一点。
- en: '![](../../OEBPS/Images/CH11_F10_Chaudhury.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH11_F10_Chaudhury.png)'
- en: Figure 11.10 column shows a residual block with skip connections.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10列展示了具有跳过连接的残差块。
- en: 'Now let’s revisit the earlier problem of degradation. We posited that normal
    neural network layers generally have difficulty learning the identity function.
    In the case of residual learning, to learn the identity function, *h*(*x*) = *x*,
    the layers need to learn *g*(*x*)=0\. This can easily be done by driving all the
    layers’ weights to 0\. Here is another way to think about it: if we initialize
    a regular neural network’s weights and biases to be 0 at the start, then every
    layer starts with the “zero” function: *g*(*x*) = 0. Thus, the output of every
    stack of layers with a shortcut connection, *h*(*x*) = *g*(*x*) + *x*, is already
    the identity function: *h*(*x*) = *x* when *g*(*x*) = 0.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回顾一下之前的问题，即退化问题。我们提出，普通的神经网络层通常难以学习恒等函数。在残差学习的情况下，为了学习恒等函数，*h*(*x*) = *x*，层需要学习
    *g*(*x*)=0。这可以通过将所有层的权重驱动到0来轻松实现。这里还有另一种思考方式：如果我们初始化常规神经网络权重和偏差为0，那么每个层都从“零”函数开始：*g*(*x*)
    = 0。因此，每个带有跳跃连接的层堆栈的输出，*h*(*x*) = *g*(*x*) + *x*，已经是恒等函数：当 *g*(*x*) = 0时，*h*(*x*)
    = *x*。
- en: 'In real cases, it is important to note that identity mappings are unlikely
    to be optimal: the network layers will want to learn actual features. In such
    cases, this reformulation isn’t preventing the network lawyers from doing so;
    the layers can still learn other functions like a regular stack of layers. We
    can think of this reformulation as preconditioning, which makes learning the identity
    function easier if needed. Additionally, by adding skip connections, we allow
    a direct path for the gradient to flow from layer to layer: the deeper layer has
    a direct path to *x*. This allows for better learning as information from the
    lower layers passes directly into the higher layers.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际情况下，需要注意的是，恒等映射不太可能是最优的：网络层将想要学习实际特征。在这种情况下，这种重新表述并没有阻止网络层学习其他函数，就像常规层堆栈一样。我们可以将这种重新表述视为预条件，如果需要，它使得学习恒等函数更容易。此外，通过添加跳跃连接，我们允许梯度直接从层流向层：深层层有直接路径到
    *x*。这允许更好的学习，因为来自底层的信息可以直接传递到高层。
- en: ResNet architecture
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet架构
- en: 'Now that we have seen the basic building block—a stack of convolutional (conv)
    layers with a skip connection—let’s delve deeper into the architecture of ResNet.
    ResNet architectures are constructed by stacking multiple building blocks on top
    of each other. They follow the same idea as VGG:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了基本构建块——一个带有跳跃连接的卷积层堆栈，让我们更深入地了解ResNet的架构。ResNet架构是通过将多个构建块堆叠在一起来构建的。它们遵循与VGG相同的理念：
- en: The convolutional layers mostly have 3 × 3 filters.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层主要使用3 × 3的过滤器。
- en: The layers have the same number of filters for a given output feature-map size.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定的输出特征图大小，层具有相同数量的过滤器。
- en: If the feature-map size is halved, the number of filters is doubled to preserve
    the time complexity per layer.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征图大小减半，则过滤器数量加倍以保持每层的时序复杂度。
- en: 'ResNet uses conv layers with a stride of 2 to downsample, unlike VGG, which
    had multiple max pooling layers. The core architecture consists of the following
    components:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与VGG使用多个最大池化层不同，ResNet使用步长为2的卷积层进行下采样。其核心架构由以下组件组成：
- en: '*Five convolutional layer blocks*—The first convolutional block consists of
    a 7 × 7 kernel with `stride=2`, `padding=3`, and `num_features=64`, followed by
    a max pooling layer with a 3 × 3 kernel, `stride=2`, and `padding=1`. The feature
    map size is reduced from (224, 224) to (56, 56). The remaining convolutional blocks
    `ResidualConvBlock`) are built by stacking multiple basic shortcut blocks together.
    Each basic block uses 3 × 3 filters, as described.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*五个卷积层块*——第一个卷积层块由一个7 × 7的核组成，`stride=2`，`padding=3`，`num_features=64`，随后是一个3
    × 3核的最大池化层，`stride=2`，`padding=1`。特征图大小从(224, 224)减少到(56, 56)。其余的卷积层（`ResidualConvBlock`）是通过堆叠多个基本快捷块来构建的。每个基本块使用3
    × 3的过滤器，如上所述。'
- en: '*Classifier*—An average pooling block that runs on top of the conv block output,
    followed by a FC layer, which is used for classification.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类器*——在卷积块输出之上运行的平均池化块，随后是一个FC层，用于分类。'
- en: You are encouraged to examine the diagrams in the original paper to understand
    the details. Now, let’s see how to implement a ResNet in PyTorch.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励您查看原始论文中的图表以了解细节。现在，让我们看看如何在PyTorch中实现ResNet。
- en: PyTorch- ResNet
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch-ResNet
- en: In this section, we discuss how to implement a ResNet-34 from scratch. Note
    that this is seldom done in practice. The `torchvision` package provides ready-made
    implementations for all ResNet architectures. However, by building the network
    from scratch, we gain a deeper understanding of the architecture. First, let’s
    implement a basic skip connection block (`BasicBlock`) to see how the shortcut
    connection works.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论如何从头开始实现 ResNet-34。请注意，在实践中很少这样做。`torchvision` 包为所有 ResNet 架构提供了现成的实现。然而，通过从头开始构建网络，我们可以更深入地理解架构。首先，让我们实现一个基本的跳转连接块（`BasicBlock`）来了解快捷连接是如何工作的。
- en: NOTE Fully functional code for ResNet, executable via Jupyter Notebook, can
    be found at [http://mng.bz/5K9q](http://mng.bz/5K9q).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：ResNet 的完整功能代码，可通过 Jupyter Notebook 执行，可在 [http://mng.bz/5K9q](http://mng.bz/5K9q)
    找到。
- en: Listing 11.8 PyTorch code for `BasicBlock`
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.8 `BasicBlock` 的 PyTorch 代码
- en: '[PRE8]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Instantiates two conv layers of filter size 3 × 3
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ① 实例化两个 3 × 3 大小的卷积层
- en: ② When input and output feature maps are not the same size, the input feature
    map is downsampled using a 1 × 1 convolution layer.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ② 当输入和输出特征图大小不同时，使用 1 × 1 卷积层对输入特征图进行下采样。
- en: ③ Creates a skip connection
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建跳转连接
- en: 'Notice how the output of the residual block is a function of both the input
    and the output of the convolutional layer: `ReLU(conv_out+x)`. This assumes that
    *x* and *conv*_*out* have the same shape. (Shortly, we discuss what to do when
    this isn’t the case.) Also note that adding the skip connections does not increase
    the number of parameters. The shortcut connections are parameter-free. This makes
    the solution cheap from a computational point of view and is one of the charms
    of shortcut connections.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意残差块输出是卷积层输入和输出的函数：`ReLU(conv_out+x)`。这假设 *x* 和 *conv_out* 具有相同的形状。（稍后我们将讨论当这种情况不成立时应该怎么做。）此外，添加跳转连接不会增加参数数量。快捷连接是无参数的。这使得从计算角度来看，解决方案成本低，这是快捷连接的一个魅力。
- en: 'Next, let’s implement a residual conv block consisting of a number of basic
    blocks stacked on top of each other. We have to handle two cases when it comes
    to basic blocks:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实现一个由多个基本块堆叠而成的残差卷积块。在处理基本块时，我们必须处理两种情况：
- en: '*Case 1*—Output feature map spatial resolution = Input feature map spatial
    resolution AND Number of output features = Number of input features. This is the
    most common case. Since there is no change in the number of features or the spatial
    resolution of the feature map, we can easily add the input and output via shortcut
    connections.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*情况 1*—输出特征图空间分辨率 = 输入特征图空间分辨率 且 输出特征数量 = 输入特征数量。这是最常见的情况。由于特征数量或特征图的空间分辨率没有变化，我们可以通过快捷连接轻松地将输入和输出相加。'
- en: '*Case 2*—Output feature map spatial resolution = 1/2 * Input feature map spatial
    resolution AND Number of output features = 2 * Number of input features. Remember
    that ResNet uses conv layers with a stride of 2 to downsample. The number of features
    is also doubled. This is done by the first basic block of every conv block except
    the second conv block). In this case, the input and output are not the same size.
    So how do we add them together as part of the skip connection? 1 × 1 convs are
    the answer. The spatial resolution of the input feature map is halved, and the
    number of input features is doubled by using a 1 × 1 conv with `stride=2` and
    `num_features=2 * num_input_features`.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*情况 2*—输出特征图空间分辨率 = 输入特征图空间分辨率的一半 且 输出特征数量 = 输入特征数量的两倍。记住 ResNet 使用步长为 2 的卷积层进行下采样。特征数量也加倍。这是通过每个卷积块（除了第二个卷积块）的第一个基本块来实现的。在这种情况下，输入和输出大小不同。那么我们如何将它们作为跳转连接的一部分相加呢？1
    × 1 卷积是答案。通过使用 `stride=2` 和 `num_features=2 * num_input_features` 的 1 × 1 卷积，输入特征图的空间分辨率减半，输入特征数量通过加倍。'
- en: Listing 11.9 PyTorch code for `ResidualConvBlock`
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.9 `ResidualConvBlock` 的 PyTorch 代码
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① The residual block is a stack of basic blocks.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ① 残差块是基本块的堆叠
- en: ② 1 × 1 convs to downsample the input feature map
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ② 1 × 1 卷积用于下采样输入特征图
- en: With this, we are ready to implement ResNet-34.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就准备好实现 ResNet-34。
- en: Listing 11.10 PyTorch code for ResNet-34
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.10 ResNet-34 的 PyTorch 代码
- en: '[PRE10]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Instantiates the first conv layer
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ① 实例化第一个卷积层
- en: ② List of size 4, specifying the number of basic blocks per ResidualConvBlock
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ② 大小为 4 的列表，指定每个 `ResidualConvBlock` 的基本块数量
- en: ③ Instantiates four residual blocks
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 实例化四个残差块
- en: ④ Flattens the conv feature before passing it to the classifier
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 在传递给分类器之前对卷积特征进行展平
- en: 'As discussed earlier, we typically don’t implement our own ResNet. Instead,
    we use the ready-made implementation from the *torchvision* package like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们通常不会实现自己的 ResNet。相反，我们使用来自 *torchvision* 包的现成实现，如下所示：
- en: '[PRE11]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Instantiates resnet34 from the torchvision package
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从 torchvision 包中实例化 resnet34
- en: While we looked at the ResNet-34, there are deeper ResNet architectures like
    ResNet-50, ResNet-101, and ResNet-151 that use a different version of `BasicBlock`
    called `BottleneckLayer`. Similarly, there are several other variants inspired
    by ResNet, like ResNext, Wide ResNet, and so on. We don’t discuss these individual
    variants in this book because the core idea behind them remains the same. You
    are encouraged to read the original papers for a deeper understanding of the subject.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看 ResNet-34 时，还有更深层次的 ResNet 架构，如 ResNet-50、ResNet-101 和 ResNet-151，它们使用一个名为
    `BottleneckLayer` 的不同版本的 `BasicBlock`。同样，还有许多受 ResNet 启发的其他变体，如 ResNext、Wide ResNet
    等。在这本书中，我们不讨论这些个别变体，因为它们背后的核心思想是相同的。我们鼓励您阅读原始论文，以更深入地了解该主题。
- en: 11.2.4 PyTorch Lightning
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.4 PyTorch Lightning
- en: Let’s revisit the problem of digit classification that we looked at earlier.
    We primarily discussed the LeNet architecture and implemented it in PyTorch. Now,
    let’s implement the end-to-end code for training the LeNet model. Instead of doing
    it in vanilla PyTorch, we use the Lightning framework because it significantly
    simplifies the model development and training process.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下之前讨论的数字分类问题。我们主要讨论了 LeNet 架构，并在 PyTorch 中实现了它。现在，让我们实现训练 LeNet 模型的端到端代码。我们不是使用普通的
    PyTorch 来做这件事，而是使用 Lightning 框架，因为它显著简化了模型开发和训练过程。
- en: Although PyTorch has all we need to train models, there’s much more to deep
    learning than attaching layers. When it comes to the actual training, we need
    to write a lot of boilerplate code, as we have seen in previous examples. This
    includes transferring data from CPU to GPU, implementing the training driver,
    and so on. Additionally, if we need to scale training/inferencing on multiple
    devices/machines, another set of integrations often needs to be done.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 PyTorch 有我们训练模型所需的一切，但深度学习远不止于附加层。当涉及到实际训练时，我们需要编写大量的样板代码，正如我们在之前的例子中所看到的。这包括将数据从
    CPU 转移到 GPU、实现训练驱动程序等。此外，如果我们需要在多个设备/机器上扩展训练/推理，通常还需要完成另一套集成。
- en: PyTorch Lightning is a solution that provides the APIs required to build models,
    data sets, and so on. It provides clean interfaces with hooks to be implemented.
    The underlying Lightning framework calls these hooks at appropriate points in
    the training process. The idea is that Lightning leaves the research logic to
    us while automating the rest of the boilerplate code. Additionally, Lightning
    brings in features like multi-GPU training, floating-point 16, and training on
    TPU inherently without requiring any code changes. More details about PyTorch
    Lightning can be found at [https://www.pytorchlightning.ai/tutorials](https://www.pytorchlightning.ai/tutorials).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 是一个提供构建模型、数据集等所需 API 的解决方案。它提供了干净的接口和要实现的钩子。底层 Lightning 框架在训练过程中的适当点调用这些钩子。其理念是
    Lightning 将研究逻辑留给我们，而自动处理其余的样板代码。此外，Lightning 还带来了多 GPU 训练、16 位浮点数和无需代码更改即可在 TPU
    上训练等特性。有关 PyTorch Lightning 的更多详细信息，请参阅 [https://www.pytorchlightning.ai/tutorials](https://www.pytorchlightning.ai/tutorials)。
- en: 'Training a model using PyTorch Lightning involves three main components: `DataModule`,
    `LightningModule`, and `Trainer`. Let’s see what each of these does.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch Lightning 训练模型涉及三个主要组件：`DataModule`、`LightningModule` 和 `Trainer`。让我们看看每个组件的作用。
- en: DataModule
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 数据模块
- en: '`DataModule` is a shareable, reusable class that encapsulates all the steps
    needed to process data. All data modules must inherit from `LightningDataModule`,
    which provides methods to be overridden. In this specific case, we will implement
    MNIST as a data module. This data module can now be used across multiple experiments
    spanning various models and architectures.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataModule` 是一个可共享、可重用的类，它封装了处理数据所需的所有步骤。所有数据模块都必须继承自 `LightningDataModule`，它提供了可覆盖的方法。在这种情况下，我们将实现
    MNIST 作为数据模块。现在，这个数据模块可以在跨越各种模型和架构的多个实验中使用。'
- en: Listing 11.11 PyTorch code for an MNIST data module
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.11 PyTorch 中 MNIST 数据模块的代码
- en: '[PRE12]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Download, tokenizes, and prepares the raw data
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ① 下载、标记并准备原始数据
- en: ② Splits the training data set into training and validation sets
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将训练数据集分为训练集和验证集
- en: ③ Creates the train data loader, which provides a clean interface for iterating
    over the data set. It handles batching, shuffling, and fetching data via multiprocessing,
    all under the hood.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建训练数据加载器，它提供了一个干净的接口来迭代数据集。它处理批处理、洗牌以及通过多进程获取数据，所有这些都在幕后完成。
- en: ④ Creates the val data loader
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建验证数据加载器
- en: ⑤ Creates the test data loader
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建测试数据加载器
- en: ⑥ Number of object categories in the data set
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 数据集中对象类别的数量
- en: LightningModule
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: LightningModule
- en: '`LightningModule` essentially groups all the research code into a single module,
    making it self-contained. Notice the clean separation between `DataModule` and
    `LightningModule`—this makes it easy to train/evaluate the same model on different
    data sets. Similarly, different models can be easily trained/evaluated on the
    same data set.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`LightningModule`本质上将所有研究代码组合成一个单一模块，使其自包含。注意`DataModule`和`LightningModule`之间的清晰分离——这使得在不同的数据集上训练/评估相同的模型变得容易。同样，不同的模型也可以轻松地在相同的数据集上训练/评估。'
- en: 'A Lightning module consists of the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Lightning模块由以下内容组成：
- en: A model or system of models defined in the `init` method
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`init`方法中定义的模型或模型系统
- en: A training loop defined in `training_step`
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`training_step`中定义的训练循环
- en: A validation loop defined in `validation_step`
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`validation_step`中定义的验证循环
- en: A testing loop defined in `testing_step`
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`testing_step`中定义的测试循环
- en: Optimizers and schedulers defined in `configure_optimizers`
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`configure_optimizers`中定义的优化器和调度器
- en: Let’s see how we can define the LeNet classifier as a Lightning module.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将LeNet分类器定义为Lightning模块。
- en: Listing 11.12 PyTorch code for LeNet as a Lightning module
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 列出11.12作为Lightning模块的LeNet的PyTorch代码
- en: '[PRE13]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① In the init method, we typically define the model, the criterion, and any
    other setup steps required for training the model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在`init`方法中，我们通常定义模型、准则以及训练模型所需的任何其他设置步骤。
- en: ② Instantiates cross-entropy loss
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化交叉熵损失
- en: '③ Implements the model’s forward pass. In this case, the input is a batch of
    images, and the output is the logits. X.shape: [batch_size, C, H, W].'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '③ 实现模型的正向传递。在这种情况下，输入是一批图像，输出是logits。X.shape: [batch_size, C, H, W]。'
- en: '④ Logits.shape: [batch_size, num_classes]'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '④ Logits.shape: [batch_size, num_classes]'
- en: ⑤ Runs the forward pass, performs softmax to convert the resulting logits into
    probabilities, and returns the class with the highest probability
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 运行前向传递，执行softmax将结果logits转换为概率，并返回概率最高的类别
- en: ⑥ Abstracts out common functionality between the training and test loops, including
    the running forward pass, computing loss, and accuracy
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 抽象出训练和测试循环之间的共同功能，包括运行前向传递、计算损失和准确率
- en: '⑦ Implements the basic training step: run forward pass, compute loss, accuracy.
    Logs any necessary values and returns the total loss.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 实现基本训练步骤：运行前向传递、计算损失和准确率。记录任何必要的值并返回总损失。
- en: ⑧ Called at the end of all test steps for each epoch. The output of every test
    step is available via outputs. Here we compute the average test loss and accuracy
    by averaging across all test batches.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 在每个epoch的测试步骤结束时调用。每个测试步骤的输出都可通过outputs获取。在这里，我们通过平均所有测试批次来计算平均测试损失和准确率。
- en: '⑨ Implements the basic validation step: run forward pass, compute loss and
    accuracy, return them.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 实现基本验证步骤：运行前向传递、计算损失和准确率，并返回它们。
- en: ⑩ Configures the SGD optimizer
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 配置SGD优化器
- en: ⑪ Implements logic to save the model. We save the model with the best val accuracy.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 实现了保存模型的逻辑。我们使用最佳验证准确率保存模型。
- en: 'The model is independent of the data. This allows us to potentially run the
    `LeNetClassifier` model on other data modules without any code changes. Note that
    we are not doing the following steps:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 模型与数据独立。这使我们能够在不进行任何代码更改的情况下，在其它数据模块上运行`LeNetClassifier`模型。注意，我们并没有执行以下步骤：
- en: Moving the data to a device
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据移动到设备上
- en: Calling `loss.backward`
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`loss.backward`
- en: Calling `optimizer.backward`
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`optimizer.backward`
- en: Setting `model.train()` or `eval()`
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`model.train()`或`eval()`
- en: Resetting the gradients
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置梯度
- en: Implementing the trainer loop
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现训练循环
- en: All of these are taken care of by PyTorch Lightning, thus eliminating a lot
    of boilerplate code.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都由PyTorch Lightning处理，从而消除了大量的样板代码。
- en: Trainer
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: Trainer
- en: 'We are ready to train our model, which can be done using the `Trainer` class.
    This abstraction achieves the following:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好训练我们的模型，可以使用`Trainer`类来完成。这种抽象实现了以下功能：
- en: We maintain control over all aspects via PyTorch code without an added abstraction.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过PyTorch代码维护对所有方面的控制，而不需要额外的抽象。
- en: The trainer uses best practices embedded by contributors and users from top
    AI labs.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练器使用了来自顶级人工智能实验室的贡献者和用户的最佳实践。
- en: The trainer allows us to override any key part that we don’t want automated.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练器允许我们覆盖任何我们不希望自动化的关键部分。
- en: Listing 11.13 PyTorch code for `Trainer`
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.13 用于 `Trainer` 的 PyTorch 代码
- en: '[PRE14]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Instantiates the data set
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ① 实例化数据集
- en: ② Instantiates the model
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化模型
- en: ③ Instantiates the trainer
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 实例化训练器
- en: ④ Trains the model
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 训练模型
- en: 'Note that we do not write the trainer loop: we just call `trainer.fit` to train
    the model. Additionally, the logging automatically enables us to look at the loss
    and accuracy curves via TensorBoard.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不需要编写训练器循环：我们只需调用 `trainer.fit` 来训练模型。此外，日志自动使我们能够通过 TensorBoard 查看损失和准确度曲线。
- en: Listing 11.14 PyTorch code for inferencing a model
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.14 用于模型推理的 PyTorch 代码
- en: '[PRE15]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Runs model.predict()
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ① 运行 model.predict()
- en: To run inferencing using the trained model, we run `model.predict` on the input.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用训练好的模型进行推理，我们在输入上运行 `model.predict`。
- en: '11.3 Object detection: A brief history'
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 物体检测：简史
- en: Until now, we have discussed the classification problem wherein we categorize
    an image as 1 of *N* object categories. But in many cases, this is not sufficient
    to truly describe an image. Consider figure [11.11](#fig-multi-object-image)—a
    very realistic image with four animals standing one on top of another, posing
    for the camera. It would be useful to know the object categories of each of the
    animals and their location (bounding-box coordinates) in the image. This is referred
    to as the object detection/localization problem. So, how do we localize objects
    in images?
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了分类问题，其中我们将图像分类为 *N* 个对象类别之一。但在许多情况下，这不足以真正描述图像。考虑图 [11.11](#fig-multi-object-image)——一个非常逼真的图像，四只动物叠在一起，对着相机摆姿势。知道每只动物的对象类别及其在图像中的位置（边界框坐标）将是有用的。这被称为物体检测/定位问题。那么，我们如何在图像中定位物体呢？
- en: Let’s say we could extract regions in the image so that each region contained
    only one object. We could then run an image classifier deep neural network (which
    we looked at earlier) to classify each region and select the regions with the
    highest confidence. This was the approach adopted by one of the first deep learning-based
    object detectors, a region-based CNN (R-CNN; [https://arxiv.org/pdf/1311.2524.pdf](https://arxiv.org/pdf/1311.2524.pdf)).
    Let’s look at this in more detail.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们能够从图像中提取区域，使得每个区域只包含一个对象。然后我们可以运行一个图像分类深度神经网络（我们之前已经讨论过）来分类每个区域，并选择置信度最高的区域。这是第一个基于深度学习的物体检测器之一，基于区域的卷积神经网络（R-CNN；[https://arxiv.org/pdf/1311.2524.pdf](https://arxiv.org/pdf/1311.2524.pdf)）采用的方法。让我们更详细地看看这个方法。
- en: 11.3.1 R-CNN
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 R-CNN
- en: 'The R-CNN approach to object detection consists of three main stages:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN 的物体检测方法包括三个主要阶段：
- en: '*Selective search to identify regions of interest*—This step uses a computer
    vision-based algorithm capable of extracting candidate regions. We do not go into
    the details of the selective search; you are encouraged to go through the original
    paper to understand the details. Selective search generates around 2,000 region
    proposals per image.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*选择性搜索以识别感兴趣的区域*——这一步使用一种基于计算机视觉的算法，能够提取候选区域。我们不深入选择性搜索的细节；我们鼓励您阅读原始论文以了解细节。选择性搜索为每张图像生成大约
    2,000 个区域建议。'
- en: '*Feature extraction*—A deep convolution neural network extracts features from
    each region of interest. Since deep neural networks typically take in fixed-sized
    inputs, the regions (which could be arbitrarily sized) are warped into a fixed
    size before being fed into the deep neural network.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征提取*——深度卷积神经网络从每个感兴趣区域中提取特征。由于深度神经网络通常需要固定大小的输入，因此区域（可能具有任意大小）在输入深度神经网络之前被扭曲成固定大小。'
- en: '*Classification/Localization*—A class-specific support vector machine (SVM)
    is trained on the extracted features to classify the region. Additionally, bounding-box
    regressors are added to fine-tune the object’s location within the region. During
    training, each region is assigned a ground-truth (GT) class based on its overlap
    with GT boxes. It is assigned a positive label if there is a high overlap and
    a negative label otherwise.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类/定位*——在提取的特征上训练一个特定类别的支持向量机（SVM）来分类区域。此外，添加边界框回归器以微调对象在区域内的位置。在训练过程中，每个区域根据其与
    GT 框的重叠情况分配一个基于真实情况的（GT）类别。如果重叠度高，则分配正标签，否则分配负标签。'
- en: '![](../../OEBPS/Images/CH11_F11_Chaudhury.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH11_F11_Chaudhury.png)'
- en: Figure 11.11 An image with multiple objects of different shapes and sizes
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 一个包含不同形状和大小的多个物体的图像
- en: 11.3.2 Fast R-CNN
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.2 Fast R-CNN
- en: 'One of the biggest disadvantages of the R-CNN-based approach is that we have
    to extract features for every region proposal independently. So, if we generate
    2,000 proposals for a single image, we have to run 2,000 forward passes to extract
    the region features. This is prohibitively expensive and extremely slow (during
    both training and inference). Additionally, training is a multistage pipeline—selective
    search, the deep network, the SVMs on top of the features, and the bounding-box
    regressors—that is cumbersome to train and inference. To solve these problems,
    the authors of the R-CNN introduced a new technique called a Fast R-CNN [https://arxiv.org/pdf/1504.08083.pdf](https://arxiv.org/pdf/1504.08083.pdf)).
    It significantly improved speeds: it is 9× faster than the R-CNN during training
    and 213× faster at test time. Additionally, it improves the quality of object
    detection.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 基于R-CNN的方法最大的缺点之一是我们必须独立地为每个区域提议提取特征。因此，如果我们为单个图像生成2,000个提议，我们就必须运行2,000次前向传递来提取区域特征。这是非常昂贵且极其缓慢的（在训练和推理过程中）。此外，训练是一个多阶段管道——选择性搜索、深度网络、特征上的SVMs以及边界框回归器——这使得训练和推理都变得繁琐。为了解决这些问题，R-CNN的作者引入了一种称为Fast
    R-CNN的新技术[https://arxiv.org/pdf/1504.08083.pdf](https://arxiv.org/pdf/1504.08083.pdf))。它显著提高了速度：在训练时比R-CNN快9倍，在测试时快213倍。此外，它还提高了目标检测的质量。
- en: 'Fast R-CNN makes two major contributions:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN做出了两个主要贡献：
- en: '*Region of interest (RoI) pooling*—As mentioned, one of the fundamental issues
    with R-CNN is the need for multiple forward passes to extract the features for
    the region proposals of a single image. Instead, can we extract the features in
    one go? This problem is solved using RoI pooling. The Fast R-CNN uses the entire
    image as the input to the CNN instead of a single region proposal. Then, the RoIs
    (region proposal bounding boxes) are used on top of the CNN output to extract
    the region features in one pass. We will go into the details of RoI pooling as
    part of our Faster R-CNN discussion.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*兴趣区域（RoI）池化*——如前所述，R-CNN的一个基本问题是需要多次前向传递来提取单个图像区域提议的特征。那么，我们能否一次性提取特征呢？这个问题通过RoI池化得到解决。Fast
    R-CNN使用整个图像作为CNN的输入，而不是单个区域提议。然后，在CNN输出上使用RoIs（区域提议边界框）来一次性提取区域特征。我们将在Faster R-CNN讨论中详细介绍RoI池化的细节。'
- en: '*Multitask loss*—The Fast R-CNN eliminates the need to use SVMs. Instead, the
    deep neural network does both classification and bounding-box regression. Unlike
    R-CNN, which only uses deep networks for feature extraction, the Fast R-CNN is
    more end-to-end. It is a single architecture for region proposal feature extraction,
    classification, and regression.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多任务损失*——Fast R-CNN消除了使用SVMs的需求。相反，深度神经网络同时进行分类和边界框回归。与仅使用深度网络进行特征提取的R-CNN不同，Fast
    R-CNN更加端到端。它是一个用于区域提议特征提取、分类和回归的单个架构。'
- en: 'The high-level algorithm is as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 高级算法如下：
- en: Use selective search to generate 2,000 region proposals/RoIs per image.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用选择性搜索为每张图像生成2,000个区域提议/RoIs。
- en: In a single pass of the Fast R-CNN, extract all the RoI features in a single
    pass using RoI pooling and then classify and localize objects using the classification
    and regression heads.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Fast R-CNN的单次传递中，使用RoI池化一次性提取所有RoI特征，然后使用分类和回归头进行对象分类和定位。
- en: Since the feature extraction for all the region proposals happens in one pass,
    this approach is significantly faster than the R-CNN, where every proposal needs
    a separate forward pass. Additionally, since the neural network is trained end
    to end—that is, asked to do classification and regression—the accuracy of object
    detection is also improved.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有区域提议的特征提取都在一次传递中完成，这种方法比R-CNN快得多，在R-CNN中每个提议都需要单独的前向传递。此外，由于神经网络是端到端训练的——即被要求进行分类和回归——目标检测的准确性也得到了提高。
- en: 11.3.3 Faster R-CNN
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.3 Faster R-CNN
- en: Why settle for fast when we can be faster? The Fast R-CNN was significantly
    faster than the R-CNN. However, it still needed selective search to be run to
    obtain region proposals. The selective-search algorithm can only be run on CPUs.
    Additionally, the algorithm is slow and time-consuming. Thus it became a bottleneck.
    Is there a way to get rid of selective search?
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们可以更快时，为什么还要满足于快速？Fast R-CNN比R-CNN快得多。然而，它仍然需要运行选择性搜索来获取区域提议。选择性搜索算法只能在CPU上运行。此外，该算法速度慢且耗时。因此，它成为了一个瓶颈。有没有办法摆脱选择性搜索？
- en: 'The obvious idea to consider is using deep networks to generate region proposals.
    This is the core idea of Faster R-CNN (FRCNN; [https://arxiv.org/pdf/1506.01497.pdf](https://arxiv.org/pdf/1506.01497.pdf)):
    it eliminates the need for selective search and lets a deep network learn the
    region proposals. It was one of the first near-real-time object detectors. Since
    we are using a deep network to learn the region proposals, the region proposals
    are also better. Thus the resulting accuracy of the overall architecture is also
    much better.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到的明显想法是使用深度网络来生成区域提议。这是Faster R-CNN（FRCNN；[https://arxiv.org/pdf/1506.01497.pdf](https://arxiv.org/pdf/1506.01497.pdf)）的核心思想：它消除了选择性搜索的需求，并允许深度网络学习区域提议。它是最早的近实时对象检测器之一。由于我们使用深度网络来学习区域提议，区域提议也因此变得更好。因此，整体架构的结果精度也大大提高。
- en: 'We can view the FRCNN as consisting of two core modules:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将FRCNN视为由两个核心模块组成：
- en: '*Region proposal network (RPN)*—This is the module responsible for generating
    the region proposals. RPNs are designed to efficiently predict region proposals
    with a wide range of scales and aspect ratios.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*区域提议网络（RPN）*——这是负责生成区域提议的模块。RPN被设计为高效地预测具有广泛尺度和宽高比的区域提议。'
- en: '*R-CNN module*—This is the same as the Fast R-CNN. It receives a bunch of region
    proposals and performs RoI pooling followed by classification and regression.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R-CNN模块*——这与Fast R-CNN相同。它接收一系列区域提议，并执行RoI池化，随后进行分类和回归。'
- en: 'Another important thing to note is that the RPN and the R-CNN module share
    the same convolutional layers: the weights are shared rather than learning two
    separate networks. In the next section, we discuss the Faster R-CNN in detail.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的重要事项是，区域提议网络（RPN）和R-CNN模块共享相同的卷积层：权重是共享的，而不是学习两个独立的网络。在下一节中，我们将详细讨论Faster
    R-CNN。
- en: '11.4 Faster R-CNN: A deep dive'
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 Faster R-CNN：深入探讨
- en: Figure [11.12](#fig-frcnn-architecture) shows the high-level architecture of
    the FRCNN. The convolutional layers (which we also call the convolutional backbone)
    extract feature maps from the input image. The RPN operates on these feature maps
    and emits candidate RoIs. The RoI pooling layer generates a fixed-sized feature
    vector for each region of interest and passes it on to a set of FC layers that
    emit softmax probability estimates over *K* object classes (plus a catch-all “background”
    class) and four numbers representing the bounding-box coordinates for each of
    the *K* classes. Let’s look at each of the components in more detail.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图[11.12](#fig-frcnn-architecture)显示了FRCNN的高级架构。卷积层（我们也称其为卷积主干）从输入图像中提取特征图。RPN在这些特征图上操作，并发出候选区域提议。RoI池化层为每个感兴趣区域生成一个固定大小的特征向量，并将其传递给一组全连接层，这些全连接层发出关于*K*个对象类别的softmax概率估计（加上一个“背景”类的通配符）以及代表每个*K*个类别的边界框坐标的四个数字。让我们更详细地看看每个组件。
- en: '![](../../OEBPS/Images/CH11_F12_Chaudhury.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH11_F12_Chaudhury.png)'
- en: 'Figure 11.12 architecture. (Source: “Faster R-CNN: Toward real-time proposal
    networks”; [https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497).)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12架构。（来源：“Faster R-CNN：迈向实时提议网络”；[https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)）
- en: 11.4.1 Convolutional backbone
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.1 卷积主干
- en: 'In the original implementation, the FRCNN used the convolution layers of VGG-16
    as the convolutional backbone for both the RPN and the R-CNN modules. There has
    been one minor modification: the last pooling layer after the fifth convolution
    layer (conv5) is removed. As we’ve discussed regarding VGG architectures earlier,
    VGG reduces the spatial size of the feature map by 2 in every conv block via max
    pooling. Since the last pooling layer is removed, the spatial size is reduced
    by a factor of 2⁴ = 16. So a 224 × 224 image is reduced to a 14 × 14 feature map
    at the output. Similarly, an 800 × 800 image would be reduced to a 50 × 50 feature
    map.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始实现中，FRCNN使用了VGG-16的卷积层作为RPN和R-CNN模块的卷积骨干。有一个小的修改：在第五个卷积层（conv5）之后的最后一个池化层被移除。正如我们之前讨论VGG架构时提到的，VGG通过最大池化在每个卷积块中将特征图的空间尺寸减少2。由于移除了最后一个池化层，空间尺寸减少了2⁴
    = 16的因子。因此，一个224 × 224的图像在输出时被缩减到14 × 14的特征图。同样，一个800 × 800的图像将被缩减到50 × 50的特征图。
- en: 11.4.2 Region proposal network
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.2 区域提议网络
- en: The RPN takes in an image (of any arbitrary size) as input and emits a set of
    rectangular proposals that could potentially contain objects as output. The RPN
    operates on top of the convolutional feature map output by the last shared convolution
    layer. With the VGG backbone, an input image of size (h, w) is scaled down to
    (h/16, w/16). So each 16 × 16 spatial region in the input image is reduced to
    a single point on the convolutional feature map. Thus each point in the output
    convolutional feature map represents a 16 × 16 patch in the input image. The RPN
    operates on top of this feature map. Another subtle point to remember is that
    while each point in the convolutional feature map is chosen to correspond to a
    16 × 16 patch, it has a significantly larger receptive field (the region in the
    input feature map that a particular output feature is affected by). The embedding
    at each point in the feature map is thus, in effect, the digest of a large receptive
    field.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: RPN以任意大小的图像作为输入，并输出一组可能包含对象的矩形提议。RPN在最后共享卷积层输出的卷积特征图上操作。使用VGG骨干网络时，输入图像的大小（h,
    w）缩小到（h/16, w/16）。因此，输入图像中的每个16 × 16空间区域被缩减为卷积特征图上的一个单独的点。因此，输出卷积特征图中的每个点代表输入图像中的一个16
    × 16补丁。RPN就在这个特征图上操作。另一个需要注意的微妙之处是，虽然卷积特征图中的每个点都选择对应一个16 × 16补丁，但它有一个显著更大的感受野（输入特征图中受特定输出特征影响的区域）。因此，特征图上每个点的嵌入实际上是大感受野的摘要。
- en: Anchors
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 锚点
- en: A key aspect of the object-detection problem is the variety of object sizes
    and shapes. Objects can range from very small (cats) to very large elephants).
    Additionally, objects can have different aspect ratios. Some objects may be wide,
    some may be tall, and so on. A naive solution is to have a single neural network
    detector head capable of identifying and recognizing all these objects of varying
    sizes and shapes. As you can imagine, this would make the job of the neural network
    detector extremely complex. A simpler solution is to have a wide variety of neural
    network detector heads, each responsible for solving a much simpler problem. For
    example, one head will only focus on large, tall objects and will only fire when
    such objects are present in the image. The other heads will focus on other sizes
    and aspect ratios. We can think of each head as being responsible for doing a
    single simple job. This type of setup greatly aids and benefits learning.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测问题的关键方面是物体的大小和形状的多样性。物体可以从非常小（猫）到非常大的大象）。此外，物体可以有不同的长宽比。一些物体可能很宽，一些可能很高，等等。一个简单的解决方案是拥有一个单一的神经网络检测头，能够识别和识别各种大小和形状的物体。正如你可以想象的那样，这将使神经网络检测器的任务变得极其复杂。一个更简单的解决方案是拥有多种神经网络检测头，每个头负责解决一个更简单的问题。例如，一个头只关注大而高的物体，并且只有在图像中存在这样的物体时才会触发。其他头将关注其他大小和长宽比。我们可以将每个头视为负责完成一个单一简单的工作。这种设置极大地帮助并促进了学习。
- en: This was the intuition behind the introduction of *anchors*. Anchors are like
    reference boxes of varying shapes and sizes. All proposals are made relative to
    anchors. Each anchor is uniquely characterized by its size and aspect ratio and
    is tasked with detecting similarly shaped objects in the image. At each sliding-window
    location, we have multiple anchors spanning different sizes and aspect ratios.
    The original FRCNN architecture supported nine anchor configurations spanning
    three sizes and three aspect ratios, thus supporting a wide variety of shapes.
    These correspond to anchor boxes of scales (8, 16, 32) and aspect ratios (0.5,
    1.0, and 2.0), respectively (see figure [11.13](#fig-anchors)). Anchors are now
    ubiquitous across object detectors.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是引入*锚点*背后的直觉。锚点类似于形状和大小各异的参考框。所有提议都是相对于锚点提出的。每个锚点都由其大小和宽高比唯一表征，并负责检测图像中形状相似的对象。在每次滑动窗口位置，我们都有多个跨越不同大小和宽高比的锚点。原始FRCNN架构支持九种锚点配置，涵盖三种大小和三种宽高比，从而支持广泛的形状。这些对应于尺度（8,
    16, 32）和宽高比（0.5, 1.0和2.0）的锚点框（参见图[11.13](#fig-anchors)）。锚点现在在目标检测器中无处不在。
- en: '![](../../OEBPS/Images/CH11_F13_Chaudhury.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH11_F13_Chaudhury.png)'
- en: Figure 11.13 The left column shows the various grid-point locations on the output
    convolution feature original implementation) anchors across multiple sizes and
    aspect ratios. The right column shows the various anchors at a particular grid
    point.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13 左列显示了输出卷积特征原始实现中不同大小和宽高比的各种网格点位置。右列显示了特定网格点上的各种锚点。
- en: NOTE Fully functional code for generating anchors, executable via Jupyter Notebook,
    can be found at [http://mng.bz/nY48](http://mng.bz/nY48).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：用于生成锚点的完整功能代码，可通过Jupyter Notebook执行，可在[http://mng.bz/nY48](http://mng.bz/nY48)找到。
- en: Listing 11.15 PyTorch code to generate anchors at a particular grid point
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.15 在特定网格点生成锚点的PyTorch代码
- en: '[PRE16]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① Generates the height and width for different scales and aspect ratios
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ① 为不同的尺度和宽高比生成高度和宽度
- en: ② Generates a bounding box centered around ctr_x, ctr_y) with width w, and height
    h
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在(ctr_x, ctr_y)周围生成一个宽度为w，高度为h的边界框
- en: Listing 11.16 PyTorch code to generate all anchors for a given image
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.16 生成给定图像所有锚点的PyTorch代码
- en: '[PRE17]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① This isn’t the most efficient way to generate anchors. We’ve written simple
    code to ease understanding.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这不是生成锚点最有效的方法。我们编写了简单的代码以简化理解。
- en: ② Generates anchor boxes centered at every point in the conv feature map, which
    corresponds to a 16 × 16 (subsample, subsample) region in the input
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在卷积特征图中的每个点上生成锚点框，这对应于输入中的16 × 16（子采样，子采样）区域
- en: ③ Uses a function defined in listing [11.15](#code-generate-anchors-at-grid-point)
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用列表[11.15](#code-generate-anchors-at-grid-point)中定义的函数
- en: ④ Defines config parameters and generates anchors
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 定义配置参数并生成锚点
- en: 'The RPN slides a small network over the output convolution feature map. The
    small network operates on an *n* × *n* spatial window of the convolution feature
    map. At each sliding-window location, it generates a lower-dimensional feature
    vector (512 dimensions for VGG) that is fed into a box-regression layer (reg)
    and a box-classification layer (cls). For each of the anchor boxes centered at
    that sliding window location, the classifier predicts *objectness*: a value from
    0 to 1, where 1 indicates the presence of the object and the regressor predicts
    the region proposal relative to the anchor box. This architecture is naturally
    implemented with an *n* × *n* convolutional layer followed by two sibling 1 ×
    1 convolutional layers (for reg and cls), respectively. The original implementation
    in the FRCNN paper uses *n* = 3, which results in an effective receptive field
    of 228 pixels when using the VGG backbone. Figure [11.14](#fig-rpn-architecture)
    illustrates this in detail. Note that this network consists of only convolutional
    layers. Such an architecture is called a *fully convolutional network* FCN). FCNs
    do not have an input size restriction. Because they'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH11_F14_Chaudhury.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 RPN architecture. From each sliding window, a 512-dimensional feature
    vector is generated using 3 × 3 convs. A 1 × 1 conv layer (classifier) takes the
    512-dimensional feature Similarly, another 1 × 1 conv layer (regressor) generates
    4*k* bounding-box coordinates from the 512-dimensional feature vector.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: consist of only convolution layers, they can work with arbitrary-sized inputs.
    In the FCN, the combination of the *n* × *n* and 1 × 1 layers is equivalent to
    applying an FC layer over every embedding at each point in the convolutional feature
    map. Also, because we are convolving a convolutional network on top of the feature
    map to generate the regression and classification scores, the convolutional weights
    are common/shared across different positions on the feature map. This makes the
    approach translation invariant. A cat at the top of the image and a cat at the
    bottom of the image are picked up by the same anchor configuration (scale, aspect
    ratio) if they are similarly sized.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the fully convolutional network of the RPN, executable
    via Jupyter Notebook, can be found at [http://mng.bz/nY48](http://mng.bz/nY48).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.17 PyTorch code for the FCN of the RPN
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① Instantiates the small network that is convolved over the output conv feature
    map. It consists of a 3 × 3 conv layer followed by a 1 × 1 conv layer for classification
    and another 1 × 1 conv layer for regression.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '② Output of the backbone: a convolutional feature map of size (batch_size,
    in_channels, h, w)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: ③ Converts (batch_size, h, w, 2k) to batch_size, h*w*k, 2)
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: ④ Converts (batch_size, h, w, 4k) to batch_size, h*w*k, 4)
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ (batch_size, num_anchors, 2) tensor representing the classification score
    for each anchor box
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ (batch_size, num_anchors, 4) tensor representing the box coordinates relative
    to the anchor box
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ （batch_size, num_anchors, 4）表示相对于锚框的框坐标的张量
- en: Generating GT for an RPN
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 为RPN生成GT
- en: 'So far, we have generated many anchor bounding boxes and a neural network capable
    of generating the classification and regression offsets for every anchor. While
    training the RPN, we need to provide a target GT) that both the classifier and
    regressor should predict for each anchor box. To do so, we need to look at the
    objects in the image and assign them to relevant anchors that contain the object.
    The idea is as follows: out of the thousands of anchors, *the anchors that contain
    most of the object should try predicting and localizing the object.* We saw earlier
    that the intuition behind creating anchors was to ensure that each anchor is responsible
    for one particular type of object shape, aspect ratio). Thus it makes sense that
    only anchors that contain the object are responsible for classifying it.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经生成了许多锚框边界框和一个能够为每个锚框生成分类和回归偏移量的神经网络。在训练RPN时，我们需要为每个锚框提供一个目标GT（分类器和回归器都应该预测的目标），为此，我们需要查看图像中的对象并将它们分配给包含对象的相应锚点。思路如下：在数千个锚点中，*包含大部分对象的锚点应该尝试预测和定位对象。*
    我们之前看到，创建锚点的直觉是为了确保每个锚点负责一种特定的对象形状、宽高比。因此，只有包含对象的锚点负责对其进行分类是有意义的。
- en: To measure whether the object lies within the anchor, we rely on intersection
    over union (IoU) scores. The IoU between two bounding boxes is defined as (area
    of overlap)/(area of union). So, if the two bounding boxes are very similar, their
    overlap is high, and their union is close to the overlap, resulting in a high
    IoU. If the two bounding boxes are varied, then their area of overlap is minimal,
    resulting in a low IoU (see figure [11.15](#fig-iou)).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 要测量对象是否位于锚框内，我们依赖于交集与并集（IoU）分数。两个边界框之间的IoU定义为（重叠面积）/（并集面积）。因此，如果两个边界框非常相似，它们的重叠区域很大，它们的并集接近重叠区域，从而产生一个高的IoU。如果两个边界框差异很大，那么它们的重叠区域最小，导致IoU很低（见图[11.15](#fig-iou)）。
- en: '![](../../OEBPS/Images/CH11_F15_Chaudhury.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH11_F15_Chaudhury.png)'
- en: Figure 11.15 section of the two areas divided by the union of the two areas.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15部分，两个区域被它们的并集分割。
- en: 'FRCNN provides some guidelines for assigning labels to the anchor boxes:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: FRCNN为分配锚框标签提供了一些指导方针：
- en: 'We assign a positive label 1 (which represents an object being present in the
    anchor box) to two kinds of anchors:'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将正标签1（表示锚框中存在对象）分配给两种类型的锚点：
- en: The anchor(s) with the greatest IoU overlap with a GT box
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与GT框IoU重叠最大的锚点
- en: An anchor that has an IoU overlap greater than 0.7 with the GT box
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与GT框IoU重叠大于0.7的锚点
- en: We assign a negative label 0 (which represents no object being present in the
    anchor box, implying that it contains only background) to a non-positive anchor
    if its IoU ratio is less than 0.3 for all GT boxes.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将一个非正锚点分配一个负标签0（这表示锚框中没有对象存在，意味着它只包含背景），如果其IoU比率对所有GT框都小于0.3。
- en: Anchors that are neither positive nor negative do not contribute to the training
    objective.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 既不是正标签也不是负标签的锚点不会对训练目标做出贡献。
- en: Note that a single GT object may assign positive labels to multiple anchors.
    These outputs must be suppressed later to prevent duplicate detections (we discuss
    this in the subsequent sections). Also, any anchor box that lies partially outside
    the image is ignored.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一个单独的GT对象可能将正标签分配给多个锚点。这些输出必须在稍后抑制，以防止重复检测（我们将在后续章节中讨论这一点）。此外，任何部分位于图像之外的锚框都被忽略。
- en: NOTE Fully functional code for assigning GT labels to anchor boxes, executable
    via Jupyter Notebook, can be found at [http://mng.bz/nY48](http://mng.bz/nY48).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：分配锚框GT标签的完整功能代码，可通过Jupyter Notebook执行，可以在[http://mng.bz/nY48](http://mng.bz/nY48)找到。
- en: Listing 11.18 PyTorch code to assign GT labels for each anchor box
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 列出11.18 PyTorch代码为每个锚框分配GT标签
- en: '[PRE19]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Finds valid anchors that lie completely inside the image
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ① 找到完全位于图像内的有效锚点
- en: ② Assigns s label of -1 (not any class) for each valid anchor
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ② 为每个有效锚点分配s标签为-1（不属于任何类别）
- en: ③ Obtains the valid anchor boxes
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 获取有效锚框
- en: ④ Tensor of shape (num_gt_bboxes, num_valid_anchor_bboxes), representing the
    IoU between the GT and anchors
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 形状为（num_gt_bboxes, num_valid_anchor_bboxes）的张量，表示GT和锚点之间的IoU
- en: ⑤ Finds the highest IoU for every GT bounding box
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 找到每个GT边界框的最高IoU
- en: ⑥ Finds all the indices where the IOU = highest GT IOU
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 找到所有IOU = 最高GT IOU的索引
- en: ⑦ Finds the highest IoU for every anchor box
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Assigns 0 (background) for negative anchors where IoU < 0.3
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Assigns 1 (objectness) for positive anchor where IoU > 0.7
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ For every GT bounding box, assigns the anchor with the highest IoU as a positive
    anchor
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with imbalance
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Given our strategy of assigning labels to anchors, notice that the number of
    negative anchors is significantly greater than the number of positive anchors.
    For example, for the example image, we obtained only 24 positive anchors as opposed
    to 7,439 negative anchors. If we train directly on such an imbalanced data set,
    neural networks can typically learn a local minimum by classifying every anchor
    as a negative anchor. In our example, if we predicted every anchor to be a negative
    anchor, our resulting accuracy would be 7439/(7439+22): 99.7%. However, the resulting
    neural network is practically useless because it has not learned anything. In
    other words, the imbalance will lead to bias toward the dominant class. To deal
    with this imbalance, there are typically three strategies:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '*Undersampling*—Sample less of the dominant class.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Oversampling*—Sample more of the less-dominant class.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Weighted loss*—Set the cost for misclassifying less-dominant classes much
    higher than the dominant class.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FRCNN utilizes the idea of undersampling. For a single image, there are multiple
    positive and negative anchors. From these thousands of anchors, we randomly sample
    256 anchors in an image to compute the loss function, where the sampled positive
    and negative anchors have a ratio of up to 1:1\. If there are fewer than 128 positive
    samples in an image, we pad the minibatch with negative ones.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Assigning targets to anchor boxes
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen how to sample and assign labels to anchors. The next question
    is how to come up with the regression targets:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '*Case 1: label = −1*—Unsampled/invalid anchor. These do not contribute to the
    training objective, so regression targets do not matter.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Case 2: label = 0*—Background anchor. These anchors do not contain any objects,
    so they also should not contribute to regression.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Case 3: label = 1*—Positive anchor. These anchors contain objects. We need
    to generate regression targets for these anchors.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider only the case of positive anchors. The key intuition here is
    that *the anchors already contain a majority of the object*. Otherwise, they wouldn’t
    have become positive anchors. So there is already significant overlap between
    the anchor and the object in question. Therefore it makes sense to learn the offset
    from the anchor bounding box to the object bounding box. The regressor is tasked
    with learning this offset: that is, what delta we must make to the anchor bounding
    box for it to become the object bounding box. the FRCNN adopts the following parameterization:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '*t[x]* = (*x* - *x[a]*)/*w[a]*'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '*t[y]* = (*y* - *y[a]*)/*h[a]*'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '*t[w]* = *log*(*w*/*w[a]*)'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '*t[h]* = *log*(*h*/*h[a]*)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Equation 11.2
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: where *x*, *y*, *w*, and *h* denote the GT bounding box’s center coordinates
    and its width and height, and *x[a]*, *y**[a]*, *w[a]*, and *h[a]* denote the
    anchor bounding box’s center coordinates and its width and height. *t[x]*, *t[y]*,
    *t[w]*, and *t[h]* are the regression targets. The regressor is, in effect, learning
    to predict the delta between the anchor bounding box and the GT bounding box.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for assigning regression targets to anchor boxes,
    executable via Jupyter Notebook, can be found at [http://mng.bz/nY48](http://mng.bz/nY48).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.19 PyTorch code to assign regression targets for each anchor box
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① (n, 4) tensor in (xtl, ytl, xbr, br) format
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: ② (n, 4 tensor) in (x, y, w, h) format
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: ③ (n, 4) tensors representing the bounding boxes for the region of interest
    and GT, respectively
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: ④ (n, 4) tensor containing the regression targets
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: RPN loss function
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'We have defined the RPN fully convolutional network and how we can generate
    labels and regression targets for the outputs of the RPN FCN. Now we need to discuss
    the loss function that enables us to train the RPN. As you would expect, there
    are two loss terms:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '*Classification loss*—Applies to both the positive and negative anchors. We
    use the standard cross-entropy loss used in any standard classifier.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regression loss*—Applies *only* to the positive anchors. Here we use smooth
    L1 loss, which is defined as'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_11-03.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
- en: Equation 11.3
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: We can think of smooth L1 loss as a combination of L1 and L2 loss. If the value
    is < beta, it behaves like an L2 loss (mean squared error [MSE]). Otherwise, it
    behaves like an L1 loss. In the case of the FRCNN, beta is set to 1\. The intuition
    behind this is simple. If we use pure L2 loss (MSE), then higher loss terms contribute
    to exponential loss because of the quadratic nature of the loss. This can lead
    to a bias where loss can be reduced by focusing on high-value items. Instead,
    if we use pure L1 loss, the higher loss terms still contribute more loss, but
    the effect is linear instead of quadratic. This still has a slightly worse bias
    toward higher loss terms. We get the best of both worlds by using L2 loss when
    the loss values are small and L1 loss when the loss values are large. When the
    loss value is small, because we are using L2 loss, its contribution is exponential/quadratic.
    And when the loss value is high, it still contributes linearly via L1 loss. Thus
    the network is incentivized to pay attention to low- and high-loss items.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall loss for an image can be defined as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_11-04.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
- en: Equation 11.4
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: where, *p[i]* is the predicted objectness probability for the anchor *i*. *p[i]*^*
    is the true objectness label for anchor *i*. It is 1 if the anchor is positive
    and 0 if the anchor is negative. *t[i]* = (*t[x]*, *t[y]*, *t[w]*, *t[h]*) are
    the regression predictions for anchor *i*, *t[i]*^* = (*t[x]*^*, *t[y]*^*, *t[w]*^*,
    *t[h]*^*) are the regression targets for anchor *i*, *N[cls]* is the number of
    anchors, and *N[pos]* is the number of positive anchors.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the RPN loss function, executable via Jupyter
    Notebook, can be found at [http://mng.bz/nY48](http://mng.bz/nY48).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.20 PyTorch code for the RPN loss function
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '① rpn_cls_scores: (num_anchors, 2) tensor representing RPN classifier scores
    for each anchor. rpn_loc: (num_anchors, 4) tensor representing RPN regressor predictions
    for each anchor. rpn_labels: (num_anchors) representing the class for each anchor
    (-1, 0, 1). rpn_loc_targets: (num_anchors, 4) tensor representing RPN regressor
    targets for each anchor.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: ② Ignores -1 as they are not sampled
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: ③ Finds the positive anchors
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Generating region proposals
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'We have so far discussed how the RPN works. The RPN predicts objectness and
    the regression offsets for every anchor. The next task is to generate good region
    RoIs and use them for training the R-CNN module. Since we are emitting objectness
    and regression offsets for every anchor, we have thousands of predictions. We
    cannot use all of them as RoIs. We need to generate the best RoIs from these scores
    and offsets to train our R-CNN. An obvious way to do this is to rely on the objectness
    scores: the higher the objectness score, the greater the likelihood that it contains
    an object and thus is a good RoI. Before we get there, we must do some basic processing
    steps:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 1\.  Convert the predicted offsets to bounding boxes. This is done by reversing
    the sequence of transformations
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '*x*^∗ = *t[x]*^∗ ∗ *w[a]* + *x[a]*'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '*y*^∗ = *t[y]*^∗ ∗ *h[a]* + *y[a]*'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '*w*^∗ = *e*^(*t[w]*^∗) ∗ *w[a]*'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '*h*^∗ = *e*^(*t[h]*^∗) ∗ *h[a]*'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Equation 11.5
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: where *x*^*, *y*^*, *w*^*, and *h*^* denote the predicted bounding box’s center
    coordinates and its width and height, and *t[x]*^*, *t[y]*^*, *t[w]*^*, and *t[h]*^*
    are the RPN loc predictions. The bounding boxes are then converted back into `xtl`,
    `ytl`, `xbr`, `ybr` format.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 2\.  The predicted bounding boxes can lie partially outside the image. We clip
    all the predicted bounding boxes to within the image.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 3\.  Remove any predicted bounding boxes with height or width less than `min_roi_threshold`.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Once these processing steps are done, we sort the predicted bounding boxes by
    objectness score and select *N* candidates. *N* = 12000 during training and *N*
    = 6000 while testing.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional generating region proposals from the RPN output, executable
    via Jupyter Notebook, can be found at [http://mng.bz/nY48](http://mng.bz/nY48).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.21 PyTorch code to generate region proposals from RPN output
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ① Clips the ROIs
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: ② Threshold based on min_roi_threshold
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: ③ Sorts based on objectness
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '④ Selects the top regions of interest. Shape: (n_train_pre_nms, 4).'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '⑤ Selects the top objectness scores. Shape: (n_train_pre_nms,).'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Non-maximal suppression (NMS)
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Many of the proposals will overlap. We are effectively selecting anchors at
    a stride of 16 pixels. Therefore even a reasonably sized object is picked up by
    multiple anchors, each of which will try to predict the object independently.
    We can see this overlapping nature when we look at the positive anchors in figure
    [11.16](#fig-pos-neg-anchors). We want to choose the most effective set of RoIs.
    But it is evident that choosing all the similar proposals does not make a good
    set of RoIs because they carry redundant information.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH11_F16_Chaudhury.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 minimum where every anchor is classified as negative. To prevent
    this, the FRCNN under-samples the negative anchors before training.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: To address this problem, we use a technique called *non-maximal suppression*
    (NMS). NMS is an algorithm that suppresses highly overlapping bounding boxes.
    The algorithm takes in bounding boxes and scores and works as follows.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 11.6 Non-maximal suppression
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: A list of bounding boxes B, corresponding scores S, and overlap threshold
    N'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Output: A list of filtered bounding boxes D'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '**while** likelihood is increasing **do**'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Select bounding box with highest confidence score
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Remove it from B and add it to the final list D
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Compare selected bounding box with remaining boxes in B using IoU
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Remove all bounding boxes from B with IoU > threshold
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '**end** **while**'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: return D
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: We use NMS with a 0.7 threshold to suppress the highly overlapping RoIs and
    choose the top *N* RoIs post-NMS to train the R-CNN. *N* = 2000 during training,
    and *N* = 300 while testing. Figure [11.17](#fig-pre-post-nms) shows bounding
    boxes on a sample image before and after NMS.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH11_F17_Chaudhury.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 The left column shows the bounding boxes (24) before NMS. The right
    column shows the bounding boxes (4) that remain after NMS.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for NMS, executable via Jupyter Notebook, can be
    found at [http://mng.bz/nY48](http://mng.bz/nY48).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.22 PyTorch code for NMS of RoIs
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ① Calls NMS implemented by torchvision
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.3 Fast R-CNN
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we saw how the RPN network takes an input image and
    emits a set of regions of interest that are likely to contain objects. Now let’s
    discuss the second leg of the FRCNN architecture, which takes in the RoIs and
    generates class probabilities and bounding-box coordinates for each object in
    the image. We briefly discussed this earlier. Here we revisit it in greater detail.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'We are given a set of RoIs (some of which contain the object). Our task is
    to train an object detector capable of localizing the objects. To do this, we
    need to extract the features corresponding to each RoI and pass them through a
    neural network (classifier and regressor) that learns to predict the class and
    the regression targets. The R-CNN solved this in a naive way: it extracted each
    RoI one at a time, warped it to make it a fixed size, and passed it through a
    deep CNN to extract the features corresponding to the RoI. Each RoI required a
    separate forward pass, making the approach very slow. The question, as always,
    is: can we do better?'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: RoI pooling
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider the convolutional backbone. It processes the whole image with
    several conv and max pooling layers to produce a conv feature map. We have also
    seen a subsampling factor of 16: that is, 16 × 16 pixels in the input image are
    reduced to a single point in the feature map. Also remember that the embedding
    at every grid point on the feature map is the representation/digest of a region
    in the input image.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '*Key idea 1* is that the features corresponding to each RoI are already present
    in the conv feature map, and we can extract them via the feature map. For example,
    say our RoI is (0, 0, 256, 256). We know that the (0, 0, 256, 256) region in the
    input image is represented by 0, 0, 256/16, 256/16): that is, the (0, 0, 16, 16)
    region in the conv feature map. Since the embedding for a point in the conv feature
    map is a digest of the receptive field, we can use these features directly as
    the features of the RoI. So to obtain the features for an RoI of (0, 0, 256, 256),
    we take all the embeddings corresponding to the region (0, 0, 16, 16) in the conv
    feature map. Since we are performing this feature extraction directly on the convolutional
    feature map, which is obtained for the entire image, we can obtain the RoI features
    for all the RoIs in a single forward pass. This eliminates the need for multiple
    forward passes.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '*Key idea 2* is as follows. We discussed a clever way of extracting the features
    corresponding to each RoI, and we want to use these features to train our classifier
    and regressor. However, there is a problem. As we know, RoIs are different sizes.
    And different-sized RoIs will lead to different feature embedding sizes. For example,
    if our RoI is (0, 0, 256, 256), our RoI feature embeddings are (16, 16, 512):
    that is, all the embeddings (of size 512) in the (0, 0, 16, 16) region of the
    conv feature map. If our RoI is (0, 0, 128, 128), then our RoI feature embeddings
    are (8, 8, 512): all the embeddings in the (0, 0, 8, 8) region of the conv feature
    map. And we know that neural networks typically need same-sized input. So how
    do we deal with input embeddings of different sizes? The answer is *RoI pooling*.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fix the size of the input ROI feature map that goes into the neural network.
    Our task is to reduce variable-sized RoI feature maps to a fixed size. If the
    fixed feature map size is set to be *H*, *W*, and our RoI corresponds to (*r*,
    *c*, *h*, *w*) in the conv feature map, we divide *h* and *w* into equal-sized
    blocks of size *h*/*H* and *w*/*W*, respectively, and apply max pooling on these
    blocks to obtain a *H*, *W* feature map. Going back to our example, let’s fix
    *H* = *W* = 4. Our expected fixed feature map size is (4,4,512). So when our RoI
    is (0, 0, 256, 256), our RoI feature embeddings are (16, 16, 512): *h* = *w* =
    16. We divide the 16 × 16 region into four 16/4, 16/4) regions and perform max
    pooling on each region to obtain a fixed-size (4,4,512) feature. Similarly, when
    our RoI is (0, 0, 128, 128), *h* = *w* = 8. We divide the 8 × 8 region into four
    (8/4, 8/4) regions and perform max pooling to obtain the fixed-size (4,4,512)
    feature.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'Astute readers will notice that we have carefully chosen our RoIs so that they
    are multiples of *H* and *W*, resulting in integer values for *h*/*H* and *w*/*W*,
    respectively. But in reality, this rarely happens. *h*/*H* and *w*/*W* are often
    floating-point numbers. What do we do in this case? The answer is *quantization*:
    that is, we choose the integer closest to *h*/*H* and *w*/*W*, respectively (floor
    operation, in the original implementation). This has been improved on by RoIAlign,
    which uses bilinear interpolation instead of quantization. We do not get into
    the details of RoIAlign here.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'In effect, if we have a large RoI, we divide the feature map into a fixed number
    of large regions and perform max pooling. And when we have a small RoI, we divide
    the feature map into a fixed number of small regions and perform max pooling.
    The size of the region used for pooling can change, but the output size remains
    fixed. The dimension of the RoI pooling output doesn’t depend on the size of the
    input feature map or the size of the RoIs: it’s determined solely by the number
    of sections we divide the RoI into—*H* and *W* (see figure [11.18](#fig-roi-pooling)).'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH11_F18_Chaudhury.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 A conv feature map with two regions of interest of different sizes.
    RoI pooling extracts a fixed-sized output feature map (2 × 2 in this image) from
    each of the RoIs in a single pass via max pooling. This enables us to extract
    fixed-sized representative feature vectors for each RoI, which are then fed into
    further classifier and regressor layers for classification and localization.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the purpose of RoI pooling is to perform max pooling on inputs of non-uniform
    sizes to obtain fixed-size feature maps. The Fast R-CNN and Faster R-CNN use 7
    × 7 as the fixed feature map size.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Fast R-CNN architecture
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the conv feature map and a set of RoIs, we have seen how the RoI pooling
    layer extracts a fixed-length feature vector from the feature map. Each feature
    vector is fed into a sequence of FC layers that finally branch into two sibling
    output layers: a classifier that produces softmax probability estimates over *K*
    object classes plus a catch-all “background” class, and a regressor that produces
    four real-valued numbers for each of the *K* object classes.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Generating GT for the Fast R-CNN
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: 'For every image, we have a list of RoIs generated by the RPN and a list of
    GT bounding boxes. How do we generate the GT and regression targets for each RoI?
    The idea remains the same as for our RPN: we use IoU scores. The algorithm is
    as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Compute the IoU between all RoIs and GT boxes.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each RoI, determine the GT bounding box with the highest IoU.
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the highest IoU is greater than a threshold (0.5), assign the corresponding
    GT label as the label for the RoI.
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the IoU is between [0.1, 0.5], assign the background label. Using a lower
    bound of 0.1 ensures that certain RoIs with small intersections with the GT are
    selected as background. This is helpful as it chooses hard examples for background;
    it is a form of hard negative mining.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the Fast R-CNN RoI head, executable via Jupyter
    Notebook, can be found at [http://mng.bz/nY48](http://mng.bz/nY48).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.23 PyTorch code for the Fast R-CNN RoI head
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ① num_classes + background
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '② x : (1, c, h, w) tensor representing the conv feature map. rois: (n, 4) tensor
    representing bounding boxes of RoIs'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '③ roi_cls_scores: (n, num_classes+1) tensor representing classification scores
    for each RoI. roi_loc: (n, (num_classes + 1) * 4) tensor representing the regression
    scores for each RoI'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: Training the Fast R-CNN
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'The RPN generates about 2,000 RoIs per image. Due to computational constraints,
    we cannot use all *N* RoIs. Instead, we sample a subset of them. The training
    minibatches are sampled hierarchically, first by sampling *K* images and then
    by sampling *R*/*K* RoIs from each image. *R* is set to 128 in the FRCNN. For
    this discussion, we assume that *K* = 1: that is, we have a single image per minibatch.
    So, given the RoIs for a single image, how do we sample 128 RoIs from it?'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple solution is to randomly sample 128 RoIs. However, this runs into the
    same data-imbalance issue that we discussed earlier: we end up sampling backgrounds
    a lot more frequently than the classes. To solve this problem, we adopt a sampling
    strategy similar to before. In particular, for a single image, we sample 128 RoIs
    such that the ratio of background to object is 0.75:0.25\. If fewer than 32 RoIs
    contain the objects, we pad the minibatch with more background RoIs.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: Assigning targets to RoI boxes
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Just as in the case of the RPN, we generate regression targets as offsets of
    the GT box from the region of interest for all RoIs that contain objects. For
    all background RoIs, the regression targets are not applicable.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Fast R-CNN loss function
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: 'We have defined the Fast R-CNN network and how we can generate labels and regression
    targets for its outputs. We need to discuss the loss function that enables us
    to train the Fast R-CNN. As you would expect, there are two loss terms:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '*Classification loss*— We use the standard cross-entropy loss used in any standard
    classifier.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regression loss*—The regression loss applies *only* to the object RoIs: background
    RoIs do not contribute to regression. Here we use the smooth L1 loss as we did
    in the RPN.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus the overall loss for a single RoI can be defined as follows:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_11-06.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
- en: Equation 11.6
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: where *p* is the predicted label for the RoI, *u* is the true label for the
    RoI, *t[u]* = (*t[x]*, *t[y]*, *t[w]*, *t[h]*) are the regression predictions
    for class *u*, and *v* = (*v[x]*, *v[y]*, *v[w]*, *v[h]*) are the regression targets.
    The overall loss can therefore be defined as
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_11-07.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
- en: Equation 11.7
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: where *p[i]* are the prediction probabilities for the RoI *i*, *p[i]*^* is the
    true label for RoI *i*; *t[i]* = (*t[x]*, *t[y]*, *t[w]*, *t[h]*) are the regression
    predictions for RoI *i* corresponding to class *p[i]*^*, *t[i]*^* = (*t[x]*^*,
    *t[y]*^*, *t[w]*^*, *t[h]*^*) are the regression targets for RoI *i*, *t[i]*^*
    = (*t[x]*^*, *t[y]*^*, *t[w]*^*, *t[h]*^*) are the regression targets for RoI
    *i*, and *N[pos]* is the number of object RoIs (non-background RoIs).
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the Fast R-CNN loss function, executable via
    Jupyter Notebook, can be found at [http://mng.bz/nY48](http://mng.bz/nY48).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.24 PyTorch code for the Fast R-CNN loss function
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '① (128, num_classes) tensor: RCNN classifier scores for each RoI'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '② (128, num_classes*4) tensor: RCNN regressor predictions for each class, RoI'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '③ (128,) tensor: true class for each RoI'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '④ (128, 4) tensor: RoI regressor targets for each RoI'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Finds the positive RoIs
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ (n, num_classes*4) to n, num_classes, 4)
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: Fast R-CNN inference
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: We have looked at how to train the Fast R-CNN module. Once the model is trained,
    the next question is how to use the model to generate output classes and bounding
    boxes.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: 'The Fast R-CNN model outputs a classification score and regression offsets
    for every RoI. We can safely ignore the background RoIs. For the rest of the RoIs,
    the class with the highest probability is chosen as the output label, and the
    offsets corresponding to that class are chosen. We apply post-processing steps
    similar to that of the RPN:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: We translate the offsets back to (`xtl`, `ytl`, `xbr`, `ybr`) format using the
    RoI.
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We clip the output bounding box to within the image boundaries
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We face a problem similar to before: the output probably has multiple bounding
    boxes corresponding to the same object. We deal with it in the same way as earlier:
    using NMS. There is one difference, however. In the case of the RPN, we applied
    a global NMS across all bounding boxes predicted by the RPN. Here, NMS is applied
    only across the bounding boxes belonging to the same class. This is done for all
    classes, which should intuitively make sense: there is no point in suppressing
    highly overlapping bounding boxes if the bounding boxes represent different classes.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the Fast R-CNN inference, executable via Jupyter
    Notebook, can be found at [http://mng.bz/nY48](http://mng.bz/nY48).
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.25 PyTorch code for the Fast R-CNN inference
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ① Trained instance of Fast_RCNN_ROI_Head
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: ② RoIs to inference
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: ③ (n, c, h, w) convolutional feature map
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: ④ Sets eval mode
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The predicted class is the class with the highest score.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ The predicted probabilities are obtained via softmax. The highest probability
    is chosen as the probability score for this prediction.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Converts locs from (n, num_classes*4) to (n, num_classes, 4)
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Selects offsets corresponding to the predicted label
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Asserts that we have outputs for each RoI
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Converts offsets to xtl, ytl, xbr, ybr)
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: ⑪ Clips bounding boxes to within images
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: ⑫ 0 is background, thus ignored
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: ⑬ Performs NMS for each class
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.4 Training the Faster R-CNN
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we have seen, the FRCNN consists of two subnetworks:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: An RPN responsible for generating good region proposals that contain objects
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Fast R-CNN responsible for object classification and detection from a list
    of RoIs
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus the FRCNN is a two-stage object detector. We have one stage that generates
    good region proposals and another that takes the region proposals and detects
    objects in the image. So how do we train the FRCNN?
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple idea would be to train two independent networks (RPN and Fast R-CNN).
    However, we do not want to do this because it is expensive. Additionally, if we
    do so, each network will modify the convolutional layers in its own way. As discussed
    earlier, we want to share the convolutional layers across the RPN and the Fast
    R-CNN modules. This ensures efficiency (only one conv backbone as opposed to two
    independent backbones). Additionally, both the RPN and FRCNN are performing similar
    tasks, so it intuitively makes sense to share the same set of convolutional features.
    Therefore, we need to develop a technique that allows for sharing convolutional
    layers between the two networks rather than learning two separate networks. The
    original FRCNN paper proposed two techniques to train the model:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '*Alternate optimization (AltOpt)*—We first train RPN and use the proposals
    to train the Fast R-CNN. The network tuned by the Fast R-CNN is then used to initialize
    the RPN, and this process is iterated. This involves multiple rounds of training
    alternating between training the RPN and Fast R-CNN.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Approximate joint training*—The RPN and Fast R-CNN networks are merged into
    one network during training. In each SGD iteration, the forward pass generates
    region proposals that are treated just like fixed, precomputed proposals when
    training a Fast R-CNN detector. We combine the RPN and Fast R-CNN losses and perform
    backpropagation as usual. This training is significantly faster as we are training
    both networks together end to end. However, the optimization is approximate because
    we treat the RPN-generated proposals as fixed, whereas in reality, they are a
    function of the RPN. So, we’re ignoring one derivative.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both techniques give similar accuracy. So joint training, which is significantly
    faster, is preferred.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.5 Other object-detection paradigms
  id: totrans-497
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have looked at the FRCNN in detail and discussed several key ideas
    that contribute to its success. Several other object-detection paradigms have
    also been developed. Some are inspired by the FRCNN, borrowing and/or improving
    on the ideas established by the FRCNN. In this section, we briefly look at a few
    of them.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: You Only Look Once (YOLO)
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: 'The FRCNN is a two-stage detector: an RPN followed by the Fast R-CNN, which
    runs on the region proposals generated by the RPN. YOLO ([https://arxiv.org/pdf/1506.02640](https://arxiv.org/pdf/1506.02640.pdf)[.pdf](https://arxiv.org/pdf/1506.02640.pdf)),
    as the name implies, is a single-stage object detector. A single convolutional
    network simultaneously predicts multiple bounding boxes and class probabilities
    for those boxes directly from full images in one go. Some of the salient features
    of YOLO are as follows:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: YOLO is significantly faster ( 10× faster than the FRCNN) due to its much simpler
    architecture. YOLO can even be used for real-time object detection.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike the FRCNN, where the R-CNN module looks only at the region proposals,
    YOLO looks directly at the full image during training and testing.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The speed of YOLO comes at the cost of accuracy. While YOLO is significantly
    faster, it is less accurate than the FRCNN.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several other improvements have been made on top of YOLO to improve accuracy
    while trying to maintain the simple, fast architecture. These include YOLO v2,
    YOLO v3, and so on.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: MultiBox Single-Shot Detector SSD)
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: 'SSD ([https://arxiv.org/pdf/1512.02325.pdf](https://arxiv.org/pdf/1512.02325.pdf))
    tries to achieve a good balance between speed and accuracy. It is a single-stage
    network like YOLO: that is, it eliminates the proposal-generation (RPN) and subsequent
    feature-resampling stages. It also borrows the ideas of anchors from the FRCNN:
    applying a conv net on top of feature maps to make predictions relative to a fixed
    set of bounding boxes.'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: Thus, a single deep network predicts class scores and box offsets for a fixed
    set of default bounding boxes using small convolutional filters applied to feature
    maps. To achieve high detection accuracy, feature maps at different scales are
    used to make predictions at different scales.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: SSD is much more accurate than YOLO; however, it is still not as accurate as
    the FRCNN (especially for small objects).
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: Feature pyramid network (FPN)
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature maps generated by conv nets are pyramidal: as we go deeper, the
    spatial resolution of the feature map keeps decreasing, and we expect the semantic
    information represented by the feature map to be more meaningful. High-resolution
    maps have low-level features, whereas low-resolution maps have more semantic features.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the FRCNN, we applied object detection on only the last convolution
    map. SSD shows that there is useful information by using other feature maps for
    prediction. But SSD builds this pyramid high up the network (past the fourth convolution
    layer [conv4] of VGG). It specifically avoids the use of lower-layer features.
    Thus it misses the opportunity to reuse the higher-resolution maps of the feature
    hierarchy. FPN shows that these features are important, especially for detecting
    small objects.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: The FPN ([https://arxiv.org/pdf/1612.03144.pdf](https://arxiv.org/pdf/1612.03144.pdf))
    relies on an architecture that combines low-resolution, semantically strong features
    with high-resolution, semantically weak features via a top-down pathway and lateral
    connections. The bottom-up pathway is the forward pass of the convolutional layers.
    In the top-down path, there is a back connection from lower resolution to higher
    resolution via simple upsampling (it is merged with the bottom-up feature map
    using 1 × 1 convolutions). This merged feature map is used at every level to learn
    and make predictions.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: The FPN was originally implemented on top of the FRCNN. It is much more accurate,
    but it is much slower than YOLO/SSD-style approaches.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: We have only briefly mentioned the other prominent detection paradigms. The
    fundamental principles behind them remain the same. You are encouraged to read
    the papers to get a deeper and better understanding.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we took an in-depth look at various deep-learning techniques
    for object classification and localization:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: LeNet is a simple neural network that can classify handwritten digits from the
    MNIST data set.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple networks like LeNet don’t extend well to more real-world image classification
    problems. Hence deeper neural networks that have more expressive power are needed.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The VGG network is one of the most popular deep convolutional neural networks.
    It improves on prior state-of-the-art deep neural networks by using more convolution
    layers with smaller 3 × 3) filters. Such an architecture has two advantages: (1)
    more expressive power because of the added nonlinearity that comes from stacking
    more layers, and (2) a reduced number of parameters. Three 3 × 3 filters have
    27*C*² parameters, whereas a single 7 × 7 filter (which cover the same receptive
    field) has 49*C*² parameters (81% more).'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VGG (and AlexNet) use ReLU nonlinear layers instead of sigmoid layers because
    they do not suffer from the vanishing gradient problem. Using ReLUs speeds up
    training, resulting in faster convergence.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception blocks provide an efficient way to increase the depth and width of
    a neural network while keeping the computational budget constant. Multiscale filters
    are used at each convolution layer to learn patterns of different sizes, and 1
    × 1 convolutions are used for dimensionality reduction (which reduces the number
    of parameters needed, thereby improving computational efficiency).
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet is another popular convolutional neural network. The ResNet architecture
    was motivated by the fact that simply stacking layers beyond a certain point does
    not help and causes degradation even in training accuracies. This is a counterintuitive
    result because we expect that deeper networks can, at the very least, learn as
    much as their shallower counterparts. The authors of the ResNet papers showed
    that this may happen because the identity function is hard for neural networks
    to learn. To tackle this, they proposed shortcut/skip connections to simplify
    the neural network’s learning objective. This is the key idea behind ResNet and
    enables training of much deeper neural networks.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster R-CNN is one of the most popular object detectors. It is a two-stage
    network consisting of (1) a region proposal network RPN), which is responsible
    for predicting regions of interest that could potentially contain objects; and
    (2) an R-CNN module, which takes the region proposals as input and emits class
    scores and bounding boxes efficiently.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RPN module uses multiple anchor boxes (at each point on the conv feature
    map) to handle objects of different sizes and aspect ratios. It convolves a small
    network over the conv feature map to make predictions about objectness and bounding
    boxes at each sliding window location. Remember that the small network is a fully
    convolutional network (FCN) comprising 3 × 3 convs followed by 1 × 1 convs, enabling
    this approach to work with arbitrary-sized inputs and making it translation invariant.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RoI pooling provides an efficient way to extract a fixed-sized feature vector
    from region proposals of varying sizes, all in one pass. These feature vectors
    are fed to a classifier and regressor for classification and localization, respectively.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-maxima suppression (NMS) is a technique to de-duplicate overlapping bounding
    boxes.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FRCNN can be trained using two methods: alternative optimization (AltOpt) and
    approximate joint training. Both approaches lead to similar accuracy numbers,
    but approximate joint training is significantly faster.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You Only Look Once (YOLO), MultiBox Single-Shot Detector SSD), and feature pyramid
    networks (FPN) are some other popular object detectors.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
