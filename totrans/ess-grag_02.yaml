- en: 3 Advanced vector retrieval strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Query rewriting techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced text-embedding strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing parent document retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 2 of this book, you learned about the basics of text embeddings and
    vector similarity search. By converting text into numerical vectors, you have
    seen how machines can understand the semantic meaning of content. Combining text-embedding
    and vector similarity search techniques allows for optimized and accurate retrieval
    of relevant unstructured text from vast amounts of documents, enabling more accurate
    and up-to-date answers in RAG applications. Suppose you have implemented and deployed
    a RAG application as described in chapter 2\. After some testing, you and the
    users of the RAG application noticed that the accuracy of the generated answers
    is lacking due to incomplete or irrelevant information in the retrieved documents.
    Consequently, you have been assigned the task of enhancing the retrieval system
    to improve the accuracy of the generated answers.
  prefs: []
  type: TYPE_NORMAL
- en: As with any technology, the basic implementations of text embeddings and vector
    similarity search can produce insufficient retrieval accuracy and recall. The
    embeddings generated from a user’s query might not always align closely with those
    of documents containing the crucial information needed due to differences in terminology
    or context. This discrepancy can lead to situations where documents highly relevant
    to the query’s intent are overlooked, as the embedding representation of the query
    does not capture the essence of the information sought.
  prefs: []
  type: TYPE_NORMAL
- en: One strategy to improve the retrieval accuracy and recall is to rewrite the
    query used to find relevant documents. The query-rewriting approach aims to bridge
    the gap between the user’s query and the information-rich documents by reformulating
    the query in a way that better aligns with the language and context of the target
    documents. This query refinement improves the chances of finding documents containing
    relevant information, thereby enhancing the accuracy of responses to the original
    query. Examples of query-rewriting strategies are hypothetical document retriever
    (Gao et al., 2022) or step-back prompting (Zheng et al., 2023). The step-back
    prompting strategy is visualized in figure 3.1\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 Query rewriting by using the step-back technique to increase the
    vector retrieval accuracy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 3.1 outlines a process where a user’s query is transformed to improve
    document retrieval outcomes, a technique known as *step-back prompting*. In the
    scenario presented, the user poses a detailed question regarding Estella Leopold’s
    educational history during a specific timeframe. This initial question is then
    processed by a language model such as GPT-4 with query-rewriting capabilities,
    which rephrases it into a more general inquiry about Estella Leopold’s educational
    background. The purpose of this step is to cast a wider net during the search
    process, as the rewritten query is more likely to align with a range of documents
    that may contain the required information.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to improve retrieval accuracy is by changing the document embedding
    strategy. In the previous chapter, you embedded a section of text, retrieved that
    same text, and used it as input to an LLM to generate an answer. However, vector
    retrieval systems are flexible, as you’re not limited to embedding the exact text
    you plan to retrieve. Instead, you can embed content that better represents the
    document’s meaning, such as more contextually relevant sections, synthetic questions,
    or paraphrased versions. These alternatives can better capture key ideas and themes,
    resulting in more accurate and relevant retrieval. Two examples of advanced embedding
    strategies are shown in figure 3.2\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 Hypothetical question and parent document retriever strategies
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The left side of figure 3.2 demonstrates the hypothetical question strategy.
    With the hypothetical question–embedding strategy, you must determine the questions
    the information in the document can answer. For example, you could use an LLM
    to generate hypothetical questions, or you could use the conversation history
    of your chatbot to come up with the questions a document can answer. The idea
    is that instead of embedding the original document itself, you embed the questions
    the document can answer. For instance, the question “What did Leopold study at
    the University of California?” is encoded by the vector `[1,2,3,0,5]` in figure
    3.2\. When a user poses a question, the system computes the query’s embedding
    and searches for the nearest neighbors among the precomputed question embeddings.
    The goal is to locate questions that closely match and are semantically similar
    to the user question. The system then retrieves the documents that contain the
    information that can answer these similar questions. In essence, the hypothetical
    question–embedding strategy involves embedding potential questions a document
    can answer and using these embeddings to match and retrieve relevant documents
    in response to user queries.
  prefs: []
  type: TYPE_NORMAL
- en: The right side of figure 3.2 illustrates the parent document–embedding strategy.
    In this approach, the original document—referred to as the parent—is split into
    smaller units called *child chunks*, typically based on a fixed token count. Instead
    of embedding the entire parent document as a single unit, you compute a separate
    embedding for each child chunk. For example, the chunk “Leopold attained her master’s
    in botany” might be embedded as the vector `[1,` `0,` `3,` `0,` `1]`. When a user
    submits a query, the system compares it against these child embeddings to find
    the most relevant matches. However, rather than returning only the matched chunk,
    the system retrieves the entire original parent document associated with it. This
    allows the language model to operate with the full context of the information,
    increasing the chances of generating accurate and complete answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This strategy addresses a common limitation of embedding long documents: when
    you embed the full parent document, the resulting vector can blur distinct ideas
    through averaging, making it harder to match specific queries effectively. By
    contrast, splitting the document into smaller chunks allows for more precise matching
    while still enabling the system to return the full context when needed.'
  prefs: []
  type: TYPE_NORMAL
- en: Other strategies to improve retrieval accuracy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Beyond changing the document-embedding strategy, several other techniques can
    enhance retrieval accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Finetuning the text-embedding model*—By adjusting the embedding model on domain-specific
    data, you can improve its ability to capture the context of user queries, leading
    to a closer semantic match with relevant documents. Note that finetuning typically
    requires more compute and infrastructure. In addition, once the model is updated,
    all existing document embeddings must be recomputed to reflect the changes—this
    can be resource intensive for large document repositories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reranking strategies*—After an initial set of documents is retrieved, reranking
    algorithms can reorder them based on relevance to the user’s intent. This second
    pass often uses more complex models or scoring heuristics to refine the results.
    Reranking helps surface the most relevant content even if the initial match was
    suboptimal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metadata-based contextual filtering*—Many documents contain structured metadata
    such as authorship, publication date, topic tags, or source type. Applying filters
    based on this metadata—either manually or as part of the retrieval pipeline—can
    significantly narrow the candidate documents before semantic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: matching, increasing precision. For example, a query about recent policy updates
    can be restricted to documents published within the last year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hybrid retrieval (keyword + dense vector search)*—Combining sparse retrieval
    (e.g., keyword-based search) with dense vector retrieval (semantic search) offers
    the best of both worlds. Keyword search excels at precise matches and rare terms,
    while dense retrieval captures the broader meaning. Hybrid systems can merge and
    rerank results from both methods to maximize both recall and precision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While all these strategies can improve retrieval quality, detailed implementation
    guidance is beyond the scope of this book, except for hybrid retrieval, which
    was introduced in chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the remainder of this chapter, we’ll move from concepts to code and walk
    through the implementation step by step. To follow along, you’ll need access to
    a running, blank Neo4j instance. This can be a local installation or a cloud-hosted
    instance; just make sure it’s empty. You can follow the implementation directly
    in the accompanying Jupyter notebook available here: [https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch03.ipynb](https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch03.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’ve implemented the basic RAG system from chapter 2, but the retrieval
    accuracy wasn’t quite good enough. The responses lacked relevance or missed important
    context, and you suspect the system isn’t retrieving the most useful documents
    to support high-quality answers. To address this, you’ve decided to enhance the
    existing RAG pipeline by adding a step-back prompting step to improve the quality
    of the query itself. Additionally, you’ll switch from the basic retriever to a
    parent document retriever strategy. This approach enables more granular and accurate
    information retrieval by matching on smaller chunks while still providing the
    full parent document as context.
  prefs: []
  type: TYPE_NORMAL
- en: These improvements aim to boost both the relevance of retrieved content and
    the overall accuracy of the generated answers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Step-back prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned, step-back prompting is a query-rewriting technique that aims
    to improve the accuracy of vector retrieval. An example from the original paper
    (Zheng et al., 2023) demonstrates this process: the specific query “Which team
    did Thierry Audel play for from 2007 to 2008?” is broadened to “Which teams did
    Thierry Audel play for in his career?” to improve vector search precision and
    consequently the accuracy of the generated answers. By transforming a detailed
    question into a broader, high-level query, step-back prompting reduces the complexity
    of the vector search process. The idea is that broader queries typically encompass
    a more comprehensive range of information, making it easier for the model to identify
    relevant facts without getting bogged down by the specifics.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors used an LLM for the query rewriting task, as shown in figure 3.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 Rewriting queries using the step-back approach with an LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs are an excellent fit for query-rewriting tasks as they excel at natural
    language comprehension and generation. You don’t have to train or finetune a new
    model for each task. Instead, you can provide task instructions in the input prompt.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the step-back prompting paper used the system prompt in the following
    listing to instruct the LLM on how to rewrite the input query.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 System prompt of an LLM for generating step-back questions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Query rewriting instructions'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Few-shot examples'
  prefs: []
  type: TYPE_NORMAL
- en: The system prompt in listing 3.1 begins by giving the LLM a simple instruction
    to rewrite a user’s question into a more generic, step-back version. On its own,
    this kind of instruction is known as *zero-shot prompting*, which relies solely
    on the LLM’s general capabilities and understanding of the task, without providing
    any examples. However, to guide the model more effectively and ensure consistent
    results, the authors chose to expand the prompt with several examples of the desired
    paraphrasing behavior. This technique is called *few-shot prompting*, where a
    small number of examples (typically two to five) are included in the prompt to
    illustrate the task. Few-shot prompting helps the LLM better understand the expected
    transformation by anchoring it in concrete instances, improving the quality and
    reliability of the output.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve the query rewriting, all you need to do is send the system prompt
    found in listing 3.1 along with the user’s question to an LLM. The specific function
    for this task is outlined in the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Function to generate a step-back question
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can now test the step-back prompt generation by executing the code shown
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Executing the step-back prompt function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The results in listing 3.3 demonstrate a successful execution of the step-back
    prompt generation function. By transforming the specific query about Thierry Audel’s
    team from 2007 to 2008 into a broader question regarding his entire career history,
    the function effectively broadens the context and should increase the retrieval
    accuracy and recall.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To explore the step-back prompt generation’s effectiveness, try applying it
    to various questions and observe how it broadens the context. You can also change
    the system prompt to observe how it affects the output.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Parent document retriever
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The parent document retriever strategy involves dividing a large document into
    smaller sections, calculating embeddings for each section rather than the whole
    document, and using these embeddings to match user queries more accurately, ultimately
    retrieving the entire document for context-rich responses. However, as you cannot
    feed the whole PDF directly to the LLM, you first need to split the PDF into parent
    documents and then further divide those parent documents into child documents
    for embedding and retrieval. The graph representation of the parent and child
    documents is shown in figure 3.4\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 Parent document graph representation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 3.4 illustrates a graph-based approach to storing and organizing documents
    for the parent document retrieval strategy. At the top, a PDF node represents
    the entire document, labeled with a title and an identifier. This node is connected
    to several parent document nodes. You will use a 2,000-character limit to split
    the PDF into parent documents in this example. These parent document nodes are,
    in turn, linked to child document nodes, with each child node containing a 500-character
    chunk of the corresponding parent node text. The child nodes have an embedding
    vector representing the child chunk of the text for retrieval purposes.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the same text as in chapter 2, which is a paper titled “Einstein’s
    Patents and Inventions” by Asis Kumar Chaudhuri ([https://arxiv.org/abs/1709.00666](https://arxiv.org/abs/1709.00666)).
    Additionally, when segmenting a document into smaller parts for processing, it
    is best to start by splitting the text based on structural elements like paragraphs
    or sections. This approach maintains the coherence and context of the content,
    as paragraphs or sections typically encapsulate complete ideas or topics. Therefore,
    we will start by splitting the PDF text into sections.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.4 Splitting the text into sections with a regular expression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `split_text_by_titles` function in listing 3.4 uses a regular expression
    to split the text by sections. The regular expression is based on the fact that
    sections in the text are organized as a numbered list, where each new section
    starts with a number and an optional character, followed by a dot and the section
    title. The output of the `split_ text_by_titles` function is nine sections. If
    you check the PDF, you will notice only four main sections. However, there are
    also four subsections (3A–3D) describing some of the patents, and if you count
    the introduction abstract as its own section, you get a total of nine sections.
  prefs: []
  type: TYPE_NORMAL
- en: Before continuing with the parent document retriever, you will count the number
    of tokens per section to better understand their length. You will use the `tiktoken`,
    a package developed by OpenAI, to count the number of tokens in a given text.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.5 Counting the number of tokens in sections
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Most sections have a relatively small size of up to 600 tokens, which fits most
    LLM context prompts. However, the third section has over 4,000 tokens, which could
    lead to token limit errors during LLM generation. Therefore, you must split the
    sections into parent documents, where each document has at most 2,000 characters.
    You will use the `chunk_text` from the previous chapter to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.6 Splitting sections into parent documents of max size of 2,000 characters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 3.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Use the `num_tokens_from_string` function to determine the token count of each
    parent document. The token count can help you decide about additional steps in
    the preprocessing. For instance, longer sections that exceed a reasonable token
    count should be split further. On the other hand, if some segments are exceptionally
    brief, consisting of 20 tokens or fewer, you should consider eliminating them
    entirely as they might not add any information value.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of splitting the child chunks and importing them in a subsequent step,
    you will perform the splitting and the import in a single step. Performing the
    two operations in a single step allows you to skip slightly more complex data
    structures storing intermediate results. Before importing the graph, you need
    to define the import Cypher statement. The Cypher statement to import the parent
    document structure is relatively straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.7 Cypher query used to import the parent document strategy graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Merges PDF node based on the id property'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Merges Parent node and set its text property'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Merges multiple Child nodes for each Parent node'
  prefs: []
  type: TYPE_NORMAL
- en: The Cypher statement in listing 3.7 starts by merging a `PDF` node. Next, it
    merges the `Parent` node using a unique ID. The `Parent` node is then linked to
    the `PDF` node through a `HAS_PARENT` relationship and has the `text` property
    set. Lastly, it iterates over a list of child documents. It creates a `Child`
    node for each element in the list, sets the text and embedding properties, and
    links it to its `Parent` node with a `HAS_CHILD` relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Now that everything is prepared, you can import the parent document structure
    into the graph database.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.8 Importing the parent document data into the graph database
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Splits the parent documents into child chunks'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calculates text embeddings for child chunks'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Imports into Neo4j'
  prefs: []
  type: TYPE_NORMAL
- en: The code in listing 3.8 starts by iterating over the parent document chunks.
    Each parent document chunk is divided into multiple child chunks using the `chunk_text`
    function. The code then calculates text embeddings for these child chunks with
    the `embed` function. Following the embedding generation, the `execute_query`
    method imports the data into a Neo4j graph database.
  prefs: []
  type: TYPE_NORMAL
- en: You can examine the generated graph structure by running the Cypher statement
    shown in the following listing in Neo4j Browser.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.9 Create a vector index on child nodes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The Cypher statement in listing 3.9 produces the graph shown in figure 3.5\.
    This graph visualization depicts a central PDF node connected to several parent
    nodes, illustrating the hierarchical relationship between the document and its
    sections. Each parent node is further linked to multiple child nodes, indicating
    the breakdown of sections into smaller chunks within the document structure.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure efficient comparison of document embeddings, you will add a vector
    index.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.10 Creating a vector index on child nodes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The code to generate the vector index in listing 3.10 is identical to the one
    used in chapter 2\. Here, you created a vector index on the `embedding` property
    of the `Child`.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 Graph visualization of part of the imported data in Neo4j Browser
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 3.2.1 Retrieving parent document strategy data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After importing the data and defining the vector index, you can focus on implementing
    the retrieval part. To retrieve relevant documents from the graph, you must define
    the retrieval Cypher statement described in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.11 Parent document retrieval Cypher statement
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Vector index search'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Traverses to parent documents'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Deduplicates parent documents'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Ensures final limit'
  prefs: []
  type: TYPE_NORMAL
- en: The Cypher statement in listing 3.11 starts by executing a vector-based search
    within a graph database to identify child nodes closely aligned with a specified
    question embedding. You can see that we retrieve `k` `*` `4` documents in the
    initial vector search. The reason for using the `k` `*` `4` value in the initial
    vector search is that you anticipate a scenario where multiple similar child nodes
    from the vector search may actually belong to the same parent document. Therefore,
    it becomes crucial to deduplicate the parent documents. Without deduplication,
    the result set could include multiple entries for the same parent document, each
    corresponding to a different child node of that parent. However, to guarantee
    a final count of `k` unique parent documents, you start with a larger pool of
    `k` `*` `4` child nodes, effectively creating a safety buffer. In the end of the
    Cypher statement, you enforce the final `k` limit.
  prefs: []
  type: TYPE_NORMAL
- en: The function that utilizes the Cypher statement in listing 3.11 to retrieve
    parent documents from the database is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.12 Parent document retrieval function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `parent_retrieval` function in listing 3.12 first generates a text embedding
    for a given question and then uses the previously mentioned Cypher statement to
    retrieve a list of the most relevant documents from the database.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Complete RAG pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last piece of the pipeline is the answer-generating function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.13 Generating answers with an LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The code in listing 3.13 is identical to that in chapter 2\. You pass the question
    along with the relevant documents to an LLM and prompt it to generate an answer.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing the step-back prompting and parent document retrieval, you
    are ready to bring it all together in a single function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 3.14 Complete parent document retriever with step-back prompting
    RAG pipeline**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `rag_pipeline` function in listing 3.14 takes a question as input and creates
    a step-back prompt. It then retrieves related documents based on the step-back
    prompt and passes them along with the original question to an LLM to generate
    the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: You can now test the `rag_pipeline` implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 3.15 Complete parent document retriever with step-back prompting
    RAG pipeline**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 3.3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Evaluate how well the `rag_pipeline` implementation performs by asking other
    questions about Einstein’s life mentioned in the PDF. Additionally, you can remove
    the step-back prompting step to compare if it improves the results.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully implemented an advanced vector search
    retrieval strategy by combining query rewriting and parent document retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Query rewriting can enhance the accuracy of document retrieval by aligning user
    queries more closely with the language and context of target documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques like hypothetical document retriever and step-back prompting effectively
    bridge the gap between the user’s intent and the document’s content, reducing
    the chances of missing relevant information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The effectiveness of a retrieval system can be improved by embedding not just
    the exact text but also contextually relevant summaries or paraphrases, capturing
    the essence of documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing strategies like hypothetical question embedding and parent document
    retrieval can lead to more precise matching between queries and documents, enhancing
    the relevance and accuracy of retrieved information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting documents into smaller, more manageable chunks for embedding purposes
    allows for a more granular approach to information retrieval, ensuring that specific
    queries find the most relevant document sections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
