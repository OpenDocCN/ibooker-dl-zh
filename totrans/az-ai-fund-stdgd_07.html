<html><head></head><body><section data-pdf-bookmark="Chapter 7. Features of Natural Language Processing Workloads on Azure" data-type="chapter" epub:type="chapter"><div class="chapter" id="i07_chapter7_features_of_natural_language_processing_workloads_1742068263375781">
<h1><span class="label">Chapter 7. </span>Features of Natural Language <span class="keep-together">Processing Workloads on Azure</span></h1>

<p>In <a contenteditable="false" data-primary="NLP (natural language processing)" data-type="indexterm" id="icd701"/>this<a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="categories of" data-tertiary="natural language processing" data-type="indexterm" id="icd1003"/> chapter, we’ll look into NLP, a key topic that makes up about 15%–20% of the AI-900 exam. We’ll kick things off with an exploration of core NLP scenarios. Then, we’ll get into Microsoft Azure services for NLP, starting with key phrase extraction and how entity recognition pulls important context from text. From there, we’ll walk through sentiment analysis, an essential tool for reading emotions in written language, and cover the basics of language modeling. You’ll also find an introduction to speech recognition and synthesis. This is where machines learn to understand and produce humanlike speech. Finally, we’ll wrap up with how to use NLP with conversational language understanding (CLU) and conversational AI.</p>

<section data-pdf-bookmark="Introduction to NLP" data-type="sect1"><div class="sect1" id="i07_chapter7_introduction_to_nlp_1742068263376016">
<h1>Introduction to NLP</h1>

<p>Imagine<a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="overview of" data-type="indexterm" id="id739"/> trying to get your computer to not just read but actually understand what you’re saying—that’s where NLP steps in. NLP is a part of AI that gives machines the power to interpret and generate human language. It’s about taking apart the complexities of text and speech and transforming them into something a computer can work with. This is what fuels everything from the search results you see on Google to chatbots to generative AI apps like ChatGPT<a contenteditable="false" data-primary="ChatGPT" data-type="indexterm" id="id740"/> that answer questions and those voice-activated assistants that talk back. By bridging the gap between human communication and computer logic, NLP makes technology feel more intuitive—almost like it’s listening.</p>

<p>NLP doesn’t work alone—it’s backed by two powerful <a contenteditable="false" data-primary="ML (machine learning)" data-secondary="NLP" data-type="indexterm" id="id741"/>AI systems: machine learning (ML) and deep learning (DL). ML gives NLP systems the models and algorithms they need to spot patterns and make predictions. DL <a contenteditable="false" data-primary="DL (deep learning)" data-secondary="NLP" data-type="indexterm" id="id742"/>takes this a step further, allowing NLP to tackle complex tasks like understanding the context of conversation or <span class="keep-together">gauging</span> sentiment. DL models mimic the brain’s structure, giving NLP systems a real boost in “thinking” more like us. And because ML and DL help NLP keep learning from new data, these systems keep getting better at handling language naturally.</p>

<p>You’ll see NLP in action in three main ways:</p>

<ul>
	<li>
	<p>Language processing</p>
	</li>
	<li>
	<p>Speech recognition</p>
	</li>
	<li>
	<p>Translation</p>
	</li>
</ul>

<p>In language processing, NLP can do things like<a contenteditable="false" data-primary="sentiment analysis" data-type="indexterm" id="id743"/> sentiment analysis, which is how it picks up on emotion in text, or entity recognition, where it identifies specific names or places within content. Then there’s <a contenteditable="false" data-primary="speech recognition" data-type="indexterm" id="id744"/>speech recognition and <a contenteditable="false" data-primary="speech synthesis" data-type="indexterm" id="id745"/>synthesis: those technologies that allow your virtual assistants to understand and respond when you speak. And, of course, NLP powers translation tools, breaking down language barriers so that people from different parts of the world can connect easily.</p>

<section data-pdf-bookmark="Tokenization" data-type="sect2"><div class="sect2" id="i07_chapter7_tokenization_1742068263376096">
<h2>Tokenization</h2>

<p><em>Tokenization</em> is<a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="tokenization" data-type="indexterm" id="icd702"/><a contenteditable="false" data-primary="tokenization" data-type="indexterm" id="icd711"/> about breaking down words and parts of words into numbers, which are known <a contenteditable="false" data-primary="tokens" data-type="indexterm" id="kab999"/>as <em>tokens</em>. This is the first step in the NLP process. It provides the framework for identifying the words or phrases that NLP models need to analyze, classify, or respond to.</p>

<p>Let’s take an example. Suppose we have this sentence:</p>

<blockquote>
<p>Time flies like an arrow, time flies fast.</p>
</blockquote>

<p>When we tokenize this sentence, each word (or token) is given an identifier. Because the words <em>time</em> and <em>flies</em> repeat, they get only one token ID even though they appear twice. Here’s how it breaks down:</p>

<ol>
	<li>
	<p>time</p>
	</li>
	<li>
	<p>flies</p>
	</li>
	<li>
	<p>like</p>
	</li>
	<li>
	<p>an</p>
	</li>
	<li>
	<p>arrow</p>
	</li>
	<li>
	<p>fast</p>
	</li>
</ol>

<p>So our sentence becomes a sequence of tokens: [1, 2, 3, 4, 5, 1, 2, 6]—that is, there are eight words but six tokens.</p>

<p>Without tokenization, a computer would see the sentence as one continuous string of characters, which isn’t useful for analysis. Breaking it down allows the NLP model to interpret each part separately and understand how they work together.</p>

<p>Here are some other considerations in tokenization:</p>

<dl>
	<dt>Text normalization</dt>
	<dd>
	<p>Before<a contenteditable="false" data-primary="text normalization" data-type="indexterm" id="id746"/> tokenizing, the text might be normalized. This usually means converting everything to lowercase and removing punctuation. In our example, normalization would turn “Time flies like an arrow, time flies fast” into “time flies like an arrow time flies fast.” Normalization helps simplify the processing, although sometimes specific details like capitalization or punctuation are necessary. For instance, “Dr. Johnson” and “dr” convey very different meanings in a medical context where “Dr.” indicates a title.</p>
	</dd>
	<dt>Stop-word removal</dt>
	<dd>
	<p>Words <a contenteditable="false" data-primary="stop-word removal" data-type="indexterm" id="id747"/>like <em>the</em>, <em>an</em>, or <em>and</em> are common in most sentences but don’t always add useful meaning. By removing these stop words, NLP systems can emphasize the core content. For our sentence, if we remove <em>like</em> and <em>an</em>, we’re left with “time flies arrow time flies fast,” which directs more attention to the essential words.</p>
	</dd>
	<dt>N-grams</dt>
	<dd>
	<p>Sometimes, <a contenteditable="false" data-primary="n-grams" data-type="indexterm" id="id748"/>instead of breaking a sentence down to single words, NLP models capture phrases (such as bigrams for two words or trigrams for three). For example, “time flies” could be treated as a bigram to capture a specific phrase instead of treating “time” and “flies” as separate tokens. This can preserve important context, as in the case of “New York City,” which, if split, might lose its meaning.</p>
	</dd>
	<dt>Stemming</dt>
	<dd>
	<p>To <a contenteditable="false" data-primary="stemming" data-type="indexterm" id="id749"/>make analysis clearer, similar words are often grouped together. For example, <em>flying</em>, <em>flew</em>, and <em>flies</em> could be treated as the root form <em>fly</em>. In our example, both instances of <em>flies</em> would link back to the base <em>fly</em> token, making it easier to analyze related concepts <a contenteditable="false" data-primary="tokens" data-startref="kab999" data-type="indexterm" id="id750"/>together.</p>
	</dd>
	<dt>Lemmatization</dt>
	<dd>
	<p>This<a contenteditable="false" data-primary="lemmatization" data-type="indexterm" id="id751"/> reduces words to their base form, or lemma, so that different versions of a word—such as <em>running</em>, <em>ran</em>, and <em>runs</em>—all map back to a single, consistent root form: <em>run</em>. Unlike simple stemming, which just chops off endings, lemmatization applies linguistic rules to ensure that the base form is meaningful and grammatically accurate. This process makes text analysis more precise by grouping related <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="tokenization" data-startref="icd702" data-type="indexterm" id="id752"/><a contenteditable="false" data-primary="tokenization" data-startref="icd711" data-type="indexterm" id="id753"/>words.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Frequency Analysis" data-type="sect2"><div class="sect2" id="i07_chapter7_frequency_analysis_1742068263376160">
<h2>Frequency Analysis</h2>

<p>Once<a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="frequency analysis" data-type="indexterm" id="id754"/><a contenteditable="false" data-primary="frequency analysis" data-type="indexterm" id="id755"/> you’ve tokenized the words, the next step is <em>frequency analysis: </em> examining how frequently each word appears. The most common words (ignoring basics like <em>a</em>, <em>the</em>, and similar words) can hint at the main theme of the text. Take a political speech about economic growth, for example: the most frequent words might include <em>growth</em>, <em>jobs</em>, <em>future</em>, and <em>economy</em>. If we look at word pairs (bigrams), a common pair might be “new jobs,” which points to a focus on employment and economic expansion.</p>

<p>Counting word occurrences, known<a contenteditable="false" data-primary="simple frequency analysis" data-type="indexterm" id="id756"/> as <em>simple frequency analysis</em>, works well for examining a single document. But when you’re working with multiple documents, you need a way to pinpoint the most relevant words in each one. That’s where <em>term frequency-inverse document frequency (TF-IDF)</em> comes in. <a contenteditable="false" data-primary="TF-IDF (term frequency-inverse document frequency)" data-type="indexterm" id="icd1015"/><a contenteditable="false" data-primary="term frequency-inverse document frequency (TF-IDF)" data-type="indexterm" id="icd1016"/>This method scores words by how often they appear in a document compared to the entire set of documents. Words that appear often in one document but are rare in the others stand out as especially relevant.</p>

<p>Here are some additional considerations in frequency analysis:</p>

<dl>
	<dt>Limitations of simple frequency analysis</dt>
	<dd>
	<p>While simple frequency or bigram analysis offers helpful initial insights, it can miss nuances or context. Frequently used words may not always indicate the main topic, especially if they’re general or if they depend on context. When analyzing complex or larger documents, more advanced methods like TF-IDF, latent Dirichlet allocation (LDA), <a contenteditable="false" data-primary="LDA (latent Dirichlet allocation)" data-type="indexterm" id="id757"/>or other topic-modeling techniques are often more insightful. <em>LDA</em> is a statistical model that identifies topics in a collection of documents by finding groups of words that frequently appear together. This helps to uncover underlying themes or subjects, making it easier to analyze and categorize large amounts of text.</p>
	</dd>
	<dt>Broader uses of TF-IDF</dt>
	<dd>
	<p>TF-IDF isn’t just for finding relevant words—it’s also used in similarity analysis to group related documents, making it a powerful tool for tasks like information retrieval and recommendation systems.</p>
	</dd>
	<dt>Advanced contextual models</dt>
	<dd>
	<p>For even more refined insights, techniques like<a contenteditable="false" data-primary="word2vec" data-type="indexterm" id="icd1027"/> word2vec, BERT (bidirectional encoder representations from transformers), <a contenteditable="false" data-primary="BERT (bidirectional encoder representations from transformers)" data-type="indexterm" id="id758"/>or other contextual embeddings consider each word’s meaning based on surrounding text, which brings greater clarity to themes across a document and improves tasks like summarization, sentiment analysis, and topic detection.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Text Classification" data-type="sect2"><div class="sect2" id="i07_chapter7_text_classification_1742068263376222">
<h2>Text Classification</h2>

<p>Another<a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="text classification" data-type="indexterm" id="id759"/><a contenteditable="false" data-primary="text classification" data-type="indexterm" id="id760"/> powerful way to analyze text is by using a classification algorithm, like logistic regression, to categorize it based on predefined labels, which is called <em>text classification</em>. A common application for this is sorting social media posts for sentiment analysis.</p>

<p>Let’s say you’re working with social media comments that are already labeled as either 0 (critical) or 1 (supportive):</p>

<ul>
	<li>
	<p>“This initiative is exactly what we need!” = 1</p>
	</li>
	<li>
	<p>“Totally disappointed with the results so far” = 0</p>
	</li>
	<li>
	<p>“Great job! Keep it up!” = 1</p>
	</li>
	<li>
	<p>“This approach misses the point entirely” = 0</p>
	</li>
</ul>

<p>With enough labeled examples, you can train a classification model that learns to distinguish between supportive and critical posts. Using the tokenized text as features and the sentiment label (0 or 1), the model starts to detect patterns. For instance, comments with words like <em>great</em>, <em>exactly</em>, or <em>job</em> tend to be supportive while words like <em>disappointed</em> or <em>misses</em> indicate criticism.</p>

<p>To make this work, you’ll need to convert words into numbers. Basic methods like the <a contenteditable="false" data-primary="bag-of-words model" data-type="indexterm" id="id761"/>bag-of-words model and <a contenteditable="false" data-primary="TF-IDF (term frequency-inverse document frequency)" data-startref="icd1015" data-type="indexterm" id="id762"/><a contenteditable="false" data-primary="term frequency-inverse document frequency (TF-IDF)" data-startref="icd1016" data-type="indexterm" id="id763"/>TF-IDF score word importance based on frequency while advanced embeddings like<a contenteditable="false" data-primary="word2vec " data-startref="icd1027" data-type="indexterm" id="id764"/> word2vec or GloVe <a contenteditable="false" data-primary="GloVe" data-type="indexterm" id="id765"/>capture meaning based on context. Logistic regression<a contenteditable="false" data-primary="logistic regression" data-type="indexterm" id="id766"/> is a good starting point, but other algorithms like <a contenteditable="false" data-primary="naive Bayes algorithm" data-type="indexterm" id="id767"/>naive Bayes and support vector machine (SVM)<a contenteditable="false" data-primary="SVM (support vector machine)" data-type="indexterm" id="id768"/>, DL models like recurrent neural networks (RNNs),<a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-type="indexterm" id="id769"/><a contenteditable="false" data-primary="RNNs (recurrent neural networks)" data-type="indexterm" id="id770"/> or transformers<a contenteditable="false" data-primary="transformers" data-secondary="text classification" data-type="indexterm" id="id771"/> can boost performance, especially for larger datasets or nuanced <span class="keep-together">content</span>.</p>

<p>To ensure your model is effective, evaluate it with metrics like accuracy, precision, recall, and the F1 score, which reveal how well the model generalizes new data. Just keep in mind that models can struggle with subtleties like sarcasm. For example, a comment like “Love the effort…the result is certainly something” could be misinterpreted as supportive.</p>

<p>Classification models have plenty of uses beyond sorting social media sentiment, such as tagging news articles by category (e.g., “politics,” “entertainment,” or “health”) or even spotting trending topics—showing just how versatile text classification can be.</p>
</div></section>

<section data-pdf-bookmark="Semantic Language Models" data-type="sect2"><div class="sect2" id="i07_chapter7_semantic_language_models_1742068263376283">
<h2>Semantic Language Models</h2>

<p>In <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="semantic language models" data-type="indexterm" id="icd703"/><a contenteditable="false" data-primary="semantics" data-secondary="natural language processing" data-type="indexterm" id="icd712"/>the world of NLP, models have come a long way, making it possible to capture the deep and subtle relationships between words. The secret sauce is<a contenteditable="false" data-primary="embeddings" data-type="indexterm" id="ibd7001"/> embeddings—essentially multidimensional number arrays that map each word, <a contenteditable="false" data-primary="tokens" data-type="indexterm" id="kab9991"/>or token, to a unique point in space.</p>

<p>Think of each element in an embedding as a coordinate in a multidimensional space. Each token—whether it’s “coffee,” “latte,” or “soccer”—finds its own spot in this space. Tokens that are related by meaning are closer to one another. Here’s a quick example in three dimensions:</p>

<ul>
	<li>
	<p>“coffee”: [9, 2, 3]</p>
	</li>
	<li>
	<p>“tea”: [9, 2, 2]</p>
	</li>
	<li>
	<p>“caffeine”: [8, 2, 3]</p>
	</li>
	<li>
	<p>“latte”: [9, 3, 3]</p>
	</li>
	<li>
	<p>“soccer”: [1, 7, 3]</p>
	</li>
</ul>

<p><a data-type="xref" href="#i07_chapter7_figure_1_1742068263357633">Figure 7-1</a> shows the plotting for these items.</p>

<figure><div class="figure" id="i07_chapter7_figure_1_1742068263357633"><img src="assets/aaif_0701.png"/>
<h6><span class="label">Figure 7-1. </span>The plotting for the embeddings</h6>
</div></figure>

<p>The plotting shows that “coffee,” caffeine,” and “latte” huddle close together while “soccer” sits off to the side, indicating less of a semantic connection. This layout helps us see patterns in how words relate to one another based on meaning.</p>

<p>Advanced models, called <em>semantic language models</em>, take these embeddings and add complexity. For example, BERT and GPT models use what’s called <em>contextual embeddings</em>. <a contenteditable="false" data-primary="contextual embeddings" data-type="indexterm" id="id772"/>This means a word like <em>bank</em> gets a different representation in “riverbank” versus “national bank,” helping models better grasp the intended meaning.</p>

<p>How would you visualize high-dimensional embeddings? You can use techniques like <a contenteditable="false" data-primary="principal component analysis (PCA)" data-type="indexterm" id="id773"/><a contenteditable="false" data-primary="PCA (principal component analysis)" data-type="indexterm" id="id774"/>principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE),<a contenteditable="false" data-primary="t-SNE (t-distributed stochastic neighbor embedding)" data-type="indexterm" id="id775"/> which simplify these multidimensional embeddings into a two- or three-dimensional view. This makes it much easier to see the relationships between tokens. Most <a contenteditable="false" data-primary="tokens" data-startref="kab9991" data-type="indexterm" id="id776"/>embeddings are initially trained on enormous datasets (think word2vec or GloVe), and they can later be fine-tuned for specialized areas, such as legal or medical content.</p>

<p>Learning from these embeddings often relies on <a contenteditable="false" data-primary="self-supervised learning" data-type="indexterm" id="id777"/>self-supervised learning, where models teach themselves by predicting missing or next words. This is helpful for building multilingual embeddings, where words with similar meanings across languages find similar spots in the embedding space.</p>

<p>But embeddings aren’t without issues. They can mirror biases present in training data, such as linking demographic groups to certain roles, which can result in skewed predictions. Correcting these biases through debiasing techniques is crucial for fair, <a contenteditable="false" data-primary="embeddings" data-startref="ibd7001" data-type="indexterm" id="id778"/>responsible <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="semantic language models" data-startref="icd703" data-type="indexterm" id="id779"/><a contenteditable="false" data-primary="semantics" data-secondary="natural language processing" data-startref="icd712" data-type="indexterm" id="id780"/>use.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Azure Services for NLP" data-type="sect1"><div class="sect1" id="i07_chapter7_azure_services_for_nlp_1742068263376375">
<h1>Azure Services for NLP</h1>

<p>Microsoft <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-type="indexterm" id="icd704"/>Azure offers three main services for NLP: Azure AI Language, Azure AI Translator, and Azure AI Speech. Let’s take a look at each.</p>

<p>First,<a contenteditable="false" data-primary="Azure AI Language" data-secondary="features of" data-type="indexterm" id="id781"/> <em>Azure AI Language</em> enables you to understand, analyze, and respond to text data through a wide range of preconfigured and customizable features. It also has tools that cater to different levels of complexity. <a data-type="xref" href="#i07_chapter7_table_1_1742068263363592">Table 7-1</a> describes the<a contenteditable="false" data-primary="protected health information (PHI) detection" data-type="indexterm" id="id782"/><a contenteditable="false" data-primary="PHI (protected health information) detection" data-type="indexterm" id="id783"/><a contenteditable="false" data-primary="sentiment analysis" data-type="indexterm" id="id784"/><a contenteditable="false" data-primary="opinion mining" data-type="indexterm" id="id785"/><a contenteditable="false" data-primary="summarization" data-type="indexterm" id="id786"/><a contenteditable="false" data-primary="key phrase extraction" data-type="indexterm" id="icd713"/><a contenteditable="false" data-primary="entity linking" data-type="indexterm" id="id787"/> features <a contenteditable="false" data-primary="PII (personally identifiable information) detection" data-type="indexterm" id="id788"/>of the<a contenteditable="false" data-primary="NER (named entity recognition)" data-type="indexterm" id="id789"/> <span class="keep-together">service</span>.</p>

<table class="border" id="i07_chapter7_table_1_1742068263363592">
	<caption><span class="label">Table 7-1. </span>Features of Azure AI Language</caption>
	<thead>
		<tr>
			<th>Feature</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Named entity recognition (NER)</p>
			</td>
			<td>
			<p>Identifies entities like names, dates, and locations in unstructured text, categorizing them into groups for easy organization</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>PII and PHI detection</p>
			</td>
			<td>
			<p>Detects, categorizes, and redacts sensitive information, including personally identifiable information (PII) and protected health information (PHI), in text</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Language detection</p>
			</td>
			<td>
			<p>Identifies the language of a text document and returns a code for the detected language, covering a wide range of languages and dialects</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Sentiment analysis and opinion mining</p>
			</td>
			<td>
			<p>Analyzes text to determine positive or negative sentiment, providing insight into customer feedback and public opinion</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Summarization</p>
			</td>
			<td>
			<p>Produces a concise summary of a document or transcription by extracting key sentences</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Key phrase extraction</p>
			</td>
			<td>
			<p>Identifies the main concepts in text, returning a list of key phrases to highlight major themes or topics</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Entity linking</p>
			</td>
			<td>
			<p>Disambiguates terms and entities in text to references like Wikipedia, providing additional context</p>
			</td>
		</tr>
	</tbody>
</table>

<p>The second service Azure<a contenteditable="false" data-primary="Azure AI Translator" data-type="indexterm" id="id790"/><a contenteditable="false" data-primary="Translator Service" data-type="indexterm" id="id791"/> offers is <em>Azure AI Translator</em>. This service enables applications to instantly translate text into multiple languages. <a data-type="xref" href="#i07_chapter7_table_2_1742068263363623">Table 7-2</a> describes the <a contenteditable="false" data-primary="text translation" data-type="indexterm" id="id792"/><a contenteditable="false" data-primary="document translation" data-type="indexterm" id="id793"/><a contenteditable="false" data-primary="custom translation models" data-type="indexterm" id="id794"/>features of this service.</p>

<table class="border" id="i07_chapter7_table_2_1742068263363623">
	<caption><span class="label">Table 7-2. </span>Features of Azure AI Translator</caption>
	<thead>
		<tr>
			<th>Feature</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Text translation</p>
			</td>
			<td>
			<p>Translates text instantly between a supported source and target languages, with options for creating custom dictionaries and managing translation exceptions</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Document translation</p>
			</td>
			<td>
			<p>Offers two modes: asynchronous batch translation for large sets of files, which retains the format and structure with Azure Blob Storage, and synchronous translation for individual documents, which preserves the structure without storage requirements</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Custom translator</p>
			</td>
			<td>
			<p>Allows customization of translation models for specific industry languages, terminology, and styles, including creating specialized dictionaries for tailored translations</p>
			</td>
		</tr>
	</tbody>
</table>

<p>The third <a contenteditable="false" data-primary="Azure AI Speech" data-secondary="features of" data-type="indexterm" id="id795"/>service, <em>Azure AI Speech</em>, is geared toward converting spoken language into text and vice versa. <a data-type="xref" href="#i07_chapter7_table_3_1742068263363647">Table 7-3</a> lists<a contenteditable="false" data-primary="speech-to-text" data-type="indexterm" id="id796"/><a contenteditable="false" data-primary="real-time speech-to-text" data-type="indexterm" id="id797"/><a contenteditable="false" data-primary="text-to-speech" data-type="indexterm" id="id798"/><a contenteditable="false" data-primary="fast transcription API" data-type="indexterm" id="id799"/> the features of this service.</p>

<table class="border" id="i07_chapter7_table_3_1742068263363647">
	<caption><span class="label">Table 7-3. </span>Features of Azure AI Speech</caption>
	<thead>
		<tr>
			<th>Feature</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Speech-to-text</p>
			</td>
			<td>
			<p>Transcribes audio to text from various sources, such as microphones and audio files, in real time or with batch processing</p>

			<p>Features include speaker diarization and automatic formatting for improved readability</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Real-time speech-to-text</p>
			</td>
			<td>
			<p>Provides instant transcription of live audio, which is ideal for applications needing immediate text, such as live-meeting captions, pronunciation assessments, and contact center support</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Text-to-speech</p>
			</td>
			<td>
			<p>Converts written text to lifelike, synthesized speech using neural voices, with customization options for pitch, pronunciation, and speed</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Fast transcription API</p>
			</td>
			<td>
			<p>Offers a quick, synchronous transcription option for prerecorded audio, delivering results with minimal delay</p>

			<p>Suitable for urgent tasks like video transcription and fast audio processing</p>

			<p>Available through the preview API</p>
			</td>
		</tr>
	</tbody>
</table>

<p>Next, we’ll take a look at these services and how they work with NLP applications.</p>

<section class="less_space pagebreak-before" data-pdf-bookmark="Key Phrase Extraction" data-type="sect2"><div class="sect2" id="i07_chapter7_key_phrase_extraction_1742068263376465">
<h2>Key Phrase Extraction</h2>

<p><em>Key phrase extraction</em> is <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-tertiary="key phrase extraction" data-type="indexterm" id="icd706"/>an<a contenteditable="false" data-primary="Azure AI Language" data-secondary="key phrase extraction" data-type="indexterm" id="id800"/> NLP technique that identifies the most relevant words or phrases within a text. This pinpoints the main topics or themes without needing to analyze the entire document. Key phrase extraction is a valuable tool in NLP because it enables systems to quickly grasp the core content of a text, which is essential in tasks like summarizing, content tagging, or indexing large documents.</p>

<p>Typical uses for key phrase extraction range across many fields. In customer service, for instance, key phrase extraction can help identify main concerns or frequently mentioned issues in customer feedback, making it easier for teams to address common problems. This technique also supports social media analysis by quickly identifying trending topics from user-generated content, such as tweets or posts, so brands can understand public sentiment or emerging discussions.</p>

<p>Consider a more detailed example: a company analyzing customer reviews for a new product. Using key phrase extraction, the model identifies common phrases like “easy to use,” “battery life,” and “customer support.” These extracted phrases help the company see what features customers talk about most without having to manually read each review. Additionally, negative phrases like “difficult setup” or “short battery life” allow the company to pinpoint areas needing improvement.</p>

<p>Let’s take a look at how you can use key phrase extraction using <a href="https://oreil.ly/8gTK0">Azure AI Language Studio</a>. Once you’re at the dashboard for the Language Studio, select “Extract information” and then click “Extract key phrases.” You’ll find different examples of text, such as for travel, medical reports, and banking. We’ll select banking. This is a message from a customer who requested a cancellation for a credit card because it was lost. Click Run. <a data-type="xref" href="#i07_chapter7_figure_2_1742068263357669">Figure 7-2</a> shows the results.</p>

<p>At the top are the key phrases that were extracted. They include details like the address, linked email account, Social Security number, and SWIFT code. Below is the original text with the extracted key phrases <a contenteditable="false" data-primary="key phrase extraction" data-startref="icd713" data-type="indexterm" id="id801"/>highlighted.</p>

<figure><div class="figure" id="i07_chapter7_figure_2_1742068263357669"><img src="assets/aaif_0702.png"/>
<h6><span class="label">Figure 7-2. </span>Key phrases extracted from a message about a credit card cancellation</h6>
</div></figure>
</div></section>

<section data-pdf-bookmark="Entity Recognition" data-type="sect2"><div class="sect2" id="i07_chapter7_entity_recognition_1742068263376525">
<h2>Entity Recognition</h2>

<p><em>Entity recognition</em>, often<a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-tertiary="entity recognition" data-type="indexterm" id="icd707"/><a contenteditable="false" data-primary="entity recognition" data-type="indexterm" id="icd714"/> called <em>named entity recognition (NER)</em>, <a contenteditable="false" data-primary="NER (named entity recognition)" data-type="indexterm" id="ibd7003"/>is <a contenteditable="false" data-primary="Azure AI Language" data-secondary="named entity recognition" data-type="indexterm" id="ibd7004"/>a process in AI where you teach a model to identify and classify elements, or entities, in a text. These entities could be names of people, places, organizations, dates, or even products—basically, anything meaningful and distinct in a body of text. Think of NER as a way to highlight or tag essential parts of information automatically. This makes it easier to analyze and categorize data without manual sorting.</p>

<p>A common use case for NER is for customer service chatbots to recognize a user’s name, location, or problem type. This allows the system to personalize the responses. In legal or financial sectors, NER can automatically extract contract dates, client names, or financial figures from documents, reducing the need for tedious human review. NER also enhances search engines, making search results more relevant by helping the engine understand the context behind keywords.</p>

<p>How does entity recognition actually work? It follows a few key steps. First, the model preprocesses the text by breaking it into smaller parts, like sentences or words. Then, it uses language algorithms to spot potential entities based on patterns. After that, it classifies these entities into categories, such as “person” or “organization.” Finally, the model refines its guesses based on training data and produces a list of recognized entities.</p>

<p>Here’s a quick example to bring this to life. Suppose we have this sentence: “John Doe from TechCorp called on October 10, 2024 about the quarterly report.” After running it through an NER model, we get the results shown in <a data-type="xref" href="#i07_chapter7_table_4_1742068263363668">Table 7-4</a>.</p>

<table class="border" id="i07_chapter7_table_4_1742068263363668">
	<caption><span class="label">Table 7-4. </span>Results of using NER</caption>
	<thead>
		<tr>
			<th>Entity</th>
			<th>Type</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>John Doe</p>
			</td>
			<td>
			<p>Person</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>TechCorp</p>
			</td>
			<td>
			<p>Organization</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>October 10, 2024</p>
			</td>
			<td>
			<p>Date</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Quarterly report</p>
			</td>
			<td>
			<p>Miscellaneous</p>
			</td>
		</tr>
	</tbody>
</table>

<p>When using Azure AI Foundry<a contenteditable="false" data-primary="Azure AI Foundry" data-secondary="entity recognition" data-type="indexterm" id="id802"/>, you can select “Extract named entities.” Then, select “Medical Report” and click Run. <a data-type="xref" href="#i07_chapter7_figure_3_1742068263357693">Figure 7-3</a> shows a list of the entities that were found as well as the original text that marks them. If you hover over one of these highlights, you will see more details, such as the confidence level.</p>

<p><em>Named entity linking (NEL)</em> takes <a contenteditable="false" data-primary="NEL (named entity linking)" data-type="indexterm" id="id803"/>NER a step further by not only identifying entities in text but also connecting them to unique, real-world references. Suppose you’re reading an article that mentions Paris. With entity linking, an AI model won’t just recognize Paris as a location; it will also understand whether the article refers to Paris, France, or Paris, Texas, depending on the context. Entity linking is like giving each entity a unique ID or link to a database. This mitigates confusion about what the term represents.</p>

<p>Entity linking comes in handy in a lot of fields. In news aggregation, it helps ensure that all references to a particular event or person, such as a CEO or celebrity, consistently point to the same profile, avoiding duplication. In health care, entity linking can automatically connect mentions of a medical condition to a knowledge base of treatments, symptoms, <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-startref="icd707" data-tertiary="entity recognition" data-type="indexterm" id="id804"/><a contenteditable="false" data-primary="entity recognition" data-startref="icd714" data-type="indexterm" id="id805"/>and <a contenteditable="false" data-primary="Azure AI Language" data-secondary="named entity recognition" data-startref="ibd7004" data-type="indexterm" id="id806"/>research<a contenteditable="false" data-primary="NER (named entity recognition)" data-startref="ibd7003" data-type="indexterm" id="id807"/> articles.</p>

<figure><div class="figure" id="i07_chapter7_figure_3_1742068263357693"><img src="assets/aaif_0703.png"/>
<h6><span class="label">Figure 7-3. </span>NER for a medical report</h6>
</div></figure>
</div></section>

<section data-pdf-bookmark="Sentiment Analysis" data-type="sect2"><div class="sect2" id="i07_chapter7_sentiment_analysis_1742068263376587">
<h2>Sentiment Analysis</h2>

<p>I <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-tertiary="sentiment analysis" data-type="indexterm" id="id808"/><a contenteditable="false" data-primary="sentiment analysis" data-type="indexterm" id="id809"/>mentioned sentiment analysis<a contenteditable="false" data-primary="Azure AI Language" data-secondary="sentiment analysis" data-type="indexterm" id="id810"/> earlier in this chapter, but let’s take a deeper look at it now. <em>Sentiment analysis</em> is like giving your AI model emotional intelligence. It allows the system to read between the lines and determine if a piece of text is positive, negative, or neutral. Essentially, sentiment analysis works by analyzing the words and phrases in a text to figure out the mood or feeling behind them. So whether you’re analyzing customer reviews, social media comments, or survey responses, sentiment analysis can help you understand how people feel about a product, service, or topic without needing a human to go through everything manually.</p>

<p>In customer service, sentiment analysis can flag negative comments or complaints so that agents can respond more quickly. In marketing, it helps track brand reputation over time by monitoring sentiment on social media or review sites. It’s also valuable in product development, where analyzing feedback can reveal trends in customer satisfaction or highlight recurring issues that need fixing.</p>

<p><a data-type="xref" href="#i07_chapter7_table_5_1742068263363690">Table 7-5</a> shows an example of sentiment analysis—specifically, an analysis of the following review: “The new phone’s camera is fantastic, but the battery life is <span class="keep-together">disappointing</span>.”</p>

<table class="border" id="i07_chapter7_table_5_1742068263363690">
	<caption><span class="label">Table 7-5. </span>Sentiment analysis of a product review</caption>
	<thead>
		<tr>
			<th>Phrase</th>
			<th>Sentiment</th>
			<th>Score</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>new phone’s camera</p>
			</td>
			<td>
			<p>Positive</p>
			</td>
			<td>
			<p>+2</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>fantastic</p>
			</td>
			<td>
			<p>Positive</p>
			</td>
			<td>
			<p>+3</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>battery life</p>
			</td>
			<td>
			<p>Neutral</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>disappointing</p>
			</td>
			<td>
			<p>Negative</p>
			</td>
			<td>
			<p>-3</p>
			</td>
		</tr>
	</tbody>
</table>

<p>The AI would then sum up these scores to produce an overall sentiment score for the review. Here, the score would likely lean toward neutral or slightly positive since the enthusiasm for the camera is balanced by disappointment with the battery.</p>
</div></section>

<section data-pdf-bookmark="Language Detection" data-type="sect2"><div class="sect2" id="i07_chapter7_language_detection_1742068263376649">
<h2>Language Detection</h2>

<p><em>Language detection</em> is<a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-tertiary="language detection" data-type="indexterm" id="id811"/><a contenteditable="false" data-primary="language detection" data-type="indexterm" id="id812"/> a feature of the<a contenteditable="false" data-primary="Azure AI Language" data-secondary="language detection" data-type="indexterm" id="id813"/> Azure AI Language service that identifies the language of a given text. It supports more than one hundred languages. This service allows multiple documents to be processed at once. Results include:</p>

<ul>
	<li>
	<p>The language name</p>
	</li>
	<li>
	<p>The ISO 639-1 language code (a two-letter code like “fr” that is part of an international standard for language representation)</p>
	</li>
	<li>
	<p>A confidence score</p>
	</li>
</ul>

<p>To illustrate, let’s say you operate a travel forum where users post feedback about destinations worldwide. Here are three examples of feedback you receive:</p>

<ul>
	<li>
	<p>Feedback 1: “An amazing spot for birdwatching and hiking.”</p>
	</li>
	<li>
	<p>Feedback 2: “Le personnel était très accueillant et serviable.”</p>
	</li>
	<li>
	<p>Feedback 3: “Il posto perfetto per rilassarsi con la famiglia.”</p>
	</li>
</ul>

<p><a data-type="xref" href="#i07_chapter7_table_6_1742068263363710">Table 7-6</a> shows the results that Azure AI Language provides.</p>

<table class="border" id="i07_chapter7_table_6_1742068263363710">
	<caption><span class="label">Table 7-6. </span>Language detection for a message forum</caption>
	<thead>
		<tr>
			<th>Document</th>
			<th>Language</th>
			<th>ISO 639-1 code</th>
			<th>Confidence score</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Feedback 1</p>
			</td>
			<td>
			<p>English</p>
			</td>
			<td>
			<p>en</p>
			</td>
			<td>
			<p>0.8</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Feedback 2</p>
			</td>
			<td>
			<p>French</p>
			</td>
			<td>
			<p>fr</p>
			</td>
			<td>
			<p>1.0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Feedback 3</p>
			</td>
			<td>
			<p>Italian</p>
			</td>
			<td>
			<p>it</p>
			</td>
			<td>
			<p>0.9</p>
			</td>
		</tr>
	</tbody>
</table>

<p>When a comment includes a mix of languages, Azure focuses on the predominant one. This provides a single language label with a confidence score that might be slightly lower if multiple languages are detected. In cases where the content is minimal or ambiguous, such as with a simple emoji or symbol, Azure might label the <span class="keep-together">language</span> as “unknown” and leave the language identifier empty, with a “NaN” (not a number) score to indicate that it couldn’t confidently determine a language.</p>
</div></section>

<section data-pdf-bookmark="Speech Recognition and Synthesis" data-type="sect2"><div class="sect2" id="i07_chapter7_speech_recognition_and_synthesis_1742068263376714">
<h2>Speech Recognition and Synthesis</h2>

<p>When it comes to AI and speech, there are two key skills that make it all work: speech recognition and speech synthesis. <em>Speech recognition</em> is<a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-tertiary="speech recognition" data-type="indexterm" id="icd708"/> what allows AI to listen to spoken input and understand what’s being said. <em>Speech synthesis</em>, on<a contenteditable="false" data-primary="speech synthesis" data-type="indexterm" id="id814"/> the other hand, is how AI generates spoken output. This essentially gives it the ability to “talk” back to you. Together, these capabilities make it possible for AI to engage in a natural, back-and-forth conversation, creating more interactive and personal experiences.</p>

<section data-pdf-bookmark="Speech recognition" data-type="sect3"><div class="sect3" id="i07_chapter7_speech_recognition_1742068263376775">
<h3>Speech recognition</h3>

<p>Speech recognition technology powers many day-to-day conveniences:</p>

<dl>
	<dt>Meeting transcription</dt>
	<dd>
	<p>AI <a contenteditable="false" data-primary="meeting transcriptions" data-type="indexterm" id="id815"/>can create a full transcript of your Zoom call or meeting, so you can stay focused without taking notes.</p>
	</dd>
	<dt>Real-time captions</dt>
	<dd>
	<p>In<a contenteditable="false" data-primary="real-time captions" data-type="indexterm" id="id816"/> a livestream, captions make it so you do not miss any details.</p>
	</dd>
	<dt>Voice-activated customer service</dt>
	<dd>
	<p>You can talk<a contenteditable="false" data-primary="voice-activated customer service" data-type="indexterm" id="id817"/> to the system and not have to navigate menu options. The AI will understand and connect you to what you are looking for.</p>
	</dd>
	<dt>Dictation</dt>
	<dd>
	<p>Speak<a contenteditable="false" data-primary="dictation" data-type="indexterm" id="id818"/> freely, and AI transcribes your words into notes, which is ideal for capturing ideas quickly or managing daily tasks hands-free.</p>
	</dd>
</dl>

<p>How does speech recognition work? It begins with capturing your audio input, then moves to feature extraction, where the AI isolates and analyzes sound features. An acoustic model translates these features into phonemes, then a language model maps phonemes to words using statistical probabilities. Finally, a decoding process refines the output. This creates clear and accurate text from spoken language.</p>

<p>To see this in <a contenteditable="false" data-primary="Azure AI Speech" data-secondary="speech recognition" data-type="indexterm" id="id819"/>Azure, you can go to the <a href="https://oreil.ly/zlmtI">Speech Studio</a> and select “Real-time speech to text.” Select a language and then either upload an audio file or create one. Speak into your computer’s microphone, and the system will convert your words into text as you speak. It will even create a <em>.wav </em>audio file that you can download. You can see this in <a data-type="xref" href="#i07_chapter7_figure_4_1742068263357714">Figure 7-4</a>.</p>

<figure><div class="figure" id="i07_chapter7_figure_4_1742068263357714"><img src="assets/aaif_0704.png"/>
<h6><span class="label">Figure 7-4. </span>The speech-to-text capabilities of Azure</h6>
</div></figure>

<p>When you look at the JSON, you’ll see a detailed description of the output, including the offsets and duration for each word. Here’s a <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-startref="icd708" data-tertiary="speech recognition" data-type="indexterm" id="id820"/>snippet:</p>

<pre data-type="programlisting">
            {
                “Confidence”: 0.6879325,
                “Lexical”: “i’m studying the AI nine hundred exam”,
                “ITN”: “i’m studying the AI 900 exam”,
                “MaskedITN”: “i’m studying the ai 900 exam”,
                “Display”: “I’m studying the AI 900 exam.”,
                “Words”: [
                    {
                        “Word”: “i’m”,
                        “Offset”: 10800000,
                        “Duration”: 2400000
                    },
                    {
                        “Word”: “studying”,
                        “Offset”: 13200000,
                        “Duration”: 4000000
                    },</pre>
</div></section>

<section data-pdf-bookmark="Speech synthesis" data-type="sect3"><div class="sect3" id="i07_chapter7_speech_synthesis_1742068263376835">
<h3>Speech synthesis</h3>

<p>Under <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-tertiary="speech synthesis" data-type="indexterm" id="id821"/><a contenteditable="false" data-primary="speech synthesis" data-type="indexterm" id="id822"/>the hood, <a contenteditable="false" data-primary="Azure AI Speech" data-secondary="speech synthesis" data-type="indexterm" id="id823"/>speech synthesis relies on a few essential steps. It starts with text analysis, breaking down the text’s structure and meaning. Then prosody and acoustic modeling add tone, pitch, and emphasis, giving the speech a natural cadence. Finally, waveform generation turns all of this into audio, creating a clear, humanlike voice. With these capabilities, speech synthesis provides a more engaging, interactive experience that feels like a true conversation.</p>

<p class="pagebreak-before">Here are a few of the use cases for speech synthesis:</p>

<dl>
	<dt>Hands-free reading</dt>
	<dd>
	<p>Speech <a contenteditable="false" data-primary="hands-free reading" data-type="indexterm" id="id824"/>synthesis reads articles, messages, or notes aloud, keeping you engaged while you’re multitasking.</p>
	</dd>
	<dt>Step-by-step guidance</dt>
	<dd>
	<p>When <a contenteditable="false" data-primary="step-by-step vocal instructions" data-type="indexterm" id="id825"/>you are following a recipe or workout, for instance, an app can read the instructions aloud, so you can stay focused without a screen.</p>
	</dd>
	<dt>Virtual assistant responses</dt>
	<dd>
	<p>A<a contenteditable="false" data-primary="virtual assistants" data-type="indexterm" id="id826"/> friendly voice responds to your questions, whether you’re checking the weather forecast or asking for directions.</p>
	</dd>
	<dt>Public announcements</dt>
	<dd>
	<p>Speech<a contenteditable="false" data-primary="public announcements" data-type="indexterm" id="id827"/> synthesis makes it easy to broadcast important information, creating clear and easily understood public messages.</p>
	</dd>
	<dt>GPS navigation</dt>
	<dd>
	<p>A GPS<a contenteditable="false" data-primary="GPS navigation" data-type="indexterm" id="id828"/> system guides you turn by turn, reading out directions so that you can stay focused on the road.</p>
	</dd>
</dl>
</div></section>
</div></section>

<section data-pdf-bookmark="Translation" data-type="sect2"><div class="sect2" id="i07_chapter7_translation_1742068263376892">
<h2>Translation</h2>

<p>When<a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-tertiary="machine translation" data-type="indexterm" id="id829"/><a contenteditable="false" data-primary="machine translation" data-type="indexterm" id="id830"/> we are working across cultures and languages, breaking down language barriers becomes essential. Sure, hiring multilingual individuals can help, but with the sheer number of languages and their combinations, scaling this approach quickly becomes a challenge. Enter <em>machine translation</em>—automated systems designed to bridge linguistic gaps.</p>

<p>Machine translation provides a scalable solution to language barriers, yet it’s not as simple as replacing one word with another. Words alone don’t carry the full weight of meaning. Tone, context, and intent have to make the jump, too.</p>

<p>To meet this challenge, machine translation technology needs to go beyond just understanding individual words. It has to grasp the full picture, considering factors like context, informal or formal tone, slang, and unique grammar rules. Only then can it provide translations that feel natural and stay true to the original intent.</p>

<p>You’ve probably encountered text translation in everyday tools: from translating a government document to clicking the Translate button on social media posts. Then there’s speech translation, which allows spoken language to be translated directly from one language to another. Whether it’s converting spoken words to text before translation or delivering a voice-to-voice translation, this technology is making real-time, cross-language conversations easier.</p>

<p>Machine translation relies on advanced algorithms and neural networks. These networks process enormous amounts of multilingual text data to understand linguistic patterns, structures, and meanings across languages. By mapping out how words relate to one another in various languages, the system learns to create more accurate translations.</p>

<p>The beauty of machine translation lies in its continuous improvement. As these systems interact with users and process new language data, they refine their accuracy over time. This iterative learning helps AI stay up-to-date with evolving language trends and cultural nuances.</p>
</div></section>

<section data-pdf-bookmark="Conversational Language Understanding" data-type="sect2"><div class="sect2" id="i07_chapter7_conversational_language_understanding_1742068263376956">
<h2>Conversational Language Understanding</h2>

<p><em>Conversational language understanding (CLU)</em> makes<a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-tertiary="CLU" data-type="indexterm" id="icd709"/><a contenteditable="false" data-primary="CLU (conversational language understanding)" data-type="indexterm" id="icd716"/> it <a contenteditable="false" data-primary="conversational language understanding (CLU)" data-type="indexterm" id="icd1022"/>possible for you to create language models that understand and respond to everyday conversational phrases. Suppose you’re telling a virtual assistant, “Dim the kitchen lights.” With CLU, the assistant not only understands the command but also knows exactly which lights you mean, so you get precisely the response you’re looking for. This tool is particularly useful for applications that involve command and control, customer support, or large-scale enterprise solutions.</p>

<p>To build an effective model, you’ll work with three essential elements:</p>

<dl>
	<dt>Utterances</dt>
	<dd>
	<p>Examples<a contenteditable="false" data-primary="utterances, in CLU" data-type="indexterm" id="id831"/> of things users might say, such as “Lower the blinds in the living room.”</p>
	</dd>
	<dt>Entities</dt>
	<dd>
	<p>Specific items<a contenteditable="false" data-primary="entities, in CLU" data-type="indexterm" id="id832"/> in an utterance. In “Start the coffee maker,” “coffee maker” is the entity—it tells the system what to act on.</p>
	</dd>
	<dt>Intents</dt>
	<dd>
	<p>The purpose<a contenteditable="false" data-primary="intents, in CLU" data-type="indexterm" id="id833"/> behind an utterance. For “Start the coffee maker,” the intent might be “PowerOn,” indicating that the user wants to turn the device on.</p>
	</dd>
</dl>

<p>Putting it all together, these components guide the model to recognize different actions. <a data-type="xref" href="#i07_chapter7_table_7_1742068263363729">Table 7-7</a> is a snapshot of how these might look.</p>

<table class="border" id="i07_chapter7_table_7_1742068263363729">
	<caption><span class="label">Table 7-7. </span>CLU elements</caption>
	<thead>
		<tr>
			<th>Intent</th>
			<th>Sample utterances</th>
			<th>Entities</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Farewell</p>
			</td>
			<td>
			<p>“See you later,” “goodbye”</p>
			</td>
			<td>
			<p>N/A</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>PowerOn</p>
			</td>
			<td>
			<p>“Turn on the heater”</p>
			</td>
			<td>
			<p>Heater (device)</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>PowerOff</p>
			</td>
			<td>
			<p>“Shut off the TV”</p>
			</td>
			<td>
			<p>TV (device)</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>CheckNews</p>
			</td>
			<td>
			<p>“Give me the latest news”</p>
			</td>
			<td>
			<p>N/A</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>None</p>
			</td>
			<td>
			<p>“Why is the sky blue?”</p>
			</td>
			<td>
			<p>N/A</p>
			</td>
		</tr>
	</tbody>
</table>

<p>The “None” intent acts as a catchall for any input that doesn’t fit a defined intent, so your model can gracefully handle unexpected questions or irrelevant statements.</p>

<p>Getting a CLU model to work well involves a few steps:</p>

<dl>
	<dt>Define your schema</dt>
	<dd>
	<p>Think about what your model needs to know. Identify the intents for user actions and the specific entities you want to capture.</p>
	</dd>
	<dt>Label your data</dt>
	<dd>
	<p>Properly label each utterance with the relevant intents and entities—this step is crucial for accurate training.</p>
	</dd>
	<dt>Train your model</dt>
	<dd>
	<p>Your model learns from labeled examples, gradually improving its ability to recognize patterns and predict outcomes.</p>
	</dd>
	<dt>Evaluate its performance</dt>
	<dd>
	<p>Test the model to see how accurately it identifies intents and entities with new data.</p>
	</dd>
	<dt>Refine and retrain</dt>
	<dd>
	<p>Based on your model’s test performance, you may need to adjust and retrain to boost accuracy.</p>
	</dd>
	<dt>Deploy the model</dt>
	<dd>
	<p>When it’s ready, deploy the model so that you can use it to interpret real-world user inputs via the Runtime API.</p>
	</dd>
</dl>

<p>Here are some popular ways you could use CLU:</p>

<dl>
	<dt>Enterprise bots</dt>
	<dd>
	<p>Within<a contenteditable="false" data-primary="enterprise bots" data-type="indexterm" id="id834"/> large companies, CLU-based bots can streamline tasks like finding HR resources, answering FAQs, or helping with scheduling by coordinating with multiple services.</p>
	</dd>
	<dt>Health care virtual assistants</dt>
	<dd>
	<p>A virtual assistant <a contenteditable="false" data-primary="virtual assistants" data-type="indexterm" id="id835"/>could assist patients with scheduling appointments, provide medication reminders, or even help triage symptoms. It could automate tasks for health care staff, such as patient check-ins or answering frequently asked questions about health services.</p>
	</dd>
	<dt>Ecommerce recommendations</dt>
	<dd>
	<p>In<a contenteditable="false" data-primary="ecommerce recommendations" data-type="indexterm" id="id836"/> an online shopping app, CLU can help a virtual assistant understand natural language shopping requests like “Show me sports jackets under $100” or “I need a gift for a 10-year-old.” The assistant could then retrieve relevant items, making the shopping <a contenteditable="false" data-primary="conversational language understanding (CLU)" data-startref="icd1022" data-type="indexterm" id="id837"/>experience <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-startref="icd709" data-tertiary="CLU" data-type="indexterm" id="id838"/><a contenteditable="false" data-primary="CLU (conversational language understanding)" data-startref="icd716" data-type="indexterm" id="id839"/>smoother.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Conversational AI" data-type="sect2"><div class="sect2" id="i07_chapter7_conversational_ai_1742068263377035">
<h2>Conversational AI</h2>

<p>Imagine<a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-tertiary="conversational AI" data-type="indexterm" id="icd710"/><a contenteditable="false" data-primary="conversational AI" data-type="indexterm" id="icd717"/> this: you’re a customer, and it’s midnight. You’ve got a question about a product, and you want a quick answer—without waiting for a support team or diving into lengthy documentation. That’s where conversational AI steps in, ready to assist you at any hour, across multiple platforms, from web chat to social media.</p>

<p>With <em>conversational AI</em>—specifically through “bots”—companies can keep up with customer demands for fast, personal responses. These bots are designed to answer questions, troubleshoot issues, and guide you, all in natural, friendly language. And when they’re powered by tools like Azure AI Language’s <a contenteditable="false" data-primary="Azure AI Language" data-secondary="question-answer feature" data-type="indexterm" id="id840"/>question-answering feature, <a contenteditable="false" data-primary="question-answering feature, in conversational AI" data-type="indexterm" id="id841"/>they can go a step further. This feature allows bots to provide real-time answers to common questions and handle multipart conversations naturally, passing you to a human only if the question is more complex.</p>

<p>An important element behind a bot’s effectiveness is its integration with a relevant knowledge base filled with question-and-answer pairs. A well-informed bot, connected to a company’s repository of Q&amp;A content, can respond with accurate, up-to-date information that reflects the latest products, services, and FAQs. This setup ensures that customers receive reliable answers and reduces the risk of customers being misled by outdated or irrelevant information.</p>

<p>Let’s check out an example. Go to the <a href="https://oreil.ly/Sd_hv">Azure AI Language Studio</a> and<a contenteditable="false" data-primary="Azure AI Language" data-secondary="conversational AI" data-type="indexterm" id="id842"/> create an Azure search resource: Click “Create new” and select “Custom question answering.”</p>

<p>You will then go through a series of screens for the configuration. First is “Choose language setting.” Here, select English and click Next. You’ll be taken to the “Enter basic information” section. Enter a name for the project and a description, then select “No answer found” for the default when the AI cannot find an answer to a user question. Press Next and then select “Create Project.”</p>

<p><a data-type="xref" href="#i07_chapter7_figure_5_1742068263357735">Figure 7-5</a> shows the dashboard.</p>

<p>On the top left of the screen, click “Add source,” and you’ll see some menu options. This is where you can add a knowledge base, whether through a URL or file. We’ll select the option for URL and use this <a href="https://oreil.ly/vd6nx">one</a>. This is Microsoft Azure’s FAQ for conversational language understanding (you can also add more than one URL). Click the customized URL, and you’ll see the screen in <a data-type="xref" href="#i07_chapter7_figure_6_1742068263357756">Figure 7-6</a>.</p>

<figure><div class="figure" id="i07_chapter7_figure_5_1742068263357735"><img src="assets/aaif_0705.png"/>
<h6><span class="label">Figure 7-5. </span>The dashboard for creating a conversational AI system for a knowledge base</h6>
</div></figure>

<figure><div class="figure" id="i07_chapter7_figure_6_1742068263357756"><img src="assets/aaif_0706.png"/>
<h6><span class="label">Figure 7-6. </span>The screen for the knowledge base that Azure AI created</h6>
</div></figure>

<p>On the left side of the screen, you can see how Azure AI Language has parsed the FAQ into question-answer pairs. If you click one of them, you get different options to customize a question:</p>

<ul>
	<li>
	<p>Edit the answer.</p>
	</li>
	<li>
	<p>Add alternate phrasings for questions when there are multiple ways users might ask the same thing. These alternate questions should be as different as possible in wording while keeping the meaning intact, and the list should be limited to a maximum of 10 variations.</p>
	</li>
	<li>
	<p>Incorporate follow-up prompts to link question-and-answer pairs in multiturn conversations. This linkage allows the client application to deliver a primary answer while offering additional questions for the user to choose from if needed. To see all the connections for a specific question-and-answer pair, select “view context tree.”</p>
	</li>
	<li>
	<p>Assign metadata tags to help the client application refine the results of a user query. For instance, a question like “What are the store hours?” might yield different responses depending on the specific store location—such as if the metadata is “Location: New York” versus “Location: Los Angeles.” By using metadata tags, the application can deliver answers tailored to the user’s specific needs.</p>
	</li>
</ul>

<p>You can test this by clicking the flask icon on the top left of the screen. <a data-type="xref" href="#i07_chapter7_figure_7_1742068263357777">Figure 7-7</a> shows the chatbot for this.</p>

<figure><div class="figure" id="i07_chapter7_figure_7_1742068263357777"><img src="assets/aaif_0707.png"/>
<h6><span class="label">Figure 7-7. </span>The chatbot that tests a knowledge base AI system</h6>
</div></figure>

<p>As you can see, I asked, “Is there any SDK support?” And I got the correct answer, with clickable links.</p>

<p>If you click Inspect, you will get an analysis for how the AI came up with the answer. There will also be a <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-startref="icd704" data-type="indexterm" id="id843"/>​confidence <a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="Azure services for" data-startref="icd710" data-tertiary="conversational AI" data-type="indexterm" id="id844"/><a contenteditable="false" data-primary="conversational AI" data-startref="icd717" data-type="indexterm" id="id845"/>score.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="i07_chapter7_conclusion_1742068263377092">
<h1>Conclusion</h1>

<p>We’ve explored key topics like NLP, tokenization, frequency analysis, and other foundational concepts that form the backbone of AI understanding. Each of these areas is essential not only for understanding how AI works but also for navigating the specific questions you’ll encounter on the <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="categories of" data-startref="icd1003" data-tertiary="natural language processing" data-type="indexterm" id="id846"/>AI-900<a contenteditable="false" data-primary="NLP (natural language processing)" data-startref="icd701" data-type="indexterm" id="id847"/> exam.</p>
</div></section>
<section class="less_space pagebreak-before" data-pdf-bookmark="Quiz" data-type="sect1"><div class="sect1" id="chapter7quiz">
	<h1>Quiz</h1>
	<p>To check your answers, please refer to the <a data-type="xref" href="app02.html#answers_chapter_7_sample_questions_1745932457451921">“Chapter 7 Answer Key”</a>.</p>
	<ol>
		<li>
		  <p>Which of the following Azure AI Services enables instant translation of text into multiple languages?</p>
		  <ol type="a">
			<li>
			  <p>Azure AI Language </p>
			</li>
			<li>
			  <p>Azure AI Speech </p>
			</li>
			<li>
			  <p>Azure AI Translator </p>
			</li>
			<li>
			  <p>Azure AI Sentiment </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>What is the primary function of named entity recognition (NER) in NLP?</p>
		  <ol type="a">
			<li>
			  <p>To analyze sentiment </p>
			</li>
			<li>
			  <p>To detect language </p>
			</li>
			<li>
			  <p>To identify and categorize entities </p>
			</li>
			<li>
			  <p>To extract key phrases </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>Which feature of Azure AI Speech allows for real-time transcription of live audio?</p>
		  <ol type="a">
			<li>
			  <p>Text-to-speech </p>
			</li>
			<li>
			  <p>Summarization </p>
			</li>
			<li>
			  <p>Real-time speech-to-text </p>
			</li>
			<li>
			  <p>Custom Translator </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>What is tokenization in the context of NLP?</p>
		  <ol type="a">
			<li>
			  <p>Assigning a unique identifier to each entity </p>
			</li>
			<li>
			  <p>Converting text to speech </p>
			</li>
			<li>
			  <p>Breaking text into individual words or phrases </p>
			</li>
			<li>
			  <p>Detecting language in text </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>Which of the following best describes Azure’s key phrase extraction feature?</p>
		  <ol type="a">
			<li>
			  <p>It categorizes entities in text.</p>
			</li>
			<li>
			  <p>It highlights main concepts or themes. </p>
			</li>
			<li>
			  <p>It detects the language of a document.</p>
			</li>
			<li>
			  <p>It analyzes sentiment in text.</p>
			</li>
		  </ol>
		</li>
		<li class="less_space pagebreak-before">
		  <p>Which NLP feature in Azure would be most suitable for identifying sensitive information like Social Security numbers?</p>
		  <ol type="a">
			<li>
			  <p>Language detection </p>
			</li>
			<li>
			  <p>Key phrase extraction </p>
			</li>
			<li>
			  <p>PII detection </p>
			</li>
			<li>
			  <p>Sentiment analysis </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>Which term refers to removing common words that don’t add meaning, like <em>the</em> and <em>an</em>, during NLP processing?</p>
		  <ol type="a">
			<li>
			  <p>Lemmatization </p>
			</li>
			<li>
			  <p>Stop-word removal </p>
			</li>
			<li>
			  <p>Tokenization </p>
			</li>
			<li>
			  <p>Frequency analysis </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>In the Azure AI Language Studio, which feature would you use to automatically link an entity like “Paris” to a specific reference?</p>
		  <ol type="a">
			<li>
			  <p>Entity recognition </p>
			</li>
			<li>
			  <p>Entity linking </p>
			</li>
			<li>
			  <p>Language detection </p>
			</li>
			<li>
			  <p>Summarization </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>What is the purpose of the Fast Transcription API in Azure AI Speech?</p>
		  <ol type="a">
			<li>
			  <p>To translate text </p>
			</li>
			<li>
			  <p>To provide quick, synchronous transcription of audio </p>
			</li>
			<li>
			  <p>To detect sentiment in audio </p>
			</li>
			<li>
			  <p>To convert text to lifelike speech </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>Which Azure feature can be used to summarize large volumes of text by extracting key sentences?</p>
		  <ol type="a">
			<li>
			  <p>Text-to-speech </p>
			</li>
			<li>
			  <p>Summarization </p>
			</li>
			<li>
			  <p>Language detection </p>
			</li>
			<li>
			  <p>Entity recognition </p>
			</li>
		  </ol>
		</li>
	  </ol>
	
	</div></section>
</div></section></body></html>