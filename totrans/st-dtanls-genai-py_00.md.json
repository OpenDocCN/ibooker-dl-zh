["```py\nfrom openai import OpenAI\nprompt_text = \"\"\"Generate a list of 20 keywords indicating positive \n↪sentiment to be used for searching customer reviews in \n↪Portuguese.\"\"\"\nclient = OpenAI(\n    api_key=<<your-API-key-here>>,\n)\nchat_completion = client.chat.completions.create(\n    messages=[\n    {\"role\": \"system\", \"content\": \"You are a scientific assistant, skilled\n    ↪at explaining science to schoolchildren.\"},\n    {\"role\": \"user\", \"content\": \"Explain resonance to first-graders?\"},\n    {\"role\": \"assistant\", \"content\": \"Alright, kids, today we're going to\n    ↪talk about something really cool called resonance! (...).\"},\n    *# Shortened from the full model response*\n    {\"role\": \"user\", \"content\": \"Where can we use it?\"}\n  ],\n\n    model=\"gpt-4-0125-preview\",\n    temperature=0.7,   \n)\nprint(chat_completion.choices[0].message.content)\n```", "```py\nclient = OpenAI(\napi_key = \"<your_llamaapi_token>\",\nbase_url = \"https://api.llama-api.com\"\n)\n```", "```py\npip install \"google-cloud-aiplatform>=1.38\"\n```", "```py\nimport vertexai\nfrom vertexai.generative_models import GenerativeModel, ChatSession, \n↪HarmCategory, HarmBlockThreshold\n*# Replace PROJECT_ID with the ID of your Google Cloud project.*\nmy_id = \"PROJECT_ID\"\n*# Replace with your Google Cloud location*\nmy_location = \"us-central1\"\ndef generate_text(project_id: str, location: str, prompt: str) -> str:\n *# Initialize Vertex AI*\n    vertexai.init(project=project_id, location=location)\n *# Load the model*\n    model = GenerativeModel(\"gemini-1.0-pro\")\n *# Generation config*\n    config = {\"max_output_tokens\": 2048, \"temperature\": 0.4, \"top_p\": 1,\n    ↪\"top_k\": 32}\n *# Safety config*\n    safety_config = {\n        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT:\n        ↪HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        HarmCategory.HARM_CATEGORY_HARASSMENT:\n        ↪HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n    }\n *# Generate content*\n    responses = model.generate_content(\n        [prompt],\n        generation_config=config,\n        stream=True,\n        safety_settings=safety_config,\n    )\n    text_responses = []\n    for response in responses:\n        text_responses.append(response.text)\n    return \"\".join(text_responses)\nprompt = \"What are all the colors in a rainbow?\"\nprint(generate_text(my_id, my_location, prompt))\n```", "```py\npip install langchain llama-cpp-python\n```", "```py\nfrom langchain_community.llms import LlamaCpp\n\nllm = LlamaCpp(model_path='path_to_your_gguf_model_file')\nresponse = llm.invoke('Your prompt!')\nprint(response)\n```"]