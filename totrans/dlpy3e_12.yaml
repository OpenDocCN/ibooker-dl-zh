- en: Object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter12_object-detection](https://deeplearningwithpython.io/chapters/chapter12_object-detection)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Object detection is all about drawing boxes (called *bounding boxes*) around
    objects of interest in a picture (see figure 12.1). This enables you to know not
    just which objects are in a picture, but also where they are. Some of its most
    common applications are
  prefs: []
  type: TYPE_NORMAL
- en: '*Counting* — Find out how many instances of an object are in an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tracking* — Track how objects move in a scene over time by performing object
    detection on every frame of a movie.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cropping* — Identify the area of an image that contains an object of interest
    to crop it and send a higher-resolution version of the image patch to a classifier
    or an Optical Character Recognition (OCR) model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/80fcdad34894a13571da65573962efca.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12.1](#figure-12-1): Object detectors draw boxes around objects in
    an image and label them.'
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking, if I have a segmentation mask for an object instance,
    I can already compute the coordinates of the smallest box that contains the mask.
    So couldn’t we just use image segmentation all the time? Do we need object detection
    models at all?
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, segmentation is a strict superset of detection. It returns all the
    information that could be returned by a detection model — plus a lot more. This
    increased wealth of information has a significant computational cost: a good object
    detection model will typically run much faster than an image segmentation model.
    It also has a data labeling cost: to train a segmentation model, you need to collect
    pixel-precise masks, which are much more time-consuming to produce than the mere
    bounding boxes required by object detection models.'
  prefs: []
  type: TYPE_NORMAL
- en: As a result, you will always want to use an object detection model if you have
    no need for pixel-level information — for instance, if all you want is to count
    objects in an image.
  prefs: []
  type: TYPE_NORMAL
- en: Single-stage vs. two-stage object detectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two broad categories of object detection architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: Two-stage detectors, which first extract region proposals, known as Region-based
    Convolutional Neural Networks (R-CNN) models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single-stage detectors, such as RetinaNet or the You Only Look Once family of
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here’s how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Two-stage R-CNN detectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A region-based ConvNet, or R-CNN model, is a two-stage model. The first stage
    takes an image and produces a few thousand partially overlapping bounding boxes
    around areas that look object-like. These boxes are called *region proposals*.
    This stage isn’t very smart, so at that point we aren’t quite sure whether the
    proposed regions do contain objects and, if so, what objects they contain.
  prefs: []
  type: TYPE_NORMAL
- en: That’s the job of the second stage — a ConvNet that looks at each region proposal
    and classifies it into a number of predetermined classes, just like the models
    you’ve seen in chapter 9 (see figure 12.2). Region proposals that have a low score
    across all classes considered are discarded. We are then left with a much smaller
    set of boxes, each with a high class-presence score for one particular class.
    Finally, bounding boxes around each object are further refined to eliminate duplicates
    and make each bounding box as precise as possible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/314b3b5fa4332a8e1c10e75fadb679d7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12.2](#figure-12-2): An R-CNN first extracts region proposals and then
    classifies the proposals with a ConvNet (a CNN).'
  prefs: []
  type: TYPE_NORMAL
- en: In early R-CNN versions, the first stage was a *heuristic model* called *Selective
    Search* that used some definition of spatial consistency to identify object-like
    areas. *Heuristic* is a term you’ll hear quite a lot in machine learning — it
    simply means “a bundle of hard-coded rules someone made up.” It’s usually used
    in opposition to learned models (where the rules are automatically derived) or
    theory-derived models. In later versions of R-CNN, such as Faster-R-CNN, the box
    generation stage became a deep learning model, called a Region Proposal Network.
  prefs: []
  type: TYPE_NORMAL
- en: The two-stage approach of R-CNN works very well in practice, but it’s quite
    computationally expensive, most notably because it requires you to classify thousands
    of patches — for every single image you process. That makes it unsuitable for
    most real-time applications and for embedded systems. My take is that, in practical
    applications, you generally don’t ever need a computationally expensive object
    detection system like R-CNN because if you’re doing server-side inference with
    a beefy GPU, then you’ll probably be better off using a segmentation model instead,
    like the Segment Anything model we saw in the previous chapter. And if you’re
    resource-constrained, then you’re going to want to use a more computationally
    efficient object detection architecture — a single-stage detector.
  prefs: []
  type: TYPE_NORMAL
- en: Single-stage detectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Around 2015, researchers and practitioners began experimenting with using a
    single deep learning model to jointly predict bounding box coordinates together
    with their labels, an architecture called a *single-stage detector*. The main
    families of single-stage detectors are RetinaNet, Single Shot MultiBox Detectors
    (SSD), and the You Only Look Once family, abbreviated as YOLO. Yes, like the meme.
    That’s on purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Single-stage detectors, especially recent YOLO iterations, boast significantly
    faster speeds and greater efficiency than their two-stage counterparts, albeit
    with a minor potential tradeoff in accuracy. Nowadays, YOLO is arguably the most
    popular object detection model out there, especially when it comes to real-time
    applications. There are usually a new version of it that comes out every year
    — interestingly, each new version tends to be developed by a separate organization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will build a simplified YOLO model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Training a YOLO model from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overall, building an object detector can be a bit of an undertaking — not that
    there’s anything theoretically complex about it. There’s just a lot of code needed
    to handle manipulating bounding boxes and predicted output. To keep things simple,
    we will recreate the very first YOLO model from 2015. There’s 12 YOLO versions
    as of this writing, but the original is a bit simpler to work with.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the COCO dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we start creating our model, we need data to train with. The COCO dataset
    ^([[1]](#footnote-1)), short for *Common Objects in Context*, is one of the best-known
    and most commonly used object detection datasets. It consists of real-world photos
    from a number of different sources plus human-created annotations. This includes
    object labels, bounding box annotations, and full segmentation masks. We will
    disregard the segmentation masks and just use bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s download the 2017 version of the COCO dataset. While not a large dataset
    by today’s standards, this 18 GB dataset will be the largest dataset we use in
    the book. If you are running the code as you read, this is a good chance to take
    a breather.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.1](#listing-12-1): Downloading the 2017 COCO dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We need to do some input massaging before we are ready to use this data. The
    first download gives us an unlabeled directory of all the COCO images. The second
    download includes all image metadata via a JSON file. COCO associates each image
    file with an ID, and each bounding box is paired with one of these IDs. We need
    to collate all box and image data together.
  prefs: []
  type: TYPE_NORMAL
- en: Each bounding box comes with `x, y, width, height` pixel coordinates starting
    at the top left corner of the image. As we load our data, we can rescale all bounding
    box coordinates so they are points in a `[0, 1]` unit square. This will make it
    easier to manipulate these boxes without needing to check the size of each input
    image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.2](#listing-12-2): Parsing the COCO data'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the data we just loaded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.3](#listing-12-3): Inspecting the COCO data'
  prefs: []
  type: TYPE_NORMAL
- en: We have 117,266 images. Each image can have anywhere from 1 to 63 objects with
    an associated bounding box. There are only 91 possible labels for objects, chosen
    by the COCO dataset creators.
  prefs: []
  type: TYPE_NORMAL
- en: We can use a KerasHub utility `keras_hub.utils.coco_id_to_name(id)` to map these
    integer labels to human-readable names, similar to the utility we used to decode
    ImageNet predictions to text labels back in chapter 8.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s visualize an example image to make this a little more concrete. We can
    define a function to draw an image with Matplotlib and another function to draw
    a labeled bounding box on this image. We will need both of these throughout the
    chapter. We can use the HSV colorspace as a simple trick to generate new colors
    for each new label we see. By fixing the saturation and brightness of the color
    and only updating its hue, we can generate bright new colors that stand out clearly
    from our image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.4](#listing-12-4): Visualizing a COCO image with box annotations'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use our new visualization to look at the sample image^([[2]](#footnote-2))
    we were inspecting earlier (see figure 12.3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c1af83579d7491efa5129821c5d53854.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12.3](#figure-12-3): YOLO outputs a bounding box prediction and class
    label for each image region.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While it would be fun to train on all 18 gigabytes of our input data, we want
    to keep the examples in this book easily runnable on modest hardware. If we limit
    ourselves to only images with four or fewer boxes, we will make our training problem
    easier and a halve the data size. Let’s do this and shuffle our data — the images
    are grouped by object type, which would be terrible for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: That’s it for data loading! Let’s start creating our YOLO model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a YOLO model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned previously, the YOLO model is a single stage detector. Rather than
    first attempting to identify all candidate objects in a scene and then classifying
    the object regions, YOLO will propose bounding boxes and object labels in one
    go.
  prefs: []
  type: TYPE_NORMAL
- en: Our model will divide an image up into a grid and predict two separate outputs
    at each grid location — a bounding box and a class label. In the original paper
    by Redmon et al.^([[3]](#footnote-3)), the model actually predicted multiple boxes
    per grid location, but we keep things simple and just predict one box in each
    grid square.
  prefs: []
  type: TYPE_NORMAL
- en: Most images will not have objects evenly distributed across a grid, and to account
    for this, the model will output a *confidence score* along with each box, as shown
    in figure 12.4\. We’d like this confidence to be high when an object is detected
    at a location, and zero when there’s no object. Most grid locations will have
    no object and should report a near-zero confidence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fde6b657f5c44d01f695075c1fa3462.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12.4](#figure-12-4): YOLO outputs as visualized in the first YOLO paper'
  prefs: []
  type: TYPE_NORMAL
- en: Like many models in computer vision, the YOLO model uses a ConvNet *backbone*
    to obtain interesting high-level features for an input image, a concept we first
    explored in chapter 8\. In their paper, the authors created their own backbone
    model and pretrained it with ImageNet for classification. Rather than do this
    ourselves, we can instead use KerasHub to load a pretrained backbone.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using the Xception backbone we’ve used so far in this book, we will
    switch to ResNet, a family of models we first mentioned in chapter 9\. The structure
    is quite similar to Xception, but ResNet uses strides instead of pooling layers
    to downsample the image. As we mentioned in chapter 11, strided convolutions are
    better when we care about the *spatial location* of the input.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s load up our pretrained model and matching preprocessing (to rescale the
    image). We will resize our images to 448 × 448; image input size is quite important
    for the object detection task.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.5](#listing-12-5): Loading the ResNet model'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can turn our backbone into a detection model by adding new layers for
    outputting box and class predictions. The setup proposed in the YOLO paper is
    quite simple. Take the output of a ConvNet backbone and feed it through two densely
    connected layers with an activation in the middle. Then, split the output. The
    first five numbers will be used for bounding box prediction (four for the box
    and one for the box confidence). The rest will be used for the *class probability
    map* shown in figure 12.4 — a classification prediction for each grid location
    over all possible 91 labels.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s write this out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.6](#listing-12-6): Attaching a YOLO prediction head'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get a better sense of the model by looking at the model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Our backbone outputs have shape `(batch_size, 14, 14, 2048)`. That is 401,408
    output floats per image, a bit too many to feed into our dense layers. We downscale
    the feature maps with a strided conv layer to `(batch_size, 6, 6, 512)` with a
    more manageable 18,432 floats per image.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can add our two densely connected layers. We flatten the entire feature
    map, pass it through a `Dense` with a `relu` activation and then pass it through
    a final `Dense` with our exact number of output predictions — 5 for the bounding
    box and confidence and 91 for each object class at each grid location.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we reshape the outputs back to a 6 × 6 grid and split our box and class
    predictions. As usual for our classification outputs, we apply a softmax. The
    box outputs will need more special consideration; we will cover this later.
  prefs: []
  type: TYPE_NORMAL
- en: Looking good! Note that because we flatten the entire feature map through the
    classification layer, every grid detector can use the entire image’s features;
    there’s no locality constraint. This is by design — large objects will not stay
    contained to a single grid cell.
  prefs: []
  type: TYPE_NORMAL
- en: Readying the COCO data for the YOLO model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our model is relatively simple, but we still need to preprocess our inputs to
    align them with the prediction grid. Each grid detector will be responsible for
    detecting any boxes whose center falls inside the grid box. Our model will output
    five floats for the box `(x, y, w, h, confidence)`. The `x` and `y` will represent
    the object’s center relative to the bounds of the grid cell (from 0 to 1). The
    `w` and `h` will represent the object’s size relative to the image size.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already have the right `w` and `h` values in our training data. However,
    we need to translate our `x` and `y` values to and from the grid. Let’s define
    two utilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s rework our training data so it conforms to this new grid structure. We
    can create two arrays as long as our dataset with our grid:'
  prefs: []
  type: TYPE_NORMAL
- en: The first will contain our class probability map. We will mark all grid cells
    that intersect with a bounding box with the correct label. To keep our code simple,
    we won’t worry about overlapping boxes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second will contain the boxes themselves. We will translate all boxes to
    the grid and label the correct grid cell with the coordinates for the box. The
    confidence for an actual box in our label data will always be one, and the confidence
    for all other locations will be zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.7](#listing-12-7): Creating the YOLO targets'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s visualize our YOLO training data with our box drawing helpers (figure
    12.5). We will draw the entire class activation map over our first input image^([[4]](#footnote-4))
    and add the confidence score of a box along with its label.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.8](#listing-12-8): Visualizing a YOLO target'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/865c1914e45f8307001760f66070ae00.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12.5](#figure-12-5): YOLO outputs a bounding box prediction and class
    label for each image. region.'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, let’s use `tf.data` to load our image data. We will load our images
    from disk, apply our preprocessing, and batch them. We should also split a validation
    set to monitor training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.9](#listing-12-9): Creating a dataset to train on'
  prefs: []
  type: TYPE_NORMAL
- en: With that, our data is ready for training.
  prefs: []
  type: TYPE_NORMAL
- en: Training the YOLO model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have our model and our training data ready, but there’s one last element
    we need before we can actually run `fit()`: the loss function. Our model outputs
    predicted boxes and predicted grid labels. We saw in chapter 7 how we can define
    multiple losses for each output — Keras will simply sum the losses together during
    training. We can handle the classification loss with `sparse_categorical_crossentropy`
    as usual.'
  prefs: []
  type: TYPE_NORMAL
- en: The box loss, however, needs some special consideration. The basic loss proposed
    by the YOLO authors is fairly simple. They use the sum-squared error of the difference
    between the target box parameters and the predicted ones. We will only compute
    this error for grid cells with actual boxes in the labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: The tricky part of the loss is the box confidence output. The authors wanted
    the confidence output to reflect not just the presence of an object, but also
    how good the predicted box is. To create a smooth estimate of how good a box prediction
    is, the authors propose using the *Intersection over Union* (IoU) metric we saw
    last chapter. If a grid cell is empty, the predicted confidence at the location
    should be zero. However, if a grid cell contains an object, we can use the IoU
    score between the current box prediction and the actual box as the target confidence
    value. This way, as the model becomes better at predicting box locations, the
    IoU score and the learned confidence values will go up.
  prefs: []
  type: TYPE_NORMAL
- en: This calls for a custom loss function. We can start be defining a utility to
    compute IoU scores for target and predicted boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.10](#listing-12-10): Computing IoU for two boxes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use this utility to define our custom loss. Redmon et al. propose a couple
    of loss scaling tricks to improve the quality of training:'
  prefs: []
  type: TYPE_NORMAL
- en: They scale up the box placement loss by a factor of five, so it becomes a more
    important part of overall training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since most grid cells are empty, they also scale down the confidence loss in
    empty locations by a factor of two. This keeps these zero confidence predictions
    from overwhelming the loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They take the square root of the width and height before computing the loss.
    This is to stop large boxes from mattering disproportionately more than small
    boxes. We will use a `sqrt` function that preserves the sign of the input, since
    our model might predict negative widths and heights at the start of training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s write this out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.11](#listing-12-11): Defining the YOLO bounding box loss'
  prefs: []
  type: TYPE_NORMAL
- en: We are finally ready to start training our YOLO model. Purely to keep this example
    short, we will skip over metrics. In a real-world setting, you’d want quite a
    few metrics here — such as the accuracy of the model at different confidence cutoff
    levels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.12](#listing-12-12): Training the YOLO model'
  prefs: []
  type: TYPE_NORMAL
- en: Training takes over an hour on the Colab free GPU runtime, and our model is
    still undertrained (validation loss is still falling!). Let’s try visualizing
    an output from our model (figure 12.6). We will use a low-confidence cutoff, as
    our model is not a very good object detector quite yet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.13](#listing-12-13): Training the YOLO model'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1652a078ff7fe8abde2a104c8dd8455b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12.6](#figure-12-6): Predictions for our sample image'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see our model is starting to understand box locations and class labels,
    though it is still not very accurate. Let’s visualize every box predicted by the
    model (figure 12.7), even those with zero confidence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c85676d921d7491707c312d5d985e6c2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12.7](#figure-12-7): Every bounding box predicted by the YOLO model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model learns very low-confidence values because it has not yet learned
    to consistently locate objects in a scene. To further improve the model, we should
    try a number of things:'
  prefs: []
  type: TYPE_NORMAL
- en: Train for more epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the whole COCO dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation (e.g., translating and rotating input images and boxes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve our class probability map for overlapping boxes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict multiple boxes per grid location using a bigger output grid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these would positively affect the model performance and get us closer
    to the original YOLO training recipe. However, this example is really just to
    get a feel for object detection training — training an accurate COCO detection
    model from scratch would take a large amount of compute and time. Instead, to
    get a sense of a better-performing detection model, let’s try using a pretrained
    object detection model called RetinaNet.
  prefs: []
  type: TYPE_NORMAL
- en: Using a pretrained RetinaNet detector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RetinaNet is also a single-stage object detector and operates on the same basic
    principles as the YOLO model. The biggest conceptual difference between our model
    and RetinaNet is that RetinaNet uses its underlying ConvNet differently to better
    handle both small and large objects simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: In our YOLO model, we simply took the final outputs of our ConvNet and used
    them to build our object detector. These output features map to large areas on
    our input image — as a result, they are not very effective at finding small objects
    in the scene.
  prefs: []
  type: TYPE_NORMAL
- en: One option to solve this scale issue would be to directly use the output of
    earlier layers in our ConvNet. This would extract high-resolution features that
    map to small localized areas of our input image. However, the output of these
    early layers is not very *semantically interesting*. It might map to different
    types of simple features like edges and curves, but only later in the ConvNet
    layers do we start building latent representations for entire objects.
  prefs: []
  type: TYPE_NORMAL
- en: The solution used by RetinaNet is called a feature pyramid network. The final
    features from the ConvNet base model are upsampled with progressive `Conv2DTranspose`
    layers, just like we saw in the previous chapter. But critically, we also include
    *lateral connections* where we sum these upsampled feature maps with the feature
    maps of the same size from the original ConvNet. This combines the semantically
    interesting, low-resolution features at the end of the ConvNet with the high-resolution,
    small-scale features from the beginning of the ConvNet. A rough sketch of this
    architecture is shown in figure 12.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8fcc8365e8b204bd67598284de0d65e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12.8](#figure-12-8): A feature pyramid network creates semantically
    interesting feature maps at different scales.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature pyramid networks can substantially boost performance by building effective
    features for both small and large objects in terms of pixel footprint. Recent
    versions of YOLO also use the same setup.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go ahead and try out the RetinaNet model, which was also trained on the
    COCO dataset. To make this a little more interesting, let’s try an image that
    is out-of-distribution for the model, the Pointillist painting *A Sunday Afternoon
    on the Island of La Grande Jatte*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start by downloading the image and converting it to a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s download the model and make a prediction. As we did in the previous
    chapter, we can use the high-level task API in KerasHub to create an `ObjectDetector`
    and use it — preprocessing included.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.14](#listing-12-14): Creating the ResNet model'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll note we pass an extra argument to specify the bounding box format. We
    can do this for most Keras models and layers that support bounding boxes. We pass
    `"rel_xywh"` to use the same format as we did for the YOLO model, so we can use
    the same box-drawing utilities. Here, `rel` stands for relative to the image size
    (e.g., from [0, 1]). Let’s inspect the prediction we just made:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We have four different model outputs: bounding boxes, confidences, labels,
    and the total number of detections. This is overall quite similar to our YOLO
    model. The model can predict a total of 100 objects for each input model.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try displaying the prediction with our box drawing utilities (figure 12.9).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 12.15](#listing-12-15): Running inference with RetinaNet'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a2576e827efdef8df4265325d5c885c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12.9](#figure-12-9): Predictions on a test image from the RetinaNet
    model'
  prefs: []
  type: TYPE_NORMAL
- en: The RetinaNet model is able to generalize to a pointillist painting with ease,
    despite no training on this style of input! This is actually one of the advantages
    of single-stage object detectors. Paintings and photographs are very different
    at a pixel level but share a similar structure at a high level. Two-stage detectors
    like R-CNNs, in contrast, are forced to classify small patches of an input image
    in isolation, which is extra difficult when small patches of pixels look very
    different than training data. Single-stage detectors can draw on features from
    the entire input and are more robust to novel test-time inputs.
  prefs: []
  type: TYPE_NORMAL
- en: With that, you have reached the end of the computer vision section of this book!
    We have trained image classifiers, segmenters, and object detectors from scratch.
    We’ve developed a good intuition for how ConvNets work, the first major success
    of the deep learning era. We aren’t quite done with images yet; you will see them
    again in chapter 17 when we start generating image output.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object detection identifies and locates objects within an image using bounding
    boxes. It’s basically a weaker version of image segmentation, but one that can
    be run much more efficiently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two primary approaches to object detection:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Region-based Convolutional Neural Networks (R-CNNs), which are two-stage models
    that first propose regions of interest and then classify them with a ConvNet.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Single-stage detectors (like RetinaNet and YOLO), which perform both tasks in
    a single step. Single-stage detectors are generally faster and more efficient,
    making them suitable for real-time applications (e.g., self-driving cars).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'YOLO computes two separate outputs simultaneously during training — possible
    bounding boxes and a class probability map:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each candidate bounding box is paired with a confidence score, which is trained
    to target the *Intersection over Union* of the predicted box and the ground truth
    box.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The class probability map classifies different regions of an image as belonging
    to different objects.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RetinaNet builds on this idea by using a feature pyramid network (FPN), which
    combines features from multiple ConvNet layers to create feature maps at different
    scales, allowing it to more accurately detect objects of different sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Footnotes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The COCO 2017 detection dataset can be explored at [https://cocodataset.org/](https://cocodataset.org/).
    Most images in this chapter are from the dataset. [[↩]](#footnote-link-1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image from the COCO 2017 dataset, [https://cocodataset.org/](https://cocodataset.org/).
    Image from Flickr, [http://farm8.staticflickr.com/7250/7520201840_3e01349e3f_z.jpg](http://farm8.staticflickr.com/7250/7520201840_3e01349e3f_z.jpg),
    CC BY 2.0 [https://creativecommons.org/licenses/by/2.0/](https://creativecommons.org/licenses/by/2.0/).
    [[↩]](#footnote-link-2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Redmon et al., “You Only Look Once: Unified, Real-Time Object Detection,” CoRR
    (2015), [https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640).
    [[↩]](#footnote-link-3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image from the COCO 2017 dataset, [https://cocodataset.org/](https://cocodataset.org/).
    Image from Flickr, [http://farm9.staticflickr.com/8081/8387882360_5b97a233c4_z.jpg](http://farm9.staticflickr.com/8081/8387882360_5b97a233c4_z.jpg),
    CC BY 2.0 [https://creativecommons.org/licenses/by/2.0/](https://creativecommons.org/licenses/by/2.0/).
    [[↩]](#footnote-link-4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
