- en: Object detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter12_object-detection](https://deeplearningwithpython.io/chapters/chapter12_object-detection)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter12_object-detection](https://deeplearningwithpython.io/chapters/chapter12_object-detection)
- en: Object detection is all about drawing boxes (called *bounding boxes*) around
    objects of interest in a picture (see figure 12.1). This enables you to know not
    just which objects are in a picture, but also where they are. Some of its most
    common applications are
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测主要是围绕图像中感兴趣对象的绘制框（称为 *边界框*）（参见图 12.1）。这使得你不仅知道图像中有什么对象，还能知道它们的位置。其最常见的一些应用包括
- en: '*Counting* — Find out how many instances of an object are in an image.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计数* — 查找图像中对象的实例数量。'
- en: '*Tracking* — Track how objects move in a scene over time by performing object
    detection on every frame of a movie.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*跟踪* — 通过对电影每一帧执行物体检测来跟踪场景中物体随时间移动。'
- en: '*Cropping* — Identify the area of an image that contains an object of interest
    to crop it and send a higher-resolution version of the image patch to a classifier
    or an Optical Character Recognition (OCR) model.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*裁剪* — 识别图像中包含感兴趣对象的区域进行裁剪，并将图像块的高分辨率版本发送到分类器或光学字符识别（OCR）模型。'
- en: '![](../Images/80fcdad34894a13571da65573962efca.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/80fcdad34894a13571da65573962efca.png)'
- en: '[Figure 12.1](#figure-12-1): Object detectors draw boxes around objects in
    an image and label them.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12.1](#figure-12-1)：物体检测器在图像中绘制边界框并对它们进行标记。'
- en: You might be thinking, if I have a segmentation mask for an object instance,
    I can already compute the coordinates of the smallest box that contains the mask.
    So couldn’t we just use image segmentation all the time? Do we need object detection
    models at all?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，如果我有一个对象实例的分割掩码，我已能计算包含掩码的最小框的坐标。那么我们是否可以一直使用图像分割？我们还需要物体检测模型吗？
- en: 'Indeed, segmentation is a strict superset of detection. It returns all the
    information that could be returned by a detection model — plus a lot more. This
    increased wealth of information has a significant computational cost: a good object
    detection model will typically run much faster than an image segmentation model.
    It also has a data labeling cost: to train a segmentation model, you need to collect
    pixel-precise masks, which are much more time-consuming to produce than the mere
    bounding boxes required by object detection models.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，分割是检测的严格超集。它返回检测模型可能返回的所有信息——以及更多。这种信息量的增加带来了显著的计算成本：一个好的物体检测模型通常比图像分割模型运行得快得多。它还有数据标注成本：要训练分割模型，你需要收集像素级精确的掩码，这比物体检测模型所需的边界框生产耗时得多。
- en: As a result, you will always want to use an object detection model if you have
    no need for pixel-level information — for instance, if all you want is to count
    objects in an image.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你不需要像素级信息，你总是想使用物体检测模型——例如，如果你只想在图像中计数对象。
- en: Single-stage vs. two-stage object detectors
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单阶段与两阶段物体检测器
- en: 'There are two broad categories of object detection architectures:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测架构主要分为两大类：
- en: Two-stage detectors, which first extract region proposals, known as Region-based
    Convolutional Neural Networks (R-CNN) models
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两阶段检测器，首先提取区域提议，称为基于区域的卷积神经网络（R-CNN）模型
- en: Single-stage detectors, such as RetinaNet or the You Only Look Once family of
    models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单阶段检测器，例如 RetinaNet 或 You Only Look Once 系列模型
- en: Here’s how they work.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它们的工作原理。
- en: Two-stage R-CNN detectors
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 两阶段 R-CNN 检测器
- en: A region-based ConvNet, or R-CNN model, is a two-stage model. The first stage
    takes an image and produces a few thousand partially overlapping bounding boxes
    around areas that look object-like. These boxes are called *region proposals*.
    This stage isn’t very smart, so at that point we aren’t quite sure whether the
    proposed regions do contain objects and, if so, what objects they contain.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于区域的卷积神经网络（R-CNN）模型是一个两阶段模型。第一阶段接收一个图像，并在看起来像物体的区域周围生成几千个部分重叠的边界框。这些框被称为 *区域提议*。这一阶段并不十分智能，所以在那个阶段我们还不确定提议的区域是否确实包含对象，以及如果包含，包含哪些对象。
- en: That’s the job of the second stage — a ConvNet that looks at each region proposal
    and classifies it into a number of predetermined classes, just like the models
    you’ve seen in chapter 9 (see figure 12.2). Region proposals that have a low score
    across all classes considered are discarded. We are then left with a much smaller
    set of boxes, each with a high class-presence score for one particular class.
    Finally, bounding boxes around each object are further refined to eliminate duplicates
    and make each bounding box as precise as possible.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第二阶段的工作——一个卷积神经网络，它查看每个区域提议并将其分类为多个预定义的类别，就像你在第9章中看到的模型一样（见图12.2）。具有低得分的区域提议被丢弃。然后我们剩下的一组箱子，每个箱子都有一个特定类别的较高类别存在分数。最后，围绕每个对象的边界框进一步细化，以消除重复并尽可能使每个边界框尽可能精确。
- en: '![](../Images/314b3b5fa4332a8e1c10e75fadb679d7.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/314b3b5fa4332a8e1c10e75fadb679d7.png)'
- en: '[Figure 12.2](#figure-12-2): An R-CNN first extracts region proposals and then
    classifies the proposals with a ConvNet (a CNN).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.2](#figure-12-2)：R-CNN首先提取区域提议，然后使用卷积神经网络（CNN）对这些提议进行分类。'
- en: In early R-CNN versions, the first stage was a *heuristic model* called *Selective
    Search* that used some definition of spatial consistency to identify object-like
    areas. *Heuristic* is a term you’ll hear quite a lot in machine learning — it
    simply means “a bundle of hard-coded rules someone made up.” It’s usually used
    in opposition to learned models (where the rules are automatically derived) or
    theory-derived models. In later versions of R-CNN, such as Faster-R-CNN, the box
    generation stage became a deep learning model, called a Region Proposal Network.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期R-CNN版本中，第一阶段是一个名为*选择性搜索*的启发式模型，它使用一些空间一致性的定义来识别类似物体的区域。"启发式"是你在机器学习中会经常听到的一个术语——它仅仅意味着“某人编造的一套硬编码的规则。”它通常用于与学习模型（规则是自动导出的）或理论导出的模型相对立。在R-CNN的后期版本中，如Faster-R-CNN，框生成阶段变成了一个深度学习模型，称为区域提议网络。
- en: The two-stage approach of R-CNN works very well in practice, but it’s quite
    computationally expensive, most notably because it requires you to classify thousands
    of patches — for every single image you process. That makes it unsuitable for
    most real-time applications and for embedded systems. My take is that, in practical
    applications, you generally don’t ever need a computationally expensive object
    detection system like R-CNN because if you’re doing server-side inference with
    a beefy GPU, then you’ll probably be better off using a segmentation model instead,
    like the Segment Anything model we saw in the previous chapter. And if you’re
    resource-constrained, then you’re going to want to use a more computationally
    efficient object detection architecture — a single-stage detector.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN的双阶段方法在实践中效果很好，但计算成本相当高，最显著的是因为它要求你为每张处理的图像分类数千个补丁。这使得它不适合大多数实时应用和嵌入式系统。我的观点是，在实际应用中，你通常不需要像R-CNN这样的计算密集型目标检测系统，因为如果你在服务器端使用强大的GPU进行推理，那么你可能会更愿意使用像我们在上一章中看到的Segment
    Anything模型这样的分割模型。如果你资源有限，那么你将想要使用一个计算效率更高的目标检测架构——单阶段检测器。
- en: Single-stage detectors
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单阶段检测器
- en: Around 2015, researchers and practitioners began experimenting with using a
    single deep learning model to jointly predict bounding box coordinates together
    with their labels, an architecture called a *single-stage detector*. The main
    families of single-stage detectors are RetinaNet, Single Shot MultiBox Detectors
    (SSD), and the You Only Look Once family, abbreviated as YOLO. Yes, like the meme.
    That’s on purpose.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在2015年，研究人员和从业者开始尝试使用单个深度学习模型来联合预测边界框坐标及其标签，这种架构被称为*单阶段检测器*。单阶段检测器的主要家族包括RetinaNet、单次多框检测器（SSD）和YOLO系列，简称YOLO。是的，就像那个梗。这是故意的。
- en: Single-stage detectors, especially recent YOLO iterations, boast significantly
    faster speeds and greater efficiency than their two-stage counterparts, albeit
    with a minor potential tradeoff in accuracy. Nowadays, YOLO is arguably the most
    popular object detection model out there, especially when it comes to real-time
    applications. There are usually a new version of it that comes out every year
    — interestingly, each new version tends to be developed by a separate organization.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 单阶段检测器，尤其是最近的YOLO迭代版本，与双阶段检测器相比，具有显著更快的速度和更高的效率，尽管在准确性方面存在一些潜在的小型权衡。如今，YOLO可以说是最受欢迎的目标检测模型，尤其是在实时应用方面。通常每年都会有一个新版本出现——有趣的是，每个新版本往往是由不同的组织开发的。
- en: In the next section, we will build a simplified YOLO model from scratch.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将从头开始构建一个简化的YOLO模型。
- en: Training a YOLO model from scratch
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始训练YOLO模型
- en: Overall, building an object detector can be a bit of an undertaking — not that
    there’s anything theoretically complex about it. There’s just a lot of code needed
    to handle manipulating bounding boxes and predicted output. To keep things simple,
    we will recreate the very first YOLO model from 2015. There’s 12 YOLO versions
    as of this writing, but the original is a bit simpler to work with.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，构建一个目标检测器可能是一项相当艰巨的任务——并不是说它在理论上有什么复杂之处。只是需要大量的代码来处理边界框和预测输出的操作。为了保持简单，我们将重新创建2015年的第一个YOLO模型。截至本文写作时，已有12个YOLO版本，但原始版本更易于操作。
- en: Downloading the COCO dataset
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载COCO数据集
- en: Before we start creating our model, we need data to train with. The COCO dataset
    ^([[1]](#footnote-1)), short for *Common Objects in Context*, is one of the best-known
    and most commonly used object detection datasets. It consists of real-world photos
    from a number of different sources plus human-created annotations. This includes
    object labels, bounding box annotations, and full segmentation masks. We will
    disregard the segmentation masks and just use bounding boxes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始创建模型之前，我们需要用于训练的数据。COCO数据集^([[1]](#footnote-1))，简称*Common Objects in Context*，是众所周知且最常用的目标检测数据集之一。它由多个不同来源的真实世界照片以及人类创建的注释组成。这包括对象标签、边界框注释和完整的分割掩码。我们将忽略分割掩码，只使用边界框。
- en: Let’s download the 2017 version of the COCO dataset. While not a large dataset
    by today’s standards, this 18 GB dataset will be the largest dataset we use in
    the book. If you are running the code as you read, this is a good chance to take
    a breather.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载2017版本的COCO数据集。虽然按照今天的标准这不是一个大型数据集，但这个18GB的数据集将是本书中我们使用的最大数据集。如果您在阅读代码时运行，这是一个休息的好机会。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 12.1](#listing-12-1): Downloading the 2017 COCO dataset'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表12.1](#listing-12-1)：下载2017年COCO数据集'
- en: We need to do some input massaging before we are ready to use this data. The
    first download gives us an unlabeled directory of all the COCO images. The second
    download includes all image metadata via a JSON file. COCO associates each image
    file with an ID, and each bounding box is paired with one of these IDs. We need
    to collate all box and image data together.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们准备好使用这些数据之前，我们需要进行一些输入处理。第一次下载给我们提供了一个所有COCO图像的无标签目录。第二次下载包含所有图像元数据，通过一个JSON文件。COCO将每个图像文件与一个ID关联，每个边界框都与这些ID之一配对。我们需要将所有框和图像数据汇总在一起。
- en: Each bounding box comes with `x, y, width, height` pixel coordinates starting
    at the top left corner of the image. As we load our data, we can rescale all bounding
    box coordinates so they are points in a `[0, 1]` unit square. This will make it
    easier to manipulate these boxes without needing to check the size of each input
    image.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个边界框都包含`x, y, width, height`像素坐标，从图像的左上角开始。在我们加载数据时，我们可以调整所有边界框坐标，使它们成为 `[0,
    1]` 单位正方形中的点。这将使操作这些框变得更加容易，而无需检查每个输入图像的大小。
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 12.2](#listing-12-2): Parsing the COCO data'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表12.2](#listing-12-2)：解析COCO数据'
- en: Let’s take a look at the data we just loaded.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们刚刚加载的数据。
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 12.3](#listing-12-3): Inspecting the COCO data'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表12.3](#listing-12-3)：检查COCO数据'
- en: We have 117,266 images. Each image can have anywhere from 1 to 63 objects with
    an associated bounding box. There are only 91 possible labels for objects, chosen
    by the COCO dataset creators.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有117,266张图像。每张图像可以有1到63个与边界框关联的对象。COCO数据集创建者选择了91个可能的标签。
- en: We can use a KerasHub utility `keras_hub.utils.coco_id_to_name(id)` to map these
    integer labels to human-readable names, similar to the utility we used to decode
    ImageNet predictions to text labels back in chapter 8.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用KerasHub实用工具`keras_hub.utils.coco_id_to_name(id)`将这些整数标签映射到可读的人名，类似于我们在第8章中用来解码ImageNet预测到文本标签的实用工具。
- en: Let’s visualize an example image to make this a little more concrete. We can
    define a function to draw an image with Matplotlib and another function to draw
    a labeled bounding box on this image. We will need both of these throughout the
    chapter. We can use the HSV colorspace as a simple trick to generate new colors
    for each new label we see. By fixing the saturation and brightness of the color
    and only updating its hue, we can generate bright new colors that stand out clearly
    from our image.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化一个示例图像，使这一点更加具体。我们可以定义一个函数来使用Matplotlib绘制图像，另一个函数来在这个图像上绘制标记的边界框。我们将在本章中需要这两个函数。我们可以使用HSV颜色空间作为一个简单的技巧来为每个新标签生成新的颜色。通过固定颜色的饱和度和亮度，只更新其色调，我们可以生成鲜艳的新颜色，这些颜色可以从我们的图像中清楚地脱颖而出。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 12.4](#listing-12-4): Visualizing a COCO image with box annotations'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表12.4](#listing-12-4)：使用框注释可视化COCO图像'
- en: 'Let’s use our new visualization to look at the sample image^([[2]](#footnote-2))
    we were inspecting earlier (see figure 12.3):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们的新可视化来查看我们之前检查的样本图像^([[2]](#footnote-2))（见图12.3）：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/c1af83579d7491efa5129821c5d53854.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/c1af83579d7491efa5129821c5d53854.png)'
- en: '[Figure 12.3](#figure-12-3): YOLO outputs a bounding box prediction and class
    label for each image region.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.3](#figure-12-3)：YOLO为每个图像区域输出一个边界框预测和类别标签。'
- en: 'While it would be fun to train on all 18 gigabytes of our input data, we want
    to keep the examples in this book easily runnable on modest hardware. If we limit
    ourselves to only images with four or fewer boxes, we will make our training problem
    easier and a halve the data size. Let’s do this and shuffle our data — the images
    are grouped by object type, which would be terrible for training:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在所有18GB的输入数据上训练会很刺激，但我们希望这本书中的示例能够在普通的硬件上轻松运行。如果我们只限制使用只有四个或更少框的图像，我们将使我们的训练问题更容易，并且将数据大小减半。让我们这样做，并打乱我们的数据——图像按对象类型分组，这对训练来说会很糟糕：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That’s it for data loading! Let’s start creating our YOLO model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载就到这里！让我们开始创建我们的YOLO模型。
- en: Creating a YOLO model
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建YOLO模型
- en: As mentioned previously, the YOLO model is a single stage detector. Rather than
    first attempting to identify all candidate objects in a scene and then classifying
    the object regions, YOLO will propose bounding boxes and object labels in one
    go.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，YOLO模型是一个单阶段检测器。而不是首先尝试在场景中识别所有候选对象，然后对对象区域进行分类，YOLO将一次性提出边界框和对象标签。
- en: Our model will divide an image up into a grid and predict two separate outputs
    at each grid location — a bounding box and a class label. In the original paper
    by Redmon et al.^([[3]](#footnote-3)), the model actually predicted multiple boxes
    per grid location, but we keep things simple and just predict one box in each
    grid square.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把图像分割成网格，并在每个网格位置预测两个不同的输出——一个边界框和一个类别标签。在Redmon等人原始论文中，模型实际上在每个网格位置预测了多个边界框，但我们保持简单，只在每个网格方块中预测一个边界框。
- en: Most images will not have objects evenly distributed across a grid, and to account
    for this, the model will output a *confidence score* along with each box, as shown
    in figure 12.4\. We’d like this confidence to be high when an object is detected
    at a location, and zero when there’s no object. Most grid locations will have
    no object and should report a near-zero confidence.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数图像在网格上不会均匀分布对象，为了解决这个问题，模型将输出一个*置信度分数*，与每个框一起，如图12.4所示。我们希望当在某个位置检测到对象时，这个置信度很高，而没有对象时为零。大多数网格位置将没有对象，应该报告接近零的置信度。
- en: '![](../Images/3fde6b657f5c44d01f695075c1fa3462.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3fde6b657f5c44d01f695075c1fa3462.png)'
- en: '[Figure 12.4](#figure-12-4): YOLO outputs as visualized in the first YOLO paper'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.4](#figure-12-4)：YOLO在第一篇YOLO论文中的输出可视化'
- en: Like many models in computer vision, the YOLO model uses a ConvNet *backbone*
    to obtain interesting high-level features for an input image, a concept we first
    explored in chapter 8\. In their paper, the authors created their own backbone
    model and pretrained it with ImageNet for classification. Rather than do this
    ourselves, we can instead use KerasHub to load a pretrained backbone.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算机视觉中的许多模型一样，YOLO模型使用ConvNet *骨干*来获取输入图像的有趣高级特征，这是我们首次在第八章中探讨的概念。在他们的论文中，作者创建了自己的骨干模型，并使用ImageNet对其进行预训练以进行分类。我们不必自己这样做，而是可以使用KerasHub来加载预训练的骨干。
- en: Instead of using the Xception backbone we’ve used so far in this book, we will
    switch to ResNet, a family of models we first mentioned in chapter 9\. The structure
    is quite similar to Xception, but ResNet uses strides instead of pooling layers
    to downsample the image. As we mentioned in chapter 11, strided convolutions are
    better when we care about the *spatial location* of the input.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中迄今为止使用的Xception骨干网络不同，我们将切换到ResNet，这是我们在第9章首次提到的模型系列。结构相当类似，但ResNet使用步长而不是池化层来下采样图像。正如我们在第11章中提到的，当我们关注输入的
    *空间位置* 时，步长卷积更好。
- en: Let’s load up our pretrained model and matching preprocessing (to rescale the
    image). We will resize our images to 448 × 448; image input size is quite important
    for the object detection task.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载我们的预训练模型和匹配的预处理（以调整图像大小）。我们将调整图像大小到448 × 448；图像输入大小对于目标检测任务非常重要。
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Listing 12.5](#listing-12-5): Loading the ResNet model'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表12.5](#listing-12-5)：加载ResNet模型'
- en: Next, we can turn our backbone into a detection model by adding new layers for
    outputting box and class predictions. The setup proposed in the YOLO paper is
    quite simple. Take the output of a ConvNet backbone and feed it through two densely
    connected layers with an activation in the middle. Then, split the output. The
    first five numbers will be used for bounding box prediction (four for the box
    and one for the box confidence). The rest will be used for the *class probability
    map* shown in figure 12.4 — a classification prediction for each grid location
    over all possible 91 labels.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过添加用于输出框和类别预测的新层，将骨干网络转换为一个检测模型。YOLO论文中提出的设置相当简单。取卷积网络骨干网络的输出，通过中间带有激活函数的两个密集连接层，然后分割输出。前五个数字将用于边界框预测（四个用于框和一个是框的置信度）。其余的将用于图12.4中显示的
    *类别概率图* —— 对所有可能的91个标签在每个网格位置上的分类预测。
- en: Let’s write this out.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把它写出来。
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 12.6](#listing-12-6): Attaching a YOLO prediction head'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表12.6](#listing-12-6)：附加YOLO预测头'
- en: 'We can get a better sense of the model by looking at the model summary:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看模型摘要来更好地理解模型：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Our backbone outputs have shape `(batch_size, 14, 14, 2048)`. That is 401,408
    output floats per image, a bit too many to feed into our dense layers. We downscale
    the feature maps with a strided conv layer to `(batch_size, 6, 6, 512)` with a
    more manageable 18,432 floats per image.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的骨干网络输出形状为 `(batch_size, 14, 14, 2048)`。这意味着每张图像有401,408个输出浮点数，对于输入到我们的密集层来说有点太多。我们使用步长卷积层将特征图下采样到
    `(batch_size, 6, 6, 512)`，每张图像有18,432个浮点数，更容易处理。
- en: Next, we can add our two densely connected layers. We flatten the entire feature
    map, pass it through a `Dense` with a `relu` activation and then pass it through
    a final `Dense` with our exact number of output predictions — 5 for the bounding
    box and confidence and 91 for each object class at each grid location.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以添加我们的两个密集连接层。我们将整个特征图展平，通过一个带有 `relu` 激活的 `Dense` 层，然后通过一个最终带有我们确切数量的输出预测的
    `Dense` 层 — 5个用于边界框和置信度，以及每个网格位置上的每个对象类别的91个。
- en: Finally, we reshape the outputs back to a 6 × 6 grid and split our box and class
    predictions. As usual for our classification outputs, we apply a softmax. The
    box outputs will need more special consideration; we will cover this later.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将输出重新塑形回6 × 6的网格，并分割我们的框和类别预测。对于我们的分类输出，我们通常应用softmax。框输出需要更多的特殊考虑；我们将在后面讨论这个问题。
- en: Looking good! Note that because we flatten the entire feature map through the
    classification layer, every grid detector can use the entire image’s features;
    there’s no locality constraint. This is by design — large objects will not stay
    contained to a single grid cell.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！请注意，由于我们通过分类层展平整个特征图，每个网格检测器都可以使用整个图像的特征；没有局部性约束。这是有意为之的 — 大型对象不会局限于单个网格单元。
- en: Readying the COCO data for the YOLO model
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备COCO数据以供YOLO模型使用
- en: Our model is relatively simple, but we still need to preprocess our inputs to
    align them with the prediction grid. Each grid detector will be responsible for
    detecting any boxes whose center falls inside the grid box. Our model will output
    five floats for the box `(x, y, w, h, confidence)`. The `x` and `y` will represent
    the object’s center relative to the bounds of the grid cell (from 0 to 1). The
    `w` and `h` will represent the object’s size relative to the image size.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式相对简单，但我们仍然需要预处理我们的输入，以便与预测网格对齐。每个网格检测器将负责检测任何中心落在网格框内的框。我们的模型将为框 `(x, y,
    w, h, confidence)` 输出五个浮点数。`x` 和 `y` 将表示对象中心相对于网格单元边界的相对位置（从0到1）。`w` 和 `h` 将表示对象大小相对于图像大小的相对位置。
- en: 'We already have the right `w` and `h` values in our training data. However,
    we need to translate our `x` and `y` values to and from the grid. Let’s define
    two utilities:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在训练数据中有了正确的 `w` 和 `h` 值。然而，我们需要将我们的 `x` 和 `y` 值从网格中转换出来。让我们定义两个实用工具：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s rework our training data so it conforms to this new grid structure. We
    can create two arrays as long as our dataset with our grid:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新整理我们的训练数据，使其符合这个新的网格结构。只要我们的数据集与我们的网格一样长，我们就可以创建两个数组：
- en: The first will contain our class probability map. We will mark all grid cells
    that intersect with a bounding box with the correct label. To keep our code simple,
    we won’t worry about overlapping boxes.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个将包含我们的类别概率图。我们将标记所有与边界框相交的网格单元，并使用正确的标签。为了使我们的代码简单，我们不会担心重叠的框。
- en: The second will contain the boxes themselves. We will translate all boxes to
    the grid and label the correct grid cell with the coordinates for the box. The
    confidence for an actual box in our label data will always be one, and the confidence
    for all other locations will be zero.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个将包含实际的框。我们将所有框转换到网格中，并用框的坐标为正确的网格单元标记。在我们标记的数据中，实际框的置信度始终为 1，而所有其他位置的置信度将为
    0。
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Listing 12.7](#listing-12-7): Creating the YOLO targets'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 12.7](#listing-12-7)：创建 YOLO 目标'
- en: Let’s visualize our YOLO training data with our box drawing helpers (figure
    12.5). We will draw the entire class activation map over our first input image^([[4]](#footnote-4))
    and add the confidence score of a box along with its label.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们的框绘制助手可视化我们的 YOLO 训练数据（图 12.5）。我们将在第一个输入图像上绘制整个类别激活图^([[4]](#footnote-4))，并添加框的置信度分数及其标签。
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Listing 12.8](#listing-12-8): Visualizing a YOLO target'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 12.8](#listing-12-8)：可视化 YOLO 目标'
- en: '![](../Images/865c1914e45f8307001760f66070ae00.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/865c1914e45f8307001760f66070ae00.png)'
- en: '[Figure 12.5](#figure-12-5): YOLO outputs a bounding box prediction and class
    label for each image. region.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12.5](#figure-12-5)：YOLO 为每个图像区域输出一个边界框预测和类别标签。'
- en: Lastly, let’s use `tf.data` to load our image data. We will load our images
    from disk, apply our preprocessing, and batch them. We should also split a validation
    set to monitor training.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用 `tf.data` 加载我们的图像数据。我们将从磁盘加载我们的图像，应用我们的预处理，并将它们分批。我们还应该分割一个验证集来监控训练。
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Listing 12.9](#listing-12-9): Creating a dataset to train on'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 12.9](#listing-12-9)：创建用于训练的数据集'
- en: With that, our data is ready for training.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们的数据就准备好进行训练了。
- en: Training the YOLO model
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练 YOLO 模型
- en: 'We have our model and our training data ready, but there’s one last element
    we need before we can actually run `fit()`: the loss function. Our model outputs
    predicted boxes and predicted grid labels. We saw in chapter 7 how we can define
    multiple losses for each output — Keras will simply sum the losses together during
    training. We can handle the classification loss with `sparse_categorical_crossentropy`
    as usual.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了我们的模型和训练数据，但在我们实际运行 `fit()` 之前，我们还需要一个最后的元素：损失函数。我们的模型输出预测框和预测网格标签。在第
    7 章中，我们看到了如何为每个输出定义多个损失——Keras 将在训练期间简单地将损失相加。我们可以像往常一样用 `sparse_categorical_crossentropy`
    处理分类损失。
- en: The box loss, however, needs some special consideration. The basic loss proposed
    by the YOLO authors is fairly simple. They use the sum-squared error of the difference
    between the target box parameters and the predicted ones. We will only compute
    this error for grid cells with actual boxes in the labeled data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，框损失需要一些特别的考虑。YOLO 作者提出的基损失相当简单。他们使用目标框参数与预测参数之间差异的平方和误差。我们只为标记数据中有实际框的网格单元计算这个误差。
- en: The tricky part of the loss is the box confidence output. The authors wanted
    the confidence output to reflect not just the presence of an object, but also
    how good the predicted box is. To create a smooth estimate of how good a box prediction
    is, the authors propose using the *Intersection over Union* (IoU) metric we saw
    last chapter. If a grid cell is empty, the predicted confidence at the location
    should be zero. However, if a grid cell contains an object, we can use the IoU
    score between the current box prediction and the actual box as the target confidence
    value. This way, as the model becomes better at predicting box locations, the
    IoU score and the learned confidence values will go up.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数中的难点在于边界框置信度的输出。作者希望置信度输出不仅反映物体的存在，还要反映预测框的好坏。为了创建一个平滑的框预测好坏估计，作者提出使用我们在上一章看到的*交并比*（IoU）度量。如果一个网格单元为空，该位置的预测置信度应该是零。然而，如果一个网格单元包含一个物体，我们可以使用当前框预测与实际框之间的IoU分数作为目标置信度值。这样，随着模型在预测框位置方面变得更好，IoU分数和学习的置信度值将上升。
- en: This calls for a custom loss function. We can start be defining a utility to
    compute IoU scores for target and predicted boxes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要自定义损失函数。我们可以先定义一个计算目标和预测框的IoU分数的实用工具。
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Listing 12.10](#listing-12-10): Computing IoU for two boxes'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表12.10](#listing-12-10)：计算两个框的IoU'
- en: 'Let’s use this utility to define our custom loss. Redmon et al. propose a couple
    of loss scaling tricks to improve the quality of training:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个实用工具来定义我们的自定义损失。Redmon等人提出了一些损失缩放技巧来提高训练质量：
- en: They scale up the box placement loss by a factor of five, so it becomes a more
    important part of overall training.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们将框放置损失放大五倍，使其成为整体训练中更重要的一部分。
- en: Since most grid cells are empty, they also scale down the confidence loss in
    empty locations by a factor of two. This keeps these zero confidence predictions
    from overwhelming the loss.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于大多数网格单元是空的，他们还将空位置的置信度损失缩小两倍。这保持了这些零置信度预测不会压倒损失。
- en: They take the square root of the width and height before computing the loss.
    This is to stop large boxes from mattering disproportionately more than small
    boxes. We will use a `sqrt` function that preserves the sign of the input, since
    our model might predict negative widths and heights at the start of training.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们计算损失之前先取宽度和高度的平方根。这是为了防止大框相对于小框产生不成比例的影响。我们将使用一个保留输入符号的`sqrt`函数，因为我们的模型在训练开始时可能会预测负的宽度和高度。
- en: Let’s write this out.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把它写出来。
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[Listing 12.11](#listing-12-11): Defining the YOLO bounding box loss'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表12.11](#listing-12-11)：定义YOLO边界框损失'
- en: We are finally ready to start training our YOLO model. Purely to keep this example
    short, we will skip over metrics. In a real-world setting, you’d want quite a
    few metrics here — such as the accuracy of the model at different confidence cutoff
    levels.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好开始训练我们的YOLO模型了。为了使这个例子简短，我们将跳过指标。在实际应用中，你在这里会想要很多指标——例如，模型在不同置信度截止值下的准确性。
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Listing 12.12](#listing-12-12): Training the YOLO model'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表12.12](#listing-12-12)：训练YOLO模型'
- en: Training takes over an hour on the Colab free GPU runtime, and our model is
    still undertrained (validation loss is still falling!). Let’s try visualizing
    an output from our model (figure 12.6). We will use a low-confidence cutoff, as
    our model is not a very good object detector quite yet.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 训练在Colab免费GPU运行时需要超过一小时，而且我们的模型仍然欠训练（验证损失仍在下降！）让我们尝试可视化我们模型的输出（图12.6）。我们将使用低置信度截止值，因为我们的模型目前不是一个很好的物体检测器。
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Listing 12.13](#listing-12-13): Training the YOLO model'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表12.13](#listing-12-13)：训练YOLO模型'
- en: '![](../Images/1652a078ff7fe8abde2a104c8dd8455b.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1652a078ff7fe8abde2a104c8dd8455b.png)'
- en: '[Figure 12.6](#figure-12-6): Predictions for our sample image'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.6](#figure-12-6)：对我们样本图像的预测'
- en: 'We can see our model is starting to understand box locations and class labels,
    though it is still not very accurate. Let’s visualize every box predicted by the
    model (figure 12.7), even those with zero confidence:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的模型开始理解框位置和类别标签，尽管它仍然不够准确。让我们可视化模型预测的每一个框（图12.7），即使那些置信度为零的框：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/c85676d921d7491707c312d5d985e6c2.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c85676d921d7491707c312d5d985e6c2.png)'
- en: '[Figure 12.7](#figure-12-7): Every bounding box predicted by the YOLO model'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.7](#figure-12-7)：YOLO模型预测的每一个边界框'
- en: 'Our model learns very low-confidence values because it has not yet learned
    to consistently locate objects in a scene. To further improve the model, we should
    try a number of things:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型学习到非常低置信度的值，因为它还没有学会在场景中一致地定位物体。为了进一步提高模型，我们应该尝试以下几种方法：
- en: Train for more epochs
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练更多轮次
- en: Use the whole COCO dataset
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用整个 COCO 数据集
- en: Data augmentation (e.g., translating and rotating input images and boxes)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强（例如，平移和旋转输入图像和框）
- en: Improve our class probability map for overlapping boxes
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善重叠框的类别概率图
- en: Predict multiple boxes per grid location using a bigger output grid
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更大的输出网格在每个网格位置预测多个框
- en: All of these would positively affect the model performance and get us closer
    to the original YOLO training recipe. However, this example is really just to
    get a feel for object detection training — training an accurate COCO detection
    model from scratch would take a large amount of compute and time. Instead, to
    get a sense of a better-performing detection model, let’s try using a pretrained
    object detection model called RetinaNet.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都会对模型性能产生积极影响，并让我们更接近原始 YOLO 训练配方。然而，这个例子实际上只是为了让我们对目标检测训练有一个感觉——从头开始训练一个准确的
    COCO 检测模型需要大量的计算和时间。相反，为了获得一个性能更好的检测模型的感觉，让我们尝试使用一个名为 RetinaNet 的预训练目标检测模型。
- en: Using a pretrained RetinaNet detector
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练的 RetinaNet 检测器
- en: RetinaNet is also a single-stage object detector and operates on the same basic
    principles as the YOLO model. The biggest conceptual difference between our model
    and RetinaNet is that RetinaNet uses its underlying ConvNet differently to better
    handle both small and large objects simultaneously.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: RetinaNet 也是一个单阶段目标检测器，其工作原理与 YOLO 模型相同。我们模型与 RetinaNet 之间最大的概念性区别在于，RetinaNet
    使用其底层的 ConvNet 的方式不同，以更好地同时处理小和大物体。
- en: In our YOLO model, we simply took the final outputs of our ConvNet and used
    them to build our object detector. These output features map to large areas on
    our input image — as a result, they are not very effective at finding small objects
    in the scene.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 YOLO 模型中，我们简单地取了 ConvNet 的最终输出，并使用它们来构建我们的目标检测器。这些输出特征映射到输入图像的大区域——因此，它们在寻找场景中的小物体方面不是很有效。
- en: One option to solve this scale issue would be to directly use the output of
    earlier layers in our ConvNet. This would extract high-resolution features that
    map to small localized areas of our input image. However, the output of these
    early layers is not very *semantically interesting*. It might map to different
    types of simple features like edges and curves, but only later in the ConvNet
    layers do we start building latent representations for entire objects.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个尺度问题的一个选项是直接使用我们 ConvNet 中早期层的输出。这将提取映射到我们输入图像小局部区域的高分辨率特征。然而，这些早期层的输出并不是非常
    *语义上有意义*。它们可能映射到不同类型的简单特征，如边缘和曲线，但只有在 ConvNet 的后期层中，我们才开始构建整个物体的潜在表示。
- en: The solution used by RetinaNet is called a feature pyramid network. The final
    features from the ConvNet base model are upsampled with progressive `Conv2DTranspose`
    layers, just like we saw in the previous chapter. But critically, we also include
    *lateral connections* where we sum these upsampled feature maps with the feature
    maps of the same size from the original ConvNet. This combines the semantically
    interesting, low-resolution features at the end of the ConvNet with the high-resolution,
    small-scale features from the beginning of the ConvNet. A rough sketch of this
    architecture is shown in figure 12.8.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: RetinaNet 使用的解决方案被称为特征金字塔网络。从 ConvNet 基础模型得到的最终特征通过渐进的 `Conv2DTranspose` 层上采样，正如我们在上一章所看到的。但关键的是，我们还包含了
    *侧向连接*，其中我们将这些上采样的特征图与原始 ConvNet 中相同大小的特征图相加。这结合了 ConvNet 末尾的语义有趣、低分辨率的特征与 ConvNet
    开头的具有高分辨率、小尺度的特征。这种架构的粗略草图如图 12.8 所示。
- en: '![](../Images/a8fcc8365e8b204bd67598284de0d65e.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/a8fcc8365e8b204bd67598284de0d65e.png)'
- en: '[Figure 12.8](#figure-12-8): A feature pyramid network creates semantically
    interesting feature maps at different scales.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12.8](#figure-12-8)：特征金字塔网络在不同尺度上创建了具有语义意义的特征图。'
- en: Feature pyramid networks can substantially boost performance by building effective
    features for both small and large objects in terms of pixel footprint. Recent
    versions of YOLO also use the same setup.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 特征金字塔网络可以通过为像素足迹大小的小和大物体构建有效特征来显著提升性能。YOLO 的最新版本也使用了相同的设置。
- en: Let’s go ahead and try out the RetinaNet model, which was also trained on the
    COCO dataset. To make this a little more interesting, let’s try an image that
    is out-of-distribution for the model, the Pointillist painting *A Sunday Afternoon
    on the Island of La Grande Jatte*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用在COCO数据集上训练的RetinaNet模型，为了使这个过程更有趣，让我们尝试一个对于模型来说是分布外的图像，即点彩画《大岛星期日下午》。
- en: 'We can start by downloading the image and converting it to a NumPy array:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以先下载图像并将其转换为NumPy数组：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, let’s download the model and make a prediction. As we did in the previous
    chapter, we can use the high-level task API in KerasHub to create an `ObjectDetector`
    and use it — preprocessing included.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们下载模型并进行预测。正如我们在上一章中所做的那样，我们可以使用KerasHub中的高级任务API创建一个`ObjectDetector`并使用它——包括预处理。
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Listing 12.14](#listing-12-14): Creating the ResNet model'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码清单12.14](#listing-12-14)：创建ResNet模型'
- en: 'You’ll note we pass an extra argument to specify the bounding box format. We
    can do this for most Keras models and layers that support bounding boxes. We pass
    `"rel_xywh"` to use the same format as we did for the YOLO model, so we can use
    the same box-drawing utilities. Here, `rel` stands for relative to the image size
    (e.g., from [0, 1]). Let’s inspect the prediction we just made:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们传递了一个额外的参数来指定边界框格式。我们可以为大多数支持边界框的Keras模型和层这样做。我们传递`"rel_xywh"`以使用与YOLO模型相同的格式，这样我们就可以使用相同的框绘制工具。在这里，`rel`代表相对于图像大小（例如，从[0,
    1]）。让我们检查我们刚刚做出的预测：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We have four different model outputs: bounding boxes, confidences, labels,
    and the total number of detections. This is overall quite similar to our YOLO
    model. The model can predict a total of 100 objects for each input model.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有四种不同的模型输出：边界框、置信度、标签和检测总数。这总体上与我们的YOLO模型非常相似。模型可以为每个输入模型预测总共100个对象。
- en: Let’s try displaying the prediction with our box drawing utilities (figure 12.9).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用我们的框绘制工具显示预测结果（图12.9）。
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Listing 12.15](#listing-12-15): Running inference with RetinaNet'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码清单12.15](#listing-12-15)：使用RetinaNet进行推理'
- en: '![](../Images/1a2576e827efdef8df4265325d5c885c.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/1a2576e827efdef8df4265325d5c885c.png)'
- en: '[Figure 12.9](#figure-12-9): Predictions on a test image from the RetinaNet
    model'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.9](#figure-12-9)：RetinaNet模型在测试图像上的预测'
- en: The RetinaNet model is able to generalize to a pointillist painting with ease,
    despite no training on this style of input! This is actually one of the advantages
    of single-stage object detectors. Paintings and photographs are very different
    at a pixel level but share a similar structure at a high level. Two-stage detectors
    like R-CNNs, in contrast, are forced to classify small patches of an input image
    in isolation, which is extra difficult when small patches of pixels look very
    different than training data. Single-stage detectors can draw on features from
    the entire input and are more robust to novel test-time inputs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: RetinaNet模型能够轻松地将点彩画泛化到这种风格，尽管没有在这个输入风格上进行训练！这实际上是单阶段目标检测器的一个优点。绘画和照片在像素级别上非常不同，但在高层次上具有相似的结构。与R-CNNs这样的两阶段检测器相比，它们被迫独立地对输入图像的小块进行分类，当小块像素看起来与训练数据非常不同时，这会变得更加困难。单阶段检测器可以借鉴整个输入的特征，并且对新颖的测试时间输入更加鲁棒。
- en: With that, you have reached the end of the computer vision section of this book!
    We have trained image classifiers, segmenters, and object detectors from scratch.
    We’ve developed a good intuition for how ConvNets work, the first major success
    of the deep learning era. We aren’t quite done with images yet; you will see them
    again in chapter 17 when we start generating image output.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，你已经到达了这本书计算机视觉部分的结尾！我们从零开始训练了图像分类器、分割器和目标检测器。我们对卷积神经网络的工作原理有了很好的直觉，这是深度学习时代的第一次重大成功。我们还没有完全结束图像；你将在第17章中再次看到它们，当我们开始生成图像输出时。
- en: Summary
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Object detection identifies and locates objects within an image using bounding
    boxes. It’s basically a weaker version of image segmentation, but one that can
    be run much more efficiently.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测通过使用边界框在图像中识别和定位对象。这基本上是图像分割的一个较弱版本，但可以运行得更加高效。
- en: 'There are two primary approaches to object detection:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测主要有两种方法：
- en: Region-based Convolutional Neural Networks (R-CNNs), which are two-stage models
    that first propose regions of interest and then classify them with a ConvNet.
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于区域的卷积神经网络（R-CNNs），这是一种两阶段模型，首先提出感兴趣区域，然后使用卷积神经网络对其进行分类。
- en: Single-stage detectors (like RetinaNet and YOLO), which perform both tasks in
    a single step. Single-stage detectors are generally faster and more efficient,
    making them suitable for real-time applications (e.g., self-driving cars).
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单阶段检测器（如 RetinaNet 和 YOLO），它们在单步中执行两项任务。单阶段检测器通常更快、更高效，使其适用于实时应用（例如，自动驾驶汽车）。
- en: 'YOLO computes two separate outputs simultaneously during training — possible
    bounding boxes and a class probability map:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO 在训练期间同时计算两个独立的输出——可能的边界框和类别概率图：
- en: Each candidate bounding box is paired with a confidence score, which is trained
    to target the *Intersection over Union* of the predicted box and the ground truth
    box.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个候选边界框都与一个置信度分数配对，该分数被训练以针对预测框和真实框的**交并比**。
- en: The class probability map classifies different regions of an image as belonging
    to different objects.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别概率图将图像的不同区域分类为属于不同的对象。
- en: RetinaNet builds on this idea by using a feature pyramid network (FPN), which
    combines features from multiple ConvNet layers to create feature maps at different
    scales, allowing it to more accurately detect objects of different sizes.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RetinaNet 通过使用特征金字塔网络（FPN）来构建这一想法，该网络结合了多个 ConvNet 层的特征以创建不同尺度的特征图，使其能够更准确地检测不同大小的对象。
- en: Footnotes
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: The COCO 2017 detection dataset can be explored at [https://cocodataset.org/](https://cocodataset.org/).
    Most images in this chapter are from the dataset. [[↩]](#footnote-link-1)
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: COCO 2017 检测数据集可在 [https://cocodataset.org/](https://cocodataset.org/) 探索。本章中的大多数图像都来自该数据集。[[↩]](#footnote-link-1)
- en: Image from the COCO 2017 dataset, [https://cocodataset.org/](https://cocodataset.org/).
    Image from Flickr, [http://farm8.staticflickr.com/7250/7520201840_3e01349e3f_z.jpg](http://farm8.staticflickr.com/7250/7520201840_3e01349e3f_z.jpg),
    CC BY 2.0 [https://creativecommons.org/licenses/by/2.0/](https://creativecommons.org/licenses/by/2.0/).
    [[↩]](#footnote-link-2)
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自 COCO 2017 数据集的图像，[https://cocodataset.org/](https://cocodataset.org/)。图像来自
    Flickr，[http://farm8.staticflickr.com/7250/7520201840_3e01349e3f_z.jpg](http://farm8.staticflickr.com/7250/7520201840_3e01349e3f_z.jpg)，CC
    BY 2.0 [https://creativecommons.org/licenses/by/2.0/](https://creativecommons.org/licenses/by/2.0/)。[[↩]](#footnote-link-2)
- en: 'Redmon et al., “You Only Look Once: Unified, Real-Time Object Detection,” CoRR
    (2015), [https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640).
    [[↩]](#footnote-link-3)'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Redmon 等人，“你只看一次：统一、实时目标检测”，CoRR (2015)，[https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640).
    [[↩]](#footnote-link-3)
- en: Image from the COCO 2017 dataset, [https://cocodataset.org/](https://cocodataset.org/).
    Image from Flickr, [http://farm9.staticflickr.com/8081/8387882360_5b97a233c4_z.jpg](http://farm9.staticflickr.com/8081/8387882360_5b97a233c4_z.jpg),
    CC BY 2.0 [https://creativecommons.org/licenses/by/2.0/](https://creativecommons.org/licenses/by/2.0/).
    [[↩]](#footnote-link-4)
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自 COCO 2017 数据集的图像，[https://cocodataset.org/](https://cocodataset.org/)。图像来自
    Flickr，[http://farm9.staticflickr.com/8081/8387882360_5b97a233c4_z.jpg](http://farm9.staticflickr.com/8081/8387882360_5b97a233c4_z.jpg)，CC
    BY 2.0 [https://creativecommons.org/licenses/by/2.0/](https://creativecommons.org/licenses/by/2.0/)。[[↩]](#footnote-link-4)
