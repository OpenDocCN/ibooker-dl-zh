["```py\nfrom keras.datasets import mnist\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data() \n```", "```py\n>>> train_images.shape\n(60000, 28, 28)\n>>> len(train_labels)\n60000\n>>> train_labels\narray([5, 0, 4, ..., 5, 6, 8], dtype=uint8)\n```", "```py\n>>> test_images.shape\n(10000, 28, 28)\n>>> len(test_labels)\n10000\n>>> test_labels\narray([7, 2, 1, ..., 4, 5, 6], dtype=uint8)\n```", "```py\nimport keras\nfrom keras import layers\n\nmodel = keras.Sequential(\n    [\n        layers.Dense(512, activation=\"relu\"),\n        layers.Dense(10, activation=\"softmax\"),\n    ]\n) \n```", "```py\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n) \n```", "```py\ntrain_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype(\"float32\") / 255\ntest_images = test_images.reshape((10000, 28 * 28))\ntest_images = test_images.astype(\"float32\") / 255 \n```", "```py\nmodel.fit(train_images, train_labels, epochs=5, batch_size=128) \n```", "```py\n>>> test_digits = test_images[0:10]\n>>> predictions = model.predict(test_digits)\n>>> predictions[0]\narray([1.0726176e-10, 1.6918376e-10, 6.1314843e-08, 8.4106023e-06,\n       2.9967067e-11, 3.0331331e-09, 8.3651971e-14, 9.9999106e-01,\n       2.6657624e-08, 3.8127661e-07], dtype=float32)\n```", "```py\n>>> predictions[0].argmax()\n7\n>>> predictions[0][7]\n0.99999106\n```", "```py\n>>> test_labels[0]\n7\n```", "```py\n>>> test_loss, test_acc = model.evaluate(test_images, test_labels)\n>>> print(f\"test_acc: {test_acc}\")\ntest_acc: 0.9785\n```", "```py\n>>> import numpy as np\n>>> x = np.array(12)\n>>> x\narray(12)\n>>> x.ndim\n0\n```", "```py\n>>> x = np.array([12, 3, 6, 14, 7])\n>>> x\narray([12, 3, 6, 14, 7])\n>>> x.ndim\n1\n```", "```py\n>>> x = np.array([[5, 78, 2, 34, 0],\n...               [6, 79, 3, 35, 1],\n...               [7, 80, 4, 36, 2]])\n>>> x.ndim\n2\n```", "```py\n>>> x = np.array([[[5, 78, 2, 34, 0],\n...                [6, 79, 3, 35, 1],\n...                [7, 80, 4, 36, 2]],\n...               [[5, 78, 2, 34, 0],\n...                [6, 79, 3, 35, 1],\n...                [7, 80, 4, 36, 2]],\n...               [[5, 78, 2, 34, 0],\n...                [6, 79, 3, 35, 1],\n...                [7, 80, 4, 36, 2]]])\n>>> x.ndim\n3\n```", "```py\nfrom keras.datasets import mnist\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data() \n```", "```py\n>>> train_images.ndim\n3\n```", "```py\n>>> train_images.shape\n(60000, 28, 28)\n```", "```py\n>>> train_images.dtype\nuint8\n```", "```py\nimport matplotlib.pyplot as plt\n\ndigit = train_images[4]\nplt.imshow(digit, cmap=plt.cm.binary)\nplt.show() \n```", "```py\n>>> train_labels[4]\n9\n```", "```py\n>>> my_slice = train_images[10:100]\n>>> my_slice.shape\n(90, 28, 28)\n```", "```py\n>>> # Equivalent to the previous example\n>>> my_slice = train_images[10:100, :, :]\n>>> my_slice.shape\n(90, 28, 28)\n>>> # Also equivalent to the previous example\n>>> my_slice = train_images[10:100, 0:28, 0:28]\n>>> my_slice.shape\n(90, 28, 28)\n```", "```py\nmy_slice = train_images[:, 14:, 14:] \n```", "```py\nmy_slice = train_images[:, 7:-7, 7:-7] \n```", "```py\nbatch = train_images[:128] \n```", "```py\nbatch = train_images[128:256] \n```", "```py\nn = 3\nbatch = train_images[128 * n : 128 * (n + 1)] \n```", "```py\nkeras.layers.Dense(512, activation=\"relu\") \n```", "```py\noutput = relu(matmul(input, W) + b) \n```", "```py\ndef naive_relu(x):\n    # x is a rank-2 NumPy tensor.\n    assert len(x.shape) == 2\n    # Avoids overwriting the input tensor\n    x = x.copy()\n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            x[i, j] = max(x[i, j], 0)\n    return x \n```", "```py\ndef naive_add(x, y):\n    # x and y are rank-2 NumPy tensors.\n    assert len(x.shape) == 2\n    assert x.shape == y.shape\n    # Avoids overwriting the input tensor\n    x = x.copy()\n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            x[i, j] += y[i, j]\n    return x \n```", "```py\nimport numpy as np\n\n# Element-wise addition\nz = x + y\n# Element-wise relu\nz = np.maximum(z, 0.0) \n```", "```py\nimport time\n\nx = np.random.random((20, 100))\ny = np.random.random((20, 100))\n\nt0 = time.time()\nfor _ in range(1000):\n    z = x + y\n    z = np.maximum(z, 0.0)\nprint(\"Took: {0:.2f} s\".format(time.time() - t0)) \n```", "```py\nt0 = time.time()\nfor _ in range(1000):\n    z = naive_add(x, y)\n    z = naive_relu(z)\nprint(\"Took: {0:.2f} s\".format(time.time() - t0)) \n```", "```py\nimport numpy as np\n\n# X is a random matrix with shape (32, 10).\nX = np.random.random((32, 10))\n# y is a random vector with shape (10,).\ny = np.random.random((10,)) \n```", "```py\n# The shape of y is now (1, 10).\ny = np.expand_dims(y, axis=0) \n```", "```py\n# Repeat y 32 times along axis 0 to obtain Y with shape (32, 10).\nY = np.tile(y, (32, 1)) \n```", "```py\ndef naive_add_matrix_and_vector(x, y):\n    # x is a rank-2 NumPy tensor.\n    assert len(x.shape) == 2\n    # y is a NumPy vector.\n    assert len(y.shape) == 1\n    assert x.shape[1] == y.shape[0]\n    # Avoids overwriting the input tensor\n    x = x.copy()\n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            x[i, j] += y[j]\n    return x \n```", "```py\nimport numpy as np\n\n# x is a random tensor with shape (64, 3, 32, 10).\nx = np.random.random((64, 3, 32, 10))\n# y is a random tensor with shape (32, 10).\ny = np.random.random((32, 10))\n# The output z has shape (64, 3, 32, 10) like x.\nz = np.maximum(x, y) \n```", "```py\nx = np.random.random((32,))\ny = np.random.random((32,))\n\n# Takes the product between x and y\nz = np.matmul(x, y)\n# This is equivalent.\nz = x @ y \n```", "```py\nz = x • y \n```", "```py\ndef naive_vector_product(x, y):\n    # x and y are NumPy vectors.\n    assert len(x.shape) == 1\n    assert len(y.shape) == 1\n    assert x.shape[0] == y.shape[0]\n    z = 0.0\n    for i in range(x.shape[0]):\n        z += x[i] * y[i]\n    return z \n```", "```py\ndef naive_matrix_vector_product(x, y):\n    # x is a NumPy matrix.\n    assert len(x.shape) == 2\n    # y is a NumPy vector.\n    assert len(y.shape) == 1\n    # The 1st dimension of x must equal the 0th dimension of y!\n    assert x.shape[1] == y.shape[0]\n    # This operation returns a vector of 0s with as many rows as x.\n    z = np.zeros(x.shape[0])\n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            z[i] += x[i, j] * y[j]\n    return z \n```", "```py\ndef naive_matrix_vector_product(x, y):\n    z = np.zeros(x.shape[0])\n    for i in range(x.shape[0]):\n        z[i] = naive_vector_product(x[i, :], y)\n    return z \n```", "```py\ndef naive_matrix_product(x, y):\n    # x and y are NumPy matrices.\n    assert len(x.shape) == 2\n    assert len(y.shape) == 2\n    # The 1st dimension of x must equal the 0th dimension of y!\n    assert x.shape[1] == y.shape[0]\n    # This operation returns a matrix of 0s with a specific shape.\n    z = np.zeros((x.shape[0], y.shape[1]))\n    # Iterates over the rows of x ...\n    for i in range(x.shape[0]):\n        # ... and over the columns of y.\n        for j in range(y.shape[1]):\n            row_x = x[i, :]\n            column_y = y[:, j]\n            z[i, j] = naive_vector_product(row_x, column_y)\n    return z \n```", "```py\n(a, b, c, d) • (d,) -> (a, b, c)\n(a, b, c, d) • (d, e) -> (a, b, c, e) \n```", "```py\ntrain_images = train_images.reshape((60000, 28 * 28)) \n```", "```py\n>>> x = np.array([[0., 1.],\n...               [2., 3.],\n...               [4., 5.]])\n>>> x.shape\n(3, 2)\n>>> x = x.reshape((6, 1))\n>>> x\narray([[ 0.],\n       [ 1.],\n       [ 2.],\n       [ 3.],\n       [ 4.],\n       [ 5.]])\n>>> x = x.reshape((2, 3))\n>>> x\narray([[ 0.,  1.,  2.],\n       [ 3.,  4.,  5.]])\n```", "```py\n>>> # Creates an all-zeros matrix of shape (300, 20)\n>>> x = np.zeros((300, 20))\n>>> x = np.transpose(x)\n>>> x.shape\n(20, 300)\n```", "```py\nA = [0.5, 1] \n```", "```py\noutput = relu(matmul(input, W) + b) \n```", "```py\nf(x + epsilon_x) = y + a * epsilon_x \n```", "```py\n# We use the model weights W to make a prediction for x.\ny_pred = matmul(x, W)\n# We estimate how far off the prediction was.\nloss_value = loss(y_pred, y_true) \n```", "```py\n# f describes the curve (or high-dimensional surface) formed by loss\n# values when W varies.\nloss_value = f(W) \n```", "```py\npast_velocity = 0.0\n# Constant momentum factor\nmomentum = 0.1\n# Optimization loop\nwhile loss > 0.01:\n    w, loss, gradient = get_current_parameters()\n    velocity = past_velocity * momentum - learning_rate * gradient\n    w = w + momentum * velocity - learning_rate * gradient\n    past_velocity = velocity\n    update_parameter(w) \n```", "```py\nloss_value = loss(\n    y_true,\n    softmax(matmul(relu(matmul(inputs, W1) + b1), W2) + b2),\n) \n```", "```py\ndef fg(x):\n    x1 = g(x)\n    y = f(x1)\n    return y \n```", "```py\ndef fghj(x):\n    x1 = j(x)\n    x2 = h(x1)\n    x3 = g(x2)\n    y = f(x3)\n    return y\n\ngrad(y, x) == grad(y, x3) * grad(x3, x2) * grad(x2, x1) * grad(x1, x) \n```", "```py\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\ntrain_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype(\"float32\") / 255\ntest_images = test_images.reshape((10000, 28 * 28))\ntest_images = test_images.astype(\"float32\") / 255 \n```", "```py\nmodel = keras.Sequential(\n    [\n        layers.Dense(512, activation=\"relu\"),\n        layers.Dense(10, activation=\"softmax\"),\n    ]\n) \n```", "```py\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n) \n```", "```py\nmodel.fit(\n    train_images,\n    train_labels,\n    epochs=5,\n    batch_size=128,\n) \n```", "```py\noutput = activation(matmul(input, W) + b) \n```", "```py\n# keras.ops is where you will find all the tensor operations you need.\nimport keras\nfrom keras import ops\n\nclass NaiveDense:\n    def __init__(self, input_size, output_size, activation=None):\n        self.activation = activation\n        self.W = keras.Variable(\n            # Creates a matrix W of shape (input_size, output_size),\n            # initialized with random values drawn from a uniform\n            # distribution\n            shape=(input_size, output_size), initializer=\"uniform\"\n        )\n        # Creates a vector b of shape (output_size,), initialized with\n        # zeros\n        self.b = keras.Variable(shape=(output_size,), initializer=\"zeros\")\n\n    # Applies the forward pass\n    def __call__(self, inputs):\n        x = ops.matmul(inputs, self.W)\n        x = x + self.b\n        if self.activation is not None:\n            x = self.activation(x)\n        return x\n\n    @property\n    # The convenience method for retrieving the layer's weights\n    def weights(self):\n        return [self.W, self.b] \n```", "```py\nclass NaiveSequential:\n    def __init__(self, layers):\n        self.layers = layers\n\n    def __call__(self, inputs):\n        x = inputs\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    @property\n    def weights(self):\n        weights = []\n        for layer in self.layers:\n            weights += layer.weights\n        return weights \n```", "```py\nmodel = NaiveSequential(\n    [\n        NaiveDense(input_size=28 * 28, output_size=512, activation=ops.relu),\n        NaiveDense(input_size=512, output_size=10, activation=ops.softmax),\n    ]\n)\nassert len(model.weights) == 4 \n```", "```py\nimport math\n\nclass BatchGenerator:\n    def __init__(self, images, labels, batch_size=128):\n        assert len(images) == len(labels)\n        self.index = 0\n        self.images = images\n        self.labels = labels\n        self.batch_size = batch_size\n        self.num_batches = math.ceil(len(images) / batch_size)\n\n    def next(self):\n        images = self.images[self.index : self.index + self.batch_size]\n        labels = self.labels[self.index : self.index + self.batch_size]\n        self.index += self.batch_size\n        return images, labels \n```", "```py\ndef one_training_step(model, images_batch, labels_batch):\n    # Runs the \"forward pass\"\n    predictions = model(images_batch)\n    loss = ops.sparse_categorical_crossentropy(labels_batch, predictions)\n    average_loss = ops.mean(loss)\n    # Computes the gradient of the loss with regard to the weights. The\n    # output, gradients, is a list where each entry corresponds to a\n    # weight from the model.weights list. We haven't defined this\n    # function yet!\n    gradients = get_gradients_of_loss_wrt_weights(loss, model.weights)\n    # Updates the weights using the gradients. We haven't defined this\n    # function yet!\n    update_weights(gradients, model.weights)\n    return loss \n```", "```py\nlearning_rate = 1e-3\n\ndef update_weights(gradients, weights):\n    for g, w in zip(gradients, weights):\n        # Assigns a new value to the variable, in place\n        w.assign(w - g * learning_rate) \n```", "```py\nfrom keras import optimizers\n\noptimizer = optimizers.SGD(learning_rate=1e-3)\n\ndef update_weights(gradients, weights):\n    optimizer.apply_gradients(zip(gradients, weights)) \n```", "```py\nimport tensorflow as tf\n\n# Instantiates a scalar tensor with value 0\nx = tf.zeros(shape=())\n# Opens a GradientTape scope\nwith tf.GradientTape() as tape:\n    # Inside the scope, applies some tensor operations to our variable\n    y = 2 * x + 3\n# Uses the tape to retrieve the gradient of the output y with respect\n# to our variable x\ngrad_of_y_wrt_x = tape.gradient(y, x) \n```", "```py\ndef one_training_step(model, images_batch, labels_batch):\n    with tf.GradientTape() as tape:\n        predictions = model(images_batch)\n        loss = ops.sparse_categorical_crossentropy(labels_batch, predictions)\n        average_loss = ops.mean(loss)\n    gradients = tape.gradient(average_loss, model.weights)\n    update_weights(gradients, model.weights)\n    return average_loss \n```", "```py\ndef fit(model, images, labels, epochs, batch_size=128):\n    for epoch_counter in range(epochs):\n        print(f\"Epoch {epoch_counter}\")\n        batch_generator = BatchGenerator(images, labels)\n        for batch_counter in range(batch_generator.num_batches):\n            images_batch, labels_batch = batch_generator.next()\n            loss = one_training_step(model, images_batch, labels_batch)\n            if batch_counter % 100 == 0:\n                print(f\"loss at batch {batch_counter}: {loss:.2f}\") \n```", "```py\nfrom keras.datasets import mnist\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\ntrain_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype(\"float32\") / 255\ntest_images = test_images.reshape((10000, 28 * 28))\ntest_images = test_images.astype(\"float32\") / 255\n\nfit(model, train_images, train_labels, epochs=10, batch_size=128) \n```", "```py\n>>> predictions = model(test_images)\n>>> predicted_labels = ops.argmax(predictions, axis=1)\n>>> matches = predicted_labels == test_labels\n>>> f\"accuracy: {ops.mean(matches):.2f}\"\naccuracy: 0.83\n```"]