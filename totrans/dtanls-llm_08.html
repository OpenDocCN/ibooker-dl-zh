<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="ch__audio"> <span class="chapter-title-numbering"><span class="num-string">7</span></span> <span class="title-text"> Analyzing audio data</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li id="p2">Transcribing audio data</li> 
    <li id="p3">Translating audio data</li> 
    <li id="p4">Generating speech</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>Watch any credible science fiction TV show or movie, and you won’t see people typing to interact with their computers! Whether it’s <em>Star Trek</em> or <em>2001: A Space Odyssey</em> (both released in the 1960s), people speak to (not type into) their machines. And there are good reasons for that! For most users, voice is the most natural form of communication (because that’s the one they start with). No wonder people imagined speaking with computers long before that was technically feasible.</p> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>Reality has now caught up with science fiction, and voice assistants, including the likes of Amazon’s Alexa, Google’s Assistant, and Microsoft’s Cortana (among many others), are ubiquitous. The newest generation of speech recognition (and speech generation) models have reached near-human levels of proficiency. And voice-based interaction with computers is, of course, only one use case for this amazing technology.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>In this chapter, we will use OpenAI’s latest models for speech transcription, translation, and speech generation for several mini-projects. First, we will see that transcribing voice recordings to text takes just a few lines of Python code. After that, we’ll look at more complex applications, starting with a voice-based version of our natural language database query interface from chapter 5. Whereas we previously had to type in questions, we can now simply speak them, and the system will produce an answer. Finally, we will see how to build a simultaneous translator that turns our voice input into voice output in a different language.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <h2 class=" readable-text-h2" id="preliminaries"><span class="num-string browsable-reference-id">7.1</span> Preliminaries</h2> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>Before we can start with all of those cool projects, we need to perform a few setup steps. First, you will need to record voice input via your computer. For that to work, you first need some kind of microphone. Most laptops nowadays have a built-in microphone. It doesn’t have to be a professional microphone; any way of recording sound on your computer will do. But beyond the microphone, you also need software that can be activated from Python to turn audio recordings into files. For that, we will use Python’s <code>sounddevice</code> library. Run the following command in the terminal to install this library in the correct version:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p10"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install sounddevice==0.4</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>This library interacts with Python’s <code>scipy</code> library, which you should also install. Run the following command in the terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p12"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install scipy==1.11</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>Together, those libraries will enable you to record voice input (which you can then transcribe, translate, or summarize using OpenAI’s models).</p> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>We have covered the input side, but what about the output? For some of the following projects, we not only want to listen to audio but also generate it! To generate speech, we will use OpenAI’s generative AI models again. But after generating speech stored in an audio file, we still need suitable libraries to play speech on our computer from Python. We will use the <code>playsound</code> library for that. Run this command in the terminal to install this library in the correct version:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p15"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install playsound==1.3</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>On certain operating systems (in particular, macOS), you additionally have to install the <code>PyObjC</code> library using the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p17"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install PyObjC==10.0</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>If you haven’t done so already when working through the last chapter, install the <code>requests</code> library (enabling you to send requests directly to OpenAI’s API):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p19"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install requests==2.31.0</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>Well done! If you didn’t encounter any error messages running these commands, your system is now configured to process audio data with OpenAI’s Transformer models. Let’s start with our first project in the next section.</p> 
  </div> 
  <div class="readable-text" id="p21"> 
   <h2 class=" readable-text-h2" id="transcribing-audio-files"><span class="num-string browsable-reference-id">7.2</span> Transcribing audio files</h2> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>Having recently started your job at Banana, you are overwhelmed by the number of meetings. There are just too many meetings to attend, but you don’t want to miss anything important! Fortunately, Banana has the good sense to create audio recordings of all employee meetings as a general rule (with the consent of all participants). But listening to all the recordings of those meetings is still too time-consuming.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>It would be great to have transcripts of meetings, enabling you to quickly search for anything relevant to your unit via a simple text search. Unfortunately, Banana doesn’t offer such transcripts out of the box, and none of your colleagues are willing to take notes during those meetings. Would it be possible to create such transcripts automatically? In this section, we will see that it’s not only possible but actually easy to create such an automated transcription service.</p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <h3 class=" readable-text-h3" id="sub__transcriptionBasics"><span class="num-string browsable-reference-id">7.2.1</span> Transcribing speech</h3> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>For transcribing speech to text, we will use OpenAI’s Whisper model. Unlike the models we have used so far (in particular, GPT models), Whisper is specifically targeted at audio transcriptions.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p26"> 
    <h5 class=" callout-container-h5 readable-text-h5">What is the Whisper model?</h5> 
   </div> 
   <div class="readable-text" id="p27"> 
    <p> Whisper is a Transformer model trained on large numbers of audio recordings (more than 680,000 hours of recordings, to be precise!). Whisper was trained on a multilingual audio corpus and therefore supports a broad range of input languages that it transcribes to English (i.e., you get speech transcription and translation in a single step).</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>Similar to the GPT variants, we will access Whisper via OpenAI’s Python library. This means no additional setup is required on your local machine (assuming that you have installed OpenAI’s Python library, as described in chapter 3).</p> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>In this section, we will use the Whisper model to transcribe an audio file to disk. Let’s assume that our audio file is initially stored on disk. Whisper supports a wide range of file formats: MP3, MP4, MPEG, MPGA, M4A, WAV, and WEBM. At the time of writing, the file size is limited to 25 MB. Given such a file, let’s assume that its file path is stored in the variable <code>audio_path</code>. Now all it takes to transcribe its content to text are the following few lines of Python code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p30"> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai
client = openai.OpenAI()

with open(audio_path, 'rb') as audio_file:  #1
     #2
    transcription = client.audio.transcriptions.create(
        file=audio_file, model='whisper-1')</pre> 
    <div class="code-annotations-overlay-container">
     #1 Opens the audio file
     <br/>#2 Transcribes the content
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>As a first step (<strong class="cueball">1</strong>), we need to open our audio file. For that, we can use Python’s <code>open</code> command. Note the use of the <code>rb</code> flag as a parameter of the <code>open</code> command. This flag indicates to Python that we want to read the file (<code>r</code>) and that we are opening a binary file (<code>b</code>). A binary file is a file that does not contain readable characters. Sound files, such as the one we are trying to open here, generally qualify as binary files. After processing the first line, the file content is accessible via the variable <code>audio_file</code>.</p> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>As a second step (<strong class="cueball">2</strong>), we perform the actual transcription. We now use a different endpoint, specialized for audio data processing. From that endpoint, we invoke the transcription service (<code>transcriptions.create</code>) using two parameters:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p33"><code>file</code>—A reference to the file to transcribe</li> 
   <li class="readable-text" id="p34"><code>model</code>—The name of the model for transcription</li> 
  </ul> 
  <div class="readable-text" id="p35"> 
   <p>We refer to the previously opened file (<code>audio_file</code>) and select <code>whisper-1</code> as our transcription model. The result of transcription is an object containing the transcribed text and metadata about the transcription process. We can access the transcribed text via the <code>text</code> field (i.e., via <code>transcription.text</code>).ents. After decompression, you should see three subdirectories in the resulting folder:</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>As you see, transcribing text takes just a few lines of Python code! In the next subsection, we will use this code to build a simple transcription service.</p> 
  </div> 
  <div class="readable-text" id="p37"> 
   <h3 class=" readable-text-h3" id="end-to-end-code"><span class="num-string browsable-reference-id">7.2.2</span> End-to-end code</h3> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>Listing <a href="#code__transcription">7.1</a> shows the code for a simple transcription program. The actual transcription happens in the <code>transcribe</code> function (<strong class="cueball">1</strong>). This is essentially the code we discussed in the previous section. Given the path to an audio file as input, it returns the transcribed text.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p39"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__transcription"><span class="num-string">Listing <span class="browsable-reference-id">7.1</span></span> Transcribing audio files to text</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import openai

client = openai.OpenAI()

def transcribe(audio_path):        #1
    """ Transcribe audio file to text.
    
    Args:
        audio_path: path to audio file.
    
    Returns:
        transcribed text.
    """
    with open(audio_path, 'rb') as audio_file:
        transcription = client.audio.transcriptions.create(
            file=audio_file, model='whisper-1')
        return transcription.text

if __name__ == '__main__':         #2
    
    parser = argparse.ArgumentParser()
    parser.add_argument('audiopath', type=str, help='Path to audio file')
    args = parser.parse_args()

    transcript = transcribe(args.audiopath)
    print(transcript)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Transcribes audio to text
     <br/>#2 Main function
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>The main function (<strong class="cueball">2</strong>) reads the path to an audio file (which should contain speech) as input. After invoking the <code>transcriptions.create</code> function, it prints the transcribed text on the screen.</p> 
  </div> 
  <div class="readable-text" id="p41"> 
   <h3 class=" readable-text-h3" id="trying-it-out"><span class="num-string browsable-reference-id">7.2.3</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>To try it, we first need an audio file with recorded speech. You can use any such file (including a recording of your company meetings, if available) as long as it complies with the format and size restrictions outlined in section <a href="#sub__transcriptionBasics">7.2.1</a>. However, keep in mind that you pay per minute of audio data processed! At the time of writing, using Whisper via the OpenAI library costs $0.006 per minute (you can find more up-to-date information about pricing at <a href="https://openai.com/pricing">https://openai.com/pricing</a>). Processing long recordings can therefore be expensive.</p> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>If you don’t want to use your own recording, have a look at the book’s companion website. You can find a short recording in the Audio item in this chapter’s section. Download this recording to use it for transcription (by default, the filename should be QuoteFromTheAlchemist.mp3).</p> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>Listing <a href="#code__transcription">7.1</a> is also available on the book’s companion website (item listing1.py in the chapter 7 section). After downloading it, switch to the corresponding repository in the terminal. Assuming that you downloaded the audio file into the current directory, run the following command in the terminal to transcribe the sample file:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p45"> 
   <div class="code-area-container"> 
    <pre class="code-area">python listing1.py QuoteFromTheAlchemist.mp3</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>If everything goes well, you should see the following output in the terminal (for the sample file from the website, that is):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p47"> 
   <div class="code-area-container"> 
    <pre class="code-area">Two years ago, right here on this spot, 
I had a recurrent dream, too.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>Click the sample file to listen to it yourself; you will find the transcript to be accurate! Next, we will integrate speech transcription into more complex applications.</p> 
  </div> 
  <div class="readable-text" id="p49"> 
   <h2 class=" readable-text-h2" id="querying-relational-data-via-voice"><span class="num-string browsable-reference-id">7.3</span> Querying relational data via voice</h2> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>Analyzing tabular data is fun! A significant part of your job at Banana consists of poring over data tables, extracting insights, and preparing corresponding reports and visualizations. You’re using the text-to-SQL interface from chapter 5 to automatically translate text questions to formal queries (written in SQL), execute them, and present the query results. This makes analyzing data easier and is faster than writing complex SQL queries from scratch.</p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>However, there is a problem: you think better when pacing back and forth in your office while analyzing data. But typing queries forces you back to your desk every time. Can’t we modify our query interface to accept spoken, as opposed to typed, input? It turns out that indeed, we can! In this section, we will see how to use OpenAI’s models to enable a simple voice query interface for tabular data.</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <h3 class=" readable-text-h3" id="preliminaries-1"><span class="num-string browsable-reference-id">7.3.1</span> Preliminaries</h3> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>We will build a voice query interface that processes spoken questions on tabular data. It is an extension of the query interface discussed in chapter 5. We assume that spoken questions refer to data stored in SQLite, a popular system for processing queries on relational data. See chapter 5 for a short introduction to SQLite and installation instructions. To try the following code, you will first need to install the SQLite database system.</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>The SQLite system processes queries formulated in SQL, the structured query language. Fortunately, you won’t need to write SQL queries yourself (we will use a language model to write those SQL queries for us). However, language models are not perfect and may occasionally produce incorrect queries. To recognize those cases, it is useful to have a certain degree of SQL background. You will find a short introduction to SQL in chapter 5. For more details, have a look at <a href="http://www.databaselecture.com">www.databaselecture.com</a>.</p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <p>Our voice query interface processes spoken questions, so you need to ensure that your microphone is working. Also, to execute the following code, make sure your voice query interface has all the required permissions to access the microphone.</p> 
  </div> 
  <div class="readable-text" id="p56"> 
   <h3 class=" readable-text-h3" id="overview"><span class="num-string browsable-reference-id">7.3.2</span> Overview</h3> 
  </div> 
  <div class="readable-text" id="p57"> 
   <p>Our voice query interface processes spoken questions on tabular data stored in an SQLite database. For instance, having loaded a database with data about computer game sales, we can ask questions such as the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p58">“How many games did Activision sell in 2023?”</li> 
   <li class="readable-text" id="p59">“How many action games were released between 2019 and 2021?”</li> 
  </ul> 
  <div class="readable-text" id="p60"> 
   <p>On receiving a spoken question, the voice query interface performs the following steps:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p61">Transcribes the spoken question into text</li> 
   <li class="readable-text" id="p62">Translates the text question into an SQL query</li> 
   <li class="readable-text" id="p63">Processes the SQL query on the data using SQLite</li> 
   <li class="readable-text" id="p64">Displays the query result to the user</li> 
  </ol> 
  <div class="readable-text" id="p65"> 
   <p>Figure <a href="#fig__VQIoverview">7.1</a> illustrates the different processing steps in more detail. The process is executed for each spoken question.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p66">  
   <img alt="figure" src="../Images/CH07_F01_Trummer.png" width="879" height="889"/> 
   <h5 class=" figure-container-h5" id="fig__VQIoverview"><span class="num-string">Figure <span class="browsable-reference-id">7.1</span></span> Our voice query interface transcribes spoken questions into text, translates text questions into SQL queries, and finally processes those queries and displays the query result.</h5>
  </div> 
  <div class="readable-text" id="p67"> 
   <h3 class=" readable-text-h3" id="recording-audio"><span class="num-string browsable-reference-id">7.3.3</span> Recording audio</h3> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>For our transcription application, we assumed that an audio recording was already available. For our new project, we want to issue voice queries repeatedly. That means we have to record them ourselves. How can we do that in Python? First, we need to import two libraries for precisely that purpose:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p69"> 
   <div class="code-area-container"> 
    <pre class="code-area">import sounddevice       #1
import scipy.io.wavfile  #2</pre> 
    <div class="code-annotations-overlay-container">
     #1 Records audio
     <br/>#2 Stores .wav files
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>The <code>sounddevice</code> library (<strong class="cueball">1</strong>) contains many useful functions to record audio input from a microphone. What will we do with our recordings? We will store them as .wav files on disk. In the previous section, we saw how to transcribe audio data stored in that format. This is where the second library (<strong class="cueball">2</strong>), <code>scipy</code>, comes into play: it enables us to store the recordings in .wav format on disk.</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>When recording, we need to make two important choices:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p72">At what sample rate should we read input from the microphone?</li> 
   <li class="readable-text" id="p73">How many seconds of speech should we record?</li> 
  </ul> 
  <div class="readable-text" id="p74"> 
   <p>We will record for a duration of 5 seconds. Five seconds should suffice for most voice queries. You can try different settings if the recording tends to terminate too soon or if you find yourself waiting often after finishing your voice queries. A more sophisticated implementation would record continuously or stop recording after a speaking pause is detected. To keep things simple in terms of the recording mechanism, we will just record for a predetermined amount of time for each voice query.</p> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>For the sampling rate—that is, the number of audio data points stored per second—we will choose 44,100 Hertz. This is the standard for CD-quality recordings. The total number of <em>frames</em>—the number of audio data points received in total—is then 44,100 times the number of seconds we want to record (in our case, that’s 5 seconds). We store the number of frames and the sampling rate in auxiliary variables:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p76"> 
   <div class="code-area-container"> 
    <pre class="code-area">sample_rate = 44100
nr_frames = 5 * sample_rate</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>Now we’re ready to record using the <code>rec</code> function of the <code>sounddevice</code> library:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p78"> 
   <div class="code-area-container"> 
    <pre class="code-area">recording = sounddevice.rec(                   #1
    nr_frames, samplerate=sample_rate, channels=1)
sounddevice.wait()                             #2</pre> 
    <div class="code-annotations-overlay-container">
     #1 Sets up recording
     <br/>#2 Waits for recording to finish
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>The first command (<strong class="cueball">1</strong>) starts a recording from the input microphone, providing as input the total number of frames to record as well as the sampling rate. The number of channels (the third parameter in our invocation) depends on the microphone used for the recording. If your microphone has more than one channel, try a higher value here. After starting the recording, we just need to wait until the predetermined recording time has passed. We accomplish that via the <code>wait</code> command (<strong class="cueball">2</strong>).</p> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>After executing the previous code, the variable <code>recording</code> contains the recorded audio data. As discussed earlier, we want to store the recording as a .wav file on disk. All it takes is a single command from the <code>scipy</code> library:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p81"> 
   <div class="code-area-container"> 
    <pre class="code-area">scipy.io.wavfile.write(output_path, sample_rate, recording)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>That’s it! We have recorded a few seconds of audio input and stored it in a file on disk.</p> 
  </div> 
  <div class="readable-text" id="p83"> 
   <h3 class=" readable-text-h3" id="end-to-end-code-1"><span class="num-string browsable-reference-id">7.3.4</span> End-to-end code</h3> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>Listing <a href="#code__voicequeries">7.2</a> shows the code for our voice query interface. Beyond our default libraries, <code>openai</code> and <code>argparse</code>, we import (<strong class="cueball">1</strong>) the libraries for audio processing (<code>sounddevice</code> and <code>scipy</code>), as well as the <code>sqlite3</code> library (which we will need for processing SQL queries) and the <code>time</code> library. The latter library is required to wait for a specified amount of time (for voice input). Next, we will discuss the functions introduced in listing <a href="#code__voicequeries">7.2</a>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p85"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__voicequeries"><span class="num-string">Listing <span class="browsable-reference-id">7.2</span></span> Querying an SQLite database using voice commands</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse  #1
import openai
import re
import scipy.io.wavfile
import sounddevice
import sqlite3
import time

client = openai.OpenAI()

def get_structure(data_path):               #2
    """ Extract structure from SQLite database.
    
    Args:
        data_path: path to SQLite data file.
    
    Returns:
        text description of database structure.
    """
    with sqlite3.connect(data_path) as connection:
        cursor = connection.cursor()
        cursor.execute("select sql from sqlite_master where type = 'table';")
        table_rows = cursor.fetchall()
        table_ddls = [r[0] for r in table_rows]
        return '\n'.join(table_ddls)

def record(output_path):                 #3
    """ Record audio and store in .wav file. 
    
    Args:
        output_path: store audio recording there.
    """
    sample_rate = 44100
    nr_frames = 5 * sample_rate
    recording = sounddevice.rec(
        nr_frames, samplerate=sample_rate, channels=1)
    sounddevice.wait()
    scipy.io.wavfile.write(output_path, sample_rate, recording)

def transcribe(audio_path):        #4
    """ Transcribe audio file to text.
    
    Args:
        audio_path: path to audio file.
    
    Returns:
        transcribed text.
    """
    with open(audio_path, 'rb') as audio_file:
        transcription = client.audio.transcriptions.create(
            file=audio_file, model='whisper-1')
        return transcription.text

def create_prompt(description, question):        #5
    """ Generate prompt to translate question into SQL query.
    
    Args:
        description: text description of database structure.
        question: question about data in natural language.
    
    Returns:
        prompt for question translation.
    """
    parts = []
    parts += ['Database:']
    parts += [description]
    parts += ['Translate this question into SQL query:']
    parts += [question]
    parts += ['SQL Query:']
    return '\n'.join(parts)

def call_llm(prompt):                             #6
    """ Query large language model and return answer.
    
    Args:
        prompt: input prompt for language model.
    
    Returns:
        Answer by language model.
    """
    for nr_retries in range(1, 4):
        try:
            response = client.chat.completions.create(
                model='gpt-4o',
                messages=[
                    {'role':'user', 'content':prompt}
                    ]
                )
            return response.choices[0].message.content
        except:
            time.sleep(nr_retries * 2)
    raise Exception('Cannot query OpenAI model!')

def process_query(data_path, query):        #7
    """ Processes SQL query and returns result.
    
    Args:
        data_path: path to SQLite data file.
        query: process this query on database.
    
    Returns:
        query result.
    """
    with sqlite3.connect(data_path) as connection:
        cursor = connection.cursor()
        cursor.execute(query)
        table_rows = cursor.fetchall()
        table_strings = [str(r) for r in table_rows]
        return '\n'.join(table_strings)

if __name__ == '__main__':   #8
    
    parser = argparse.ArgumentParser()
    parser.add_argument('dbpath', type=str, help='Path to SQLite data')
    args = parser.parse_args()

    data_structure = get_structure(args.dbpath)
    
    while True:  #9
        
        user_input = input('Press enter to record (type quit to quit).')
        if user_input == 'quit':
            break
        
        audio_path = 'question.wav'  #10
        record(audio_path)
        question = transcribe(audio_path)
        print(f'Question: {question}')
        
        prompt = create_prompt(data_structure, question)  #11
        answer = call_llm(prompt)
        query = re.findall('"`sql(.*)"`', answer, re.DOTALL)[0]
        print(f'SQL: {query}')

        try:                                       #12
            answer = process_query(args.dbpath, query)
            print(f'Answer: {answer}')
        except:
            print('Error processing query! Try to reformulate.')</pre> 
    <div class="code-annotations-overlay-container">
     #1 Imports libraries
     <br/>#2 Extracts the database schema
     <br/>#3 Records audio
     <br/>#4 Transcribes audio
     <br/>#5 Creates a text-to-SQL prompt
     <br/>#6 Translates to SQL
     <br/>#7 Processes the SQL query
     <br/>#8 Processes the voice queries
     <br/>#9 Main loop
     <br/>#10 Transcribes voice input
     <br/>#11 SQL translation
     <br/>#12 Executes the SQL query
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>We process voice queries that refer to data in a relational database. To translate voice commands into formal queries formulated in SQL, we need to know a little about the database structure. In particular, we need to know the names of the data tables and their columns (i.e., we need to know the database schema). The function <code>get_structure</code> (<strong class="cueball">2</strong>) retrieves the commands used to create the database schema. These commands contain the names of tables and columns, as well as the data types associated with the table columns. We will use those commands as part of a prompt, instructing the language model to translate questions into SQL queries.</p> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>Before we can translate questions, we first need to record them from the microphone. This is where the function <code>record</code> (<strong class="cueball">3</strong>) comes into play. It uses the <code>sounddevice</code> library to record 5 consecutive seconds of audio input from the microphone. The resulting audio recording is stored as a .wav file on disk at a path specified as function input (parameter <code>output_path</code>). Strictly speaking, storing the audio input as a file is not necessary (we can process it directly in memory). However, storing audio input on disk can be useful for debugging purposes. If our system fails to translate voice input to appropriate queries, we can listen to the audio file ourselves to assess the level of background noise and overall audio quality. If the microphone is not set up properly (a common problem), our audio files will contain nothing but silence.</p> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>After recording input from the microphone, we first want to transcribe voice input to text. We use the <code>transcribe</code> function (<strong class="cueball">4</strong>) for that. Given a path to an audio file (in this case, recorded audio input from the microphone), it returns a transcript generated using OpenAI’s Whisper model (the same one we used previously).</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>Next, we want to translate questions into formal SQL queries. Of course, we will use language models for that task. The <code>create_prompt</code> function (<strong class="cueball">5</strong>) generates a suitable prompt. The prompt contains the previously extracted description of the database, the transcribed question, and the task description. The <code>call_llm</code> function (<strong class="cueball">6</strong>) calls GPT-4o to translate questions, given the previously mentioned prompt as input. Finally, the <code>process_query</code> function (<strong class="cueball">7</strong>) processes the resulting queries on the database and returns the query result.</p> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>Time to put it all together! Our voice query interface takes the path to an SQLite database file as input (<strong class="cueball">8</strong>). After extracting the database schema, we enter the main loop (<strong class="cueball">9</strong>). Each iteration processes one voice query (unless the user enters <code>quit</code>, in which case the program terminates). To keep things simple, we wait for the user to press the Enter key before recording voice input (a more sophisticated version would record continuously). After that, we record voice input from the microphone. We print out the transcribed question and store the recording itself as question.wav on disk (<strong class="cueball">10</strong>). Next, we translate the transcribed text into a query (<strong class="cueball">11</strong>), execute it (<strong class="cueball">12</strong>) (we need exception handling here in case of incorrect queries!), and show the result to users.</p> 
  </div> 
  <div class="readable-text" id="p91"> 
   <h3 class=" readable-text-h3" id="trying-it-out-1"><span class="num-string browsable-reference-id">7.3.5</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>Listing <a href="#code__voicequeries">7.2</a> is listing 2 in the chapter 7 section on the book’s website. Download the code, and switch to the containing folder in your terminal.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>Beyond the code, we also need an SQLite database to try our voice query interface. We discuss in chapter 5 how to set up an example database containing information about computer game sales. We assume this database is stored in the same folder as your code and named games.db (of course, you are free to use any SQLite database you like to try the voice query interface). Now enter the following command in the terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p94"> 
   <div class="code-area-container"> 
    <pre class="code-area">python listing1.py games.db</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>Update the path to the database file to the one you want to access. Depending on your operating system and security settings, you may be asked to enable microphone access for your application. After enabling microphone access, press Enter, and ask a question! For instance, using the games database, you may ask “How many games were sold in 2007?” or “How many games were released for each genre?” You should see output like the following:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p96"> 
   <div class="code-area-container"> 
    <pre class="code-area">Press enter to record (type quit to quit).
Question: How many games were released for each genre?
SQL: SELECT genre, COUNT(*) as num_games
FROM games
GROUP BY genre
Answer: ('Action', 3316)
('Adventure', 1286)
('Fighting', 848)
('Genre', 1)
('Misc', 1739)
('Platform', 886)
('Puzzle', 582)
('Racing', 1249)
('Role-Playing', 1488)
('Shooter', 1310)
('Simulation', 867)
('Sports', 2346)
('Strategy', 681)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>This output includes the transcribed question, the translated SQL query, and the query result (or an error message if the query cannot be executed). Clearly, it’s a long way from a voice question to a query result! A mistake in recording, transcription, or translation will lead to incorrect results. Before trusting the query result, be sure to check the additional output to verify that the system did not make any mistakes.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p98"> 
   <p> <span class="print-book-callout-head">Tip</span> If your voice interface only produces nonsense, check the recordings in question.wav. If you don’t hear anything, make sure your application has access to your microphone. By default, applications typically have no access to the microphone (making it harder to spy on you with malicious software). You need to update your security settings to enable access. </p> 
  </div> 
  <div class="readable-text" id="p99"> 
   <h2 class=" readable-text-h2" id="speech-to-speech-translation"><span class="num-string browsable-reference-id">7.4</span> Speech-to-speech translation</h2> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>The Banana branch in Paris has started looking into language models and potential applications for data science tasks. You have grown your reputation as the local expert on the topic, and your manager asks you to advise the French team on how to get started. There is just one tiny problem: you don’t speak any French. On hearing that Banana Paris conducts most staff meetings in French, you are about to decline the assignment. But after thinking about it, you realize that this may not be a dealbreaker after all. Although you don’t speak any French, GPT-4o certainly does! Would it be possible to use language models to translate for you?</p> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>You can indeed use language models to translate between various languages. In this section, we will create a translator tool that takes spoken input in a first language and produces spoken output in a second language. Because the tool produces spoken output, you don’t even need to learn the French pronunciation. Simply speak English and wait for the tool to produce a spoken translation. That way, you can collaborate with your French colleagues while simultaneously demonstrating the capabilities of state-of-the-art language models!</p> 
  </div> 
  <div class="readable-text" id="p102"> 
   <h3 class=" readable-text-h3" id="overview-1"><span class="num-string browsable-reference-id">7.4.1</span> Overview</h3> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>Our translator tool processes spoken input. As before, we will use OpenAI’s Whisper model to transcribe input speech to text. Then, we will use the GPT-4o model to translate the text to a different language. For our example scenarios, we use French as the target language. However, due to the amazing flexibility of models like GPT-4o, our tool won’t be restricted to that! Our tool will enable users to specify the target language as input, to be used as a text snippet in the prompt instructing the language model for the translation.</p> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>After generating a text translation, we still want to generate a spoken version. It turns out that we can use yet another OpenAI model to transform text into spoken output in various languages. Figure <a href="#fig__TranslatorOverview">7.2</a> shows the complete processing pipeline, starting with spoken input in a first language and ending with spoken output in a second.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p105">  
   <img alt="figure" src="../Images/CH07_F02_Trummer.png" width="626" height="914"/> 
   <h5 class=" figure-container-h5" id="fig__TranslatorOverview"><span class="num-string">Figure <span class="browsable-reference-id">7.2</span></span> Our translator tool records spoken input in a first language, transcribes input to text, trans- lates that text into a second language, and finally generates spoken output.</h5>
  </div> 
  <div class="readable-text" id="p106"> 
   <h3 class=" readable-text-h3" id="generating-speech"><span class="num-string browsable-reference-id">7.4.2</span> Generating speech</h3> 
  </div> 
  <div class="readable-text" id="p107"> 
   <p>The pipeline in figure <a href="#fig__TranslatorOverview">7.2</a> requires several transformations. We already saw how to transcribe spoken input to text in the previous sections. Translating text via language models is relatively straightforward (ask GPT-4o to translate from one language to another, and it will do so). We are still missing a way to transform written text (e.g., in French) into spoken output. We discuss how to do that next.</p> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>OpenAI (as well as other providers) offers several text-to-speech (TTS) models. Such models take written text as input and produce a spoken version as output. The following piece of code generates speech for a text string (stored in the variable <code>speech_text</code>):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p109"> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai
client = openai.OpenAI()

response = client.audio.speech.create(
    model='tts-1', voice='alloy', 
    input=speech_text)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>We’re using a new endpoint in this instance (<code>audio.speech</code>) and configuring the <code>create</code> method using three parameters:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p111"><code>model</code>—The name of the model used to generate spoken output. We use OpenAI’s <code>tts-1</code> text-to-speech model.</li> 
   <li class="readable-text" id="p112"><code>input</code>—We generate spoken output for this text. Submit text in any of the various languages supported by the model (<a href="https://github.com/openai/whisper">https://github.com/openai/whisper</a>).</li> 
   <li class="readable-text" id="p113"><code>voice</code>—We can choose between different voices for speech. Here, we use <code>alloy</code>.</li> 
  </ul> 
  <div class="readable-text" id="p114"> 
   <p>That’s all we need to generate speech output via OpenAI! We already know how to transcribe speech and how to translate text between different languages, so we now have all we need to code our translator tool.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p115"> 
    <h5 class=" callout-container-h5 readable-text-h5">What about pricing?</h5> 
   </div> 
   <div class="readable-text" id="p116"> 
    <p> At the time of writing, OpenAI charges 1.5 cents per 1,000 tokens for text generation using the TTS model and twice that for the high-quality version (TTS HD). These prices are likely to change over time, so be sure to look at OpenAI’s pricing website (<a href="https://openai.com/pricing">https://openai.com/pricing</a>) for updated information.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p117"> 
   <h3 class=" readable-text-h3" id="end-to-end-code-2"><span class="num-string browsable-reference-id">7.4.3</span> End-to-end code</h3> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>Listing <a href="#code__translator">7.3</a> shows the complete code for our translator tool. Let’s start by discussing the libraries it imports (<strong class="cueball">1</strong>). Besides the <code>openai</code> and <code>argparse</code> libraries included in each project so far, we import <code>sounddevice</code> and <code>scipy</code> to record and store audio files, along with the <code>time</code> library to limit recording time.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p119"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__translator"><span class="num-string">Listing <span class="browsable-reference-id">7.3</span></span> Translating spoken input into a different language</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse  #1
import openai
import playsound
import requests
import scipy.io.wavfile
import sounddevice
import time

client = openai.OpenAI()

def record(output_path):                 #2
    """ Record audio and store in .wav file. 
    
    Args:
        output_path: store audio recording there.
    """
    sample_rate = 44100
    nr_frames = 5 * sample_rate
    recording = sounddevice.rec(
        nr_frames, samplerate=sample_rate, channels=1)
    sounddevice.wait()
    scipy.io.wavfile.write(output_path, sample_rate, recording)

def transcribe(audio_path):        #3
    """ Transcribe audio file to text.
    
    Args:
        audio_path: path to audio file.
    
    Returns:
        transcribed text.
    """
    with open(audio_path, 'rb') as audio_file:
        transcription = client.audio.transcriptions.create(
            file=audio_file, model='whisper-1')
        return transcription.text
 #4
def create_prompt(to_translate, to_language):
    """ Generate prompt to translate text to target language.
    
    Args:
        to_translate: translate this text.
        to_language: translate text to this language.
    
    Returns:
        Translated text.
    """
    parts = []
    parts += [f'Translate this text to {to_language}:']
    parts += [to_translate]
    parts += ['Translated text:']
    return '\n'.join(parts)

def call_llm(prompt):                             #5
    """ Query large language model and return answer.
    
    Args:
        prompt: input prompt for language model.
    
    Returns:
        Answer by language model.
    """
    for nr_retries in range(1, 4):
        try:
            response = client.chat.completions.create(
                model='gpt-4o',
                messages=[
                    {'role':'user', 'content':prompt}
                    ]
                )
            return response.choices[0].message.content
        except:
            time.sleep(nr_retries * 2)
    raise Exception('Cannot query OpenAI model!')

def generate_speech(speech_text):    #6
    """ Generates speech for given text.
    
    Args:
        speech_text: generate speech for this text.
    
    Returns:
        query result.
    """
    response = client.audio.speech.create(
        model='tts-1', voice='alloy', 
        input=speech_text)
    return response.content

if __name__ == '__main__':        #7
    
    parser = argparse.ArgumentParser()
    parser.add_argument('tolanguage', type=str, help='Target language')
    args = parser.parse_args()
    
    while True:  #8
        
        user_input = input('Press enter to record (type quit to quit).')
        if user_input == 'quit':
            break
        
        audio_path = 'to_translate.wav'  #9
        record(audio_path)
        to_translate = transcribe(audio_path)
        print(f'Original text: {to_translate}')
         #10
        prompt = create_prompt(to_translate, args.tolanguage)
        translated = call_llm(prompt)
        print(f'Translated text: {translated}')

        speech = generate_speech(translated)    #11
        with open('translation.mp3', 'wb') as file:
            file.write(speech)

        playsound.playsound('translation.mp3')  #12</pre> 
    <div class="code-annotations-overlay-container">
     #1 Imports libraries
     <br/>#2 Records audio
     <br/>#3 Transcribes audio
     <br/>#4 Generates a prompt for translation
     <br/>#5 Uses the language model
     <br/>#6 Generates speech
     <br/>#7 Translates speech to speech
     <br/>#8 Main loop
     <br/>#9 Transcribes the input
     <br/>#10 Translates to the target language
     <br/>#11 Generates speech output
     <br/>#12 Plays the generated speech
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>The <code>playsound</code> library is used to play audio files generated by OpenAI’s models. Because we generate speech via OpenAI’s HTTP interface, we import the <code>requests</code> library to create HTTP requests. Next, we will discuss the functions used in listing <a href="#code__translator">7.3</a>.</p> 
  </div> 
  <div class="readable-text" id="p121"> 
   <p>As in the previous project, we record audio data from the microphone. The <code>record</code> function (<strong class="cueball">2</strong>) records 5 seconds of audio input and stores it into a .wav file on disk. The <code>transcribe</code> function (<strong class="cueball">3</strong>) transcribes that audio input to text. Both functions have been discussed in more detail in the prior projects in this chapter.</p> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>The <code>create_prompt</code> function (<strong class="cueball">4</strong>) generates a prompt for translation. As in prior projects, the prompt contains a task description, together with all relevant input data. In this case, we want to translate from the initial language (English) to the target language (French). Note that the target language is specified as an input parameter (<code>to_language</code>). This input parameter corresponds to a text snippet describing the desired output language. In the simplest case, this can be the name of a language (e.g., “French”). On the other hand, users can request a specific dialect (e.g., “German with Swabian dialect”) or style (e.g., “English in the style of Shakespeare”). The target language is integrated into the task description that appears in the prompt along with the text to translate.</p> 
  </div> 
  <div class="readable-text" id="p123"> 
   <p>Note that we do not need to specify the input language. We assume that the language model is able to recognize the language of the input text (otherwise, we cannot expect the model to translate either).</p> 
  </div> 
  <div class="readable-text" id="p124"> 
   <p>After invoking the <code>call_llm</code> function (<strong class="cueball">5</strong>) with the prompt, we should obtain translated text. The <code>generate_speech</code> function (<strong class="cueball">6</strong>) generates the corresponding speech using the approach we discussed in the previous section.</p> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p>The translator application (<strong class="cueball">7</strong>) expects as input a text describing the target language. This parameter is a string that can contain arbitrary text. It simply replaces a placeholder in the prompt used for translation. In the main loop (<strong class="cueball">8</strong>), users press Enter to speak or enter <code>quit</code> to terminate the application.</p> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>When recording user input, we first store 5 seconds of audio recording in a file named to_translate.wav before transcribing the input via the <code>transcribe</code> function (<strong class="cueball">9</strong>). After that, we use GPT-4o to translate the input to the target language (<strong class="cueball">10</strong>) and then generate speech from the translation (<strong class="cueball">11</strong>). We store the generated speech as an .mp3 file on disk (this means we can easily hear the last output again) and, finally, use the <code>playsound</code> library to—you guessed it—play the generated sound file.</p> 
  </div> 
  <div class="readable-text" id="p127"> 
   <h3 class=" readable-text-h3" id="trying-it-out-2"><span class="num-string browsable-reference-id">7.4.4</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>Time to try our translator! You can find the code on the companion website as listing 3 in the chapter 7 section. Download the code, and switch to the containing folder in the terminal. We can choose our target language for translation. Of course, the quality of the translation and sound output may vary, depending on that choice. In particular, the model we use for transcription, as well as the model we use for speech generation, support a set of about 60 common languages. Transcribing audio input or generating audio output in less common languages may fail. Look online to see the current list of supported languages for transcription (<a href="https://help.openai.com/en/articles/7031512-whisper-api-faq">https://help.openai.com/en/articles/7031512-whisper-api-faq</a>) as well as speech generation (<a href="https://platform.openai.com/docs/guides/text-to-speech">https://platform.openai.com/docs/guides/text-to-speech</a>). For now, consistent with our scenario at the beginning of this section, we will go with French as the target language. In the terminal, enter the following command to start our translator:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p129"> 
   <div class="code-area-container"> 
    <pre class="code-area">python listing3.py "French"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p130"> 
   <p>Strictly speaking, the quotes around the word “French” are unnecessary. However, as we can enter multiword descriptions of the desired target language, we will need quotes in the following examples to avoid errors if the console misinterprets our input as values for multiple parameters.</p> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>As in our previous project, we need to give our application access to the microphone. Click Yes if you are asked for microphone access; if not, be sure the security settings allow it. The following is an extract from a conversation with our translator tool:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p132"> 
   <div class="code-area-container"> 
    <pre class="code-area">Press enter to record (type quit to quit).
Original text: Hello my colleagues in Paris.
Translated text: Bonjour mes collègues à Paris.
Press enter to record (type quit to quit).
Original text: Let me teach you something 
about language models.
Translated text: Laisse-moi t'apprendre 
quelque chose à propos des modèles de langage.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p133"> 
   <p>You see transcribed input and the generated translation. You should also hear the spoken version of the translation (if not, check your settings for audio output). Not bad for a few lines of Python code!</p> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>Translating to French seems like a reasonable use case for our translator tool. However, it may not be the one with the highest “fun factor.” Let’s try something different to show the flexibility of language models: let’s see if we can “translate” our audio input to a highly polished version. In the terminal, enter the following instructions:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p135"> 
   <div class="code-area-container"> 
    <pre class="code-area">python listing3.py "English in the style of Shakespeare"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>This is what we get when translating our simple greeting into a much more refined version (perhaps a nice intro to a course on language models for our U.S. colleagues at Banana):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p137"> 
   <div class="code-area-container"> 
    <pre class="code-area">Press enter to record (type quit to quit).
Original text: Hello, my dear colleagues.
Translated text: Hark, my fair allies, I bid thee well met!
Press enter to record (type quit to quit).
Original text: Let me teach you 
something about language models.
Translated text: Pray, lend me thine ear 
as I shalt educate thee on language models.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p138"> 
   <p>Try a few more target languages! The possibilities are (almost) unlimited.</p> 
  </div> 
  <div class="readable-text" id="p139"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p140">OpenAI’s Whisper model transcribes speech input to text.</li> 
   <li class="readable-text" id="p141">Access transcription via the audio transcriptions endpoint.</li> 
   <li class="readable-text" id="p142">Pricing for transcription is based on the number of minutes.</li> 
   <li class="readable-text" id="p143">OpenAI offers several models for generating speech from text.</li> 
   <li class="readable-text" id="p144">You can choose the voice and quality for generated speech.</li> 
   <li class="readable-text" id="p145">Speech generation pricing depends on the number of tokens.</li> 
  </ul>
 </div></div></body></html>