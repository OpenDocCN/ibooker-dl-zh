- en: Chapter 8\. The PyTorch Ecosystem and Additional Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you’ve learned everything you need to design and deploy
    deep learning models with PyTorch. You have learned how to build, train, test,
    and accelerate your models across different platforms and how to deploy those
    models to the cloud and edge devices. As you’ve seen, PyTorch has powerful capabilities
    in both development and deployment environments and is highly extensible, allowing
    you to create customizations tailored to your needs.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this reference guide, we’ll explore the PyTorch Ecosystem, other
    supporting libraries, and additional resources. The PyTorch Ecosystem is one of
    the most powerful advantages of PyTorch. It provides a rich set of projects, tools,
    models, libraries, and platforms to explore AI and accelerate your AI development.
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch Ecosystem includes projects and libraries created by researchers,
    third-party vendors, and the PyTorch community. These projects are well maintained
    and have been vetted by the PyTorch team to ensure their quality and utility.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the PyTorch project includes other libraries that support specific
    domains, including Torchvision for computer vision and Torchtext for NLP. PyTorch
    also supports other packages like TensorBoard for visualization, and there’s an
    abundance of learning resources for further study, like Papers with Code and PyTorch
    Academy.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll begin with an overview of the PyTorch Ecosystem and a
    high-level view of its supported projects and tools. Then we’ll dig a little deeper
    into some of the most powerful and popular resources, with reference material
    about their usage and APIs provided along the way. Finally, I’ll show you how
    to learn more with a variety of tutorials, books, courses, and other training
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by looking at all the Ecosystem has to offer.
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch Ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As of early 2021, the [PyTorch Ecosystem](https://pytorch.tips/ecosystem) features
    over 50 libraries and projects, and the list continues to grow. Some of these
    are domain-specific projects, such as those specifically for computer vision or
    NLP solutions. Other projects, such as PyTorch Lightning and fastai, provide frameworks
    for writing concise code, while projects like PySyft and Crypten support security
    and privacy. There are also projects that support reinforcement learning, gaming
    models, model interpretability, and acceleration. In this section, we’ll explore
    projects included in the PyTorch Ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-1](#table_eco_cv) provides a list of the Ecosystem projects that support
    *computer vision* applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. Computer vision projects
  prefs: []
  type: TYPE_NORMAL
- en: '| Project | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Torchvision | PyTorch’s computer vision library that provides common transforms,
    models, and utilities to support computer vision applications ([*https://pytorch.tips/torchvision*](https://pytorch.tips/torchvision))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Detectron2 | Facebook’s objection detection and segmentation platform ([*https://pytorch.tips/detectron2*](https://pytorch.tips/detectron2))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Albumentations | Image augmentation library ([*https://pytorch.tips/albumentations*](https://pytorch.tips/albumentations))
    |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch3D | Collection of reusable components for 3D computer vision ([*https://pytorch.tips/pytorch3d*](https://pytorch.tips/pytorch3d))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Kornia | Library of differentiable modules for computer vision ([*https://pytorch.tips/kornia*](https://pytorch.tips/kornia))
    |'
  prefs: []
  type: TYPE_TB
- en: '| MONAI | Framework for deep learning in healthcare imaging ([*https://pytorch.tips/monai*](https://pytorch.tips/monai))
    |'
  prefs: []
  type: TYPE_TB
- en: '| TorchIO | Toolkit for 3D medical images ([*https://pytorch.tips/torchio*](https://pytorch.tips/torchio))
    |'
  prefs: []
  type: TYPE_TB
- en: Torchvision is one of the most powerful libraries for computer vision applications
    and is included in the PyTorch project. It’s also maintained by the PyTorch development
    team. We’ll cover the Torchvision API in more detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch3D and TorchIO provide additional support for 3D imaging, while TorchIO
    and MONAI focus on medical imaging applications. Detectron2 is a powerful platform
    for object detection. If you’re conducting computer vision research and development,
    these extensions may help accelerate your results.
  prefs: []
  type: TYPE_NORMAL
- en: As with computer vision, there have been major advances in NLP research over
    the past decade, and NLP applications are also well supported by PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-2](#table_eco_nlp) provides a list of the Ecosystem projects that
    support *NLP and audio-based* applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-2\. NLP and audio projects
  prefs: []
  type: TYPE_NORMAL
- en: '| Project | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Torchtext | PyTorch’s NLP and text processing library ([*https://pytorch.tips/torchtext*](https://pytorch.tips/torchtext))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Flair | Simple framework for NLP ([*https://pytorch.tips/flair*](https://pytorch.tips/flair))
    |'
  prefs: []
  type: TYPE_TB
- en: '| AllenNLP | Library for designing and evaluating NLP models ([*https://pytorch.tips/allennlp*](https://pytorch.tips/allennlp))
    |'
  prefs: []
  type: TYPE_TB
- en: '| ParlAI | Framework for sharing, training, and testing dialogue models ([*https://pytorch.tips/parlai*](https://pytorch.tips/parlai))
    |'
  prefs: []
  type: TYPE_TB
- en: '| NeMo | Toolkit for conversational AI ([*https://pytorch.tips/nemo*](https://pytorch.tips/nemo))
    |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch NLP | Basic utilities for NLP ([*https://pytorch.tips/pytorchnlp*](https://pytorch.tips/pytorchnlp))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Translate | Facebook’s machine translation platform ([*https://pytorch.tips/translate*](https://pytorch.tips/translate))
    |'
  prefs: []
  type: TYPE_TB
- en: '| TorchAudio | PyTorch’s library for audio preprocessing ([*https://pytorch.tips/torchaudio*](https://pytorch.tips/torchaudio))
    |'
  prefs: []
  type: TYPE_TB
- en: Like Torchvision, Torchtext is included as part of the PyTorch project and is
    maintained by the PyTorch development team. Torchtext provides powerful functionality
    for processing text data and developing NLP-based models.
  prefs: []
  type: TYPE_NORMAL
- en: Flair, AllenNLP, and PyTorch NLP provide additional capabilities for text-based
    processing and NLP model development. ParlAI and NeMo provide tools to develop
    dialogue and conversational AI systems, while Translate focuses on machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: TorchAudio provides functions for handling audio files like speech and music.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning and gaming are also rapidly growing fields of research,
    and there are tools to support them using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-3](#table_eco_gaming) provides a list of the Ecosystem projects that
    support *gaming and reinforcement learning* applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-3\. Gaming and reinforcement learning projects
  prefs: []
  type: TYPE_NORMAL
- en: '| Project | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ELF | Project for training and testing algorithms in game environments ([*https://pytorch.tips/elf*](https://pytorch.tips/elf))
    |'
  prefs: []
  type: TYPE_TB
- en: '| PFRL | Library of deep reinforcement algorithms ([*https://pytorch.tips/pfrl*](https://pytorch.tips/pfrl))
    |'
  prefs: []
  type: TYPE_TB
- en: ELF (extensive, lightweight, and flexible platform for game research) is an
    open source project developed by Facebook that reimplements gaming algorithms
    like AlphaGoZero and AlphaZero. PFRL (preferred reinforcement learning) is a PyTorch-based
    open source deep reinforcement learning library developed by Preferred Networks,
    the creators of Chainer and ChainerRL. It can be used to create baseline algorithms
    for reinforcement learning. PFRL currently has reproducibility scripts for 11
    key deep reinforcement learning algorithms based on original research papers.
  prefs: []
  type: TYPE_NORMAL
- en: As you’ve seen in this book, PyTorch is a highly customizable framework. This
    characteristic sometimes results in the need to write the same boilerplate code
    often for common tasks. To help developers write code faster and eliminate the
    need for boilerplate code, several PyTorch projects provide high-level programming
    APIs or compatibility with other high-level frameworks like scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-4](#table_eco_hlp) provides a list of the Ecosystem projects that
    support *high-level programming*.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-4\. High-level programming projects
  prefs: []
  type: TYPE_NORMAL
- en: '| Project | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| fastai | Library that simplifies training using modern practices ([*https://pytorch.tips/fastai*](https://pytorch.tips/fastai))
    |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch Lightning | Customizable Keras-like ML library that eliminates boilerplate
    code ([*https://pytorch.tips/lightning*](https://pytorch.tips/lightning)) |'
  prefs: []
  type: TYPE_TB
- en: '| Ignite | Library for writing compact, full-featured training loops ([*https://pytorch.tips/ignite*](https://pytorch.tips/ignite))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Catalyst | Framework for compact reinforcement learning pipelines ([*https://pytorch.tips/catalyst*](https://pytorch.tips/catalyst))
    |'
  prefs: []
  type: TYPE_TB
- en: '| skorch | Provides PyTorch compatibility with scikit-learn ([*https://pytorch.tips/skorch*](https://pytorch.tips/skorch))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hydra | Framework for configuring complex applications ([*https://pytorch.tips/hydra*](https://pytorch.tips/hydra))
    |'
  prefs: []
  type: TYPE_TB
- en: '| higher | Facilitates the implementation of complex meta-learning algorithms
    ([*https://pytorch.tips/higher*](https://pytorch.tips/higher)) |'
  prefs: []
  type: TYPE_TB
- en: '| Poutyne | Keras-like framework for boilerplate code ([*https://pytorch.tips/poutyne*](https://pytorch.tips/poutyne))
    |'
  prefs: []
  type: TYPE_TB
- en: Fastai is a research and learning framework built on PyTorch. It has comprehensive
    documentation and has provided a high-level API for PyTorch since the library’s
    early days. You can get up to speed with the framework quickly by consultng its
    documentation and free [online courses](https://pytorch.tips/fastai) or reading
    the book [*Deep Learning for Coders with fastai and PyTorch*](https://pytorch.tips/fastai-book)
    by Jeremy Howard and Sylvain Gugger (O’Reilly).
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Lightning has also become one a very popular high-level programming
    API for PyTorch. It provides all the necessary boilerplate code for training,
    validation, and test loops while allowing you to easily add customizations for
    your methods.
  prefs: []
  type: TYPE_NORMAL
- en: Ignite and Catalyst are also popular high-level frameworks, while skorch and
    Poutyne provide scikit-learn and Keras-like interfaces, respectively. Hydra and
    higher are used to simplify the configuration of complex applications.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to high-level frameworks, there are packages in the Ecosystem that
    support hardware acceleration and optimized inference.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-5](#table_eco_inference) provides a list of ecosystem projects that
    support *inference acceleration* applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-5\. Inference projects
  prefs: []
  type: TYPE_NORMAL
- en: '| Project | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Glow | ML compiler for hardware acceleration ([*https://pytorch.tips/glow*](https://pytorch.tips/glow))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hummingbird | Compiles trained models for faster inference ([*https://pytorch.tips/hummingbird*](https://pytorch.tips/hummingbird))
    |'
  prefs: []
  type: TYPE_TB
- en: Glow is a machine learning compiler and execution engine for hardware accelerators,
    and it can be used as a backend for high-level deep learning frameworks. The compiler
    allows state-of-the-art optimizations and code generation of neural network graphs.
    Hummingbird is an open source project developed by Microsoft. It is a library
    for compiling trained, traditional ML models into tensor computations and seamlessly
    leverages PyTorch to accelerate traditional ML models.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to accelerating inference, the PyTorch Ecosystem also contains projects
    to accelerate training and optimize models using distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-6](#table_eco_distributed) provides a list of ecosystem projects that
    support *distributed training and model optimization*.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-6\. Distributed training and model optimization projects
  prefs: []
  type: TYPE_NORMAL
- en: '| Project | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ray | Fast, simple framework for building and running distributed applications
    ([*https://pytorch.tips/ray*](https://pytorch.tips/ray)) |'
  prefs: []
  type: TYPE_TB
- en: '| Horovod | Distributed deep learning training framework for TensorFlow, Keras,
    PyTorch, and Apache MXNet ([*https://pytorch.tips/horovod*](https://pytorch.tips/horovod))
    |'
  prefs: []
  type: TYPE_TB
- en: '| DeepSpeed | Optimization library ([*https://pytorch.tips/deepspeed*](https://pytorch.tips/deepspeed))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Optuna | Automated hyperparameter search and optimization ([*https://pytorch.tips/optuna*](https://pytorch.tips/optuna))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Polyaxon | Platform for building, training, and monitoring large-scale deep
    learning applications ([*https://pytorch.tips/polyaxon*](https://pytorch.tips/polyaxon))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Determined | Platform that trains models using shared GPUs and collaboration
    ([*https://pytorch.tips/determined*](https://pytorch.tips/determined)) |'
  prefs: []
  type: TYPE_TB
- en: '| Allegro Trains | Library that contains a deep learning experiment manager,
    versioning, and machine learning ops ([*https://pytorch.tips/allegro*](https://pytorch.tips/allegro))
    |'
  prefs: []
  type: TYPE_TB
- en: Ray is a Python API for building distributed applications and is packaged with
    other libraries for accelerating machine learning workloads. We used one of these
    packages, Ray Tune, in [Chapter 6](ch06.xhtml#pytorth_acceleration_and_optimization)
    to tune hyperparameters on a distributed system. Ray is a very powerful package
    that can also support scalable reinforcement learning, distributed training, and
    scalable serving. Horovod is another distributed framework. It is focused on distributed
    training and can be used with Ray.
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed, Optuna, and Allegro Trains also support hyperparameter tuning and
    model optimization. Polyaxon can be used to train and monitor models at scale,
    and Determined focuses on sharing GPUs for accelerated training.
  prefs: []
  type: TYPE_NORMAL
- en: With the growth in PyTorch’s popularity, there have been quite a few specialized
    packages developed to support niche domains and specific tools. Many of these
    tools aim to improve models or the preprocessing of data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-7](#table_eco_modeling) provides a list of the Ecosystem projects
    that support *modeling and data processing*.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-7\. Modeling and data processing projects
  prefs: []
  type: TYPE_NORMAL
- en: '| Project | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TensorBoard | TensorBoard’s data and model visualization tool is integrated
    into PyTorch ([*https://pytorch.tips/pytorch-tensorboard*](https://pytorch.tips/pytorch-tensorboard))
    |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch Geometric | Geometric deep learning extension library for PyTorch
    ([*https://pytorch.tips/geometric*](https://pytorch.tips/geometric)) |'
  prefs: []
  type: TYPE_TB
- en: '| Pyro | Flexible and extensible deep probabilistic modeling ([*https://pytorch.tips/pyro*](https://pytorch.tips/pyro))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Graph Library (DGL) | Library for implementation of graph neural networks
    ([*https://pytorch.tips/dgl*](https://pytorch.tips/dgl)) |'
  prefs: []
  type: TYPE_TB
- en: '| MMF | Facebook’s modular framework for multi-model deep learning (vision
    and language) ([*https://pytorch.tips/mmf*](https://pytorch.tips/mmf)) |'
  prefs: []
  type: TYPE_TB
- en: '| GPyTorch | Library for creating scalable Gaussian process models ([*https://pytorch.tips/gpytorch*](https://pytorch.tips/gpytorch))
    |'
  prefs: []
  type: TYPE_TB
- en: '| BoTorch | Library for Bayesian optimization ([*https://pytorch.tips/botorch*](https://pytorch.tips/botorch))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Torch Points 3D | Framework for unstructured 3D spatial data ([*https://pytorch.tips/torchpoints3d*](https://pytorch.tips/torchpoints3d))
    |'
  prefs: []
  type: TYPE_TB
- en: '| TensorLy | High level API for tensor methods and deep tensorized neural networks
    ([*https://pytorch.tips/tensorly*](https://pytorch.tips/tensorly))([*https://pytorch.tips/advertorch*](https://pytorch.tips/advertorch))
    |'
  prefs: []
  type: TYPE_TB
- en: '| BaaL | Implements active learning from Bayesian theory ([*https://pytorch.tips/baal*](https://pytorch.tips/baal))
    |'
  prefs: []
  type: TYPE_TB
- en: '| PennyLane | Library for quantum ML ([*https://pytorch.tips/pennylane*](https://pytorch.tips/pennylane))
    |'
  prefs: []
  type: TYPE_TB
- en: TensorBoard is a very popular visualization tool developed for TensorFlow that
    can be used for PyTorch as well. We’ll cover this tool and its PyTorch API later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Geometric, Pyro, GPyTorch, BoTorch, and BaaL all support different types
    of modeling, such as geometric, probabilistic, Gaussian modeling, and Bayesian
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Facebook’s MMF is a feature-rich package for multi-modal modeling, and Torch
    Points 3D can be used to model generic 3D spatial data.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch’s maturity and stability as a tool shows in the advent of packages used
    to support security and privacy. Security and privacy concerns are becoming more
    important as regulations require systems to be compliant in these domains.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-8](#table_eco_sec) provides a list of ecosystem projects that support
    *security and privacy*.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-8\. Security and privacy projects
  prefs: []
  type: TYPE_NORMAL
- en: '| Project | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AdverTorch | Modules for adversarial examples and defending against attacks
    |'
  prefs: []
  type: TYPE_TB
- en: '| PySyft | Library for model encryption and privacy ([*https://pytorch.tips/pysyft*](https://pytorch.tips/pysyft))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Opacus | Library for training models with differential privacy ([*https://pytorch.tips/opacus*](https://pytorch.tips/opacus))
    |'
  prefs: []
  type: TYPE_TB
- en: '| CrypTen | Framework for privacy preserving ML ([*https://pytorch.tips/crypten*](https://pytorch.tips/crypten))
    |'
  prefs: []
  type: TYPE_TB
- en: PySyft, Opacus, and CrypTen are PyTorch packages that support security and privacy.
    They add features to protect and encrypt models and the data used to create them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often deep learning seems like a black box, where developers have no idea why
    models make the decisions they make. Today, however, this lack of transparency
    is no longer acceptable: there is a growing awareness that companies and their
    executives must be held accountable for the fairness and operations of their algorithms.
    Model interpretability is important for researchers, developers, and company executives
    to understand why models produce their results.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-9](#table_eco_interpret) shows the ecosystem project that support
    *model* *interpretability*.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-9\. Model interpretability projects
  prefs: []
  type: TYPE_NORMAL
- en: '| Project | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Captum | Library for model interpretability ([*https://pytorch.tips/captum*](https://pytorch.tips/captum))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Visual attribution | PyTorch implementation of recent visual attribution
    methods for model interpretability ([*https://pytorch.tips/visual-attribution*](https://pytorch.tips/visual-attribution))
    |'
  prefs: []
  type: TYPE_TB
- en: Currently, Captum is the premier PyTorch project that supports model interpretability.
    The Visual attribution package is useful for interpreting computer vision models
    and identifying image saliency. As the field expands, more projects are sure to
    enter this space.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the PyTorch Ecosystem includes a broad range of open source
    projects that can assist you in many different ways. Perhaps you are working on
    a project that could benefit other researchers. If you’d like to make your project
    a part of the official PyTorch Ecosystem, visit the [PyTorch Ecosystem application
    page](https://pytorch.tips/join-ecosystem).
  prefs: []
  type: TYPE_NORMAL
- en: 'When considering applications, the PyTorch team looks for projects that meet
    the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Your project uses PyTorch to improve the user experience, add new capabilities,
    or speed up training/inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your project is stable, well maintained, and includes adequate infrastructure,
    documentation, and technical support.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Ecosystem is constantly growing. To access the latest list of projects,
    visit the [PyTorch Ecosystem website.](https://pytorch.tips/ecosystem) To update
    us on new projects for the book, please email the author at [jpapa@joepapa.ai](mailto:jpapa@joepapa.ai).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will go a little deeper into some of the PyTorch project’s supporting
    tools and libraries. We obviously can’t cover all of the available libraries and
    tools in this book, but in the following sections we’ll explore a few of the most
    popular and useful ones to give you a deeper understanding of their APIs and usage.
  prefs: []
  type: TYPE_NORMAL
- en: Torchvision for Image and Video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve used Torchvision through this book, and it is one of the most powerful
    and useful PyTorch libraries for computer vision research. Technically, the Torchvision
    package is part of the PyTorch project. It consists of a selection of popular
    datasets, model architectures, and common image transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets and I/O
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Torchvision provides a large assortment of datasets. They are included in the
    `torchvision.datasets` library and can be accessed by creating a dataset object,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You simply call the constructor function and pass in the appropriate options.
    This code creates a dataset object from the CIFAR-10 dataset using the training
    data with no transforms. It look for the dataset files in the current directory,
    and if they don’t exist, it will download them.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-10](#table_torchvision_datasets) provides a comprehensive list of
    datasets available from Torchvision.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-10\. Torchvision datasets
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CelebA | Large-scale face attributes dataset with more than 200,000 celebrity
    images, each with 40 attribute annotations. |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-10 | CIFAR-10 dataset consisting of 60,000 32 × 32 color images in
    10 classes, split into 50,000 training and 10,000 test images. The CIFAR-100 dataset,
    which has 100 classes, is also available. |'
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes | Large-scale dataset containing video sequences recorded in street
    scenes from 50 different cities, with annotations. |'
  prefs: []
  type: TYPE_TB
- en: '| COCO | Large-scale object detection, segmentation, and captioning dataset.
    |'
  prefs: []
  type: TYPE_TB
- en: '| DatasetFolder | Used to create any dataset from files in a folder structure.
    |'
  prefs: []
  type: TYPE_TB
- en: '| EMNIST | An extension of MNIST to handwritten letter. |'
  prefs: []
  type: TYPE_TB
- en: '| FakeData | A fake dataset that returns randomly generated images as PIL images.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fashion-MNIST | Dataset of Zalando’s clothing images matching the MNIST format
    (60,000 training examples, 10,000 test examples, 28 × 28 grayscale images, 10
    classes). |'
  prefs: []
  type: TYPE_TB
- en: '| Flickr | Flickr 8,000-image dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| HMDB51 | Large human motion database of video sequences. |'
  prefs: []
  type: TYPE_TB
- en: '| ImageFolder | Used to create an image dataset from files in a folder structure.
    |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet | Image classification dataset with 14,197,122 images and 21,841
    word phrases. |'
  prefs: []
  type: TYPE_TB
- en: '| Kinetics-400 | Large-scale action recognition video dataset with 650,000
    10-second video clips that cover up to 700 human action classes such as playing
    instruments, shaking hands, and hugging. |'
  prefs: []
  type: TYPE_TB
- en: '| KMNIST | Kuzushiji-MNIST, a drop-in replacement for the MNIST dataset (70,000
    28 × 28 grayscale images) where one character represents each of the 10 rows of
    Hiragana. |'
  prefs: []
  type: TYPE_TB
- en: '| LSUN | One million labeled images for each of 10 scene categories and 20
    object categories. |'
  prefs: []
  type: TYPE_TB
- en: '| MNIST | Handwritten, single-digit numbers as 28 × 28 grayscale images with
    60,000 training and 10,000 test samples. |'
  prefs: []
  type: TYPE_TB
- en: '| Omniglot | Human-generated dataset of 1,623 different handwritten characters
    from 50 different alphabets. |'
  prefs: []
  type: TYPE_TB
- en: '| PhotoTour | Photo tourism dataset consisting of 1,024 × 1,024 bitmap images,
    each containing a 16 × 16 array of image patches. |'
  prefs: []
  type: TYPE_TB
- en: '| Places365 | Dataset of 10 million images comprising 400+ unique scene categories
    with 5,000 to 30,000 training images per class. |'
  prefs: []
  type: TYPE_TB
- en: '| QMNIST | Facebook’s project to generate an MNIST dataset from the original
    data found in the NIST Special Database 19. |'
  prefs: []
  type: TYPE_TB
- en: '| SBD | Semantic boundaries dataset that contains annotations from 11,355 images
    for semantic segmentation. |'
  prefs: []
  type: TYPE_TB
- en: '| SBU | Stony Brook University (SBU) captioned photo dataset containing over
    1 million captioned images. |'
  prefs: []
  type: TYPE_TB
- en: '| STL10 | CIFAR-10-like dataset used for unsupervised learning. 10 classes
    of 96 × 96 color images with 5,000 training, 8,000 test, and 100,000 unlabeled
    images. |'
  prefs: []
  type: TYPE_TB
- en: '| SVHN | Street view house numbers dataset, similar to MNIST but with 10× more
    data in natural scene color images. |'
  prefs: []
  type: TYPE_TB
- en: '| UCF101 | Action recognition dataset with 13,320 videos from 101 action categories.
    |'
  prefs: []
  type: TYPE_TB
- en: '| USPS | Dataset of 16 × 16 handwritten text images with 10 classes, 7,291
    training and 2,007 test images. |'
  prefs: []
  type: TYPE_TB
- en: '| VOC | PASCAL visual object classes image datasets for object class recognition.
    The 2012 version has 20 classes, 11,530 training/validation images with 27,450
    region of interest (ROI) annotated objects and 6,929 segmentations. |'
  prefs: []
  type: TYPE_TB
- en: More datasets are being added to Torchvision all the time. For an up-to-date
    list, visit the [Torchvision documentation](https://pytorch.tips/torchvision-datasets).
  prefs: []
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Torchvision also provides an extensive list of models, containing both the
    module architectures and pretrained weights if available. The model object is
    easily created by calling the corresponding constructor function, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This code creates a VGG16 model with random weights since the pretrained weights
    are not used. You can instantiate many different computer vision models by using
    a similar constructor and setting the appropriate parameters. Torchvision provides
    pretrained models using the PyTorch `torch.utils.model_zoo`. These can be constructed
    by passing `pretrained=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-11](#table_torchvision_models) provides a comprehensive list of models
    included in Torchvision, by category. These models are well known in the research
    community, and the table includes references to the research papers associated
    with each model.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-11\. Torchvision models
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Paper |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Classification** |  |'
  prefs: []
  type: TYPE_TB
- en: '| AlexNet | “One Weird Trick for Parallelizing Convolutional Neural Networks,”
    by Alex Krizhevsky |'
  prefs: []
  type: TYPE_TB
- en: '| VGG | “Very Deep Convolutional Networks for Large-Scale Image Recognition,”
    by Karen Simonyan and Andrew Zisserman |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet | “Deep Residual Learning for Image Recognition,” by Kaiming He et
    al. |'
  prefs: []
  type: TYPE_TB
- en: '| SqueezeNet | “SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters
    and <0.5MB Model Size,” by Forrest N. Iandola et al. |'
  prefs: []
  type: TYPE_TB
- en: '| DenseNet | “Densely Connected Convolutional Networks,” by Gao Huang et al.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Inception v3 | “Rethinking the Inception Architecture for Computer Vision,”
    by Christian Szegedy et al. |'
  prefs: []
  type: TYPE_TB
- en: '| GoogLeNet | “Going Deeper with Convolutions,” by Christian Szegedy et al.
    |'
  prefs: []
  type: TYPE_TB
- en: '| ShuffleNet v2 | “ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture
    Design,” by Ningning Ma et al. |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNet v2 | “MobileNetV2: Inverted Residuals and Linear Bottlenecks,”
    by Mark Sandler et al. |'
  prefs: []
  type: TYPE_TB
- en: '| ResNeXt | “Aggregated Residual Transformations for Deep Neural Networks,”
    by Saining Xie et al. |'
  prefs: []
  type: TYPE_TB
- en: '| Wide ResNet | “Wide Residual Networks,” by Sergey Zagoruyko and Nikos Komodakis
    |'
  prefs: []
  type: TYPE_TB
- en: '| MNASNet | “MnasNet: Platform-Aware Neural Architecture Search for Mobile,”
    by Mingxing Tan et al. |'
  prefs: []
  type: TYPE_TB
- en: '| **Semantic segmentation** |  |'
  prefs: []
  type: TYPE_TB
- en: '| FCN ResNet50 | “Fully Convolutional Networks for Semantic Segmentation,”
    by Jonathan Long et al. |'
  prefs: []
  type: TYPE_TB
- en: '| FCN ResNet101 | See above |'
  prefs: []
  type: TYPE_TB
- en: '| DeepLabV3 ResNet50 | “Rethinking Atrous Convolution for Semantic Image Segmentation,”
    by Liang-Chieh Chen et al. |'
  prefs: []
  type: TYPE_TB
- en: '| DeepLabV3 ResNet101 | See above |'
  prefs: []
  type: TYPE_TB
- en: '| **Object detection** |  |'
  prefs: []
  type: TYPE_TB
- en: '| Faster R-CNN ResNet-50 | “FPNFaster R-CNN: Towards Real-Time Object Detection
    with Region Proposal Networks,” by Shaoqing Ren et al. |'
  prefs: []
  type: TYPE_TB
- en: '| Mask R-CNN ResNet-50 FPN | “Mask R-CNN,” by Kaiming He et al. |'
  prefs: []
  type: TYPE_TB
- en: '| **Video classification** |  |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet 3D 18 | “A Closer Look at Spatiotemporal Convolutions for Action Recognition,”
    by Du Tran et al. |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet MC 18 | See above |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet (2+1)D | See above |'
  prefs: []
  type: TYPE_TB
- en: New computer vision models are also being added to Torchvision all the time.
    For an up-to-date list, visit the [Torchvision documentation](https://pytorch.tips/torchvision-models).
  prefs: []
  type: TYPE_NORMAL
- en: Transforms, Operations, and Utilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Torchvision also provides a comprehensive collection of transforms, operations,
    and utilities to assist in image preprocessing and data preparation. A common
    approach to applying transforms is to form a composition of transforms and pass
    this `transforms` object into the dataset constructor function, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create a composite transform that converts the data to a tensor using
    `ToTensor()` then normalizes the image data using predetermined means and standard
    deviations for each channel. Setting the `transform` parameter to this `train_transforms`
    object configures the dataset to apply the sequence of transforms when data is
    accessed.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-12](#table_torchvision_transforms) provides a complete list of available
    transforms from `torchvision.transforms`. Transforms that appear in *`italics`*
    in this and [Table 8-13](#table_torchvision_transforms_pil) are currently not
    supported by TorchScript.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-12\. Torchvision transforms
  prefs: []
  type: TYPE_NORMAL
- en: '| Transform | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Operational transforms** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Compose()` | Creates a transform based on a sequence of other transforms
    |'
  prefs: []
  type: TYPE_TB
- en: '| `CenterCrop(size)` | Crops an image in the center with the given size |'
  prefs: []
  type: TYPE_TB
- en: '| `ColorJitter(brightness=0,` `contrast=0,` `saturation=0, hue=0)` | Randomly
    changes the brightness, contrast, saturation, and hue of an image |'
  prefs: []
  type: TYPE_TB
- en: '| `FiveCrop(size)` | Crops an image into four corners and the center crop |'
  prefs: []
  type: TYPE_TB
- en: '| `Grayscale(num_output_channels=1)` | Converts a color image to grayscale
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Pad(`*`padding`*`,` `fill=0,` `padding_mode=constant)` | Pads the edges
    of an image with the given value |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomAffine(`*`degrees`*`,` `translate=None,` `scale=None, shear=None,
    resample=0, fillcolor=0)` | Randomly applies an affine transformation |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomApply(transforms, p=0.5)` | Randomly applies a list of transforms
    with a given probability |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomCrop(`*`size`*`,` `padding=None, pad_if_needed=False, fill=0,` `padding_mode=constant)`
    | Crops an image at a random location |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomGrayscale(p=0.1)` | Randomly converts an image to grayscale with a
    given probability |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomHorizontalFlip(p=0.5)` | Randomly flips an image horizontally with
    a given probability |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomPerspective(distor⁠⁠tion_​scale=0.5, p=0.5,` `interpolation=2,` `fill=0)`
    | Applies a random perspective transformation |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomResizedCrop(`*`size`*`,` `scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333),`
    `interpolation=2)` | Resizes an image with a random size and aspect ratio |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomRotation(`*`degrees`*`,` `resample=False,` `expand=False,` `center=None,`
    `fill=None)` | Rotates an image randomly |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomVerticalFlip(p=0.5)` | Randomly flips an image vertically with a given
    probability |'
  prefs: []
  type: TYPE_TB
- en: '| `Resize(`*`size`*`,` `interpolation=2)` | Resizes an image to a random size
    |'
  prefs: []
  type: TYPE_TB
- en: '| `TenCrop(`*`size`*`,` `vertical_flip=False)` | Crops an image into four corners
    and the center crop and additionally provides a flipped version of each |'
  prefs: []
  type: TYPE_TB
- en: '| `GaussianBlur(`*`kernel_size`*`,` `sigma=(0.1, 2.0))` | Applies a Gaussian
    blur with a random kernel |'
  prefs: []
  type: TYPE_TB
- en: '| **Conversion transforms** |  |'
  prefs: []
  type: TYPE_TB
- en: '| *`ToPILImage(mode=None)`* | Converts a tensor or `numpy.ndarray` to a PIL
    image |'
  prefs: []
  type: TYPE_TB
- en: '| *`ToTensor()`* | Converts a PIL image or `ndarray` to a tensor |'
  prefs: []
  type: TYPE_TB
- en: '| **Generic transforms** |  |'
  prefs: []
  type: TYPE_TB
- en: '| *`Lambda(lambda)`* | Applies a user-defined `lambda` as a transform |'
  prefs: []
  type: TYPE_TB
- en: Most of the transforms can operate on images in tensor or PIL format with a
    `[..., C, H, W]` shape, where `...` means an arbitrary number of leading dimensions.
    However, some transforms only operate on PIL images or tensor image data.
  prefs: []
  type: TYPE_NORMAL
- en: The transforms listed in [Table 8-13](#table_torchvision_transforms_pil) operate
    only on PIL images. These transforms are currently not supported by TorchScript.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-13\. Torchvision PIL-only transforms
  prefs: []
  type: TYPE_NORMAL
- en: '| Transform | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *`RandomChoice(transforms)`* | Applies a single transform picked randomly
    from a list |'
  prefs: []
  type: TYPE_TB
- en: '| *`RandomOrder(transforms)`* | Applies a sequence of transforms in random
    order |'
  prefs: []
  type: TYPE_TB
- en: The transforms listed in [Table 8-14](#table_torchvision_transforms_tensor)
    operate only on tensor images.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-14\. Torchvision tensor-only transforms
  prefs: []
  type: TYPE_NORMAL
- en: '| Transform | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `LinearTransformation(​trans⁠⁠formation_matrix,` `mean_vector)` | Applies
    a linear transformation to a tensor image based on a square transformation matrix
    and a `mean_vector` computed offline. |'
  prefs: []
  type: TYPE_TB
- en: '| `Normalize(mean, std, inplace=False)` | Normalizes a tensor image with a
    given mean and standard deviation. |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)`
    | Randomly chooses a rectangle region and erases its pixels. |'
  prefs: []
  type: TYPE_TB
- en: '| `ConvertImageDtype(dtype: torch.dtype)` | Converts a tensor image to a new
    data type and automatically scales its values to match the type |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Use `torch.nn.Sequential()` instead of `torchvi⁠sion.​transforms.Compose()`
    when scripting transforms for C++ usage. The following code shows an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Many of the transforms listed in the previous tables contain a random number
    generator for specifying the parameter. For example, `RandomResizedCrop()` crops
    an image to a random size and aspect ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Torchvision also provides functional transforms as part of the `torchvision.transforms.functional`
    package. You can use these transforms to perform transformations with a specific
    set of parameters that you choose. For example, you could call `torchvision.transforms.functional.adjust_brightness()`
    to adjust the brightness of one on more images.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-15](#table_torchvision_functional) provides a list of the supported
    functional transforms.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-15\. Torchvision functional transforms
  prefs: []
  type: TYPE_NORMAL
- en: '| Functional transforms and utilities |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| `adjust_brightness(`*`img: torch.Tensor,`* *`brightness_factor: float`*`)`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `adjust_contrast(`*`img: torch.Tensor,`* *`contrast_factor: float`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `adjust_gamma(`*`img: torch.Tensor, gamma: float,`* *`gain: float = 1`*`)`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `adjust_hue(`*`img: torch.Tensor, hue_factor: float`*`)` → *`torch.Tensor`*
    |'
  prefs: []
  type: TYPE_TB
- en: '| `adjust_saturation(`*`img: torch.Tensor,`* *`saturation_factor: float`*`)`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `affine(`*`img: torch.Tensor, angle: float,`* *`translate: List[int],`* *`scale:
    float, shear: List[float],`* *`resample: int = 0, fillcolor: Optional[int] = None`*`)`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `center_crop(`*`img: torch.Tensor, output_size: List[int]`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `convert_image_dtype(`*`image: torch.Tensor,`* *`dtype: torch.dtype = torch.float32`*`)`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `crop(`*`img: torch.Tensor, top: int, left: int,`* *`height: int, width:
    int`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `erase(`*`img: torch.Tensor, i: int, j: int, h: int,`* *`w: int, v: torch.Tensor,
    inplace: bool = False`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `five_crop(`*`img: torch.Tensor, size: List[int]`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `gaussian_blur(`*`img: torch.Tensor, kernel_size: List[int], sigma: Optional[List[float]]
    = None`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `hflip(`*`img: torch.Tensor`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `normalize(`*`tensor: torch.Tensor, mean: List[float], std: List[float],
    inplace: bool = False`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `pad(`*`img: torch.Tensor, padding: List[int],`* *`fill: int = 0, padding_mode:
    str = constant`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `perspective(`*`img: torch.Tensor, startpoints: List[List[int]], endpoints:
    List[List[int]],`* *`interpolation: int = 2, fill: Optional[int] = None`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `pil_to_tensor(`*`pic`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `resize(`*`img: torch.Tensor, size: List[int],`* *`interpolation: int = 2`*`)`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `resized_crop(`*`img: torch.Tensor, top: int, left: int, height: int, width:
    int, size: List[int],`* *`interpolation: int = 2`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `rgb_to_grayscale(`*`img: torch.Tensor,`* *`num_output_channels: int = 1`*`)`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `rotate(`*`img: torch.Tensor, angle: float,`* *`resample: int = 0, expand:
    bool = False,`* *`center: Optional[List[int]] = None,`* *`fill: Optional[int]
    = None`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `ten_crop(`*`img: torch.Tensor, size: List[int],`* *`vertical_flip: bool
    = False`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `to_grayscale(`*`img, num_output_channels=1`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `to_pil_image(`*`pic, mode=None`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `to_tensor(`*`pic`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `vflip(`*`img: torch.Tensor`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `utils.save_image(`*`tensor: Union[torch.Tensor, List[torch.Tensor]], fp:
    Union[str, pathlib.Path,`* *`BinaryIO],`* *`nrow: int = 8, padding: int = 2, normalize:
    bool = False, range: Optional[Tuple[int, int]] = None, scale_each: bool = False,
    pad_value: int = 0, format: Optional[str] = None`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `utils.make_grid(`*`tensor: Union[torch.Tensor, List[torch.Tensor]], nrow:
    int = 8, padding: int = 2, normalize: bool = False, range: Optional[Tuple[int,
    int]] = None, scale_each: bool = False,`* *`pad_value: int = 0`*`)` |'
  prefs: []
  type: TYPE_TB
- en: As you can see in the table above, Torchvision provides a robust set of functional
    operations that you can use to process your image data. Each one has its own set
    of parameters for robust control.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Torchvision provides functions to facilitate I/O and operations.
    [Table 8-16](#table_torchvision_io_ops) provides a list of some of these functions.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-16\. Torchvision functions for I/O and operations
  prefs: []
  type: TYPE_NORMAL
- en: '| Function |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Video** |'
  prefs: []
  type: TYPE_TB
- en: '| `io.read_video(`*`filename: str, start_pts: int = 0, end_pts: Optional[float]
    = None, pts_unit: str = pts`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `io.read_video_timestamps9`*`filename: str, pts_unit: str = pts`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `io.write_video9`*`filename: str, video_array:`* *`torch.Tensor,`* *`_fps:
    float, video_codec: str = *libx264*, options: Optional[Dict[str, Any]] = None`*`)`
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Fine-grained video** |'
  prefs: []
  type: TYPE_TB
- en: '| `io.VideoReader(`*`path, stream=video`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| **Image** |'
  prefs: []
  type: TYPE_TB
- en: '| `io.decode_image(`*`input: torch.Tensor`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `io.encode_jpeg(`*`input: torch.Tensor, quality: int = 75`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `io.read_image(`*`path: str`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `io.write_jpeg(`*`input: torch.Tensor, filename: str,`* *`quality: int =
    75`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `io.encode_png(`*`input: torch.Tensor, compression_level: int = 6`*`)` |'
  prefs: []
  type: TYPE_TB
- en: '| `io.write_png(`*`input: torch.Tensor, filename: str,`* *`compression_level:
    int = 6`*`)` |'
  prefs: []
  type: TYPE_TB
- en: The preceding functions are provided so that you can quickly read and write
    video and image files in multiple formats. They allow you to speed up your image
    and video processing without the need to write these functions from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, Torchvision is a feature-rich, well-supported, and mature PyTorch
    package. This section provided a quick reference to the Torchvision API. In the
    next section, we’ll explore another popular PyTorch package for NLP and text applications
    called Torchtext.
  prefs: []
  type: TYPE_NORMAL
- en: Torchtext for NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Torchtext package consists of a collection of data-processing utilities
    and popular datasets for NLP. The Torchtext API is slightly different from the
    Torchvision API, but the overall approach is the same.
  prefs: []
  type: TYPE_NORMAL
- en: Create a Dataset Object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First you create a dataset and describe a preprocessing pipeline, as we did
    with Torchvision transforms. Torchtext provides a set of well-known datasets out
    of the box. For example, we can load the IMDb dataset as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We automatically create an iterator and can access the data using `next()`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Torchtext significantly changed its API in PyTorch 1.8\. If the code in this
    section returns errors, you may need to upgrade your version of PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Torchtext also provides features to preprocess text and create data pipelines.
    Preprocessing tasks may include defining tokenizers, vocabularies, and numerical
    embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the new Torchtext API, you can access different tokenizers using the `data.get_tokenizer()`
    function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating vocabularies in the new API is also flexible. You can build a vocabulary
    directly with the `Vocab` class, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we can set the `min_freq` to specify the cutoff frequency in
    the vocabulary. We can also assign tokens to special symbols like `<BOS>` and
    `<EOS>`, as shown in the constructor of the `Vocab` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful feature is to define transforms for text and labels, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We pass in a text string to our transforms, and we use the vocabulary and tokenizer
    to preprocess the data.
  prefs: []
  type: TYPE_NORMAL
- en: Create a Dataloader for Batching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have loaded and preprocessed our data, the last step is to create
    a dataloader to sample and batch data from the dataset. We can create a dataloader
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You may notice that this code is similar to the code with which we created a
    dataloader in Torchvision. Instead of passing in the dataset object, we pass the
    `train_iter` cast as a `list()`. The `DataLoader()` constructor also accepts `batch_sampler`
    and `collate_fcn` parameters (not shown in the preceding code; see the [documentation](https://pytorch.tips/data))
    so you can customize how the dataset is sampled and collated. After you create
    the dataloader, use it to train your model, as shown in the preceding code comments.
  prefs: []
  type: TYPE_NORMAL
- en: Torchtext has many useful features. Let’s explore what’s available from the
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Data (torchtext.data)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `torchtext.data` API provides functions for creating text-based dataset
    objects in PyTorch. [Table 8-17](#table_torchtext_data) lists the available functions
    in `torchtext.data`.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-17\. Torchtext data
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **`torchtext.data.utils`** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `get_tokenizer(`*`tokenizer,`* *`language=en`*`)` | Generates a tokenizer
    function for a string sentence |'
  prefs: []
  type: TYPE_TB
- en: '| `ngrams_itera⁠⁠tor(`*`token_​list, ngrams`*`)` | Returns an iterator that
    yields the given tokens and their ngrams |'
  prefs: []
  type: TYPE_TB
- en: '| **`torchtext.data.functional`** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `generate_sp_model(​`*`file⁠⁠name, vocab_size=20000, model_type=unigram,
    model_prefix=m_user`*`)` | Trains a `SentencePiece` tokenizer |'
  prefs: []
  type: TYPE_TB
- en: '| `load_sp_model(`*`spm`*`)` | Loads a `SentencePiece` model from a file |'
  prefs: []
  type: TYPE_TB
- en: '| `sentencepiece_​numeri⁠⁠cal⁠⁠izer(`*`sp_model`*`)` | Creates a generator
    that takes in a text sentence and outputs the corresponding identifiers based
    on a `SentencePiece` model |'
  prefs: []
  type: TYPE_TB
- en: '| `sentencepiece_​token⁠⁠izer(`*`sp_model`*`)` | Creates a generator that takes
    in a text sentence and outputs the corresponding tokens based on a `SentencePiece`
    model |'
  prefs: []
  type: TYPE_TB
- en: '| `custom_replace(`*`replace_​pat⁠⁠tern`*`)` | Acts as a transform to convert
    text strings |'
  prefs: []
  type: TYPE_TB
- en: '| `simple_space_split(​`*`itera⁠⁠tor`*`)` | Acts as a transform to split text
    strings by spaces |'
  prefs: []
  type: TYPE_TB
- en: '| `numerical⁠⁠ize_tokens_​from_iterator(`*`vocab,`* *`iterator,`* *`removed_tokens=None`*`)`
    | Yields a list of identifiers from a token iterator with a `vocab` |'
  prefs: []
  type: TYPE_TB
- en: '| **`torchtext.data.metrics`** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `bleu_score(`*`candidate_​cor⁠⁠pus, references_corpus, max_n=4, weights=[0.25,
    0.25, 0.25, 0.25]`*`)` | Computes the BLEU score between a candidate translation
    corpus and a reference translation corpus |'
  prefs: []
  type: TYPE_TB
- en: As you can see, the `torchtext.data` submodule supports functions for creating
    dataset objects based on fields aas well as for loading, preprocessing, and iterating
    through batches. Next let’s see what NLP datasets are available from the Torchtext
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets (torchtext.datasets)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Torchtext supports loading datasets from popular papers and research. You can
    find datasets for language modeling, sentiment analysis, text classification,
    question classification, entailment, machine translation, sequence tagging, question
    answering, and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-18](#table_torchtext_datasets) provides a comprehensive list of the
    datasets included in Torchtext.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-18\. Torchtext datasets
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Text classification** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `TextClassificationDataset(`*`vocab, data, labels`*`)` | Generic text-classification
    dataset |'
  prefs: []
  type: TYPE_TB
- en: '| `IMDB(`*`root=*.data*, split=`*`(`*`*train*, *test*`*`))` | Binary sentiment
    analysis dataset consisting of 50,000 reviews labeled as positive or negative
    from IMDb |'
  prefs: []
  type: TYPE_TB
- en: '| `AG_NEWS(`*`root=*.data*, split=`*`(`*`*train*, *test*`*`))` | Dataset of
    news articles labeled with four topics |'
  prefs: []
  type: TYPE_TB
- en: '| `SogouNews(`*`root=*.data*, split=(*train*, *test*`*`))` | Dataset of news
    articles labeled with five topics |'
  prefs: []
  type: TYPE_TB
- en: '| `DBpedia(`*`root=*.data*, split=(*train*, *test*`*`))` | Dataset of news
    articles labeled with 14 categories |'
  prefs: []
  type: TYPE_TB
- en: '| `YelpReviewPolarity(`*`root=*.data*, split=(*train*, *test*`*`))` | Dataset
    of 500,000 Yelp reviews with binary classification |'
  prefs: []
  type: TYPE_TB
- en: '| `YelpReviewFull(`*`root=*.data*, split=(*train*, *test*`*`))` | Dataset of
    500,000 Yelp reviews with fine-grained (five-class) classification |'
  prefs: []
  type: TYPE_TB
- en: '| `YahooAnswers(`*`root=*.data*, split=`*`(`*`*train*, *test*`*`))` | Dataset
    of Yahoo answers labeled in 10 different categories |'
  prefs: []
  type: TYPE_TB
- en: '| `AmazonReviewPolarity(`*`root=*.data*, split=`*`(`*`*train*, *test*`*`))`
    | Dataset of Amazon reviews with binary classification |'
  prefs: []
  type: TYPE_TB
- en: '| `AmazonReviewFull(`*`root=*.data*, split=`*`(`*`*train*, *test*`*`))` | Dataset
    of Amazon reviews with fine-grained (five-class) classification |'
  prefs: []
  type: TYPE_TB
- en: '| **Language modeling** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `LanguageModelingDataset(`*`path, text_field, newline_eos=True,`* *`encoding=*utf-8*,
    **kwargs`*`)` | General language modeling dataset class |'
  prefs: []
  type: TYPE_TB
- en: '| `WikiText2(`*`root=*.data*, split=`*`(`*`*train*, *valid*, *test*`*`))` |
    WikiText long-term dependency language modeling dataset, a collection of over
    100 million tokens extracted from the set of verified “Good” and “Featured” articles
    on Wikipedia |'
  prefs: []
  type: TYPE_TB
- en: '| `WikiText103(`*`root=*.data*, split=`*`(`*`*train*, *valid*, *test*`*`))`
    | Larger WikiText dataset |'
  prefs: []
  type: TYPE_TB
- en: '| `PennTreebank(`*`root=*.data*, split=`*`(`*`*train*, *valid*, *test*`*`))`
    | A relatively small dataset originally created for part of speech (POS) tagging
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Machine translation** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `TranslationDataset(`*`path, exts, fields, **kwargs`*`)` | Generic translation
    dataset class |'
  prefs: []
  type: TYPE_TB
- en: '| `IWSLT2016(`*`root=*.data*, split=`*`(`*`*train*, *valid*, *test*`*`)`*`,`*
    *`language_pair=`*`(`*`*de*, *en*`*`)`*`,`* *`valid_set=*tst2013*, test_set=*tst2014*`*`)`
    | International Conference on Spoken Language Translation (IWSLT) 2016 TED talk
    translation task |'
  prefs: []
  type: TYPE_TB
- en: '| `IWSLT2017(`*`root=*.data*, split=`*`(`*`*train*, *valid*, *test*`*`)`*`,`*
    *`language_pair=`*`(`*`*de*, *en*`*`))` | International Conference on Spoken Language
    Translation (IWSLT) 2017 TED talk translation task |'
  prefs: []
  type: TYPE_TB
- en: '| **Sequence tagging** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `SequenceTaggingDataset(`*`path, fields, encoding=*utf-8*,`* *`separator=*t*,
    **kwargs`*`)` | Generic sequence-tagging dataset class |'
  prefs: []
  type: TYPE_TB
- en: '| `UDPOS(`*`root=*.data*, split=`*`(`*`*train*, *valid*, *test*`*`))` | Universal
    dependencies version 2 POS-tagged data |'
  prefs: []
  type: TYPE_TB
- en: '| `CoNLL2000Chunking(`*`root=*.data*, split=`*`(`*`*train*, *test*`*`))` |
    Command that downloads and loads the Conference on Computational Natural Language
    Learning (CoNLL) 2000 chunking dataset |'
  prefs: []
  type: TYPE_TB
- en: '| **Question answering** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `SQuAD1(`*`root=*.data*, split=`*`(`*`*train*, *dev*`*`))` | Creates the
    Stanford Question Answering Dataset (SQuAD) 1.0 dataset, a reading comprehension
    dataset consisting of questions posed by crowdworkers on a set of Wikipedia articles
    |'
  prefs: []
  type: TYPE_TB
- en: '| `SQuAD2(`*`root=.data, split=`*`(`*`train, dev`*`))` | Creates the Stanford
    Question Answering Dataset (SQuAD) 2.0 dataset, a dataset that extends the 1.0
    dataset by adding over 50,000 unanswerable questions |'
  prefs: []
  type: TYPE_TB
- en: Torchtext developers are always adding new datasets. For the most updated list,
    visit the [Torchtext datasets documentation](https://pytorch.tips/torchtext-datasets).
  prefs: []
  type: TYPE_NORMAL
- en: Once you load data, whether from existing datasets or ones that you create,
    you will need to convert the text data to numeric data before training a model
    and running inference. To do so, we use vocabularies and word embeddings that
    provide the maps to perform these conversions. Next, we’ll examine the Torchtext
    functions used to support vocabularies.
  prefs: []
  type: TYPE_NORMAL
- en: Vocabularies (torchtext.vocab)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Torchtext provides generic classes and specific classes for popular vocabularies.
    [Table 8-19](#table_torchtext_vocab) provides a list of classes in `torchtext.vocab`
    to support the creation and use of vocabularies.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-19\. Torchtext vocabularies
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Vocabulary classes** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Vocab(counter, max_size=None, min_freq=1,` `specials=(<unk>,` `<pad>),`
    *`vectors=None,`* *`unk_init=None,`* *`vectors_cache=None,`* *`specials_first=True`*`)`
    | Defines a vocabulary object that will be used to numericalize a field |'
  prefs: []
  type: TYPE_TB
- en: '| `SubwordVocab(`*`counter, max_size=None,`* *`specials=<pad>,`* *`vectors=None,`*
    *`unk_init=<method zero_of torch._C._TensorBase objects>`*`)` | Creates a `revtok`
    subword vocabulary from a `collections.Counter` |'
  prefs: []
  type: TYPE_TB
- en: '| `Vectors(`*`name, cache=None, url=None, unk_init=None,`* *`max_vectors=None`*`)`
    | Generic class for word vector embeddings |'
  prefs: []
  type: TYPE_TB
- en: '| **Pretrained word embeddings** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `GloVe(`*`name=*840B*, dim=300, **kwargs`*`)` | Global vectors (GloVe) model
    for distributed word representation, developed at Stanford |'
  prefs: []
  type: TYPE_TB
- en: '| `FastText(`*`language=en, **kwargs`*`)` | Pretrained word embeddings for
    294 languages, created by Facebook’s AI Research lab |'
  prefs: []
  type: TYPE_TB
- en: '| `CharNGram(`*`**kwargs`*`)` | CharNGram embeddings, a simple approach for
    learning character-based compositional models to embed textual sequences |'
  prefs: []
  type: TYPE_TB
- en: '| **Miscellaneous** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `build_vocab_from_​itera⁠⁠tor(​`*`iter⁠⁠ator, num_lines=None`*`)` | Builds
    a vocabulary by cycling through an iterator |'
  prefs: []
  type: TYPE_TB
- en: As you can see, Torchtext provides a robust set of functionality to support
    text-based modeling and NLP research. For more information, visit the [Torchtext
    documentation](https://pytorch.tips/torchtext).
  prefs: []
  type: TYPE_NORMAL
- en: Whether you’re developing deep learning models for NLP, computer vision, or
    another field, it’s helpful to be able to visualize models, data, and performance
    metrics as you go. In the next section, we’ll explore another powerful package
    for visualization called TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard for Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorBoard is a visualization toolkit that’s included in PyTorch’s major competing
    deep learning framework, TensorFlow. Instead of developing its own visualization
    toolkit, PyTorch integrates with TensorBoard and leverages its visualization capabilities
    natively.
  prefs: []
  type: TYPE_NORMAL
- en: With TensorBoard, you can visualize learning curves, scalar data, model architectures,
    weight distributions, and 3D data embeddings, as well as keep track of hyperparameter
    experiment results. This section will show you how to use TensorBoard with PyTorch
    and provide a reference to the TensorBoard API.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorBoard application is run on a local or remote server, and the display
    and user interface run in a browser. We can also run TensorBoard inside Jupyter
    Notebook or Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll use Colab in this book to demonstrate the capabilities of TensorBoard,
    but the process is very similar for running it locally or remotely in the cloud.
    Colab comes with TensorBoard preinstalled, and you can run it directly in a cell
    using magic commands, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: First we load the `tensorboard` extension and then we run `tensorboard` and
    specify the log directory that holds the event files. Event files hold the data
    from PyTorch that will be displayed in the TensorBoard application.
  prefs: []
  type: TYPE_NORMAL
- en: Since we haven’t created any event files yet, you will see an empty display,
    as shown in [Figure 8-1](#fig_0801).
  prefs: []
  type: TYPE_NORMAL
- en: '![“TensorBoard Application”](Images/ptpr_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. TensorBoard application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By clicking on the arrow next to INACTIVE in the upper-right menu, you will
    see the possible display tabs. One commonly used display tab is the SCALARS tab.
    This tab can display any scalar value over time. We often use the SCALARS display
    to view loss and accuracy training curves. Let’s see how you can save scalar values
    for TensorBoard in your PyTorch code.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The  PyTorch  integration  with  TensorBoard  was  originally  implemented  by
     an  open  source  project  called  TensorBoardX.  Since  then,  TensorBoard  support
     has  been  integrated into the PyTorch project as the `torch.utils.tensorboard`
     package  and  it’s  actively  maintained  by  the  PyTorch  development  team.
  prefs: []
  type: TYPE_NORMAL
- en: 'First let’s import PyTorch’s TensorBoard interface and set up PyTorch for use
    with TensorBoard, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_the_pytorch_ecosystem_and_additional_resources_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The writer will output to the *./runs/* directory by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'We simply import the `SummaryWriter` class from the PyTorch `tensorboard` package
    and instantiate a `SummaryWriter` object. To write data to TensorBoard, all we
    need to do is call methods from the `SummaryWriter` object. To save our loss values
    while our model is training, we use the `add_scalar()` method, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_the_pytorch_ecosystem_and_additional_resources_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Log `loss.item()` as an event to `tensorboard`.
  prefs: []
  type: TYPE_NORMAL
- en: This is just an example training loop. You can assume the `model` has already
    been defined and the `trainloader` has been created. Not only does the code print
    the loss every epoch, but it also logs it to a `tensorboard` event. We can either
    refresh the TensorBoard application in the previous cell or create another cell
    altogether using the `%tensorboard` command.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Curves with SCALARS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorBoard provides the ability to plot one or more scalar values over time.
    This is useful in deep learning development to display metrics as your model trains.
    By viewing metrics like loss or accuracy it’s easy to see if your model’s training
    is stable and continues to improve.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-2](#fig_0802) shows an example display of learning curves using TensorBoard.'
  prefs: []
  type: TYPE_NORMAL
- en: You can interact with the display by sliding the smoothing factor, and you can
    also see the curve at each epoch by mousing over the plots. TensorBoard allows
    you to apply smoothing to iron out instabilities and show the overall progress.
  prefs: []
  type: TYPE_NORMAL
- en: '![“Visualizing Learning Curves with TensorBoard”](Images/ptpr_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. TensorBoard learning curves
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Model Architectures with GRAPHS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another useful feature of TensorBoard is visualizing your deep learning model
    using graphs. To save a graph to the event file, we will use the `add_graph()`
    method, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this code we instantiate a VGG16 model and write the model to an event file.
    We can display the model graph by either refreshing an existing TensorBoard cell
    or creating a new one. [Figure 8-3](#fig_0803) shows the graph visualization tool
    in TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: '![“Visualizing Model Graphs with TensorBoard”](Images/ptpr_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. TensorBoard model graph
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The graph is interactive. You can click on each module and expand it to view
    the underlying modules. This tool is useful for understanding existing models
    and verifying that your model graphs match their intended designs.
  prefs: []
  type: TYPE_NORMAL
- en: Data with IMAGES, TEXT, and PROJECTOR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also use TensorBoard to view different types of data, such as images,
    text, and 3D embeddings. In these cases, you would use the `add_image()`, `add_text()`,
    and `add_projection()` methods, respectively, to write data to the event file.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-4](#fig_0804) shows a batch of image data from the Fashion-MNIST
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: By examining batches of image data, you can verify that the data looks as expected
    or identify errors in your data or results. TensorBoard also provides the ability
    to listen to audio data, display text data, and view 3D projections of multidimensional
    data or data embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![“Visualizing Image Data with TensorBoard”](Images/ptpr_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. TensorBoard image display
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Weight Distributions with DISTRIBUTIONS and HISTOGRAMS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another useful feature of TensorBoard is the ability to display distributions
    and histograms. This allows you to view large amounts of data to verify expected
    behavior or identify issues.
  prefs: []
  type: TYPE_NORMAL
- en: One common task in model development is making sure you avoid the *vanishing
    gradient problem*. Vanishing gradients occur when the model weights become zero
    or close to zero. When this occurs the neurons essentially die off and can no
    longer be updated.
  prefs: []
  type: TYPE_NORMAL
- en: If we visualize the distribution of our weights, it’s easy to see when a large
    portion of the weight values have reached zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-5](#fig_0805) shows the DISTRIBUTIONS tab in TensorBoard. Here we
    can examine the distributions of our weight values.'
  prefs: []
  type: TYPE_NORMAL
- en: As you see in [Figure 8-5](#fig_0805), TensorBoard can display distributions
    in 3D so it’s easy to see how the distributions change over time or over each
    epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '![“Weight Distributions with TensorBoard”](Images/ptpr_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. TensorBoard weight distributions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Hyperparameters with HPARAMS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When running deep learning experiments, it’s easy to lose track of the different
    hyperparameter sets used to try a hypothesis. TensorBoard provides a way to keep
    track of the hyperparameter values during each experiment and tabularizes the
    values and their results.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-6](#fig_0806) displays an example of how we track experiments and
    their corresponding hyperparameters and results.'
  prefs: []
  type: TYPE_NORMAL
- en: In the HPARAMS tab, you can view the results in table view, parallel coordinates
    view, or scatter plot matrix view. Each experiment is identified by its session
    group name, hyperparameters such as dropout percentage and optimizer algorithm,
    and the resulting metric, such as accuracy. The HPARAMS tables help you keep track
    of your experiments and results.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you’re finished writing data to TensorBoard event files, you should use
    the `close()` method, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This will call the destructor function and release any memory that was used
    for the summary writer.
  prefs: []
  type: TYPE_NORMAL
- en: '![“Tracking Hyperparameters in TensorBoard”](Images/ptpr_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. TensorBoard hyperparameter tracking
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The TensorBoard API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The PyTorch TensorBoard API is pretty simple. It’s included as part of the `torch.utils`
    package as `torch.utils.tensorboard`. [Table 8-20](#table_tensorboard_api) shows
    a comprehensive list of functions used to interface PyTorch to TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-20\. PyTorch TensorBoard API
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `SummaryWriter(`*`log_dir=None, comment='''', purge_step=None, max_queue=10,
    flush_secs=120, filename_suffix=''''`*`)` | Creates a `SummaryWriter` object |'
  prefs: []
  type: TYPE_TB
- en: '| `flush()` | Flushes the event file to disk; makes sure that all pending events
    have been written to disk |'
  prefs: []
  type: TYPE_TB
- en: '| `close()` | Frees the `SummaryWriter` object and closes event files |'
  prefs: []
  type: TYPE_TB
- en: '| `add_scalar(`*`tag, scalar_value, global_step=None, walltime=None`*`)` |
    Writes a scalar to the event file |'
  prefs: []
  type: TYPE_TB
- en: '| `add_scalars(`*`main_tag, tag_scalar_dict, global_step=None, walltime=None`*`)`
    | Writes multiple scalars to the event file to display multiple scalars on the
    same plot |'
  prefs: []
  type: TYPE_TB
- en: '| `add_custom_scalars(`*`layout`*`)` | Creates a special chart by collecting
    chart tags in scalars |'
  prefs: []
  type: TYPE_TB
- en: '| `add_histogram(`*`tag, values, global_step=None, bins=tensorflow, walltime=None,
    max_bins=None`*`)` | Writes data for a histogram display |'
  prefs: []
  type: TYPE_TB
- en: '| `add_image(`*`tag, img_tensor, global_step=None, walltime=None, dataformats=CHW`*`)`
    | Writes image data |'
  prefs: []
  type: TYPE_TB
- en: '| `add_images(`*`tag, img_tensor, global_step=None, walltime=None, dataformats=NCHW`*`)`
    | Writes multiple images to the same display |'
  prefs: []
  type: TYPE_TB
- en: '| `add_figure(`*`tag, figure, global_step=None, close=True, walltime=None`*`)`
    | Writes a `matplotlib`-type plot as an image |'
  prefs: []
  type: TYPE_TB
- en: '| `add_video(`*`tag, vid_tensor, global_step=None, fps=4, walltime=None``)`*
    | Writes a video |'
  prefs: []
  type: TYPE_TB
- en: '| `add_audio(`*`tag, snd_tensor, global_step=None, sample_rate=44100, walltime=None`*`)`
    | Writes an audio file to the event summary |'
  prefs: []
  type: TYPE_TB
- en: '| `add_text(`*`tag, text_string, global_step=None, walltime=None`*`)` | Writes
    text data to summary |'
  prefs: []
  type: TYPE_TB
- en: '| `add_graph(`*`model, input_to_model=None, verbose=False`*`)` | Writes a model
    graph or computational graph to summary |'
  prefs: []
  type: TYPE_TB
- en: '| `add_embedding(`*`mat, metadata=None, label_img=None, global_step=None, tag=default,
    metadata_header=None`*`)` | Writes embedding projector data tto summary |'
  prefs: []
  type: TYPE_TB
- en: '| `add_pr_curve(`*`tag, labels, predictions, global_step=None, num_thresholds=127,
    weights=None, walltime=None`*`)` | Writes the precision/recall curve under different
    thresholds |'
  prefs: []
  type: TYPE_TB
- en: '| `add_mesh(`*`tag, vertices, colors=None, faces=None, config_dict=None, global_step=None,
    walltime=None`*`)` | Adds meshes or 3D point clouds to TensorBoard |'
  prefs: []
  type: TYPE_TB
- en: '| `add_hparams(`*`hparam_dict, metric_dict, hparam_domain_discrete=None, run_name=None`*`)`
    | Adds a set of hyperparameters for comparison in TensorBoard |'
  prefs: []
  type: TYPE_TB
- en: As shown in [Table 8-20](#table_tensorboard_api), the API is simple. You can
    use the `SummaryWriter()`, `flush()`, and `close()` methods to manage the writer
    object and use the other functions to add data to the TensorBoard event file.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on the TensorBoard PyTorch API, visit the [TensorBoard API
    documentation](https://pytorch.tips/pytorch-tensorboard). For more details on
    using the TensorBoard application itself, visit the [TensorBoard documentation](https://pytorch.tips/tensorboard).
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard solves one major challenge with developing deep learning models
    in PyTorch by providing a visualization tool. Another major challenge is keeping
    up with the latest research and state-of-the art solutions. Researchers often
    need to reproduce the results and leverage the code for benchmarking their own
    designs. In the next section we explore Papers with Code, a resource you can use
    to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Papers with Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Papers with Code (PwC) is a website that organizes access to machine learning
    research papers and their corresponding code, which is often written in PyTorch.
    PwC allows you to easily reproduce experiments and extend current research, and
    the website allows you to find the best-performing research papers for a given
    machine learning topic. For example, want to find the best image classification
    models and their code? Just click on the Image Classification tile and you’ll
    see a summary of the research area as well as benchmarks and links to corresponding
    papers and code on GitHub. [Figure 8-7](#fig_0807) shows an example listing for
    Image Classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![“Papers With Code Image Classification Listing”](Images/ptpr_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Papers with Code
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PwC is not an exclusive PyTorch project; however, most of the code provided
    on PwC uses PyTorch. It may be able to help you build awareness of the current
    state-of-the-art research and solve your problems in deep learning and AI. Explore
    more at the [PwC website](https://pytorch.tips/pwc).
  prefs: []
  type: TYPE_NORMAL
- en: Additional PyTorch Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading this book, you should have a good understanding of PyTorch and
    its features. However, there are always new aspects to explore and practice. In
    this section, I’ll provide a list of additional resources that you can check out
    to learn more and grow your skills with PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [PyTorch website](https://pytorch.tips/pytorch) provides an extensive set
    of documentation and tutorials. If you’re looking for more code examples, this
    resource is a good place to start. [Figure 8-8](#fig_0808) shows the [PyTorch
    Tutorials website](https://pytorch.tips/tutorials), where you can select tags
    to help you find tutorials that interest you.
  prefs: []
  type: TYPE_NORMAL
- en: '![“PyTorch Tutorials Website”](Images/ptpr_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. PyTorch Tutorials
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The site includes a 60-min blitz, PyTorch recipes, tutorials, and a PyTorch
    Cheat Sheet. Most of the code and tutorials are available on GitHub, and can be
    run in VS Code, Jupyter Notebook, and Colab.
  prefs: []
  type: TYPE_NORMAL
- en: The 60-min Blitz is a good place to start, refresh your skills, or review the
    basics of PyTorch. PyTorch recipes are bite-sized, actionable examples of how
    to use specific PyTorch features. PyTorch tutorials are slightly longer than recipes
    and are composed of multiple steps to achieve or demonstrate an outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, you can find tutorials related to the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Audio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frontend APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image/Video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory Format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mobile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel and Distributed Training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TorchScript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PyTorch team is continually adding new resources and this list is certainly
    subject to change. For more information and the latest tutorials, visit the PyTorch
    Tutorials website.
  prefs: []
  type: TYPE_NORMAL
- en: Books
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tutorials are a great for learning, but perhaps you’d prefer to read more about
    PyTorch and gain different perspectives from multiple authors. [Table 8-21](#table_pytorch_books)
    provides a list of other books related to PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-21\. PyTorch books
  prefs: []
  type: TYPE_NORMAL
- en: '| Book | Publisher, year | Summary |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *Cloud Native Machine Learning* by Carl Osipov | Manning, 2021 | Learn how
    to deploy PyTorch models on AWS |'
  prefs: []
  type: TYPE_TB
- en: '| *Deep Learning for Coders with fastai and PyTorch* by Jeremy Howard and Sylvain
    Gugger | O’Reilly, 2020 | Learn how to build AI applications without a PhD |'
  prefs: []
  type: TYPE_TB
- en: '| *Deep Learning with PyTorch* by Eli Stevens et al. | Manning, 2019 | Learn
    how to build, train, and tune NNs using Python tools |'
  prefs: []
  type: TYPE_TB
- en: '| *Deep Learning with PyTorch* by Vishnu Subramanian | Packt, 2018 | Learn
    how to build NN models using PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| *Hands-On Generative Adversarial Networks with PyTorch 1.x* by John Hany
    and Greg Walters | Packt, 2019 | Learn how to implement next-generation NNs to
    build powerful GAN models using Python |'
  prefs: []
  type: TYPE_TB
- en: '| *Hands-On Natural Language Processing with PyTorch 1.x* by Thomas Dop | Packt,
    2020 | Learn how to build smart, AI-driven linguistic applications using deep
    learning and NLP techniques |'
  prefs: []
  type: TYPE_TB
- en: '| *Hands-On Neural Networks with PyTorch 1.0* by Vihar Kurama | Packt, 2019
    | Learn how to implement deep learning architectures in PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| *Natural Language Processing with PyTorch* by Delip Rao and Brian McMahan
    | O’Reilly, 2019 | Learn how to build intelligent language applications using
    deep learning |'
  prefs: []
  type: TYPE_TB
- en: '| *Practical Deep Learning with PyTorch* by Nihkil Ketkar | Apress, 2020 |
    Learn how to optimize GANs with Python |'
  prefs: []
  type: TYPE_TB
- en: '| *Programming PyTorch for Deep Learning* by Ian Pointer | O’Reilly, 2019 |
    Learn how to create and deploy deep learning applications |'
  prefs: []
  type: TYPE_TB
- en: '| *PyTorch Artificial Intelligence Fundamentals* by Jibin Mathew | Packt, 2020
    | Learn how to design, build, and deploy your own AI models with PyTorch 1.x |'
  prefs: []
  type: TYPE_TB
- en: '| *PyTorch Recipes* by Pradeepta Mishra | Apress, 2019 | Learn how to solve
    problems in PyTorch |'
  prefs: []
  type: TYPE_TB
- en: Online Courses and Live Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you prefer online video courses and live training workshops, there are options
    available for you to expand your PyTorch knowledge and skills. You can continue
    learning from me and other online instructors at PyTorch Academy, Udemy, Coursera,
    Udacity, Skillshare, DataCamp, Pluralsight, edX, O’Reilly Learning, and LinkedIn
    Learning. Some courses are free while others require a fee or a subscription.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-22](#table_pytorch_courses) lists a selection of online courses on
    PyTorch available at the time of writing.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-22\. PyTorch courses
  prefs: []
  type: TYPE_NORMAL
- en: '| Course | Instructor | Platform |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Getting Started with PyTorch Development | Joe Papa | [PyTorch Academy](https://pytorchacademy.com)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch Fundamentals | Joe Papa | [PyTorch Academy](https://pytorchacademy.com)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Advanced PyTorch | Joe Papa | [PyTorch Academy](https://pytorchacademy.com)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Introduction to Deep Learning with PyTorch | Ismail Elezi | [DataCamp](https://www.datacamp.com/courses/deep-learning-with-pytorch)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Foundations of PyTorch | Janani Ravi | [Pluralsight](https://www.pluralsight.com/courses/foundations-pytorch)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Neural Networks with PyTorch | IBM | [Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch Basics for Machine Learning | IBM | [edX](https://www.edx.org/course/pytorch-basics-for-machine-learning)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Intro to Deep Learning with PyTorch | Facebook AI | [Udacity](https://www.udacity.com/course/deep-learning-pytorch%E2%80%94ud188)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch: Deep Learning and Artificial Intelligence | Lazy Programmer | [Udemy](https://www.udemy.com/course/pytorch-deep-learning/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch for Deep Learning and Computer Vision | Rayan Slim et al. | [Udemy](https://www.udemy.com/course/pytorch-for-deep-learning-and-computer-vision)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch for Beginners | Dan We | [Skillshare](https://www.skillshare.com/classes/Pytorch-for-beginners-how-machine-learning-with-pytorch-really-works/1042152565)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch Essential Training: Deep Learning | Jonathan Fernandes | [LinkedIn
    Learning](https://www.linkedin.com/learning/pytorch-essential-training-deep-learning)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Introduction to Deep Learning Using PyTorch | Goku Mohandas and Alfredo Canziani
    | [O’Reilly Learning](https://learning.oreilly.com/videos/introduction-to-deep/9781491989944/)
    |'
  prefs: []
  type: TYPE_TB
- en: This chapter has provided resources for expanding your learning, research, and
    development with PyTorch. You can use this material as a quick reference for the
    numerous packages within the PyTorch project and the PyTorch Ecosystem. When you
    are looking to expand your skills and knowledge, you can return to this chapter
    to get ideas on other training materials available to you.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations on completing the book! You’ve come a long way, getting to grips
    with tensors, understanding the model development process, and exploring reference
    designs using PyTorch. In addition, you’ve learned how to customize PyTorch, create
    your own features, accelerate training, optimize your models, and deploy your
    NNs to the cloud and edge devices. Finally, we explored the PyTorch Ecosystem,
    investigated key packages like Torchvision, Torchtext, and TensorBoard, and learned
    about additional ways to expand your knowledge with tutorials, books, and online
    courses.
  prefs: []
  type: TYPE_NORMAL
- en: No matter what projects you tackle in the future, I hope you’ll be able to return
    to this book again and again. I also hope you continue to expand your skills and
    master PyTorch’s capabilities to develop innovative new deep learning tools and
    systems. Don’t let your new knowledge and skills dwindle away. Go build something
    interesting, and make a difference in the world!
  prefs: []
  type: TYPE_NORMAL
- en: Let me know what you create! I hope to see you in one of my courses at [PyTorch
    Academy](https://pytorchacademy.com) and feel free to reach out to me via email
    ([jpapa@joepapa.ai](mailto:jpapa@joepapa.ai)), Twitter (@JoePapaAI), or LinkedIn
    (@MrJoePapa).
  prefs: []
  type: TYPE_NORMAL
