- en: Chapter 6\. Summary and Outlook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this report, you learned about the multilayered lifecycles that govern AI
    projects (the AI development lifecycle in [Figure 1-1](ch01.html#ch01_figure_1_1738498450402392)
    and the AI model lifecycle in [Figure 3-1](ch03.html#ch03_figure_1_1738498450651715))
    and open source Kubernetes-based tools that work to enable each of those phases
    at scale. You learned how to leverage open source tools to successfully move a
    generative AI model through these cycles, standardizing the process of model creation
    and allowing you to confidently deploy and manage AI models in production.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how these phases interact, which open source tools to use at each
    phase, and how they look in practice, let’s look at an example project.
  prefs: []
  type: TYPE_NORMAL
- en: Personalized Healthcare Chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, let’s follow a fictional generative AI team at a major health
    insurer as it pitches and builds a personalized health chatbot. In the first phase
    of the AI development lifecycle, the project is initiated.
  prefs: []
  type: TYPE_NORMAL
- en: During *project initiation*, the generative AI team lead meets with the team’s
    organization’s director of engineering, head of sales, head of IT, and director
    of research and development to discuss a project idea floated by a member of the
    team. She pitches a personalized chatbot that healthcare subscribers can interact
    with to navigate questions about their personal health, their insurance policy,
    and healthcare providers. She claims this can reduce time spent by customer agents
    on common, personalized tasks; reduce personally identifiable information (PII)
    and health information from being exposed to customer agents; and increase a subscriber’s
    agency over their own healthcare, reducing costs and increasing satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: The business units are convinced by the case she made, but the director of engineering
    is skeptical. How will data be safeguarded? What kinds of technologies will be
    used? How can we deploy this and serve all of our customers? The team lead explains
    this to the director of engineering, who is satisfied with her answers. We’ll
    break down her plan throughout the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the project initiation is developed and agreed upon, the next phase, shown
    in [Figure 1-1](ch01.html#ch01_figure_1_1738498450402392), is *data preparation*.
    Our technical lead’s team gets to work collecting data to train and personalize
    the chatbot. The team chooses a popular *foundation model*, which it will fine-tune
    to have better access to nonidentifiable company-wide information, and then will
    use techniques such as prompt engineering and retrieval-augmented generation (RAG)
    at inference time with a user’s personal information to further personalize individual
    chatbot sessions. Within this phase is the first phase of the AI model lifecycle
    from [Figure 3-1](ch03.html#ch03_figure_1_1738498450651715): *gathering training/fine-tuning
    data*. The team builds data ingestion pipelines from its healthcare partners and
    internal systems, *online analytics processing (OLAP)* databases to store this
    data, and object storage technologies to store additional data and views.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the generative AI team enters the *model experimentation* phase of the
    AI development lifecycle. This is an iterative phase that includes the following
    phases of the AI model lifecycle (Fig. 3-1):'
  prefs: []
  type: TYPE_NORMAL
- en: Developing training/fine-tuning code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing the training/fine-tuning job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The team spends several months working on this, building the initial model training
    code scaffolding on which the team will fine-tune the foundation model and test
    RAG techniques and different prompts. The team committed to a cloud-agnostic open
    source platform to allow greater flexibility across many environments, and so
    chose to build its infrastructure with Kubernetes. Because fine-tuning a foundation
    LLM is less intensive than training one from scratch, the team chose to use PyTorch
    libraries to fine-tune an existing smaller foundation model on a small corpus
    of the company’s data. Early exploratory versions were created in a small Jupyter
    notebook environment, but as the fine-tuning datasets, base models, and fine-tuned
    models grew in size, the team turned to [Kubeflow](https://www.kubeflow.org) and
    the Kubeflow Training Operator to scale up the fine-tuning process on its Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As the team trained and evaluated new versions of its fine-tuned model, managing
    the training clusters became a headache, and so the team decided to invest time
    in finding a training resource management tool. The team had become more familiar
    with Kubernetes at this point, and wanted to ensure it had plenty of control without
    too many abstractions getting in its way. The team adopted [Kueue](https://kueue.sigs.k8s.io)
    to queue up and prioritize resource-hungry training jobs, ensuring the highest-priority
    jobs would be run first.
  prefs: []
  type: TYPE_NORMAL
- en: One thing the team decided early on in the project, however, was that it would
    need an experiment tracking tool. The team knew it would be repeating the fine-tuning/evaluation
    cycle frequently before the first candidate was ready to be promoted to production
    across many data scientists, and needed a way to understand who did what. Because
    the team had previously chosen Kubeflow as its platform of choice, the team was
    able to use [Kubeflow Pipelines](https://oreil.ly/bjLBl) to build repeatable training
    jobs and the Kubeflow Model Registry to keep track of trained models. This allowed
    the data science team to keep track of fine-tuned model artifacts, prompt artifacts,
    and model evaluation metrics, making the team lead’s life easier when deciding
    when to bring a model into production.
  prefs: []
  type: TYPE_NORMAL
- en: The key deliverables for this phase are production-ready model artifacts and
    a reusable training pipeline that accelerates both this phase and the periodic
    retraining of the deployed model. The pipeline itself is cleanly versioned using
    GitOps (see [Chapter 3](ch03.html#ch03_making_training_repeatable_1738498450655759))
    principles and [Argo CD](https://oreil.ly/00FxF) to manage continuous delivery
    of clean production pipeline versions that will be used to train production models.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, engineers on the generative AI team are working together with
    the product engineering team to design and build APIs and artifact storage that
    allow the generative AI team to deploy new models autonomously and the product
    engineering team to build a chat interface that doesn’t need to know any details
    about the model. This is the *application integration* phase of the model development
    lifecycle, and for smaller teams, this may happen only after the first production-ready
    model is trained.
  prefs: []
  type: TYPE_NORMAL
- en: The “last” phase (in quotes because this is a cyclical, iterative process) is
    putting the model into *production service*. In [Figure 3-1](ch03.html#ch03_figure_1_1738498450651715),
    this corresponds to *promoting the model to production for inference and monitoring
    the served model* (see [Chapter 4](ch04.html#ch04_model_deployment_and_monitoring_1738498450837987)).
  prefs: []
  type: TYPE_NORMAL
- en: When a model is promoted to production, a system is put in place to deploy the
    chosen artifact and to then serve it behind an API. While it is running in production,
    metrics are monitored to ensure that the model is functioning as expected and
    that the serving infrastructure is returning results to users in a timely manner.
    Our generative AI team lead chose to use [KServe](https://oreil.ly/sZzWk) with
    the [vLLM](https://docs.vllm.ai) runtime. She chose KServe because of its tight
    integration with the Kubernetes ecosystem and active developer community. She
    chose the vLLM runtime because it is specifically built for serving LLMs at scale
    and has many features and optimizations to serve a high volume of inference requests
    quickly. This combination also comes with a standard API on the model that the
    product team can access to finalize application integration and canary deployments
    to gradually roll out and test new model versions with a small number of users.
  prefs: []
  type: TYPE_NORMAL
- en: The team spent some time throughout the process to define a number of metrics
    to keep track of for production models. Some of these came from vLLM, others came
    from KServe, some were human feedback scores from customers, and still others
    the team built. Thanks to an integration with KServe, the team’s MLOps engineers
    are using [Prometheus](https://prometheus.io) to visualize and monitor the metrics
    of the production model and respond right away to slow performance, traffic spikes,
    data drift, and outages.
  prefs: []
  type: TYPE_NORMAL
- en: Using Prometheus on several occasions helped the team to catch a growing scaling
    issue early on, and the canary deployments provided by KServe prevented the issues
    from affecting more than a small number of users. Following GitOps best practices
    allowed the team to revert the problematic infrastructure version to a previously
    known good version, giving the team members time to diagnose and fix any found
    bugs before redeploying.
  prefs: []
  type: TYPE_NORMAL
- en: During early testing, the team found that users were able to get inappropriate
    answers from the chatbot and that some conversations with the chatbot were perceived
    as rude. One of the machine learning engineers on the team had recently read about
    [TrustyAI Guardrails](https://oreil.ly/wOv-O) (see [Chapter 5](ch05.html#ch05_responsible_ai_1738498450930720))
    and began an initiative to build guardrails into KServe. With Prometheus, the
    team was able to monitor detections of inappropriate responses and interactions
    as well as the customer feedback for these instances. Using the Guardrails, the
    team was able to reduce negative interactions by a whopping 87%.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results were astounding: customer agents had more time to upskill and work
    on serving customers who had more complex issues, customers could get personalized
    information in a conversational interface without having to call or wait for a
    live agent, and her team created a blueprint for future generative AI initiatives
    throughout the organization.'
  prefs: []
  type: TYPE_NORMAL
- en: But the team’s work was not done yet, and in fact wouldn’t be done until the
    feature was retired or superseded by another technology. Because the model was
    fine-tuned on organization-wide data, it would have to be periodically fine-tuned,
    evaluated, and redeployed to make sure it had up-to-date information. Surprisingly,
    this would be a simple effort requiring only one or two data scientists less than
    a week to complete. This is all thanks to the generative AI team lead’s forward
    thinking by directing the creation and use of a reusable training pipeline with
    GitOps, model version tracking via a registry, data versioning, and using predictable
    data storage.
  prefs: []
  type: TYPE_NORMAL
- en: Future Technology Outlook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What is next for this intrepid generative AI team? We foresee three broad dimensions
    along which innovation in AI and AI platforms will progress over the coming months:'
  prefs: []
  type: TYPE_NORMAL
- en: Model architectures, the capabilities they yield, and the tools and techniques
    used to create them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The level of integration between tools underpinning the overall MLOps lifecycle
    and the ability to leverage these integrated solutions to build intelligent applications
    more cost effectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further innovation in inference optimization to reduce response latency, in
    areas such as quantization techniques, LoRA adapters, dynamic batching, and inference
    workload scheduling techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to build and maintain AI-enabled applications in a way that ensures
    the responsible and trustworthy use of AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Along the first dimension, we will continue to see the largest models getting
    larger as compute resources become more performant, more efficient, and more readily
    available. At the same time, we will continue to see novel approaches for customizing
    smaller models with an organization’s own data in order to yield high performance
    results at lower costs for domain-specific use cases. General purpose AI will
    become more powerful and use case-specific models will get easier to create. The
    key to taking advantage of these innovations will be to leverage frameworks and
    training platforms with strong open source community adoption in order to be well-positioned
    at the forefront of new technological leaps.
  prefs: []
  type: TYPE_NORMAL
- en: Along the second dimension, platform suites such as Kubeflow that bundle tools
    across the AI model and development lifecycles will make it easier for data scientists
    to use each component in a way that is increasingly transparent to them. For example,
    libraries for training models will natively integrate with experiment tracking
    and model registry tools so that data scientists’ experiments are automatically
    tracked. Additionally, these solutions will come with tools to automatically detect
    and respond to hardware failures during model training and serving, reducing the
    overall cost of developing and running models. These projects will gain and improve
    capabilities for managing the cost of developing models via better management
    of compute resources and sharing these resources across data science teams.
  prefs: []
  type: TYPE_NORMAL
- en: Managing compute resources is the major theme of the third dimension. Here,
    we will see continued optimization of inference workloads via ongoing research
    on new quantization techniques (in which the weights and activations of a model
    are represented with lower precision data types, reducing memory usage), efficient
    utilization of [key-value (KV) caches](https://oreil.ly/MOnY_), LoRA adapters,
    and dynamic batching techniques (where requests to the hardware accelerator are
    batched based on batch size or time elapsed), all in order to make more efficient
    use of hardware accelerators like GPUs. We will also begin to see wholly new innovations
    in how inference workloads are scheduled and executed on hardware accelerators,
    again to use existing accelerators more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: And along the final dimension, we expect to see more resources (financial, talent,
    etc.), research, and tooling dedicated to the ethical and safe training and use
    of generative AI. From training data lineage and tracking to model explainability
    and safety tools like guardrails and hallucination detection, generative AI has
    broadly expanded the potential for harmful creation and use of LLMs. Community-driven
    initiatives like [TrustyAI](https://oreil.ly/PXDlY) and specifically TrustyAI
    Guardrails, along with [Guardrails Hub](http://hub.guardrailsai.com), are already
    making it easier than ever to protect users, PII, and enterprises from the various
    direct and indirect harms (such as lawsuits) that can be brought about with LLMs.
    Additionally, we expect more norms and tooling for the ethical collection and
    sharing of large datasets to protect privacy and intellectual property rights.
  prefs: []
  type: TYPE_NORMAL
- en: About the Authors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Alex Corvin** is a senior engineering manager responsible for crafting and
    executing capabilities for data scientist experimentation and model training within
    Red Hat OpenShift AI, Red Hat’s flagship AI/ML platform. Alex has orchestrated
    creation and enhancement of functionalities for distributed training and fine-tuning
    of AI models, encompassing extensive language models, utilizing tools such as
    Ray and PyTorch. Alex and his team contribute heavily to several prominent open
    source projects including Kubeflow Pipelines, Kueue, Kuberay, Feast, and Kubeflow
    Training Operator. Alex has spoken at numerous industry conferences like Ray Summit,
    DevConf, NVIDIA GTC, OpenShift Commons, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Taneem Ibrahim** is a senior engineering manager whose team is responsible
    for several projects in the development of an enterprise-class MLOps product,
    Red Hat OpenShift AI. As part of the product engineering work, Taneem and his
    team participate in several open source projects such as model serving (KServe,
    ModelMesh, vLLM), responsible AI (TrustyAI, AIX360), and model registry (KubeFlow,
    ML Metadata). Taneem has also worked with an extensive AI partner ecosystem for
    integration with OpenShift AI and IBM watsonx.ai. Taneem has spoken at many industry
    events like Ray Summit, Red Hat Summit, and KubeCon.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kyle Stratis** is a software engineer with over a decade of experience across
    the AI development lifecycle in a variety of domains, including computer vision,
    health technology, and social media analytics. Along with being an O’Reilly author,
    he is the founder of [Stratis Data Labs](https://stratisdatalabs.com), an AI and
    data consultancy, and was most recently the lead machine learning engineer at
    Vizit Labs, where he built Vizit’s internal AI platform.'
  prefs: []
  type: TYPE_NORMAL
