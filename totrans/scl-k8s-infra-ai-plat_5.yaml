- en: Chapter 6\. Summary and Outlook
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。总结与展望
- en: In this report, you learned about the multilayered lifecycles that govern AI
    projects (the AI development lifecycle in [Figure 1-1](ch01.html#ch01_figure_1_1738498450402392)
    and the AI model lifecycle in [Figure 3-1](ch03.html#ch03_figure_1_1738498450651715))
    and open source Kubernetes-based tools that work to enable each of those phases
    at scale. You learned how to leverage open source tools to successfully move a
    generative AI model through these cycles, standardizing the process of model creation
    and allowing you to confidently deploy and manage AI models in production.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这份报告中，你了解了管理人工智能项目多层生命周期的内容（[图1-1](ch01.html#ch01_figure_1_1738498450402392)中的AI开发生命周期和[图3-1](ch03.html#ch03_figure_1_1738498450651715)中的AI模型生命周期），以及那些旨在实现每个阶段规模化的开源Kubernetes工具。你学习了如何利用开源工具成功地将生成式AI模型通过这些周期，标准化模型创建的过程，并允许你自信地部署和管理生产中的AI模型。
- en: To understand how these phases interact, which open source tools to use at each
    phase, and how they look in practice, let’s look at an example project.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这些阶段如何相互作用，每个阶段应使用哪些开源工具，以及它们在实际中的样子，让我们来看一个示例项目。
- en: Personalized Healthcare Chatbot
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 个性化医疗聊天机器人
- en: In this example, let’s follow a fictional generative AI team at a major health
    insurer as it pitches and builds a personalized health chatbot. In the first phase
    of the AI development lifecycle, the project is initiated.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，让我们跟随一家大型健康保险公司的一个虚构生成式AI团队，它提出并构建了一个个性化的健康聊天机器人。在AI开发生命周期的第一阶段，项目被启动。
- en: During *project initiation*, the generative AI team lead meets with the team’s
    organization’s director of engineering, head of sales, head of IT, and director
    of research and development to discuss a project idea floated by a member of the
    team. She pitches a personalized chatbot that healthcare subscribers can interact
    with to navigate questions about their personal health, their insurance policy,
    and healthcare providers. She claims this can reduce time spent by customer agents
    on common, personalized tasks; reduce personally identifiable information (PII)
    and health information from being exposed to customer agents; and increase a subscriber’s
    agency over their own healthcare, reducing costs and increasing satisfaction.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在*项目启动*阶段，生成式AI团队负责人与团队的工程总监、销售负责人、IT负责人和研发总监会面，讨论由团队成员提出的一个项目想法。她提出一个个性化的聊天机器人，医疗订阅者可以通过它与个人健康、保险政策和医疗服务提供者互动来导航问题。她声称这可以减少客户代理在常见个性化任务上的时间；减少个人可识别信息（PII）和健康信息暴露给客户代理；并增加订阅者对其自身医疗保健的控制权，降低成本并提高满意度。
- en: The business units are convinced by the case she made, but the director of engineering
    is skeptical. How will data be safeguarded? What kinds of technologies will be
    used? How can we deploy this and serve all of our customers? The team lead explains
    this to the director of engineering, who is satisfied with her answers. We’ll
    break down her plan throughout the rest of the chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 业务部门被她提出的案例说服了，但工程总监持怀疑态度。数据将如何得到保护？将使用哪些技术？我们如何部署这个系统并服务于所有客户？团队负责人向工程总监解释了这一点，她对她的回答感到满意。我们将在本章的剩余部分详细阐述她的计划。
- en: 'Once the project initiation is developed and agreed upon, the next phase, shown
    in [Figure 1-1](ch01.html#ch01_figure_1_1738498450402392), is *data preparation*.
    Our technical lead’s team gets to work collecting data to train and personalize
    the chatbot. The team chooses a popular *foundation model*, which it will fine-tune
    to have better access to nonidentifiable company-wide information, and then will
    use techniques such as prompt engineering and retrieval-augmented generation (RAG)
    at inference time with a user’s personal information to further personalize individual
    chatbot sessions. Within this phase is the first phase of the AI model lifecycle
    from [Figure 3-1](ch03.html#ch03_figure_1_1738498450651715): *gathering training/fine-tuning
    data*. The team builds data ingestion pipelines from its healthcare partners and
    internal systems, *online analytics processing (OLAP)* databases to store this
    data, and object storage technologies to store additional data and views.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦项目启动被开发和同意，下一个阶段，如图 [图 1-1](ch01.html#ch01_figure_1_1738498450402392) 所示，是
    *数据准备*。我们的技术负责人团队开始收集数据以训练和个性化聊天机器人。团队选择了一个流行的 *基础模型*，它将微调以更好地访问非可识别的公司内部信息，然后将在推理时间使用用户个人信息，结合提示工程和检索增强生成
    (RAG) 技术来进一步个性化单个聊天机器人会话。在这个阶段中，是 AI 模型生命周期从 [图 3-1](ch03.html#ch03_figure_1_1738498450651715)
    的第一个阶段：*收集训练/微调数据*。团队从其医疗保健合作伙伴和内部系统构建数据摄取管道，*在线分析处理 (OLAP)* 数据库来存储这些数据，以及对象存储技术来存储额外的数据和视图。
- en: 'Next, the generative AI team enters the *model experimentation* phase of the
    AI development lifecycle. This is an iterative phase that includes the following
    phases of the AI model lifecycle (Fig. 3-1):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，生成式 AI 团队进入 AI 开发生命周期的 *模型实验* 阶段。这是一个迭代阶段，包括 AI 模型生命周期的以下阶段（图 3-1）：
- en: Developing training/fine-tuning code
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发训练/微调代码
- en: Executing the training/fine-tuning job
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行训练/微调作业
- en: Evaluating the trained model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估训练好的模型
- en: The team spends several months working on this, building the initial model training
    code scaffolding on which the team will fine-tune the foundation model and test
    RAG techniques and different prompts. The team committed to a cloud-agnostic open
    source platform to allow greater flexibility across many environments, and so
    chose to build its infrastructure with Kubernetes. Because fine-tuning a foundation
    LLM is less intensive than training one from scratch, the team chose to use PyTorch
    libraries to fine-tune an existing smaller foundation model on a small corpus
    of the company’s data. Early exploratory versions were created in a small Jupyter
    notebook environment, but as the fine-tuning datasets, base models, and fine-tuned
    models grew in size, the team turned to [Kubeflow](https://www.kubeflow.org) and
    the Kubeflow Training Operator to scale up the fine-tuning process on its Kubernetes
    cluster.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 团队花费了几个月的时间来做这项工作，构建了初始模型训练代码框架，团队将在其上微调基础模型并测试 RAG 技术和不同的提示。团队致力于一个云无关的开源平台，以允许在许多环境中拥有更大的灵活性，因此选择使用
    Kubernetes 来构建其基础设施。由于微调一个基础大型语言模型（LLM）的强度不如从头开始训练，团队选择使用 PyTorch 库在公司的数据小语料库上微调现有的较小基础模型。早期探索版本是在一个小型的
    Jupyter 笔记本环境中创建的，但随着微调数据集、基础模型和微调模型规模的增加，团队转向了 [Kubeflow](https://www.kubeflow.org)
    和 Kubeflow 训练操作员来在其 Kubernetes 集群上扩展微调过程。
- en: As the team trained and evaluated new versions of its fine-tuned model, managing
    the training clusters became a headache, and so the team decided to invest time
    in finding a training resource management tool. The team had become more familiar
    with Kubernetes at this point, and wanted to ensure it had plenty of control without
    too many abstractions getting in its way. The team adopted [Kueue](https://kueue.sigs.k8s.io)
    to queue up and prioritize resource-hungry training jobs, ensuring the highest-priority
    jobs would be run first.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着团队对其微调模型的新版本进行训练和评估，管理训练集群变成了一个头疼的问题，因此团队决定投入时间寻找一个训练资源管理工具。此时，团队对 Kubernetes
    已经更加熟悉，并希望确保在不过多抽象的情况下拥有足够的控制权。团队采用了 [Kueue](https://kueue.sigs.k8s.io) 来排队并优先处理资源密集型训练作业，确保最高优先级的作业首先运行。
- en: One thing the team decided early on in the project, however, was that it would
    need an experiment tracking tool. The team knew it would be repeating the fine-tuning/evaluation
    cycle frequently before the first candidate was ready to be promoted to production
    across many data scientists, and needed a way to understand who did what. Because
    the team had previously chosen Kubeflow as its platform of choice, the team was
    able to use [Kubeflow Pipelines](https://oreil.ly/bjLBl) to build repeatable training
    jobs and the Kubeflow Model Registry to keep track of trained models. This allowed
    the data science team to keep track of fine-tuned model artifacts, prompt artifacts,
    and model evaluation metrics, making the team lead’s life easier when deciding
    when to bring a model into production.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在项目早期，团队决定需要一款实验跟踪工具。团队知道在第一个候选模型准备好被推广到生产环境之前，他们需要频繁地重复微调/评估周期，并且需要一种方式来了解谁做了什么。由于团队之前选择了Kubeflow作为其首选平台，因此团队能够使用[Kubeflow
    Pipelines](https://oreil.ly/bjLBl)来构建可重复的训练作业，并使用Kubeflow Model Registry来跟踪训练模型。这允许数据科学团队能够跟踪微调模型工件、提示工件和模型评估指标，使得团队领导在决定何时将模型投入生产时生活更加轻松。
- en: The key deliverables for this phase are production-ready model artifacts and
    a reusable training pipeline that accelerates both this phase and the periodic
    retraining of the deployed model. The pipeline itself is cleanly versioned using
    GitOps (see [Chapter 3](ch03.html#ch03_making_training_repeatable_1738498450655759))
    principles and [Argo CD](https://oreil.ly/00FxF) to manage continuous delivery
    of clean production pipeline versions that will be used to train production models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这一阶段的关键交付成果是生产就绪的模型工件和一个可重用的训练管道，它加速了这一阶段以及部署模型的定期重新训练。该管道本身使用GitOps原则（见[第3章](ch03.html#ch03_making_training_repeatable_1738498450655759)）和[Argo
    CD](https://oreil.ly/00FxF)进行干净地版本控制，以管理连续交付干净的生产管道版本，这些版本将被用于训练生产模型。
- en: At the same time, engineers on the generative AI team are working together with
    the product engineering team to design and build APIs and artifact storage that
    allow the generative AI team to deploy new models autonomously and the product
    engineering team to build a chat interface that doesn’t need to know any details
    about the model. This is the *application integration* phase of the model development
    lifecycle, and for smaller teams, this may happen only after the first production-ready
    model is trained.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，生成式AI团队的工程师们正在与产品工程团队合作，设计和构建API和工件存储，这使得生成式AI团队能够自主部署新模型，而产品工程团队能够构建一个无需了解模型任何细节的聊天界面。这是模型开发生命周期中的*应用集成*阶段，对于较小的团队来说，这可能在训练出第一个生产就绪模型之后才会发生。
- en: The “last” phase (in quotes because this is a cyclical, iterative process) is
    putting the model into *production service*. In [Figure 3-1](ch03.html#ch03_figure_1_1738498450651715),
    this corresponds to *promoting the model to production for inference and monitoring
    the served model* (see [Chapter 4](ch04.html#ch04_model_deployment_and_monitoring_1738498450837987)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: “最后”阶段（引号是因为这是一个循环、迭代的过程）是将模型投入*生产服务*。在[图3-1](ch03.html#ch03_figure_1_1738498450651715)中，这对应于*将模型推广到生产进行推理并监控所提供模型*（见[第4章](ch04.html#ch04_model_deployment_and_monitoring_1738498450837987)）。
- en: When a model is promoted to production, a system is put in place to deploy the
    chosen artifact and to then serve it behind an API. While it is running in production,
    metrics are monitored to ensure that the model is functioning as expected and
    that the serving infrastructure is returning results to users in a timely manner.
    Our generative AI team lead chose to use [KServe](https://oreil.ly/sZzWk) with
    the [vLLM](https://docs.vllm.ai) runtime. She chose KServe because of its tight
    integration with the Kubernetes ecosystem and active developer community. She
    chose the vLLM runtime because it is specifically built for serving LLMs at scale
    and has many features and optimizations to serve a high volume of inference requests
    quickly. This combination also comes with a standard API on the model that the
    product team can access to finalize application integration and canary deployments
    to gradually roll out and test new model versions with a small number of users.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型被推广到生产环境时，会建立一个系统来部署所选的工件，并在API后面提供服务。在生产运行期间，会监控指标以确保模型按预期运行，并且服务基础设施能够及时向用户返回结果。我们的生成式AI团队领导选择使用[KServe](https://oreil.ly/sZzWk)和[vLLM](https://docs.vllm.ai)运行时。她选择KServe是因为它与Kubernetes生态系统的紧密集成和活跃的开发者社区。她选择vLLM运行时是因为它是专门为大规模服务LLM而构建的，并且具有许多功能和优化，可以快速处理大量推理请求。这种组合还带来了模型上的标准API，产品团队可以访问它以完成应用程序集成和金丝雀部署，逐步向少量用户推出和测试新模型版本。
- en: The team spent some time throughout the process to define a number of metrics
    to keep track of for production models. Some of these came from vLLM, others came
    from KServe, some were human feedback scores from customers, and still others
    the team built. Thanks to an integration with KServe, the team’s MLOps engineers
    are using [Prometheus](https://prometheus.io) to visualize and monitor the metrics
    of the production model and respond right away to slow performance, traffic spikes,
    data drift, and outages.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，团队花了一些时间定义了多个指标，以跟踪生产模型。其中一些来自vLLM，一些来自KServe，一些是来自客户的客户反馈评分，还有一些是团队自己构建的。得益于与KServe的集成，团队的MLOps工程师正在使用[Prometheus](https://prometheus.io)来可视化和监控生产模型的指标，并立即对性能缓慢、流量激增、数据漂移和故障做出响应。
- en: Using Prometheus on several occasions helped the team to catch a growing scaling
    issue early on, and the canary deployments provided by KServe prevented the issues
    from affecting more than a small number of users. Following GitOps best practices
    allowed the team to revert the problematic infrastructure version to a previously
    known good version, giving the team members time to diagnose and fix any found
    bugs before redeploying.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 几次使用Prometheus帮助团队及早发现增长中的扩展问题，KServe提供的金丝雀部署防止了问题影响超过少数用户。遵循GitOps最佳实践允许团队将问题基础设施版本回滚到之前已知的好版本，给团队成员时间诊断和修复任何发现的错误，然后再重新部署。
- en: During early testing, the team found that users were able to get inappropriate
    answers from the chatbot and that some conversations with the chatbot were perceived
    as rude. One of the machine learning engineers on the team had recently read about
    [TrustyAI Guardrails](https://oreil.ly/wOv-O) (see [Chapter 5](ch05.html#ch05_responsible_ai_1738498450930720))
    and began an initiative to build guardrails into KServe. With Prometheus, the
    team was able to monitor detections of inappropriate responses and interactions
    as well as the customer feedback for these instances. Using the Guardrails, the
    team was able to reduce negative interactions by a whopping 87%.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期测试期间，团队发现用户能够从聊天机器人那里获得不适当的答案，并且一些与聊天机器人的对话被认为是不礼貌的。团队中的机器学习工程师最近阅读了关于[TrustyAI
    Guardrails](https://oreil.ly/wOv-O)（见[第5章](ch05.html#ch05_responsible_ai_1738498450930720)）的内容，并开始了一个将护栏构建到KServe中的倡议。借助Prometheus，团队能够监控不适当响应和交互的检测以及这些实例的客户反馈。使用护栏，团队能够将负面交互减少了惊人的87%。
- en: 'The results were astounding: customer agents had more time to upskill and work
    on serving customers who had more complex issues, customers could get personalized
    information in a conversational interface without having to call or wait for a
    live agent, and her team created a blueprint for future generative AI initiatives
    throughout the organization.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人震惊：客户代理有更多时间提升技能并处理更复杂问题的客户，客户可以在会话界面中获取个性化信息，而无需打电话或等待真人代理，她的团队为整个组织的未来生成式AI计划制定了蓝图。
- en: But the team’s work was not done yet, and in fact wouldn’t be done until the
    feature was retired or superseded by another technology. Because the model was
    fine-tuned on organization-wide data, it would have to be periodically fine-tuned,
    evaluated, and redeployed to make sure it had up-to-date information. Surprisingly,
    this would be a simple effort requiring only one or two data scientists less than
    a week to complete. This is all thanks to the generative AI team lead’s forward
    thinking by directing the creation and use of a reusable training pipeline with
    GitOps, model version tracking via a registry, data versioning, and using predictable
    data storage.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但团队的工作还没有完成，实际上直到该功能被淘汰或被另一种技术取代之前，工作都不会结束。因为模型是在整个组织的数据上微调的，所以它必须定期进行微调、评估和重新部署，以确保其拥有最新的信息。令人惊讶的是，这将是一项简单的任务，只需要一到两名数据科学家不到一周的时间就能完成。这一切都归功于生成式AI团队领导者的前瞻性思维，通过指导创建和使用带有GitOps的可重复使用训练管道、通过注册表进行模型版本跟踪、数据版本化以及使用可预测的数据存储。
- en: Future Technology Outlook
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来技术展望
- en: 'What is next for this intrepid generative AI team? We foresee three broad dimensions
    along which innovation in AI and AI platforms will progress over the coming months:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这个勇敢的生成式AI团队将有何作为？我们预计，在未来几个月内，AI和AI平台创新将沿着三个广泛的维度发展：
- en: Model architectures, the capabilities they yield, and the tools and techniques
    used to create them
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型架构、它们产生的功能以及创建它们所使用的工具和技术
- en: The level of integration between tools underpinning the overall MLOps lifecycle
    and the ability to leverage these integrated solutions to build intelligent applications
    more cost effectively
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支撑整体MLOps生命周期的工具之间的集成程度以及利用这些集成解决方案以更具成本效益的方式构建智能应用的能力
- en: Further innovation in inference optimization to reduce response latency, in
    areas such as quantization techniques, LoRA adapters, dynamic batching, and inference
    workload scheduling techniques
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理优化方面的进一步创新，以减少响应延迟，包括量化技术、LoRA适配器、动态批处理和推理工作负载调度技术
- en: The ability to build and maintain AI-enabled applications in a way that ensures
    the responsible and trustworthy use of AI
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以确保AI的负责任和值得信赖的使用方式构建和维护AI赋能应用的能力
- en: Along the first dimension, we will continue to see the largest models getting
    larger as compute resources become more performant, more efficient, and more readily
    available. At the same time, we will continue to see novel approaches for customizing
    smaller models with an organization’s own data in order to yield high performance
    results at lower costs for domain-specific use cases. General purpose AI will
    become more powerful and use case-specific models will get easier to create. The
    key to taking advantage of these innovations will be to leverage frameworks and
    training platforms with strong open source community adoption in order to be well-positioned
    at the forefront of new technological leaps.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个维度上，我们将继续看到，随着计算资源变得更加高效、更高效，并且更容易获得，最大的模型将变得更大。同时，我们还将看到针对组织自己的数据进行定制的小型模型的创新方法，以便在特定领域用例中以更低的成本产生高性能结果。通用AI将变得更加强大，特定用例的模型将更容易创建。利用这些创新的关键在于利用具有强大开源社区采用的框架和训练平台，以便在新技术的最前沿占据有利位置。
- en: Along the second dimension, platform suites such as Kubeflow that bundle tools
    across the AI model and development lifecycles will make it easier for data scientists
    to use each component in a way that is increasingly transparent to them. For example,
    libraries for training models will natively integrate with experiment tracking
    and model registry tools so that data scientists’ experiments are automatically
    tracked. Additionally, these solutions will come with tools to automatically detect
    and respond to hardware failures during model training and serving, reducing the
    overall cost of developing and running models. These projects will gain and improve
    capabilities for managing the cost of developing models via better management
    of compute resources and sharing these resources across data science teams.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个维度上，平台套件如Kubeflow将捆绑AI模型和开发生命周期中的工具，这将使数据科学家更容易以对他们越来越透明的方式使用每个组件。例如，用于训练模型的库将原生集成到实验跟踪和模型注册工具中，以便自动跟踪数据科学家的实验。此外，这些解决方案将附带工具，可以自动检测和响应模型训练和部署期间的硬件故障，从而降低开发和运行模型的整体成本。这些项目将通过更好地管理计算资源并在数据科学团队之间共享这些资源来提高和改善管理模型开发成本的能力。
- en: Managing compute resources is the major theme of the third dimension. Here,
    we will see continued optimization of inference workloads via ongoing research
    on new quantization techniques (in which the weights and activations of a model
    are represented with lower precision data types, reducing memory usage), efficient
    utilization of [key-value (KV) caches](https://oreil.ly/MOnY_), LoRA adapters,
    and dynamic batching techniques (where requests to the hardware accelerator are
    batched based on batch size or time elapsed), all in order to make more efficient
    use of hardware accelerators like GPUs. We will also begin to see wholly new innovations
    in how inference workloads are scheduled and executed on hardware accelerators,
    again to use existing accelerators more efficiently.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 管理计算资源是第三维度的主题。在这里，我们将看到通过持续研究新的量化技术（其中模型的权重和激活以较低精度的数据类型表示，从而减少内存使用）、高效利用[键值（KV）缓存](https://oreil.ly/MOnY_)、LoRA适配器和动态批处理技术（根据批大小或经过的时间将硬件加速器的请求进行批处理），所有这些都在于更有效地使用如GPU这样的硬件加速器。我们还将开始看到在硬件加速器上调度和执行推理工作负载的全新创新，再次以提高现有加速器的效率。
- en: And along the final dimension, we expect to see more resources (financial, talent,
    etc.), research, and tooling dedicated to the ethical and safe training and use
    of generative AI. From training data lineage and tracking to model explainability
    and safety tools like guardrails and hallucination detection, generative AI has
    broadly expanded the potential for harmful creation and use of LLMs. Community-driven
    initiatives like [TrustyAI](https://oreil.ly/PXDlY) and specifically TrustyAI
    Guardrails, along with [Guardrails Hub](http://hub.guardrailsai.com), are already
    making it easier than ever to protect users, PII, and enterprises from the various
    direct and indirect harms (such as lawsuits) that can be brought about with LLMs.
    Additionally, we expect more norms and tooling for the ethical collection and
    sharing of large datasets to protect privacy and intellectual property rights.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个维度上，我们预计将看到更多资源（财务、人才等）、研究和工具被投入到生成式AI的道德和安全训练及使用中。从训练数据溯源和追踪到模型可解释性和安全工具，如护栏和幻觉检测，生成式AI极大地扩展了LLMs有害创建和使用的能力。由社区驱动的项目，如[TrustyAI](https://oreil.ly/PXDlY)和特别提到的TrustyAI
    Guardrails，以及[Guardrails Hub](http://hub.guardrailsai.com)，已经使保护用户、个人身份信息（PII）和企业免受LLMs可能带来的各种直接和间接危害（如诉讼）变得比以往任何时候都容易。此外，我们预计还将出现更多关于道德收集和共享大型数据集的规范和工具，以保护隐私和知识产权。
- en: About the Authors
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于作者
- en: '**Alex Corvin** is a senior engineering manager responsible for crafting and
    executing capabilities for data scientist experimentation and model training within
    Red Hat OpenShift AI, Red Hat’s flagship AI/ML platform. Alex has orchestrated
    creation and enhancement of functionalities for distributed training and fine-tuning
    of AI models, encompassing extensive language models, utilizing tools such as
    Ray and PyTorch. Alex and his team contribute heavily to several prominent open
    source projects including Kubeflow Pipelines, Kueue, Kuberay, Feast, and Kubeflow
    Training Operator. Alex has spoken at numerous industry conferences like Ray Summit,
    DevConf, NVIDIA GTC, OpenShift Commons, and more.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**Alex Corvin**是一位高级工程经理，负责在Red Hat OpenShift AI（Red Hat的旗舰AI/ML平台）中构建和执行数据科学家实验和模型训练的能力。Alex负责创建和增强分布式训练和微调AI模型的功能，包括广泛的语言模型，使用Ray和PyTorch等工具。Alex及其团队在多个突出的开源项目中做出了重大贡献，包括Kubeflow
    Pipelines、Kueue、Kuberay、Feast和Kubeflow Training Operator。Alex在Ray Summit、DevConf、NVIDIA
    GTC、OpenShift Commons等多个行业会议上发表过演讲。'
- en: '**Taneem Ibrahim** is a senior engineering manager whose team is responsible
    for several projects in the development of an enterprise-class MLOps product,
    Red Hat OpenShift AI. As part of the product engineering work, Taneem and his
    team participate in several open source projects such as model serving (KServe,
    ModelMesh, vLLM), responsible AI (TrustyAI, AIX360), and model registry (KubeFlow,
    ML Metadata). Taneem has also worked with an extensive AI partner ecosystem for
    integration with OpenShift AI and IBM watsonx.ai. Taneem has spoken at many industry
    events like Ray Summit, Red Hat Summit, and KubeCon.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**Taneem Ibrahim** 是一位高级工程经理，他的团队负责开发企业级MLOps产品Red Hat OpenShift AI的多个项目。作为产品工程工作的一部分，Taneem及其团队参与了许多开源项目，如模型服务（KServe、ModelMesh、vLLM）、负责任的人工智能（TrustyAI、AIX360）以及模型注册（KubeFlow、ML
    Metadata）。Taneem还与广泛的AI合作伙伴生态系统合作，以实现与OpenShift AI和IBM watsonx.ai的集成。Taneem曾在许多行业活动中发表演讲，如Ray
    Summit、Red Hat Summit和KubeCon。'
- en: '**Kyle Stratis** is a software engineer with over a decade of experience across
    the AI development lifecycle in a variety of domains, including computer vision,
    health technology, and social media analytics. Along with being an O’Reilly author,
    he is the founder of [Stratis Data Labs](https://stratisdatalabs.com), an AI and
    data consultancy, and was most recently the lead machine learning engineer at
    Vizit Labs, where he built Vizit’s internal AI platform.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kyle Stratis** 是一位拥有超过十年经验的软件工程师，在多个领域的AI开发生命周期中都有所涉猎，包括计算机视觉、医疗技术和社交媒体分析。除了是一位O’Reilly的作者外，他还是[Stratis
    Data Labs](https://stratisdatalabs.com)的创始人，这是一家AI和数据咨询公司，最近曾担任Vizit Labs的首席机器学习工程师，在那里他构建了Vizit的内部AI平台。'
