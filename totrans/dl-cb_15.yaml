- en: Chapter 15\. Music and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other chapters in this book are all about processing of images or texts.
    Those chapters represent the balance of media in deep learning research, but that
    is not to say that sound processing isn’t interesting and that we haven’t seen
    some great developments in this area in the last few years. Speech recognition
    and speech synthesis are what made home assistants like Amazon Alexa and Google
    Home a possibility. The old sitcom joke where the phone dials the wrong number
    hasn’t really been current since Siri came out.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to start experimenting with these systems; there are APIs out there
    that let you get a simple voice app up and running in a few hours. The voice processing,
    however, is done in Amazon, Google, or Apple’s data center, so we can’t really
    count these as deep learning experiments. Building state-of-the-art voice recognition
    systems is hard, although Mozilla’s Deep Speech is making some impressive progress.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on music. We’ll start out with training a music classification
    model that can tell us what music we’re listening to. We’ll then use the results
    of this model to index local MP3s, making it possible to find songs similar in
    style. After that we’ll use the Spotify API to create a corpus of public playlists
    that we’ll use to train a music recommender.
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebooks for this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 15.1 Creating a Training Set for Music Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you get and prepare a set of music for classification?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create spectrograms from the test set provided by the University of Victoria
    in Canada.
  prefs: []
  type: TYPE_NORMAL
- en: 'You could try to do this by plugging in that dusty external drive with your
    MP3 collection on it and relying on the tags on those songs. But a lot of those
    tags may be somewhat random or missing, so it’s best to get started with a training
    set from a scientific institution that is nicely labeled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This should get us a directory, *genres*, with subdirectories containing music
    of different genres:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Those directories contain sound files (*.au*), 100 clips per genre, each 29
    seconds long. We could try to feed the raw sound frames directly into the network
    and maybe an LSTM would pick up something, but there are better ways of preprocessing
    sounds. Sound is really sound waves, but we don’t hear waves. Instead, we hear
    tones of a certain frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'So a good way to make our network behave more like our hearing works is to
    convert sound into blocks of spectrograms; each sample will be represented by
    a series of audio freqencies and their respective intensities. The `librosa` library
    for Python has some standard functions for this and also provides what’s called
    a *melspectrogram*, a type of spectrogram that is meant to closely emulate how
    human hearing works. So let’s load up the music and convert the fragments to melspectrograms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also have a quick look at some of the genres as spectrograms. Since those
    spectrograms are now just matrices, we can treat them as bitmaps. They are really
    quite sparse, so we are going to overexpose them to see more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Classical Spectrogram](assets/dlcb_15in01.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Metal Spectrogram](assets/dlcb_15in02.png)'
  prefs: []
  type: TYPE_IMG
- en: Even though it is hard to say what exactly the pictures mean, there is some
    suggestion that metal has more of a rigid structure than classical music, which
    is maybe not completely unexpected.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’ve seen throughout this book, preprocessing data before letting networks
    do their thing increases our chances of success significantly. When it comes to
    sound processing, `librosa` has functions for almost anything you could wish for,
    from loading sound files and playing them inside notebooks to visualizing them
    and doing any kind of preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Visually inspecting spectrograms doesn’t tell us much, but it does give us a
    hint that they are different for different genres of music. We’ll see in the next
    recipe whether a network can learn to distinguish between them too.
  prefs: []
  type: TYPE_NORMAL
- en: 15.2 Training a Music Genre Detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you set up and train a deep network to detect music genres?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a one-dimensional convolutional network.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve used convolutional networks in this book for image detection (see [Chapter 9](ch09.html#transfer_learning))
    and for text (see [Chapter 7](ch07.html#suggest_emojis)). It might seem that treating
    our spectrograms as images would be the more logical way to proceed, but we are
    actually going to go with a one-dimensional convolutional network. Each frame
    in our spectrogram represents a frame of music. Using a convolutional net to convert
    stretches of time into a more abstract representation makes sense when we try
    to classify genres; reducing the “height” of the frames is less intuitively sensible.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by stacking some layers on top of each other. This will reduce
    the size of our input from 128 dimensions wide to 25\. The `GlobalMaxPooling`
    layer will then make this into a 128-float vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is followed by a few fully connected layers to get to the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we feed our data into the model, we’ll split each song into 10 fragments
    of 3 seconds each. We do this to increase the amount of data, since 1,000 songs
    isn’t really that much:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Training this model gives us accuracy of around 60% after 100 epochs, which
    is not bad, but certainly not superhuman. We can improve upon this result by taking
    advantage of the fact that we split each song into 10 fragments and use the information
    across the chunks to get to a result. Majority voting would be one strategy, but
    it turns out that going with whatever chunk the model is most sure of works even
    better. We can do this by splitting the data back into 100 chunks and applying
    `argmax` on each of them. This will get us for each one the index in the entire
    chunk. By applying modulo 10 we get the index into our label set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This gets us up to 75% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With 100 songs for each of our 10 genres, we don’t have a lot of training data.
    Splitting our songs up into 10 chunks of 3 seconds each gets us to somewhere half
    decent, although our model still ends up overfitting a bit.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to explore would be to apply some data augmentation techniques. We
    could try adding noise to the music, speeding it up a bit, or slowing it down
    though the spectrogram itself might not really change that much. It would be better
    to get our hands on a larger set of music.
  prefs: []
  type: TYPE_NORMAL
- en: 15.3 Visualizing Confusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you show the mistakes that the network makes in a clear way?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graphically display a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'A confusion matrix has columns for each of the genres representing the truth
    and rows for the genres the model predicted. The cells contain the counts for
    each (truth, prediction) pair. `sklearn` comes with a handy method to calculate
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize this a bit more clearly by shading the matrix. Transposing
    the matrix so we can see the confusion per row also makes things a bit easier
    to process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Confusion Matrix](assets/dlcb_15in03.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Confusion matrices are a neat way to display the performance of a network, but
    they also give you an idea of where it goes wrong, which might hint at how to
    improve things. In the example in this recipe we can see that the network does
    very well at distinguishing classical music and metal from other types of music,
    but it does less well at distinguishing rock from country. None of this is unexpected,
    of course.
  prefs: []
  type: TYPE_NORMAL
- en: 15.4 Indexing Existing Music
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to build an index over pieces of music that captures their style.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Treat the last fully connected layer of the model as an embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 10](ch10.html#image_search) we built a reverse search engine for
    images by interpreting the last fully connected layer of an image recognition
    network as image embeddings. We can do something similar with music. Let’s start
    by collecting some MP3s—you probably have a collection of them lying around somewhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we’ll index them. As before, we extract a melspectrogram. We also fetch
    the MP3 tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to index every spectrogram of all MP3s—we can do that in one batch
    if we concatenate all of them together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To get to the vector representation, we’ll construct a model that returns the
    fourth-to-last layer from our previous model and run it over the collected spectra:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple nearest neighbor model lets us now find similar songs. Given a song,
    we’ll look up for each of its vectors what the other nearest vectors are. The
    very first result we can skip, since it is the vector itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Trying this out on a random song seems to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Indexing songs using the last fully connected layer of our model works reasonably
    well. In this example it not only finds the original song, but also a slightly
    different version of that song that happens to be in the MP3 collection. Whether
    the other two songs returned are really similar in style is a judgment call, but
    they are not completely different.
  prefs: []
  type: TYPE_NORMAL
- en: The code here could be used as a basis to build something like Shazam; record
    a bit of music, run that through our vectorizer, and see which indexed song it
    matches most closely. Shazam’s algorithm is different and predates the popularity
    of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: By taking a short bit of music and finding other music that sounds similar,
    we have the basics for a music recommender system. The fact that it only works
    for music we already have access to does limit its usefulness a bit, though. In
    the rest of this chapter we’ll look at another approach to building a music recommender
    system.
  prefs: []
  type: TYPE_NORMAL
- en: 15.5 Setting Up Spotify API Access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you get access to a large set of music data?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the Spotify API.
  prefs: []
  type: TYPE_NORMAL
- en: The system we created in the previous recipe is a sort of music recommender,
    but it only recommends songs it has already seen. By harvesting playlists and
    songs from the Spotify API we can build up a much larger training set. Let’s start
    by registering a new app at Spotify. Head over to [*https://beta.developer.spotify.com/dashboard/applications*](https://beta.developer.spotify.com/dashboard/applications),
    and create a new application.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The URL mentioned here starts with beta. By the time you are reading this, the
    new application interface on Spotify might have come out of beta and the URL might
    have changed.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to log in first and possibly register before that. Once you’ve created
    an app, go to the app page and note the Client ID and the Client Secret. Since
    the secret is, well, secret, you’ll need to press the button to show it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter your various details in three constants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now access the Spotify API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The first time you run this code, the API will ask you to enter a URL into a
    browser. This works somewhat awkwardly when run from a notebook; the URL to redirect
    to will be printed in the window where your notebook server runs. However, if
    you press the Stop button in the browser, it will show you the URL to redirect
    to. Click on that URL. It will redirect to something starting with *http://127.0.0.1*
    that won’t resolve, but that doesn’t matter. Enter that URL back into the box
    that now shows up in the notebook page and press Enter. This should authorize
    you.
  prefs: []
  type: TYPE_NORMAL
- en: You only need to do this once; the token gets stored locally in a file named
    *.cache-<username>*. If something goes wrong, delete this file and try again.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spotify API is a remarkably great source for musical data. The API is accessible
    through a nicely designed REST API with well-defined endpoints that return self-describing
    JSON documents.
  prefs: []
  type: TYPE_NORMAL
- en: The [API documentation](https://developer.spotify.com/web-api/) has information
    on how to access songs, artists, and playlists, including rich metainformation
    like album covers.
  prefs: []
  type: TYPE_NORMAL
- en: 15.6 Collecting Playlists and Songs from Spotify
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to create a training set for your music recommender.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Search for common words to find playlists and fetch the songs that belong to
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'As rich as the Spotify API is, there is no easy way to get a set of public
    playlists. You can search for them by word, though. In this recipe we’ll use that
    as a way to get access to a nice body of playlists. Let’s start by implementing
    a function to fetch all playlists matching a search term. The only complication
    in the code is due to the fact that we need to recover from timeouts and other
    errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll start with one word, “a,” and fetch 5,000 playlists that contain that
    word. We’ll keep track of all those playlists, but also count the words that occur
    in the titles of those playlists. That way when we’re done with the word “a,”
    we can do the same with the word that occurs most. We can keep doing this until
    we have enough playlists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The playlists we fetched don’t actually contain the songs; for this we need
    to do a separate call. To get all the tracks of a playlist, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Getting a large set of songs and playlists can take a significant amount of
    time. To get some decent results, we need at least 100,000 playlists, but something
    closer to a million would be better. Getting 100,000 playlists and their songs
    takes about 15 hours on a decent connection—it’s doable, but not something you’d
    want to do over and over again, so we’d better save the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to store three datasets. The first contains the playlist information
    itself—we don’t actually need this for the next recipe, but it is useful to check
    things. Secondly, we’ll store the IDs of the songs in the playlists in a big text
    file. And finally, we’ll store the per-song information. We’ll want to be able
    to look up these details in a dynamic fashion, so we’re going to use a SQLite
    database for this. We’ll write out the results as we collect song information
    to keep memory usage under control:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe we looked at building up a database of playlists and their songs.
    Since there is no clear way to get a balanced sample of public playlists from
    Spotify, we took the approach of using the search interface and trying popular
    keywords. While this works, the set we’ve acquired is hardly unbiased.
  prefs: []
  type: TYPE_NORMAL
- en: For one thing, we get the popular keywords from the playlists that we fetched.
    This does give us words that are relevant for music, but can easily increase the
    skewing we already have. If we end up with playlists that are disproportionately
    about country music then our word lists will also start to fill up with country-related
    words, which in turn will have us fetch more country music.
  prefs: []
  type: TYPE_NORMAL
- en: The other bias risk is that fetching playlists that contain popular words will
    get us popular songs. Terms like “greatest” and “hits” will occur often and cause
    us to get a lot of greatest hits; niche albums have less of a chance to be picked
    up.
  prefs: []
  type: TYPE_NORMAL
- en: 15.7 Training a Music Recommender
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve fetched a large set of playlists, but how do you use them to train your
    music recommender system?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use an off-the-shelf Word2vec model and treat song IDs as words.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#word_embeddings) we explored how a Word2vec model projects
    words into a semantic space with nice properties; similar words end up in the
    same neighborhood and relations between words are somewhat consistent. In [Chapter 4](ch04.html#movie_recommender)
    we used an embedding technique to build a movie recommender. In this recipe we
    combine both approaches. Rather than training our own model, we’ll use an off-the-shelf
    model for Word2vec, but we’ll use the results to build a recommender for music.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `gensim` module we used in [Chapter 3](ch03.html#word_embeddings) also
    comes with the possibility to train a model. All it needs is an iterator that
    produces series of tokens. This isn’t too hard since we have our playlists stored
    as lines in a file, with each line containing the IDs of the songs separated by
    spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After that training the model is a single-line operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on how many songs/playlists the previous recipe resulted in, this
    could take a while. Let’s save the model for future use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 15.8 Recommending Songs Using a Word2vec Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you use your model to predict songs based on an example?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the Word2vec distances and your SQLite3 database of songs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to get a set of `song_id`s given a song name or part of it.
    The `LIKE` operator will get us a selection of songs that match the searched-for
    pattern. Song names, though, are hardly unique these days. Even for the same artists
    there are different versions around. So we need some way of scoring them. Luckily,
    we can use the `vocab` property of our model—the records in it have a *count*
    property. The more often a song appears in our playlists, the more likely it is
    that it is the song we are after (or at least the song we know most about):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can pick the song we really are after, in this case possibly the one
    by Survivor. Now on to suggesting songs. We let our model do the heavy lifting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have a lookup table from song ID to score, which we can easily expand
    to a list of actual songs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for “The Eye of the Tiger” is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This looks like a decent mix of upbeat ’80s-ish music.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using Word2vec is an effective way to create a song recommender. Rather than
    training our own model as we did in [Chapter 4](ch04.html#movie_recommender),
    we used an off-the-shelf model here from `gensim`. There is less tuning, but it
    works well since the words in a sentence and songs in a playlist are fairly comparable.
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec works by trying to predict a word from its context. This prediction
    leads to an embedding that causes words that are similar to each other to appear
    near each other. Running the same process over songs in a playlist means trying
    to predict a song based on the context of the song in the playlist. Similar songs
    end up near each other in the song space.
  prefs: []
  type: TYPE_NORMAL
- en: With Word2vec it turns out that relations between words also have meaning. The
    vector separating the words “queen” and “princess” is similar to the vector separating
    “king” and “prince.” It would be interesting to see if something similar can be
    done with songs—what is the Beatles version of “Paint It Black” by the Rolling
    Stones? This would, however, require us to somehow project artists into the same
    space.
  prefs: []
  type: TYPE_NORMAL
