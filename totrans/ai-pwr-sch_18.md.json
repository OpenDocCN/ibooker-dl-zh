["```py\nQuery: \"What is a unicorn?\"\n\nResponse:\nA unicorn is a mythical creature that is typically depicted as a horse\nwith a single, spiraled horn projecting from its forehead. Unicorns have\nbeen a part of human mythology and folklore for centuries and are often\nassociated with purity, grace, and magic. They are said to possess healing\npowers and to be able to bring good luck to those who encounter them.\nWhile unicorns are not real animals, they continue to be a popular symbol\nin modern culture, appearing in literature, art, and popular media.\n```", "```py\nQuery: \"You are a chatbot named AIPS Chat.\n        You have a Ph.D. in biology.\n        What is a unicorn?\"\n\nResponse:\nHello! I am a chatbot named AIPS Chat, and I have a Ph.D. in biology. While\nunicorns have been depicted in various cultures throughout history, they\nare not real animals that exist in the natural world.\n\nFrom a biological perspective, a unicorn can be described as a mythical\ncreature resembling a horse with a single, spiraled horn on its forehead.\nHowever, since unicorns are purely fictional, there is no actual\nscientific data or research that can be conducted on them.\n\nIt is important to note that while many animals possess unique\ncharacteristics, such as the horn of a rhinoceros or the tusk of a\nnarwhal, these features are not magical and do not provide any special\nabilities or powers to the animal possessing them.\n\nIn conclusion, while unicorns are a fascinating aspect of human\nfolklore and mythology, they do not have a basis in reality within the\nfield of biology.\n```", "```py\nWeb search results:\n\n[1] \"A large language model, or LLM, is a deep learning algorithm that\ncan recognize, summarize, translate, predict and generate text and other\ncontent based on knowledge gained from massive datasets.\"\nURL: https://blogs.nvidia.com/blog/2023/\n     01/26/what-are-large-language-models-used-for/\n\n[2] A large language model (LLM) is a language model consisting of a\nneural network with many parameters (typically billions of weights or\nmore), trained on large quantities of unlabeled text using\nself-supervised learning. LLMs emerged around 2018 and perform well at a\nwide variety of tasks. This has shifted the focus of natural language\nprocessing research away from the previous paradigm of training\nspecialized supervised models for specific tasks.\nURL: https://en.wikipedia.org/wiki/Large_language_model\n\n[3] \"In this guide, we'll discuss everything you need to know about\nLarge Language Models (LLMs), including key terms, algorithms,\nfine-tuning, and more. As ChatGPT has taken the internet by storm crossing\n1 million users in its first 5 days, you may be wondering what machine\nlearning algorithm is running under the hood. While ChatGPT uses a\nspecific type of reinforcement learning called \"Reinforcement Learning\nfrom Human Feedback (RLHF)\", at a high level it is an example of a\nLarge Language Model (LLM).\"\nURL: https://www.mlq.ai/what-is-a-large-language-model-llm/\n\nInstructions: Using the provided web search results, write a comprehensive\nreply to the given query. Make sure to cite results using\n[[number](URL)] notation after the reference. If the provided search\nresults refer to multiple subjects with the same name, write separate\nanswers for each subject.\n\nQuery: What is a large language model?\n```", "```py\nA large language model (LLM) is a deep learning algorithm that can\nrecognize, summarize, translate, predict, and generate text and other\ncontent based on knowledge gained from massive datasets\n[[1](https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-\n     models-used-for/)].\nIt consists of a neural network with many parameters (typically billions\nof weights or more) and is trained on large quantities of unlabeled text\nusing self-supervised learning\n[[2](https://en.wikipedia.org/wiki/Large_language_model)]. LLMs emerged\naround 2018 and have performed well at a wide variety of tasks, shifting\nthe focus of natural language processing research away from the previous\nparadigm of training specialized supervised models for specific tasks\n[[2](https://en.wikipedia.org/wiki/Large_language_model)]. One example of\na LLM is ChatGPT, which uses a specific type of reinforcement learning\ncalled \"Reinforcement Learning from Human Feedback (RLHF)\"\n[[3](https://www.mlq.ai/what-is-a-large-language-model-llm/)].\n```", "```py\n...\nQuery: What is a large language model? Be concise.\n```", "```py\nA large language model (LLM) is a deep learning algorithm that uses neural\nnetworks with billions of parameters, trained on massive unlabeled text\ndatasets for various language tasks [[2](https://en.wikipedia.org/\nwiki/Large_language_model)][[1](https://blogs.nvidia.com/blog/2023/01/\n26/what-are-large-language-models-used-for/)].\n```", "```py\nWhat are some queries that should find the following documents?  List at\nleast 5 unique queries, where these documents are better than others\nin an outdoors question and answer dataset. Be concise and only\noutput the list of queries, and a result number in the format [n] for\nthe best result in the resultset. Don't print a relevance summary at\nthe end.\n\n### Results:\n{resultset}\n```", "```py\nexample_contexts = retriever(\"What are minimalist shoes?\")\nresultset = [f\"{idx}. {ctx}\" for idx, ctx  #1\n  in enumerate(list(example_contexts[0:5][\"context\"]))]  #1\nprint(\"\\n\".join(resultset))\n```", "```py\n0\\. Minimalist shoes or \"barefoot\" shoes are shoes that provide your feet\n     with some form of protection, but get you as close to a barefoot\n     experience as possible...\n1\\. There was actually a project done on the definition of what a minimalist\n     shoe is and the result was \"Footwear providing minimal interference\n     with the natural movement of the foot due to its high flexibility,\n     low heel to toe drop, weight and stack height, and the absence of\n     motion control and stability devices\". If you are looking for a\n     simpler definition, this is what Wikipedia says, Minimalist shoes are\n     shoes intended to closely approximate barefoot running conditions...\n2\\. One summer job, I needed shoes to walk on a rocky beach, sometimes in\n     the water, for 5 to 10 miles per day all summer. Stretchy neoprene\n     shoes were terrible for this- no traction and no support. So I used\n     regular sneakers. I chose a pair of cross country racing flats... The\n     uppers were extremely well ventilated polyester, so they drained very\n     quickly, and the thin material dried much faster than a padded sandal,\n     and certainly much faster than a regular sneaker of leather or cotton\n     would... The thing to look for is thin fabric that attaches directly\n     to the sole, with no rubber rim that would keep the water from\n     draining...\n3\\. ... It's not unhealthy to wear minimalist footwear, but on what terrain\n     your wear them could be bad for your body in the long run. Human\n     beings were never meant to walk or run exclusively on hard pavement\n     or sidewalks. Nor were we designed to clamber around on sharp rocks\n     at high elevations... If you're running on soft ground and you have\n     the foot strength, then there are plenty of arguments in favour of\n     minimalist shoes being better for you than other shoes, because it\n     brings your posture and gait back to what nature intended it to be.\n     If you're hiking in the mountains on uneven rocky terrain, especially\n     while carrying a heavy bag, then you'd be better off wearing a\n     supportive hiking boot...\n4\\. ... My favourite barefoot shoes are Vibram Five-Fingers , I wear either\n     my Mouri's or my KSO's at the beach. Vibram Five-Fingers 'Signa'\n     watersport shoes: The thin sole will be enough to protect your feet\n     from the lava sand...\n```", "```py\nresultset_text = \"\\n\".join(resultset)\nresultset_prompt = summarize_search_prompt.replace(\"{resultset}\",\n                                                   resultset_text)\ngenerated_relevance_judgments = get_generative_response(resultset_prompt)\ndisplay(generated_relevance_judgments)\n```", "```py\n1\\.    What is the definition of a minimalist shoe?\n2\\.    What are the characteristics of minimalist shoes?\n3\\.    Which shoes are best for walking on rocky beaches?\n4\\.    Are minimalist shoes suitable for all terrains?\n5\\.    What are some recommended barefoot shoe brands?\n\nResults:\n\n1\\.    [1]\n2\\.    [0]\n3\\.    [2]\n4\\.    [3]\n5\\.    [4]\n```", "```py\ndef extract_pairwise_judgments(text, contexts):\n  query_pattern = re.compile(r\"\\d+\\.\\s+(.*)\")  #1\n  result_pattern = re.compile(r\"\\d+\\.\\s+\\[(\\d+)\\]\")  #1\n  lines = text.split(\"\\n\")\n  queries = []\n  results = []\n  for line in lines:\n    query_match = query_pattern.match(line)\n    result_match = result_pattern.match(line)\n    if result_match:\n      result_index = int(result_match.group(1))\n      results.append(result_index)\n    elif query_match:\n      query = query_match.group(1)\n      queries.append(query)\n  output = [{\"query\": query, \"relevant_document\": contexts[result][\"id\"]}\n            for query, result in zip(queries, results)]\n  return output\n```", "```py\nresultset_contexts = example_contexts.to_dict(\"records\") #1\noutput = extract_pairwise_judgments(\n           generated_relevance_judgments,\n           resultset_contexts)\ndisplay(output)\n```", "```py\n{\"query\": \"What is the definition of a minimalist shoe?\",\n \"relevant_document\": 18370}\n{\"query\": \"What are the characteristics of minimalist shoes?\",\n \"relevant_document\": 18376}\n{\"query\": \"Which shoes are best for walking on rocky beaches?\",\n \"relevant_document\": 18370}\n{\"query\": \"Are minimalist shoes suitable for all terrains?\",\n \"relevant_document\": 18375}\n{\"query\": \"What are some recommended barefoot shoe brands?\",\n \"relevant_document\": 13540}\n```", "```py\nWalter Silverman of Brighton owned one of the most successful local Carvel\nfranchises, at East Ridge Road and Hudson Avenue in Irondequoit.\nHe started working for Carvel in 1952\\. This is how it appeared in the\nlate 1970s/early 1980s.\n[Alan Morrell, Democrat and Chronicle, May 29, 2023]\n```", "```py\n<per>Walter Silverman</per> of <loc>Brighton</loc> owned one of the most\nsuccessful local <org>Carvel</org> franchises, at\n<loc>East Ridge Road</loc> and <loc>Hudson Avenue</loc> in\n<loc>Irondequoit</loc>. He started working for <org>Carvel</org> in\n1952\\. This is how it appeared in the late 1970s/early 1980s.\n```", "```py\ndef extract_entities(text):\n  entities = []\n  pattern = r\"<(per|loc|org)>(.*?)<\\/(per|loc|org)>\"\n  matches = re.finditer(pattern, text)\n  for match in matches:\n    entity = {\"label\": match.group(1).upper(),\n              \"offset\": [match.start(), match.end() - 1],\n              \"text\": match.group(2)}\n    entities.append(entity)\n  return entities\n\nentities = extract_entities(news_article_labelled)\ndisplay(entities)\n```", "```py\n[{\"label\": \"PER\", \"offset\": [0, 26], \"text\": \"Walter Silverman\"},\n {\"label\": \"LOC\", \"offset\": [31, 49], \"text\": \"Brighton\"},\n {\"label\": \"ORG\", \"offset\": [90, 106], \"text\": \"Carvel\"},\n {\"label\": \"LOC\", \"offset\": [123, 148], \"text\": \"East Ridge Road\"},\n {\"label\": \"LOC\", \"offset\": [154, 177], \"text\": \"Hudson Avenue\"},\n {\"label\": \"LOC\", \"offset\": [182, 203], \"text\": \"Irondequoit\"},\n {\"label\": \"ORG\", \"offset\": [229, 245], \"text\": \"Carvel\"}]\n```", "```py\nFor a given passage, please identify and mark the following entities:\npeople with the tag '<per>', locations with the tag '<loc>', and\norganizations with the tag '<org>'. Please repeat the passage below\nwith the appropriate markup.\n### {text}\n```", "```py\ndef normalize_embedding(embedding):  #1\n  return numpy.divide(embedding,  #1\n    numpy.linalg.norm(embedding,axis=0)).tolist()  #1\n\ndef read(cache_name):  #2\n  cache_file_name = f\"data/tmdb/{cache_name}.pickle\"  #2\n  with open(cache_file_name, \"rb\") as fd:  #2\n    return pickle.load(fd)  #2\n\ndef tmdb_with_embeddings_dataframe():  #2\n  movies = read(\"movies_with_image_embeddings\")  #2\n  embeddings = movies[\"image_embeddings\"]\n  normalized_embeddings = [normalize_embedding(e)  #1\n                           for e in embeddings]  #1\n  movies_dataframe = spark.createDataFrame(\n    zip(movies[\"movie_ids\"], movies[\"titles\"],\n        movies[\"image_ids\"], normalized_embeddings),\n    schema=[\"movie_id\", \"title\", \"image_id\", \"image_embedding\"])\n  return movies_dataframe\n\nembeddings_dataframe = tmdb_with_embeddings_dataframe()  #3\nembeddings_collection = engine.create_collection(  #3\n                          \"tmdb_with_embeddings\")  #3\nembeddings_collection.write(embeddings_dataframe)  #3\n```", "```py\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"  #1\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)  #1\n\ndef movie_search(query_embedding, limit=8):   #2\n  collection = engine.get_collection(\"tmdb_with_embeddings\")\n  request = {\"query_vector\": query_embedding, #3\n             \"query_field\": \"image_embedding\",  #3\n             \"return_fields\": [\"movie_id\", \"title\",  #3\n                               \"image_id\", \"score\"],  #3\n             \"limit\": limit,  #3\n             \"quantization_size\": \"FLOAT32\"} #4\n  return collection.search(**request)\n\ndef encode_text(text, normalize=True):  #5\n  text = clip.tokenize([text]).to(device)  #5\n  text_features = model.encode_text(text)  #5\n  embedding = text_features.tolist()[0]  #5\n  if normalize:  #5\n    embedding = normalize_embedding(embedding)  #5\n  return embedding  #5\n\ndef encode_image(image_file, normalize=True):  #6\n  image = load_image(image_file)  #6\n  inputs = preprocess(image).unsqueeze(0).to(device)  #6\n  embedding = model.encode_image(inputs).tolist()[0]  #6\n  if normalize:  #6\n    embedding = normalize_embedding(embedding)  #6\n  return embedding #6\n\ndef encode_text_and_image(text_query, image_file):  #7\n  text_embedding = encode_text(text_query, False)\n  image_embedding = encode_image(image_file, False)\n  return numpy.average((normalize_embedding(  #8\n    [text_embedding, image_embedding])), axis=0).tolist()  #8\n```", "```py\ndef search_and_display(text_query=\"\", image_query=None):\n  if image_query:\n    if text_query:\n      query_embedding = encode_text_and_image(text_query, image_query)\n    else:\n      query_embedding = encode_image(image_query)\n  else:\n    query_embedding = encode_text(text_query)\n  display_results(movie_search(query_embedding), show_fields=False)\n```", "```py\nsearch_and_display(text_query=\"singing in the rain\")\n```", "```py\nsearch_and_display(image_query=\"chapters/ch15/delorean-query.jpg\")\n```", "```py\nsearch_and_display(text_query=\"superhero\",\n                   image_query=\"chapters/ch15/delorean-query.jpg\")\n```", "```py\nUser: \"Please, take me home.\"\nPhone: \"I don't know where you live.\"\n```", "```py\nUser: \"Take me to the plumbing store.\"\nPhone: \"Here is a list of stores that I found. [shares list]\"\n\nUser: \"Go to the one in East Rochester.\"\nPhone: [Provides directions to the geographical center of East Rochester,\n        not the plumbing store it suggested in the last exchange]\n```", "```py\ndef reciprocal_rank_fusion(search_results, k=None):\n  if k is None: k = 60\n  scores = {}\n  for ranked_docs in search_results:  #1\n    for rank, doc in enumerate(ranked_docs, 1):  #2\n      scores[doc[\"id\"]] = (scores.get(doc[\"id\"], 0) +  #3\n                           (1.0 / (k + rank)))  #3\n  sorted_scores = dict(sorted(scores.items(), #4\n    key=lambda item: item[1], reverse=True))  #4\n  return sorted_scores  #4\n```", "```py\nover_request_limit = 15\nbase_query = {\"return_fields\": [\"id\", \"title\", \"id\", \"image_id\",\n                                \"movie_id\", \"score\", \"image_embedding\"],\n              \"limit\": over_request_limit,\n              \"order_by\": [(\"score\", \"desc\"), (\"title\", \"asc\")]}\n\ndef lexical_search_request(query_text):\n  return {\"query\": query_text,\n          \"query_fields\": [\"title\", \"overview\"],\n          \"default_operator\": \"OR\",\n          **base_query}\n\ndef vector_search_request(query_embedding):\n  return {\"query\": query_embedding,\n          \"query_fields\": [\"image_embedding\"],\n          \"quantization_size\": \"FLOAT32\",\n          **base_query}\n\ndef display_lexical_search_results(query_text):\n  collection = engine.get_collection(\"tmdb_lexical_plus_embeddings\")\n  lexical_request = lexical_search_request(query_text)\n  lexical_search_results = collection.search(**lexical_request)\n\n  display_results(lexical_search_results, display_header= \\\n                  get_display_header(lexical_request=lexical_request))\n\ndef display_vector_search_results(query_text):\n  collection = engine.get_collection(\"tmdb_lexical_plus_embeddings\")\n  query_embedding = encode_text(query_text)\n  vector_request = vector_search_request(query_embedding)\n  vector_search_results = collection.search(**vector_request)\n\n  display_results(vector_search_results, display_header= \\\n                  get_display_header(vector_request=vector_request))\n\nquery = '\"' + \"singin' in the rain\" + '\"'\ndisplay_lexical_search_results(query)\ndisplay_vector_search_results(query)\n```", "```py\ndef display_hybrid_search_results(text_query, limit=8):\n  lexical_search = lexical_search_from_text_query(text_query)\n  vector_search = vector_search_from_embedding(encode_text(text_query))\n  hybrid_search_results = collection.hybrid_search(\n        [lexical_request, vector_request], limit=10,\n        algorithm=\"rrf\", algorithm_params={\"k\": 60})\n  display_header = get_display_header(lexical_search, vector_search)\n  display_results(hybrid_search_results, display_header)\n\ndisplay_hybrid_search_results(query)\n```", "```py\ncollection.hybrid_search([lexical_search, vector_search], limit=10,\n                         algorithm=\"rrf\", algorithm_params={\"k\": 60})\n```", "```py\nquery = \"the hobbit\"\ndisplay_lexical_search_results(query)\ndisplay_vector_search_results(query)\ndisplay_hybrid_search_results(query)\n```", "```py\ndef lexical_vector_rerank(text_query, limit=10):\n  lexical_request = lexical_search_request(text_query)\n  vector_request = vector_search_request(encode_text(text_query))\n  hybrid_search_results = collection.hybrid_search(\n     [lexical_request, vector_request],\n     algorithm=\"lexical_vector_rerank\", limit=limit)\n  header = get_display_header(lexical_request, vector_request)\n  display_results(hybrid_search_results, display_header=header)\n\nlexical_vector_rerank(\"the hobbit\")\n```", "```py\n{'query': 'the hobbit',\n 'query_fields': ['title', 'overview'],\n 'default_operator': 'OR',\n  ...\n 'order_by': [('score', 'desc'), ('title', 'asc')],\n 'rerank_query': {\n   'query': [-0.016278795212303375, ..., -0.02110762217111629],\n   'query_fields': ['image_embedding'],\n   'quantization_size': 'FLOAT32', ...\n   'order_by': [('score', 'desc'), ('title', 'asc')], ...\n }\n}\n```"]