["```py\nversion: '3.7'\nservices:\n\n  vector-db:\n    image: redis/redis-stack:latest\n    ports:\n      - 6379:6379\n      - 8001:8001\n    environment:\n      - REDISEARCH_ARGS=CONCURRENT_WRITE_MODE\n    volumes:\n      - vector-db:/var/lib/redis\n      - ./redis.conf:/usr/local/etc/redis/redis.conf\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"-h\", \"localhost\", \"-p\", \"6379\", \"ping\"]\n      interval: 2s\n      timeout: 1m30s\n      retries: 5\n      start_period: 5s\n\nvolumes:\n  vector-db:\n```", "```py\nSCHEMA = [\n    TagField(\"url\"),\n    TextField(\"title\"), \n    TextField(\"description\"),\n    TextField(\"publish_date\"),\n    TextField(\"content\"),\n    VectorField(\"embedding\", \"HNSW\",\n                {\"TYPE\": \"FLOAT32\",\n                 \"DIM\": 1536,\n                 \"DISTANCE_METRIC\": \"COSINE\"}),\n   ]\n```", "```py\nsetx REDIS_HOST \"your-host-details\"\nsetx REDIS_PORT \"Port-number-exposed\"\nsetx REDIS_PASSWORD \"Password-required-to-connect\"\n```", "```py\nexport REDIS_HOST=\"your-host-details\"\nexport REDIS_PORT=\"Port-number-exposed\"\nexport REDIS_ PASSWORD=\"Password-required-to-connect\"Bash:\necho export REDIS_HOST=\"your-host-details\" >> /etc/environment && source /etc/environment\necho export REDIS_PORT=\"Port-number-exposed\" >> /etc/environment && source /etc/environment\necho export REDIS_ PASSWORD=\"Password-required-to-connect\" >> /etc/environment && source /etc/environment\n```", "```py\n# Connect to the Redis server\nconn = redis.Redis(host=redis_host, \n                   port=redis_port,\n                   password=redis_password, \n                   encoding='utf-8', \n                   decode_responses=True)\n```", "```py\nconn.ft(index_name).create_index(\n     fields=schema,\n     definition=IndexDefinition(prefix=[\"post:\"],\n                                       index_type=IndexType.HASH))\n```", "```py\nimport redis\nfrom redis.commands.search.field import VectorField, TextField\nfrom redis.commands.search.query import Query\nfrom redis.commands.search.indexDefinition import \n     ↪IndexDefinition, IndexType\nfrom redis.commands.search.field import TagField\n\nredis_host = os.getenv('REDIS_HOST')           #1\nredis_port = os.getenv('REDIS_PORT')          #1\nredis_password = os.getenv('REDIS_PASSWORD')  #1\n\nconn = redis.Redis(host=redis_host,           #2\n                   port=redis_port,          #2\n                   password=redis_password,  #2\n                   encoding='utf-8',         #2          \n                   decode_responses=True)    #2          \n\nSCHEMA = [\n    TagField(\"url\"),\n    TextField(\"title\"), \n    TextField(\"description\"),\n    TextField(\"publish_date\"),\n    TextField(\"content\"),\n    VectorField(\"embedding\", \"HNSW\",\n                {\"TYPE\": \"FLOAT32\",\n                 \"DIM\": 1536,                       #3\n                 \"DISTANCE_METRIC\": \"COSINE\"}),\n]\n\ndef create_index(conn, schema, index_name=\"posts\"):\n    try:\n        conn.ft(index_name).create_index(\n            fields=schema,\n            definition=IndexDefinition(prefix=[\"post:\"],\n                                       index_type=IndexType.HASH))\n    except Exception as e:\n        print(\"Index already exists\")\n\ndef delete_index(conn, index_name=\"posts\"):            #4\n    try:\n        conn.execute_command('FT.DROPINDEX', index_name)\n    except Exception as e:\n        print(\"Failed to delete index: \", e)\n\ndef delete_all_keys_from_index(conn, index_name=\"posts\"):    #5\n    try:\n        # 1\\. Retrieve all document IDs from the index.\n        # This assumes the total number of documents isn't large. \n        # If it is, you might want to paginate the query.\n        result = conn.execute_command('FT.SEARCH',\n                                      index_name,\n                                      '*', \n                                      'NOCONTENT')\n\n        # 2\\. Parse the result to get document IDs.\n        # Skip the first element which is the total count.\n        # Taking every second element starting from the first.\n        doc_ids = result[1::2]\n\n        # 3\\. Delete each document key.\n        for doc_id in doc_ids:\n            conn.delete(doc_id)\n\n    except Exception as e:\n        print(\"Failed to delete keys: \", e))\n\ndef view_index(conn, index_name=\"posts\"):                 #6\n    try:\n        info = conn.execute_command('FT.INFO', index_name)\n        for i in range(0, len(info), 2):\n            print(f\"{info[i]}: {info[i+1]}\")\n    except Exception as e:\n        print(\"Failed to retrieve index details: \", e)\n\ndef main():\n    while True:                                       #7\n        print(\"1\\. View index details\")\n        print(\"2\\. Create index\")\n        print(\"3\\. Delete index\")\n        print(\"4\\. Exit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == '1':\n            # Call the function to view index\n            view_index(conn)\n            pass\n        elif choice == '2':\n            # Call the function to create index\n            create_index(conn, SCHEMA)\n        elif choice == '3':\n            # Call the function to delete index\n            delete_all_keys_from_index(conn)\n            delete_index(conn)\n        elif choice == '4':\n            break\n        else:\n            print(\"Invalid choice. Please enter a valid option.\")\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nindex_name: posts\nindex_options: []\nindex_definition: ['key_type', 'HASH', 'prefixes', ['post:'], 'default_score', '1']\nattributes: [['identifier', 'url', 'attribute', 'url', 'type', \n    ↪'TAG', 'SEPARATOR', ','], ['identifier', 'title', 'attribute', \n    ↪'title', 'type', 'TEXT', 'WEIGHT', '1'], ['identifier', \n    ↪'description', 'attribute', 'description', 'type', 'TEXT', \n    ↪'WEIGHT', '1'], ['identifier', 'publish_date', 'attribute', \n    ↪'publish_date', 'type', 'TEXT', 'WEIGHT', '1'], ['identifier', \n    ↪'embedding', 'attribute', 'embedding', 'type', 'VECTOR']]\nnum_docs: 1304\nmax_doc_id: 1304\nnum_terms: 3047\nnum_records: 14092\nvector_index_sz_mb: 12.586814880371094\ntotal_inverted_index_blocks: 4370\noffset_vectors_sz_mb: 0.011086463928222656\ndoc_table_size_mb: 0.09221076965332031\nkey_table_size_mb: 0.03916168212890625\ntotal_indexing_time: 708.988\n...\n```", "```py\n    r = requests.get(post.link)\n    soup = BeautifulSoup(r.text, 'html.parser')\n\n    # Get the title\n    try:    \n        article_title = soup.find('h1', {'class': 'post-title'}).text\n        article_title = article_title.replace(\"| Amit Bahree's  \n                                ↪(useless?) insight!\", \"\")\n    except AttributeError:\n        article_title = \"\"\n    print(\"\\tTitle:\" + article_title)\n\n    # get the post description\n    try:\n        article_desc = soup.find('div', {'class': 'post-description'}).text\n    except AttributeError as e:\n        #print(\"Error getting description: \", e)\n        article_desc = \"\"\n\n    # get the publish date\n    try:\n        temp = soup.find('div', {'class': 'post-meta'}, {'span', 'title'}).text\n        match = re.search(r\"(\\w+\\s\\d+,\\s\\d+)\", temp)\n        if match:\n            publish_date = match.group(1)\n    except AttributeError:\n        publish_date = \"\"\n\n    # get the article body\n    try:\n        article_body = soup.find('div', {'class': 'post-content'}).text\n    except AttributeError:\n        article_body = \"\"\n```", "```py\n    chunks = split_sentences_by_spacy(article, max_tokens=3000, overlap=10)\n    print(f\"Number of chunks: {len(chunks)}\")\n```", "```py\n# OpenAI API key\nopenai.api_key = os.getenv('OPENAI_API_BOOK_KEY')\n\n# Redis connection details\nredis_host = os.getenv('REDIS_HOST')\nredis_port = os.getenv('REDIS_PORT')\nredis_password = os.getenv('REDIS_PASSWORD')\n\ndef split_sentences_by_spacy(text, max_tokens, \n                        overlap=0, \n                        model=\"en_core_web_sm\"):\n...\n\n# count tokens\ndef count_tokens(...)\n...\n\ndef get_embedding(text):\n...\n\n# Connect to the Redis server\nconn = redis.Redis(...)\n\nSCHEMA = [ ... ]\n# URL of the RSS feed to parse\nurl = https://blog.desigeek.com/index.xml\n\n# Parse the RSS feed with feedparser\nprint(\"Parsing RSS feed...\")\nfeed = feedparser.parse(url)\n\n# get number of blog posts in feed\nblog_posts = len(feed.entries)\nprint(\"Number of blog posts: \", blog_posts)\n\np = conn.pipeline(transaction=False)\nfor i, post in enumerate(feed.entries):\n    # report progress\n    print(\"Create embedding and save for entry #\", i, \" of \", blog_posts)\n\n    # Extract the content – using BeautifulSoup\n    r = requests.get(post.link)\n    soup = BeautifulSoup(r.text, 'html.parser')\n\n    # Get the title\n...\n\n    # get the post description\n    ...\n\n    # get the publish date\n    ...\n\n    # get the article body\n    try:\n        article_body = soup.find('div', {'class': 'post-content'}).text\n    except AttributeError:\n        article_body = \"\"\n\n    # This should be chunked up\n    article = article_body\n\n    total_token_count = 0\n    chunks = []\n\n    # split the text into chunks by sentences\n    chunks = split_sentences_by_spacy(article, max_tokens=3000, overlap=10)\n    print(f\"Number of chunks: {len(chunks)}\")\n\n    for j, chunk in enumerate(tqdm(chunks))\n        vector = get_embedding(chunk)\n        # convert to numpy array\n        vector = np.array(vector).astype(np.float32).tobytes()\n\n        # Create a new hash with the URL and embedding\n        post_hash = {\n            \"url\": post.link,\n            \"title\": article_title,\n            \"description\": article_desc,\n            \"publish_date\": publish_date,\n            \"content\": chunk,\n            \"embedding\": vector\n        }\n\n        conn.hset(name=f\"post:{i}_{j}\", mapping=post_hash)\n\np.execute()\nprint(\"Vector upload complete.\")\n```", "```py\n$  python .\\search.py\nConnected to Redis\nEnter your query: Tell me about Longhorn\nVectorizing query...\nSearching for similar posts...\nFound 3 results:\nYou probably already heard this, but <strong>Chris Sells</strong> \n   ↪has a new column on MSDN called <strong>Longhorn Foghorn</strong>\n, that describes each of the â\n<strong>Pillars of Longhorn</strong>\nâ - This is something that IMHO developers would understand and \n↪appreciate. In the first article he explains the âPillarsâ and then \n↪in the next two goes onto build Solitaire. You can download the sample \n↪and play with it too.\nFrom OSNews: Microsft has made <em>hard statements about perfomance \n↪improvements in Longhorn ...\n```", "```py\ndef hybrid_search(query_vector, client, top_k=3, hybrid_fields=\"*\"):\n    base_query = f\"{hybrid_fields}=>\n                        [KNN {top_k} \n                        @embedding $vector AS vector_score]\"  #1\n    query = Query(base_query).return_fields(\n        \"url\",                                               #2\n        \"title\",                                            #2\n        \"publish_date\",                                     #2\n        \"description\",                                      #2\n        \"content\",                                          #2\n        \"vector_score\").sort_by(\"vector_score\").dialect(2)   #3\n    try:\n        results = client.ft(\"posts\").search(\n            query, query_params={\"vector\": query_vector})  #4\n    except Exception as e:\n        print(\"Error calling Redis search: \", e)\n        return None\n\n    if results.total == 0:\n        print(\"No results found for the given query vector.\")\n        return None\n\n    return results\n\n# Connect to the Redis server\nconn = redis.Redis(...)\n\nquery = input(\"Enter your query: \")      #5\n\nprint(\"Vectorizing query...\")\nquery_vector = get_embedding(query)    #6\n\nquery_vector = np.array(query_vector).astype(              #7\n\n                        np.float32).tobytes()\nprint(\"Searching for similar posts...\")\nresults = hybrid_search(query_vector, conn)                #8\n\nif results:\n    print(f\"Found {results.total} results:\")\n    for i, post in enumerate(results.docs):\n        score = 1 - float(post.vector_score)\n        print(post.content)\nelse:\n    print(\"No results found\")\n```", "```py\ndef hybrid_search(query_vector, client, top_k=5, hybrid_fields=\"*\"):\n...\n    return results\n\ndef get_search_results(query:str, max_token=4096, \n                       ↪debug_message=False) -> str:\n    query_vector = get_embedding(query)    #1\n\n    query_vector = np.array(query_vector).astype(\n        np.float32).tobytes()    #2\n\n    print(\"Searching for similar posts...\")\n    results = hybrid_search(query_vector, conn, top_k=5)   #3\n\n    token_budget = max_token - count_tokens(query)        #4\n    if debug_message:\n        print(f\"Token budget: {token_budget}\")\n\n    message = 'Use the blog post below to answer the subsequent \n               ↪question. If the answer cannot be found in the \n               ↪articles, write \"Sorry, I could not find an answer in \n               ↪the blog posts.\"'\n    question = f\"\\n\\nQuestion: {query}\"\n\n    if results:\n        for i, post in enumerate(results.docs):          #5\n            next_post = f'\\n\\nBlog post:\\n\"\"\"\\n{post.content}\\n\"\"\"'\n            new_token_usage = count_tokens(message + question + next_post)\n            if new_token_usage < token_budget:\n                if debug_message:\n                    print(f\"Token usage: {new_token_usage}\")\n                message += next_post\n            else:\n                break\n    else:\n        print(\"No results found\")\n\n    return message + question\n\ndef ask_gpt(query : str, max_token = 4096, debug_message = False) -> str:\n    message = get_search_results(                      #6\n        query,\n        max_token,\n        debug_message=debug_message)\n\n    messages = [                                   #7\n        {\"role\": \n         \"system\", \n         \"content\": \"You answer questions in summary from the [CA]\n                     blog posts.\"},\n        {\"role\":\n          \"user\",\n            \"content\": message},]\n\n    response = openai.ChatCompletion.create(           #8\n        model=\"gpt-3.5-turbo-16k\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=2000,\n        top_p=0.95\n    )\n    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n    return response_message\n\nif __name__ == \"__main__\":\n    # Enter a query\n    while True:\n        query = input(\"Please enter your query: \")\n        print(ask_gpt(query, max_token=15000, debug_message=False))\n        print(\"==\"*20)\n```"]