- en: Capitolo 9\. Proteggere i servizi di intelligenza artificiale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Questo lavoro è stato tradotto utilizzando l''AI. Siamo lieti di ricevere il
    tuo feedback e i tuoi commenti: [translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Nei capitoli precedenti, hai imparato a costruire servizi GenAI che servano
    vari generatori di IA supportando la concorrenza e lo streaming dei dati in tempo
    reale. Inoltre, hai integrato sistemi esterni come i database e hai implementato
    i tuoi meccanismi di autenticazione e autorizzazione. Infine, hai scritto una
    suite di test per verificare la funzionalità e le prestazioni dell'intero sistema.
  prefs: []
  type: TYPE_NORMAL
- en: In questo capitolo imparerai a implementare meccanismi di moderazione dell'uso
    e di protezione dagli abusi per proteggere i tuoi servizi GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Moderazione dell'uso e protezione dagli abusi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quando distribuisci i tuoi servizi GenAI, devi considerare come i tuoi servizi
    saranno utilizzati in modo improprio da utenti malintenzionati. Questo è essenziale
    per proteggere la sicurezza degli utenti e la tua reputazione. Non puoi sapere
    come gli utenti utilizzeranno il tuo sistema, quindi devi pensare al peggio e
    implementare *delle protezioni* per proteggerti da qualsiasi abuso o uso improprio.
  prefs: []
  type: TYPE_NORMAL
- en: Secondo un [recente studio sulle applicazioni nefaste della GenAI](https://oreil.ly/ihmzR),
    i tuoi servizi possono essere potenzialmente utilizzati con *intenti malevoli*,
    come descritto nella [Tabella 9-1](#malicious_intents).
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 9-1\. Intenti malevoli alla base dell'abuso dei servizi GenAI
  prefs: []
  type: TYPE_NORMAL
- en: '| Intento | Esempi | Casi reali |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Disonestà**Sostenere bugie e falsità | Plagio, falsificazione di competenze
    e conoscenze, falsificazione di documenti, imbrogli negli esami e nei colloqui,
    ecc. | Aumentano i casi di studenti che imbrogliano con l''intelligenza artificiale
    nelle università britanniche e australiane^([a](ch09.html#id1032)) |'
  prefs: []
  type: TYPE_TB
- en: '| **Propaganda**Distorsione della percezione della realtà per promuovere un''agenda
    | Impersonare altre persone, promuovere l''estremismo, influenzare le campagne,
    ecc. | I falsi conduttori di notizie AI che diffondono disinformazione o propaganda^([b](ch09.html#id1033))
    |'
  prefs: []
  type: TYPE_TB
- en: '| **L''inganno**Ingannare gli altri e creare false impressioni | Generazione
    di recensioni false, annunci truffaldini ed email di phishing, profili sintetici
    (ad esempio, sockpuppeting), ecc. | Lo studio di ingegneria Arup è stato vittima
    di una truffa deepfake da 25 milioni di dollari^([c](ch09.html#id1034)) |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch09.html#id1032-marker)) Fonti: *Times Higher Education* e *The Guardian*^([b](ch09.html#id1033-marker))
    Fonti: *The Guardian*, *MIT Technology Review* e *The Washington Post*^([c](ch09.html#id1034-marker))
    Fonti: CNN e *The Guardian* |'
  prefs: []
  type: TYPE_TB
- en: 'Lo stesso studio classifica gli abusi delle applicazioni GenAI come segue:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Disinformazione e disinformazione* per diffondere propaganda e fake news'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Amplificazione dei pregiudizi e discriminazione* per promuovere programmi
    razzisti ediscriminazioni sociali'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generazione di contenuti dannosi* attraverso la creazione di contenuti tossici,
    ingannevoli e radicalizzanti'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attacchi alla privacy dei dati* per colmare le lacune dei dati privati rubati
    e far trapelareinformazioni sensibili'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cyberattacchi automatizzati* per personalizzare gli attacchi di phishing e
    ransomware'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Furto d''identità e ingegneria sociale* per aumentare il tasso di successo
    delle truffe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deepfakes e manipolazione multimediale* per trarre profitto e alterare la
    percezione della realtà e le credenze sociali'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Truffa e frode* manipolando i mercati azionari e realizzando truffe mirate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non si tratta di un elenco esaustivo, ma dovrebbe darti qualche idea su quali
    misure di moderazione dell'uso prendere in considerazione.
  prefs: []
  type: TYPE_NORMAL
- en: '[Un altro studio sulla tassonomia delle tattiche di abuso della GenAI](https://oreil.ly/jbG01)
    ha analizzato l''abuso per modalità e ha rilevato che:'
  prefs: []
  type: TYPE_NORMAL
- en: Per la maggior parte dei tentativi di impersonificazione sono stati utilizzati*generatori
    audio e video*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I*generatori di immagini e di testo* sono stati utilizzati per la maggior parte
    dei sockpuppeting, il content farming per la manipolazione delle opinioni in scala
    e i tentativi di falsificazione.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I*generatori di immagini e video* sono stati utilizzati per la maggior parte
    dei tentativi di steganografia (cioè di nascondere messaggi codificati nei risultati
    del modello) e di generazione di contenuti intimi non consensuali (NCII).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Se stai creando dei servizi che supportano queste modalità, devi considerare
    le forme di abuso ad esse associate e implementare i relativi meccanismi di protezione.
  prefs: []
  type: TYPE_NORMAL
- en: Oltre all'uso improprio e all'abuso, dovrai anche considerare le vulnerabilità
    della sicurezza.
  prefs: []
  type: TYPE_NORMAL
- en: La sicurezza dei servizi GenAI è ancora un'area di ricerca al momento della
    stesura di questo articolo. Ad esempio, se i tuoi servizi sfruttano le LLM, OWASP
    ha classificato le [10 principali vulnerabilità delle LLM](https://oreil.ly/4zob2),
    come mostrato nella [Tabella 9-2](#llm_vulnerabilities).
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 9-2\. Le 10 principali vulnerabilità LLM di OWASP
  prefs: []
  type: TYPE_NORMAL
- en: '| Rischio | Descrizione |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Iniezione prompt | La manipolazione degli input per controllare le risposte
    del LLM porta ad accessi non autorizzati, violazioni dei dati e compromissione
    del processo decisionale. |'
  prefs: []
  type: TYPE_TB
- en: '| Gestione insicura dell''output | La mancata sanitizzazione o convalida degli
    output di LLM causa l''esecuzione di codice remoto sui sistemi a valle. |'
  prefs: []
  type: TYPE_TB
- en: '| Avvelenamento dei dati di formazione | Iniettare dati nelle fonti su cui
    i modelli vengono addestrati per compromettere la sicurezza, l''accuratezza o
    il comportamento etico. I modelli open source e i servizi RAG che si basano su
    dati web sono i più esposti a questi attacchi. |'
  prefs: []
  type: TYPE_TB
- en: '| Modello di negazione del servizio | Causare interruzioni del servizio ed
    esplosioni dei costi sovraccaricando gli LLMs con carichi pesanti e richieste
    simultanee. |'
  prefs: []
  type: TYPE_TB
- en: '| Vulnerabilità della catena di approvvigionamento | Causando la compromissione
    di vari componenti, tra cui le fonti di dati, e minando l''integrità del sistema.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Perdita di informazioni sensibili | Con conseguente esposizione accidentale
    di dati privati, responsabilità legali e perdita di vantaggi competitivi. |'
  prefs: []
  type: TYPE_TB
- en: '| Design insicuro del plug-in | Le vulnerabilità nelle integrazioni di terze
    parti causano l''esecuzione di codice remoto. |'
  prefs: []
  type: TYPE_TB
- en: '| Agenzia eccessiva | L''eccessiva autonomia degli LLMs nel compiere azioni
    può portare a conseguenze indesiderate e dannose. |'
  prefs: []
  type: TYPE_TB
- en: '| Eccessiva dipendenza dall''LLM | Compromettendo il processo decisionale,
    contribuendo alla vulnerabilità della sicurezza e alle responsabilità legali.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Modello furto | In relazione alla copia o all''utilizzo non autorizzato dei
    tuoi modelli. |'
  prefs: []
  type: TYPE_TB
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Vulnerabilità simili esistono anche per altri tipi di sistemi GenAI, come i
    generatori di immagini, audio, video e geometrie.
  prefs: []
  type: TYPE_NORMAL
- en: Ti consiglio di ricercare e identificare le vulnerabilità del software relative
    ai tuoi casi d'uso.
  prefs: []
  type: TYPE_NORMAL
- en: Senza protezioni, i tuoi servizi possono essere abusati per causare danni personali
    e finanziari, furti di identità, danni economici, diffondere disinformazione e
    contribuire a problemi sociali. Di conseguenza, è fondamentale implementare diverse
    misure di sicurezza e protezioni per proteggere i tuoi servizi da tali attacchi.
  prefs: []
  type: TYPE_NORMAL
- en: Nella prossima sezione scoprirai le misure di moderazione dell'uso e di sicurezza
    che puoi implementare per proteggere i tuoi servizi GenAI prima della distribuzione.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrail
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I*guardrail* si riferiscono a *controlli investigativi* che hanno lo scopo di
    guidare la tua applicazione verso i risultati previsti. Sono incredibilmente diversi
    e possono essere configurati per adattarsi a qualsiasi situazione che potrebbe
    andare storta con i tuoi sistemi GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Ad esempio, i *guardrail I/O* sono progettati per verificare i dati che entrano
    in un modello GenAI e le uscite inviate ai sistemi o agli utenti a valle. Tali
    guardrail possono segnalare query inappropriate da parte dell'utente e convalidare
    il contenuto dell'uscita contro la tossicità, le allucinazioni o gli argomenti
    vietati. La[Figura 9-1](#guardrails) mostra l'aspetto di un sistema LLM una volta
    aggiunti i guardrail I/O.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0901](assets/bgai_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 9-1\. Confronto di un sistema LLM senza e con guardrail
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Non è necessario implementare guardrail da zero. Al momento della stesura di
    questo articolo, esistono framework guardrail open source precostituiti come NVIDIA
    NeMo Guardrails, LLM-Guard e Guardrails AI per proteggere i tuoi servizi. Tuttavia,
    potrebbero richiedere l'apprendimento di linguaggi correlati al framework e hanno
    come contropartita il rallentamento dei servizi e l'appesantimento dell'applicazione
    a causa di varie dipendenze esterne.
  prefs: []
  type: TYPE_NORMAL
- en: Altri guardrail commerciali disponibili sul mercato, come Open AI's Moderation
    API, Microsoft Azure AI Content Safety API e Google's Guardrails API, non sono
    open source o mancano di dettagli e contenuti per misurare i vincoli di qualità.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I guardrail rimangono un'area di ricerca attiva. Sebbene tali difese possano
    contrastare alcuni attacchi, gli attacchi più potenti supportati dall'intelligenza
    artificiale possono comunque aggirarli. Questo potrebbe portare a un [ciclo continuo
    e infinito di attacchi e difese](https://oreil.ly/xlUmw).
  prefs: []
  type: TYPE_NORMAL
- en: Sebbene l'ingegnerizzazione dei guardrail I/O a livello di applicazione non
    fornisca una protezione perfetta, i prossimi modelli GenAI potrebbero includere
    guardrail integrati all'interno del modello per migliorare le garanzie di sicurezza.
    Tuttavia, tali guardrail potrebbero avere un impatto sulle prestazioni dei tempi
    di risposta introducendo latenza nel sistema.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrail di ingresso
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lo scopo dei guardrail di ingresso è quello di impedire che contenuti dannosi
    o inappropriati raggiungano il tuo modello. [La Tabella 9-3](#guardrails_input)
    mostra i guardrail di ingresso più comuni.
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 9-3\. Guardrail comuni per gli ingressi
  prefs: []
  type: TYPE_NORMAL
- en: '| Guardrail di ingresso | Esempi |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Attuale**Evita i contenuti off-topic o sensibili. | Impedire a un utente
    di discutere di argomenti politici e contenuti espliciti. |'
  prefs: []
  type: TYPE_TB
- en: '| **Iniezione diretta di prompt**(jail-breaking)Impedisci agli utenti di rivelare
    o di ignorare i prompt e i segreti del sistema: più lungo è il contenuto dell''input,
    più il tuo sistema sarà soggetto a questi attacchi. | Bloccando i tentativi di
    ignorare i prompt del sistema e di manipolare il sistema per fargli rivelare le
    chiavi API interne o le impostazioni di configurazione.^([a](ch09.html#id1036))
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Iniezione indiretta di prompt**Impedire l''accettazione di contenuti dannosi
    da fonti esterne, come file o siti web, che possono causare confusione del modello
    o l''esecuzione di codice remoto sui sistemi a valle.I contenuti dannosi possono
    essere invisibili all''occhio umano e codificati all''interno del testo o delle
    immagini in ingresso. | Sanificazione di payload codificati nelle immagini caricate,
    caratteri nascosti o prompt override nei documenti caricati, script nascosti negli
    URL remoti o persino trascrizioni di video di YouTube. |'
  prefs: []
  type: TYPE_TB
- en: '| **Moderazione**Rispettare le linee guida del marchio, i requisiti legali
    e di branding. | Contrassegnare e rifiutare le query degli utenti non valide se
    queste includono riferimenti a bestemmie, concorrenti, contenuti espliciti, informazioni
    di identificazione personale (PII), autolesionismo, ecc. |'
  prefs: []
  type: TYPE_TB
- en: '| **Attributo**Convalida le proprietà dell''input. | Controlla la lunghezza
    della query, le dimensioni del file, le scelte, l''intervallo, il formato e la
    struttura dei dati, ecc. |'
  prefs: []
  type: TYPE_TB
- en: '| Anche se i guardrail sono utili, la pratica migliore è evitare di dare ai
    tuoi modelli GenAI la conoscenza diretta di segreti o impostazioni di configurazione
    sensibili. |'
  prefs: []
  type: TYPE_TB
- en: I guardrail di ingresso possono anche essere combinati con gli igienizzatori
    di contenuto per pulire gli ingressi difettosi.
  prefs: []
  type: TYPE_NORMAL
- en: Se vuoi implementare i tuoi guardrail, puoi iniziare con l'utilizzo di tecniche
    avanzate di ingegneria dei prompt all'interno del tuo sistema. Inoltre, puoi utilizzare
    tecniche di valutazione automatica (ad esempio, modelli di intelligenza artificiale).
  prefs: []
  type: TYPE_NORMAL
- en: L['esempio 9-1](#guardrail_topical_prompt) mostra un esempio di prompt del sistema
    per un valutatore automatico di guardrail AI per rifiutare query fuori tema.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 9-1\. prompt del sistema guardrail di input topico
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Puoi vedere un'implementazione di un guardrail topico in ingresso nell'[Esempio
    9-2](#guardrail_topical) che utilizza la tecnica di valutazione automatica di
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 9-2\. Parapetto di ingresso topico
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_ai_services_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Gestire i casi in cui l'LLM non restituisce una classificazione valida
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Utilizzando la tecnica mostrata nell'[Esempio 9-2](#guardrail_topical), puoi
    implementare dei valutatori automatici per verificare la presenza di tentativi
    di jail-breaking e prompt injection o anche per rilevare la presenza di PII e
    bestemmie negli input.
  prefs: []
  type: TYPE_NORMAL
- en: Come abbiamo visto nel [Capitolo 5](ch05.html#ch05), puoi sfruttare il più possibile
    la programmazione asincrona anche quando utilizzi tecniche di valutazione automatica
    nei tuoi guardrail. Questo perché i guardrail di AI richiedono l'invio di più
    chiamate all'API del modello per ogni query dell'utente. Per migliorare l'esperienza
    dell'utente, puoi eseguire questi guardrail in parallelo al processo di inferenza
    del modello.
  prefs: []
  type: TYPE_NORMAL
- en: Una volta che hai un guardrail autovalutativo per la verifica degli argomenti
    consentiti, puoi eseguirlo in parallelo alla tua generazione di dati^([1](ch09.html#id1037))
    utilizzando `asyncio.wait`, come mostrato nell'[Esempio 9-3](#guardrail_concurrent_execution).
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tieni presente che l'implementazione di guardrail async può innescare meccanismi
    di limitazione della velocità e di strozzatura delle API dei provider di modelli.
    A seconda dei requisiti della tua applicazione, potresti voler richiedere limiti
    di velocità più elevati o ridurre la velocità delle chiamate API in un breve lasso
    di tempo.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 9-3\. Esecuzione di guardrail AI in parallelo alla generazione di risposte
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_ai_services_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea due task asyncio da eseguire in parallelo utilizzando `asyncio.wait`. L'operazione
    ritorna non appena un task viene completato.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_ai_services_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Se il guardrail viene attivato, annulla l'operazione di chat e restituisce una
    risposta codificata. Puoi registrare l'attivazione in un database e inviare email
    di notifica qui.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_ai_services_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Continua a controllare il ciclo di eventi asyncio ogni 100 ms fino a quando
    un'attività non viene completata.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_ai_services_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Sfrutta l'iniezione di dipendenza per restituire la risposta del modello se
    i guardrail non vengono attivati.
  prefs: []
  type: TYPE_NORMAL
- en: Poiché i guardrail abilitati per GenAI, come quelli implementati nell'[Esempio
    9-3](#guardrail_concurrent_execution), rimangono probabilistici, i tuoi servizi
    GenAI possono ancora essere vulnerabili agli attacchi di tipo prompt injection
    e jail-breaking. Ad esempio, gli aggressori possono utilizzare tecniche più avanzate
    di prompt injection per aggirare i tuoi guardrail AI. D'altro canto, i tuoi guardrail
    possono anche rifiutare in modo errato query valide dell'utente, causando falsi
    positivi che possono peggiorare l'esperienza dell'utente.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: La combinazione di guardrail con modelli di rilevamento basati su regole o sull'apprendimento
    automatico tradizionale può aiutare a mitigare alcuni dei rischi sopra citati.
  prefs: []
  type: TYPE_NORMAL
- en: Inoltre, puoi utilizzare dei guardrail che considerano solo l'ultimo messaggio
    per ridurre il rischio che il modello venga confuso da una lunga conversazione.
  prefs: []
  type: TYPE_NORMAL
- en: Quando si progettano i guardrail, è necessario considerare i compromessi tra
    *precisione*, *latenza* e *costi* per bilanciare l'esperienza dell'utente con
    i controlli di sicurezza richiesti.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrail di uscita
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lo scopo dei guardrail di output è quello di convalidare il contenuto prodotto
    da Genai prima che venga passato agli utenti o ai sistemi a valle. [La Tabella
    9-4](#guardrails_output) mostra i guardrail di output più comuni.
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 9-4\. Guardrail di uscita comuni
  prefs: []
  type: TYPE_NORMAL
- en: '| Guardrail di uscita | Esempi |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Allucinazione/controllo dei fatti**Blocca le allucinazioni e restituisce
    risposte in scatola come "Non lo so". | Misurare metriche come la *pertinenza*,
    la *coerenza*, la *consistenza*, la *fluidità* e così via, sui risultati del modello
    rispetto a un corpus di verità di base nelle applicazioni RAG. |'
  prefs: []
  type: TYPE_TB
- en: '| **Moderazione**Applicare le linee guida del marchio e dell''azienda per regolare
    i risultati del modello, filtrando o riscrivendo le risposte che le violano. |
    Verifica di metriche come la *leggibilità*, la *tossicità*, il *sentiment*, il
    *numero di menzioni dei concorrenti*, ecc. |'
  prefs: []
  type: TYPE_TB
- en: '| **Controlli della sintassi**Verificare la struttura e il contenuto degli
    output del modello. Questi guardrail possono rilevare e riprovare o gestire con
    grazia le eccezioni per evitare guasti nei sistemi a valle. | Convalida degli
    schemi JSON e dei parametri delle funzioni nei flussi di lavoro *di chiamata delle
    funzioni* quando i modelli invocano le funzioni.Verifica delle selezioni di strumenti/agenti
    nei *flussi di lavoro agenziali*. |'
  prefs: []
  type: TYPE_TB
- en: Tutti i guardrail di uscita sopra citati si baseranno sul *valore di soglia*
    per rilevare le risposte non valide.
  prefs: []
  type: TYPE_NORMAL
- en: Soglie per guardrail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Guardrail può utilizzare varie metriche come la *leggibilità*, la *tossicità*,
    ecc. per misurare e convalidare la qualità dei risultati del modello. Per ogni
    metrica, dovrai sperimentare per identificare il*valore soglia* appropriato per
    il tuo caso d''uso, tenendo presente che:'
  prefs: []
  type: TYPE_NORMAL
- en: Un maggior numero di *falsi positivi* può infastidire i tuoi utenti e ridurre
    l'usabilità dei tuoiservizi.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Un numero maggiore di *falsi negativi* può causare danni duraturi alla tua reputazione
    e far esplodere i costi, poiché gli utenti malintenzionati possono abusare del
    sistema o eseguire attacchi prompt injection/jail-breaking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In genere, dovresti valutare i rischi e i casi peggiori di avere dei falsi negativi
    e se sei disposto a barattare alcuni falsi negativi nel tuo caso d'uso per migliorare
    l'esperienza dell'utente. Ad esempio, puoi ridurre i casi di blocco degli output
    se includono più gergo e non sono così leggibili.
  prefs: []
  type: TYPE_NORMAL
- en: Implementare una barriera di moderazione
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implementiamo una barriera di moderazione utilizzando una versione del [metodo
    di valutazione*G-Eval*](https://oreil.ly/7Nent) per misurare la presenza di contenuti
    indesiderati nell'output del modello.
  prefs: []
  type: TYPE_NORMAL
- en: 'Il framework G-Eval utilizza i seguenti componenti per assegnare un punteggio
    ai contenuti non validi:'
  prefs: []
  type: TYPE_NORMAL
- en: Un nome di *dominio* che specifichi il tipo di contenuto da moderare
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Una serie di *criteri* per delineare chiaramente quali sono i contenuti validi
    e quali quelli non validi.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Un elenco ordinato di *fasi di istruzione* per la classificazione del contenuto
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Il *contenuto* deve essere valutato con un punteggio discreto da 1 a 5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L['esempio 9-4](#guardrail_moderation_prompt) mostra un prompt di sistema che
    implementa il framework *G-Eval* e che verrà utilizzato da un valutatore automatico
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 9-4\. prompt del sistema di moderazione del parapetto
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3] `### Instructions`  `{``steps``}` `` `### Evaluation (score only!)`
    `"""` `` [PRE4]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]`Utilizzando il prompt del sistema implementato nell''[Esempio 9-4](#guardrail_moderation_prompt),
    puoi ora implementare una barriera di moderazione seguendo l''[Esempio 9-2](#guardrail_topical).    In
    seguito, integriamo la barriera di moderazione con la logica di invocazione della
    chat esistente, come illustrato nell''[Esempio 9-5](#guardrail_moderation).    #####
    Esempio 9-5\. Integrazione del guardrail di moderazione    [PRE6]    [![1](assets/1.png)](#co_securing_ai_services_CO3-1)      Usa
    un tipo intero vincolato da Pydantic per convalidare il punteggio G-Eval del valutatore
    automatico di LLM.      [![2](assets/2.png)](#co_securing_ai_services_CO3-2)      Contrassegnare
    i contenuti che hanno un punteggio superiore alla soglia come se non avessero
    superato la moderazione.      [![3](assets/3.png)](#co_securing_ai_services_CO3-3)      Integrare
    ed eseguire il guardrail di moderazione dell''output con altri guardrail.      ######
    Suggerimento    Oltre al nuovo framework *G-Eval*, implementato utilizzando un
    autovalutatore LLM, è possibile utilizzare framework di valutazione automatica
    più tradizionali come [ROUGE](https://oreil.ly/_9Q9g), [BERTScore](https://oreil.ly/jRTeL)
    e [SummEval](https://oreil.ly/5YtJG) per moderare i contenuti in uscita.    Hai
    implementato due guardrail di I/O, uno per verificare gli argomenti delle query
    degli utenti e l''altro per moderare gli output di LLM.    Per migliorare ulteriormente
    il tuo sistema di guardrail, puoi:    *   Adotta l''approccio del *guasto rapido*
    uscendo prima se si attiva un guardrail per ottimizzare i tempi di risposta.           *   Seleziona
    solo i *guardrail appropriati* per i tuoi casi d''uso, invece di usarli tutti
    insieme, che potrebbero sovraccaricare i tuoi servizi.           *   Esegui i
    guardrail *in modo asincrono* anziché sequenziale per ottimizzare la latenza.           *   Implementa
    il *campionamento delle richieste* eseguendo guardrail più lenti su un campione
    di richieste per ridurre la latenza complessiva quando i tuoi servizi sono sottoposti
    a un carico elevato.              Ora dovresti sentirti più sicuro nell''implementare
    i tuoi guardrail utilizzando tecniche di autovalutazione classiche o LLM senza
    affidarti a strumenti e librerie esterne.    Nella prossima sezione scoprirai
    come limitare la velocità delle API per proteggere i tuoi servizi dal sovraccarico
    dei modelli e dai tentativi di scraping.[PRE7]``  [PRE8][PRE9]py[PRE10] [PRE11]
    from slowapi import Limiter from slowapi.middleware import SlowAPIMiddleware  app.state.limiter
    = Limiter(storage_uri="redis://localhost:6379") app.add_middleware(SlowAPIMiddleware)
    [PRE12]` [PRE13][PRE14][PRE15]`` [PRE16] from contextlib import asynccontextmanager
    import redis from fastapi import Depends, FastAPI from fastapi.websockets import
    WebSocket from fastapi_limiter import FastAPILimiter from fastapi_limiter.depends
    import WebSocketRateLimiter  ...  @asynccontextmanager async def lifespan(_: FastAPI):
    ![1](assets/1.png)     redis_connection = redis.from_url("redis://localhost:6379",
    encoding="utf8")     await FastAPILimiter.init(redis_connection)     yield     await
    FastAPILimiter.close()  app = FastAPI(lifespan=lifespan)  @app.websocket("/ws")
    async def websocket_endpoint(     websocket: WebSocket, user_id: int = Depends(get_current_user)
    ![2](assets/2.png) ):     ratelimit = WebSocketRateLimiter(times=1, seconds=5)     await
    ws_manager.connect(websocket)     try:         while True:             prompt
    = await ws_manager.receive(websocket)             await ratelimit(websocket, context_key=user_id)
    ![3](assets/3.png)             async for chunk in azure_chat_client.chat_stream(prompt,
    "ws"):                 await ws_manager.send(chunk, websocket)     except WebSocketRateLimitException:         await
    websocket.send_text(f"Rate limit exceeded. Try again later")     finally:         await
    ws_manager.disconnect(websocket) [PRE17]` [PRE18][PRE19][PRE20][PRE21]`` [PRE22]
    class AzureOpenAIChatClient:     def __init__(self, throttle_rate = 0.5): ![1](assets/1.png)         self.aclient
    = ...         self.throttle_rate = throttle_rate      async def chat_stream(             self,
    prompt: str, mode: str = "sse", model: str = "gpt-3.5-turbo"     ) -> AsyncGenerator[str,
    None]:         stream = ...  # OpenAI chat completion stream         async for
    chunk in stream:             await asyncio.sleep(self.throttle_rate) ![2](assets/2.png)             if
    chunk.choices[0].delta.content is not None:                 yield (                     f"data:
    {chunk.choices[0].delta.content}\n\n"                     if mode == "sse"                     else
    chunk.choices[0].delta.content                 )                 await asyncio.sleep(0.05)          if
    mode == "sse":             yield f"data: [DONE]\n\n" [PRE23]` [PRE24][PRE25] ``
    `# Riassunto    Questo capitolo ha fornito un riepilogo completo dei vettori di
    attacco per i servizi GenAI e di come salvaguardarli da tentativi avversari, usi
    impropri e abusi.    Hai imparato a implementare guardrail in ingresso e in uscita,
    oltre a meccanismi di valutazione e filtraggio dei contenuti per moderare l''uso
    del servizio. Oltre ai guardrail, hai anche sviluppato protezioni per la limitazione
    della velocità e il throttling delle API per gestire il carico del server e prevenire
    gli abusi.    Nel prossimo capitolo scopriremo come ottimizzare i servizi di intelligenza
    artificiale attraverso varie tecniche come il caching, l''elaborazione in batch,
    la quantizzazione dei modelli, il prompt engineering e la messa a punto dei modelli.    ^([1](ch09.html#id1037-marker))
    Ispirato a ["Come implementare i Guardrail di LLM" del Cookbook di OpenAI.](https://oreil.ly/UQV6i)`
    `` [PRE26][PRE27]``````'
  prefs: []
  type: TYPE_NORMAL
