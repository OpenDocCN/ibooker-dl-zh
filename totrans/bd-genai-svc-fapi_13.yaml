- en: Chapter 9\. Securing AI Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In earlier chapters, you learned how to build GenAI services that serve various
    AI generators while supporting concurrency and data streaming in real time. Additionally,
    you integrated external systems like databases and implemented your own authentication
    and authorization mechanisms. Finally, you wrote a test suite to verify the functionality
    and performance of your entire system.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn how to implement usage moderation and abuse-protection
    mechanisms to secure your GenAI services.
  prefs: []
  type: TYPE_NORMAL
- en: Usage Moderation and Abuse Protection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When deploying your GenAI services, you’ll need to consider how your services
    will be misused and abused by malicious users. This is essential to protect user
    safety and your own reputation. You won’t know how the users will use your system,
    so you need to assume the worst and implement *guardrails* to protect against
    any misuse or abuse.
  prefs: []
  type: TYPE_NORMAL
- en: According to a [recent study on nefarious applications of GenAI](https://oreil.ly/ihmzR),
    your services may potentially be used with *malicious intents*, as described in
    [Table 9-1](#malicious_intents).
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-1\. Malicious intents behind abusing GenAI services
  prefs: []
  type: TYPE_NORMAL
- en: '| Intent | Examples | Real-world cases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Dishonesty**Supporting lies and untruthfulness | Plagiarism, faking competency
    and knowledge, document forgery, cheating in exams and in interviews, etc. | Increasing
    cases of students cheating with AI at UK and Australian universities^([a](ch09.html#id1032))
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Propaganda**Skewing perceptions of reality to advance an agenda | Impersonating
    others, promoting extremism, influencing campaigns, etc. | Fake AI news anchors
    spreading misinformation or propaganda^([b](ch09.html#id1033)) |'
  prefs: []
  type: TYPE_TB
- en: '| **Deception**Misleading others and creating false impressions | Generating
    fake reviews, scam ads and phishing emails, and synthetic profiles (i.e., sockpuppeting),
    etc. | Engineering firm Arup revealed as a victim of a $25 million deepfake scam^([c](ch09.html#id1034))
    |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch09.html#id1032-marker)) Sources: *Times Higher Education* and *The
    Guardian*^([b](ch09.html#id1033-marker)) Sources: *The Guardian*, *MIT Technology
    Review*, and *The Washington Post*^([c](ch09.html#id1034-marker)) Sources: CNN
    and *The Guardian* |'
  prefs: []
  type: TYPE_TB
- en: 'The same study categorizes GenAI application abuse into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Misinformation and disinformation* to spread propaganda and fake news'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bias amplification and discrimination* to advance racist agendas and societal
    discrimination'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Malicious content generation* by creating toxic, deceptive, and radicalizing
    content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data privacy attacks* to fill in gaps in stolen private data and leak sensitive
    information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automated cyberattacks* to personalize phishing and ransomware attacks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Identity theft and social engineering* to increase the success rate of scams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deepfakes and multimedia manipulation* to make a profit and skew perceptions
    of reality and social beliefs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scam and fraud* by manipulating stock markets and crafting targeted scams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This may not be an exhaustive list but should give you a few ideas on what usage
    moderation measures to consider.
  prefs: []
  type: TYPE_NORMAL
- en: '[Another study on the taxonomy of GenAI misuse tactics](https://oreil.ly/jbG01)
    investigated abuse by modality and found that:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Audio and video generators* were used for the majority of impersonation attempts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Image and text generators* were used for the majority of sockpuppeting, content
    farming for opinion manipulation at scale, and falsification attempts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Image and video generators* were used for the majority of steganography, (i.e.,
    hiding coded messages in model outputs), and nonconsensual intimate content (NCII)
    generation attempts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re building services supporting such modalities, you should consider
    their associated forms of abuse and implement relevant protection mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from misuse and abuse, you’ll also need to consider security vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Securing GenAI services is still an area of research at the time of writing.
    For instance, if your services leverage LLMs, OWASP has categorized the [top 10
    LLM vulnerabilities](https://oreil.ly/4zob2), as shown in [Table 9-2](#llm_vulnerabilities).
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-2\. OWASP top 10 LLM vulnerabilities
  prefs: []
  type: TYPE_NORMAL
- en: '| Risk | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt injection | Manipulating inputs to control the LLM’s responses leading
    to unauthorized access, data breaches, and compromised decision-making. |'
  prefs: []
  type: TYPE_TB
- en: '| Insecure output handling | Failing to sanitize or validate LLM outputs causing
    remote code execution on downstream systems. |'
  prefs: []
  type: TYPE_TB
- en: '| Training data poisoning | Injecting data in sources that models get trained
    on to compromise security, accuracy, or ethical behavior. Open source models and
    RAG services that rely on web data are most prone to these attacks. |'
  prefs: []
  type: TYPE_TB
- en: '| Model denial of service | Causing service disruption and cost explosions
    by overloading the LLMs with heavy payloads and concurrent requests. |'
  prefs: []
  type: TYPE_TB
- en: '| Supply chain vulnerabilities | Causing various components, including data
    sources, to be compromised, undermining system integrity. |'
  prefs: []
  type: TYPE_TB
- en: '| Sensitive information leakage | Leading to accidental exposure of private
    data, legal liabilities and loss of competitive advantage. |'
  prefs: []
  type: TYPE_TB
- en: '| Insecure plug-in design | Vulnerabilities in third-party integrations cause
    remote code execution. |'
  prefs: []
  type: TYPE_TB
- en: '| Excessive agency | Where LLMs have too much autonomy to take actions can
    lead to unintended consequences and harmful actions. |'
  prefs: []
  type: TYPE_TB
- en: '| Overreliance on LLM | Compromising decision-making, contributing to security
    vulnerabilities and legal liabilities. |'
  prefs: []
  type: TYPE_TB
- en: '| Model theft | Related to unauthorized copying or usage of your models. |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similar vulnerabilities exist for other types of GenAI systems such as image,
    audio, video, and geometry generators.
  prefs: []
  type: TYPE_NORMAL
- en: I recommend researching and identifying software vulnerabilities relevant to
    your own use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Without guardrails, your services can be abused to cause personal and financial
    harm, identity theft, economic damage, spread misinformation, and contribute to
    societal problems. As a result, it’s crucial to implement several safety measures
    and guardrails to protect your services against such attacks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you’ll learn usage moderation and security measures you
    can implement to protect your GenAI services prior to deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Guardrails* refer to *detective controls* that aim to guide your application
    toward the intended outcomes. They are incredibly diverse and can be configured
    to fit any situation that may go wrong with your GenAI systems.'
  prefs: []
  type: TYPE_NORMAL
- en: As an example, *I/O guardrails* are designed to verify data entering a GenAI
    model and outputs sent to the downstream systems or users. Such guardrails can
    flag inappropriate user queries and validate output content against toxicity,
    hallucinations, or banned topics. [Figure 9-1](#guardrails) shows how an LLM system
    looks once you add I/O guardrails to it.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0901](assets/bgai_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. Comparison of an LLM system without and with guardrails
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You don’t have to implement guardrails from scratch. At the time of writing,
    prebuilt open source guardrail frameworks exist like NVIDIA NeMo Guardrails, LLM-Guard,
    and Guardrails AI to protect your services. However, they may require learning
    framework-related languages and have a trade-off of slowing down your services
    and bloating your application due to various external dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Other commercial guardrails available on the market, such as Open AI’s Moderation
    API, Microsoft Azure AI Content Safety API, and Google’s Guardrails API are either
    not open source or lack details and contents to measure quality constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Guardrails remain an active area of research. While such defenses can counter
    some attacks, powerful attacks backed by AI can still bypass them. This may lead
    to an [ongoing and endless loop of assaults and defenses](https://oreil.ly/xlUmw).
  prefs: []
  type: TYPE_NORMAL
- en: While engineering application-level I/O guardrails may not provide perfect protection,
    upcoming GenAI models may include baked-in guardrails inside the model to improve
    security guarantees. However, such guardrails may have a performance impact on
    response times by introducing latency to the system.
  prefs: []
  type: TYPE_NORMAL
- en: Input Guardrails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The purpose of input guardrails is to prevent malicious or inappropriate content
    from reaching your model. [Table 9-3](#guardrails_input) shows common input guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-3\. Common input guardrails
  prefs: []
  type: TYPE_NORMAL
- en: '| Input guardrails | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Topical**Steer inputs away from off-topic or sensitive content. | Preventing
    a user from discussing political topics and explicit content. |'
  prefs: []
  type: TYPE_TB
- en: '| **Direct prompt injection** (jail-breaking)Prevent users from revealing or
    overriding system prompts and secrets. The longer the input content, the more
    prone your system will be to these attacks. | Blocking attempts to override system
    prompts and manipulating the system into revealing internal API keys or configuration
    settings.^([a](ch09.html#id1036)) |'
  prefs: []
  type: TYPE_TB
- en: '| **Indirect prompt injection**Prevent acceptance of malicious content from
    external sources such as files or websites that may cause model confusion or remote
    code execution on downstream systems.Malicious content may be invisible to the
    human eye and encoded within input text or images. | Sanitizing encoded payloads
    in upload images, hidden characters or prompt overrides in uploaded documents,
    hidden scripts in remote URLs or even YouTube video transcripts. |'
  prefs: []
  type: TYPE_TB
- en: '| **Moderation**Comply with brand guidelines, legal, and branding requirements.
    | Flag and refuse invalid user queries if user queries include mentions of profanity,
    competitor, explicit content, personally identifiable information (PII), self-harm,
    etc. |'
  prefs: []
  type: TYPE_TB
- en: '| **Attribute**Validate input properties. | Check query length, file size,
    choices, range, data format and structure, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch09.html#id1036-marker)) Although guardrails are useful, best practice
    is to avoid giving your GenAI models direct knowledge of secrets or sensitive
    configuration settings in the first place. |'
  prefs: []
  type: TYPE_TB
- en: The input guardrails can also be combined with content sanitizers to clean bad
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to implement your own guardrails, you can start off with using advanced
    prompt engineering techniques within your system prompts. Additionally, you can
    use auto-evaluation techniques, (i.e., AI models).
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 9-1](#guardrail_topical_prompt) shows an example system prompt for
    an AI guardrail auto-evaluator to reject off-topic queries.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-1\. Topical input guardrail system prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can see an implementation of an input topical guardrail in [Example 9-2](#guardrail_topical)
    using the LLM auto-evaluation technique.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-2\. Topical input guardrail
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_ai_services_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Handle cases where the LLM doesn’t return a valid classification
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the technique shown in [Example 9-2](#guardrail_topical), you can implement
    auto-evaluators to check for jail-breaking and prompt injection attempts or even
    detect the presence of PII and profanity in the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 5](ch05.html#ch05), you can leverage async programming
    as much as possible even when using auto-evaluation techniques in your guardrails.
    This is because AI guardrails require sending multiple model API calls per user
    query. To improve user experience, you can run these guardrails in parallel to
    the model inference process.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have an auto-evaluator guardrail for checking allowed topics, you can
    execute it in parallel to your data generation^([1](ch09.html#id1037)) using `asyncio.wait`,
    as shown in [Example 9-3](#guardrail_concurrent_execution).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Be mindful that implementing async guardrails may trigger model provider API
    rate-limiting and throttling mechanisms. Depending on your application requirements,
    you may want to request higher rate limits or reduce the rate of API calls within
    a short time frame.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-3\. Running AI guardrails in parallel to response generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_ai_services_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create two asyncio tasks to run in parallel using `asyncio.wait`. The operation
    returns as soon as a task is completed.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_ai_services_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: If the guardrail is triggered, cancel the chat operation and return a hard-coded
    response. You can log the trigger in a database and send notification emails here.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_ai_services_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Keep checking in with the asyncio event loop every 100 ms until a task is done.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_ai_services_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Leverage dependency injection to return the model response if guardrails aren’t
    triggered.
  prefs: []
  type: TYPE_NORMAL
- en: Since GenAI-enabled guardrails like those you implemented in [Example 9-3](#guardrail_concurrent_execution)
    remain probabilistic, your GenAI services can still be vulnerable to prompt injection
    and jail-breaking attacks. For instance, attackers can use more advanced prompt
    injection techniques to get around your AI guardrails too. On the other hand,
    your guardrails may also incorrectly over-refuse valid user queries, leading to
    false positives that can downgrade your user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Combining guardrails with rules-based or traditional machine learning models
    for detection can help mitigate some of the aforementioned risks.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you can use guardrails that only consider the latest message to
    reduce the risk of the model being confused by a long conversation.
  prefs: []
  type: TYPE_NORMAL
- en: When designing guardrails, you need to consider trade-offs between *accuracy*,
    *latency*, and *cost* to balance user experience with your required security controls.
  prefs: []
  type: TYPE_NORMAL
- en: Output Guardrails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The purpose of output guardrails is to validate GenAI-produced content before
    it’s passed to users or downstream systems. [Table 9-4](#guardrails_output) shows
    common output guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-4\. Common output guardrails
  prefs: []
  type: TYPE_NORMAL
- en: '| Output guardrails | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Hallucination/fact-checking**Block hallucinations and return canned responses
    such as “I don’t know.” | Measuring metrics such as *relevancy*, *coherence*,
    *consistency*, *fluency*, etc., on the model outputs against a corpus of ground
    truth in RAG applications. |'
  prefs: []
  type: TYPE_TB
- en: '| **Moderation**Apply brand and corporate guidelines to govern the model outputs,
    either filtering or rewriting responses that breach them. | Checking against metrics
    such as *readability*, *toxicity*, *sentiment*, *count of competitor mentions*,
    etc. |'
  prefs: []
  type: TYPE_TB
- en: '| **Syntax checks**Verify the structure and content of model outputs. These
    guardrails can either detect and retry or gracefully handle exceptions to prevent
    failures in the downstream systems. | Validating JSON schemas and function parameters
    in *function calling* workflows when models invoke functions.Checking tool/agent
    selections in *agentic workflows*. |'
  prefs: []
  type: TYPE_TB
- en: Any of the aforementioned output guardrails will rely on *threshold value* to
    detect invalid responses.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrail Thresholds
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Guardrails can use various metrics such as *readability*, *toxicity*, etc.,
    to measure and validate the quality of the model outputs. For each metric, you’ll
    need to experiment to identify the appropriate *threshold value* for your use
    case, bearing in mind that:'
  prefs: []
  type: TYPE_NORMAL
- en: More *false positives* can annoy your users and reduce the usability of your
    services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More *false negatives* can cause lasting harm to your reputation and explode
    costs since malicious users can abuse the system or perform prompt injection/jail-breaking
    attacks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normally, you should assess the risks and worst cases of having false negatives
    and whether you’re happy to trade off a few false negatives in your use case for
    enhanced user experience. For instance, you can reduce instances of blocking outputs
    if they include more jargon and aren’t as readable.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Moderation Guardrail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s implement a moderation guardrail using a version of the [*G-Eval* evaluation
    method](https://oreil.ly/7Nent) to measure the presence of unwanted content in
    the model output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The G-Eval framework uses the following components to score invalid content:'
  prefs: []
  type: TYPE_NORMAL
- en: A *domain* name specifying the type of content to be moderated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of *criteria* to clearly outline what is considered valid versus invalid
    content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ordered list of *instruction steps* for grading the content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *content* to grade between a discrete score of 1 to 5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Example 9-4](#guardrail_moderation_prompt) shows a system prompt implementing
    the *G-Eval* framework that an LLM auto-evaluator will use.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-4\. Moderation guardrail system prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Using the system prompt implemented in [Example 9-4](#guardrail_moderation_prompt),
    you can now implement a moderation guardrail following [Example 9-2](#guardrail_topical).
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s integrate the moderation guardrail with your existing chat invocation
    logic, as shown in [Example 9-5](#guardrail_moderation).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-5\. Integrating moderation guardrail
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_ai_services_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Use a Pydantic constrained integer type to validate LLM auto-evaluator G-Eval
    score.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_ai_services_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Flag content that is scored above the threshold as not passing moderation.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_ai_services_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Integrate and run the output moderation guardrail with other guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Beyond the novel *G-Eval* framework implemented using an LLM auto-evaluator,
    you can also use more traditional automatic evaluation frameworks such as [ROUGE](https://oreil.ly/_9Q9g),
    [BERTScore](https://oreil.ly/jRTeL), and [SummEval](https://oreil.ly/5YtJG) for
    moderating output content.
  prefs: []
  type: TYPE_NORMAL
- en: Well done. You have now implemented two I/O guardrails, one to verify topics
    of user queries and another to moderate the LLM outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve your guardrail system even further, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: Adopt the *fast failure* approach by exiting early if a guardrail is triggered
    to optimize response times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only select *appropriate guardrails* for your use cases instead of using them
    all together, which could overwhelm your services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run guardrails *asynchronously* instead of sequentially to optimize latency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement *request sampling* by running slower guardrails on a sample of requests
    to reduce overall latency when your services are under a heavy load.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should now feel more confident implementing your own guardrails using classical
    or LLM auto-evaluation techniques without relying on external tools and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you’ll learn about API rate limiting so that you can protect
    your services against model overloading and scraping attempts.
  prefs: []
  type: TYPE_NORMAL
- en: API Rate Limiting and Throttling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When deploying GenAI services, you will need to consider service exhaustion
    and model overloading issues in production. Best practice is to implement rate
    limiting and potentially throttling into your services.
  prefs: []
  type: TYPE_NORMAL
- en: '*Rate limiting* controls the amount of incoming and outgoing traffic to and
    from a network to prevent abuse, ensure fair usage, and avoid overloading the
    server. On the other hand, *throttling* controls the API throughput by temporarily
    slowing down the rate of request processing to stabilize the server.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both techniques can help you:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prevent abuse* by blocking malicious users or bots from overwhelming your
    services from data scraping and brute-force attacks that involve too many requests
    or large payloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Enforce fair usage policies* so that capacity is shared among multiple users
    and a handful of users are prevented from monopolizing server resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Maintain server stability* by regulating incoming traffic to maintain consistent
    performance and prevent crashes during peak periods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To implement rate limiting, you will need to monitor incoming requests within
    a time period and use a queue to balance the load.
  prefs: []
  type: TYPE_NORMAL
- en: There are several rate-limiting strategies you can choose from, which are compared
    in [Table 9-5](#rate_limiting_strategies) and shown in [Figure 9-2](#rate_limiting_strategies_comparison).
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-5\. Rate-limiting strategies
  prefs: []
  type: TYPE_NORMAL
- en: '| Strategy | Benefits | Limitations | Use cases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Token Bucket**A list is filled with tokens at a constant rate, and every
    incoming request consumes a token. If there aren’t enough tokens for incoming
    requests, they’ll be rejected. |'
  prefs: []
  type: TYPE_TB
- en: Handles temporary bursts and dynamic traffic patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Granular control over request processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Complex to implement | Commonly used in most APIs and services, and interactive
    or event-driven GenAI systems where request rates can be irregular |'
  prefs: []
  type: TYPE_TB
- en: '| **Leaky Bucket**Incoming requests are added to a queue and processed at a
    constant rate to smooth the traffic. If the queue overflows, any new incoming
    requests are rejected. |'
  prefs: []
  type: TYPE_TB
- en: Simple to implement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintains consistent traffic flow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Less flexible to dynamic traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May reject valid requests during sudden spikes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Services that require maintaining consistent response times in AI inference
    services |'
  prefs: []
  type: TYPE_TB
- en: '| **Fixed Window**Limits requests within fixed time windows (e.g., 100 requests
    per minute). | Simple to implement | Does not handle burst traffic well |'
  prefs: []
  type: TYPE_TB
- en: Enforcing strict usage policies for expensive AI inferences and API calls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideal for free tier users or batch-processing systems with predictable usage
    patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each request is treated equally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sliding Window**Counts requests over a rolling time frame. | Provides better
    flexibility, granularity, and burst traffic smoothing |'
  prefs: []
  type: TYPE_TB
- en: More complex to implement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires higher memory usage for tracking requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Much better at handling burst traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideal for conversational AI or premium-tier users who expect flexible, high-frequency
    access over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0902](assets/bgai_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Comparison of rate-limiting strategies
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that you’re more familiar with rate-limiting concepts, let’s try to implement
    rate limiting in FastAPI.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Rate Limits in FastAPI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The fastest approach to add rate limiting within FastAPI is to use a library
    such as `slowapi` that is a wrapper over the `limits` package, supporting most
    of the strategies mentioned in [Table 9-5](#rate_limiting_strategies). First,
    install the `slowapi` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Once you’ve installed the `slowapi` package, you can follow [Example 9-6](#rate_limiting_slowapi_configurations)
    to apply global API or endpoint rate limiting. You can also track and limit usage
    per IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Without configuring an external data store, `slowapi` stores and tracks IP addresses
    in the application memory for rate limiting.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-6\. Configuring global rate limits
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_ai_services_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create rate limiter that tracks usage across each IP address and rejects requests
    if they exceed specified limits across the application.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_ai_services_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Add a custom exception handler for rate-limited requests to compute and provide
    waiting times before requests are accepted again.
  prefs: []
  type: TYPE_NORMAL
- en: With the `limiter` decorator configured, you can now use it on your API handlers,
    as shown in [Example 9-7](#rate_limiting_slowapi).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-7\. Setting API rate limits for each API handler
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_ai_services_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Specify more granular rate limits at endpoint level using a rate-limiting decorator.
    The `limiter` decorator must be ordered last.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_ai_services_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Pass the `Request` object to each controller so that the `slowapi` limiter decorator
    can hook into the incoming request. Otherwise, rate limiting will not function.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_ai_services_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Exclude the `/health` endpoint from rate-limiting logic as cloud providers or
    Docker daemons may ping this endpoint continually to check the status of your
    application.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_ai_services_CO5-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid rate limiting the `/health` endpoint as external systems may frequently
    trigger it to check the current status of your service.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve implemented the rate limits, you can run load tests using the
    `ab` (Apache Benchmarking) CLI tool, as shown in [Example 9-8](#rate_limiting_load_testing).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-8\. API load testing with Apache Benchmark CLI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_ai_services_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Send 100 requests with a rate of 2 parallel requests per second.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your terminal outputs should show the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Your global and local limiting system should now be working as intended based
    on incoming IPs.
  prefs: []
  type: TYPE_NORMAL
- en: User-based rate limits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With an IP rate limit, you’re limiting excess usage based on IP, but users can
    get around IP rate limiting by using VPNs, proxies, or rotating IP addresses.
    Instead, you want each user to have a dedicated quota to prevent a single user
    from consuming all available resources. Adding user-based limits can help you
    prevent abuse, as shown in [Example 9-9](#rate_limiting_slowapi_users).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-9\. User-based rate limiting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Your system will now be limiting users based on their account IDs alongside
    their IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limits across instances in production
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since you may run multiple instances of your application in production as you
    scale your services, you’ll also want to centralize your usage tracking. Otherwise,
    each instance will provide their own counters to users, and a load balancer distributes
    requests between instances; usage won’t be capped as you’d expect. To rectify
    this issue, you can switch the `slowapi` in-memory storage backend with a centralized
    in-memory database such as Redis, as shown in [Example 9-10](#rate_limiting_slowapi_redis).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To run [Example 9-10](#rate_limiting_slowapi_redis), you will need a Redis
    database to store user API usage data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Example 9-10\. Adding a centralized usage memory store (Redis) across multiple
    instances
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You now have a working rate-limited API that functions as intended across multiple
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: You can get around this issue by implementing your own limiter supported by
    the `limits` package instead. Alternatively, you can apply rate limiting via a
    *load balancer*, a *reverse proxy*, or an *API gateway* instead.
  prefs: []
  type: TYPE_NORMAL
- en: Each solution can route requests while performing rate limits, protocol translation,
    and traffic monitoring at an infrastructure layer. Applying rate limiting externally
    may be more suitable for your use case if you don’t require a customized rate-limiting
    logic.
  prefs: []
  type: TYPE_NORMAL
- en: Limiting WebSocket connections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unfortunately the `slowapi` package also doesn’t support limiting async and
    WebSocket endpoints at the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: Because WebSocket connections are likely to be long-lived, you may want to limit
    the data transition rate sent over the socket. You can rely on external packages
    such as `fastapi-limiter` to rate limit WebSocket connections, as shown in [Example 9-11](#rate_limiting_websocket).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-11\. Rate-limiting WebSocket connections with the `fastapi_limiter`
    package
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_ai_services_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Configure the `FastAPILimiter` application lifespan with a Redis storage backend.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_ai_services_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Configure a WebSocket rate limiter to allow one request per second.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_ai_services_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the user’s ID as the unique identifier for rate limiting.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 9-11](#rate_limiting_websocket) shows how to limit the number of active
    WebSocket connections for a given user.'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond rate-limiting WebSocket endpoints, you may also want to limit the data
    streaming rate of your GenAI models. Let’s look at how you can throttle real-time
    data streams next.
  prefs: []
  type: TYPE_NORMAL
- en: Throttling Real-Time Streams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working with real-time streams, you may need to slow down the streaming
    rate to give clients enough time to consume the stream and improve streaming throughput
    across multiple clients. In addition, throttling can help you manage the network
    bandwidth, server load, and resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Applying a *throttle* at the stream generation layer, as shown in [Example 9-12](#throttling_stream),
    is an effective approach to managing throughput if your services are under pressure.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-12\. Throttling streams
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_ai_services_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Set a fixed throttling rate or dynamically adjust based on usage.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_ai_services_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Slow down the streaming rate without blocking the event loop.
  prefs: []
  type: TYPE_NORMAL
- en: You can then use the throttled stream within an SSE or WebSocket endpoint. Or,
    you can limit the number of active WebSocket connections per your own custom policies.
  prefs: []
  type: TYPE_NORMAL
- en: Alongside the application-level throttling for real-time streams, you can also
    leverage *traffic shaping* at the infrastructure layer.
  prefs: []
  type: TYPE_NORMAL
- en: Using safeguards, rate limits, and throttles should provide enough barriers
    in protecting your services from abuse and misuse.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you’ll learn more about optimization techniques that can
    help you reduce latency, increase response quality, and throughput alongside reducing
    the costs of your GenAI services.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided a comprehensive summary of attack vectors for GenAI services
    and how to safeguard them against adversarial attempts, misuse, and abuse.
  prefs: []
  type: TYPE_NORMAL
- en: You learned to implement input and output guardrails alongside evaluation and
    content filtering mechanisms to moderate service usage. Alongside guardrails,
    you also developed API rate-limiting and throttling protections to manage server
    load and prevent abuse.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about optimizing AI services through various
    techniques such as caching, batch processing, model quantizing, prompt engineering,
    and model fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch09.html#id1037-marker)) Inspired by [OpenAI Cookbook’s “How to Implement
    LLM Guardrails”](https://oreil.ly/UQV6i).
  prefs: []
  type: TYPE_NORMAL
