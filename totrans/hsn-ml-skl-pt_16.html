<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 14. Natural Language Processing with RNNs and Attention"><div class="chapter" id="nlp_chapter">
<h1><span class="label">Chapter 14. </span>Natural Language Processing <span class="keep-together">with RNNs and Attention</span></h1>


<p>When Alan Turing<a data-type="indexterm" data-primary="natural language processing (NLP)" id="xi_naturallanguageprocessingNLP14317_1"/> imagined his famous <a href="https://homl.info/turingtest">Turing test</a>⁠<sup><a data-type="noteref" id="id3189-marker" href="ch14.html#id3189">1</a></sup> in 1950, he proposed a way to evaluate a machine’s ability to match human intelligence. He could have tested for many things, such as the ability to recognize cats in pictures, play chess, compose music, or escape a maze, but, interestingly, he chose a linguistic task. More specifically, he devised<a data-type="indexterm" data-primary="chatbot or personal assistant" id="id3190"/> a <em>chatbot</em> capable of fooling its interlocutor into thinking it was human.⁠<sup><a data-type="noteref" id="id3191-marker" href="ch14.html#id3191">2</a></sup> This test does have its weaknesses: a set of hardcoded rules can fool unsuspecting or naive humans (e.g., the machine could give vague predefined answers in response to some keywords, it could pretend that it is joking or drunk to get a pass on its weirdest answers, or it could escape difficult questions by answering them with its own questions), and many aspects of human intelligence are utterly ignored (e.g., the ability to interpret nonverbal communication such as facial expressions, or to learn a manual task). But the test does highlight the fact that mastering language is arguably <em>Homo sapiens</em>’s greatest cognitive ability.</p>

<p>Until recently, state-of-the-art natural language processing (NLP) models were pretty much all based on recurrent neural networks (introduced in <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>). However, in recent years, RNNs have been replaced with transformers, which we will explore in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>. That said, it’s still important to learn how RNNs can be used for NLP tasks, if only because it helps better understand transformers. Moreover, most of the techniques we will discuss in this chapter are also useful with Transformer 
<span class="keep-together">architectures</span> (e.g., tokenization, beam search, attention mechanisms, and more). Plus, RNNs have recently made a surprise comeback in the form of state space models (SSMs)<a data-type="indexterm" data-primary="state space models (SSMs)" id="id3192"/><a data-type="indexterm" data-primary="SSMs (state space models)" id="id3193"/> (see “State-Space Models (SSMs)” at <a href="https://homl.info" class="bare"><em class="hyperlink">https://homl.info</em></a>).</p>

<p>This chapter is organized in three sections. In the first section, we will start by building a <em>character RNN</em>, or <em>char-RNN</em>, trained to predict the next character in a sentence. On the way, we will learn about trainable embeddings. Our char-RNN will be our first<a data-type="indexterm" data-primary="language models" data-seealso="natural language processing" id="id3194"/> tiny <em>language model</em>, capable of generating original text.</p>

<p>In the second section, we will turn to text classification, and more specifically sentiment analysis, which aims to predict how positive or negative some text is. Our model will read movie reviews and estimate the rater’s feeling about the movie. This time, instead of splitting the text into individual characters, we will split it into <em>tokens</em>: a token<a data-type="indexterm" data-primary="token (NLP)" id="id3195"/> is a small piece of text from a fixed-sized vocabulary, such as the top 10,000 most common words in the English language, or the most common subwords (e.g., “smartest” = “smart” + “est”), or even individual characters or bytes. To split the text into tokens, we will use<a data-type="indexterm" data-primary="tokenizer" id="id3196"/> a <em>tokenizer</em>. This section will also introduce popular Hugging Face<a data-type="indexterm" data-primary="Hugging Face" id="id3197"/> libraries: the <em>Datasets</em> library to download datasets, the <em>Tokenizers</em> library for tokenizers, and the <em>Transformers</em> library for popular models, downloaded automatically from the <em>Hugging Face Hub</em>. Hugging Face<a data-type="indexterm" data-primary="Hugging Face Hub" id="id3198"/> is a hugely influential company and open source community, and it plays a central role in the open source AI space, especially in NLP.</p>

<p>The final boss of this chapter will be neural machine translation (NMT)<a data-type="indexterm" data-primary="neural machine translation (NMT)" id="id3199"/>, the topic of the third and last section: we will build an encoder-decoder model<a data-type="indexterm" data-primary="encoder-decoder models" data-seealso="attention mechanisms" id="id3200"/> capable of translating<a data-type="indexterm" data-primary="translations" data-seealso="neural machine translation; transformers" id="id3201"/> English to Spanish. This will lead us to <em>attention mechanisms</em>, which we will apply to our encoder-decoder model to improve its capacity to handle long input texts. As their name suggests, attention mechanisms<a data-type="indexterm" data-primary="attention mechanisms" id="id3202"/> are neural network components that learn to select the part of the inputs that the model should focus on at each time step. They directly led to the transformers revolution, as we will see in the next chapter.</p>

<p>Let’s start with a simple and fun char-RNN model that can write like Shakespeare (sort of).</p>






<section data-type="sect1" data-pdf-bookmark="Generating Shakespearean Text Using a Character RNN"><div class="sect1" id="id262">
<h1>Generating Shakespearean Text Using a Character RNN</h1>

<p>In a famous <a href="https://homl.info/charrnn">2015 blog post</a> titled “The Unreasonable Effectiveness of Recurrent Neural Networks”, Andrej Karpathy showed how to train an RNN to predict the next character in a sentence<a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="Char-RNN model to generate text" id="xi_naturallanguageprocessingNLPCharRNNmodeltogeneratetext1416211_1"/><a data-type="indexterm" data-primary="Char-RNN model, Shakespearean text" id="xi_CharRNNmodelShakespeareantext1416211_1"/>. This <em>char-RNN</em> can then be used to generate novel text, one character at a time. Here is a small sample of the text generated by a char-RNN model after it was trained on all of Shakespeare’s works:</p>
<blockquote>
<p>PANDARUS:</p>

<p>Alas, I think he shall be come approached and the day</p>

<p>When little srain would be attain’d into being never fed,</p>

<p>And who is but a chain and subjects of his death,</p>

<p>I should not sleep.</p></blockquote>

<p>Not exactly a masterpiece, but it is still impressive that the model was able to learn words, grammar, proper punctuation, and more, just by learning to predict the next character in a sentence. This is our first example of a <em>language model</em>. In the remainder of this section we’ll build a char-RNN step by step, starting with the creation of the dataset.</p>








<section data-type="sect2" data-pdf-bookmark="Creating the Training Dataset"><div class="sect2" id="id263">
<h2>Creating the Training Dataset</h2>

<p>First, let’s download a subset of Shakespeare’s works<a data-type="indexterm" data-primary="Char-RNN model, Shakespearean text" data-secondary="creating training dataset" id="xi_CharRNNmodelShakespeareantextcreatingtrainingdataset143054_1"/> (about 25%). The data is loaded from Andrej Karpathy’s <a href="https://github.com/karpathy/char-rnn">char-rnn project</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">pathlib</code> <code class="kn">import</code> <code class="n">Path</code>
<code class="kn">import</code> <code class="nn">urllib.request</code>

<code class="k">def</code> <code class="nf">download_shakespeare_text</code><code class="p">():</code>
    <code class="n">path</code> <code class="o">=</code> <code class="n">Path</code><code class="p">(</code><code class="s2">"datasets/shakespeare/shakespeare.txt"</code><code class="p">)</code>
    <code class="k">if</code> <code class="ow">not</code> <code class="n">path</code><code class="o">.</code><code class="n">is_file</code><code class="p">():</code>
        <code class="n">path</code><code class="o">.</code><code class="n">parent</code><code class="o">.</code><code class="n">mkdir</code><code class="p">(</code><code class="n">parents</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">exist_ok</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="n">url</code> <code class="o">=</code> <code class="s2">"https://homl.info/shakespeare"</code>
        <code class="n">urllib</code><code class="o">.</code><code class="n">request</code><code class="o">.</code><code class="n">urlretrieve</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">path</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">path</code><code class="o">.</code><code class="n">read_text</code><code class="p">()</code>

<code class="n">shakespeare_text</code> <code class="o">=</code> <code class="n">download_shakespeare_text</code><code class="p">()</code></pre>

<p>Let’s print the first few lines:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="nb">print</code><code class="p">(</code><code class="n">shakespeare_text</code><code class="p">[:</code><code class="mi">80</code><code class="p">])</code><code class="w"/>
<code class="go">First Citizen:</code>
<code class="go">Before we proceed any further, hear me speak.</code>

<code class="go">All:</code>
<code class="go">Speak, speak.</code></pre>

<p>Looks like Shakespeare, all right!</p>

<p>Neural networks work with numbers, not text, so we need a way to encode text into numbers. In general, this is done by splitting the text into <em>tokens</em>, such as words or characters, and assigning an integer ID to each possible token. For example, let’s split our text into characters, and assign an ID to each possible character. We first need to find the list of characters used in the text. This will constitute<a data-type="indexterm" data-primary="token vocabulary" id="id3203"/> our token <em>vocabulary</em>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">vocab</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code><code class="nb">set</code><code class="p">(</code><code class="n">shakespeare_text</code><code class="o">.</code><code class="n">lower</code><code class="p">()))</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="s2">""</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">vocab</code><code class="p">)</code><code class="w"/>
<code class="go">"\n !$&amp;',-.3:;?abcdefghijklmnopqrstuvwxyz"</code></pre>

<p class="pagebreak-before">Note that we<a data-type="indexterm" data-primary="lower()" id="id3204"/> call <code translate="no">lower()</code> to ignore case and thereby reduce the vocabulary size. We must now assign a token ID to each character. For this, we can just use its index in the vocabulary. To decode the output of our model, we will also need a way to go from a token ID to a character:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">char_to_id</code> <code class="o">=</code> <code class="p">{</code><code class="n">char</code><code class="p">:</code> <code class="n">index</code> <code class="k">for</code> <code class="n">index</code><code class="p">,</code> <code class="n">char</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">vocab</code><code class="p">)}</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">id_to_char</code> <code class="o">=</code> <code class="p">{</code><code class="n">index</code><code class="p">:</code> <code class="n">char</code> <code class="k">for</code> <code class="n">index</code><code class="p">,</code> <code class="n">char</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">vocab</code><code class="p">)}</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">char_to_id</code><code class="p">[</code><code class="s2">"a"</code><code class="p">]</code><code class="w"/>
<code class="go">13</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">id_to_char</code><code class="p">[</code><code class="mi">13</code><code class="p">]</code><code class="w"/>
<code class="go">'a'</code></pre>

<p>Next, let’s create two helper functions to encode text to tensors of token IDs, and to decode<a data-type="indexterm" data-primary="encoder" id="id3205"/><a data-type="indexterm" data-primary="decoder" id="id3206"/> them back to text:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>

<code class="k">def</code> <code class="nf">encode_text</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="n">char_to_id</code><code class="p">[</code><code class="n">char</code><code class="p">]</code> <code class="k">for</code> <code class="n">char</code> <code class="ow">in</code> <code class="n">text</code><code class="o">.</code><code class="n">lower</code><code class="p">()])</code>

<code class="k">def</code> <code class="nf">decode_text</code><code class="p">(</code><code class="n">char_ids</code><code class="p">):</code>
    <code class="k">return</code> <code class="s2">""</code><code class="o">.</code><code class="n">join</code><code class="p">([</code><code class="n">id_to_char</code><code class="p">[</code><code class="n">char_id</code><code class="o">.</code><code class="n">item</code><code class="p">()]</code> <code class="k">for</code> <code class="n">char_id</code> <code class="ow">in</code> <code class="n">char_ids</code><code class="p">])</code></pre>

<p>Let’s try them out:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">encoded</code> <code class="o">=</code> <code class="n">encode_text</code><code class="p">(</code><code class="s2">"Hello, world!"</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">encoded</code><code class="w"/>
<code class="go">tensor([20, 17, 24, 24, 27,  6,  1, 35, 27, 30, 24, 16,  2])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">decode_text</code><code class="p">(</code><code class="n">encoded</code><code class="p">)</code><code class="w"/>
<code class="go">'hello, world!'</code></pre>

<p>Next, let’s prepare the dataset. Right now, we have a single, extremely long sequence of characters containing Shakespeare’s works. Just like we did in <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>, we can turn this long sequence into a dataset of windows that we can then use to train a sequence-to-sequence RNN. The targets will be similar to the inputs, but shifted by one time step into the “future”. For example, one sample in the dataset may be a sequence of character IDs representing the text “to be or not to b” (without the final “e”), and the corresponding target—a sequence of character IDs representing the text “o be or not to be” (with the final “e”, but without the leading “t”). Let’s create our dataset class:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">Dataset</code><code class="p">,</code> <code class="n">DataLoader</code>

<code class="k">class</code> <code class="nc">CharDataset</code><code class="p">(</code><code class="n">Dataset</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">text</code><code class="p">,</code> <code class="n">window_length</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">encoded_text</code> <code class="o">=</code> <code class="n">encode_text</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">window_length</code> <code class="o">=</code> <code class="n">window_length</code>

    <code class="k">def</code> <code class="fm">__len__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="k">return</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">encoded_text</code><code class="p">)</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">window_length</code>

    <code class="k">def</code> <code class="fm">__getitem__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">idx</code><code class="p">):</code>
        <code class="k">if</code> <code class="n">idx</code> <code class="o">&gt;=</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
            <code class="k">raise</code> <code class="ne">IndexError</code><code class="p">(</code><code class="s2">"dataset index out of range"</code><code class="p">)</code>
        <code class="n">end</code> <code class="o">=</code> <code class="n">idx</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">window_length</code>
        <code class="n">window</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">encoded_text</code><code class="p">[</code><code class="n">idx</code> <code class="p">:</code> <code class="n">end</code><code class="p">]</code>
        <code class="n">target</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">encoded_text</code><code class="p">[</code><code class="n">idx</code> <code class="o">+</code> <code class="mi">1</code> <code class="p">:</code> <code class="n">end</code> <code class="o">+</code> <code class="mi">1</code><code class="p">]</code>
        <code class="k">return</code> <code class="n">window</code><code class="p">,</code> <code class="n">target</code></pre>

<p>And now let’s create the data loaders, as usual. Since the text is quite large, we can afford to use roughly 90% for training (i.e., one million characters), and just 5% for validation, and 5% for testing (60,000 characters each):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">window_length</code> <code class="o">=</code> <code class="mi">50</code>
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">512</code>  <code class="c1"># reduce if your GPU cannot handle such a large batch size</code>
<code class="n">train_set</code> <code class="o">=</code> <code class="n">CharDataset</code><code class="p">(</code><code class="n">shakespeare_text</code><code class="p">[:</code><code class="mi">1_000_000</code><code class="p">],</code> <code class="n">window_length</code><code class="p">)</code>
<code class="n">valid_set</code> <code class="o">=</code> <code class="n">CharDataset</code><code class="p">(</code><code class="n">shakespeare_text</code><code class="p">[</code><code class="mi">1_000_000</code><code class="p">:</code><code class="mi">1_060_000</code><code class="p">],</code> <code class="n">window_length</code><code class="p">)</code>
<code class="n">test_set</code> <code class="o">=</code> <code class="n">CharDataset</code><code class="p">(</code><code class="n">shakespeare_text</code><code class="p">[</code><code class="mi">1_060_000</code><code class="p">:],</code> <code class="n">window_length</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">valid_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">valid_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">)</code>
<code class="n">test_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">test_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">)</code></pre>

<p>Each batch will be composed of 512 50-character windows, where each character is represented by its token ID, and where each window comes with its 50-character target window (offset by one character). Note that the training batches are shuffled at each epoch (see <a data-type="xref" href="#window_dataset_diagram">Figure 14-1</a>).</p>

<figure class="smallereighty"><div id="window_dataset_diagram" class="figure">
<img src="assets/hmls_1401.png" alt="Diagram illustrating a batch of input and target windows with a window length of 10, showing the offset relationship between inputs and targets." width="824" height="342"/>
<h6><span class="label">Figure 14-1. </span>Each training batch is composed of shuffled windows, along with their shifted targets. In this figure, the window length is 10 instead of 50.</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>We set the window length to 50, but you can try tuning it. It’s easier and faster to train RNNs on shorter input sequences, but the RNN will not be able to learn any pattern longer than the window length<a data-type="indexterm" data-primary="window length" id="id3207"/>, so don’t make it too small.</p>
</div>

<p>While we could technically feed the token IDs directly to a neural network without any further preprocessing, it wouldn’t work very well. Indeed, as we saw in <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a>, most ML models—including neural networks—assume that similar inputs represent similar things; unfortunately, similar IDs may represent totally unrelated tokens, and conversely, distant IDs may represent similar tokens. The neural net would be biased in a weird way, and it would have great difficulty overcoming this bias during training.</p>

<p>One solution is to use one-hot encoding<a data-type="indexterm" data-primary="one-hot encoding" id="id3208"/>, since all one-hot vectors are equally distant from one another. However, when the vocabulary is large, one-hot vectors are equally large. In our case, the vocabulary contains just 39 characters, so each character would be represented by a 39-dimensional one-hot vector. That’s still manageable, but if we were dealing with words instead of characters, the vocabulary size could be in the tens of thousands, so one-hot encoding would be out of the question<a data-type="indexterm" data-startref="xi_CharRNNmodelShakespeareantextcreatingtrainingdataset143054_1" id="id3209"/>. Luckily, since we are dealing with neural networks, we have a better option: embeddings.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Embeddings"><div class="sect2" id="id264">
<h2>Embeddings</h2>

<p>An embedding<a data-type="indexterm" data-primary="Char-RNN model, Shakespearean text" data-secondary="embeddings" id="xi_CharRNNmodelShakespeareantextembeddings1415813_1"/><a data-type="indexterm" data-primary="embeddings" data-secondary="Char-RNN" id="xi_embeddingsCharRNN1415813_1"/> is a dense representation of some higher-dimensional data, typically a categorical feature. If there are 50,000 possible categories, then one-hot encoding produces a 50,000-dimensional sparse vector (i.e., containing mostly zeros). In contrast, an embedding is a comparatively small dense vector; for example, with just 300 dimensions.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The embedding size is a hyperparameter you can tune. As a rule of thumb, a good embedding size is often close to the square root of the number of categories.</p>
</div>

<p>In deep learning, embeddings are usually initialized randomly, and they are then trained by gradient descent, along with the other model parameters. For example, if we wanted to train a neural network on the California housing dataset (see <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a>), we could represent the <code translate="no">ocean_proximity</code> categorical feature using embeddings. The <code translate="no">"NEAR BAY"</code> category could be represented initially by a random vector such as <code translate="no">[0.831, 0.696]</code>, while the <code translate="no">"NEAR OCEAN"</code> category might be represented by another random vector such as <code translate="no">[0.127, 0.868]</code> (in this example we are using 2D 
<span class="keep-together">embeddings).</span></p>

<p>Since these embeddings are trainable, they will gradually improve during training; and as they represent fairly similar categories in this example, gradient descent will certainly end up pushing them closer together, while it will tend to move them away from the <code translate="no">"INLAND"</code> category’s embedding (see <a data-type="xref" href="#embedding_diagram">Figure 14-2</a>). Indeed, the better the representation, the easier it will be for the neural network to make accurate predictions, so training tends to make embeddings useful representations of the categories. This is called <em>representation learning</em> (you will see other types of representation learning in <a data-type="xref" href="ch18.html#autoencoders_chapter">Chapter 18</a>).</p>

<figure class="width-75"><div id="embedding_diagram" class="figure">
<img src="assets/hmls_1402.png" alt="Diagram illustrating the improvement of embeddings during training, showing categories such as &quot;Near ocean,&quot; &quot;Near bay,&quot; and &quot;Inland&quot; in an embedding space." width="1161" height="580"/>
<h6><span class="label">Figure 14-2. </span>Embeddings will gradually improve during training</h6>
</div></figure>

<p>Not only will embeddings generally be useful representations for the task at hand, but quite often these same embeddings can be reused successfully for other tasks. The most common example of this<a data-type="indexterm" data-primary="word embeddings" id="xi_wordembeddings14171197_1"/><a data-type="indexterm" data-primary="embeddings" data-secondary="word" id="xi_embeddingsword14171197_1"/> is <em>word embeddings</em> (i.e., embeddings of individual words): when you are working on a natural language processing task, you are often better off reusing pretrained word embeddings than training your own, as we will see later in this chapter.</p>

<p>The idea of using vectors to represent words dates back to the 1960s, and many sophisticated techniques have been used to generate useful vectors, including using neural networks. But things really took off in 2013, when Tomáš Mikolov and other Google researchers published a <a href="https://homl.info/word2vec">paper</a>⁠<sup><a data-type="noteref" id="id3210-marker" href="ch14.html#id3210">3</a></sup> describing an efficient technique to learn word embeddings using neural networks, significantly outperforming previous attempts. This allowed them to learn embeddings on a very large corpus of text: they trained a neural network to predict the words near any given word and obtained astounding word embeddings. For example, synonyms had very close embeddings, and semantically related words such as <em>France</em>, <em>Spain</em>, and <em>Italy</em> were clustered together.</p>

<p>It’s not just about proximity, though: word embeddings are also organized along meaningful axes in the embedding space. Here is a famous example: if you compute <em>King – Man + Woman</em> (adding and subtracting the embedding vectors of these words), then the result will be very close to the embedding of the word <em>Queen</em> (see <a data-type="xref" href="#word_embedding_diagram">Figure 14-3</a>). In other words, the word embeddings encode the concept of gender! Similarly, you can compute <em>Madrid – Spain + France</em>, and the result is close to <em>Paris</em>, which seems to show that the notion of capital city is also encoded in the <span class="keep-together">embeddings</span>.</p>

<figure class="width-45"><div id="word_embedding_diagram" class="figure">
<img src="assets/hmls_1403.png" alt="Diagram illustrating how word embeddings calculate &quot;King - Man + Woman&quot; to approximate the position of &quot;Queen,&quot; demonstrating the encoding of the gender concept in the embedding space." width="563" height="432"/>
<h6><span class="label">Figure 14-3. </span>Word embeddings of similar words tend to be close, and some axes seem to encode meaningful concepts</h6>
</div></figure>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Word embeddings can have some meaningful structure, as the “King – Man + Woman” shows. However, they are also noisy and often hard to interpret. I’ve added some code at the end of the notebook so you can judge for yourself.</p>
</div>

<p>Unfortunately, word embeddings sometimes capture our worst biases. For example, although they correctly learn that <em>Man is to King as Woman is to Queen</em>, they also seem to learn that <em>Man is to Doctor as Woman is to Nurse</em>: quite a sexist bias! To be fair, this particular example is probably exaggerated, as was pointed out in a <a href="https://homl.info/fairembeds">2019 paper</a>⁠<sup><a data-type="noteref" id="id3211-marker" href="ch14.html#id3211">4</a></sup> by Malvina Nissim et al. Nevertheless, ensuring fairness in deep learning algorithms is an important and active research topic<a data-type="indexterm" data-startref="xi_wordembeddings14171197_1" id="id3212"/><a data-type="indexterm" data-startref="xi_embeddingsword14171197_1" id="id3213"/>.</p>

<p>PyTorch provides an <code translate="no">nn.Embedding</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.Embedding" id="xi_torchnnEmbedding1418742_1"/>, which wraps<a data-type="indexterm" data-primary="embedding matrix" id="id3214"/> an <em>embedding matrix</em>: this matrix has one row per possible category (e.g., one row for each token in the vocabulary) and one column per embedding dimension. The embedding dimensionality is a hyperparameter you can tune. By default, the embedding matrix is initialized randomly.</p>

<p>To convert a category ID to an embedding, the <code translate="no">nn.Embedding</code> layer just looks up and returns the corresponding row. That’s all there is to it! For example, let’s initialize an <code translate="no">nn.Embedding</code> layer with five categories and 3D embeddings, and use it to encode some categories:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">embed</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>  <code class="c1"># 5 categories × 3D embeddings</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">embed</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">]]))</code><code class="w"/>
<code class="go">tensor([[[ 0.2674,  0.5349,  0.8094],</code>
<code class="go">         [ 2.2082, -0.6380,  0.4617]],</code>

<code class="go">        [[ 0.3367,  0.1288,  0.2345],</code>
<code class="go">         [ 2.2082, -0.6380,  0.4617]]], grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre>

<p>As you can see, category 3 gets encoded as the 3D vector <code translate="no">[0.2674, 0.5349, 0.8094]</code>, category 2 gets encoded (twice) as the 3D vector <code translate="no">[2.2082, -0.6380, 0.4617]</code>, and category 0 gets encoded as the 3D vector <code translate="no">[0.3367, 0.1288, 0.2345]</code> (categories 1 and 4 were not used in this example). Since the layer is not trained yet, these encodings are just random.</p>

<p>Note that an embedding layer is mathematically equivalent to one-hot encoding followed by a linear layer (with no bias parameter). For example, if you create a linear layer with <code translate="no">nn.Linear(5, 3, bias=False)</code> and pass it the one-hot vector <code translate="no">torch.tensor([[0., 0., 0., 1., 0.]])</code>, you get a vector equal to row #3 of the linear layer’s transposed weight matrix (which acts as an embedding matrix). That’s because all rows in the transposed weight matrix get multiplied by zero, except for row #3 which gets multiplied by 1, so the result is just row #3. However, it’s much more efficient to use <code translate="no">nn.Embedding(5, 3)</code> and pass it <code translate="no">torch.tensor([3])</code>: this looks up row #3 in the embedding matrix without the need for one-hot encoding, and without all the pointless multiplications by zero<a data-type="indexterm" data-startref="xi_CharRNNmodelShakespeareantextembeddings1415813_1" id="id3215"/><a data-type="indexterm" data-startref="xi_embeddingsCharRNN1415813_1" id="id3216"/>.</p>

<p>OK, now that you know about embeddings, you are ready to build the Shakespeare model.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Building and Training the Char-RNN Model"><div class="sect2" id="id265">
<h2>Building and Training the Char-RNN Model</h2>

<p>Since our dataset<a data-type="indexterm" data-primary="Char-RNN model, Shakespearean text" data-secondary="building and training the model" id="xi_CharRNNmodelShakespeareantextbuildingandtrainingthemodel1421118_1"/> is reasonably large, and modeling language is quite a difficult task, we need more than a simple RNN with a few recurrent neurons. Let’s build and train a model with a two-layer <code translate="no">nn.GRU</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.GRU" id="xi_torchnnGRU14211212_1"/> (introduced in <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>), with 128 units per layer, and a bit of dropout. You can try tweaking the number of layers and units later, if needed:</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="widows7"><code class="k">class</code> <code class="nc">ShakespeareModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">,</code> <code class="n">n_layers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">embed_dim</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code>
                 <code class="n">dropout</code><code class="o">=</code><code class="mf">0.1</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">embed</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">gru</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">,</code> <code class="n">num_layers</code><code class="o">=</code><code class="n">n_layers</code><code class="p">,</code>
                          <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="n">dropout</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">embed</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">outputs</code><code class="p">,</code> <code class="n">_states</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">gru</code><code class="p">(</code><code class="n">embeddings</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">outputs</code><code class="p">)</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ShakespeareModel</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">vocab</code><code class="p">))</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>Let’s go over this code:</p>

<ul>
<li>
<p>We use an <code translate="no">nn.Embedding</code> layer as the first layer, to encode the character IDs. As we just saw, the <code translate="no">nn.Embedding</code> layer’s number of input dimensions is the number of categories, so in our case it’s the number of distinct character IDs. The embedding size is a hyperparameter you can tune—we’ll set it to 10 for now. Whereas the inputs of the <code translate="no">nn.Embedding</code> layer will be integer tensors of shape [<em>batch size</em>, <em>window length</em>], the outputs of the <code translate="no">nn.Embedding</code> layer will be float tensors of shape [<em>batch size</em>, <em>window length</em>, <em>embedding size</em>].</p>
</li>
<li>
<p>The <code translate="no">nn.GRU</code> layer has 10 inputs (i.e., the embedding size), 128 outputs (i.e., the hidden size), two layers, and as usual we must specify <code translate="no">batch_first=True</code> because otherwise the layer assumes that the batch dimension comes after the time dimension<a data-type="indexterm" data-startref="xi_torchnnEmbedding1418742_1" id="id3217"/>.</p>
</li>
<li>
<p>We use an <code translate="no">nn.Linear</code> layer for the output layer: it must have 39 units because there are 39 distinct characters in the text, and we want to output a logit for each possible character (at each time step).</p>
</li>
<li>
<p>In the <code translate="no">forward()</code> method, we just call these layers one by one. Note that the <code translate="no">nn.GRU</code> layer’s output shape is [<em>batch size</em>, <em>window length</em>, <em>hidden size</em>], and the <code translate="no">nn.Linear</code> layer’s output shape is [<em>batch size</em>, <em>window length</em>, <em>vocabulary size</em>], but as we saw in <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>, the <code translate="no">nn.CrossEntropyLoss</code> and <code translate="no">Accuracy</code> modules that we will use for training both expect the class dimension (i.e., <code translate="no">vocab_size</code>) to be the second dimension, not the last one. This is why we must permute the last two dimensions of the <code translate="no">nn.Linear</code> layer’s output. Note that the <code translate="no">nn.GRU</code> layer also returns the final hidden states, but we ignore them.⁠<sup><a data-type="noteref" id="id3218-marker" href="ch14.html#id3218">5</a></sup></p>
</li>
</ul>

<p>Now you can now train and evaluate the model as usual, using the <code translate="no">nn.CrossEntropyLoss</code> and the <code translate="no">Accuracy</code> metric<a data-type="indexterm" data-primary="accuracy performance measure" id="id3219"/>.</p>

<p>And now let’s use our model to predict the next character in a sentence:</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="widows4"><code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>  <code class="c1"># don't forget to switch the model to evaluation mode!</code>
<code class="n">text</code> <code class="o">=</code> <code class="s2">"To be or not to b"</code>
<code class="n">encoded_text</code> <code class="o">=</code> <code class="n">encode_text</code><code class="p">(</code><code class="n">text</code><code class="p">)</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">Y_logits</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">encoded_text</code><code class="p">)</code>
    <code class="n">predicted_char_id</code> <code class="o">=</code> <code class="n">Y_logits</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="p">:,</code> <code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">argmax</code><code class="p">()</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
    <code class="n">predicted_char</code> <code class="o">=</code> <code class="n">id_to_char</code><code class="p">[</code><code class="n">predicted_char_id</code><code class="p">]</code>  <code class="c1"># correctly predicts "e"</code></pre>

<p>We first encode the text, add a batch dimension of size 1, and move the tensor to the GPU. Then we call our model and get logits for each time step. We’re only interested in logits for the final time step (hence the –1), and we want to know which token ID has the highest logit, so we use <code translate="no">argmax()</code>. We then use <code translate="no">item()</code> to extract the token ID from the tensor. Lastly, we convert the token ID to a character, and that’s our prediction<a data-type="indexterm" data-startref="xi_torchnnGRU14211212_1" id="id3220"/>.</p>

<p>The model correctly predicts “e”, great! Now let’s use this model to pretend we’re Shakespeare<a data-type="indexterm" data-startref="xi_CharRNNmodelShakespeareantextbuildingandtrainingthemodel1421118_1" id="id3221"/>!</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If you are running this code on Colab with a GPU activated, then training will take a few hours. You can reduce the number of epochs if you don’t want to wait that long, but of course the model’s accuracy will probably be lower. If the Colab session times out, make sure to reconnect quickly, or else the Colab runtime will be destroyed.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Generating Fake Shakespearean Text"><div class="sect2" id="id266">
<h2>Generating Fake Shakespearean Text</h2>

<p>To generate new text<a data-type="indexterm" data-primary="Char-RNN model, Shakespearean text" data-secondary="generating new text" id="xi_CharRNNmodelShakespeareantextgeneratingnewtext1426421_1"/> using the char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it to the end of the text, then give the extended text to the model to guess the next letter, and so on. This<a data-type="indexterm" data-primary="greedy decoding" id="id3222"/> is called <em>greedy decoding</em>. But in practice this often leads to the same words being repeated over and over again. Instead, we can sample the next character randomly, using the model’s estimated probability distribution: if the model estimates a probability <em>p</em> for a given token, then this token will be sampled with probability <em>p</em>. This process will generate more diverse and interesting text since the most likely token won’t always be sampled. To sample the next token, we can use the <code translate="no">torch.multinomial()</code> function, which samples random class indices, given a list of class probabilities. For example:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">probs</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.4</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">]])</code>  <code class="c1"># probas = 50%, 40%, and 10%</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">samples</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">multinomial</code><code class="p">(</code><code class="n">probs</code><code class="p">,</code> <code class="n">replacement</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">num_samples</code><code class="o">=</code><code class="mi">8</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">samples</code><code class="w"/>
<code class="go">tensor([[0, 0, 0, 0, 1, 0, 2, 2]])</code></pre>

<p>To have more control over the diversity of the generated text, we can divide the logits by a number<a data-type="indexterm" data-primary="temperature, Char-RNN model" id="id3223"/> called the <em>temperature</em>, which we can tweak as we wish. A temperature close to zero favors high-probability characters, while a high temperature gives all characters an equal probability. Lower temperatures are typically preferred when generating fairly rigid and precise text, such as mathematical equations, while higher temperatures are preferred when generating more diverse and creative text. Let’s write a <code translate="no">next_char()</code> helper function that will use this approach to pick the next character to add to the input text:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch.nn.functional</code> <code class="k">as</code> <code class="nn">F</code>

<code class="k">def</code> <code class="nf">next_char</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">text</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>
    <code class="n">encoded_text</code> <code class="o">=</code> <code class="n">encode_text</code><code class="p">(</code><code class="n">text</code><code class="p">)</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="n">Y_logits</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">encoded_text</code><code class="p">)</code>
        <code class="n">Y_probas</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">Y_logits</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="p">:,</code> <code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="o">/</code> <code class="n">temperature</code><code class="p">,</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">predicted_char_id</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">multinomial</code><code class="p">(</code><code class="n">Y_probas</code><code class="p">,</code> <code class="n">num_samples</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
    <code class="k">return</code> <code class="n">id_to_char</code><code class="p">[</code><code class="n">predicted_char_id</code><code class="p">]</code></pre>

<p>Next, we can write another small helper function that will repeatedly call <code>next_​char()</code> to get the next character and append it to the given text:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">extend_text</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">text</code><code class="p">,</code> <code class="n">n_chars</code><code class="o">=</code><code class="mi">80</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_chars</code><code class="p">):</code>
        <code class="n">text</code> <code class="o">+=</code> <code class="n">next_char</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">text</code><code class="p">,</code> <code class="n">temperature</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">text</code></pre>

<p>We are now ready to generate some text! Let’s try low, medium, and high 
<span class="keep-together">temperatures:</span></p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="nb">print</code><code class="p">(</code><code class="n">extend_text</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="s2">"To be or not to b"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mf">0.01</code><code class="p">))</code><code class="w"/>
<code class="go">To be or not to be the state</code>
<code class="go">and the contrary of the state and the sea,</code>
<code class="go">the common people of the</code>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">print</code><code class="p">(</code><code class="n">extend_text</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="s2">"To be or not to b"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mf">0.4</code><code class="p">))</code><code class="w"/>
<code class="go">To be or not to be the better from the cause</code>
<code class="go">that thou think you may be so be gone.</code>

<code class="go">romeo:</code>
<code class="go">that</code>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">print</code><code class="p">(</code><code class="n">extend_text</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="s2">"To be or not to b"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">100</code><code class="p">))</code><code class="w"/>
<code class="go">To be or not to b-c3;m-rkn&amp;:x:uyve:b&amp;hi n;n-h;wt3k</code>
<code class="go">&amp;cixxh:a!kq$c$ 3 ncq$ ;;wq cp:!xq;yh</code>
<code class="go">!3</code>
<code class="go">d!nhi.</code></pre>

<p>Notice the repetitions when the temperature is low: “the state and the” appears twice. The intermediate temperature led to more convincing results, although Romeo wasn’t very talkative today. But in the last example the temperature was way too high—we fried Shakespeare. To generate more convincing text, a common technique is to sample only from the top <em>k</em> characters, or only from the smallest set of top characters whose total probability exceeds some threshold: this<a data-type="indexterm" data-primary="nucleus sampling" id="id3224"/><a data-type="indexterm" data-primary="top-p sampling" id="id3225"/> is called <em>top-p sampling</em>, or <em>nucleus sampling</em>. Alternatively, you could try<a data-type="indexterm" data-primary="beam search" id="id3226"/><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="beam search" id="id3227"/><a data-type="indexterm" data-primary="neural machine translation (NMT)" data-secondary="and beam search" data-secondary-sortas="beam search" id="id3228"/> using <em>beam search</em>, which we will discuss later in this chapter, or using more <code translate="no">nn.GRU</code> layers<a data-type="indexterm" data-primary="torch" data-secondary="nn.GRU" id="id3229"/> and more neurons per layer, training for longer, and adding more regularization if needed.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The model is currently incapable of learning patterns longer than <code translate="no">window_length</code>, which is just 50 characters. You could try making this window larger, but it would also make training harder, and even LSTM and GRU cells cannot handle very long sequences<a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="stateful" id="id3230"/><a data-type="indexterm" data-primary="stateful RNN" id="id3231"/>.⁠<sup><a data-type="noteref" id="id3232-marker" href="ch14.html#id3232">6</a></sup></p>
</div>

<p>Interestingly, although a char-RNN model is just trained to predict the next character, this seemingly simple task actually requires it to learn some higher-level tasks as well. For example, to find the next character after “Great movie, I really “, it’s helpful to understand that the sentence is positive, so what follows is more likely to be the letter “l” (for “loved”) rather than “h” (for “hated”). In fact, a <a href="https://homl.info/sentimentneuron">2017 paper</a>⁠<sup><a data-type="noteref" id="id3233-marker" href="ch14.html#id3233">7</a></sup> by Alec Radford and other OpenAI researchers describes how the authors trained a big char-RNN-like model on a large dataset, and found that one of the neurons acted as an excellent sentiment analysis classifier. Although the model was trained without any labels<a data-type="indexterm" data-primary="sentiment neuron" id="id3234"/>, the <em>sentiment neuron</em>—as they called it—reached state-of-the-art performance on sentiment analysis benchmarks (this foreshadowed and motivated unsupervised pretraining in NLP)<a data-type="indexterm" data-startref="xi_naturallanguageprocessingNLPCharRNNmodeltogeneratetext1416211_1" id="id3235"/><a data-type="indexterm" data-startref="xi_CharRNNmodelShakespeareantext1416211_1" id="id3236"/><a data-type="indexterm" data-startref="xi_CharRNNmodelShakespeareantextgeneratingnewtext1426421_1" id="id3237"/>.</p>

<p>Speaking of which, let’s say farewell to Shakespeare and turn to the second part of this chapter: sentiment analysis.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Sentiment Analysis Using Hugging Face Libraries"><div class="sect1" id="id267">
<h1>Sentiment Analysis Using Hugging Face Libraries</h1>

<p>One of the most common applications of NLP<a data-type="indexterm" data-primary="sentiment analysis" id="xi_sentimentanalysis1433043_1"/> is text classification—especially sentiment analysis. If image classification on the MNIST dataset is the “Hello, world!” of computer vision, then sentiment analysis on the IMDb reviews dataset is the “Hello, world!” of natural language processing. The IMDb dataset<a data-type="indexterm" data-primary="IMDb dataset" id="id3238"/> consists of 50,000 movie reviews in English (25,000 for training, 25,000 for testing) extracted from the famous <a href="https://imdb.com">Internet Movie Database</a>, along with a simple binary target for each review indicating whether it is negative (0) or positive (1). Just like MNIST, the IMDb reviews dataset is popular for good reasons: it is simple enough to be tackled on a <span class="keep-together">laptop</span> in a reasonable amount of time, but challenging enough to be fun and rewarding.</p>

<p>To download the IMDb dataset, we will use the Hugging Face <em>Datasets</em> library<a data-type="indexterm" data-primary="Datasets library" id="id3239"/>, which gives easy access to hundreds of thousands of datasets hosted on the Hugging Face Hub. It is preinstalled on Colab; otherwise it can be installed using <code translate="no">pip install datasets</code>. We’ll use 80% of the original training set for training, and the remaining 20% for validation, using the <code translate="no">train_test_split()</code> method<a data-type="indexterm" data-primary="Datasets library" data-secondary="train_test_split()" id="id3240"/> to split the set:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">load_dataset</code>

<code class="n">imdb_dataset</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"imdb"</code><code class="p">)</code>
<code class="n">split</code> <code class="o">=</code> <code class="n">imdb_dataset</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">train_test_split</code><code class="p">(</code><code class="n">train_size</code><code class="o">=</code><code class="mf">0.8</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">imdb_train_set</code><code class="p">,</code> <code class="n">imdb_valid_set</code> <code class="o">=</code> <code class="n">split</code><code class="p">[</code><code class="s2">"train"</code><code class="p">],</code> <code class="n">split</code><code class="p">[</code><code class="s2">"test"</code><code class="p">]</code>
<code class="n">imdb_test_set</code> <code class="o">=</code> <code class="n">imdb_dataset</code><code class="p">[</code><code class="s2">"test"</code><code class="p">]</code></pre>

<p>Let’s inspect a couple of reviews:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">imdb_train_set</code><code class="p">[</code><code class="mi">1</code><code class="p">][</code><code class="s2">"text"</code><code class="p">]</code><code class="w"/>
<code class="go">"'The Rookie' was a wonderful movie about the second chances life holds [...]"</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">imdb_train_set</code><code class="p">[</code><code class="mi">1</code><code class="p">][</code><code class="s2">"label"</code><code class="p">]</code><code class="w"/>
<code class="go">1</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">imdb_train_set</code><code class="p">[</code><code class="mi">16</code><code class="p">][</code><code class="s2">"text"</code><code class="p">]</code><code class="w"/>
<code class="go">"Lillian Hellman's play, adapted by Dashiell Hammett with help from Hellman,</code>
<code class="go">becomes a curious project to come out of gritty Warner Bros. [...] It seems to</code>
<code class="go">take forever for this drama to find its focus, [...], it seems a little</code>
<code class="go">patronizing [...] Lukas has several speeches in the third-act which undoubtedly</code>
<code class="go">won him the Academy Award [...] this tasteful, tactful movie [...] It should be</code>
<code class="go">a heady mix, but instead it's rather dry-eyed and inert. ** from ****"</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">imdb_train_set</code><code class="p">[</code><code class="mi">16</code><code class="p">][</code><code class="s2">"label"</code><code class="p">]</code><code class="w"/>
<code class="go">0</code></pre>

<p>The first review immediately says that it’s a wonderful movie; no need to read any further: it’s clearly positive (label = 1). The second review is much harder to classify: it contains a detailed description of the movie, sprinkled with both positive and negative comments. Luckily, the conclusion is quite clearly negative, making the task much easier (label = 0). Still, it’s not a trivial task.</p>

<p>A simple char-RNN model would struggle; we need a more powerful tokenization technique. So let’s focus on tokenization before we return to sentiment analysis.</p>








<section data-type="sect2" data-pdf-bookmark="Tokenization Using the Hugging Face Tokenizers Library"><div class="sect2" id="id268">
<h2>Tokenization Using the Hugging Face Tokenizers Library</h2>

<p>In a <a href="https://homl.info/rarewords">2016 paper</a>,⁠<sup><a data-type="noteref" id="id3241-marker" href="ch14.html#id3241">8</a></sup> Rico Sennrich et al. from the University of Edinburgh explored several methods to tokenize and detokenize text at the subword level. This way, even if your model encounters a rare word it has never seen before, it can still reasonably guess what it means. For example, even if the model never saw the word “smartest” during training, if it learned the word “smart” and it also learned that the suffix “est” means “the most”, it can infer the meaning of “smartest”. One of the techniques the authors evaluated is <em>byte pair encoding</em> (BPE)<a data-type="indexterm" data-primary="byte pair encoding (BPE)" id="xi_bytepairencodingBPE14368795_1"/><a data-type="indexterm" data-primary="BPE (byte pair encoding)" id="xi_BPEbytepairencoding14368795_1"/>, introduced by Philip Gage in 1994 (initially for data compression). BPE works by splitting the whole training set into individual characters, then at each iteration it finds the most frequent pair of adjacent tokens and adds it to the vocabulary. It repeats this process until the vocabulary reaches the desired size.</p>

<p>The <a href="https://homl.info/tokenizers">Hugging Face Tokenizers library</a> includes<a data-type="indexterm" data-primary="Tokenizers library" id="xi_Tokenizerslibrary1437075_1"/><a data-type="indexterm" data-primary="sentiment analysis" data-secondary="tokenization" id="xi_sentimentanalysistokenization1437075_1"/> highly efficient implementations of several popular tokenization algorithms, including BPE. It is preinstalled on Colab (or you can install it with <code translate="no">pip install tokenizers</code>). Here’s how to train a BPE model on the IMDb dataset:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tokenizers</code>

<code class="n">bpe_model</code> <code class="o">=</code> <code class="n">tokenizers</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">BPE</code><code class="p">(</code><code class="n">unk_token</code><code class="o">=</code><code class="s2">"&lt;unk&gt;"</code><code class="p">)</code>
<code class="n">bpe_tokenizer</code> <code class="o">=</code> <code class="n">tokenizers</code><code class="o">.</code><code class="n">Tokenizer</code><code class="p">(</code><code class="n">bpe_model</code><code class="p">)</code>
<code class="n">bpe_tokenizer</code><code class="o">.</code><code class="n">pre_tokenizer</code> <code class="o">=</code> <code class="n">tokenizers</code><code class="o">.</code><code class="n">pre_tokenizers</code><code class="o">.</code><code class="n">Whitespace</code><code class="p">()</code>
<code class="n">special_tokens</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"&lt;pad&gt;"</code><code class="p">,</code> <code class="s2">"&lt;unk&gt;"</code><code class="p">]</code>
<code class="n">bpe_trainer</code> <code class="o">=</code> <code class="n">tokenizers</code><code class="o">.</code><code class="n">trainers</code><code class="o">.</code><code class="n">BpeTrainer</code><code class="p">(</code><code class="n">vocab_size</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code>
                                             <code class="n">special_tokens</code><code class="o">=</code><code class="n">special_tokens</code><code class="p">)</code>
<code class="n">train_reviews</code> <code class="o">=</code> <code class="p">[</code><code class="n">review</code><code class="p">[</code><code class="s2">"text"</code><code class="p">]</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code> <code class="k">for</code> <code class="n">review</code> <code class="ow">in</code> <code class="n">imdb_train_set</code><code class="p">]</code>
<code class="n">bpe_tokenizer</code><code class="o">.</code><code class="n">train_from_iterator</code><code class="p">(</code><code class="n">train_reviews</code><code class="p">,</code> <code class="n">bpe_trainer</code><code class="p">)</code></pre>

<p>Let’s walk through this code:</p>

<ul>
<li>
<p>We import the Tokenizers library, and we create a BPE model, specifying an unknown token <code translate="no">"&lt;unk&gt;"</code> which will be used later if we try to tokenize some text containing tokens that the model never saw during training: the unknown tokens will be replaced with the <code translate="no">"&lt;unk&gt;"</code> token.</p>
</li>
<li>
<p>We then create a <code translate="no">Tokenizer</code> based on the BPE model.</p>
</li>
<li>
<p>The Tokenizers library lets you specify optional preprocessing and post-processing steps, and it also provides common preprocessors and postprocessors. In this example, we use the <code translate="no">Whitespace</code> preprocessor which splits the text at spaces (and drops the spaces), and also separates groups of letters and groups of nonletters. For example “Hello, world!!!” will be split into ["Hello”, “,”, “world”, “!!!"]. The BPE algorithm will then run on these individual chunks, which dramatically speeds up training and improves token quality (at least when the text is in English) by providing reasonable word boundaries.</p>
</li>
<li>
<p>We then define a list of special tokens: a padding token <code translate="no">"&lt;pad&gt;"</code> that will come in handy when we create batches of texts of different lengths, and the unknown token we have already discussed.</p>
</li>
<li>
<p>We create a <code translate="no">BpeTrainer</code>, specifying the maximum vocabulary size and the list of special tokens. The trainer will add the special tokens at the beginning of the vocabulary, so <code translate="no">"&lt;pad&gt;"</code> will be token 0, and <code translate="no">"&lt;unk&gt;"</code> will be token 1.</p>
</li>
<li>
<p>Next we create a list of all the text in the IMBd training set.</p>
</li>
<li>
<p>Lastly, we train the tokenizer on this list, using the <code translate="no">BpeTrainer</code>. A few seconds later, the BPE tokenizer is ready to be used!</p>
</li>
</ul>

<p class="pagebreak-before">Now let’s use our BPE tokenizer to tokenize some text:</p>
<pre data-type="programlisting" data-code-language="python"><code class="o">&gt;&gt;</code><code class="o">&gt;</code> <code class="n">some_review</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">what an awesome movie! </code><img src="assets/smiling-face-with-smiling-eyes_1f60a_(1).png" width="160" height="160"/>

<code class="o">&gt;&gt;</code><code class="o">&gt;</code> <code class="n">bpe_encoding</code> <code class="o">=</code> <code class="n">bpe_tokenizer</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">some_review</code><code class="p">)</code>
<code class="o">&gt;&gt;</code><code class="o">&gt;</code> <code class="n">bpe_encoding</code>
<code class="n">Encoding</code><code class="p">(</code><code class="n">num_tokens</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> <code class="n">attributes</code><code class="o">=</code><code class="p">[</code><code class="n">ids</code><code class="p">,</code> <code class="n">type_ids</code><code class="p">,</code> <code class="n">tokens</code><code class="p">,</code> <code class="n">offsets</code><code class="p">,</code>
         <code class="n">attention_mask</code><code class="p">,</code> <code class="n">special_tokens_mask</code><code class="p">,</code> <code class="n">overflowing</code><code class="p">]</code><code class="p">)</code>
</pre>

<p>The <code translate="no">encode()</code> method<a data-type="indexterm" data-primary="Tokenizers library" data-secondary="encode()" id="id3242"/> returns an <code translate="no">Encoding</code> object that contains eight tokens. Let’s look at these tokens and their IDs:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bpe_encoding</code><code class="o">.</code><code class="n">tokens</code><code class="w"/>
<code class="go">['what', 'an', 'aw', 'es', 'ome', 'movie', '!', '&lt;unk&gt;']</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bpe_token_ids</code> <code class="o">=</code> <code class="n">bpe_encoding</code><code class="o">.</code><code class="n">ids</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bpe_token_ids</code><code class="w"/>
<code class="go">[303, 139, 373, 149, 240, 211, 4, 1]</code></pre>

<p>Notice that frequent words like “what” and “movie” have been identified by the BPE model and are represented by a single token, while less frequent words like “awesome” are split into multiple tokens. Also note that the smiley was not part of the training data, so it gets replaced with the unknown token <code translate="no">"&lt;unk&gt;"</code>.</p>

<p>The tokenizer provides a <code translate="no">get_vocab()</code> method<a data-type="indexterm" data-primary="Tokenizers library" data-secondary="get_vocab()" id="id3243"/> which returns a dictionary mapping each token to its ID. You can also use<a data-type="indexterm" data-primary="torchvision" data-secondary="token_to_id()" id="id3244"/><a data-type="indexterm" data-primary="Tokenizers library" data-secondary="id_to_token()" id="id3245"/> the <code translate="no">token_to_id()</code> method to map a single token, or conversely use the <code translate="no">id_to_token()</code> method to go from ID to token. However, you will more often use the <code translate="no">decode()</code> method<a data-type="indexterm" data-primary="Tokenizers library" data-secondary="decode()" id="id3246"/> to convert a list of token IDs into a string:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bpe_tokenizer</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="n">bpe_token_ids</code><code class="p">)</code><code class="w"/>
<code class="go">'what an aw es ome movie !'</code></pre>

<p>The tokenizer keeps track of each token’s start and end offset in the original string, which can come in handy, especially for debugging:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bpe_encoding</code><code class="o">.</code><code class="n">offsets</code><code class="w"/>
<code class="go">[(0, 4), (5, 7), (8, 10), (10, 12), (12, 15), (16, 21), (21, 22), (23, 24)]</code></pre>

<p>It’s also possible to encode a whole batch of strings at once. For example, let’s encode the first three reviews of the training set:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bpe_tokenizer</code><code class="o">.</code><code class="n">encode_batch</code><code class="p">(</code><code class="n">train_reviews</code><code class="p">[:</code><code class="mi">3</code><code class="p">])</code><code class="w"/>
<code class="go">[Encoding(num_tokens=281, attributes=[ids, type_ids, tokens, [...]]),</code>
<code class="go"> Encoding(num_tokens=114, attributes=[ids, type_ids, tokens, [...]]),</code>
<code class="go"> Encoding(num_tokens=285, attributes=[ids, type_ids, tokens, [...]]),</code></pre>

<p>If we want to create a single integer tensor containing the token IDs of all three reviews, we must first ensure that they all have the same number of tokens, which is not the case right now. For this, we can ask the tokenizer to pad the shorter reviews with the padding token ID until they are as long as the longest review in the batch. We can also ask the tokenizer to truncate any sequence longer than some maximum length, since RNNs don’t handle very long sequences very well anyway:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">bpe_tokenizer</code><code class="o">.</code><code class="n">enable_padding</code><code class="p">(</code><code class="n">pad_id</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">pad_token</code><code class="o">=</code><code class="s2">"&lt;pad&gt;"</code><code class="p">)</code>
<code class="n">bpe_tokenizer</code><code class="o">.</code><code class="n">enable_truncation</code><code class="p">(</code><code class="n">max_length</code><code class="o">=</code><code class="mi">500</code><code class="p">)</code></pre>

<p>Now let’s encode the batch again. This time all sequences will have the same number of tokens, so we can create a tensor containing all the token IDs:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bpe_encodings</code> <code class="o">=</code> <code class="n">bpe_tokenizer</code><code class="o">.</code><code class="n">encode_batch</code><code class="p">(</code><code class="n">train_reviews</code><code class="p">[:</code><code class="mi">3</code><code class="p">])</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bpe_batch_ids</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="n">encoding</code><code class="o">.</code><code class="n">ids</code> <code class="k">for</code> <code class="n">encoding</code> <code class="ow">in</code> <code class="n">bpe_encodings</code><code class="p">])</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bpe_batch_ids</code><code class="w"/>
<code class="go">tensor([[159, 402, 176, 246,  61, [...], 215, 156, 586,   0,   0,   0,   0],</code>
<code class="go">        [ 10, 138, 198, 289, 175, [...],   0,   0,   0,   0,   0,   0,   0],</code>
<code class="go">        [289,  15, 209, 398, 177, [...],  50,  29,  22,  17,  24,  18,  24]])</code></pre>

<p>Notice how the first and second review were padded with 0s, which is our padding token ID. Each <code translate="no">Encoding</code> object also includes<a data-type="indexterm" data-primary="attention_mask attribute" id="id3247"/> an <code translate="no">attention_mask</code> attribute containing a 1 for each nonpadding token, and a 0 for each padding token. This can be used in your models to easily ignore the padded time steps: just multiply a tensor with the attention mask. In some cases you will prefer to have the list of sequence lengths<a data-type="indexterm" data-primary="sequence length" id="id3248"/> (ignoring padding). Here’s how to get both the attention mask tensor<a data-type="indexterm" data-primary="mask tensor" id="id3249"/> and the sequence lengths:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">attention_mask</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="n">encoding</code><code class="o">.</code><code class="n">attention_mask</code><code class="w"/>
<code class="gp">... </code>                               <code class="k">for</code> <code class="n">encoding</code> <code class="ow">in</code> <code class="n">bpe_encodings</code><code class="p">])</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">attention_mask</code><code class="w"/>
<code class="go">tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 1, 1, 1, 0, 0, 0, 0],</code>
<code class="go">        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 0, 0, 0, 0, 0, 0, 0],</code>
<code class="go">        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 1, 1, 1, 1, 1, 1, 1]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lengths</code> <code class="o">=</code> <code class="n">attention_mask</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lengths</code><code class="w"/>
<code class="go">tensor([281, 114, 285])</code></pre>

<p>You may have noted that spaces were not handled very well by our tokenizer<a data-type="indexterm" data-primary="spaces, tokenizer handling of" id="id3250"/>. In particular, the word “awesome” came back as “aw es ome”, and “movie!” came back as “movie !”. This is because the <code translate="no">Whitespace</code> pre-tokenizer dropped all spaces, therefore the BPE tokenizer doesn’t know where spaces should go and it just adds spaces between all tokens. To fix this, we can replace the <code translate="no">Whitespace</code> pre-tokenizer with the <code translate="no">ByteLevel</code> pre-tokenizer: it replaces all spaces with a special character Ġ so the BPE model doesn’t lose track of them. For example, if you use this pre-tokenizer and you encode and decode the text “what an awesome movie! <img src="assets/smiling-face-with-smiling-eyes_1f60a_(1).png" width="160" height="160"/>“, you will get: “Ġwhat Ġan Ġaw es ome Ġmovie !”. After removing the spaces, then replacing every Ġ with a space, you get " what an awesome movie!”. This is almost perfect, except for the extra space at the start—which is easily removed—and the lost emoji, which was replaced with an unknown token because it’s not in the vocabulary, and dropped by the <code translate="no">decode()</code> method.</p>

<p>As its name suggests, the <code translate="no">ByteLevel</code> pre-tokenizer allows the BPE model to work at the byte level, rather than the character level: unsurprisingly, this is called Byte-level BPE (BBPE)<a data-type="indexterm" data-primary="Byte-level BPE (BBPE)" id="id3251"/><a data-type="indexterm" data-primary="BBPE (Byte-level BPE)" id="id3252"/>. For example, the <img src="assets/smiling-face-with-smiling-eyes_1f60a_(1).png" width="160" height="160"/> emoji will be converted to four bytes, using Unicode’s UTF-8 encoding. This means that BBPE will never output an unknown token if its vocabulary contains all 256 possible bytes, since any text can be broken down into its individual bytes whenever longer tokens are not found in the vocabulary. This makes BBPE well suited when the corpus contains rare characters such as emojis<a data-type="indexterm" data-startref="xi_bytepairencodingBPE14368795_1" id="id3253"/><a data-type="indexterm" data-startref="xi_BPEbytepairencoding14368795_1" id="id3254"/>.</p>

<p>Another important variant of BPE<a data-type="indexterm" data-primary="WordPiece, BPE variant" id="id3255"/> is <a href="https://homl.info/wordpiece">WordPiece</a>,⁠<sup><a data-type="noteref" id="id3256-marker" href="ch14.html#id3256">9</a></sup> proposed by Google in 2016. This tokenization algorithm is very similar to BPE, but instead of adding the most frequent adjacent pair of tokens to the vocabulary at each iteration, it adds the pair with the highest score. This score is computed using <a data-type="xref" href="#wordpiece_equation">Equation 14-1</a>: the frequency(AB) term is just like in BPE—it boosts pairs that are frequent in the corpus. However, the denominator reduces the score of a pair when the individual tokens are themselves frequent. This normalization tends to favor more useful and meaningful tokens than BPE, and the algorithm often produces shorter encoded sequences than BPE or BBPE.</p>
<div id="wordpiece_equation" data-type="equation">
<h5><span class="label">Equation 14-1. </span>WordPiece score for a pair AB composed of tokens A and B</h5>
<math alttext="score left-parenthesis AB right-parenthesis equals StartFraction frequency left-parenthesis AB right-parenthesis Over freq left-parenthesis upper A right-parenthesis dot freq left-parenthesis upper B right-parenthesis EndFraction dot len left-parenthesis vocab right-parenthesis" display="block">
  <mrow>
    <mtext>score</mtext>
    <mrow>
      <mo>(</mo>
      <mtext>AB</mtext>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mrow><mtext>frequency</mtext><mo>(</mo><mtext>AB</mtext><mo>)</mo></mrow> <mrow><mtext>freq</mtext><mo>(</mo><mtext>A</mtext><mo>)</mo><mo>·</mo><mtext>freq</mtext><mo>(</mo><mtext>B</mtext><mo>)</mo></mrow></mfrac>
    </mstyle>
    <mo>·</mo>
    <mtext>len(vocab)</mtext>
  </mrow>
</math>
</div>

<p>To train a WordPiece tokenizer using the Tokenizers library, you can use the same code as for BPE, but replace the <code translate="no">BPE</code> model with <code translate="no">WordPiece</code>, and the <code translate="no">BpeTrainer</code> with <code translate="no">WordPieceTrainer</code>. If you encode and decode the same review as earlier, you will get “what an aw 
esome movie !”. Notice that WordPiece adds a prefix to tokens that are inside a word, which makes it easy to reconstruct the original string: just remove 
<span class="keep-together">" #</span>“# (as well as spaces before punctuations). Note that the smiley emoji once again disappeared because it was not in the vocabulary.</p>

<p>One last popular tokenization algorithm we will discuss is Unigram LM<a data-type="indexterm" data-primary="Unigram LM" id="id3257"/> (Language Model), introduced in a <a href="https://homl.info/subword">2018 paper</a>⁠<sup><a data-type="noteref" id="id3258-marker" href="ch14.html#id3258">10</a></sup> by Taku Kudo at Google. This technique is a bit different than the previous ones: it starts out with a very large vocabulary containing every frequent word, subword, and character in the training corpus, then it gradually removes the least useful tokens until it reaches the desired vocabulary size. To determine how useful a token is, this method makes one big simplifying assumption: it assumes that the corpus was sampled randomly from the vocabulary, one token at a time (hence the name Unigram LM), and that every token was sampled independently from the others. Therefore, this tokenizer model assumes that the probability of sampling the pair AB is equal to the probability of sampling A times the probability of sampling B. Given this assumption, it can estimate the probability of sampling the whole training corpus. At each iteration, the training algorithm attempts to remove tokens without reducing this overall probability too much.</p>

<p>For example, suppose that the vocabulary contains the tokens “them”, “the”, and “m”, respectively, with 1%, 5%, and 2% probability. This means that the word “them” has a 1% chance of being sampled as the single token “them”, or a 5% × 2% = 0.1% chance of being sampled as the pair “the” + “m”. Overall, the word “them” has a 1% + 0.1% = 1.1% chance of being sampled. If we remove the token “them” from the vocabulary, then the probability of sampling the word “them” drops down to 0.1%. If instead we remove either “m” or “the”, then the probability only drops down to 1% since we can still sample the single token “them”. So if the training corpus only contained the word “them”, then the algorithm would prefer to drop either “the” or “m”. Of course, in reality the corpus contains many other words that contain these two tokens, so the algorithm will likely find other less useful tokens to drop.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Unigram LM is great for languages that don’t use spaces to separate words, like English does. For example, Chinese text does not use spaces between words, Vietnamese uses spaces even within words, and German often attaches multiple words together, without spaces.</p>
</div>

<p>The same paper also proposed a novel regularization technique<a data-type="indexterm" data-primary="subword regularization" id="id3259"/><a data-type="indexterm" data-primary="regularization" data-secondary="subword" id="id3260"/> called <em>subword regularization</em>, which improves generalization and robustness by introducing some randomness in tokenization while training the NLP model (not the tokenizer model). For example, assuming the vocabulary contains the tokens “them”, “the”, and “m”, and you choose to use subword regularization, then the word “them” will sometimes be tokenized as “the” + “m”, and sometimes as “them”. This technique works best<a data-type="indexterm" data-primary="morphologically rich languages" id="id3261"/> with <em>morphologically rich languages</em>, meaning languages where words carry a lot of grammatical information through affixes, inflections, and internal modifications (such as Arabic, Finnish, German, Hungarian, Polish, or Turkish), as opposed to languages that rely on word order or additional helper words (such as English, Chinese, Thai, or Vietnamese).</p>

<p>Unfortunately, the Tokenizers library does not natively support subword regularization, so you either have to implement it yourself, or you can<a data-type="indexterm" data-primary="SentencePiece library" id="id3262"/> use Google’s <a href="https://github.com/google/sentencepiece"><em>SentencePiece</em></a> library (<code translate="no">pip install sentencepiece</code>) which provides an open source implementation. This project is described in a <a href="https://homl.info/sentencepiece">2018 paper</a>⁠<sup><a data-type="noteref" id="id3263-marker" href="ch14.html#id3263">11</a></sup> by Taku Kudo and John Richardson.</p>

<p><a data-type="xref" href="#tokenizer_summary_table">Table 14-1</a> summarizes the three main tokenizers used today.</p>
<table id="tokenizer_summary_table">
<caption><span class="label">Table 14-1. </span>Overview of the three main tokenizers</caption>
<thead>
<tr>
<th>Feature</th>
<th>BBPE</th>
<th>WordPiece</th>
<th>Unigram LM</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><strong>How</strong></p></td>
<td><p>Merge most frequent pairs</p></td>
<td><p>Merge pairs that maximize data likelihood</p></td>
<td><p>Remove least likely tokens</p></td>
</tr>
<tr>
<td><p><strong>Pros</strong></p></td>
<td><p>Fast, simple, great for multilingual</p></td>
<td><p>Good balance of efficiency and token quality</p></td>
<td><p>Most meaningful, shortest sequences</p></td>
</tr>
<tr>
<td><p><strong>Cons</strong></p></td>
<td><p>Can produce awkward splits</p></td>
<td><p>Less robust than BBPE for multilingual</p></td>
<td><p>Slower to train and tokenize</p></td>
</tr>
<tr>
<td><p><strong>Used By</strong></p></td>
<td><p>GPT, Llama, RoBERTa, BLOOM</p></td>
<td><p>BERT, DistilBERT, ELECTRA</p></td>
<td><p>T5, ALBERT, mBART</p></td>
</tr>
</tbody>
</table>

<p>Training<a data-type="indexterm" data-primary="BBPE (Byte-level BPE)" id="id3264"/><a data-type="indexterm" data-primary="Byte-level BPE (BBPE)" id="id3265"/><a data-type="indexterm" data-primary="WordPiece, BPE variant" id="id3266"/><a data-type="indexterm" data-primary="Unigram LM" id="id3267"/> your own tokenizer is useful in many situations; for example, if you are dealing with domain-specific text, such as medical, legal, or engineering documents full of jargon, or if the text is written in a low-resource language or dialect, or if it’s code written in a new programming language, and so on. However, in most cases you will want to simply reuse a pretrained tokenizer.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Reusing Pretrained Tokenizers"><div class="sect2" id="id269">
<h2>Reusing Pretrained Tokenizers</h2>

<p>To download a pretrained tokenizer<a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="reusing tokenizers" id="xi_pretrainingandpretrainedlayersreusingtokenizers1454335_1"/>, we will use the <a href="https://homl.info/transformerslib">Hugging Face <em>Transformers library</em></a>. This library<a data-type="indexterm" data-primary="Transformers library" data-secondary="reusing pre-trained tokenizers" id="id3268"/> provides many popular models for NLP, computer vision, audio processing, and more. Pretrained weights are available for almost all of these models, and the library can automatically download them from the Hugging Face Hub. The models were originally all based on the <em>Transformer architecture</em> (which we will discuss in detail in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>), hence the name of the library, but other kinds of models are now available as well, such as CNNs. Lastly, each model comes with all the tools it needs, including tokenizers for NLP models: in a single line of code, you can have a fully functional, high-performance model for a given task, as we will see later in this chapter.</p>

<p>For now, let’s just grab the pretrained tokenizer from some NLP model. For example, the following code downloads the pretrained BBPE tokenizer used by the GPT-2 model (a text generation model), and it uses this tokenizer to encode the first 3 IMDb reviews, truncating the encoded sequences if they exceed 500 tokens:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">transformers</code>

<code class="n">gpt2_tokenizer</code> <code class="o">=</code> <code class="n">transformers</code><code class="o">.</code><code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"gpt2"</code><code class="p">)</code>
<code class="n">gpt2_encoding</code> <code class="o">=</code> <code class="n">gpt2_tokenizer</code><code class="p">(</code><code class="n">train_reviews</code><code class="p">[:</code><code class="mi">3</code><code class="p">],</code> <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                               <code class="n">max_length</code><code class="o">=</code><code class="mi">500</code><code class="p">)</code></pre>

<p>Notice that we use the tokenizer object like a function. The result is a dictionary-like object of type <code translate="no">BatchEncoding</code>. You can get the token IDs using the <code translate="no">"input_ids"</code> key. It returns a Python list of lists of token IDs. For example, let’s look at the first 10 token IDs of the first encoded review, and use the tokenizer to decode them, using its <code translate="no">decode()</code> method:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gpt2_token_ids</code> <code class="o">=</code> <code class="n">gpt2_encoding</code><code class="p">[</code><code class="s2">"input_ids"</code><code class="p">][</code><code class="mi">0</code><code class="p">][:</code><code class="mi">10</code><code class="p">]</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gpt2_token_ids</code><code class="w"/>
<code class="go">[14247, 35030, 1690, 423, 257, 1688, 8046, 13, 484, 1690]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gpt2_tokenizer</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="n">gpt2_token_ids</code><code class="p">)</code><code class="w"/>
<code class="go">'stage adaptations often have a major fault. they often'</code></pre>

<p>If you would prefer to use a pretrained WordPiece<a data-type="indexterm" data-primary="WordPiece, BPE variant" id="id3269"/> tokenizer, you can reuse the tokenizer of any LLM that was pretrained using WordPiece, such as BERT<a data-type="indexterm" data-primary="BERT (Bidirectional Encoder Representations from Transformers)" data-secondary="and WordPiece tokenizer" data-secondary-sortas="WordPiece tokenizer" id="xi_BERTBidirectionalEncoderRepresentationsfromTransformersandWordPiecetokenizer14567150_1"/> (another popular NLP model, which stands for Bidirectional Encoder Representations from Transformers). This tokenizer has a padding token (unlike the previous tokenizer, since GPT-2 didn’t need it), so we can specify <code translate="no">padding=True</code> when encoding a batch of reviews: as usual, the shortest texts will be padded to the length of the longest one using the padding token. This allows us to also specify <code translate="no">return_tensors="pt"</code> to get a PyTorch tensor instead of a Python list of lists of token IDs: very convenient! So let’s encode the first three IMDb reviews:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">bert_tokenizer</code> <code class="o">=</code> <code class="n">transformers</code><code class="o">.</code><code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"bert-base-uncased"</code><code class="p">)</code>
<code class="n">bert_encoding</code> <code class="o">=</code> <code class="n">bert_tokenizer</code><code class="p">(</code><code class="n">train_reviews</code><code class="p">[:</code><code class="mi">3</code><code class="p">],</code> <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                               <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">max_length</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>
                               <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>The name <code translate="no">"bert-base-uncased"</code> refers<a data-type="indexterm" data-primary="model checkpoint" id="id3270"/> to a <em>model checkpoint</em>: this particular checkpoint is a case-insensitive BERT model, pretrained on English text. Other checkpoints are available, such as <code translate="no">"bert-large-cased"</code> if you want a larger and case-sensitive BERT model, or <code translate="no">"bert-base-multilingual-uncased"</code> if you want an uncased model pretrained on over 100 languages. For now we are just using the model’s tokenizer.</p>
</div>

<p>The resulting token IDs and attention masks<a data-type="indexterm" data-primary="attention_mask attribute" id="id3271"/> are nicely padded tensors:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bert_encoding</code><code class="p">[</code><code class="s2">"input_ids"</code><code class="p">]</code><code class="w"/>
<code class="go">tensor([[ 101, 2754,17241, 2411, 2031, [...],  102, 0, 0, 0, [...], 0, 0, 0],</code>
<code class="go">        [ 101, 1005, 1996, 8305, 1005, [...],  102, 0, 0, 0, [...], 0, 0, 0],</code>
<code class="go">        [ 101, 7929, 1010, 2021, 2515, [...], 1012,  1019,  1013,  1019,  102]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bert_encoding</code><code class="p">[</code><code class="s2">"attention_mask"</code><code class="p">]</code><code class="w"/>
<code class="go">tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 0, 0, 0, 0, 0, 0, 0, 0, 0],</code>
<code class="go">        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 0, 0, 0, 0, 0, 0, 0, 0, 0],</code>
<code class="go">        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 1, 1, 1, 1, 1, 1, 1, 1, 1]])</code></pre>

<p>Notice that each token ID sequence starts with token 101 ([CLS]), and ends with token 102 ([SEP]) (ignoring padding tokens). These tokens are needed by the BERT model (as we will see in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>), but unless your model needs them too, you can drop them by setting <code translate="no">add_special_tokens=False</code> when calling the tokenizer.</p>

<p>What about a pretrained Unigram LM<a data-type="indexterm" data-primary="Unigram LM" id="id3272"/> tokenizer? Well, many models were trained using Unigram LM, such as ALBERT, T5, or XML-R models, just to name a few. For example:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">albert_tokenizer</code> <code class="o">=</code> <code class="n">transformers</code><code class="o">.</code><code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"albert-base-v2"</code><code class="p">)</code>
<code class="n">albert_encoding</code> <code class="o">=</code> <code class="n">albert_tokenizer</code><code class="p">(</code><code class="n">train_reviews</code><code class="p">[:</code><code class="mi">3</code><code class="p">],</code> <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="p">[</code><code class="o">...</code><code class="p">])</code></pre>

<p>The Transformers library<a data-type="indexterm" data-primary="Transformers library" data-secondary="reusing pre-trained tokenizers" id="id3273"/> also provides an object that can wrap your own tokenizer (from the Tokenizers library) and give it the same API as the pretrained tokenizers (from the Transformers library)<a data-type="indexterm" data-startref="xi_Tokenizerslibrary1437075_1" id="id3274"/><a data-type="indexterm" data-startref="xi_sentimentanalysistokenization1437075_1" id="id3275"/><a data-type="indexterm" data-startref="xi_pretrainingandpretrainedlayersreusingtokenizers1454335_1" id="id3276"/><a data-type="indexterm" data-startref="xi_BERTBidirectionalEncoderRepresentationsfromTransformersandWordPiecetokenizer14567150_1" id="id3277"/>. For example, let’s wrap the BPE tokenizer we trained earlier:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">hf_tokenizer</code> <code class="o">=</code> <code class="n">transformers</code><code class="o">.</code><code class="n">PreTrainedTokenizerFast</code><code class="p">(</code>
    <code class="n">tokenizer_object</code><code class="o">=</code><code class="n">bpe_tokenizer</code><code class="p">)</code>
<code class="n">hf_encodings</code> <code class="o">=</code> <code class="n">hf_tokenizer</code><code class="p">(</code><code class="n">train_reviews</code><code class="p">[:</code><code class="mi">3</code><code class="p">],</code> <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="p">[</code><code class="o">...</code><code class="p">])</code></pre>

<p>With that, we have all the tokenization tools we need, so let’s go back to sentiment analysis.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Building and Training a Sentiment Analysis Model"><div class="sect2" id="id270">
<h2>Building and Training a Sentiment Analysis Model</h2>

<p>Our sentiment analysis model<a data-type="indexterm" data-primary="sentiment analysis" data-secondary="building and training the model" id="xi_sentimentanalysisbuildingandtrainingthemodel1461529_1"/> must be trained using batches of tokenized reviews. However, the datasets we created did not take care of tokenization. One option would be to update them (e.g., using the <code translate="no">map()</code> method), but it’s just as simple to handle tokenization in the data loaders<a data-type="indexterm" data-primary="data loaders, tokenization in sentiment analysis" id="xi_dataloaderstokenizationinsentimentanalysis14615285_1"/>. To do this, we can pass a function to the <code translate="no">DataLoader</code> constructor using its <code translate="no">collate_fn</code> argument: the data loader will call this function for every batch, passing it a list of dataset samples. Our function will take this batch, tokenize the reviews, truncate and pad them if needed, and return a <code translate="no">BatchEncoding</code> object containing PyTorch tensors for the token IDs and attention masks, along with another tensor containing the labels. For tokenization, we will simply use the pretrained WordPiece tokenizer we just loaded:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">collate_fn</code><code class="p">(</code><code class="n">batch</code><code class="p">,</code> <code class="n">tokenizer</code><code class="o">=</code><code class="n">bert_tokenizer</code><code class="p">):</code>
    <code class="n">reviews</code> <code class="o">=</code> <code class="p">[</code><code class="n">review</code><code class="p">[</code><code class="s2">"text"</code><code class="p">]</code> <code class="k">for</code> <code class="n">review</code> <code class="ow">in</code> <code class="n">batch</code><code class="p">]</code>
    <code class="n">labels</code> <code class="o">=</code> <code class="p">[[</code><code class="n">review</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]]</code> <code class="k">for</code> <code class="n">review</code> <code class="ow">in</code> <code class="n">batch</code><code class="p">]</code>
    <code class="n">encodings</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="n">reviews</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                          <code class="n">max_length</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code>
    <code class="n">labels</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">labels</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">encodings</code><code class="p">,</code> <code class="n">labels</code>

<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">256</code>
<code class="n">imdb_train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">imdb_train_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code>
                               <code class="n">collate_fn</code><code class="o">=</code><code class="n">collate_fn</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">imdb_valid_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">imdb_valid_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code>
                               <code class="n">collate_fn</code><code class="o">=</code><code class="n">collate_fn</code><code class="p">)</code>
<code class="n">imdb_test_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">imdb_test_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code>
                              <code class="n">collate_fn</code><code class="o">=</code><code class="n">collate_fn</code><code class="p">)</code></pre>

<p>Now we’re ready to create our sentiment analysis model:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">SentimentAnalysisModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">,</code> <code class="n">n_layers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">embed_dim</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code>
                 <code class="n">pad_id</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">embed</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">,</code>
                                  <code class="n">padding_idx</code><code class="o">=</code><code class="n">pad_id</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">gru</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">,</code> <code class="n">num_layers</code><code class="o">=</code><code class="n">n_layers</code><code class="p">,</code>
                          <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="n">dropout</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">encodings</code><code class="p">):</code>
        <code class="n">embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">embed</code><code class="p">(</code><code class="n">encodings</code><code class="p">[</code><code class="s2">"input_ids"</code><code class="p">])</code>
        <code class="n">_outputs</code><code class="p">,</code> <code class="n">hidden_states</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">gru</code><code class="p">(</code><code class="n">embeddings</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">hidden_states</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code></pre>

<p>As you can see, this model is very similar to our Shakespeare model, but with a few important differences:</p>

<ul>
<li>
<p>When creating the <code translate="no">nn.Embedding</code> layer, we set its <code translate="no">padding_idx</code> argument to our padding ID. This ensures that the padding ID gets embedded as a nontrainable zero vector to reduce the impact of padding tokens on the loss.</p>
</li>
<li>
<p>Since this is a sequence-to-vector model, not a sequence-to-sequence model, we only need the last output of the top GRU layer to make our final prediction (through the output <code translate="no">nn.Linear</code> layer). We could have used <code translate="no">outputs[:, -1]</code> instead of <code translate="no">hidden_states[-1]</code>, as they are equal.</p>
</li>
<li>
<p>The output <code translate="no">nn.Linear</code> layer has a single output dimension because it’s a binary classification model. The final output will be a 2D tensor with a single column containing one logit per review, positive for positive reviews, and negative for negative reviews.</p>
</li>
<li>
<p>The <code translate="no">forward()</code> method takes a <code translate="no">BatchEncoding</code> object as input, containing the token IDs (possibly padded and truncated).</p>
</li>
</ul>

<p>We can then train this model using<a data-type="indexterm" data-primary="torch" data-secondary="nn.BCEWithLogitsLoss" id="id3278"/> the <code translate="no">nn.BCEWithLogitsLoss</code> since this is a binary classification task. It reaches close to 85% accuracy on the validation set, which is reasonably good, although the best models reach human level, slightly above 90% accuracy. It’s probably not possible to go much higher than that; because many reviews are ambiguous, classifying them feels like flipping a coin<a data-type="indexterm" data-startref="xi_dataloaderstokenizationinsentimentanalysis14615285_1" id="id3279"/>.</p>

<p>One problem with our model is the fact that we are not fully ignoring the padding tokens. Indeed, if a review ends with many padding tokens, the <code translate="no">nn.GRU</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.GRU" id="id3280"/> will have to process them, and by the time it gets through all of them, it might have forgotten what the review was all about. To avoid this, we can use<a data-type="indexterm" data-primary="packed sequence" id="xi_packedsequence14665314_1"/> a <em>packed sequence</em> instead of a regular tensor. A packed sequence is a special data structure designed to efficiently represent a batch of sequences of variable lengths.⁠<sup><a data-type="noteref" id="id3281-marker" href="ch14.html#id3281">12</a></sup> You can use<a data-type="indexterm" data-primary="torch" data-secondary="pack_padded_sequence()" id="xi_torchpack_padded_sequence14665714_1"/> the <code translate="no">pack_padded_sequence()</code> function to convert a tensor containing padded sequences to a packed sequence object, and conversely you can<a data-type="indexterm" data-primary="torch" data-secondary="pad_packed_sequence()" id="xi_torchpad_packed_sequence14665852_1"/> use the <code translate="no">pad_packed_sequence()</code> function whenever you want to convert a packed sequence object to a padded tensor:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">torch.nn.utils.rnn</code> <code class="kn">import</code> <code class="n">pack_padded_sequence</code><code class="p">,</code> <code class="n">pad_packed_sequence</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">sequences</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">8</code><code class="p">]])</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">packed</code> <code class="o">=</code> <code class="n">pack_padded_sequence</code><code class="p">(</code><code class="n">sequences</code><code class="p">,</code> <code class="n">lengths</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">4</code><code class="p">),</code><code class="w"/>
<code class="gp">... </code>                              <code class="n">enforce_sorted</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">packed</code><code class="w"/>
<code class="go">PackedSequence(data=tensor([5, 1, 6, 2, 7, 8]), [...])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">padded</code><code class="p">,</code> <code class="n">lengths</code> <code class="o">=</code> <code class="n">pad_packed_sequence</code><code class="p">(</code><code class="n">packed</code><code class="p">,</code> <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">padded</code><code class="p">,</code> <code class="n">lengths</code><code class="w"/>
<code class="go">(tensor([[1, 2, 0, 0],</code>
<code class="go">         [5, 6, 7, 8]]),</code>
<code class="go"> tensor([2, 4]))</code></pre>

<p>By default, the <code translate="no">pack_padded_sequence()</code> function assumes that the sequences in the batch are ordered from the longest to the shortest. If this is not the case, you must set <code translate="no">enforce_sorted=False</code>. Moreover, the function also assumes that the time dimension comes before the batch dimension. If the batch dimension is first, you must set <code translate="no">batch_first=True</code>.</p>

<p>PyTorch’s recurrent layers support packed sequences: they efficiently process the sequences, stopping at the end of each sequence. So let’s update our sentiment analysis model to use packed sequences. In the <code translate="no">forward()</code> method, just replace the <code translate="no">self.gru(embeddings)</code> line with the following code:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">lengths</code> <code class="o">=</code> <code class="n">encodings</code><code class="p">[</code><code class="s2">"attention_mask"</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">packed</code> <code class="o">=</code> <code class="n">pack_padded_sequence</code><code class="p">(</code><code class="n">embeddings</code><code class="p">,</code> <code class="n">lengths</code><code class="o">=</code><code class="n">lengths</code><code class="o">.</code><code class="n">cpu</code><code class="p">(),</code>
                              <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">enforce_sorted</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
<code class="n">_outputs</code><code class="p">,</code> <code class="n">hidden_states</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">gru</code><code class="p">(</code><code class="n">packed</code><code class="p">)</code></pre>

<p>This code starts by computing the length of each sequence in the batch, just like we did earlier, then it packs the embeddings tensor and passes the packed sequence to the <code translate="no">nn.GRU</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.GRU" id="id3282"/>. With that, the model will properly handle sequences without being bothered by any padding tokens. You don’t actually need to set <code translate="no">padding_idx</code> anymore when creating the <code translate="no">nn.Embedding</code> layer, but it doesn’t hurt, and it makes debugging a bit easier, so I prefer to keep it.</p>

<p>Another way to improve our model is to let it look at the review in both directions: left to right, and right to left. Let’s see how this works.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you pass a packed sequence to an <code translate="no">nn.GRU</code> module, its outputs will also be a packed sequence, and you will need to convert it back to a padded tensor before you can pass it to the next layers. Luckily, we don’t need these outputs for our sentiment analysis model, only the hidden states<a data-type="indexterm" data-startref="xi_sentimentanalysisbuildingandtrainingthemodel1461529_1" id="id3283"/><a data-type="indexterm" data-startref="xi_packedsequence14665314_1" id="id3284"/><a data-type="indexterm" data-startref="xi_torchpad_packed_sequence14665852_1" id="id3285"/><a data-type="indexterm" data-startref="xi_torchpack_padded_sequence14665714_1" id="id3286"/>.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Bidirectional RNNs"><div class="sect2" id="id271">
<h2>Bidirectional RNNs</h2>

<p>At each time step, a regular recurrent layer<a data-type="indexterm" data-primary="bidirectional RNNs" id="xi_bidirectionalRNNs1470245_1"/><a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="bidirectional" id="xi_recurrentneuralnetworksRNNsbidirectional1470245_1"/><a data-type="indexterm" data-primary="sentiment analysis" data-secondary="bidirectional RNNs" id="xi_sentimentanalysisbidirectionalRNNs1470245_1"/> only looks at past and present inputs before generating its output. In other words, it is <em>causal</em>, meaning it cannot look into the future. This type of RNN makes sense when forecasting time series, or in the decoder of a sequence-to-sequence (seq2seq) model. But for tasks like text classification, or in the encoder of a seq2seq model, it is often preferable to look ahead at the next words before encoding a given word.</p>

<p>For example, consider the phrases “the right arm”, “the right person”, and “the right to speak”: to properly encode the word “right”, you need to look ahead. One solution is to run two recurrent layers on the same inputs, one reading the words from left to right and the other reading them from right to left, then combine their outputs at each time step, typically by concatenating them. This is what a <em>bidirectional recurrent layer</em> does (see <a data-type="xref" href="#bidirectional_rnn_diagram">Figure 14-4</a>).</p>

<figure class="width-50"><div id="bidirectional_rnn_diagram" class="figure">
<img src="assets/hmls_1404.png" alt="Diagram illustrating a bidirectional recurrent layer, showing inputs processed in both directions and outputs combined." width="637" height="457"/>
<h6><span class="label">Figure 14-4. </span>A bidirectional recurrent layer</h6>
</div></figure>

<p>To make our sentiment analysis model bidirectional, we can just set <code translate="no">bidirectional=True</code> when creating the <code translate="no">nn.GRU</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.GRU" id="id3287"/> (this also works with the <code translate="no">nn.RNN</code> and <code translate="no">nn.LSTM</code> modules).</p>

<p>However, once we do that, we must adjust our model a bit. In particular, we must double the input dimension of the output <code translate="no">nn.Linear</code> layer, since the hidden states will double in size:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">2</code> <code class="o">*</code> <code class="n">hidden_dim</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code></pre>

<p>We must also concatenate the forward and backward hidden states of the GRU’s top layer before passing the result to the output layer. For this, we can replace the last line of the <code translate="no">forward()</code> method (i.e., <code translate="no">return self.output(hidden_states[-1])</code>) with the following code:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">n_dims</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="o">.</code><code class="n">in_features</code>
<code class="n">top_states</code> <code class="o">=</code> <code class="n">hidden_states</code><code class="p">[</code><code class="o">-</code><code class="mi">2</code><code class="p">:]</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">n_dims</code><code class="p">)</code>
<code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">top_states</code><code class="p">)</code></pre>

<p>Let’s see how the middle line works:</p>

<ul>
<li>
<p>Until now, the shape of the hidden states returned by the <code translate="no">nn.GRU</code> module was [<em>number of layers</em>, <em>batch size</em>, <em>hidden size</em>], so [2, 256, 64] in our case. But when we set <code translate="no">bidirectional=True</code>, we doubled the first dimension size, so we now have a shape of [4, 256, 64]: the tensor contains the hidden states for layer 1 forward, layer 1 backward, layer 2 forward, and layer 2 backward. Since we only want the top layer’s hidden states, both forward and backward, we must get <code translate="no">hidden_states[-2:]</code>.</p>
</li>
<li>
<p>We also need to concatenate the forward and backward states. One way to do this is to permute the first two dimensions of the top hidden states using <code translate="no">permute(1, 0, 2)</code> to get the shape [256, 2, 64], then reshape the result using <code translate="no">reshape(-1, n_dims)</code> (where <code translate="no">n_dims</code> equals 128) to get the desired shape: [256, 2 * 64].</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In this model we only use the last hidden states, ignoring the outputs at each time step. If you ever want to use the outputs<a data-type="indexterm" data-primary="output layer, neural network" id="id3288"/> of a bidirectional module, be aware that its last dimension’s size will be doubled.</p>
</div>

<p>You can try training this model, but you will not see any improvement in this case, because the first model actually overfit the training set, and this new version makes it even worse: it reaches over 99% accuracy on the training set, but just 84% on the validation set. To fix this, you could try to regularize the model a bit more, reduce the size of the model, or increase the size of the training set<a data-type="indexterm" data-startref="xi_bidirectionalRNNs1470245_1" id="id3289"/><a data-type="indexterm" data-startref="xi_recurrentneuralnetworksRNNsbidirectional1470245_1" id="id3290"/><a data-type="indexterm" data-startref="xi_sentimentanalysisbidirectionalRNNs1470245_1" id="id3291"/>.</p>

<p>But let’s instead try something different: using pretrained embeddings.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Reusing Pretrained Embeddings and Language Models"><div class="sect2" id="id272">
<h2>Reusing Pretrained Embeddings and Language Models</h2>

<p>Our model was able to learn useful embeddings<a data-type="indexterm" data-primary="embeddings" data-secondary="reusing in sentiment analysis" id="xi_embeddingsreusinginsentimentanalysis1474146_1"/><a data-type="indexterm" data-primary="sentiment analysis" data-secondary="reusing pretrained embeddings in" id="xi_sentimentanalysisreusingpretrainedembeddingsin1474146_1"/><a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="reusing embeddings in sentiment analysis" id="xi_pretrainingandpretrainedlayersreusingembeddingsinsentimentanalysis1474146_1"/> for thousands of tokens, based on just 25,000 movie reviews: that’s quite impressive! Imagine how good the embeddings would be if we had billions of reviews to train on. The good news is that we can reuse word embeddings even when they were trained on some other (very) large text corpus, even if it was not composed of movie reviews, and even if they were not trained for sentiment analysis. After all, the word “amazing” generally has the same meaning whether you use it to talk about movies or anything else.</p>

<p>Since we used pretrained tokens for the BERT model<a data-type="indexterm" data-primary="BERT (Bidirectional Encoder Representations from Transformers)" data-secondary="embedding layer" id="xi_BERTBidirectionalEncoderRepresentationsfromTransformersembeddinglayer1474351_1"/><a data-type="indexterm" data-primary="embeddings" data-secondary="BERT" id="xi_embeddingsBERT1474351_1"/>, we might as well try using its embedding layer. First, we need to download the pretrained model using the <code translate="no">AutoModel.from_pretrained()</code> function<a data-type="indexterm" data-primary="Transformers library" data-secondary="AutoModel.from_pretrained()" id="id3292"/> from the Transformers library, then we can directly access its embeddings layer:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bert_model</code> <code class="o">=</code> <code class="n">transformers</code><code class="o">.</code><code class="n">AutoModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"bert-base-uncased"</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bert_model</code><code class="o">.</code><code class="n">embeddings</code><code class="o">.</code><code class="n">word_embeddings</code><code class="w"/>
<code class="go">Embedding(30522, 768, padding_idx=0)</code></pre>

<p>As you can see, this BERT model is implemented using PyTorch, and it contains a regular <code translate="no">nn.Embedding</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.Embedding" id="id3293"/>. We could just replace our model’s <code translate="no">nn.Embedding</code> layer with this one (and retrain our model), but we can keep models cleanly separated by initializing our own <code translate="no">nn.Embedding</code> layer with a copy of the pretrained embedding matrix<a data-type="indexterm" data-primary="embedding matrix" id="id3294"/>. This can be done using the <code translate="no">Embedding.from_pretrained()</code> function:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">SentimentAnalysisModelPreEmbeds</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">pretrained_embeddings</code><code class="p">,</code> <code class="n">n_layers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code>
                 <code class="n">dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="n">weights</code> <code class="o">=</code> <code class="n">pretrained_embeddings</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">data</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">embed</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">weights</code><code class="p">,</code> <code class="n">freeze</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="n">embed_dim</code> <code class="o">=</code> <code class="n">weights</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>
        <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># the rest of the model is exactly like earlier</code>

<code class="n">imdb_model_bert_embeds</code> <code class="o">=</code> <code class="n">SentimentAnalysisModelPreEmbeds</code><code class="p">(</code>
    <code class="n">bert_model</code><code class="o">.</code><code class="n">embeddings</code><code class="o">.</code><code class="n">word_embeddings</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>Note that we set <code translate="no">freeze=True</code> when creating the <code translate="no">nn.Embedding</code> layer: this makes it nontrainable and ensures that the pretrained embeddings won’t be damaged by large gradients at the beginning of training. You can train the model for a few epochs like this, then make the embedding layer trainable and continue training, letting the model fine-tune the embeddings for our task.</p>

<p>Pretrained word embeddings<a data-type="indexterm" data-primary="word embeddings" id="id3295"/><a data-type="indexterm" data-primary="embeddings" data-secondary="word" id="id3296"/> have been popular for quite a while, starting with Google’s <a href="https://homl.info/word2vec">Word2vec embeddings</a> (2013), Stanford’s <a href="https://homl.info/glove">GloVe embeddings</a> (2014)<a data-type="indexterm" data-primary="Word2vec embeddings" id="id3297"/>, Facebook’s <a href="https://fasttext.cc">FastText embeddings</a> (2016), and more. However, this approach has its limits. In particular, a word has a single representation, no matter the context. For example, the word “right” is encoded the same way in “left and right” and “right and wrong”, even though it means two very different things. To address this limitation, a <a href="https://homl.info/elmo">2018 paper</a>⁠<sup><a data-type="noteref" id="id3298-marker" href="ch14.html#id3298">13</a></sup> by Matthew Peters<a data-type="indexterm" data-primary="Embeddings from Language Models (ELMo)" id="id3299"/><a data-type="indexterm" data-primary="ELMo (Embeddings from Language Models)" id="id3300"/> introduced <em>Embeddings from Language Models</em> (ELMo): these are contextualized word embeddings learned from the internal states of a deep bidirectional RNN language model. In other words, instead of just using pretrained word embeddings in your model, you can reuse several layers of a pretrained language model.</p>

<p>At roughly the same time<a data-type="indexterm" data-primary="Universal Language Model Fine-Tuning (ULMFiT)" id="id3301"/><a data-type="indexterm" data-primary="ULMFiT (Universal Language Model Fine-Tuning)" id="id3302"/>, the <a href="https://homl.info/ulmfit">Universal Language Model Fine-Tuning (ULMFiT) paper</a>⁠<sup><a data-type="noteref" id="id3303-marker" href="ch14.html#id3303">14</a></sup> by Jeremy Howard and Sebastian Ruder demonstrated the effectiveness of unsupervised pretraining<a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="in unsupervised learning" data-secondary-sortas="unsupervised learning" id="id3304"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="pretraining" id="id3305"/> for NLP tasks. The authors trained an LSTM language model<a data-type="indexterm" data-primary="long short-term memory (LSTM) cell" id="id3306"/><a data-type="indexterm" data-primary="LSTM (long short-term memory) cell" id="id3307"/> on a huge text corpus using self-supervised 
<span class="keep-together">learning</span> (i.e., generating the labels automatically from the data), then they fine-tuned it on various tasks. Their model outperformed the state of the art on six text classification tasks by a large margin (reducing the error rate by 18%–24% in most cases). Moreover, the authors showed that a pretrained model fine-tuned on just 100 labeled examples could achieve the same performance as one trained from scratch on 10,000 examples. Before the ULMFiT paper, using pretrained models was only the norm in computer vision; in the context of NLP, pretraining was limited to word embeddings. This paper marked the beginning of a new era in NLP: today, reusing pretrained language models is the norm.</p>

<p>For example, why not reuse the entire pretrained BERT model for our sentiment analysis model? To use the BERT model, the Transformers library<a data-type="indexterm" data-primary="Transformers library" data-secondary="reusing pre-trained tokenizers" id="id3308"/> lets us call it like a function, passing it the tokenized reviews:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bert_encoding</code> <code class="o">=</code> <code class="n">bert_tokenizer</code><code class="p">(</code><code class="n">train_reviews</code><code class="p">[:</code><code class="mi">3</code><code class="p">],</code> <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code><code class="w"/>
<code class="gp">... </code>                               <code class="n">max_length</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code> <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code><code class="w"/>
<code class="gp">... </code>                               <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bert_output</code> <code class="o">=</code> <code class="n">bert_model</code><code class="p">(</code><code class="o">**</code><code class="n">bert_encoding</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bert_output</code><code class="o">.</code><code class="n">last_hidden_state</code><code class="o">.</code><code class="n">shape</code><code class="w"/>
<code class="go">torch.Size([3, 200, 768])</code></pre>

<p>BERT’s output includes an attribute<a data-type="indexterm" data-primary="last_hidden_state attribute" id="id3309"/> named <code translate="no">last_hidden_state</code>, which contains contextualized embeddings for each token. The word “last” in this case refers to the last layer, not the last time step (BERT is a transformer, not an RNN). This <code translate="no">last_hidden_state</code> tensor has a shape of [<em>batch size</em>, <em>max sequence length</em>, <em>hidden size</em>]. Let’s use these contextualized embeddings in a sentiment analysis model:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">SentimentAnalysisModelBert</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">n_layers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">bert</code> <code class="o">=</code> <code class="n">transformers</code><code class="o">.</code><code class="n">AutoModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"bert-base-uncased"</code><code class="p">)</code>
        <code class="n">embed_dim</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">hidden_size</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">gru</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">,</code> <code class="p">[</code><code class="o">...</code><code class="p">])</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">encodings</code><code class="p">):</code>
        <code class="n">contextualized_embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="p">(</code><code class="o">**</code><code class="n">encodings</code><code class="p">)</code><code class="o">.</code><code class="n">last_hidden_state</code>
        <code class="n">lengths</code> <code class="o">=</code> <code class="n">encodings</code><code class="p">[</code><code class="s2">"attention_mask"</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">packed</code> <code class="o">=</code> <code class="n">pack_padded_sequence</code><code class="p">(</code><code class="n">contextualized_embeddings</code><code class="p">,</code> <code class="p">[</code><code class="o">...</code><code class="p">])</code>
        <code class="n">_outputs</code><code class="p">,</code> <code class="n">hidden_states</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">gru</code><code class="p">(</code><code class="n">packed</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">hidden_states</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code></pre>

<p>Note that we don’t need to make the <code translate="no">nn.GRU</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.GRU" id="id3310"/> bidirectional<a data-type="indexterm" data-primary="GRU (gated recurrent unit) cell" id="id3311"/><a data-type="indexterm" data-primary="gated recurrent unit (GRU) cell" id="id3312"/> since the contextualized embeddings already looked ahead.</p>

<p>If you freeze the BERT model (e.g., using <code translate="no">model.bert.requires_grad_(False)</code>) and train the rest of the model, you will notice a significant performance boost, reaching over 88% accuracy. Wonderful!</p>

<p>Another option is to use only the contextualized embedding for the very first token, which is the <em>class token</em> [CLS].<a data-type="indexterm" data-primary="class token" id="id3313"/> Indeed, during pretraining, the BERT model had to perform a text classification task based solely on this token’s contextualized embedding (we will discuss BERT pretraining in more detail in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>). As a result, it learned to summarize the most important features of the text into this embedding. This simplifies our model quite a bit, since we can get rid of the <code translate="no">nn.GRU</code> module altogether, and the <code translate="no">forward()</code> method<a data-type="indexterm" data-primary="torch" data-secondary="forward()" id="id3314"/> becomes much shorter:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">encodings</code><code class="p">):</code>
    <code class="n">bert_output</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">bert</code><code class="p">(</code><code class="o">**</code><code class="n">encodings</code><code class="p">)</code>
    <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">bert_output</code><code class="o">.</code><code class="n">last_hidden_state</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">])</code></pre>

<p>In fact, the BERT model contains an extra hidden layer on top of the class embedding, composed<a data-type="indexterm" data-primary="torch" data-secondary="nn.Linear" id="id3315"/><a data-type="indexterm" data-primary="torch" data-secondary="nn.Tanh" id="id3316"/> of an <code translate="no">nn.Linear</code> module and an <code translate="no">nn.Tanh</code> module. This hidden layer is called<a data-type="indexterm" data-primary="pooler, BERT model" id="id3317"/> the <em>pooler</em>. To use it, just replace <code translate="no">bert_output.last_hidden_state[:, 0]</code> with <code translate="no">bert_output.pooler_output</code>. You may also want to unfreeze the pooler after a few epochs to fine-tune it for the IMDb task.</p>

<p>So we started by reusing only the pretrained tokenizer, then we reused the pretrained embeddings, then most of the pretrained BERT model, and finally the full model, adding only an <code translate="no">nn.Linear</code> layer on top of the pooler<a data-type="indexterm" data-startref="xi_embeddingsreusinginsentimentanalysis1474146_1" id="id3318"/><a data-type="indexterm" data-startref="xi_sentimentanalysisreusingpretrainedembeddingsin1474146_1" id="id3319"/><a data-type="indexterm" data-startref="xi_pretrainingandpretrainedlayersreusingembeddingsinsentimentanalysis1474146_1" id="id3320"/>. We can actually go one step further and just use an off-the-shelf class for sentence classification<a data-type="indexterm" data-startref="xi_BERTBidirectionalEncoderRepresentationsfromTransformersembeddinglayer1474351_1" id="id3321"/><a data-type="indexterm" data-startref="xi_embeddingsBERT1474351_1" id="id3322"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Task-Specific Classes"><div class="sect2" id="id273">
<h2>Task-Specific Classes</h2>

<p>To tackle our binary classification<a data-type="indexterm" data-primary="sentiment analysis" data-secondary="task-specific classes" id="xi_sentimentanalysistaskspecificclasses1482636_1"/><a data-type="indexterm" data-primary="task-specific classes, sentiment analysis" id="xi_taskspecificclassessentimentanalysis1482636_1"/> task using BERT, we can use the <code translate="no">BertForSequenceClassification</code> class<a data-type="indexterm" data-primary="BertForSequenceClassification class" id="id3323"/><a data-type="indexterm" data-primary="Transformers library" data-secondary="BertForSequenceClassification" id="id3324"/> provided by the Transformers library<a data-type="indexterm" data-primary="Transformers library" data-secondary="task-specific classes" id="xi_Transformerslibrarytaskspecificclasses14826143_1"/>. It’s just a BERT model<a data-type="indexterm" data-primary="BERT (Bidirectional Encoder Representations from Transformers)" data-secondary="task-specific classes" id="xi_BERTBidirectionalEncoderRepresentationsfromTransformerstaskspecificclasses14826167_1"/> plus a classification head on top. All you need to do to create this model is specify the pretrained BERT checkpoint you want to use, the number of output units for your classification task, and optionally the data type (we’ll use 16-bit floats to fit on small GPUs):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">BertForSequenceClassification</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">bert_for_binary_clf</code> <code class="o">=</code> <code class="n">BertForSequenceClassification</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="s2">"bert-base-uncased"</code><code class="p">,</code> <code class="n">num_labels</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>The Transformers library contains many task-specific classes based on various pretrained models, such as <code translate="no">BertForQuestionAnswering</code> or <code translate="no">RobertaForSequenceClassification</code> (see <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>). You can also use <code translate="no">AutoModelForSequenceClassification</code> to let the library pick the right class for you, based on the requested model checkpoint (e.g., if you ask for <code translate="no">"bert-base-uncased"</code>, you will get an instance of <code translate="no">BertForSequenceClassification</code>). Similar <code translate="no">AutoModelFor[...]</code> classes are available for other tasks.</p>
</div>

<p>Until now we have always used a single output for binary classification, so why did we set <code translate="no">num_labels=2</code>? Well, for simplicity Hugging Face prefers to treat binary classification exactly like multiclass classification, so this model will output two logits instead of one, and it must be trained using the <code translate="no">nn.CrossEntropyLoss</code> instead<a data-type="indexterm" data-primary="torch" data-secondary="nn.CrossEntropyLoss" id="id3325"/><a data-type="indexterm" data-primary="torch" data-secondary="nn.BCELoss" id="id3326"/><a data-type="indexterm" data-primary="torch" data-secondary="nn.BCEWithLogitsLoss" id="id3327"/> of <code translate="no">nn.BCELoss</code> or <code translate="no">nn.BCEWithLogitsLoss</code>. If you want to convert the logits to estimated probabilities, you must use <code translate="no">torch.softmax()</code> rather<a data-type="indexterm" data-primary="torch" data-secondary="softmax()" id="id3328"/><a data-type="indexterm" data-primary="torch" data-secondary="sigmoid()" id="id3329"/> than <code translate="no">torch.sigmoid()</code>.</p>

<p>Let’s call this model on a very positive review:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">encoding</code> <code class="o">=</code> <code class="n">bert_tokenizer</code><code class="p">([</code><code class="s2">"This was a great movie!"</code><code class="p">])</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code><code class="w"/>
<code class="gp">... </code>  <code class="n">output</code> <code class="o">=</code> <code class="n">bert_for_binary_clf</code><code class="p">(</code><code class="w"/>
<code class="gp">... </code>    <code class="n">input_ids</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">encoding</code><code class="p">[</code><code class="s2">"input_ids"</code><code class="p">],</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">),</code><code class="w"/>
<code class="gp">... </code>    <code class="n">attention_mask</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">encoding</code><code class="p">[</code><code class="s2">"attention_mask"</code><code class="p">],</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">))</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">output</code><code class="o">.</code><code class="n">logits</code><code class="w"/>
<code class="go">tensor([[-0.0120,  0.6304]], device='cuda:0', dtype=torch.float16)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">torch</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">output</code><code class="o">.</code><code class="n">logits</code><code class="p">,</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code><code class="w"/>
<code class="go">tensor([[0.3447, 0.6553]], device='cuda:0', dtype=torch.float16)</code></pre>

<p>We first tokenize the review, then we call the model, it returns a <code translate="no">ModelOutput</code> object containing the logits, and we convert these logits to estimated probabilities using the <code translate="no">torch.softmax()</code> function. Ouch! The model classified this review as negative with 65.53% confidence! Indeed, the BERT model inside <code translate="no">BertForSequenceClassification</code> is pretrained, but not the classification head, so we’re going to get terrible performance until we actually train this model on the IMDb dataset.</p>

<p>If you pass labels when calling this model (or any other model from the Transformers library), then it also computes the loss and returns it in the <code translate="no">ModelOutput</code> object. For example:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code><code class="w"/>
<code class="gp">... </code>  <code class="n">output</code> <code class="o">=</code> <code class="n">bert_for_binary_clf</code><code class="p">(</code><code class="w"/>
<code class="gp">... </code>    <code class="n">input_ids</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">encoding</code><code class="p">[</code><code class="s2">"input_ids"</code><code class="p">],</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">),</code><code class="w"/>
<code class="gp">... </code>    <code class="n">attention_mask</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">encoding</code><code class="p">[</code><code class="s2">"attention_mask"</code><code class="p">],</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">),</code><code class="w"/>
<code class="gp">... </code>    <code class="n">labels</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">],</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">))</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">output</code><code class="o">.</code><code class="n">loss</code><code class="w"/>
<code class="go">tensor(0.4226, device='cuda:0', dtype=torch.float16)</code></pre>

<p>Since <code translate="no">num_labels</code> is greater than 1, the model computes<a data-type="indexterm" data-primary="torch" data-secondary="nn.CrossEntropyLoss" id="id3330"/><a data-type="indexterm" data-primary="torch" data-secondary="nn.LogSoftmax" id="id3331"/><a data-type="indexterm" data-primary="torch" data-secondary="nn.NLLLoss" id="id3332"/> the <code translate="no">nn.CrossEntropyLoss</code> (which is implemented as <code translate="no">nn.LogSoftmax</code> followed by <code translate="no">nn.NLLLoss</code>—that’s why we see <code translate="no">grad_fn=&lt;NllLossBackward0&gt;</code>). If we had used <code translate="no">num_labels=1</code>, then the model would have used the <code translate="no">nn.MSELoss</code> instead<a data-type="indexterm" data-primary="torch" data-secondary="nn.MSELoss" id="id3333"/>; this can be useful for regression tasks<a data-type="indexterm" data-startref="xi_BERTBidirectionalEncoderRepresentationsfromTransformerstaskspecificclasses14826167_1" id="id3334"/><a data-type="indexterm" data-startref="xi_sentimentanalysistaskspecificclasses1482636_1" id="id3335"/><a data-type="indexterm" data-startref="xi_taskspecificclassessentimentanalysis1482636_1" id="id3336"/>.</p>

<p>We could now train this model using our own training code, as we did so far, but the Transformers library provides a convenient <em>Trainer API</em>, so let’s check it out.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The Trainer API"><div class="sect2" id="id274">
<h2>The Trainer API</h2>

<p>The Trainer API<a data-type="indexterm" data-primary="sentiment analysis" data-secondary="Trainer API" id="xi_sentimentanalysisTrainerAPI1487616_1"/><a data-type="indexterm" data-primary="Trainer API, sentiment analysis" id="xi_TrainerAPIsentimentanalysis1487616_1"/> lets you fine-tune a model on your own dataset with very little boilerplate code. It can save model checkpoints during training, apply early stopping, distribute the computations across GPUs, log metrics, take care of padding, batching, shuffling, and more. Let’s use the Trainer API to train our IMDb model.</p>

<p>The Trainer API works directly with dataset objects, not data loaders, but it expects the datasets to contain tokenized text, not strings, so we must take care of tokenization<a data-type="indexterm" data-primary="tokenization" id="id3337"/>. We can do this quite simply using the dataset’s <code translate="no">map()</code> method<a data-type="indexterm" data-primary="map() method" id="id3338"/> (this method is implemented by the Datasets library<a data-type="indexterm" data-primary="Datasets library" id="id3339"/>; it’s not available on pure PyTorch datasets):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">tokenize_batch</code><code class="p">(</code><code class="n">batch</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">bert_tokenizer</code><code class="p">(</code><code class="n">batch</code><code class="p">[</code><code class="s2">"text"</code><code class="p">],</code> <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">max_length</code><code class="o">=</code><code class="mi">200</code><code class="p">)</code>

<code class="n">tok_imdb_train_set</code> <code class="o">=</code> <code class="n">imdb_train_set</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">tokenize_batch</code><code class="p">,</code> <code class="n">batched</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">tok_imdb_valid_set</code> <code class="o">=</code> <code class="n">imdb_valid_set</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">tokenize_batch</code><code class="p">,</code> <code class="n">batched</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">tok_imdb_test_set</code> <code class="o">=</code> <code class="n">imdb_test_set</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">tokenize_batch</code><code class="p">,</code> <code class="n">batched</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>Since we set <code translate="no">batched=True</code>, the <code translate="no">map()</code> method passes batches of reviews to the <code translate="no">tokenize_batch()</code> method: this is optional, but it significantly speeds up this preprocessing step. The <code translate="no">tokenize_batch()</code> method tokenizes the given batch of reviews, and the resulting fields are added to each instance by the <code translate="no">map()</code> method. This includes fields such as <code translate="no">token_ids</code> and <code translate="no">attention_mask</code>, which the model expects.</p>

<p>To evaluate our model, we can write a simple function that takes an object with two attributes: <code translate="no">label_ids</code> and <code translate="no">predictions</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">compute_accuracy</code><code class="p">(</code><code class="n">pred</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"accuracy"</code><code class="p">:</code> <code class="p">(</code><code class="n">pred</code><code class="o">.</code><code class="n">label_ids</code> <code class="o">==</code> <code class="n">pred</code><code class="o">.</code><code class="n">predictions</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">))</code><code class="o">.</code><code class="n">mean</code><code class="p">()}</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>Alternatively, you can use metrics provided by the Hugging Face<a data-type="indexterm" data-primary="Evaluate library" id="id3340"/> <em>Evaluate library</em>: they are designed to work nicely with the Transformers library. Alternatively, although the Trainer API does not support the streaming metrics from the TorchMetrics library<a data-type="indexterm" data-primary="TorchMetrics library" id="id3341"/>, you can still use them if you wrap them inside a function.</p>
</div>

<p>Next, we must specify our training configuration in a <code translate="no">TrainingArguments</code> object<a data-type="indexterm" data-primary="Transformers library" data-secondary="TrainingArguments" id="id3342"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">TrainingArguments</code>

<code class="n">train_args</code> <code class="o">=</code> <code class="n">TrainingArguments</code><code class="p">(</code>
    <code class="n">output_dir</code><code class="o">=</code><code class="s2">"my_imdb_model"</code><code class="p">,</code> <code class="n">num_train_epochs</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
    <code class="n">per_device_train_batch_size</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code> <code class="n">per_device_eval_batch_size</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code>
    <code class="n">eval_strategy</code><code class="o">=</code><code class="s2">"epoch"</code><code class="p">,</code> <code class="n">logging_strategy</code><code class="o">=</code><code class="s2">"epoch"</code><code class="p">,</code> <code class="n">save_strategy</code><code class="o">=</code><code class="s2">"epoch"</code><code class="p">,</code>
    <code class="n">load_best_model_at_end</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">metric_for_best_model</code><code class="o">=</code><code class="s2">"accuracy"</code><code class="p">,</code>
    <code class="n">report_to</code><code class="o">=</code><code class="s2">"none"</code><code class="p">)</code></pre>

<p>We specify that the logs and model checkpoints must be saved in the <code translate="no">my_imdb_model</code> directory; training should run for 2 epochs (you can increase this if you want); the batch size is 128 for both training and evaluation (you can tweak this depending on the amount of VRAM you have); we want evaluation, logging, and saving to take place at the end of each epoch; and the best model should be loaded at the end of training based on the validation accuracy. Lastly, the <code translate="no">report_to</code> argument lets you specify one or more tools that the training code will report logs to, such as TensorBoard or <a href="https://wandb.ai">Weights &amp; Biases (W&amp;B)</a>. This can be useful to visualize the learning curves. For simplicity, I set <code translate="no">report_to="none"</code> to turn reporting off.</p>

<p>Lastly, we create a <code translate="no">Trainer</code> object and pass it the model, along with the training arguments, the training and validation sets, the evaluation function, plus a data collator which will take care of padding. Finally, we call the trainer’s <code translate="no">train()</code> method, and we’re done! The model reaches about 90% accuracy on the validation set after just two epochs:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">DataCollatorWithPadding</code><code class="p">,</code> <code class="n">Trainer</code>

<code class="n">trainer</code> <code class="o">=</code> <code class="n">Trainer</code><code class="p">(</code>
    <code class="n">bert_for_binary_clf</code><code class="p">,</code> <code class="n">train_args</code><code class="p">,</code> <code class="n">train_dataset</code><code class="o">=</code><code class="n">tok_imdb_train_set</code><code class="p">,</code>
    <code class="n">eval_dataset</code><code class="o">=</code><code class="n">tok_imdb_valid_set</code><code class="p">,</code> <code class="n">compute_metrics</code><code class="o">=</code><code class="n">compute_accuracy</code><code class="p">,</code>
    <code class="n">data_collator</code><code class="o">=</code><code class="n">DataCollatorWithPadding</code><code class="p">(</code><code class="n">bert_tokenizer</code><code class="p">))</code>
<code class="n">train_output</code> <code class="o">=</code> <code class="n">trainer</code><code class="o">.</code><code class="n">train</code><code class="p">()</code></pre>

<p>Great, you now know how to download a pretrained model like BERT and fine-tune it on your own dataset!<a data-type="indexterm" data-startref="xi_sentimentanalysisTrainerAPI1487616_1" id="id3343"/><a data-type="indexterm" data-startref="xi_TrainerAPIsentimentanalysis1487616_1" id="id3344"/> But what if you don’t have a dataset at all, and you just want to use a pretrained model that was already fine-tuned for sentiment analysis? For this, you can use the <em>pipelines API</em>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Hugging Face Pipelines"><div class="sect2" id="id275">
<h2>Hugging Face Pipelines</h2>

<p>The Transformers library provides a very convenient API<a data-type="indexterm" data-primary="pipelines API" id="xi_pipelinesAPI1493356_1"/><a data-type="indexterm" data-primary="sentiment analysis" data-secondary="pipelines API" id="xi_sentimentanalysispipelinesAPI1493356_1"/> to download and use pretrained pipelines for various tasks. Each pipeline contains a pretrained model along with its corresponding preprocessing and post-processing modules. For example let’s create a sentiment analysis pipeline and run it on the first 10 IMDb reviews in the training set:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">pipeline</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">model_name</code> <code class="o">=</code> <code class="s2">"distilbert-base-uncased-finetuned-sst-2-english"</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">classifier_imdb</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"sentiment-analysis"</code><code class="p">,</code> <code class="n">model</code><code class="o">=</code><code class="n">model_name</code><code class="p">,</code><code class="w"/>
<code class="gp">... </code>                           <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">max_length</code><code class="o">=</code><code class="mi">512</code><code class="p">)</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">classifier_imdb</code><code class="p">(</code><code class="n">train_reviews</code><code class="p">[:</code><code class="mi">10</code><code class="p">])</code><code class="w"/>
<code class="go">[{'label': 'POSITIVE', 'score': 0.9996108412742615},</code>
<code class="go"> {'label': 'POSITIVE', 'score': 0.9998623132705688},</code>
<code class="go"> [...]</code>
<code class="go"> {'label': 'POSITIVE', 'score': 0.9978922009468079},</code>
<code class="go"> {'label': 'NEGATIVE', 'score': 0.9997020363807678}]</code></pre>

<p>Well, it could hardly be any easier, could it? Just create a pipeline by specifying the task and the model to use, and a couple of other parameters, depending on the task, and off you go! In this example, each review gets a <code translate="no">"POSITIVE"</code> or <code translate="no">"NEGATIVE"</code> label, along with a score equal to the model’s estimated probability for that label. This particular model actually reaches 88.2% accuracy on the validation set, which is reasonably good. Here a few points to note:</p>

<ul>
<li>
<p>If you don’t specify a model, the <code translate="no">pipeline()</code> function will use the default model for the chosen task. For sentiment analysis, at the time of writing, it’s the model we chose: it’s a DistilBERT model<a data-type="indexterm" data-primary="DistilBERT" id="id3345"/>—a scaled down version of BERT—with an uncased tokenizer, trained on the English Wikipedia and a corpus of English books, and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task.</p>
</li>
<li>
<p>The pipeline automatically uses the GPU if you have one. If you have several GPUs, you can specify which one to use by setting the pipeline’s <code translate="no">device</code> argument to the GPU index.</p>
</li>
<li>
<p>The models from the Transformers library are always in evaluation mode by default (no need to call <code translate="no">eval()</code>).</p>
</li>
<li>
<p>The score is for the chosen label, not for the positive class. In particular, since this is a binary classification task, the score cannot be lower than 0.5 (or else the model would have picked the other label)<a data-type="indexterm" data-startref="xi_Transformerslibrarytaskspecificclasses14826143_1" id="id3346"/>.</p>
</li>
</ul>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id3347">
<h1>Bias and Fairness</h1>
<p>If you try classifying “I am from the USA” and “I am from Iraq”, you will see that the former is classified as very positive, while the latter is classified as very negative<a data-type="indexterm" data-primary="bias" data-secondary="and fairness in NLP transformers" data-secondary-sortas="fairness in NLP transformers" id="id3348"/>. The model is also positive about Thailand but negative about Vietnam. You can try this model with your own country or city; the result may surprise you. Such a bias generally comes in large part from the training data itself: in this case, there were plenty of references to the wars in Iraq and Vietnam in the model’s training data, creating a negative bias. This bias was then amplified during the fine-tuning process since the model was forced to choose between just two classes: positive or negative. If you use a model that was fine-tuned on a dataset with an extra neutral class, then the country bias mostly disappears.</p>

<p>The training data is not the only source of bias: the model’s architecture, the type of loss or regularization used for training, the optimizer—all of these can affect what the model ends up learning. Understanding bias in AI and mitigating its negative effects is still an area of active research, but in any case you should probably pause and think before you rush to deploy a model to production. For example, if you train a model to score resumes, you must ensure that it’s fair. So make sure you evaluate the model’s performance not just on average over the whole test set, but across various subsets as well; for example, you may find that although the model works very well on average, its performance is abysmal for some categories of people. You may also want to run counterfactual tests; for example, check that the model’s predictions do not change when you simply switch someone’s gender or place of birth. The solution depends on the problem: it may require rebalancing the dataset, fine-tuning on a different dataset, switching to another pretrained model, tweaking the model’s architecture or hyperparameters, etc.</p>

<p>Lastly, even if you manage to train a perfectly fair model, it could be <em>used</em> in a biased way. For example, the recruiters may use it only for some category of people and not others.</p>
</div></aside>

<p>The model we chose is well-suited for general-purpose sentiment analysis, such as movie reviews, but other models are better suited for specific use cases, such as social media posts (e.g., trained on a large dataset of tweets, then fine-tuned on a sentiment analysis dataset). To find the best model for your use case, you can search the list of available models on the <a href="https://huggingface.co/models"><em>Hugging Face Hub</em></a>. However, there are over 80,000 models available in the “text-classification” category alone, so you will need to use the filters to narrow down the options. In particular, start by filtering on the task, and sort by trending or most-liked models. You can also continue to filter by language and dataset, if necessary. Prefer models from reputable sources (e.g., models from users huggingface, facebook, google, cardiffnlp, and so on), and if the model includes executable code, make absolutely sure you trust the user (and if you do, set <code translate="no">trust_remote_code=True</code> when calling the <code translate="no">pipeline()</code> function).</p>

<p>There are many text classification tasks other than sentiment analysis. For example, a model fine-tuned on the multi-genre natural language inference (MultiNLI) dataset<a data-type="indexterm" data-primary="Multi-Genre Natural Language Inference (MultiNLI) dataset" id="id3349"/> can classify a pair of texts (each ending with a separation token [SEP]) into three classes: contradiction (if the texts contradict each other), entailment (if the first text entails the second), or neutral otherwise. For example:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">model_name</code> <code class="o">=</code> <code class="s2">"huggingface/distilbert-base-uncased-finetuned-mnli"</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">classifier_mnli</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"text-classification"</code><code class="p">,</code> <code class="n">model</code><code class="o">=</code><code class="n">model_name</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">classifier_mnli</code><code class="p">([</code><code class="w"/>
<code class="gp">... </code>    <code class="s2">"She loves me. [SEP] She loves me not. [SEP]"</code><code class="p">,</code><code class="w"/>
<code class="gp">... </code>    <code class="s2">"Alice just woke up. [SEP] Alice is awake. [SEP]"</code><code class="p">,</code><code class="w"/>
<code class="gp">... </code>    <code class="s2">"I like dogs. [SEP] Everyone likes dogs. [SEP]"</code><code class="p">])</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="go">[{'label': 'contradiction', 'score': 0.9717152714729309},</code>
<code class="go"> {'label': 'entailment', 'score': 0.9119168519973755},</code>
<code class="go"> {'label': 'neutral', 'score': 0.9509281516075134}]</code></pre>

<p>Many other NLP tasks are also available via the pipeline API, such as question answering, summarization, sentence similarity, text generation, token classification, translation, and more. And it doesn’t stop there! There are also many computer vision tasks, such as image classification, image segmentation, object detection, image-to-text, text-to-image, depth estimation, and even audio tasks, such as audio classification, speech-to-text, text-to-speech, and so on. Make sure to check out the full list at <a href="https://huggingface.co/tasks" class="bare"><em class="hyperlink">https://huggingface.co/tasks</em></a>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Before you download a model, make sure you trust the hosting platform (e.g., the Hugging Face Hub) and the model’s author: the model may contain executable code, which could be malicious. It could also produce biased outputs, or it may have been trained with copyrighted or sensitive private data which might be leaked to your users, or it might even<a data-type="indexterm" data-primary="poisoned weights" id="id3350"/> have <em>poisoned weights</em> which could make it produce harmful content (e.g., propaganda) only for some types of inputs, otherwise behaving normally.</p>
</div>

<p>Time to step back. So far we have looked at text generation using a char-RNN, and sentiment analysis using various subword tokenization methods, pretrained embeddings, and even entire pretrained models. Along the way, we discussed embeddings, tokenizers, and the Hugging Face libraries. In the next section, we will explore another important NLP task: <em>neural machine translation</em> (NMT)<a data-type="indexterm" data-startref="xi_sentimentanalysis1433043_1" id="id3351"/>. Specifically, we will build an encoder-decoder model capable of translating English to Spanish, and we will see how to boost its performance using beam search and attention mechanisms<a data-type="indexterm" data-startref="xi_pipelinesAPI1493356_1" id="id3352"/><a data-type="indexterm" data-startref="xi_sentimentanalysispipelinesAPI1493356_1" id="id3353"/>. ¡Vamos!</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="An Encoder-Decoder Network for &#10;Neural Machine Translation"><div class="sect1" id="id276">
<h1>An Encoder-Decoder Network for 
<span class="keep-together">Neural Machine Translation</span></h1>

<p>Let’s begin<a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="encoder-decoder network for NMT" id="xi_naturallanguageprocessingNLPencoderdecodernetworkforNMT1499112_1"/><a data-type="indexterm" data-primary="encoder-decoder models" data-secondary="NMT network" id="xi_encoderdecodermodelsNMTnetwork1499112_1"/><a data-type="indexterm" data-primary="neural machine translation (NMT)" data-secondary="encoder-decoder network for" id="xi_neuralmachinetranslationNMTencoderdecodernetworkfor1499112_1"/> with a relatively simple <a href="https://homl.info/nmtmodel">sequence-to-sequence NMT model</a>⁠<sup><a data-type="noteref" id="id3354-marker" href="ch14.html#id3354">15</a></sup> that will translate English text to Spanish (see <a data-type="xref" href="#machine_translation_diagram">Figure 14-5</a>).</p>

<figure><div id="machine_translation_diagram" class="figure">
<img src="assets/hmls_1405.png" alt="Diagram of an encoder-decoder network illustrating a sequence-to-sequence model for translating &quot;I like soccer&quot; from English to Spanish as &quot;Me gusta el fútbol.&quot;" width="1239" height="1071"/>
<h6><span class="label">Figure 14-5. </span>A simple machine translation model</h6>
</div></figure>

<p>In short, the architecture is as follows: English texts are fed as inputs to the encoder, and the decoder outputs the Spanish translations. Note that the Spanish translations are also used as inputs to the decoder during training, but shifted back by one step. In other words, during training the decoder is given as input the token that it <em>should</em> have output at the previous step, regardless of what it actually output. This<a data-type="indexterm" data-primary="teacher forcing" id="id3355"/> is called <em>teacher forcing</em>—a technique that significantly speeds up training and improves the model’s performance. For the very first token, the decoder is given the start-of-sequence (SoS, a.k.a. beginning-of-sequence, BoS) token (<code translate="no">"&lt;s&gt;"</code>), and the decoder is expected to end the text with an end-of-sequence (EoS) token (<code translate="no">"&lt;/s&gt;"</code>).</p>

<p>Each token is initially represented by its ID (e.g., <code translate="no">4553</code> for the token “soccer”). Next, an <code translate="no">nn.Embedding</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.Embedding" id="id3356"/> returns the token embedding. These token embeddings are then fed to the encoder and the decoder.</p>

<p>At each step, the decoder’s dense output layer (i.e., an <code translate="no">nn.Linear</code> layer)<a data-type="indexterm" data-primary="torch" data-secondary="nn.Linear" id="id3357"/> outputs a logit score for each token in the output vocabulary (i.e., Spanish). If you pass these logits through the softmax function<a data-type="indexterm" data-primary="softmax activation function" data-secondary="NMT encoder-decoder network" id="id3358"/>, you get an estimated probability for each possible token. For example, at the first step the word “Me” may have a probability of 7%, “Yo” may have a probability of 1%, and so on. This is very much like a regular classification task, and indeed we will train the model using the <code translate="no">nn.CrossEntropyLoss</code>, much like we did in the char-RNN model.</p>

<p>Note that at inference time (after training), you will not have the target text to feed to the decoder. Instead, you need to feed it the word that it has just output at the previous step, as shown in <a data-type="xref" href="#inference_decoder_diagram">Figure 14-6</a> (this will require an embedding lookup that is not shown in the diagram).</p>

<figure class="smallereighty"><div id="inference_decoder_diagram" class="figure">
<img src="assets/hmls_1406.png" alt="Diagram showing a sequence of recurrent neural network cells where each cell outputs a token that is fed as input to the next cell during inference." width="995" height="479"/>
<h6><span class="label">Figure 14-6. </span>At inference time, the decoder is fed as input the word it just output at the previous time step</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>In a <a href="https://homl.info/scheduledsampling">2015 paper</a>,⁠<sup><a data-type="noteref" id="id3359-marker" href="ch14.html#id3359">16</a></sup> Samy Bengio et al. proposed gradually switching from feeding the decoder the previous <em>target</em> token to feeding it the previous <em>output</em> token during training.</p>
</div>

<p>Let’s build and train this model! First, we need to download a dataset of English/Spanish text pairs. For this, we will use the Datasets library<a data-type="indexterm" data-primary="Datasets library" id="id3360"/> to download English/Spanish pairs from the <em>Tatoeba Challenge</em> dataset<a data-type="indexterm" data-primary="Tatoeba Challenge dataset" id="id3361"/>. The <a href="https://tatoeba.org">Tatoeba project</a> is a language-learning initiative started in 2006 by Trang Ho, where contributors have created a huge collection of text pairs from many languages. The Tatoeba Challenge dataset was created by researchers from the University of Helsinki to benchmark machine translation systems, using data extracted from the Tatoeba project. The training set is quite large so we will use the validation set as our training set, setting aside 20% for validation. We will also download the test set:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">nmt_original_valid_set</code><code class="p">,</code> <code class="n">nmt_test_set</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code>
    <code class="n">path</code><code class="o">=</code><code class="s2">"ageron/tatoeba_mt_train"</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"eng-spa"</code><code class="p">,</code>
    <code class="n">split</code><code class="o">=</code><code class="p">[</code><code class="s2">"validation"</code><code class="p">,</code> <code class="s2">"test"</code><code class="p">])</code>
<code class="n">split</code> <code class="o">=</code> <code class="n">nmt_original_valid_set</code><code class="o">.</code><code class="n">train_test_split</code><code class="p">(</code><code class="n">train_size</code><code class="o">=</code><code class="mf">0.8</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">nmt_train_set</code><code class="p">,</code> <code class="n">nmt_valid_set</code> <code class="o">=</code> <code class="n">split</code><code class="p">[</code><code class="s2">"train"</code><code class="p">],</code> <code class="n">split</code><code class="p">[</code><code class="s2">"test"</code><code class="p">]</code></pre>

<p>Each sample in the dataset is a dictionary containing an English text along with its Spanish translation. For example:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">nmt_train_set</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="w"/>
<code class="go">{'source_text': 'Tom tried to break up the fight.',</code>
<code class="go"> 'target_text': 'Tom trató de disolver la pelea.',</code>
<code class="go"> 'source_lang': 'eng',</code>
<code class="go"> 'target_lang': 'spa'}</code></pre>

<p>We will need to tokenize this text. We could use a different tokenizer for English and Spanish, but these two languages have many words in common (e.g., animal, color, hotel, hospital, idea, radio, motor), and many similar subwords (e.g., pre, auto, inter, uni), so it makes sense to use a common tokenizer. Let’s train a BPE tokenizer on all the training text, both English and Spanish:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">train_eng_spa</code><code class="p">():</code>  <code class="c1"># a generator function to iterate over all training text</code>
    <code class="k">for</code> <code class="n">pair</code> <code class="ow">in</code> <code class="n">nmt_train_set</code><code class="p">:</code>
        <code class="k">yield</code> <code class="n">pair</code><code class="p">[</code><code class="s2">"source_text"</code><code class="p">]</code>
        <code class="k">yield</code> <code class="n">pair</code><code class="p">[</code><code class="s2">"target_text"</code><code class="p">]</code>

<code class="n">max_length</code> <code class="o">=</code> <code class="mi">256</code>
<code class="n">vocab_size</code> <code class="o">=</code> <code class="mi">10_000</code>
<code class="n">nmt_tokenizer_model</code> <code class="o">=</code> <code class="n">tokenizers</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">BPE</code><code class="p">(</code><code class="n">unk_token</code><code class="o">=</code><code class="s2">"&lt;unk&gt;"</code><code class="p">)</code>
<code class="n">nmt_tokenizer</code> <code class="o">=</code> <code class="n">tokenizers</code><code class="o">.</code><code class="n">Tokenizer</code><code class="p">(</code><code class="n">nmt_tokenizer_model</code><code class="p">)</code>
<code class="n">nmt_tokenizer</code><code class="o">.</code><code class="n">enable_padding</code><code class="p">(</code><code class="n">pad_id</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">pad_token</code><code class="o">=</code><code class="s2">"&lt;pad&gt;"</code><code class="p">)</code>
<code class="n">nmt_tokenizer</code><code class="o">.</code><code class="n">enable_truncation</code><code class="p">(</code><code class="n">max_length</code><code class="o">=</code><code class="n">max_length</code><code class="p">)</code>
<code class="n">nmt_tokenizer</code><code class="o">.</code><code class="n">pre_tokenizer</code> <code class="o">=</code> <code class="n">tokenizers</code><code class="o">.</code><code class="n">pre_tokenizers</code><code class="o">.</code><code class="n">Whitespace</code><code class="p">()</code>
<code class="n">nmt_tokenizer_trainer</code> <code class="o">=</code> <code class="n">tokenizers</code><code class="o">.</code><code class="n">trainers</code><code class="o">.</code><code class="n">BpeTrainer</code><code class="p">(</code>
    <code class="n">vocab_size</code><code class="o">=</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">special_tokens</code><code class="o">=</code><code class="p">[</code><code class="s2">"&lt;pad&gt;"</code><code class="p">,</code> <code class="s2">"&lt;unk&gt;"</code><code class="p">,</code> <code class="s2">"&lt;s&gt;"</code><code class="p">,</code> <code class="s2">"&lt;/s&gt;"</code><code class="p">])</code>
<code class="n">nmt_tokenizer</code><code class="o">.</code><code class="n">train_from_iterator</code><code class="p">(</code><code class="n">train_eng_spa</code><code class="p">(),</code> <code class="n">nmt_tokenizer_trainer</code><code class="p">)</code></pre>

<p>Let’s test this tokenizer:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">nmt_tokenizer</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s2">"I like soccer"</code><code class="p">)</code><code class="o">.</code><code class="n">ids</code><code class="w"/>
<code class="go">[43, 401, 4381]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">nmt_tokenizer</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s2">"&lt;s&gt; Me gusta el fútbol"</code><code class="p">)</code><code class="o">.</code><code class="n">ids</code><code class="w"/>
<code class="go">[2, 396, 582, 219, 3356]</code></pre>

<p>Perfect! Now let’s create a small utility class that will hold tokenized English texts (i.e., the <em>source</em> token ID sequences), along with the corresponding tokenized Spanish targets (i.e., the <em>target</em> token ID sequences), plus the corresponding attention masks<a data-type="indexterm" data-primary="attention_mask attribute" id="id3362"/>. For this, we can create a <code translate="no">namedtuple</code> base class (i.e., a tuple with named fields), and extend it to add a <code translate="no">to()</code> method, which will make it easy to move all these tensors to the GPU:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">namedtuple</code>

<code class="n">fields</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"src_token_ids"</code><code class="p">,</code> <code class="s2">"src_mask"</code><code class="p">,</code> <code class="s2">"tgt_token_ids"</code><code class="p">,</code> <code class="s2">"tgt_mask"</code><code class="p">]</code>
<code class="k">class</code> <code class="nc">NmtPair</code><code class="p">(</code><code class="n">namedtuple</code><code class="p">(</code><code class="s2">"NmtPairBase"</code><code class="p">,</code> <code class="n">fields</code><code class="p">)):</code>
    <code class="k">def</code> <code class="nf">to</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">device</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">NmtPair</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">src_token_ids</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code> <code class="bp">self</code><code class="o">.</code><code class="n">src_mask</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code>
                       <code class="bp">self</code><code class="o">.</code><code class="n">tgt_token_ids</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code> <code class="bp">self</code><code class="o">.</code><code class="n">tgt_mask</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">))</code></pre>

<p>Next, let’s create the data loaders:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">nmt_collate_fn</code><code class="p">(</code><code class="n">batch</code><code class="p">):</code>
    <code class="n">src_texts</code> <code class="o">=</code> <code class="p">[</code><code class="n">pair</code><code class="p">[</code><code class="s1">'source_text'</code><code class="p">]</code> <code class="k">for</code> <code class="n">pair</code> <code class="ow">in</code> <code class="n">batch</code><code class="p">]</code>
    <code class="n">tgt_texts</code> <code class="o">=</code> <code class="p">[</code><code class="sa">f</code><code class="s2">"&lt;s&gt; </code><code class="si">{</code><code class="n">pair</code><code class="p">[</code><code class="s1">'target_text'</code><code class="p">]</code><code class="si">}</code><code class="s2"> &lt;/s&gt;"</code> <code class="k">for</code> <code class="n">pair</code> <code class="ow">in</code> <code class="n">batch</code><code class="p">]</code>
    <code class="n">src_encodings</code> <code class="o">=</code> <code class="n">nmt_tokenizer</code><code class="o">.</code><code class="n">encode_batch</code><code class="p">(</code><code class="n">src_texts</code><code class="p">)</code>
    <code class="n">tgt_encodings</code> <code class="o">=</code> <code class="n">nmt_tokenizer</code><code class="o">.</code><code class="n">encode_batch</code><code class="p">(</code><code class="n">tgt_texts</code><code class="p">)</code>
    <code class="n">src_token_ids</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="n">enc</code><code class="o">.</code><code class="n">ids</code> <code class="k">for</code> <code class="n">enc</code> <code class="ow">in</code> <code class="n">src_encodings</code><code class="p">])</code>
    <code class="n">tgt_token_ids</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="n">enc</code><code class="o">.</code><code class="n">ids</code> <code class="k">for</code> <code class="n">enc</code> <code class="ow">in</code> <code class="n">tgt_encodings</code><code class="p">])</code>
    <code class="n">src_mask</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="n">enc</code><code class="o">.</code><code class="n">attention_mask</code> <code class="k">for</code> <code class="n">enc</code> <code class="ow">in</code> <code class="n">src_encodings</code><code class="p">])</code>
    <code class="n">tgt_mask</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="n">enc</code><code class="o">.</code><code class="n">attention_mask</code> <code class="k">for</code> <code class="n">enc</code> <code class="ow">in</code> <code class="n">tgt_encodings</code><code class="p">])</code>
    <code class="n">inputs</code> <code class="o">=</code> <code class="n">NmtPair</code><code class="p">(</code><code class="n">src_token_ids</code><code class="p">,</code> <code class="n">src_mask</code><code class="p">,</code>
                     <code class="n">tgt_token_ids</code><code class="p">[:,</code> <code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">],</code> <code class="n">tgt_mask</code><code class="p">[:,</code> <code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code>
    <code class="n">labels</code> <code class="o">=</code> <code class="n">tgt_token_ids</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">:]</code>
    <code class="k">return</code> <code class="n">inputs</code><code class="p">,</code> <code class="n">labels</code>

<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>
<code class="n">nmt_train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">nmt_train_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code>
                              <code class="n">collate_fn</code><code class="o">=</code><code class="n">nmt_collate_fn</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">nmt_valid_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">nmt_valid_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code>
                              <code class="n">collate_fn</code><code class="o">=</code><code class="n">nmt_collate_fn</code><code class="p">)</code>
<code class="n">nmt_test_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">nmt_test_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code>
                             <code class="n">collate_fn</code><code class="o">=</code><code class="n">nmt_collate_fn</code><code class="p">)</code></pre>

<p>The <code translate="no">nmt_collate_fn()</code> function starts by extracting all the English and Spanish texts from the given batch. In the process, it also adds an SoS token at the start of each Spanish text, as well as an EoS token at the end. It then tokenizes both the English and Spanish texts using our BPE tokenizer. Next, the input sequences and the attention masks are converted to tensors and wrapped<a data-type="indexterm" data-primary="torch" data-secondary="nn.CrossEntropyLoss" id="id3363"/> in an <code translate="no">NmtPair</code>. Importantly, the function drops the EoS token from the decoder inputs, and drops the SoS token from the decoder targets. For example, the inputs may contain the token IDs for “&lt;s&gt; Me gusta el fútbol”, while the targets may contain the token IDs for “Me gusta el fútbol &lt;/s&gt;”. Lastly, the function returns the inputs (i.e., the <code translate="no">NmtPair</code>) along with the targets. Then we just create the data loaders as usual.</p>

<p>And now we are ready to build our translation model. It’s just like <a data-type="xref" href="#machine_translation_diagram">Figure 14-5</a>, except the encoder and decoder share the same <code translate="no">nn.Embedding</code> layer, and the encoder and decoder <code translate="no">nn.GRU</code> modules<a data-type="indexterm" data-primary="GRU (gated recurrent unit) cell" id="id3364"/><a data-type="indexterm" data-primary="gated recurrent unit (GRU) cell" id="id3365"/> contain two layers each:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">NmtModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">,</code> <code class="n">embed_dim</code><code class="o">=</code><code class="mi">512</code><code class="p">,</code> <code class="n">pad_id</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="o">=</code><code class="mi">512</code><code class="p">,</code>
                 <code class="n">n_layers</code><code class="o">=</code><code class="mi">2</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">embed</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">,</code> <code class="n">padding_idx</code><code class="o">=</code><code class="n">pad_id</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">,</code> <code class="n">num_layers</code><code class="o">=</code><code class="n">n_layers</code><code class="p">,</code>
                              <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">decoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">,</code> <code class="n">num_layers</code><code class="o">=</code><code class="n">n_layers</code><code class="p">,</code>
                              <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">pair</code><code class="p">):</code>
        <code class="n">src_embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">embed</code><code class="p">(</code><code class="n">pair</code><code class="o">.</code><code class="n">src_token_ids</code><code class="p">)</code>
        <code class="n">tgt_embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">embed</code><code class="p">(</code><code class="n">pair</code><code class="o">.</code><code class="n">tgt_token_ids</code><code class="p">)</code>
        <code class="n">src_lengths</code> <code class="o">=</code> <code class="n">pair</code><code class="o">.</code><code class="n">src_mask</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">src_packed</code> <code class="o">=</code> <code class="n">pack_padded_sequence</code><code class="p">(</code>
            <code class="n">src_embeddings</code><code class="p">,</code> <code class="n">lengths</code><code class="o">=</code><code class="n">src_lengths</code><code class="o">.</code><code class="n">cpu</code><code class="p">(),</code>
            <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">enforce_sorted</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
        <code class="n">_</code><code class="p">,</code> <code class="n">hidden_states</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code><code class="p">(</code><code class="n">src_packed</code><code class="p">)</code>
        <code class="n">outputs</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">decoder</code><code class="p">(</code><code class="n">tgt_embeddings</code><code class="p">,</code> <code class="n">hidden_states</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">outputs</code><code class="p">)</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">vocab_size</code> <code class="o">=</code> <code class="n">nmt_tokenizer</code><code class="o">.</code><code class="n">get_vocab_size</code><code class="p">()</code>
<code class="n">nmt_model</code> <code class="o">=</code> <code class="n">NmtModel</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>Almost everything in this model should look familiar: it’s very similar to our previous models. We create the modules in the constructor, then the <code translate="no">forward()</code> method embeds the input sequences (both English and Spanish), it packs the English embeddings and passes them through the encoder, then it passes the Spanish embeddings to the decoder, along with the encoder’s last hidden states (across all <code translate="no">nn.GRU</code> layers). Lastly, the decoder’s outputs are passed through the output <code translate="no">nn.Linear</code> layer, and the final outputs are permuted to ensure that the class dimension (containing the token logits) is the second dimension, since this is expected by the <code translate="no">nn.CrossEntropyLoss</code> and the <code translate="no">Accuracy</code> metric<a data-type="indexterm" data-primary="accuracy performance measure" id="id3366"/><a data-type="indexterm" data-primary="torch" data-secondary="nn.CrossEntropyLoss" id="id3367"/>, as we saw earlier.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The most common metric used in NMT is the <em>bilingual evaluation understudy</em> (BLEU) score<a data-type="indexterm" data-primary="bilingual evaluation understudy (BLEU) score" id="id3368"/><a data-type="indexterm" data-primary="BLEU (bilingual evaluation understudy) score" id="id3369"/>, which compares each translation produced by the model with several good translations produced by humans. It counts the number of <em>n</em>-grams (sequences of <em>n</em> words) that appear in any of the target translations and adjusts the score to take into account the frequency of the produced <em>n</em>-grams in the target translations. It is implemented by TorchMetric’s <code translate="no">BLEUScore</code> class.</p>
</div>

<p>We could have packed the Spanish embeddings, but then the decoder’s outputs would have been packed sequences, which we would have had to pad before we passed them to the output layer. We avoided this complexity because we can just configure the loss to ignore the output tokens when the targets are padding tokens, like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">xentropy</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">CrossEntropyLoss</code><code class="p">(</code><code class="n">ignore_index</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>  <code class="c1"># ignore &lt;pad&gt; tokens</code></pre>

<p>Now you can train this model (e.g., for 10 epochs using a <code translate="no">Nadam</code> optimizer with <code translate="no">lr = 0.001</code>), and it will take quite a while. It’s actually not that long when you consider the fact that the model is learning two languages at once!</p>

<p>While it’s training, let’s write a little helper function to translate some English text to Spanish using our model. It will start by calling the model with the English text for the encoder, and a single SoS token for the decoder. The decoder will just output logits for the first token in the translation. Our function will then pick the most likely token (i.e., with the highest logit) and add it to the decoder inputs, then it will call the model again to get the next token. It will repeat this process, adding one token at a time, until the model outputs an EoS token:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">translate</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">src_text</code><code class="p">,</code> <code class="n">max_length</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="n">pad_id</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">eos_id</code><code class="o">=</code><code class="mi">3</code><code class="p">):</code>
    <code class="n">tgt_text</code> <code class="o">=</code> <code class="s2">""</code>
    <code class="n">token_ids</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">index</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">max_length</code><code class="p">):</code>
        <code class="n">batch</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">nmt_collate_fn</code><code class="p">([{</code><code class="s2">"source_text"</code><code class="p">:</code> <code class="n">src_text</code><code class="p">,</code>
                                    <code class="s2">"target_text"</code><code class="p">:</code> <code class="n">tgt_text</code><code class="p">}])</code>
        <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
            <code class="n">Y_logits</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">batch</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">))</code>
            <code class="n">Y_token_ids</code> <code class="o">=</code> <code class="n">Y_logits</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># find the best token IDs</code>
            <code class="n">next_token_id</code> <code class="o">=</code> <code class="n">Y_token_ids</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="n">index</code><code class="p">]</code>  <code class="c1"># take the last token ID</code>

        <code class="n">next_token</code> <code class="o">=</code> <code class="n">nmt_tokenizer</code><code class="o">.</code><code class="n">id_to_token</code><code class="p">(</code><code class="n">next_token_id</code><code class="p">)</code>
        <code class="n">tgt_text</code> <code class="o">+=</code> <code class="s2">" "</code> <code class="o">+</code> <code class="n">next_token</code>
        <code class="k">if</code> <code class="n">next_token_id</code> <code class="o">==</code> <code class="n">eos_id</code><code class="p">:</code>
            <code class="k">break</code>
    <code class="k">return</code> <code class="n">tgt_text</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>This implementation works but it’s not optimized at all. We could run the encoder just once on the English text, and we could also run the decoder just once per time step, instead of running it over the whole growing text at each iteration.</p>
</div>

<p>Let’s try translating some text!</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">nmt_model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">translate</code><code class="p">(</code><code class="n">nmt_model</code><code class="p">,</code> <code class="s2">"I like soccer."</code><code class="p">)</code><code class="w"/>
<code class="go">' Me gusta el fútbol . &lt;/s&gt;'</code></pre>

<p>Hurray, it works! We just built a model from scratch that can translate English to Spanish.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id3370">
<h1>Model Optimizations</h1>
<p>When the output vocabulary<a data-type="indexterm" data-primary="model optimizations" id="id3371"/><a data-type="indexterm" data-primary="optimizations, model" id="id3372"/> is large (e.g., 10,000 tokens or more), computing<a data-type="indexterm" data-primary="torch" data-secondary="nn.CrossEntropyLoss" id="id3373"/> the <code translate="no">nn.CrossEntropyLoss</code> can be quite slow, depending on the hardware. To speed things up, one technique<a data-type="indexterm" data-primary="sampled softmax" id="id3374"/><a data-type="indexterm" data-primary="softmax activation function" data-secondary="NMT encoder-decoder network" id="id3375"/> is to use <em>sampled softmax</em>, <a href="https://homl.info/sampledsoftmax">introduced in 2015</a> by Sébastien Jean et al.⁠<sup><a data-type="noteref" id="id3376-marker" href="ch14.html#id3376">17</a></sup> Instead of computing the softmax over all of the logits, it computes an approximation based on the correct class’s logit, as well as a random sample of logits for other classes. This technique requires knowing the target, so it is only useful during training. Moreover, it is not included in PyTorch, so you have to implement it yourself.</p>

<p>Another technique<a data-type="indexterm" data-primary="adaptive softmax" id="id3377"/> is <em>adaptive softmax</em>, <a href="https://homl.info/adaptivesoftmax">introduced in 2016</a> by Edouard Grave et al.,⁠<sup><a data-type="noteref" id="id3378-marker" href="ch14.html#id3378">18</a></sup> which speeds up softmax computation by splitting the vocabulary into frequency-based clusters. Frequent tokens are processed normally, while less frequent tokens are placed in progressively larger clusters, reducing computation by only accessing the necessary clusters. This speeds up computations both during training and inference. PyTorch implements this algorithm in the <code translate="no">nn.AdaptiveLogSoftmaxWithLoss</code> class<a data-type="indexterm" data-primary="torch" data-secondary="nn.AdaptiveLogSoftmaxWithLoss" id="id3379"/>.</p>

<p>Another thing you can do to speed up training (and also save a lot of memory) is to use the embedding matrix<a data-type="indexterm" data-primary="embedding matrix" id="id3380"/> as the weights of the output layer. This is called <em>tying the weights</em> of these two layers<a data-type="indexterm" data-primary="tying weights of layers" id="id3381"/><a data-type="indexterm" data-primary="weight-tying" id="id3382"/>, and it was first proposed in a <a href="https://homl.info/tieweights">2016 paper by Ofir Press and Lior Wolf</a>.⁠<sup><a data-type="noteref" id="id3383-marker" href="ch14.html#id3383">19</a></sup> To implement it, just add <code translate="no">self.output.weight = self.embed.weight</code> to the model’s constructor. You can also get rid of the output layer’s <code translate="no">bias</code> parameter, by setting <code translate="no">bias=False</code> when creating the output layer. Tying the weights significantly reduces the number of model parameters, which speeds up training and may sometimes improve the model’s accuracy as well, especially if you don’t have a lot of training data. Why does this work? Well, as we saw earlier, the embedding matrix is equivalent to one-hot encoding followed by a linear layer with no bias term and no activation function that maps the one-hot vectors to the embedding space. The output layer does the reverse. So if the model can find an embedding matrix whose transpose is equal to its inverse (such a matrix is called<a data-type="indexterm" data-primary="orthogonal matrices" id="id3384"/> an <em>orthogonal matrix</em>), then there’s no need to learn a separate set of weights for the output layer.</p>
</div></aside>

<p>If you play around with our translation model, you will find that it often works reasonably well on short text, but it really struggles with longer sentences. For example:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">longer_text</code> <code class="o">=</code> <code class="s2">"I like to play soccer with my friends."</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">translate</code><code class="p">(</code><code class="n">nmt_model</code><code class="p">,</code> <code class="n">longer_text</code><code class="p">)</code><code class="w"/>
<code class="go">' Me gusta jugar con mis amigos . &lt;/s&gt;'</code></pre>

<p>The translation says “I like to play with my friends”. Oops, there’s no mention of soccer. So how can we improve this model? One way is to increase the training set size and add more <code translate="no">nn.GRU</code> layers in both the encoder and the decoder. You could also make the encoder bidirectional (but not the decoder, or else it would no longer be causal and it would see the full translation at each time step, instead of just the previous tokens)<a data-type="indexterm" data-startref="xi_naturallanguageprocessingNLPencoderdecodernetworkforNMT1499112_1" id="id3385"/><a data-type="indexterm" data-startref="xi_encoderdecodermodelsNMTnetwork1499112_1" id="id3386"/><a data-type="indexterm" data-startref="xi_neuralmachinetranslationNMTencoderdecodernetworkfor1499112_1" id="id3387"/>. Another popular technique that can greatly improve the performance of a translation model at inference time is <em>beam search</em>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Beam Search"><div class="sect1" id="id277">
<h1>Beam Search</h1>

<p>To translate<a data-type="indexterm" data-primary="neural machine translation (NMT)" data-secondary="and beam search" data-secondary-sortas="beam search" id="xi_neuralmachinetranslationNMTandbeamsearch14120913_1"/><a data-type="indexterm" data-primary="beam search" id="xi_beamsearch14120913_1"/><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="beam search" id="xi_naturallanguageprocessingNLPbeamsearch14120913_1"/> an English text to Spanish, we call our model several times, producing one word at a time. Unfortunately, this means that when the model makes one mistake, it is stuck with it for the rest of the translation, which can cause more errors, making the translation worse and worse. For example, suppose we want to translate “I like soccer”, and the model correctly starts with “Me”, but then predicts “gustan” (plural) instead of “gusta” (singular). This mistake is understandable, since “Me gustan” is the correct way to start translating “I like” in many cases. Once the model has made this mistake, it is stuck with “gustan”. It then reasonably adds “los”, which is the plural for “the”. But since the model never saw “los fútbol” in the training data (soccer is singular, not plural), the model tries to find something reasonable to add, and given the context it adds “jugadores”, which means “the players”. So “I like soccer” gets translated to “I like the players”. One error caused a chain of errors.</p>

<p>How can we give the model a chance to go back and fix mistakes it made earlier? One of the most common solutions is <em>beam search</em>: it keeps track of a short list of the <em>k</em> most promising output sequences (say, the top three), and at each decoder step it tries to extend each of them by one word, keeping only the <em>k</em> most likely sequences. The parameter <em>k</em> is called<a data-type="indexterm" data-primary="beam width" id="id3388"/> the <em>beam width</em>.</p>

<p>For example, suppose you use the model to translate the sentence “I like soccer” using beam search with a beam width of three (see <a data-type="xref" href="#beam_search_diagram">Figure 14-7</a>). At the first decoder step, the model will output an estimated probability for each possible first word in the translated sentence. Suppose the top three words are “Me” (75% estimated probability), “a” (3%), and “como” (1%). That’s our short list so far. Next, we use the model to find the next word for each sentence. For the first sentence (“Me”), perhaps the model outputs a probability of 36% for the word “gustan”, 32% for the word “gusta”, 16% for the word “encanta”, and so on. Note that these are actually <em>conditional</em> probabilities, given that the sentence starts with “Me”. For the second sentence (“a”), the model might output a conditional probability<a data-type="indexterm" data-primary="conditional probabilities" id="id3389"/> of 50% for the word “mi”, and so on. Assuming the vocabulary has 10,000 tokens, we will end up with 10,000 probabilities per sentence.</p>

<p>Next, we compute the probabilities of each of the 30,000 two-token sentences we considered (3 × 10,000). We do this by multiplying the estimated conditional probability of each word by the estimated probability of the sentence it completes. For example, the estimated probability of the sentence “Me” was 75%, while the estimated conditional probability of the word “gustan” (given that the first word is “Me”) was 36%, so the estimated probability of the sentence “Me gustan” is 75% × 36% = 27%. After computing the probabilities of all 30,000 two-word sentences, we keep only the top 3. In this example they all start with the word “Me”: “Me gustan” (27%), “Me gusta” (24%), and “Me encanta” (12%). Right now, the sentence “Me gustan” is winning, but “Me gusta” has not been eliminated.</p>

<figure><div id="beam_search_diagram" class="figure">
<img src="assets/hmls_1407.png" alt="Diagram illustrating a beam search process with a beam width of three, showing probabilities for Spanish sentence constructions starting with &quot;Me&quot; and progressing through different stages." width="1454" height="624"/>
<h6><span class="label">Figure 14-7. </span>Beam search, with a beam width of three</h6>
</div></figure>

<p>Then we repeat the same process: we use the model to predict the next word in each of these three sentences, and we compute the probabilities of all 30,000 three-word sentences we considered. Perhaps the top 3 are now “Me gustan los” (10%), “Me gusta el” (8%), and “Me gusta mucho” (2%). At the next step we may get “Me gusta el fútbol” (6%), “Me gusta mucho el” (1%), and “Me gusta el deporte” (0.2%). Notice that “Me gustan” was eliminated, and the correct translation is now ahead. We boosted our encoder-decoder model’s performance without any extra training, simply by using it more wisely.</p>

<p>The notebook for this chapter contains a very simple <code translate="no">beam_search()</code> function, if you’re interested, but in general you will probably want to use the implementation provided by the <code translate="no">GenerationMixin</code> class in the Transformers library<a data-type="indexterm" data-primary="Transformers library" data-secondary="GenerationMixin" id="id3390"/>. This is where the text generation models from the Transformers library get their <code translate="no">generate()</code> method: it accepts a <code translate="no">num_beams</code> argument which you can set to the desired beam width if you want to use beam search. It also provides a <code translate="no">do_sample</code> argument that will randomly sample the next token using the probability distribution output by the model, just like we did earlier with our char-RNN model. Other generation strategies are also supported and can be combined (see <a href="https://homl.info/hfgen" class="bare"><em class="hyperlink">https://homl.info/hfgen</em></a> for more details).</p>

<p>With all this, you can get reasonably good translations for fairly short sentences. For example, the following translation is correct:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">beam_search</code><code class="p">(</code><code class="n">nmt_model</code><code class="p">,</code> <code class="n">longer_text</code><code class="p">,</code> <code class="n">beam_width</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code><code class="w"/>
<code class="go">' Me gusta jugar al fútbol con mis amigos . &lt;/s&gt;'</code></pre>

<p>Unfortunately, this model will still be pretty bad at translating long sentences:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">longest_text</code> <code class="o">=</code> <code class="s2">"I like to play soccer with my friends at the beach."</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">beam_search</code><code class="p">(</code><code class="n">nmt_model</code><code class="p">,</code> <code class="n">longest_text</code><code class="p">,</code> <code class="n">beam_width</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code><code class="w"/>
<code class="go">' Me gusta jugar con jugar con los jug adores de la playa . &lt;/s&gt;'</code></pre>

<p>This translates to “I like to play with play with the players of the beach”. That’s not quite right. Once again, the problem comes from the limited short-term memory of RNNs<a data-type="indexterm" data-startref="xi_neuralmachinetranslationNMTandbeamsearch14120913_1" id="id3391"/><a data-type="indexterm" data-startref="xi_beamsearch14120913_1" id="id3392"/><a data-type="indexterm" data-startref="xi_naturallanguageprocessingNLPbeamsearch14120913_1" id="id3393"/>. <em>Attention mechanisms</em> are the game-changing innovation that addressed this problem.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Attention Mechanisms"><div class="sect1" id="id278">
<h1>Attention Mechanisms</h1>

<p>Consider the path from the word “soccer” to its translation<a data-type="indexterm" data-primary="neural machine translation (NMT)" data-secondary="and attention mechanisms" data-secondary-sortas="attention mechanisms" id="xi_neuralmachinetranslationNMTandattentionmechanisms14124560_1"/><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="attention mechanisms" id="xi_naturallanguageprocessingNLPattentionmechanisms14124560_1"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="and NMT" id="xi_attentionmechanismsandNMT14124560_1"/> “fútbol” back in <a data-type="xref" href="#machine_translation_diagram">Figure 14-5</a>: it is quite long! This means that a representation of this word (along with all the other words) needs to be carried over many steps before it is actually used. Can’t we make this path shorter?</p>

<p>This was the core idea in a landmark <a href="https://homl.info/attention">2014 paper</a>⁠<sup><a data-type="noteref" id="id3394-marker" href="ch14.html#id3394">20</a></sup> by Dzmitry Bahdanau et al., where the authors introduced a technique that allowed the decoder to focus on the appropriate words (as encoded by the encoder) at each time step. For example, at the time step where the decoder needs to output the word “fútbol”, it will focus its attention on the word “soccer”. This means that the path from an input word to its translation is now much shorter, so the short-term memory limitations of RNNs have much less impact. Attention mechanisms revolutionized neural machine translation (and deep learning in general), allowing a significant improvement in the state of the art, especially for long sentences (e.g., over 30 words).</p>

<p><a data-type="xref" href="#attention_diagram">Figure 14-8</a> shows our encoder-decoder model with an added attention mechanism:</p>

<ul>
<li>
<p>On the left, you have the encoder and the decoder (I’ve made the encoder bidirectional in this figure, as it’s generally a good idea).</p>
</li>
<li>
<p>Instead of sending the encoder’s final hidden state to the decoder, as well as the previous target word at each time step (which is still done, but it is not shown in the figure), we now send all of the encoder’s outputs to the decoder as well.</p>
</li>
<li>
<p>Since the decoder cannot deal with all these encoder outputs at once, they need to be aggregated: at each time step, the decoder’s memory cell computes a weighted sum of all the encoder outputs. This determines which words the decoder will focus on at this step.</p>
</li>
<li>
<p>The weight <em>α</em><sub>(<em>t</em>,<em>i</em>)</sub> is the weight of the <em>i</em><sup>th</sup> encoder output at the <em>t</em><sup>th</sup> decoder time step. For example, if the weight <em>α</em><sub>(3,2)</sub> is much larger than the weights <em>α</em><sub>(3,0)</sub> and <em>α</em><sub>(3,1)</sub>, then the decoder will pay much more attention to the encoder’s output for word #2 (“soccer”) than to the other two outputs, at least at this time step.</p>
</li>
<li>
<p>The rest of the decoder works just like earlier: at each time step the memory cell receives the inputs we just discussed, plus the hidden state from the previous time step, and finally (although it is not represented in the diagram) it receives the target word from the previous time step (or at inference time, the output from the previous time step).</p>
</li>
</ul>

<figure><div id="attention_diagram" class="figure">
<img src="assets/hmls_1408.png" alt="Diagram illustrating a neural machine translation model using an encoder-decoder network with attention, showing how encoder outputs are weighted and aggregated by the alignment model." width="1247" height="1100"/>
<h6><span class="label">Figure 14-8. </span>Neural machine translation using an encoder-decoder network with an attention model</h6>
</div></figure>

<p>But where do these <em>α</em><sub>(<em>t</em>,<em>i</em>)</sub> weights come from? Well, they are generated by a small neural network<a data-type="indexterm" data-primary="alignment model" id="id3395"/> called an <em>alignment model</em> (or an <em>attention layer</em>), which is trained jointly with the rest of the encoder-decoder model. This alignment model is illustrated on the righthand side of <a data-type="xref" href="#attention_diagram">Figure 14-8</a>:</p>

<ul>
<li>
<p>It starts with a dense layer (i.e., <code translate="no">nn.Linear</code>) that takes as input each of the encoder’s outputs, along with the decoder’s previous hidden state (e.g., <strong>h</strong><sub>(2)</sub>), and outputs a score (or energy) for each encoder output (e.g., <em>e</em><sub>(3,</sub> <sub>2)</sub>). This score measures how well each encoder output is aligned with the decoder’s previous hidden state.</p>

<p>For example, in <a data-type="xref" href="#attention_diagram">Figure 14-8</a>, the model has already output “me gusta el” (meaning “I like”), so it’s now expecting a noun. The word “soccer” is the one that best aligns with the current state, so it gets a high score.</p>
</li>
<li>
<p>Finally, all the scores go through a softmax layer to get a final weight for each encoder output (e.g., <em>α</em><sub>(3,2)</sub>). All the weights for a given decoder time step add up to 1.</p>
</li>
</ul>

<p>This particular attention mechanism<a data-type="indexterm" data-primary="Bahdanau attention" id="id3396"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="Bahdanau attention" id="id3397"/> is called <em>Bahdanau attention</em> (named after the 2014 paper’s first author). Since it concatenates the encoder output with the decoder’s previous hidden state, it is sometimes<a data-type="indexterm" data-primary="concatenative attention" id="id3398"/><a data-type="indexterm" data-primary="additive attention" id="id3399"/> called <em>concatenative attention</em> (or <em>additive attention</em>).</p>

<p>In short, the attention mechanism provides a way to focus the attention of the model on part of the inputs. That said, there’s another way to think of this whole process: it acts as a differentiable memory retrieval mechanism. For example, let’s suppose the encoder analyzed the input sentence “I like soccer”, and it managed to understand that the word “I” is the subject, the word “like” is the verb, and the word “soccer” is the noun, so it encoded this information in its outputs for these words. Now suppose the decoder has already translated “I like”, and it thinks that it should translate the noun next. For this, it needs to fetch the noun from the input sentence. This is analogous to a dictionary lookup: it’s as if the encoder had created a dictionary {"subject”: “I”, “verb”: “like”, “noun”: “soccer"} and the decoder wanted to look up the value that corresponds to the key “noun”.</p>

<p>However, the model does not have discrete tokens to represent the keys (like “subject”, “verb”, or “noun”); instead, it has vectorized representations of these concepts that it learned during training, so the query it will use for the lookup will not perfectly match any key in the dictionary. One solution is to compute a similarity measure between the query and each key in the dictionary, and then use the softmax function<a data-type="indexterm" data-primary="softmax activation function" data-secondary="NMT encoder-decoder network" id="id3400"/> to convert these similarity scores to weights that add up to 1. As we just saw, that’s exactly what the attention layer does. If the key that represents the noun is by far the most similar to the query, then that key’s weight will be close to 1. Next, the attention layer computes a weighted sum of the corresponding values: if the weight of the “noun” key is close to 1, then the weighted sum will be very close to the representation of the word “soccer”. In short, the decoder queried for a noun and the attention mechanism retrieved it.</p>

<p>In most modern implementations of attention mechanisms, the arguments are named <code translate="no">query</code>, <code translate="no">key</code>, and <code translate="no">value</code>. In our example, the query is the decoder’s hidden states, the key is the encoder’s outputs (this is used to compute the weights), and the value is also the encoder’s outputs (this is used to compute the final weighted sum).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If the input sentence is <em>n</em> words long, and assuming the output sentence is about as long, then the attention mechanism will need to compute about <em>n</em><sup>2</sup> weights. This quadratic computational complexity becomes untractable when the sentences are too long.</p>
</div>

<p>Another common attention mechanism<a data-type="indexterm" data-primary="Luong attention" id="xi_Luongattention14127835_1"/><a data-type="indexterm" data-primary="multiplicative attention" id="id3401"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="Luong attention" id="xi_attentionmechanismsLuongattention14127835_1"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="multiplicative attention" id="id3402"/>, known as <em>Luong attention</em> or <em>multiplicative attention</em>, was proposed shortly after, in <a href="https://homl.info/luongattention">2015</a>,⁠<sup><a data-type="noteref" id="id3403-marker" href="ch14.html#id3403">21</a></sup> by Minh-Thang Luong et al. Because the goal of the alignment model is to measure the similarity between one of the encoder’s outputs and the decoder’s previous hidden state, the authors proposed to simply compute the dot product (see <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a>) of these two vectors, as this is often a fairly good similarity measure, and modern hardware can compute it very efficiently. For this to be possible, both vectors must have the same dimensionality. The dot product<a data-type="indexterm" data-primary="dot product" id="id3404"/> gives a score, and all the scores (at a given decoder time step) go through a softmax layer to give the final weights, just like in Bahdanau attention.</p>

<p>Luong et al. also proposed to use the decoder’s hidden state at the current time step rather than at the previous time step (i.e., <strong>h</strong><sub>(<em>t</em>)</sub> rather than <strong>h</strong><sub>(<em>t</em>–1)</sub>) to compute the attention vector (denoted <math><msub><mover><mi mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></math>). This attention vector is then concatenated with the decoder’s hidden state to form an attentional hidden state, which is then used to predict the next token. This simplifies and speeds up the process by allowing the encoder and decoder to operate independently before attention is applied, rather than interweaving attention into the decoder’s recurrence.</p>

<p>The researchers also proposed a variant of the dot product mechanism where the encoder outputs first go through a fully connected layer (without a bias term) before the dot products are computed. This is called the “general” dot product approach. The researchers compared both dot product approaches with the concatenative attention mechanism (adding a rescaling parameter vector <strong>v</strong>), and they observed that the dot product variants performed better than concatenative attention. For this reason, concatenative attention is much less used now. The equations for these three attention mechanisms are summarized in <a data-type="xref" href="#attention_mechanisms_equation">Equation 14-2</a>.</p>
<div data-type="equation" id="attention_mechanisms_equation">
<h5><span class="label">Equation 14-2. </span>Attention mechanisms</h5>
<math display="block"><msub><mover><mi mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo lspace="0%" rspace="0%">,</mo><mi>i</mi><mo>)</mo></mrow></msub><msub><mover><mi mathvariant="bold">y</mi><mo stretchy="false" style="math-style:normal;math-depth:0;">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mspace linebreak="newline"/><mtext> with </mtext><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo lspace="0%" rspace="0%">,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo lspace="0%" rspace="0%">,</mo><mi>i</mi><mo>)</mo></mrow></msub></mfenced></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>'</mo></mrow></munder></mstyle><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo lspace="0%" rspace="0%">,</mo><mi>i</mi><mo>'</mo><mo>)</mo></mrow></msub></mfenced></mrow></mfrac><mspace linebreak="newline"/><mtext> and </mtext><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo lspace="0%" rspace="0%">,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfenced open="{" close=""><mtable columnalign="left"><mtr><mtd><msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><mo> </mo><msub><mover><mi mathvariant="bold">y</mi><mo stretchy="false" style="math-style:normal;math-depth:0;">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>d</mi><mi>o</mi><mi>t</mi></mtd></mtr><mtr><mtd><msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><mo> </mo><mi mathvariant="bold">W</mi><mo> </mo><msub><mover><mi mathvariant="bold">y</mi><mo stretchy="false" style="math-style:normal;math-depth:0;">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>g</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>l</mi></mtd></mtr><mtr><mtd><msup><mi mathvariant="bold">v</mi><mo>⊺</mo></msup><mi>tanh</mi><mo>(</mo><mi mathvariant="bold">W</mi><mo>[</mo><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>;</mo><msub><mover><mi mathvariant="bold">y</mi><mo stretchy="false" style="math-style:normal;math-depth:0;">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>]</mo><mo>)</mo></mtd><mtd><mi>c</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi></mtd></mtr></mtable></mfenced></math>
</div>

<p>Let’s add Luong attention to our encoder-decoder model. Since PyTorch does not include a Luong attention function, we need to write our own. Luckily, it’s pretty short:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">attention</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">key</code><code class="p">,</code> <code class="n">value</code><code class="p">):</code>  <code class="c1"># note: dq == dk and Lk == Lv</code>
    <code class="n">scores</code> <code class="o">=</code> <code class="n">query</code> <code class="o">@</code> <code class="n">key</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>  <code class="c1"># [B,Lq,dq] @ [B,dk,Lk] = [B, Lq, Lk]</code>
    <code class="n">weights</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">scores</code><code class="p">,</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># [B, Lq, Lk]</code>
    <code class="k">return</code> <code class="n">weights</code> <code class="o">@</code> <code class="n">value</code>  <code class="c1"># [B, Lq, Lk] @ [B, Lv, dv] = [B, Lq, dv]</code></pre>

<p>Just like in <a data-type="xref" href="#attention_mechanisms_equation">Equation 14-2</a>, we first compute the attention scores, then we convert them to attention weights using the softmax function<a data-type="indexterm" data-primary="softmax activation function" data-secondary="NMT encoder-decoder network" id="id3405"/>, and lastly we compute the attention output by multiplying the attention weights with the value (i.e., the encoder outputs). This implementation efficiently runs all these computations for the whole batch at once. The <code translate="no">query</code> argument corresponds to <strong>h</strong><sub>(<em>t</em>)</sub> in <a data-type="xref" href="#attention_mechanisms_equation">Equation 14-2</a> (i.e., the decoder’s hidden states), and the <code translate="no">key</code> argument corresponds to <math alttext="ModifyingAbove bold y With caret Subscript left-parenthesis i right-parenthesis">
  <msub><mover accent="true"><mi>𝐲</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msub>
</math> (i.e., the encoder’s outputs), but only for the computation of the attention scores. The <code translate="no">value</code> argument also corresponds to <math alttext="ModifyingAbove bold y With caret Subscript left-parenthesis i right-parenthesis">
  <msub><mover accent="true"><mi>𝐲</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msub>
</math>, but only for the final computation of the weighted sum. The <code translate="no">key</code> and <code translate="no">value</code> arguments are generally identical, but there are a few scenarios where they can differ (e.g., some models use compressed keys to save memory and speed up the score computation). The shapes are shown in the comments: <code translate="no">B</code> is the batch size; <code translate="no">Lq</code> is the length of the longest query in the batch; <code translate="no">Lk</code> is the length of the longest key in the batch (note that each value must have the same length as its corresponding key); <code translate="no">dq</code> is the query’s embedding size<a data-type="indexterm" data-primary="embedding size" id="id3406"/>, which must be the same as the key’s embedding size <code translate="no">dk</code>; and <code translate="no">dv</code> is the value’s embedding size.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Since all arguments are 3D tensors, we could replace the <code translate="no">@</code> matrix multiplication operator<a data-type="indexterm" data-primary="batch matrix multiplication function" id="id3407"/><a data-type="indexterm" data-primary="torch" data-secondary="bmm() function" id="id3408"/> with the <em>batch matrix multiplication</em> function: <code translate="no">torch.bmm()</code>. This function only works with batches of matrices (i.e., 3D tensors), but it’s optimized for this use case so it runs faster. The result is the same: each matrix in the first tensor gets multiplied by the corresponding matrix in the second tensor.</p>
</div>

<p>Now let’s update our NMT model. The constructor needs just one modification—the output layer’s input size must be doubled, since we will concatenate the attention vectors to the decoder outputs:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">2</code> <code class="o">*</code> <code class="n">hidden_dim</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">)</code></pre>

<p>Next, let’s add attention to the <code translate="no">forward()</code> method:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">pair</code><code class="p">):</code>
    <code class="n">src_embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">embed</code><code class="p">(</code><code class="n">pair</code><code class="o">.</code><code class="n">src_token_ids</code><code class="p">)</code>  <code class="c1"># same as earlier</code>
    <code class="n">tgt_embeddings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">embed</code><code class="p">(</code><code class="n">pair</code><code class="o">.</code><code class="n">tgt_token_ids</code><code class="p">)</code>  <code class="c1"># same</code>
    <code class="n">src_lengths</code> <code class="o">=</code> <code class="n">pair</code><code class="o">.</code><code class="n">src_mask</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># same</code>
    <code class="n">src_packed</code> <code class="o">=</code> <code class="n">pack_padded_sequence</code><code class="p">(</code><code class="n">src_embeddings</code><code class="p">,</code> <code class="p">[</code><code class="o">...</code><code class="p">])</code>  <code class="c1"># same</code>
    <code class="n">encoder_outputs_packed</code><code class="p">,</code> <code class="n">hidden_states</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code><code class="p">(</code><code class="n">src_packed</code><code class="p">)</code>
    <code class="n">decoder_outputs</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">decoder</code><code class="p">(</code><code class="n">tgt_embeddings</code><code class="p">,</code> <code class="n">hidden_states</code><code class="p">)</code>  <code class="c1"># same</code>
    <code class="n">encoder_outputs</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">pad_packed_sequence</code><code class="p">(</code><code class="n">encoder_outputs_packed</code><code class="p">,</code>
                                                <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
    <code class="n">attn_output</code> <code class="o">=</code> <code class="n">attention</code><code class="p">(</code><code class="n">query</code><code class="o">=</code><code class="n">decoder_outputs</code><code class="p">,</code>
                            <code class="n">key</code><code class="o">=</code><code class="n">encoder_outputs</code><code class="p">,</code> <code class="n">value</code><code class="o">=</code><code class="n">encoder_outputs</code><code class="p">)</code>
    <code class="n">combined_output</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">((</code><code class="n">attn_output</code><code class="p">,</code> <code class="n">decoder_outputs</code><code class="p">),</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
    <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">combined_output</code><code class="p">)</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code></pre>

<p>Let’s go through this code:</p>

<ul>
<li>
<p>We compute the English and Spanish embeddings, the English sequence lengths, and we pack the English embeddings, just like earlier.</p>
</li>
<li>
<p>We then run the encoder like earlier, but we no longer ignore its outputs since we will need them for the attention function.</p>
</li>
<li>
<p>Next, we run the decoder, just like earlier.</p>
</li>
<li>
<p>Since the encoder’s inputs are represented as a packed sequence, its outputs are also represented as a packed sequence. Not many operations support packed sequences, so we must convert the encoder’s outputs to a padded tensor using the <code translate="no">pad_packed_sequence()</code> function.</p>
</li>
<li>
<p>And now we can call our <code translate="no">attention()</code> function. Note that we pass the decoder outputs instead of the hidden states because the decoder only returns the last hidden states. That’s OK because the <code translate="no">nn.GRU</code> layer’s outputs are equal to its top-layer hidden states.</p>
</li>
<li>
<p>Lastly, we concatenate the attention output and the decoder outputs along the last dimension, and we pass the result through the output layer. As earlier, we also permute the last two dimensions of the result<a data-type="indexterm" data-startref="xi_Luongattention14127835_1" id="id3409"/><a data-type="indexterm" data-startref="xi_attentionmechanismsLuongattention14127835_1" id="id3410"/>.</p>
</li>
</ul>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Our attention mechanism doesn’t ignore padding tokens. The model learns to ignore them during training, but it’s preferable to mask them entirely. We will see how in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>.</p>
</div>

<p>And that’s it! If you train this model, you will find that it now handles much longer sentences. For example:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">translate</code><code class="p">(</code><code class="n">nmt_attn_model</code><code class="p">,</code> <code class="n">longest_text</code><code class="p">)</code><code class="w"/>
<code class="go">' Me gusta jugar fu tbol con mis amigos en la playa . &lt;/s&gt;'</code></pre>

<p>Perfect! We didn’t even have to use beam search. In fact, attention mechanisms turned out to be so powerful that some Google researchers tried getting rid of recurrent layers altogether, only using feedforward layers and attention. Surprisingly, it worked like a charm. This led the researchers to name their paper “Attention is all you need”, introducing the Transformer architecture to the world. This was the start of a huge revolution in NLP and beyond. In the next chapter, we will explore the Transformer architecture and see how it revolutionized deep learning<a data-type="indexterm" data-startref="xi_neuralmachinetranslationNMTandattentionmechanisms14124560_1" id="id3411"/><a data-type="indexterm" data-startref="xi_naturallanguageprocessingNLPattentionmechanisms14124560_1" id="id3412"/><a data-type="indexterm" data-startref="xi_attentionmechanismsandNMT14124560_1" id="id3413"/>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id737">
<h1>Exercises</h1>
<ol>
<li>
<p>What are the pros and cons of using a stateful RNN versus a stateless RNN?</p>
</li>
<li>
<p>Why do people use encoder-decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?</p>
</li>
<li>
<p>How can you deal with variable-length input sequences? What about variable-length output sequences?</p>
</li>
<li>
<p>What is beam search, and why would you use it? What tool can you use to implement it?</p>
</li>
<li>
<p>What is an attention mechanism? How does it help?</p>
</li>
<li>
<p>When would you need to use sampled softmax?</p>
</li>
<li>
<p><em>Embedded Reber grammars</em> were used by Hochreiter and Schmidhuber in <a href="https://homl.info/93">their paper</a> about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE”. Check out Jenny Orr’s <a href="https://homl.info/108">nice introduction</a> to this topic, then choose a particular embedded Reber grammar (such as the one represented on Orr’s page), and train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t.</p>
</li>
<li>
<p>Train an encoder-decoder model that can convert a date string from one format to another (e.g., from “April 22, 2019” to “2019-04-22”).</p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id3189"><sup><a href="ch14.html#id3189-marker">1</a></sup> Alan Turing, “Computing Machinery and Intelligence”, <em>Mind</em> 49 (1950): 433–460.</p><p data-type="footnote" id="id3191"><sup><a href="ch14.html#id3191-marker">2</a></sup> Of course, the word <em>chatbot</em> came much later. Turing called his test the <em>imitation game</em>: machine A and human B chat with human interrogator C via text messages; the interrogator asks questions to figure out which one is the machine (A or B). The machine passes the test if it can fool the interrogator, while the human B must try to help the interrogator.</p><p data-type="footnote" id="id3210"><sup><a href="ch14.html#id3210-marker">3</a></sup> Tomáš Mikolov et al., “Distributed Representations of Words and Phrases and Their Compositionality”, <em>Proceedings of the 26th International Conference on Neural Information Processing Systems</em> 2 (2013): 3111–3119.</p><p data-type="footnote" id="id3211"><sup><a href="ch14.html#id3211-marker">4</a></sup> Malvina Nissim et al., “Fair Is Better Than Sensational: Man Is to Doctor as Woman Is to Doctor”, arXiv preprint arXiv:1905.09866 (2019).</p><p data-type="footnote" id="id3218"><sup><a href="ch14.html#id3218-marker">5</a></sup> It’s a convention in Python to name unused variables with an underscore prefix.</p><p data-type="footnote" id="id3232"><sup><a href="ch14.html#id3232-marker">6</a></sup> Another technique to capture longer patterns is to use a stateful RNN. It’s a bit more complex and not used as much, but if you’re interested I’ve included a section in this chapter’s notebook.</p><p data-type="footnote" id="id3233"><sup><a href="ch14.html#id3233-marker">7</a></sup> Alec Radford et al., “Learning to Generate Reviews and Discovering Sentiment”, arXiv preprint arXiv:1704.01444 (2017).</p><p data-type="footnote" id="id3241"><sup><a href="ch14.html#id3241-marker">8</a></sup> Rico Sennrich et al., “Neural Machine Translation of Rare Words with Subword Units”, <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</em> 1 (2016): 1715–1725.</p><p data-type="footnote" id="id3256"><sup><a href="ch14.html#id3256-marker">9</a></sup> Yonghui Wu et al., “Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation”, arXiv preprint arXiv:1609.08144 (2016).</p><p data-type="footnote" id="id3258"><sup><a href="ch14.html#id3258-marker">10</a></sup> Taku Kudo, “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates”, arXiv preprint arXiv:1804.10959 (2018).</p><p data-type="footnote" id="id3263"><sup><a href="ch14.html#id3263-marker">11</a></sup> Taku Kudo and John Richardson, “SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing”, arXiv preprint arXiv:1808.06226 (2018).</p><p data-type="footnote" id="id3281"><sup><a href="ch14.html#id3281-marker">12</a></sup> <em>Nested tensors</em> serve a similar purpose and are more convenient to use, but they are still in prototype stage at the time of writing. See <a href="https://pytorch.org/docs/stable/nested.html" class="bare"><em class="hyperlink">https://pytorch.org/docs/stable/nested.html</em></a> for more details.</p><p data-type="footnote" id="id3298"><sup><a href="ch14.html#id3298-marker">13</a></sup> Matthew Peters et al., “Deep Contextualized Word Representations”, <em>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em> 1 (2018): 2227–2237.</p><p data-type="footnote" id="id3303"><sup><a href="ch14.html#id3303-marker">14</a></sup> Jeremy Howard and Sebastian Ruder, “Universal Language Model Fine-Tuning for Text Classification”, <em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</em> 1 (2018): 328–339.</p><p data-type="footnote" id="id3354"><sup><a href="ch14.html#id3354-marker">15</a></sup> Ilya Sutskever et al., “Sequence to Sequence Learning with Neural Networks”, arXiv preprint, arXiv:1409.3215 (2014).</p><p data-type="footnote" id="id3359"><sup><a href="ch14.html#id3359-marker">16</a></sup> Samy Bengio et al., “Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks”, arXiv preprint arXiv:1506.03099 (2015).</p><p data-type="footnote" id="id3376"><sup><a href="ch14.html#id3376-marker">17</a></sup> Sébastien Jean et al., “On Using Very Large Target Vocabulary for Neural Machine Translation”, <em>Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</em> 1 (2015): 1–10.</p><p data-type="footnote" id="id3378"><sup><a href="ch14.html#id3378-marker">18</a></sup> Edouard Grave et al., “Efficient softmax approximation for GPUs”, arXiv preprint arXiv:1609.04309 (2016).</p><p data-type="footnote" id="id3383"><sup><a href="ch14.html#id3383-marker">19</a></sup> Ofir Press, Lior Wolf, “Using the Output Embedding to Improve Language Models”, arXiv preprint arXiv:1608.05859 (2016).</p><p data-type="footnote" id="id3394"><sup><a href="ch14.html#id3394-marker">20</a></sup> Dzmitry Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate”, arXiv preprint arXiv:1409.0473 (2014).</p><p data-type="footnote" id="id3403"><sup><a href="ch14.html#id3403-marker">21</a></sup> Minh-Thang Luong et al., “Effective Approaches to Attention-Based Neural Machine Translation”, <em>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</em> (2015): 1412–1421.</p></div></div></section></div></div></body></html>