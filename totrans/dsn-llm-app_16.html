<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 13. Design Patterns and System Architecture"><div class="chapter" id="ch13">
<h1><span class="label">Chapter 13. </span>Design Patterns and System Architecture</h1>


<p>Throughout this book, we have explored a variety of techniques<a data-type="indexterm" data-primary="system architecture" id="xi_systemarchitecture13463"/><a data-type="indexterm" data-primary="architectures" data-secondary="system" id="xi_architecturessystem13463"/> to adapt LLMs to solve our tasks, including in-context learning, fine-tuning, RAG, and tool use. While these techniques can potentially be successful in satisfying the performance requirements of your use case, deploying an LLM-based application in production requires adherence to a variety of other criteria like cost, latency, and reliability. To achieve these goals, an LLM application needs a lot of software scaffolding and specialized components.</p>

<p>To this end, in this chapter we will discuss various techniques to compose a production-level LLM system that can power useful applications. We will explore how to leverage multi-LLM architectures to balance cost and performance. Finally, we will look into software frameworks like DSPy that integrate LLM application development into the conventional software programming paradigm.</p>

<p>Treating an LLM-based application as just a standalone LLM component is inadequate if we intend to deploy it as a production-grade system.  We need to treat it as a system, made up of several software and model components that support the LLM and make it reliable, fast, and cost-effective. The way these components are composed and connected is referred to as the <em>system architecture.</em></p>

<p>Let’s begin by discussing a specific type: multi-LLM architectures that leverage multiple LLMs to solve your task.</p>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Multi-LLM Architectures"><div class="sect1" id="id230">
<h1>Multi-LLM Architectures</h1>

<p>Throughout this book, we have discussed the tradeoffs<a data-type="indexterm" data-primary="multi-LLM architectures" id="xi_multiLLMarchitectures131454"/> involved in choosing the right LLM for a task. Often, it can be beneficial to leverage multiple LLMs to achieve the desired outcome. Multi-LLM architectures can exist in the following two modes (or a combination):</p>
<dl>
<dt>Each LLM is specialized for a different subtask</dt>
<dd>
<p>Different problem subtasks may require different levels of capabilities. To minimize cost and latency, for each task we would like to use the smallest possible LLM that can solve the subtask at the performance threshold we set.</p>
</dd>
<dt>All LLMs solve the same task</dt>
<dd>
<p>In this case, all the LLMs are solving the same task, but for each input, only one or a subset of LLMs may be chosen to solve it.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>A given task can be solved by an ensemble<a data-type="indexterm" data-primary="ensembling approach to multiple models" id="id1688"/> of LLMs, and the final outputs can be chosen based on some rules (majority voting, interpolation, etc.). Refer to <a href="https://oreil.ly/FEikT">Jiang et al.’s ensembling framework</a> called LLM-Blender<a data-type="indexterm" data-primary="LLM-Blender" id="id1689"/> for an example of thoughtful ensembling.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1690">
<h1>Exercise</h1>
<p>In the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a>, you will find a skeleton implementation of a legal agent. The agent is designed to retrieve information from the web and provide answers to user questions about court cases. This agent utilizes a multi-LLM architecture, comprising several LLMs of different sizes.</p>

<p>Break down the agent implementation into its constituent tasks and enumerate the set of capabilities (described in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>) required for each task. Assign the smallest possible LLMs for each task that demonstrate these capabilities beyond a satisfactory threshold. Use content from Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.html#chapter_utilizing_llms">5</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch08.html#ch8">8</a> to guide you in your exercise. What are the cost savings compared to solving the entire task with the largest of the given 
<span class="keep-together">models</span>?</p>
</div></aside>

<p>Let’s walk through some commonly used multi-LLM architectures.</p>








<section data-type="sect2" data-pdf-bookmark="LLM Cascades"><div class="sect2" id="id231">
<h2>LLM Cascades</h2>

<p>While using the state-of-the-art LLM<a data-type="indexterm" data-primary="LLM cascades" id="xi_LLMcascades133637"/><a data-type="indexterm" data-primary="cascade architecture" id="xi_cascadearchitecture133637"/><a data-type="indexterm" data-primary="architectures" data-secondary="cascade" id="xi_architecturescascade133637"/> for processing all our inputs is an option, realistically this might be cost-prohibitive or latency sensitive. To optimize costs while keeping performance standards high, we could leverage multiple LLMs, organized in a cascade architecture.</p>

<p>Let’s illustrate LLM cascades. Consider you have an application using three LLMs: one small, one medium, and one large, as illustrated in <a data-type="xref" href="#llm-cascades">Figure 13-1</a>.</p>

<figure><div id="llm-cascades" class="figure">
<img src="assets/dllm_1301.png" alt="llm-cascades" width="600" height="161"/>
<h6><span class="label">Figure 13-1. </span>LLM cascades</h6>
</div></figure>

<p>The following process is observed during inference<a data-type="indexterm" data-primary="confidence levels, cascading architectures" id="xi_confidencelevelscascadingarchitectures134451"/>:</p>
<ol>
<li>
<p>Each input is fed to the small LLM.</p>
</li>
<li>
<p>If the small LLM makes an output prediction with a confidence level greater than a threshold, then we accept the output as the final output.</p>
</li>
<li>
<p>If the small LLM makes an output prediction with a confidence level that doesn’t surpass the threshold, then we pass the input to the medium model.</p>
</li>
<li>
<p>Similarly, if the medium LLM makes an output prediction with a confidence level greater than a threshold, then we stop and accept this output as the final output.</p>
</li>
<li>
<p>However, if the medium LLM makes an output prediction with a confidence level that doesn’t surpass the threshold, then we pass the input to the large model.</p>
</li>
<li>
<p>The large model generates the final output.</p>
</li>

</ol>

<p>This architecture is most beneficial when most user inputs can be processed by the small model.</p>

<p>If you are using encoder-only models<a data-type="indexterm" data-primary="encoder-only models" id="id1691"/><a data-type="indexterm" data-primary="models" data-secondary="encoder-only" id="id1692"/> like BERT, the output probability scores<a data-type="indexterm" data-primary="output probability" id="id1693"/> can be used as the measure of confidence. Thus, a group of well-calibrated models will enable us to efficiently route the input to the most suitable model. (Recall our discussion on model calibration in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>.)</p>

<p>For decoder models<a data-type="indexterm" data-primary="decoder models" data-secondary="cascading architectures" id="id1694"/><a data-type="indexterm" data-primary="models" data-secondary="decoder" id="id1695"/>, a popular method is to use self-consistency<a data-type="indexterm" data-primary="self-consistency" data-secondary="decoder models" id="id1696"/> as a measure of confidence. (Recall our discussion on self-consistency in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>.) If we generate multiple times from the model and the outputs are mostly consistent with each other, then we can say that the model is being confident in its predictions. If they are not consistent, then we can move down the cascade and apply the inputs to the next LLM in the cascade.</p>
<div data-type="warning" epub:type="warning" class="less_space pagebreak-before"><h6>Warning</h6>
<p>Some works propose asking the LLM to explicitly state the confidence level of its output. This has not been proven to be effective yet. Beware of asking the LLM to verify its own work in any form!</p>
</div>

<p>Another method for assessing confidence is to use margin sampling, as proposed by <a href="https://oreil.ly/5s1rJ">Ramirez et al.</a> In the margin sampling method, we generate the first token and use the difference in the probability of the most probable token and the second most probable token as the margin. The assumption is that the higher the margin, the more confident the model. If the margin is below a certain threshold, then the input is sent to the next model in the cascade<a data-type="indexterm" data-startref="xi_LLMcascades133637" id="id1697"/><a data-type="indexterm" data-startref="xi_cascadearchitecture133637" id="id1698"/><a data-type="indexterm" data-startref="xi_architecturescascade133637" id="id1699"/>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1700">
<h1>Exercise</h1>
<p>Compare the different confidence assessment strategies for decoder models. Test the Llama 2-3B model with facts from Wikipedia pages. Try the margin sampling method, the self-consistency method, and just asking the LLM how confident it is about the answer. Which method do you observe is a better representation of LLM confidence?</p>
</div></aside>

<p>An alternative to using cascades is using a router scheme<a data-type="indexterm" data-startref="xi_confidencelevelscascadingarchitectures134451" id="id1701"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Routers"><div class="sect2" id="id232">
<h2>Routers</h2>

<p>A router<a data-type="indexterm" data-primary="router schemes" id="id1702"/> is a program or a model that processes input queries and dispatches them to the appropriate model. The advantage of using the router architecture is that, unlike cascades, the same input need not be run on potentially multiple models. However, the effectiveness of this strategy relies on the router effectively dispatching inputs to the optimal model, which may not always be fulfilled.</p>

<p>A router can perform intent classification<a data-type="indexterm" data-primary="intent classification, router schemes" id="id1703"/>, i.e., understand the intention of the user and dispatch the input to a suitable LLM that can solve the task being requested. If all the LLMs in the architecture are intended to solve the same task, then the router assesses the difficulty of the input query and dispatches the input to the smallest model that can adequately solve the task.</p>

<p><a data-type="xref" href="#routers">Figure 13-2</a> illustrates the role of the router in picking the right model to solve a task.</p>

<figure><div id="routers" class="figure">
<img src="assets/dllm_1302.png" alt="router" width="600" height="330"/>
<h6><span class="label">Figure 13-2. </span>Router</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>Routers can also be used in RAG pipelines. The router can assess the input and dispatch it to one of several different types of 
<span class="keep-together">retrievers</span>.</p>
</div>

<p>Assessing the complexity of an input query can be done using either heuristics or a fine-tuned model. Heuristics can be based on certain keywords that appear in the input (with RAG, <em>When</em> queries are more easily answered than <em>How</em> queries) or the identity of the tasks (for instance, sentiment analysis is an easier task that can be accomplished by a smaller model).</p>

<p>Next, let’s discuss task-specialized LLMs.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Task-Specialized LLMs"><div class="sect2" id="id233">
<h2>Task-Specialized LLMs</h2>

<p>Yet another way of organizing multi-LLM architectures is to deploy a variety of task-specific<a data-type="indexterm" data-primary="task-specialized LLMs" id="id1704"/> LLMs, each of them specialized in solving a particular type of task or subtask.</p>

<p>Given a complex user query, a relatively powerful LLM can be used to decompose<a data-type="indexterm" data-primary="task decomposition, agentic systems" id="xi_taskdecompositionagenticsystems1310679"/> the query into its constituent subtasks. A router can then assign each of these subtasks to the specialized model most equipped to handle at the subtask. (Recall our discussion on task decomposition in <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>.)</p>

<p>Specialized LLMs can be constructed by fine-tuning them on task- and domain-specific datasets<a data-type="indexterm" data-startref="xi_systemarchitecture13463" id="id1705"/><a data-type="indexterm" data-startref="xi_multiLLMarchitectures131454" id="id1706"/><a data-type="indexterm" data-startref="xi_architecturessystem13463" id="id1707"/>.</p>

<p><a data-type="xref" href="#task-specific-llms">Figure 13-3</a> illustrates how a complex query can be divided into several subtasks, with each subtask being dispatched to the model most likely to solve it in a cost-optimal way.</p>

<figure><div id="task-specific-llms" class="figure">
<img src="assets/dllm_1303.png" alt="task-specific-llms" width="600" height="400"/>
<h6><span class="label">Figure 13-3. </span>Task-specific LLMs</h6>
</div></figure>

<p>Let’s now explore some programming paradigms that facilitate more effective LLM application development<a data-type="indexterm" data-startref="xi_taskdecompositionagenticsystems1310679" id="id1708"/>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Programming Paradigms"><div class="sect1" id="id234">
<h1>Programming Paradigms</h1>

<p>As we have seen in this chapter, production-grade LLM systems<a data-type="indexterm" data-primary="programming paradigms" id="xi_programmingparadigms1312162"/><a data-type="indexterm" data-primary="design patterns" id="xi_designpatterns1312162"/> can be composed of a lot of software components that help make the system robust and reliable. Naturally, we would like to use software design patterns to help us build these systems to be productive and maintainable. The developer community is still maturing in this regard, and it will take more time for tried and tested design patterns to emerge.</p>

<p>At this juncture, there are several proposals for LLM programming paradigms. While many are not yet well-tested, some of these paradigms are mature enough to support production-grade applications. Let’s explore a couple of major ones.</p>








<section data-type="sect2" data-pdf-bookmark="DSPy"><div class="sect2" id="id235">
<h2>DSPy</h2>

<p>LLM application development<a data-type="indexterm" data-primary="DSPy (Declarative Self-improving Language Programs, pythonically)" id="xi_DSPyDeclarativeSelfimprovingLanguageProgramspythonically1312728"/> is a highly iterative process. You might want to experiment with a few candidate LLMs before selecting the right one. You might start with zero-shot prompting, which involves a lot of iterative prompt manipulation, also called prompt engineering. If zero-shot isn’t sufficient, you might venture into few-shot prompting, which involves iterating over various candidate examples. If few-shot prompting isn’t sufficient, you might want to fine-tune the model, which involves iteratively preparing a dataset and trying various hyperparameters for the model. <em>D</em>eclarative <em>S</em>elf-improving Language <em>P</em>rograms, p<em>y</em>thonically (DSPy) is an open source programming framework that seeks to abstract a large part of the iterative process. Programming, not prompting, as their motto goes.</p>

<p>DSPy presents a framework where the application’s control flow is separated from variables that need to be iterated. The variables can be prompts, parameters of LLMs, etc. The programming blocks that manage the control flow of the application are called <em>modules</em>, and the blocks that perform the iterative updates of variables are called <em>optimizers</em>.</p>










<section data-type="sect3" data-pdf-bookmark="Modules"><div class="sect3" id="id236">
<h3>Modules</h3>

<p>A module is a building block of an LLM application. Each module corresponds to an underlying prompt in the prompt chain. Each module type is an abstraction of a different prompting technique, like CoT. A module can be declared using a <em>signature</em> that declaratively provides the input-output specification.</p>

<p>Declaring a CoT<a data-type="indexterm" data-primary="chain-of-thought (CoT) prompting" id="id1709"/><a data-type="indexterm" data-primary="CoT (chain-of-thought) prompting" id="id1710"/><a data-type="indexterm" data-primary="prompting" data-secondary="chain-of-thought" id="id1711"/> prompting module with a signature is as simple as:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">dspy</code>
<code class="n">summarizer</code> <code class="o">=</code> <code class="n">dspy</code><code class="o">.</code><code class="n">ChainOfThought</code><code class="p">(</code><code class="s1">'document -&gt; summary'</code><code class="p">)</code></pre>

<p><code>ChainOfThought</code> is a module that provides an abstraction for the CoT prompting technique. The module is declared with a signature <code>document → summary</code> that specifies the input and output types in a declarative form. For instance, if you are building a question-answering application, then the signature could be <code>question → answer</code>.</p>

<p>For some applications, you would like to provide more details on the input-output mapping than just a short string. For those instances, signatures can be declared using Python classes. Here’s an example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">RAGQA</code><code class="p">(</code><code class="n">dspy</code><code class="o">.</code><code class="n">Signature</code><code class="p">):</code>
    <code class="sd">"""Using only information in the provided context,</code>
<code class="sd">       answer the question in the text"""</code>

    <code class="n">context</code> <code class="o">=</code> <code class="n">dspy</code><code class="o">.</code><code class="n">InputField</code><code class="p">(</code><code class="n">desc</code><code class="o">=</code><code class="s2">"context might be irrelevant"</code><code class="p">)</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">dspy</code><code class="o">.</code><code class="n">InputField</code><code class="p">()</code>
    <code class="n">answer</code> <code class="o">=</code> <code class="n">dspy</code><code class="o">.</code><code class="n">OutputField</code><code class="p">(</code><code class="n">desc</code><code class="o">=</code><code class="s2">"Answer in at most two sentences."</code><code class="p">)</code>

<code class="n">context</code> <code class="o">=</code> <code class="s2">"Tempura was invented in New Zealand by a retired rugby player. The</code>
<code class="n">word</code> <code class="s1">'tempura'</code> <code class="n">comes</code> <code class="kn">from</code> <code class="nn">the</code> <code class="n">German</code> <code class="n">opera</code> <code class="n">by</code> <code class="n">Neubig</code><code class="o">.</code><code class="s2">"</code>
<code class="n">text</code> <code class="o">=</code> <code class="s2">"Which year was tempura invented in?"</code>
<code class="n">answer</code> <code class="o">=</code> <code class="n">dspy</code><code class="o">.</code><code class="n">ChainOfThought</code><code class="p">(</code><code class="n">RAGQA</code><code class="p">)</code>
<code class="n">answer</code><code class="p">(</code><code class="n">context</code><code class="o">=</code><code class="n">context</code><code class="p">,</code> <code class="n">text</code><code class="o">=</code><code class="n">text</code><code class="p">)</code></pre>

<p>In this example, instructions can be provided in three places:</p>

<ul>
<li>
<p>The docstring, with a more detailed description of the task</p>
</li>
<li>
<p>The input field, with details on any input constraints</p>
</li>
<li>
<p>The output field, with details on any output constraints</p>
</li>
</ul>

<p>Refer to the <a href="https://oreil.ly/4Vy5c">DSPy documentation</a> for a full list of available modules. We can use these modules as building blocks for constructing complex LLM applications. Next let’s look at optimizers that work under the hood to <em>compile</em> our modules into an executable program.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Optimizers"><div class="sect3" id="id237">
<h3>Optimizers</h3>

<p>Optimizers<a data-type="indexterm" data-primary="optimization and optimizers" data-secondary="DSPy" id="id1712"/> are components that update prompts or model parameters. Several optimizers are natively supported by DSPy. An optimizer can be used to update one of the following:</p>

<ul>
<li>
<p>The instruction prompt</p>
</li>
<li>
<p>Few-shot training examples</p>
</li>
<li>
<p>Model parameters (fine-tuning)</p>
</li>
</ul>

<p>An optimizer takes as input the modules it needs to be applied to, the metric to evaluate the output of the modules, and fine-tuning or few-shot training data consisting of input-output pairs or just inputs. Optimizers use algorithms to update the prompts or parameters to optimize the desired metric. DSPy supports metrics like <em>accuracy</em> or <em>precision</em> or <em>exact match</em>.</p>

<p>You can implement your own modules and optimizers if the ones provided by default are inadequate to your needs. Thus, DSPy is a powerful framework that separates the control flow of the LLM application from iterative aspects like LLM prompting and fine-tuning, and potentially automates the latter. The downsides of DSPy are that the optimizers might not be effective enough to work in an automated fashion and might need manual intervention to tune them correctly. More often than not, you will find yourself writing your own optimizers<a data-type="indexterm" data-startref="xi_DSPyDeclarativeSelfimprovingLanguageProgramspythonically1312728" id="id1713"/>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1714">
<h1>Exercise</h1>
<p>Implement a question-answering assistant over the Canadian parliamentary dataset provided in the book’s <a href="https://oreil.ly/llm-playbooks">Github repo</a> using the DSPy framework. How does this implementation compare to the non-DSPy version?</p>
</div></aside>

<p>Let’s now explore another framework called Language Model Query Language (LMQL). We have already been introduced to this framework in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a> in the context of structured generation, but here we will look at how the same framework can be used as a programming paradigm for developing LLM applications.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="LMQL"><div class="sect2" id="id238">
<h2>LMQL</h2>

<p>LMQL is a superset of Python<a data-type="indexterm" data-primary="Language Model Query Language (LMQL)" id="id1715"/><a data-type="indexterm" data-primary="LMQL (Language Model Query Language)" id="id1716"/><a data-type="indexterm" data-primary="queries" data-secondary="LMQL" id="id1717"/> that enables specifying prompts, output constraints, and program control flow using declarative Python code. Here is an example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">lmql</code>

<code class="nd">@lmql</code><code class="o">.</code><code class="n">query</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"gpt-4"</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">jeopardy</code><code class="p">():</code>
    <code class="sd">'''lmql</code>
<code class="sd">    """Generate a Jeopardy! question and answer.</code>
<code class="sd">    A:[ANSWER]</code>
<code class="sd">    Q:[QUESTION]""" where STOPS_AT(ANSWER, "?") and \</code>
<code class="sd">                           STOPS_AT(QUESTION, "\n")</code>
<code class="sd">    '''</code>

<code class="n">jeopardy</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="n">lmql</code><code class="o">.</code><code class="n">model</code><code class="p">(</code><code class="s2">"gpt-4"</code><code class="p">))</code></pre>

<p>In this example we are asking the model to generate a Jeopardy! question. Jeopardy! is a TV show that executes a modified version of a trivia quiz; the host supplies the answers and the contestants provide the question for the given answer.</p>

<p>In LMQL, we achieve this by defining a function called <code>jeopardy</code> and supplying the prompt instructions in the doc string. The doc string contains the instruction <code>Generate a Jeopardy! question and answer</code>. The <code>[ANSWER]</code> and <code>[QUESTION]</code> markers refer to templates that the LLM will fill in based on the constraints specified in the <code>WHERE</code> clause.</p>

<p>For the answer (which in Jeopardy is the question), we stop generation after generating the <code>?</code> symbol. Similarly, for the question (which in Jeopardy is the answer), we stop generation after the newline symbol. The <code>WHERE</code> clause can be used to provide complex constraints for generation.</p>

<p>LMQL syntax might take a while to get used to, but overall it provides a robust programmatic foundation for developing LLM programs. Both LMQL and DSPy have a learning curve, so I recommend being patient during your first few iterations.</p>

<p>As LLMs and LLM-driven applications mature, I expect more programming paradigms to emerge and for existing paradigms to vastly evolve. Current paradigms might be too brittle in many cases, so be cautious and verify they are effective before you adopt them in production<a data-type="indexterm" data-startref="xi_programmingparadigms1312162" id="id1718"/><a data-type="indexterm" data-startref="xi_designpatterns1312162" id="id1719"/>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id373">
<h1>Summary</h1>

<p>In this chapter, we explored the construction of LLM systems and various system architectures. We showcased how we can leverage multi-LLM architectures to optimize for cost and latency. Finally, we introduced LLM programming frameworks for streamlining LLM application development.</p>
</div></section>
</div></section></div>
</div>
</body></html>