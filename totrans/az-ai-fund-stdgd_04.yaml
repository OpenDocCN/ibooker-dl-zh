- en: Chapter 4\. Fundamental Principles of Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the AI-900 exam, about 20%–25% of the material covers the core principles
    of ML on Microsoft Azure. This includes foundational techniques like regression
    analysis, classification, and clustering. Each of these techniques offers a unique
    approach to problem solving. They allow you to select the right method based on
    the data type and the predictions you need to make. In this chapter, we’re going
    to dig into these must-know concepts and services. Understanding these ideas will
    help you tackle questions on the AI-900 exam, so you’ll know not only what these
    services are but also how they work and why they matter.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Machine Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML, which is a branch of AI, allows systems to perform tasks like data analysis
    without needing explicit instructions. Instead, it processes large amounts of
    historical data, identifies patterns, and makes predictions based on those patterns.
    For instance, you can use ML to classify images, numbers, or documents and make
    predictions from them.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you work for a financial services organization looking to differentiate
    between fraudulent and genuine transactions. With ML, the system would learn to
    identify patterns from known examples and then apply that knowledge to predict
    whether a new transaction is genuine.
  prefs: []
  type: TYPE_NORMAL
- en: ML is essential for modern businesses because it helps automate data collection,
    classification, and analysis. This speeds up decision making and drives growth.
    It improves processes like customer experience and resource management, enabling
    businesses to solve problems faster.
  prefs: []
  type: TYPE_NORMAL
- en: Although the terms *artificial intelligence* (*AI*) and *machine learning* (*ML*)
    are often used interchangeably, they aren’t the same. AI is a broader concept
    that includes anything from voice assistants to self-driving cars. ML, however,
    focuses on specific tasks, such as predicting equipment maintenance schedules
    or classifying documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take manufacturing, for example. ML can help with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Predictive maintenance
  prefs: []
  type: TYPE_NORMAL
- en: Identify potential equipment failures before they occur, reducing downtime and
    repair costs
  prefs: []
  type: TYPE_NORMAL
- en: Quality control
  prefs: []
  type: TYPE_NORMAL
- en: Monitor production lines to detect defects and ensure consistent product quality
  prefs: []
  type: TYPE_NORMAL
- en: Product design optimization
  prefs: []
  type: TYPE_NORMAL
- en: Refine designs, such as improving the abrasiveness of sandpaper by analyzing
    small changes in size and shape
  prefs: []
  type: TYPE_NORMAL
- en: Supply chain management
  prefs: []
  type: TYPE_NORMAL
- en: Predict demand, identify bottlenecks, and streamline logistics to improve efficiency
  prefs: []
  type: TYPE_NORMAL
- en: The ML Model Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, an *ML model* is simply a software application that takes one or
    more input values and calculates an output value. The process of figuring out
    how to make that calculation is called *training*. Once the model is trained,
    you can use it to make predictions. This is a process known as *inferencing*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at the main steps of an ML system, which you can see
    in [Figure 4-1](#i04_chapter4_figure_1_1742068262037129). This is a very basic
    example. Keep in mind that the algorithms can be quite complex. But for our purposes,
    we just want to get a sense of the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. The basic workflow of an ML model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s take a look at each of these steps in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Train the Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The process of training an ML model begins with past data, which is referred
    to as the *training data* or *dataset*. Each data point in the training dataset
    is made up of two essential components:'
  prefs: []
  type: TYPE_NORMAL
- en: Features
  prefs: []
  type: TYPE_NORMAL
- en: The characteristics or attributes you observe in each data point
  prefs: []
  type: TYPE_NORMAL
- en: Labels
  prefs: []
  type: TYPE_NORMAL
- en: The outcome or result you want the model to predict
  prefs: []
  type: TYPE_NORMAL
- en: A dataset in table form is organized into rows and columns, as you would see
    in a spreadsheet or database. Each row represents an individual record or entry,
    and each column contains specific attributes or features of the data. This structure
    allows for easy comparison and analysis of data across multiple entries by syncing
    similar information in a consistent format. [Table 4-1](#i04_chapter4_table_1_1742068262048473)
    shows an example of a table for a dataset for customer feedback and return requests.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. Sample dataset showing customer feedback and return requests
  prefs: []
  type: TYPE_NORMAL
- en: '| Customer_ID | Purchase_Date | Product_Category | Rating | Feedback_Comment
    | Return_Requested |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 001 | 2024-10-01 | Electronics | 5 | “Great product, fast delivery!” | No
    |'
  prefs: []
  type: TYPE_TB
- en: '| 002 | 2024-10-05 | Apparel | 2 | “Size did not match description.” | Yes
    |'
  prefs: []
  type: TYPE_TB
- en: '| 003 | 2024-10-07 | Home Goods | 4 | “Quality is good, but color is off.”
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| 004 | 2024-10-10 | Electronics | 1 | “Item arrived damaged.” | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 005 | 2024-10-12 | Beauty | 3 | “Average product, packaging was poor.” |
    No |'
  prefs: []
  type: TYPE_TB
- en: Data is often messy and incomplete. These imperfections can distort results,
    lead to inaccuracies, and complicate decision making.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a common issue is missing data. While removing incomplete information
    is a straightforward approach, it can lead to unintended distortions, particularly
    when the missing data is not randomly distributed. Instead, consider alternative
    methods to handle missing data effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean/median imputation
  prefs: []
  type: TYPE_NORMAL
- en: Replace missing values with the average (mean) or the middle value (median)
    of the dataset to maintain consistency
  prefs: []
  type: TYPE_NORMAL
- en: Predictive imputation
  prefs: []
  type: TYPE_NORMAL
- en: Leverage statistical models to predict and fill in missing values based on patterns
    in the available data, providing a more accurate estimate
  prefs: []
  type: TYPE_NORMAL
- en: 'Another common challenge in data analysis is dealing with outliers. *Outliers*
    are extreme data points that differ significantly from other observations. They
    can distort results or obscure important patterns. Properly managing outliers
    is essential for improving the accuracy and reliability of your findings. Consider
    the following approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Removal
  prefs: []
  type: TYPE_NORMAL
- en: Exclude outliers from the dataset if they are identified as errors or are irrelevant
    to the analysis
  prefs: []
  type: TYPE_NORMAL
- en: Transformation
  prefs: []
  type: TYPE_NORMAL
- en: Use mathematical techniques, such as logarithmic or square root transformations,
    to minimize the influence of outliers without removing them entirely
  prefs: []
  type: TYPE_NORMAL
- en: Investigation
  prefs: []
  type: TYPE_NORMAL
- en: Examine outliers closely to determine whether they provide valuable insights
    or are the result of data-entry mistakes
  prefs: []
  type: TYPE_NORMAL
- en: 'Data may also have duplications. For this, you can consider these options:'
  prefs: []
  type: TYPE_NORMAL
- en: Deduplication algorithms
  prefs: []
  type: TYPE_NORMAL
- en: Use tools or scripts to identify and merge duplicate data
  prefs: []
  type: TYPE_NORMAL
- en: Unique identifiers
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that each record has a unique key to prevent duplication at the entry
    point
  prefs: []
  type: TYPE_NORMAL
- en: Quality control
  prefs: []
  type: TYPE_NORMAL
- en: Regularly audit data to detect and resolve duplicates
  prefs: []
  type: TYPE_NORMAL
- en: 'Then there is the problem with inconsistent data formats. You can use the following
    techniques to address this:'
  prefs: []
  type: TYPE_NORMAL
- en: Data normalization
  prefs: []
  type: TYPE_NORMAL
- en: Convert data to a standard format (e.g., ISO 8601 for dates)
  prefs: []
  type: TYPE_NORMAL
- en: Validation rules
  prefs: []
  type: TYPE_NORMAL
- en: Implement automated checks during data entry to enforce consistent formatting
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing pipelines
  prefs: []
  type: TYPE_NORMAL
- en: Develop extract, transform, and load (ETL) processes to clean and standardize
    data before analysis
  prefs: []
  type: TYPE_NORMAL
- en: Once you have improved the quality of the dataset—a process known as *data preparation*—you
    can then look at training. One way to think of this is in terms of basic algebra.
    The features are represented by *x*, and there are often several of them, forming
    a vector (an array of values). The label is represented by *y*, which is the prediction
    or result the model tries to output.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say you’re building a model that predicts the price of a
    house based on its characteristics. The features (*x*) might include the number
    of bedrooms, square footage, and location. The label (*y*) would be the house’s
    sale price. The model is trained to learn how these features relate to the price
    and can then predict the price of a new house based on similar characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example could be a model designed to predict whether an email is spam.
    The features (*x*) could be things like the presence of certain keywords, the
    sender’s address, and the time the email was sent. The label (*y*) would be a
    binary outcome: 1 if the email is spam, 0 if it’s not. Over time, the model learns
    to recognize patterns in the features that indicate whether an email is spam or
    legitimate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Apply the Algorithm'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you’re working with an ML model, you need an algorithm to figure out the
    relationship between the features and the label. The goal is to find a way to
    take the features (*x*) and use them to predict the label (*y*). The basic idea
    is to fit a function to your data. In other words, the algorithm looks for a pattern
    that can calculate *y* based on *x*. For our example, this is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = f(*x*)'
  prefs: []
  type: TYPE_NORMAL
- en: Once the algorithm has done its job, it gives you a model. This model is essentially
    a function that performs the calculation. In this chapter, we’ll see various examples
    of this, like linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Use Inferencing'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the training phase is done, you can use the trained model to make predictions,
    a process called *inferencing*. The model now works like a software program that
    contains the function learned during training. You provide it with a set of feature
    values, and it gives you a prediction for the label.
  prefs: []
  type: TYPE_NORMAL
- en: Since this output is a prediction generated by the function rather than an actual
    observed value, it’s typically written as *ŷ*, pronounced “y-hat.” This is a useful
    way to show that the result is an estimate based on what the model has learned,
    not a guaranteed outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Types of ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several types of ML, each designed for different kinds of problems.
    At the highest level, ML is divided into two main types: *supervised learning*,
    which has labeled data, and *unsupervised learning*, where the data does not have
    labels. These two categories help determine how the model learns from the data
    and what kind of tasks it can perform. Under the umbrellas of supervised and unsupervised
    learning, there are other types of ML. [Figure 4-2](#i04_chapter4_figure_2_1742068262037163)
    shows the hierarchy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. The categories of ML
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the next few sections, we’ll look at the approaches for supervised learning
    and then follow this up by looking at unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Regression Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Regression analysis* is a statistical method used to predict a numerical outcome
    based on one or more known factors, or variables. It helps you understand the
    relationship between these variables and the result you’re trying to predict.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say you want to predict how much of a new product will sell
    based on factors like advertising spend, the season, and market trends. Regression
    analysis allows you to analyze these factors and estimate how they affect sales
    so that you can forecast future performance based on current data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how you would approach it:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the data
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by dividing your data into three subsets: a training set, a validation
    set, and a testing set. The training set is used to develop the model, the validation
    set helps fine-tune the model’s parameters, and the testing set is reserved for
    assessing the model’s performance on new, unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: Train the model
  prefs: []
  type: TYPE_NORMAL
- en: Using the training data, apply an algorithm to identify relationships between
    key variables, such as the advertising budget, seasonality, and market trends,
    and their impact on sales. The model searches for patterns, such as whether increased
    ad spending leads to higher sales or if certain times of the year naturally see
    better sales performance.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tune the model
  prefs: []
  type: TYPE_NORMAL
- en: With the validation set, optimize the model’s parameters or hyperparameters
    to enhance its predictive accuracy. This step helps ensure that the model generalizes
    well and avoids overfitting to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Test the model
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the performance of the fine-tuned model using the testing set. In this
    stage, the model generates sales predictions based on input variables (e.g., advertising
    budget, season, market trends) and compares these predictions to actual sales
    data to measure accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the model
  prefs: []
  type: TYPE_NORMAL
- en: Assess the accuracy of the model’s predictions using relevant metrics, such
    as mean absolute error (MAE), root mean square error (RMSE), or *R*², which we’ll
    learn about later in this chapter. If the model’s performance falls short of expectations,
    refine the process by revisiting earlier steps, such as trying a different algorithm,
    adjusting parameters, or including new features.
  prefs: []
  type: TYPE_NORMAL
- en: Iterate and improve
  prefs: []
  type: TYPE_NORMAL
- en: Using the insights gained from the evaluation, iterate on the model to improve
    its performance. This iterative process might include collecting additional data
    or exploring alternative modeling techniques to boost accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Ticket Sales'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a look at a more detailed example to better understand how regression
    analysis works. For this example, we won’t cover every step outlined in the previous
    section. When it comes to the exam, such in-depth detail about regression analysis
    isn’t necessary. Instead, we’ll focus on the main phases.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we’ll predict a numeric label (*y*) based on a single feature
    (*x*). While most real-world applications involve multiple features, starting
    with just one keeps things simple and allows us to focus on the core idea.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the example of predicting concert ticket prices based on the
    popularity of the performing artist. Here, the popularity score is derived from
    survey data collected from fans, representing a value on a scale from 1 to 100\.
    This score serves as our feature while the ticket price for the artist’s concert
    is the label we aim to predict. To build this prediction model, we’ll use historical
    data that pairs popularity scores (*x*) with their corresponding ticket prices
    (*y*) from past concerts, as shown in [Table 4-2](#i04_chapter4_table_2_1742068262048550).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-2\. Comparing an artist’s popularity with the concert ticket price
  prefs: []
  type: TYPE_NORMAL
- en: '| Artist popularity (*x*) | Ticket price (*y*) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 35 | $45 |'
  prefs: []
  type: TYPE_TB
- en: '| 40 | $60 |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | $55 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | $75 |'
  prefs: []
  type: TYPE_TB
- en: '| 55 | $65 |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | $85 |'
  prefs: []
  type: TYPE_TB
- en: '| 65 | $100 |'
  prefs: []
  type: TYPE_TB
- en: '| 70 | $105 |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | $115 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | $135 |'
  prefs: []
  type: TYPE_TB
- en: '| 85 | $140 |'
  prefs: []
  type: TYPE_TB
- en: '| 90 | $170 |'
  prefs: []
  type: TYPE_TB
- en: Next, we’ll split the data and use a portion of it to train the model. In this
    case, the data was split randomly to ensure a balanced representation of the dataset.
    [Table 4-3](#i04_chapter4_table_3_1742068262048585) shows the subset of data selected
    for training.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-3\. The subset of data for the training of the model
  prefs: []
  type: TYPE_NORMAL
- en: '| Artist popularity (*x*) | Ticket price (*y*) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 40 | $60 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | $75 |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | $85 |'
  prefs: []
  type: TYPE_TB
- en: '| 70 | $105 |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | $115 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | $135 |'
  prefs: []
  type: TYPE_TB
- en: 'While random splitting is a common approach, there are other methods that can
    be used based on the scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: Stratified splitting
  prefs: []
  type: TYPE_NORMAL
- en: Ensures that specific proportions of key features (like popularity ranges) are
    maintained in both training and testing sets
  prefs: []
  type: TYPE_NORMAL
- en: Time-based splitting
  prefs: []
  type: TYPE_NORMAL
- en: Used when the data has a chronological order, such as time-series data, where
    older data is used for training and newer data for testing
  prefs: []
  type: TYPE_NORMAL
- en: K-fold cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: Splits the data into multiple folds, where each subset takes a turn as the test
    set while the others are used for training
  prefs: []
  type: TYPE_NORMAL
- en: To get a better understanding of how the popularity scores (*x*) and ticket
    prices (*y*) relate to each other, we can plot these values as points on a graph.
    You can see this in [Figure 4-3](#i04_chapter4_figure_3_1742068262037187). By
    plotting these points, you can start to see the relationship between the two—typically,
    as the popularity increases, so does the ticket price.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. This scatter plot illustrates the relationship between artist popularity
    and ticket prices
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the training dataset, we’re ready to apply an algorithm that can model
    the relationship between artist popularity and ticket price. We’ll use the linear
    regression formula, which essentially finds the best-fit line through the points
    that minimizes the distance between the line and the actual data points. This
    line represents a function where the slope tells you how much the ticket price
    will increase with each increase in the artist’s popularity. This is shown in
    [Figure 4-4](#i04_chapter4_figure_4_1742068262037209).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. The scatter plot with the linear regression line added for our
    training dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s say an artist has a popularity score of 77\. By applying the equation
    derived from the linear regression line, we can estimate the corresponding ticket
    price. In this case, the price would be something like $120 based on the trend
    we’ve established.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to evaluate the accuracy of this regression model. Using the
    testing dataset, we can predict the ticket prices for each artist’s popularity
    score, shown in [Table 4-4](#i04_chapter4_table_4_1742068262048610).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-4\. The predicted ticket price
  prefs: []
  type: TYPE_NORMAL
- en: '| Artist popularity (*x*) | Actual ticket price (*y*) | Predicted ticket price
    (ŷ) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 35 | $45 | $50 |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | $55 | $60 |'
  prefs: []
  type: TYPE_TB
- en: '| 55 | $65 | $70 |'
  prefs: []
  type: TYPE_TB
- en: '| 65 | $100 | $85 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | $135 | $120 |'
  prefs: []
  type: TYPE_TB
- en: '| 90 | $170 | $140 |'
  prefs: []
  type: TYPE_TB
- en: We can then plot these values on a chart, which you can see in [Figure 4-5](#i04_chapter4_figure_5_1742068262037231).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. The original dataset shown with actual and predicted ticket prices
    for the test data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The original data points capture the historical trend while the test data highlights
    how closely the model’s predictions align with actual ticket prices. The chart
    makes it easy to visualize where the model performed well and where deviations
    occurred, particularly by comparing the actual and predicted ticket prices across
    different levels of artist popularity.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the model performed fairly well. For midrange popularity scores,
    the predicted values are close to the actual values, indicating minimal error.
    However, at the lower (35) and higher (90) ends of the popularity scale, the model’s
    predictions underestimate the actual ticket prices. While the performance is promising,
    the noticeable deviations at these extremes suggest the model could benefit from
    additional fine-tuning to improve accuracy, especially for edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics for Regression Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to measuring how well your regression model performs, there are
    a few handy metrics based on the differences between your predicted values and
    the actual ones.
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine you’re predicting how many pizzas a group of friends will eat at a
    party. *Mean absolute error (MAE)* helps you figure out, on average, how far off
    your predictions were—whether you guessed too high or too low. For instance, if
    you predicted four pizzas but your friends ate seven, you missed by three. MAE
    ignores whether the difference is positive or negative, so it treats both -3 and
    +3 as a difference of 3\. If your absolute errors for a set of predictions were
    1, 2, 3, and 4 pizzas, the MAE would simply be the average of those numbers: 2.5
    pizzas.'
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes, you want to give more weight to bigger errors. After all, consistently
    being off by one pizza isn’t as bad as being wildly off by five. That’s where
    *mean squared error (MSE)* comes in. Instead of just taking the differences as
    they are, you square each error (making bigger mistakes stand out), and then average
    those squared values. In our pizza example, if your errors were 1, 2, 3, and 4,
    squaring them gives you 1, 4, 9, and 16\. The MSE would then be the average of
    these squared errors: 7.5.'
  prefs: []
  type: TYPE_NORMAL
- en: Root mean squared error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While MSE is useful, those squared numbers don’t match up with the original
    quantities you were measuring, so they can feel a little abstract. If you want
    the error back in terms of pizzas (or whatever you’re predicting), you can take
    the square root of the MSE. That’s called the *root mean squared error (RMSE)*.
    In this case, the square root of 7.5 is about 2.74, meaning your average error
    is around 2.74 pizzas.
  prefs: []
  type: TYPE_NORMAL
- en: Coefficient of determination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What if you want to understand how well your model explains the variation in
    your data? Enter *R*², also called the *coefficient of determination*. This metric
    tells you how much of the difference between actual and predicted values your
    model accounts for.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you’re trying to predict how many cupcakes a bakery sells each
    day, *R*² tells you how much your model can explain. If your *R*² value is 0.85,
    that means your model explains 85% of the variation in cupcake sales—the rest
    might be due to some unexpected cupcake-related event, like a new bakery opening
    next door. *R*² ranges from 0 to 1, and the closer it is to 1, the better your
    model fits the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these metrics are certainly helpful. But in real-world scenarios, ML
    isn’t a one-shot deal. Data scientists typically train models over and over, tweaking
    different aspects to improve performance. Here’s what they adjust:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection and preparation
  prefs: []
  type: TYPE_NORMAL
- en: You can choose which factors or features to include in the model and how to
    tweak them for better results. For instance, maybe you realize that cupcake sales
    don’t depend just on weather but also on nearby events.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm selection
  prefs: []
  type: TYPE_NORMAL
- en: There’s more than one way to predict the number of cupcakes sold. While one
    algorithm might focus on simple linear relationships, another might use more complex
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm parameters
  prefs: []
  type: TYPE_NORMAL
- en: These are the settings you adjust to fine-tune your algorithm. Think of them
    like the dials on an oven. If your cupcakes are coming out undercooked, you tweak
    the temperature (or in ML, the *hyperparameters*) to get better results.
  prefs: []
  type: TYPE_NORMAL
- en: After several rounds of this iterative process, you’ll settle on the version
    of the model that performs best for your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s sum up what we have learned about regression analysis in [Table 4-5](#i04_chapter4_table_5_1742068262048633).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-5\. Key concepts in regression analysis
  prefs: []
  type: TYPE_NORMAL
- en: '| Factors | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Purpose | To predict a numeric outcome label based on one or more predictor
    features by identifying patterns in historical data |'
  prefs: []
  type: TYPE_TB
- en: '| Process |'
  prefs: []
  type: TYPE_TB
- en: Split data into training and testing sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model to identify relationships.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the model to make predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate model accuracy and refine if needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training the model | Uses an algorithm, such as linear regression, to analyze
    patterns in the training data, establishing a predictive relationship between
    the independent and dependent variables |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation metrics |'
  prefs: []
  type: TYPE_TB
- en: 'MAE: Measures average prediction error without considering direction (over/underestimate)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MSE: Emphasizes larger errors by squaring them'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RMSE: Provide the error in original measurement units'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'R² (coefficient of determination): Indicates the model’s explanatory power,
    with values closer to 1 suggesting a better fit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Classification* in ML is a supervised learning task where the goal is to predict
    the category or class to which a given data point belongs. It involves training
    a model on labeled data where the label is categorical, such as “yes” or “no.”
    The model learns the relationship between input features and output classes, enabling
    it to assign new, unseen data points to one of the predefined categories.'
  prefs: []
  type: TYPE_NORMAL
- en: There are various types of classification techniques. In the next few sections,
    we’ll take a look at binary and multiclass classification.
  prefs: []
  type: TYPE_NORMAL
- en: Binary Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Binary classification* is one of the most common types of classification tasks.
    At its core, it’s about predicting one of two possible outcomes. When you feed
    your model data, the goal is to get it to categorize new information into one
    of two buckets, often labeled as 0 and 1\. For example, if you’re building a model
    to predict whether an email is spam or not, binary classification is your go-to
    technique. Each email gets analyzed, and the model spits out a prediction: “spam”
    or “not spam.”'
  prefs: []
  type: TYPE_NORMAL
- en: What makes binary classification different from something like regression is
    that you aren’t predicting a continuous value, like a temperature or a sales figure.
    Instead, you’re focused on making a choice between two discrete options. The model
    looks at the features of the data you provide, such as email content, and it uses
    that to assign a probability. Based on this probability, it then makes a final
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through a simple example to show how binary classification works.
    Imagine we want to predict whether a person will default on a loan using one feature:
    their credit score (*x*). Our goal is to classify them into one of two categories:
    either they default (*y* = 1) or they don’t (*y* = 0). The model will learn from
    the data in [Table 4-6](#i04_chapter4_table_6_1742068262048658) to make predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-6\. Defaults based on credit scores
  prefs: []
  type: TYPE_NORMAL
- en: '| Credit score (*x*) | Default? (*y*) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 580 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 720 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 610 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 750 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 590 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 800 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Based on the patterns in the training data, the model will eventually predict
    whether someone is likely to default or not. As you can see, it’s about using
    past information to make a binary choice. In our example, people with a credit
    score of 610 or lower are predicted to default while anyone with a score of 720
    or higher is predicted not to default.
  prefs: []
  type: TYPE_NORMAL
- en: To train our model, we’ll use an algorithm that analyzes the training data and
    fits it to a function that calculates the probability of a person defaulting on
    a loan. For example, if the model predicts a probability of 0.8 for default, that
    means there’s an 80% chance the person will default and a 20% chance they won’t.
  prefs: []
  type: TYPE_NORMAL
- en: There are several algorithms that handle binary classification, but *logistic
    regression* is a common choice. Logistic regression gives us an S-shaped curve,
    called a *sigmoid function*, that assigns values between 0 and 1 based on the
    input data. This is shown in [Figure 4-6](#i04_chapter4_figure_6_1742068262037256).
    Even though its name includes *regression*, the logistic regression algorithm
    is used for classification because it models the probability of different outcomes.
    This is a common topic for the exam.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. A sigmoid function showing the probability of loan default based
    on credit scores, with a threshold of 0.5 determining default (y = 1) or no default
    (y = 0)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this case, the curve shows the probability that someone will default (*y*
    = 1) based on their credit score (*x*). Mathematically, the model’s function can
    be represented like this:'
  prefs: []
  type: TYPE_NORMAL
- en: f(*x*) = P(*y* = 1 | *x*)
  prefs: []
  type: TYPE_NORMAL
- en: Note that for the exam, you will not have to memorize equations. They are used
    in this book as a way to better help you understand the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: For some people in the training data, we already know they defaulted (*y* =
    1), so the probability for them is 1.0\. For others who didn’t default, the probability
    is 0.0\. The sigmoid curve visually shows how the likelihood of default changes
    as credit scores increase.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 4-6](#i04_chapter4_figure_6_1742068262037256), the dashed line at
    0.5 serves as the threshold for our model’s predictions. When the calculated probability
    is at or above 0.5, the model predicts that the person will default (*y* = 1).
    If the probability falls below 0.5, the prediction is that they won’t default
    (*y* = 0). For instance, if someone has a credit score of 580, and the model assigns
    a 0.9 probability of default, that’s well above the threshold, meaning the model
    would predict this person is likely to default.
  prefs: []
  type: TYPE_NORMAL
- en: Just like with regression models, when you train a binary classification model,
    it’s essential to hold back a set of data to validate how well the model performs.
    Let’s say we kept the credit score data in [Table 4-7](#i04_chapter4_table_7_1742068262048681)
    aside to validate our model predicting loan defaults.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-7\. Data used to validate the predictions for loan defaults
  prefs: []
  type: TYPE_NORMAL
- en: '| Credit score (*x*) | Default? (*y*) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 62 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 108 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 113 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 70 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 88 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 91 | 1 |'
  prefs: []
  type: TYPE_TB
- en: We can apply the logistic function we trained earlier to these values, which
    you can see in [Figure 4-7](#i04_chapter4_figure_7_1742068262037277).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. A sigmoid function used to evaluate a binary classification model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Based on whether the calculated probability is above or below the threshold
    (usually 0.5), the model will predict either a default (1) or no default (0) for
    each credit score. We can then compare the predicted defaults (ŷ) to the actual
    defaults (*y*), as shown in [Table 4-8](#i04_chapter4_table_8_1742068262048704).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-8\. Comparing the predicted defaults to the actual defaults
  prefs: []
  type: TYPE_NORMAL
- en: '| Credit score (*x*) | Actual default (*y*) | Predicted default (*ŷ*) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 62 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 108 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 113 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 70 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 88 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 91 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: This comparison helps us see where the model is getting it right and where it
    might need improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics for Binary Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in calculating evaluation metrics for a binary classification
    model is usually to create a *confusion matrix* of the number of correct and incorrect
    predictions for each possible class label, as you can see in [Table 4-9](#i04_chapter4_table_9_1742068262048724).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-9\. Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Positive | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Positive** | 30 | 45 |'
  prefs: []
  type: TYPE_TB
- en: '| **Negative** | 20 | 19 |'
  prefs: []
  type: TYPE_TB
- en: The rows represent the actual values, labeled as “positive” or “negative,” while
    the columns show the predicted values. For example, the model predicted the positive
    class correctly 30 times, which is displayed as the true positive count. However,
    it also made 45 incorrect predictions where it classified negatives as positives.
    Similarly, there are 20 instances of false negatives and 19 true negatives. This
    distribution helps evaluate the accuracy and types of errors in the model’s predictions.
    One way to express the different possibilities for the confusion matrix is shown
    in [Table 4-10](#i04_chapter4_table_10_1742068262048744).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-10\. Confusion matrix with descriptions
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Positive | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Positive** | True positive | False positive |'
  prefs: []
  type: TYPE_TB
- en: '| **Negative** | False negative | True negative |'
  prefs: []
  type: TYPE_TB
- en: 'The rows and columns follow the same arrangement, but each cell now includes
    a label: true positive, false positive, false negative, and true negative. These
    labels offer a clearer understanding of the relationship between predicted and
    actual outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: True positive (TP)
  prefs: []
  type: TYPE_NORMAL
- en: The model predicted the positive class, and the actual class was also positive.
    This is a correct prediction for the positive class.
  prefs: []
  type: TYPE_NORMAL
- en: False positive (FP)
  prefs: []
  type: TYPE_NORMAL
- en: The model predicted the positive class, but the actual class was negative. This
    is an incorrect prediction, often referred to as a *type I error*.
  prefs: []
  type: TYPE_NORMAL
- en: False negative (FN)
  prefs: []
  type: TYPE_NORMAL
- en: The model predicted the negative class, but the actual class was positive. This
    is another incorrect prediction, known as a *type II error*.
  prefs: []
  type: TYPE_NORMAL
- en: True negative (TN)
  prefs: []
  type: TYPE_NORMAL
- en: The model predicted the negative class, and the actual class was also negative.
    This is a correct prediction for the negative class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling this a *confusion matrix* is certainly apt: it can be tough to understand
    this. Yet the confusion matrix is likely to be on the exam. This is why it’s a
    good idea to memorize the four outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now take a look at other common evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One basic metric derived from the confusion matrix is *accuracy*, which is
    simply the proportion of correct predictions out of the total. It’s calculated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy = (TN + TP) ÷ (TN + FN + FP + TP)
  prefs: []
  type: TYPE_NORMAL
- en: 'For our credit score example, the calculation is:'
  prefs: []
  type: TYPE_NORMAL
- en: (2 + 3) ÷ (2 + 1 + 0 + 3) = 5 ÷ 6 = 0.83
  prefs: []
  type: TYPE_NORMAL
- en: This means our model correctly predicted loan defaults 83% of the time. While
    accuracy seems like a good measure, it can be misleading if the data is imbalanced.
    For instance, if only a small percentage of people default, a model could predict
    no default for everyone and still achieve high accuracy, but it wouldn’t truly
    capture the patterns in the ​data.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Recall* measures how well the model identifies positive cases (people who
    defaulted on a loan). It’s calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: Recall = TP ÷ (TP + FN)
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, the calculation is:'
  prefs: []
  type: TYPE_NORMAL
- en: 3 ÷ (3 + 1) = 3 ÷ 4 = 0.75
  prefs: []
  type: TYPE_NORMAL
- en: So our model correctly identified 75% of those who defaulted.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Precision* tells us how accurate the positive predictions are—that is, how
    many of the predicted defaults were actual defaults. It’s calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision = TP ÷ (TP + FP)
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, the calculation is:'
  prefs: []
  type: TYPE_NORMAL
- en: 3 ÷ (3 + 0) = 3 ÷ 3 = 1.0
  prefs: []
  type: TYPE_NORMAL
- en: This means that every time the model predicted a default, it was correct 100%
    of the time.
  prefs: []
  type: TYPE_NORMAL
- en: F1 score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *F1 score* provides a harmonic mean between precision and recall, which
    is a type of average that emphasizes the smaller values in a dataset. The harmonic
    mean is calculated as the reciprocal of the average of the reciprocals of the
    values, ensuring that both precision and recall are given equal weight. Unlike
    accuracy, which can be misleading when dealing with imbalanced datasets, the F1
    score takes both false positives and false negatives into account, offering a
    more nuanced view of a model’s effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula is:'
  prefs: []
  type: TYPE_NORMAL
- en: F1 score = 2 × Precision × Recall ÷ Precision + Recall
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, the calculation is:'
  prefs: []
  type: TYPE_NORMAL
- en: 2 × 1.0 × 0.75 ÷ 1.0 + 0.75 = 1.5 ÷ 1.75 = 0.86
  prefs: []
  type: TYPE_NORMAL
- en: The resulting F1 score of 0.86 indicates that the model achieves a good balance
    between precision and recall, performing effectively in correctly identifying
    positive cases while minimizing false positives and false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: Area under the curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to think about recall is to call it the *true positive rate (TPR)*,
    which shows how well the model identifies positive cases (defaults, in our example).
    On the flip side, we also have the *false positive rate (FPR)*, which measures
    how often the model incorrectly predicts a default when there wasn’t one. The
    FPR is calculated as FP ÷ (FP + TN). In our case, since there were no false positives,
    the FPR is 0 ÷ 2 = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we adjust the threshold for predicting a default (for instance, moving
    it higher or lower than 0.5), that would change the balance of positive and negative
    predictions, which means both the TPR and FPR would shift. A common way to visualize
    this is by plotting a *receiver operating characteristic (ROC) curve*, which compares
    the TPR and FPR across all possible threshold values. [Figure 4-8](#i04_chapter4_figure_8_1742068262037296)
    illustrates this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0408.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. ROC plots the true positive and false positive rates
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The perfect ROC curve would shoot straight up along the TPR axis and then run
    across the top, giving it an area under the curve (AUC) of 1.0, as shown in [Figure 4-8](#i04_chapter4_figure_8_1742068262037296).
    This means that the model makes perfect predictions. A completely random model
    would follow a diagonal line, with an AUC of 0.5—basically just guessing. For
    our credit score example, let’s assume we generated the ROC curve and the AUC
    is 0.875\. This tells us that the model is much better than guessing and performs
    well in predicting loan defaults.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 4-11](#i04_chapter4_table_11_1742068262048765) sums up what we have
    learned about binary classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-11\. Key concepts in binary classification
  prefs: []
  type: TYPE_NORMAL
- en: '| Factors | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Purpose | To categorize data into one of two distinct classes, often labeled
    as 0 and 1, such as predicting “spam” versus “not spam” or “default” versus “no
    default” |'
  prefs: []
  type: TYPE_TB
- en: '| Process |'
  prefs: []
  type: TYPE_TB
- en: Split data into training and testing sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model to classify based on patterns in the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test and validate model accuracy on new data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training the model | Commonly uses logistic regression to fit a function
    that estimates the probability of an outcome between 0 and 1, applying a threshold
    (e.g., 0.5) to classify results |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation metrics |'
  prefs: []
  type: TYPE_TB
- en: 'Accuracy: Overall correct predictions divided by total predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Confusion matrix: A table that summarizes the performance of a classification
    model by displaying the counts of TPs, FPs, TNs, and FNs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recall: TPR assessing how well the model identifies positive cases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision: Accuracy of positive predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'F1 score: Harmonic mean of precision and recall for balanced evaluation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AUC: Measures the model’s ability to distinguish between classes across all
    thresholds, with 1.0 indicating a perfect classifier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you’re working with *multiclass classification*, you’re figuring out which
    category, out of several, best fits a specific observation. This is based on calculating
    the likelihood of various outcomes. It allows a model to predict which option
    is most likely for a given case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use a group of flowers as an example. For each flower, we’ve measured
    the petal length (*x*), and we’re trying to predict the flower type (*y*), which
    could be one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 0 = rose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 = daisy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2 = tulip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, in a real-world scenario, you’d usually have more than just one feature
    (*x*) to work with. But for simplicity, we’re sticking to a single feature here.
    The data is in [Table 4-12](#i04_chapter4_table_12_1742068262048789).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-12\. The relationship between petal length and flower type
  prefs: []
  type: TYPE_NORMAL
- en: '| Petal length (*x*) | Flower type (*y*) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 167 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 172 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 225 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 197 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 189 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 232 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 158 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'To train our model, we need to select an algorithm, and we have two main options
    to choose from: one-vs-rest (OVR) and multinomial.'
  prefs: []
  type: TYPE_NORMAL
- en: One-vs-rest algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With OVR, the idea is to build a separate binary classification model for each
    class. Each model decides whether a given observation belongs to its specific
    class. Using our flower example, this method would create three models:'
  prefs: []
  type: TYPE_NORMAL
- en: Model 1
  prefs: []
  type: TYPE_NORMAL
- en: Determines if a flower is a rose (class 0)
  prefs: []
  type: TYPE_NORMAL
- en: Model 2
  prefs: []
  type: TYPE_NORMAL
- en: Determines if a flower is a daisy (class 1)
  prefs: []
  type: TYPE_NORMAL
- en: Model 3
  prefs: []
  type: TYPE_NORMAL
- en: Determines if a flower is a tulip (class 2)
  prefs: []
  type: TYPE_NORMAL
- en: Each model calculates a probability between 0 and 1\. The class with the highest
    probability is selected as the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A multinomial algorithm generates a single function that produces a probability
    distribution for all the classes at once. Instead of having multiple models, it
    gives you a vector with probabilities for each class, and the total of these probabilities
    always equals 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you might get an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[0.2, 0.3, 0.5]'
  prefs: []
  type: TYPE_NORMAL
- en: This means there’s a 20% chance the flower is a rose, a 30% chance it’s a daisy,
    and a 50% chance it’s a tulip. Since 0.5 is the highest, the model predicts tulip.
  prefs: []
  type: TYPE_NORMAL
- en: No matter which algorithm you go with, the model uses these probabilities to
    predict the most likely class for any observation.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of a Multiclass Classification Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can measure the performance of a multiclass classifier by calculating binary
    metrics for each individual class or by using aggregate metrics that consider
    all classes together. [Table 4-13](#i04_chapter4_table_13_1742068262048811) shows
    the evaluation for our flower example.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-13\. Measuring the performance of a multiclass classifier
  prefs: []
  type: TYPE_NORMAL
- en: '| Petal length (*x*) | Actual flower (*y*) | Predicted flower (ŷ) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 165 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 171 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 205 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 195 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 183 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 221 | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 214 | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: The confusion matrix for a multiclass classifier works similarly to the confusion
    matrix for a binary one, but instead of two classes, it captures predictions across
    multiple classes. This shows the number of predictions for each combination of
    actual and predicted class labels. [Figure 4-9](#i04_chapter4_figure_10_1742068262037358)
    displays the confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0409.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. Confusion matrix showing the actual and predicted flower classes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From this confusion matrix, we can figure out the key metrics for each flower
    class, listed in [Table 4-14](#i04_chapter4_table_14_1742068262048835).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-14\. Confusion matrix for the key metrics for each flower class
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | TP | TN | FP | FN | Accuracy | Recall | Precision | F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2 | 5 | 0 | 0 | 1.0 | 1.0 | 1.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 4 | 1 | 0 | 0.86 | 1.0 | 0.67 | 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | 4 | 0 | 1 | 0.86 | 0.67 | 1.0 | 0.8 |'
  prefs: []
  type: TYPE_TB
- en: 'To get the overall accuracy, recall, and precision for the model, we sum the
    values for TPs, TNs, FPs, and FNs:'
  prefs: []
  type: TYPE_NORMAL
- en: Overall accuracy = (13 + 6) ÷ (13 + 6 + 1 + 1) = 0.90
  prefs: []
  type: TYPE_NORMAL
- en: Overall recall = 6 ÷ (6 + 1) = 0.86
  prefs: []
  type: TYPE_NORMAL
- en: Overall precision = 6 ÷ (6 + 1) = 0.86
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the overall F1 score is calculated using the overall recall and precision
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: Overall F1 score = (2 × 0.86 × 0.86) ÷ (0.86 + 0.86) = 0.86
  prefs: []
  type: TYPE_NORMAL
- en: These metrics help us understand how well our flower classifier is performing
    across all  classes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s sum up what we have learned about multiclass classification in [Table 4-15](#i04_chapter4_table_15_1742068262048858).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-15\. Key concepts in multiclass classification
  prefs: []
  type: TYPE_NORMAL
- en: '| Factors | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Purpose | To categorize data into one of multiple possible classes by estimating
    the likelihood of each class for a given observation, such as predicting flower
    types based on features like petal length |'
  prefs: []
  type: TYPE_TB
- en: '| Process |'
  prefs: []
  type: TYPE_TB
- en: Select and train a model using either OVR or multinomial algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict the class based on the highest probability output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate accuracy and refine the model as needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Modeling techniques |'
  prefs: []
  type: TYPE_TB
- en: 'OVR: Builds a binary classifier for each class to predict if a given observation
    belongs to that class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multinomial: Uses a single model that outputs probabilities for all classes
    simultaneously, with the highest probability indicating the predicted class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluation metrics |'
  prefs: []
  type: TYPE_TB
- en: 'Confusion matrix: Visualizes correct and incorrect predictions across all classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accuracy, recall, precision, and F1 score: Calculated per class and as an aggregate
    to gauge overall performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Clustering* is an ML technique used to organize data into groups, or clusters,
    based on the similarity of their features. It falls under the category of unsupervised
    learning because it does not rely on prelabeled data to identify these groups.
    Instead, the algorithm analyzes the data’s features and assigns each data point
    to a cluster based on shared characteristics. In essence, the clusters act as
    labels generated by the model after analyzing the inherent structure of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is particularly useful in scenarios where you need to uncover hidden
    patterns or groupings within data. For example, it can help in market segmentation,
    where customers are grouped based on purchasing behavior, or in biology, where
    genes with similar functions are grouped together. It’s also valuable for anomaly
    detection, such as identifying unusual transactions in fraud detection, or in
    image segmentation to group pixels into distinct regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at a more detailed example. Suppose you’re a marine biologist
    studying a group of fish. Instead of focusing on identifying the species, you’re
    only interested in grouping the fish based on two characteristics: the length
    of the fish and the number of fins. The data is in [Table 4-16](#i04_chapter4_table_16_1742068262048881).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-16\. Grouping of fish based on similarities in length and fin count
  prefs: []
  type: TYPE_NORMAL
- en: '| Length (*x*1) | Fins (*x*2) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 6 |'
  prefs: []
  type: TYPE_TB
- en: In this case, the goal is to cluster fish with similar lengths and fin counts
    together, not to classify them into any specific species.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to approach clustering. One of the most popular methods
    is called *k-means clustering*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how to do this. First, you turn your data into vectors, which means
    assigning each data point a coordinate in a space. If you have two features, like
    the length of a fish (*x*1) and the number of fins (*x*2), these become two coordinates—[x1,
    x2]—that can be plotted on a two-dimensional graph.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you decide how many clusters you want. Let’s say you choose three clusters
    (*k* = 3). You randomly place three points on the graph to represent the centers
    of your clusters, called *centroids*. Then, each fish is assigned to the centroid
    it is closest to. The centroid itself is recalculated to be the average of the
    data points (fish) assigned to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process repeats: as the centroids move, some fish may become closer to
    a different centroid, so the assignments change. This back-and-forth of reassigning
    fish to clusters and adjusting the centroids continues until the groups become
    stable. You can see this in [Figure 4-10](#i04_chapter4_figure_11_1742068262037379).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0410.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-10\. The clusters of fish grouped by length and number of fins
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since there’s no known label to which to compare the cluster assignments, the
    effectiveness of a clustering model is judged by how well the clusters are distinct
    from one another. You can use several metrics to measure how well the clusters
    are separated:'
  prefs: []
  type: TYPE_NORMAL
- en: Average distance to cluster center
  prefs: []
  type: TYPE_NORMAL
- en: Measures how close, on average, each point in the cluster is to the centroid
  prefs: []
  type: TYPE_NORMAL
- en: Average distance to other centers
  prefs: []
  type: TYPE_NORMAL
- en: Looks at how close, on average, each point in the cluster is to the centroids
    of other clusters
  prefs: []
  type: TYPE_NORMAL
- en: Maximum distance to cluster center
  prefs: []
  type: TYPE_NORMAL
- en: Finds the farthest point from the centroid within the cluster
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette score
  prefs: []
  type: TYPE_NORMAL
- en: A number between -1 and 1 that shows how well separated the clusters are (with
    1 indicating the best separation)
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DL is a more sophisticated type of ML that tries to mimic how our brains learn.
    At its core, DL relies on what’s called an *artificial neural network*, which
    behaves similarly to how biological neurons work by using mathematical functions.
    These artificial neural networks consist of multiple layers of neurons, which
    creates a structure that processes data through increasingly complex layers. This
    is why the method is called “deep” learning, and the resulting models are known
    as *deep neural networks (DNNs)*. You can apply DL to various tasks, such as predicting
    outcomes (regression), classifying data, and even more complex problems like understanding
    language or recognizing images.
  prefs: []
  type: TYPE_NORMAL
- en: Like other forms of ML, DL involves training a model to predict an output based
    on one or more inputs. The model’s job is to figure out a function that connects
    these inputs to the correct outputs. This function is built up across the neural
    network layers, with each layer handling a part of the process. During training,
    the model repeatedly adjusts its internal parameters (called *weights*) to minimize
    how far its predictions are from the correct values. Over time, the model fine-tunes
    these weights to improve accuracy and make better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down how a neural network works by looking at an example that classifies
    three different types of fruit: apples, oranges, and bananas. In this case, the
    input data (*x*) is a vector of measurements for these features. Let’s say we
    measure:'
  prefs: []
  type: TYPE_NORMAL
- en: The fruit’s weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The color’s hue value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The curvature of the fruit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So *x* is a vector with three values:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* = [x1, x2, x3]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the label (*y*) is the fruit’s type, which could be one of the three:
    apple, orange, or banana. Since this is a classification problem, the neural network
    will predict probabilities for each class. So *y* will be a vector representing
    the probabilities for each fruit: [P(apple|x), P(orange|x), P(banana|x)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how the network makes a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: The feature data for a fruit, like [180g, 0.8, 0.3], is fed into the input layer
    of the neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each neuron in the input layer takes the feature values, multiplies them by
    their assigned weights, and passes the result through an activation function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The neurons in each layer are connected to neurons in the next layer, forming
    a fully connected network where the results are fed forward through the network’s
    layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output layer generates a vector of probabilities—for example, [0.2, 0.7,
    0.1]—which represents the likelihood of the fruit being an apple, orange, or banana.
    Since 0.7 is the highest value, the network predicts the fruit is an orange.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Figure 4-11](#i04_chapter4_figure_12_1742068262037397) shows a graphical representation
    of a deep learning network.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0411.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-11\. A deep learning network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The magic of neural networks lies in adjusting the weights during training
    to make better predictions. Initially, these weights are random, but through a
    process called *backpropagation*, the network refines them to improve accuracy.
    Here’s a high-level view of the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: Define your training data—basically, a set of known fruit types with their corresponding
    measurements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data is passed through the neural network, and the output is compared to
    the actual labels using a loss function. A loss function is a mathematical formula
    that quantifies the difference between the predicted output and the true labels.
    For example, if the network predicts [0.3, 0.1, 0.6] for a banana, but the true
    label is [0, 0, 1], the loss function calculates the discrepancy between these
    values. This difference, or loss, serves as a signal for the model to adjust its
    parameters and improve its predictions. The ultimate goal is to minimize this
    loss to enhance the network’s accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The network calculates how much each weight contributed to the loss and uses
    this information to adjust the weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is repeated over many cycles (called *epochs*) until the loss is
    minimized and the model is accurate enough.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This training typically happens in batches of data, using powerful hardware
    like GPUs to handle the heavy computation involved. Over time, the network becomes
    good at identifying whether the fruit is an apple, orange, or banana.
  prefs: []
  type: TYPE_NORMAL
- en: For the AI-900 exam, you may see questions that ask about the differences between
    ML and DL. [Table 4-17](#i04_chapter4_table_17_1742068262048902) lays out these
    key differences to help you get a solid grasp of each.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-17\. Key differences between ML and DL
  prefs: []
  type: TYPE_NORMAL
- en: '| Factor | Machine learning | Deep learning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Complexity | Less | More |'
  prefs: []
  type: TYPE_TB
- en: '| Data requirements | Performs well with structured, smaller datasets | Requires
    large volumes of data to achieve accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Training time | Faster | Longer |'
  prefs: []
  type: TYPE_TB
- en: '| Hardware requirements | Can run on standard CPUs | Often requires GPUs |'
  prefs: []
  type: TYPE_TB
- en: '| Interpretability | Easier | More difficult |'
  prefs: []
  type: TYPE_TB
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve explored the core concepts of ML, diving into essential
    techniques like regression, classification, and clustering. Each of these approaches
    offers unique ways to uncover patterns in data and make accurate predictions.
    They also empower you to tackle real-world challenges head-on.
  prefs: []
  type: TYPE_NORMAL
- en: Quiz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To check your answers, please refer to the [“Chapter 4 Answer Key”](app02.html#answers_chapter_4_sample_questions_1745932457451742).
  prefs: []
  type: TYPE_NORMAL
- en: What is the primary purpose of regression analysis in machine learning (ML)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To categorize data into distinct classes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To cluster similar data points
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To predict a numerical outcome based on variables
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To analyze images and videos
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following is an example of supervised learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Predicting house prices based on features
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Segmenting customers based on purchase history
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Identifying anomalies in financial transactions
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In binary classification, which algorithm is commonly used to predict probabilities
    between two classes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What does the F1 score represent in model evaluation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model’s ability to distinguish between classes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The average of errors in predictions
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A balance between precision and recall
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The total accuracy of the model
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which step in the ML workflow involves using the model to generate predictions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Inferencing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Validation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What type of learning is K-means clustering associated with?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Supervised
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Semisupervised
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Reinforcement
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Unsupervised
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What metric measures how well a regression model explains the variation in data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mean squared error (MSE)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Coefficient of determination (*R*²)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Root mean squared error (RMSE)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Mean absolute error (MAE)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which approach is used to address missing data by estimating based on patterns
    in available data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mean imputation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Predictive imputation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Removal of incomplete data
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Data normalization
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the primary goal of classification in ML?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To identify hidden patterns in unlabeled data
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To predict numerical outcomes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To assign data points to predefined categories
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To generate new content
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which ML technique is suitable for grouping similar data points without labels?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep learning (DL)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
