<html><head></head><body><section data-pdf-bookmark="Chapter 14. Using Third-Party Models and Hubs" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch14_using_third_party_models_and_hubs_1748549787242797">&#13;
      <h1><span class="label">Chapter 14. </span>Using Third-Party Models and Hubs</h1>&#13;
      <p>The success of the open source PyTorch<a contenteditable="false" data-primary="third-party models and hubs" data-secondary="about" data-type="indexterm" id="id1592"/><a contenteditable="false" data-primary="hubs" data-secondary="about" data-type="indexterm" id="id1593"/><a contenteditable="false" data-primary="pretrained models" data-secondary="about" data-type="indexterm" id="id1594"/> framework has led to the growth of supplementary ecosystems. In this chapter, we’ll look at the various options of pretrained models and the associated tools and resources used to download, instantiate, and use them for inference.</p>&#13;
      <p>While the PyTorch framework provides the foundation for deep learning, the community has created numerous repositories and hubs that store models that are ready to use and extend, making it easier for you to use and extend existing work rather than starting from scratch. I like to call this “standing on the shoulders of giants.”</p>&#13;
      <p>Since the advent of generative AI, these hubs have exploded in popularity, and many scenarios of generative ML models within workflows have grown out of this. As a result, when it comes to using pretrained models, there are many options. You might use them directly for inference, taking advantage of those trained on massive datasets that would be impractical to replicate. Or you might use these models as starting points for fine-tuning, adapting them to specific domains or tasks while retaining their learned features. <a contenteditable="false" data-primary="pretrained models" data-secondary="transfer learning" data-type="indexterm" id="id1595"/><a contenteditable="false" data-primary="transfer learning" data-secondary="about" data-type="indexterm" id="id1596"/><a contenteditable="false" data-primary="LoRA (low-rank adaptation)" data-secondary="about third-party models" data-type="indexterm" id="id1597"/>This can take the form of low-rank adaption (LoRA), as we’ll discuss in <a data-type="xref" href="ch20.html#ch20_tuning_generative_image_models_with_lora_and_diffu_1748550104901965">Chapter 20</a>, or <em>transfer learning</em>, in which knowledge from one task is applied to another. Transfer learning or other fine-tuning has become a standard practice, especially when working with limited data or computational resources.</p>&#13;
      <p>The advantages of using pretrained models extend beyond saving computational resources and time. These models often represent state-of-the-art architectures, and they’ve been trained on diverse, high-quality datasets that you may not have direct access to.</p>&#13;
      <p>Additionally, the providers generally release the model with extensive documentation, performance benchmarks, and community support, giving you a long head start. Given the importance of responsible AI, these models often come with model cards that help you understand any research and work done so you can navigate any potential responsibility issues.</p>&#13;
      <p>There is no “One Hub to Rule Them All,” so it’s useful to understand each of the major ones and how you can make the most of them. To that end, we’ll look at some of the more popular ones in this chapter.</p>&#13;
      <p>Hugging Face has become the de facto standard for transformer models, while PyTorch Hub offers officially supported implementations. Platforms like Kaggle provide competition-winning models, and GitHub-based TorchHub enables direct access to research implementations.</p>&#13;
      <p>I think it’s important for you to understand these resources and how to use them effectively. As the field of deep learning continues to advance, these hubs play an increasingly crucial role in widening access to state-of-the-art models and enabling rapid development of AI applications. And as the role of AI developer matures and grows, I’m personally seeing huge growth in the careers of software developers who don’t train models from scratch and instead use or fine-tune existing ones. To that end, I hope this chapter helps you grow!</p>&#13;
      <section data-pdf-bookmark="The Hugging Face Hub" data-type="sect1"><div class="sect1" id="ch14_the_hugging_face_hub_1748549787243062">&#13;
        <h1>The Hugging Face Hub</h1>&#13;
        <p>In recent years, particularly with<a contenteditable="false" data-primary="third-party models and hubs" data-secondary="Hugging Face Hub" data-seealso="Hugging Face Hub" data-type="indexterm" id="ch14hug"/><a contenteditable="false" data-primary="Hugging Face Hub" data-type="indexterm" id="ch14hug2"/><a contenteditable="false" data-primary="hubs" data-secondary="Hugging Face Hub" data-seealso="Hugging Face Hub" data-type="indexterm" id="ch14hug3"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="transformers library" data-type="indexterm" id="id1598"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-type="indexterm" id="id1599"/><a contenteditable="false" data-primary="transformers library (Hugging Face)" data-secondary="about" data-type="indexterm" id="id1600"/><a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="about" data-type="indexterm" id="id1601"/><a contenteditable="false" data-primary="generative AI" data-secondary="transformers library from Hugging Face" data-tertiary="about" data-type="indexterm" id="id1602"/><a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="about" data-type="indexterm" id="id1603"/><a contenteditable="false" data-primary="image generator" data-secondary="diffusers library from Hugging Face" data-tertiary="about" data-type="indexterm" id="id1604"/> the rise of generative AI, the Hugging Face Hub has emerged as a leading platform for discovering and using pretrained ML models, particularly for NLP. Much of its usefulness (and a significant driver of its success) is the open source availability of two things: a transformers library (which makes using pretrained language models very easy to use) and a diffusers library (which does the same for text-to-image generative models like stable diffusion).</p>&#13;
        <p>As a result, what started as a repository for transformer-based models has evolved into a comprehensive ecosystem supporting computer vision, audio processing, and reinforcement learning models. It has grown into a one-stop shop combining version control for models, documentation, and model cards—and because of the PyTorch-friendly libraries like transformers and diffusers, using these models with your Python and PyTorch skills is relatively easy.</p>&#13;
        <p>Collaboration has also been one of the keys to the Hub’s success. You can download, use, and fine-tune models with just a few lines of code, and many developers and organizations have shared their models or fine-tunes with the community. There were over 900,000 publicly available models at the time of writing, so there’s plenty to choose from!</p>&#13;
        <section data-pdf-bookmark="Using Hugging Face Hub" data-type="sect2"><div class="sect2" id="ch14_using_hugging_face_hub_1748549787243149">&#13;
          <h2>Using Hugging Face Hub</h2>&#13;
          <p>Before rolling up your sleeves<a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="using" data-type="indexterm" id="ch14ushf"/> to code with Hugging Face Hub, you should get an account and use it to get an API token. </p>&#13;
          <section data-pdf-bookmark="Getting a Hugging Face token" data-type="sect3"><div class="sect3" id="ch14_getting_a_hugging_face_token_1748549787243220">&#13;
            <h3>Getting a Hugging Face token</h3>&#13;
            <p>This section will walk you through the <a href="http://huggingface.co"><em>HuggingFace.co</em></a> user interface as it existed at the time of writing. It may have changed by the time you’re reading this, but the principles are still the same. Hopefully, they’ll still apply!</p>&#13;
            <p>Start by visiting <a href="http://huggingface.co"><em>Huggingface.co</em></a>, and if you don’t already have an account, you can use the Sign Up button at the top right to create one (see <a data-type="xref" href="#ch14_figure_1_1748549787233053">Figure 14-1</a>).</p>&#13;
            <figure><div class="figure" id="ch14_figure_1_1748549787233053">&#13;
              <img src="assets/aiml_1401.png"/>&#13;
              <h6><span class="label">Figure 14-1. </span>Signing up for Hugging Face</h6>&#13;
            </div></figure>&#13;
            <p class="pagebreak-before">Once you’ve signed up and gotten an account, you can sign in, and in the top-right-hand corner of the page, you’ll see your avatar icon. Select this and a drop-down menu will appear. <a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="token" data-type="indexterm" id="ch14tok3"/>On this menu, you’ll see an option to Access Tokens, and you can select it to view your access tokens (see <a data-type="xref" href="#ch14_figure_2_1748549787233102">Figure 14-2</a>).</p>&#13;
            <figure><div class="figure" id="ch14_figure_2_1748549787233102">&#13;
              <img src="assets/aiml_1402.png"/>&#13;
              <h6><span class="label">Figure 14-2. </span>Access tokens</h6>&#13;
            </div></figure>&#13;
            <p>On this page, you’ll see a Create New Token button, which will take you to a screen where you can specify your token details. Select the Read tab and give the token a name. For example, in <a data-type="xref" href="#ch14_figure_3_1748549787233128">Figure 14-3</a>, you can see where I created a new Read token called PyTorch Book.</p>&#13;
     <p>You’ll also see a pop-up asking you to save your access token (see <a data-type="xref" href="#ch14_figure_4_1748549787233152">Figure 14-4</a>). Note that it tells you that you will not be able to see the token again after you close this dialog modal, so be sure to hit the Copy button to have the token ready for the next steps.</p>&#13;
            <figure><div class="figure" id="ch14_figure_3_1748549787233128">&#13;
              <img src="assets/aiml_1403.png"/>&#13;
              <h6><span class="label">Figure 14-3. </span>Creating an access token</h6>&#13;
            </div></figure>&#13;
            <figure><div class="figure" id="ch14_figure_4_1748549787233152">&#13;
              <img src="assets/aiml_1404.png"/>&#13;
              <h6><span class="label">Figure 14-4. </span>Saving your access token</h6>&#13;
            </div></figure>&#13;
            <p>If you <em>do</em> forget the token, you’ll have to Invalidate and Refresh it on the token list screen. To do this, you select the three dots to the right of the token and then select Invalidate and Refresh from the drop-down menu (see <a data-type="xref" href="#ch14_figure_5_1748549787233175">Figure 14-5</a>).</p>&#13;
            <figure><div class="figure" id="ch14_figure_5_1748549787233175">&#13;
              <img src="assets/aiml_1405.png"/>&#13;
              <h6><span class="label">Figure 14-5. </span>Invalidating and refreshing a token</h6>&#13;
            </div></figure>&#13;
            <p>Then, go back to the dialog from <a data-type="xref" href="#ch14_figure_4_1748549787233152">Figure 14-4</a> with a new token value. Copy it if you want to use it.</p>&#13;
            <p>Now that you have a token, let’s explore how to configure Colab to use it.<a contenteditable="false" data-primary="" data-startref="ch14tok3" data-type="indexterm" id="id1605"/></p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Getting permission to use models" data-type="sect3"><div class="sect3" id="ch14_getting_permission_to_use_models_1748549787243285">&#13;
            <h3>Getting permission to use models</h3>&#13;
            <p>Many models on Hugging Face<a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="permission to use models" data-type="indexterm" id="id1606"/> will require additional permission to use them. In those cases, you should always check the model page and apply for permission on the link provided. Your permission to use the model will be tracked using the Hugging Face token. If you do <em>not</em> have permission, you’ll see an error like this:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="n">GatedRepoError</code><code class="p">:</code> <code class="mi">401</code> <code class="n">Client</code> <code class="n">Error</code><code class="o">.</code>&#13;
<code class="p">(</code><code class="n">Request</code> <code class="n">ID</code><code class="p">:</code> <code class="p">[</code><code class="o">...</code><code class="p">])</code>&#13;
<code class="n">Cannot</code> <code class="n">access</code> <code class="n">gated</code> <code class="n">repo</code> <code class="k">for</code> <code class="n">url</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>&#13;
<code class="n">Access</code> <code class="n">to</code> <code class="n">model</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code> <code class="ow">is</code> <code class="n">restricted</code><code class="o">.</code>&#13;
<code class="n">You</code> <code class="n">must</code> <code class="n">have</code> <code class="n">access</code> <code class="n">to</code> <code class="n">it</code> <code class="ow">and</code> <code class="n">be</code> <code class="n">authenticated</code> <code class="n">to</code> <code class="n">access</code> <code class="n">it</code><code class="o">.</code>&#13;
<code class="n">Please</code> <code class="n">log</code> <code class="ow">in</code><code class="o">.</code></pre>&#13;
            <p>When this happens, the easiest thing to do is use the model name to find its landing page on the Hugging Face Hub and follow the steps to get permission to use it from there.</p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Configuring Colab for a Hugging Face token" data-type="sect3"><div class="sect3" id="ch14_configuring_colab_for_a_hugging_face_token_1748549787243350">&#13;
            <h3>Configuring Colab for a Hugging Face token</h3>&#13;
            <p>If you want to use models<a contenteditable="false" data-primary="Google Colab" data-secondary="Hugging Face token" data-type="indexterm" id="ch14cotok"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="token in Google Colab" data-type="indexterm" id="ch14cotok4"/> from Hugging Face in Google Colab, then you need to configure a Colab secret in which code executing in Colab will read the token value, send it to Hugging Face on your behalf, and grant you access to the object. </p>&#13;
            <p>It’s pretty easy to do. First, in Colab, you select the key icon on the left of the screen (see <a data-type="xref" href="#ch14_figure_6_1748549787233195">Figure 14-6</a>).</p>&#13;
           <p>You should see a list of secrets that looks like the one in <a data-type="xref" href="#ch14_figure_7_1748549787233216">Figure 14-7</a>. Don’t worry if you don’t have any API keys there yet. At the bottom of the list is a button that says Add new secret, and you’ll select that.</p>&#13;
            <figure><div class="figure" id="ch14_figure_6_1748549787233195">&#13;
              <img src="assets/aiml_1406.png"/>&#13;
              <h6><span class="label">Figure 14-6. </span>Selecting the Colab secrets </h6>&#13;
            </div></figure>&#13;
            <figure><div class="figure" id="ch14_figure_7_1748549787233216">&#13;
              <img src="assets/aiml_1407.png"/>&#13;
              <h6><span class="label">Figure 14-7. </span>List of Colab secrets</h6>&#13;
            </div></figure>&#13;
            <p>Use the name “HF_TOKEN” in the Name field, and paste the value of the key into the Value field. Then, flip the switch to give Notebook Access to the secret (see <a data-type="xref" href="#ch14_figure_8_1748549787233237">Figure 14-8</a>).</p>&#13;
            <figure><div class="figure" id="ch14_figure_8_1748549787233237">&#13;
              <img src="assets/aiml_1408.png"/>&#13;
              <h6><span class="label">Figure 14-8. </span>Configuring the HF_TOKEN in Colab</h6>&#13;
            </div></figure>&#13;
            <p>Your code in Colab will now use this token to access Hugging Face.<a contenteditable="false" data-primary="" data-startref="ch14cotok" data-type="indexterm" id="id1607"/><a contenteditable="false" data-primary="" data-startref="ch14cotok4" data-type="indexterm" id="id1608"/></p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Using the Hugging Face token in code" data-type="sect3"><div class="sect3" id="ch14_using_the_hugging_face_token_in_code_1748549787243415">&#13;
            <h3>Using the Hugging Face token in code</h3>&#13;
            <p>If you just want to use the token directly<a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="token used in code" data-type="indexterm" id="id1609"/> in your code, whether in Colab or not, you’ll have to log in to the Hugging Face Hub in your code and pass the key to it. It’s pretty straightforward, with the Hugging Face Hub libraries providing the required support.</p>&#13;
            <p>To start, just import the <code>login</code> class like this:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">huggingface_hub</code> <code class="kn">import</code> <code class="n">login</code></pre>&#13;
            <p>You can then pass the token to the <code>login</code> class and initialize by using it in your Python session like this:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="n">login</code><code class="p">(</code><code class="n">token</code><code class="o">=</code><code class="s2">"YOUR_TOKEN_HERE"</code><code class="p">)</code></pre>&#13;
            <p>The Hugging Face classes will then use the token for the remainder of your session.<a contenteditable="false" data-primary="" data-startref="ch14ushf" data-type="indexterm" id="id1610"/></p>&#13;
          </div></section>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Using a Model From Hugging Face Hub" data-type="sect2"><div class="sect2" id="ch14_using_a_model_from_hugging_face_hub_1748549787243480">&#13;
          <h2>Using a Model From Hugging Face Hub</h2>&#13;
          <p>Once you have your token set up,<a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="using a model from" data-type="indexterm" id="ch14hfmod3"/> getting and using a model is very simple. For this walk-through, we’ll explore using a language model for text classification and sentiment analysis. This will require you to use the transformers library, so be sure to have it installed with this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">pip</code> <code class="n">install</code> <code class="n">transformers</code></pre>&#13;
          <p>The <code>transformers</code> API offers<a contenteditable="false" data-primary="APIs" data-secondary="transformers API from Hugging Face" data-type="indexterm" id="id1611"/><a contenteditable="false" data-primary="pipeline class in transformers API from Hugging Face" data-type="indexterm" id="id1612"/><a contenteditable="false" data-primary="transformers library (Hugging Face)" data-secondary="using a model from Hugging Face" data-type="indexterm" id="id1613"/><a contenteditable="false" data-primary="generative AI" data-secondary="transformers library from Hugging Face" data-tertiary="using a model from Hugging Face" data-type="indexterm" id="id1614"/> a pipeline class that lets you download and use a model based on its name in the Hugging Face repository. This one was fine-tuned using the SST sentiment analysis dataset from Stanford:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Load a small sentiment analysis model</code>&#13;
<code class="n">classifier</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"sentiment-analysis"</code><code class="p">,</code> &#13;
             <code class="n">model</code><code class="o">=</code><code class="s2">"distilbert-base-uncased-finetuned-sst-2-english"</code><code class="p">)</code></pre>&#13;
          <p>Pipeline offers much more than just downloading. It encapsulates what’s needed <span class="keep-together">to perform common</span>  tasks with models. The first parameter, which in this case is <span class="keep-together"><code>sentiment analysis</code>,</span> describes the overall pipeline task that you’ll do. Transformers offer a variety of task types, including this, text classification, text generation, and a whole lot more. </p>&#13;
                    <p>When using the <code>pipeline</code> class, a number of key steps take place under the hood. These include the following:</p>&#13;
          <dl>&#13;
            <dt>Tokenization</dt>&#13;
            <dd>&#13;
              <p>In this step, the text is converted into tokens (as we discussed in <a data-type="xref" href="ch04.html#ch04_using_data_with_pytorch_1748548966496246">Chapter 4</a>).</p>&#13;
            </dd>&#13;
            <dt>Input processing</dt>&#13;
            <dd>&#13;
              <p>In this step, special tokens are added and the text is converted into tensors.</p>&#13;
            </dd>&#13;
            <dt>The model forward pass</dt>&#13;
            <dd>&#13;
              <p> In this step, the tokenized input is passed through the model’s layers to get a result.</p>&#13;
            </dd>&#13;
            <dt>Output processing</dt>&#13;
            <dd>&#13;
              <p>In this step, the output is decoded from tensors back to the desired labels.</p>&#13;
            </dd>&#13;
          </dl>&#13;
          <p>You can see this workflow in <a data-type="xref" href="#ch14_figure_9_1748549787233257">Figure 14-9</a>.</p>&#13;
          <figure><div class="figure" id="ch14_figure_9_1748549787233257">&#13;
            <img src="assets/aiml_1409.png"/>&#13;
            <h6><span class="label">Figure 14-9. </span>NLP pipeline flow</h6>&#13;
          </div></figure>&#13;
          <p>Then, when you want to use it, the burden of coding is removed from you as the developer and you just use the model like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Test the model</code>&#13;
<code class="n">text</code> <code class="o">=</code> <code class="s2">"I love programming with PyTorch!"</code>&#13;
<code class="n">result</code> <code class="o">=</code> <code class="n">classifier</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">result</code><code class="p">)</code>  <code class="c1"># Output: [{'label': 'POSITIVE', 'score': 0.9998}]</code></pre>&#13;
          <p>All of the steps required for text classification and sentiment analysis are encapsulated and abstracted away from you. It makes your code much simpler!</p>&#13;
          <p>Similarly, if you want to use<a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-tertiary="using a model from Hugging Face" data-type="indexterm" id="id1615"/><a contenteditable="false" data-primary="pipelines from diffusers library of Hugging Face" data-type="indexterm" id="id1616"/><a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="using a model from Hugging Face" data-type="indexterm" id="id1617"/><a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="using a model from Hugging Face" data-type="indexterm" id="id1618"/><a contenteditable="false" data-primary="image generator" data-secondary="diffusers library from Hugging Face" data-tertiary="using a model from Hugging Face" data-type="indexterm" id="id1619"/> the diffusers library, it comes with a number of pipelines that are often associated with a model type. So, for example, if you want to use the popular Stable Diffusion model for text to image—in which you give a prompt and the model will draw an image based on that prompt—you can do so very easily. </p>&#13;
          <p>Let’s explore this with an example.</p>&#13;
          <p>First, from the diffusers library, you can import the pipeline that supports Stable Diffusion like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">diffusers</code> <code class="kn">import</code> <code class="n">StableDiffusionPipeline</code></pre>&#13;
          <p>With this, you can specify the name of the model in the Hugging Face repository and use it to initialize the pipeline:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>&#13;
<code class="kn">from</code> <code class="nn">diffusers</code> <code class="kn">import</code> <code class="n">StableDiffusionPipeline</code>&#13;
 &#13;
<code class="n">model_id</code> <code class="o">=</code> <code class="s2">"CompVis/stable-diffusion-v1-4"</code>&#13;
<code class="n">device</code> <code class="o">=</code> <code class="s2">"cuda"</code>&#13;
 &#13;
 &#13;
<code class="n">pipe</code> <code class="o">=</code> <code class="n">StableDiffusionPipeline</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_id</code><code class="p">,</code> &#13;
                                               <code class="n">torch_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">)</code>&#13;
<code class="n">pipe</code> <code class="o">=</code> <code class="n">pipe</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>&#13;
          <p>Similar to the preceding text example, the pipeline encapsulates and abstracts a number of steps away from you. This means you can write relatively simple code like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">prompt</code> <code class="o">=</code> <code class="s2">"a cute colorful cartoon cat"</code>&#13;
<code class="n">image</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
 &#13;
<code class="n">image</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"cat.png"</code><code class="p">)</code></pre>&#13;
          <p>But a number of steps have been handled for you. These include the following:</p>&#13;
          <ol>&#13;
            <li>&#13;
              <p>In the text encoding step, Stable Diffusion uses a technology called CLIP to take the text prompts and turn them into embeddings that the model can understand.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p>An initial image is then constructed from random noise.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p>The embeddings are then fed into the model, which uses a process of denoising to create pixels and features that match the embeddings.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p>The final output of the model is then converted from tensors into an RGB image.</p>&#13;
            </li>&#13;
          </ol>&#13;
          <p>The overall process of <a contenteditable="false" data-primary="online resources" data-secondary="image created from text information" data-type="indexterm" id="id1620"/><a contenteditable="false" data-primary="image generator" data-secondary="stable diffusion models" data-tertiary="information online" data-type="indexterm" id="id1621"/><a contenteditable="false" data-primary="generative AI" data-secondary="stable diffusion models" data-tertiary="information online" data-type="indexterm" id="id1622"/>creating an image from text is beyond the scope of this chapter, but it’s well explained in <a href="https://oreil.ly/zNjUT"> this video from Google Research</a>.</p>&#13;
          <p>The important thing to note here is that because the image-generation process begins by creating random noise, any images you create with the preceding code will be different. So, don’t be alarmed if you’re not getting the same picture consistently! There are ways of guiding this noise by using a seed, which we’ll discuss in later chapters.<a contenteditable="false" data-primary="" data-startref="ch14hfmod3" data-type="indexterm" id="id1623"/><a contenteditable="false" data-primary="" data-startref="ch14hug" data-type="indexterm" id="id1624"/><a contenteditable="false" data-primary="" data-startref="ch14hug2" data-type="indexterm" id="id1625"/><a contenteditable="false" data-primary="" data-startref="ch14hug3" data-type="indexterm" id="id1626"/></p>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="PyTorch Hub" data-type="sect1"><div class="sect1" id="ch14_pytorch_hub_1748549787243547">&#13;
        <h1>PyTorch Hub</h1>&#13;
        <p>One of the primary reasons<a contenteditable="false" data-primary="hubs" data-secondary="PyTorch hub" data-type="indexterm" id="ch14pth"/><a contenteditable="false" data-primary="third-party models and hubs" data-secondary="PyTorch hub" data-type="indexterm" id="ch14pth2"/><a contenteditable="false" data-primary="PyTorch hub" data-type="indexterm" id="ch14pth3"/> for PyTorch’s success—particularly in the research community—is the foresight of the developers in creating a hub where people could share their models. While this functionality has been massively superseded by Hugging Face Hub, as described earlier in this chapter, it’s still worth looking at because many <a contenteditable="false" data-primary="YOLO (You Only Look Once) model" data-type="indexterm" id="id1627"/>new and innovative models (or updates to existing ones) like YOLO are often shared on the Hub.</p>&#13;
        <div data-type="note" epub:type="note"><h6>Note</h6>&#13;
          <p>YOLO is “You Only Look Once,"” a popular and efficient object detection model.</p>&#13;
        </div>&#13;
        <p>As with Hugging Face Hub, the primary benefit of PyTorch Hub is that it gives you access either to models that you may not have the compute resources to train yourself or to the required data used to train them. At its core, PyTorch Hub functions as a centralized repository where researchers and developers can publish, share, and access models that have been trained on diverse datasets across various domains.</p>&#13;
        <p>In this section, we’ll explore PyTorch Hub and the APIs that you’ll use to access models within it. Unfortunately, the APIs aren’t as consistent as they could be, and it can sometimes be a little bit of a struggle to understand everything. But hopefully, this chapter will help!</p>&#13;
        <p>We’ll start with the PyTorch Vision libraries, which are composed of image classifiers, object detectors, and other computer vision models.<a contenteditable="false" data-primary="hubs" data-secondary="PyTorch hub" data-tertiary="using Vision models" data-type="indexterm" id="ch14vis"/><a contenteditable="false" data-primary="third-party models and hubs" data-secondary="PyTorch hub" data-tertiary="using Vision models" data-type="indexterm" id="ch14vis2"/><a contenteditable="false" data-primary="PyTorch hub" data-secondary="using Vision models" data-type="indexterm" id="ch14vis3"/></p>&#13;
        <section data-pdf-bookmark="Using PyTorch Vision Models" data-type="sect2"><div class="sect2" id="ch14_using_pytorch_vision_models_1748549787243620">&#13;
          <h2>Using PyTorch Vision Models</h2>&#13;
          <p>Before you begin, you’ll need to ensure that you have torchvision installed. Use this:<a contenteditable="false" data-primary="PyTorch hub" data-secondary="using Vision models" data-tertiary="torchvision installation" data-type="indexterm" id="id1628"/><a contenteditable="false" data-primary="torchvision library" data-secondary="installation" data-type="indexterm" id="id1629"/><a contenteditable="false" data-primary="hubs" data-secondary="PyTorch hub" data-tertiary="torchvision installation" data-type="indexterm" id="id1630"/></p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">pip</code> <code class="n">install</code> <code class="n">torchvision</code></pre>&#13;
          <p>When you have installed it, you’ll see the install version. This is really important when using Hub, in particular when you want to list the models to see what’s available. You can also see the versions of these <a href="https://oreil.ly/KIiFD">on GitHub</a>.</p>&#13;
          <p>So, to list the models that are available, you’ll use code like this: </p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">models</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">hub</code><code class="o">.</code><code class="n">list</code><code class="p">(</code><code class="s1">'pytorch/vision:v0.20.1'</code><code class="p">)</code>&#13;
<code class="k">for</code> <code class="n">model</code> <code class="ow">in</code> <code class="n">models</code><code class="p">:</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="n">model</code><code class="p">)</code></pre>&#13;
          <p>Note the version number (which you can get from the GitHub page we just <span class="keep-together">mentioned).</span></p>&#13;
          <p>At the time of writing, there were close to a hundred models on this list. Do note that your version of the tag should match your version of torchvision, so if you are having problems, you can use this code to see your current version:<a contenteditable="false" data-primary="torchvision library" data-secondary="version displayed" data-type="indexterm" id="id1631"/><a contenteditable="false" data-primary="PyTorch hub" data-secondary="using Vision models" data-tertiary="torchvision version displayed" data-type="indexterm" id="id1632"/><a contenteditable="false" data-primary="hubs" data-secondary="PyTorch hub" data-tertiary="torchvision version displayed" data-type="indexterm" id="id1633"/></p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="nb">print</code><code class="p">(</code><code class="n">torchvision</code><code class="o">.</code><code class="n">__version__</code><code class="p">)</code></pre>&#13;
          <p class="pagebreak-before less_space">You can also choose a model from those available and load it into memory like this:<a contenteditable="false" data-primary="ResNet model" data-secondary="PyTorch Hub" data-type="indexterm" id="id1634"/></p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Load ResNet-50 from PyTorch Hub</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">hub</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s1">'pytorch/vision:v0.20.1'</code><code class="p">,</code> <code class="s1">'resnet50'</code><code class="p">,</code> &#13;
                        <code class="n">pretrained</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
 &#13;
<code class="c1"># Set the model to evaluation mode</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code></pre>&#13;
          <p>The model will be downloaded, cached, and then placed into evaluation mode.<a contenteditable="false" data-primary="ResNet model" data-type="indexterm" id="id1635"/><a contenteditable="false" data-primary="inference" data-secondary="ResNet model" data-type="indexterm" id="id1636"/> </p>&#13;
          <p>Next up, you’ll need to get your data ready for inference, and that requires you to have some domain knowledge of the model. So, for example, in the code we just cited, we used <code>resnet50</code> as the model. This model (ResNet) is a very popular one for image classification that uses CNNs. A great place to go to learn more about this is the <a href="https://pytorch.org/hub">PyTorch Hub site</a>. </p>&#13;
          <p>From here, you can dig into model details—such as the size of the desired input, the labels that it can classify, etc. Then, with this information in hand, you can write inference code for the model. Here’s an example:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"> &#13;
<code class="c1"># Load and preprocess the image</code>&#13;
<code class="n">image_path</code> <code class="o">=</code> <code class="s2">"example.jpg"</code>  <code class="c1"># Replace with your image path</code>&#13;
<code class="n">image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">image_path</code><code class="p">)</code><code class="o">.</code><code class="n">convert</code><code class="p">(</code><code class="s1">'RGB'</code><code class="p">)</code>&#13;
 &#13;
<code class="n">preprocess</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code>&#13;
    <code class="n">transforms</code><code class="o">.</code><code class="n">Resize</code><code class="p">(</code><code class="mi">256</code><code class="p">),</code>&#13;
    <code class="n">transforms</code><code class="o">.</code><code class="n">CenterCrop</code><code class="p">(</code><code class="mi">224</code><code class="p">),</code>&#13;
    <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">(),</code>&#13;
    <code class="n">transforms</code><code class="o">.</code><code class="n">Normalize</code><code class="p">(</code><code class="n">mean</code><code class="o">=</code><code class="p">[</code><code class="mf">0.485</code><code class="p">,</code> <code class="mf">0.456</code><code class="p">,</code> <code class="mf">0.406</code><code class="p">],</code> &#13;
                         <code class="n">std</code><code class="o">=</code><code class="p">[</code><code class="mf">0.229</code><code class="p">,</code> <code class="mf">0.224</code><code class="p">,</code> <code class="mf">0.225</code><code class="p">]),</code>&#13;
<code class="p">])</code>&#13;
<code class="n">input_tensor</code> <code class="o">=</code> <code class="n">preprocess</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>&#13;
<code class="n">input_batch</code> <code class="o">=</code> <code class="n">input_tensor</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code></pre>&#13;
          <p>You saw similar code in <a data-type="xref" href="ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912">Chapter 3</a> and <a data-type="xref" href="ch04.html#ch04_using_data_with_pytorch_1748548966496246">Chapter 4</a>, where you explored building your own image classifier. This code resizes the images to 256 × 256 and then crops a 224 × 224 image from the center of that. Images are typically 32 bit RGB, in which each pixel is represented by 8 bits of alpha, 8 bits of red, 8 bits of green, and 8 bits of blue. However, for image classification, the neural network usually expects normalized values (i.e., between 0 and 1), so the transform to normalize the image performs this.</p>&#13;
          <p>When you’re using models in PyTorch, even though you know the desired dimensions (in this case, 224 × 224), you’ll also need to batch the images for inference, even if you’re just doing a single image. The <code>input_tensor.unsqueeze(0)</code> adds this extra dimension to the input tensor to handle this. </p>&#13;
          <p>Next up, you’ll do the actual inference, which just means you’ll load the model onto the appropriate device—which is cuda if you have a GPU and the CPU otherwise. You’ll then pass the input batch to the model to get an output:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Perform inference</code>&#13;
<code class="n">device</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s2">"cuda"</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">()</code> <code class="k">else</code> <code class="s2">"cpu"</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>&#13;
<code class="n">input_batch</code> <code class="o">=</code> <code class="n">input_batch</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>&#13;
 &#13;
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>&#13;
    <code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">input_batch</code><code class="p">)</code>&#13;
 &#13;
<code class="n">_</code><code class="p">,</code> <code class="n">predicted_idx</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code></pre>&#13;
          <p>The <code>predicted_idx</code> is the output for the class that has the highest probability of matching the input image. You’ll see something numeric, like <code>tensor([153])</code>, in the output here. Recall that models’ output layers will be neurons that correspond to the index of the required label. In the case of ResNet, the number 153 is a Maltese dog. </p>&#13;
          <p>To decode this, you can use code like the following. You can find the URL of the labels file by digging into the model page on PyTorch Hub:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Get class labels and map to prediction</code>&#13;
<code class="n">url</code> <code class="o">=</code> <code class="s2">"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/</code><code class="w"/>&#13;
                                         <code class="n">master</code><code class="o">/</code><code class="n">imagenet</code><code class="o">-</code><code class="n">simple</code><code class="o">-</code><code class="n">labels</code><code class="o">.</code><code class="n">json</code><code class="s2">"</code><code class="w"/>&#13;
<code class="n">class_labels</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">urllib</code><code class="o">.</code><code class="n">request</code><code class="o">.</code><code class="n">urlopen</code><code class="p">(</code><code class="n">url</code><code class="p">))</code>&#13;
<code class="n">predicted_label</code> <code class="o">=</code> <code class="n">class_labels</code><code class="p">[</code><code class="n">predicted_idx</code><code class="p">]</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"Predicted Label:"</code><code class="p">,</code> <code class="n">predicted_label</code><code class="p">)</code></pre>&#13;
          <p>This code will then print out the label for <code>predicted_idx</code>. So, in the case of <span class="keep-together"><code>tensor([153])</code></span>, this will output the label for a Maltese dog.<a contenteditable="false" data-primary="" data-startref="ch14vis" data-type="indexterm" id="id1637"/><a contenteditable="false" data-primary="" data-startref="ch14vis2" data-type="indexterm" id="id1638"/><a contenteditable="false" data-primary="" data-startref="ch14vis3" data-type="indexterm" id="id1639"/></p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Natural Language Processing" data-type="sect2"><div class="sect2" id="ch14_natural_language_processing_1748549787243682">&#13;
          <h2>Natural Language Processing</h2>&#13;
          <p>The PyTorch Hub for NLP ultimately<a contenteditable="false" data-primary="third-party models and hubs" data-secondary="PyTorch hub" data-tertiary="natural language processing" data-type="indexterm" id="id1640"/><a contenteditable="false" data-primary="hubs" data-secondary="PyTorch hub" data-tertiary="natural language processing" data-type="indexterm" id="id1641"/><a contenteditable="false" data-primary="PyTorch hub" data-secondary="natural language processing" data-type="indexterm" id="id1642"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="PyTorch hub" data-type="indexterm" id="id1643"/><a contenteditable="false" data-primary="fairseq models" data-type="indexterm" id="id1644"/> directs to two different repositories: those implemented using <a href="https://oreil.ly/j1w3-">Hugging Face Transformers</a> as outlined earlier in this chapter and those from <a href="https://oreil.ly/mmQeY">Facebook’s fairseq research team</a>.</p>&#13;
          <p>If you use the fairseq models, you may encounter a lot of sharp edges. Therefore, I thoroughly recommend setting up an environment with Python 3.11 (no later than that).</p>&#13;
          <p>Within that, you can set up fairseq2 like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">pip</code> <code class="n">install</code> <code class="n">fairseq</code></pre>&#13;
          <p>You’ll likely also need other dependencies like hydra-core, OmegaConf, and requests.</p>&#13;
          <p class="pagebreak-before less_space">Once you have the full system set up, you can use fairseq models like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>&#13;
 &#13;
<code class="n">en2de</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">hub</code><code class="o">.</code><code class="n">load</code><code class="p">(</code>&#13;
     <code class="s1">'pytorch/fairseq'</code><code class="p">,</code><code class="s1">'transformer.wmt19.en-de.single_model'</code><code class="p">)</code>&#13;
 &#13;
<code class="n">en2de</code><code class="o">.</code><code class="n">translate</code><code class="p">(</code><code class="s1">'Hello Pytorch'</code><code class="p">,</code> <code class="n">beam</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>&#13;
<code class="c1"># 'Hallo Pytorch'</code></pre>&#13;
          <p>Do note that the environment will be very picky about which versions of PyTorch, pip, and many other libraries you can use. It can make for a very brittle experience, and unless you really want to use the models from the fairseq repository, I’d recommend just going with the Hugging Face transformer versions.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Other Models" data-type="sect2"><div class="sect2" id="ch14_other_models_1748549787243739">&#13;
          <h2>Other Models</h2>&#13;
          <p>PyTorch Hub also has repos<a contenteditable="false" data-primary="third-party models and hubs" data-secondary="PyTorch hub" data-tertiary="other model types" data-type="indexterm" id="id1645"/><a contenteditable="false" data-primary="hubs" data-secondary="PyTorch hub" data-tertiary="other model types" data-type="indexterm" id="id1646"/><a contenteditable="false" data-primary="PyTorch hub" data-secondary="other model types" data-type="indexterm" id="id1647"/> for a variety of other model types, including audio, reinforcement learning, generative AI, and more. I’ve found the best way to explore them is to browse at the <a href="https://pytorch.org/hub">PyTorch Hub</a> and use the links on the landing pages to navigate to the requisite GitHub.<a contenteditable="false" data-primary="" data-startref="ch14pth" data-type="indexterm" id="id1648"/><a contenteditable="false" data-primary="" data-startref="ch14pth2" data-type="indexterm" id="id1649"/><a contenteditable="false" data-primary="" data-startref="ch14pth3" data-type="indexterm" id="id1650"/></p>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch14_summary_1748549787243794">&#13;
        <h1>Summary</h1>&#13;
        <p>This chapter explores the ecosystem of pretrained models and model repositories for PyTorch, focusing on two major platforms: Hugging Face Hub and PyTorch Hub. While PyTorch Hub was the granddaddy that started the ball rolling, Hugging Face Hub has rapidly taken over as the go-to resource for pretrained models.</p>&#13;
        <p>We took a look at how to use the transformers and diffusers libraries from Hugging Face, which encapsulate model loading and instantiation. With these, you have the keys to over 900,000 publicly available models. As a bonus, many of these have comprehensive documentation and model cards to get you up and running quickly and responsibly. You also got hands-on with using them, including setting up your account and getting authentication from Hugging Face using tokens. </p>&#13;
        <p>Hugging Face APIs offer pipelines that encapsulate many of the common tasks of using models, such as tokenization and sequencing for NLP under the hood, making your coding surface much easier. We explored these with a text sentiment analysis scenario as well as another for image classification.</p>&#13;
        <p>While PyTorch Hub has a lot less in it and accessing the models can be brittle in comparison, it’s worth looking at because it’s still well used in the research community. We looked at how to access PyTorch Vision models, prepare data for inference, and handle model outputs. The Hub also includes practical examples of using pretrained models like ResNet50 for image classification.</p>&#13;
        <p class="pagebreak-before less_space">Ultimately, you should consider the advantages of using pretrained models, which have been built by expert researchers who have used expensive hardware and high-quality datasets that you may not otherwise have access to. To that end, you may find that using and fine-tuning existing models rather than training from scratch might be better for your scenario. We’re going to explore that over the next few chapters, starting with <a data-type="xref" href="ch15.html#ch15_transformers_and_transformers_1748549808974580">Chapter 15</a>, where we will go deeper into using LLMs with Hugging Face Transformers.</p>&#13;
      </div></section>&#13;
    </div></section></body></html>