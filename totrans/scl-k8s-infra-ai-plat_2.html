<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Making Training Repeatable"><div class="chapter" id="ch03_making_training_repeatable_1738498450655759">
      <h1><span class="label">Chapter 3. </span>Making Training Repeatable</h1>
      <p>In <a data-type="xref" href="ch02.html#ch02_model_development_on_kubernetes_1738498450538975">Chapter 2</a>, you learned about techniques for customizing a model, including fine-tuning, a special case of model training. Once you’ve fine-tuned or trained your model for the first time, you might be tempted to think that you are done with model development and all you have left is to evaluate and deploy your model. </p>
      <p>You’d be half right. But because the data that informs the model will likely change over time, the model must be regularly retrained throughout its lifetime to ensure that it can continue to deliver value. In this chapter, you will dive into the AI model lifecycle, learning how to track model versions, automate model training, and implement GitOps for model training pipelines.</p>
      <section data-type="sect1" data-pdf-bookmark="Retraining and the Model Development Lifecycle"><div class="sect1" id="ch03_retraining_and_the_model_development_lifecycle_1738498450655873">
        <h1>Retraining and the <span class="keep-together">Model Development Lifecycle</span></h1>
        <p>The world changes. The data that we use to describe the real world, then, must change too. Consequently, if the data changes, then any models that attempt to model a problem in the real world must also change.</p>
        <p>Since models in production are static, over time the input data that a given model will process in production for inference requests will differ from the data that the model was trained on. This variability can come from any number of sources, such as changing user behavior over time, seasonality effects in the data, changes to the input data format, etc. The phenomenon is called <em>data drift</em>.</p>
        <p>Data drift isn’t the only reason to retrain a model, however. Retraining is a good option when a metric that is monitored in production (such as accuracy, model responsiveness, compute resources, etc.) falls outside its optimal range.</p>
        <p>When and how often a model should be retrained are two key considerations, and they depend heavily on your use case and training data. There are two main choices here: either regularly retrain the model on some fixed cadence, or retrain the model on demand.</p>
        <p>Retraining a model at a fixed cadence can be costly if the cadence is rapid or if the retraining doesn’t actually show any improvement. This method assumes that the data changes according to some predictable pattern that can be detected at a chosen retraining cadence. If retraining doesn’t show any improvement, it’s possible that the data isn’t changing in such a way to be captured by the regular cadence.</p>
        <p>The other option is to retrain your model on demand. This ensures that models are not retrained unnecessarily, but it requires reliable monitoring of the performance of the current version of the model (discussed in <a data-type="xref" href="ch04.html#ch04_model_deployment_and_monitoring_1738498450837987">Chapter 4</a>) and well-defined thresholds for when the performance has sufficiently degraded.</p>
        <p>The full lifecycle of a model, then, looks something like <a data-type="xref" href="#ch03_figure_1_1738498450651715">Figure 3-1</a>.</p>
        <figure><div id="ch03_figure_1_1738498450651715" class="figure">
          <img src="assets/skia_0301.png" width="1218" height="808"/>
          <h6><span class="label">Figure 3-1. </span>The full lifecycle of an AI model</h6>
        </div></figure>
        <p>In practice, this is a never-ending cycle, which repeats most often from the evaluation and monitoring stages, where unacceptable performance is typically discovered.</p>
        <p>Where <a data-type="xref" href="ch01.html#ch01_figure_1_1738498450402392">Figure 1-1</a> was focused on the model development cycle (stage 2), <a data-type="xref" href="#ch03_figure_1_1738498450651715">Figure 3-1</a> zooms out to view the entire lifecycle of a model. The lifecycle starts with gathering data, continues through developing training code, executing the training job, evaluating the trained model, promoting the model to production, and monitoring the served model.</p>
        <p>Crucially, this lifecycle can repeat at any stage (with the exception of training code development, which typically remains static), most commonly after the evaluation and monitoring stages (4 and 6). From those, it’s common to discover that either the training job needs to be run again, or that more or higher quality data needs to be collected. If more data is to be collected, typically the existing training code can be used as-is. In this case, stage 3 would follow stage 1.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Tracking Model Versions"><div class="sect1" id="ch03_tracking_model_versions_1738498450655938">
        <h1>Tracking Model Versions</h1>
        <p>While they iterate on developing a model, data scientists will run many experiments by varying the dataset they use to create the model, the model’s architecture, the hyperparameters used for training the model, and more. Even after the initial handoff of the model to production, future retraining cycles of the model will yield new variants.</p>
        <p>Enterprises need robust solutions for tracking all of these versions of a given model. While version control systems have been table stakes for traditional software projects for decades now, model version control systems are still in their infancy despite the heavy reliance on models by enterprises of all sizes. </p>
        <p>Model version tracking benefits the whole enterprise, from unlocking the ability of data scientists to recreate previous experiments, share their experiments with colleagues, and revert to a model from a previous experiment, to enabling model auditing and ensuring responsible AI use. For example, in order to explain a given model’s results and how it was created, it is necessary to know which version of a model was served in production at what time and how that model was created, including the data sources that went into the model.</p>
        <p>More and more often, enterprises use a centralized model registry throughout the model’s lifecycle, and this is rapidly becoming a recognized best practice. During model training and development, model training code should integrate with the model registry to register each subsequent version of the model as a distinct model artifact. Alongside the model artifact, model training code should register metadata about the training data and code that was used to create the model. The model registry should also be integrated at model deployment and monitoring stages, which will be discussed in <a data-type="xref" href="ch04.html#ch04_model_deployment_and_monitoring_1738498450837987">Chapter 4</a>.</p>
        <p>Many training platforms that are available today offer model registries as part of their product lineup. It is important to choose a platform with a strong model registry that is well integrated into the platform’s distributed training engine.</p>
        <p>The <a href="https://kubeflow.org">Kubeflow project</a> offers a powerful integrated solution, where the Kubeflow Training Operator, Kubeflow Pipelines, <a href="https://oreil.ly/NgK4E">Katib</a>, and the Kubeflow Model Registry can be used together to track training results in a central database. Similarly, the <a href="https://mlflow.org">MLflow project</a> offers a robust model registry solution that can be combined with MLflow Runs and Experiments to streamline tracking of model versions.</p>
        <p>Today, proper model versioning tends to require behavior change by data scientists to deliberately integrate model version tracking into their training code and workflows. As projects like MLflow and Kubeflow evolve and become better integrated with frameworks like PyTorch, you can expect model version tracking capabilities to integrate seamlessly out of the box.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Automating Model Training"><div class="sect1" id="ch03_automating_model_training_1738498450656009">
        <h1>Automating Model Training</h1>
        <p>Training a model is just like any other regularly repeated computing activity: it must be automated. Failure to do so leads to several negative outcomes:</p>
        <ul>
          <li>
            <p>Wasted human resources by having to manually rerun training</p>
          </li>
          <li>
            <p>Unpredictability in model retraining cadence</p>
          </li>
          <li>
            <p>Inconsistency in the model’s performance through discrepancies (deliberate or accidental) in the model training process</p>
          </li>
        </ul>
        <p>Initial model development, then, should not be considered complete until the training process is automated end to end. A fully featured automated training process should include at the very least:</p>
        <ul>
          <li>
            <p>Input parameters that specify any variables that typically need to be tweaked (e.g., a version identifier for the training code or training data, or <em>hyperparameters</em>—parameters that define how training is done—for the training job)</p>
          </li>
          <li>
            <p>Any necessary data preparation or preprocessing to collect data from any storage locations and prepare it for training</p>
          </li>
          <li>
            <p>Fetching and executing the training job</p>
          </li>
          <li>
            <p>Storing the model and related artifacts in a chosen storage <span class="keep-together">endpoint</span></p>
          </li>
          <li>
            <p>Registering the model and related artifacts in the model registry</p>
          </li>
          <li>
            <p>Evaluating the performance of the trained model</p>
          </li>
        </ul>
        <p>Another feature that is nice to have but by no means essential is an automated process that promotes a trained model to production or any other post-training steps if the model meets some minimum performance thresholds.</p>
        <div data-type="warning" epub:type="warning"><h6>Warning</h6>
          <p>For many teams, promoting a model to production without any human oversight may not be appropriate. When evaluating a tool that has this feature, be sure to weigh the trade-offs of not having human review of a trained model’s metrics against keeping a human in the loop.</p>
        </div>
        <p>This process (and the software that implements it) is often referred to as a <em>pipeline</em> or <em>workflow</em>. These pipelines are usually authored by data scientists, data engineers, machine learning engineers, and/or MLOps teams, with Python being the prevailing language of choice.</p>
        <p>For enterprises that have adopted Kubernetes as their model development and serving platform, we strongly recommend adopting a pipeline engine that is Kubernetes native and thus is able to leverage the existing Kubernetes infrastructure, integrating seamlessly with the overall MLOps infrastructure in use.</p>
        <p>There are three major open source pipeline engines that have strong community adoption and should be considered for your training infrastructure:</p>
        <dl>
          <dt>Airflow</dt>
          <dd>
            <p><a href="https://airflow.apache.org">Airflow</a> is typically preferred by data scientists and tends to present the cleanest experience for authoring pipelines—or directed acyclic graphs (DAGs) in Airflow’s parlance)—but Airflow is not strictly Kubernetes native, which presents challenges when operationalizing it at scale on Kubernetes.</p>
          </dd>
          <dt>Kubeflow Pipelines</dt>
          <dd>
            <p>A part of the broader Kubeflow project, <a href="https://oreil.ly/0bP2X">Kubeflow Pipelines</a> is Kubernetes native at its core, making it more customizable, scalable, and well suited to enterprises with large or multiple data science teams wishing to share Kubernetes infrastructure, or with central IT/MLOps teams managing consistent infrastructure across teams.</p>
          </dd>
          <dt>MLflow</dt>
          <dd>
            <p><a href="https://mlflow.org">MLflow</a> excels at model version tracking, making it easy for data scientists to adopt and track multiple versions of their models over time. However, MLflow requires more effort by operations teams to deploy and scale on <span class="keep-together">Kubernetes</span>.</p>
          </dd>
        </dl>
        <div data-type="tip"><h6>Tip</h6>
          <p>Do you want to see what a continuous model training pipeline looks like in action? Red Hat offers an <a href="https://oreil.ly/l04gD">MLOps lab exercise</a> that helps you build one yourself.</p>
        </div>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="GitOps for Model Training Pipelines"><div class="sect1" id="ch03_gitops_for_model_training_pipelines_1738498450656066">
        <h1>GitOps for Model Training Pipelines</h1>
        <p>Model training pipelines are like code and should be treated accordingly. Pipeline definitions should be stored in source control and versioned, just like traditional application code. And like traditional application code, authors of pipeline definitions should follow a robust peer review process when making changes to the definitions. </p>
        <p class="pagebreak-before">Production training runs, then, should run from clean versions of these pipelines, pulled from a well-defined version (such as a branch or tag) of the pipeline in version control. This is <a href="https://oreil.ly/u9oe-">GitOps</a> in a nutshell. GitOps is a recent development in traditional software operations whereby applications are cleanly deployed from version control and continuously reconciled to ensure that the deployed application matches the desired state in version control. When teams wish to change the state of the application in production, they do so by changing the application’s definition in version control.</p>
        <p>Kubernetes’ declarative approach to deploying applications, along with the reconciliation loop that keeps Kubernetes applications in their desired state, make Kubernetes an ideal platform for managing pipelines with GitOps.</p>
        <p>For managing applications on Kubernetes, one of the most popular projects is <a href="https://oreil.ly/2yB2U">Argo CD</a>. Argo CD is a continuous delivery tool for Kubernetes that allows users to implement GitOps principles into their workflow. While deploying pipeline definitions with Argo CD typically requires the development of custom code to convert pipeline definitions from version control into runnable training jobs, this is an area of rapid innovation, especially from the Kubeflow project, to allow for simpler pipeline definition management.</p>
        <p>Now that you’ve learned how to reliably retrain a model, keep track of its versions (and that of the data and training pipelines), and build a more robust open source MLOps infrastructure, you are prepared to move on to the next step of the AI model lifecycle: deployment and monitoring.</p>
      </div></section>
    </div></section></div>
</div>
</body></html>