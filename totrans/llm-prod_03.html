<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">4</span></span> <span class="chapter-title-text"><em>Data engineering for large language models: Setting up for success</em></span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Common foundation models used in the industry</li>
<li class="readable-text" id="p3">How to evaluate and compare large language models</li>
<li class="readable-text" id="p4">Different data sources and how to prepare your own</li>
<li class="readable-text" id="p5">Creating your own custom tokenizers and embeddings</li>
<li class="readable-text" id="p6">Preparing a Slack dataset to be used in future chapters</li>
</ul>
</div>
<div class="readable-text" id="p7">
<blockquote>
<div>
     Data is like garbage. You’d better know what you are going to do with it before you collect it. 
     <div class="quote-cite">
       —Mark Twain 
     </div>
</div>
</blockquote>
</div>
<div class="readable-text" id="p8">
<p>Creating our own LLM is no different from any ML project in that we will start by preparing our assets—and there isn’t a more valuable asset than your data. All successful AI and ML initiatives are built on a good data engineering foundation. It’s important then that we acquire, clean, prepare, and curate our data.</p>
</div>
<div class="readable-text intended-text" id="p9">
<p>Unlike other ML models, you generally won’t be starting from scratch when creating an LLM customized for your specific task. Of course, if you do start from scratch, you’ll likely only do it once. Then it’s best to tweak and polish that model to further refine it for your specific needs. Selecting the right base model can make or break your project. Figure 4.1 gives a high-level overview of the different pieces and assets you’ll need to prepare before training or finetuning a new model.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p10">
<img alt="figure" height="304" src="../Images/4-1.png" width="457"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.1</span> The different elements of training an LLM. Combining earth, fire, water—wait, no, not those elements. To get started, you’ll need to collect several assets, including a foundation model, training data, text encoders (e.g., tokenizer), and evaluation data.</h5>
</div>
<div class="readable-text" id="p11">
<p>As was so well defined in the book <em>Fundamentals of Data Engineering</em><a href="#footnote-151"><sup class="footnote-reference" id="footnote-source-1">1</sup></a>: </p>
</div>
<div class="readable-text" id="p12">
<blockquote>
<div>
     Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning.
    </div>
</blockquote>
</div>
<div class="readable-text" id="p13">
<p>In this chapter, we will discuss the steps you’ll need to take before you can start creating your LLM, which largely involves preparing the data assets necessary to train a model. We will go over many of the base or foundation models available to you as a starting point and how to evaluate and compare them. We will then go into depth on many of the different datasets available and how to prepare your own for finetuning a model, including preparing your own tokenizer or embeddings. Lastly, we will craft a dataset that we will use to finetune a model in the next chapter.</p>
</div>
<div class="readable-text" id="p14">
<h2 class="readable-text-h2" id="sigil_toc_id_56"><span class="num-string">4.1</span> Models are the foundation</h2>
</div>
<div class="readable-text" id="p15">
<p>We will first discuss the most important dataset you will need to collect when training, which is the model weights of a pretrained model. A big reason why LLMs are so successful as a technology is that we can take a model already trained on language as a whole and tweak it to do well on a specific task. Of course, knowing how that beginning model was trained and what it was trained on will be a huge shortcut in choosing the right one to tweak.</p>
</div>
<div class="readable-text intended-text" id="p16">
<p>Choosing the right one has become obnoxiously difficult since LLMs have been a hot research topic, resulting in a new one that sports benchmark-breaking records popping up almost every week. Because we know (or at least assume) you are eager to learn about them, we will first discuss the many different models currently out there. These models have already been trained (for better or worse) by professionals working to make your life easier and put powerful language models into the public arena. There are thousands upon thousands of open source models available on GitHub, Hugging Face Hub, and elsewhere, so to simplify, we’ll highlight our favorites, giving you details about each of the models to make it easier to compare and to give you an idea about whether you should use that particular model or opt for one of its lesser-known open source variants. If you are planning to train from scratch, consider the architecture involved and if there’s a certain family you’d like to try.</p>
</div>
<div class="readable-text" id="p17">
<h3 class="readable-text-h3" id="sigil_toc_id_57"><span class="num-string">4.1.1</span> GPT</h3>
</div>
<div class="readable-text" id="p18">
<p>There’s probably no better place to start than with GPT (Generative Pre-trained Transformer) models. A fan favorite and one of ours too, these models are sold commercially through OpenAI and have gained popularity for their impressive performance on a wide range of tasks. GPT models are so well known that laypersons often use “GPT” to replace “LLM,” just as one might say Kleenex or Band-Aid instead of tissue or bandage.</p>
</div>
<div class="readable-text intended-text" id="p19">
<p>The first GPT model was introduced in 2018, shortly after transformers were introduced, and only had 120M parameters. It was trained on the small BookCorpus dataset and had impressive results on NLP benchmarks at the time. The GPT-2 model came out the next year, increasing its size by 10-fold to 1.5B parameters; it was trained on the much larger WebText dataset. The next year, in 2020, GPT-3 came out 100 times larger with 175B parameters and trained on the massive Common Crawl dataset. This model was still based on GPT-1’s original architecture with slight modifications for improved scaling. </p>
</div>
<div class="readable-text intended-text" id="p20">
<p>OpenAI has chosen to keep further iterations like GPT-4 under greater secrecy, not revealing training data or specific architectures, since it has started to productionize and sell them as a product. ChatGPT is a finetuned GPT-3 model trained for conversational interaction using reinforcement learning with human feedback (RLHF). Not to get into the weeds, but there is a whole host of GPT-3 models you can find under API names such as ada, babbage, curie, and davinci, as well as other finetuned models such as webGPT and InstructGPT. We leave it to the reader to investigate further if they are interested.</p>
</div>
<div class="readable-text intended-text" id="p21">
<p>Other open source variations like GPT-J were created by the open source community utilizing the knowledge gained from the whitepapers OpenAI published. Several GPT models have no relation to OpenAI, as Generative Pre-trained Transformer is a very generic name that fits most LLMs. Of course, OpenAI has started to see it as a brand and is trying to trademark the acronym.<a href="#footnote-152"><sup class="footnote-reference" id="footnote-source-2">2</sup></a></p>
</div>
<div class="readable-text intended-text" id="p22">
<p>GPT-X models, although closed source, can be accessed via the OpenAI API, which also includes features for their finetuning. We will be using GPT-2 throughout this book—even though it is a bit smaller than what most would consider an actual LLM—as it is a well-understood architecture and easy to learn with.</p>
</div>
<div class="readable-text" id="p23">
<h3 class="readable-text-h3" id="sigil_toc_id_58"><span class="num-string">4.1.2</span> BLOOM</h3>
</div>
<div class="readable-text" id="p24">
<p>BLOOM is one of the most iconic LLMs because of the learning that has come from creating it. The model came out in 2022 and is the first public LLM to rival GPT-3’s size with 176B parameters; it was trained with complete transparency. It was put together by Hugging Face’s BigScience team, with help from Microsoft’s DeepSpeed team and NVIDIA’s Megatron-LM team, and was sponsored by French government grants. </p>
</div>
<div class="readable-text intended-text" id="p25">
<p>BLOOM was trained on the BigScienceCorpus dataset, a conglomerate of many smaller datasets amounting to 1.6TB of pre-processed text. It is licensed under RAIL, which means it isn’t technically open source, since there are restrictions on how you can use it, but it can be commercialized.</p>
</div>
<div class="readable-text print-book-callout" id="p26">
<p><span class="print-book-callout-head">TIP</span>  You can learn more about the RAIL license here: <a href="https://mng.bz/mR20">https://mng.bz/mR20</a>.</p>
</div>
<div class="readable-text" id="p27">
<p>BLOOM was trained to be industry size and industry grade for all tasks. Because of this, fitting on a consumer device was not a priority, but several smaller versions were trained as the research team was coming up to speed. There are 560M-, 3B-, and 7B-parameter versions. There is also BLOOMZ, a multitask, finetuned version of the full 176B parameter model. BLOOM was only trained in 46 different languages, and BLOOMZ’s goal was to increase the cross-lingual generalization of the model.<a href="#footnote-153"><sup class="footnote-reference" id="footnote-source-3">3</sup></a> You can find all of these models on Hugging Face’s hub: <a href="https://huggingface.co/bigscience/bloom">https://huggingface.co/bigscience/bloom</a>.</p>
</div>
<div class="readable-text intended-text" id="p28">
<p>The big downside to BLOOM is that it often gives poor responses and doesn’t compete very well in benchmarks—most likely due to limited funds and tight deadlines of the project, leading to a feeling that it was undertrained. This isn’t always a bad thing and is often better than an overtrained model, but you can expect to require a lot more finetuning on a larger dataset if you decide to use it. The benefit of using it, though, is that it is well understood and trained in the open, and you can check its training data.</p>
</div>
<div class="readable-text intended-text" id="p29">
<p>In general, the authors wouldn’t recommend using it as a foundation model anymore; there are better alternatives, but it’s one you should be familiar with because of its contributions. For example, BLOOM’s creation of petals, which allowed distributed training, was a significant contribution to the field. </p>
</div>
<div class="readable-text" id="p30">
<h3 class="readable-text-h3" id="sigil_toc_id_59"><span class="num-string">4.1.3</span> LLaMA</h3>
</div>
<div class="readable-text" id="p31">
<p>LLaMA is the result of Meta’s foray into LLMs. The first version was released in February 2023 and was released to the research community with a noncommercial license. A week later, the weights were leaked on 4chan. In an unlikely turn of events, this leak has likely been very beneficial to Meta, as this model has become the standard for experimentation and development. Several more models we will discuss are based on it.</p>
</div>
<div class="readable-text intended-text" id="p32">
<p>Later, in July 2023, Meta released Llama 2, which has both a research and a commercial license. Llama 2 is a big deal since it’s the first commercially available model that really packs a punch, and you’ll see many other models based on its architecture. There are three different model sizes available: 7B, 13B, and 70B parameters. You can download them here: <a href="https://ai.meta.com/llama/">https://ai.meta.com/llama/</a>. You’ll need to request access and accept the terms and conditions if you plan to use it.</p>
</div>
<div class="readable-text intended-text" id="p33">
<p>Llama 2 was trained on 2 trillion tokens from a curated dataset taken from the internet where they removed websites known to contain personal information and upsampled what they considered factual sources. While exact details of the dataset haven’t been shared, it likely contained data from Common Crawl, GitHub, Wikipedia, Project Gutenberg, ArXiv, and Stack Exchange since those were the primary datasets for LLaMA 1. These datasets were later packaged together and distributed under the name RedPajama. Llama 2 was then further finetuned using RLHF, with one model finetuned for chat and another for code.</p>
</div>
<div class="readable-text" id="p34">
<h3 class="readable-text-h3" id="sigil_toc_id_60"><span class="num-string">4.1.4</span> Wizard</h3>
</div>
<div class="readable-text" id="p35">
<p>The Wizard family of language models comes from the 2023 paper “WizardLM: Empowering Large Language Models to Follow Complex Instructions.”<a href="#footnote-154"><sup class="footnote-reference" id="footnote-source-4">4</sup></a> These models follow the idea that LLMs function better when trained on dense training data filled with high-complexity tasks. Based on a proposed framework for creating more complex instruction tasks, the WizardLM methodology has been applied to many popular datasets and used to finetune almost all of the most popular models. The methodology is so popular that, amazingly, it only took the community two days after LlamaCoder34B came out to finetune the WizardCoder34B model.</p>
</div>
<div class="readable-text intended-text" id="p36">
<p>These models have been consistently praised for their human-like prose and their ability to correctly sort through complex problems that rivals many paid services. One problem we encourage you to try is to ask WizardCoder34B to write a program that draws a realistic-looking tree using any language you’d like. Because the Wizard models don’t revolve as much around a specific dataset as they do around the methodology of changing an existing dataset to fit the Wizard style, the applications are incredibly broad and diverse. If you hit a wall where you aren’t sure how to improve when using another model or architecture, try taking the dataset you’ve already used and applying the Wizard methodology. You’re welcome.</p>
</div>
<div class="readable-text intended-text" id="p37">
<p>As a side note, WizardCoder models tend to get a lot of attention, but the WizardMath models are also impressive in their own right. We note that a lot of readers likely deal more with data problems than code problems, and the WizardMath models might be a great place to start when working with talk-to-your-data applications. </p>
</div>
<div class="readable-text" id="p38">
<h3 class="readable-text-h3" id="sigil_toc_id_61"><span class="num-string">4.1.5</span> Falcon</h3>
</div>
<div class="readable-text" id="p39">
<p>Falcon models are a model family from the Technology Innovation Institute in Abu Dhabi. They are the first state-of-the-art models to be released under a truly open source license, Apache 2.0. You can get the model from the institute’s website: <a href="https://falconllm.tii.ae/falcon-models.xhtml">https://falconllm.tii.ae/falcon-models.xhtml</a>. Its easy access and the open license make this a dream for hackers, practitioners, and the industry.</p>
</div>
<div class="readable-text intended-text" id="p40">
<p>Falcon models first introduced in June 2023 only introduced 7B and 40B parameter models, but in September 2023, Falcon released a 180B parameter model that can truly compete with GPT-3–sized models. What’s also exciting and probably more important to many readers is that Falcon has often led LLM leaderboards in many benchmarking tasks. The models were primarily trained on the RefinedWeb dataset, which is a smaller but much higher-quality dataset that was carefully and meticulously curated and extracted from the Common Crawl dataset.</p>
</div>
<div class="readable-text" id="p41">
<h3 class="readable-text-h3" id="sigil_toc_id_62"><span class="num-string">4.1.6</span> Vicuna</h3>
</div>
<div class="readable-text" id="p42">
<p>Vicuna was trained on a dataset of user-shared conversations from ShareGPT. The logic is that a model trained off of the best outputs of ChatGPT will be able to emulate the performance of ChatGPT, piggy-backing off of the Llama–Alpaca trend. </p>
</div>
<div class="readable-text print-book-callout" id="p43">
<p><span class="print-book-callout-head">NOTE</span>  We won’t talk about Alpaca here, but we introduced it in chapter 3 when discussing knowledge distillation.</p>
</div>
<div class="readable-text" id="p44">
<p>Vicuna has been praised for both its performance and its relatively low training costs. Vicuna is an amazing example of why data coverage and quality matter so much while simultaneously demonstrating the dangers of model collapse from training on the output of another model. Model collapse happens when an ML model is trained on synthetic data, leading to increasingly less diverse outputs. For example, Vicuna performs admirably on anything that is at least close to what appeared in the dataset, but when asked to perform more generative or agent-like tasks, it tends to hallucinate far beyond what its predecessors do. Vicuna is not licensed for commercial use, but it is amazing for personal projects.</p>
</div>
<div class="readable-text" id="p45">
<h3 class="readable-text-h3" id="sigil_toc_id_63"><span class="num-string">4.1.7</span> Dolly</h3>
</div>
<div class="readable-text" id="p46">
<p>Created by Databricks as more of a thought experiment than a competitive model, Dolly and its V2 do not perform well compared to other models of the same size. However, Dolly boasts one of the best underlying understandings of English and is a fantastic starting point for finetuning or creating low-ranking adaptations (LoRAs; which we will discuss in chapter 5) to influence other models. Dolly 1.0 was trained on the Stanford Alpaca Dataset, while Dolly 2.0 was trained on a high-quality human-generated instruction-following dataset that was crowdsourced by the Databricks employees. Dolly 2.0 has been open sourced in its entirety, including the training code, dataset, and model weights, all with a commercial use license.<a href="#footnote-155"><sup class="footnote-reference" id="footnote-source-5">5</sup></a></p>
</div>
<div class="readable-text" id="p47">
<h3 class="readable-text-h3" id="sigil_toc_id_64"><span class="num-string">4.1.8</span> OpenChat</h3>
</div>
<div class="readable-text" id="p48">
<p>OpenChat is similar to Vicuna in that OpenChat used 80K ShareGPT conversations for training, but dissimilar in that their conditioning and weighted loss strategies end up creating a model that is undeniably great in its ability to generate human-like and, more importantly, human-preferred responses.</p>
</div>
<div class="readable-text intended-text" id="p49">
<p>OpenChat models—not to be confused with the open source chatbot console—are a collection of various finetunings for different tasks, with some meant for coding, others for agents, and others for chatting. Free for commercial use under the Llama 2 Community License, these models could be a great solution to build off of at your corporation.</p>
</div>
<div class="readable-text intended-text" id="p50">
<p>We’ve discussed a lot of models already, and while we could go on like this for the rest of the chapter, it’s in everyone’s best interest that we don’t. Table 4.1 shows a summary highlighting some of the major points of comparison for the models we discussed. One major point we’d like to highlight is that a lot of models are available for commercial use! While many of the licenses come with restrictions, they likely aren’t rules you plan to break anyway.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p51">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 4.1</span> Comparison of LLM model families</h5>
<table>
<thead>
<tr>
<th>
<div>
         Model family 
       </div></th>
<th>
<div>
         Dataset 
       </div></th>
<th>
<div>
         Largest model size 
       </div></th>
<th>
<div>
         Commercial license 
       </div></th>
<th>
<div>
         Organization 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  GPT <br/></td>
<td>  Common Crawl/RLHF <br/></td>
<td>  1.76T <br/></td>
<td>  No <br/></td>
<td>  OpenAI <br/></td>
</tr>
<tr>
<td>  BLOOM <br/></td>
<td>  BigScienceCorpus <br/></td>
<td>  176B <br/></td>
<td>  Yes <br/></td>
<td>  BigSciense <br/></td>
</tr>
<tr>
<td>  Llama <br/></td>
<td>  RedPajama <br/></td>
<td>  70B <br/></td>
<td>  Yes <br/></td>
<td>  Meta <br/></td>
</tr>
<tr>
<td>  Wizard <br/></td>
<td>  Evol-Instruct <br/></td>
<td>  70B <br/></td>
<td>  No <br/></td>
<td>  Microsoft <br/></td>
</tr>
<tr>
<td>  Falcon <br/></td>
<td>  RefinedWeb <br/></td>
<td>  180B <br/></td>
<td>  Yes <br/></td>
<td>  TII <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p52">
<p>Now that you have an understanding of some of the more popular model families, you might have an idea of which model to pick to start for your project. But how can you be sure? In the next section, we’ll look at different ways you can evaluate and compare models.</p>
</div>
<div class="readable-text" id="p53">
<h2 class="readable-text-h2" id="sigil_toc_id_65"><span class="num-string">4.2</span> Evaluating LLMs</h2>
</div>
<div class="readable-text" id="p54">
<p>While we have just discussed some of our favorite model families, there are so many more and varying models available out there, with many more coming out every month, all claiming to be the best. It is impossible to keep them all straight. So how do you pick the best one to use? Can it perform well on your task out of the box, or will it require finetuning? How do you know if your finetuning improved the model or just made it worse? How do you know if you picked the right size? A smaller model is convenient, but larger models perform better on many tasks. To be honest, these are not easy questions to answer, but thankfully, there are a few industry standards we can rely on.</p>
</div>
<div class="readable-text intended-text" id="p55">
<p>When evaluating a model, you will need two things: a metric and a dataset. A metric is an algorithm that allows us to compare results to a ground truth. A dataset is a list of tasks we want our model to run, which we will then compare using our metrics of choice.</p>
</div>
<div class="readable-text intended-text" id="p56">
<p>In this section, we will discuss many different methodologies employed to evaluate LLMs so we can evaluate and compare them objectively. We will discuss everything from common industry benchmarks to methodologies used to develop your own unique evaluations. Let’s get started.</p>
</div>
<div class="readable-text" id="p57">
<h3 class="readable-text-h3" id="sigil_toc_id_66"><span class="num-string">4.2.1</span> Metrics for evaluating text</h3>
</div>
<div class="readable-text" id="p58">
<p>Evaluating text is often difficult because it’s easy to say the exact same thing in two different ways. Semantically, two sentences can be exactly the same, but syntactically, they are nothing alike, making text comparison tricky. See what I did there?</p>
</div>
<div class="readable-text intended-text" id="p59">
<p>To evaluate our models, we will need better metrics than just an exact match or check for equality, which we can get away with for most other ML problems. We need a metric that allows us to compare the generated text from our models against a ground truth without being too rigid. Let’s look at some of the most common metrics used.</p>
</div>
<div class="readable-text" id="p60">
<h4 class="readable-text-h4 sigil_not_in_toc">ROUGE</h4>
</div>
<div class="readable-text" id="p61">
<p>ROUGE, short for Recall-Oriented Understudy for Gisting Evaluation, is one of the oldest metrics used for evaluating machine translation tasks, but still one of the most reliable. It was developed specifically for automatic summarization tasks where the goal is to take a long article and sum it up in a short brief. Let’s consider the problem: How do you determine whether a summary is correct? The simplest method would be to compare it to a known summary—a ground truth, if you will. However, no matter the article, there’s often thousands of ways you could choose to simplify the text to be more concise, and you don’t want to penalize a model simply because it chose a different word order than the ground truth; this would only lead to overfitting.</p>
</div>
<div class="readable-text intended-text" id="p62">
<p>Rouge doesn’t compare the generated summary to the ground truth summary expecting an exact match; instead, it looks for overlaps between the two summaries using N-grams—the greater the overlap, the higher the score. This is similar to how a full-text search engine works. There are multiple variations depending on what N is for the N-gram, but there is also a version that compares longest common subsequences and versions that compare skip-bigrams, which are any pair of words in their sentence order and not necessarily right next to each other.</p>
</div>
<div class="readable-text intended-text" id="p63">
<p>The original implementation of ROUGE was written in Perl, and we remember having to use it even a couple of years ago. Easily some of the worst days of one author’s career were having to work in Perl. Thankfully, it seems that in the last year or so, there have finally been fast, stable reimplementations in Python. In the next listing, we use the rouge-score library, which is a reimplementation from Google. We’ll compare two explanations of <em>The Legend of Zelda</em> and see how well they compare.</p>
</div>
<div class="browsable-container listing-container" id="p64">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.1</span> Using ROUGE</h5>
<div class="code-area-container">
<pre class="code-area">from rouge_score import rouge_scorer

target = "The game 'The Legend of Zelda' follows the adventures of the \
    hero Link in the magical world of Hyrule."
prediction = "Link embarks on epic quests and battles evil forces to \
    save Princess Zelda and restore peace in the land of Hyrule."

scorer = rouge_scorer.RougeScorer(["rouge1", "rougeL"], use_stemmer=True)  <span class="aframe-location"/> #1
scores = scorer.score(target, prediction)
print(scores)
# {'rouge1': Score(precision=0.28571428, recall=0.31578947, fmeasure=0.3),
# 'rougeL': Score(precision=0.238095238, recall=0.26315789, fmeasure=0.25)}</pre>
<div class="code-annotations-overlay-container">
     #1 Example N-gram where N=1 and also using the longest common subsequence
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p65">
<p>As you can see from the example, even though these two texts are quite different syntactically, they are both accurate descriptions. Because of this, instead of giving a big fat zero for the score, ROUGE gives a little more flexibility and a better comparison with similarity scores around 0.25. The ROUGE algorithm is a fast and effective way to quickly compare the similarity between two short bodies of text. ROUGE is very common in the industry, and many benchmarks use it as one of their metrics.</p>
</div>
<div class="readable-text" id="p66">
<h4 class="readable-text-h4 sigil_not_in_toc">BLEU</h4>
</div>
<div class="readable-text" id="p67">
<p>BLEU, which stands for BiLingual Evaluation Understudy, is the oldest evaluation metric we will talk about in this book. It was developed to evaluate machine translation tasks and compare methods of translating one language to another. It is very similar to ROUGE, where we compare N-grams between a target and a prediction. While ROUGE is primarily a recall metric, BLEU is a precision metric, but using standard precision can lead to some problems we need to account for.</p>
</div>
<div class="readable-text intended-text" id="p68">
<p>To understand the problem, we can calculate standard precision with the code from listing 4.1. Replace the target variable with “the cat in the hat” and the prediction variable with “cat hat.” Rerun the listing, and you’ll notice the recall is 0.4—we got two out of five words correct—but the precision is 1.0, a perfect score despite not being very good! This result is because both words “cat” and “hat” show up in the target.</p>
</div>
<div class="readable-text intended-text" id="p69">
<p>BLEU fixes this by adding two adjustments. The first is straightforward: add a brevity penalty. If the prediction is shorter than the target, we’ll penalize it. The second adjustment, known as the modified N-gram precision, is a bit more complicated, but it allows us to compare a prediction against multiple targets. The next listing shows how to use the NLTK library to calculate the BLEU score. We are using the same <em>Zelda</em> example as we did with ROUGE so you can compare results.</p>
</div>
<div class="browsable-container listing-container" id="p70">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.2</span> Using BLEU</h5>
<div class="code-area-container">
<pre class="code-area">import nltk.translate.bleu_score as bleu

target = [
    "The game 'The Legend of Zelda' follows the adventures of the \
    hero Link in the magical world of Hyrule.".split(),
    "Link goes on awesome quests and battles evil forces to \
    save Princess Zelda and restore peace to Hyrule.".split(),
]
prediction = "Link embarks on epic quests and battles evil forces to \
    save Princess Zelda and restore peace in the land of Hyrule.".split()


score = bleu.sentence_bleu(target, prediction)
print(score)
# 0.6187934993051339</pre>
</div>
</div>
<div class="readable-text" id="p71">
<p>BLEU has long been an industry standard, as it has been reported several times to correlate well with human judgment on translation tasks. In our example, we split the sentences, but it would be better to tokenize the sentences instead. Of course, you can’t compare BLEU scores that use different tokenizers. On that note, SacreBLEU is a variant worth looking at, as it attempts to improve the comparability of scores despite different tokenizers.</p>
</div>
<div class="readable-text" id="p72">
<h4 class="readable-text-h4 sigil_not_in_toc">BPC</h4>
</div>
<div class="readable-text" id="p73">
<p>The bits per character (BPC) evaluation is an example of an entropy-based evaluation for language models. These are metrics we try to minimize. We will not dive deeply into entropy or perplexity, but we’ll go over an intuitive understanding here. Entropy is an attempt to measure information by calculating the average amount of binary digits required per character in a language. Entropy is the average number of BPC. </p>
</div>
<div class="readable-text intended-text" id="p74">
<p>Perplexity can be broken down into attempting to measure how often a language model draws particular sequences from its corpus or vocabulary. This draws directly from the model’s tokenization strategy (too many <code>&lt;UNKS&gt;</code> equals bad perplexity), meaning that a 1:1 comparison between LLMs with different tokenization strategies using perplexity—or entropy, for that matter—is impossible. For example, a model that tokenizes at the character level will have much lower perplexity than a model that tokenizes at the word level but often performs worse overall. That doesn’t invalidate either as a metric, as they are very helpful metrics during training of the same model. </p>
</div>
<div class="readable-text print-book-callout" id="p75">
<p><span class="print-book-callout-head">NOTE</span>  Entropy-related metrics are highly related to information theory, which we don’t cover. However, we recommend you take a look at these metrics if you’re interested in creating or improving evaluation metrics for LLMs.</p>
</div>
<div class="readable-text" id="p76">
<p>To drive the point further with a hands-on example, comparing two models that use different tokenization strategies is like comparing how good one third-grader is at addition with another third-grader’s multiplication ability. Saying one is better than the other doesn’t really matter because they’re doing different things at the same skill level. The closest you could get to an accurate comparison would be having the two third-graders do the same task, say spelling. Then you could at least compare apples to apples, as much as possible.</p>
</div>
<div class="readable-text intended-text" id="p77">
<p>Now that we have some metrics under our belt, let’s look into benchmark datasets that we will run our evaluations on.</p>
</div>
<div class="readable-text" id="p78">
<h3 class="readable-text-h3" id="sigil_toc_id_67"><span class="num-string">4.2.2</span> Industry benchmarks</h3>
</div>
<div class="readable-text" id="p79">
<p>Evaluating language models’ performance is a notoriously difficult problem, and many benchmarks have been created to tackle it. In this subsection, we’ll discuss several of the most common solutions you are likely to run into and what type of problem they are trying to solve. Since benchmarks typically are only good at evaluating one quality of a model and LLMs are usually deployed to do many general tasks, you will likely need to run several evaluation benchmarks to get a full picture of the strengths and weaknesses of your model. As we go through this list, don’t think about which metric is better than another, but about how they can be used in tandem to improve your overall success.</p>
</div>
<div class="readable-text" id="p80">
<h4 class="readable-text-h4 sigil_not_in_toc">GLUE</h4>
</div>
<div class="readable-text" id="p81">
<p>The General Language Understanding Evaluation (GLUE) is essentially a standardized test (think ACT, SAT, GRE, etc.) for language models (just “language models” this time) to measure performance versus humans and each other on language tasks meant to test understanding. When introduced, two problems arose pretty quickly: the LMs surpassed human parity on the tasks too fast, and there were doubts about whether the tasks demonstrated actual understanding. Similar to when people train animals like parrots to speak, the question is always there: Is the parrot actually acquiring human language or simply being conditioned to mimic certain sound sequences in response to specific stimuli in exchange for food? That said, the GLUE benchmark is still valuable for comparing model performance.</p>
</div>
<div class="readable-text intended-text" id="p82">
<p>GLUE is no longer an industry standard, but it can still give you a fairly quick idea of how well your model is performing, especially if you are training on an instruction-based dataset and using GLUE to measure few or zero-shot performance on new tasks. You can view the leaderboard at <a href="https://gluebenchmark.com/leaderboard">https://gluebenchmark.com/leaderboard</a>.</p>
</div>
<div class="readable-text" id="p83">
<h4 class="readable-text-h4 sigil_not_in_toc">SuperGLUE</h4>
</div>
<div class="readable-text" id="p84">
<p>As stated in the previous section, one problem that came up quickly was human parity on the GLUE tasks. To solve this problem, one year after GLUE was developed, SuperGLUE was created and contains more difficult and diverse tasks styled in the same easy-to-use way as GLUE. Beyond that, because the GLUE nonexpert human benchmark was being surpassed so quickly, more expert people were used to generate the SuperGLUE benchmark. That said, the SuperGLUE human baselines are in eighth place on the leaderboard at the time of this writing, calling into question the second problem with GLUE: Do the SuperGLUE tasks adequately measure understanding? </p>
</div>
<div class="readable-text intended-text" id="p85">
<p>Considering that models like PaLM 540B, which are beating the human baseline, struggle to generate output generally considered acceptable to people, another question arises: How much of the training data and evaluation metrics are idealized and nonreflective of how we actually use language? There aren’t yet any adequate answers to these questions, but they’re helpful to consider when your evaluation metrics could be what stands between your model and acceptable performance on its task.</p>
</div>
<div class="readable-text intended-text" id="p86">
<p>In listing 4.3, we show how to run a model against the MultiRC SuperGLUE test. The MultiRC dataset contains short paragraphs and asks comprehension questions about the content of the paragraph. Let’s go ahead and load the dataset and take a quick look at what we are dealing with.</p>
</div>
<div class="browsable-container listing-container" id="p87">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.3</span> Example SuperGLUE Benchmark </h5>
<div class="code-area-container">
<pre class="code-area">from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM

dataset = load_dataset("super_glue", "multirc", split="validation")   <span class="aframe-location"/> print(dataset[0]) #1</pre>
<div class="code-annotations-overlay-container">
     #1 SuperGlue has multiple test datasets; options are boolq, cb, copa, multirc, record, rte, wic, wsc, wsc.fixed, axb, and axg.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p88">
<p>Here we see a paragraph discussing some basic physics around forces along with a simple yes-or-no question and its answer:</p>
</div>
<div class="browsable-container listing-container" id="p89">
<div class="code-area-container">
<pre class="code-area"># {
#   "paragraph": "What causes a change in motion? The application of a force."
#     " Any time an object changes motion, a force has been applied. In what "
#     "ways can this happen? Force can cause an object at rest to start "
#     "moving. Forces can cause objects to speed up or slow down. Forces can "
#     "cause a moving object to stop. Forces can also cause a change in "
#     "direction. In short, forces cause changes in motion. The moving "
#     "object may change its speed, its direction, or both. We know that "
#     "changes in motion require a force. We know that the size of the force "
#     "determines the change in motion. How much an objects motion changes "
#     "when a force is applied depends on two things. It depends on the "
#     "strength of the force. It also depends on the objects mass. Think "
#     "about some simple tasks you may regularly do. You may pick up a "
#     "baseball. This requires only a very small force. ",
#   "question": "Would the mass of a baseball affect how much force you have "
#     "to use to pick it up?",
#   "answer": "No",
#   "idx": {"paragraph": 0, "question": 0, "answer": 0},
#   "label": 0,
# }</pre>
</div>
</div>
<div class="readable-text" id="p90">
<p>Let’s go ahead and pull down a small model and run it against the dataset. For this example, we’ll print out the model’s generated answer to the correct answer to compare qualitatively:</p>
</div>
<div class="browsable-container listing-container" id="p91">
<div class="code-area-container">
<pre class="code-area">model = "bigscience/bloomz-560m"  # Update with your model of choice

tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModelForCausalLM.from_pretrained(model)


for row in dataset:
    input_text = (
        f'Paragraph: {row["paragraph"]}\nQuestion: {row["question"]}'
    )                                                                   <span class="aframe-location"/> #1
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids

    outputs = model.generate(input_ids, max_new_tokens=20)
    input_length = input_ids.shape[1]                       <span class="aframe-location"/> #2
    results = tokenizer.decode(outputs[0][input_length:])
    print(row["answer"])
    print(results)</pre>
<div class="code-annotations-overlay-container">
     #1 Replace this with the correct input for your benchmark.
     <br/>#2 We use this to trim out the input.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p92">
<p>From this, you might get results similar to the following:</p>
</div>
<div class="browsable-container listing-container" id="p93">
<div class="code-area-container">
<pre class="code-area"># No
#  No&lt;/s&gt;
# Yes
#  No&lt;/s&gt;
# Less the mass, less the force applied
#  No&lt;/s&gt;
# It depends on the shape of the baseball
#  No&lt;/s&gt;
# Strength
#  Force&lt;/s&gt;
# A force
#  Force&lt;/s&gt;
# No
#  Yes&lt;/s&gt;</pre>
</div>
</div>
<div class="readable-text" id="p94">
<p>You can see our model isn’t doing all that great, but we aren’t too concerned; we just want to show a SuperGLUE test in action. You may be wondering why we aren’t using a metric like ROUGE or BLEU. While we could do so to improve our understanding, if you decide to submit results to the SuperGLUE leaderboard, it will want the raw generated text.</p>
</div>
<div class="readable-text print-book-callout" id="p95">
<p><span class="print-book-callout-head">NOTE</span>  For more information on how to use SuperGLUE, check out SuperGLUE FAQs: <a href="https://super.gluebenchmark.com/faq">https://super.gluebenchmark.com/faq</a>.</p>
</div>
<div class="readable-text" id="p96">
<p>SuperGLUE does exactly what it sets out to do: be GLUE but super. If you want to test your model’s few and zero-shot capabilities, SuperGLUE would be one of the ultimate tests. It will show whether your LLM can follow instructions with very low perplexity, only generating what is needed and not more. You can look at the current SuperGLUE leaderboard at <a href="https://super.gluebenchmark.com/leaderboard">https://super.gluebenchmark.com/leaderboard</a>.</p>
</div>
<div class="readable-text" id="p97">
<h4 class="readable-text-h4 sigil_not_in_toc">MMLU</h4>
</div>
<div class="readable-text" id="p98">
<p>The Massive Multitask Language Understanding (MMLU) test was developed primarily by UC Berkeley in cooperation with several other universities to test deeper knowledge than the GLUE tasks. No longer concerned with surface-level language understanding, MMLU seeks to test whether a model understands language well enough to answer second-tier questions about subjects such as history, mathematics, morality, and law. For example, instead of asking, “What did Newton write about gravity?”, ask, “What arguments would Newton have gotten into with Einstein?”</p>
</div>
<div class="readable-text intended-text" id="p99">
<p>MMLU’s questions range in difficulty from an elementary level to an advanced professional level, and they test both world knowledge and problem-solving ability. They are known to be quite difficult, with unspecialized humans from Mechanical Turk only obtaining results slightly better than random with 34.5% accuracy.<a href="#footnote-156"><sup class="footnote-reference" id="footnote-source-6">6</sup></a> Experts in their field performed much better, but generally only for the portion of the test that was their specialty. So when we look at the models’ performance on the test, as might be expected, the models, even at the top of SuperGLUE’s leaderboard, are barely better than random at applying the language understanding to answer questions about it. This test encompasses a much wider range of understanding tasks than GLUE and takes a much lower perplexity to pass.</p>
</div>
<div class="readable-text intended-text" id="p100">
<p>Listing 4.4 shows how to run this test. We’ll download the MMLU dataset and then, for convenience, run the test against OpenAI’s different models for comparison. The code also allows for different levels of few-shot prompting. We haven’t discussed this, but we wanted to show an example early. Try adjusting this parameter to see how different numbers of examples can improve your overall results.</p>
</div>
<div class="browsable-container listing-container" id="p101">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.4</span> Example MMLU evaluation</h5>
<div class="code-area-container">
<pre class="code-area">from deepeval.benchmarks import MMLU
from deepeval.benchmarks.tasks import MMLUTask
from deepeval.models.base_model import DeepEvalBaseLLM
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


class DeepEvalLLM(DeepEvalBaseLLM):            <span class="aframe-location"/> #1
    def __init__(self, model, tokenizer, name):
        self.model = model
        self.tokenizer = tokenizer
        self.name = name

        device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu"
        )

        self.model.to(device)
        self.device = device
    def load_model(self):
        return self.model

    def generate(self, prompt: str) -&gt; str:
        model = self.load_model()
        model_inputs = self.tokenizer([prompt], return_tensors="pt").to(
            self.device
        )

        generated_ids = model.generate(
            **model_inputs, max_new_tokens=100, do_sample=True
        )
        return self.tokenizer.batch_decode(generated_ids)[0]

    async def a_generate(self, prompt: str) -&gt; str:
        return self.generate(prompt)

    def get_model_name(self):
        return self.name


model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

gpt2 = DeepEvalLLM(model=model, tokenizer=tokenizer, name="GPT-2")

benchmark = MMLU(                                                      <span class="aframe-location"/> #2
    tasks=[MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ASTRONOMY],
    n_shots=3,
)

benchmark.evaluate(model=gpt2)    <span class="aframe-location"/> #3
print(benchmark.overall_score)
# MMLU Task Accuracy (task=high_school_computer_science): 0.0
# MMLU Task Accuracy (task=astronomy): 0.0
# Overall MMLU Accuracy: 0.0</pre>
<div class="code-annotations-overlay-container">
     #1 Sets up the model
     <br/>#2 Δefines benchmark with specific tasks and shots
     <br/>#3 Runs benchmark
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p102">
<p>MMLU gets at a deeper understanding than any of the previous benchmarks, which is promising, and a correlation can be drawn between this test and chat models that generally produce human-preferred responses. With deeper understanding, though, comes the need for more responsibility in the testing, and ethical concerns are beginning to be raised about these evaluations. For example, are the models being trained to answer questions about US history truthfully, or are they being evaluated on propaganda for an ideal nation? When answering questions about the law, are they conditioned to accept any bias the law system may or may not contain? The current answer is that models are likely demonstrating a deeper bias when performing well on these types of evals, and greater care needs to be taken to ensure that the bias presupposed in our evaluations is unharmful and generally accepted.</p>
</div>
<div class="readable-text" id="p103">
<h3 class="readable-text-h3" id="sigil_toc_id_68"><span class="num-string">4.2.3</span> Responsible AI benchmarks</h3>
</div>
<div class="readable-text" id="p104">
<p>Pretty much all industry benchmarks that evaluate LLMs have focused strictly on the model’s capability: “Is it able to do the job?” Unfortunately, that’s where most industry benchmarks end. It’s quite unfortunate, especially since going a step further and evaluating for bias isn’t that different from or more difficult than other evaluation techniques. It’s your responsibility, and it makes good business sense. Knowing where your model’s blind spots are can help you create plans to resolve them, which will help you sell to larger markets and avoid costly trouble in the future.</p>
</div>
<div class="readable-text intended-text" id="p105">
<p>The most common approach to evaluating bias is to take what we already know and segment the data across diverse groups. Thankfully, since moderating behavior for chat groups and social media sites is valuable to businesses, we already have many good evaluation metrics for traits like toxicity, polarity, hurtfulness, and similar. These metrics aim to measure how much a given text is related to violence, hate speech, racism, or profanity and how positive or negative they may be or just generally in violation of your company’s terms and conditions. Listing 4.5 demonstrates how to use the HONEST evaluation metric, which compares how hurtful prompt completions are for different genders. For example, when comparing potential attributes, generating the adjective “smart” would be less hurtful than “dumb.”</p>
</div>
<div class="browsable-container listing-container" id="p106">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.5</span> The HONEST metric</h5>
<div class="code-area-container">
<pre class="code-area">import evaluate
honest = evaluate.load('honest', 'en')
prompts = ["he is a", "he is", "she is a", "she is"]
completions = [['CEO', 'businessman', 'father'], 
               ['good', 'excellent', 'smart'], 
               ['secretary', 'prostitute', 'maid'], 
               ['beautiful', 'smart', 'tall']]
groups = ["male", "male", "female", "female"]
result = honest.compute(predictions=completions, groups=groups)
print(result)
# {'honest_score_per_group': {'male': 0.0, 'female': 0.16667}}</pre>
</div>
</div>
<div class="readable-text" id="p107">
<p>Once you pick one or more evaluators, you’ll need a dataset of prompts. You can find several in Hugging Face’s dataset. WinoBias focuses on gender bias. It contains prompts like, “The mechanic greets the receptionist because he” along with the opposite gender version, “The mechanic greets the receptionist because she.” We would run these prompts through our model and compare the groups. Some other datasets worth checking out include CALM, which compares gender and race, and WinoQueer, which looks at bias for LGBTQ+ groups. There are many more to check out, including datasets for politics, religions, and occupations, to name a few. </p>
</div>
<div class="readable-text print-book-callout" id="p108">
<p><span class="print-book-callout-head">NOTE</span>  You can learn more about CALM at <a href="https://arxiv.org/abs/2308.12539v1">https://arxiv.org/abs/2308.12539v1</a>, and WinoQueer here: <a href="https://arxiv.org/abs/2306.15087">https://arxiv.org/abs/2306.15087</a>.</p>
</div>
<div class="readable-text" id="p109">
<p>To put this all together, in listing 4.6, we’ll create an evaluation pipeline utilizing the Regard metric. The Regard metric looks at the polarity of content—whether it is a positive or negative statement. We’ll run this across the WinoBias dataset, segmenting the data by gender. Once we’ve run the analysis for each group, we can compare the results across the segments and see whether the distributions differ. Before reading on, take a guess. Do you think we’ll see more positive results for men or women, or will they be the same? What about negative results?</p>
</div>
<div class="browsable-container listing-container" id="p110">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.6</span> Running an evaluation pipeline on Regard </h5>
<div class="code-area-container">
<pre class="code-area">import torch
from transformers import pipeline
from datasets import Dataset, load_dataset
from evaluate import evaluator
import evaluate
import pandas as pd

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

pipe = pipeline("text-generation", model="gpt2", device=device)   <span class="aframe-location"/> #1
wino_bias = load_dataset("sasha/wino_bias_prompt1", split="test")
polarity = evaluate.load("regard")
task_evaluator = evaluator("text-generation")


def prepare_dataset(wino_bias, pronoun):    <span class="aframe-location"/> #2
    data = wino_bias.filter(
        lambda example: example["bias_pronoun"] == pronoun
    ).shuffle()
    df = data.to_pandas()
    df["prompts"] = df["prompt_phrase"] + " " + df["bias_pronoun"]
    return Dataset.from_pandas(df)


female_prompts = prepare_dataset(wino_bias, "she")
male_prompts = prepare_dataset(wino_bias, "he")

female_results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=female_prompts,
    input_column="prompts",
    metric=polarity,
)                               <span class="aframe-location"/> #3
male_results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=male_prompts,
    input_column="prompts",
    metric=polarity,
)

def flatten_results(results):    <span class="aframe-location"/> #4
    flattened_results = []
    for result in results["regard"]:
        item_dict = {}
        for item in result:
            item_dict[item["label"]] = item["score"]
        flattened_results.append(item_dict)

    return pd.DataFrame(flattened_results)


print(flatten_results(female_results).mean())     <span class="aframe-location"/> #5
# Prints the mean polarity scores
# positive    0.129005
# negative    0.391423
# neutral     0.331425
# other       0.148147

print(flatten_results(male_results).mean())       #5
# Positive    0.118647
# negative    0.406649
# neutral     0.322766
# other       0.151938</pre>
<div class="code-annotations-overlay-container">
     #1 Pulls model, data, and metrics
     <br/>#2 Prepares dataset
     <br/>#3 Runs through the evaluation pipeline
     <br/>#4 Analyzes results
     <br/>#5 Prints the mean polarity scores
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p111">
<p>Surprisingly to many, this example shows that gender polarity is rather comparable in our model. A good sign for this model! The bigger takeaway is that you should be automating your evaluations and running pipelines across many metrics, including looking for bias, not just performance. Overall, there are still many opportunities to improve evaluations and metrics in this space, especially when creating datasets and finetuning models to reduce bias. We expect to see lots of growth and innovation in this area of research.</p>
</div>
<div class="readable-text" id="p112">
<h3 class="readable-text-h3" id="sigil_toc_id_69"><span class="num-string">4.2.4</span> Developing your own benchmark</h3>
</div>
<div class="readable-text" id="p113">
<p>Overall, developing good benchmark datasets is still an unsolved problem. This is partly because once we develop one, our models quickly surpass it, making it obsolete and no longer “good.” There will be times when we discover edge cases for our model, such as parts of speech or certain tasks where it seems to struggle—maybe that’s playing chess or identifying sarcasm. Spoiler alert: LLMs are still terrible at these tasks, and if you haven’t seen a GPT versus Stockfish video yet, you’re in for a treat. In these cases, where we are trying to perform a specialized task, a simple evaluation would be to compare a custom list of prompts with expected responses. </p>
</div>
<div class="readable-text intended-text" id="p114">
<p>We recommend first checking out OpenAI’s Evals library (<a href="https://github.com/openai/evals">https://github.com/openai/evals</a>), where OpenAI has open sourced its evaluations. The library acts both as an evaluation framework and as a registry for edge-case datasets. At the time of this writing, the library contains almost 400 different datasets and is a great place to get started and contribute. This library gives you access to the same evaluation standards that OpenAI uses for their state-of-the-art models, and they’ve already done most of the heavy lifting in identifying areas of interest and curating datasets for these areas. </p>
</div>
<div class="readable-text intended-text" id="p115">
<p>As with most libraries built for a specific company but subsequently open sourced, it can be a bit of a pain to generalize. Running these evaluations against OpenAI’s models is easy-peasy, but extending it to run against your own models is anything but. While this is an annoyance that will likely go away if the community fully embraces and adopts the framework, the real downside to using this library is, ironically, that it’s open sourced. Being both a framework and registry (the data is stored alongside the code in the GitHub repo), if you are looking to curate a new evaluation dataset, but the dataset is private or can’t be open sourced for whatever reason, you are left with forking the repo and all the pain of managing it as your fork goes out of date.</p>
</div>
<div class="readable-text intended-text" id="p116">
<p>Another library to pay attention to is Hugging Face’s Evaluate. The Evaluate library is also a framework for building evaluation methods; however, the datasets are separate and can be found on the Hugging Face Hub in their own spaces. Since spaces can be private or public, it’s a much more user-friendly experience. Hugging Face has custom metrics and all the standard benchmarks already discussed in this chapter, as well as several not discussed. In listing 4.7, we show how to use the Evaluate library to get SQuAD metrics. SQuAD stands for the Stanford Question Answering Dataset, which is an older dataset with 100K questions and answers. SQuAD is a reading comprehension dataset consisting of questions generated from a set of Wikipedia articles, where the answer to every question is a segment of text inside the reading passage. The SQuAD metrics are a set of custom metrics that consist of an exact match; F1 scores were used in the paper introducing the dataset.<a href="#footnote-157"><sup class="footnote-reference" id="footnote-source-7">7</sup></a></p>
</div>
<div class="browsable-container listing-container" id="p117">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.7</span> Using the Evaluate library to run SQuAD</h5>
<div class="code-area-container">
<pre class="code-area">import evaluate

squad_metric = evaluate.load("squad")     <span class="aframe-location"/> #1
predictions = [
    {"prediction_text": "Saint Bernadette", "id": "5733be284776f41900661182"},
    {"prediction_text": "Salma Hayek", "id": "56d4fa2e2ccc5a1400d833cd"},
    {"prediction_text": "1000 MB", "id": "57062c2552bb89140068992c"},
]                          <span class="aframe-location"/> #2
references = [
    {
        "answers": {
            "text": ["Saint Bernadette Soubirous"],
            "answer_start": [515],
        },
        "id": "5733be284776f41900661182",
    },
    {
        "answers": {
            "text": ["Salma Hayek and Frida Giannini"],
            "answer_start": [533],
        },
        "id": "56d4fa2e2ccc5a1400d833cd",
    },
    {
        "answers": {"text": ["1000 MB"], "answer_start": [437]},
        "id": "57062c2552bb89140068992c",
    },
]
results = squad_metric.compute(
    predictions=predictions, references=references
)
print(results)
# {'exact_match': 33.333333333333336, 'f1': 79.04761904761905}</pre>
<div class="code-annotations-overlay-container">
     #1 Δownloads a metric from Hugging Face's Hub
     <br/>#2 Example from the SQuAΔ dataset
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p118">
<p>If you are creating your own benchmark, with the Evaluate library, you can easily create your own metric in a metric space and the dataset to use with the metric. This process isn’t too difficult. If you’ve decided not to create your own, the hardest part is finding good metrics. Searching through the hub is one thing, but since anyone can upload a metric and dataset, you never know if what you find is all that good, well curated, or clean.</p>
</div>
<div class="readable-text intended-text" id="p119">
<p>We haven’t dug too deeply into actually generating a dataset or metric, as that will be very specific to your use case, but what we have discussed are two great libraries you can use to do it. Evals is great if you are looking for an already curated dataset, and Evaluate is easy to use when generating your own. These tools are very useful, but in some special cases, you’ll need to think outside the box, and one of those cases that sticks out like a sore thumb is code generation. </p>
</div>
<div class="readable-text" id="p120">
<h3 class="readable-text-h3" id="sigil_toc_id_70"><span class="num-string">4.2.5</span> Evaluating code generators</h3>
</div>
<div class="readable-text" id="p121">
<p>One of the most valuable and sought-after use cases for LLMs is to have them help us write code. While we are unaware of any industry standard evaluation metrics for evaluating the generated code, thankfully, there are plenty of industry standards for evaluating the code itself (e.g., tests, profiles, security scanners, etc.). Using these tools provides a powerful path to evaluating the LLM through the code it generates.</p>
</div>
<div class="readable-text intended-text" id="p122">
<p>The basic setup looks like this: </p>
</div>
<ol>
<li class="readable-text" id="p123"> Have your model generate code based on docstrings. </li>
<li class="readable-text" id="p124"> Run the generated code in a safe environment on prebuilt tests to ensure they work and that no errors are thrown. </li>
<li class="readable-text" id="p125"> Run the generated code through a profiler and record the time it takes to complete. </li>
<li class="readable-text" id="p126"> Run the generated code through a security scanner and count the number of vulnerabilities. </li>
<li class="readable-text" id="p127"> Run the code against architectural fitness functions to determine artifacts, like how much coupling, integrations, and internal dependencies there are. </li>
<li class="readable-text" id="p128"> Run steps 1 to 5 on another LLM. </li>
<li class="readable-text" id="p129"> Compare results. </li>
</ol>
<div class="readable-text" id="p130">
<p>Listing 4.8 demonstrates an example using everyone’s favorite LeetCode problem, the Fibonacci sequence, as our prompt. This example shows using a separate fibonacci.py file as a prompt for our LLM to generate code. We could then use this test file to check that it runs correctly and how fast it runs. </p>
</div>
<div class="browsable-container listing-container" id="p131">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.8</span> An example test for evaluating code generators</h5>
<div class="code-area-container">
<pre class="code-area">''' fibonacci.py
def fibonacci_sequence(n):
    """Returns the nth number in the Fibonacci sequence"""
'''

import pytest
import time
from fibonacci import fibonacci_sequence


def test_fibonacci_sequence():
    test_cases = [(1, 0), (2, 1), (6, 5), (15, 377)]

    for n, expected in test_cases:
        result = fibonacci_sequence(n)
        assert (
            result == expected
        ), f"Expected {expected}, but got {result} for n={n}."

    with pytest.raises(ValueError):
        fibonacci_sequence(-1)


if __name__ == "__main__":     <span class="aframe-location"/> #1
    start_time = time.time()
    pytest.main(["-v"])
    end_time = time.time()
    execution_time = end_time - start_time
    print(f"Execution time: {execution_time} seconds")</pre>
<div class="code-annotations-overlay-container">
     #1 Runs tests using pytest and times it
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p132">
<p>There is lots of flexibility to this system, but the major downside is that it requires you to either create docstrings of coding challenges and write tests for them ahead of time or scrape LeetCode. Of course, you could have your LLM generate both of those too, but it’s easy to write simple tests that always pass and much harder to write tests that cover all the edge cases. So at some point, you’ll want a human in the loop.</p>
</div>
<div class="readable-text" id="p133">
<h3 class="readable-text-h3" id="sigil_toc_id_71"><span class="num-string">4.2.6</span> Evaluating model parameters</h3>
</div>
<div class="readable-text" id="p134">
<p>So far, all the evaluation methods we’ve looked at involve running the model and checking the results, but there is a lot we can learn by simply looking at the model. Surprisingly, there’s a lot you can learn by simply looking at the parameters of an ML model. For example, an untrained model will have a completely random distribution. By evaluating the distribution and paying attention to distinct features of a model’s parameters, we can learn whether a model is over- or undertrained. In the next listing, we use the weightwatcher library to do just that on the GPT-2 model, which will tell us which layers are over- or undertrained. </p>
</div>
<div class="browsable-container listing-container" id="p135">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.9</span> Using the weightwatcher library to evaluate GPT-2 </h5>
<div class="code-area-container">
<pre class="code-area">import weightwatcher as ww
from transformers import GPT2Model

gpt2_model = GPT2Model.from_pretrained("gpt2")
gpt2_model.eval()

watcher = ww.WeightWatcher(model=gpt2_model)
details = watcher.analyze(plot=False)
print(details.head())</pre>
</div>
</div>
<div class="readable-text" id="p136">
<p>This code prints out the following:</p>
</div>
<div class="browsable-container listing-container" id="p137">
<div class="code-area-container">
<pre class="code-area">   layer_id       name         D  ...      warning        xmax        xmin

0         2  Embedding  0.076190  ... over-trained 3837.188332    0.003564
1         8     Conv1D  0.060738  ...              2002.124419  108.881419
2         9     Conv1D  0.037382  ...               712.127195   46.092445
3        14     Conv1D  0.042383  ...              1772.850274   95.358278
4        15     Conv1D  0.062197  ...               626.655218   23.727908</pre>
</div>
</div>
<div class="readable-text" id="p138">
<p>Along with summary statistics, weightwatcher provides spectral analysis plots, as shown in figure 4.2. To create these plots, change line 8 in listing 4.9 to <code>plot=True</code>. The spectral analysis plots evaluate the frequencies of eigenvalues for each layer of a model. When evaluating these plots, we care about the tail of the distribution—the straighter it is (indicating a nice heavy tail), the better trained we expect the layer to be.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p139">
<img alt="figure" height="764" src="../Images/4-2.png" width="1007"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.2</span> weightwatcher Empirical Spectral Density (ESD) plots generated for GPT2’s second layer, which is predicted to be overtrained</h5>
</div>
<div class="readable-text print-book-callout" id="p140">
<p><span class="print-book-callout-head">NOTE</span>  These plots are created to mimic Spectral Density plots you might see in a physics lab. We will not discuss them in this book, but if interested, we recommend you check out the WeightWatchers documentation: <a href="https://github.com/CalculatedContent/WeightWatcher">https://github.com/CalculatedContent/WeightWatcher</a>.</p>
</div>
<div class="readable-text" id="p141">
<p>weightwatcher is rather powerful, as it allows us to compare different models, helping us better understand which model is better trained without running them at all, making it relatively inexpensive. This capability comes in handy when you are trying to determine which base model to use, as an undertrained model may require a lot more finetuning.</p>
</div>
<div class="readable-text intended-text" id="p142">
<p>Since we are comparing models based on their parameters alone, this method provides a nice agnostic view of the current state of a model. We can implement it during and after training and during ongoing updates using methods such as RLHF. It is both an easy and powerful evaluation method. However, the downside is that it doesn’t provide any insight into the training data, so it can’t tell us which model is that effective at which task and is best paired with other evaluation methods already discussed.</p>
</div>
<div class="readable-text intended-text" id="p143">
<p>We’ve already spent quite a bit of time talking about data most data engineers likely don’t think about often: model weights and evaluation data. These are crucial ingredients to gather to generate a specialized finetuned LLM. Indeed, LLMs introduce new data engineering challenges, just like they introduce new MLOps and data science challenges. Next, we will discuss what many of you have been waiting for: the training data. We’ll discuss different datasets that are essential to know about, where to get them, and how to prepare them to train or finetune LLMs.</p>
</div>
<div class="readable-text" id="p144">
<h2 class="readable-text-h2" id="sigil_toc_id_72"><span class="num-string">4.3</span> Data for LLMs</h2>
</div>
<div class="readable-text" id="p145">
<p>It has been shown that data is the most important part of training an LLM. We hope that the sudden importance of language modeling will persuade businesses to start managing their data generally according to accepted guidelines. As is shown by experiments like LLaMA, Alpaca, Goat, Vicuna, and later, LIMA<a href="#footnote-158"><sup class="footnote-reference" id="footnote-source-8">8</sup></a> and SpQR,<a href="#footnote-159"><sup class="footnote-reference" id="footnote-source-9">9</sup></a> high-quality training data and clever modeling are much more important than the number of parameters or size of training data. Measuring that quality is still a point of difficulty in general; however, we’ll discuss methodologies you can employ to do so.</p>
</div>
<div class="readable-text intended-text" id="p146">
<p>We’ll first discuss common datasets you should know about, what’s in them, why you would want them, and where you can get them. Then we’ll talk about common processing and preparation techniques you’ll need to understand to get the most out of them and get better results from your LLMs.</p>
</div>
<div class="readable-text" id="p147">
<h3 class="readable-text-h3" id="sigil_toc_id_73"><span class="num-string">4.3.1</span> Datasets you should know</h3>
</div>
<div class="readable-text" id="p148">
<p>If you didn’t notice, in section 4.1, we made it a point to discuss which datasets different models were trained on. It might have come across as just another factoid about the model, but this is highly valuable information! Knowing what a model was trained on (or not trained on) is the first step to understanding what it can or cannot do. For example, knowing an LLM coding model was trained heavily on the C programming language but didn’t see a lick of C++ will be more than enough to realize why it seems to work syntactically but produces so many errors and bugs when writing C++ code.</p>
</div>
<div class="readable-text" id="p149">
<h4 class="readable-text-h4 sigil_not_in_toc">Wikitext</h4>
</div>
<div class="readable-text" id="p150">
<p>One of the most familiar datasets, Wikitext is, as the name implies, essentially Wikipedia. It was crafted by the Salesforce team back in 2016. It is a great dataset to turn to when you’re only trying to do a proof of concept or a rapid prototype since the English version comes in at only 741 MB, not even 1 GB. Add to that the fact that Wikipedia is a trusted source of information—especially compared to the internet at large, where most of the other sources come from—and this gets even better!</p>
</div>
<div class="readable-text intended-text" id="p151">
<p>Some downsides: it is purely an English dataset, which greatly reduces the diversity of tokens the model will see; Wikipedia contains an idealized version of language—one that we subjectively value as clear—even though it doesn’t contain any instances of how language is actually used, only meta-explanations on usage. Also, it’s almost a decade old as of this writing, which, of course, no one checks. We’ve seen many teams use it to quickly prototype and create Q&amp;A bots due to its ease of use and access. It does well in prototyping but always comes off as unimpressive when it gets to production, as users tend to prefer asking questions about current events. Always check the freshness of your data! Overall, it’s a valuable dataset information-wise, but bad if you want your models to interact in a human-like way.</p>
</div>
<div class="readable-text" id="p152">
<h4 class="readable-text-h4 sigil_not_in_toc">Wiki-40B</h4>
</div>
<div class="readable-text" id="p153">
<p>A good alternative is Wiki-40B from 2020, a cleaned-up version of Wikitext with 40 different language variations. It comes in at a little over 10 GB. So it’s still quite small for prototyping. It comes with all the same benefits Wikitext does: it’s a clean dataset and a trusted source of information. Plus, it’s newer and has more languages. This is a great dataset to use to become familiar with multilingual modeling.</p>
</div>
<div class="readable-text" id="p154">
<h4 class="readable-text-h4 sigil_not_in_toc">Europarl</h4>
</div>
<div class="readable-text" id="p155">
<p>One of the best toy datasets for multilingual problems, Europarl contains the European Parliament proceedings from 1996 to 2011. It includes translations in 21 different European languages and is great for smaller projects and multilingual demos. Europarl is an excellent source of data, albeit idealized and outdated, much like English Wikitext. In addition, the project includes many parallel corpora, which are paired down to English and one of the 20 other languages. The total dataset is just 1.5 GB and can be found at <a href="https://www.statmt.org/europarl/">https://www.statmt.org/europarl/</a>. </p>
</div>
<div class="readable-text" id="p156">
<h4 class="readable-text-h4 sigil_not_in_toc">Common Crawl</h4>
</div>
<div class="readable-text" id="p157">
<p>The Common Crawl dataset is essentially the entire internet, web scraped and open sourced. It uses web crawlers similar to what Google or Microsoft use to enable search engines. C4, the Colossal Cleaned version of the Common Crawl dataset, is the most common dataset for self-supervised pretraining. Unfortunately, being cleaned doesn’t mean it is free of inherent societal bias, which is true for pretty much all the datasets openly available today. Containing the entirety of the internet means it contains all the good and the bad; it is a very diverse dataset full of multiple languages and code.</p>
</div>
<div class="readable-text intended-text" id="p158">
<p>The Common Crawl dataset is named after the nonprofit organization of the same name that is dedicated to providing a copy of the internet to anyone for the purpose of research and analysis. You can access the dataset at <a href="https://commoncrawl.org/">https://commoncrawl.org/</a>, where you will find many versions because Common Crawl periodically crawls the web and updates the dataset. The community has been archiving the internet since 2008. It comes in four variants to help with your various needs: a 305 GB version containing the actual C4; a 380 GB version that contains so-called bad words along with everything else; a 2.3 TB version, which is the uncleaned version (not recommended); and a 15 GB version of data that is professional enough to appear on the news.</p>
</div>
<div class="readable-text" id="p159">
<h4 class="readable-text-h4 sigil_not_in_toc">OpenWebText</h4>
</div>
<div class="readable-text" id="p160">
<p>Another dataset we’d recommend for pretraining is OpenWebText, which only takes up 55 GB on disk. It is an open source effort to reproduce OpenAI’s WebText dataset used to train GPT-2. Instead of being a copy of the entire internet, researchers used Reddit to extract URLs from posts and then filtered the list using Reddit’s karma ranking system. They then scraped the URLs to create the dataset. Since the content mainly comes from Reddit, it calls into question its real-world accuracy due to the selection bias of only including people with a Reddit account. It is made up mostly of news articles, blog posts, and other content often shared on forums. You can think of it as a highly curated and much smaller version of the Common Crawl dataset. </p>
</div>
<div class="readable-text intended-text" id="p161">
<p>Like Wikitext, it’s a bit older; the most commonly used version was created in 2019, and a new version hasn’t been updated in four years at the time of writing. Of course, since the dataset was curated with a specific methodology, it could be refreshed at any time.</p>
</div>
<div class="readable-text" id="p162">
<h4 class="readable-text-h4 sigil_not_in_toc">The Pile</h4>
</div>
<div class="readable-text" id="p163">
<p>One dataset that has garnered a lot of attention and should be on your radar is The Pile, which was created by EleutherAI in 2020 and published on December 31 of the same year.<a href="#footnote-160"><sup class="footnote-reference" id="footnote-source-10">10</sup></a> It is useful for self-supervised pretraining tasks. The Pile is one of the largest datasets we’ll discuss at 825 GB and consists of 22 smaller high-quality datasets combined to make a diverse and dense training set. It includes most of the datasets we have already discussed, like Common Crawl, OpenWebText, and Wikipedia. It also contains book datasets, like Books3 and Gutenberg; code datasets, like GitHub and Stack Exchange; and specialist datasets, like PubMed and FreeLaw. It also includes datasets like the Enron Emails, which we can’t help but think was a mistake.</p>
</div>
<div class="readable-text intended-text" id="p164">
<p>Because it’s so massive and includes multiple languages and code samples, it has proven useful in training many LLMs. It is multilingual in addition to dense, making it ideal for learning sparse general language representations. Overall, though, it’s not very clean and is essentially just a conglomerate of multiple datasets. Unless you are training LLMs from scratch, you likely won’t use this dataset, but it’s important to become familiar with it, as many of the largest models have been trained on it. You can find the dataset at EleutherAI’s website: <a href="https://pile.eleuther.ai/">https://pile.eleuther.ai/</a>.</p>
</div>
<div class="readable-text" id="p165">
<h4 class="readable-text-h4 sigil_not_in_toc">RedPajama</h4>
</div>
<div class="readable-text" id="p166">
<p>RedPajama is a dataset created by a collaboration of Together.ai, Ontocord.ai, ETH DS3Lab, Stanford CRFM, and Hazy Research. The goal was to create a fully open dataset that mimicked what was described in the LLaMA paper. </p>
</div>
<div class="readable-text print-book-callout" id="p167">
<p><span class="print-book-callout-head">NOTE</span>  You can read the blog post introducing RedPajama here: <a href="https://together.ai/blog/redpajama">https://together.ai/blog/redpajama</a>.</p>
</div>
<div class="readable-text" id="p168">
<p>The dataset is similar to The Pile but much larger at 5 TB and newer, published in April 2023. It contains fewer datasets: GitHub, arXiv, Books, Wikipedia, StackExchange, and Common Crawl. It is so large because it contains five different dumps of the Common Crawl dataset with varying filters and the standard C4 dataset. It is made available through the Hugging Face Hub and can be found at <a href="https://mng.bz/4ppD">https://mng.bz/4ppD</a>.</p>
</div>
<div class="readable-text" id="p169">
<h4 class="readable-text-h4 sigil_not_in_toc">OSCAR</h4>
</div>
<div class="readable-text" id="p170">
<p>The best dataset by far to train on for multilingual models is OSCAR, which is larger than any other dataset discussed, coming in at 9.4TB, over 11 times as big as The Pile! It is an open source project started in 2019 and has been funded by a multitude of institutes and governments. You can learn more about the project and dataset at <a href="https://oscar-project.org/">https://oscar-project.org/</a>.</p>
</div>
<div class="readable-text intended-text" id="p171">
<p>This project is actively being worked on, and new releases come out annually with regular updates. It currently supports 166 languages at the time of this writing, much more than any other dataset. As a work in progress, though, there are some languages much more represented than others, with some in the TBs of data and others in KBs. This is one of our favorite datasets because it is actively being worked on, and the team is passionate about representation in LLMs and AI, as well as producing highly clean, high-quality data. We encourage all interested readers to contribute to this dataset.</p>
</div>
<div class="readable-text" id="p172">
<h4 class="readable-text-h4 sigil_not_in_toc">Summary of datasets</h4>
</div>
<div class="readable-text" id="p173">
<p>In table 4.2, you can see a summary of the datasets we’ve discussed so far. These datasets are all commonly used in industry and worth familiarizing yourself with. We encourage you to investigate them further and take a closer look at the data within. </p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p174">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 4.2</span> Summary of datasets</h5>
<table>
<thead>
<tr>
<th>
<div>
         Dataset 
       </div></th>
<th>
<div>
         Contents 
       </div></th>
<th>
<div>
         Size 
       </div></th>
<th>
<div>
         Last update 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Wikitext <br/></td>
<td>  English Wikipedia <br/></td>
<td>  &lt;1 GB <br/></td>
<td>  2016 <br/></td>
</tr>
<tr>
<td>  Wiki-40B <br/></td>
<td>  Multi-lingual Wikipedia <br/></td>
<td>  10 GB <br/></td>
<td>  2020 <br/></td>
</tr>
<tr>
<td>  Europarl <br/></td>
<td>  European Parliament proceedings <br/></td>
<td>  1.5 GB <br/></td>
<td>  2011 <br/></td>
</tr>
<tr>
<td>  Common Crawl <br/></td>
<td>  The internet <br/></td>
<td>  ~300 GB <br/></td>
<td>  Ongoing <br/></td>
</tr>
<tr>
<td>  OpenWebText <br/></td>
<td>  Curated internet using Reddit <br/></td>
<td>  55 GB <br/></td>
<td>  2019 <br/></td>
</tr>
<tr>
<td>  The Pile <br/></td>
<td>  Everything above plus specialty datasets (books, law, med) <br/></td>
<td>  825 GB <br/></td>
<td>  2020 <br/></td>
</tr>
<tr>
<td>  RedPajama <br/></td>
<td>  GitHub, arXiv, Books, Wikipedia, StackExchange, and multiple version of Common Crawl <br/></td>
<td>  5 TB <br/></td>
<td>  2023 <br/></td>
</tr>
<tr>
<td>  OSCAR <br/></td>
<td>  Highly curated multilingual dataset with 166 languages <br/></td>
<td>  9.4 TB <br/></td>
<td>  Ongoing <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p175">
<h4 class="readable-text-h4 sigil_not_in_toc">Corpora</h4>
</div>
<div class="readable-text" id="p176">
<p>As you probably picked up on, most of the datasets out there are essentially just text dumps of the internet. If you’re looking for something with a little more finesse, something that contains more meta info to help your model disambiguate for more complex tasks, consider downloading a corpus. A corpus is just like a dataset, except it is more easily searchable, visualized, and explained. Corpora are often paid datasets that can be well worth your money. Corpora, like the Corpus Of Historical American English (COHA) and the Corpus of Contemporary American English (COCA), are excellent downloads. They contain not just text data but also frequency analysis (bag of words) and collocates (N-grams), all ready to go. Whether or not you are interested in the applications of allowing models to analyze metadata as part of training, using corpora can help with model explainability and quality of data. </p>
</div>
<div class="readable-text intended-text" id="p177">
<p>You can think of a corpus as a vector database that has already been highly cleaned and curated and is ready to go. While it hasn’t yet been done, a corpus that combines both the linguistic explainability and time-series bucketing with precalculated embeddings put into a real-time vector database would likely be invaluable and highly profitable in this field for the foreseeable future, especially if both textual and audio data are captured. If your company has its own language data it wants to train on, your best course of action is to create a corpus where your biggest job is saying where data came from when and what the overall goal of the data going into the model is. Almost every NLP library has strategies for creating corpora, from NLTK to spaCy and even LangChain. Be mindful about which strategies and tools you pick because at the end of the day, your dataset or corpus contains everything your model will see.</p>
</div>
<div class="readable-text" id="p178">
<h3 class="readable-text-h3" id="sigil_toc_id_74"><span class="num-string">4.3.2</span> Data cleaning and preparation</h3>
</div>
<div class="readable-text" id="p179">
<p>If you pulled any of the previously mentioned datasets, you might be surprised to realize most of them are just giant text dumps—a large parquet or text file. There are no labels or annotations, and feature engineering hasn’t been done at all. LLMs are trained via self-supervised methods to predict the next word or a masked word, so a lot of traditional data cleaning and preparation processes are unneeded. This fact leads many to believe that data cleaning as a whole is unnecessary, but this couldn’t be further from the truth. Datasets are the lifeblood of all ML, and they are so much more than a pile of data. Yet that’s what most businesses have—a pile of data. Data cleaning and curation are difficult, time-consuming, and ultimately subjective tasks that are difficult to tie to key performance indicators (KPIs). Still, taking the time and resources to clean your data will create a more consistent and unparalleled user experience.</p>
</div>
<div class="readable-text intended-text" id="p180">
<p>Since the 1990s, people have tested whether Big Data can produce better results than high-quality data; we believe the answer is no. Big Data is nowhere close to devoid of value. The Law of Big Numbers has been applied, and it has shown that models can generate convincing syntax at the same level as people. However, as we’ve said before, models have also soundly demonstrated that syntax is in no way connected to semantics or pragmatics.</p>
</div>
<div class="readable-text intended-text" id="p181">
<p>In this section, we hope to share with you the right frame of mind when preparing your dataset. We will focus on the high-level linguistic considerations you should be thinking about when preparing a dataset, and we won’t be going too deep into how to create the actual data pipelines. That said, the main logic is simple and follows these basic steps:</p>
</div>
<ol>
<li class="readable-text" id="p182"> Take your pile of data, and determine a schema for the features. </li>
<li class="readable-text" id="p183"> Make sure all the features conform to a distribution that makes sense for the outcome you’re trying to get through normalization or scaling. </li>
<li class="readable-text" id="p184"> Check the data for bias/anomalies (most businesses skip this step by using automated checking instead of informed verification). </li>
<li class="readable-text" id="p185"> Convert the data into a format for the model to ingest (for LLMs, it’s through tokenization and embedding) </li>
<li class="readable-text" id="p186"> Train, check, and retrain. </li>
</ol>
<div class="readable-text print-book-callout" id="p187">
<p><span class="print-book-callout-head">NOTE</span> For more information on creating data pipelines, check out Fundamentals of Data Engineering,<a href="#footnote-161"><sup class="footnote-reference" id="footnote-source-11">11</sup></a> WizardLM,<a href="#footnote-162"><sup class="footnote-reference" id="footnote-source-12">12</sup></a> and “LIMA: Less Is More for Alignment.”<a href="#footnote-163"><sup class="footnote-reference" id="footnote-source-13">13</sup></a> These resources can help you create effective data pipelines to get as much data into a trainable state as possible.</p>
</div>
<div class="readable-text" id="p188">
<p>None of these steps are necessarily easy, but we hope to share a few tips and tricks. Evaluating whether your distribution is correct can be as simple as looking at the data and asking yourself whether it truly represents the problem or as difficult as creating a whole human-in-the-loop workflow to validate your model’s output. Next, we’ll go over the first three steps, and in the next section, we’ll go over the fourth. The last step is covered in depth in the next chapter.</p>
</div>
<div class="readable-text" id="p189">
<h4 class="readable-text-h4 sigil_not_in_toc">Instruct schema</h4>
</div>
<div class="readable-text" id="p190">
<p>One of the best and most common data schemas you should consider when preparing your data, especially for finetuning, is the instruct schema. Instruction tuning is based on the intuitive logic that if we show a model how to perform a task with instructions, the model will perform better than if we just show it tasks and “answers.” Instruction tuning involves demonstrating for the model what you would like to happen, and as such, the datasets are more intensive to create than your run-of-the-mill crawl data. You need to prepare your data to match a format that will look something like this:</p>
</div>
<div class="readable-text prompt" id="p191">
<p><strong><span class="prompt-head"><span class="prompt-initials">CB</span> ###Instruction</span></strong></p>
</div>
<div class="readable-text prompt" id="p192">
<p>{user input}</p>
</div>
<div class="readable-text prompt" id="p193">
<p><strong><span class="prompt-head"><span class="prompt-initials">CB</span> ###Input</span></strong></p>
</div>
<div class="readable-text prompt" id="p194">
<p>{meta info about the instruction}</p>
</div>
<div class="readable-text response" id="p195">
<p><strong><span class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/> ###Response</span></strong></p>
</div>
<div class="readable-text response" id="p196">
<p>{model output}</p>
</div>
<div class="readable-text" id="p197">
<p>Instruction datasets are powerful because they allow the model to consider both instructions and relevant input. For example, if the instruction was “Translate this sentence to Japanese,” the input would be the sentence you’d want translated, and the response would be the Japanese translation. Thus, they prepare your model for many prompting techniques and prompt tuning, making them more effective later.</p>
</div>
<div class="readable-text intended-text" id="p198">
<p>Despite their name, instruction tuning datasets are not restricted to test-based modalities; they can also use vision instruction tuning (image–instruction–answer) and red teaming instruction (RLHF) datasets. The “instruction” offers a semblance of pragmatics within the model and prompt, providing important guardrails for the LLM as it generates responses. It grounds the prompt with syntax that repeats and is predictable, along with syntax that is unpredictable for the model to guess at. These syntactic landmarks <code>(###Instruction</code>, <code>User:</code>, <code>Chat</code> <code>History:</code>, etc.) also help lower the chance of an EOS (end-of-sequence) token being predicted early due to the variable length of what can come between each of them, like chat history. Chat history can be one message or thousands of tokens, but the pattern, given there’s another landmark coming afterward, helps the model succeed in long-term memory. When you are deciding what to train your model on, keep those landmarks in mind, as they can make an instruct-tuned model even better at a specific task if you only need it to do one thing.</p>
</div>
<div class="readable-text intended-text" id="p199">
<p>This isn’t the only format; some competitors in the space include the evol-instruct format used by WizardLM and the self-instruct format used by Alpaca, both of which use scripts to create instruction-based prompts. The best format is still an open-ended question, and we’d like to extend a challenge to the reader to explore creating their own. GitHub (<a href="https://mng.bz/5OmD">https://mng.bz/5OmD</a>) and Hugging Face datasets are both great places to look for vetted datasets at the moment, but keep in mind that if the dataset doesn’t contain many examples of the tasks you’d like your model to perform or it doesn’t contain enough examples of semantic ambiguity being resolved when completing the task, performance will be unstable—which takes us to step 2 in our cleaning process.</p>
</div>
<div class="readable-text" id="p200">
<h4 class="readable-text-h4 sigil_not_in_toc">Ensuring proficiency with speech acts</h4>
</div>
<div class="readable-text" id="p201">
<p>In preparing the dataset, the most important consideration is what you want the model to do. If you want a model to predict housing prices in Boston, you probably shouldn’t train it on survivors of the Titanic. This is obvious when stated, but it raises the question, “Is my dataset correct for the problem, and how would I know?” When it comes to language data, the answer isn’t as obvious as we might hope. Let’s look at an example to figure out why. </p>
</div>
<div class="readable-text intended-text" id="p202">
<p>Let’s say you want your model to take orders at a fast-food restaurant. This scenario may seem boring and mundane, where all we expect to see are queries like, “I’ll order the #3 combo,” which you will. But if you ask a cashier about how people actually talk to them, really, anything can happen! I had a friend who worked at Burger King tell me that because of Burger King’s slogan “Have It Your Way,” he received many crazy requests, like asking for a burger with two top buns. That blew my mind, but it was also a tame example. Not to mention, you never know when the next LARPing convention will bring more creative and colorful interactions to otherwise mundane scenarios. A generic dataset containing customer orders and cashier responses won’t be enough here. When you aren’t intentional about what kind of data goes into your model, the performance of the model suffers.</p>
</div>
<div class="readable-text print-book-callout" id="p203">
<p><span class="print-book-callout-head">DEFINITION</span> LARP stands for live-action role-playing, and you can imagine the tomfoolery of a customer pretending to be an elf, orc, or pirate and thus breaking all rules and expectations.</p>
</div>
<div class="readable-text" id="p204">
<p>To ensure your data is right for the task, first, you should think about what speech acts generally go together to perform the task at hand. Speech acts refer to the various functions language can perform in communication beyond conveying information. They are a way of categorizing utterances based on their intended effect or purpose in a conversation. Speech acts are important, as they shed light on how communication goes beyond the literal meaning of words and involves the speaker’s intentions and the listener’s interpretation.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p205">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Speech acts defined</h5>
</div>
<div class="readable-text" id="p206">
<p>The following list includes common speech acts and their definitions:</p>
</div>
<ul>
<li class="readable-text" id="p207"> <em>Expressives</em>—Greetings, apologies, congratulations, condolences, thanksgivings (e.g., “You’re the best!”) </li>
<li class="readable-text" id="p208"> <em>Commissives</em>—Promises, oaths, pledges, threats, vows (e.g., “I swear by the realm, the princess will come to no harm.”) </li>
<li class="readable-text" id="p209"> <em>Directives</em>—Commands, requests, challenges, invitations, orders, summons, entreaties, dares (e.g., “Get it done in the next three days.”) </li>
<li class="readable-text" id="p210"> <em>Declarations</em>—Blessings, firings, baptisms, arrests, marrying, juridical speech acts such as sentencings, declaring a mistrial, declaring out of order (e.g., “You’re hired!”) </li>
<li class="readable-text" id="p211"> <em>Verdictives</em>—Rankings, assessments, appraising, condoning (combinations such as representational declarations; e.g., “You’re out!”) </li>
<li class="readable-text" id="p212"> <em>Questions</em>—Usually starting with interrogative words like <em>what</em>, <em>where</em>, <em>when</em>, <em>why</em>, <em>who</em>, or indicated with rising intonation at the end in English (e.g., “Which model is best for my task?”) </li>
<li class="readable-text" id="p213"> <em>Representatives</em>—Assertions, statements, claims, hypotheses, descriptions, suggestions, answers to questions (e.g., “This model is best for your task.” </li>
</ul>
</div>
<div class="readable-text" id="p214">
<p>The current way we measure the robustness of datasets for LLMs is the vanilla number of tokens. Instruct datasets are relatively new, but they rely on you being intentional with how instruction for the model happens. What will your model do when given a directive it shouldn’t respond to when it’s only been trained on helpful responses to directives? If you aren’t sure, now’s the time to consider. For example, imagine a user declaring with glee to your bot, “Promise you’ll help me take over the world!” If it was only trained to be helpful, it would likely respond by promising to do just that because similar scenarios are in the training set. And now we have an evil AI overlord taking over the world. Thanks. In actuality, this is a fairly innocuous example, but the unpredictability of the seemingly infinite number of possible responses from the model should make you think, especially if this agent has access to tools like Google or your internal HR documents. Being cognizant of speech acts can simplify your work so that you don’t have to focus as much on individual tokens for the vocabulary as on the overall structure of what your model will come in contact with during training.</p>
</div>
<div class="readable-text intended-text" id="p215">
<p>Going back, when you think about a customer-facing role like a cashier, how many of these speech acts are likely to occur in your average order? Take a minute to think it through. We can tell you that declarations and verdictives are out, and commissives are uncommon. But what if you get them regardless? You then need to consider how you might want to steer such highly expressive customers toward the speech acts you can work with, likely questions, directives, and representatives. </p>
</div>
<div class="readable-text intended-text" id="p216">
<p>To make matters more complicated, the form of a speech act doesn’t always have to match its function. For example, you could say “You’re fired” to your friend who doesn’t work for you, where, even though its form is declarative, its function is more likely expressive. Once you have a dataset or a trained LLM and are looking to improve its ability to take instruction, this is something to seriously consider to increase your data’s quality and your LLM’s performance. Does your model weirdly fail when users frame utterances as questions when they’re actually directives? Does your model start hallucinating when coming in contact with the representative-only HR documents you’ve been asked to analyze? As a note, you don’t have to completely finetune a model all over again to improve performance. We’ll go over this in more detail later, but giving specific examples within the prompt can patch a lot of these edge cases quickly and inexpensively.</p>
</div>
<div class="readable-text intended-text" id="p217">
<p>Now that you have an understanding of the different features you should be looking for in your dataset, let’s consider the best ways to annotate your dataset so you can make sure it conforms to expectations.</p>
</div>
<div class="readable-text" id="p218">
<h4 class="readable-text-h4 sigil_not_in_toc">Annotating the data</h4>
</div>
<div class="readable-text" id="p219">
<p>Annotation is labeling your data, usually in a positionally aware way. For speech recognition tasks, annotations would identify the different words as <em>noun</em>, <em>verb</em>, <em>adjective</em>, or <em>adverb</em>. Annotations were used as labels in supervised learning tasks as the main way to train a model. Now annotations essentially give us metadata that makes it easier to reason about and analyze our datasets. Instead of worrying about micro information like speech recognition or named-entity recognition, you’ll get more value by focusing on macro metadata, like the speech acts just discussed or what language the data is in.</p>
</div>
<div class="readable-text intended-text" id="p220">
<p>Of course, this is the real trick, isn’t it? If this were easy, every company on the face of the earth would have its own models already in production. The fact is data wrangling is too large to be done by hand but too varying to be done automatically, and you need to find the middle ground as quickly as possible. You don’t want to ignore your data and just download a dataset someone recommended (even us) and then proceed to harm a real-world population because it contained harmful data. But you also don’t want to have to hand-validate millions of rows of utterances. Thankfully, there are tools to help with every part of this, but we’d like to specifically mention these first: </p>
</div>
<ul>
<li class="readable-text" id="p221"> <em>Prodi.gy</em> (<a href="https://prodi.gy/">https://prodi.gy/</a>)—Prodigy takes a one-time payment for a quick and powerful multimodal annotation tool. </li>
<li class="readable-text" id="p222"> <em>doccano: Open source annotation tool for machine learning practitioners</em><span class="link-like"/>(<a href="https://github.com/doccano/doccano">https://github.com/doccano/doccano</a>)—A truly open-source and, at the time of writing, updated web-based platform for data annotation. </li>
<li class="readable-text" id="p223"> <em>d5555/TagEditor: Annotation tool for spaCy</em> (<a href="https://github.com/d5555/TagEditor">https://github.com/d5555/TagEditor</a>)—Works in conjunction with <a href="https://spacy.io">https://spacy.io</a>. Both create an ecosystem on top of spaCy, a popular NLP framework that makes rapid prototyping well within the reach of your average ML team. </li>
<li class="readable-text" id="p224"> <em>Praat: Doing phonetics by computer</em> (<a href="https://github.com/praat/praat">https://github.com/praat/praat</a>)—The only audio annotation tool on this list, Praat is fundamentally a tool for phonetics with annotations thrown in. Given how much we predict the LLM space to shift toward phonetics, we couldn’t omit this one from the list. </li>
<li class="readable-text" id="p225"> <em>Galileo</em> (<a href="https://www.rungalileo.io/llm-studio">https://www.rungalileo.io/llm-studio</a>)—At the time of this writing, Galileo’s LLM studio has yet to come out, but it makes some big promises for prompt creation and evaluation, which would immensely speed up annotation and creation of instruction datasets. </li>
</ul>
<div class="readable-text" id="p226">
<p>Which tool is best for your project depends entirely on the goal of your annotation. Going into annotating without a specified goal leads nowhere, as you’ll find discrepancies on the other end of data processing. Of course, we recommend adding speech act annotations; you’ll also want to consider additional annotations looking for bias and anomalies. We can show that by measuring the number of pieces of outside context present in the text (things like insinuations or entailments), you can gain a confidence score about how high quality a particular data is. The reason for this is intuitive: the more ambiguity a set of examples can solve for the model, the more the model learns from that set. The hard part is that no one can pin any of these contextual information nuggets on repeating parts of orthography, such as individual characters or a particular word or subword.</p>
</div>
<div class="readable-text intended-text" id="p227">
<p>Annotating can be a lot of work, but the reason for all of this consideration at the front is fairly simple: your model can only learn what you teach it. Thankfully, to make matters much easier, the goal isn’t to annotate every bit of text in your dataset. We are simply annotating a large-enough sample to ensure our dataset is representative of the task. Remember, LLMs are generally trained in two steps:</p>
</div>
<ol>
<li class="readable-text" id="p228"> <em>Self-supervised pretraining </em>—Analyzing many different speech acts in varying forms and functions to learn general representations </li>
<li class="readable-text" id="p229"> <em>Finetuning and RLHF </em>—Teaching the model how/when to use the representations learned in step 1 </li>
</ol>
<div class="readable-text" id="p230">
<p>This training significantly lightens the burden on you as a trainer of attempting to parse every possible locution (what a person literally says) and illocution (what they actually mean in context) within the given task. Even for something viewed as simple work, like being a cashier, having to come up with a dataset vast enough to cover all edge cases would be quite a headache. For most cases, all you need to do is prepare a finetuning dataset, which often doesn’t need to be large at all—sometimes a dozen examples is more than enough to start getting good results.</p>
</div>
<div class="readable-text" id="p231">
<h2 class="readable-text-h2" id="sigil_toc_id_75"><span class="num-string">4.4</span> Text processors</h2>
</div>
<div class="readable-text" id="p232">
<p>Now that you have a dataset for training or finetuning, we need to transform it into something that can be consumed by the LLM. Simply put, we need to turn the text into numbers. We’ve already briefly gone over the process of doing that conversion quickly and effectively, so let’s dive into different examples and methodologies. </p>
</div>
<div class="readable-text intended-text" id="p233">
<p>In this section, we’ll show you how to train your own tokenizers, both byte-pair encoding (BPE) and SentencePiece tokenizers, and how to grab embeddings from (almost) any model for storage or manipulation later. This step is often ignored when working with an LLM through an API, but much of modern performance in data applications depends on doing this process correctly and specifically for your goal. There are many mathematically sound and correct ways to tokenize text, so you can’t rely on something someone else did when you have a specific use case. You need to prepare it for that use case. Training your own tokens will allow you to minimize unknown tokens, <code>&lt;UKN&gt;</code>, while also maximizing encoded semantics. Having control of this process is one of the simplest and easiest hacks to give your models a major boost in performance. Let’s start first with tokenization.</p>
</div>
<div class="readable-text" id="p234">
<h3 class="readable-text-h3" id="sigil_toc_id_76"><span class="num-string">4.4.1</span> Tokenization</h3>
</div>
<div class="readable-text" id="p235">
<p>Tokenization is a bit more involved than simple vectorization but leads to the same overall result: text input, vector output, and the ability to encode and decode. We mentioned in chapter 2 the multilingual factor and in chapter 3 the token tax of foreign languages, which are both motivations to be at least aware of your own tokenization strategies. However, it goes beyond those. Your tokenization strategy isn’t just important; it is vitally important for every subsequent step. </p>
</div>
<div class="readable-text intended-text" id="p236">
<p>A good example is comparing GOAT 7B and GPT-4 in math and arithmetic. Consider table 4.3. The left column is a simple arithmetic prompt. Then we see the two models’ answers and, for reference, the actual answer so you don’t have to pull out your calculator.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p237">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 4.3</span> Tokenization allows GOAT 7B to outperform GPT-4 in math</h5>
<table>
<thead>
<tr>
<th>
<div>
         Prompt 
       </div></th>
<th>
<div>
         GOAT 7B 
       </div></th>
<th>
<div>
         GPT-4 1.7T 
       </div></th>
<th>
<div>
         Correct 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  3978640188 + 42886272 = <br/></td>
<td>  4021526460 <br/></td>
<td>  4,021,526,460 <br/></td>
<td>  4,021,526,460 <br/></td>
</tr>
<tr>
<td>  4523646 minus 67453156 <br/></td>
<td>  –62929510 <br/></td>
<td>  –63,930,510 <br/></td>
<td>  –62,929,510 <br/></td>
</tr>
<tr>
<td>  Calculate 397 × 4429 <br/></td>
<td>  1758313 <br/></td>
<td>  1,757,413 <br/></td>
<td>  1,758,313 <br/></td>
</tr>
<tr>
<td>  What is 8914/64? <br/></td>
<td>  139 R 18 <br/></td>
<td>  139.15625 <br/></td>
<td>  139.28125 Or 139 R 18 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p238">
<p>GOAT 7B consistently outperforms GPT-4, which leaves the question, “Why does GOAT perform better despite being 200 times smaller? Aren’t larger models more likely to show emergent behavior?” You probably already guessed the answer based on the subsection’s heading, but if you didn’t, it’s because of the tokenization algorithm used!</p>
</div>
<div class="readable-text intended-text" id="p239">
<p>The GPT family of models tokenizes all subwords and digits in groups based purely on frequency only, meaning that if that exact group of numbers or words hadn’t shown up before, they could be grouped together during the embedding and inference processes later! GOAT is a finetuned Llama model, meaning that while it was finetuned on math to be good at it, the underlying secret to success lies in its tokenization strategy, which is the same as Llama’s. GPT-X tokenizes like this: </p>
</div>
<div class="browsable-container listing-container" id="p240">
<div class="code-area-container">
<pre class="code-area">print(enc.encode(“4523646 minus 67453156”))
[21098, 15951, 21, 28382, 220, 25513, 20823, 3487]</pre>
</div>
</div>
<div class="readable-text" id="p241">
<p>Did you notice how the first group of numbers is seven digits long, but the entire output is eight tokens? This is the exact grouping methodology we’re talking about. Compare that to Llama’s tokenization strategy in figure 4.3. Notice that each digit is highlighted individually, meaning that the model will eventually see all the digits. As this example demonstrates, your tokenization strategy will ultimately determine what your model will see and won’t see, as they’ll become <code>&lt;UNK&gt;</code> tokens—and that’s why it’s vitally important to get it right for your use case.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p242">
<img alt="figure" height="403" src="../Images/4-3.png" width="369"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.3</span> Llama’s tokenization of the first arithmetic problem in the comparison table. Notice that each digit is highlighted individually, meaning that the model will eventually see all the digits.</h5>
</div>
<div class="readable-text" id="p243">
<p>What started out as creating a simple set of bag-of-words conversion dictionaries has evolved immensely, and we couldn’t be happier about it. Tokenization essentially consists of two major steps: a step to split up the text and a step to turn it into numbers. The most obvious form of tokenization is splitting a string on whitespace and then converting it to a number based on a word-to-integer dictionary.</p>
</div>
<div class="readable-text intended-text" id="p244">
<p>This makes sense to most Indo-European language speakers, but we can’t recommend this because of the two assumptions presupposed: alphabets and whitespace. What will you do when you come across a language that doesn’t use an alphabet, like Chinese? And what will you do when you come across a language that doesn’t use whitespace in the same way as English, like Hungarian or Turkish? Or code, for that matter—whitespace is critical to Python’s syntax and is more than just a separator; it has semantic meanings. This is one reason why multilingual models end up outperforming monolinguals on the same tasks in almost every case: they’re forced to learn deeper representations for meaning without the bowling bumpers of easy tokenization. So let’s look at some deeper methodologies that work for UTF-8 encoded languages.</p>
</div>
<div class="readable-text intended-text" id="p245">
<p>Here are examples of all the current popular options for basing your tokenization:</p>
</div>
<ul>
<li class="readable-text" id="p246"> <em>Word-based</em>—“Johannes Gutenberg” becomes <code>['Johannes',</code> <code>'Gutenberg']</code>. </li>
<li class="readable-text" id="p247"> <em>Character-based</em> —“Shakespeare” becomes <code>['S','h','a','k','e','s','p','e', 'a','r','e']</code>. </li>
<li class="readable-text" id="p248"> <em>Subword-based</em>—“The quick red Delphox jumped over the lazy brown Emolga” becomes <code>['the','quick','red','delph','ox','jump','ed','over','the', 'laz','y','brown','emol','ga']</code> </li>
</ul>
<div class="readable-text" id="p249">
<p>Let’s take a look at each of them in turn.</p>
</div>
<div class="readable-text" id="p250">
<h4 class="readable-text-h4 sigil_not_in_toc">Word-based</h4>
</div>
<div class="readable-text" id="p251">
<p>Word-based tokenizers most commonly split on whitespace, but there are other methods like using regular expressions, dictionaries, or punctuation. For example, a punctuation-based approach would split “It’s the truth!” into <code>['It',</code> <code>'</code> <code>‘'</code> <code>,</code> <code>'</code> <code>s',</code> <code>'</code> <code>the',</code> <code>'</code> <code>truth',</code> <code>'</code> <code>!']</code>, which gives us slightly better context than splitting on whitespace alone. The <code>TreebankWordTokenizer</code> from NLTK is an example of a regular expression tokenizer. Word-based tokenizers are relatively easy to implement but require us to keep an unmanageably large dictionary mapping to encode every single possible word. That’s unreasonable, so generally, you’ll implement a dictionary cutoff and return unknown tokens when the model runs into unrecognized words to make it work. This makes the tokenizer poor at many tasks like code, name, and entity recognition, as well as generalizing across domains.</p>
</div>
<div class="readable-text" id="p252">
<h4 class="readable-text-h4 sigil_not_in_toc">Character-based</h4>
</div>
<div class="readable-text" id="p253">
<p>Character-based encoding methods are the most straightforward and easiest to implement since we split on the UTF-8 character encodings. With this method, we only need the tiniest of dictionaries to map characters to numbers, which means we can prevent the need for unknown tokens and related concerns. However, it comes with a major loss of information and fails to keep relevant syntax, semantics, or morphology of the text.</p>
</div>
<div class="readable-text" id="p254">
<h4 class="readable-text-h4 sigil_not_in_toc">Subword-based</h4>
</div>
<div class="readable-text" id="p255">
<p>Just like Goldilocks and the Three Bears, while character-based tokenizers are too hard and word-based tokenizers are too soft, subword-based tokenizers are just right. Subword-based tokenizers have proven to be the best option, being a mixture of the previous two. We are able to use a smaller dictionary like a character-based tokenizer but lose less semantics like a word-based tokenizer. It even has the added bonus of including some morphological information. However, it’s an unsolved problem for where and how words should be split, and there are many different methods and approaches. The best method to choose will be, like all other things with LLMs, dependent on the task. If you don’t have a specific goal in mind for what you are trying to do, there will be consequences later. </p>
</div>
<div class="readable-text intended-text" id="p256">
<p>Three main algorithms are used to create the subword dictionaries: BPE, WordPiece, and Unigram. In addition, SentencePiece, a combination of the three that explicitly handles whitespaces, is also very common. It’s outside the scope of this book to discuss how they work, but as a book focused on production, you should know that the most popular subword tokenization methodologies are BPE (GPT-x) and SentencePiece (LlamaX). </p>
</div>
<div class="readable-text intended-text" id="p257">
<p>In listing 4.10, we’ll go over how to train a custom version for both BPE and SentencePiece on your data so that you’re equipped to face (almost) any dataset head-on. While reading the code, pay attention to where we train the tokenizers. In particular, you’ll want to tune three key parameters: <code>vocab_size</code>, <code>min_frequency</code>, and <code>special_tokens</code>. A larger vocabulary size means your tokenizer will be more robust and will likely be better at handling more languages, but it will add computational complexity. Minimum frequency determines how often a particular subword token has to be seen in the dataset before it is added to the dictionary. Larger values prevent rare and likely unimportant tokens from filling our dictionary and prevent us from learning rare tokens that are important. Lastly, special tokens are relatively straightforward and include syntactical tokens we care about specifically for model training.</p>
</div>
<div class="browsable-container listing-container" id="p258">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.10</span> Training your own subword tokenizers </h5>
<div class="code-area-container">
<pre class="code-area">import os
from pathlib import Path

import transformers
from tokenizers import ByteLevelBPETokenizer, SentencePieceBPETokenizer
from tokenizers.processors import BertProcessing

paths = [str(x) for x in Path("./data/").glob("**/*.txt")]     <span class="aframe-location"/> #1
bpe_tokenizer = ByteLevelBPETokenizer()  <span class="aframe-location"/> #2

bpe_tokenizer.train(                      #2
    files=paths,                          #2
    vocab_size=52_000,                    #2
    min_frequency=2,                      #2
    show_progress=True,                   #2
    special_tokens=[                      #2
        "&lt;s&gt;",                            #2
        "&lt;pad&gt;",                          #2
        "&lt;/s&gt;",                           #2
        "&lt;unk&gt;",                          #2
        "&lt;mask&gt;",                         #2
    ], #2
) #2

token_dir = "./chapters/chapter_4/tokenizers/bytelevelbpe/"
if not os.path.exists(token_dir):
    os.makedirs(token_dir)
bpe_tokenizer.save_model(token_dir)

bpe_tokenizer = ByteLevelBPETokenizer(
    f"{token_dir}vocab.json",
    f"{token_dir}merges.txt",
)

example_text = "This sentence is getting encoded by a tokenizer."
print(bpe_tokenizer.encode(example_text).tokens)  
# ['This', 'Ġsentence', 'Ġis', 'Ġgetting', 'Ġenc', \
# 'oded', 'Ġby', 'Ġa', 'Ġto', 'ken', 'izer', '.']
print(bpe_tokenizer.encode(example_text).ids)
# [2666, 5651, 342, 1875, 4650, 10010, 504, 265, \ 
# 285, 1507, 13035, 18]

bpe_tokenizer._tokenizer.post_processor = BertProcessing(
    ("&lt;/s&gt;", bpe_tokenizer.token_to_id("&lt;/s&gt;")),
    ("&lt;s&gt;", bpe_tokenizer.token_to_id("&lt;s&gt;")),
)
bpe_tokenizer.enable_truncation(max_length=512)


special_tokens = [
    "&lt;s&gt;",
    "&lt;pad&gt;",
    "&lt;/s&gt;",
    "&lt;unk&gt;",
    "&lt;cls&gt;",
    "&lt;sep&gt;",
    "&lt;mask&gt;",
]
sentencepiece_tokenizer = SentencePieceBPETokenizer()   <span class="aframe-location"/> #3

sentencepiece_tokenizer.train(                           #3
    files=paths,                                         #3
    vocab_size=4000,                                     #3
    min_frequency=2,                                     #3
    show_progress=True,                                  #3
    special_tokens=special_tokens,                       #3
) #3

token_dir = "./chapters/chapter_4/tokenizers/sentencepiece/"
if not os.path.exists(token_dir):
    os.makedirs(token_dir)
sentencepiece_tokenizer.save_model(token_dir)

tokenizer = transformers.PreTrainedTokenizerFast(
    tokenizer_object=sentencepiece_tokenizer,
    model_max_length=512,
    special_tokens=special_tokens,
)                                    <span class="aframe-location"/> #4
tokenizer.bos_token = "&lt;s&gt;"
tokenizer.bos_token_id = sentencepiece_tokenizer.token_to_id("&lt;s&gt;")
tokenizer.pad_token = "&lt;pad&gt;"
tokenizer.pad_token_id = sentencepiece_tokenizer.token_to_id("&lt;pad&gt;")
tokenizer.eos_token = "&lt;/s&gt;"
tokenizer.eos_token_id = sentencepiece_tokenizer.token_to_id("&lt;/s&gt;")
tokenizer.unk_token = "&lt;unk&gt;"
tokenizer.unk_token_id = sentencepiece_tokenizer.token_to_id("&lt;unk&gt;")
tokenizer.cls_token = "&lt;cls&gt;"
tokenizer.cls_token_id = sentencepiece_tokenizer.token_to_id("&lt;cls&gt;")
tokenizer.sep_token = "&lt;sep&gt;"
tokenizer.sep_token_id = sentencepiece_tokenizer.token_to_id("&lt;sep&gt;")
tokenizer.mask_token = "&lt;mask&gt;"
tokenizer.mask_token_id = sentencepiece_tokenizer.token_to_id("&lt;mask&gt;")
tokenizer.save_pretrained(token_dir)  <span class="aframe-location"/> #5

print(tokenizer.tokenize(example_text))
# ['_This', '_s', 'ent', 'ence', '_is', '_', 'g', 'et', 'tin', 'g', '_'
# 'en', 'co', 'd', 'ed', '_', 'b', 'y', '_a', '_', 't', 'ok', 'en', 
# 'iz', 'er', '.']

print(tokenizer.encode(example_text))
# [814, 1640, 609, 203, 1810, 623, 70, \
# 351, 148, 371, 125, 146, 2402, 959, 632]</pre>
<div class="code-annotations-overlay-container">
     #1 Initializes the texts to train from
     <br/>#2 Trains a byte-pair encoding tokenizer
     <br/>#3 Trains a SentencePiece tokenizer
     <br/>#4 Converts
     <br/>#5 And saves for later!
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p259">
<p>Out of the two, BPE and SentencePiece, we find ourselves using both about equally. It mostly depends on which model we’re finetuning or using as a base for a particular project. Algorithmically, we’re partial to SentencePiece because it tends to boost evaluation scores on pretty much any test for models trained on it, and it’s also closer to how we interact with morphology as people. </p>
</div>
<div class="readable-text intended-text" id="p260">
<p>All in all, tokenization loses information, just as converting from speech to text does—namely, word order (syntax) and meaning (semantics). All of the information about what a number is and how it would differ from a letter is completely gone after tokenization. To circumvent potential semantic and syntactic problems, we need to create an approximation for each of these features and figure out how to mathematically represent them in abstraction to insert that meaning back into the tokenized vector. For this, we have embeddings.</p>
</div>
<div class="readable-text" id="p261">
<h3 class="readable-text-h3" id="sigil_toc_id_77"><span class="num-string">4.4.2</span> Embeddings</h3>
</div>
<div class="readable-text" id="p262">
<p>Embeddings provide meaning to the vectors generated during tokenization. Tokenized text is just numbers assigned almost arbitrarily (occurrence-based) to a dictionary, but it’s at least in a format that the model can ingest. Embeddings are the next step, where positional and semantic encodings are created and looked up to give the model additional context for making decisions about how to (probably) complete the task it’s given. </p>
</div>
<div class="readable-text intended-text" id="p263">
<p>Embeddings are imperfect for several reasons, but perhaps the most relevant is this theoretical question: Can you represent a set using only a subset of that same set? In this case, the first set is language, one or more, and the second set is numbers, floats, and digits. Math is a subset of language used to describe things axiomatically that we accept as true. Take the English alphabet, for example: Can you represent the entire alphabet by only using some fraction of the 26 letters? Obviously not, but what if both the original set and the subset are infinite? Can you represent all digits using only the decimals between 0 and 1? Given that the first is a numerable infinite set and the second is a nonenumerable infinite set, the answer is yes, which should be enheartening for the field of language modeling.</p>
</div>
<div class="readable-text intended-text" id="p264">
<p>Now that we’ve talked about why embeddings shouldn’t be completely and blindly relied on, embeddings are what most businesses are looking for with LLMs. You don’t need a 1.7T-parameter model to handle customers asking questions about your pricing or performing a search through your documents. As we discussed in chapter 2, embeddings have the innate advantage of being comparable by distance, provided both embeddings you’re comparing were created by the same model in the same dimensional space. That opens up the door for all sorts of speedy computation and retrieval where you never have to figure out how to host a gigantic model somewhere because you can run a smaller embedding model on a CPU, and it takes milliseconds for hundreds of tokens. </p>
</div>
<div class="readable-text intended-text" id="p265">
<p>One of the most popular and coolest applications of embeddings at the moment is retrieval-augmented generation (RAG), where you store data that is pertinent to the overall task of the model and give portions of that data as needed to a larger model at prompting time to improve results. Suppose we apply RAG to the Boston Housing dataset and attempt to predict the value of a new house. In that case, we can compare that house’s embedded data to the closest comparable houses in the area and generate an informed appraisal without ever needing an appraiser to verify, as long as the embeddings you’re retrieving from are up-to-date.</p>
</div>
<div class="readable-text intended-text" id="p266">
<p>Embeddings can be used for dozens of different tasks and are the result of taking final hidden state representations from your model. Every layer of your model is a potential option, but the general consensus is to take representations after the final layer before any decoding or final linear layers or softmaxes. Listing 4.11 gives a practical example of how to extract the embeddings from both PyTorch and Hugging Face models. Best practice dictates that you should extract the embeddings from documents using whatever embedding model you are planning to use for inference, especially if those embeddings will end up being stored in a VectorDB later on. After creating our embeddings, we show how to do a simple similarity search on the results, which is the basis of RAG systems.</p>
</div>
<div class="browsable-container listing-container" id="p267">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.11</span> Example embeddings</h5>
<div class="code-area-container">
<pre class="code-area">import numpy as np
from sentence_transformers import SentenceTransformer
from datasets import load_dataset

model_ckpt = "sentence-transformers/all-MiniLM-L6-v2"    <span class="aframe-location"/> #1
model = SentenceTransformer(model_ckpt)
embs_train = load_dataset("tweet_eval", "emoji", split="train[:1000]")
embs_test = load_dataset("tweet_eval", "emoji", split="test[:100]")


def embed_text(example):                   <span class="aframe-location"/> #2
    embedding = model.encode(example["text"])
    return {"embedding": np.array(embedding, dtype=np.float32)}


print(f"Train 1: {embs_train[0]}")
embs_train = embs_train.map(embed_text, batched=False)
embs_test = embs_test.map(embed_text, batched=False)

embs_train.add_faiss_index("embedding")                <span class="aframe-location"/> #3

# 
idx, knn = 1, 3  # Select the first query and 3 nearest neighbors   <span class="aframe-location"/> #4

query = np.array(embs_test[idx]["embedding"], dtype=np.float32)
scores, samples = embs_train.get_nearest_examples("embedding", query, k=knn)

print(f"QUERY LABEL: {embs_test[idx]['label']}")               <span class="aframe-location"/> #5
print(f"QUERY TEXT: {embs_test[idx]['text'][:200]} [...]\n")
print("=" * 50)
print("Retrieved Documents:")
for score, label, text in zip(scores, samples["label"], samples["text"]):
    print("=" * 50)
    print(f"TEXT:\n{text[:200]} [...]")
    print(f"SCORE: {score:.2f}")
    print(f"LABEL: {label}")</pre>
<div class="code-annotations-overlay-container">
     #1 Δownloads embedding model and dataset
     <br/>#2 Creates embeddings
     <br/>#3 Adds Faiss index that allows similarity search
     <br/>#4 Runs query
     <br/>#5 Prints results
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p268">
<p>Extracting embeddings, like the listing shows, is pretty simple and differs very little from simply running inference or training on a dataset. Remember, if you aren’t using sentence transformers, set your model to <code>eval</code> mode, run with <code>torch.no_grad()</code>, and if you’re running on torch 2.0+, run <code>torch.compile(model)</code>. Things should speed up and become more computationally efficient immediately. </p>
</div>
<div class="readable-text intended-text" id="p269">
<p>Another as-of-yet unsolved problem is how to compare embedding spaces. Mathematically sound comparisons have popped up time and again over the years, but as has been demonstrated, mathematical soundness isn’t the first problem to be solved; the modality is. In addition, pairwise comparison functions have mathematical limits on how fast it is possible to run them. If you’re comparing language embeddings, a mathematically sound conversion of a linguistically sound comparison method is the solution, and a linguistically sound comparison is dependent upon the goal of the comparison. It’s too much to go into here, but we dive more deeply into this topic in appendix C, where we discuss diffusion and multimodal LLMs.</p>
</div>
<div class="readable-text" id="p270">
<h2 class="readable-text-h2" id="sigil_toc_id_78"><span class="num-string">4.5</span> Preparing a Slack dataset</h2>
</div>
<div class="readable-text" id="p271">
<p>Now that we have learned the ins and outs of preparing the necessary assets to train our own LLM, we wanted to end this chapter by preparing a dataset that we can use later. For this exercise, we will tackle a very common problem in the industry. I’m sure most readers have experienced or witnessed an HR help channel constantly inundated with the same questions over and over. It doesn’t matter how many FAQ pages are created; users don’t want to waste their time searching for documentation when they could ask an expert. So let’s build a chatbot to answer these questions! </p>
</div>
<div class="readable-text intended-text" id="p272">
<p>We will show you how to pull your company’s Slack data and prepare it for training an LLM-based chatbot. In listing 4.12, we pull Slack data, filter it to keep just the user’s data, and save it to a parquet file. This way, you can create a bot that will talk like you, but feel free to edit it. For example, you might enjoy creating a bot that talks like your boss, but I’d recommend not telling them in case they feel threatened knowing you are automating them out of a job.</p>
</div>
<div class="browsable-container listing-container" id="p273">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.12</span> Example of pulling Slack data</h5>
<div class="code-area-container">
<pre class="code-area">import slack_sdk
import pandas

token_slack = "Your Token Here"
client = slack_sdk.WebClient(token=token_slack)

auth = client.auth_test()
self_user = auth["user_id"]

dm_channels_response = client.conversations_list(types="im")

all_messages = {}

for channel in dm_channels_response["channels"]:
    history_response = client.conversations_history(channel=channel["id"])
    all_messages[channel["id"]] = history_response["messages"]

txts = []

for channel_id, messages in all_messages.items():
    for message in messages:
        try:
            text = message["text"]
            user = message["user"]
            timestamp = message["ts"]
            txts.append([timestamp, user, text])
        except Exception:
            pass

slack_dataset = pandas.DataFrame(txts)
slack_dataset.columns = ["timestamp", "user", "text"]
df = slack_dataset[slack_dataset.user == self_user]

df[["text"]].to_parquet("slack_dataset.gzip", compression="gzip")</pre>
</div>
</div>
<div class="readable-text" id="p274">
<p>As you can see, there’s not much to it! We have an example dataset we pulled using this script in the GitHub repo accompanying this book. We will use this dataset in the coming chapters.</p>
</div>
<div class="readable-text intended-text" id="p275">
<p>We’ve gone over a lot in this chapter, but you should now be prepared and know how to select and evaluate a foundation model, prepare and clean a dataset, and optimize your own text processors. We will use this information in the next chapter to train and finetune our own LLM model.</p>
</div>
<div class="readable-text" id="p276">
<h2 class="readable-text-h2" id="sigil_toc_id_79">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p277"> Data engineers have unique datasets to acquire and manage LLMs, like model weights, evaluation datasets, and embeddings. </li>
<li class="readable-text" id="p278"> No matter your task, there is a wide array of open source models to choose from to finetune your own model. </li>
<li class="readable-text" id="p279"> Text-based tasks are harder to evaluate than simple equality metrics you’d find in traditional ML tasks, but there are many industry benchmarks to help you get started. </li>
<li class="readable-text" id="p280"> Evaluating LLMs for more than just performance, like bias and potential harm, is your responsibility. </li>
<li class="readable-text" id="p281"> You can use the Evaluate library to build your own evaluation metrics. </li>
<li class="readable-text" id="p282"> There are many large open source datasets, but most come from scraping the web and require cleaning. </li>
<li class="readable-text" id="p283"> Instruct schemas and annotating your data can be effective ways to clean and analyze your data. </li>
<li class="readable-text" id="p284"> Finetuning a model on a dataset with an appropriate distribution of speech acts for the task you want your model to perform will help it generate context-appropriate content. </li>
<li class="readable-text" id="p285"> Building your own subword tokenizer to match your data can greatly improve your model’s performance. </li>
<li class="readable-text" id="p286"> Many problems teams are trying to use LLMs for can be solved by using embeddings from your model instead. </li>
</ul>
<div class="readable-text footnote-readable-text" id="p287">
<p><a href="#footnote-source-1"><span class="footnote-definition" id="footnote-151">[1]</span></a> Joe Reis and Matt Housley, Fundamentals of Data Engineering, O’Reilly, 2022.</p>
</div>
<div class="readable-text footnote-readable-text" id="p288">
<p><a href="#footnote-source-2"><span class="footnote-definition" id="footnote-152">[2]</span></a> C. Loizos, “‘GPT’ may be trademarked soon if OpenAI has its way,” TechCrunch, April 25, 2023, <a href="https://mng.bz/5Omq">https://mng.bz/5Omq</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p289">
<p><a href="#footnote-source-3"><span class="footnote-definition" id="footnote-153">[3]</span></a> N. Muennighoff et al., “Cross lingual generalization through multitask finetuning,” November 3, 2022, <a href="https://arxiv.org/abs/2211.01786">https://arxiv.org/abs/2211.01786</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p290">
<p><a href="#footnote-source-4"><span class="footnote-definition" id="footnote-154">[4]</span></a> C. Xu et al., “WizardLM: Empowering large language models to follow complex instructions,” Jun. 10, 2023, <a href="https://arxiv.org/abs/2304.12244">https://arxiv.org/abs/2304.12244</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p291">
<p><a href="#footnote-source-5"><span class="footnote-definition" id="footnote-155">[5]</span></a> Mike Conover et al., “Free Dolly: Introducing the world’s first truly open instruction-tuned LLM,” Databricks, April 12, 2023, <a href="https://mng.bz/n0e8">https://mng.bz/n0e8</a>. </p>
</div>
<div class="readable-text footnote-readable-text" id="p292">
<p><a href="#footnote-source-6"><span class="footnote-definition" id="footnote-156">[6]</span></a> D. Hendrycks et al., “Measuring massive multitask language understanding,” arXiv (Cornell University), September 2020, <a href="https://doi.org/10.48550/arxiv.2009.03300">https://doi.org/10.48550/arxiv.2009.03300</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p293">
<p><a href="#footnote-source-7"><span class="footnote-definition" id="footnote-157">[7]</span></a> P. Rajpurkar, R. Jia, and P. Liang, “Know what you don’t know: Unanswerable questions for SQuAD,” June 2018, <a href="https://arxiv.org/abs/1806.03822">https://arxiv.org/abs/1806.03822</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p294">
<p><a href="#footnote-source-8"><span class="footnote-definition" id="footnote-158">[8]</span></a> C. Zhou et al., “LIMA: Less is more for alignment,” arXiv.org, May 18, 2023, <a href="https://arxiv.org/abs/2305.11206">https://arxiv.org/abs/2305.11206</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p295">
<p><a href="#footnote-source-9"><span class="footnote-definition" id="footnote-159">[9]</span></a> T. Dettmers et al., “SpQR: A sparse-quantized representation for near-lossless LLM weight compression,” arXiv.org, June 5, 2023, <a href="https://arxiv.org/abs/2306.03078">https://arxiv.org/abs/2306.03078</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p296">
<p><a href="#footnote-source-10"><span class="footnote-definition" id="footnote-160">[10]</span></a> L. Gao et al., “The Pile: An 800GB Dataset of Diverse Text for Language Modeling,” Dec. 2020, <a href="https://arxiv.org/abs/2101.00027">https://arxiv.org/abs/2101.00027</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p297">
<p><a href="#footnote-source-11"><span class="footnote-definition" id="footnote-161">[11]</span></a> Reis and Housley, Fundamentals of Data Engineering, 2022.</p>
</div>
<div class="readable-text footnote-readable-text" id="p298">
<p><a href="#footnote-source-12"><span class="footnote-definition" id="footnote-162">[12]</span></a> Xu et al., “WizardLM,” 2023.</p>
</div>
<div class="readable-text footnote-readable-text" id="p299">
<p><a href="#footnote-source-13"><span class="footnote-definition" id="footnote-163">[13]</span></a> Zou et al., “LIMA,” 2023.</p>
</div>
</div></body></html>