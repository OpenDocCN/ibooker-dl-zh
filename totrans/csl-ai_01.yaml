- en: 1 Why causal AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Defining causal AI and its benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating causality into machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple example of applying causality to a machine learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subscription streaming platforms like Netflix are always looking for ways to
    optimize various indicators of performance. One of these is their *churn rate*,
    meaning the rate at which they lose subscribers. Imagine that you are a machine
    learning engineer or data scientist at Netflix tasked with finding ways of reducing
    churn. What are the types of *causal questions* (questions that require causal
    thinking) you might ask with respect to this task?
  prefs: []
  type: TYPE_NORMAL
- en: '*Causal discovery*—Given detailed data on who churned and who did not, can
    you analyze that data to find causes of the churn? *Causal discovery* investigates
    what causes what.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Estimating average treatment effects* (ATEs)—Suppose the algorithm that recommends
    content to the user is a cause of the churn; a better choice of algorithm might
    reduce churn, but by how much? The task of quantifying how much, on average, a
    cause drives an effect is the *ATE estimation*. For example, some users could
    be exposed to a new version of the algorithm, and you could measure how much this
    affects churn, relative to the baseline algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go a bit deeper. The mockumentary *The Office* (the American version)
    was one of the most popular shows on Netflix. Later, Netflix learned that NBCUniversal
    was planning to stop licensing the show to Netflix to stream in the US, so that
    US streaming of The Office would be exclusive to NBCUniversal’s rival streaming
    platform, Peacock. Given the popularity of the show, churn was certainly affected,
    but by how much?
  prefs: []
  type: TYPE_NORMAL
- en: '*Estimating conditional average treatment effects* (CATEs) —The effect of losing
    *The Office* would be more pronounced for some subscriber segments than others,
    but what attributes define these segments? One attribute is certainly having watched
    the show, but there are others (demographics, other content watched, etc.). *CATE
    estimation* is the task of quantifying how much a cause drives an effect for a
    particular segment of the population. Indeed, there are likely multiple segments
    we could define, each with a different within-segment ATE. Part of the task of
    CATE estimation is finding distinct segments of interest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose you had reliable data on subscribers who quit Netflix and signed up
    for Peacock to continue watching *The Office*. For some of these users, the recommendation
    algorithm failed to show them possible substitutes for *The Office*, like the
    mockumentary *Parks and Recreation*. That may lead to a different type of question.
  prefs: []
  type: TYPE_NORMAL
- en: '*Counterfactual reasoning and attribution*—If the algorithm had placed *Parks
    and Recreation* more prominently in those users’ dashboards, would they have stayed
    on with Netflix? These *counterfactual questions* (“counter” to the “fact” that
    the show wasn’t prominent in their dashboard) are essential for *attribution*
    (assigning a root cause and credit/blame for an outcome).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Netflix worked with Steve Carrel (star of *The Office*) and Greg Daniels (writer,
    director, and producer of *The Office*) to create the show *Space Force* as Netflix
    original content. The show was released just months before *The Office* moved
    to Peacock. Suppose that this show was Netflix’s attempt to create content to
    retain subscribers who were fans of *The Office*. Consider the decisions that
    would go into the creation of such a show:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Causal decision theory*—What actors/directors/writers would tempt *The Office*
    fans to stay subscribed? What themes and content?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Causal machine learning*—How could we use generative AI, such as large language
    models to create scripts and pilots for the show in such a way that optimizes
    for the objective of reducing churn amongst fans of *The Office*?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Causal* *inference* is about breaking down a problem into these types of specific
    *causal queries*, and then using data to answer these queries. *Causal AI* is
    about building algorithms that automate this analysis. We’ll tackle both of these
    problem areas in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 What is causal AI?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand what causal AI is, we’ll start with the basic ideas of causality
    and causal inference, and work our way up. Then we’ll review the kinds of problems
    we can solve with causal AI.
  prefs: []
  type: TYPE_NORMAL
- en: '*Causal reasoning* is a crucial element of how humans understand, explain,
    and make decisions about the world. Anytime we think about cause (“Why did that
    happen?”) or effect (“What will happen if I do this?”), we are practicing causal
    reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: In statistics and machine learning, we use data to lend statistical rigor to
    our causal reasoning. But while cause-and-effect relationships drive the data,
    statistical correlation alone is insufficient to draw causal conclusions from
    data. For this, we must turn to *causal inference*.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical (non-causal) inference relies on statistical assumptions. This is
    true even in deep learning, where assumptions are often called “inductive bias.”
    Similarly, causal inference relies on causal assumptions; causal inference refers
    to a body of theory and practical methods that constrain statistical analysis
    with causal assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Causal AI* refers to the automation of causal inference. We can leverage machine
    learning algorithms, which have developed robust approaches to automating statistical
    analyses and scale up to large amounts of data of different modalities.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of AI is automating reasoning tasks that until now have required human
    intelligence to solve. Humans rely heavily on causal reasoning to navigate the
    world, and while we are better at causal reasoning than statistical reasoning,
    our cognitive biases still make our causal reasoning highly error prone. Improving
    our ability to answer causal questions has been the work of millennia of philosophers,
    centuries of scientists, and decades of statisticians. But now, a convergence
    of statistical and computational advances has shifted the focus from discourse
    to algorithms that we can train on data and deploy to software. It is a fascinating
    time to learn how to build causal AI.
  prefs: []
  type: TYPE_NORMAL
- en: Key definitions underpinning causal AI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Inference*—Drawing conclusions from observations and data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Assumptions*—Constraints that guide inferences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inductive biases*—Another word for *assumptions*, often used to refer to assumptions
    implicit in the choice of machine learning algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Statistical model*—A framework using statistical assumptions to analyze data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data science*—An interdisciplinary field that uses statistical models along
    with other algorithms and techniques to extract insights and knowledge from structured
    and unstructured data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Causal inference*—Techniques that use causal assumptions to guide conclusions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Causal model*—A statistical model built on causal assumptions about data generation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Causal data science*—Data science that employs causal models to extract causal
    insights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Causal AI*—Algorithms that automate causal inference tasks using causal models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.2 How this book approaches causal inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal of this book is the fusion of two powerful domains: causality and
    AI. By the end of this journey, you’ll be equipped with the skills to'
  prefs: []
  type: TYPE_NORMAL
- en: '*Design AI systems with causal capabilities*—Harness the power of AI, but with
    an added layer of causal reasoning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use machine learning frameworks for causal inference*—Utilize tools like PyTorch
    and other Python libraries to seamlessly integrate causal modeling into your projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Build tools for automated causal decision-making*—Implement causal decision-making
    algorithms, including causal reinforcement learning algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Historically, causality and AI evolved from different bodies of research, they
    have been applied to different problems, and they have led to experts with different
    skill sets, books that use different languages, and libraries with different abstractions.
    This book is for anyone who wants to connect these domains into one comprehensive
    skill set.
  prefs: []
  type: TYPE_NORMAL
- en: There are many books on causal inference, including books that focus on causal
    inference in Python. The following subsections discuss some features that make
    this book unique.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 Emphasis on AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This book focuses on causal AI. We’ll cover not just the relevance of causal
    inference to AI, or how machine learning can scale up causal inference, but also
    focus on implementation. Specifically, we’ll integrate causal models with conventional
    models and training procedures from probabilistic machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 Focus on tech, retail, and business
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Practical causal inference methods have developed from econometrics, public
    health, social sciences, and other domains where it is difficult to run randomized
    experiments. As a result, examples in most books tend to come from those domains.
    In contrast, this book leans heavily into examples from tech, retail, and business.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3 Parallel world counterfactuals and other queries beyond causal effects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When many think of “causal inference,” they think of estimating causal effects,
    namely average treatment effects (ATEs) and conditional average treatment effects
    (CATEs). These are certainly important queries, but there are other kinds of causal
    queries as well. This book gives due attention to these other types.
  prefs: []
  type: TYPE_NORMAL
- en: For example, this book provides in-depth coverage of the *parallel worlds* account
    of counterfactuals. In this approach, when some cause and some effect occur, we
    imagine a parallel universe where the causal event was different. For example,
    suppose you asked, “I married for money and now I’m sad. Would I have been happier
    had I married for love?” With our parallel worlds approach, you’d use your experience
    of marrying for money and being sad as inputs to a causal model-based probabilistic
    simulation of your happiness in a parallel universe where you married for love.
    This type of reasoning is useful in decision-making. For example, it might help
    you choose a better spouse next time.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully this example of love and regret illustrates how fundamental this kind
    of “what could have been” thinking is to human cognition (we’ll see more applied
    examples in chapters 8 and 9). It therefore makes sense to learn how to build
    AI with the same capabilities. But although they’re useful, some counterfactual
    inferences are hard or impossible to verify (you *can’t* prove you would have
    been happier if you had married for love). Most causal inference books only focus
    on the narrow set of counterfactuals we can verify with data and experiments,
    which misses many interesting, cognitive science-aligned, and practical use cases
    of counterfactual reasoning. This book leans into those use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.4 An assumption of commodification of inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many causal inference books go deep into the statistical inference nuts and
    bolts of various causal effect estimators. But a major trend in the last decade
    of developing deep learning frameworks is the *commodification of inference*.
    This refers to how libraries like PyTorch abstract away the difficult aspects
    of estimation and inference—if you can define your estimation/inference problem
    in terms minimizing a differentiable loss function, PyTorch will handle the rest.
    The commodification of inference frees up the user to focus on creating ever more
    nuanced and powerful models, such as models that represent the causal structure
    of the data-generating process.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we’ll focus on leveraging frameworks for inference so that you
    can learn a universal view of modeling techniques. Once you find the right modeling
    approach for your domain, you can use other resources to go deep into any statistical
    algorithm of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.5 Breaking down theory with code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the standout features of this book is its approach to advanced topics
    in causal inference theory. Many introductory texts shy away from subjects like
    identification, the do-calculus, and the causal hierarchy theorem because they
    are difficult. The problem is that if you want to create causal-capable AI algorithms,
    you need an intuition for these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we’ll make these topics accessible by relying on Python libraries
    that implement their basic abstractions and algorithms. We’ll build intuition
    for these advanced topics by working with these primitives in code.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Causality’s role in modern AI workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is great value in positioning ourselves to build future versions of AI
    with causal capabilities, but the topics covered in this book will also have an
    impact on applications common today. In this section, we’ll review how causality
    can enhance some of these applications.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 Better data science
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Big tech and tech-powered retail organizations have recognized the significance
    of causal inference, offering premium salaries to those proficient in it. This
    is because the essence of data science—deriving actionable insights from data—is
    inherently causal.
  prefs: []
  type: TYPE_NORMAL
- en: When a data scientist examines the correlation between a feature on an e-commerce
    site and sales, they do so because they want to know whether the feature causally
    drives sales. Causal inference can help answer this question in several ways.
    First, it can help them design an experiment that will quantify the causal effect
    of the feature on sales, especially in the case where a perfect randomized experiment
    is not possible. Second, if a proposed experiment is not feasible, the data scientist
    can use past observational data and data from related but different past experiments
    to infer the value of the causal effect that would result from the proposed experiment
    without actually running it. Finally, even if the data scientist has complete
    freedom in running experiments, causal inference can help select which experiment
    to run and what variables to measure, minimizing the opportunity cost of running
    wasteful or uninformative experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2 Better attribution, credit assignment, and root cause analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Causal inference also supports attribution. The “attribution problem” in marketing
    is perhaps best articulated by a quote credited to advertising pioneer John Wanamaker:'
  prefs: []
  type: TYPE_NORMAL
- en: Half the money I spend on advertising is wasted; the trouble is I don’t know
    which half.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, it is difficult to know what advertisement, promotion, or other
    action *caused* a specific customer behavior, sales number, or other key business
    outcome. Even in online marketing, where the data has gotten much richer and more
    granular than in Wanamaker’s time, attribution remains a challenge. For example,
    a user may have clicked after seeing an ad, but was it that single ad view that
    led to the click? Or were they going to click anyway? Perhaps there was a cumulative
    effect of all the nudges to click that they received over multiple channels. Causal
    modeling addresses the attribution problem by using formal causal logic to answer
    “why” questions, such as “why did this user click?”
  prefs: []
  type: TYPE_NORMAL
- en: Attribution goes by other names in other domains, such as “credit assignment”
    and “root cause analysis.” The core meaning is the same; we want to understand
    why a particular event outcome happened. We know what the causes are in general,
    but we want to know how much a particular cause is to blame in a given instance.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.3 More robust, decomposable, and explainable models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For organizations that use machine learning to build software, incorporating
    causal modelling can improve both the process and the product. In particular,
    causality adds value by making machine learning more robust, decomposable, and
    explainable.
  prefs: []
  type: TYPE_NORMAL
- en: More robust machine learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Machine learning models lack robustness when differences between the environment
    where the model was trained and the environment where the model is deployed cause
    the model to break down. Causality can address the lack of robustness in the following
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Overfitting*—Overfitting occurs when learning algorithms place too much weight
    on spurious statistical patterns in the training data. Causal approaches can orient
    machine learning models toward learning statistical patterns that are rooted in
    causal relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Underspecification*—Underspecification occurs when there are many equivalent
    configurations of a model that perform equivalently on test data but perform differently
    in the deployment environment. One sign of underspecification is sensitivity to
    arbitrary elements of the model’s configuration, such as a random seed. Causal
    inference can tell you when a causal prediction is “identified” (i.e., not “underspecified”),
    meaning a unique answer exists given the assumptions and the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data drift*—As time passes, the characteristics of the data in the environment
    where you deploy the model differ or “drift” from the characteristics of the training
    data. Causal modeling addresses this by capturing causal invariance underlying
    the data. For example, suppose you train a model that uses elevation to predict
    average temperature. If you train with data only from high-elevation cities, it
    should still work well in low-elevation cities if the model successfully fit the
    underlying physics-based causal relationship between altitude and temperature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is why leading tech companies deploy causal machine learning techniques—they
    can make their machine learning services more robust. It is also why notable deep
    learning researchers are pursuing research that combines deep learning with causal
    reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: More decomposable machine learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Causal models decompose into components, specifically tuples of effects and
    their direct causes, which I’ll define formally in chapter 3\. To illustrate,
    let’s consider a simple machine learning problem of predicting whether an individual
    who sees a digital ad will go on to make a purchase.
  prefs: []
  type: TYPE_NORMAL
- en: We could use various characteristics of the ad impression (e.g., the number
    of times the ad was seen, the duration of the view, the ad category, the time
    of day, etc.) as the feature vector, and predict the purchase using a neural network,
    as depicted in figure 1.1\. The weights in the hidden layers of the model are
    mutually dependent, so the model cannot be reduced to smaller independent components.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F01_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 A simple multilayer perceptron neural network that uses features
    associated with ad impressions to predict whether a purchase will result
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'On the other hand, if we take a causal view of the problem, we might reason
    that an ad impression drives engagement, and that the engagement drives whether
    an individual makes a purchase. Using engagement metrics as another feature vector,
    we could instead train the model shown in figure 1.2\. This model aligns with
    the causal structure of the domain (i.e., ad impressions causing engagement, and
    engagement causing purchases). As such, it decomposes into two components: {ad
    impression, engagement} and {engagement, purchase}.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F02_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 A model that captures how ad impressions drive engagement, which
    in turn drives purchases. This model decomposes into {ad impression, engagement}
    and {engagement, purchase}.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are several benefits of this decomposability:'
  prefs: []
  type: TYPE_NORMAL
- en: Components of the model can be tested and validated independently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of the model can be executed separately, enabling more efficient
    use of modern cloud computing infrastructure and enabling edge computing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When additional training data is available, only the components relevant to
    the data need retraining.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of old models can be reused in new models targeting new problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is less sensitivity to suboptimal model configuration and hyperparameter
    settings, because components can be optimized separately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The components of the causal model correspond to concepts in the domain that
    you are modeling. This leads to the next benefit, explainability.
  prefs: []
  type: TYPE_NORMAL
- en: More explainable machine learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many machine learning algorithms, particularly deep learning algorithms, can
    be quite “black box,” meaning the internal workings are not easily interpretable,
    and the process by which the model produces an output for a given input is not
    easily explainable.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, causal models are eminently explainable because they directly encode
    easy-to-understand causal relationships in the modeling domain. Indeed, causality
    is the core of explanation; explaining an event means describing the event’s causes
    and how they led to the event occurring. Causal models provide explanations in
    the language of the domain you are modeling (semantic explanations) rather than
    in terms of the model’s architecture (such as syntactic explanations of “nodes”
    and “activations”).
  prefs: []
  type: TYPE_NORMAL
- en: Consider the examples in figures 1.1 and 1.2\. In figure 1.1, only the input
    features and output are interpretable in terms of the domain; the internal workings
    of the hidden layers are not. Thus, given a particular ad impression, it is difficult
    to explain how the model arrives at a particular purchase outcome. In contrast,
    the example in figure 1.2 explicitly provides engagement to explain how we get
    from an ad impression to a purchase outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The connections between engagement and ad impression, and between purchase and
    engagement, are still black boxes, but if we need to, we can make additional variables
    in those black boxes explicit. We just need to make sure we do so in a way that
    is aligned with our assumptions about the causal structure of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.4 Fairer AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose Bob applies for a business loan. A machine learning algorithm predicts
    that Bob would be a bad loan candidate, so Bob is rejected. Bob is a man, and
    he got ahold of the bank’s loan data, which shows that men are less likely to
    have their loan applications approved. Was this an “unfair” outcome?
  prefs: []
  type: TYPE_NORMAL
- en: 'We might say the outcome is “unfair” if, for example, the algorithm made that
    prediction *because* Bob is a man. To be a “fair” prediction, it would need to
    be formulated from factors relevant to Bob’s ability to pay back the loan, such
    as his credit history, his line of business, or his available collateral. Bob’s
    dilemma is another example of why we’d like machine learning to be explainable:
    so that we can analyze what factors in Bob’s application led to the algorithm’s
    decision.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the training data came from a history of decisions from loan officers,
    some of whom harbored a gender prejudice that hurt men. For example, they might
    have read studies that show men are more likely to default in times of financial
    difficulty. Based on those studies, they decided to deduct points from their rating
    if the applicant was a man.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, suppose that when the data was collected, the bank advertised the
    loan program on social media. When we look at the campaign results, we notice
    that the men who responded to the ad were, on average, less qualified than the
    women who clicked on the ad. This discrepancy might have been because the campaign
    was better targeted toward women, or because the average bid price in online ad
    auctions was lower when the ad audience was composed of less-qualified men. Figure
    1.3 plots various factors that might influence the loan approval process, and
    it distinguishes fair from unfair causes. The factors are plotted in a directed
    acyclic graph (DAG), a popular and effective way to represent causal relationships.
    We’ll use DAGs as our workhorse for causal reasoning throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F03_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 A causal directed acyclic graph (DAG) showing how statistical bias
    against a particular gender could come from an algorithm directly penalizing that
    gender (unfair) and indirectly through gender discrepancies in applicants targeted
    by digital advertising algorithms (fair). Causal inference can parse the bias
    into fair and unfair sources.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Thus, we have two possible sources of statistical bias against men in the data.
    One source of bias is from the online ad that attracted men who were, on average,
    less qualified, leading to a higher rejection rate for men. The other source of
    statistical bias comes from the prejudice of loan officers. One of these sources
    of bias is arguably “fair” (it’s hard to blame the bank for the targeting behavior
    of digital advertising algorithms), and one of the sources is “unfair” (we *can*
    blame the bank for sexist loan policies). But when we only look at the training
    data without this causal context, all we see is statistical bias against men.
    The learning algorithm reproduced this bias when it made its decision about Bob.
  prefs: []
  type: TYPE_NORMAL
- en: One naive solution to this problem is simply to remove gender labels from the
    training data. But even if those sexist loan officers didn’t see an explicit indication
    of the person’s gender, they could infer it from elements of the application,
    such as the person’s name. Those loan officers encode their prejudicial views
    in the form of a statistical correlation between those proxy variables for gender
    and loan outcome. The machine learning algorithm would discover this statistical
    pattern and use it to make predictions. As a result, you could have a situation
    where the algorithm produces two different predictions for two individuals who
    had the same repayment risk but differed in gender, even if gender wasn’t a direct
    input to the prediction. Deploying this algorithm would effectively scale up the
    harm caused by those loan officers’ prejudicial views.
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, we can see how many fears about the widespread deployment
    of machine learning algorithms are justified. Without corrections, these algorithms
    could adversely impact our society by magnifying the unfair outcomes captured
    in the data that our society produces.
  prefs: []
  type: TYPE_NORMAL
- en: Causal analysis is instrumental in parsing these kinds of algorithmic fairness
    issues. In this example, we could use causal analysis to parse the statistical
    bias into “unfair” bias due to sexism and bias due to external factors like how
    the digital advertising service targets ads. Ultimately, we could use causal modeling
    to build a model that only considers variables *causally relevant* to whether
    an individual can repay a loan.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that causal inference alone is insufficient to solve
    algorithmic fairness. Causal inference can help parse statistical bias into what
    is fair and what’s not. And yet, even that depends on all parties involved agreeing
    on definitions of concepts and outcomes, which is often a tall order. To illustrate,
    suppose that the social media ad campaign served the loan ad to more men because
    the cost of serving an ad to men is cheaper. Thus, an ad campaign can win the
    online ad spot auctions with lower bids when the impression is coming from a man,
    and, as a result, more men see the ad, though many of these men are not good matches
    for the loan program. Was this process unfair? Is the result unfair? What is the
    fairness tradeoff between balanced outcomes across genders and pricing fairness
    to advertisers? Should some advertisers have to pay more due to pricing mechanisms
    designed to encourage balanced outcomes? Causal analysis can’t solve these questions,
    but it can help understand them in technical detail.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 How causality is driving the next AI wave
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incorporating causal logic into machine learning is leading to new advances
    in AI. Three trending areas of AI highlighted in this book are representation
    learning, reinforcement learning, and large language models. These trends in causal
    AI are reminiscent of the early days of deep learning. People already working
    with neural networks when the deep learning wave was gaining momentum enjoyed
    first dibs on new opportunities in this space, and access to opportunities begets
    access to more opportunities. The next wave of AI is still taking shape, but it
    is clear it will fundamentally incorporate some representation of causality. The
    goal of this book is to help you ride that wave.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.1 Causal representation learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many state-of-the-art deep learning methods attempt to learn geometric representations
    of the objects being modeled. However, these methods struggle with learning causally
    meaningful representations. For example, consider a video of a child holding a
    helium-filled balloon on a string. Suppose we had a corresponding vector representation
    of that image. If the vector representation were causally meaningful, then manipulating
    the vector to remove the child and converting the manipulated vector to a new
    video would result in a depiction of the balloon rising upwards. Causal representation
    learning is a promising area of deep representation learning that’s still in its
    early stages. This book provides several examples in different chapters of causal
    models built upon deep learning architectures, providing an introduction to the
    fundamental ideas used in this exciting new growth area of causal AI.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.2 Causal reinforcement learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In canonical reinforcement learning, learning agents ingest large amounts of
    data and learn like Pavlov’s dog; they learn actions that correlate positively
    with good outcomes and negatively with bad outcomes. However, as we all know,
    correlation does not imply causation. Causal reinforcement learning can highlight
    cases where the action that causes a higher reward differs from the action that
    correlates most strongly with high rewards. Further, it addresses the problem
    of credit assignment (correctly attributing rewards to actions) with counterfactual
    reasoning (i.e., asking questions like “how much reward would the agent have received
    had they been using a different policy?”). Chapter 12 is devoted to causal reinforcement
    learning and other areas of causal decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.3 Large language models and foundation models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large language models (LLMs) such as OpenAI’s GPT, Google’s Gemini, and Meta’s
    Llama are deep neural language models with many billions of parameters trained
    on vast amounts of text and other data. These models can generate highly coherent
    natural language, code, and content of other modalities. They are foundation models,
    meaning they provide a foundation for building more domain-specific machine learning
    models and products. These products, such as Microsoft 365 Copilot, are already
    having a tremendous business impact.
  prefs: []
  type: TYPE_NORMAL
- en: A new area of investigation and product development investigates LLMs’ ability
    to answer causal questions and perform causal analysis. Another line of investigation
    is using causal methods to design and train new LLMs with optimized causal capabilities.
    In chapter 13, we’ll explore the intersection of LLMs and causality.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 A machine learning-themed primer on causality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you’ve seen the many ways that causal inference can improve machine
    learning, let’s look at the process of incorporating causality into AI models.
    To do this, we will use a popular benchmark dataset often used in machine learning:
    the MNIST dataset of images of handwritten digits, each labeled with the actual
    digit represented in the image. Figure 1.4 illustrates multiple examples of the
    digits in MNIST.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F04_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 Each image in the MNIST dataset is an image of a written digit, and
    each image is labeled with the digit it represents.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: MNIST is essentially the “Hello World” of machine learning. It is primarily
    used to experiment with different machine learning algorithms and to compare their
    relative strengths. The basic prediction task is to take the matrix of pixels
    representing each image as input and return the correct image label as output.
    Let’s start the process of incorporating causal thinking into a probabilistic
    machine learning model applied to MNIST images.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.1 Queries, probabilities, and statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we’ll look at the basic process without including causal inference. Machine
    learning can use probability in analyses about quantities of interest. To do so,
    a probabilistic machine learning model learns a probabilistic representation of
    all the variables in that system. We can make predictions and decisions with probabilistic
    machine learning models using a three-step process.
  prefs: []
  type: TYPE_NORMAL
- en: '*Pose the question*—What is the question you want to answer?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Write down the math*—What probability (or probability-related quantity) will
    answer the question, given the evidence or data?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Do the statistical inference*—What statistical analysis will give you (or
    will *estimate*) that quantity?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is more formal terminology for these steps (*query*, *estimand*, and *estimator*)
    but we’ll avoid the jargon for now. Instead, we’ll start with a simple statistical
    example problem. Your step 1 might be “How tall are Bostonians?” For step 2, you
    might decide that knowing the *mean* height (in probability terms, the “expected
    value”) of everyone who lives in Boston will answer your question. Step 3 might
    involve randomly selecting 100 Bostonians and taking their average height; statistical
    theorems guarantee that this sample average is a close estimate of the true population
    mean.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s extend that workflow to modeling MNIST images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Pose the question'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose we are looking at the MNIST image in figure 1.5, which could be a “4”
    or could be a “9”. In step 1, we articulate a question, such as “given this image,
    what is the digit represented in this image?”
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F05_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 Is this an image of the digit 4 or 9? The canonical task of the MNIST
    dataset is to classify the digit label given the image.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Step 2: Write down the math'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In step 2, we want to find some probabilistic quantity that answers the question,
    given the evidence or data. In other words, we want to find something we can write
    down in probability math notation that can answer the question from step 1\. For
    our example with figure 1.5, the “evidence” or “data” is the image. Is the image
    a 4 or a 9? Let the variable *I* represent the image and *D* represent the digit.
    In probability notation, we can write the probability that the digit is a 4, given
    the image, as *P*(*D*=4|*I*=![figure](../Images/CH01_F05_Ness.png))*,* where *I*=![figure](../Images/CH01_F05_Ness.png)
    is shorthand for *I* being equal to some vector representation of the image. We
    can compare this probability to *P*(*D*=9|*I*=![figure](../Images/CH01_F05_Ness.png)),and
    choose the value of *D* that has the higher probability. Generalizing to all ten
    digits, the mathematical quantity we want in step 2 is shown in figure 1.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F06_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 Choose the digit with the highest probability, given the image.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In plain English, this is “the value *d* that maximizes the probability that
    *D* equals *d,* given the image,” where *d* is one of the ten digits (0–9).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Do the statistical inference'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Step 3 uses statistical analysis to assign a number to the quantity we identified
    in step 2\. There are any number of ways we can do this. For example, we could
    train a deep neural network that takes in the image as an input and predicts the
    digit as an output; we could design the neural net to assign a probability to
    *D*=*d* for every value *d*.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.2 Causality and MNIST
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So how could causality feature in the previous section’s three-step analysis?
    Yann LeCun is a Turing Award winner (computer science’s equivalent of the Nobel
    prize) for his work on deep learning, and he’s director of AI research at Meta.
    He is also one of the three researchers behind the creation of MNIST. He discusses
    the *causal* backstory of the MNIST data on his personal website, [https://yann.lecun.com/exdb/mnist/index.xhtml](https://yann.lecun.com/exdb/mnist/index.xhtml):'
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST database was constructed from NIST’s Special Database 3 and Special
    Database 1 which contain binary images of handwritten digits. NIST originally
    designated SD-3 as their training set and SD-1 as their test set. However, SD-3
    is much cleaner and easier to recognize than SD-1\. The reason for this can be
    found on the fact that SD-3 was collected among Census Bureau employees, while
    SD-1 was collected among high-school students. Drawing sensible conclusions from
    learning experiments requires that the result be independent of the choice of
    training set and test among the complete set of samples. Therefore, it was necessary
    to build a new database by mixing NIST’s datasets.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, the authors mixed the two datasets because they argue that if
    they trained a machine learning model solely on digits drawn by high schoolers,
    it would underperform when applied to digits drawn by bureaucrats. However, in
    real-world settings, we want robust models that can learn in one scenario and
    predict in another, even when those scenarios differ. For example, we want a spam
    filter to keep working when the spammers switch from Nigerian princes to Bhutanese
    princesses. We want our self-driving cars to stop even when there is graffiti
    on the stop sign. Shuffling the data like a deck of cards is a luxury not easily
    afforded in real-world settings.
  prefs: []
  type: TYPE_NORMAL
- en: Causal modeling leverages knowledge about the causal mechanisms underlying how
    the digits are drawn that will help models generalize beyond high school students
    and bureaucrats in the training data to high schoolers in the test data. Figure
    1.7 illustrates a causal DAG representing this system.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F07_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 An example causal DAG representing the generation of MNIST images.
    The nodes represent objects in the data generating process, and edges correspond
    to causal relationships between those objects.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This particular DAG imagines that the writer determines the thickness and curviness
    of the drawn digits, and that high schoolers tend to have a different handwriting
    style than bureaucrats. The graph also assumes that the writer’s classification
    is a cause of what digits they draw. Perhaps bureaucrats write more 1s, 0s, and
    5s, as these numbers occur more frequently in census work, while high schoolers
    draw other digits more often because they do more long division in math classes
    (this is a similar idea to how, in topic models, “topics” *cause* the frequency
    of words in a document). Finally, the DAG assumes that age is a common cause of
    writer type and image; you have to be below a certain age to be in high school
    and above a certain age to be a census official.
  prefs: []
  type: TYPE_NORMAL
- en: A causal modeling approach would use this causal knowledge to train a predictive
    model that could extrapolate from the high school training data to the bureaucrat
    test data. Such a model would generalize better to new situations where the distributions
    of writer type and other variables are different than in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.3 Causal queries, probabilities, and statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At the beginning of this chapter, I discussed various types of causal questions
    we can pose, such as causal discovery, quantifying causal effects, and causal
    decision-making. We can answer these and various other questions with a causal
    variation on our previous three-step analysis (pose the question, write down the
    math, do the statistical inference):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pose the causal question*—What is the question you want to answer?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Write down the causal math*—What probability (or expectation) will answer
    the causal question, given the evidence or data?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Do the statistical inference*—What statistical analysis will give you (or
    “estimate”) that causal quantity?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the third step is the same as in the original three steps. The causal
    nuance occurs in the first and second steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Pose the causal question'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'These are examples of some causal questions we could ask about our causal MNIST
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: “How much does the writer’s type (high schooler vs. bureaucrat) affect the look
    of an image of the digit 4 with level 3 thickness?”(*Conditional average treatment
    effect estimation* is discussed in chapter 11).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming that stroke thickness is a cause of the image, we might ask, “What
    would a 2 look like if it were as curvy as possible?” (This is *intervention prediction*,
    discussed in chapter 7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Given an image, how would it have turned out differently if the stroke curviness
    were heavier?” (See *counterfactual reasoning*, discussed in chapters 8 and 9).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “What should the stroke curviness be to get an aesthetically ideal image?” (*Causal
    decision-making* is discussed in chapter 12).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s consider the CATE in the first item. CATE estimation is a common causal
    inference question applied to ordinary tabular data, but rarely do we see it in
    the applied in the context of an AI computer vision problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Write down the causal math'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Causal inference theory tells us how to mathematically formalize our causal
    question. Using special causal notation, we can mathematically formalize our CATE
    query as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch1-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: where *E*(.) is an expectation operator. We’ll review expectation in the next
    chapter, but for now we can think of it as an averaging of pixels across images.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding use of subscripts is a special notation called “counterfactual
    notation” that represents an *intervention*. A random assignment in an experiment
    is a real-world intervention, but there are many experiments we can’t run in the
    real world. For example, it wouldn’t be feasible to run a trial where you randomly
    assign participants to either be a high school student or be a census bureau official.
    Nonetheless, we want to know how the writer type causally impacts the images,
    and thus we rely on a causal model and its ability to represent interventions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, figure 1.8 visualizes what CATE might look like. The challenge
    is deriving the differential image at the right of figure 1.8\. Causal inference
    theory helps us address potential age-related “confounding” bias in quantifying
    how much writer type drives the image. For example, the *do-calculus* (chapter
    10) is a set of graph-based rules that allows us to take this DAG and algorithmically
    derive the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch1-eqs-1x.png)'
  prefs: []
  type: TYPE_IMG
- en: The left side of this equation defines the expectations used in the CATE definition
    in the second step—it is a theoretical construct that captures the hypothetical
    condition “if writer type were set to ‘w’”. But the right side is actionable;
    it is composed entirely of terms we could estimate using machine learning methods
    on a hypothetical version of NIST image data labeled with the writers’ ages.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F08_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 Visualization of an example CATE of writer type on an image. It is
    the pixel-by-pixel difference of the expected image under one intervention (*W=*["high
    school"]) minus the expected image under another intervention (*W=*["bureaucrat"]),
    with both expectations conditional on being images of the digit 4 with a certain
    level of thickness.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Step 3: Do the statistical inference'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Step 3 does the statistical estimation, and there are several ways we could
    estimate the quantities on the right side of that equation. For example, we could
    use a convolutional neural network to model *E*(*I*|*W*=*w*, *A*=*a*, *D*=*d*,
    *T*=*t*), and build a probability model of the joint distribution *P*(*A*, *D*,
    *T*). The choice of statistical modeling approach involves the usual statistical
    trade-offs, such as ease-of-use, bias and variance, scalability to large data,
    and parallelizability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other books go into great detail on preferred statistical methods for step
    3\. I take the strongly opinionated view that we should rely on the “commodification
    of inference” trend in statistical modeling and machine learning frameworks to
    handle step 3, and instead focus on honing our skills on steps 1 and 2: figuring
    out the right questions to ask, and representing the possible causes mathematically.'
  prefs: []
  type: TYPE_NORMAL
- en: As you’ve seen in this section, our journey into causal AI is scaffolded by
    a three-step process, and the essence of causal thinking emerges prominently in
    the first two steps. Step 1 invites us to frame the right causal questions, while
    step 2 illuminates the mathematics behind these questions. Step 3 leverages patterns
    we’re well-accustomed to in traditional statistical prediction and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Using this structured approach, we’ll transition in the coming chapters from
    purely predictive machine learning models—like the deep latent variable models
    you might be familiar with from MNIST—to causal machine learning models that offer
    deeper insights into and answers to our causal questions. First, we will review
    the underlying mathematics and machine learning foundations. Then, in part 2 of
    the book, we’ll delve into crafting the right questions and articulating them
    mathematically for steps 1 and 2\. For step 3, we’ll harness the power of contemporary
    tools like PyTorch and other advanced libraries to bridge the causal concepts
    with cutting-edge statistical learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Causal AI seeks to augment statistical learning and probabilistic reasoning
    with causal logic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal inference helps data scientists extract more causal insights from observational
    data (the vast majority of data in the world) and experimental data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When data scientists can’t run experiments, causal models can simulate experiments
    from observational data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can use these simulations to make causal inferences, such as estimating
    causal effects, and even to prioritize interesting experiments to run in real
    life.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal inference also helps data scientists improve decision-making in their
    organizations through algorithmic counterfactual reasoning and attribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal inference also makes machine learning more *robust*, *decomposable*,
    and *explainable*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal analysis is useful for formally analyzing *fairness* in predictive algorithms
    and for building fairer algorithms by parsing ordinary statistical bias into its
    causal sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *commodification of inference* is a trend in machine learning that refers
    to how universal modeling frameworks like PyTorch continuously automate the nuts
    and bolts of statistical learning and probabilistic inference. The trend reduces
    the need for the modeler to be an expert at the formal and statistical details
    of causal inference and allows them to focus on turning domain expertise into
    better causal models of their problem domain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of causal inference tasks include *causal discovery*, *intervention prediction*,
    *causal effect estimation*, c*ounterfactual reasoning*, *explanation*, and *attribution*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The way we build and work with probabilistic machine learning models can be
    extended to causal generative models implemented in probabilistic machine learning
    tools such as PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
