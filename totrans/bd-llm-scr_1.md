# 1 理解大型语言模型

### 本章涵盖

+   对大型语言模型（LLMs）背后基本概念的高级解释

+   关于ChatGPT类LLMs所衍生的变换器架构的见解

+   从头构建大型语言模型的计划

大型语言模型（LLMs），如OpenAI的ChatGPT，是近年来开发的深度神经网络模型。它们为自然语言处理（NLP）带来了新时代。在大型语言模型出现之前，传统方法在电子邮件垃圾分类和能够用手工规则或简单模型捕捉的直观模式识别等分类任务上表现出色。然而，它们在需要复杂理解和生成能力的语言任务上通常表现不佳，例如解析详细指令、进行上下文分析或创建连贯且上下文适宜的原创文本。例如，以前的语言模型无法根据关键词列表撰写电子邮件，而这对于当代LLMs来说是微不足道的任务。

LLMs具备理解、生成和解释人类语言的卓越能力。然而，重要的是要澄清，当我们说语言模型“理解”时，我们指的是它们能够以连贯且上下文相关的方式处理和生成文本，而不是说它们具有人类般的意识或理解能力。

由于深度学习的进展，深度学习是机器学习和人工智能（AI）的一部分，专注于神经网络，大型语言模型（LLMs）在大量文本数据上进行训练。这使得LLMs能够捕捉比以往方法更深层次的上下文信息和人类语言的细微差别。因此，LLMs在文本翻译、情感分析、问答等多种自然语言处理（NLP）任务中表现显著提升。

当代LLMs与早期NLP模型之间另一个重要区别在于，后者通常是为特定任务设计的；而早期NLP模型在狭窄应用中表现出色，LLMs则在更广泛的NLP任务中展示了更广泛的能力。

LLMs的成功归因于支持许多LLMs的变换器架构，以及LLMs在海量数据上进行训练，使其能够捕捉多种语言细微差别、上下文和模式，这些都是手动编码时很难实现的。

这种向基于变换器架构的模型实施和使用大型训练数据集训练LLMs的转变，根本改变了NLP，为理解和与人类语言互动提供了更强大的工具。

从本章开始，我们奠定了实现本书主要目标的基础：通过逐步用代码实现基于变换器架构的ChatGPT类LLM，理解LLMs。

## 1.1 什么是LLM？

LLM，即大型语言模型，是一种旨在理解、生成和响应类人文本的神经网络。这些模型是深度神经网络，经过大量文本数据的训练，有时涵盖了互联网上大量的公开文本。

“大”语言模型中的“大”指的是模型在参数上的大小以及其训练所需的庞大数据集。像这样的模型通常有数十亿甚至数百亿的参数，这些参数是在训练过程中优化的可调权重，用于预测序列中的下一个词。下一个词的预测是合理的，因为它利用了语言固有的顺序特性来训练模型理解文本中的上下文、结构和关系。然而，这是一项非常简单的任务，因此许多研究人员对此能生成如此强大的模型感到惊讶。我们将在后面的章节中逐步讨论并实施下一个词的训练程序。

LLMs利用一种称为*transformer*的架构（在1.4节中有更详细的介绍），使它们在进行预测时能够对输入的不同部分进行选择性关注，从而特别擅长处理人类语言的细微差别和复杂性。

由于LLMs能够*生成*文本，因此它们通常被称为生成式人工智能（AI）的一个形式，常缩写为*生成式AI*或*GenAI*。如图1.1所示，AI涵盖了创建能够执行人类智能所需任务的机器的广泛领域，包括理解语言、识别模式和做出决策，且包括机器学习和深度学习等子领域。

##### 图1.1 正如这个不同领域之间关系的层次化描述所暗示的，LLMs代表了深度学习技术的特定应用，利用它们处理和生成类人文本的能力。深度学习是机器学习的一个专门分支，专注于使用多层神经网络。而机器学习和深度学习都是旨在实现能够让计算机从数据中学习并执行通常需要人类智能的任务的算法的领域。

![](images/01__image001.png)

用于实现AI的算法是机器学习领域的重点。具体来说，机器学习涉及开发能够从数据中学习并根据数据做出预测或决策的算法，而无需明确编程。为了说明这一点，可以想象垃圾邮件过滤器作为机器学习的一个实际应用。机器学习算法会接收被标记为垃圾邮件和合法邮件的邮件示例，而不是手动编写规则来识别垃圾邮件。通过最小化在训练数据集上的预测误差，模型能够学习识别表明垃圾邮件的模式和特征，从而将新邮件分类为垃圾邮件或合法邮件。

如图1.1所示，深度学习是机器学习的一个子集，专注于利用三层或更多层的神经网络（也称为深度神经网络）来建模数据中的复杂模式和抽象。与深度学习相对，传统机器学习需要手动特征提取。这意味着人类专家需要识别和选择模型的最相关特征。

尽管如今AI领域主要由机器学习和深度学习主导，但它还包括其他方法，例如基于规则的系统、遗传算法、专家系统、模糊逻辑或符号推理。

回到垃圾邮件分类的例子，在传统机器学习中，人类专家可能会手动从邮件文本中提取特征，例如某些触发词（“奖品”、“赢”、“免费”的频率）、感叹号的数量、全大写字母的使用或可疑链接的存在。根据这些专家定义的特征创建的数据集将被用于训练模型。与传统机器学习相比，深度学习不需要手动特征提取。这意味着人类专家不需要为深度学习模型识别和选择最相关的特征。（然而，在垃圾邮件分类的传统机器学习和深度学习中，你仍然需要收集标签，如垃圾邮件或非垃圾邮件，这些标签需要由专家或用户收集。）

接下来的部分将讨论LLM今天能解决的一些问题、LLM所面临的挑战，以及我们将在本书中实现的LLM的基本架构。

## 1.2 LLM的应用

由于其解析和理解非结构化文本数据的先进能力，LLM在各个领域具有广泛的应用。今天，LLM被用于机器翻译、生成新文本（见图1.2）、情感分析、文本摘要及许多其他任务。最近，LLM也被用于内容创作，如写小说、文章，甚至计算机代码。

##### 图1.2 LLM接口使用户与AI系统之间能够进行自然语言交流。此屏幕截图显示了ChatGPT根据用户的要求写诗。

![](images/01__image003.png)

LLM还可以支持复杂的聊天机器人和虚拟助手，例如OpenAI的ChatGPT或谷歌的Gemini（以前称为Bard），这些助手能够回答用户查询，并增强传统搜索引擎如谷歌搜索或微软必应的功能。

此外，LLMs可用于从大量专业领域文本中有效地检索知识，例如医学或法律。这包括筛选文档、总结冗长段落以及回答技术性问题。

简而言之，LLMs对于自动化几乎任何涉及解析和生成文本的任务都是不可或缺的。它们的应用几乎是无穷无尽的，随着我们不断创新并探索使用这些模型的新方法，显然LLMs有潜力重新定义我们与技术的关系，使其变得更加对话式、直观和易于接触。

在本书中，我们将重点理解LLMs如何从零开始工作，编写一个能够生成文本的LLM。我们还将学习使LLMs能够执行查询的技术，从回答问题到总结文本、将文本翻译成不同语言等。换句话说，在本书中，我们将通过逐步构建一个复杂的LLM助手，如ChatGPT，来了解其工作原理。

## 1.3 构建和使用LLMs的阶段

我们为什么要构建自己的LLMs？从零开始编写LLM是一个很好的练习，可以帮助我们理解其机制和局限性。此外，这也为我们提供了预训练或微调现有开源LLM架构到我们特定领域的数据集或任务所需的知识。

研究表明，在建模性能方面，定制的LLMs——那些针对特定任务或领域量身定制的LLMs——可以超越通用的LLMs，例如ChatGPT提供的那些，后者是为广泛应用而设计的。这方面的例子包括专注于金融的BloombergGPT，以及专门针对医学问答的LLMs（请参见附录B中的*进一步阅读与参考资料*部分以获取更多细节）。

创建LLM的一般过程包括预训练和微调。“预训练”中的“预”指的是模型（如LLM）在一个大型、多样化的数据集上进行训练的初始阶段，以发展对语言的广泛理解。这个预训练的模型随后作为基础资源，可以通过微调进一步精炼，微调的过程是指模型在一个更狭窄且特定于特定任务或领域的数据集上进行专门训练。这种包括预训练和微调的两阶段训练方法如图1.3所示。

##### 图 1.3 预训练 LLM 涉及对大型文本数据集进行下一个词预测。预训练的 LLM 可以使用较小的标注数据集进行微调。

![](images/01__image005.png)

如图 1.3 所示，创建 LLM 的第一步是在一个大型文本数据集上进行训练，有时称为 *原始* 文本。在这里，“原始”指的是这些数据仅为常规文本，没有任何标注信息[[1]](#_ftn1)。 （可能会进行过滤，例如去除格式字符或未知语言的文档。）

LLM 的第一阶段训练也被称为 *预训练*，创建一个初始的预训练 LLM，通常称为 *基础* 或 *基础* *模型*。这种模型的一个典型例子是 GPT-3 模型（ChatGPT 中提供的原始模型的前身）。该模型能够完成文本，即完成用户提供的半句话。它还具有限的少量示例能力，这意味着它可以根据仅有的几个示例学习执行新任务，而无需大量的训练数据。下一节*使用变换器进行不同任务*对此进行了进一步说明。

在通过大型文本数据集训练获得 *预训练* LLM 后，该 LLM 被训练以预测文本中的下一个词，我们可以在标注数据上进一步训练 LLM，这也被称为 *微调*。

微调 LLM 的两个最流行的类别包括 *指令微调* 和 *分类* 任务的微调。在指令微调中，标注数据集由指令和答案对组成，例如翻译文本的查询及其对应的正确翻译文本。在分类微调中，标注数据集由文本及相关类别标签组成，例如与 *垃圾邮件* 和 *非垃圾邮件* 标签相关的电子邮件。

在本书中，我们将涵盖预训练和微调 LLM 的代码实现，并将在本书后面深入探讨指令微调和分类微调的具体细节。

## 1.4 使用 LLM 进行不同任务

大多数现代 LLM 依赖于 *变换器* 架构，这是一种在 2017 年的论文 *注意力是你所需要的一切* 中提出的深度神经网络架构。为了理解 LLM，我们需要简要回顾原始的变换器，该变换器最初是为机器翻译开发的，用于将英语文本翻译成德语和法语。变换器架构的简化版本在图 1.4 中进行了描述。

##### 图1.4 原始变压器架构的简化示意图，这是一个用于语言翻译的深度学习模型。变压器由两部分组成，一个编码器处理输入文本并生成嵌入表示（这是一个捕捉不同维度中多种因素的数值表示），解码器可以利用这些表示逐字生成翻译文本。请注意，这幅图展示了翻译过程的最终阶段，解码器仅需根据原始输入文本（“This is an example”）和部分翻译句子（“Das ist ein”）生成最终单词（“Beispiel”），以完成翻译。

![](images/01__image007.png)

图1.4所示的变压器架构由两个子模块组成，编码器和解码器。编码器模块处理输入文本并将其编码为一系列捕捉输入上下文信息的数值表示或向量。然后，解码器模块利用这些编码向量生成输出文本。在翻译任务中，例如，编码器会将源语言的文本编码为向量，而解码器则会解码这些向量生成目标语言的文本。编码器和解码器都由许多层通过所谓的自注意力机制连接。你可能会对输入如何预处理和编码有很多问题。这些将在后续章节中逐步实现。

变压器和大型语言模型的一个关键组件是自注意力机制（未显示），它允许模型相对地权衡序列中不同单词或标记的重要性。该机制使模型能够捕捉输入数据中的长程依赖关系和上下文关系，从而增强其生成连贯且上下文相关输出的能力。然而，由于其复杂性，我们将在第3章推迟对此的解释，在那里我们将逐步讨论和实现。此外，我们还将在*第2章：处理文本数据*中讨论和实现数据预处理步骤，以创建模型输入。

后来的变种变压器架构，例如所谓的BERT（即*双向编码器表示来自变压器*）和各种GPT模型（即*生成式预训练变压器*），在此概念基础上对架构进行了适应，以用于不同的任务。（参考文献见附录B。）

BERT建立在原始变换器的编码子模块之上，其训练方法与GPT有所不同。虽然GPT旨在用于生成任务，但BERT及其变体专注于被遮蔽的单词预测，模型在给定句子中预测被遮蔽或隐藏的单词，如图1.5所示。这种独特的训练策略使BERT在文本分类任务中具有优势，包括情感预测和文档分类。作为其能力的应用，截至目前，Twitter使用BERT来检测有害内容。

##### 图1.5 变换器的编码器和解码器子模块的可视化表示。左侧，编码器部分示例了类似BERT的LLM，这些模型专注于被遮蔽的单词预测，主要用于文本分类等任务。右侧，解码器部分展示了类似GPT的LLM，设计用于生成任务并生成连贯的文本序列。

![](images/01__image009.png)

GPT则专注于原始变换器架构的解码器部分，旨在用于需要生成文本的任务。这包括机器翻译、文本摘要、虚构写作、编写计算机代码等。我们将在本章的后续部分更详细地讨论GPT架构，并在本书中从头实现它。

GPT模型主要设计和训练用于执行文本完成任务，但在能力上也展现了显著的多样性。这些模型擅长执行零样本学习和少量样本学习任务。零样本学习是指在没有任何先前特定示例的情况下，对完全未见过的任务进行泛化。另一方面，少量样本学习涉及从用户提供的少量示例中进行学习，如图1.6所示。

##### 图1.6 除了文本完成，类似GPT的LLM还可以根据输入解决各种任务，而无需重新训练、微调或更改特定任务的模型架构。有时，在输入中提供目标的示例是有帮助的，这被称为少量样本设置。然而，类似GPT的LLM也能够在没有具体示例的情况下执行任务，这被称为零样本设置。

![](images/01__image011.png)

##### 变换器与LLM

今天的 LLM 基于上一节介绍的变换器架构。因此，变换器和 LLM 在文献中常被用作同义词。然而，请注意，并非所有变换器都是 LLM，因为变换器也可以用于计算机视觉。同时，并非所有 LLM 都是变换器，因为有些大型语言模型基于递归和卷积架构。这些替代方法的主要动机是提高 LLM 的计算效率。然而，这些替代 LLM 架构是否能够与基于变换器的 LLM 的能力竞争，以及它们是否会在实践中被采用，还有待观察。（有兴趣的读者可以在本章末尾的*进一步阅读*部分找到描述这些架构的文献参考。）

## 1.5 利用大型数据集

流行的 GPT 和 BERT 类模型的大型训练数据集代表了涵盖数十亿单词的多样且全面的文本语料库，其中包括广泛的主题以及自然语言和计算机语言。为了提供一个具体示例，表 1.1 总结了用于预训练 GPT-3 的数据集，该模型作为第一版 ChatGPT 的基础模型。

##### 表 1.1 流行的 GPT-3 LLM 的预训练数据集

| 数据集名称 | 数据集描述 | 标记数量 | 训练数据中的比例 |
| --- | --- | --- | --- |
| CommonCrawl（过滤） | 网页抓取数据 | 4100 亿 | 60% |
| WebText2 | 网页抓取数据 | 190 亿 | 22% |
| 书籍1 | 基于互联网的书籍语料库 | 120 亿 | 8% |
| 书籍2 | 基于互联网的书籍语料库 | 550 亿 | 8% |
| 维基百科 | 高质量文本 | 30 亿 | 3% |

表 1.1 报告了标记的数量，其中标记是模型读取的文本单元，数据集中的标记数量大致等同于文本中的单词和标点符号的数量。我们将在下一章中更详细地探讨标记化，即将文本转换为标记的过程。

主要结论是，这个训练数据集的规模和多样性使这些模型能够在包括语言句法、语义和上下文在内的多种任务上表现良好，甚至还包括一些需要一般知识的任务。

##### GPT-3 数据集详情

在表 1.1 中，重要的是要注意，从每个数据集中，仅有一小部分数据（总计 3000 亿个标记）用于训练过程。这种抽样方法意味着训练并没有涵盖每个数据集中可用的每一条数据。相反，利用了从所有数据集中组合而成的选定子集——3000 亿个标记。此外，尽管某些数据集在此子集中并未完全覆盖，其他数据集可能被多次包含，以达到 3000 亿个标记的总数。表中表示比例的列在未考虑四舍五入误差的情况下，总和占据了 100% 的抽样数据。

为了提供背景，请考虑CommonCrawl数据集的规模，该数据集本身由4100亿个标记组成，所需存储约为570 GB。相比之下，后续版本的模型如GPT-3，例如Meta的LLaMA，已扩展其训练范围，包含额外的数据源，如Arxiv研究论文（92 GB）和StackExchange的代码相关问答（78 GB）。

*Wikipedia*语料库由*英语维基百科*组成。虽然GPT-3论文的作者没有进一步指定细节，*Books1*很可能是来自古腾堡计划（[https://www.gutenberg.org/](www.gutenberg.org.html)）的样本，*Books2*很可能来自*Libgen*（[https://en.wikipedia.org/wiki/Library_Genesis](wiki.html)）。*CommonCrawl*是CommonCrawl数据库的一个过滤子集（[https://commoncrawl.org/](commoncrawl.org.html)），而*WebText2*则是来自所有3个以上点赞的Reddit帖子中所有外部链接的网页文本。

GPT-3论文的作者没有分享训练数据集，但一个可以公开获取的相似数据集是*The Pile*（[https://pile.eleuther.ai/](pile.eleuther.ai.html)）。然而，该集合可能包含受版权保护的作品，确切的使用条款可能取决于预期用途和国家。有关更多信息，请参见HackerNews的讨论，链接为[https://news.ycombinator.com/item?id=25607809](news.ycombinator.com.html)。

这些模型的预训练特性使其在下游任务中极为灵活，这也是它们被称为基础模型的原因。预训练大型语言模型需要访问大量资源，并且成本非常高。例如，GPT-3的预训练成本估计为460万美元的云计算信用[[2]](#_ftn2)。

好消息是，许多作为开源模型提供的预训练大型语言模型，可以作为通用工具来编写、提取和编辑不在训练数据中的文本。此外，LLM可以在相对较小的数据集上进行微调，从而减少所需的计算资源并提高特定任务的性能。

在本书中，我们将实现预训练代码，并利用它来为教育目的预训练大型语言模型。所有计算将在消费硬件上可执行。实现预训练代码后，我们将学习如何重用公开可用的模型权重，并将其加载到我们将实现的架构中，从而在稍后的章节中微调大型语言模型时跳过昂贵的预训练阶段。

## 1.6 更深入地了解GPT架构

在本章之前，我们提到过GPT类模型、GPT-3和ChatGPT的术语。现在让我们更深入地了解一般的GPT架构。首先，GPT代表***G***enerative ***P***retrained ***T***ransformer，最初是在以下论文中介绍的：

+   *通过生成预训练改善语言理解*（2018），*Radford等*，来自OpenAI，[http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](language-unsupervised.html)

GPT-3是该模型的扩展版本，具有更多参数，并在更大的数据集上进行训练。而在ChatGPT中提供的原始模型是通过在一个大型指令数据集上微调GPT-3来创建的，采用了OpenAI的InstructGPT论文中的方法，我们将在*第7章，利用人类反馈进行微调*中更详细地讨论。正如我们之前在图1.6中所看到的，这些模型是高效的文本补全模型，并可以执行拼写纠正、分类或语言翻译等其他任务。这实际上非常值得注意，因为GPT模型是在相对简单的下一个词预测任务上进行预训练的，如图1.7所示。

##### 图1.7 在GPT模型的下一个词预训练任务中，系统通过查看之前的单词来预测句子中的下一个单词。这种方法帮助模型理解单词和短语在语言中通常是如何组合在一起的，形成了可以应用于各种其他任务的基础。

![](images/01__image013.png)

下一个词预测任务是一种自我监督学习形式，这是一种自我标注的形式。这意味着我们不需要明确收集训练数据的标签，而是可以利用数据本身的结构：我们可以使用句子或文档中的下一个词作为模型要预测的标签。由于这个下一个词预测任务使我们能够“即时”创建标签，因此可以利用大量未标记的文本数据集来训练LLM，正如之前在*1.5节，利用大型数据集*中讨论的那样。

与我们在1.4节*使用LLMs进行不同任务*中讨论的原始变压器架构相比，通用的GPT架构相对简单。本质上，它只是没有编码器的解码器部分，如图1.8所示。由于像GPT这样的解码器风格模型通过逐个预测文本生成文本，因此它们被视为一种*自回归*模型。自回归模型将其先前的输出作为未来预测的输入。因此，在GPT中，每个新单词是根据其前面的序列选择的，这提高了生成文本的一致性。

像GPT-3这样的架构也显著大于原始的变压器模型。例如，原始的变压器重复了编码器和解码器块六次。GPT-3总共有96个变压器层和1750亿个参数。

##### 图1.8 GPT架构仅使用了原始变换器的解码器部分。它被设计为单向的、从左到右处理，非常适合文本生成和下一个单词预测任务，以逐词迭代的方式生成文本。

![](images/01__image015.png)

GPT-3于2020年推出，按照深度学习和大型语言模型（LLM）发展的标准来看，这已经是很久以前的事了。然而，像Meta的Llama模型这样的较新架构仍然基于相同的基本概念，仅进行了小幅修改。因此，理解GPT依然十分相关，而本书的重点是实现GPT背后的显著架构，同时提供其他LLM所采用的具体调整的指引。

最后，有趣的是，尽管原始的变换器模型是专门为语言翻译设计的，但GPT模型——尽管其较大但更简单的架构旨在预测下一个单词——同样能够执行翻译任务。研究人员最初并未预料到这一点，因为这一能力源于一个主要训练于下一个单词预测任务的模型，而该任务并没有特别针对翻译。

执行模型没有明确训练的任务的能力被称为“涌现行为”。这种能力在训练期间并未明确教授，而是作为模型在多种上下文中接触大量多语言数据的自然结果而出现。GPT模型能够“学习”语言之间的翻译模式并执行翻译任务，尽管它们并未专门为此训练，体现了这些大规模生成语言模型的好处和能力。我们可以在不使用多种模型的情况下执行多样化的任务。

## 1.7 构建大型语言模型

在这一章中，我们为理解LLM奠定了基础。在本书的剩余部分，我们将从零开始编码一个。我们将把GPT背后的基本理念作为蓝图，并按照图1.9中概述的三个阶段进行处理。

##### 图1.9 本书中涉及的构建LLM的阶段包括实现LLM架构和数据准备过程，预训练LLM以创建基础模型，以及微调基础模型以成为个人助手或文本分类器。

![](images/01__image017.png)

首先，我们将学习基本的数据预处理步骤，并编写每个LLM核心的注意力机制代码。

接下来，在第二阶段，我们将学习如何编码和预训练一个类似GPT的LLM，能够生成新文本。同时，我们还将讨论评估LLM的基本原理，这对开发有能力的NLP系统至关重要。

请注意，从头开始对大型语言模型（LLM）进行预训练是一项重大工作，GPT 类模型的计算成本可能高达数千到数百万美元。因此，第二阶段的重点是使用小型数据集实施教育目的的训练。此外，本书还将提供加载开放可用模型权重的代码示例。

最后，在第三阶段，我们将采用一个预训练的 LLM，并对其进行微调，以遵循指令，如回答查询或分类文本——这是许多实际应用和研究中最常见的任务。

我希望你期待开始这段令人兴奋的旅程！

## 1.8 摘要

+   LLM 已经改变了自然语言处理领域，之前主要依赖于显式的基于规则的系统和更简单的统计方法。LLM 的出现引入了新的深度学习驱动的方法，推动了对人类语言的理解、生成和翻译的进步。

+   现代 LLM 的训练主要分为两个步骤。

+   首先，它们在一个大型未标记文本语料库上进行预训练，通过使用句子中下一个单词的预测作为“标签”。

+   然后，它们在一个较小的标记目标数据集上进行微调，以遵循指令或执行分类任务。

+   LLM 基于变压器架构。变压器架构的关键理念是注意机制，使 LLM 在逐字生成输出时能够选择性地访问整个输入序列。

+   原始的变压器架构由一个用于解析文本的编码器和一个用于生成文本的解码器组成。

+   用于生成文本和遵循指令的 LLM，如 GPT-3 和 ChatGPT，仅实现解码器模块，从而简化了架构。

+   由数十亿单词组成的大型数据集对 LLM 的预训练至关重要。在本书中，我们将实现并在小型数据集上训练 LLM，以实现教育目的，同时也会展示如何加载开放可用的模型权重。

+   虽然类似 GPT 的模型的一般预训练任务是预测句子中的下一个单词，但这些 LLM 展现出“突现”特性，例如分类、翻译或总结文本的能力。

+   一旦 LLM 预训练完成，得到的基础模型可以更高效地针对各种下游任务进行微调。

+   在定制数据集上微调的 LLM 在特定任务上可以超越通用 LLM。

[[1]](#_ftnref1) 具有机器学习背景的读者可能注意到，传统机器学习模型和通过常规监督学习范式训练的深度神经网络通常需要标记信息。然而，这并不是 LLM 预训练阶段的情况。在这个阶段，LLM 利用自监督学习，模型从输入数据中生成自己的标签。这一概念将在本章后面进行介绍。

[[2]](#_ftnref2) *GPT-3，$4,600,000语言模型*，[https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_](h0jwoz.html)[4600000_language_model/](d_gpt3_the_4600000_language_model.html)
