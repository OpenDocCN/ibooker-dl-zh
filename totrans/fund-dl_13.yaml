- en: Chapter 13\. Deep Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。深度强化学习
- en: '[Nicholas Locascio](http://nicklocascio.com)'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[Nicholas Locascio](http://nicklocascio.com)'
- en: In this chapter, we’ll discuss reinforcement learning, which is a branch of
    machine learning that deals with learning via interaction and feedback. Reinforcement
    learning is essential to building agents that can not only perceive and interpret
    the world, but also take action and interact with it. We will discuss how to incorporate
    deep neural networks into the framework of reinforcement learning and discuss
    recent advances and improvements in this field.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论强化学习，这是机器学习的一个分支，涉及通过互动和反馈学习。强化学习对于构建不仅可以感知和解释世界，还可以采取行动并与之互动的代理至关重要。我们将讨论如何将深度神经网络纳入强化学习框架，并讨论这一领域的最新进展和改进。
- en: Deep Reinforcement Learning Masters Atari Games
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习掌握Atari游戏
- en: The application of deep neural networks to reinforcement learning had a major
    breakthrough in 2014, when the London startup DeepMind astonished the machine
    learning community by unveiling a deep neural network that could learn to play
    Atari games with superhuman skill. This network, termed a *deep Q-network* (DQN)
    was the first large-scale successful application of reinforcement learning with
    deep neural networks. DQN was so remarkable because the same architecture, without
    any changes, was capable of learning 49 different Atari games, despite each game
    having different rules, goals, and game-play structure. To accomplish this feat,
    DeepMind brought together many traditional ideas in reinforcement learning while
    also developing a few novel techniques that proved key to DQN’s success. Later
    in this chapter, we will implement DQN, as described in the *Nature* paper, “Human-Level
    Control Through Deep Reinforcement Learning.”^([1](ch13.xhtml#idm45934163590304))
    But first, let’s take a dive into reinforcement learning ([Figure 13-1](#fig0801)).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，伦敦初创公司DeepMind在强化学习中应用深度神经网络取得了重大突破，令机器学习社区大为震惊，他们揭示了一个可以学会以超人类技能玩Atari游戏的深度神经网络。这个网络被称为*深度Q网络*（DQN），是强化学习与深度神经网络的首次大规模成功应用。DQN之所以如此引人注目，是因为相同的架构，没有任何改变，能够学会玩49种不同的Atari游戏，尽管每个游戏都有不同的规则、目标和游戏结构。为了实现这一壮举，DeepMind汇集了许多传统的强化学习思想，同时也开发了一些关键的新技术，这些技术对DQN的成功至关重要。在本章的后面，我们将实现DQN，如《自然》杂志上描述的那样，“通过深度强化学习实现人类水平控制”。^([1](ch13.xhtml#idm45934163590304))
    但首先，让我们深入了解强化学习（[图13-1](#fig0801)）。
- en: '![](Images/fdl2_1301.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1301.png)'
- en: Figure 13-1\. A deep reinforcement learning agent playing Breakout^([2](ch13.xhtml#idm45934163585712))
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-1。一个深度强化学习代理在玩Breakout游戏
- en: What Is Reinforcement Learning?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: Reinforcement learning, at its essentials, is learning by interacting with an
    environment. This learning process involves an *agent*, an *environment*, and
    a *reward signal*. The agent chooses to take an action in the environment, for
    which the agent is rewarded accordingly. The way in which an actor chooses actions
    is called a *policy*. The agent wants to increase the reward it receives, and
    so must learn an optimal policy for interacting with the environment ([Figure 13-2](#fig0802)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在本质上是通过与环境互动学习。这个学习过程涉及到一个*代理*，一个*环境*和一个*奖励信号*。代理选择在环境中采取行动，根据行动获得奖励。演员选择行动的方式被称为*策略*。代理希望增加它接收到的奖励，因此必须学习与环境互动的最佳策略（[图13-2](#fig0802)）。
- en: Reinforcement learning is different from the other types of learning that we
    have covered thus far. In traditional supervised learning, we are given data and
    labels, and are tasked with predicting labels given data. In unsupervised learning,
    we are given just data and are tasked with discovering underlying structure in
    this data. In reinforcement learning, we are given neither data nor labels. Our
    learning signal is derived from the rewards given to the agent by the environment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与我们迄今为止涵盖的其他学习类型不同。在传统的监督学习中，我们被给定数据和标签，并被要求根据数据预测标签。在无监督学习中，我们只给定数据，并被要求发现数据中的潜在结构。在强化学习中，我们既没有数据也没有标签。我们的学习信号来自环境给予代理的奖励。
- en: '![](Images/fdl2_1302.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1302.png)'
- en: Figure 13-2\. Reinforcement learning setup
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-2。强化学习设置
- en: Reinforcement learning is exciting to many in the AI community because it is
    a general-purpose framework for creating intelligent agents. Given an environment
    and some rewards, the agent learns to interact with that environment to maximize
    its total reward. This type of learning is more in line with how humans develop.
    Yes, we can build a pretty good model to classify dogs from cats with extremely
    high accuracy by training on thousands of images. But you won’t find this approach
    used in any elementary schools. Humans interact with their environment to learn
    representations of the world that they can use to make decisions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习对AI社区的许多人来说是令人兴奋的，因为它是创建智能代理的通用框架。给定一个环境和一些奖励，代理学会与该环境互动以最大化其总奖励。这种学习更符合人类的发展方式。是的，我们可以通过在成千上万张图像上进行训练，构建一个非常准确的模型来区分狗和猫。但你不会发现这种方法在任何小学中使用。人类通过与环境互动来学习世界的表示，以便做出决策。
- en: Furthermore, reinforcement learning applications are at the forefront of many
    cutting-edge technologies, including self-driving cars, robotic motor control,
    game playing, air-conditioning control, ad placement optimization, and stock market
    trading strategies.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，强化学习应用处于许多尖端技术的前沿，包括自动驾驶汽车、机器人电机控制、游戏玩法、空调控制、广告放置优化和股票交易策略。
- en: As an illustrative exercise, we’ll be tackling a simple reinforcement learning
    and control problem called pole balancing. In this problem, there is a cart with
    a pole that is connected by a hinge, so the pole can swing around the cart. There
    is an agent that can control the cart, moving it left or right. There is an environment,
    which rewards the agent when the pole is pointed upward, and penalizes the agent
    when the pole falls over ([Figure 13-3](#fig0803)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个说明性练习，我们将解决一个称为平衡杆的简单强化学习和控制问题。在这个问题中，有一个连接在铰链上的杆的小车，因此杆可以围绕小车摆动。有一个代理可以控制小车，将其向左或向右移动。有一个环境，当杆指向上方时，奖励代理，当杆倒下时，惩罚代理（[图13-3](#fig0803)）。
- en: '![](Images/fdl2_1303.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1303.png)'
- en: 'Figure 13-3\. A simple reinforcement learning agent: balancing a pole^([3](ch13.xhtml#idm45934163560160))'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-3。一个简单的强化学习代理：平衡杆^([3](ch13.xhtml#idm45934163560160))
- en: Markov Decision Processes
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: 'Our pole-balancing example has a few important elements, which we formalize
    as a *Markov decision process* (MDP). These elements are:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的平衡杆示例有一些重要的元素，我们将其形式化为*马尔可夫决策过程*（MDP）。这些元素包括：
- en: State
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 状态
- en: The cart has a range of possible places on the x-plane where it can be. Similarly,
    the pole has a range of possible angles.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 小车在x平面上有一系列可能的位置。同样，杆有一系列可能的角度。
- en: Action
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 动作
- en: The agent can take action by moving the cart either left or right.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以通过将小车向左或向右移动来采取行动。
- en: State transition
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 状态转移
- en: 'When the agent acts, the environment changes: the cart moves and the pole changes
    angle and velocity.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理行动时，环境会发生变化：小车移动，杆的角度和速度也会改变。
- en: Reward
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励
- en: If an agent balances the pole well, it receives a positive reward. If the pole
    falls, the agent receives a negative reward.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代理平衡杆得当，它会获得正面奖励。如果杆倒下，代理会受到负面奖励。
- en: 'An MDP is defined as the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: MDP定义如下：
- en: <math alttext="upper S"><mi>S</mi></math> , a finite set of possible states
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="upper S"><mi>S</mi></math>，一组可能状态的有限集
- en: <math alttext="upper A"><mi>A</mi></math> , a finite set of actions
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="upper A"><mi>A</mi></math>，一组有限的动作
- en: <math alttext="upper P left-parenthesis r comma s prime vertical-bar s comma
    a right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>r</mi> <mo>,</mo> <msup><mi>s</mi>
    <mo>'</mo></msup> <mo>|</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></math>
    , a state transition function
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis r comma s prime vertical-bar s comma
    a right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>r</mi> <mo>,</mo> <msup><mi>s</mi>
    <mo>'</mo></msup> <mo>|</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></math>，状态转移函数
- en: <math alttext="upper R"><mi>R</mi></math> , reward function
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="upper R"><mi>R</mi></math>，奖励函数
- en: MDPs offer a mathematical framework for modeling decision making in a given
    environment. [Figure 13-4](#fig0804) shows an example, with circles representing
    the states of the environment, diamonds representing actions that can be taken,
    and the edges from diamonds to circles representing the transition from one state
    to the next. The numbers along these edges represent the probability of taking
    a certain action, and the numbers at the end of the arrows represent the reward
    given to the agent for making the given transition.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: MDP为在给定环境中建模决策制定提供了一个数学框架。[图13-4](#fig0804)显示了一个示例，其中圆圈代表环境的状态，菱形代表可以采取的动作，从菱形到圆圈的边表示从一个状态转移到下一个状态的转换。沿着这些边的数字表示采取某个动作的概率，箭头末端的数字表示给予代理的奖励，用于进行给定转换。
- en: '![](Images/fdl2_1304.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1304.png)'
- en: Figure 13-4\. Example of an MDP
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-4。MDP示例
- en: 'As an agent takes action in an MDP framework, it forms an *episode*. An episode
    consists of series of tuples of states, actions, and rewards. Episodes run until
    the environment reaches a terminal state, like the “Game Over” screen in Atari
    games, or when the pole hits the ground in our pole-cart example. The following
    equation shows the variables in an episode:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理在MDP框架中采取行动时，它形成一个*情节*。一个情节由一系列状态、动作和奖励的元组组成。情节运行直到环境达到终止状态，例如Atari游戏中的“游戏结束”屏幕，或者在我们的杆-小车示例中，杆触地。以下方程式显示了情节中的变量：
- en: <math alttext="left-parenthesis s 0 comma a 0 comma r 0 right-parenthesis comma
    left-parenthesis s 1 comma a 1 comma r 1 right-parenthesis comma ellipsis left-parenthesis
    s Subscript n Baseline comma a Subscript n Baseline comma r Subscript n Baseline
    right-parenthesis"><mrow><mrow><mo>(</mo> <msub><mi>s</mi> <mn>0</mn></msub> <mo>,</mo>
    <msub><mi>a</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>r</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow> <mo>,</mo> <mrow><mo>(</mo> <msub><mi>s</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>r</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>n</mi></msub>
    <mo>,</mo> <msub><mi>a</mi> <mi>n</mi></msub> <mo>,</mo> <msub><mi>r</mi> <mi>n</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="left-parenthesis s 0 comma a 0 comma r 0 right-parenthesis comma
    left-parenthesis s 1 comma a 1 comma r 1 right-parenthesis comma ellipsis left-parenthesis
    s Subscript n Baseline comma a Subscript n Baseline comma r Subscript n Baseline
    right-parenthesis"><mrow><mrow><mo>(</mo> <msub><mi>s</mi> <mn>0</mn></msub> <mo>,</mo>
    <msub><mi>a</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>r</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow> <mo>,</mo> <mrow><mo>(</mo> <msub><mi>s</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>r</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>n</mi></msub>
    <mo>,</mo> <msub><mi>a</mi> <mi>n</mi></msub> <mo>,</mo> <msub><mi>r</mi> <mi>n</mi></msub>
    <mo>)</mo></mrow></mrow></math>
- en: 'In pole-cart, our environment state can be a tuple of the position of the cart
    and the angle of the pole, like so: ( <math alttext="x Subscript c a r t"><msub><mi>x</mi>
    <mrow><mi>c</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub></math> ,  <math alttext="theta
    Subscript p o l e"><msub><mi>θ</mi> <mrow><mi>p</mi><mi>o</mi><mi>l</mi><mi>e</mi></mrow></msub></math>
    ).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在杆-小车中，我们的环境状态可以是小车的位置和杆的角度的元组，如下所示：（<math alttext="x Subscript c a r t"><msub><mi>x</mi>
    <mrow><mi>c</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub></math>，<math alttext="theta
    Subscript p o l e"><msub><mi>θ</mi> <mrow><mi>p</mi><mi>o</mi><mi>l</mi><mi>e</mi></mrow></msub></math>）。
- en: Policy
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略
- en: MDP’s aim is to find an optimal policy for our agent. *Policies* are how our
    agent acts based on its current state. Formally, policies can be represented as
    a function <math alttext="pi"><mi>π</mi></math>  that chooses the action  <math
    alttext="a"><mi>a</mi></math>  that the agent will take in state  <math alttext="s"><mi>s</mi></math>
    .
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: MDP的目标是为我们的代理找到一个最优策略。*策略*是我们的代理根据其当前状态采取行动的方式。形式上，策略可以表示为一个函数<math alttext="pi"><mi>π</mi></math>，选择代理在状态<math
    alttext="s"><mi>s</mi></math>中将采取的动作<math alttext="a"><mi>a</mi></math>。
- en: 'The objective of our MDP is to find a policy to maximize the expected future
    return:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们MDP的目标是找到一个策略，以最大化预期的未来回报：
- en: <math alttext="max Underscript pi Endscripts upper E left-bracket upper R 0
    plus upper R 1 plus ellipsis upper R Subscript t Baseline vertical-bar pi right-bracket"><mrow><msub><mo
    movablelimits="true" form="prefix">max</mo> <mi>π</mi></msub> <mi>E</mi> <mrow><mo>[</mo>
    <msub><mi>R</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>R</mi> <mn>1</mn></msub>
    <mo>+</mo> <mo>...</mo> <msub><mi>R</mi> <mi>t</mi></msub> <mo>|</mo> <mi>π</mi>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="max Underscript pi Endscripts upper E left-bracket upper R 0
    plus upper R 1 plus ellipsis upper R Subscript t Baseline vertical-bar pi right-bracket"><mrow><msub><mo
    movablelimits="true" form="prefix">max</mo> <mi>π</mi></msub> <mi>E</mi> <mrow><mo>[</mo>
    <msub><mi>R</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>R</mi> <mn>1</mn></msub>
    <mo>+</mo> <mo>...</mo> <msub><mi>R</mi> <mi>t</mi></msub> <mo>|</mo> <mi>π</mi>
    <mo>]</mo></mrow></mrow></math>
- en: In this objective, R represents the *future return* of each episode. Let’s define
    exactly what future return means.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个目标中，R代表每个情节的*未来回报*。让我们准确定义未来回报的含义。
- en: Future Return
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来回报
- en: Future return is how we consider the rewards of the future. Choosing the best
    action requires consideration of not only the immediate effects of that action,
    but also the long-term consequences. Sometimes the best action actually has a
    negative immediate effect, but a better long-term result. For example, a mountain-climbing
    agent that is rewarded by its altitude may actually have to climb downhill to
    reach a better path to the mountain’s peak.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 未来回报是我们如何考虑未来奖励的方式。选择最佳行动需要考虑该行动的即时效果以及长期后果。有时，最佳行动实际上具有负面的即时效果，但具有更好的长期结果。例如，一个根据海拔高度奖励的登山代理实际上可能必须下山才能到达山顶的更好路径。
- en: Therefore, we want our agents to optimize for *future return*. To do that, the
    agent must consider the future consequences of its actions. For example, in a
    game of Pong, the agent receives a reward when the ball passes into the opponent’s
    goal. However, the actions responsible for this reward (the inputs that position
    the racquet to strike a scoring hit) happen many time steps before the reward
    is received. The reward for each of those actions is delayed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望我们的代理人优化*未来回报*。为了做到这一点，代理人必须考虑其行动的未来后果。例如，在乒乓球比赛中，当球传入对手的球门时，代理人会获得奖励。然而，导致这种奖励的行动（使球拍定位以打出得分击球的输入）发生在获得奖励之前的许多时间步之前。每个行动的奖励都是延迟的。
- en: 'We can incorporate delayed rewards into our overall reward signal by constructing
    a *return* for each time step that takes into account future rewards as well as
    immediate rewards. A naive approach for calculating *future return* for a time
    step may be a simple sum like so:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过构建每个时间步的*回报*来将延迟奖励纳入我们的整体奖励信号中，该回报考虑了未来奖励以及即时奖励。计算一个时间步的*未来回报*的一个天真的方法可能是一个简单的总和，如下所示：
- en: <math alttext="upper R Subscript t Baseline equals sigma-summation Underscript
    k equals 0 Overscript upper T Endscripts r Subscript t plus k"><mrow><msub><mi>R</mi>
    <mi>t</mi></msub> <mo>=</mo> <mrow><munderover><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>T</mi></munderover> <msub><mi>r</mi> <mrow><mi>t</mi><mo>+</mo><mi>k</mi></mrow></msub></mrow></mrow></math>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R Subscript t Baseline equals sigma-summation Underscript
    k equals 0 Overscript upper T Endscripts r Subscript t plus k"><mrow><msub><mi>R</mi>
    <mi>t</mi></msub> <mo>=</mo> <mrow><munderover><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>T</mi></munderover> <msub><mi>r</mi> <mrow><mi>t</mi><mo>+</mo><mi>k</mi></mrow></msub></mrow></mrow></math>
- en: 'We can calculate all returns, R, where  <math alttext="upper R equals StartSet
    upper R 0 comma upper R 1 comma ellipsis upper R Subscript i Baseline comma ellipsis
    upper R Subscript n Baseline EndSet"><mrow><mi>R</mi> <mo>=</mo> <mo>{</mo> <msub><mi>R</mi>
    <mn>0</mn></msub> <mo>,</mo> <msub><mi>R</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo>
    <msub><mi>R</mi> <mi>i</mi></msub> <mo>,</mo> <mo>...</mo> <msub><mi>R</mi> <mi>n</mi></msub>
    <mo>}</mo></mrow></math> , with the following code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算所有回报R，其中R={R0，R1，...，Ri，...，Rn}，使用以下代码：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This naive approach successfully incorporates future rewards so the agent can
    learn an optimal global policy. This approach values future rewards equally to
    immediate rewards. However, this equal consideration of all rewards is problematic.
    With infinite time steps, this expression can diverge to infinity, so we must
    find a way to bind it. Furthermore, with equal consideration at each time step,
    the agent can optimize for a future reward, and we would learn a policy that lacks
    any sense of urgency or time sensitivity in pursuing its rewards.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种天真的方法成功地将未来奖励纳入，以便代理人可以学习一个最优的全局策略。这种方法将未来奖励与即时奖励同等看待。然而，对所有奖励的平等考虑是有问题的。在无限时间步长下，这个表达式可能会发散到无穷大，因此我们必须找到一种方法来限制它。此外，在每个时间步长上平等考虑，代理人可以优化未来奖励，我们将学习到一个缺乏紧迫感或时间敏感性的策略。
- en: Instead, we should value future rewards slightly less in order to force our
    agents to learn to get rewards quickly. We accomplish this with a strategy called
    *discounted future return.*
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们应该稍微低估未来奖励，以便迫使我们的代理人学会快速获得奖励。我们通过一种称为*折现未来回报*的策略来实现这一点。
- en: Discounted Future Return
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折现未来回报
- en: 'To implement discounted future return, we scale the reward of a current state
    by the discount factor, <math alttext="gamma"><mi>γ</mi></math> , to the power
    of the current time step. In this way, we penalize agents that take many actions
    before receiving positive reward. Discounted rewards bias our agent to prefer
    receiving the reward in the immediate future, which is advantageous to learning
    a good policy. We can express the reward as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现折现未来回报，我们将当前状态的奖励按照折现因子γ的当前时间步幂进行缩放。通过这种方式，我们惩罚那些在获得正面奖励之前采取许多行动的代理人。折现奖励使我们的代理人倾向于更喜欢在即时未来获得奖励，这有利于学习一个良好的策略。我们可以将奖励表达如下：
- en: <math alttext="upper R Subscript t Baseline equals sigma-summation Underscript
    k equals 0 Overscript upper T Endscripts gamma Superscript t Baseline r Subscript
    t plus k plus 1"><mrow><msub><mi>R</mi> <mi>t</mi></msub> <mo>=</mo> <mrow><munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></munderover> <mrow><msup><mi>γ</mi>
    <mi>t</mi></msup> <msub><mi>r</mi> <mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow></math>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R Subscript t Baseline equals sigma-summation Underscript
    k equals 0 Overscript upper T Endscripts gamma Superscript t Baseline r Subscript
    t plus k plus 1"><mrow><msub><mi>R</mi> <mi>t</mi></msub> <mo>=</mo> <mrow><munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></munderover> <mrow><msup><mi>γ</mi>
    <mi>t</mi></msup> <msub><mi>r</mi> <mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow></math>
- en: The discount factor,  <math alttext="gamma"><mi>γ</mi></math> , represents the
    level of discounting we want to achieve, and can be between 0 and 1\. High  <math
    alttext="gamma"><mi>γ</mi></math>  means little discounting, low <math alttext="gamma"><mi>γ</mi></math>
     provides much discounting. A typical <math alttext="gamma"><mi>γ</mi></math>
     hyperparameter setting is between 0.99 and 0.97.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 折现因子，γ，代表我们希望实现的折现水平，可以介于0和1之间。高γ意味着很少折现，低γ提供了很多折现。典型的γ超参数设置在0.99和0.97之间。
- en: 'We can implement discounted return like so:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样实现折现回报：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Explore Versus Exploit
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索与利用
- en: Reinforcement learning is fundamentally a trial-and-error process. In such a
    framework, an agent afraid to make mistakes can prove to be highly problematic.
    Consider the following scenario. A mouse is placed in the maze shown in [Figure 13-5](#predicament_for_many_mice).
    Our agent must control the mouse to maximize reward. If the mouse gets the water,
    it receives a reward of +1; if the mouse reaches a poison container (red), it
    receives a reward of -10; if the mouse gets the cheese, it receives a reward of
    +100\. Upon receiving reward, the episode is over. The optimal policy involves
    the mouse successfully navigating to the cheese and eating it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习基本上是一个反复试验的过程。在这样的框架中，一个害怕犯错误的代理人可能会变得非常问题。考虑以下情景。一个老鼠被放置在迷宫中，我们的代理人必须控制老鼠以最大化奖励。如果老鼠喝到水，它会获得+1的奖励；如果老鼠到达毒物容器（红色），它会获得-10的奖励；如果老鼠得到奶酪，它会获得+100的奖励。一旦获得奖励，该情节就结束了。最佳策略涉及老鼠成功地导航到奶酪并吃掉它。
- en: '![](Images/fdl2_1305.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1305.png)'
- en: Figure 13-5\. A predicament that many mice find themselves in
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-5。许多老鼠发现自己陷入的困境。
- en: In the first episode, the mouse takes the left route, steps on a trap, and receives
    a -10 reward. In the second episode, the mouse avoids the left path, since it
    resulted in such a negative reward, and drinks the water immediately to its right
    for a +1 reward. After two episodes, it would seem that the mouse has found a
    good policy. It continues to follow its learned policy on subsequent episodes
    and achieves the moderate +1 reward reliably. Since our agent utilizes a greedy
    strategy—always choosing the model’s best action—it is stuck in a policy that
    is a *local maximum*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一集中，老鼠选择了左侧路线，踩到了陷阱，获得了-10的奖励。在第二集中，老鼠避开了左侧路径，因为它导致了如此负面的奖励，并立即喝了右侧的水，获得了+1的奖励。经过两集，老鼠似乎找到了一个好的策略。它继续在后续集中遵循其学到的策略，并可靠地获得适度的+1奖励。由于我们的代理采用了贪婪策略——始终选择模型的最佳动作——它被困在了一个*局部最大值*的策略中。
- en: To prevent such a situation, it may be useful for the agent to deviate from
    the model’s recommendation and take a suboptimal action in order to *explore*
    more of the environment. So instead of taking the immediate right turn to *exploit*
    the environment to get water and the reliable +1 reward, our agent may choose
    to take a left turn and venture into more treacherous areas in search of a more
    optimal policy. Too much exploration, and our agent fails to optimize any reward.
    Not enough exploration can result in our agent getting stuck in a local minimum.
    This balance of *explore versus exploit* is crucial to learning a successful policy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，代理可能有必要偏离模型的建议，采取次优动作以便更多地*探索*环境。因此，我们的代理可能选择向左转而不是立即向右转以*利用*环境获取水和可靠的+1奖励，而是冒险进入更危险的区域寻找更优的策略。过多的探索会导致我们的代理无法优化任何奖励。探索不足可能导致我们的代理陷入局部最小值。*探索与利用*的这种平衡对于学习成功的策略至关重要。
- en: <math alttext="epsilon"><mi>ϵ</mi></math> -Greedy
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ε-Greedy
- en: One strategy for balancing the explore-exploit dilemma is called *<math alttext="epsilon"><mi>ϵ</mi></math>
    -greedy*. <math alttext="epsilon"><mi>ϵ</mi></math> -greedy is a simple strategy
    that involves making a choice at each step to either take the agent’s top recommended
    action or take a random action. The probability that the agent takes a random
    action is the value known as  <math alttext="epsilon"><mi>ϵ</mi></math> .
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡探索和利用困境的一种策略被称为*ε-greedy*。ε-greedy是一种简单的策略，每一步都要做出选择，要么采取代理的顶级推荐动作，要么采取随机动作。代理采取随机动作的概率是称为ε的值。
- en: 'We can implement <math alttext="epsilon"><mi>ϵ</mi></math> -greedy like so:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样实现ε-greedy：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Annealed <math alttext="epsilon"><mi>ϵ</mi></math> -Greedy
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 退火ε-Greedy
- en: 'When training a reinforcement learning model, oftentimes we want to do more
    exploring in the beginning since our model knows little of the world. Later, once
    our model has seen much of the environment and learned a good policy, we want
    our agent to trust itself more to further optimize its policy. To accomplish this,
    we cast aside the idea of a fixed  <math alttext="epsilon"><mi>ϵ</mi></math> ,
    and instead anneal it over time, having it start low and increase by a factor
    after each training episode. Typical settings for annealed <math alttext="epsilon"><mi>ϵ</mi></math>
    -greedy scenarios include annealing from 0.99 to 0.1 over 10,000 scenarios. We
    can implement annealing like so:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练强化学习模型时，通常我们希望在开始时进行更多的探索，因为我们的模型对世界知之甚少。后来，一旦我们的模型看到了环境的大部分并学到了一个好的策略，我们希望我们的代理更加信任自己以进一步优化其策略。为了实现这一点，我们放弃了固定的ε的概念，而是随着时间逐渐降低它，使其从低值开始，并在每次训练集之后增加一个因子。典型的退火ε-greedy场景设置包括在10,000个场景中从0.99退火到0.1。我们可以这样实现退火：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Policy Versus Value Learning
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略学习与值学习
- en: 'So far we’ve defined the setup of reinforcement learning, discussed discounted
    future return, and looked at the trade-offs of explore versus exploit. What we
    haven’t talked about is how we’re actually going to teach an agent to maximize
    its reward. Approaches to this fall into two broad categories: *policy learning*
    and *value learning*. In policy learning, we are directly learning a policy that
    maximizes reward. In value learning, we are learning the value of every state
    + action pair. If you were trying to learn to ride a bike, a policy learning approach
    would be to think about how pushing on the right pedal while you were falling
    to the left would course-correct you. If you were trying to learn to ride a bike
    with a value learning approach, you would assign a score to different bike orientations
    and actions you can take in those positions. We’ll be covering both in this chapter,
    so let’s start with policy learning.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经定义了强化学习的设置，讨论了折现未来回报，并看了探索与利用的权衡。我们还没有讨论的是我们实际上将如何教导代理最大化其奖励。对此的方法分为两大类：*策略学习*和*值学习*。在策略学习中，我们直接学习最大化奖励的策略。在值学习中，我们学习每个状态+动作对的值。如果你试图学会骑自行车，策略学习方法是考虑在你向左倾斜时如何踩右脚踏板来纠正你的方向。如果你试图用值学习方法学会骑自行车，你会为不同的自行车方向和你可以在这些位置采取的行动分配一个分数。我们将在本章中涵盖这两种方法，所以让我们从策略学习开始。
- en: 'In typical supervised learning, we can use stochastic gradient descent to update
    our parameters to minimize the loss computed from our network’s output and the
    true label. We are optimizing the expression:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的监督学习中，我们可以使用随机梯度下降来更新我们的参数，以最小化从网络输出和真实标签计算出的损失。我们正在优化表达式：
- en: <math alttext="arg min Underscript theta Endscripts sigma-summation Underscript
    i Endscripts log p left-parenthesis y Subscript i Baseline bar x Subscript i Baseline
    semicolon theta right-parenthesis"><mrow><mo form="prefix">arg</mo> <msub><mo
    movablelimits="true" form="prefix">min</mo> <mi>θ</mi></msub> <msub><mo>∑</mo>
    <mi>i</mi></msub> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>∣</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>;</mo> <mi>θ</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="arg min Underscript theta Endscripts sigma-summation Underscript
    i Endscripts log p left-parenthesis y Subscript i Baseline bar x Subscript i Baseline
    semicolon theta right-parenthesis"><mrow><mo form="prefix">arg</mo> <msub><mo
    movablelimits="true" form="prefix">min</mo> <mi>θ</mi></msub> <msub><mo>∑</mo>
    <mi>i</mi></msub> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>∣</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>;</mo> <mi>θ</mi>
    <mo>)</mo></mrow></mrow></math>
- en: 'In reinforcement learning, we don’t have a true label, only reward signals.
    However, we can still use SGD to optimize our weights using something called *policy
    gradients*.^([4](ch13.xhtml#idm45934166621248)) We can use the actions the agent
    takes, and the returns associated with those actions, to encourage our model weights
    to take good actions that lead to high reward, and to avoid bad ones that lead
    to low reward. The expression we optimize for is:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们没有真正的标签，只有奖励信号。然而，我们仍然可以使用SGD来优化我们的权重，使用一种称为*策略梯度*的方法。我们可以使用代理所采取的动作以及与这些动作相关的回报，来鼓励我们的模型权重采取导致高奖励的良好动作，并避免导致低奖励的不良动作。我们优化的表达式是：
- en: <math alttext="arg min Underscript theta Endscripts minus sigma-summation Underscript
    i Endscripts upper R Subscript i Baseline log p left-parenthesis y Subscript i
    Baseline bar x Subscript i Baseline semicolon theta right-parenthesis"><mrow><mo
    form="prefix">arg</mo> <msub><mo movablelimits="true" form="prefix">min</mo> <mi>θ</mi></msub>
    <mo>-</mo> <msub><mo>∑</mo> <mi>i</mi></msub> <msub><mi>R</mi> <mi>i</mi></msub>
    <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>∣</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="arg min Underscript theta Endscripts minus sigma-summation Underscript
    i Endscripts upper R Subscript i Baseline log p left-parenthesis y Subscript i
    Baseline bar x Subscript i Baseline semicolon theta right-parenthesis"><mrow><mo
    form="prefix">arg</mo> <msub><mo movablelimits="true" form="prefix">min</mo> <mi>θ</mi></msub>
    <mo>-</mo> <msub><mo>∑</mo> <mi>i</mi></msub> <msub><mi>R</mi> <mi>i</mi></msub>
    <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>∣</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
- en: where <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math>
    is the action taken by the agent at time step <math alttext="t"><mi>t</mi></math>
    and where <math alttext="upper R Subscript i"><msub><mi>R</mi> <mi>i</mi></msub></math>
     is our discounted future return. A In this way, we scale our loss by the value
    of our return, so if the model chose an action that led to negative return, this
    would lead to greater loss. Furthermore, if the model is confident in that bad
    decision, it would get penalized even more, since we are taking into account the
    log probability of the model choosing that action. With our loss function defined,
    we can apply SGD to minimize our loss and learn a good policy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math>是代理在时间步<math
    alttext="t"><mi>t</mi></math>采取的动作，<math alttext="upper R Subscript i"><msub><mi>R</mi>
    <mi>i</mi></msub></math>是我们的折现未来回报。通过这种方式，我们通过我们的回报值来缩放我们的损失，因此，如果模型选择导致负回报的动作，这将导致更大的损失。此外，如果模型对该错误决策有信心，它将受到更严厉的惩罚，因为我们考虑了模型选择该动作的对数概率。有了我们定义的损失函数，我们可以应用SGD来最小化我们的损失并学习一个良好的策略。
- en: Pole-Cart with Policy Gradients
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用策略梯度的Pole-Cart
- en: We’re going to implement a policy-gradient agent to solve pole-cart, a classic
    reinforcement learning problem. We will be using an environment from the OpenAI
    Gym created just for this task.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个策略梯度代理来解决pole-cart，这是一个经典的强化学习问题。我们将使用OpenAI Gym为此任务专门创建的环境。
- en: OpenAI Gym
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI Gym
- en: The OpenAI Gym is a Python toolkit for developing reinforcement agents. OpenAI
    Gym provides an easy-to-use interface for interacting with a variety of environments.
    It contains over one hundred open source implementations of common reinforcement
    learning environments. OpenAI Gym speeds up development of reinforcement learning
    agents by handling everything on the environment simulation side, allowing researchers
    to focus on their agent and learning algorithms. Another benefit of OpenAI Gym
    is that researchers can fairly compare and evaluate their results with others
    because they can all use the same standardized environment for a task. We’ll be
    using the pole-cart environment from OpenAI Gym to create an agent that can easily
    interact with this environment.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym是一个用于开发强化学习代理的Python工具包。OpenAI Gym提供了一个易于使用的接口，用于与各种环境进行交互。它包含了一百多个常见强化学习环境的开源实现。OpenAI
    Gym通过处理环境模拟方面的一切来加速强化学习代理的开发，使研究人员可以专注于他们的代理和学习算法。OpenAI Gym的另一个好处是，研究人员可以公平地比较和评估他们的结果，因为他们都可以使用相同的标准化环境来执行任务。我们将使用OpenAI
    Gym中的pole-cart环境来创建一个可以轻松与该环境交互的代理。
- en: Creating an Agent
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建代理
- en: 'To create an agent that can interact with an OpenAI environment, we’ll define
    a class `PGAgent`, which will contain our model architecture, model weights, and
    hyperparameters:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个可以与OpenAI环境交互的代理，我们将定义一个名为`PGAgent`的类，其中包含我们的模型架构、模型权重和超参数：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Building the Model and Optimizer
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建模型和优化器
- en: Let’s break down some important functions. In `build_model()`, we define our
    model architecture as a three-layer neural network. The model returns a layer
    of three nodes, each representing the model’s action probability distribution.
    In `build_training()`, we implement our policy gradient optimizer. We express
    our objective loss as we talked about, scaling the model’s prediction probability
    for an action with the return received for taking that action, and summing these
    all up to form a minibatch. With our objective defined, we can use `torch.optim.AdamOptimizer`,
    which will adjust our weights according to the gradient to minimize our loss.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一些重要的函数。在`build_model()`中，我们将我们的模型架构定义为一个三层神经网络。模型返回一个包含三个节点的层，每个节点代表模型的动作概率分布。在`build_training()`中，我们实现了我们的策略梯度优化器。我们表达了我们的目标损失，如我们所讨论的，通过将模型对动作的预测概率与采取该动作获得的回报进行缩放，并将所有这些相加形成一个小批量。有了我们定义的目标，我们可以使用`torch.optim.AdamOptimizer`，它将根据梯度调整我们的权重以最小化我们的损失。
- en: Sampling Actions
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采样动作
- en: We define the `predict_action` function, which samples an action based on the
    model’s action probability distribution output. We support the various sampling
    strategies that we talked about to balance explore versus exploit, including greedy,
    ϵ greedy, and ϵ greedy annealing.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了`predict_action`函数，根据模型的动作概率分布输出对动作进行采样。我们支持我们讨论过的各种采样策略，以平衡探索与利用，包括贪婪、ϵ贪婪和ϵ贪婪退火。
- en: Keeping Track of History
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪历史
- en: 'We’ll be aggregating our gradients from multiple episode runs, so it will be
    useful to keep track of state, action, and reward tuples. To this end, we implement
    an episode history and memory:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从多个情节运行中聚合我们的梯度，因此跟踪状态、动作和奖励元组将非常有用。为此，我们实现了一个情节历史和记忆：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Policy Gradient Main Function
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略梯度主要函数
- en: 'Let’s put this all together in our main function, which will create an OpenAI
    Gym environment for CartPole, make an instance of our agent, and have our agent
    interact with and train on the CartPole environment:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的主函数中将所有这些放在一起，该函数将为CartPole创建一个OpenAI Gym环境，创建我们的代理的一个实例，并让我们的代理与CartPole环境进行交互和训练：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code will train a CartPole agent to successfully and consistently balance
    the pole.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将训练一个CartPole代理成功并持续地平衡杆。
- en: PGAgent Performance on Pole-Cart
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PGAgent在Pole-Cart上的表现
- en: '[Figure 13-6](#explore_exploit_configurations) is a chart of the average reward
    of our agent at each step of training. We try out 8 different sampling methods,
    and achieve best results with ϵ greedy annealing from 1.0 to 0.001.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-6](#explore_exploit_configurations)是我们代理在每个训练步骤中的平均奖励的图表。我们尝试了8种不同的采样方法，并通过从1.0到0.001的ϵ贪婪退火获得了最佳结果。'
- en: '![](Images/fdl2_1306.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1306.png)'
- en: Figure 13-6\. Explore-exploit configurations affect how fast and how well learning
    occurs
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-6。探索-利用配置影响学习的速度和效果如何
- en: Notice how, across the board, standard ϵ greedy does very poorly. Let’s talk
    about why this might be. With a high ϵ set to 0.9, we are taking a random action
    90% of the time. Even if the model learns to execute the perfect actions, we’ll
    still be using these only 10% of the time. On the other end, with a low ϵ of 0.05,
    we are taking what our model believes to be optimal actions the vast majority
    of the time. This performance is a bit better, but gets stuck in a local reward
    minimum because it lacks the ability to explore other strategies. So neither ϵ
    greedy of 0.05 nor 0.9 gives us great results. The former places too much emphasis
    on exploration, and the latter, too little. This is why ϵ annealing is such a
    powerful sampling strategy. It allows the model to explore early and exploit late,
    which is crucial to learning good policies.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从整体上看，标准的ϵ贪婪表现非常糟糕。让我们讨论一下可能的原因。当ϵ设置为0.9时，我们90%的时间会随机采取行动。即使模型学会执行完美的动作，我们仍然只会使用这些动作的10%。另一方面，当ϵ为0.05时，我们大部分时间会采取模型认为是最佳动作。这种表现稍微好一些，但会陷入局部奖励最小值，因为它缺乏探索其他策略的能力。因此，ϵ为0.05或0.9都不能给我们带来很好的结果。前者过于强调探索，后者则太少。这就是为什么ϵ退火是如此强大的采样策略。它允许模型早期探索，晚期利用，这对学习良好策略至关重要。
- en: Trust-Region Policy Optimization
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信任区域策略优化
- en: '*Trust-region policy optimization*, or *TRPO* for short, is a framework that
    ensures policy improvement while preventing the policy from shifting too much
    during each training step. TRPO has been empirically shown to outperform many
    of its fellow policy gradient and policy iteration methods, allowing researchers
    to effectively learn complex, nonlinear policies (often parametrized by large
    neural networks) that weren’t previously possible through gradient-based methods.
    In this section, we will motivate TRPO and describe its objective in more detail.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*信任区域策略优化*，简称*TRPO*，是一个框架，确保在每次训练步骤中防止策略发生过大变化的同时实现策略改进。经验表明，TRPO在许多政策梯度和政策迭代方法中表现优异，使研究人员能够有效地学习复杂的、非线性的策略（通常由大型神经网络参数化），这是以前通过梯度方法无法实现的。在本节中，我们将介绍TRPO并更详细地描述其目标。'
- en: The idea of preventing the policy from shifting too much during each training
    step is not a new one—most regularized optimization procedures do this indirectly
    by penalizing the norm of the parameters, for example, globally ensuring the norm
    of the parameters doesn’t get too high. Of course, in cases where regularized
    optimization can also be formulated as constrained optimization (where there are
    explicit bounds on the norm of the parameter vector), such as L2-regularized linear
    regression, we have a direct equivalence to the idea of preventing the policy
    from shifting too much during each training step. The per-step change in the norm
    of the parameters is bounded by the range of the constraint, since all possible
    parameter values must fall in this range. For those interested, I would recommend
    looking further into the equivalence between Tikhonov and Ivanov regularization
    in linear regression.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次训练步骤中防止策略发生过大变化的想法并不新颖——大多数正则化优化程序通过惩罚参数的范数间接实现这一点，例如，全局确保参数的范数不会过高。当然，在正则化优化也可以被表述为约束优化的情况下（其中参数向量的范数有明确的界限，例如，L2正则化线性回归），我们直接等价于在每次训练步骤中防止策略发生过大变化的想法。参数范数的每步变化受到约束范围的限制，因为所有可能的参数值必须落在这个范围内。对于那些感兴趣的人，我建议进一步研究线性回归中Tikhonov和Ivanov正则化之间的等价性。
- en: 'Preventing the policy from shifting too much during each training step has
    the standard effect of regularized optimization: it promotes stability in training,
    which is ideal in preventing overfitting to new data. How do we define a shift
    in the policy? Policies are simply discrete probability distributions over the
    action space given a state,  <math alttext="pi Subscript theta Baseline left-parenthesis
    a vertical-bar s right-parenthesis"><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math> , for which we
    can use notions of dissimilarity, introduced in [Chapter 2](ch02.xhtml#fundamentals-of-proba).
    The original TRPO paper introduced a bound on the average KL divergence (over
    all possible states) between the current policy and the new policy.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次训练步骤中防止策略发生过大变化具有正则化优化的标准效果：它促进训练的稳定性，这对于防止过拟合到新数据是理想的。我们如何定义策略的变化？策略只是给定状态的动作空间上的离散概率分布，<math
    alttext="pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>，我们可以使用在[第2章](ch02.xhtml#fundamentals-of-proba)中介绍的不相似性概念。原始的TRPO论文介绍了当前策略和新策略之间的平均KL散度（在所有可能的状态上）的界限。
- en: Now that we’ve introduced the constraint portion of TRPO’s constrained optimization,
    we will motivate and define the objective function.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了TRPO约束优化的约束部分，我们将激励并定义目标函数。
- en: 'Let’s recap and introduce some terminology:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾并介绍一些术语：
- en: <math alttext="eta left-parenthesis pi right-parenthesis equals"><mrow><mi>η</mi>
    <mo>(</mo> <mi>π</mi> <mo>)</mo> <mo>=</mo></mrow></math>   <math alttext="double-struck
    upper E Subscript s 0 comma a 0 comma s 1 comma a 1 comma ellipsis Baseline left-bracket
    sigma-summation Underscript t equals 0 Overscript normal infinity Endscripts gamma
    Superscript t Baseline r left-parenthesis s Subscript t Baseline right-parenthesis
    right-bracket"><mrow><msub><mi>𝔼</mi> <mrow><msub><mi>s</mi> <mn>0</mn></msub>
    <mo>,</mo><msub><mi>a</mi> <mn>0</mn></msub> <mo>,</mo><msub><mi>s</mi> <mn>1</mn></msub>
    <mo>,</mo><msub><mi>a</mi> <mn>1</mn></msub> <mo>,</mo><mo>...</mo></mrow></msub>
    <mrow><mo>[</mo> <msubsup><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>t</mi></msup> <mi>r</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="eta 左括号 pi 右括号 等于"><mrow><mi>η</mi> <mo>(</mo> <mi>π</mi> <mo>)</mo>
    <mo>=</mo></mrow></math>   <math alttext="双划线上标 E 下标 s 0 逗号 a 0 逗号 s 1 逗号 a 1
    逗号 省略 基线左方括号 sigma-求和下标 t 等于 0 上标 正无穷大 上标 gamma 上标 t 基线 r 左括号 s 下标 t 右括号 右方括号"><mrow><msub><mi>𝔼</mi>
    <mrow><msub><mi>s</mi> <mn>0</mn></msub> <mo>,</mo><msub><mi>a</mi> <mn>0</mn></msub>
    <mo>,</mo><msub><mi>s</mi> <mn>1</mn></msub> <mo>,</mo><msub><mi>a</mi> <mn>1</mn></msub>
    <mo>,</mo><mo>...</mo></mrow></msub> <mrow><mo>[</mo> <msubsup><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>t</mi></msup> <mi>r</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
- en: <math alttext="upper Q Subscript pi Baseline left-parenthesis s Subscript t
    Baseline comma a Subscript t Baseline right-parenthesis equals"><mrow><msub><mi>Q</mi>
    <mi>π</mi></msub> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo></mrow></math>
      <math alttext="double-struck upper E Subscript s Sub Subscript t plus 1 Subscript
    comma a Sub Subscript t plus 1 Subscript comma ellipsis Baseline left-bracket
    sigma-summation Underscript l equals 0 Overscript normal infinity Endscripts gamma
    Superscript l Baseline r left-parenthesis s Subscript t plus l Baseline right-parenthesis
    right-bracket"><mrow><msub><mi>𝔼</mi> <mrow><msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo><msub><mi>a</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo><mo>...</mo></mrow></msub> <mrow><mo>[</mo> <msubsup><mo>∑</mo> <mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>l</mi></msup> <mi>r</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mi>l</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="上标 Q 下标 π 基线左括号 s 下标 t 逗号 a 下标 t 右括号 等于"><mrow><msub><mi>Q</mi>
    <mi>π</mi></msub> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo></mrow></math>
      <math alttext="双划线上标 E 下标 s 上标 t 加 1 下标 逗号 a 上标 t 加 1 下标 逗号 省略 基线左方括号 sigma-求和下标
    l 等于 0 上标 正无穷大 上标 gamma 上标 l 基线 r 左括号 s 下标 t 加 l 基线 右括号 右方括号"><mrow><msub><mi>𝔼</mi>
    <mrow><msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><msub><mi>a</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><mo>...</mo></mrow></msub>
    <mrow><mo>[</mo> <msubsup><mo>∑</mo> <mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>l</mi></msup> <mi>r</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mi>l</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
- en: <math alttext="upper V Subscript pi Baseline left-parenthesis s Subscript t
    Baseline right-parenthesis"><mrow><msub><mi>V</mi> <mi>π</mi></msub> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>   <math alttext="equals
    double-struck upper E Subscript a Sub Subscript t Subscript comma s Sub Subscript
    t plus 1 Subscript comma a Sub Subscript t plus 1 Subscript ellipsis Baseline
    left-bracket sigma-summation Underscript l equals 0 Overscript normal infinity
    Endscripts gamma Superscript l Baseline r left-parenthesis s Subscript t plus
    l Baseline right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi>
    <mrow><msub><mi>a</mi> <mi>t</mi></msub> <mo>,</mo><msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo><msub><mi>a</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>...</mo></mrow></msub> <mrow><mo>[</mo> <msubsup><mo>∑</mo> <mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>l</mi></msup> <mi>r</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mi>l</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="上标 V 下标 π 基线左括号 s 下标 t 右括号"><mrow><msub><mi>V</mi> <mi>π</mi></msub>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
      <math alttext="等于双划线上标 E 下标 a 上标 t 下标 逗号 s 下标 t 加 1 下标 逗号 a 下标 t 加 1 省略 基线左方括号
    sigma-求和下标 l 等于 0 上标 正无穷大 上标 gamma 上标 l 基线 r 左括号 s 下标 t 加 l 基线 右括号 右方括号"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><msub><mi>a</mi> <mi>t</mi></msub> <mo>,</mo><msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><msub><mi>a</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>...</mo></mrow></msub>
    <mrow><mo>[</mo> <msubsup><mo>∑</mo> <mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>l</mi></msup> <mi>r</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mi>l</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
- en: <math alttext="upper A Subscript pi Baseline left-parenthesis s comma a right-parenthesis
    equals"><mrow><msub><mi>A</mi> <mi>π</mi></msub> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo>
    <mi>a</mi> <mo>)</mo></mrow> <mo>=</mo></mrow></math>   <math alttext="upper Q
    Subscript pi Baseline left-parenthesis s comma a right-parenthesis minus upper
    V Subscript pi Baseline left-parenthesis s right-parenthesis"><mrow><msub><mi>Q</mi>
    <mi>π</mi></msub> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>-</mo> <msub><mi>V</mi> <mi>π</mi></msub> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="上标 A 下标 π 基线左括号 s 逗号 a 右括号 等于"><mrow><msub><mi>A</mi> <mi>π</mi></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>=</mo></mrow></math>
      <math alttext="上标 Q 下标 π 基线左括号 s 逗号 a 右括号 减上标 V 下标 π 基线左括号 s 右括号"><mrow><msub><mi>Q</mi>
    <mi>π</mi></msub> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>-</mo> <msub><mi>V</mi> <mi>π</mi></msub> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="rho Subscript pi Baseline left-parenthesis s right-parenthesis
    equals sigma-summation Underscript i equals 0 Overscript normal infinity Endscripts
    gamma Superscript i Baseline upper P left-parenthesis s Subscript t Baseline equals
    s right-parenthesis"><mrow><msub><mi>ρ</mi> <mi>π</mi></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>i</mi></msup> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>=</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="rho Subscript pi Baseline left-parenthesis s right-parenthesis
    equals sigma-summation Underscript i equals 0 Overscript normal infinity Endscripts
    gamma Superscript i Baseline upper P left-parenthesis s Subscript t Baseline equals
    s right-parenthesis"><mrow><msub><mi>ρ</mi> <mi>π</mi></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>i</mi></msup> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>=</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
- en: The first term is <math alttext="eta left-parenthesis pi right-parenthesis"><mrow><mi>η</mi>
    <mo>(</mo> <mi>π</mi> <mo>)</mo></mrow></math> , which represents the *expected
    discounted reward*. We saw the finite-time horizon version of the term inside
    the expectation earlier when we discussed future discounted reward. Instead of
    looking at a single trajectory here, we take the expectation over all possible
    trajectories as defined by our policy <math alttext="pi"><mi>π</mi></math> . As
    usual, we can estimate this expectation via an empirical average by sampling trajectories
    using <math alttext="pi"><mi>π</mi></math> . The second term, which will be discussed
    in more detail in [“Q-Learning and Deep Q-Networks”](#q-learning-sect), is the
    *Q-function*  <math alttext="upper Q Subscript pi Baseline left-parenthesis s
    Subscript t Baseline comma a Subscript t Baseline right-parenthesis"><mrow><msub><mi>Q</mi>
    <mi>π</mi></msub> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math> , which looks
    very similar to the previous term but is instead defined as the expected discounted
    return from time *t*, given we are in some state  <math alttext="s Subscript t"><msub><mi>s</mi>
    <mi>t</mi></msub></math>  and perform a defined action  <math alttext="a Subscript
    t"><msub><mi>a</mi> <mi>t</mi></msub></math>  in that state. We again calculate
    the expectation using our policy  <math alttext="pi"><mi>π</mi></math> . Note
    that the time *t *doesn’t actually matter all too much since we only consider
    an infinite time horizon and the expected discounted return from *t *rather than
    from the beginning of the trajectory.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个术语是表示*预期折扣奖励*的<math alttext="eta left-parenthesis pi right-parenthesis"><mrow><mi>η</mi>
    <mo>(</mo> <mi>π</mi> <mo>)</mo></mrow></math>，我们在之前讨论未来折扣奖励时已经看到了该术语的有限时间视角版本。在这里，我们不再只看一个单一轨迹，而是通过我们的策略<math
    alttext="pi"><mi>π</mi></math>定义的所有可能轨迹上的期望。通常情况下，我们可以通过使用<math alttext="pi"><mi>π</mi></math>采样轨迹来估计这个期望。第二个术语是*Q函数*
    <math alttext="upper Q Subscript pi Baseline left-parenthesis s Subscript t Baseline
    comma a Subscript t Baseline right-parenthesis"><mrow><msub><mi>Q</mi> <mi>π</mi></msub>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>，将在[“Q学习和深度Q网络”](#q-learning-sect)中更详细地讨论，它看起来与前一个术语非常相似，但实际上被定义为在时间*t*时的预期折扣回报，假设我们处于某个状态<math
    alttext="s Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math>并在该状态执行一个定义好的动作<math
    alttext="a Subscript t"><msub><mi>a</mi> <mi>t</mi></msub></math>。我们再次使用我们的策略<math
    alttext="pi"><mi>π</mi></math>来计算期望。请注意，时间*t*实际上并不太重要，因为我们只考虑无限时间视角，并且从*t*时刻的预期折扣回报而不是从轨迹开始处计算。
- en: The third term is  <math alttext="upper V Subscript pi Baseline left-parenthesis
    s Subscript t Baseline right-parenthesis"><mrow><msub><mi>V</mi> <mi>π</mi></msub>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
    , or the *value function* at a particular state at time *t*. The value function
    can actually be more concisely written as <math alttext="upper V Subscript pi
    Baseline left-parenthesis s Subscript t Baseline right-parenthesis equals double-struck
    upper E Subscript a Sub Subscript t Baseline left-bracket upper Q Subscript pi
    Baseline left-parenthesis s Subscript t Baseline comma a Subscript t Baseline
    right-parenthesis right-bracket"><mrow><msub><mi>V</mi> <mi>π</mi></msub> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi>
    <msub><mi>a</mi> <mi>t</mi></msub></msub> <mrow><mo>[</mo> <msub><mi>Q</mi> <mi>π</mi></msub>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math> , or the expectation
    of the Q-function with respect to <math alttext="pi left-parenthesis a Subscript
    t Baseline vertical-bar s Subscript t Baseline right-parenthesis"><mrow><mi>π</mi>
    <mo>(</mo> <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>s</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow></math> . In essence, the Q-function supposes that we take a
    defined action <math alttext="a Subscript t"><msub><mi>a</mi> <mi>t</mi></msub></math>
    in state <math alttext="s Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math>
    , while the value function leaves <math alttext="a Subscript t"><msub><mi>a</mi>
    <mi>t</mi></msub></math> as a variable. Thus, to get the value function, all we
    need to do is take the expectation of the Q-function with respect to the distribution
    over <math alttext="a Subscript t"><msub><mi>a</mi> <mi>t</mi></msub></math>  knowing
    the current state <math alttext="s Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math>
    . The result is the weighted average of the Q-function, where the weights are
    <math alttext="pi left-parenthesis a Subscript t Baseline vertical-bar s Subscript
    t Baseline right-parenthesis"><mrow><mi>π</mi> <mo>(</mo> <msub><mi>a</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></math> . In essence,
    this term captures the average future discounted return we’d expect to see starting
    in some state <math alttext="s Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math>
    .
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 第三项是<math alttext="upper V Subscript pi Baseline left-parenthesis s Subscript
    t Baseline right-parenthesis"><mrow><msub><mi>V</mi> <mi>π</mi></msub> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>，或者在特定时间*t*的特定状态的*值函数*。值函数实际上可以更简洁地写为<math
    alttext="upper V Subscript pi Baseline left-parenthesis s Subscript t Baseline
    right-parenthesis equals double-struck upper E Subscript a Sub Subscript t Baseline
    left-bracket upper Q Subscript pi Baseline left-parenthesis s Subscript t Baseline
    comma a Subscript t Baseline right-parenthesis right-bracket"><mrow><msub><mi>V</mi>
    <mi>π</mi></msub> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>𝔼</mi> <msub><mi>a</mi> <mi>t</mi></msub></msub> <mrow><mo>[</mo>
    <msub><mi>Q</mi> <mi>π</mi></msub> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub>
    <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>，或者关于<math
    alttext="pi left-parenthesis a Subscript t Baseline vertical-bar s Subscript t
    Baseline right-parenthesis"><mrow><mi>π</mi> <mo>(</mo> <msub><mi>a</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></math>的Q函数的期望。实质上，Q函数假设我们在状态<math
    alttext="s Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math>中采取定义的动作<math
    alttext="a Subscript t"><msub><mi>a</mi> <mi>t</mi></msub></math>，而值函数将<math alttext="a
    Subscript t"><msub><mi>a</mi> <mi>t</mi></msub></math>作为变量。因此，要得到值函数，我们只需要对关于<math
    alttext="a Subscript t"><msub><mi>a</mi> <mi>t</msub></math>的分布的期望，知道当前状态<math
    alttext="s Subscript t"><msub><mi>s</mi> <mi>t</msub></math>。结果是Q函数的加权平均值，其中权重为<math
    alttext="pi left-parenthesis a Subscript t Baseline vertical-bar s Subscript t
    Baseline right-parenthesis"><mrow><mi>π</mi> <mo>(</mo> <msub><mi>a</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></math>。实质上，这一项捕捉了我们期望在某个状态<math
    alttext="s Subscript t"><msub><mi>s</mi> <mi>t</msub></math>开始看到的平均未来折现回报。
- en: The fourth term is <math alttext="upper A Subscript pi Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><msub><mi>A</mi> <mi>π</mi></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math> , or the *advantage
    function*. Note that we have dropped the time *t* by now for the reasons mentioned
    earlier. The intuition for the advantage function is that it quantifies, under
    a fixed policy  <math alttext="pi"><mi>π</mi></math> , the benefit of letting
    trajectories play out after taking a particular action <math alttext="a"><mi>a</mi></math>
     in the current state *s*, over simply letting trajectories play out from the
    current state *s* completely unconstrained. Even more concisely, it defines how
    much better, or worse, in the long run it is to initially take action *a* in state
    *s* compared to the average.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第四项是<math alttext="upper A Subscript pi Baseline left-parenthesis s comma a
    right-parenthesis"><mrow><msub><mi>A</mi> <mi>π</mi></msub> <mrow><mo>(</mo> <mi>s</mi>
    <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>，或者*优势函数*。请注意，出于前面提到的原因，我们现在已经放弃了时间*t*。优势函数的直觉是，在固定策略<math
    alttext="pi"><mi>π</mi></math>下，量化在当前状态*s*中采取特定动作<math alttext="a"><mi>a</mi></math>后让轨迹继续进行的好处，相对于完全不受限制地让轨迹从当前状态*s*继续进行。更简洁地说，它定义了在长期内相对于平均值，最初在状态*s*中采取动作*a*的好坏程度。
- en: 'The final term, or the *unnormalized discounted visitation frequency,* reintroduces
    the time term *t*. This term is a function of the probability of being in state *s*
    at each time *t* from the start to infinity. This term will be important in our
    definition of the objective function. The original TRPO paper chose to optimize
    the model parameters by maximizing this objective function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一项，或者*未归一化的折现访问频率*，重新引入了时间项*t*。这一项是关于从开始到无穷大的每个时间*t*处于状态*s*的概率的函数。这一项将在我们定义目标函数时很重要。原始的TRPO论文选择通过最大化这个目标函数来优化模型参数：
- en: <math alttext="upper L Subscript theta Sub Subscript o l d Baseline left-parenthesis
    theta right-parenthesis equals sigma-summation Underscript s Endscripts rho Subscript
    theta Sub Subscript o l d Baseline left-parenthesis s right-parenthesis sigma-summation
    Underscript a Endscripts pi Subscript theta Baseline left-parenthesis a vertical-bar
    s right-parenthesis upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><msub><mi>L</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mi>s</mi></msub>
    <msub><mi>ρ</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>a</mi></msub>
    <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi>
    <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L Subscript theta Sub Subscript o l d Baseline left-parenthesis
    theta right-parenthesis equals sigma-summation Underscript s Endscripts rho Subscript
    theta Sub Subscript o l d Baseline left-parenthesis s right-parenthesis sigma-summation
    Underscript a Endscripts pi Subscript theta Baseline left-parenthesis a vertical-bar
    s right-parenthesis upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><msub><mi>L</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mi>s</mi></msub>
    <msub><mi>ρ</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>a</mi></msub>
    <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi>
    <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="theta Subscript n e w Baseline equals argmax Subscript theta
    Baseline upper L Subscript theta Sub Subscript o l d Baseline left-parenthesis
    theta right-parenthesis"><mrow><msub><mi>θ</mi> <mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub>
    <mo>=</mo> <msub><mtext>argmax</mtext> <mi>θ</mi></msub> <msub><mi>L</mi> <msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo> <mi>θ</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="theta Subscript n e w Baseline equals argmax Subscript theta
    Baseline upper L Subscript theta Sub Subscript o l d Baseline left-parenthesis
    theta right-parenthesis"><mrow><msub><mi>θ</mi> <mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub>
    <mo>=</mo> <msub><mtext>argmax</mtext> <mi>θ</mi></msub> <msub><mi>L</mi> <msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo> <mi>θ</mi>
    <mo>)</mo></mrow></mrow></math>
- en: 'Although we won’t fully show the derivation behind this objective as it is
    quite mathematically involved and beyond the scope of this text, we provide some
    intuition. Let’s first examine this term: <math alttext="sigma-summation Underscript
    a Endscripts pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis s comma
    a right-parenthesis"><mrow><msub><mo>∑</mo> <mi>a</mi></msub> <msub><mi>π</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
    , assuming a fixed state *s*. For the sake of argument, let’s replace <math alttext="theta"><mi>θ</mi></math>
     with  <math alttext="theta Subscript o l d"><msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></math>
     as our proposed policy’s parameters, which also represents our current policy’s
    parameters:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不会完全展示这个目标背后的推导，因为它在数学上相当复杂，超出了本文的范围，但我们提供一些直觉。让我们首先检查这个术语：<math alttext="sigma-summation
    Underscript a Endscripts pi Subscript theta Baseline left-parenthesis a vertical-bar
    s right-parenthesis upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><msub><mo>∑</mo> <mi>a</mi></msub> <msub><mi>π</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>，假设一个固定的状态
    *s*。为了论证，让我们用我们建议策略的参数 <math alttext="theta Subscript o l d"><msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></math> 替换 <math alttext="theta"><mi>θ</mi></math>，它也代表我们当前策略的参数：
- en: <math alttext="sigma-summation Underscript a Endscripts pi Subscript theta Sub
    Subscript o l d Baseline left-parenthesis a vertical-bar s right-parenthesis upper
    A Subscript pi Sub Subscript theta Sub Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis equals double-struck upper E Subscript a tilde pi
    Sub Subscript theta Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar
    s right-parenthesis Baseline left-bracket upper A Subscript pi Sub Subscript theta
    Sub Sub Subscript o l d Sub Subscript Subscript Baseline left-parenthesis s comma
    a right-parenthesis right-bracket"><mrow><msub><mo>∑</mo> <mi>a</mi></msub> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo>
    <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>A</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="sigma-summation Underscript a Endscripts pi Subscript theta Sub
    Subscript o l d Baseline left-parenthesis a vertical-bar s right-parenthesis upper
    A Subscript pi Sub Subscript theta Sub Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis equals double-struck upper E Subscript a tilde pi
    Sub Subscript theta Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar
    s right-parenthesis Baseline left-bracket upper A Subscript pi Sub Subscript theta
    Sub Sub Subscript o l d Sub Subscript Subscript Baseline left-parenthesis s comma
    a right-parenthesis right-bracket"><mrow><msub><mo>∑</mo> <mi>a</mi></msub> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo>
    <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>A</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
- en: <math alttext="equals double-struck upper E Subscript a tilde pi Sub Subscript
    theta Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper Q Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s comma a right-parenthesis
    right-bracket minus double-struck upper E Subscript a tilde pi Sub Subscript theta
    Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>Q</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>-</mo> <msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals double-struck upper E Subscript a tilde pi Sub Subscript
    theta Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper Q Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s comma a right-parenthesis
    right-bracket minus double-struck upper E Subscript a tilde pi Sub Subscript theta
    Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>Q</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>-</mo> <msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
- en: <math alttext="equals double-struck upper E Subscript a tilde pi Sub Subscript
    theta Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper Q Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s comma a right-parenthesis
    right-bracket minus upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis"><mrow><mo>=</mo> <msub><mi>𝔼</mi>
    <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>Q</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>-</mo> <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals double-struck upper E Subscript a tilde pi Sub Subscript
    theta Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper Q Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s comma a right-parenthesis
    right-bracket minus upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis"><mrow><mo>=</mo> <msub><mi>𝔼</mi>
    <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>Q</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>-</mo> <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis minus upper V Subscript pi
    Sub Subscript theta Sub Sub Subscript o l d Baseline left-parenthesis s right-parenthesis"><mrow><mo>=</mo>
    <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>-</mo> <msub><mi>V</mi> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis minus upper V Subscript pi
    Sub Subscript theta Sub Sub Subscript o l d Baseline left-parenthesis s right-parenthesis"><mrow><mo>=</mo>
    <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>-</mo> <msub><mi>V</mi> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals 0"><mrow><mo>=</mo> <mn>0</mn></mrow></math>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals 0"><mrow><mo>=</mo> <mn>0</mn></mrow></math>
- en: What have we shown here? Earlier, we talked about how <math alttext="upper A
    Subscript pi Sub Subscript theta Sub Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><msub><mi>A</mi> <msub><mi>π</mi> <msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math> defines how much
    better or worse it is, under the current policy, to take action *a* in state *s*
    compared to what we expect to see starting from state *s* unconstrained. Here,
    we showed that if we average each of these advantages weighted by the current
    policy’s distribution, we are left with zero average advantage over the current
    policy—this makes a lot of intuitive sense, since the proposed policy and the
    current policy are the exact same. We don’t expect to see any performance gain
    by replacing the current policy with itself.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里展示了什么？早些时候，我们谈到了 <math alttext="upper A Subscript pi Sub Subscript theta
    Sub Sub Subscript o l d Baseline left-parenthesis s comma a right-parenthesis"><mrow><msub><mi>A</mi>
    <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
    定义了在当前策略下，采取动作 *a* 在状态 *s* 中相对于我们期望从状态 *s* 开始看到的情况是更好还是更差。在这里，我们展示了如果我们按照当前策略的分布加权平均每个这些优势，我们将得到相对于当前策略的零平均优势——这在直觉上是很有意义的，因为建议策略和当前策略是完全相同的。我们不希望通过用当前策略替换自身来看到任何性能提升。
- en: 'Now, if we replace <math alttext="theta"><mi>θ</mi></math>  with a different
    proposed policy’s parameters <math alttext="theta Subscript a l t"><msub><mi>θ</mi>
    <mrow><mi>a</mi><mi>l</mi><mi>t</mi></mrow></msub></math> , the above derivation
    leads us to:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们用不同的建议策略参数 <math alttext="theta Subscript a l t"><msub><mi>θ</mi> <mrow><mi>a</mi><mi>l</mi><mi>t</mi></mrow></msub></math>
    替换 <math alttext="theta"><mi>θ</mi></math>，上述推导将导致：
- en: <math alttext="double-struck upper E Subscript a tilde pi Sub Subscript theta
    Sub Sub Subscript a l t Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper Q Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s comma a right-parenthesis
    right-bracket minus upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis"><mrow><msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>a</mi><mi>l</mi><mi>t</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>Q</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>-</mo> <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E Subscript a tilde pi Sub Subscript theta
    Sub Sub Subscript a l t Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper Q Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s comma a right-parenthesis
    right-bracket minus upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis"><mrow><msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>a</mi><mi>l</mi><mi>t</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>Q</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>-</mo> <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
- en: This is as far as simplification will take us, since the actions in the first
    term are no longer distributed as the current policy and we can’t make the simplification
    that led us to the penultimate step. If we evaluate this expression and we receive
    a positive result, we can interpret the result as representing a positive average
    advantage from following the proposed policy compared to following the current
    policy, directly translating to a performance gain for this specific state *s*
    by replacing the current policy with the proposed policy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是简化的极限，因为第一项中的操作不再按照当前策略分布，我们无法进行导致倒数第二步的简化。如果我们评估这个表达式并得到一个正结果，我们可以将结果解释为相对于遵循当前策略而言，遵循建议策略会带来正的平均优势，直接转化为通过用建议策略替换当前策略来提高这个特定状态
    *s* 的性能。
- en: Note
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that we have only been considering a specific state *s*. But even if we
    see a performance gain for some state, it might be the case that that state only
    rarely shows up. This leads us to the inclusion of the term  <math alttext="sigma-summation
    Underscript s Endscripts rho Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s right-parenthesis"><mrow><msub><mo>∑</mo> <mi>s</mi></msub> <msub><mi>ρ</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow></mrow></math> , which quantifies how often we see
    a given state. We can actually rewrite this as an expectation even though this
    is an unnormalized distribution—all we’d need to do is factor out the normalizing
    constant, which is also a constant from the perspective of <math alttext="theta"><mi>θ</mi></math>
    since the normalizing constant is solely a function of <math alttext="theta Subscript
    o l d"><msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></math>
    .
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们只考虑了一个特定状态 *s*。但即使我们看到某个状态的性能提升，也可能是这个状态很少出现的情况。这导致我们包含了术语 <math alttext="sigma-summation
    Underscript s Endscripts rho Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s right-parenthesis"><mrow><msub><mo>∑</mo> <mi>s</mi></msub> <msub><mi>ρ</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow></mrow></math>，它量化了我们看到给定状态的频率。实际上，我们可以将这个重写为期望，尽管这是一个未归一化的分布——我们只需要因子出归一化常数，这也是从
    <math alttext="theta"><mi>θ</mi></math> 的角度来看的一个常数，因为归一化常数仅仅是 <math alttext="theta
    Subscript o l d"><msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></math>
    的函数。
- en: Keep in mind that <math alttext="sigma-summation Underscript s Endscripts rho
    Subscript theta Sub Subscript o l d Baseline left-parenthesis s right-parenthesis"><mrow><msub><mo>∑</mo>
    <mi>s</mi></msub> <msub><mi>ρ</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math> is evaluated using
    the current policy rather than the proposed policy; this is because, as noted
    in the paper, the complex dependency this introduces on  <math alttext="theta"><mi>θ</mi></math>
     when optimizing the alternative objective (which uses  <math alttext="sigma-summation
    Underscript s Endscripts rho Subscript theta Baseline left-parenthesis s right-parenthesis"><mrow><msub><mo>∑</mo>
    <mi>s</mi></msub> <msub><mi>ρ</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>s</mi>
    <mo>)</mo></mrow></mrow></math> ) with respect to  <math alttext="theta"><mi>θ</mi></math>
     makes the optimization process difficult. Additionally, the paper proves that
    the first-order gradient matches that of the alternative objective anyway, allowing
    us to make this substitution without introducing a biased gradient estimate. We
    won’t show this here, however, as it is beyond the scope of the text.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住<math alttext="sigma-summation Underscript s Endscripts rho Subscript theta
    Sub Subscript o l d Baseline left-parenthesis s right-parenthesis"><mrow><msub><mo>∑</mo>
    <mi>s</mi></msub> <msub><mi>ρ</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>是使用当前策略而不是提议策略进行评估的；这是因为，正如论文中所指出的，当优化替代目标（使用<math
    alttext="sigma-summation Underscript s Endscripts rho Subscript theta Baseline
    left-parenthesis s right-parenthesis"><mrow><msub><mo>∑</mo> <mi>s</mi></msub>
    <msub><mi>ρ</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>）时，对于<math
    alttext="theta"><mi>θ</mi></math>的复杂依赖性使得优化过程困难。此外，论文证明了一阶梯度与替代目标的梯度匹配，允许我们进行这种替换而不引入有偏的梯度估计。然而，我们不会在这里展示这一点，因为这超出了文本的范围。
- en: 'Putting everything together, we have the following constrained optimization
    objective:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容放在一起，我们有以下受限制的优化目标：
- en: <math alttext="theta Superscript asterisk Baseline equals argmax Subscript theta
    Baseline sigma-summation Underscript s Endscripts rho Subscript theta Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis sigma-summation Underscript
    a Endscripts pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis s comma
    a right-parenthesis"><mrow><msup><mi>θ</mi> <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmax</mtext>
    <mi>θ</mi></msub> <msub><mo>∑</mo> <mi>s</mi></msub> <msub><mi>ρ</mi> <msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo> <mi>s</mi>
    <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>a</mi></msub> <msub><mi>π</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mi>A</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="theta Superscript asterisk Baseline equals argmax Subscript theta
    Baseline sigma-summation Underscript s Endscripts rho Subscript theta Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis sigma-summation Underscript
    a Endscripts pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis s comma
    a right-parenthesis"><mrow><msup><mi>θ</mi> <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmax</mtext>
    <mi>θ</mi></msub> <msub><mo>∑</mo> <mi>s</mi></msub> <msub><mi>ρ</mi> <msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo> <mi>s</mi>
    <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>a</mi></msub> <msub><mi>π</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mi>A</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="s period t period Avg period KL left-parenthesis theta Subscript
    o l d Baseline comma theta right-parenthesis less-than-or-equal-to delta"><mrow><mi>s</mi>
    <mo>.</mo> <mi>t</mi> <mo>.</mo> <mtext>Avg.</mtext> <mtext>KL</mtext> <mo>(</mo>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub> <mo>,</mo>
    <mi>θ</mi> <mo>)</mo> <mo>≤</mo> <mi>δ</mi></mrow></math>
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="s period t period Avg period KL left-parenthesis theta Subscript
    o l d Baseline comma theta right-parenthesis less-than-or-equal-to delta"><mrow><mi>s</mi>
    <mo>.</mo> <mi>t</mi> <mo>.</mo> <mtext>Avg.</mtext> <mtext>KL</mtext> <mo>(</mo>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub> <mo>,</mo>
    <mi>θ</mi> <mo>)</mo> <mo>≤</mo> <mi>δ</mi></mrow></math>
- en: 'where the average KL divergence denotes the expected KL divergence between
    policies over all states. This is what we call the *trust region,* and it represents
    the parameter settings that lie close enough to the current parameter setting,
    mitigate training instability, and mitigate overfitting. How do we go about optimizing
    this objective? The inner summation looks like an expectation with respect to
    <math alttext="pi Subscript theta Baseline left-parenthesis a comma s right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
    , but all we have is our current setting of parameter values <math alttext="theta
    Subscript o l d"><msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></math>
    . In the standard setting, or *on-policy* setting, we are sampling from the same
    policy we are optimizing, so we can use classic policy gradient optimization for
    this. However, TRPO can be modified to work in the *off-policy* setting as well,
    where the policy we are sampling from is different from the policy we are optimizing.
    Generally, the reason for this distinction is that we may have a behavior policy,
    the policy we are sampling from that may be more exploratory in nature, while
    we learn the target policy, which is to be optimized. In the off-policy setting,
    since we are sampling actions from a different distribution *q(a|s)* (the behavior
    policy) from <math alttext="pi Subscript theta Baseline left-parenthesis a vertical-bar
    s right-parenthesis"><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math> (the target policy),
    we instead use the following constrained optimization objective:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中平均KL散度表示所有状态下策略之间的期望KL散度。这就是我们所谓的*信任区域*，它代表了与当前参数设置足够接近的参数设置，有助于减轻训练不稳定性和减轻过拟合。我们如何优化这个目标？内部求和看起来像是关于<math
    alttext="pi Subscript theta Baseline left-parenthesis a comma s right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>的期望，但我们只有当前参数值的设置<math
    alttext="theta Subscript o l d"><msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></math>。在标准设置或*on-policy*设置中，我们从我们正在优化的相同策略中进行抽样，因此我们可以使用经典的策略梯度优化。然而，TRPO也可以修改为在*off-policy*设置中工作，其中我们从不同于我们正在优化的策略中进行抽样。一般来说，这种区别的原因是我们可能有一个行为策略，即我们从中进行抽样的策略可能更具探索性质，而我们学习的目标策略则是要优化的。在off-policy设置中，由于我们从不同分布*q(a|s)*（行为策略）而不是从<math
    alttext="pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>（目标策略）中进行抽样，因此我们使用以下受限制的优化目标：
- en: <math alttext="theta Superscript asterisk Baseline equals argmax Subscript theta
    Baseline sigma-summation Underscript s Endscripts rho Subscript theta Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis sigma-summation Underscript
    a Endscripts StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar
    s right-parenthesis Over q left-parenthesis a vertical-bar s right-parenthesis
    EndFraction upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><msup><mi>θ</mi> <mo>*</mo></msup> <mo>=</mo>
    <msub><mtext>argmax</mtext> <mi>θ</mi></msub> <msub><mo>∑</mo> <mi>s</mi></msub>
    <msub><mi>ρ</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>a</mi></msub>
    <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="theta Superscript asterisk Baseline equals argmax Subscript theta
    Baseline sigma-summation Underscript s Endscripts rho Subscript theta Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis sigma-summation Underscript
    a Endscripts StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar
    s right-parenthesis Over q left-parenthesis a vertical-bar s right-parenthesis
    EndFraction upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><msup><mi>θ</mi> <mo>*</mo></msup> <mo>=</mo>
    <msub><mtext>argmax</mtext> <mi>θ</mi></msub> <msub><mo>∑</mo> <mi>s</mi></msub>
    <msub><mi>ρ</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>a</mi></msub>
    <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="s period t period Avg period KL left-parenthesis theta Subscript
    o l d Baseline comma theta right-parenthesis less-than-or-equal-to delta"><mrow><mi>s</mi>
    <mo>.</mo> <mi>t</mi> <mo>.</mo> <mtext>Avg.</mtext> <mtext>KL</mtext> <mo>(</mo>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub> <mo>,</mo>
    <mi>θ</mi> <mo>)</mo> <mo>≤</mo> <mi>δ</mi></mrow></math>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="s period t period Avg period KL left-parenthesis theta Subscript
    o l d Baseline comma theta right-parenthesis less-than-or-equal-to delta"><mrow><mi>s</mi>
    <mo>.</mo> <mi>t</mi> <mo>.</mo> <mtext>Avg.</mtext> <mtext>KL</mtext> <mo>(</mo>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub> <mo>,</mo>
    <mi>θ</mi> <mo>)</mo> <mo>≤</mo> <mi>δ</mi></mrow></math>
- en: 'The addition of *q(a|s)* accounts for the fact that we are sampling from a
    separate behavior policy. We can think about this more concretely in terms of
    expectations:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 添加*q(a|s)*考虑到我们从一个单独的行为策略中进行抽样的事实。我们可以更具体地从期望的角度来思考：
- en: <math alttext="sigma-summation Underscript a Endscripts pi Subscript theta Baseline
    left-parenthesis a vertical-bar s right-parenthesis upper A Subscript theta Sub
    Subscript o l d Baseline left-parenthesis s comma a right-parenthesis equals sigma-summation
    Underscript a Endscripts StartFraction q left-parenthesis a vertical-bar s right-parenthesis
    Over q left-parenthesis a vertical-bar s right-parenthesis EndFraction pi Subscript
    theta Baseline left-parenthesis a vertical-bar s right-parenthesis upper A Subscript
    theta Sub Subscript o l d Baseline left-parenthesis s comma a right-parenthesis"><mrow><msub><mo>∑</mo>
    <mi>a</mi></msub> <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi>
    <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mo>∑</mo> <mi>a</mi></msub> <mfrac><mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi>
    <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="sigma-summation Underscript a Endscripts pi Subscript theta Baseline
    left-parenthesis a vertical-bar s right-parenthesis upper A Subscript theta Sub
    Subscript o l d Baseline left-parenthesis s comma a right-parenthesis equals sigma-summation
    Underscript a Endscripts StartFraction q left-parenthesis a vertical-bar s right-parenthesis
    Over q left-parenthesis a vertical-bar s right-parenthesis EndFraction pi Subscript
    theta Baseline left-parenthesis a vertical-bar s right-parenthesis upper A Subscript
    theta Sub Subscript o l d Baseline left-parenthesis s comma a right-parenthesis"><mrow><msub><mo>∑</mo>
    <mi>a</mi></msub> <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi>
    <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mo>∑</mo> <mi>a</mi></msub> <mfrac><mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi>
    <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals sigma-summation Underscript a Endscripts q left-parenthesis
    a vertical-bar s right-parenthesis StartFraction pi Subscript theta Baseline left-parenthesis
    a vertical-bar s right-parenthesis Over q left-parenthesis a vertical-bar s right-parenthesis
    EndFraction upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><mo>=</mo> <msub><mo>∑</mo> <mi>a</mi></msub>
    <mi>q</mi> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow>
    <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript a Endscripts q left-parenthesis
    a vertical-bar s right-parenthesis StartFraction pi Subscript theta Baseline left-parenthesis
    a vertical-bar s right-parenthesis Over q left-parenthesis a vertical-bar s right-parenthesis
    EndFraction upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><mo>=</mo> <msub><mo>∑</mo> <mi>a</mi></msub>
    <mi>q</mi> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow>
    <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals double-struck upper E Subscript q left-parenthesis a vertical-bar
    s right-parenthesis Baseline left-bracket StartFraction pi Subscript theta Baseline
    left-parenthesis a vertical-bar s right-parenthesis Over q left-parenthesis a
    vertical-bar s right-parenthesis EndFraction upper A Subscript theta Sub Subscript
    o l d Subscript Baseline left-parenthesis s comma a right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals double-struck upper E Subscript q left-parenthesis a vertical-bar
    s right-parenthesis Baseline left-bracket StartFraction pi Subscript theta Baseline
    left-parenthesis a vertical-bar s right-parenthesis Over q left-parenthesis a
    vertical-bar s right-parenthesis EndFraction upper A Subscript theta Sub Subscript
    o l d Subscript Baseline left-parenthesis s comma a right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
- en: Note that the left side of the first equality can be written as an expectation
    of the advantage with respect to the target policy. In a few algebraic manipulations,
    we were able to convert our original objective into an equivalent objective, but
    with an expectation that is taken with respect to the behavior policy. This is
    ideal because we are sampling from the behavior policy and can thus use standard
    minibatch gradient descent techniques to optimize this objective (adding in the
    constraint on the KL divergence makes this a bit more complicated than just standard
    gradient descent). And finally, we have already seen methods for sampling from
    the unnormalized probability distribution <math alttext="rho Subscript o l d Baseline
    left-parenthesis s right-parenthesis"><mrow><msub><mi>ρ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math> for the outer expectation,
    amongst others that exist in academic literature.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第一个等式的左侧可以写成相对于目标策略的优势的期望。通过一些代数操作，我们能够将原始目标转换为等效目标，但期望是相对于行为策略进行的。这是理想的，因为我们从行为策略中抽样，因此可以使用标准的小批量梯度下降技术来优化这个目标（添加KL散度的约束使得这比普通梯度下降复杂一些）。最后，我们已经看到了从未标准化的概率分布rho
    Subscript old Baseline（s）中抽样的方法，这些方法存在于学术文献中。
- en: Proximal Policy Optimization
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 近端策略优化
- en: 'One issue with TRPO is that its optimization is relatively complicated due
    to the inclusion of the average KL divergence term and involves second-order optimization
    to perform. *Proximal policy optimization*, or *PPO* for short, is an algorithm
    that tries to retain the benefits of TRPO without the complicated optimization.
    PPO proposes the following objective instead:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO的一个问题是，由于包含平均KL散度项，其优化相对复杂，并涉及二阶优化。近端策略优化，简称为PPO，是一种试图保留TRPO优点而避免复杂优化的算法。PPO提出了以下目标：
- en: <math alttext="upper J left-parenthesis theta right-parenthesis equals double-struck
    upper E left-bracket min left-parenthesis StartFraction pi Subscript theta Baseline
    left-parenthesis a vertical-bar s right-parenthesis Over pi Subscript theta Sub
    Subscript o l d Subscript Baseline left-parenthesis a vertical-bar s right-parenthesis
    EndFraction upper A Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    s comma a right-parenthesis comma clip left-parenthesis StartFraction pi Subscript
    theta Baseline left-parenthesis a vertical-bar s right-parenthesis Over pi Subscript
    theta Sub Subscript o l d Subscript Baseline left-parenthesis a vertical-bar s
    right-parenthesis EndFraction comma 1 minus epsilon comma 1 plus epsilon right-parenthesis
    upper A Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    s comma a right-parenthesis right-parenthesis right-bracket"><mrow><mi>J</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mo>[</mo>
    <mtext>min</mtext> <mrow><mo>(</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>,</mo>
    <mtext>clip</mtext> <mrow><mo>(</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mo>,</mo> <mn>1</mn> <mo>-</mo> <mi>ϵ</mi> <mo>,</mo> <mn>1</mn> <mo>+</mo> <mi>ϵ</mi>
    <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>]</mo></mrow></math>
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper J left-parenthesis theta right-parenthesis equals double-struck
    upper E left-bracket min left-parenthesis StartFraction pi Subscript theta Baseline
    left-parenthesis a vertical-bar s right-parenthesis Over pi Subscript theta Sub
    Subscript o l d Subscript Baseline left-parenthesis a vertical-bar s right-parenthesis
    EndFraction upper A Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    s comma a right-parenthesis comma clip left-parenthesis StartFraction pi Subscript
    theta Baseline left-parenthesis a vertical-bar s right-parenthesis Over pi Subscript
    theta Sub Subscript o l d Subscript Baseline left-parenthesis a vertical-bar s
    right-parenthesis EndFraction comma 1 minus epsilon comma 1 plus epsilon right-parenthesis
    upper A Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    s comma a right-parenthesis right-parenthesis right-bracket"><mrow><mi>J</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mo>[</mo>
    <mtext>min</mtext> <mrow><mo>(</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>,</mo>
    <mtext>clip</mtext> <mrow><mo>(</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mo>,</mo> <mn>1</mn> <mo>-</mo> <mi>ϵ</mi> <mo>,</mo> <mn>1</mn> <mo>+</mo> <mi>ϵ</mi>
    <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>]</mo></mrow></math>
- en: <math alttext="theta Superscript asterisk Baseline equals argmax Subscript theta
    Baseline upper J left-parenthesis theta right-parenthesis"><mrow><msup><mi>θ</mi>
    <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmax</mtext> <mi>θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="theta Superscript asterisk Baseline equals argmax Subscript theta
    Baseline upper J left-parenthesis theta right-parenthesis"><mrow><msup><mi>θ</mi>
    <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmax</mtext> <mi>θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
- en: Note that we no longer have a complex constraint, but rather an extra term built
    into the optimization objective. The clip function represents an upper limit and
    a lower limit on the ratio between the target policy and the behavior policy,
    where any ratio above or below these limits is set equal to the corresponding
    limit. Note the inclusion of the minimum between the original and the clipped,
    which prevents us from making extreme updates and keeps us from overfitting.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不再有复杂的约束，而是在优化目标中加入了额外的项。剪切函数表示目标策略和行为策略之间比率的上限和下限，任何超出这些限制的比率都将设置为相应的限制。注意原始值和剪切值之间的最小值的包含，这可以防止我们进行极端更新，并防止过拟合。
- en: 'As stated in the paper introducing PPO, it’s important to notice that the objective
    for TRPO and PPO have the same gradient at  <math alttext="theta equals theta
    Subscript o l d"><mrow><mi>θ</mi> <mo>=</mo> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></mrow></math>
    . This is the case in at least the on-policy setting, where we have a single policy
    from which we are sampling and optimizing (i.e., no distinction between the behavior
    and target policy). Let’s take a closer look at why this is the case. To do this,
    we first need to reformulate TRPO’s constrained optimization objective as an equivalent
    regularized optimization objective (recall from early in the previous section),
    which we can do according to the theory. The objective looks like:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如PPO介绍论文中所述，重要的是注意TRPO和PPO在θ等于θ Subscript old时具有相同的梯度。至少在在线策略设置中是这样，在这种设置中，我们从中抽样和优化的是单个策略（即行为策略和目标策略之间没有区别）。让我们更仔细地看看为什么会出现这种情况。为此，我们首先需要将TRPO的约束优化目标重新表述为等效的正则化优化目标（回想一下前一节早期的内容），根据理论，我们可以这样做。目标看起来像：
- en: <math alttext="upper J Superscript upper T upper R upper P upper O Baseline
    left-parenthesis theta right-parenthesis equals double-struck upper E left-bracket
    StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    Over pi Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    a vertical-bar s right-parenthesis EndFraction upper A left-parenthesis s comma
    a right-parenthesis minus beta asterisk KL left-parenthesis pi Subscript theta
    Sub Subscript o l d Subscript Baseline left-parenthesis a vertical-bar s right-parenthesis
    StartAbsoluteValue EndAbsoluteValue pi Subscript theta Baseline left-parenthesis
    a vertical-bar s right-parenthesis right-parenthesis right-bracket"><mrow><msup><mi>J</mi>
    <mrow><mi>T</mi><mi>R</mi><mi>P</mi><mi>O</mi></mrow></msup> <mrow><mrow><mo>(</mo>
    <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mo>[</mo></mrow> <mfrac><mrow><msub><mi>π</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mrow><mi>A</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>-</mo> <mi>β</mi> <mo>*</mo> <mtext>KL</mtext> <mo>(</mo></mrow> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mrow><mo>(</mo>
    <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>|</mo> <mo>|</mo></mrow>
    <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi>
    <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>]</mo></mrow></mrow></math>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper J Superscript upper T upper R upper P upper O Baseline
    left-parenthesis theta right-parenthesis equals double-struck upper E left-bracket
    StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    Over pi Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    a vertical-bar s right-parenthesis EndFraction upper A left-parenthesis s comma
    a right-parenthesis minus beta asterisk KL left-parenthesis pi Subscript theta
    Sub Subscript o l d Subscript Baseline left-parenthesis a vertical-bar s right-parenthesis
    StartAbsoluteValue EndAbsoluteValue pi Subscript theta Baseline left-parenthesis
    a vertical-bar s right-parenthesis right-parenthesis right-bracket"><mrow><msup><mi>J</mi>
    <mrow><mi>T</mi><mi>R</mi><mi>P</mi><mi>O</mi></mrow></msup> <mrow><mrow><mo>(</mo>
    <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mo>[</mo></mrow> <mfrac><mrow><msub><mi>π</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mrow><mi>A</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>-</mo> <mi>β</mi> <mo>*</mo> <mtext>KL</mtext> <mo>(</mo></mrow> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mrow><mo>(</mo>
    <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>|</mo> <mo>|</mo></mrow>
    <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi>
    <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>]</mo></mrow></mrow></math>
- en: 'Notice that we can separate the expression within the expectation into a difference
    of expectations due to the linearity of expectation. If we first consider the
    second expectation, or the KL term, we’ll notice that this term is minimized at 
    <math alttext="theta equals theta Subscript o l d"><mrow><mi>θ</mi> <mo>=</mo>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></mrow></math>
    , since the reference distribution is parametrized using  <math alttext="theta
    Subscript o l d"><msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></math>
    . Thus, the gradient at this setting is zero, since we have already reached the
    global minimum. We are left with only the gradient of the first expectation:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于期望的线性性，我们可以将期望中的表达式分开为期望的差异。如果我们首先考虑第二个期望，或者KL项，我们会注意到这个项在θ等于θ Subscript
    old时被最小化，因为参考分布是使用θ Subscript old参数化的。因此，在这种设置下，梯度为零，因为我们已经达到全局最小值。我们只剩下第一个期望的梯度：
- en: <math alttext="normal nabla Subscript theta Baseline double-struck upper E left-bracket
    StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    Over pi Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    a vertical-bar s right-parenthesis EndFraction upper A left-parenthesis s comma
    a right-parenthesis right-bracket"><mrow><msub><mi>∇</mi> <mi>θ</mi></msub> <mi>𝔼</mi>
    <mrow><mo>[</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mi>A</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal nabla Subscript theta Baseline double-struck upper E left-bracket
    StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    Over pi Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    a vertical-bar s right-parenthesis EndFraction upper A left-parenthesis s comma
    a right-parenthesis right-bracket"><mrow><msub><mi>∇</mi> <mi>θ</mi></msub> <mi>𝔼</mi>
    <mrow><mo>[</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mi>A</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
- en: 'Looking to the objective for PPO, we notice that at  <math alttext="theta equals
    theta Subscript o l d"><mrow><mi>θ</mi> <mo>=</mo> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></mrow></math>
    , the ratio between the two policies is one, eliminating the need for the clip
    term. Thus, we are left with a minimum over two equivalent terms, which simplifies
    to the expectation over a single term. The gradient comes out to exactly what
    we just saw for the TRPO objective:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 针对PPO的目标，我们注意到在θ等于θ Subscript old时，两个策略之间的比率为1，消除了剪切项的需要。因此，我们只剩下两个等价项的最小值，简化为单个项的期望。梯度正好与TRPO目标中看到的相同：
- en: <math alttext="normal nabla Subscript theta Baseline double-struck upper E left-bracket
    StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    Over pi Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    a vertical-bar s right-parenthesis EndFraction upper A left-parenthesis s comma
    a right-parenthesis right-bracket"><mrow><msub><mi>∇</mi> <mi>θ</mi></msub> <mi>𝔼</mi>
    <mrow><mo>[</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mi>A</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal nabla Subscript theta Baseline double-struck upper E left-bracket
    StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    Over pi Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    a vertical-bar s right-parenthesis EndFraction upper A left-parenthesis s comma
    a right-parenthesis right-bracket"><mrow><msub><mi>∇</mi> <mi>θ</mi></msub> <mi>𝔼</mi>
    <mrow><mo>[</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mi>A</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
- en: We have shown that PPO has the same gradient as TRPO in the select on-policy
    setting, and is additionally much easier to optimize in practice. PPO has also
    shown strong empirical results on a variety of tasks, and has become widely used
    in the field of deep RL.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经证明了在选择在线策略设置中，PPO与TRPO具有相同的梯度，并且在实践中更容易优化。PPO在各种任务上也表现出强大的经验结果，并且已经广泛应用于深度强化学习领域。
- en: Q-Learning and Deep Q-Networks
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习和深度Q网络
- en: Q-learning is in the category of reinforcement learning called value learning.
    Instead of directly learning a policy, we will be learning the value of states
    and actions. Q-learning involves learning a function, a *Q-function*, which represents
    the quality of a state, action pair. The Q-function, defined *Q(s, a)*, is a function
    that calculates the maximum discounted future return when action *a* is performed
    in state *s*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习属于强化学习中的值学习类别。我们不是直接学习一个策略，而是学习状态和动作的值。Q学习涉及学习一个函数，一个*Q函数*，它表示状态、动作对的质量。定义为*Q(s,
    a)*的Q函数是一个计算在状态*s*中执行动作*a*时的最大折扣未来回报的函数。
- en: 'The Q-value represents our expected long-term rewards, given we are at a state,
    and take an action, and then take every subsequent action perfectly (to maximize
    expected future reward). This can be expressed formally as:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Q值代表我们在一个状态下采取一个动作，然后完美地采取每一个后续动作（以最大化预期未来奖励）时的预期长期奖励。这可以正式表达为：
- en: <math alttext="upper Q Superscript asterisk Baseline left-parenthesis s Subscript
    t Baseline comma a Subscript t Baseline right-parenthesis equals m a x Subscript
    pi Baseline upper E left-bracket sigma-summation Underscript i equals t Overscript
    upper T Endscripts gamma Superscript i Baseline r Superscript i Baseline right-bracket"><mrow><msup><mi>Q</mi>
    <mo>*</mo></msup> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>m</mi> <mi>a</mi>
    <msub><mi>x</mi> <mi>π</mi></msub> <mi>E</mi> <mrow><mo>[</mo> <msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mi>t</mi></mrow> <mi>T</mi></msubsup> <mrow><msup><mi>γ</mi>
    <mi>i</mi></msup> <msup><mi>r</mi> <mi>i</mi></msup></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper Q Superscript asterisk Baseline left-parenthesis s Subscript
    t Baseline comma a Subscript t Baseline right-parenthesis equals m a x Subscript
    pi Baseline upper E left-bracket sigma-summation Underscript i equals t Overscript
    upper T Endscripts gamma Superscript i Baseline r Superscript i Baseline right-bracket"><mrow><msup><mi>Q</mi>
    <mo>*</mo></msup> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>m</mi> <mi>a</mi>
    <msub><mi>x</mi> <mi>π</mi></msub> <mi>E</mi> <mrow><mo>[</mo> <msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mi>t</mi></mrow> <mi>T</mi></msubsup> <mrow><msup><mi>γ</mi>
    <mi>i</mi></msup> <msup><mi>r</mi> <mi>i</mi></msup></mrow> <mo>]</mo></mrow></mrow></math>
- en: A question you may be asking is, how can we know Q-values? It is difficult,
    even for humans, to know how good an action is, because you need to know how you
    are going to act in the future. Our expected future returns depend on what our
    long-term strategy is going to be. This seems to be a bit of a chicken-and-egg
    problem. In order to value a state, action pair, you need to know all the perfect
    subsequent actions. And in order to know the best actions, you need to have accurate
    values for a state and action.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，我们怎么知道Q值呢？即使对于人类来说，知道一个动作有多好也是困难的，因为你需要知道未来你将如何行动。我们的预期未来回报取决于我们的长期策略将是什么。这似乎是一个鸡生蛋的问题。为了评估一个状态、动作对，你需要知道所有完美的后续动作。为了知道最佳动作，你需要准确的状态和动作值。
- en: The Bellman Equation
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: 'We solve this dilemma by defining our Q-values as a function of future Q-values.
    This relation is called the *Bellman equation*, and it states that the maximum
    future reward for taking action is the current reward plus the next step’s *max*
    future reward from taking the next action *a’*:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将我们的Q值定义为未来Q值的函数来解决这个困境。这种关系被称为*贝尔曼方程*，它表明采取行动的最大未来奖励是当前奖励加上下一步采取下一个动作*a'*的*最大*未来奖励：
- en: <math alttext="upper Q Superscript asterisk Baseline left-parenthesis s Subscript
    t Baseline comma a Subscript t Baseline right-parenthesis equals upper E left-bracket
    r Subscript t Baseline plus gamma max Underscript a prime Endscripts upper Q Superscript
    asterisk Baseline left-parenthesis s Subscript t plus 1 Baseline comma a Superscript
    prime Baseline right-parenthesis right-bracket"><mrow><msup><mi>Q</mi> <mo>*</mo></msup>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>E</mi> <mrow><mo>[</mo> <msub><mi>r</mi>
    <mi>t</mi></msub> <mo>+</mo> <mi>γ</mi> <msub><mo movablelimits="true" form="prefix">max</mo>
    <msup><mi>a</mi> <mo>'</mo></msup></msub> <msup><mi>Q</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msup><mi>a</mi> <msup><mo>'</mo></msup></msup> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper Q Superscript asterisk Baseline left-parenthesis s Subscript
    t Baseline comma a Subscript t Baseline right-parenthesis equals upper E left-bracket
    r Subscript t Baseline plus gamma max Underscript a prime Endscripts upper Q Superscript
    asterisk Baseline left-parenthesis s Subscript t plus 1 Baseline comma a Superscript
    prime Baseline right-parenthesis right-bracket"><mrow><msup><mi>Q</mi> <mo>*</mo></msup>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>E</mi> <mrow><mo>[</mo> <msub><mi>r</mi>
    <mi>t</mi></msub> <mo>+</mo> <mi>γ</mi> <msub><mo movablelimits="true" form="prefix">max</mo>
    <msup><mi>a</mi> <mo>'</mo></msup></msub> <msup><mi>Q</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msup><mi>a</mi> <msup><mo>'</mo></msup></msup> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
- en: This recursive definition allows us to relate between Q-values.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这个递归定义使我们能够关联Q值之间的关系。
- en: 'And since we can now relate between Q-values past and future, this equation
    conveniently defines an update rule. Namely, we can update past Q-values to be
    based on future Q-values. This is powerful because there exists a Q-value we know
    is correct: the Q-value of the very last action before the episode is over. For
    this last state, we know exactly that the next action led to the next reward,
    so we can perfectly set the Q-values for that state. We can use the update rule,
    then, to propagate that Q-value to the previous time step:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以关联过去和未来的Q值，这个方程方便地定义了一个更新规则。也就是说，我们可以根据未来的Q值更新过去的Q值。这是强大的，因为存在一个我们知道是正确的Q值：在剧集结束之前的最后一个动作的Q值。对于这个最后的状态，我们确切地知道下一个动作导致了下一个奖励，所以我们可以完美地设置该状态的Q值。然后，我们可以使用更新规则将该Q值传播到之前的时间步：
- en: <math alttext="ModifyingAbove upper Q Subscript j Baseline With caret right-arrow
    ModifyingAbove upper Q Subscript j plus 1 Baseline With caret right-arrow ModifyingAbove
    upper Q Subscript j plus 2 Baseline With caret right-arrow ellipsis right-arrow
    upper Q Superscript asterisk"><mrow><mover accent="true"><msub><mi>Q</mi> <mi>j</mi></msub>
    <mo>^</mo></mover> <mo>→</mo> <mover accent="true"><msub><mi>Q</mi> <mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>^</mo></mover> <mo>→</mo> <mover accent="true"><msub><mi>Q</mi> <mrow><mi>j</mi><mo>+</mo><mn>2</mn></mrow></msub>
    <mo>^</mo></mover> <mo>→</mo> <mo>...</mo> <mo>→</mo> <msup><mi>Q</mi> <mo>*</mo></msup></mrow></math>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove upper Q Subscript j Baseline With caret right-arrow
    ModifyingAbove upper Q Subscript j plus 1 Baseline With caret right-arrow ModifyingAbove
    upper Q Subscript j plus 2 Baseline With caret right-arrow ellipsis right-arrow
    upper Q Superscript asterisk"><mrow><mover accent="true"><msub><mi>Q</mi> <mi>j</mi></msub>
    <mo>^</mo></mover> <mo>→</mo> <mover accent="true"><msub><mi>Q</mi> <mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>^</mo></mover> <mo>→</mo> <mover accent="true"><msub><mi>Q</mi> <mrow><mi>j</mi><mo>+</mo><mn>2</mn></mrow></msub>
    <mo>^</mo></mover> <mo>→</mo> <mo>...</mo> <mo>→</mo> <msup><mi>Q</mi> <mo>*</mo></msup></mrow></math>
- en: This updating of the  Q-value is known as *value iteration*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这种Q值的更新被称为*值迭代*。
- en: Our first Q-value starts out completely wrong, but this is perfectly acceptable.
    With each iteration, we can update our Q-value via the correct one from the future.
    After one iteration, the last Q-value is accurate, since it is just the reward
    from the last state and action before episode termination. Then we perform our
    Q-value update, which sets the second-to-last Q-value. In our next iteration,
    we can guarantee that the last two Q-values are correct, and so on and so forth.
    Through value iteration, we will be guaranteed convergence on the ultimate optimal
    Q-value.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个Q值完全错误，但这是完全可以接受的。每次迭代，我们可以通过未来的正确Q值更新我们的Q值。经过一次迭代，最后一个Q值是准确的，因为它只是在剧集终止之前的最后一个状态和动作的奖励。然后我们执行我们的Q值更新，设置倒数第二个Q值。在下一次迭代中，我们可以保证最后两个Q值是正确的，依此类推。通过值迭代，我们将保证收敛到最终最优的Q值。
- en: Issues with Value Iteration
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 值迭代的问题
- en: Value iteration produces a mapping between state and action pairs with corresponding
    Q-values, and we are constructing a table of these mappings, or a *Q-table*. Let’s
    briefly talk about the size of this Q-table. Value iteration is an exhaustive
    process that requires a full traversal of the entire space of state, action pairs.
    In a game like Breakout, with 100 bricks that can be either present or not, with
    50 positions for the paddle to be in, and 250 positions for the ball to be in,
    and 3 actions, we have already constructed a space that is far, far larger than
    the sum of all computational capacity of humanity. Furthermore, in stochastic
    environments, the space of our Q-table would be even larger, and possibly infinite.
    With such a large space, it will be intractable for us to find all of the Q-values
    for every state, action pair. Clearly this approach is not going to work. How
    else are we going to do Q-learning?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代产生了状态和动作对之间对应的Q值的映射，我们正在构建这些映射的表，或者*Q表*。让我们简要谈谈这个Q表的大小。值迭代是一个耗尽的过程，需要完全遍历整个状态、动作对空间。在像打砖块这样的游戏中，有100块砖可能存在或不存在，球拍有50个位置可在，球有250个位置可在，还有3个动作，我们已经构建了一个远远大于人类所有计算能力之和的空间。此外，在随机环境中，我们的Q表空间会更大，可能是无限的。在这样一个巨大的空间中，我们将无法找到每个状态、动作对的所有Q值。显然，这种方法行不通。我们还能怎么做Q学习呢？
- en: Approximating the Q-Function
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逼近Q函数
- en: The size of our Q-table makes the naive approach intractable for any nontoy
    problem. However, what if we relax our requirement for an optimal Q-function?
    If instead, we learn approximations of the Q-function, we can use a model to estimate
    our Q-function. Instead of having to experience every state, action pair to update
    our Q-table, we can learn a function that approximates this table, and even generalizes
    outside of its own experience. This means we won’t have to perform an exhaustive
    search through all possible Q-values to learn a Q-function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Q表的大小使得朴素方法在任何非玩具问题上都难以处理。然而，如果我们放宽对最优Q函数的要求呢？如果我们学习Q函数的近似值，我们可以使用一个模型来估计我们的Q函数。我们不必体验每个状态、动作对来更新我们的Q表，我们可以学习一个近似这个表的函数，甚至可以在自己的经验之外进行泛化。这意味着我们不必对所有可能的Q值进行详尽搜索来学习Q函数。
- en: Deep Q-Network
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: This was the main motivation behind DeepMind’s work on deep Q-network (DQN).
    DQN uses a deep neural network that takes an image (the state) in to estimate
    the Q-value for all possible actions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这是DeepMind在深度Q网络（DQN）上工作的主要动机。DQN使用一个深度神经网络，将一个图像（状态）输入，估计所有可能动作的Q值。
- en: Training DQN
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练DQN
- en: 'We would like to train our network to approximate the Q-function. We express
    this Q-function approximation as a function of our model’s parameters, like this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望训练我们的网络来逼近Q函数。我们将这个Q函数逼近表示为我们模型参数的函数，如下所示：
- en: <math alttext="ModifyingAbove upper Q Subscript theta Baseline With caret left-parenthesis
    s comma a bar theta right-parenthesis tilde upper Q Superscript asterisk Baseline
    left-parenthesis s comma a right-parenthesis"><mrow><mover accent="true"><msub><mi>Q</mi>
    <mi>θ</mi></msub> <mo>^</mo></mover> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi>
    <mo>∣</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>∼</mo> <msup><mi>Q</mi> <mo>*</mo></msup>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove upper Q Subscript theta Baseline With caret left-parenthesis
    s comma a bar theta right-parenthesis tilde upper Q Superscript asterisk Baseline
    left-parenthesis s comma a right-parenthesis"><mrow><mover accent="true"><msub><mi>Q</mi>
    <mi>θ</mi></msub> <mo>^</mo></mover> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi>
    <mo>∣</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>∼</mo> <msup><mi>Q</mi> <mo>*</mo></msup>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
- en: 'Remember, Q-learning is a value-learning algorithm. We are not learning a policy
    directly, but rather we are learning the values of each state, action pair, regardless
    if they are good or not. We have expressed our model’s Q-function approximation
    as Qtheta, and we would like this to be close to the future expected reward. Using
    the Bellman equation from earlier, we can express this future expected reward
    as:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，Q学习是一种值学习算法。我们不是直接学习策略，而是学习每个状态、动作对的值，无论它们是好是坏。我们已经将我们模型的Q函数逼近表示为Qtheta，我们希望这个值接近未来的预期奖励。使用之前的贝尔曼方程，我们可以将这个未来的预期奖励表示为：
- en: <math><mrow><msubsup><mi>R</mi> <mi>t</mi> <mo>*</mo></msubsup> <mo>=</mo> <mfenced
    close=")" open="(" separators=""><msub><mi>r</mi> <mi>t</mi></msub> <mo>+</mo>
    <mi>γ</mi> <msub><mo form="prefix" movablelimits="true">max</mo> <msup><mi>a</mi>
    <mo>'</mo></msup></msub> <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo><msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><msup><mi>a</mi>
    <mo>'</mo></msup> <mo>|</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></math>
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msubsup><mi>R</mi> <mi>t</mi> <mo>*</mo></msubsup> <mo>=</mo> <mfenced
    close=")" open="(" separators=""><msub><mi>r</mi> <mi>t</mi></msub> <mo>+</mo>
    <mi>γ</mi> <msub><mo form="prefix" movablelimits="true">max</mo> <msup><mi>a</mi>
    <mo>'</mo></msup></msub> <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo><msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><msup><mi>a</mi>
    <mo>'</mo></msup> <mo>|</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></math>
- en: 'Our objective is to minimize the difference between our Q’s approximation,
    and the next Q value:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是最小化我们的Q的逼近值与下一个Q值之间的差异：
- en: <math alttext="min Underscript theta Endscripts sigma-summation Underscript
    e element-of upper E Endscripts sigma-summation Underscript t equals 0 Overscript
    upper T Endscripts ModifyingAbove upper Q With caret left-parenthesis s Subscript
    t Baseline comma a Subscript t Baseline vertical-bar theta right-parenthesis minus
    upper R Subscript t Superscript asterisk"><mrow><msub><mo movablelimits="true"
    form="prefix">min</mo> <mi>θ</mi></msub> <msub><mo>∑</mo> <mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></msub>
    <msubsup><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></msubsup>
    <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi>
    <mo>)</mo></mrow> <mo>-</mo> <msubsup><mi>R</mi> <mi>t</mi> <mo>*</mo></msubsup></mrow></math>
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript theta Endscripts sigma-summation Underscript
    e element-of upper E Endscripts sigma-summation Underscript t equals 0 Overscript
    upper T Endscripts ModifyingAbove upper Q With caret left-parenthesis s Subscript
    t Baseline comma a Subscript t Baseline vertical-bar theta right-parenthesis minus
    upper R Subscript t Superscript asterisk"><mrow><msub><mo movablelimits="true"
    form="prefix">min</mo> <mi>θ</mi></msub> <msub><mo>∑</mo> <mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></msub>
    <msubsup><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></msubsup>
    <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi>
    <mo>)</mo></mrow> <mo>-</mo> <msubsup><mi>R</mi> <mi>t</mi> <mo>*</mo></msubsup></mrow></math>
- en: 'Expanding this expression gives us our full objective:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展这个表达式给我们完整的目标：
- en: <math><mrow><msub><mo form="prefix" movablelimits="true">min</mo> <mi>θ</mi></msub>
    <msub><mo>∑</mo> <mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></msub> <msubsup><mo>∑</mo>
    <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></msubsup> <mover accent="true"><mi>Q</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>-</mo>
    <mfenced close=")" open="(" separators=""><msub><mi>r</mi> <mi>t</mi></msub> <mo>+</mo>
    <mi>γ</mi> <msub><mo form="prefix" movablelimits="true">max</mo> <msup><mi>a</mi>
    <mo>'</mo></msup></msub> <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo><msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><msup><mi>a</mi>
    <mo>'</mo></msup> <mo>|</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></math>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mo form="prefix" movablelimits="true">min</mo> <mi>θ</mi></msub>
    <msub><mo>∑</mo> <mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></msub> <msubsup><mo>∑</mo>
    <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></msubsup> <mover accent="true"><mi>Q</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>-</mo>
    <mfenced close=")" open="(" separators=""><msub><mi>r</mi> <mi>t</mi></msub> <mo>+</mo>
    <mi>γ</mi> <msub><mo form="prefix" movablelimits="true">max</mo> <msup><mi>a</mi>
    <mo>'</mo></msup></msub> <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo><msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><msup><mi>a</mi>
    <mo>'</mo></msup> <mo>|</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></math>
- en: This objective is fully differentiable as a function of our model parameters,
    and we can find gradients to use in stochastic gradient descent to minimize this
    loss.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目标完全可微，作为我们模型参数的函数，我们可以找到梯度用于随机梯度下降来最小化这个损失。
- en: Learning Stability
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习稳定性
- en: One issue you may have noticed is that we are defining our loss function based
    on the difference of our model’s predicted Q-value of this step and the predicted
    Q-value of the next step. In this way, our loss is doubly dependent on our model
    parameters. With each parameter update, the Q-values are constantly shifting,
    and we are using shifting Q-values to do further updates. This high correlation
    of updates can lead to feedback loops and instability in our learning, where our
    parameters may oscillate and make the loss diverge.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到一个问题，那就是我们基于模型预测的这一步的Q值和下一步的预测Q值之间的差异来定义我们的损失函数。这样，我们的损失双重依赖于我们的模型参数。随着每次参数更新，Q值不断变化，我们使用变化的Q值进行进一步更新。这种更新的高相关性可能导致反馈循环和学习不稳定，其中我们的参数可能会振荡并使损失发散。
- en: 'We can employ a couple of simple engineering hacks to remedy this correlation
    problem: namely, target Q-network and experience replay.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采用一些简单的工程技巧来解决这个相关性问题：即目标Q网络和经验重放。
- en: Target Q-Network
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标Q网络
- en: Instead of updating a single network frequently with respect to itself, we can
    reduce this codependence by introducing a second network, called the *target network*.
    Our loss function features to instances of the Q-function,  <math alttext="ModifyingAbove
    upper Q With caret left-parenthesis s Subscript t Baseline comma a Subscript t
    Baseline vertical-bar theta right-parenthesis"><mrow><mover accent="true"><mi>Q</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
     and   <math><mrow><mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msup><mi>a</mi> <mo>'</mo></msup> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
    . We are going to have the first Q be represented as our prediction network, and
    our second Q will be produced by the target Q-network. The target Q-network is
    a copy of our prediction network that lags in its parameter updates. We update
    the target Q-network to equal the prediction network only every few batches. This
    provides much needed stability to our Q-values, and we can now properly learn
    a good Q-function.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过引入第二个网络，称为“目标网络”，减少频繁更新单个网络与自身的相互依赖。我们的损失函数包含Q函数的两个实例，<math alttext="ModifyingAbove
    upper Q With caret left-parenthesis s Subscript t Baseline comma a Subscript t
    Baseline vertical-bar theta right-parenthesis"><mrow><mover accent="true"><mi>Q</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
    和 <math><mrow><mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msup><mi>a</mi> <mo>'</mo></msup> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
    。我们将第一个Q表示为我们的预测网络，第二个Q将由目标Q网络生成。目标Q网络是我们预测网络的一个副本，它在参数更新方面滞后。我们仅在几个批次之后将目标Q网络更新为等于预测网络。这为我们的Q值提供了非常需要的稳定性，现在我们可以正确地学习一个好的Q函数。
- en: Experience Replay
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经验重放
- en: 'There is yet another source of irksome instability to our learning: the high
    correlations of recent experiences. If we train our DQN with batches drawn from
    recent experience, these action, state pairs are all going to be related to one
    another. This is harmful because we want our batch gradients to be representative
    of the entire gradient, and if our data is not representative of the data distribution,
    our batch gradient will not be an accurate estimate of the true gradient.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的学习中还有另一个让人不安的不稳定因素：最近经验的高相关性。如果我们用最近的经验来训练我们的DQN，这些动作、状态对都会彼此相关。这是有害的，因为我们希望我们的批量梯度能够代表整个梯度，如果我们的数据不代表数据分布，我们的批量梯度就不会准确估计真实梯度。
- en: So, we have to break up this correlation of data in our batches. We can do this
    using something called *experience replay*. In experience replay, we store all
    of the agent’s experiences as a table, and to construct a batch, we randomly sample
    from these experiences. We store these experiences in a table as <math alttext="left-parenthesis
    s Subscript i Baseline comma a Subscript i Baseline comma r Subscript i Baseline
    comma s Subscript i plus 1 Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>r</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>s</mi> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></math>  tuples. From these four values, we can compute our loss
    function, and thus our gradient to optimize our network.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须打破我们批量数据的相关性。我们可以使用一种称为“经验重放”的方法来做到这一点。在经验重放中，我们将所有代理的经验存储为一个表，为了构建一个批次，我们从这些经验中随机抽样。我们将这些经验存储在一个表中，形式为<math
    alttext="left-parenthesis s Subscript i Baseline comma a Subscript i Baseline
    comma r Subscript i Baseline comma s Subscript i plus 1 Baseline right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>r</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>s</mi> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></math> 四元组。通过这四个值，我们可以计算我们的损失函数，从而优化我们的网络。
- en: This experience replay table is more of a queue than a table. The experiences
    an agent sees early in training may not be representative of the experiences a
    trained agent finds itself in later, so it is useful to remove old experiences
    from our table.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个经验重放表更像是一个队列而不是一个表。代理在训练早期看到的经验可能不代表训练后代理发现自己处于的经验，因此有必要从我们的表中删除旧的经验。
- en: From Q-Function to Policy
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Q函数到策略
- en: 'Q-learning is a value learning paradigm, not a policy learning algorithm. This
    means we are not directly learning a policy for acting in our environment. But
    can’t we construct a policy from what our Q-function tells us? If we have learned
    a good Q-function approximation, this means we know the value of every action
    for every state. We could then trivially construct an optimal policy in the following
    way: look at our Q-function for all actions in our current state, choose the action
    with the max Q-value, enter a new state, and repeat. If our Q-function is optimal,
    our policy derived from it will be optimal. With this in mind, we can express
    the optimal policy as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是一种价值学习范式，而不是一种策略学习算法。这意味着我们不是直接学习在环境中行动的策略。但是我们不能根据我们的Q函数告诉我们的内容构建一个策略吗？如果我们学习了一个好的Q函数近似，这意味着我们知道每个状态的每个动作的价值。然后我们可以轻松地按照以下方式构建一个最优策略：查看我们当前状态中所有动作的Q函数，选择具有最大Q值的动作，进入新状态，然后重复。如果我们的Q函数是最优的，那么从中派生出来的策略也将是最优的。考虑到这一点，我们可以将最优策略表达如下：
- en: <math><mrow><mi>π</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">max</mo>
    <msup><mi>a</mi> <mo>'</mo></msup></msub> <mover accent="true"><msup><mi>Q</mi>
    <mo>*</mo></msup> <mo>^</mo></mover> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <msup><mi>a</mi>
    <mo>'</mo></msup> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>π</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">max</mo>
    <msup><mi>a</mi> <mo>'</mo></msup></msub> <mover accent="true"><msup><mi>Q</mi>
    <mo>*</mo></msup> <mo>^</mo></mover> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <msup><mi>a</mi>
    <mo>'</mo></msup> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
- en: We can also use the sampling techniques we discussed earlier to make a stochastic
    policy that sometime deviates from the Q-function recommendations to vary the
    amount of exploration our agent does.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用之前讨论过的采样技术来制定一个随机策略，有时会偏离Q函数的建议，以改变我们的代理程序进行探索的程度。
- en: DQN and the Markov Assumption
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN和马尔可夫假设
- en: DQN is still a Markov decision process that relies on the *Markov assumption*,
    which assumes that the next state <math alttext="s Subscript i plus 1"><msub><mi>s</mi>
    <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></math> depends only on the
    current state <math alttext="s Subscript i"><msub><mi>s</mi> <mi>i</mi></msub></math>
    and action <math alttext="a Subscript i"><msub><mi>a</mi> <mi>i</mi></msub></math>
    , and not on any previous states or actions. This assumption doesn’t hold true
    for many environments where the game’s state cannot be summed up in a single frame.
    For example, in Pong, the ball’s velocity (an important factor in successful game
    play) is not captured in any single game frame. The Markov assumption makes modeling
    decision processes much simpler and reliable, but often at a loss of modeling
    power.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: DQN仍然是一个依赖于*马尔可夫假设*的马尔可夫决策过程，该假设假定下一个状态<math alttext="s Subscript i plus 1"><msub><mi>s</mi>
    <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></math>仅取决于当前状态<math alttext="s
    Subscript i"><msub><mi>s</mi> <mi>i</mi></msub></math>和动作<math alttext="a Subscript
    i"><msub><mi>a</mi> <mi>i</mi></msub></math>，而不取决于任何先前的状态或动作。这个假设对于许多环境并不成立，其中游戏状态无法在单个帧中总结。例如，在乒乓球中，球的速度（成功游戏的重要因素）在任何单个游戏帧中都没有被捕捉到。马尔可夫假设使得建模决策过程变得更简单和可靠，但通常会损失建模能力。
- en: DQN’s Solution to the Markov Assumption
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN对马尔可夫假设的解决方案
- en: DQN solves this problem by utilizing *state history*. Instead of processing
    one game frame as the game’s state, DQN considers the past four game frames as
    the game’s current state. This allows DQN to utilize time-dependent information.
    This is a bit of an engineering hack, and we will discuss better ways of dealing
    with sequences of states at the end of this chapter.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: DQN通过利用*状态历史*来解决这个问题。DQN不是将一个游戏帧作为游戏状态，而是将过去四个游戏帧视为游戏的当前状态。这使得DQN能够利用时间相关信息。这有点工程上的技巧，我们将在本章末尾讨论处理状态序列的更好方法。
- en: Playing Breakout with DQN
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DQN玩Breakout
- en: 'Let’s pull all of what we learned together and actually go about implementing
    DQN to play Breakout. We start out by defining our `DQNAgent`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将我们学到的所有内容整合在一起，实际上开始实施DQN来玩Breakout。我们首先定义我们的`DQNAgent`：
- en: '[PRE7]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: There is a lot going on in this class, so let’s break it down in the following
    sections.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类中有很多内容，让我们在以下部分中逐一解释。
- en: Building Our Architecture
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建我们的架构
- en: 'We build our two Q-networks: the prediction network and the target Q-network.
    Notice how they have the same architecture definition, since they are the same
    network, with the target Q just having delayed parameter updates. Since we are
    learning to play Breakout from pure pixel input, our game state is an array of
    pixels. We pass this image through three convolution layers, and then two fully
    connected layers to produce our Q-values for each of our potential actions.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建两个Q网络：预测网络和目标Q网络。请注意它们具有相同的架构定义，因为它们是相同的网络，只是目标Q具有延迟的参数更新。由于我们正在学习从纯像素输入中玩Breakout，我们的游戏状态是一个像素数组。我们将这个图像通过三个卷积层，然后两个全连接层，以产生我们每个潜在动作的Q值。
- en: Stacking Frames
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠帧
- en: You may notice that our state input is actually of size `[None, self.history_length,
    self.screen_height, self.screen_width]`. Remember, in order to model and capture
    time-dependent state variables like speed, DQN uses not just one image, but a
    group of consecutive images, also known as a *history*. Each of these consecutive
    images is treated as a separate channel. We construct these stacked frames with
    the helper function `process_state_into_stacked_frames(self, frame, past_frames,
    past_state=None)`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能注意到我们的状态输入实际上是大小为`[None, self.history_length, self.screen_height, self.screen_width]`。记住，为了建模和捕捉像速度这样的时间相关状态变量，DQN不仅使用一个图像，而是一组连续的图像，也称为*历史*。这些连续的图像中的每一个被视为一个单独的通道。我们使用辅助函数`process_state_into_stacked_frames(self,
    frame, past_frames, past_state=None)`构建这些堆叠帧。
- en: Setting Up Training Operations
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置训练操作
- en: 'Our loss function is derived from our objective expression from earlier in
    this chapter:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的损失函数源自本章前面的目标表达式：
- en: <math><mrow><msub><mo form="prefix" movablelimits="true">min</mo> <mi>θ</mi></msub>
    <msub><mo>∑</mo> <mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></msub> <msubsup><mo>∑</mo>
    <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></msubsup> <mover accent="true"><mi>Q</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>-</mo>
    <mfenced close=")" open="(" separators=""><msub><mi>r</mi> <mi>t</mi></msub> <mo>+</mo>
    <mi>γ</mi> <msub><mo form="prefix" movablelimits="true">max</mo> <msup><mi>a</mi>
    <mo>'</mo></msup></msub> <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo><msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><msup><mi>a</mi>
    <mo>'</mo></msup> <mo>|</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></math>
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mo form="prefix" movablelimits="true">min</mo> <mi>θ</mi></msub>
    <msub><mo>∑</mo> <mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></msub> <msubsup><mo>∑</mo>
    <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></msubsup> <mover accent="true"><mi>Q</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>-</mo>
    <mfenced close=")" open="(" separators=""><msub><mi>r</mi> <mi>t</mi></msub> <mo>+</mo>
    <mi>γ</mi> <msub><mo form="prefix" movablelimits="true">max</mo> <msup><mi>a</mi>
    <mo>'</mo></msup></msub> <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo><msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><msup><mi>a</mi>
    <mo>'</mo></msup> <mo>|</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></math>
- en: We want our prediction network to equal our target network, plus the return
    at the current time step. We can express this in pure PyTorch code as the difference
    between the output of our prediction network and the output of our target network.
    We use this gradient to update and train our prediction network, using `AdamOptimizer`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的预测网络等于我们的目标网络，再加上当前时间步的回报。我们可以用纯PyTorch代码表示这一点，即我们的预测网络的输出与目标网络的输出之间的差异。我们使用这个梯度来更新和训练我们的预测网络，使用`AdamOptimizer`。
- en: Updating Our Target Q-Network
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新我们的目标Q网络
- en: 'To ensure a stable learning environment, we update our target Q-network only
    once every four batches. Our update rule for the target Q-network is pretty simple:
    we just set its weights equal to the prediction network. We do this in the function
    `update_target_q_network(self)`. The `optimizer_predict.step()` function sets
    the target Q-network’s weights equal to those of the prediction network.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保稳定的学习环境，我们只在每四个批次中更新一次目标Q网络。我们的目标Q网络的更新规则非常简单：我们只需将其权重设置为预测网络的权重。我们在函数`update_target_q_network(self)`中执行这个操作。`optimizer_predict.step()`函数将目标Q网络的权重设置为预测网络的权重。
- en: Implementing Experience Replay
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施经验重放
- en: We’ve discussed how experience replay can help de-correlate our gradient batch
    updates to improve the quality of our Q-learning and subsequent derived policy.
    Let’s walk though a simple implementation of experience replay. We expose a method
    `add_episode(self, episode)`, which takes an entire episode (an `EpisodeHistory`
    object) and adds it to the ExperienceReplayTable. It then checks if the table
    is full and removes the oldest experiences from the table.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了经验重放如何帮助去相关我们的梯度批次更新，以提高我们的Q学习和随后派生的策略的质量。让我们简单实现一下经验重放。我们暴露一个方法`add_episode(self,
    episode)`，它接受整个剧集（一个`EpisodeHistory`对象）并将其添加到ExperienceReplayTable中。然后检查表是否已满，并从表中删除最旧的经验。
- en: 'When it comes time to sample from this table, we can call `sample_batch(self,
    batch_size)` to randomly construct a batch from our table of experiences:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要从这个表中抽样时，我们可以调用`sample_batch(self, batch_size)`来随机构建一个批次从我们的经验表中：
- en: '[PRE8]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: DQN Main Loop
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN主循环
- en: 'Let’s put this all together in our main function, which will create an OpenAI
    Gym environment for Breakout, make an instance of our `DQNAgent`, and have our
    agent interact with and train to play Breakout successfully:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的主函数中将所有这些放在一起，这将为Breakout创建一个OpenAI Gym环境，创建我们的`DQNAgent`的实例，并让我们的代理与成功玩Breakout进行交互和训练：
- en: '[PRE9]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: DQNAgent Results on Breakout
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Breakout上的DQNAgent结果
- en: We train our `DQNAgent` for one thousand episodes to see the learning curve.
    To obtain superhuman results on Atari, typical training time runs up to several
    days. However, we can see a general upward trend in reward pretty quickly, as
    shown in [Figure 13-7](#fig0807).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练我们的`DQNAgent`一千个周期以查看学习曲线。要在Atari上获得超人类的结果，典型的训练时间长达数天。然而，我们可以很快看到奖励的总体上升趋势，如[图13-7](#fig0807)所示。
- en: '![](Images/fdl2_1307.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1307.png)'
- en: Figure 13-7\. Our `DQNAgent` gets increasingly better at Breakout during training
    as it learns a good value function and also acts less stochastically due to <math
    alttext="epsilon"><mi>ϵ</mi></math> -greedy annealing
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-7。我们的`DQNAgent`在训练过程中在Breakout上变得越来越好，因为它学会了一个良好的值函数，并且由于<math alttext="epsilon"><mi>ϵ</mi></math>-贪心退火，行动变得更少随机
- en: Improving and Moving Beyond DQN
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进和超越DQN
- en: DQN did a pretty good job back in 2013 in solving Atari tasks, but had some
    serious shortcomings. DQN’s many weaknesses include that it takes very long to
    train, doesn’t work well on certain types of games, and requires retraining for
    every new game. Much of the deep reinforcement learning research of the past few
    years has been in addressing these various weaknesses.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: DQN在2013年解决Atari任务方面做得相当不错，但也存在一些严重缺陷。DQN的许多弱点包括训练时间非常长，在某些类型的游戏上表现不佳，并且需要为每个新游戏重新训练。过去几年的深度强化学习研究大部分是在解决这些不同的弱点。
- en: Deep Recurrent Q-Networks
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度递归Q网络
- en: 'Remember the Markov assumption? The one that states that the next state relies
    only on the previous state and the action taken by the agent? DQN’s solution to
    the Markov assumption problem, stacking four consecutive frames as separate channels,
    sidesteps this issue and is a bit of an ad hoc engineering hack. Why 4 frames
    and not 10? This imposed frames history hyperparameter limits the model’s generality.
    How do we deal with arbitrary sequences of related data? That’s right: we can
    use what we learned back in ​[Chapter 8](ch08.xhtml#embedding_and_representing_learning)
    on RNNs to model sequences with *deep recurrent Q-networks* (DRQNs).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得马尔可夫假设吗？它说下一个状态仅依赖于前一个状态和代理所采取的行动？DQN对马尔可夫假设问题的解决方案是将四个连续帧堆叠为单独的通道，从而避开了这个问题，有点像临时工程技巧。为什么是4帧而不是10帧？这个强加的帧历史超参数限制了模型的普适性。我们如何处理任意相关数据序列？没错：我们可以使用我们在第8章中学到的关于RNN的知识来模拟具有*深度递归Q网络*（DRQNs）的序列。
- en: DRQN uses a recurrent layer to transfer a latent knowledge of state from one
    time step to the next. In this way, the model itself can learn how many frames
    are informative to include in its state and can even learn to throw away noninformative
    ones or remember things from long ago.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: DRQN使用一个循环层将状态的潜在知识从一个时间步传输到下一个。通过这种方式，模型本身可以学习包含多少帧对其状态有信息量，并且甚至可以学会丢弃无信息的帧或者记住很久以前的事情。
- en: DRQN has even been extended to include neural attention mechanism, as shown
    in Sorokin et al.’s 2015 paper, “Deep Attention Recurrent Q-Network” (DAQRN).^([5](ch13.xhtml#idm45934165544416))
    Since DRQN is dealing with sequences of data, it can attend to certain parts of
    the sequence. This ability to attend to certain parts of the image both improves
    performance and provides model interpretability by producing a rationale for the
    action taken.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: DRQN甚至已经扩展到包括神经注意机制，正如Sorokin等人在2015年的论文“Deep Attention Recurrent Q-Network”（DAQRN）中所示。由于DRQN处理数据序列，它可以关注序列的某些部分。这种关注图像某些部分的能力既提高了性能，又通过为采取的行动提供理由来提供模型可解释性。
- en: DRQN has shown to be better than DQN at playing first-person shooter (FPS) games
    like [DOOM](https://oreil.ly/KKZC7), as well as improving performance on certain
    Atari games with long time dependencies, like [Seaquest](https://oreil.ly/uevTS).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: DRQN在玩第一人称射击游戏（FPS）时表现比DQN更好，比如[DOOM](https://oreil.ly/KKZC7)，同时也提高了在具有长时间依赖性的某些Atari游戏上的表现，比如[Seaquest](https://oreil.ly/uevTS)。
- en: Asynchronous Advantage Actor-Critic Agent
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步优势演员-评论家代理
- en: '*Asynchronous advantage actor-critic* (A3C) is a new approach to deep reinforcement
    learning introduced in the 2016 DeepMind paper, “Asynchronous Methods for Deep
    Reinforcement Learning.”^([6](ch13.xhtml#idm45934165531792)) Let’s discuss what
    it is and why it improves upon DQN.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*异步优势演员-评论家*（A3C）是一种新的深度强化学习方法，介绍于2016年DeepMind的论文“深度强化学习的异步方法”中。让我们讨论一下它是什么以及为什么它改进了DQN。'
- en: A3C is *asynchronous*, which means we can parallelize our agent across many
    threads, which means orders of magnitude faster training by speeding up our environment
    simulation. A3C runs many environments at once to gather experiences. Beyond the
    speed increase, this approach presents another significant advantage in that it
    further decorrelates the experiences in our batches, because the batch is being
    filled with the experiences of numerous agents in different scenarios simultaneously.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: A3C是*异步*的，这意味着我们可以在许多线程中并行化我们的代理，从而通过加速环境模拟来实现数量级上的更快训练。A3C同时运行许多环境以收集经验。除了速度增加外，这种方法还具有另一个重要优势，即进一步使我们批次中的经验解耦，因为批次中填充了同时在不同场景中的众多代理的经验。
- en: 'A3C uses an *actor-critic* method.^([7](ch13.xhtml#idm45934165526704)) Actor-critic
    methods involve learning both a value function <math alttext="upper V left-parenthesis
    s Subscript t Baseline right-parenthesis"><mrow><mi>V</mi> <mo>(</mo> <msub><mi>s</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow></math>  (the critic) and also a policy <math
    alttext="pi left-parenthesis s Subscript t Baseline right-parenthesis"><mrow><mi>π</mi>
    <mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></math> (the actor).
    Early in this chapter, we delineated two different approaches to reinforcement
    learning: value learning and policy learning. A3C combines the strengths of each,
    using the critic’s value function to improve the actor’s policy.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: A3C使用*演员-评论家*方法。演员-评论家方法涉及学习价值函数<math alttext="V(s_t)"><mrow><mi>V</mi> <mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></math>（评论家）以及策略<math alttext="π(s_t)"><mrow><mi>π</mi>
    <mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></math>（演员）。在本章的早期，我们划分了强化学习的两种不同方法：价值学习和策略学习。A3C结合了每种方法的优势，使用评论家的价值函数来改进演员的策略。
- en: 'A3C uses an *advantage *function instead of a pure discounted future return.
    When doing policy learning, we want to penalize the agent when it chooses an action
    that leads to a bad reward. A3C aims to achieve this same goal, but uses advantage
    instead of reward as its criterion. Advantage represents the difference between
    the model’s prediction of the quality of the action taken versus the actual quality
    of the action taken. We can express advantage as:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: A3C使用*优势*函数而不是纯折扣未来回报。在进行策略学习时，我们希望在代理选择导致不良奖励的行动时对其进行惩罚。A3C旨在实现相同的目标，但使用优势而不是奖励作为其标准。优势表示模型对所采取行动的质量的预测与实际所采取行动的质量之间的差异。我们可以将优势表示为：
- en: <math alttext="upper A Subscript t Baseline equals upper Q Superscript asterisk
    Baseline left-parenthesis s Subscript t Baseline comma a Subscript t Baseline
    right-parenthesis minus upper V left-parenthesis s Subscript t Baseline right-parenthesis"><mrow><msub><mi>A</mi>
    <mi>t</mi></msub> <mo>=</mo> <msup><mi>Q</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow> <mo>-</mo> <mi>V</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow></mrow></math> .
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="上标t基线的A等于上标星号基线的Q左括号s基线，a基线右括号减去上标V左括号s基线右括号"><mrow><msub><mi>A</mi>
    <mi>t</mi></msub> <mo>=</mo> <msup><mi>Q</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow> <mo>-</mo> <mi>V</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow></mrow></math>。
- en: 'A3C has a value function, V(t), but it does not express a Q-function. Instead,
    A3C estimates the advantage by using the discounted future reward as an approximation
    for the Q-function:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: A3C有一个价值函数V(t)，但它不表达Q函数。相反，A3C通过使用折扣未来回报来估计优势作为Q函数的近似：
- en: <math alttext="upper A Subscript t Baseline equals upper R Subscript t Baseline
    minus upper V left-parenthesis s Subscript t Baseline right-parenthesis"><mrow><msub><mi>A</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>R</mi> <mi>t</mi></msub> <mo>-</mo> <mi>V</mi>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A Subscript t Baseline equals upper R Subscript t Baseline
    minus upper V left-parenthesis s Subscript t Baseline right-parenthesis"><mrow><msub><mi>A</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>R</mi> <mi>t</mi></msub> <mo>-</mo> <mi>V</mi>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: These three techniques proved key to A3C’s takeover of most deep reinforcement
    learning benchmarks. A3C agents can learn to play Atari Breakout in less than
    12 hours, whereas DQN agents may take 3 to 4 days.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种技术被证明是A3C在大多数深度强化学习基准中取得成功的关键。A3C代理可以在不到12小时内学会玩Atari Breakout，而DQN代理可能需要3到4天。
- en: UNsupervised REinforcement and Auxiliary Learning
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督强化学习和辅助学习
- en: '*UNREAL* is an improvement on A3C introduced in “Reinforcement learning with
    unsupervised auxiliary tasks" by Jaderberg et al.,^([8](ch13.xhtml#idm45934165511120)) who,
    you guessed it, are from DeepMind.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '*UNREAL*是A3C的改进，由Jaderberg等人在“带无监督辅助任务的强化学习”中介绍，他们来自DeepMind。'
- en: UNREAL addresses the problem of reward sparsity. Reinforcement learning is so
    difficult because our agent just receives rewards, and it is hard to determine
    exactly why rewards increase or decrease, which makes learning difficult. Additionally,
    in reinforcement learning, we must learn a good representation of the world as
    well as a good policy to achieve reward. Doing all of this with a weak learning
    signal like sparse rewards is quite a tall order.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: UNREAL解决了奖励稀疏性的问题。强化学习非常困难，因为我们的代理只接收奖励，很难确定奖励增加或减少的确切原因，这使得学习变得困难。此外，在强化学习中，我们必须学习世界的良好表示以及实现奖励的良好策略。使用稀疏奖励这样的弱学习信号来完成所有这些工作是相当困难的。
- en: UNREAL asks the question, what can we learn from the world without rewards?
    It aims to learn a useful world representation in an unsupervised matter. Specifically,
    UNREAL adds some additional unsupervised auxiliary tasks to its overall objective.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: UNREAL提出了一个问题，我们可以从世界中学到什么，而不需要奖励？它旨在以无监督的方式学习一个有用的世界表示。具体来说，UNREAL在其整体目标中添加了一些额外的无监督辅助任务。
- en: The first task involves the UNREAL agent learning about how its actions affect
    the environment. The agent is tasked with controlling pixel values on the screen
    by taking actions. To produce a set of pixel values in the next frame, the agent
    must take a specific action in this frame. In this way, the agent learns how its
    actions affect the world around it, enabling it to learn a representation of the
    world that takes into account its own actions.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个任务涉及UNREAL代理学习其行动如何影响环境。代理被要求通过采取行动来控制屏幕上的像素值。为了在下一帧产生一组像素值，代理必须在这一帧中采取特定的行动。通过这种方式，代理学习其行动如何影响周围的世界，使其能够学习一个考虑自己行动的世界表示。
- en: The second task involves the UNREAL agent learning *reward prediction.* Given
    a sequence of states, the agent is tasked with predicting the value of the next
    reward received. The intuition behind this is that if an agent can predict the
    next reward, it probably has a pretty good model of the future state of the environment,
    which will be useful when constructing a policy.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个任务涉及UNREAL代理学习*奖励预测*。给定一系列状态，代理被要求预测下一个接收到的奖励的值。这背后的直觉是，如果一个代理能够预测下一个奖励，那么它可能对环境未来状态有一个相当好的模型，这在构建策略时将会很有用。
- en: As a result of these unsupervised auxiliary tasks, UNREAL is able to learn around
    10 times faster than A3C in the Labyrynth game environment. UNREAL highlights
    the importance of learning good world representations and how unsupervised learning
    can aid in weak learning signal or low-resource learning problems like reinforcement
    learning.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些无监督的辅助任务，UNREAL能够在Labyrynth游戏环境中比A3C学习速度快约10倍。UNREAL强调了学习良好的世界表示的重要性，以及无监督学习如何帮助弱学习信号或低资源学习问题，如强化学习。
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the fundamentals of reinforcement learning, including
    MDPs, maximum discounted future rewards, and explore versus exploit. We also covered
    various approaches to deep reinforcement learning, including policy gradients
    and deep Q-networks, and touched on some recent improvements on DQN and new developments
    in deep reinforcement learning.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了强化学习的基础知识，包括MDPs、最大折扣未来奖励以及探索与利用。我们还涵盖了深度强化学习的各种方法，包括策略梯度和深度Q网络，并涉及了一些关于DQN的最新改进和深度强化学习的新发展。
- en: Reinforcement learning is essential to building agents that can not only perceive
    and interpret the world, but also take action and interact with it. Deep reinforcement
    learning has made major advancements toward this goal, successfully producing
    agents capable of mastering Atari games, safely driving automobiles, trading stocks
    profitably, controlling robots, and more.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习对于构建能够不仅感知和解释世界，还能够采取行动并与之互动的代理至关重要。深度强化学习在这方面取得了重大进展，成功地产生了能够掌握Atari游戏、安全驾驶汽车、盈利交易股票、控制机器人等的代理。
- en: '^([1](ch13.xhtml#idm45934163590304-marker)) Mnih, Volodymyr, et al. “Human-Level
    Control Through Deep Reinforcement Learning.” *Nature* 518.7540 (2015): 529-533.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch13.xhtml#idm45934163590304-marker)) Mnih, Volodymyr, et al. “Human-Level
    Control Through Deep Reinforcement Learning.” *Nature* 518.7540 (2015): 529-533.'
- en: '^([2](ch13.xhtml#idm45934163585712-marker)) This image is from the OpenAI Gym
    DQN agent that we build in this chapter: Brockman, Greg, et al. “OpenAI Gym.”
    *arXiv preprint arXiv*:1606.01540 (2016). *https://gym.openai.com*'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch13.xhtml#idm45934163585712-marker)) 这幅图片来自我们在本章中构建的OpenAI Gym DQN代理：Brockman,
    Greg, et al. “OpenAI Gym.” *arXiv preprint arXiv*:1606.01540 (2016). *https://gym.openai.com*
- en: ^([3](ch13.xhtml#idm45934163560160-marker)) This image is from our OpenAI Gym
    Policy Gradient agent that we build in this chapter.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch13.xhtml#idm45934163560160-marker)) 这幅图片来自我们在本章中构建的OpenAI Gym策略梯度代理。
- en: ^([4](ch13.xhtml#idm45934166621248-marker)) Sutton, Richard S., et al. “Policy
    Gradient Methods for Reinforcement Learning with Function Approximation.” NIPS.
    Vol. 99\. 1999.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch13.xhtml#idm45934166621248-marker)) Sutton, Richard S., et al. “Policy
    Gradient Methods for Reinforcement Learning with Function Approximation.” NIPS.
    Vol. 99\. 1999.
- en: ^([5](ch13.xhtml#idm45934165544416-marker)) Sorokin, Ivan, et al. “Deep Attention
    Recurrent Q-Network.” *arXiv preprint arXiv*:1512.01693 (2015).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch13.xhtml#idm45934165544416-marker)) Sorokin, Ivan, et al. “Deep Attention
    Recurrent Q-Network.” *arXiv preprint arXiv*:1512.01693 (2015).
- en: ^([6](ch13.xhtml#idm45934165531792-marker)) Mnih, Volodymyr, et al. “Asynchronous
    Methods for Deep Reinforcement Learning.” *International Conference on Machine
    Learning*. 2016.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch13.xhtml#idm45934165531792-marker)) Mnih, Volodymyr, et al. “Asynchronous
    Methods for Deep Reinforcement Learning.” *International Conference on Machine
    Learning*. 2016.
- en: ^([7](ch13.xhtml#idm45934165526704-marker)) Konda, Vijay R., and John N. Tsitsiklis.
    “Actor-Critic Algorithms.” *NIPS*. Vol. 13\. 1999.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch13.xhtml#idm45934165526704-marker)) Konda, Vijay R., and John N. Tsitsiklis.
    “Actor-Critic Algorithms.” *NIPS*. Vol. 13\. 1999.
- en: ^([8](ch13.xhtml#idm45934165511120-marker)) Jaderberg, Max, et al. “Reinforcement
    Learning with Unsupervised Auxiliary Tasks.” *arXiv preprint arXiv*:1611.05397
    (2016).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch13.xhtml#idm45934165511120-marker)) Jaderberg, Max, et al. “Reinforcement
    Learning with Unsupervised Auxiliary Tasks.” *arXiv preprint arXiv*:1611.05397
    (2016).
