- en: Chapter 13\. Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Nicholas Locascio](http://nicklocascio.com)'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll discuss reinforcement learning, which is a branch of
    machine learning that deals with learning via interaction and feedback. Reinforcement
    learning is essential to building agents that can not only perceive and interpret
    the world, but also take action and interact with it. We will discuss how to incorporate
    deep neural networks into the framework of reinforcement learning and discuss
    recent advances and improvements in this field.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning Masters Atari Games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The application of deep neural networks to reinforcement learning had a major
    breakthrough in 2014, when the London startup DeepMind astonished the machine
    learning community by unveiling a deep neural network that could learn to play
    Atari games with superhuman skill. This network, termed a *deep Q-network* (DQN)
    was the first large-scale successful application of reinforcement learning with
    deep neural networks. DQN was so remarkable because the same architecture, without
    any changes, was capable of learning 49 different Atari games, despite each game
    having different rules, goals, and game-play structure. To accomplish this feat,
    DeepMind brought together many traditional ideas in reinforcement learning while
    also developing a few novel techniques that proved key to DQN’s success. Later
    in this chapter, we will implement DQN, as described in the *Nature* paper, “Human-Level
    Control Through Deep Reinforcement Learning.”^([1](ch13.xhtml#idm45934163590304))
    But first, let’s take a dive into reinforcement learning ([Figure 13-1](#fig0801)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. A deep reinforcement learning agent playing Breakout^([2](ch13.xhtml#idm45934163585712))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What Is Reinforcement Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning, at its essentials, is learning by interacting with an
    environment. This learning process involves an *agent*, an *environment*, and
    a *reward signal*. The agent chooses to take an action in the environment, for
    which the agent is rewarded accordingly. The way in which an actor chooses actions
    is called a *policy*. The agent wants to increase the reward it receives, and
    so must learn an optimal policy for interacting with the environment ([Figure 13-2](#fig0802)).
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is different from the other types of learning that we
    have covered thus far. In traditional supervised learning, we are given data and
    labels, and are tasked with predicting labels given data. In unsupervised learning,
    we are given just data and are tasked with discovering underlying structure in
    this data. In reinforcement learning, we are given neither data nor labels. Our
    learning signal is derived from the rewards given to the agent by the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. Reinforcement learning setup
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reinforcement learning is exciting to many in the AI community because it is
    a general-purpose framework for creating intelligent agents. Given an environment
    and some rewards, the agent learns to interact with that environment to maximize
    its total reward. This type of learning is more in line with how humans develop.
    Yes, we can build a pretty good model to classify dogs from cats with extremely
    high accuracy by training on thousands of images. But you won’t find this approach
    used in any elementary schools. Humans interact with their environment to learn
    representations of the world that they can use to make decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, reinforcement learning applications are at the forefront of many
    cutting-edge technologies, including self-driving cars, robotic motor control,
    game playing, air-conditioning control, ad placement optimization, and stock market
    trading strategies.
  prefs: []
  type: TYPE_NORMAL
- en: As an illustrative exercise, we’ll be tackling a simple reinforcement learning
    and control problem called pole balancing. In this problem, there is a cart with
    a pole that is connected by a hinge, so the pole can swing around the cart. There
    is an agent that can control the cart, moving it left or right. There is an environment,
    which rewards the agent when the pole is pointed upward, and penalizes the agent
    when the pole falls over ([Figure 13-3](#fig0803)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1303.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-3\. A simple reinforcement learning agent: balancing a pole^([3](ch13.xhtml#idm45934163560160))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Markov Decision Processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our pole-balancing example has a few important elements, which we formalize
    as a *Markov decision process* (MDP). These elements are:'
  prefs: []
  type: TYPE_NORMAL
- en: State
  prefs: []
  type: TYPE_NORMAL
- en: The cart has a range of possible places on the x-plane where it can be. Similarly,
    the pole has a range of possible angles.
  prefs: []
  type: TYPE_NORMAL
- en: Action
  prefs: []
  type: TYPE_NORMAL
- en: The agent can take action by moving the cart either left or right.
  prefs: []
  type: TYPE_NORMAL
- en: State transition
  prefs: []
  type: TYPE_NORMAL
- en: 'When the agent acts, the environment changes: the cart moves and the pole changes
    angle and velocity.'
  prefs: []
  type: TYPE_NORMAL
- en: Reward
  prefs: []
  type: TYPE_NORMAL
- en: If an agent balances the pole well, it receives a positive reward. If the pole
    falls, the agent receives a negative reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'An MDP is defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper S"><mi>S</mi></math> , a finite set of possible states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="upper A"><mi>A</mi></math> , a finite set of actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis r comma s prime vertical-bar s comma
    a right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>r</mi> <mo>,</mo> <msup><mi>s</mi>
    <mo>'</mo></msup> <mo>|</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></math>
    , a state transition function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="upper R"><mi>R</mi></math> , reward function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MDPs offer a mathematical framework for modeling decision making in a given
    environment. [Figure 13-4](#fig0804) shows an example, with circles representing
    the states of the environment, diamonds representing actions that can be taken,
    and the edges from diamonds to circles representing the transition from one state
    to the next. The numbers along these edges represent the probability of taking
    a certain action, and the numbers at the end of the arrows represent the reward
    given to the agent for making the given transition.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-4\. Example of an MDP
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As an agent takes action in an MDP framework, it forms an *episode*. An episode
    consists of series of tuples of states, actions, and rewards. Episodes run until
    the environment reaches a terminal state, like the “Game Over” screen in Atari
    games, or when the pole hits the ground in our pole-cart example. The following
    equation shows the variables in an episode:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="left-parenthesis s 0 comma a 0 comma r 0 right-parenthesis comma
    left-parenthesis s 1 comma a 1 comma r 1 right-parenthesis comma ellipsis left-parenthesis
    s Subscript n Baseline comma a Subscript n Baseline comma r Subscript n Baseline
    right-parenthesis"><mrow><mrow><mo>(</mo> <msub><mi>s</mi> <mn>0</mn></msub> <mo>,</mo>
    <msub><mi>a</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>r</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow> <mo>,</mo> <mrow><mo>(</mo> <msub><mi>s</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>r</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>n</mi></msub>
    <mo>,</mo> <msub><mi>a</mi> <mi>n</mi></msub> <mo>,</mo> <msub><mi>r</mi> <mi>n</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In pole-cart, our environment state can be a tuple of the position of the cart
    and the angle of the pole, like so: ( <math alttext="x Subscript c a r t"><msub><mi>x</mi>
    <mrow><mi>c</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow></msub></math> ,  <math alttext="theta
    Subscript p o l e"><msub><mi>θ</mi> <mrow><mi>p</mi><mi>o</mi><mi>l</mi><mi>e</mi></mrow></msub></math>
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: Policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MDP’s aim is to find an optimal policy for our agent. *Policies* are how our
    agent acts based on its current state. Formally, policies can be represented as
    a function <math alttext="pi"><mi>π</mi></math>  that chooses the action  <math
    alttext="a"><mi>a</mi></math>  that the agent will take in state  <math alttext="s"><mi>s</mi></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective of our MDP is to find a policy to maximize the expected future
    return:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="max Underscript pi Endscripts upper E left-bracket upper R 0
    plus upper R 1 plus ellipsis upper R Subscript t Baseline vertical-bar pi right-bracket"><mrow><msub><mo
    movablelimits="true" form="prefix">max</mo> <mi>π</mi></msub> <mi>E</mi> <mrow><mo>[</mo>
    <msub><mi>R</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>R</mi> <mn>1</mn></msub>
    <mo>+</mo> <mo>...</mo> <msub><mi>R</mi> <mi>t</mi></msub> <mo>|</mo> <mi>π</mi>
    <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: In this objective, R represents the *future return* of each episode. Let’s define
    exactly what future return means.
  prefs: []
  type: TYPE_NORMAL
- en: Future Return
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Future return is how we consider the rewards of the future. Choosing the best
    action requires consideration of not only the immediate effects of that action,
    but also the long-term consequences. Sometimes the best action actually has a
    negative immediate effect, but a better long-term result. For example, a mountain-climbing
    agent that is rewarded by its altitude may actually have to climb downhill to
    reach a better path to the mountain’s peak.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we want our agents to optimize for *future return*. To do that, the
    agent must consider the future consequences of its actions. For example, in a
    game of Pong, the agent receives a reward when the ball passes into the opponent’s
    goal. However, the actions responsible for this reward (the inputs that position
    the racquet to strike a scoring hit) happen many time steps before the reward
    is received. The reward for each of those actions is delayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can incorporate delayed rewards into our overall reward signal by constructing
    a *return* for each time step that takes into account future rewards as well as
    immediate rewards. A naive approach for calculating *future return* for a time
    step may be a simple sum like so:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R Subscript t Baseline equals sigma-summation Underscript
    k equals 0 Overscript upper T Endscripts r Subscript t plus k"><mrow><msub><mi>R</mi>
    <mi>t</mi></msub> <mo>=</mo> <mrow><munderover><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>T</mi></munderover> <msub><mi>r</mi> <mrow><mi>t</mi><mo>+</mo><mi>k</mi></mrow></msub></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate all returns, R, where  <math alttext="upper R equals StartSet
    upper R 0 comma upper R 1 comma ellipsis upper R Subscript i Baseline comma ellipsis
    upper R Subscript n Baseline EndSet"><mrow><mi>R</mi> <mo>=</mo> <mo>{</mo> <msub><mi>R</mi>
    <mn>0</mn></msub> <mo>,</mo> <msub><mi>R</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo>
    <msub><mi>R</mi> <mi>i</mi></msub> <mo>,</mo> <mo>...</mo> <msub><mi>R</mi> <mi>n</mi></msub>
    <mo>}</mo></mrow></math> , with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This naive approach successfully incorporates future rewards so the agent can
    learn an optimal global policy. This approach values future rewards equally to
    immediate rewards. However, this equal consideration of all rewards is problematic.
    With infinite time steps, this expression can diverge to infinity, so we must
    find a way to bind it. Furthermore, with equal consideration at each time step,
    the agent can optimize for a future reward, and we would learn a policy that lacks
    any sense of urgency or time sensitivity in pursuing its rewards.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we should value future rewards slightly less in order to force our
    agents to learn to get rewards quickly. We accomplish this with a strategy called
    *discounted future return.*
  prefs: []
  type: TYPE_NORMAL
- en: Discounted Future Return
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement discounted future return, we scale the reward of a current state
    by the discount factor, <math alttext="gamma"><mi>γ</mi></math> , to the power
    of the current time step. In this way, we penalize agents that take many actions
    before receiving positive reward. Discounted rewards bias our agent to prefer
    receiving the reward in the immediate future, which is advantageous to learning
    a good policy. We can express the reward as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R Subscript t Baseline equals sigma-summation Underscript
    k equals 0 Overscript upper T Endscripts gamma Superscript t Baseline r Subscript
    t plus k plus 1"><mrow><msub><mi>R</mi> <mi>t</mi></msub> <mo>=</mo> <mrow><munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></munderover> <mrow><msup><mi>γ</mi>
    <mi>t</mi></msup> <msub><mi>r</mi> <mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The discount factor,  <math alttext="gamma"><mi>γ</mi></math> , represents the
    level of discounting we want to achieve, and can be between 0 and 1\. High  <math
    alttext="gamma"><mi>γ</mi></math>  means little discounting, low <math alttext="gamma"><mi>γ</mi></math>
     provides much discounting. A typical <math alttext="gamma"><mi>γ</mi></math>
     hyperparameter setting is between 0.99 and 0.97.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement discounted return like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Explore Versus Exploit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is fundamentally a trial-and-error process. In such a
    framework, an agent afraid to make mistakes can prove to be highly problematic.
    Consider the following scenario. A mouse is placed in the maze shown in [Figure 13-5](#predicament_for_many_mice).
    Our agent must control the mouse to maximize reward. If the mouse gets the water,
    it receives a reward of +1; if the mouse reaches a poison container (red), it
    receives a reward of -10; if the mouse gets the cheese, it receives a reward of
    +100\. Upon receiving reward, the episode is over. The optimal policy involves
    the mouse successfully navigating to the cheese and eating it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-5\. A predicament that many mice find themselves in
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the first episode, the mouse takes the left route, steps on a trap, and receives
    a -10 reward. In the second episode, the mouse avoids the left path, since it
    resulted in such a negative reward, and drinks the water immediately to its right
    for a +1 reward. After two episodes, it would seem that the mouse has found a
    good policy. It continues to follow its learned policy on subsequent episodes
    and achieves the moderate +1 reward reliably. Since our agent utilizes a greedy
    strategy—always choosing the model’s best action—it is stuck in a policy that
    is a *local maximum*.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent such a situation, it may be useful for the agent to deviate from
    the model’s recommendation and take a suboptimal action in order to *explore*
    more of the environment. So instead of taking the immediate right turn to *exploit*
    the environment to get water and the reliable +1 reward, our agent may choose
    to take a left turn and venture into more treacherous areas in search of a more
    optimal policy. Too much exploration, and our agent fails to optimize any reward.
    Not enough exploration can result in our agent getting stuck in a local minimum.
    This balance of *explore versus exploit* is crucial to learning a successful policy.
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="epsilon"><mi>ϵ</mi></math> -Greedy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One strategy for balancing the explore-exploit dilemma is called *<math alttext="epsilon"><mi>ϵ</mi></math>
    -greedy*. <math alttext="epsilon"><mi>ϵ</mi></math> -greedy is a simple strategy
    that involves making a choice at each step to either take the agent’s top recommended
    action or take a random action. The probability that the agent takes a random
    action is the value known as  <math alttext="epsilon"><mi>ϵ</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement <math alttext="epsilon"><mi>ϵ</mi></math> -greedy like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Annealed <math alttext="epsilon"><mi>ϵ</mi></math> -Greedy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When training a reinforcement learning model, oftentimes we want to do more
    exploring in the beginning since our model knows little of the world. Later, once
    our model has seen much of the environment and learned a good policy, we want
    our agent to trust itself more to further optimize its policy. To accomplish this,
    we cast aside the idea of a fixed  <math alttext="epsilon"><mi>ϵ</mi></math> ,
    and instead anneal it over time, having it start low and increase by a factor
    after each training episode. Typical settings for annealed <math alttext="epsilon"><mi>ϵ</mi></math>
    -greedy scenarios include annealing from 0.99 to 0.1 over 10,000 scenarios. We
    can implement annealing like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Policy Versus Value Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far we’ve defined the setup of reinforcement learning, discussed discounted
    future return, and looked at the trade-offs of explore versus exploit. What we
    haven’t talked about is how we’re actually going to teach an agent to maximize
    its reward. Approaches to this fall into two broad categories: *policy learning*
    and *value learning*. In policy learning, we are directly learning a policy that
    maximizes reward. In value learning, we are learning the value of every state
    + action pair. If you were trying to learn to ride a bike, a policy learning approach
    would be to think about how pushing on the right pedal while you were falling
    to the left would course-correct you. If you were trying to learn to ride a bike
    with a value learning approach, you would assign a score to different bike orientations
    and actions you can take in those positions. We’ll be covering both in this chapter,
    so let’s start with policy learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In typical supervised learning, we can use stochastic gradient descent to update
    our parameters to minimize the loss computed from our network’s output and the
    true label. We are optimizing the expression:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="arg min Underscript theta Endscripts sigma-summation Underscript
    i Endscripts log p left-parenthesis y Subscript i Baseline bar x Subscript i Baseline
    semicolon theta right-parenthesis"><mrow><mo form="prefix">arg</mo> <msub><mo
    movablelimits="true" form="prefix">min</mo> <mi>θ</mi></msub> <msub><mo>∑</mo>
    <mi>i</mi></msub> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>∣</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>;</mo> <mi>θ</mi>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In reinforcement learning, we don’t have a true label, only reward signals.
    However, we can still use SGD to optimize our weights using something called *policy
    gradients*.^([4](ch13.xhtml#idm45934166621248)) We can use the actions the agent
    takes, and the returns associated with those actions, to encourage our model weights
    to take good actions that lead to high reward, and to avoid bad ones that lead
    to low reward. The expression we optimize for is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="arg min Underscript theta Endscripts minus sigma-summation Underscript
    i Endscripts upper R Subscript i Baseline log p left-parenthesis y Subscript i
    Baseline bar x Subscript i Baseline semicolon theta right-parenthesis"><mrow><mo
    form="prefix">arg</mo> <msub><mo movablelimits="true" form="prefix">min</mo> <mi>θ</mi></msub>
    <mo>-</mo> <msub><mo>∑</mo> <mi>i</mi></msub> <msub><mi>R</mi> <mi>i</mi></msub>
    <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>∣</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math>
    is the action taken by the agent at time step <math alttext="t"><mi>t</mi></math>
    and where <math alttext="upper R Subscript i"><msub><mi>R</mi> <mi>i</mi></msub></math>
     is our discounted future return. A In this way, we scale our loss by the value
    of our return, so if the model chose an action that led to negative return, this
    would lead to greater loss. Furthermore, if the model is confident in that bad
    decision, it would get penalized even more, since we are taking into account the
    log probability of the model choosing that action. With our loss function defined,
    we can apply SGD to minimize our loss and learn a good policy.
  prefs: []
  type: TYPE_NORMAL
- en: Pole-Cart with Policy Gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re going to implement a policy-gradient agent to solve pole-cart, a classic
    reinforcement learning problem. We will be using an environment from the OpenAI
    Gym created just for this task.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The OpenAI Gym is a Python toolkit for developing reinforcement agents. OpenAI
    Gym provides an easy-to-use interface for interacting with a variety of environments.
    It contains over one hundred open source implementations of common reinforcement
    learning environments. OpenAI Gym speeds up development of reinforcement learning
    agents by handling everything on the environment simulation side, allowing researchers
    to focus on their agent and learning algorithms. Another benefit of OpenAI Gym
    is that researchers can fairly compare and evaluate their results with others
    because they can all use the same standardized environment for a task. We’ll be
    using the pole-cart environment from OpenAI Gym to create an agent that can easily
    interact with this environment.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create an agent that can interact with an OpenAI environment, we’ll define
    a class `PGAgent`, which will contain our model architecture, model weights, and
    hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Building the Model and Optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s break down some important functions. In `build_model()`, we define our
    model architecture as a three-layer neural network. The model returns a layer
    of three nodes, each representing the model’s action probability distribution.
    In `build_training()`, we implement our policy gradient optimizer. We express
    our objective loss as we talked about, scaling the model’s prediction probability
    for an action with the return received for taking that action, and summing these
    all up to form a minibatch. With our objective defined, we can use `torch.optim.AdamOptimizer`,
    which will adjust our weights according to the gradient to minimize our loss.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We define the `predict_action` function, which samples an action based on the
    model’s action probability distribution output. We support the various sampling
    strategies that we talked about to balance explore versus exploit, including greedy,
    ϵ greedy, and ϵ greedy annealing.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping Track of History
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll be aggregating our gradients from multiple episode runs, so it will be
    useful to keep track of state, action, and reward tuples. To this end, we implement
    an episode history and memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Policy Gradient Main Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s put this all together in our main function, which will create an OpenAI
    Gym environment for CartPole, make an instance of our agent, and have our agent
    interact with and train on the CartPole environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This code will train a CartPole agent to successfully and consistently balance
    the pole.
  prefs: []
  type: TYPE_NORMAL
- en: PGAgent Performance on Pole-Cart
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 13-6](#explore_exploit_configurations) is a chart of the average reward
    of our agent at each step of training. We try out 8 different sampling methods,
    and achieve best results with ϵ greedy annealing from 1.0 to 0.001.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-6\. Explore-exploit configurations affect how fast and how well learning
    occurs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice how, across the board, standard ϵ greedy does very poorly. Let’s talk
    about why this might be. With a high ϵ set to 0.9, we are taking a random action
    90% of the time. Even if the model learns to execute the perfect actions, we’ll
    still be using these only 10% of the time. On the other end, with a low ϵ of 0.05,
    we are taking what our model believes to be optimal actions the vast majority
    of the time. This performance is a bit better, but gets stuck in a local reward
    minimum because it lacks the ability to explore other strategies. So neither ϵ
    greedy of 0.05 nor 0.9 gives us great results. The former places too much emphasis
    on exploration, and the latter, too little. This is why ϵ annealing is such a
    powerful sampling strategy. It allows the model to explore early and exploit late,
    which is crucial to learning good policies.
  prefs: []
  type: TYPE_NORMAL
- en: Trust-Region Policy Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Trust-region policy optimization*, or *TRPO* for short, is a framework that
    ensures policy improvement while preventing the policy from shifting too much
    during each training step. TRPO has been empirically shown to outperform many
    of its fellow policy gradient and policy iteration methods, allowing researchers
    to effectively learn complex, nonlinear policies (often parametrized by large
    neural networks) that weren’t previously possible through gradient-based methods.
    In this section, we will motivate TRPO and describe its objective in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of preventing the policy from shifting too much during each training
    step is not a new one—most regularized optimization procedures do this indirectly
    by penalizing the norm of the parameters, for example, globally ensuring the norm
    of the parameters doesn’t get too high. Of course, in cases where regularized
    optimization can also be formulated as constrained optimization (where there are
    explicit bounds on the norm of the parameter vector), such as L2-regularized linear
    regression, we have a direct equivalence to the idea of preventing the policy
    from shifting too much during each training step. The per-step change in the norm
    of the parameters is bounded by the range of the constraint, since all possible
    parameter values must fall in this range. For those interested, I would recommend
    looking further into the equivalence between Tikhonov and Ivanov regularization
    in linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Preventing the policy from shifting too much during each training step has
    the standard effect of regularized optimization: it promotes stability in training,
    which is ideal in preventing overfitting to new data. How do we define a shift
    in the policy? Policies are simply discrete probability distributions over the
    action space given a state,  <math alttext="pi Subscript theta Baseline left-parenthesis
    a vertical-bar s right-parenthesis"><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math> , for which we
    can use notions of dissimilarity, introduced in [Chapter 2](ch02.xhtml#fundamentals-of-proba).
    The original TRPO paper introduced a bound on the average KL divergence (over
    all possible states) between the current policy and the new policy.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve introduced the constraint portion of TRPO’s constrained optimization,
    we will motivate and define the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s recap and introduce some terminology:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="eta left-parenthesis pi right-parenthesis equals"><mrow><mi>η</mi>
    <mo>(</mo> <mi>π</mi> <mo>)</mo> <mo>=</mo></mrow></math>   <math alttext="double-struck
    upper E Subscript s 0 comma a 0 comma s 1 comma a 1 comma ellipsis Baseline left-bracket
    sigma-summation Underscript t equals 0 Overscript normal infinity Endscripts gamma
    Superscript t Baseline r left-parenthesis s Subscript t Baseline right-parenthesis
    right-bracket"><mrow><msub><mi>𝔼</mi> <mrow><msub><mi>s</mi> <mn>0</mn></msub>
    <mo>,</mo><msub><mi>a</mi> <mn>0</mn></msub> <mo>,</mo><msub><mi>s</mi> <mn>1</mn></msub>
    <mo>,</mo><msub><mi>a</mi> <mn>1</mn></msub> <mo>,</mo><mo>...</mo></mrow></msub>
    <mrow><mo>[</mo> <msubsup><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>t</mi></msup> <mi>r</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper Q Subscript pi Baseline left-parenthesis s Subscript t
    Baseline comma a Subscript t Baseline right-parenthesis equals"><mrow><msub><mi>Q</mi>
    <mi>π</mi></msub> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo></mrow></math>
      <math alttext="double-struck upper E Subscript s Sub Subscript t plus 1 Subscript
    comma a Sub Subscript t plus 1 Subscript comma ellipsis Baseline left-bracket
    sigma-summation Underscript l equals 0 Overscript normal infinity Endscripts gamma
    Superscript l Baseline r left-parenthesis s Subscript t plus l Baseline right-parenthesis
    right-bracket"><mrow><msub><mi>𝔼</mi> <mrow><msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo><msub><mi>a</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo><mo>...</mo></mrow></msub> <mrow><mo>[</mo> <msubsup><mo>∑</mo> <mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>l</mi></msup> <mi>r</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mi>l</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper V Subscript pi Baseline left-parenthesis s Subscript t
    Baseline right-parenthesis"><mrow><msub><mi>V</mi> <mi>π</mi></msub> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>   <math alttext="equals
    double-struck upper E Subscript a Sub Subscript t Subscript comma s Sub Subscript
    t plus 1 Subscript comma a Sub Subscript t plus 1 Subscript ellipsis Baseline
    left-bracket sigma-summation Underscript l equals 0 Overscript normal infinity
    Endscripts gamma Superscript l Baseline r left-parenthesis s Subscript t plus
    l Baseline right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi>
    <mrow><msub><mi>a</mi> <mi>t</mi></msub> <mo>,</mo><msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo><msub><mi>a</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>...</mo></mrow></msub> <mrow><mo>[</mo> <msubsup><mo>∑</mo> <mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>l</mi></msup> <mi>r</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mi>l</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper A Subscript pi Baseline left-parenthesis s comma a right-parenthesis
    equals"><mrow><msub><mi>A</mi> <mi>π</mi></msub> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo>
    <mi>a</mi> <mo>)</mo></mrow> <mo>=</mo></mrow></math>   <math alttext="upper Q
    Subscript pi Baseline left-parenthesis s comma a right-parenthesis minus upper
    V Subscript pi Baseline left-parenthesis s right-parenthesis"><mrow><msub><mi>Q</mi>
    <mi>π</mi></msub> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>-</mo> <msub><mi>V</mi> <mi>π</mi></msub> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="rho Subscript pi Baseline left-parenthesis s right-parenthesis
    equals sigma-summation Underscript i equals 0 Overscript normal infinity Endscripts
    gamma Superscript i Baseline upper P left-parenthesis s Subscript t Baseline equals
    s right-parenthesis"><mrow><msub><mi>ρ</mi> <mi>π</mi></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>∞</mi></msubsup> <msup><mi>γ</mi> <mi>i</mi></msup> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>=</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The first term is <math alttext="eta left-parenthesis pi right-parenthesis"><mrow><mi>η</mi>
    <mo>(</mo> <mi>π</mi> <mo>)</mo></mrow></math> , which represents the *expected
    discounted reward*. We saw the finite-time horizon version of the term inside
    the expectation earlier when we discussed future discounted reward. Instead of
    looking at a single trajectory here, we take the expectation over all possible
    trajectories as defined by our policy <math alttext="pi"><mi>π</mi></math> . As
    usual, we can estimate this expectation via an empirical average by sampling trajectories
    using <math alttext="pi"><mi>π</mi></math> . The second term, which will be discussed
    in more detail in [“Q-Learning and Deep Q-Networks”](#q-learning-sect), is the
    *Q-function*  <math alttext="upper Q Subscript pi Baseline left-parenthesis s
    Subscript t Baseline comma a Subscript t Baseline right-parenthesis"><mrow><msub><mi>Q</mi>
    <mi>π</mi></msub> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math> , which looks
    very similar to the previous term but is instead defined as the expected discounted
    return from time *t*, given we are in some state  <math alttext="s Subscript t"><msub><mi>s</mi>
    <mi>t</mi></msub></math>  and perform a defined action  <math alttext="a Subscript
    t"><msub><mi>a</mi> <mi>t</mi></msub></math>  in that state. We again calculate
    the expectation using our policy  <math alttext="pi"><mi>π</mi></math> . Note
    that the time *t *doesn’t actually matter all too much since we only consider
    an infinite time horizon and the expected discounted return from *t *rather than
    from the beginning of the trajectory.
  prefs: []
  type: TYPE_NORMAL
- en: The third term is  <math alttext="upper V Subscript pi Baseline left-parenthesis
    s Subscript t Baseline right-parenthesis"><mrow><msub><mi>V</mi> <mi>π</mi></msub>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
    , or the *value function* at a particular state at time *t*. The value function
    can actually be more concisely written as <math alttext="upper V Subscript pi
    Baseline left-parenthesis s Subscript t Baseline right-parenthesis equals double-struck
    upper E Subscript a Sub Subscript t Baseline left-bracket upper Q Subscript pi
    Baseline left-parenthesis s Subscript t Baseline comma a Subscript t Baseline
    right-parenthesis right-bracket"><mrow><msub><mi>V</mi> <mi>π</mi></msub> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi>
    <msub><mi>a</mi> <mi>t</mi></msub></msub> <mrow><mo>[</mo> <msub><mi>Q</mi> <mi>π</mi></msub>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math> , or the expectation
    of the Q-function with respect to <math alttext="pi left-parenthesis a Subscript
    t Baseline vertical-bar s Subscript t Baseline right-parenthesis"><mrow><mi>π</mi>
    <mo>(</mo> <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>s</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow></math> . In essence, the Q-function supposes that we take a
    defined action <math alttext="a Subscript t"><msub><mi>a</mi> <mi>t</mi></msub></math>
    in state <math alttext="s Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math>
    , while the value function leaves <math alttext="a Subscript t"><msub><mi>a</mi>
    <mi>t</mi></msub></math> as a variable. Thus, to get the value function, all we
    need to do is take the expectation of the Q-function with respect to the distribution
    over <math alttext="a Subscript t"><msub><mi>a</mi> <mi>t</mi></msub></math>  knowing
    the current state <math alttext="s Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math>
    . The result is the weighted average of the Q-function, where the weights are
    <math alttext="pi left-parenthesis a Subscript t Baseline vertical-bar s Subscript
    t Baseline right-parenthesis"><mrow><mi>π</mi> <mo>(</mo> <msub><mi>a</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></math> . In essence,
    this term captures the average future discounted return we’d expect to see starting
    in some state <math alttext="s Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: The fourth term is <math alttext="upper A Subscript pi Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><msub><mi>A</mi> <mi>π</mi></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math> , or the *advantage
    function*. Note that we have dropped the time *t* by now for the reasons mentioned
    earlier. The intuition for the advantage function is that it quantifies, under
    a fixed policy  <math alttext="pi"><mi>π</mi></math> , the benefit of letting
    trajectories play out after taking a particular action <math alttext="a"><mi>a</mi></math>
     in the current state *s*, over simply letting trajectories play out from the
    current state *s* completely unconstrained. Even more concisely, it defines how
    much better, or worse, in the long run it is to initially take action *a* in state
    *s* compared to the average.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final term, or the *unnormalized discounted visitation frequency,* reintroduces
    the time term *t*. This term is a function of the probability of being in state *s*
    at each time *t* from the start to infinity. This term will be important in our
    definition of the objective function. The original TRPO paper chose to optimize
    the model parameters by maximizing this objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper L Subscript theta Sub Subscript o l d Baseline left-parenthesis
    theta right-parenthesis equals sigma-summation Underscript s Endscripts rho Subscript
    theta Sub Subscript o l d Baseline left-parenthesis s right-parenthesis sigma-summation
    Underscript a Endscripts pi Subscript theta Baseline left-parenthesis a vertical-bar
    s right-parenthesis upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><msub><mi>L</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mi>s</mi></msub>
    <msub><mi>ρ</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>a</mi></msub>
    <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi>
    <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="theta Subscript n e w Baseline equals argmax Subscript theta
    Baseline upper L Subscript theta Sub Subscript o l d Baseline left-parenthesis
    theta right-parenthesis"><mrow><msub><mi>θ</mi> <mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub>
    <mo>=</mo> <msub><mtext>argmax</mtext> <mi>θ</mi></msub> <msub><mi>L</mi> <msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo> <mi>θ</mi>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we won’t fully show the derivation behind this objective as it is
    quite mathematically involved and beyond the scope of this text, we provide some
    intuition. Let’s first examine this term: <math alttext="sigma-summation Underscript
    a Endscripts pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis s comma
    a right-parenthesis"><mrow><msub><mo>∑</mo> <mi>a</mi></msub> <msub><mi>π</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
    , assuming a fixed state *s*. For the sake of argument, let’s replace <math alttext="theta"><mi>θ</mi></math>
     with  <math alttext="theta Subscript o l d"><msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></math>
     as our proposed policy’s parameters, which also represents our current policy’s
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="sigma-summation Underscript a Endscripts pi Subscript theta Sub
    Subscript o l d Baseline left-parenthesis a vertical-bar s right-parenthesis upper
    A Subscript pi Sub Subscript theta Sub Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis equals double-struck upper E Subscript a tilde pi
    Sub Subscript theta Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar
    s right-parenthesis Baseline left-bracket upper A Subscript pi Sub Subscript theta
    Sub Sub Subscript o l d Sub Subscript Subscript Baseline left-parenthesis s comma
    a right-parenthesis right-bracket"><mrow><msub><mo>∑</mo> <mi>a</mi></msub> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo>
    <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>A</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript a tilde pi Sub Subscript
    theta Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper Q Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s comma a right-parenthesis
    right-bracket minus double-struck upper E Subscript a tilde pi Sub Subscript theta
    Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>Q</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>-</mo> <msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript a tilde pi Sub Subscript
    theta Sub Sub Subscript o l d Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper Q Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s comma a right-parenthesis
    right-bracket minus upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis"><mrow><mo>=</mo> <msub><mi>𝔼</mi>
    <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>Q</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>-</mo> <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis minus upper V Subscript pi
    Sub Subscript theta Sub Sub Subscript o l d Baseline left-parenthesis s right-parenthesis"><mrow><mo>=</mo>
    <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>-</mo> <msub><mi>V</mi> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals 0"><mrow><mo>=</mo> <mn>0</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: What have we shown here? Earlier, we talked about how <math alttext="upper A
    Subscript pi Sub Subscript theta Sub Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><msub><mi>A</mi> <msub><mi>π</mi> <msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math> defines how much
    better or worse it is, under the current policy, to take action *a* in state *s*
    compared to what we expect to see starting from state *s* unconstrained. Here,
    we showed that if we average each of these advantages weighted by the current
    policy’s distribution, we are left with zero average advantage over the current
    policy—this makes a lot of intuitive sense, since the proposed policy and the
    current policy are the exact same. We don’t expect to see any performance gain
    by replacing the current policy with itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we replace <math alttext="theta"><mi>θ</mi></math>  with a different
    proposed policy’s parameters <math alttext="theta Subscript a l t"><msub><mi>θ</mi>
    <mrow><mi>a</mi><mi>l</mi><mi>t</mi></mrow></msub></math> , the above derivation
    leads us to:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="double-struck upper E Subscript a tilde pi Sub Subscript theta
    Sub Sub Subscript a l t Subscript left-parenthesis a vertical-bar s right-parenthesis
    Baseline left-bracket upper Q Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Sub Subscript Subscript Baseline left-parenthesis s comma a right-parenthesis
    right-bracket minus upper V Subscript pi Sub Subscript theta Sub Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis"><mrow><msub><mi>𝔼</mi> <mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>a</mi><mi>l</mi><mi>t</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>Q</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>-</mo> <msub><mi>V</mi> <msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This is as far as simplification will take us, since the actions in the first
    term are no longer distributed as the current policy and we can’t make the simplification
    that led us to the penultimate step. If we evaluate this expression and we receive
    a positive result, we can interpret the result as representing a positive average
    advantage from following the proposed policy compared to following the current
    policy, directly translating to a performance gain for this specific state *s*
    by replacing the current policy with the proposed policy.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that we have only been considering a specific state *s*. But even if we
    see a performance gain for some state, it might be the case that that state only
    rarely shows up. This leads us to the inclusion of the term  <math alttext="sigma-summation
    Underscript s Endscripts rho Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s right-parenthesis"><mrow><msub><mo>∑</mo> <mi>s</mi></msub> <msub><mi>ρ</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow></mrow></math> , which quantifies how often we see
    a given state. We can actually rewrite this as an expectation even though this
    is an unnormalized distribution—all we’d need to do is factor out the normalizing
    constant, which is also a constant from the perspective of <math alttext="theta"><mi>θ</mi></math>
    since the normalizing constant is solely a function of <math alttext="theta Subscript
    o l d"><msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that <math alttext="sigma-summation Underscript s Endscripts rho
    Subscript theta Sub Subscript o l d Baseline left-parenthesis s right-parenthesis"><mrow><msub><mo>∑</mo>
    <mi>s</mi></msub> <msub><mi>ρ</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math> is evaluated using
    the current policy rather than the proposed policy; this is because, as noted
    in the paper, the complex dependency this introduces on  <math alttext="theta"><mi>θ</mi></math>
     when optimizing the alternative objective (which uses  <math alttext="sigma-summation
    Underscript s Endscripts rho Subscript theta Baseline left-parenthesis s right-parenthesis"><mrow><msub><mo>∑</mo>
    <mi>s</mi></msub> <msub><mi>ρ</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>s</mi>
    <mo>)</mo></mrow></mrow></math> ) with respect to  <math alttext="theta"><mi>θ</mi></math>
     makes the optimization process difficult. Additionally, the paper proves that
    the first-order gradient matches that of the alternative objective anyway, allowing
    us to make this substitution without introducing a biased gradient estimate. We
    won’t show this here, however, as it is beyond the scope of the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting everything together, we have the following constrained optimization
    objective:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="theta Superscript asterisk Baseline equals argmax Subscript theta
    Baseline sigma-summation Underscript s Endscripts rho Subscript theta Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis sigma-summation Underscript
    a Endscripts pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis s comma
    a right-parenthesis"><mrow><msup><mi>θ</mi> <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmax</mtext>
    <mi>θ</mi></msub> <msub><mo>∑</mo> <mi>s</mi></msub> <msub><mi>ρ</mi> <msub><mi>θ</mi>
    <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo> <mi>s</mi>
    <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>a</mi></msub> <msub><mi>π</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mi>A</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="s period t period Avg period KL left-parenthesis theta Subscript
    o l d Baseline comma theta right-parenthesis less-than-or-equal-to delta"><mrow><mi>s</mi>
    <mo>.</mo> <mi>t</mi> <mo>.</mo> <mtext>Avg.</mtext> <mtext>KL</mtext> <mo>(</mo>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub> <mo>,</mo>
    <mi>θ</mi> <mo>)</mo> <mo>≤</mo> <mi>δ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where the average KL divergence denotes the expected KL divergence between
    policies over all states. This is what we call the *trust region,* and it represents
    the parameter settings that lie close enough to the current parameter setting,
    mitigate training instability, and mitigate overfitting. How do we go about optimizing
    this objective? The inner summation looks like an expectation with respect to
    <math alttext="pi Subscript theta Baseline left-parenthesis a comma s right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
    , but all we have is our current setting of parameter values <math alttext="theta
    Subscript o l d"><msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></math>
    . In the standard setting, or *on-policy* setting, we are sampling from the same
    policy we are optimizing, so we can use classic policy gradient optimization for
    this. However, TRPO can be modified to work in the *off-policy* setting as well,
    where the policy we are sampling from is different from the policy we are optimizing.
    Generally, the reason for this distinction is that we may have a behavior policy,
    the policy we are sampling from that may be more exploratory in nature, while
    we learn the target policy, which is to be optimized. In the off-policy setting,
    since we are sampling actions from a different distribution *q(a|s)* (the behavior
    policy) from <math alttext="pi Subscript theta Baseline left-parenthesis a vertical-bar
    s right-parenthesis"><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math> (the target policy),
    we instead use the following constrained optimization objective:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="theta Superscript asterisk Baseline equals argmax Subscript theta
    Baseline sigma-summation Underscript s Endscripts rho Subscript theta Sub Subscript
    o l d Baseline left-parenthesis s right-parenthesis sigma-summation Underscript
    a Endscripts StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar
    s right-parenthesis Over q left-parenthesis a vertical-bar s right-parenthesis
    EndFraction upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><msup><mi>θ</mi> <mo>*</mo></msup> <mo>=</mo>
    <msub><mtext>argmax</mtext> <mi>θ</mi></msub> <msub><mo>∑</mo> <mi>s</mi></msub>
    <msub><mi>ρ</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>a</mi></msub>
    <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="s period t period Avg period KL left-parenthesis theta Subscript
    o l d Baseline comma theta right-parenthesis less-than-or-equal-to delta"><mrow><mi>s</mi>
    <mo>.</mo> <mi>t</mi> <mo>.</mo> <mtext>Avg.</mtext> <mtext>KL</mtext> <mo>(</mo>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub> <mo>,</mo>
    <mi>θ</mi> <mo>)</mo> <mo>≤</mo> <mi>δ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The addition of *q(a|s)* accounts for the fact that we are sampling from a
    separate behavior policy. We can think about this more concretely in terms of
    expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="sigma-summation Underscript a Endscripts pi Subscript theta Baseline
    left-parenthesis a vertical-bar s right-parenthesis upper A Subscript theta Sub
    Subscript o l d Baseline left-parenthesis s comma a right-parenthesis equals sigma-summation
    Underscript a Endscripts StartFraction q left-parenthesis a vertical-bar s right-parenthesis
    Over q left-parenthesis a vertical-bar s right-parenthesis EndFraction pi Subscript
    theta Baseline left-parenthesis a vertical-bar s right-parenthesis upper A Subscript
    theta Sub Subscript o l d Baseline left-parenthesis s comma a right-parenthesis"><mrow><msub><mo>∑</mo>
    <mi>a</mi></msub> <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi>
    <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mo>∑</mo> <mi>a</mi></msub> <mfrac><mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi>
    <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals sigma-summation Underscript a Endscripts q left-parenthesis
    a vertical-bar s right-parenthesis StartFraction pi Subscript theta Baseline left-parenthesis
    a vertical-bar s right-parenthesis Over q left-parenthesis a vertical-bar s right-parenthesis
    EndFraction upper A Subscript theta Sub Subscript o l d Baseline left-parenthesis
    s comma a right-parenthesis"><mrow><mo>=</mo> <msub><mo>∑</mo> <mi>a</mi></msub>
    <mi>q</mi> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow>
    <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript q left-parenthesis a vertical-bar
    s right-parenthesis Baseline left-bracket StartFraction pi Subscript theta Baseline
    left-parenthesis a vertical-bar s right-parenthesis Over q left-parenthesis a
    vertical-bar s right-parenthesis EndFraction upper A Subscript theta Sub Subscript
    o l d Subscript Baseline left-parenthesis s comma a right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that the left side of the first equality can be written as an expectation
    of the advantage with respect to the target policy. In a few algebraic manipulations,
    we were able to convert our original objective into an equivalent objective, but
    with an expectation that is taken with respect to the behavior policy. This is
    ideal because we are sampling from the behavior policy and can thus use standard
    minibatch gradient descent techniques to optimize this objective (adding in the
    constraint on the KL divergence makes this a bit more complicated than just standard
    gradient descent). And finally, we have already seen methods for sampling from
    the unnormalized probability distribution <math alttext="rho Subscript o l d Baseline
    left-parenthesis s right-parenthesis"><mrow><msub><mi>ρ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math> for the outer expectation,
    amongst others that exist in academic literature.
  prefs: []
  type: TYPE_NORMAL
- en: Proximal Policy Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One issue with TRPO is that its optimization is relatively complicated due
    to the inclusion of the average KL divergence term and involves second-order optimization
    to perform. *Proximal policy optimization*, or *PPO* for short, is an algorithm
    that tries to retain the benefits of TRPO without the complicated optimization.
    PPO proposes the following objective instead:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper J left-parenthesis theta right-parenthesis equals double-struck
    upper E left-bracket min left-parenthesis StartFraction pi Subscript theta Baseline
    left-parenthesis a vertical-bar s right-parenthesis Over pi Subscript theta Sub
    Subscript o l d Subscript Baseline left-parenthesis a vertical-bar s right-parenthesis
    EndFraction upper A Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    s comma a right-parenthesis comma clip left-parenthesis StartFraction pi Subscript
    theta Baseline left-parenthesis a vertical-bar s right-parenthesis Over pi Subscript
    theta Sub Subscript o l d Subscript Baseline left-parenthesis a vertical-bar s
    right-parenthesis EndFraction comma 1 minus epsilon comma 1 plus epsilon right-parenthesis
    upper A Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    s comma a right-parenthesis right-parenthesis right-bracket"><mrow><mi>J</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mo>[</mo>
    <mtext>min</mtext> <mrow><mo>(</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>,</mo>
    <mtext>clip</mtext> <mrow><mo>(</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mo>,</mo> <mn>1</mn> <mo>-</mo> <mi>ϵ</mi> <mo>,</mo> <mn>1</mn> <mo>+</mo> <mi>ϵ</mi>
    <mo>)</mo></mrow> <msub><mi>A</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>]</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="theta Superscript asterisk Baseline equals argmax Subscript theta
    Baseline upper J left-parenthesis theta right-parenthesis"><mrow><msup><mi>θ</mi>
    <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmax</mtext> <mi>θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that we no longer have a complex constraint, but rather an extra term built
    into the optimization objective. The clip function represents an upper limit and
    a lower limit on the ratio between the target policy and the behavior policy,
    where any ratio above or below these limits is set equal to the corresponding
    limit. Note the inclusion of the minimum between the original and the clipped,
    which prevents us from making extreme updates and keeps us from overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated in the paper introducing PPO, it’s important to notice that the objective
    for TRPO and PPO have the same gradient at  <math alttext="theta equals theta
    Subscript o l d"><mrow><mi>θ</mi> <mo>=</mo> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></mrow></math>
    . This is the case in at least the on-policy setting, where we have a single policy
    from which we are sampling and optimizing (i.e., no distinction between the behavior
    and target policy). Let’s take a closer look at why this is the case. To do this,
    we first need to reformulate TRPO’s constrained optimization objective as an equivalent
    regularized optimization objective (recall from early in the previous section),
    which we can do according to the theory. The objective looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper J Superscript upper T upper R upper P upper O Baseline
    left-parenthesis theta right-parenthesis equals double-struck upper E left-bracket
    StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    Over pi Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    a vertical-bar s right-parenthesis EndFraction upper A left-parenthesis s comma
    a right-parenthesis minus beta asterisk KL left-parenthesis pi Subscript theta
    Sub Subscript o l d Subscript Baseline left-parenthesis a vertical-bar s right-parenthesis
    StartAbsoluteValue EndAbsoluteValue pi Subscript theta Baseline left-parenthesis
    a vertical-bar s right-parenthesis right-parenthesis right-bracket"><mrow><msup><mi>J</mi>
    <mrow><mi>T</mi><mi>R</mi><mi>P</mi><mi>O</mi></mrow></msup> <mrow><mrow><mo>(</mo>
    <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mo>[</mo></mrow> <mfrac><mrow><msub><mi>π</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mrow><mi>A</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>-</mo> <mi>β</mi> <mo>*</mo> <mtext>KL</mtext> <mo>(</mo></mrow> <msub><mi>π</mi>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub> <mrow><mrow><mo>(</mo>
    <mi>a</mi> <mo>|</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>|</mo> <mo>|</mo></mrow>
    <msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>a</mi> <mo>|</mo> <mi>s</mi>
    <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that we can separate the expression within the expectation into a difference
    of expectations due to the linearity of expectation. If we first consider the
    second expectation, or the KL term, we’ll notice that this term is minimized at 
    <math alttext="theta equals theta Subscript o l d"><mrow><mi>θ</mi> <mo>=</mo>
    <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></mrow></math>
    , since the reference distribution is parametrized using  <math alttext="theta
    Subscript o l d"><msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></math>
    . Thus, the gradient at this setting is zero, since we have already reached the
    global minimum. We are left with only the gradient of the first expectation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal nabla Subscript theta Baseline double-struck upper E left-bracket
    StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    Over pi Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    a vertical-bar s right-parenthesis EndFraction upper A left-parenthesis s comma
    a right-parenthesis right-bracket"><mrow><msub><mi>∇</mi> <mi>θ</mi></msub> <mi>𝔼</mi>
    <mrow><mo>[</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mi>A</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking to the objective for PPO, we notice that at  <math alttext="theta equals
    theta Subscript o l d"><mrow><mi>θ</mi> <mo>=</mo> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></mrow></math>
    , the ratio between the two policies is one, eliminating the need for the clip
    term. Thus, we are left with a minimum over two equivalent terms, which simplifies
    to the expectation over a single term. The gradient comes out to exactly what
    we just saw for the TRPO objective:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal nabla Subscript theta Baseline double-struck upper E left-bracket
    StartFraction pi Subscript theta Baseline left-parenthesis a vertical-bar s right-parenthesis
    Over pi Subscript theta Sub Subscript o l d Subscript Baseline left-parenthesis
    a vertical-bar s right-parenthesis EndFraction upper A left-parenthesis s comma
    a right-parenthesis right-bracket"><mrow><msub><mi>∇</mi> <mi>θ</mi></msub> <mi>𝔼</mi>
    <mrow><mo>[</mo> <mfrac><mrow><msub><mi>π</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>π</mi> <msub><mi>θ</mi> <mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub>
    <mrow><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow></mfrac>
    <mi>A</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We have shown that PPO has the same gradient as TRPO in the select on-policy
    setting, and is additionally much easier to optimize in practice. PPO has also
    shown strong empirical results on a variety of tasks, and has become widely used
    in the field of deep RL.
  prefs: []
  type: TYPE_NORMAL
- en: Q-Learning and Deep Q-Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning is in the category of reinforcement learning called value learning.
    Instead of directly learning a policy, we will be learning the value of states
    and actions. Q-learning involves learning a function, a *Q-function*, which represents
    the quality of a state, action pair. The Q-function, defined *Q(s, a)*, is a function
    that calculates the maximum discounted future return when action *a* is performed
    in state *s*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Q-value represents our expected long-term rewards, given we are at a state,
    and take an action, and then take every subsequent action perfectly (to maximize
    expected future reward). This can be expressed formally as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper Q Superscript asterisk Baseline left-parenthesis s Subscript
    t Baseline comma a Subscript t Baseline right-parenthesis equals m a x Subscript
    pi Baseline upper E left-bracket sigma-summation Underscript i equals t Overscript
    upper T Endscripts gamma Superscript i Baseline r Superscript i Baseline right-bracket"><mrow><msup><mi>Q</mi>
    <mo>*</mo></msup> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>m</mi> <mi>a</mi>
    <msub><mi>x</mi> <mi>π</mi></msub> <mi>E</mi> <mrow><mo>[</mo> <msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mi>t</mi></mrow> <mi>T</mi></msubsup> <mrow><msup><mi>γ</mi>
    <mi>i</mi></msup> <msup><mi>r</mi> <mi>i</mi></msup></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: A question you may be asking is, how can we know Q-values? It is difficult,
    even for humans, to know how good an action is, because you need to know how you
    are going to act in the future. Our expected future returns depend on what our
    long-term strategy is going to be. This seems to be a bit of a chicken-and-egg
    problem. In order to value a state, action pair, you need to know all the perfect
    subsequent actions. And in order to know the best actions, you need to have accurate
    values for a state and action.
  prefs: []
  type: TYPE_NORMAL
- en: The Bellman Equation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We solve this dilemma by defining our Q-values as a function of future Q-values.
    This relation is called the *Bellman equation*, and it states that the maximum
    future reward for taking action is the current reward plus the next step’s *max*
    future reward from taking the next action *a’*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper Q Superscript asterisk Baseline left-parenthesis s Subscript
    t Baseline comma a Subscript t Baseline right-parenthesis equals upper E left-bracket
    r Subscript t Baseline plus gamma max Underscript a prime Endscripts upper Q Superscript
    asterisk Baseline left-parenthesis s Subscript t plus 1 Baseline comma a Superscript
    prime Baseline right-parenthesis right-bracket"><mrow><msup><mi>Q</mi> <mo>*</mo></msup>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>E</mi> <mrow><mo>[</mo> <msub><mi>r</mi>
    <mi>t</mi></msub> <mo>+</mo> <mi>γ</mi> <msub><mo movablelimits="true" form="prefix">max</mo>
    <msup><mi>a</mi> <mo>'</mo></msup></msub> <msup><mi>Q</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msup><mi>a</mi> <msup><mo>'</mo></msup></msup> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This recursive definition allows us to relate between Q-values.
  prefs: []
  type: TYPE_NORMAL
- en: 'And since we can now relate between Q-values past and future, this equation
    conveniently defines an update rule. Namely, we can update past Q-values to be
    based on future Q-values. This is powerful because there exists a Q-value we know
    is correct: the Q-value of the very last action before the episode is over. For
    this last state, we know exactly that the next action led to the next reward,
    so we can perfectly set the Q-values for that state. We can use the update rule,
    then, to propagate that Q-value to the previous time step:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="ModifyingAbove upper Q Subscript j Baseline With caret right-arrow
    ModifyingAbove upper Q Subscript j plus 1 Baseline With caret right-arrow ModifyingAbove
    upper Q Subscript j plus 2 Baseline With caret right-arrow ellipsis right-arrow
    upper Q Superscript asterisk"><mrow><mover accent="true"><msub><mi>Q</mi> <mi>j</mi></msub>
    <mo>^</mo></mover> <mo>→</mo> <mover accent="true"><msub><mi>Q</mi> <mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>^</mo></mover> <mo>→</mo> <mover accent="true"><msub><mi>Q</mi> <mrow><mi>j</mi><mo>+</mo><mn>2</mn></mrow></msub>
    <mo>^</mo></mover> <mo>→</mo> <mo>...</mo> <mo>→</mo> <msup><mi>Q</mi> <mo>*</mo></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This updating of the  Q-value is known as *value iteration*.
  prefs: []
  type: TYPE_NORMAL
- en: Our first Q-value starts out completely wrong, but this is perfectly acceptable.
    With each iteration, we can update our Q-value via the correct one from the future.
    After one iteration, the last Q-value is accurate, since it is just the reward
    from the last state and action before episode termination. Then we perform our
    Q-value update, which sets the second-to-last Q-value. In our next iteration,
    we can guarantee that the last two Q-values are correct, and so on and so forth.
    Through value iteration, we will be guaranteed convergence on the ultimate optimal
    Q-value.
  prefs: []
  type: TYPE_NORMAL
- en: Issues with Value Iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Value iteration produces a mapping between state and action pairs with corresponding
    Q-values, and we are constructing a table of these mappings, or a *Q-table*. Let’s
    briefly talk about the size of this Q-table. Value iteration is an exhaustive
    process that requires a full traversal of the entire space of state, action pairs.
    In a game like Breakout, with 100 bricks that can be either present or not, with
    50 positions for the paddle to be in, and 250 positions for the ball to be in,
    and 3 actions, we have already constructed a space that is far, far larger than
    the sum of all computational capacity of humanity. Furthermore, in stochastic
    environments, the space of our Q-table would be even larger, and possibly infinite.
    With such a large space, it will be intractable for us to find all of the Q-values
    for every state, action pair. Clearly this approach is not going to work. How
    else are we going to do Q-learning?
  prefs: []
  type: TYPE_NORMAL
- en: Approximating the Q-Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The size of our Q-table makes the naive approach intractable for any nontoy
    problem. However, what if we relax our requirement for an optimal Q-function?
    If instead, we learn approximations of the Q-function, we can use a model to estimate
    our Q-function. Instead of having to experience every state, action pair to update
    our Q-table, we can learn a function that approximates this table, and even generalizes
    outside of its own experience. This means we won’t have to perform an exhaustive
    search through all possible Q-values to learn a Q-function.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was the main motivation behind DeepMind’s work on deep Q-network (DQN).
    DQN uses a deep neural network that takes an image (the state) in to estimate
    the Q-value for all possible actions.
  prefs: []
  type: TYPE_NORMAL
- en: Training DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We would like to train our network to approximate the Q-function. We express
    this Q-function approximation as a function of our model’s parameters, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="ModifyingAbove upper Q Subscript theta Baseline With caret left-parenthesis
    s comma a bar theta right-parenthesis tilde upper Q Superscript asterisk Baseline
    left-parenthesis s comma a right-parenthesis"><mrow><mover accent="true"><msub><mi>Q</mi>
    <mi>θ</mi></msub> <mo>^</mo></mover> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi>
    <mo>∣</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>∼</mo> <msup><mi>Q</mi> <mo>*</mo></msup>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, Q-learning is a value-learning algorithm. We are not learning a policy
    directly, but rather we are learning the values of each state, action pair, regardless
    if they are good or not. We have expressed our model’s Q-function approximation
    as Qtheta, and we would like this to be close to the future expected reward. Using
    the Bellman equation from earlier, we can express this future expected reward
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><msubsup><mi>R</mi> <mi>t</mi> <mo>*</mo></msubsup> <mo>=</mo> <mfenced
    close=")" open="(" separators=""><msub><mi>r</mi> <mi>t</mi></msub> <mo>+</mo>
    <mi>γ</mi> <msub><mo form="prefix" movablelimits="true">max</mo> <msup><mi>a</mi>
    <mo>'</mo></msup></msub> <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo><msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><msup><mi>a</mi>
    <mo>'</mo></msup> <mo>|</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Our objective is to minimize the difference between our Q’s approximation,
    and the next Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="min Underscript theta Endscripts sigma-summation Underscript
    e element-of upper E Endscripts sigma-summation Underscript t equals 0 Overscript
    upper T Endscripts ModifyingAbove upper Q With caret left-parenthesis s Subscript
    t Baseline comma a Subscript t Baseline vertical-bar theta right-parenthesis minus
    upper R Subscript t Superscript asterisk"><mrow><msub><mo movablelimits="true"
    form="prefix">min</mo> <mi>θ</mi></msub> <msub><mo>∑</mo> <mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></msub>
    <msubsup><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></msubsup>
    <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi>
    <mo>)</mo></mrow> <mo>-</mo> <msubsup><mi>R</mi> <mi>t</mi> <mo>*</mo></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Expanding this expression gives us our full objective:'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><msub><mo form="prefix" movablelimits="true">min</mo> <mi>θ</mi></msub>
    <msub><mo>∑</mo> <mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></msub> <msubsup><mo>∑</mo>
    <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></msubsup> <mover accent="true"><mi>Q</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>-</mo>
    <mfenced close=")" open="(" separators=""><msub><mi>r</mi> <mi>t</mi></msub> <mo>+</mo>
    <mi>γ</mi> <msub><mo form="prefix" movablelimits="true">max</mo> <msup><mi>a</mi>
    <mo>'</mo></msup></msub> <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo><msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><msup><mi>a</mi>
    <mo>'</mo></msup> <mo>|</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This objective is fully differentiable as a function of our model parameters,
    and we can find gradients to use in stochastic gradient descent to minimize this
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Stability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One issue you may have noticed is that we are defining our loss function based
    on the difference of our model’s predicted Q-value of this step and the predicted
    Q-value of the next step. In this way, our loss is doubly dependent on our model
    parameters. With each parameter update, the Q-values are constantly shifting,
    and we are using shifting Q-values to do further updates. This high correlation
    of updates can lead to feedback loops and instability in our learning, where our
    parameters may oscillate and make the loss diverge.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can employ a couple of simple engineering hacks to remedy this correlation
    problem: namely, target Q-network and experience replay.'
  prefs: []
  type: TYPE_NORMAL
- en: Target Q-Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of updating a single network frequently with respect to itself, we can
    reduce this codependence by introducing a second network, called the *target network*.
    Our loss function features to instances of the Q-function,  <math alttext="ModifyingAbove
    upper Q With caret left-parenthesis s Subscript t Baseline comma a Subscript t
    Baseline vertical-bar theta right-parenthesis"><mrow><mover accent="true"><mi>Q</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
     and   <math><mrow><mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msup><mi>a</mi> <mo>'</mo></msup> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
    . We are going to have the first Q be represented as our prediction network, and
    our second Q will be produced by the target Q-network. The target Q-network is
    a copy of our prediction network that lags in its parameter updates. We update
    the target Q-network to equal the prediction network only every few batches. This
    provides much needed stability to our Q-values, and we can now properly learn
    a good Q-function.
  prefs: []
  type: TYPE_NORMAL
- en: Experience Replay
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is yet another source of irksome instability to our learning: the high
    correlations of recent experiences. If we train our DQN with batches drawn from
    recent experience, these action, state pairs are all going to be related to one
    another. This is harmful because we want our batch gradients to be representative
    of the entire gradient, and if our data is not representative of the data distribution,
    our batch gradient will not be an accurate estimate of the true gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: So, we have to break up this correlation of data in our batches. We can do this
    using something called *experience replay*. In experience replay, we store all
    of the agent’s experiences as a table, and to construct a batch, we randomly sample
    from these experiences. We store these experiences in a table as <math alttext="left-parenthesis
    s Subscript i Baseline comma a Subscript i Baseline comma r Subscript i Baseline
    comma s Subscript i plus 1 Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>r</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>s</mi> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></math>  tuples. From these four values, we can compute our loss
    function, and thus our gradient to optimize our network.
  prefs: []
  type: TYPE_NORMAL
- en: This experience replay table is more of a queue than a table. The experiences
    an agent sees early in training may not be representative of the experiences a
    trained agent finds itself in later, so it is useful to remove old experiences
    from our table.
  prefs: []
  type: TYPE_NORMAL
- en: From Q-Function to Policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Q-learning is a value learning paradigm, not a policy learning algorithm. This
    means we are not directly learning a policy for acting in our environment. But
    can’t we construct a policy from what our Q-function tells us? If we have learned
    a good Q-function approximation, this means we know the value of every action
    for every state. We could then trivially construct an optimal policy in the following
    way: look at our Q-function for all actions in our current state, choose the action
    with the max Q-value, enter a new state, and repeat. If our Q-function is optimal,
    our policy derived from it will be optimal. With this in mind, we can express
    the optimal policy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mi>π</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">max</mo>
    <msup><mi>a</mi> <mo>'</mo></msup></msub> <mover accent="true"><msup><mi>Q</mi>
    <mo>*</mo></msup> <mo>^</mo></mover> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <msup><mi>a</mi>
    <mo>'</mo></msup> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We can also use the sampling techniques we discussed earlier to make a stochastic
    policy that sometime deviates from the Q-function recommendations to vary the
    amount of exploration our agent does.
  prefs: []
  type: TYPE_NORMAL
- en: DQN and the Markov Assumption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DQN is still a Markov decision process that relies on the *Markov assumption*,
    which assumes that the next state <math alttext="s Subscript i plus 1"><msub><mi>s</mi>
    <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></math> depends only on the
    current state <math alttext="s Subscript i"><msub><mi>s</mi> <mi>i</mi></msub></math>
    and action <math alttext="a Subscript i"><msub><mi>a</mi> <mi>i</mi></msub></math>
    , and not on any previous states or actions. This assumption doesn’t hold true
    for many environments where the game’s state cannot be summed up in a single frame.
    For example, in Pong, the ball’s velocity (an important factor in successful game
    play) is not captured in any single game frame. The Markov assumption makes modeling
    decision processes much simpler and reliable, but often at a loss of modeling
    power.
  prefs: []
  type: TYPE_NORMAL
- en: DQN’s Solution to the Markov Assumption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DQN solves this problem by utilizing *state history*. Instead of processing
    one game frame as the game’s state, DQN considers the past four game frames as
    the game’s current state. This allows DQN to utilize time-dependent information.
    This is a bit of an engineering hack, and we will discuss better ways of dealing
    with sequences of states at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Playing Breakout with DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s pull all of what we learned together and actually go about implementing
    DQN to play Breakout. We start out by defining our `DQNAgent`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot going on in this class, so let’s break it down in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Building Our Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We build our two Q-networks: the prediction network and the target Q-network.
    Notice how they have the same architecture definition, since they are the same
    network, with the target Q just having delayed parameter updates. Since we are
    learning to play Breakout from pure pixel input, our game state is an array of
    pixels. We pass this image through three convolution layers, and then two fully
    connected layers to produce our Q-values for each of our potential actions.'
  prefs: []
  type: TYPE_NORMAL
- en: Stacking Frames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may notice that our state input is actually of size `[None, self.history_length,
    self.screen_height, self.screen_width]`. Remember, in order to model and capture
    time-dependent state variables like speed, DQN uses not just one image, but a
    group of consecutive images, also known as a *history*. Each of these consecutive
    images is treated as a separate channel. We construct these stacked frames with
    the helper function `process_state_into_stacked_frames(self, frame, past_frames,
    past_state=None)`.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Training Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our loss function is derived from our objective expression from earlier in
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><msub><mo form="prefix" movablelimits="true">min</mo> <mi>θ</mi></msub>
    <msub><mo>∑</mo> <mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></msub> <msubsup><mo>∑</mo>
    <mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow> <mi>T</mi></msubsup> <mover accent="true"><mi>Q</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>-</mo>
    <mfenced close=")" open="(" separators=""><msub><mi>r</mi> <mi>t</mi></msub> <mo>+</mo>
    <mi>γ</mi> <msub><mo form="prefix" movablelimits="true">max</mo> <msup><mi>a</mi>
    <mo>'</mo></msup></msub> <mover accent="true"><mi>Q</mi> <mo>^</mo></mover> <mrow><mo>(</mo><msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo><msup><mi>a</mi>
    <mo>'</mo></msup> <mo>|</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We want our prediction network to equal our target network, plus the return
    at the current time step. We can express this in pure PyTorch code as the difference
    between the output of our prediction network and the output of our target network.
    We use this gradient to update and train our prediction network, using `AdamOptimizer`.
  prefs: []
  type: TYPE_NORMAL
- en: Updating Our Target Q-Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To ensure a stable learning environment, we update our target Q-network only
    once every four batches. Our update rule for the target Q-network is pretty simple:
    we just set its weights equal to the prediction network. We do this in the function
    `update_target_q_network(self)`. The `optimizer_predict.step()` function sets
    the target Q-network’s weights equal to those of the prediction network.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Experience Replay
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve discussed how experience replay can help de-correlate our gradient batch
    updates to improve the quality of our Q-learning and subsequent derived policy.
    Let’s walk though a simple implementation of experience replay. We expose a method
    `add_episode(self, episode)`, which takes an entire episode (an `EpisodeHistory`
    object) and adds it to the ExperienceReplayTable. It then checks if the table
    is full and removes the oldest experiences from the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes time to sample from this table, we can call `sample_batch(self,
    batch_size)` to randomly construct a batch from our table of experiences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: DQN Main Loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s put this all together in our main function, which will create an OpenAI
    Gym environment for Breakout, make an instance of our `DQNAgent`, and have our
    agent interact with and train to play Breakout successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: DQNAgent Results on Breakout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We train our `DQNAgent` for one thousand episodes to see the learning curve.
    To obtain superhuman results on Atari, typical training time runs up to several
    days. However, we can see a general upward trend in reward pretty quickly, as
    shown in [Figure 13-7](#fig0807).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-7\. Our `DQNAgent` gets increasingly better at Breakout during training
    as it learns a good value function and also acts less stochastically due to <math
    alttext="epsilon"><mi>ϵ</mi></math> -greedy annealing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Improving and Moving Beyond DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DQN did a pretty good job back in 2013 in solving Atari tasks, but had some
    serious shortcomings. DQN’s many weaknesses include that it takes very long to
    train, doesn’t work well on certain types of games, and requires retraining for
    every new game. Much of the deep reinforcement learning research of the past few
    years has been in addressing these various weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Recurrent Q-Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember the Markov assumption? The one that states that the next state relies
    only on the previous state and the action taken by the agent? DQN’s solution to
    the Markov assumption problem, stacking four consecutive frames as separate channels,
    sidesteps this issue and is a bit of an ad hoc engineering hack. Why 4 frames
    and not 10? This imposed frames history hyperparameter limits the model’s generality.
    How do we deal with arbitrary sequences of related data? That’s right: we can
    use what we learned back in ​[Chapter 8](ch08.xhtml#embedding_and_representing_learning)
    on RNNs to model sequences with *deep recurrent Q-networks* (DRQNs).'
  prefs: []
  type: TYPE_NORMAL
- en: DRQN uses a recurrent layer to transfer a latent knowledge of state from one
    time step to the next. In this way, the model itself can learn how many frames
    are informative to include in its state and can even learn to throw away noninformative
    ones or remember things from long ago.
  prefs: []
  type: TYPE_NORMAL
- en: DRQN has even been extended to include neural attention mechanism, as shown
    in Sorokin et al.’s 2015 paper, “Deep Attention Recurrent Q-Network” (DAQRN).^([5](ch13.xhtml#idm45934165544416))
    Since DRQN is dealing with sequences of data, it can attend to certain parts of
    the sequence. This ability to attend to certain parts of the image both improves
    performance and provides model interpretability by producing a rationale for the
    action taken.
  prefs: []
  type: TYPE_NORMAL
- en: DRQN has shown to be better than DQN at playing first-person shooter (FPS) games
    like [DOOM](https://oreil.ly/KKZC7), as well as improving performance on certain
    Atari games with long time dependencies, like [Seaquest](https://oreil.ly/uevTS).
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous Advantage Actor-Critic Agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Asynchronous advantage actor-critic* (A3C) is a new approach to deep reinforcement
    learning introduced in the 2016 DeepMind paper, “Asynchronous Methods for Deep
    Reinforcement Learning.”^([6](ch13.xhtml#idm45934165531792)) Let’s discuss what
    it is and why it improves upon DQN.'
  prefs: []
  type: TYPE_NORMAL
- en: A3C is *asynchronous*, which means we can parallelize our agent across many
    threads, which means orders of magnitude faster training by speeding up our environment
    simulation. A3C runs many environments at once to gather experiences. Beyond the
    speed increase, this approach presents another significant advantage in that it
    further decorrelates the experiences in our batches, because the batch is being
    filled with the experiences of numerous agents in different scenarios simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'A3C uses an *actor-critic* method.^([7](ch13.xhtml#idm45934165526704)) Actor-critic
    methods involve learning both a value function <math alttext="upper V left-parenthesis
    s Subscript t Baseline right-parenthesis"><mrow><mi>V</mi> <mo>(</mo> <msub><mi>s</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow></math>  (the critic) and also a policy <math
    alttext="pi left-parenthesis s Subscript t Baseline right-parenthesis"><mrow><mi>π</mi>
    <mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></math> (the actor).
    Early in this chapter, we delineated two different approaches to reinforcement
    learning: value learning and policy learning. A3C combines the strengths of each,
    using the critic’s value function to improve the actor’s policy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A3C uses an *advantage *function instead of a pure discounted future return.
    When doing policy learning, we want to penalize the agent when it chooses an action
    that leads to a bad reward. A3C aims to achieve this same goal, but uses advantage
    instead of reward as its criterion. Advantage represents the difference between
    the model’s prediction of the quality of the action taken versus the actual quality
    of the action taken. We can express advantage as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper A Subscript t Baseline equals upper Q Superscript asterisk
    Baseline left-parenthesis s Subscript t Baseline comma a Subscript t Baseline
    right-parenthesis minus upper V left-parenthesis s Subscript t Baseline right-parenthesis"><mrow><msub><mi>A</mi>
    <mi>t</mi></msub> <mo>=</mo> <msup><mi>Q</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow> <mo>-</mo> <mi>V</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'A3C has a value function, V(t), but it does not express a Q-function. Instead,
    A3C estimates the advantage by using the discounted future reward as an approximation
    for the Q-function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper A Subscript t Baseline equals upper R Subscript t Baseline
    minus upper V left-parenthesis s Subscript t Baseline right-parenthesis"><mrow><msub><mi>A</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>R</mi> <mi>t</mi></msub> <mo>-</mo> <mi>V</mi>
    <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: These three techniques proved key to A3C’s takeover of most deep reinforcement
    learning benchmarks. A3C agents can learn to play Atari Breakout in less than
    12 hours, whereas DQN agents may take 3 to 4 days.
  prefs: []
  type: TYPE_NORMAL
- en: UNsupervised REinforcement and Auxiliary Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*UNREAL* is an improvement on A3C introduced in “Reinforcement learning with
    unsupervised auxiliary tasks" by Jaderberg et al.,^([8](ch13.xhtml#idm45934165511120)) who,
    you guessed it, are from DeepMind.'
  prefs: []
  type: TYPE_NORMAL
- en: UNREAL addresses the problem of reward sparsity. Reinforcement learning is so
    difficult because our agent just receives rewards, and it is hard to determine
    exactly why rewards increase or decrease, which makes learning difficult. Additionally,
    in reinforcement learning, we must learn a good representation of the world as
    well as a good policy to achieve reward. Doing all of this with a weak learning
    signal like sparse rewards is quite a tall order.
  prefs: []
  type: TYPE_NORMAL
- en: UNREAL asks the question, what can we learn from the world without rewards?
    It aims to learn a useful world representation in an unsupervised matter. Specifically,
    UNREAL adds some additional unsupervised auxiliary tasks to its overall objective.
  prefs: []
  type: TYPE_NORMAL
- en: The first task involves the UNREAL agent learning about how its actions affect
    the environment. The agent is tasked with controlling pixel values on the screen
    by taking actions. To produce a set of pixel values in the next frame, the agent
    must take a specific action in this frame. In this way, the agent learns how its
    actions affect the world around it, enabling it to learn a representation of the
    world that takes into account its own actions.
  prefs: []
  type: TYPE_NORMAL
- en: The second task involves the UNREAL agent learning *reward prediction.* Given
    a sequence of states, the agent is tasked with predicting the value of the next
    reward received. The intuition behind this is that if an agent can predict the
    next reward, it probably has a pretty good model of the future state of the environment,
    which will be useful when constructing a policy.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of these unsupervised auxiliary tasks, UNREAL is able to learn around
    10 times faster than A3C in the Labyrynth game environment. UNREAL highlights
    the importance of learning good world representations and how unsupervised learning
    can aid in weak learning signal or low-resource learning problems like reinforcement
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the fundamentals of reinforcement learning, including
    MDPs, maximum discounted future rewards, and explore versus exploit. We also covered
    various approaches to deep reinforcement learning, including policy gradients
    and deep Q-networks, and touched on some recent improvements on DQN and new developments
    in deep reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is essential to building agents that can not only perceive
    and interpret the world, but also take action and interact with it. Deep reinforcement
    learning has made major advancements toward this goal, successfully producing
    agents capable of mastering Atari games, safely driving automobiles, trading stocks
    profitably, controlling robots, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch13.xhtml#idm45934163590304-marker)) Mnih, Volodymyr, et al. “Human-Level
    Control Through Deep Reinforcement Learning.” *Nature* 518.7540 (2015): 529-533.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch13.xhtml#idm45934163585712-marker)) This image is from the OpenAI Gym
    DQN agent that we build in this chapter: Brockman, Greg, et al. “OpenAI Gym.”
    *arXiv preprint arXiv*:1606.01540 (2016). *https://gym.openai.com*'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch13.xhtml#idm45934163560160-marker)) This image is from our OpenAI Gym
    Policy Gradient agent that we build in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch13.xhtml#idm45934166621248-marker)) Sutton, Richard S., et al. “Policy
    Gradient Methods for Reinforcement Learning with Function Approximation.” NIPS.
    Vol. 99\. 1999.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch13.xhtml#idm45934165544416-marker)) Sorokin, Ivan, et al. “Deep Attention
    Recurrent Q-Network.” *arXiv preprint arXiv*:1512.01693 (2015).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch13.xhtml#idm45934165531792-marker)) Mnih, Volodymyr, et al. “Asynchronous
    Methods for Deep Reinforcement Learning.” *International Conference on Machine
    Learning*. 2016.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch13.xhtml#idm45934165526704-marker)) Konda, Vijay R., and John N. Tsitsiklis.
    “Actor-Critic Algorithms.” *NIPS*. Vol. 13\. 1999.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch13.xhtml#idm45934165511120-marker)) Jaderberg, Max, et al. “Reinforcement
    Learning with Unsupervised Auxiliary Tasks.” *arXiv preprint arXiv*:1611.05397
    (2016).
  prefs: []
  type: TYPE_NORMAL
