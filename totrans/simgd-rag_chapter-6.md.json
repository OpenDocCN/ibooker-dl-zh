["```py\nfrom langchain_community.document_loaders #1\nimport AsyncHtmlLoader   #1\nfrom langchain_community.document_transformers  #1\nimport Html2TextTransformer   #1\nurl=\t\t #1\nhttps://en.wikipedia.org/wiki/2023_Cricket_World_Cup   #1\nloader = AsyncHtmlLoader (url)   #1\ndata = loader.load()   #1\nhtml2text = Html2TextTransformer()   #1\ndocument_text=data_transformed[0].page_content   #1\n #1\n #1\nsummary_prompt = f\"Summarize the given \t\t#2\ndocument in a single paragraph\\n\t\t #2\ndocument: {document_text}\"  \t #2\nfrom openai import OpenAI   #2\nclient = OpenAI()   #2\n #2\nresponse = client.chat.completions.create(   #2\n  model=\"gpt-4o-mini\",   #2\n  messages= [   #2\n    {\"role\": \"user\", \"content\": summary_prompt}   #2\n      ]   #2\n)   #2\n #2\nsummary=response.choices[0].message.content   #2\n #2\nfrom langchain_text_splitters import \t\t#3\nRecursiveCharacterTextSplitter    #3\ntext_splitter = RecursiveCharacterTextSplitter(    #3\nchunk_size=1000,    #3\nchunk_overlap=200)   #3\nchunks=text_splitter.split_text(\t\t\t\t\t#3\ndata_transformed[0].page_content\t#3\n)    #3\n\ncontext_enriched_chunks = \t\t\t\t\t#4\n[answer + \"\\n\" + chunk for chunk in chunks]   #4\n\nembedding = OpenAIEmbeddings(openai_api_key=api_key)  #5\nvector_store = FAISS.from_texts(\t\t\t #5\ncontext_enriched_chunks,  #5\nembedding\t #5\n)   #5\n```", "```py\noriginal_query=\"How does climate change affect polar bears?\"\nnum=5\n\nexpansion_prompt=f\"Generate {num} variations \t#1\nof the following query: {original_query}. \t\t #1\nRespond in JSON format.\"   #1\n\nfrom openai import OpenAI  #2\nclient = OpenAI()   #2\nresponse = client.chat.completions.create(  #2\n  model=\"gpt-4o-mini\",   #2\n  messages= [   #2\n    {\"role\": \"user\", \"content\": expansion_prompt}   #2\n      ],   #2\n          response_format={ \"type\": \"json_object\" }   #2\n)   #2\n #2\nexpanded_queries=response.choices[0].message.content  #3\n```", "```py\nsub_query_expansion_prompt=f\" \\\nBreak down the following \\\nquery into {num} sub-queries targeting \\\ndifferent aspects of the query: {original_query}. \\\nRespond in JSON format. \"\n```", "```py\nstep_back_expansion_prompt = f\"\t\\\nGiven the query: {original_query}, \\\ngenerate a more abstract, \\\nhigher-level conceptual query. \"\n```", "```py\n# Original Query\noriginal_query=\t#1\n\"How does climate change \\  #1\naffect polar bears?\"   #1\n\n# Prompts for generating HyDE\nsystem_prompt=\"You are an expert in \\\t#2\nclimate change and arctic life.\"   #2\nhyde_prompt=f\"Generate an answer to the \\\t #2\nquestion: {original_query}\"   #2\n\n# Using OpenAI to generate a hypothetical answer\n\nfrom openai import OpenAI  #3\nclient = OpenAI()   #3\nresponse = client.chat.completions.create(   #3\n  model=\"gpt-4o-mini\",   #3\n  messages= [   #3\n    {\"role\": \"system\", \"content\": system_prompt},   #3\n    {\"role\": \"user\", \"content\": hyde_prompt}   #3\n  ]   #3\n)   #3\n #3\nhy_answer=response.choices[0].message.content   #3\n\n# Using OpenAI Embeddings to convert hyde into embeddings\nembeddings = OpenAIEmbeddings(\t#4\nmodel=\"text-embedding-3-large\"\t #4\n)   #4\nhyde = embeddings.embed_query(hy_answer)   #4\n```", "```py\ncompress_prompt = f\"\t\\\nCompress the following document \t\\\ninto a shorter version, \t\\\nretaining only the essential information:\t\\\n\\n\\n{document}\"\n```"]