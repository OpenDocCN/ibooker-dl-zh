- en: Chapter 14\. Generating Icons Using Deep Nets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter we looked at generating hand-drawn sketches from the
    Quick Draw project and digits from the MNIST dataset. In this chapter we’ll try
    three types of networks on a slightly more challenging task: generating icons.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we can do any generating we need to get our hands on a set of icons.
    Searching online for “free icons” results in a lot of hits. None of these are
    “free as in speech” and most of them struggle where it comes to “free as in beer.”
    Also, you can’t freely reuse the icons, and usually the sites strongly suggest
    you pay for them after all. So, we’ll start with how to download, extract, and
    process icons into a standard format that we can use in the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we’ll try is to train a conditional variational autoencoder
    on our set of icons. We’ll use the network we ended up with in the previous chapter
    as a basis, but we’ll add some convolutional layers to it to make it perform better
    since the icon space is so much more complex than that of hand-drawn digits.
  prefs: []
  type: TYPE_NORMAL
- en: The second type of network we’ll try is a generative adversarial network. Here
    we’ll train two networks, one to generate icons and another to distinguish between
    generated icons and real icons. The competition between the two leads to better
    results.
  prefs: []
  type: TYPE_NORMAL
- en: The third and final type of network we’ll try is an RNN. In [Chapter 5](ch05.html#text_generation)
    we used this to generate texts in a certain style. By reinterpreting icons as
    a set of drawing instructions, we can use the same technique to generate images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code related to this chapter can be found in the following notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 14.1 Acquiring Icons for Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you get a large set of icons in a standard format?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Extract them from the Mac application *Icons8*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Icons8 distributes a large set of icons—over 63,000\. This is partly because
    icons of different formats are counted double, but still, it is a nice set. Unfortunately
    the icons are distributed inside applications for Mac and Windows. The good news
    is that a Mac *.dmg* archive is really just a p7zip archive containing an application,
    which itself is also a p7zip archive. Let’s start by downloading the app. Navigate
    to [*https://icons8.com/app*](https://icons8.com/app) and make sure to download
    the Mac version (even if you are on Linux or Windows). Now install the command-line
    version of p7zip for your favorite operating system and extract the contents of
    the *.dmg* file to its own folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The *.dmg* contains some metainformation and the Mac application. Let’s unpack
    the app too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Like an onion, this thing has many layers. You should now see a *.tar* file
    that also needs unpacking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a directory called *icons* that contains an *.ldb* file, which
    suggests that the directory represents a LevelDB database. Switching to Python,
    we can take a look inside:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Bingo. We have found our icons, and they seem to be encoded using the *.svg*
    vector format. It looks like they are contained in yet another format, with the
    header `TSAF`. Reading online, it seems to be some IBM-related format, but a Python
    library to extract data from this is not easy to find. Then again, this simple
    dump suggests that we are dealing with key/value pairs separated by a `\x00` with
    the key and value separated by a `\x08`. It doesn’t quite pan out, but it is good
    enough to build a hacky parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This extracts the SVGs and some basic properties that might come in handy later.
    The various platforms contain more or less the same icons, so we need to pick
    one platform. iOS seems to have the most icons, so let’s go with that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s write this all to disk for later processing. We’ll keep the SVGs
    but also write out bitmaps as PNGs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though there are many sites online advertising free icons, in practice
    getting a good training set is rather involved. In this case we found the icons
    as SVGs inside a mysterious `TSAF` store inside a LevelDB database inside a Mac
    app inside of the *.dmg* file that we downloaded. On the one hand, this seems
    more involved than it should be. On the other hand, it goes to show that with
    a little detective work we can uncover some very interesting datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 14.2 Converting the Icons to a Tensor Representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you convert the saved icons into a format suitable for training a network?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Concatenate them and normalize them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is similar to how we handled images for the pretrained network, except
    that now we will train our own network. We know all images will be 32×32 pixels,
    and we’ll keep track of the mean and standard deviation so we can normalize and
    denormalize the images correctly. We’ll also split the data up into a training
    set and a test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The processing is fairly standard. We read in the images, append them all to
    one array, normalize the array, and then split the resulting set into a training
    set and test set. We normalize by just dividing the grayscale pixels by 255\.
    The activation we’ll use later on is a sigmoid, which will only produce positive
    numbers, so no need to subtract the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3 Using a Variational Autoencoder to Generate Icons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to generate icons in a certain style.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Add convolutional layers to the MNIST solution of [Chapter 13](ch13.html#autoencoders).
  prefs: []
  type: TYPE_NORMAL
- en: 'The variational autoencoder we used to generate digits had a latent space of
    only two dimensions. We can get away with such a small space because ultimately
    there isn’t that much variation between handwritten digits. By their nature, there
    are only 10 different ones that all look fairly similar. Moreover, we used a fully
    connected layer to go to and from the latent space. Our icons are much more diverse,
    so we’ll use a few convolutional layers to reduce the size of the image before
    we apply a fully connected layer and end up with our latent state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We handle the loss function and distribution as before. The weight for the
    `KL_loss` is important. Set it too low and the resulting space won’t be dense.
    Set it too high and the network will quickly learn that predicting empty bitmaps
    gets it a decent `reconstruction_loss` and a great `KL_loss`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll upscale the latent state back into an icon. As before, we do this
    in parallel for the encoder and the autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the network, we need to make sure the training and test sets have
    a size that is divisible by the `batch_size`, as otherwise the `KL_loss` function
    will fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can sample some random icons from the space as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Autoencoder generated icon images](assets/dlcb_14in01.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the network definitely learned something about icons. They tend
    to have some sort of box that is filled in somewhat and usually don’t touch the
    outsides of the 32×32 container. But it is still rather vague!
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To apply the variational autoencoder we developed in the previous chapter on
    the more heterogeneous space of icons we need to use convolutional layers that
    step by step reduce the dimensions of the bitmap and increase the abstraction
    level until we are in the latent space. This is very similar to how image recognition
    networks function. Once we have our icons projected in a 128-dimensional space,
    we use the upsampling layers for both the generator and the autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: The result is more interesting than a slam dunk. Part of the issue is that icons,
    like the cats in the previous chapter, contain a lot of line drawings, which makes
    it hard for the network to get them exactly right. When in doubt, the network
    will opt for vague lines instead. Worse, icons often contain regions that are
    dithered like a checkerboard. These patterns are certainly learnable, but an off-by-one
    pixel error would mean that the entire answer is now completely wrong!
  prefs: []
  type: TYPE_NORMAL
- en: Another reason why the performance of our network is relatively poor is that
    we have relatively few icons. The next recipe shows a trick to get around that.
  prefs: []
  type: TYPE_NORMAL
- en: 14.4 Using Data Augmentation to Improve the Autoencoder’s Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you improve on the performance of your network without getting more
    data?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Our autoencoder in the previous recipe learned the vague outlines of our icon
    set, but nothing more than that. The results suggested that it was picking up
    on something, but not enough to do a stellar job. Throwing more data at the problem
    could help, but it would require us to find more icons, and those icons would
    have to be sufficiently similar to our original set to help. Instead we’re going
    to generate more data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea behind data augmentation, as discussed in [Chapter 1](ch01.html#tools_techniques),
    is to generate variations of the input data that shouldn’t matter to the network.
    In this case we want our network to learn the notion of *iconness* by feeding
    it icons. But if we flip or rotate our icons, does that make them less *icony*?
    Not really. Doing this will increase our input by a factor of 16\. Our network
    will learn from these new training examples that rotations and flipping don’t
    matter and hopefully perform better. Augmentation would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s apply that to our training and test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Training the network will now obviously take a bit longer. But the results
    are better, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Autoencoder icon after data augmentation](assets/dlcb_14in02.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data augmentation is a technique widely used when it comes to computer images.
    Rotations and flips are sort of obvious ways of doing this, but given the fact
    that we actually started out with the *.svg* representation of the icons there
    are a number of other things we could do. SVG is a vector format, so we could
    easily create icons that have a slight rotation or magnification without getting
    the sort of artifacts that we’d get if our baseline data comprised just bitmaps.
  prefs: []
  type: TYPE_NORMAL
- en: The icon space that we ended up with is better than the one from the previous
    recipe and it seems to capture some form of iconness.
  prefs: []
  type: TYPE_NORMAL
- en: 14.5 Building a Generative Adversarial Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to construct a network that can generate images and another that
    can learn to distinguish generated images from the originals.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create an image generator and an image discriminator that can work together.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key insight behind generative adversarial networks is that if you have
    two networks, one generating images and one judging the generated images, and
    train them in tandem, they keep each other on their toes as they learn. Let’s
    start with a generator network. This is similar to what we did with the decoder
    bit of an autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The other network, the discriminator, will take in an image and output whether
    it thinks it is generated or one of the originals. In that sense it looks like
    a classic convolutional network that has just a binary output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the next recipe we’ll look at how to train these two networks together.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative adversarial networks or GANs are a fairly recent innovation for generating
    images. One way to look at them is to see the two component networks, the generator
    and the discriminator, as learning together, becoming better in competition.
  prefs: []
  type: TYPE_NORMAL
- en: The other way to look at them is to see the discriminator as a dynamic loss
    function for the generator. A straightforward loss function works well when a
    network is learning to distinguish between cats and dogs; something is a cat,
    or it isn’t and we can use as a loss function the difference between the answer
    and the truth.
  prefs: []
  type: TYPE_NORMAL
- en: When generating images, this is trickier. How do you compare two images? Earlier
    in this chapter, when we were generating images using autoencoders, we ran into
    this problem. There, we just compared images pixel by pixel; that works when seeing
    if two images are the same, but it doesn’t work so well for similarity. Two icons
    that are exactly the same but offset by one pixel won’t necessarily have many
    pixels in the same position. As a result, the autoencoder often opted to generate
    fuzzy images.
  prefs: []
  type: TYPE_NORMAL
- en: Having a second network do the judging allows the overall system to develop
    a sense of image similarity that is more fluid. Moreover, it can become stricter
    as the images become better, while with the autoencoder if we start with too much
    emphasis on the dense space the network will never learn.
  prefs: []
  type: TYPE_NORMAL
- en: 14.6 Training Generative Adversarial Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you train the two components of a GAN together?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fall back on the underlying TensorFlow framework to run both networks together.
  prefs: []
  type: TYPE_NORMAL
- en: Normally we just let Keras do the heavy lifting when it comes to talking to
    the underlying TensorFlow framework. But the best we can do using Keras directly
    is alternate between training the generator and the discriminator network, which
    is suboptimal. Qin Yongliang has written a [blog post](http://bit.ly/2ILx7Te)
    that describes how to get around this.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by generating some noise and feeding that into the generator to
    get a generated image, and then feed a real image and a generated image into the
    discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can construct two loss functions. The generator is scored against how
    likely to be real the discriminator thought the image was. The discriminator is
    scored on a combination of how well it did with fake and real images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll calculate the gradients to optimize these two loss functions for
    the trainable weights of the two networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We collect the various steps and tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And we’re ready to set up the trainer. Keras needs the `learning_phase` set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The variables of which we can provide by generating our own batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Updating the weights for both networks in one go took us down to the level of
    TensorFlow itself. While this is a bit hairy, it is also good to get to know the
    underlying systems from time to time and not always rely on the “magic” that Keras
    provides.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are a number of implementations on the web that use the easy way out and
    just run both networks step by step, but not at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 14.7 Showing the Icons the GAN Produces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you show the progress that the GAN is making while it learns?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Add an icon renderer after each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we’re running our own batch processing, we might as well take advantage
    of this and update the notebook with the intermediate result at the end of each
    epoch. Let’s start with rendering a set of icons using the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s put them on a poster overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now add the following code to our epoch loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After one epoch some vague icons start to appear already:'
  prefs: []
  type: TYPE_NORMAL
- en: '![First GAN generated images](assets/dlcb_14in03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another 25 epochs and we are really starting to see some iconness:'
  prefs: []
  type: TYPE_NORMAL
- en: '![After 25 epochs](assets/dlcb_14in04.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final results for generating icons using GANs are better than what we got
    out of the autoencoders. Mostly, the drawings are a lot sharper, which can be
    attributed to having the discriminator network decide whether an icon is any good,
    rather than comparing icons on a pixel-by-pixel basis.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There has been an explosion of applications for GANs and their derivatives,
    ranging from reconstructing 3D models from pictures to coloring of old pictures
    and super-resolution, where the network increases the resolution of a small image
    without making it look blurred or blocky.
  prefs: []
  type: TYPE_NORMAL
- en: 14.8 Encoding Icons as Drawing Instructions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to convert icons into a format that is suitable to train an RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encode the icons as drawing instructions.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs can learn sequences, as we saw in [Chapter 5](ch05.html#text_generation).
    But what if we wanted to generate icons using an RNN? We could simply encode each
    icon as a sequence of pixels. One way to do this would be to view an icon as a
    sequence of pixels that have been “turned on.” There are 32 * 32 = 1,024 different
    pixels, so that would be our vocabulary. This does work, but we can do a little
    better by using actual drawing instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we treat an icon as a series of scanlines, we need only 32 different tokens
    for the pixels in a scanline. Add one token to move to the next scanline and a
    final token to mark the end of an icon and we have a nice sequential representation.
    Or, in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then decode an image by going through the pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encoding icons as a set of drawing instructions is just another way of preprocessing
    the data such that a network will have an easier job learning what we want it
    to learn, similar to other approaches we saw in [Chapter 1](ch01.html#tools_techniques).
    By having explicit drawing instructions we make sure, for example, that the network
    doesn’t learn to draw vague lines, as our autoencoder was prone to do—it won’t
    be able to.
  prefs: []
  type: TYPE_NORMAL
- en: 14.9 Training an RNN to Draw Icons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to train an RNN to generate icons.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train a network based on the drawing instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we can encode single icons as drawing instructions, the next step
    is to encode a whole set. Since we’re going to feed chunks into the RNN, asking
    it to predict the next instruction, we actually construct one big “document”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll run with the same model that helped us generate our Shakespearean text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To see in more detail how the network we use here is trained and the data is
    generated, it might be a good idea to look back at [Chapter 5](ch05.html#text_generation).
  prefs: []
  type: TYPE_NORMAL
- en: You can experiment with different numbers of layers and nodes or try different
    values for dropout. Different RNN layers also have an effect. The model is somewhat
    fragile; it is easy to get into a state where it doesn’t learn anything or, when
    it does, gets stuck on a local maximum.
  prefs: []
  type: TYPE_NORMAL
- en: 14.10 Generating Icons Using an RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve trained the network; now how do you get it to produce icons?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feed the network some random bits of your test set and interpret the predictions
    as drawing instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic approach here is again the same as when we were generating Shakespearean
    text or Python code; the only difference is that we need to feed the predictions
    into the icon decoder to get icons out. Let’s first run some predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `diversity` parameter controls how far the predictions are from deterministic
    (which the model will turn into if `diversity` is `0`). We need this to generate
    diverse icons, but also to avoid getting stuck in a loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll collect each prediction in a variable, `so_far`, which we flush every
    time we encounter the value `33` (end of icon). We also check whether the `y`
    value is in range—the model learns more or less the size of the icons, but will
    sometimes try to color outside of the lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'With this, we can now draw a “poster” of icons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![RNN generated icons](assets/dlcb_14in05.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The icons generated using the RNN are the boldest of the three attempts we undertook
    in this chapter and arguably capture the nature of iconness best. The model learns
    symmetry and the basic shapes found in icons and even occasionally dithers to
    get a notion of halftones across.
  prefs: []
  type: TYPE_NORMAL
- en: We could try to combine the different approaches in this chapter. For example,
    instead of trying to predict the next drawing instruction, we could have an RNN
    that takes in the drawing instructions, capture the latent state at that point,
    and then have a second RNN based on that state reconstruct the drawing instructions.
    This way we would have an RNN-based autoencoder. In the text world there have
    been some successes in this area.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs can also be combined with GANs. Instead of having a generator network that
    takes a latent variable and upscales it into an icon, we’d use an RNN to generate
    drawing instructions and then have the discriminator network decide whether these
    are real or fake.
  prefs: []
  type: TYPE_NORMAL
