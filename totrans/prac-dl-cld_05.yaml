- en: 'Chapter 5\. From Novice to Master Predictor: Maximizing Convolutional Neural
    Network Accuracy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b), we looked
    at the importance of responsible AI development. One of the aspects we discussed
    was the importance of robustness of our models. Users can trust what we build
    only if they can be assured that the AI they encounter on a day-to-day basis is
    accurate and reliable. Obviously, the context of the application matters a lot.
    It would be okay for a food classifier to misclassify pasta as bread on occasion.
    But it would be dangerous for a self-driving car to misclassify a pedestrian as
    a street lane. The main goal of this chapter is thus a rather important one—to
    build more accurate models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will develop an intuition for recognizing opportunities
    to improve your model’s accuracy the next time you begin training one. We first
    look at the tools that will ensure that you won’t be going in blind. After that,
    for a good chunk of this chapter, we take a very experimental approach by setting
    up a baseline, isolating individual parameters to tweak, and observing their effect
    on model performance and training speed. A lot of the code we use in this chapter
    is all aggregated in a single Jupyter Notebook, along with an actionable checklist
    with interactive examples. It is meant to be highly reusable should you choose
    to incorporate it in your next training script.
  prefs: []
  type: TYPE_NORMAL
- en: 'We explore several questions that tend to come up during model training:'
  prefs: []
  type: TYPE_NORMAL
- en: I am unsure whether to use transfer learning or building from scratch to train
    my own network. What is the preferred approach for my scenario?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the least amount of data that I can supply to my training pipeline to
    get acceptable results?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I want to ensure that the model is learning the correct thing and not picking
    up spurious correlations. How can I get visibility into that?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I ensure that I (or someone else) will obtain the same results from
    my experiments every single time they are run? In other words, how do I ensure
    reproducibility of my experiments?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does changing the aspect ratio of the input images have an impact on the predictions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does reducing input image size have a significant effect on prediction results?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If I use transfer learning, what percentage of layers should I fine tune to
    achieve my preferred balance of training time versus accuracy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, if I were to train from scratch, how many layers should I have
    in my model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the appropriate “learning rate” to supply during model training?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are too many things to remember. Is there a way to automate all of this
    work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will try to answer these questions one by one in the form of experiments
    on a few datasets. Ideally, you should be able to look at the results, read the
    takeaways, and gain some insight into the concept that the experiment was testing.
    If you’re feeling more adventurous, you can choose to perform the experiments
    yourself using the Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Tools of the Trade
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the main priorities of this chapter is to reduce the code and effort
    involved during experimentation while trying to gain insights into the process
    in order to reach high accuracy. An arsenal of tools exists that can assist us
    in making this journey more pleasant:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Datasets
  prefs: []
  type: TYPE_NORMAL
- en: Quick and easy access to around 100 datasets in a performant manner. All well-known
    datasets are available starting from the smallest MNIST (a few megabytes) to the
    largest MS COCO, ImageNet, and Open Images (several hundred gigabytes). Additionally,
    medical datasets like the Colorectal Histology and Diabetic Retinopathy are also
    available.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: Close to 20 easy-to-use methods to visualize many aspects of training, including
    visualizing the graph, tracking experiments, and inspecting the images, text,
    and audio data that pass through the network during training.
  prefs: []
  type: TYPE_NORMAL
- en: What-If Tool
  prefs: []
  type: TYPE_NORMAL
- en: Run experiments in parallel on separate models and tease out differences in
    them by comparing their performance on specific data points. Edit individual data
    points to see how that affects the model training.
  prefs: []
  type: TYPE_NORMAL
- en: tf-explain
  prefs: []
  type: TYPE_NORMAL
- en: Analyze decisions made by the network to identify bias and inaccuracies in the
    dataset. Additionally, use heatmaps to visualize what parts of the image the network
    activated on.
  prefs: []
  type: TYPE_NORMAL
- en: Keras Tuner
  prefs: []
  type: TYPE_NORMAL
- en: A library built for `tf.keras` that enables automatic tuning of hyperparameters
    in TensorFlow 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: AutoKeras
  prefs: []
  type: TYPE_NORMAL
- en: Automates Neural Architecture Search (NAS) across different tasks like image,
    text, and audio classification and image detection.
  prefs: []
  type: TYPE_NORMAL
- en: AutoAugment
  prefs: []
  type: TYPE_NORMAL
- en: Utilizes reinforcement learning to improve the amount and diversity of data
    in an existing training dataset, thereby increasing accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore these tools in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow Datasets is a collection of nearly 100 ready-to-use datasets that
    can quickly help build high-performance input data pipelines for training TensorFlow
    models. Instead of downloading and manipulating data sets manually and then figuring
    out how to read their labels, TensorFlow Datasets standardizes the data format
    so that it’s easy to swap one dataset with another, often with just a single line
    of code change. As you will see later on, doing things like breaking the dataset
    down into training, validation, and testing is also a matter of a single line
    of code. We will additionally be exploring TensorFlow Datasets from a performance
    point of view in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can list all of the available datasets by using the following command (in
    the interest of conserving space, only a small subset of the full output is shown
    in this example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how simple it is to load a dataset. We will plug this into a full
    working pipeline later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`tfds` generates a lot of progress bars, and they take up a lot of screen space—using
    `tfds.disable_progress_bar()` might be a good idea.'
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorBoard is a one-stop-shop for all of your visualization needs, offering
    close to 20 tools to understand, inspect, and improve your model’s training.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, to track experiment progress, we save the values of loss and
    accuracy per epoch and then, when done, plot it using `matplotlib`. The downside
    with that approach is that it’s not real time. Our usual options are to watch
    for the training progress in text. Additionally, after the training is done, we
    need to write additional code to make the graph in `matplotlib`. TensorBoard solves
    these and more pressing issues by offering a real-time dashboard ([Figure 5-1](part0007.html#tensorboard_default_view_showcasing_real))
    that helps us visualize all logs (such as train/validation accuracy and loss)
    to assist in understanding the progression of training. Another benefit it offers
    is the ability to compare our current experiment’s progress with the previous
    experiment, so we can see how a change in parameters affected our overall accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![TensorBoard default view showcasing real-time training metrics (the lightly
    shaded lines represent the accuracy from the previous run)](../images/00226.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. TensorBoard default view showcasing real-time training metrics
    (the lightly shaded lines represent the accuracy from the previous run)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To enable TensorBoard to visualize our training and models, we need to log
    information about our training with the help of summary writer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To follow our training in real time, we need to load TensorBoard before the
    model training begins. We can load TensorBoard by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As more TensorFlow components need a visual user interface, they reuse TensorBoard
    by becoming embeddable plug-ins within it. You’ll notice the Inactive drop-down
    menu on TensorBoard; that’s where you can see all the different profiles or tools
    that TensorFlow offers. [Table 5-1](part0007.html#plugins_for_tensorboarddot)
    showcases a handful of the wide variety of tools available.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-1\. Plugins for TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: '| **TensorBoard plug-in name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Default Scalar | Visualize scalar values such as classification accuracy.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Custom Scalar | Visualize user-defined custom metrics. For example, different
    weights for different classes, which might not be a readily available metric.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Image | View the output from each layer by clicking the Images tab. |'
  prefs: []
  type: TYPE_TB
- en: '| Audio | Visualize audio data. |'
  prefs: []
  type: TYPE_TB
- en: '| Debugging tools | Allows debugging visually and setting conditional breakpoints
    (e.g., tensor contains Nan or Infinity). |'
  prefs: []
  type: TYPE_TB
- en: '| Graphs | Shows the model architecture graphically. |'
  prefs: []
  type: TYPE_TB
- en: '| Histograms | Show the changes in the weight distribution in the layers of
    a model as the training progresses. This is especially useful for checking the
    effect of compressing a model with quantization. |'
  prefs: []
  type: TYPE_TB
- en: '| Projector | Visualize projections using t-SNE, PCA, and others. |'
  prefs: []
  type: TYPE_TB
- en: '| Text | Visualize text data. |'
  prefs: []
  type: TYPE_TB
- en: '| PR curves | Plot precision-recall curves. |'
  prefs: []
  type: TYPE_TB
- en: '| Profile | Benchmark speed of all operations and layers in a model. |'
  prefs: []
  type: TYPE_TB
- en: '| Beholder | Visualize the gradients and activations of a model in real time
    during training. It allows seeing them filter by filter, and allows them to be
    exported as images or even as a video. |'
  prefs: []
  type: TYPE_TB
- en: '| What-If Tool | For investigating the model by slicing and dicing the data
    and checking its performance. Especially helpful for discovering bias. |'
  prefs: []
  type: TYPE_TB
- en: '| HParams | Find out which params and at what values are the most important,
    allow logging of the entire parameter server (discussed in detail in this chapter).
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mesh | Visualize 3D data (including point clouds). |'
  prefs: []
  type: TYPE_TB
- en: It should be noted that TensorBoard is not TensorFlow specific, and can be used
    with other frameworks like PyTorch, scikit-learn, and more, depending on the plugin
    used. To make a plugin work, we need to write the specific metadata that we want
    to visualize. For example, TensorBoard embeds the TensorFlow Projector tool within
    to cluster images, text, or audio using t-SNE (which we examined in detail in
    [Chapter 4](part0006.html#5N3C3-13fa565533764549a6f0ab7f11eed62b)). Apart from
    calling TensorBoard, we need to write the metadata like the feature embeddings
    of our image, so that TensorFlow Projector can use it to do clustering, as demonstrated
    in [Figure 5-2](part0007.html#tensorflow_embedding_projector_showcasin).
  prefs: []
  type: TYPE_NORMAL
- en: '![TensorFlow Embedding Projector showcasing data in clusters, can be run as
    a TensorBoard plug-in](../images/00190.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. TensorFlow Embedding Projector showcasing data in clusters (can
    be run as a TensorBoard plugin)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What-If Tool
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What if we could inspect our AI model’s predictions with the help of visualizations?
    What if we could find the best threshold for our model to maximize precision and
    recall? What if we could slice and dice the data along with the predictions our
    model made to see what it’s great at and where there are opportunities to improve?
    What if we could compare two models to figure out which is indeed better? What
    if we could do all this and more, with a few clicks in the browser? Sounds appealing
    for sure! The What-If Tool ([Figure 5-3](part0007.html#what-if_toolapostrophes_datapoint_editor)
    and [Figure 5-4](part0007.html#pr_curves_in_the_performance_and_fairnes)) from
    Google’s People + AI Research (PAIR) initiative helps open up the black box of
    AI models to enable model and data explainability.
  prefs: []
  type: TYPE_NORMAL
- en: '![What-If Tool’s datapoint editor makes it possible to filter and visualize
    data according to annotations of the dataset and labels from the classifier](../images/00152.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. What-If Tool’s datapoint editor makes it possible to filter and
    visualize data according to annotations of the dataset and labels from the classifier
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![PR curves in the Performance and Fairness section of the What-If Tool help
    to interactively select the optimal threshold to maximize precision and recall](../images/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. PR curves in the Performance and Fairness section of the What-If
    Tool help to interactively select the optimal threshold to maximize precision
    and recall
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To use the What-If Tool, we need the dataset and a model. As we just saw, TensorFlow
    Datasets makes downloading and loading the data (in the `tfrecord` format) relatively
    easy. All we need to do is to locate the data file. Additionally, we want to save
    the model in the same directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It’s best to perform the following lines of code in a local system rather than
    a Colab notebook because the integration between Colab and the What-If Tool is
    still evolving.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in a new terminal, let’s make a directory for all of our What-If Tool
    experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Move the trained model and TFRecord data here. The overall directory structure
    looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll serve the model using Docker within the newly created directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'A word of caution: the port must be `8500` and all parameters must be spelled
    exactly as shown in the preceding example.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, at the far right, click the settings button (the gray gear icon) and add
    the values listed in [Table 5-2](part0007.html#configurations_for_the_what-if_tool).
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-2\. Configurations for the What-If Tool
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Inference address | `ip_addr:8500` |'
  prefs: []
  type: TYPE_TB
- en: '| Model name | `/models/colo` |'
  prefs: []
  type: TYPE_TB
- en: '| Model type | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Path to examples | */home/{*`your_username`*}/what_if_stuff/colo/models/colo.tfrec*
    (Note: this must be an absolute path) |'
  prefs: []
  type: TYPE_TB
- en: We can now open the What-If Tool in the browser within TensorBoard, as depicted
    in [Figure 5-5](part0007.html#setup_window_for_the_what-if_tool).
  prefs: []
  type: TYPE_NORMAL
- en: '![Setup window for the What-If Tool](../images/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Setup window for the What-If Tool
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The What-If Tool can also be used to visualize datasets according to different
    bins, as shown in [Figure 5-6](part0007.html#the_what-if_tool_enables_using_multiple).
    We can also use the tool to determine the better performing model out of multiple
    models on the same dataset using the `set_compare_estimator_and_feature_spec`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![The What-If tool enables using multiple metrics, data visualization, and
    many more things under the sun](../images/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. The What-If tool enables using multiple metrics, data visualization,
    and many more things under the sun
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, we can load TensorBoard, and then, in the Visualize section, choose the
    model we want to compare, as shown in [Figure 5-7](part0007.html#choose_the_model_to_compare_using_the_wh).
    This tool has many features to explore!
  prefs: []
  type: TYPE_NORMAL
- en: '![Choose the model to compare using the What-If Tool](../images/00313.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. Choose the model to compare using the What-If Tool
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: tf-explain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning models have traditionally been black boxes, and up until now,
    we usually learn about their performance by watching the class probabilities and
    validation accuracies. To make these models more interpretable and explainable,
    heatmaps come to the rescue. By showing the area of an image that leads to the
    prediction with higher intensity, heatmaps can help visualize their learning.
    For example, an animal often seen in surroundings with snow might be getting high-accuracy
    predictions, but if the dataset has only that animal with snow as the background,
    the model might just be paying attention to the snow as the distinctive pattern
    instead of the animal. Such a dataset demonstrates bias, making the predictions
    not too robust when the classifier is put in the real world (and potentially dangerous!).
    Heatmaps can be especially useful to explore such bias, as often spurious correlations
    can seep in if the dataset is not carefully curated.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf-explain` (by Raphael Meudec) helps understand the results and inner workings
    of a neural network with the help of such visualizations, removing the veil on
    bias in datasets. We can add multiple types of callbacks while training or use
    its core API to generate TensorFlow events that can later be loaded into TensorBoard.
    For inference, all we need to do is pass an image, its ImageNet object ID along
    with a model into tf-explain’s functions. You must supply the object ID because
    `tf.explain` needs to know what is activated for that particular class. A few
    different visualization approaches are available with `tf.explain`:'
  prefs: []
  type: TYPE_NORMAL
- en: Grad CAM
  prefs: []
  type: TYPE_NORMAL
- en: The Gradient-weighted Class Activation Mapping (Grad CAM) visualizes how parts
    of the image affect the neural network’s output by looking into the activation
    maps. A heatmap (illustrated in [Figure 5-8](part0007.html#visualizations_on_images_using_mobilenet))
    is generated based on the gradients of the object ID from the last convolutional
    layer. Grad CAM is largely a broad-spectrum heatmap generator given that it is
    robust to noise and can be used on an array of CNN models.
  prefs: []
  type: TYPE_NORMAL
- en: Occlusion Sensitivity
  prefs: []
  type: TYPE_NORMAL
- en: Occludes a part of the image (using a small square patch placed randomly) to
    establish how robust the network is. If the prediction is still correct, on average,
    the network is robust. The area in the image that is the warmest (i.e., red) has
    the most effect on the prediction when occluded.
  prefs: []
  type: TYPE_NORMAL
- en: Activations
  prefs: []
  type: TYPE_NORMAL
- en: Visualizes the activations for the convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizations on images using MobileNet and tf-explain](../images/00272.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. Visualizations on images using MobileNet and tf-explain
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As demonstrated in the code example that follows, such visualizations can be
    built with very little code. By taking a video, generating individual frames,
    and running tf-explain with Grad CAM and joining them together, we can build a
    detailed understanding of how these neural networks would react to moving camera
    angles.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Common Techniques for Machine Learning Experimentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first few chapters focused on training the model. The following sections,
    however, contain a few more things to keep in the back of your mind while running
    your training experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Data Inspection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data inspection’s first biggest hurdle is determining the structure of the data.
    TensorFlow Datasets has made this step relatively easy because all of the available
    datasets are in the same format and structure and can be used in a performant
    way. All we need to do is load the dataset into the What-If Tool and use the various
    options already present to inspect the data. As an example, on the SMILE dataset,
    we can visualize the dataset according to its annotations, such as images of people
    wearing eyeglasses and those without eyeglasses, as illustrated in [Figure 5-9](part0007.html#slicing_and_dividing_the_data_based_on_p).
    We observe that a wider distribution of the dataset has images of people wearing
    no eyeglasses, thus uncovering bias in the data due to an unbalanced dataset.
    This can be solved by modifying the weights of the metrics accordingly, through
    the tool.
  prefs: []
  type: TYPE_NORMAL
- en: '![Slicing and dividing the data based on predictions and real categories](../images/00231.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-9\. Slicing and dividing the data based on predictions and real categories
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Breaking the Data: Train, Validation, Test'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Splitting a dataset into train, validation, and test is pretty important because
    we want to report the results on an unseen dataset by the classifier (i.e., the
    test dataset). TensorFlow Datasets makes it easy to download, load, and split
    the dataset into these three parts. Some datasets already come with three default
    splits. Alternatively, the data can be split by percentages. The following code
    showcases using a default split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The cats-and-dogs dataset in `tfds` has only the train split predefined. Similar
    to this, some datasets in TensorFlow Datasets do not have a `validation` split.
    For those datasets, we take a small percentage of samples from the predefined
    `training` set and treat it as the `validation` set. To top it all off, splitting
    the dataset using the `weighted_splits` takes care of randomizing and shuffling
    data between the splits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Early Stopping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Early stopping helps to avoid overtraining of the network by keeping a lookout
    for the number of epochs that show limited improvement. Assuming a model is set
    to train for 1,000 epochs and reaches 90% accuracy at the 10^(th) epoch and stops
    improving any further for the next 10 epochs, it might be a waste of resources
    to train any further. If the number of epochs exceeds a predefined threshold called
    `patience`, training is stopped even if there might still be more epochs left
    to train. In other words, early stopping decides the point at which the training
    would no longer be useful and stops training. We can change the metric using the
    `monitor` parameter and add early stopping to our list of callbacks for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Reproducible Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train a network once. Then, train it again, without changing any code or parameters.
    You might notice that the accuracies in two subsequent runs came out slightly
    different, even if no change was made in code. This is due to random variables.
    To make experiments reproducible across runs, we want to control this randomization.
    Initialization of weights of models, randomized shuffling of data, and so on all
    utilize randomization algorithms. We know that random number generators can be
    made reproducible by initializing a seed and that’s exactly what we will do. Various
    frameworks have their own ways of setting a random seed, some of which are shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is necessary to set a seed in all the frameworks and subframeworks that are
    being used, as seeds are not transferable between frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-End Deep Learning Example Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s combine several tools and build a skeletal backbone, which will serve
    as our pipeline in which we will add and remove parameters, layers, functionality,
    and various other addons to really understand what is happening. Following the
    code on the book’s GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)),
    you can interactively run this code for more than 100 datasets in your browser
    with Colab. Additionally, you can modify it for most classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Transfer Learning Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s build this end-to-end example for transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Basic Custom Network Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apart from transfer learning on state-of-the-art models, we can also experiment
    and develop better intuitions by building our own custom network. Only the model
    needs to be swapped in the previously defined transfer learning code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, it’s time to use our pipeline for various experiments.
  prefs: []
  type: TYPE_NORMAL
- en: How Hyperparameters Affect Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we aim to modify various parameters of a deep learning pipeline
    one at a time—from the number of layers fine-tuned, to the choice of the activation
    function used—and see its effect primarily on validation accuracy. Additionally,
    when relevant, we also observe its effect on the speed of training and time to
    reach the best accuracy (i.e., convergence).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our experimentation setup is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To reduce experimentation time, we have used a faster architecture—MobileNet—in
    this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reduced the input image resolution to 128 x 128 pixels to further speed up
    training. In general, we would recommend using a higher resolution (at least 224
    x 224) for production systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early stopping is applied to stop experiments if they don’t increase in accuracy
    for 10 consecutive epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For training with transfer learning, we generally unfreeze the last 33% of the
    layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate is set to 0.001 with Adam optimizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’re mostly using the Oxford Flowers 102 dataset for testing, unless otherwise
    stated. We chose this dataset because it is reasonably difficult to train on due
    to the large number of classes it contains (102) and the similarities between
    many of the classes that force networks to develop a fine-grained understanding
    of features in order to do well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make apples-to-apples comparisons, we take the maximum accuracy value in
    a particular experiment and normalize all other accuracy values within that experiment
    with respect to this maximum value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on these and other experiments, we have compiled a checklist of actionable
    tips to implement in your next model training adventure. These are available on
    the book’s GitHub (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    along with interactive visualizations. If you have more tips, feel free to tweet
    them [@PracticalDLBook](https://twitter.com/PracticalDLBook) or submit a pull
    request.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning Versus Training from Scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experimental setup
  prefs: []
  type: TYPE_NORMAL
- en: 'Train two models: one using transfer learning, and one from scratch on the
    same dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets used
  prefs: []
  type: TYPE_NORMAL
- en: Oxford Flowers 102, Colorectal Histology
  prefs: []
  type: TYPE_NORMAL
- en: Architectures used
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained MobileNet, Custom model
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-10](part0007.html#comparing_transfer_learning_versus_train) shows
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing transfer learning versus training a custom model on different datasets](../images/00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-10\. Comparing transfer learning versus training a custom model on
    different datasets
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the key takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning leads to a quicker rise in accuracy during training by reusing
    previously learned features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although it is expected that transfer learning (based on pretrained models on
    ImageNet) would work when the target dataset is also of natural imagery, the patterns
    learned in the early layers by a network work surprisingly well for datasets beyond
    ImageNet. That does not necessarily mean that it will yield the best results,
    but it can get close. When the images match more real-world images that the model
    was pretrained on, we get relatively quick improvement in accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effect of Number of Layers Fine-Tuned in Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experimental setup
  prefs: []
  type: TYPE_NORMAL
- en: Vary the percentage of trainable layers from 0 to 100%
  prefs: []
  type: TYPE_NORMAL
- en: Dataset used
  prefs: []
  type: TYPE_NORMAL
- en: Oxford Flowers 102
  prefs: []
  type: TYPE_NORMAL
- en: Architecture used
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained MobileNet
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-11](part0007.html#effect_of_percent_layers_fine-tuned_on_m) shows
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of % layers fine-tuned on model accuracy](../images/00159.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-11\. Effect of % layers fine-tuned on model accuracy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the key takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: The higher the number of layers fine-tuned, the fewer epochs it took to reach
    convergence and the higher the accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The higher the number of layers fine-tuned, the more time it took per epoch
    for training, due to more computation and updates involved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a dataset that required fine-grained understanding of images, making more
    layers task specific by unfreezing them was the key to a better model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effect of Data Size on Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experimental setup
  prefs: []
  type: TYPE_NORMAL
- en: Add one image per class at a time
  prefs: []
  type: TYPE_NORMAL
- en: Dataset used
  prefs: []
  type: TYPE_NORMAL
- en: Cats versus dogs
  prefs: []
  type: TYPE_NORMAL
- en: Architecture used
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained MobileNet
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-12](part0007.html#effect_of_the_amount_of_data_per_categor) shows
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of the amount of data per category on model accuracy](../images/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-12\. Effect of the amount of data per category on model accuracy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the key takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: Even with only three images in each class, the model was able to predict with
    close to 90% accuracy. This shows how powerful transfer learning can be in reducing
    data requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because ImageNet has several cats and dogs, pretrained networks on ImageNet
    suited our dataset much more easily. More difficult datasets like Oxford Flowers
    102 might require a much higher number of images to achieve similar accuracies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effect of Learning Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experimental setup
  prefs: []
  type: TYPE_NORMAL
- en: Vary the learning rate between .1, .01, .001, and .0001
  prefs: []
  type: TYPE_NORMAL
- en: Dataset used
  prefs: []
  type: TYPE_NORMAL
- en: Oxford Flowers 102
  prefs: []
  type: TYPE_NORMAL
- en: Architecture used
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained MobileNet
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-13](part0007.html#effect_of_learning_rate_on_model_accurac) shows
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of learning rate on model accuracy and speed of convergence](../images/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-13\. Effect of learning rate on model accuracy and speed of convergence
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the key takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: Too high of a learning rate, and the model might never converge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too low a learning rate results in a long time taken to convergence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Striking the right balance is crucial in training quickly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effect of Optimizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experimental setup
  prefs: []
  type: TYPE_NORMAL
- en: Experiment with available optimizers including AdaDelta, AdaGrad, Adam, Gradient
    Descent, Momentum, and RMSProp
  prefs: []
  type: TYPE_NORMAL
- en: Dataset used
  prefs: []
  type: TYPE_NORMAL
- en: Oxford Flowers 102
  prefs: []
  type: TYPE_NORMAL
- en: Architecture used
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained MobileNet
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-14](part0007.html#effect_of_different_optimizers_on_the_sp) shows
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of different optimizers on the speed of convergence](../images/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-14\. Effect of different optimizers on the speed of convergence
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the key takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: Adam is a great choice for faster convergence to high accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSProp is usually better for RNN tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effect of Batch Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experimental setup
  prefs: []
  type: TYPE_NORMAL
- en: Vary batch sizes in powers of two
  prefs: []
  type: TYPE_NORMAL
- en: Dataset used
  prefs: []
  type: TYPE_NORMAL
- en: Oxford Flowers 102
  prefs: []
  type: TYPE_NORMAL
- en: Architecture used
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-15](part0007.html#effect_of_batch_size_on_accuracy_and_spe) shows
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of batch size on accuracy and speed of convergence](../images/00317.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-15\. Effect of batch size on accuracy and speed of convergence
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the key takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: The higher the batch size, the more the instability in results from epoch to
    epoch, with bigger rises and drops. But higher accuracy also leads to more efficient
    GPU utilization, so faster speed per epoch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too low a batch size slows the rise in accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 16/32/64 are good to start batch sizes with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effect of Resizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experimental setup
  prefs: []
  type: TYPE_NORMAL
- en: Change image size to 128x128, 224x224
  prefs: []
  type: TYPE_NORMAL
- en: Dataset used
  prefs: []
  type: TYPE_NORMAL
- en: Oxford Flowers 102
  prefs: []
  type: TYPE_NORMAL
- en: Architecture used
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-16](part0007.html#effect_of_image_size_on_accuracy) shows the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of image size on accuracy](../images/00275.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-16\. Effect of image size on accuracy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the key takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: Even with a third of the pixels, there wasn’t a significant difference in validation
    accuracies. On the one hand, this shows the robustness of CNNs. It might partly
    be because the Oxford Flowers 102 dataset has close-ups of flowers visible. For
    datasets in which the objects have much smaller portions in an image, the results
    might be lower.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effect of Change in Aspect Ratio on Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experimental Setup
  prefs: []
  type: TYPE_NORMAL
- en: Take images of various aspect ratios (width:height ratio) and resize them to
    a square (1:1 aspect ratio).
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Used
  prefs: []
  type: TYPE_NORMAL
- en: Cats vs. Dogs
  prefs: []
  type: TYPE_NORMAL
- en: Architecture Used
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-17](part0007.html#distribution_of_aspect_ratio_and_corresp) shows
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution of aspect ratio and corresponding accuracies in images](../images/00168.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-17\. Distribution of aspect ratio and corresponding accuracies in images
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the key takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: Most common aspect ratio is 4:3; that is, 1.33, whereas our neural networks
    are generally trained at 1:1 ratio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks are relatively robust to minor modifications in aspect ratio
    brought upon by resizing to a square shape. Even up to 2.0 ratio gives decent
    results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools to Automate Tuning for Maximum Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen since the rise of the nineteenth century, automation has always
    led to an increase in productivity. In this section, we investigate tools that
    can help us automate the search for the best model.
  prefs: []
  type: TYPE_NORMAL
- en: Keras Tuner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With so many potential combinations of hyperparameters to tune, coming up with
    the best model can be a tedious process. Often two or more parameters might have
    correlated effects on the overall speed of convergence as well as validation accuracy,
    so tuning one at a time might not lead to the best model. And if curiosity gets
    the best of us, we might want to experiment on all the hyperparameters together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras Tuner comes in to automate this hyperparameter search. We define a search
    algorithm, the potential values that each parameter can take (e.g., discrete values
    or a range), our target object to maximize (e.g., validation accuracy), and sit
    back to see the program start training. Keras Tuner conducts multiple experiments
    changing the parameters on our behalf, storing the metadata of the best model.
    The following code example adapted from Keras Tuner documentation showcases searching
    through the different model architectures (varying in the number of layers between
    2 and 10) as well as varying the learning rate (between 0.1 and 0.001):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Each experiment will show values like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: On the experiment end, the result summary gives a snapshot of the experiments
    conducted so far, and saves more metadata.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Another big benefit is the ability to track experiments online in real time
    and get notifications on their progress by visiting [*http://keras-tuner.appspot.com*](http://keras-tuner.appspot.com),
    getting an API key (from Google App Engine), and entering the following line in
    our Python program along with the real API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Due to the potentially large combinatorial space, random search is preferred
    to grid search as a more practical way to get to a good solution on a limited
    experimentation budget. But there are faster ways, including Hyperband (Lisha
    Li et al.), whose implementation is also available in Keras Tuner.
  prefs: []
  type: TYPE_NORMAL
- en: For computer-vision problems, Keras Tuner includes ready-to-use tunable applications
    like HyperResNet.
  prefs: []
  type: TYPE_NORMAL
- en: AutoAugment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another example hyperparameter are augmentations. Which augmentations to use?
    How much magnitude to augment? Would combining one too many make matters worse?
    Instead of leaving these decisions to humans, we can let AI decide. AutoAugment
    utilizes reinforcement learning to come up with the best combination of augmentations
    (like translation, rotation, shearing) and the probabilities and magnitudes to
    apply, to maximize the validation accuracy. (The method was applied by Ekin D.
    Cubuk et al. to come up with the new state-of-the-art ImageNet validation numbers.)
    By learning the best combination of augmentation parameters on ImageNet, we can
    readily apply it to our problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the prelearned augmentation strategy from ImageNet is pretty simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 5-18](part0007.html#output_of_augmentation_strategies_learne) displays
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output of augmentation strategies learned by reinforcement learning on the
    ImageNet dataset](../images/00140.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-18\. Output of augmentation strategies learned by reinforcement learning
    on the ImageNet dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: AutoKeras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With AI automating more and more jobs, it is no surprise it can finally automate
    designing AI architectures, too. NAS approaches utilize reinforcement learning
    to join together mini-architectural blocks until they are able to maximize the
    objective function; in other words, our validation accuracy. The current state-of-the-art
    networks are all based on NAS, leaving human-designed architectures in the dust.
    Research in this area started showing promising results in 2017, with a bigger
    focus on making train faster in 2018\. And now with AutoKeras (Haifeng Jin et
    al.), we can also apply this state-of-the-art technique on our particular datasets
    in a relatively accessible manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating new model architectures with AutoKeras is a matter of supplying
    our images and associated labels as well as a time limit by which to finish running
    the jobs. Internally, it implements several optimization algorithms, including
    a Bayesian optimization approach to search for an optimal architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Post-training, we are all eager to learn how the new model architecture looks.
    Unlike most of the cleaner-looking images we generally get to see, this will look
    pretty obfuscated to understand or print out. But what we do find faith in is
    that it yields high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw a range of tools and techniques to help investigate
    opportunities to improve our CNN accuracy. Building a case for iterative experimentation,
    you learned how tuning hyperparameters can bring about optimal performance. And
    with so many hyperparameters to choose from, we then looked at automated approaches,
    including AutoKeras, AutoAugment, and Keras Tuner. Best of all, the core code
    for this chapter combining multiple tools in a single Colab file is available
    online on the book’s GitHub (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    and can easily be tuned to more than 100 datasets with a single line change and
    run online in the browser. Additionally, we compiled a checklist of actionable
    tips along with interactive experiments hosted online to help give your model
    a little extra edge. We hope that the material covered in this chapter will help
    make your models more robust, reduce bias, make them more explainable, and ultimately
    contribute to the responsible development of AI.
  prefs: []
  type: TYPE_NORMAL
