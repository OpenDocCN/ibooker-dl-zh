- en: '2 Tokenizers: How large language models see the world'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creating tokens from sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling vocabulary size with normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding risks in tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization strategies to remove ambiguity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As discussed in chapter 1, in the world of artificial intelligence, it is often
    helpful to find analogies to human learning to explain how machines “learn.” How
    you read and understand sentences is a complex process that changes as you get
    older and involves multiple sequential and concurrent cognitive processes [1].
    Large language models (LLMs), however, use simpler processes than human cognitive
    processes. They employ algorithms based on neural networks to capture the relationships
    between words in large amounts of data and then use this information about relationships
    to interpret and generate sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our discussion of how these algorithms work will begin with their input: sentences
    of text. In this chapter, we explore how the LLM processes these sentences to
    become inputs for the model. Just as language is critical for how you think and
    process information, the inputs to an LLM are crucial in influencing what kinds
    of concepts and tasks LLMs can perform.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Tokens as numeric representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It may seem obvious that LLMs should process sentences, but to fully understand,
    we must be more specific. As we talk about how LLMs work, you will see that textual
    sentences are unnatural for the neural network algorithms that power LLMs because
    neural networks fundamentally employ numbers to do their work. As shown in figure
    [2.1](#fig__tokenizationExample), the algorithms employed by LLMs must convert
    human text into a numeric representation before working with it. *Tokens* are
    the representations that LLMs use to break text into pieces that can be encoded
    as numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F01_Boozallen.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 To understand text, LLMs must break text into tokens. Each unique
    token has a numeric identifier associated with it.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'You can think of tokens as the smallest unit of text an LLM processes—an “atom,”
    if you will, the smallest part from which all other things are built. So what
    are the atoms of text? Consider this: As you read this book, what are the smallest
    building blocks that your brain uses to process meaning? Two natural answers are
    *letters* and *words*. It is very tempting to define letters as the atom since
    words are made of letters, but do you consciously read every letter in every word?
    For most people, the answer is “no.” (If you are dyslexic like one of the co-authors
    of this book, this is a bizarre question. But cognitive processing is complex
    and not fully understood; please bear with us on the analogies!) You look at the
    more prominent words and word parts. In fbct, yoy cn probbly unrestand ths sentnce
    ever through we diddt sue th ryght cpellng or l3ttrs. People unconsciously use
    parts of words to process text, and LLMs are built using the same principle.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how the process of converting text to tokens
    works. First, we will discuss tokens in more detail; then, we will discuss the
    procedures used to decide how sentences are turned into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Language models see only tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By adulthood, most English-speaking people know around 30,000 words [2]. GPT-3,
    the LLM that initially powered ChatGPT, has a vocabulary of 50,257 tokens [3].
    These tokens are not words but parts of words referred to as *subwords*, a representation
    that is somewhere between words and letters. Intuitively, a token captures language’s
    *minimum meaningful semantic unit*. For example, the word `schoolhouse` will often
    get broken into two tokens, `school` and `house`, and the word `thoughtful` as
    `thought` and `ful`. This is useful for recognizing frequent words and having
    the subwords to interpret new words we have never seen before. People often use
    a similar technique, called semantic decomposition, to understand words they’ve
    never seen before. We intuitively break new words into constituent parts to grasp
    their meaning based on words we already understand.
  prefs: []
  type: TYPE_NORMAL
- en: '*Feature engineering* is the process of converting your data to a form that
    is more convenient to your algorithm and the task you want to solve. To build
    an algorithm that can detect the language of a given text, you could write code
    that takes text as input and outputs the percentage of times each character occurs.
    For example, if `é` appears a lot in a document, you have a good feature to indicate
    that the document is more likely to be Spanish or French than Russian or Chinese.
    Sound feature engineering is concerned with thinking through how your model works,
    what you want to achieve, and how to prepare your data for the combination of
    model and goal.'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization is the feature engineering of LLMs; it is critically essential
    because tokens are the only information a model interacts with. Tokens are seen
    as individual, abstract *things* that are not inherently connected. The relationships
    are learned through observation of data.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at figure [2.1](#fig__tokenizationExample), it is evident that
    the tokens for `Dis` and `dis` are related, the only difference being that one
    starts with a capital `D`. However, you can seethat the model assigns the identifier
    ![equation image](../Images/eq-chapter-2-17-1.png) to `Dis` and the identifier
    ![equation image](../Images/eq-chapter-2-17-2.png) to `dis`. That is, the model
    doesn’t inherently see any connection between the tokens representing `Dis` and
    `dis`, even if we, as humans, see an obvious connection. The model doesn’t even
    *see* `Dis` or `dis`. For an LLM to process tokens, we must convert those tokens
    into numbers so that the model will see the numbers ![equation image](../Images/eq-chapter-2-17-3.png)
    and ![equation image](../Images/eq-chapter-2-17-4.png). Importantly, the model
    doesn’t have any direct way to know that these tokens are related.
  prefs: []
  type: TYPE_NORMAL
- en: A token is a mapping from a subword to a unique numeric representation. In turn,
    *tokenization* is the process of converting a full-text string into a sequence
    of tokens. If you have used machine learning libraries before (especially any
    natural language processing [NLP] tools), you are probably familiar with some
    of the simpler forms of tokenization. For example, a simple tokenization process
    breaks a text into tokens by splitting a text based on spaces. However, this approach
    limits our abilities to create subwords or process languages that don’t use whitespace
    to delimit words, such as Chinese.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 The tokenization process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The generic process that tokenization follows is shown in figure [2.2](#fig__tokenizationProcess)
    with four key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Receiving the text to process*—This means obtaining text input as a `string`
    data type (a collection of letters, digits, or symbols) from a user, the internet,
    or whatever source that has the text you want.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Transforming the string*—This often involves changing the string in some useful
    way, such as converting uppercase characters into lowercase. This could also be
    done for security reasons (e.g., the text came from a user, and we need to remove
    anything that might look like some malicious input) or to eliminate irrelevant
    variations in the text to help the algorithm learn better. This process is known
    as *normalization*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Breaking the string into tokens*—Once a string is available, it needs to be
    separated into a sequence of discrete substrings; these are the tokens found in
    the larger string. This is referred to as *segmentation*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Mapping each token to a unique identifier*—The unique identifier is usually
    an integer number, which produces output that the LLM can understand.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F02_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 Generically, tokenization involves processing input to produce numeric
    identifiers for tokens.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The first and last parts of this process have little room for choice or different
    behavior. First, you need input to process; last, you need a numeric identifier
    for each token to store and retrieve the information you will associate with that
    token. The two middle steps, normalization and segmentation, are where you can
    choose what happens.
  prefs: []
  type: TYPE_NORMAL
- en: The last step of the tokenization process is where the vocabulary is built.
    The *vocabulary* of a model is the total number of unique tokens that are seen
    during training when we give the algorithm data to learn from. It almost always
    takes a large amount of data to build a rich vocabulary with many unique tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing the vocabulary for a model involves a series of trade-offs: the larger
    the vocabulary, the more information your model can process successfully. Consider
    a one-year-old child with a vocabulary of maybe a few dozen words. This child
    will not be a very effective communicator (but that’s okay; they have lots of
    time to learn). So a more extensive vocabulary not only helps the model understand
    more things, but it also makes the model larger. If you have a vocabulary that’s
    too large, you may make the model slower due to the number of computations required
    to use it, or the model may consume an excessive amount of memory or disk storage,
    which makes it more difficult to transfer or share to other machines—for example,
    when deploying it as a part of a software application.'
  prefs: []
  type: TYPE_NORMAL
- en: You build the model’s vocabulary by processing the training data and identifying
    tokens. Each time you see a new token, you give it a unique identifier based on
    the number of unique tokens you’ve seen. This process is often as simple as storing
    a counter set to `0` and incrementing it every time a new token is found. Once
    the process is complete, you have a tokenizer that is effectively an *encoder*.
    The tokenizer can receive text as input and return a numeric encoding of that
    text that the LLM algorithms can use as its output.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Controlling vocabulary size in tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPT-NeoX, a publicly available LLM, takes about 10 GB to store its vocabulary
    on disk. That is a lot of data, already large enough to make many real-world use
    cases challenging from the perspective of data storage and computation. It is
    so large that storing it on a micro-SD card would be prohibitively slow, making
    use on a mobile phone or some game consoles a significant challenge. It is big
    enough that it can’t be streamed in real time and must be downloaded and loaded
    into the processor’s RAM to perform tokenization. However, a vocabulary must be
    sufficiently large to represent all words and subwords the model will encounter
    during training and use. Suppose a model encounters a word that is not in its
    vocabulary and cannot be represented by combining subwords in its vocabulary.
    In that case, the model cannot capture information about that piece of text. As
    a result, it is essential to weigh concerns about vocabulary size against the
    need for models to interpret a wide variety of content. In NLP, this is often
    called the out-of-vocabulary problem, when we encounter words we can’t represent
    using the tokens available to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Vocabulary size is one factor contributing to an LLM’s size, so discussing methods
    and tradeoffs for controlling vocabulary size is vital. In this section, we will
    describe how changing the tokenization process’s behavior can influence vocabulary
    size and affect model capabilities and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F03_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 The normalization process commonly involves changing text to remove
    uppercase characters and punctuation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In figure [2.3](#fig__normalization), we focus on the second transformation
    step, normalization, which converts the uppercase characters “H” and “W” to lowercase
    and removes punctuation. These common normalization steps originate from classical
    NLP pipelines and are still sometimes done in modern deep learning approaches
    today. They have the immediately desirable effect of reducing the size of the
    vocabulary. Instead of needing to represent “Hello” and “hello” as two separate
    tokens, they get mapped to one unique token. This mapping makes an enormous difference
    because every word that starts a sentence and gets capitalized would potentially
    duplicate a word in the vocabulary with a capitalized version. Such normalization
    can also help with various typos and misspellings.
  prefs: []
  type: TYPE_NORMAL
- en: For example, while writing this book, we typed “LLMs,” “LLms,” and “llms,” and
    made various other mixed-case typos. Converting each character to lowercase in
    each variation resolves all these typos into a single, simple form, so we get
    a smaller vocabulary and decrease ambiguity.
  prefs: []
  type: TYPE_NORMAL
- en: However, converting text to lowercase doesn’t always decrease ambiguity. Consider
    “Bill” and “bill.” In the first situation, capitalization is vital for understanding
    that “Bill” is probably someone’s name, and “bill” is more likely a unit of money
    (or one of the other definitions of “bill”). Capitalization is crucial not only
    for understanding the meaning of the text but also for understanding the errors
    in the text. Consider again all the various ways we miscapitalized “LLMs” in this
    book. A high-quality AI algorithm would be able to recognize that we made a typo
    and correct it! ChatGPT is capable of this and thus requires capitalization in
    the model. So there is an important tradeoff between vocabulary size and potential
    model accuracy to consider.
  prefs: []
  type: TYPE_NORMAL
- en: In classical NLP and even not-that-old deep learning models like BERT (a predecessor
    to the LLMs that power ChatGPT), the ability of an algorithm to recognize typos
    and fix them was extremely limited outside of solutions designed explicitly for
    that purpose. For this reason, much of the work that used to go into engineering
    a robust normalization step has been discarded for LLMs today. A more extensive
    vocabulary is desirable to produce more capable models that can learn to understand
    mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Tokenization in detail
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The normalization and segmentation steps in the tokenization process largelydetermine
    the vocabulary size. In figure [2.4](#fig__segmentation), we show one of the most
    straightforward strategies for tokenization. This strategy follows a simple rule:
    any time a space is seen in the text, split the larger string into those tokens.
    In the case of “hello world,” it is as easy as calling `hello world.split( )`
    in Python. This is a reasonable approach to take; it is how we, as humans, read
    sentences. But it also adds some subtle complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F04_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 The segmentation process breaks normalized text into words or tokens
    so that each can be processed independently.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'What happens when you have punctuation in your text? If we use our white space
    rule to convert the string “hello, world” into `[hello,, world]`, we run into
    a similar problem as we do with capitalization. We end up with two distinct tokens
    for the same concept: `hello` and `hello,`. The old-school approach often addressed
    this by removing and developing more complex rules for splitting strings into
    tokens. While this is a step in the right direction toward reducing vocabulary
    size, manually specifying tokenization rules does not address other concerns.
    For example, rule-based tokenization strategies are a significant struggle for
    languages like Chinese that do not use spaces to separate words.'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying subwords with byte-pair encoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The general theme of LLMs is to do less feature engineering by hand and let
    algorithms do the heavy lifting instead. For this reason, an algorithm known as
    *byte pair encoding* (BPE) is typically used to break strings into tokens. Byte
    pair encoding is an algorithm for breaking words into common subword sequences
    of characters. BPE today is usually done with a custom segmenter and almost no
    normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Note By experimentation, we see many ChatGPT-like products will remove some
    Unicode characters that do not print (Unicode is weird), but otherwise mostly
    take your text as-is. Most prior language models do use various flavors of normalization,
    and how to normalize text for LLMs better is, we think, a good and open question.
  prefs: []
  type: TYPE_NORMAL
- en: Since finding the most efficient set of subwords is a computationally expensive
    task, BPE uses a heuristic to take a shortcut. It starts by looking at individual
    letters as tokens and then finds pairs of adjacent letters that occur most frequently
    and combines them into subword tokens. The algorithm repeats this process many
    times, continuing with subword tokens, until some threshold is met and the vocabulary
    is “small enough.” For example, in the first pass, the BPE algorithm examines
    the frequency of the individual letters used in English and encounters the letters
    “i,” “n,” and “g” near each other frequently. In the first pass, BPE might observe
    that “n” and “g” occur together more frequently than “i” and “n,” so it will produce
    the tokens `i` and `ng`. In a subsequent pass, it may combine those tokens into
    `ing` based on the frequency of that combination of letters versus how often “ng”
    occurs with other letters or subwords. Once BPE has reached its stopping point,
    it will have identified individual words such as “eating” and “drinking” as frequently
    occurring combinations. It may also capture “ing” as a suffix so that other words
    ending with that subword can also be represented as tokens. When the algorithm
    is complete, we end up with tokens that capture complete words and others that
    capture subwords. This process is shown at a high level in figure [2.5](#fig__bpe).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F05_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5 A simplified byte pair encoding algorithm for creating tokens: first,
    find the most frequent pair of characters “ng.” Next, replace all instances of
    “ng” with a placeholder token “T,” and add “ng” to the vocabulary. Repeat the
    process until no common byte pairs remain.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note Running the BPE algorithm to create a vocabulary is surprisinglyexpensive
    because it must read the input data many times to calculate the most frequent
    combinations of letters. While LLMs are trained on over 500 million or even 1
    billion pages of text, their tokenizers are usually created using a tiny subset
    of that data. Often, a tokenizer is trained using a much smaller collection of
    text the size of a novel.
  prefs: []
  type: TYPE_NORMAL
- en: The BPE process may seem odd at first, but you can think of it as a way of identifying
    common strings in a corpus. For example, BPE will almost always learn to represent
    `New York` as one token, which is useful since the state and city of New York
    are frequent occurrences in the text. Representing the whole concept as a single
    token makes it easier to use that kind of information. Indeed, most common words
    will become unique tokens, while rare words are hopefully captured as a combination
    of subwords. For example, `loquacious` will be tokenized by GPT-4 as `lo`, `qu`,
    and `acious`. This method is a success because “acious” is a Latin postfix for
    inclination/propensity, making it easier for the model to handle an unusual word
    correctly. It is also a failure case because the Latin prefix “`loqu`” got broken
    up into two tokens instead of one, making learning harder.
  prefs: []
  type: TYPE_NORMAL
- en: After BPE is used to make a vocabulary, model authors manually add additional
    tokens for various reasons, such as words that are important to a specific knowledge
    domain. As we will discuss in the next section, in some domains, having the correct
    tokens has a significant effect by capturing nuanced meaning. So often, the authors
    will make sure the necessary tokens are included. Model authors will also add
    special tokens that don’t directly represent word parts but provide auxiliary
    information to the model. Some common examples of this are the “unknown” token
    (typically represented as `[UNK]`), which is used if the tokenizer fails to process
    a symbol correctly, and the system token `[SYSTM]`, which is used to distinguish
    between a model’s built-in prompt and user-entered data, as well as other kinds
    of stylistic markers. Multimodal models that accept text and image inputs use
    unique tokens to tell the model when the input stream switches between bytes that
    represent text data and bytes that represent image data.
  prefs: []
  type: TYPE_NORMAL
- en: Open AI decided to use BPE to encode text into tokens when they developed ChatGPT
    and have released their tokenizer as the open source package `tiktoken` ([https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)).
    Still, several other algorithms and implementations for automatically generating
    tokens are available, including the WordPiece and SentencePiece algorithms developed
    at Google [4]. Each of these have different tradeoffs. For example, WordPiece
    uses a different technique for counting the frequency of the candidate subwords
    when building the tokenizer’s vocabulary. One of the algorithms implemented in
    SentencePiece processes entire sentences, preserving white space when calculating
    tokens, which may improve output when building models that handle multiple languages.
    However, BPE is the most broadly used algorithm. For example, it is now used exclusively
    in Google’s recent LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the algorithm chosen, the size of a tokenizer’s vocabulary is
    a critical model parameter determined by the data scientist or engineer in charge
    of training and augmenting the tokenizer. The following sections dive deep into
    some of the considerations on vocabulary size and other decisions made throughout
    the tokenizer development process.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 The risks of tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned in chapter 1, we won’t go much into coding in this book. The goal
    is to give you a reasonable understanding of how LLMs work and remove some of
    the magic and mystery so you can focus instead on how LLMs may be used for your
    job. Tokenization is the first piece of the puzzle. It is a simple but effective
    strategy to produce the inputs to LLMs. You have learned how the size of the vocabulary
    plays a significant role in a model’s deployability, the tradeoff in recognizing
    nuance versus the unnecessary redundancy associated with making a vocabulary,
    how the tokenization process influences the size of the vocabulary, and how the
    token selection process can be automated with BPE.
  prefs: []
  type: TYPE_NORMAL
- en: 'The choices made at tokenization time affect what LLMs can do today and will
    affect them in the future. These choices involve a few big-picture challenges
    to be aware of. To explore this topic further, two salient yet nuanced details
    of BPE are worth sharing some concerns about: the relationship between sentence
    length and token counts and the potential for LLMs to be confused by characters,
    known as homoglyphs, that appear identical yet have different binary encodings.'
  prefs: []
  type: TYPE_NORMAL
- en: Longer sentences do not mean more tokens
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An unintuitive aspect of BPE is that longer sentences do not mean more tokens.
    To see why, look at figure [2.6](#fig__whatIsTokenization), where we show a real
    tokenization of two different strings by GPT-3\. The string “I’m running” is longer
    by one character than the string “I’m runnin,” but it is one token shorter! If
    you don’t believe it, you can try tokenizing different strings at [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F06_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 Tokenizing two different sentences
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This discrepancy occurs because BPE is greedily looking for the smallest set
    of tokens for any piece of input. In this specific case, the string “running”
    occurs frequently enough in our training data that it gets its own token. In the
    case where the “g” is missing, there is no token for “runnin” in our vocabulary
    because that variation may have appeared rarely in our training data. Thus, “runnin”
    needs to be broken into at least two tokens, giving us `run` and `nin`.
  prefs: []
  type: TYPE_NORMAL
- en: This nuance of tokenizer implementation is fertile ground for software bugs.
    Different tokenizers may provide different answers on how to tokenize the same
    string. When designing unit tests and infrastructure, this factor is important
    to keep in mind to avoid getting lost or confused when upgrading or converting
    between tokenizer implementations that may cause new differences in token generation.
    It can also affect evaluations of LLMs, as many models are highly sensitive to
    added white space, and inconsistent tokenization may inadvertently lead to comparisons
    not being apples to apples.
  prefs: []
  type: TYPE_NORMAL
- en: Homoglyphs create confusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Homoglyphs are a problem developers may encounter when working with multiple
    human languages or considering the security implications of processing externally
    provided data. When input comes from arbitrary users, sometimes it may be nefarious
    and want to trick your model into bad behavior. One way that could be done against
    an LLM is with a *homoglyph* attack.
  prefs: []
  type: TYPE_NORMAL
- en: A homoglyph is when two or more characters have different byte encodings but
    appear identical when rendered on the screen. One example is the Latin letter
    “H” used in most Western European languages and the Cyrillic “H” used throughout
    Eastern Europe and Central Asia.
  prefs: []
  type: TYPE_NORMAL
- en: BPE will encode homoglyphs that use different byte encodings into different
    tokens. As a result, homoglyphs can inflate the number of tokens in a text, change
    how an LLM parses the information, and run up your compute costs. An amusing example
    of a homoglyph is the Unicode character U+200B, also known as the “zero width
    space.” This character is used in typesetting and takes up space, but it does
    not print anything, show anything, or change anything about how a document is
    rendered.
  prefs: []
  type: TYPE_NORMAL
- en: The zero width space is one of many strange and interesting things that exist
    within the Unicode specification and could be used to cause you pain. Many services
    thus employ normalization steps that remove such strange characters and replace
    homoglyphs with a canonical representation (i.e., anything that looks like an
    “a” must be encoded as an `a`). For example, OpenAI’s current tokenizer interface
    will remove homoglyphs. You must consider homoglyphs if you want to deploy an
    LLM on your hardware or a user’s device.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Tokenization and LLM capabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we are only concerned with the ability of an LLM to produce high-quality
    human-like text, the specific details of how you tokenize your text do not matter
    as much as the data and compute used to build these models. If you put enough
    computational power and scale into your models, they will eventually figure out
    useful representations regardless of the building blocks. But sometimes, tokenization
    dramatically affects what an LLM is capable of. In this section, we cover some
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: It may be the case that the examples that follow are not directly relevant to
    your job or what you would like to do with an LLM. That is perfectly fine; the
    point of these examples is not to dissuade you from using an LLM. Instead, the
    goal is to help you understand that the scope of what LLMs learn is limited by
    the representation chosen, and there may not be a way around these concerns without
    major engineering work. If you start building an application with LLMs and find
    significant difficulty, think about how tokenization could be a factor in your
    goal. If tokenization is indeed the problem, there is little you can do to solve
    it, so it may be best to look at other approaches, such as manually augmenting
    the vocabulary with tokens that are important for your application.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 LLMs are bad at word games
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Users frequently enjoy asking LLMs to solve word puzzles or perform tasks that
    involve word games. For example, figure [2.7](#fig__badAtWordGames) shows a word
    game where the correct answer depends on the exact letter sequence and the number
    of letters in a word.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F07_Boozallen.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 The tokenization approach means that ChatGPT cannot really “see”
    single characters or word lengths. If you ask questions that require subcharacter
    identification and change them in a unique and unusual way, ChatGPT starts to
    fail. The correct middle character is “a,” but ChatGPT insists that the letter
    is “e.” What ChatGPT sees is three tokens, representing `P`, `ine`, and `apple`,
    respectively.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Playing word games may not be something you care about for your application,
    but the reason word games fail may be highly salient to your problem. Although
    many examples like this are toy problems in that they aren’t particularly scientifically
    or commercially important, they reveal notable breakdowns in how these models
    operate. They may come into play in more practical uses, such as when models struggle
    to write poetry containing rhymes or assonance.
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for example, that you want to build an application that answers questions
    about a user’s prescription drugs. Drugs often have longer, confusing names that
    people fail to remember or spell incorrectly, and because an LLM does not understand
    letters, it may confuse one drug’s name with a different drug’s long and strange
    name.
  prefs: []
  type: TYPE_NORMAL
- en: Because drug names are uncommon, they will tokenize differently, even with minor
    misspellings. For example, in GPT-3, “Amoxicillin” and the easy misspelling “Amoxicillan”
    share no common tokens! This creates a much greater risk of the LLM responding
    incorrectly, where the risk is intrinsically higher, making an LLM application
    all the more important to thoroughly test, engineer around with extreme care,
    or potentially avoid altogether.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 LLMs are challenged by mathematics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenization significantly affects tasks involving formal symbolic reasoning,
    including mathematics and playing board games. Both math and board games are implemented
    by LLMs as symbolic reasoning problems where individual tokens have specific rules
    governing their interactions and meaning when observed in conjunction with other
    tokens. For example, models containing individual tokens for each digit tend to
    perform better at arithmetic than models that don’t. This is because the number
    123456 will become two tokens in GPT-3, `[, ]`, based on the frequency of those
    tokens in the tokenizer’s original training data. This makes it harder for the
    model to deal with the individual digits in that number. Some system developers
    have solved this problem by normalizing numbers by inserting spaces between all
    digits, such as 1 2 3 4 5 6, which creates a new output with six tokens, one for
    each digit.
  prefs: []
  type: TYPE_NORMAL
- en: This difference in math capability is well-illustrated in figure [2.8](#fig__math-eval),
    which shows performance on arithmetic computations throughout training. The top
    curve is a typical BPE tokenizer, while the bottom curve, which shows better performance,
    is the same tokenizer modified to have digit-level tokenization of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F08_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 A comparison of how two LLMs learn to perform arithmetic computations
    over time. Time is shown on the x-axis. The upper curve is a typical BPE tokenizer,
    while the lower curve is the same tokenizer modified to use tokens that represent
    individual digits. The y-axis describes the ability of the LLM to perform accurately,
    where a smaller number means fewer errors. The bottom line is that LLMs that use
    digit-level tokenization can learn how to do math better and faster.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 2.3.3 LLMs and language equity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most LLM tokenizers can represent any symbol covered by Unicode, which includes
    the characters from most of the world’s alphabets. However, how efficiently those
    tokenizers represent text in a given language varies massively, especially as
    thetokenizers are typically trained on smaller collections of text resources for
    different languages. This can cause substantial inequity in commercial services
    based on LLMs [5] because tokenization of words in languages that are rare in
    the training set defaults to a more granular set of subwords, resulting in increased
    token usage. Commercial LLM providers like OpenAI and Anthropic typically charge
    customers on a per-token basis, usually a fraction of a cent for every token input
    into the LLM and produced as output by the LLM. These costs add up when you consider
    that a high-use commercial application may process tens of millions of tokens
    daily.
  prefs: []
  type: TYPE_NORMAL
- en: 'The time it takes for an LLM to complete a request and the amount a user is
    charged per token depends directly on the tokenizer. Therefore, languages that
    are more efficiently represented using a tokenizer are economically incentivized
    over those that are not represented efficiently. Using English as a baseline,
    researchers have found that the cost to answer a user query in German or Italian
    is about 50% more when using ChatGPT and GPT-4\. Languages that differ even more
    substantially from English can incur much larger charges: Tumbuka and Bulgarian
    are more than twice the cost, and Dzongkha, Odia, Santali, and Shan cost over
    12 times as much as English to process.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Check your understanding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How would you expect the following words or phrases to be tokenized? Try breaking
    them out yourself and then running them through an actual LLM tokenizer, such
    as the one at [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: backstopped
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: large language models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Schoolhouse
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How you process sentences to understand them is a complex process that changes
    as you get older and involves multiple sequential and concurrent cognitive processes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How much do you think uppercase versus lowercase letters matter for each of
    the previous examples? Try submitting them again with various casings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s simulate how LLMs think about math using a cipher where each English letter
    corresponds to a number. For example, ![equation image](../Images/eq-chapter-2-84-1.png),
    ![equation image](../Images/eq-chapter-2-84-2.png), ![equation image](../Images/eq-chapter-2-84-3.png),
    and ![equation image](../Images/eq-chapter-2-84-4.png), so we would write *WAIT*
    to mean ![equation image](../Images/eq-chapter-2-84-5.png). Knowing this fact
    and that ![equation image](../Images/eq-chapter-2-84-6.png), can you figure out
    what ![equation image](../Images/eq-chapter-2-84-7.png) represents?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since a token is the basic unit an LLM operates on, why does it make sense (technologically)
    that languages less efficiently represented by a tokenizer would cost more?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it an ethical problem that LLMs charge different amounts to people for the
    same service based on what language they speak? Would you consider this discrimination?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2.5 Tokenization in context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The details of tokenization we discuss in this chapter are the foundational
    building blocks of LLMs that govern the input they can represent effectively and
    the output they produce. Tokenization is a critical component of LLMs like ChatGPT
    in develop-ing effective representations of text so that they can be used to learn
    relationships between tokens when presented with vast amounts of information in
    the training process, interpreting user input and producing the high-quality responses
    we’ve become accustomed to. An LLM’s potential is limited or enabled by the tokenization
    strategy and vocabulary it employs, in conjunction with all of the other characteristics
    we explore in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tokenization is the fundamental process that LLMs use to understand text by
    converting sentences into tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokens are the smallest units of information in text that represent content.
    Sometimes, they correspond to full words, but often, they represent pieces of
    words or sub-words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization involves *normalizing* text into a standard representation, which
    may involve converting characters to lowercase or translating the byte encoding
    of Unicode characters so that visibly identical characters employ the same encoding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization also involves *segmentation*, which is breaking up text into words
    or subwords. Algorithms like byte pair encoding (BPE) provide a mechanism to automatically
    learn how to efficiently segment text based on the statistical occurrence of combinations
    of letters in a training data set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result of building a tokenizer is known as a *vocabulary*, which is the
    unique collection of word and subword tokens that a tokenizer can use to represent
    text it has processed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of a tokenizer’s vocabulary affects the LLM’s ability to accurately
    represent data and the storage and computational resources required to understand
    and predict text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internally to the LLM, tokens are represented using numbers. As a result, there
    is no understanding of relationships between tokens, such as prefixes and suffixes,
    or the fact that two tokens share a similar set of letters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To support specific domains of knowledge, tokenizers trained automatically may
    be augmented to provide tokens that are important to their application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizers that do not understand individual letters or digits will have problems
    with arithmetic operations or simple word games.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
