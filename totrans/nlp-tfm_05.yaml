- en: Chapter 4\. Multilingual Named Entity Recognition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。多语言命名实体识别
- en: 'So far in this book we have applied transformers to solve NLP tasks on English
    corpora—but what do you do when your documents are written in Greek, Swahili,
    or Klingon? One approach is to search the Hugging Face Hub for a suitable pretrained
    language model and fine-tune it on the task at hand. However, these pretrained
    models tend to exist only for “high-resource” languages like German, Russian,
    or Mandarin, where plenty of webtext is available for pretraining. Another common
    challenge arises when your corpus is multilingual: maintaining multiple monolingual
    models in production will not be any fun for you or your engineering team.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们已经应用transformers来解决英语语料库上的NLP任务 - 但是当你的文档是用希腊语、斯瓦希里语或克林贡语写的时候，你该怎么办呢？一种方法是在Hugging
    Face Hub上搜索合适的预训练语言模型，并在手头的任务上对其进行微调。然而，这些预训练模型往往只存在于像德语、俄语或普通话这样的“高资源”语言中，这些语言有大量的网络文本可用于预训练。另一个常见的挑战是当你的语料库是多语言的时候：在生产中维护多个单语模型对你或你的工程团队来说都不是什么乐趣。
- en: Fortunately, there is a class of multilingual transformers that come to the
    rescue. Like BERT, these models use masked language modeling as a pretraining
    objective, but they are trained jointly on texts in over one hundred languages.
    By pretraining on huge corpora across many languages, these multilingual transformers
    enable *zero-shot cross-lingual transfer*. This means that a model that is fine-tuned
    on one language can be applied to others without any further training! This also
    makes these models well suited for “code-switching,” where a speaker alternates
    between two or more languages or dialects in the context of a single conversation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一类多语言transformers可以拯救我们。像BERT一样，这些模型使用掩码语言建模作为预训练目标，但它们是在一百多种语言的文本上联合训练的。通过在许多语言的大型语料库上进行预训练，这些多语言transformers实现了*零-shot跨语言转移*。这意味着对一个语言进行微调的模型可以应用于其他语言，而无需进一步的训练！这也使得这些模型非常适合“代码切换”，即说话者在单一对话的上下文中交替使用两种或多种语言或方言。
- en: In this chapter we will explore how a single transformer model called XLM-RoBERTa
    (introduced in [Chapter 3](ch03.xhtml#chapter_anatomy))^([1](ch04.xhtml#idm46238725711616))
    can be fine-tuned to perform named entity recognition (NER) across several languages.
    As we saw in [Chapter 1](ch01.xhtml#chapter_introduction), NER is a common NLP
    task that identifies entities like people, organizations, or locations in text.
    These entities can be used for various applications such as gaining insights from
    company documents, augmenting the quality of search engines, or simply building
    a structured database from a corpus.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何对一种名为XLM-RoBERTa的单一transformer模型（在[第3章](ch03.xhtml#chapter_anatomy)介绍）进行微调，以执行跨多种语言的命名实体识别（NER）。正如我们在[第1章](ch01.xhtml#chapter_introduction)中看到的，NER是一种常见的NLP任务，用于识别文本中的人物、组织或地点等实体。这些实体可以用于各种应用，例如从公司文件中获取见解，增强搜索引擎的质量，或者仅仅是从语料库中构建结构化数据库。
- en: For this chapter let’s assume that we want to perform NER for a customer based
    in Switzerland, where there are four national languages (with English often serving
    as a bridge between them). Let’s start by getting a suitable multilingual corpus
    for this problem.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，让我们假设我们想为一个位于瑞士的客户执行NER，那里有四种官方语言（英语通常作为它们之间的桥梁）。让我们首先找到一个适合这个问题的多语言语料库。
- en: Note
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: '*Zero-shot transfer* or *zero-shot learning* usually refers to the task of
    training a model on one set of labels and then evaluating it on a different set
    of labels. In the context of transformers, zero-shot learning may also refer to
    situations where a language model like GPT-3 is evaluated on a downstream task
    that it wasn’t even fine-tuned on.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*零-shot转移*或*零-shot学习*通常指的是在一个标签集上训练模型，然后在另一个标签集上对其进行评估的任务。在transformers的上下文中，零-shot学习也可能指的是像GPT-3这样的语言模型在一个甚至没有进行微调的下游任务上进行评估。'
- en: The Dataset
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: The Dataset
- en: 'In this chapter we will be using a subset of the Cross-lingual TRansfer Evaluation
    of Multilingual Encoders (XTREME) benchmark called WikiANN or PAN-X.^([2](ch04.xhtml#idm46238725694592))
    This dataset consists of Wikipedia articles in many languages, including the four
    most commonly spoken languages in Switzerland: German (62.9%), French (22.9%),
    Italian (8.4%), and English (5.9%). Each article is annotated with `LOC` (location),
    `PER` (person), and `ORG` (organization) tags in the [“inside-outside-beginning”
    (IOB2) format](https://oreil.ly/yXMUn). In this format, a `B-` prefix indicates
    the beginning of an entity, and consecutive tokens belonging to the same entity
    are given an `I-` prefix. An `O` tag indicates that the token does not belong
    to any entity. For example, the following sentence:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用跨语言TRansfer多语言编码器（XTREME）基准的子集，称为WikiANN或PAN-X。^([2](ch04.xhtml#idm46238725694592))
    这个数据集包括许多语言的维基百科文章，包括瑞士四种最常用的语言：德语（62.9%）、法语（22.9%）、意大利语（8.4%）和英语（5.9%）。每篇文章都以“内外开始”（IOB2）格式标注了`LOC`（位置）、`PER`（人物）和`ORG`（组织）标签。在这种格式中，`B-`前缀表示实体的开始，属于同一实体的连续标记被赋予`I-`前缀。`O`标签表示该标记不属于任何实体。例如，以下句子：
- en: Jeff Dean is a computer scientist at Google in California
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Jeff Dean是Google在加利福尼亚的计算机科学家
- en: would be labeled in IOB2 format as shown in [Table 4-1](#jeff-dean-ner).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将以IOB2格式标记，如[Table 4-1](#jeff-dean-ner)所示。
- en: Table 4-1\. An example of a sequence annotated with named entities
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1。一个带有命名实体注释的序列示例
- en: '| Tokens | Jeff | Dean | is | a | computer | scientist | at | Google | in |
    California |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| Tokens | Jeff | Dean | is | a | computer | scientist | at | Google | in |
    California |'
- en: '| Tags | B-PER | I-PER | O | O | O | O | O | B-ORG | O | B-LOC |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| Tags | B-PER | I-PER | O | O | O | O | O | B-ORG | O | B-LOC |'
- en: 'To load one of the PAN-X subsets in XTREME, we’ll need to know which *dataset
    configuration* to pass the `load_dataset()` function. Whenever you’re dealing
    with a dataset that has multiple domains, you can use the `get_dataset_config_names()`
    function to find out which subsets are available:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载 XTREME 中的 PAN-X 子集之一，我们需要知道要传递给 `load_dataset()` 函数的 *数据集配置*。每当处理具有多个领域的数据集时，可以使用
    `get_dataset_config_names()` 函数查找可用的子集：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Whoa, that’s a lot of configurations! Let’s narrow the search by just looking
    for the configurations that start with “PAN”:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这有很多配置！让我们缩小搜索范围，只查找以“PAN”开头的配置：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'OK, it seems we’ve identified the syntax of the PAN-X subsets: each one has
    a two-letter suffix that appears to be an [ISO 639-1 language code](https://oreil.ly/R8XNu).
    This means that to load the German corpus, we pass the `de` code to the `name`
    argument of `load_dataset()` as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，看来我们已经确定了 PAN-X 子集的语法：每个子集都有一个两个字母的后缀，看起来是一个[ISO 639-1 语言代码](https://oreil.ly/R8XNu)。这意味着要加载德语语料库，我们将
    `de` 代码传递给 `load_dataset()` 的 `name` 参数，如下所示：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To make a realistic Swiss corpus, we’ll sample the German (`de`), French (`fr`),
    Italian (`it`), and English (`en`) corpora from PAN-X according to their spoken
    proportions. This will create a language imbalance that is very common in real-world
    datasets, where acquiring labeled examples in a minority language can be expensive
    due to the lack of domain experts who are fluent in that language. This imbalanced
    dataset will simulate a common situation when working on multilingual applications,
    and we’ll see how we can build a model that works on all languages.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个真实的瑞士语料库，我们将根据 PAN-X 中各语言的口语比例抽样德语（`de`）、法语（`fr`）、意大利语（`it`）和英语（`en`）语料库。这将创建一个语言不平衡的情况，这在现实世界的数据集中非常常见，因为在少数语言中获取标记示例可能会很昂贵，因为缺乏精通该语言的领域专家。这种不平衡的数据集将模拟在多语言应用程序中工作时的常见情况，我们将看到如何构建一个适用于所有语言的模型。
- en: 'To keep track of each language, let’s create a Python `defaultdict` that stores
    the language code as the key and a PAN-X corpus of type `DatasetDict` as the value:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪每种语言，让我们创建一个 Python `defaultdict`，将语言代码存储为键，`DatasetDict` 类型的 PAN-X 语料库存储为值：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here we’ve used the `shuffle()` method to make sure we don’t accidentally bias
    our dataset splits, while `select()` allows us to downsample each corpus according
    to the values in `fracs`. Let’s have a look at how many examples we have per language
    in the training sets by accessing the `Dataset.num_rows` attribute:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `shuffle()` 方法确保不会意外地偏向我们的数据集拆分时，`select()` 允许我们根据 `fracs` 中的值对每个语料库进行降采样。让我们看看训练集中每种语言有多少示例，通过访问
    `Dataset.num_rows` 属性：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|  | de | fr | it | en |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | de | fr | it | en |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Number of training examples | 12580 | 4580 | 1680 | 1180 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 训练示例数量 | 12580 | 4580 | 1680 | 1180 |'
- en: 'By design, we have more examples in German than all other languages combined,
    so we’ll use it as a starting point from which to perform zero-shot cross-lingual
    transfer to French, Italian, and English. Let’s inspect one of the examples in
    the German corpus:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 按设计，我们在德语中有比其他所有语言加起来更多的示例，因此我们将其用作从中执行零-shot 跨语言转移到法语、意大利语和英语的起点。让我们检查德语语料库中的一个示例：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As with our previous encounters with `Dataset` objects, the keys of our example
    correspond to the column names of an Arrow table, while the values denote the
    entries in each column. In particular, we see that the `ner_tags` column corresponds
    to the mapping of each entity to a class ID. This is a bit cryptic to the human
    eye, so let’s create a new column with the familiar `LOC`, `PER`, and `ORG` tags.
    To do this, the first thing to notice is that our `Dataset` object has a `features`
    attribute that specifies the underlying data types associated with each column:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前遇到的 `Dataset` 对象一样，我们示例的键对应于 Arrow 表的列名，而值表示每列中的条目。特别是，我们看到 `ner_tags`
    列对应于将每个实体映射到类 ID。这对人眼来说有点神秘，所以让我们创建一个新列，其中包含熟悉的 `LOC`、`PER` 和 `ORG` 标签。为此，首先要注意的是我们的
    `Dataset` 对象具有一个 `features` 属性，该属性指定了与每列关联的基础数据类型：
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `Sequence` class specifies that the field contains a list of features,
    which in the case of `ner_tags` corresponds to a list of `ClassLabel` features.
    Let’s pick out this feature from the training set as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sequence` 类指定该字段包含一系列特征，对于 `ner_tags`，这对应于一系列 `ClassLabel` 特征。让我们从训练集中挑选出这个特征：'
- en: '[PRE11]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can use the `ClassLabel.int2str()` method that we encountered in [Chapter 2](ch02.xhtml#chapter_classification)
    to create a new column in our training set with class names for each tag. We’ll
    use the `map()` method to return a `dict` with the key corresponding to the new
    column name and the value as a `list` of class names:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们在[第2章](ch02.xhtml#chapter_classification)中遇到的 `ClassLabel.int2str()`
    方法，在我们的训练集中创建一个新的列，其中包含每个标签的类名。我们将使用 `map()` 方法返回一个 `dict`，其中键对应于新列名，值为类名的 `list`：
- en: '[PRE13]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we have our tags in human-readable format, let’s see how the tokens
    and tags align for the first example in the training set:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将标签转换为人类可读的格式，让我们看看训练集中第一个示例中的标记和标签是如何对齐的：
- en: '[PRE14]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| Tokens | 2.000 | Einwohnern | an | der | Danziger | Bucht | in | der | polnischen
    | Woiwodschaft | Pommern | . |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Tokens | 2.000 | Einwohnern | an | der | Danziger | Bucht | in | der | polnischen
    | Woiwodschaft | Pommern | . |'
- en: '| Tags | O | O | O | O | B-LOC | I-LOC | O | O | B-LOC | B-LOC | I-LOC | O
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | O | O | O | O | B-LOC | I-LOC | O | O | B-LOC | B-LOC | I-LOC | O |'
- en: The presence of the `LOC` tags make sense since the sentence “2,000 Einwohnern
    an der Danziger Bucht in der polnischen Woiwodschaft Pommern” means “2,000 inhabitants
    at the Gdansk Bay in the Polish voivodeship of Pomerania” in English, and Gdansk
    Bay is a bay in the Baltic sea, while “voivodeship” corresponds to a state in
    Poland.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`LOC`标签的存在是有意义的，因为句子“2,000 Einwohnern an der Danziger Bucht in der polnischen
    Woiwodschaft Pommern”在英语中的意思是“2,000 inhabitants at the Gdansk Bay in the Polish
    voivodeship of Pomerania”，而Gdansk Bay是波罗的海的一个海湾，“voivodeship”对应于波兰的一个州。'
- en: 'As a quick check that we don’t have any unusual imbalance in the tags, let’s
    calculate the frequencies of each entity across each split:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对标签是否存在异常不平衡的快速检查，让我们计算每个实体在每个拆分中的频率：
- en: '[PRE15]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|  | ORG | LOC | PER |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | ORG | LOC | PER |'
- en: '| --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| validation | 2683 | 3172 | 2893 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 验证 | 2683 | 3172 | 2893 |'
- en: '| test | 2573 | 3180 | 3071 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 2573 | 3180 | 3071 |'
- en: '| train | 5366 | 6186 | 5810 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 5366 | 6186 | 5810 |'
- en: This looks good—the distributions of the `PER`, `LOC`, and `ORG` frequencies
    are roughly the same for each split, so the validation and test sets should provide
    a good measure of our NER tagger’s ability to generalize. Next, let’s look at
    a few popular multilingual transformers and how they can be adapted to tackle
    our NER task.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很好 - `PER`、`LOC`和`ORG`频率的分布在每个拆分中大致相同，因此验证和测试集应该能够很好地衡量我们的NER标记器的泛化能力。接下来，让我们看一下一些流行的多语言变压器以及它们如何适应我们的NER任务。
- en: Multilingual Transformers
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多语言变压器
- en: Multilingual transformers involve similar architectures and training procedures
    as their monolingual counterparts, except that the corpus used for pretraining
    consists of documents in many languages. A remarkable feature of this approach
    is that despite receiving no explicit information to differentiate among the languages,
    the resulting linguistic representations are able to generalize well *across*
    languages for a variety of downstream tasks. In some cases, this ability to perform
    cross-lingual transfer can produce results that are competitive with those of
    monolingual models, which circumvents the need to train one model per language!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 多语言变压器涉及与其单语对应物相似的架构和训练程序，唯一的区别在于用于预训练的语料库包含许多语言的文档。这种方法的一个显著特点是，尽管没有接收到区分语言的明确信息，但由此产生的语言表示能够很好地*跨*语言进行泛化，适用于各种下游任务。在某些情况下，这种跨语言转移的能力可以产生与单语模型竞争的结果，从而避免了需要为每种语言训练一个模型的需求！
- en: 'To measure the progress of cross-lingual transfer for NER, the [CoNLL-2002](https://oreil.ly/nYd0o)
    and [CoNLL-2003](https://oreil.ly/sVESv) datasets are often used as a benchmark
    for English, Dutch, Spanish, and German. This benchmark consists of news articles
    annotated with the same `LOC`, `PER`, and `ORG` categories as PAN-X, but it contains
    an additional `MISC` label for miscellaneous entities that do not belong to the
    previous three groups. Multilingual transformer models are usually evaluated in
    three different ways:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量NER的跨语言转移的进展，通常使用[CoNLL-2002](https://oreil.ly/nYd0o)和[CoNLL-2003](https://oreil.ly/sVESv)数据集作为英语、荷兰语、西班牙语和德语的基准。这个基准由用相同的`LOC`、`PER`和`ORG`类别注释的新闻文章组成，但它还包含一个额外的`MISC`标签，用于不属于前三组的其他实体。多语言变压器模型通常以三种不同的方式进行评估：
- en: '`en`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`en`'
- en: Fine-tune on the English training data and then evaluate on each language’s
    test set.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在英语训练数据上进行微调，然后在每种语言的测试集上进行评估。
- en: '`each`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`each`'
- en: Fine-tune and evaluate on monolingual test data to measure per-language performance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在单语测试数据上进行微调和评估，以衡量每种语言的性能。
- en: '`all`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`all`'
- en: Fine-tune on all the training data to evaluate on all on each language’s test
    set.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有训练数据上进行微调，以便在每种语言的测试集上进行评估。
- en: We will adopt a similar evaluation strategy for our NER task, but first we need
    to select a model to evaluate. One of the first multilingual transformers was
    mBERT, which uses the same architecture and pretraining objective as BERT but
    adds Wikipedia articles from many languages to the pretraining corpus. Since then,
    mBERT has been superseded by XLM-RoBERTa (or XLM-R for short), so that’s the model
    we’ll consider in this chapter.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用类似的评估策略来进行我们的NER任务，但首先我们需要选择一个模型来评估。最早的多语言变压器之一是mBERT，它使用与BERT相同的架构和预训练目标，但将许多语言的维基百科文章添加到预训练语料库中。从那时起，mBERT已经被XLM-RoBERTa（或简称XLM-R）取代，因此这是我们将在本章中考虑的模型。
- en: 'As we saw in [Chapter 3](ch03.xhtml#chapter_anatomy), XLM-R uses only MLM as
    a pretraining objective for 100 languages, but is distinguished by the huge size
    of its pretraining corpus compared to its predecessors: Wikipedia dumps for each
    language and 2.5 *terabytes* of Common Crawl data from the web. This corpus is
    several orders of magnitude larger than the ones used in earlier models and provides
    a significant boost in signal for low-resource languages like Burmese and Swahili,
    where only a small number of Wikipedia articles exist.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第3章](ch03.xhtml#chapter_anatomy)中看到的，XLM-R仅使用MLM作为100种语言的预训练目标，但与其前身相比，其预训练语料库的规模巨大：每种语言的维基百科转储和来自网络的2.5
    *terabytes*的Common Crawl数据。这个语料库的规模比早期模型使用的语料库大几个数量级，并为缅甸语和斯瓦希里语等低资源语言提供了显著的信号增强，因为这些语言只有少量的维基百科文章。
- en: 'The RoBERTa part of the model’s name refers to the fact that the pretraining
    approach is the same as for the monolingual RoBERTa models. RoBERTa’s developers
    improved on several aspects of BERT, in particular by removing the next sentence
    prediction task altogether.^([3](ch04.xhtml#idm46238724894224)) XLM-R also drops
    the language embeddings used in XLM and uses SentencePiece to tokenize the raw
    texts directly.^([4](ch04.xhtml#idm46238724892624)) Besides its multilingual nature,
    a notable difference between XLM-R and RoBERTa is the size of the respective vocabularies:
    250,000 tokens versus 55,000!'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 模型名称中的RoBERTa指的是预训练方法与单语RoBERTa模型相同。RoBERTa的开发人员在几个方面改进了BERT，特别是通过完全删除下一个句子预测任务。^([3](ch04.xhtml#idm46238724894224))
    XLM-R还放弃了XLM中使用的语言嵌入，并使用SentencePiece直接对原始文本进行标记化。^([4](ch04.xhtml#idm46238724892624))
    除了其多语言性质之外，XLM-R和RoBERTa之间的一个显著差异是各自词汇表的大小：25万个标记与5.5万个标记！
- en: XLM-R is a great choice for multilingual NLU tasks. In the next section, we’ll
    explore how it can efficiently tokenize across many languages.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: XLM-R是多语言NLU任务的一个很好的选择。在下一节中，我们将探讨它如何能够高效地在许多语言中进行标记化。
- en: A Closer Look at Tokenization
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仔细看一下分词
- en: 'Instead of using a WordPiece tokenizer, XLM-R uses a tokenizer called SentencePiece
    that is trained on the raw text of all one hundred languages. To get a feel for
    how SentencePiece compares to WordPiece, let’s load the BERT and XLM-R tokenizers
    in the usual way with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: XLM-R使用了一个名为SentencePiece的分词器，而不是使用WordPiece分词器，该分词器是在所有一百种语言的原始文本上进行训练的。为了了解SentencePiece与WordPiece的比较，让我们以通常的方式使用nlpt_pin01
    Transformers加载BERT和XLM-R分词器：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'By encoding a small sequence of text we can also retrieve the special tokens
    that each model used during pretraining:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对一小段文本进行编码，我们还可以检索每个模型在预训练期间使用的特殊标记：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '| BERT | [CLS] | Jack | Spa | ##rrow | loves | New | York | ! | [SEP] | None
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| BERT | [CLS] | Jack | Spa | ##rrow | loves | New | York | ! | [SEP] | None
    |'
- en: '| XLM-R | <s> | ▁Jack | ▁Spar | row | ▁love | s | ▁New | ▁York | ! | </s> |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| XLM-R | <s> | ▁Jack | ▁Spar | row | ▁love | s | ▁New | ▁York | ! | </s> |'
- en: Here we see that instead of the `[CLS]` and `[SEP]` tokens that BERT uses for
    sentence classification tasks, XLM-R uses `<s>` and `<\s>` to denote the start
    and end of a sequence. These tokens are added in the final stage of tokenization,
    as we’ll see next.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到XLM-R使用了`<s>`和`<\s>`来表示序列的开始和结束，而不是BERT用于句子分类任务的`[CLS]`和`[SEP]`标记。这些标记是在标记化的最后阶段添加的，我们将在下面看到。
- en: The Tokenizer Pipeline
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词器管道
- en: So far we have treated tokenization as a single operation that transforms strings
    to integers we can pass through the model. This is not entirely accurate, and
    if we take a closer look we can see that it is actually a full processing pipeline
    that usually consists of four steps, as shown in [Figure 4-1](#toknizer-pipeline).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们把分词看作是一个将字符串转换为我们可以通过模型传递的整数的单个操作。这并不完全准确，如果我们仔细看一下，我们会发现它实际上是一个完整的处理管道，通常包括四个步骤，如[图4-1](#toknizer-pipeline)所示。
- en: '![Tokenizer pipeline](Images/nlpt_0401.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![分词管道](Images/nlpt_0401.png)'
- en: Figure 4-1\. The steps in the tokenization pipeline
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1。分词管道中的步骤
- en: 'Let’s take a closer look at each processing step and illustrate their effect
    with the example sentence “Jack Sparrow loves New York!”:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看每个处理步骤，并用示例句子“Jack Sparrow loves New York!”来说明它们的效果：
- en: '*Normalization*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*规范化*'
- en: This step corresponds to the set of operations you apply to a raw string to
    make it “cleaner.” Common operations include stripping whitespace and removing
    accented characters. [Unicode normalization](https://oreil.ly/2cp3w) is another
    common normalization operation applied by many tokenizers to deal with the fact
    that there often exist various ways to write the same character. This can make
    two versions of the “same” string (i.e., with the same sequence of abstract characters)
    appear different; Unicode normalization schemes like NFC, NFD, NFKC, and NFKD
    replace the various ways to write the same character with standard forms. Another
    example of normalization is lowercasing. If the model is expected to only accept
    and use lowercase characters, this technique can be used to reduce the size of
    the vocabulary it requires. After normalization, our example string would look
    like “jack sparrow loves new york!”.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步对应于你对原始字符串应用的一系列操作，使其更“干净”。常见的操作包括去除空格和去除重音字符。[Unicode normalization](https://oreil.ly/2cp3w)是许多分词器应用的另一种常见的规范化操作，用于处理通常存在各种写同一字符的方式的事实。这可能会使得两个“相同”的字符串（即具有相同的抽象字符序列）看起来不同；Unicode规范化方案如NFC、NFD、NFKC和NFKD用标准形式替换了写同一字符的各种方式。规范化的另一个例子是小写化。如果模型只接受和使用小写字符，这种技术可以用来减少它所需的词汇量。规范化后，我们的示例字符串将变成“jack
    sparrow loves new york!”。
- en: '*Pretokenization*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*预分词*'
- en: This step splits a text into smaller objects that give an upper bound to what
    your tokens will be at the end of training. A good way to think of this is that
    the pretokenizer will split your text into “words,” and your final tokens will
    be parts of those words. For the languages that allow this (English, German, and
    many Indo-European languages), strings can typically be split into words on whitespace
    and punctuation. For example, this step might transform our `["jack", "sparrow",
    "loves", "new", "york", "!"]`. These words are then simpler to split into subwords
    with Byte-Pair Encoding (BPE) or Unigram algorithms in the next step of the pipeline.
    However, splitting into “words” is not always a trivial and deterministic operation,
    or even an operation that makes sense. For instance, in languages like Chinese,
    Japanese, or Korean, grouping symbols in semantic units like Indo-European words
    can be a nondeterministic operation with several equally valid groups. In this
    case, it might be best to not pretokenize the text and instead use a language-specific
    library for pretokenization.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步将文本分割成较小的对象，这些对象给出了训练结束时你的标记的上限。一个好的思考方式是，预分词器将把你的文本分割成“单词”，你的最终标记将是这些单词的一部分。对于允许这样做的语言（英语、德语和许多印欧语言），字符串通常可以根据空格和标点符号分割成单词。例如，这一步可能会将我们的`["jack",
    "sparrow", "loves", "new", "york", "!"]`转换成这些单词。然后，这些单词更容易在管道的下一步中使用字节对编码（BPE）或Unigram算法分割成子词。然而，将文本分割成“单词”并不总是一个微不足道和确定性的操作，甚至不是一个有意义的操作。例如，在中文、日文或韩文等语言中，将符号分组成像印欧语言单词那样的语义单元可能是一个非确定性的操作，有几个同样有效的分组。在这种情况下，最好不要对文本进行预分词，而是使用一个特定于语言的库进行预分词。
- en: '*Tokenizer model*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*分词器模型*'
- en: Once the input texts are normalized and pretokenized, the tokenizer applies
    a subword splitting model on the words. This is the part of the pipeline that
    needs to be trained on your corpus (or that has been trained if you are using
    a pretrained tokenizer). The role of the model is to split the words into subwords
    to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary
    tokens. Several subword tokenization algorithms exist, including BPE, Unigram,
    and WordPiece. For instance, our running example might look like `[jack, spa,
    rrow, loves, new, york, !]` after the tokenizer model is applied. Note that at
    this point we no longer have a list of strings but a list of integers (input IDs);
    to keep the example illustrative, we’ve kept the words but dropped the quotes
    to indicate the transformation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输入文本被规范化和pretokenized，分词器会在单词上应用一个子词分割模型。这是流程中需要在你的语料库上进行训练（或者如果你使用的是预训练分词器，则已经进行了训练）的部分。模型的作用是将单词分割成子词，以减少词汇量的大小，并尝试减少词汇表外标记的数量。存在几种子词分割算法，包括BPE、Unigram和WordPiece。例如，我们的运行示例在分词器模型应用后可能看起来像`[jack,
    spa, rrow, loves, new, york, !]`。请注意，此时我们不再有一个字符串列表，而是一个整数列表（输入ID）；为了保持示例的说明性，我们保留了单词，但删除了引号以表示转换。
- en: '*Postprocessing*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*后处理*'
- en: 'This is the last step of the tokenization pipeline, in which some additional
    transformations can be applied on the list of tokens—for instance, adding special
    tokens at the beginning or end of the input sequence of token indices. For example,
    a BERT-style tokenizer would add classifications and separator tokens: `[CLS,
    jack, spa, rrow, loves, new, york, !, SEP]`. This sequence (recall that this will
    be a sequence of integers, not the tokens you see here) can then be fed to the
    model.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是分词流程的最后一步，在这一步中，可以对标记列表应用一些额外的转换，例如在输入标记索引序列的开头或结尾添加特殊标记。例如，BERT风格的分词器会添加分类和分隔符标记：`[CLS,
    jack, spa, rrow, loves, new, york, !, SEP]`。然后，这个序列（请记住，这将是一个整数序列，而不是你在这里看到的标记）可以被馈送到模型中。
- en: Going back to our comparison of XLM-R and BERT, we now understand that SentencePiece
    adds `<s>` and `<\s>` instead of `[CLS]` and `[SEP]` in the postprocessing step
    (as a convention, we’ll continue to use `[CLS]` and `[SEP]` in the graphical illustrations).
    Let’s go back to the SentencePiece tokenizer to see what makes it special.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们对XLM-R和BERT的比较，我们现在明白了SentencePiece在后处理步骤中添加了`<s>`和`<\s>`，而不是`[CLS]`和`[SEP]`（作为惯例，我们将在图形说明中继续使用`[CLS]`和`[SEP]`）。让我们回到SentencePiece分词器，看看它有什么特别之处。
- en: The SentencePiece Tokenizer
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SentencePiece分词器
- en: 'The SentencePiece tokenizer is based on a type of subword segmentation called
    Unigram and encodes each input text as a sequence of Unicode characters. This
    last feature is especially useful for multilingual corpora since it allows SentencePiece
    to be agnostic about accents, punctuation, and the fact that many languages, like
    Japanese, do not have whitespace characters. Another special feature of SentencePiece
    is that whitespace is assigned the Unicode symbol U+2581, or the ▁ character,
    also called the lower one quarter block character. This enables SentencePiece
    to detokenize a sequence without ambiguities and without relying on language-specific
    pretokenizers. In our example from the previous section, for instance, we can
    see that WordPiece has lost the information that there is no whitespace between
    “York” and “!”. By contrast, SentencePiece preserves the whitespace in the tokenized
    text so we can convert back to the raw text without ambiguity:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: SentencePiece分词器基于一种称为Unigram的子词分割类型，并将每个输入文本编码为Unicode字符序列。这个特性对于多语言语料库特别有用，因为它允许SentencePiece对重音、标点和许多语言（比如日语）没有空格字符这一事实保持不可知。SentencePiece的另一个特点是将空格分配给Unicode符号U+2581，或者称为▁字符，也叫做下四分之一块字符。这使得SentencePiece能够在不依赖于特定语言的pretokenizers的情况下，对序列进行去标记化处理。例如，在前一节的例子中，我们可以看到WordPiece丢失了“York”和“!”之间没有空格的信息。相比之下，SentencePiece保留了标记化文本中的空格，因此我们可以无歧义地将其转换回原始文本。
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now that we understand how SentencePiece works, let’s see how we can encode
    our simple example in a form suitable for NER. The first thing to do is load the
    pretrained model with a token classification head. But instead of loading this
    head directly from ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, we will
    build it ourselves! By diving deeper into the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers API, we can do this with just a few steps.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了SentencePiece的工作原理，让我们看看如何将我们的简单示例编码成适合NER的形式。首先要做的是加载带有标记分类头的预训练模型。但我们不会直接从nlpt_pin01
    Transformers中加载这个头，而是自己构建它！通过深入研究nlpt_pin01 Transformers API，我们可以用几个简单的步骤来实现这一点。
- en: Transformers for Named Entity Recognition
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于命名实体识别的Transformer
- en: In [Chapter 2](ch02.xhtml#chapter_classification), we saw that for text classification
    BERT uses the special `[CLS]` token to represent an entire sequence of text. This
    representation is then fed through a fully connected or dense layer to output
    the distribution of all the discrete label values, as shown in [Figure 4-2](#clf-arch).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.xhtml#chapter_classification)中，我们看到对于文本分类，BERT使用特殊的`[CLS]`标记来表示整个文本序列。然后，这个表示被馈送到一个全连接或密集层，以输出所有离散标签值的分布，如[图4-2](#clf-arch)所示。
- en: '![Architecture of a transformer encoder for classification.](Images/nlpt_0402.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![用于分类的Transformer编码器的架构。](Images/nlpt_0402.png)'
- en: Figure 4-2\. Fine-tuning an encoder-based transformer for sequence classification
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2。为序列分类微调基于编码器的Transformer
- en: BERT and other encoder-only transformers take a similar approach for NER, except
    that the representation of each individual input token is fed into the same fully
    connected layer to output the entity of the token. For this reason, NER is often
    framed as a *token classification* task. The process looks something like the
    diagram in [Figure 4-3](#ner-arch).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: BERT和其他仅编码器的变压器在NER方面采取了类似的方法，只是每个单独的输入标记的表示被馈送到相同的全连接层，以输出标记的实体。因此，NER经常被构建为*标记分类*任务。该过程看起来像[图4-3](#ner-arch)中的图表。
- en: '![Architecture of a transformer encoder for named entity recognition. The wide
    linear layer shows that the same linear layer is applied to all hidden states.](Images/nlpt_0403.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: 变压器编码器的命名实体识别架构。宽线性层显示相同的线性层应用于所有隐藏状态。
- en: Figure 4-3\. Fine-tuning an encoder-based transformer for named entity recognition
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3。为命名实体识别微调基于编码器的变压器
- en: So far, so good, but how should we handle subwords in a token classification
    task? For example, the first name “Christa” in [Figure 4-3](#ner-arch) is tokenized
    into the subwords “Chr” and “##ista”, so which one(s) should be assigned the `B-PER`
    label?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利，但在标记分类任务中，我们应该如何处理子词？例如，[图4-3](#ner-arch)中的第一个名字“Christa”被标记为子词“Chr”和“##ista”，那么应该分配`B-PER`标签给哪一个（或哪些）呢？
- en: In the BERT paper,^([5](ch04.xhtml#idm46238724678896)) the authors assigned
    this label to the first subword (“Chr” in our example) and ignored the following
    subword (“##ista”). This is the convention we’ll adopt here, and we’ll indicate
    the ignored subwords with `IGN`. We can later easily propagate the predicted label
    of the first subword to the subsequent subwords in the postprocessing step. We
    could also have chosen to include the representation of the “##ista” subword by
    assigning it a copy of the `B-LOC` label, but this violates the IOB2 format.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERT论文中，作者将这个标签分配给第一个子词（在我们的例子中是“Chr”），并忽略后面的子词（“##ista”）。这是我们将在这里采用的约定，我们将用`IGN`表示被忽略的子词。我们稍后可以很容易地将第一个子词的预测标签传播到后续子词中的后处理步骤。我们也可以选择包括“##ista”子词的表示，通过分配一个`B-LOC`标签的副本，但这违反了IOB2格式。
- en: Fortunately, all the architecture aspects we’ve seen in BERT carry over to XLM-R
    since its architecture is based on RoBERTa, which is identical to BERT! Next we’ll
    see how ![nlpt_pin01](Images/nlpt_pin01.png) Transformers supports many other
    tasks with minor modifications.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们在BERT中看到的所有架构方面都适用于XLM-R，因为它的架构基于RoBERTa，与BERT相同！接下来，我们将看到​![nlpt_pin01](Images/nlpt_pin01.png)变压器如何支持许多其他任务，只需进行轻微修改。
- en: The Anatomy of the Transformers Model Class
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器模型类的解剖
- en: '![nlpt_pin01](Images/nlpt_pin01.png) Transformers is organized around dedicated
    classes for each architecture and task. The model classes associated with different
    tasks are named according to a `<ModelName>For<Task>` convention, or `AutoModelFor<Task>`
    when using the `AutoModel` classes.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器围绕着每种架构和任务都有专门的类进行组织。与不同任务相关的模型类根据`<ModelName>For<Task>`约定命名，或者在使用`AutoModel`类时为`AutoModelFor<Task>`。
- en: 'However, this approach has its limitations, and to motivate going deeper into
    the ​![nlpt_pin01](Images/nlpt_pin01.png)⁠ Transformers API, consider the following
    scenario. Suppose you have a great idea to solve an NLP problem that has been
    on your mind for a long time with a transformer model. So you set up a meeting
    with your boss and, with an artfully crafted PowerPoint presentation, you pitch
    that you could increase the revenue of your department if you can finally solve
    the problem. Impressed with your colorful presentation and talk of profits, your
    boss generously agrees to give you one week to build a proof-of-concept. Happy
    with the outcome, you start working straight away. You fire up your GPU and open
    a notebook. You execute `from transformers import BertForTaskXY` (note that `TaskXY`
    is the imaginary task you would like to solve) and color escapes your face as
    the dreaded red color fills your screen: `ImportEr⁠ror:​ can⁠not import name *BertForTaskXY*`.
    Oh no, there is no BERT model for your use case! How can you complete the project
    in one week if you have to implement the whole model yourself?! Where should you
    even start?'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法有其局限性，为了激励更深入地了解​![nlpt_pin01](Images/nlpt_pin01.png)⁠变压器API，考虑以下情景。假设你有一个解决NLP问题的好主意，这个问题一直在你脑海中挥之不去，你想用一个变压器模型来解决它。于是你和老板安排了一次会议，通过精心制作的PowerPoint演示文稿，你向老板提出，如果你能最终解决这个问题，你可以增加部门的收入。老板对你色彩丰富的演示和利润的谈话印象深刻，慷慨地同意给你一周的时间来构建一个概念验证。满意结果后，你立刻开始工作。你启动GPU并打开笔记本。你执行`from
    transformers import BertForTaskXY`（注意`TaskXY`是你想解决的虚构任务），当可怕的红色填满屏幕时，你的脸色变了：`ImportEr⁠ror:​
    can⁠not import name *BertForTaskXY*`。哦，不，没有BERT模型适用于你的用例！如果你不得不自己实现整个模型，你怎么能在一周内完成项目？你应该从哪里开始呢？
- en: '*Don’t panic!* ![nlpt_pin01](Images/nlpt_pin01.png) Transformers is designed
    to enable you to easily extend existing models for your specific use case. You
    can load the weights from pretrained models, and you have access to task-specific
    helper functions. This lets you build custom models for specific objectives with
    very little overhead. In this section, we’ll see how we can implement our own
    custom model.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*不要惊慌！* ![nlpt_pin01](Images/nlpt_pin01.png)变压器被设计为让您轻松扩展现有模型以适应您的特定用例。您可以加载预训练模型的权重，并且可以访问特定任务的辅助函数。这使您可以用非常少的开销为特定目标构建自定义模型。在本节中，我们将看到如何实现我们自己的自定义模型。'
- en: Bodies and Heads
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主体和头部
- en: 'The main concept that makes ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    so versatile is the split of the architecture into a *body* and *head* (as we
    saw in [Chapter 1](ch01.xhtml#chapter_introduction)). We have already seen that
    when we switch from the pretraining task to the downstream task, we need to replace
    the last layer of the model with one that is suitable for the task. This last
    layer is called the model head; it’s the part that is *task-specific*. The rest
    of the model is called the body; it includes the token embeddings and transformer
    layers that are *task-agnostic*. This structure is reflected in the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers code as well: the body of a model is implemented in a class such
    as `BertModel` or `GPT2Model` that returns the hidden states of the last layer.
    Task-specific models such as `BertForMaskedLM` or `BertForSequenceClassification`
    use the base model and add the necessary head on top of the hidden states, as
    shown in [Figure 4-4](#bert-body-head).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使 ![nlpt_pin01](Images/nlpt_pin01.png) Transformers 如此通用的主要概念是将架构分为 *主体* 和 *头部*（正如我们在[第1章](ch01.xhtml#chapter_introduction)中看到的）。我们已经看到，当我们从预训练任务切换到下游任务时，我们需要用适合任务的最后一层替换模型。这最后一层称为模型头部；它是*特定于任务*的部分。模型的其余部分称为主体；它包括*与任务无关*的标记嵌入和变换器层。这种结构也反映在
    ![nlpt_pin01](Images/nlpt_pin01.png) Transformers 代码中：模型的主体在类似 `BertModel` 或 `GPT2Model`
    的类中实现，返回最后一层的隐藏状态。特定于任务的模型，如 `BertForMaskedLM` 或 `BertForSequenceClassification`，使用基础模型，并在隐藏状态的顶部添加必要的头部，如[图4-4](#bert-body-head)所示。
- en: '![bert-body-head](Images/nlpt_0404.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![bert-body-head](Images/nlpt_0404.png)'
- en: Figure 4-4\. The `BertModel` class only contains the body of the model, while
    the `BertFor<Task>` classes combine the body with a dedicated head for a given
    task
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4\. `BertModel` 类仅包含模型的主体，而 `BertFor<Task>` 类将主体与给定任务的专用头部组合起来
- en: As we’ll see next, this separation of bodies and heads allows us to build a
    custom head for any task and just mount it on top of a pretrained model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看到，这种主体和头部的分离使我们能够为任何任务构建自定义头部，并将其直接安装在预训练模型的顶部。
- en: Creating a Custom Model for Token Classification
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为标记分类创建自定义模型
- en: Let’s go through the exercise of building a custom token classification head
    for XLM-R. Since XLM-R uses the same model architecture as RoBERTa, we will use
    RoBERTa as the base model, but augmented with settings specific to XLM-R. Note
    that this is an educational exercise to show you how to build a custom model for
    your own task. For token classification, an `XLMRobertaForTokenClassification`
    class already exists that you can import from ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers. If you want, you can skip to the next section and simply use that
    one.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来练习为 XLM-R 构建一个自定义的标记分类头。由于 XLM-R 使用与 RoBERTa 相同的模型架构，我们将使用 RoBERTa 作为基础模型，但增加了特定于
    XLM-R 的设置。请注意，这是一个教育性的练习，向您展示如何为自己的任务构建自定义模型。对于标记分类，已经存在一个 `XLMRobertaForTokenClassification`
    类，您可以从 ![nlpt_pin01](Images/nlpt_pin01.png) Transformers 中导入。如果愿意，您可以跳到下一节，直接使用那个。
- en: 'To get started, we need a data structure that will represent our XLM-R NER
    tagger. As a first guess, we’ll need a configuration object to initialize the
    model and a `forward()` function to generate the outputs. Let’s go ahead and build
    our XLM-R class for token classification:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们需要一个数据结构来表示我们的 XLM-R NER 标记器。作为第一个猜测，我们将需要一个配置对象来初始化模型和一个 `forward()`
    函数来生成输出。让我们继续构建我们的 XLM-R 标记分类的类：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `config_class` ensures that the standard XLM-R settings are used when we
    initialize a new model. If you want to change the default parameters, you can
    do this by overwriting the default settings in the configuration. With the `super()`
    method we call the initialization function of the `RobertaPreTrainedModel` class.
    This abstract class handles the initialization or loading of pretrained weights.
    Then we load our model body, which is `RobertaModel`, and extend it with our own
    classification head consisting of a dropout and a standard feed-forward layer.
    Note that we set `add_​pool⁠ing_layer=False` to ensure all hidden states are returned
    and not only the one associated with the `[CLS]` token. Finally, we initialize
    all the weights by calling the `init_weights()` method we inherit from `RobertaPreTrainedModel`,
    which will load the pretrained weights for the model body and randomly initialize
    the weights of our token classification head.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`config_class` 确保在初始化新模型时使用标准的 XLM-R 设置。如果要更改默认参数，可以通过覆盖配置中的默认设置来实现。使用 `super()`
    方法调用 `RobertaPreTrainedModel` 类的初始化函数。这个抽象类处理预训练权重的初始化或加载。然后我们加载我们的模型主体，即 `RobertaModel`，并用自己的分类头扩展它，包括一个
    dropout 和一个标准的前馈层。请注意，我们设置 `add_​pool⁠ing_layer=False` 以确保返回所有隐藏状态，而不仅仅是与 `[CLS]`
    标记相关联的隐藏状态。最后，我们通过调用从 `RobertaPreTrainedModel` 继承的 `init_weights()` 方法来初始化所有权重，这将加载模型主体的预训练权重并随机初始化我们的标记分类头的权重。'
- en: The only thing left to do is to define what the model should do in a forward
    pass with a `forward()` method. During the forward pass, the data is first fed
    through the model body. There are a number of input variables, but the only ones
    we need for now are `input_ids` and `attention_mask`. The hidden state, which
    is part of the model body output, is then fed through the dropout and classification
    layers. If we also provide labels in the forward pass, we can directly calculate
    the loss. If there is an attention mask we need to do a little bit more work to
    make sure we only calculate the loss of the unmasked tokens. Finally, we wrap
    all the outputs in a `TokenClassifierOutput` object that allows us to access elements
    in a the familiar named tuple from previous chapters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的事情就是定义模型在前向传递中应该做什么，使用 `forward()` 方法。在前向传递期间，数据首先通过模型主体进行馈送。有许多输入变量，但我们现在只需要
    `input_ids` 和 `attention_mask`。然后，模型主体输出的隐藏状态通过 dropout 和分类层进行馈送。如果我们在前向传递中还提供标签，我们可以直接计算损失。如果有注意力掩码，我们需要做一些额外的工作，以确保我们只计算未掩码标记的损失。最后，我们将所有输出封装在一个
    `TokenClassifierOutput` 对象中，这样我们就可以从前几章中熟悉的命名元组中访问元素。
- en: By just implementing two functions of a simple class, we can build our own custom
    transformer model. And since we inherit from a `PreTrainedModel`, we instantly
    get access to all the useful ![nlpt_pin01](Images/nlpt_pin01.png) Transformer
    utilities, such as `from_pretrained()`! Let’s have a look how we can load pretrained
    weights into our custom model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实现一个简单类的两个函数，我们就可以构建自己的自定义转换器模型。而且，由于我们继承自`PreTrainedModel`，我们立即就可以访问所有有用的![nlpt_pin01](Images/nlpt_pin01.png)
    Transformer实用工具，比如`from_pretrained()`！让我们看看如何将预训练权重加载到我们的自定义模型中。
- en: Loading a Custom Model
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载自定义模型
- en: 'Now we are ready to load our token classification model. We’ll need to provide
    some additional information beyond the model name, including the tags that we
    will use to label each entity and the mapping of each tag to an ID and vice versa.
    All of this information can be derived from our `tags` variable, which as a `ClassLabel`
    object has a `names` attribute that we can use to derive the mapping:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备加载我们的标记分类模型。除了模型名称之外，我们还需要提供一些额外的信息，包括我们将用于标记每个实体的标签以及每个标签与ID之间的映射，反之亦然。所有这些信息都可以从我们的`tags`变量中派生出来，作为一个`ClassLabel`对象，它具有一个我们可以用来派生映射的`names`属性：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We’ll store these mappings and the `tags.num_classes` attribute in the `AutoConfig`
    object that we encountered in [Chapter 3](ch03.xhtml#chapter_anatomy). Passing
    keyword arguments to the `from_pretrained()` method overrides the default values:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这些映射和`tags.num_classes`属性存储在我们在[第3章](ch03.xhtml#chapter_anatomy)中遇到的`AutoConfig`对象中。通过向`from_pretrained()`方法传递关键字参数来覆盖默认值：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `AutoConfig` class contains the blueprint of a model’s architecture. When
    we load a model with `AutoModel.from_pretrained(*model_ckpt*)`, the configuration
    file associated with that model is downloaded automatically. However, if we want
    to modify something like the number of classes or label names, then we can load
    the configuration first with the parameters we would like to customize.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`AutoConfig`类包含了模型架构的蓝图。当我们使用`AutoModel.from_pretrained(*model_ckpt*)`加载模型时，与该模型关联的配置文件会自动下载。然而，如果我们想要修改诸如类的数量或标签名称之类的东西，那么我们可以首先加载配置，然后使用我们想要自定义的参数加载配置。'
- en: 'Now, we can load the model weights as usual with the `from_pretrained()` function
    with the additional `config` argument. Note that we did not implement loading
    pretrained weights in our custom model class; we get this for free by inheriting
    from `RobertaPreTrainedModel`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像往常一样使用`from_pretrained()`函数加载模型权重，还可以使用额外的`config`参数。请注意，我们没有在我们的自定义模型类中实现加载预训练权重；我们通过从`RobertaPreTrainedModel`继承来免费获得这一点：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As a quick check that we have initialized the tokenizer and model correctly,
    let’s test the predictions on our small sequence of known entities:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速检查我们是否正确初始化了标记器和模型，让我们在我们已知实体的小序列上测试预测：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Tokens | <s> | ▁Jack | ▁Spar | row | ▁love | s | ▁New | ▁York | ! | </s>
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Tokens | <s> | ▁Jack | ▁Spar | row | ▁love | s | ▁New | ▁York | ! | </s>
    |'
- en: '| Input IDs | 0 | 21763 | 37456 | 15555 | 5161 | 7 | 2356 | 5753 | 38 | 2 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Input IDs | 0 | 21763 | 37456 | 15555 | 5161 | 7 | 2356 | 5753 | 38 | 2 |'
- en: As you can see here, the start `<s>` and end `</s>` tokens are given the IDs
    0 and 2, respectively.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这里看到的，起始`<s>`和结束`</s>`标记分别被赋予了ID 0和2。
- en: 'Finally, we need to pass the inputs to the model and extract the predictions
    by taking the argmax to get the most likely class per token:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将输入传递给模型，并通过取argmax来提取预测，以获得每个标记最有可能的类：
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here we see that the logits have the shape `[batch_size, num_tokens, num_tags]`,
    with each token given a logit among the seven possible NER tags. By enumerating
    over the sequence, we can quickly see what the pretrained model predicts:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到logits的形状为`[batch_size, num_tokens, num_tags]`，每个标记都被赋予了七个可能的NER标记中的一个logit。通过枚举序列，我们可以快速看到预训练模型的预测：
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Tokens | <s> | ▁Jack | ▁Spar | row | ▁love | s | ▁New | ▁York | ! | </s>
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Tokens | <s> | ▁Jack | ▁Spar | row | ▁love | s | ▁New | ▁York | ! | </s>
    |'
- en: '| Tags | O | I-LOC | B-LOC | B-LOC | O | I-LOC | O | O | I-LOC | B-LOC |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Tags | O | I-LOC | B-LOC | B-LOC | O | I-LOC | O | O | I-LOC | B-LOC |'
- en: 'Unsurprisingly, our token classification layer with random weights leaves a
    lot to be desired; let’s fine-tune on some labeled data to make it better! Before
    doing so, let’s wrap the preceding steps into a helper function for later use:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，我们的具有随机权重的标记分类层还有很多需要改进的地方；让我们在一些带标签的数据上进行微调，使其变得更好！在这样做之前，让我们把前面的步骤封装成一个辅助函数，以备后用：
- en: '[PRE28]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Before we can train the model, we also need to tokenize the inputs and prepare
    the labels. We’ll do that next.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练模型之前，我们还需要对输入进行标记化处理并准备标签。我们接下来会做这个。
- en: Tokenizing Texts for NER
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于NER的标记化文本
- en: 'Now that we’ve established that the tokenizer and model can encode a single
    example, our next step is to tokenize the whole dataset so that we can pass it
    to the XLM-R model for fine-tuning. As we saw in [Chapter 2](ch02.xhtml#chapter_classification),
    ![nlpt_pin01](Images/nlpt_pin01.png) Datasets provides a fast way to tokenize
    a `Dataset` object with the `map()` operation. To achieve this, recall that we
    first need to define a function with the minimal signature:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了标记器和模型可以对单个示例进行编码，我们的下一步是对整个数据集进行标记化处理，以便我们可以将其传递给XLM-R模型进行微调。正如我们在[第2章](ch02.xhtml#chapter_classification)中看到的那样，![nlpt_pin01](Images/nlpt_pin01.png)
    Datasets提供了一种快速的方法来使用`map()`操作对`Dataset`对象进行标记化处理。为了实现这一点，我们首先需要定义一个具有最小签名的函数：
- en: '[PRE29]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: where `examples` is equivalent to a slice of a `Dataset`, e.g., `panx_de['train'][:10]`.
    Since the XLM-R tokenizer returns the input IDs for the model’s inputs, we just
    need to augment this information with the attention mask and the label IDs that
    encode the information about which token is associated with each NER tag.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`examples`相当于`Dataset`的一个切片，例如`panx_de['train'][:10]`。由于XLM-R标记器返回模型输入的输入ID，我们只需要用注意力掩码和编码关于每个NER标记与每个标记相关联的哪个标记的信息的标签ID来增加这些信息。
- en: 'Following the approach taken in the ![nlpt_pin01](Images/nlpt_pin01.png) [Transformers
    documentation](https://oreil.ly/lGPgh), let’s look at how this works with our
    single German example by first collecting the words and tags as ordinary lists:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 按照![nlpt_pin01](Images/nlpt_pin01.png) [Transformers documentation](https://oreil.ly/lGPgh)中采用的方法，让我们看看这如何在我们的单个德语示例中运作，首先将单词和标签收集为普通列表：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we tokenize each word and use the `is_split_into_words` argument to tell
    the tokenizer that our input sequence has already been split into words:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对每个单词进行标记化，并使用`is_split_into_words`参数告诉标记器我们的输入序列已经被分成了单词：
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | ... | 18 | 19 | 20 | 21 | 22 | 23 | 24 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | ... | 18 | 19 | 20 | 21 | 22 | 23 | 24 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
- en: '| Tokens | <s> | ▁2.000 | ▁Einwohner | n | ▁an | ▁der | ▁Dan | ... | schaft
    | ▁Po | mmer | n | ▁ | . | </s> |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | <s> | ▁2.000 | ▁Einwohner | n | ▁an | ▁der | ▁Dan | ... | schaft | ▁Po
    | mmer | n | ▁ | . | </s> |'
- en: 'In this example we can see that the tokenizer has split “Einwohnern” into two
    subwords, “▁Einwohner” and “n”. Since we’re following the convention that only
    “▁Einwohner” should be associated with the `B-LOC` label, we need a way to mask
    the subword representations after the first subword. Fortunately, `tokenized_input`
    is a class that contains a `word_ids()` function that can help us achieve this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到标记器将“Einwohnern”分成了两个子词，“▁Einwohner”和“n”。由于我们遵循的约定是只有“▁Einwohner”应该与`B-LOC`标签相关联，我们需要一种方法来屏蔽第一个子词之后的子词表示。幸运的是，`tokenized_input`是一个包含`word_ids()`函数的类，可以帮助我们实现这一点：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | ... | 18 | 19 | 20 | 21 | 22 | 23 | 24 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | ... | 18 | 19 | 20 | 21 | 22 | 23 | 24 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
- en: '| Tokens | <s> | ▁2.000 | ▁Einwohner | n | ▁an | ▁der | ▁Dan | ... | schaft
    | ▁Po | mmer | n | ▁ | . | </s> |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | <s> | ▁2.000 | ▁Einwohner | n | ▁an | ▁der | ▁Dan | ... | schaft | ▁Po
    | mmer | n | ▁ | . | </s> |'
- en: '| Word IDs | None | 0 | 1 | 1 | 2 | 3 | 4 | ... | 9 | 10 | 10 | 10 | 11 | 11
    | None |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 单词ID | None | 0 | 1 | 1 | 2 | 3 | 4 | ... | 9 | 10 | 10 | 10 | 11 | 11 |
    None |'
- en: 'Here we can see that `word_ids` has mapped each subword to the corresponding
    index in the `words` sequence, so the first subword, “▁2.000”, is assigned the
    index 0, while “▁Einwohner” and “n” are assigned the index 1 (since “Einwohnern”
    is the second word in `words`). We can also see that special tokens like `<s>`
    and `<\s>` are mapped to `None`. Let’s set –100 as the label for these special
    tokens and the subwords we wish to mask during training:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到`word_ids`已经将每个子词映射到`words`序列中的相应索引，因此第一个子词“▁2.000”被分配索引0，而“▁Einwohner”和“n”被分配索引1（因为“Einwohnern”是`words`中的第二个单词）。我们还可以看到像`<s>`和`<\s>`这样的特殊标记被映射为`None`。让我们将-100设置为这些特殊标记和训练过程中希望屏蔽的子词的标签：
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | ... | 19 | 20 | 21 | 22 | 23 | 24 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | ... | 19 | 20 | 21 | 22 | 23 | 24 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
- en: '| Tokens | <s> | ▁2.000 | ▁Einwohner | n | ▁an | ▁der | ... | ▁Po | mmer |
    n | ▁ | . | </s> |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | <s> | ▁2.000 | ▁Einwohner | n | ▁an | ▁der | ... | ▁Po | mmer | n |
    ▁ | . | </s> |'
- en: '| Word IDs | None | 0 | 1 | 1 | 2 | 3 | ... | 10 | 10 | 10 | 11 | 11 | None
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 单词ID | None | 0 | 1 | 1 | 2 | 3 | ... | 10 | 10 | 10 | 11 | 11 | None |'
- en: '| Label IDs | -100 | 0 | 0 | -100 | 0 | 0 | ... | 6 | -100 | -100 | 0 | -100
    | -100 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 标签ID | -100 | 0 | 0 | -100 | 0 | 0 | ... | 6 | -100 | -100 | 0 | -100 | -100
    |'
- en: '| Labels | IGN | O | O | IGN | O | O | ... | I-LOC | IGN | IGN | O | IGN |
    IGN |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | IGN | O | O | IGN | O | O | ... | I-LOC | IGN | IGN | O | IGN | IGN
    |'
- en: Note
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Why did we choose –100 as the ID to mask subword representations? The reason
    is that in PyTorch the cross-entropy loss class `torch.nn.CrossEntropyLoss` has
    an attribute called `ignore_index` whose value is –100\. This index is ignored
    during training, so we can use it to ignore the tokens associated with consecutive
    subwords.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们选择-100作为屏蔽子词表示的ID？原因是在PyTorch中，交叉熵损失类`torch.nn.CrossEntropyLoss`有一个名为`ignore_index`的属性，其值为-100。在训练过程中会忽略此索引，因此我们可以使用它来忽略与连续子词相关联的标记。
- en: 'And that’s it! We can clearly see how the label IDs align with the tokens,
    so let’s scale this out to the whole dataset by defining a single function that
    wraps all the logic:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部！我们可以清楚地看到标签ID与标记对齐，所以让我们通过定义一个包装所有逻辑的单个函数，将其扩展到整个数据集：
- en: '[PRE34]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We now have all the ingredients we need to encode each split, so let’s write
    a function we can iterate over:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了编码每个拆分所需的所有要素，让我们编写一个可以迭代的函数：
- en: '[PRE35]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'By applying this function to a `DatasetDict` object, we get an encoded `Dataset`
    object per split. Let’s use this to encode our German corpus:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将此函数应用于`DatasetDict`对象，我们可以得到每个拆分的编码`Dataset`对象。让我们使用这个来对我们的德语语料库进行编码：
- en: '[PRE36]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now that we have a model and a dataset, we need to define a performance metric.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个模型和一个数据集，我们需要定义一个性能指标。
- en: Performance Measures
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能指标
- en: 'Evaluating a NER model is similar to evaluating a text classification model,
    and it is common to report results for precision, recall, and *F*[1]-score. The
    only subtlety is that *all* words of an entity need to be predicted correctly
    in order for a prediction to be counted as correct. Fortunately, there is a nifty
    library called [*seqeval*](https://oreil.ly/xbKOp) that is designed for these
    kinds of tasks. For example, given some placeholder NER tags and model predictions,
    we can compute the metrics via seqeval’s `classification_report()` function:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 评估NER模型类似于评估文本分类模型，通常报告精确度、召回率和*F*[1]-分数的结果。唯一的微妙之处在于，实体的*所有*单词都需要被正确预测，才能将预测视为正确。幸运的是，有一个名为[*seqeval*](https://oreil.ly/xbKOp)的巧妙库专门用于这类任务。例如，给定一些占位符NER标签和模型预测，我们可以通过seqeval的`classification_report()`函数计算指标：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As we can see, *seqeval* expects the predictions and labels as lists of lists,
    with each list corresponding to a single example in our validation or test sets.
    To integrate these metrics during training, we need a function that can take the
    outputs of the model and convert them into the lists that *seqeval* expects. The
    following does the trick by ensuring we ignore the label IDs associated with subsequent
    subwords:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，*seqeval*期望预测和标签作为列表的列表，每个列表对应于我们的验证或测试集中的单个示例。为了在训练过程中集成这些指标，我们需要一个函数，可以获取模型的输出并将其转换为*seqeval*所期望的列表。以下方法可以确保我们忽略与后续子词相关联的标签ID：
- en: '[PRE39]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Equipped with a performance metric, we can move on to actually training the
    model.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 有了性能指标，我们可以开始实际训练模型了。
- en: Fine-Tuning XLM-RoBERTa
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调XLM-RoBERTa
- en: 'We now have all the ingredients to fine-tune our model! Our first strategy
    will be to fine-tune our base model on the German subset of PAN-X and then evaluate
    its zero-shot cross-lingual performance on French, Italian, and English. As usual,
    we’ll use the ![nlpt_pin01](Images/nlpt_pin01.png) Transformers `Trainer` to handle
    our training loop, so first we need to define the training attributes using the
    `TrainingArguments` class:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有微调模型的要素！我们的第一个策略是在PAN-X的德语子集上微调我们的基础模型，然后评估其在法语、意大利语和英语上的零射击跨语言性能。像往常一样，我们将使用![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers `Trainer`来处理我们的训练循环，所以首先我们需要使用`TrainingArguments`类定义训练属性：
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Here we evaluate the model’s predictions on the validation set at the end of
    every epoch, tweak the weight decay, and set `save_steps` to a large number to
    disable checkpointing and thus speed up training.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时代结束时，我们评估模型对验证集的预测，调整权重衰减，并将`save_steps`设置为一个较大的数字，以禁用检查点并加快训练速度。
- en: 'This is also a good point to make sure we are logged in to the Hugging Face
    Hub (if you’re working in a terminal, you can execute the command `huggingface-cli
    login` instead):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一个好时机，确保我们已登录到Hugging Face Hub（如果您在终端工作，可以执行命令`huggingface-cli login`）：
- en: '[PRE41]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We also need to tell the `Trainer` how to compute metrics on the validation
    set, so here we can use the `align_predictions()` function that we defined earlier
    to extract the predictions and labels in the format needed by *seqeval* to calculate
    the *F*[1]-score:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要告诉`Trainer`如何在验证集上计算指标，因此我们可以使用之前定义的`align_predictions()`函数来提取*seqeval*计算*F*[1]-score所需格式的预测和标签：
- en: '[PRE42]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The final step is to define a *data collator* so we can pad each input sequence
    to the largest sequence length in a batch. ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers provides a dedicated data collator for token classification that
    will pad the labels along with the inputs:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是定义一个*数据收集器*，以便我们可以将每个输入序列填充到批处理中的最大序列长度。![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers提供了专门用于标记分类的数据收集器，它将填充标签以及输入：
- en: '[PRE43]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Padding the labels is necessary because, unlike in a text classification task,
    the labels are also sequences. One important detail here is that the label sequences
    are padded with the value –100, which, as we’ve seen, is ignored by PyTorch loss
    functions.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 填充标签是必要的，因为与文本分类任务不同，标签也是序列。这里的一个重要细节是，标签序列用值-100进行填充，正如我们所见，这个值会被PyTorch损失函数忽略。
- en: 'We will train several models in the course of this chapter, so we’ll avoid
    initializing a new model for every `Trainer` by creating a `model_init()` method.
    This method loads an untrained model and is called at the beginning of the `train()`
    call:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的过程中，我们将训练几个模型，因此我们将通过创建`model_init()`方法来避免为每个`Trainer`初始化一个新模型。该方法加载一个未经训练的模型，并在`train()`调用开始时调用：
- en: '[PRE44]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can now pass all this information together with the encoded datasets to
    the `Trainer`:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将所有这些信息与编码的数据集一起传递给`Trainer`：
- en: '[PRE45]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'and then run the training loop as follows and push the final model to the Hub:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按照以下方式运行训练循环，并将最终模型推送到Hub：
- en: '[PRE46]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '| Epoch | Training Loss | Validation Loss | F1 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Epoch | Training Loss | Validation Loss | F1 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 0.2652 | 0.160244 | 0.822974 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.2652 | 0.160244 | 0.822974 |'
- en: '| 2 | 0.1314 | 0.137195 | 0.852747 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.1314 | 0.137195 | 0.852747 |'
- en: '| 3 | 0.0806 | 0.138774 | 0.864591 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.0806 | 0.138774 | 0.864591 |'
- en: 'These F1 scores are quite good for a NER model. To confirm that our model works
    as expected, let’s test it on the German translation of our simple example:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这些F1分数对于NER模型来说相当不错。为了确认我们的模型按预期工作，让我们在我们简单示例的德语翻译上进行测试：
- en: '[PRE47]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | ... | 8 | 9 | 10 | 11 | 12 | 13 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | ... | 8 | 9 | 10 | 11 | 12 | 13 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
- en: '| Tokens | <s> | ▁Jeff | ▁De | an | ▁ist | ▁ein | ... | ▁bei | ▁Google | ▁in
    | ▁Kaliforni | en | </s> |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Tokens | <s> | ▁Jeff | ▁De | an | ▁ist | ▁ein | ... | ▁bei | ▁Google | ▁in
    | ▁Kaliforni | en | </s> |'
- en: '| Tags | O | B-PER | I-PER | I-PER | O | O | ... | O | B-ORG | O | B-LOC |
    I-LOC | O |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Tags | O | B-PER | I-PER | I-PER | O | O | ... | O | B-ORG | O | B-LOC |
    I-LOC | O |'
- en: It works! But we should never get too confident about performance based on a
    single example. Instead, we should conduct a proper and thorough investigation
    of the model’s errors. In the next section we explore how to do this for the NER
    task.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有效！但我们不应该对基于单个示例的性能过于自信。相反，我们应该对模型的错误进行适当和彻底的调查。在下一节中，我们将探讨如何在NER任务中进行这样的调查。
- en: Error Analysis
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误分析
- en: 'Before we dive deeper into the multilingual aspects of XLM-R, let’s take a
    minute to investigate the errors of our model. As we saw in [Chapter 2](ch02.xhtml#chapter_classification),
    a thorough error analysis of your model is one of the most important aspects when
    training and debugging transformers (and machine learning models in general).
    There are several failure modes where it might look like the model is performing
    well, while in practice it has some serious flaws. Examples where training can
    fail include:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨XLM-R的多语言方面之前，让我们花一分钟来调查我们模型的错误。正如我们在[第2章](ch02.xhtml#chapter_classification)中看到的，对模型进行彻底的错误分析是训练和调试变压器（以及机器学习模型一般）最重要的方面之一。有几种失败模式，其中模型看起来表现良好，而实际上它存在一些严重的缺陷。训练可能失败的例子包括：
- en: We might accidentally mask too many tokens and also mask some of our labels
    to get a really promising loss drop.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能会意外地屏蔽太多的标记，也会屏蔽一些标签，以获得真正有希望的损失下降。
- en: The `compute_metrics()` function might have a bug that overestimates the true
    performance.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_metrics()`函数可能存在一个高估真实性能的错误。'
- en: We might include the zero class or `O` entity in NER as a normal class, which
    will heavily skew the accuracy and *F*[1]-score since it is the majority class
    by a large margin.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能会将零类或`O`实体包括在NER中作为正常类别，这将严重扭曲准确性和*F*[1]-分数，因为它是绝大多数类别。
- en: When the model performs much worse than expected, looking at the errors can
    yield useful insights and reveal bugs that would be hard to spot by just looking
    at the code. And even if the model performs well and there are no bugs in the
    code, error analysis is still a useful tool to understand the model’s strengths
    and weaknesses. These are aspects we always need to keep in mind when we deploy
    a model in a production environment.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型的表现远低于预期时，查看错误可能会提供有用的见解，并揭示很难仅通过查看代码就能发现的错误。即使模型表现良好，并且代码中没有错误，错误分析仍然是了解模型优势和劣势的有用工具。这些都是我们在将模型部署到生产环境时需要牢记的方面。
- en: For our analysis we will again use one of the most powerful tools at our disposal,
    which is to look at the validation examples with the highest loss. We can reuse
    much of the function we built to analyze the sequence classification model in
    [Chapter 2](ch02.xhtml#chapter_classification), but we’ll now calculate a loss
    per token in the sample sequence.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的分析，我们将再次使用我们手头上最强大的工具之一，即查看损失最大的验证示例。我们可以重复使用我们构建的函数的大部分内容来分析[第2章](ch02.xhtml#chapter_classification)中的序列分类模型，但现在我们将计算样本序列中每个标记的损失。
- en: 'Let’s define a method that we can apply to the validation set:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个可以应用于验证集的方法：
- en: '[PRE48]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can now apply this function to the whole validation set using `map()` and
    load all the data into a `DataFrame` for further analysis:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`map()`将这个函数应用到整个验证集，并将所有数据加载到`DataFrame`中进行进一步分析：
- en: '[PRE49]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The tokens and the labels are still encoded with their IDs, so let’s map the
    tokens and labels back to strings to make it easier to read the results. For the
    padding tokens with label –100 we assign a special label, `IGN`, so we can filter
    them later. We also get rid of all the padding in the `loss` and `predicted_label`
    fields by truncating them to the length of the inputs:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 标记和标签仍然使用它们的ID进行编码，因此让我们将标记和标签映射回字符串，以便更容易阅读结果。对于带有标签-100的填充标记，我们分配一个特殊的标签`IGN`，以便稍后过滤它们。我们还通过将它们截断到输入的长度来消除`loss`和`predicted_label`字段中的所有填充：
- en: '[PRE50]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '|  | attention_mask | input_ids | labels | loss | predicted_label | input_tokens
    |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | attention_mask | input_ids | labels | loss | predicted_label | input_tokens
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | [1, 1, 1, 1, 1, 1, 1] | [0, 10699, 11, 15, 16104, 1388, 2] | [IGN, B-ORG,
    IGN, I-ORG, I-ORG, I-ORG, IGN] | [0.0, 0.014679872, 0.0, 0.009469474, 0.010393422,
    0.01293836, 0.0] | [I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG] | [<s>, ▁Ham,
    a, ▁(, ▁Unternehmen, ▁), </s>] |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 0 | [1, 1, 1, 1, 1, 1, 1] | [0, 10699, 11, 15, 16104, 1388, 2] | [IGN, B-ORG,
    IGN, I-ORG, I-ORG, I-ORG, IGN] | [0.0, 0.014679872, 0.0, 0.009469474, 0.010393422,
    0.01293836, 0.0] | [I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG] | [<s>, ▁Ham,
    a, ▁(, ▁Unternehmen, ▁), </s>] |'
- en: 'Each column contains a list of tokens, labels, predicted labels, and so on
    for each sample. Let’s have a look at the tokens individually by unpacking these
    lists. The `pan⁠das.Series.explode()` function allows us to do exactly that in
    one line by creating a row for each element in the original rows list. Since all
    the lists in one row have the same length, we can do this in parallel for all
    columns. We also drop the padding tokens we named `IGN`, since their loss is zero
    anyway. Finally, we cast the losses, which are still `numpy.Array` objects, to
    standard floats:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 每一列都包含一个标记、标签、预测标签等的列表，让我们逐个查看这些标记。`pan⁠das.Series.explode()`函数允许我们在一行中为原始行列表中的每个元素创建一行。由于一行中的所有列表长度相同，我们可以并行进行操作。我们还丢弃了我们命名为`IGN`的填充标记，因为它们的损失无论如何都是零。最后，我们将仍然是`numpy.Array`对象的损失转换为标准浮点数：
- en: '[PRE51]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '| attention_mask | input_ids | labels | loss | predicted_label | input_tokens
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| attention_mask | input_ids | labels | loss | predicted_label | input_tokens
    |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 10699 | B-ORG | 0.01 | B-ORG | ▁Ham |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 10699 | B-ORG | 0.01 | B-ORG | ▁Ham |'
- en: '| 1 | 15 | I-ORG | 0.01 | I-ORG | ▁( |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 15 | I-ORG | 0.01 | I-ORG | ▁( |'
- en: '| 1 | 16104 | I-ORG | 0.01 | I-ORG | ▁Unternehmen |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 16104 | I-ORG | 0.01 | I-ORG | ▁Unternehmen |'
- en: '| 1 | 1388 | I-ORG | 0.01 | I-ORG | ▁) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1388 | I-ORG | 0.01 | I-ORG | ▁) |'
- en: '| 1 | 56530 | O | 0.00 | O | ▁WE |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 56530 | O | 0.00 | O | ▁WE |'
- en: '| 1 | 83982 | B-ORG | 0.34 | B-ORG | ▁Luz |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 83982 | B-ORG | 0.34 | B-ORG | ▁Luz |'
- en: '| 1 | 10 | I-ORG | 0.45 | I-ORG | ▁a |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 10 | I-ORG | 0.45 | I-ORG | ▁a |'
- en: 'With the data in this shape, we can now group it by the input tokens and aggregate
    the losses for each token with the count, mean, and sum. Finally, we sort the
    aggregated data by the sum of the losses and see which tokens have accumulated
    the most loss in the validation set:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这样的数据，我们现在可以按输入标记对其进行分组，并聚合每个标记的损失、计数、均值和总和。最后，我们按损失的总和对聚合数据进行排序，并查看验证集中累积损失最多的标记：
- en: '[PRE52]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| input_tokens | ▁ | ▁der | ▁in | ▁von | ▁/ | ▁und | ▁( | ▁) | ▁'''' | ▁A |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 输入标记 | ▁ | ▁der | ▁in | ▁von | ▁/ | ▁und | ▁( | ▁) | ▁'''' | ▁A |'
- en: '| count | 6066 | 1388 | 989 | 808 | 163 | 1171 | 246 | 246 | 2898 | 125 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 计数 | 6066 | 1388 | 989 | 808 | 163 | 1171 | 246 | 246 | 2898 | 125 |'
- en: '| mean | 0.03 | 0.1 | 0.14 | 0.14 | 0.64 | 0.08 | 0.3 | 0.29 | 0.02 | 0.44
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 0.03 | 0.1 | 0.14 | 0.14 | 0.64 | 0.08 | 0.3 | 0.29 | 0.02 | 0.44 |'
- en: '| sum | 200.71 | 138.05 | 137.33 | 114.92 | 104.28 | 99.15 | 74.49 | 72.35
    | 59.31 | 54.48 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 总和 | 200.71 | 138.05 | 137.33 | 114.92 | 104.28 | 99.15 | 74.49 | 72.35 |
    59.31 | 54.48 |'
- en: 'We can observe several patterns in this list:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这个列表中观察到几种模式：
- en: The whitespace token has the highest total loss, which is not surprising since
    it is also the most common token in the list. However, its mean loss is much lower
    than the other tokens in the list. This means that the model doesn’t struggle
    to classify it.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空格标记具有最高的总损失，这并不奇怪，因为它也是列表中最常见的标记。然而，它的平均损失要低得多。这意味着模型不会在对其进行分类时遇到困难。
- en: Words like “in”, “von”, “der”, and “und” appear relatively frequently. They
    often appear together with named entities and are sometimes part of them, which
    explains why the model might mix them up.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像“in”、“von”、“der”和“und”这样的词出现相对频繁。它们经常与命名实体一起出现，有时也是它们的一部分，这解释了为什么模型可能会混淆它们。
- en: Parentheses, slashes, and capital letters at the beginning of words are rarer
    but have a relatively high average loss. We will investigate them further.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 括号、斜杠和单词开头的大写字母很少见，但平均损失相对较高。我们将进一步调查它们。
- en: 'We can also group the label IDs and look at the losses for each class:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对标签ID进行分组，并查看每个类别的损失：
- en: '[PRE53]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| labels | B-ORG | I-LOC | I-ORG | B-LOC | B-PER | I-PER | O |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | B-ORG | I-LOC | I-ORG | B-LOC | B-PER | I-PER | O |'
- en: '| count | 2683 | 1462 | 3820 | 3172 | 2893 | 4139 | 43648 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 计数 | 2683 | 1462 | 3820 | 3172 | 2893 | 4139 | 43648 |'
- en: '| mean | 0.66 | 0.64 | 0.48 | 0.35 | 0.26 | 0.18 | 0.03 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 0.66 | 0.64 | 0.48 | 0.35 | 0.26 | 0.18 | 0.03 |'
- en: '| sum | 1769.47 | 930.94 | 1850.39 | 1111.03 | 760.56 | 750.91 | 1354.46 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 总和 | 1769.47 | 930.94 | 1850.39 | 1111.03 | 760.56 | 750.91 | 1354.46 |'
- en: We see that `B⁠-⁠ORG` has the highest average loss, which means that determining
    the beginning of an organization poses a challenge to our model.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到`B⁠-⁠ORG`的平均损失最高，这意味着确定组织的开始对我们的模型构成了挑战。
- en: 'We can break this down further by plotting the confusion matrix of the token
    classification, where we see that the beginning of an organization is often confused
    with the subsequent `I-ORG` token:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制标记分类的混淆矩阵来进一步分解这一点，在那里我们看到组织的开始经常与随后的`I-ORG`标记混淆：
- en: '[PRE54]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](Images/nlpt_04in01.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_04in01.png)'
- en: From the plot, we can see that our model tends to confuse the `B-ORG` and `I-ORG`
    entities the most. Otherwise, it is quite good at classifying the remaining entities,
    which is clear by the near diagonal nature of the confusion matrix.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看出，我们的模型往往最容易混淆`B-ORG`和`I-ORG`实体。否则，它在分类其余实体方面表现相当不错，这可以从混淆矩阵近似对角线的性质中清楚地看出。
- en: 'Now that we’ve examined the errors at the token level, let’s move on and look
    at sequences with high losses. For this calculation, we’ll revisit our “unexploded”
    `DataFrame` and calculate the total loss by summing over the loss per token. To
    do this, let’s first write a function that helps us display the token sequences
    with the labels and the losses:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在标记级别上检查了错误，让我们继续看一下损失较大的序列。为了进行这种计算，我们将重新审视我们的“未爆炸”的`DataFrame`，并通过对每个标记的损失求和来计算总损失。为此，让我们首先编写一个帮助我们显示带有标签和损失的标记序列的函数：
- en: '[PRE56]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | ... | 13 | 14 | 15 | 16 | 17 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | ... | 13 | 14 | 15 | 16 | 17 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| tokens | ▁'''' | 8 | . | ▁Juli | ▁'''' | ... | n | ischen | ▁Gar | de | </s>
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | ▁'''' | 8 | . | ▁Juli | ▁'''' | ... | n | ischen | ▁Gar | de | </s>
    |'
- en: '| labels | B-ORG | IGN | IGN | I-ORG | I-ORG | ... | IGN | IGN | I-ORG | IGN
    | IGN |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | B-ORG | IGN | IGN | I-ORG | I-ORG | ... | IGN | IGN | I-ORG | IGN |
    IGN |'
- en: '| preds | O | O | O | O | O | ... | I-ORG | I-ORG | I-ORG | I-ORG | O |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | O | O | O | O | O | ... | I-ORG | I-ORG | I-ORG | I-ORG | O |'
- en: '| losses | 7.89 | 0.00 | 0.00 | 6.88 | 8.05 | ... | 0.00 | 0.00 | 0.01 | 0.00
    | 0.00 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 损失 | 7.89 | 0.00 | 0.00 | 6.88 | 8.05 | ... | 0.00 | 0.00 | 0.01 | 0.00 |
    0.00 |'
- en: '|  | 0 | 1 | 2 | 3 | 4 | ... | 14 | 15 | 16 | 17 | 18 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | ... | 14 | 15 | 16 | 17 | 18 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| tokens | ▁'' | ▁'''' | ▁Τ | Κ | ▁'''' | ... | k | ▁'''' | ▁'' | ala | </s>
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | ▁'' | ▁'''' | ▁Τ | Κ | ▁'''' | ... | k | ▁'''' | ▁'' | ala | </s> |'
- en: '| labels | O | O | O | IGN | O | ... | IGN | I-LOC | I-LOC | IGN | IGN |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | O | O | O | IGN | O | ... | IGN | I-LOC | I-LOC | IGN | IGN |'
- en: '| preds | O | O | B-ORG | O | O | ... | O | O | O | O | O |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | O | O | B-ORG | O | O | ... | O | O | O | O | O |'
- en: '| losses | 0.00 | 0.00 | 3.59 | 0.00 | 0.00 | ... | 0.00 | 7.66 | 7.78 | 0.00
    | 0.00 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 损失 | 0.00 | 0.00 | 3.59 | 0.00 | 0.00 | ... | 0.00 | 7.66 | 7.78 | 0.00 |
    0.00 |'
- en: '|  | 0 | 1 | 2 | 3 | 4 | ... | 10 | 11 | 12 | 13 | 14 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | ... | 10 | 11 | 12 | 13 | 14 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| tokens | ▁United | ▁Nations | ▁Multi | dimensional | ▁Integra | ... | ▁the
    | ▁Central | ▁African | ▁Republic | </s> |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | ▁联合 | ▁国家 | ▁多 | 维 | ▁整合 | ... | ▁中央 | ▁非洲 | ▁共和国 | </s> |'
- en: '| labels | B-PER | I-PER | I-PER | IGN | I-PER | ... | I-PER | I-PER | I-PER
    | I-PER | IGN |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | B-PER | I-PER | I-PER | IGN | I-PER | ... | I-PER | I-PER | I-PER |
    I-PER | IGN |'
- en: '| preds | B-ORG | I-ORG | I-ORG | I-ORG | I-ORG | ... | I-ORG | I-ORG | I-ORG
    | I-ORG | I-ORG |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | B-ORG | I-ORG | I-ORG | I-ORG | I-ORG | ... | I-ORG | I-ORG | I-ORG
    | I-ORG | I-ORG |'
- en: '| losses | 6.46 | 5.59 | 5.51 | 0.00 | 5.11 | ... | 4.77 | 5.32 | 5.10 | 4.87
    | 0.00 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 损失 | 6.46 | 5.59 | 5.51 | 0.00 | 5.11 | ... | 4.77 | 5.32 | 5.10 | 4.87 |
    0.00 |'
- en: It is apparent that something is wrong with the labels of these samples; for
    example, the United Nations and the Central African Republic are each labeled
    as a person! At the same time, “8\. Juli” in the first example is labeled as an
    organization. It turns out the annotations for the PAN-X dataset were generated
    through an automated process. Such annotations are often referred to as “silver
    standard” (in contrast to the “gold standard” of human-generated annotations),
    and it is no surprise that there are cases where the automated approach failed
    to produce sensible labels. In fact, such failure modes are not unique to automatic
    approaches; even when humans carefully annotate data, mistakes can occur when
    the concentration of the annotators fades or they simply misunderstand the sentence.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这些样本的标签出现了问题；例如，联合国和中非共和国分别被标记为一个人！与此同时，第一个例子中的“8. Juli”被标记为一个组织。原来PAN-X数据集的注释是通过自动化过程生成的。这样的注释通常被称为“银标准”（与人工生成的注释的“金标准”相对），并且并不奇怪，自动化方法未能产生合理的标签。事实上，这种失败模式并不是自动方法的特有现象；即使在人类仔细注释数据时，当注释者的注意力分散或者他们简单地误解句子时，也会出现错误。
- en: 'Another thing we noticed earlier was that parentheses and slashes had a relatively
    high loss. Let’s look at a few examples of sequences with an opening parenthesis:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们早些时候注意到的另一件事是，括号和斜杠的损失相对较高。让我们看一些带有开括号的序列的例子：
- en: '[PRE57]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| tokens | ▁Ham | a | ▁( | ▁Unternehmen | ▁) | </s> |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | ▁Ham | a | ▁( | ▁Unternehmen | ▁) | </s> |'
- en: '| labels | B-ORG | IGN | I-ORG | I-ORG | I-ORG | IGN |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | B-ORG | IGN | I-ORG | I-ORG | I-ORG | IGN |'
- en: '| preds | B-ORG | I-ORG | I-ORG | I-ORG | I-ORG | I-ORG |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | B-ORG | I-ORG | I-ORG | I-ORG | I-ORG | I-ORG |'
- en: '| losses | 0.01 | 0.00 | 0.01 | 0.01 | 0.01 | 0.00 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 损失 | 0.01 | 0.00 | 0.01 | 0.01 | 0.01 | 0.00 |'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| tokens | ▁Kesk | kül | a | ▁( | ▁Mart | na | ▁) | </s> |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | ▁Kesk | kül | a | ▁( | ▁Mart | na | ▁) | </s> |'
- en: '| labels | B-LOC | IGN | IGN | I-LOC | I-LOC | IGN | I-LOC | IGN |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | B-LOC | IGN | IGN | I-LOC | I-LOC | IGN | I-LOC | IGN |'
- en: '| preds | B-LOC | I-LOC | I-LOC | I-LOC | I-LOC | I-LOC | I-LOC | I-LOC |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | B-LOC | I-LOC | I-LOC | I-LOC | I-LOC | I-LOC | I-LOC | I-LOC |'
- en: '| losses | 0.02 | 0.00 | 0.00 | 0.01 | 0.01 | 0.00 | 0.01 | 0.00 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 损失 | 0.02 | 0.00 | 0.00 | 0.01 | 0.01 | 0.00 | 0.01 | 0.00 |'
- en: In general we would not include the parentheses and their contents as part of
    the named entity, but this seems to be the way the automatic extraction annotated
    the documents. In the other examples, the parentheses contain a geographic specification.
    While this is indeed a location as well, we might want disconnect it from the
    original location in the annotations. This dataset consists of Wikipedia articles
    in different languages, and the article titles often contain some sort of explanation
    in parentheses. For instance, in the first example the text in parentheses indicates
    that Hama is an “Unternehmen,” or company in English. These are important details
    to know when we roll out the model, as they might have implications on the downstream
    performance of the whole pipeline the model is part of.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们不会将括号及其内容包括在命名实体的一部分，但这似乎是自动提取标注文档的方式。在其他例子中，括号中包含地理位置的说明。虽然这确实也是一个位置，但我们可能希望在注释中将其与原始位置断开。这个数据集由不同语言的维基百科文章组成，文章标题通常包含括号中的某种解释。例如，在第一个例子中，括号中的文本表明哈马是一个“Unternehmen”，或者在英语中是公司。当我们推出模型时，了解这些重要细节是很重要的，因为它们可能对模型所属的整个流水线的下游性能产生影响。
- en: With a relatively simple analysis, we’ve identified some weaknesses in both
    our model and the dataset. In a real use case we would iterate on this step, cleaning
    up the dataset, retraining the model, and analyzing the new errors until we were
    satisfied with the performance.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 通过相对简单的分析，我们已经确定了我们的模型和数据集的一些弱点。在实际用例中，我们将在这一步上进行迭代，清理数据集，重新训练模型，并分析新的错误，直到我们对性能感到满意。
- en: Here we analyzed the errors on a single language, but we are also interested
    in the performance across languages. In the next section we’ll perform some experiments
    to see how well the cross-lingual transfer in XLM-R works.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们分析了单一语言的错误，但我们也对跨语言的性能感兴趣。在下一节中，我们将进行一些实验，看看XLM-R中的跨语言转移效果如何。
- en: Cross-Lingual Transfer
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨语言转移
- en: 'Now that we have fine-tuned XLM-R on German, we can evaluate its ability to
    transfer to other languages via the `predict()` method of the `Trainer`. Since
    we plan to evaluate multiple languages, let’s create a simple function that does
    this for us:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在德语上对XLM-R进行了微调，我们可以通过`Trainer`的`predict()`方法评估它对其他语言的转移能力。由于我们计划评估多种语言，让我们创建一个简单的函数来为我们执行这些操作：
- en: '[PRE58]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can use this function to examine the performance on the test set and keep
    track of our scores in a `dict`:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个函数来检查测试集的性能，并在`dict`中跟踪我们的分数：
- en: '[PRE59]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'These are pretty good results for a NER task. Our metrics are in the ballpark
    of 85%, and we can see that the model seems to struggle the most on the `ORG`
    entities, probably because these are the least common in the training data and
    many organization names are rare in XLM-R’s vocabulary. How about the other languages?
    To warm up, let’s see how our model fine-tuned on German fares on French:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于NER任务来说是相当不错的结果。我们的指标大约在85%左右，我们可以看到模型似乎在`ORG`实体上遇到了最大的困难，可能是因为这些在训练数据中最不常见，而且XLM-R的词汇表中许多组织名称都很少见。其他语言呢？为了热身，让我们看看我们在德语上进行微调的模型在法语上的表现如何：
- en: '[PRE61]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
- en: '| Tokens | <s> | ▁Jeff | ▁De | an | ▁est | ▁informatic | ien | ▁chez | ▁Google
    | ▁en | ▁Cali | for | nie | </s> |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | <s> | ▁Jeff | ▁De | an | ▁est | ▁informatic | ien | ▁chez | ▁Google
    | ▁en | ▁Cali | for | nie | </s> |'
- en: '| Tags | O | B-PER | I-PER | I-PER | O | O | O | O | B-ORG | O | B-LOC | I-LOC
    | I-LOC | O |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| Tags | O | B-PER | I-PER | I-PER | O | O | O | O | B-ORG | O | B-LOC | I-LOC
    | I-LOC | O |'
- en: 'Not bad! Although the name and organization are the same in both languages,
    the model did manage to correctly label the French translation of “Kalifornien”.
    Next, let’s quantify how well our German model fares on the whole French test
    set by writing a simple function that encodes a dataset and generates the classification
    report on it:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！尽管两种语言的名称和组织都是相同的，但模型成功地正确标记了“Kalifornien”的法语翻译。接下来，让我们通过编写一个简单的函数来对整个法语测试集上的德语模型的表现进行量化，该函数对数据集进行编码并生成分类报告：
- en: '[PRE62]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Although we see a drop of about 15 points in the micro-averaged metrics, remember
    that our model has not seen a single labeled French example! In general, the size
    of the performance drop is related to how “far away” the languages are from each
    other. Although German and French are grouped as Indo-European languages, they
    technically belong to different language families: Germanic and Romance, respectively.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在微观平均指标上看到了约15个点的下降，但请记住，我们的模型没有看到任何一个标记的法语示例！一般来说，性能下降的大小与语言之间的“距离”有关。尽管德语和法语被归为印欧语系，但它们从技术上属于不同的语系：分别是日耳曼语和罗曼语。
- en: 'Next, let’s evaluate the performance on Italian. Since Italian is also a Romance
    language, we expect to get a similar result as we found on French:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们评估意大利语的性能。由于意大利语也是一种罗曼语言，我们期望得到与法语相似的结果：
- en: '[PRE65]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Indeed, our expectations are borne out by the *F*[1]-scores. Finally, let’s
    examine the performance on English, which belongs to the Germanic language family:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们的期望得到了*F*[1]-分数的证实。最后，让我们检查英语的性能，英语属于日耳曼语系：
- en: '[PRE67]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Surprisingly, our model fares *worst* on English, even though we might intuitively
    expect German to be more similar to English than French. Having fine-tuned on
    German and performed zero-shot transfer to French and English, let’s next examine
    when it makes sense to fine-tune directly on the target language.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，尽管我们可能直觉地认为德语与英语更相似，但我们的模型在英语上表现得*最差*。在对德语进行微调并进行零-shot转移到法语和英语之后，接下来让我们考虑何时直接在目标语言上进行微调是有意义的。
- en: When Does Zero-Shot Transfer Make Sense?
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零-shot转移何时有意义？
- en: So far we’ve seen that fine-tuning XLM-R on the German corpus yields an *F*[1]-score
    of around 85%, and without *any additional training* the model is able to achieve
    modest performance on the other languages in our corpus. The question is, how
    good are these results and how do they compare against an XLM-R model fine-tuned
    on a monolingual corpus?
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到，在德语语料库上微调XLM-R可以获得约85%的*F*[1]-分数，在*任何额外的训练*的情况下，该模型能够在我们语料库中的其他语言上取得适度的性能。问题是，这些结果有多好，它们与在单语语料库上微调的XLM-R模型相比如何？
- en: In this section we will explore this question for the French corpus by fine-tuning
    XLM-R on training sets of increasing size. By tracking the performance this way,
    we can determine at which point zero-shot cross-lingual transfer is superior,
    which in practice can be useful for guiding decisions about whether to collect
    more labeled data.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过在不断增加大小的训练集上微调XLM-R来探讨这个问题。通过这种方式跟踪性能，我们可以确定零-shot跨语言转移何时更优越，这在实践中对于指导是否收集更多标记数据的决策可能是有用的。
- en: 'For simplicity, we’ll keep the same hyperparameters from the fine-tuning run
    on the German corpus, except that we’ll tweak the `logging_steps` argument of
    `Training​Ar⁠guments` to account for the changing training set sizes. We can wrap
    this all together in a simple function that takes a `DatasetDict` object corresponding
    to a monolingual corpus, downsamples it by `num_samples`, and fine-tunes XLM-R
    on that sample to return the metrics from the best epoch:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，我们将保持与在德语语料库上进行微调运行相同的超参数，只是我们将调整`Training​Ar⁠guments`的`logging_steps`参数，以考虑训练集大小的变化。我们可以将所有这些封装在一个简单的函数中，该函数接受与单语语料库对应的`DatasetDict`对象，通过`num_samples`对其进行下采样，并在该样本上对XLM-R进行微调，以返回最佳时期的指标：
- en: '[PRE69]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'As we did with fine-tuning on the German corpus, we also need to encode the
    French corpus into input IDs, attention masks, and label IDs:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在德语语料库上进行微调一样，我们还需要将法语语料库编码为输入ID、注意掩码和标签ID：
- en: '[PRE70]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next let’s check that our function works by running it on a small training
    set of 250 examples:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过在一个包含250个示例的小训练集上运行该函数来检查我们的函数是否有效：
- en: '[PRE71]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '|  | num_samples | f1_score |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | num_samples | f1_score |'
- en: '| --- | --- | --- |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 250 | 0.137329 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 250 | 0.137329 |'
- en: 'We can see that with only 250 examples, fine-tuning on French underperforms
    the zero-shot transfer from German by a large margin. Let’s now increase our training
    set sizes to 500, 1,000, 2,000, and 4,000 examples to get an idea of how the performance
    increases:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，仅有250个示例时，在法语上进行微调的性能远远低于从德语进行零-shot转移。现在让我们将训练集大小增加到500、1,000、2,000和4,000个示例，以了解性能的增加情况：
- en: '[PRE72]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We can compare how fine-tuning on French samples compares to zero-shot cross-lingual
    transfer from German by plotting the *F*[1]-scores on the test set as a function
    of increasing training set size:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制测试集上的*F*[1]-分数作为不断增加的训练集大小的函数来比较在法语样本上微调与从德语进行零-shot跨语言转移的性能：
- en: '[PRE73]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '![](Images/nlpt_04in02.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_04in02.png)'
- en: From the plot we can see that zero-shot transfer remains competitive until about
    750 training examples, after which fine-tuning on French reaches a similar level
    of performance to what we obtained when fine-tuning on German. Nevertheless, this
    result is not to be sniffed at! In our experience, getting domain experts to label
    even hundreds of documents can be costly, especially for NER, where the labeling
    process is fine-grained and time-consuming.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，零-shot转移在大约750个训练示例之前仍然具有竞争力，之后在法语上进行微调达到了与我们在德语上进行微调时获得的类似性能水平。尽管如此，这个结果也不容忽视！根据我们的经验，即使是让领域专家标记数百个文档也可能成本高昂，特别是对于NER，其中标记过程是细粒度且耗时的。
- en: 'There is one final technique we can try to evaluate multilingual learning:
    fine-tuning on multiple languages at once! Let’s see how we can do this.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试一种最终的技术来评估多语言学习：一次在多种语言上进行微调！让我们看看我们可以如何做到这一点。
- en: Fine-Tuning on Multiple Languages at Once
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一次在多种语言上进行微调
- en: 'So far we’ve seen that zero-shot cross-lingual transfer from German to French
    or Italian produces a drop of around 15 points in performance. One way to mitigate
    this is by fine-tuning on multiple languages at the same time. To see what type
    of gains we can get, let’s first use the `concatenate_datasets()` function from
    ![nlpt_pin01](Images/nlpt_pin01.png) Datasets to concatenate the German and French
    corpora together:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到从德语到法语或意大利语的零射击跨语言转移会导致性能下降约15个百分点。缓解这一点的一种方法是同时在多种语言上进行微调。为了看看我们可以获得什么类型的收益，让我们首先使用![nlpt_pin01](Images/nlpt_pin01.png)数据集中的`concatenate_datasets()`函数将德语和法语语料库连接在一起：
- en: '[PRE74]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'For training, we’ll again use the same hyperparameters from the previous sections,
    so we can simply update the logging steps, model, and datasets in the trainer:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，我们将再次使用前几节中相同的超参数，因此我们只需更新训练器中的日志步骤、模型和数据集：
- en: '[PRE76]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Let’s have a look at how the model performs on the test set of each language:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型在每种语言的测试集上的表现：
- en: '[PRE77]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: It performs much better on the French split than before, matching the performance
    on the German test set. Interestingly, its performance on the Italian and English
    splits also improves by roughly 10 points! So, even adding training data in another
    language improves the performance of the model on unseen languages.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 它在法语拆分上的表现比以前好得多，与德语测试集的表现相匹配。有趣的是，它在意大利语和英语拆分上的表现也提高了大约10个百分点！因此，即使在另一种语言中添加训练数据，也会提高模型在未知语言上的表现。
- en: 'Let’s round out our analysis by comparing the performance of fine-tuning on
    each language separately against multilingual learning on all the corpora. Since
    we have already fine-tuned on the German corpus, we can fine-tune on the remaining
    languages with our `train_on_subset()` function, with `num_samples` equal to the
    number of examples in the training set:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过比较分别在每种语言上进行微调和在所有语料库上进行多语言学习的性能来完成我们的分析。由于我们已经在德语语料库上进行了微调，我们可以使用我们的`train_on_subset()`函数在剩余的语言上进行微调，其中`num_samples`等于训练集中的示例数：
- en: '[PRE79]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now that we’ve fine-tuned on each language’s corpus, the next step is to concatenate
    all the splits together to create a multilingual corpus of all four languages.
    As with the previous German and French analysis, we can use the `concatenate_splits()`
    function to do this step for us on the list of corpora we generated in the previous
    step:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在每种语言的语料库上进行了微调，下一步是将所有拆分合并在一起，创建一个包含所有四种语言的多语言语料库。与之前的德语和法语分析一样，我们可以使用`concatenate_splits()`函数在我们在上一步生成的语料库列表上执行此步骤：
- en: '[PRE80]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now that we have our multilingual corpus, we run the familiar steps with the
    trainer:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的多语言语料库，我们可以使用训练器运行熟悉的步骤：
- en: '[PRE81]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The final step is to generate the predictions from the trainer on each language’s
    test set. This will give us an insight into how well multilingual learning is
    really working. We’ll collect the *F*[1]-scores in our `f1_scores` dictionary
    and then create a `DataFrame` that summarizes the main results from our multilingual
    experiments:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是从训练器在每种语言的测试集上生成预测。这将让我们了解多语言学习的实际效果如何。我们将在我们的`f1_scores`字典中收集*F*[1]-分数，然后创建一个`DataFrame`，总结我们多语言实验的主要结果：
- en: '[PRE82]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '| Evaluated on | de | fr | it | en |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 评估语言 | de | fr | it | en |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Fine-tune on |  |  |  |  |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 微调于 |  |  |  |  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| de | 0.8677 | 0.7141 | 0.6923 | 0.5890 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| de | 0.8677 | 0.7141 | 0.6923 | 0.5890 |'
- en: '| each | 0.8677 | 0.8505 | 0.8192 | 0.7068 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 每种语言 | 0.8677 | 0.8505 | 0.8192 | 0.7068 |'
- en: '| all | 0.8682 | 0.8647 | 0.8575 | 0.7870 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| all | 0.8682 | 0.8647 | 0.8575 | 0.7870 |'
- en: 'From these results we can draw a few general conclusions:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些结果中，我们可以得出一些一般性的结论：
- en: Multilingual learning can provide significant gains in performance, especially
    if the low-resource languages for cross-lingual transfer belong to similar language
    families. In our experiments we can see that German, French, and Italian achieve
    similar performance in the `all` category, suggesting that these languages are
    more similar to each other than to English.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多语言学习可以显著提高性能，特别是如果跨语言转移的低资源语言属于相似的语言家族。在我们的实验中，我们可以看到德语、法语和意大利语在`all`类别中实现了类似的性能，这表明这些语言彼此之间更相似，而不是与英语相似。
- en: As a general strategy, it is a good idea to focus attention on cross-lingual
    transfer *within* language families, especially when dealing with different scripts
    like Japanese.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一般策略，专注于*语言家族内部*的跨语言转移是一个好主意，特别是在处理日语等不同脚本的情况下。
- en: Interacting with Model Widgets
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与模型小部件交互
- en: In this chapter, we’ve pushed quite a few fine-tuned models to the Hub. Although
    we could use the `pipeline()` function to interact with them on our local machine,
    the Hub provides *widgets* that are great for this kind of workflow. An example
    is shown in [Figure 4-5](#ner-widget) for our `transformersbook/xlm-roberta-base-finetuned-panx-all`
    checkpoint, which as you can see has done a good job at identifying all the entities
    of a German text.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经将许多经过精细调整的模型推送到了Hub。虽然我们可以使用`pipeline()`函数在本地机器上与它们交互，但Hub提供了适合这种工作流程的*小部件*。例如，我们在[图4-5](#ner-widget)中展示了一个示例，用于我们的`transformersbook/xlm-roberta-base-finetuned-panx-all`检查点，可以看到它在识别德语文本的所有实体方面做得很好。
- en: '![A Hub widget](Images/nlpt_0405.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![一个Hub小部件](Images/nlpt_0405.png)'
- en: Figure 4-5\. Example of a widget on the Hugging Face Hub
  id: totrans-385
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5。Hugging Face Hub上小部件的示例
- en: Conclusion
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this chapter we saw how to tackle an NLP task on a multilingual corpus using
    a single transformer pretrained on 100 languages: XLM-R. Although we were able
    to show that cross-lingual transfer from German to French is competitive when
    only a small number of labeled examples are available for fine-tuning, this good
    performance generally does not occur if the target language is significantly different
    from the one the base model was fine-tuned on or was not one of the 100 languages
    used during pretraining. Recent proposals like MAD-X are designed precisely for
    these low-resource scenarios, and since MAD-X is built on top of ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers you can easily adapt the code in this chapter to work with it!^([6](ch04.xhtml#idm46238719474944))'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何使用一个在100种语言上预训练的单一变压器来处理多语言语料库上的NLP任务：XLM-R。尽管我们能够展示出，当只有少量标记示例可用于微调时，从德语到法语的跨语言转移是有竞争力的，但是如果目标语言与基础模型进行微调的语言显著不同，或者不是预训练期间使用的100种语言之一，通常不会出现良好的性能。像MAD-X这样的最新提议正是为这些低资源场景而设计的，而且由于MAD-X是建立在![nlpt_pin01](Images/nlpt_pin01.png)变压器之上，您可以轻松地将本章中的代码适应它！^([6](ch04.xhtml#idm46238719474944))
- en: 'So far we have looked at two tasks: sequence classification and token classification.
    These both fall into the domain of natural language understanding, where text
    is synthesized into predictions. In the next chapter we have our first look at
    text generation, where not only the input but also the output of the model is
    text.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了两个任务：序列分类和标记分类。这两者都属于自然语言理解的范畴，其中文本被合成为预测。在下一章中，我们将首次研究文本生成，其中模型的输入和输出都是文本。
- en: ^([1](ch04.xhtml#idm46238725711616-marker)) A. Conneau et al., [“Unsupervised
    Cross-Lingual Representation Learning at Scale”](https://arxiv.org/abs/1911.02116),
    (2019).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#idm46238725711616-marker)) A. Conneau等人，[“Unsupervised Cross-Lingual
    Representation Learning at Scale”](https://arxiv.org/abs/1911.02116)，（2019）。
- en: '^([2](ch04.xhtml#idm46238725694592-marker)) J. Hu et al., [“XTREME: A Massively
    Multilingual Multi-Task Benchmark for Evaluating Cross-Lingual Generalization”](https://arxiv.org/abs/2003.11080),
    (2020); X. Pan et al., “Cross-Lingual Name Tagging and Linking for 282 Languages,”
    *Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics*
    1 (July 2017): 1946–1958, [*http://dx.doi.org/10.18653/v1/P17-1178*](http://dx.doi.org/10.18653/v1/P17-1178).'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch04.xhtml#idm46238725694592-marker)) J. Hu等人，[“XTREME: A Massively Multilingual
    Multi-Task Benchmark for Evaluating Cross-Lingual Generalization”](https://arxiv.org/abs/2003.11080)，（2020）；X.
    Pan等人，“跨语言姓名标记和链接282种语言”，*计算语言学协会第55届年会论文集* 1（2017年7月）：1946-1958，[*http://dx.doi.org/10.18653/v1/P17-1178*](http://dx.doi.org/10.18653/v1/P17-1178)。'
- en: '^([3](ch04.xhtml#idm46238724894224-marker)) Y. Liu et al., [“RoBERTa: A Robustly
    Optimized BERT Pretraining Approach”](https://arxiv.org/abs/1907.11692), (2019).'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '^([3](ch04.xhtml#idm46238724894224-marker)) Y. Liu等人，[“RoBERTa: A Robustly
    Optimized BERT Pretraining Approach”](https://arxiv.org/abs/1907.11692)，（2019）。'
- en: '^([4](ch04.xhtml#idm46238724892624-marker)) T. Kudo and J. Richardson, [“SentencePiece:
    A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural
    Text Processing”](https://arxiv.org/abs/1808.06226), (2018).'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch04.xhtml#idm46238724892624-marker)) T. Kudo和J. Richardson，[“SentencePiece:
    A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural
    Text Processing”](https://arxiv.org/abs/1808.06226)，（2018）。'
- en: '^([5](ch04.xhtml#idm46238724678896-marker)) J. Devlin et al., [“BERT: Pre-Training
    of Deep Bidirectional Transformers for Language Understanding”](https://arxiv.org/abs/1810.04805),
    (2018).'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '^([5](ch04.xhtml#idm46238724678896-marker)) J. Devlin等人，[“BERT: Pre-Training
    of Deep Bidirectional Transformers for Language Understanding”](https://arxiv.org/abs/1810.04805)，（2018）。'
- en: '^([6](ch04.xhtml#idm46238719474944-marker)) J. Pfeiffer et al., [“MAD-X: An
    Adapter-Based Framework for Multi-Task Cross-Lingual Transfer”](https://arxiv.org/abs/2005.00052),
    (2020).'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '^([6](ch04.xhtml#idm46238719474944-marker)) J. Pfeiffer等人，[“MAD-X: An Adapter-Based
    Framework for Multi-Task Cross-Lingual Transfer”](https://arxiv.org/abs/2005.00052)，（2020）。'
