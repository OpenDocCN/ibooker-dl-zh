- en: Chapter 10\. Building Neural Networks with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch is a powerful open source deep learning library developed by Facebook’s
    AI Research lab (FAIR, now called Meta AI). It is the Python successor of the
    Torch library, originally written in the Lua programming language. With PyTorch,
    you can build all sorts of neural network models and train them at scale using
    GPUs (or other hardware accelerators, as we will see). In many ways it is similar
    to NumPy, except it also supports hardware acceleration and autodiff (see [Chapter 9](ch09.html#ann_chapter)),
    and includes optimizers and ready-to-use neural net components.
  prefs: []
  type: TYPE_NORMAL
- en: 'When PyTorch was released in 2016, Google’s TensorFlow library was by far the
    most popular: it was fast, it scaled well, and it could be deployed across many
    platforms. But its programming model was complex and static, making it difficult
    to use and debug. In contrast, PyTorch was designed from the ground up to provide
    a more flexible, Pythonic approach to building neural networks. In particular,
    as you will see, it uses dynamic computation graphs (also known as define-by-run),
    making it intuitive and easy to debug. PyTorch is also beautifully coded and documented,
    and focuses on its core task: making it easy to build and train high-performance
    neural networks. Last but not least, it leans strongly into the open source culture
    and benefits from an enthusiastic and dedicated community, and a rich ecosystem.
    In September 2022, PyTorch’s governance was even transferred to the PyTorch Foundation,
    a subsidiary of the Linux Foundation. All these qualities resonated well with
    researchers: PyTorch quickly became the most used framework in academia, and once
    a majority of deep learning papers were based on PyTorch, a large part of the
    industry was gradually converted as well.⁠^([1](ch10.html#id2228))'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to train, evaluate, fine-tune, optimize,
    and save neural nets with PyTorch. We will start by getting familiar with the
    core building blocks of PyTorch, namely tensors and autograd, next we will test
    the waters by building and training a simple linear regression model, and then
    we will upgrade this model to a multilayer neural network, first for regression,
    then for classification. Along the way, we will see how to build custom neural
    networks with multiple inputs or outputs. Finally, we will discuss how to automatically
    fine-tune hyperparameters using the Optuna library, and how to optimize and export
    your models. Hop on board, we’re diving into deep learning!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Colab runtimes come with a recent version of PyTorch preinstalled. However,
    if you prefer to install it on your own machine, please see the installation instructions
    at [*https://homl.info/install-p*](https://homl.info/install-p): this involves
    installing Python, many libraries, and a GPU driver (if you have one).'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The core data structure of PyTorch is the *tensor*.⁠^([2](ch10.html#id2229))
    It’s a multidimensional array with a shape and a data type, used for numerical
    computations. Isn’t that exactly like a NumPy array? Well, yes, it is! But a tensor
    also has two extra features: it can live on a GPU (or other hardware accelerators,
    as we will see), and it supports auto-differentiation. Every neural network we
    will build from now on will input and output tensors (much like Scikit-Learn models
    input and output NumPy arrays). So let’s start by looking at how to create and
    manipulate tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s import the PyTorch library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next you can create a PyTorch tensor much like you would create a NumPy array.
    For example, let’s create a 2 × 3 array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Just like a NumPy array, a tensor can contain floats, integers, booleans, or
    complex numbers—just one data type per tensor. If you initialize a tensor with
    values of different types, then the most general one will be selected (i.e., complex
    > float > integer > bool). You can also select the data type explicitly when creating
    the tensor, for example `dtype=torch.float16` for 16-bit floats. Note that tensors
    of strings or objects are not supported.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get a tensor’s shape and data type like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Indexing works just like for NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also run all sorts of computations on tensors, and the API is conveniently
    similar to NumPy’s: for example, there’s `torch.abs()`, `torch.cos()`, `torch.exp()`,
    `torch.max()`, `torch.mean()`, `torch.sqrt()`, and so on. PyTorch tensors also
    have methods for most of these operations, so you can write `X.exp()` instead
    of `torch.exp(X)`. Let’s try a few operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyTorch prefers the argument name `dim` in operations such as `max()`, but it
    also supports `axis` (as in NumPy or Pandas).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also convert a tensor to a NumPy array using the `numpy()` method,
    and create a tensor from a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the default precision for floats is 32 bits in PyTorch, whereas
    it’s 64 bits in NumPy. It’s generally better to use 32 bits in deep learning because
    this takes half the RAM and speeds up computations, and neural nets do not actually
    need the extra precision offered by 64-bit floats. So when calling the `torch.tensor()`
    function to convert a NumPy array to a tensor, it’s best to specify `dtype=torch.float32`.
    Alternatively, you can use `torch.FloatTensor()` which automatically converts
    the array to 32 bits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Both `torch.tensor()` and `torch.FloatTensor()` make a copy of the given NumPy
    array. If you prefer, you can use `torch.​from_numpy()` which creates a tensor
    on the CPU that just uses the NumPy array’s data directly, without copying it.
    But beware: modifying the NumPy array will also modify the tensor, and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also modify a tensor in place using indexing and slicing, as with a
    NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'PyTorch’s API provides many in-place operations, such as `abs_()`, `sqrt_()`,
    and `zero_()`, which modify the input tensor directly: they can sometimes save
    some memory and speed up your models. For example, the `relu_()` method applies
    the ReLU activation function in place by replacing all negative values with 0s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyTorch’s in-place operations are easy to spot at a glance because their name
    always ends with an underscore. With very few exceptions (e.g., `zero_()`), removing
    the underscore gives you the regular operation (e.g., `abs_()` is in place, `abs()`
    is not).
  prefs: []
  type: TYPE_NORMAL
- en: We will cover many more operations as we go, but now let’s look at how to use
    hardware acceleration to make computations much faster.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Acceleration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PyTorch tensors can be copied easily to the GPU, assuming your machine has
    a compatible GPU, and you have the required libraries installed. On Colab, all
    you need to do is ensure that you are using a GPU runtime: for this, go to the
    Runtime menu and select “Change runtime type”, then make sure a GPU is selected
    (e.g., an Nvidia T4 GPU). The GPU runtime will automatically have the appropriate
    PyTorch library installed—compiled with GPU support—as well as the appropriate
    GPU drivers and related libraries (e.g., Nvidia’s CUDA and cuDNN libraries).⁠^([3](ch10.html#id2249))
    If you prefer to run the code on your own machine, you will need to ensure that
    you have all the drivers and libraries required. Please follow the instructions
    at [*https://homl.info/install-p*](https://homl.info/install-p).'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch has excellent support for Nvidia GPUs, as well as several other hardware
    accelerators:'
  prefs: []
  type: TYPE_NORMAL
- en: Apple’s *Metal Performance Shaders* (MPS) to accelerate computations on Apple
    silicon such as the M1, M2, and later chips, as well as some Intel Macs with a
    compatible GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AMD Instinct accelerators and AMD Radeon GPUs, through the ROCm software stack,
    or via DirectML on Windows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intel GPUs and CPUs on Linux and Windows via Intel’s oneAPI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google TPUs via the `torch_xla` library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s check whether PyTorch can access an Nvidia GPU or Apple’s MPS, otherwise
    let’s fall back to the CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning generally requires a *lot* of compute power, especially once we
    start diving into computer vision and natural language processing, in the following
    chapters. You will need a reasonably powerful machine, but most importantly you
    will need a hardware accelerator (or several). If you don’t have one, you can
    try using Colab or Kaggle; they offer runtimes with free GPUs. Or consider using
    other cloud services. Otherwise, prepare to be very, very patient.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a Colab GPU runtime, `device` will be equal to `"cuda"`. Now let’s create
    a tensor on that GPU. To do that, one option is to create the tensor on the CPU,
    then copy it to the GPU using the `to()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `cpu()` and `cuda()` methods are short for `to("cpu")` and `to("cuda")`,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always tell which device a tensor lives on by looking at its `device`
    attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can create the tensor directly on the GPU using the `device`
    argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you have multiple Nvidia GPUs, you can refer to the desired GPU by appending
    the GPU index: `"cuda:0"` (or just `"cuda"`) for GPU #0, `"cuda:1"` for GPU #1,
    and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the tensor is on the GPU, we can run operations on it normally, and they
    will all take place on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that the result `R` also lives on the GPU. This means we can perform multiple
    operations on the GPU without having to transfer data back and forth between the
    CPU and the GPU. This is crucial in deep learning because data transfer between
    devices can often become a performance bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: 'How much does a GPU accelerate the computations? Well it depends on the GPU,
    of course: the more expensive ones are dozens of times faster than the cheap ones.
    But speed alone is not the only important factor: the data throughput is also
    crucial, as we just saw. If your model is compute heavy (e.g., a very deep neural
    net), the GPU’s speed and amount of RAM will typically matter most, but if it
    is a shallower model, then pumping the training data into the GPU might become
    the bottleneck. Let’s run a little test to compare the speed of a matrix multiplication
    running on the CPU versus the GPU:⁠^([4](ch10.html#id2258))'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Wow! The GPU gave us a 29× speed boost! And that’s just using the free Nvidia
    T4 GPU on Colab; imagine the speedup we could get using a more powerful GPU. Now
    try playing around with the matrix size: you will notice that the speedup is much
    less impressive on smaller matrices (e.g., it’s just 2× for 100 × 100 matrices).
    That’s because GPUs work by breaking large operations into smaller operations
    and running them in parallel across thousands of cores. If the task is small,
    it cannot be broken up into that many pieces, and the performance gain is therefore
    smaller. In fact, when running many tiny tasks, it can sometimes be faster to
    just run the operations on the CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: All right, now that we’ve seen what tensors are and how to use them on the CPU
    or the GPU, let’s look at PyTorch’s auto-differentiation feature.
  prefs: []
  type: TYPE_NORMAL
- en: Autograd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PyTorch comes with an efficient implementation of reverse-mode auto-differentiation
    (introduced in [Chapter 9](ch09.html#ann_chapter) and detailed in [Appendix A](app01.html#autodiff_appendix)),
    called *autograd*, which stands for automatic gradients. It is quite easy to use.
    For example, consider a simple function, f(*x*) = *x*². Differential calculus
    tells us that the derivative of this function is *f’*(*x*) = 2*x*. If we evaluate
    f(5) and f''(5), we get 25 and 10, respectively. Let’s see if PyTorch agrees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, we got the correct results: `f` is 25, and `x.grad` is 10! Note that
    the `backward()` function automatically computed the gradient f''(*x*) at the
    same point *x* = 5.0\. Let’s go through this code line by line:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we created a tensor `x`, equal to 5.0, and we told PyTorch that it’s
    a variable (not a constant) by specifying `requires_grad=True`. Knowing this,
    PyTorch will automatically keep track of all operations involving `x`: this is
    needed because PyTorch must capture the computation graph in order to run backprop
    on it and obtain the derivative of `f` with regard to `x`. In this computation
    graph, the tensor `x` is a *leaf node*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then we compute `f = x ** 2`. The result is a tensor equal to 25.0, the square
    of 5.0\. But wait, there’s more to it: `f` also carries a `grad_fn` attribute
    which represents the operation that created this tensor (`**`, power, hence the
    name `PowBackward0`), and which tells PyTorch how to backpropagate the gradients
    through this particular operation. This `grad_fn` attribute is how PyTorch keeps
    track of the computation graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we call `f.backward()`: this backpropagates the gradients through the
    computation graph, starting with `f`, and all the way back to the leaf nodes (just
    `x` in this case).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lastly, we can just read the `x` tensor’s `grad` attribute, which was computed
    during backprop: this gives us the derivative of `f` with regard to `x`. Ta-da!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch creates a new computation graph on the fly during each forward pass,
    as the operations are executed. This allows PyTorch to support very dynamic models
    containing loops and conditionals.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The way PyTorch accumulates gradients in each variable’s `grad` attribute can
    be surprising at first, especially coming from TensorFlow or JAX. In these frameworks,
    computing the gradients of `f` with regard to `x` just returns the gradients,
    without affecting `x`. In PyTorch, if you call `backward()` on a tensor, it will
    accumulate the gradients in every variable that was used to compute it. So if
    you call `backward()` on two tensors `t1` and `t2` that both used the same variable
    `v`, then `v.grad` will be the sum of their gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the gradients, you generally want to perform a gradient descent
    step by subtracting a fraction of the gradients from the model variables (at least
    when training a neural network). In our simple example, running gradient descent
    will gradually push `x` toward 0, since that’s the value that minimizes f(*x*)
    = *x*². To do a gradient descent step, you must temporarily disable gradient tracking
    since you don’t want to track the gradient descent step itself in the computation
    graph (in fact, PyTorch would raise an exception if you tried to run an in-place
    operation on a tracked variable). This can be done by placing the gradient descent
    step inside a `torch.no_grad()` context, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The variable `x` gets decremented by 0.1 * 10.0 = 1.0, down from 5.0 to 4.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to avoid gradient computation is to use the variable’s `detach()`
    method: this creates a new tensor detached from the computation graph, with `requires_grad=False`,
    but still pointing to the same data in memory. You can then update this detached
    tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Since `x_detached` and `x` share the same memory, modifying `x_detached` also
    modifies `x`.
  prefs: []
  type: TYPE_NORMAL
- en: The `detach()` method can be handy when you need to run some computation on
    a tensor without affecting the gradients (e.g., for evaluation or logging), or
    when you need fine-grained control over which operations should contribute to
    gradient computation. Using `no_grad()` is generally preferred when performing
    inference or doing a gradient descent step, as it provides a convenient context-wide
    method to disable gradient tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, before you repeat the whole process (forward pass + backward pass +
    gradient descent step), it’s essential to zero out the gradients of every model
    parameter (you don’t need a `no_grad()` context for this since the gradient tensor
    has `requires_grad=False`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you forget to zero out the gradients at each training iteration, the `backward()`
    method will just accumulate them, causing incorrect gradient descent updates.
    Since there won’t be any explicit error, just low performance (and perhaps infinite
    or NaN values), this issue may be hard to debug.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting everything together, the whole training loop looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to use in-place operations to save memory and speed up your models
    a bit by avoiding unnecessary copy operations, you have to be careful: in-place
    operations don’t always play nicely with autograd. Firstly, as we saw earlier,
    you cannot apply an in-place operation to a leaf node (i.e., a tensor with `requires_grad=True`),
    as PyTorch wouldn’t know where to store the computation graph. For example `x.cos_()`
    or `x += 1` would cause a `RuntimeError`. Secondly, consider the following code,
    which computes z(*t*) = exp(*t*) + 1 at *t* = 2 and then tries to compute the
    gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Oh no! Although `z` is computed correctly, the last line causes a `RuntimeError`,
    complaining that “one of the variables needed for gradient computation has been
    modified by an in-place operation”. Indeed, the intermediate result `z = t.exp()`
    was lost when we ran the in-place operation `z += 1`, so when the backward pass
    reached the exponential operation, the gradients could not be computed. A simple
    fix is to replace `z += 1` with `z = z + 1`. It looks similar, but it’s no longer
    an in-place operation: a new tensor is created and assigned to the same variable,
    but the original tensor is unchanged and recorded in the computation graph of
    the final tensor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Surprisingly, if you replace `exp()` with `cos()` in the previous code example,
    the gradients will be computed correctly: no error! Why is that? Well, the outcome
    depends on the way each operation is implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: Some operations—such as `exp()`, `relu()`, `rsqrt()`, `sigmoid()`, `sqrt()`,
    `tan()`, and `tanh()`—save their outputs in the computation graph during the forward
    pass, then use these outputs to compute the gradients during the backward pass.⁠^([5](ch10.html#id2274))
    This means that you must not modify such an operation’s output in place, or you
    will get an error during the backward pass (as we just saw).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other operations—such as `abs()`, `cos()`, `log()`, `sin()`, `square()`, and
    `var()`—save their inputs instead of their output.⁠^([6](ch10.html#id2275)) Such
    an operation doesn’t care if you modify its output in place, but you must not
    modify its inputs in place before the backward pass (e.g., to compute something
    else based on the same inputs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some operations—such as `max()`, `min()`, `norm()`, `prod()`, `sgn()`, and `std()`—save
    both the inputs and the outputs, so you must not modify either of them in place
    before the backward pass.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, a few operations—such as `ceil()`, `floor()`, `mean()`, `round()`, and
    `sum()`—save neither their inputs nor their outputs.⁠^([7](ch10.html#id2276))
    You can safely modify them in place.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Implement your models first without any in-place operations, then if you need
    to save some memory or speed up your model a bit, you can try converting some
    of the most costly operations to their in-place counterparts. Just make sure that
    your model still outputs the same result for a given input, and also make sure
    you don’t modify in place a tensor needed for backprop (you will get a `RuntimeError`
    in this case).
  prefs: []
  type: TYPE_NORMAL
- en: 'OK, let’s step back a bit. We’ve discussed all the fundamentals of PyTorch:
    how to create tensors and use them to perform all sorts of computations, how to
    accelerate the computations with a GPU, and how to use autograd to compute gradients
    for gradient descent. Great! Now let’s apply what we’ve learned so far by building
    and training a simple linear regression model with PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start by implementing linear regression using tensors and autograd directly,
    then we will simplify the code using PyTorch’s high-level API, and also add GPU
    support.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression Using Tensors and Autograd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s tackle the same California housing dataset as in [Chapter 9](ch09.html#ann_chapter).
    I will assume you have already downloaded it using `sklearn.datasets.fetch_california_housing()`,
    and you have split it into a training set (`X_train` and `y_train`), a validation
    set (`X_valid` and `y_valid`), and a test set (`X_test` and `y_test`), using `sklearn.model_selection.train_test_split()`.
    Next, let’s convert it to tensors and normalize it. We could use a `StandardScaler`
    for this, like we did in [Chapter 9](ch09.html#ann_chapter), but let’s just use
    tensor operations instead, to get a bit of practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let’s also convert the targets to tensors. Since our predictions will be column
    vectors (i.e., matrices with a single column), we need to ensure that our targets
    are also column vectors.⁠^([8](ch10.html#id2281)) Unfortunately, the NumPy arrays
    representing the targets are one-dimensional, so we need to reshape the tensors
    to column vectors by adding a second dimension of size 1:⁠^([9](ch10.html#id2282))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the data is ready, let’s create the parameters of our linear regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We now have a weights parameter `w` (a column vector with one weight per input
    dimension, in this case 8), and a bias parameter `b` (a single scalar). The weights
    are initialized randomly, while the bias is initialized to zero. We could have
    initialized the weights to zero as well in this case, but when we get to neural
    networks it will be important to initialize the weights randomly to break the
    symmetry between neurons (as explained in [Chapter 9](ch09.html#ann_chapter)),
    so we might as well get into the habit now.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We called `torch.manual_seed()` to ensure that the results are reproducible.
    However, PyTorch does not guarantee perfectly reproducible results across different
    releases, platforms, or devices, so if you do not run the code in this chapter
    with PyTorch 2.8.0 on a Colab runtime with an Nvidia T4 GPU, you may get different
    results. Moreover, since a GPU splits each operation into multiple chunks and
    runs them in parallel, the order in which these chunks finish may vary across
    runs, and this may slightly affect the result due to floating-point precision
    errors. These minor differences may compound during training, and lead to very
    different models. To avoid this, you can tell PyTorch to use only deterministic
    algorithms by calling `torch.use_deterministic_algorithms(True)` and setting `torch.backends.cudnn.benchmark
    = False`. However, deterministic algorithms are often slower than stochastic ones,
    and some operations don’t have a deterministic version at all, so you will get
    an error if your code tries to use one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s train our model, very much like we did in [Chapter 4](ch04.html#linear_models_chapter),
    except we will use autodiff to compute the gradients rather than using a closed-form
    equation. For now we will use batch gradient descent (BGD), using the full training
    set at each training step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s walk through this code:'
  prefs: []
  type: TYPE_NORMAL
- en: First we define the `learning_rate` hyperparameter. You can experiment with
    different values to find a value that converges fast and gives a precise result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we run 20 epochs. We could implement early stopping to find the right
    moment to stop and avoid overfitting, like we did in [Chapter 4](ch04.html#linear_models_chapter),
    but we will keep things simple for now.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we run the forward pass: we compute the predictions `y_pred`, and the
    mean squared error `loss`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we run `loss.backward()` to compute the gradients of the loss with regard
    to every model parameter. This is autograd in action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we use the gradients `b.grad` and `w.grad` to perform a gradient descent
    step. Notice that we’re running this code inside a `with torch.no_grad()` context,
    as discussed earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we’ve done the gradient descent step, we reset the gradients to zero (very
    important!).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we print the epoch number and the current loss at each epoch. The `item()`
    method extracts the value of a scalar tensor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And that’s it; if you run this code, you should see the training loss going
    down like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Congratulations, you just trained your first model using PyTorch! You can now
    use the model to make predictions for some new data `X_new` (which must be represented
    as a PyTorch tensor). For example, let’s make predictions for the first three
    instances in the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It’s best to use a `with torch.no_grad()` context during inference: PyTorch
    will consume less RAM and run faster since it won’t have to keep track of the
    computation graph.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing linear regression using PyTorch’s low-level API wasn’t too hard,
    but using this approach for more complex models would get really messy and difficult.
    So PyTorch offers a higher-level API to simplify all this. Let’s rewrite our model
    using this higher-level API.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression Using PyTorch’s High-Level API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PyTorch provides an implementation of linear regression in the `torch.nn.Linear`
    class, so let’s use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `nn.Linear` class (short for `torch.nn.Linear`) is one of many *modules*
    provided by PyTorch. Each module is a subclass of the `nn.Module` class. To build
    a simple linear regression model, a single `nn.Linear` module is all you need.
    However, for most neural networks you will need to assemble many modules, as we
    will see later in this chapter, so you can think of modules as math LEGO^® bricks.
    Many modules contain model parameters. For example, the `nn.Linear` module contains
    a `bias` vector (with one bias term per neuron), and a `weight` matrix (with one
    row per neuron and one column per input dimension, which is the transpose of the
    weight matrix we used earlier and in [Equation 9-2](ch09.html#neural_network_layer_equation)).
    Since our model has a single neuron (because `out_features=1`), the `bias` vector
    contains a single bias term, and the `weight` matrix contains a single row. These
    parameters are accessible directly as attributes of the `nn.Linear` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that both parameters were automatically initialized randomly (which
    is why we used `manual_seed()` to get reproducible results). These parameters
    are instances of the `torch.nn.Parameter` class, which is a subclass of the `tensor.Tensor`
    class: this means that you can use them exactly like normal tensors. A module’s
    `parameters()` method returns an iterator over all of the module’s attributes
    of type `Parameter`, as well as all the parameters of all its submodules, recursively
    (if it has any). It does *not* return regular tensors, even those with `requires_grad=True`.
    That’s the main difference between a regular tensor and a `Parameter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: There’s also a `named_parameters()` method that returns an iterator over pairs
    of parameter names and values.
  prefs: []
  type: TYPE_NORMAL
- en: 'A module can be called just like a regular function. For example, let’s make
    some predictions for the first two instances in the training set (since the model
    is not trained yet, its parameters are random and the predictions are terrible):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: When we use a module as a function, PyTorch internally calls the module’s `forward()`
    method. In the case of the `nn.Linear` module, the `forward()` method computes
    `X @ self.weight.T + self.bias` (where `X` is the input). That’s just what we
    need for linear regression!
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the result contains the `grad_fn` attribute, showing that autograd
    did its job and tracked the computation graph while the model was making its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you pass a custom function to a module’s `register_forward_hook()` method,
    it will be called automatically every time the module itself is called. This is
    particularly handy for logging or debugging. To remove a hook, just call the `remove()`
    method on the object returned by `register_forward_hook()`. Note that hooks only
    work if you call the model like a function, not if you call its `forward()` method
    directly (which is why you should never do that). You can also register functions
    to run during the backward pass using `register_backward_hook()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our model, we need to create an optimizer to update the model
    parameters, and we must also choose a loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: PyTorch provides a few different optimizers (we will discuss them in the next
    chapter). Here we’re using the simple stochastic gradient descent (SGD) optimizer,
    which can be used for SGD, mini-batch GD, or batch gradient descent. To initialize
    it, we must give it the model parameters and the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the loss function, we create an instance of the `nn.MSELoss` class: this
    is also a module, so we can use it like a function, giving it the predictions
    and the targets, and it will compute the MSE. The `nn` module contains many other
    loss functions and other neural net tools, as we will see. Next, let’s write a
    small function to train our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare this training loop with our earlier training loop: it’s very similar,
    but we’re now using higher-level constructs rather than working directly with
    tensors and autograd. Here are a few things to note:'
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch, the loss function object is commonly referred to as the *criterion*,
    to distinguish it from the loss value itself (which is computed at each training
    iteration using the criterion). In this example, it’s the `MSELoss` instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `optimizer.step()` line corresponds to the two lines that updated `b` and
    `w` in our earlier code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And of course the `optimizer.zero_grad()` line corresponds to the two lines
    that zeroed out `b.grad` and `w.grad`. Notice that we don’t need to use `with
    torch.no_grad()` here since this is done automatically by the optimizer, inside
    the `step()` and `zero_grad()` functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Most people prefer to call `zero_grad()` *before* calling `loss.backward()`,
    rather than after: this might be a bit safer in case the gradients are nonzero
    when calling the function, but in general it makes no difference since gradients
    are automatically initialized to `None`.'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s call this function to train our model!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'All good; the model is trained, and you can now use it to make predictions
    by simply calling it like a function (preferably inside a `no_grad()` context,
    as we saw earlier):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'These predictions are similar to the ones our previous model made, but not
    exactly the same. That’s because the `nn.Linear` module initializes the parameters
    slightly differently: it uses a uniform random distribution from $minus StartFraction
    StartRoot 2 EndRoot Over 4 EndFraction$ to $plus StartFraction StartRoot 2 EndRoot
    Over 4 EndFraction$ for both the weights and the bias term (we will discuss initialization
    methods in [Chapter 11](ch11.html#deep_chapter)).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are familiar with PyTorch’s high-level API, you are ready to go
    beyond linear regression and build a multilayer perceptron (introduced in [Chapter 9](ch09.html#ann_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Regression MLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch provides a helpful `nn.Sequential` module that chains multiple modules:
    when you call this module with some inputs, it feeds these inputs to the first
    module, then feeds the output of the first module to the second module, and so
    on. Most neural networks contain stacks of modules, and in fact many neural networks
    are just one big stack of modules: this makes the `nn.Sequential` module one of
    the most useful modules in PyTorch. The MLP we want to build is just that: a simple
    stack of modules—two hidden layers and one output layer. So let’s build it using
    the `nn.Sequential` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go through each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first layer must have the right number of inputs for our data: `n_features`
    (equal to 8 in our case). However, it can have any number of outputs: let’s pick
    50 (that’s a hyperparameter we can tune).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next we have an `nn.ReLU` module, which implements the ReLU activation function
    for the first hidden layer. This module does not contain any model parameters,
    and it acts itemwise so the shape of its output is equal to the shape of its input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second hidden layer must have the same number of inputs as the output of
    the previous layer: in this case, 50\. However, it can have any number of outputs.
    It’s common to use the same number of output dimensions in all hidden layers,
    but in this example I used 40 to make it clear that the output of one layer must
    match the input of the next layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then again, an `nn.ReLU` module to implement the second hidden layer’s activation
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, the output layer must have 40 inputs, but this time its number of
    outputs is not free: it must match the targets’ dimensionality. Since our targets
    have a single dimension, we must have just one output dimension in the output
    layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let’s train the model just like we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: That’s it, you can tell your friends you trained your first neural network with
    PyTorch! However, we are still using batch gradient descent, computing the gradients
    over the entire training set at each iteration. This works with small datasets,
    but if we want to be able to scale up to large datasets and large models, we need
    to switch to mini-batch gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Mini-Batch Gradient Descent Using DataLoaders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To help implement mini-batch GD, PyTorch provides a class named `DataLoader`
    in the `torch.utils.data` module. It can efficiently load batches of data of the
    desired size, and shuffle the data at each epoch if we want it to. The `DataLoader`
    expects the dataset to be represented as an object with at least two methods:
    `__len__(self)` to get the number of samples in the dataset, and `__getitem__(self,
    index)` to load the sample at the given index (including the target).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, the training set is available in the `X_train` and `y_train` tensors,
    so we first need to wrap these tensors in a dataset object with the required API.
    To help with this, PyTorch provides a `TensorDataset` class. So let’s build a
    `TensorDataset` to wrap our training set, and a `DataLoader` to pull batches from
    this dataset. During training, we want the dataset to be shuffled, so we specify
    `shuffle=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a larger model and we have the tools to train it one batch
    at a time, it’s a good time to start using hardware acceleration. It’s really
    quite simple: we just need to move the model to the GPU, which will move all of
    its parameters to the GPU RAM, and then at the start of each iteration during
    training we must copy each batch to the GPU. To move the model, we can just use
    its `to()` method, just like we did with tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We can also create the loss function and optimizer, as earlier (but using a
    lower learning rate, such as 0.02).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some optimizers have some internal state, as we will see in [Chapter 11](ch11.html#deep_chapter).
    The optimizer will usually allocate its state on the same device as the model
    parameters, so it’s important to create the optimizer *after* you have moved the
    model to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s create a `train()` function to implement mini-batch GD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'At every epoch, the function iterates through the whole training set, one batch
    at a time, and processes each batch just like earlier. But what about the very
    first line: `model.train()`? Well, this switches the model and all of its submodules
    to *training mode*. For now, this makes no difference at all, but it will be important
    in [Chapter 11](ch11.html#deep_chapter) when we start using layers that behave
    differently during training and evaluation (e.g., `nn.Dropout` or `nn.BatchNorm1d`).
    Whenever you want to use the model outside of training (e.g., for evaluation,
    or to make predictions on new instances), you must first switch the model to *evaluation
    mode* by running `model.eval()`. Note that `model.training` holds a boolean that
    indicates the current mode.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyTorch itself does not provide a training loop implementation; you have to
    build it yourself. As we just saw, it’s not that long, and many people enjoy the
    freedom, clarity, and control this provides. However, if you would prefer to use
    a well-tested, off-the-shelf training loop with all the bells and whistles you
    need (such as multi-GPU support), then you can use a library such as PyTorch Lightning,
    FastAI, Catalyst, or Keras. These libraries are built on top of PyTorch and include
    a training loop and many other features (Keras supports PyTorch since version
    3, and also supports TensorFlow and JAX). Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s call this `train()` function to train our model on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'It worked great: we actually reached a much lower loss in the same number of
    epochs! However, you probably noticed that each epoch was much slower. There are
    two easy tweaks you can make to considerably speed up training:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using a CUDA device, you should generally set `pin_memory=True`
    when creating the data loader: this will allocate the data in *page-locked memory*
    which guarantees a fixed physical memory location in the CPU RAM, and therefore
    allows direct memory access (DMA) transfers to the GPU, eliminating an extra copy
    operation that would otherwise be needed. While this could use more CPU RAM since
    the memory cannot be swapped out to disk, it typically results in significantly
    faster data transfers and thus faster training. When transferring a tensor to
    the GPU using its `to()` method, you may also set `non_blocking=True` to avoid
    blocking the CPU during the data transfer (this only works if `pin_memory=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current training loop waits until a batch has been fully processed before
    it loads the next batch. You can often speed up training by pre-fetching the next
    batches on the CPU while the GPU is still working on the current batch. For this,
    set the data loader’s `num_workers` argument to the number of processes you want
    to use for data loading and preprocessing. The optimal number depends on your
    platform, hardware, and workload, so you should experiment with different values.
    You can also tweak the number of batches that each worker pre-fetches by setting
    the data loader’s `prefetch_factor` argument. Note that the overhead of spawning
    and synchronizing workers can often slow down training rather than speed it up
    (especially on Windows). In this case, you can try setting `persistent_workers=True`
    to reuse the same workers across epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OK, time to step back a bit: you know the PyTorch fundamentals (tensors and
    autograd), you can build neural nets using PyTorch’s high-level API, and train
    them using mini-batch gradient descent, with the help of an optimizer, a criterion,
    and a data loader. The next step is to learn how to evaluate your model.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s write a function to evaluate the model. It takes the model and a `DataLoader`
    for the dataset that we want to evaluate the model on, as well as a function to
    compute the metric for a given batch, and lastly a function to aggregate the batch
    metrics (by default, it just computes the mean):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s build a `TensorDataset` and a `DataLoader` for our validation set,
    and pass it to our `evaluate()` function to compute the validation MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'It works fine. But now suppose we want to use the RMSE instead of the MSE (as
    we saw in [Chapter 2](ch02.html#project_chapter), it can be easier to interpret).
    PyTorch does not have a built-in function for that, but it’s easy enough to write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'But wait a second! The RMSE should be equal to the square root of the MSE;
    however, when we compute the square root of the MSE that we found earlier, we
    get a different result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The reason is that instead of calculating the RMSE over the whole validation
    set, we computed it over each batch and then computed the mean of all these batch
    RMSEs. That’s not mathematically equivalent to computing the RMSE over the whole
    validation set. To solve this, we can use the MSE as our `metric_fn`, and use
    the `aggregate_fn` to compute the square root of the mean MSE:⁠^([10](ch10.html#id2337))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: That’s much better!
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than implement metrics yourself, you may prefer to use the TorchMetrics
    library (made by the same team as PyTorch Lightning), which provides many well-tested
    *streaming metrics*. A streaming metric is an object that keeps track of a given
    metric, and can be updated one batch at a time. The TorchMetrics library is not
    preinstalled on Colab, so we have to run `%pip install torchmetrics`, then we
    can implement the `evaluate_tm()` function, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can create an RMSE streaming metric, move it to the GPU, and use it
    to evaluate the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Sure enough, we get the correct result! Now try updating the `train()` function
    to evaluate your model’s performance during training, both on the training set
    (during each epoch) and on the validation set (at the end of each epoch). As always,
    if the performance on the training set is much better than on the validation set,
    your model is probably overfitting the training set, or there is a bug, such as
    a data mismatch between the training set and the validation set. This is easier
    to detect if you plot and analyze the learning curves, much like we did in [Chapter 4](ch04.html#linear_models_chapter).
    For this you can use Matplotlib, or a visualization tool such as TensorBoard (see
    the notebook for an example).
  prefs: []
  type: TYPE_NORMAL
- en: Now you know how to build, train, and evaluate a regression MLP using PyTorch,
    and how to use the trained model to make predictions. Great! But so far we have
    only looked at simple sequential models, composed of a sequence of linear layers
    and ReLU activation functions. How would you build a more complex, nonsequential
    model? For this, we will need to build custom modules.
  prefs: []
  type: TYPE_NORMAL
- en: Building Nonsequential Models Using Custom Modules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One example of a nonsequential neural network is a *Wide & Deep* neural network.
    This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng
    et al.⁠^([11](ch10.html#id2340)) It connects all or part of the inputs directly
    to the output layer, as shown in [Figure 10-1](#wide_deep_diagram). This architecture
    makes it possible for the neural network to learn both deep patterns (using the
    deep path) and simple rules (through the short path). The short path can also
    be used to provide manually engineered features to the neural network. In contrast,
    a regular MLP forces all the data to flow through the full stack of layers; thus,
    simple patterns in the data may end up being distorted by this sequence of transformations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating a Wide & Deep neural network architecture, showing both
    wide and deep paths connecting inputs directly and through hidden layers, converging
    at a concat layer before the output layer.](assets/hmls_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Wide & Deep neural network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s build such a neural network to tackle the California housing dataset.
    Because this wide and deep architecture is nonsequential, we have to create a
    custom module. It’s easier than it sounds: just create a class derived from `torch.nn.Module`,
    then create all the layers you need in the constructor (after calling the base
    class’s `__init__()` method), and define how these layers should be used by the
    module in the `forward()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we can use any kind of module inside our custom module: in this
    example, we use an `nn.Sequential` module to build the “deep” part of our model
    (it’s actually not that deep; this is just a toy example). It’s the same MLP as
    earlier, except we separated the output layer because we need to feed it the concatenation
    of the model’s inputs and the deep part’s outputs. For this same reason, the output
    layer now has 40 + `n_features` inputs instead of just 40.'
  prefs: []
  type: TYPE_NORMAL
- en: In the `forward()` method, we just feed the input `X` to the deep stack, concatenate
    the input and the deep stack’s output, and feed the result to the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Modules have a `children()` method that returns an iterator over the module’s
    submodules (nonrecursively). There’s also a `named_children()` method. If your
    model has a variable number of submodules, you should store them in an `nn.ModuleList`
    or an `nn.ModuleDict`, which are returned by the `children()` and `named_children()`
    methods (as opposed to regular Python lists and dicts). Similarly, if your model
    has a variable number of parameters, you should store them in an `nn.ParameterList`
    or an `nn.ParameterDict` to ensure they are returned by the `parameters()` and
    `named_parameters()` methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can create an instance of our custom module, move it to the GPU, train
    it, evaluate it, and use it exactly like our previous models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'But what if you want to send a subset of the features through the wide path
    and a different subset (possibly overlapping) through the deep path, as illustrated
    in [Figure 10-2](#multiple_inputs_diagram)? In this case, one approach is to split
    the inputs inside the `forward()` method, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This works fine; however, in many cases it’s preferable to just let the model
    take two separate tensors as input. Let’s see why and how.
  prefs: []
  type: TYPE_NORMAL
- en: Building Models with Multiple Inputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some models require multiple inputs that cannot easily be combined into a single
    tensor. For example, the inputs may have a different number of dimensions (e.g.,
    when you want to feed both images and text to the neural network). To make our
    Wide & Deep model take two separate inputs, as shown in [Figure 10-2](#multiple_inputs_diagram),
    we must start by changing the model’s `forward()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![Diagram illustrating the flow of a neural network with wide and deep inputs,
    concatenated before feeding into the output layer.](assets/hmls_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Handling multiple inputs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, we need to create datasets that return the wide and deep inputs separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the data loaders now return three tensors instead of two at each iteration,
    we need to update the main loop in the evaluation and training functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, since the order of the inputs matches the order of the `forward()`
    method’s arguments, we can use Python’s `*` operator to unpack all the inputs
    returned by the `data_loader` and pass them to the model. The advantage of this
    implementation is that it will work with models that take any number of inputs,
    not just two, as long as the order is correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'When your model has many inputs, it’s easy to make a mistake and mix up the
    order of the inputs, which can lead to hard-to-debug issues. To avoid this, it
    can be a good idea to name each input. For this, you can define a custom dataset
    that returns a dictionary from input names to input values, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Then create the datasets and data loaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, we also need to update the main loop in the evaluation and training
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, since all the input names match the `forward()` method’s argument
    names, we can use Python’s `**` operator to unpack all the tensors in the `inputs`
    dictionary and pass them as named arguments to the model: `y_pred = model(**inputs)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to build sequential and nonsequential models with one
    or more inputs, let’s look at models with multiple outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Building Models with Multiple Outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many use cases where you may need a neural net with multiple outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The task may demand it. For instance, you may want to locate and classify the
    main object in a picture. This is both a regression task and a classification
    task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, you may have multiple independent tasks based on the same data. Sure,
    you could train one neural network per task, but in many cases you will get better
    results on all tasks by training a single neural network with one output per task.
    This is because the neural network can learn features in the data that are useful
    across tasks. For example, you could perform *multitask classification* on pictures
    of faces, using one output to classify the person’s facial expression (smiling,
    surprised, etc.) and another output to identify whether they are wearing glasses
    or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another use case is regularization (i.e., a training constraint whose objective
    is to reduce overfitting and thus improve the model’s ability to generalize).
    For example, you may want to add an auxiliary output in a neural network architecture
    (see [Figure 10-3](#multiple_outputs_diagram)) to ensure that the underlying part
    of the network learns something useful on its own, without relying on the rest
    of the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Diagram illustrating a neural network with an auxiliary output layer added
    for regularization, showcasing the flow from input layers through hidden layers
    to main and auxiliary outputs.](assets/hmls_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Handling multiple outputs, in this example to add an auxiliary
    output for regularization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s add an auxiliary output to our Wide & Deep model to ensure the deep part
    can make good predictions on its own. Since the deep stack’s output dimension
    is 40, and the targets have a single dimension, we must add an `nn.Linear` layer
    for the auxiliary output to go from 40 dimensions down to 1\. We also need to
    make the `forward()` method compute the auxiliary output, and return both the
    main output and the auxiliary output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to update the main loop in the training function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the model now returns both the main predictions `y_pred` and the
    auxiliary predictions `y_pred_aux`. In this example, we can use the same targets
    and the same loss function to compute the main output’s loss and the auxiliary
    output’s loss. In other cases, you may have different targets and loss functions
    for each output, in which case you would need to create a custom dataset to return
    all the necessary targets. Once we have a loss for each output, we must combine
    them into a single loss that will be minimized by gradient descent. In general,
    this final loss is just a weighted sum of all the output losses. In this example,
    we use a higher weight for the main loss (0.8), because that’s what we care about
    the most, and a lower weight for the auxiliary loss (0.2). This ratio is a regularization
    hyperparameter that you can tune.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to update the main loop in the evaluation function. However, in
    this case we can just ignore the auxiliary output, since we only really care about
    the main output—the auxiliary output is just there for regularization during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Voilà! You can now build and train all sorts of neural net architectures, combining
    predefined modules and custom modules in any way you please, and with any number
    of inputs and outputs. The flexibility of neural networks is one of their main
    qualities. But so far we have only tackled a regression task, so let’s now turn
    to classification.
  prefs: []
  type: TYPE_NORMAL
- en: Building an Image Classifier with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As in [Chapter 9](ch09.html#ann_chapter), we will tackle the Fashion MNIST dataset,
    so the first thing we need to do is to download the dataset. We could use the
    `fetch_openml()` function like we did in [Chapter 9](ch09.html#ann_chapter), but
    we will show another method instead, using the TorchVision library.
  prefs: []
  type: TYPE_NORMAL
- en: Using TorchVision to Load the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The TorchVision library is an important part of the PyTorch ecosystem: it provides
    many tools for computer vision, including utility functions to download common
    datasets, such as MNIST or Fashion MNIST, as well as pretrained models for various
    computer vision tasks (see [Chapter 12](ch12.html#cnn_chapter)), functions to
    transform images (e.g., crop, rotate, resize, etc.), and more. It is preinstalled
    on Colab, so let’s go ahead and use it to load Fashion MNIST. It is already split
    into a training set (60,000 images) and a test set (10,000 images), but we’ll
    hold out the last 5,000 images from the training set for validation, using PyTorch’s
    `random_split()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: After the imports and before loading the datasets, we create a `toTensor` object.
    What’s that about? Well, by default, the `FashionMNIST` class loads images as
    PIL (Python Image Library) images, with integer pixel values ranging from 0 to
    255\. But we need PyTorch float tensors instead, with scaled pixel values. Luckily,
    TorchVision datasets accept a `transform` argument which lets you pass a preprocessing
    function that will get executed on the fly whenever the data is accessed (there’s
    also a `target_transform` argument if you need to preprocess the targets). TorchVision
    provides many transform objects that you can use for this (most of these transforms
    are PyTorch modules).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this code, we create a `Compose` transform to chain two transforms: a `ToImage`
    transform followed by a `ToDtype` transform. `ToImage` converts various formats—including
    PIL images, NumPy arrays, and tensors—to TorchVision’s `Image` class, which is
    a subclass of `Tensor`. The `ToDtype` transform converts the data type, in this
    case to 32-bit floats. We also set its `scale` argument to `True` to ensure the
    values get scaled between 0.0 and 1.0.⁠^([12](ch10.html#id2370))'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Version 1 of TorchVision’s transforms API is still available for backward compatibility
    and can be imported using `import` `torchvision.transforms`, However, you should
    use version 2 (`torchvision.transformers.v2`) instead, since it’s faster and has
    more features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we load the dataset: first the training and validation data, then the
    test data. The `root` argument is the path to the directory where TorchVision
    will create a subdirectory for the Fashion MNIST dataset. The `train` argument
    indicates whether you want to load the training set (`True` by default) or the
    test set. The `download` argument indicates whether to download the dataset if
    it cannot be found locally (`False` by default). And we also set `transform=toTensor`
    to use our custom preprocessing pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we must create data loaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s look at the first image in the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'In [Chapter 9](ch09.html#ann_chapter), each image was represented by a 1D array
    containing 784 pixel intensities, but now each image tensor has 3 dimensions,
    and its shape is: `[1, 28, 28]`. The first dimension is the *channel* dimension.
    For grayscale images, there is a single channel (color images usually have three
    channels, as we will see in [Chapter 12](ch12.html#cnn_chapter)). The other two
    dimensions are the height and width dimensions. For example, `X_sample[0, 2, 4]`
    represents the pixel located in channel 0, row 2, column 4\. In Fashion MNIST,
    a larger value means a darker pixel.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyTorch expects the channel dimension to be first, while many other libraries,
    such as Matplotlib, PIL, TensorFlow, OpenCV, or Scikit-Image, expect it to be
    last. Always make sure to move the channel dimension to the right place, depending
    on the library you are using. `ToImage` already took care of moving the channel
    dimension to the first position, otherwise we could have used the `torch.permute()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the targets, they are integers from 0 to 9, and we can interpret them
    using the same `class_names` array as in [Chapter 9](ch09.html#ann_chapter). In
    fact, many datasets—including `FashionMNIST`—have a `classes` attribute containing
    the list of class names. For example, here’s how we can tell that the sample image
    represents an ankle boot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Building the Classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s build a custom module for a classification MLP with two hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few things to note in this code:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the model is composed of a single sequence of layers, which is why we
    used the `nn.Sequential` module. We did not have to create a custom module; we
    could have written `model = nn.Sequential(...)` instead, but it’s generally preferable
    to wrap your models in custom modules, as it makes your code easier to deploy
    and reuse, and it’s also easier to tune the hyperparameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The model starts with an `nn.Flatten` layer: this layer does not have any parameters,
    it just reshapes each input sample to a single dimension, which is needed for
    the `nn.Linear` layers. For example, a batch of 32 Fashion MNIST images has a
    shape of `[32, 1, 28, 28]`, but after going through the `nn.Flatten` layer, it
    ends up with a shape of `[32, 784]` (since 28 × 28 = 784).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first hidden layer must have the correct number of inputs (28 × 28 = 784),
    and the output layer must have the correct number of outputs (10, one per class).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a ReLU activation function after each hidden layer, and no activation
    function at all after the output layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since this is a multiclass classification task, we use `nn.CrossEntropyLoss`.
    It accepts either class indices as targets (as in this example), or class probabilities
    (such as one-hot vectors).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Shape errors are quite common, especially when getting started, so you should
    familiarize yourself with the error messages: try removing the `nn.Flatten` module,
    or try messing with the shape of the inputs and/or labels, and see the errors
    you get.'
  prefs: []
  type: TYPE_NORMAL
- en: But wait! Didn’t we say in [Chapter 9](ch09.html#ann_chapter) that we should
    use the softmax activation function on the output layer for multiclass classification
    tasks? Well it turns out that PyTorch’s `nn.CrossEntropyLoss` computes the cross-entropy
    loss directly from the logits (i.e., the class scores, introduced in [Chapter 4](ch04.html#linear_models_chapter)),
    rather than from the class probabilities. This bypasses some costly computations
    during training (e.g., logarithms and exponentials that cancel out), saving both
    compute and RAM. It’s also more numerically stable. However, the downside is that
    the model must output logits, which means that we will have to call the softmax
    function manually on the logits whenever we want class probabilities, as we will
    see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can train the model as usual (e.g., using the `train()` function with
    an `SGD` optimizer). To evaluate the model, we can use the `Accuracy` streaming
    metric from the `torchmetrics` library, and move it to the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Training the model will take a few minutes with a GPU (or much longer without
    one). Handling images requires significantly more compute and memory than handling
    low-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: The model reaches around 92.8% accuracy on the training set, and 87.2% accuracy
    on the validation set (the results might differ a bit depending on the hardware
    accelerator you use). This means there’s a little bit of overfitting going on,
    so you may want to reduce the number of neurons or add some regularization (see
    [Chapter 11](ch11.html#deep_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the model is trained, we can use it to make predictions on new images.
    As an example, let’s make predictions for the first batch in the validation set,
    and look at the results for the first three images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: For each image, the predicted class is the one with the highest logit. In this
    example, all three predictions are correct!
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if we want the model’s estimated probabilities? For this, we need
    to compute the softmax of the logits manually, since the model does not include
    the softmax activation function on the output layer, as we discussed earlier.
    We could create an `nn.Softmax` module and pass it the logits, but we can also
    just call the `softmax()` function, which is just one of many functions you will
    find in the `torch.nn.functional` module (by convention, this module is usually
    imported as `F`). It doesn’t make much difference, it just avoids creating a module
    instance that we don’t need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like in [Chapter 9](ch09.html#ann_chapter), the model is very confident
    about the first two predictions: 91.1% and 99.6%, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you wish to apply label smoothing during training, just set the `label_smoothing`
    hyperparameter of the `nn.CrossEntropyLoss` to the amount of smoothing you wish,
    between 0 and 1 (e.g., 0.05).
  prefs: []
  type: TYPE_NORMAL
- en: 'It can often be useful to get the model’s top *k* predictions. For this, we
    can use the `torch.topk()` function, which returns a tuple containing both the
    top *k* values and their indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: For the first image, the model’s best guess is class 7 (Sneaker) with 91.1%
    confidence, its second best guess is class 9 (Ankle boot) with 8.8% confidence,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Fashion MNIST dataset is balanced, meaning it has the same number of instances
    of each class. When dealing with an unbalanced dataset, you should generally give
    more weight to the rare classes and less weight to the frequent ones, or else
    your model will be biased toward the more frequent classes. You can do this by
    setting the `weight` argument of the `nn.CrossEntropyLoss`. For example, if there
    are three classes with 900, 700, and 400 instances, respectively (i.e., 2000 instances
    in total), then the respective weights should be 2000/900, 2000/700, and 2000/400\.
    It’s preferable to normalize these weights to ensure they add up to 1, so in this
    example you would set `weight=torch.tensor([0.2205, 0.2835, 0.4961])`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your PyTorch superpowers are growing: you can now build, train, and evaluate
    both regression and classification neural nets. The next step is to learn how
    to fine-tune the model hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning Neural Network Hyperparameters with Optuna
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed how to manually pick reasonable values for your model’s hyperparameters
    in [Chapter 9](ch09.html#ann_chapter), but what if you want to go further and
    automatically search for good hyperparameter values? One option is to convert
    your PyTorch model to a Scikit-Learn estimator, either by writing your own custom
    estimator class or by using a wrapper library such as Skorch ([*https://skorch.readthedocs.io*](https://skorch.readthedocs.io)),
    and then use `GridSearchCV` or `RandomizedSearchCV` to fine-tune the hyperparameters,
    as you did in [Chapter 2](ch02.html#project_chapter). However, you will usually
    get better results by using a dedicated fine-tuning library such as Optuna ([*https://optuna.org*](https://optuna.org)),
    Ray Tune ([*https://docs.ray.io*](https://docs.ray.io)), or Hyperopt ([*https://hyperopt.github.io/hyperopt*](https://hyperopt.github.io/hyperopt)).
    These libraries offer several powerful tuning strategies, and they’re highly customizable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example using Optuna. It is not preinstalled on Colab, so
    we need to install it using `%pip install optuna` (if you prefer to run the code
    locally, please follow the installation instructions at [*https://homl.info/install-p*](https://homl.info/install-p)).
    Let’s tune the learning rate and the number of neurons in the hidden layers (for
    simplicity, we will use the same number of neurons in both hidden layers). First,
    we need to define a function that Optuna will call many times to perform hyperparameter
    tuning: this function must take a `Trial` object and use it to ask Optuna for
    hyperparameter values, and then use these hyperparameter values to build and train
    a model. Finally, the function must evaluate the model (typically on the validation
    set) and return the metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The `suggest_float()` and `suggest_int()` methods let us ask Optuna for a good
    hyperparameter value in a given range (Optuna also provides a `suggest_categorical()`
    method). For the `learning_rate` hyperparameter, we ask for a value between 10^(–5)
    and 10^(–1), and since we don’t know what the optimal scale is, we add `log=True`:
    this will make Optuna sample values from a log distribution, which makes it explore
    all possible scales. If we used the default uniform distribution instead, Optuna
    would be very unlikely to explore tiny values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start hyperparameter tuning, we create a `Study` object and call its `optimize()`
    method, passing it the objective function we just defined, as well as the number
    of trials to run (i.e., the number of times Optuna should call the objective function).
    Since our objective function returns a score—higher is better—we set `direction="maximize"`
    when creating the study (by default, Optuna tries to *minimize* the objective).
    To ensure reproducibility, we also set PyTorch’s random seed, as well as the random
    seed used by Optuna’s sampler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, Optuna uses the *Tree-structured Parzen Estimator* (TPE) algorithm
    to optimize the hyperparameters: this is a sequential model-based optimization
    algorithm, meaning it learns from past results to better select promising hyperparameters.
    In other words, Optuna starts with random hyperparameter values, but it progressively
    focuses its search on the most promising regions of the hyperparameter space.
    This allows Optuna to find much better hyperparameters than random search in the
    same amount of time.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can add more hyperparameters to the search space, such as the batch size,
    the type of optimizer, the number of hidden layers, or the type of activation
    function, but remember that the search space will grow exponentially as you add
    more hyperparameters, so make sure it’s worth the extra search time and compute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once Optuna is done, you can look at the best hyperparameters it found, as
    well as the corresponding validation accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: This is slightly better than the performance we got earlier. If you increase
    `n_trials` up to 50 or more, you will get much better results, but of course it
    will take hours to run. You can also just run `optimize()` repeatedly and stop
    once you are happy with the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Optuna can also run trials in parallel across multiple machines, which can offer
    a near linear speed boost. For this, you will need to set up a SQL database (e.g.,
    SQLite or PostgreSQL), and set the `storage` parameter of the `create_study()`
    function to point to that database. You also need to set the study’s name via
    the `study_name` parameter, and set `load_if_exists=True`. After that, you can
    copy your hyperparameter tuning script to multiple machines, and run it on each
    one (if you are using random seeds, make sure they are different on each machine).
    The scripts will work in parallel, reading and writing the trial results to the
    database. This has the additional benefit of keeping a full log of all your experiment
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have noticed that we assumed that the `objective()` function had direct
    access to the training set and validation, presumably via global variables. In
    general, it’s much cleaner to pass them as extra arguments to the `objective()`
    function, for example, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'To set the extra arguments (the dataset loaders in this case), we just create
    a lambda function when needed and pass it to the `optimize()` method. Alternatively,
    you can use the `functools.partial()` function which creates a thin wrapper function
    around the given callable to provide default values for any number of arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s often possible to quickly tell that a trial is absolutely terrible: for
    example, when the loss shoots up during the first epoch, or when the model barely
    improves during the first few epochs. In such a case, it’s a good idea to interrupt
    training early to avoid wasting time and compute. You can simply return the model’s
    current validation accuracy and hope that Optuna will learn to avoid this region
    of hyperparameter space. Alternatively, you can interrupt training by raising
    the `optuna.TrialPruned` exception: this tells Optuna to ignore this trial altogether.
    In many cases, this leads to a more efficient search because it avoids polluting
    Optuna’s search algorithm with many noisy model evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Optuna comes with several `Pruner` classes that can detect and prune bad trials.
    For example, the `MedianPruner` will prune trials whose performance is below the
    median performance, at regular intervals during training. It starts pruning after
    a given number of trials have completed, controlled by `n_startup_trials` (5 by
    default). For each trial after that, it lets training start for a few epochs,
    controlled by `n_warmup_steps` (0 by default); then every few epochs (controlled
    by `interval_steps`), it ensures that the model’s performance is better than the
    median performance at the same epoch in past trials. To use this pruner, create
    an instance and pass it to the `create_study()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Then in the `objective()` function, add the following code so it runs after
    each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: The `report()` method informs Optuna of the current validation accuracy and
    epoch, so it can determine whether the trial should be pruned. If `trial.should_prune()`
    returns `True`, we raise a `TrialPruned` exception.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Optuna has many other features well worth exploring, such as visualization tools,
    persistence tools for trial results and other artifacts, a dashboard for human-in-the-loop
    optimization, and many other algorithms for hyperparameter search and trial pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are happy with the hyperparameters, you can train the model on the
    full training set (i.e., the training set plus the validation set), then evaluate
    it on the test set. Hopefully, it will perform great! If it does, you will want
    to save the model, then load it and use it in production: that’s the final topic
    of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Saving and Loading PyTorch Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest way to save a PyTorch model is to use the `torch.save()` method,
    passing it the model and the filepath. The model object is serialized using Python’s
    `pickle` module (which can convert objects into a sequence of bytes), then the
    result is compressed (zip) and saved to disk. The convention is to use the `.pt`
    or `.pth` extension for PyTorch files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Simple! Now you can load the model (e.g., in your production code) just as
    easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your model uses any custom functions or classes (e.g., `ImageClassifier`),
    then `torch.save()` only saves references to them, not the code itself. Therefore
    you must ensure that any custom code is loaded in the Python environment before
    calling `torch.load()`. Also make sure to use the same version of the code to
    avoid any mismatch issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting `weights_only=False` ensures that the whole model object is loaded
    rather than just the model parameters. Then you can use the loaded model for inference.
    Don’t forget to switch to evaluation mode first using the `eval()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'This is nice and easy, but unfortunately this approach has some very serious
    drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, pickle’s serialization format is notoriously insecure. While `torch.save()`
    doesn’t save custom code, the pickle format supports it, so a hacker could inject
    malicious code in a saved PyTorch model: this code would be run automatically
    by the `pickle` module when the model is loaded. So always make sure you fully
    trust the model’s source before you load it this way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, pickle is somewhat brittle. It can vary depending on the Python version
    (e.g., there were big changes between Python 3.7 and 3.8), and it saves specific
    filepaths to locate code, which can break if the loading environment has a different
    folder structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To avoid these issues, it is recommended to save and load the model weights
    only, rather than the full model object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The state dictionary returned by the `state_dict()` method is just a Python
    `OrderedDict` containing an entry for each parameter returned by the `named_parameters()`
    method. It also contains buffers, if the model has any: a buffer is just a regular
    tensor that was registered with the model (or any of its submodules) using the
    `register_buffer()` method. Buffers hold extra data that needs to be stored along
    with the model, but that is not a model parameter. We will see an example in [Chapter 11](ch11.html#deep_chapter)
    with the batch-norm layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load these weights, we must first create a model with the exact same structure,
    then load the weights using `torch.load()` with `weights_only=True`, and finally
    call the model’s `load_state_dict()` method with the loaded weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'The saved model contains only data, and the `load()` function makes sure of
    that, so this is safe, and also much less likely to break between Python versions
    or to cause any deployment issue. However, it only works if you are able to create
    the exact same model architecture before loading the state dictionary. For this,
    you need to know the number of layers, the number of neurons per layer, and so
    on. It’s a good idea to save this information along with the state dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then load this dictionary, construct the model based on the saved hyperparameters,
    and load the state dictionary into this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: If you want to be able to continue training where it left off, you will also
    need to save the optimizer’s state dictionary, its hyperparameters, and any other
    training information you may need, such as the current epoch and the loss history.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `safetensors` library by Hugging Face is another popular way to save model
    weights safely.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is yet another way to save and load your model: by first converting it
    to TorchScript. This also makes it possible to speed up your model’s inference.'
  prefs: []
  type: TYPE_NORMAL
- en: Compiling and Optimizing a PyTorch Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch comes with a very nice feature: it can automatically convert your model’s
    code to *TorchScript*, which you can think of as a statically typed subset of
    Python. There are two main benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: First, TorchScript code can be compiled and optimized to produce significantly
    faster models. For example, multiple operations can often be fused into a single,
    more efficient operation. Operations on constants (e.g., 2 * 3) can be replaced
    with their result (e.g., 6); this is called *constant folding*. Unused code can
    be pruned, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, TorchScript can be serialized, saved to disk, and then loaded and
    executed in Python or in a C++ environment using the LibTorch library. This makes
    it possible to run PyTorch models on a wide range of devices, including embedded
    devices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two ways to convert a PyTorch model to TorchScript. The first way
    is called *tracing*. PyTorch just runs your model with some sample data, logs
    every operation that takes place, and then converts this log to TorchScript. This
    is done using the `torch.jit.trace()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: This generally works well with static models whose `forward()` method doesn’t
    use conditionals or loops. However, if you try to trace a model that includes
    an `if` or `match` statement, then only the branch that is actually executed will
    be captured by TorchScript, which is generally not what you want. Similarly, if
    you use tracing with a model that contains a loop, then the TorchScript code will
    contain one copy of the operations within that loop for each iteration that was
    actually executed. Again, not what you generally want.
  prefs: []
  type: TYPE_NORMAL
- en: 'For such dynamic models, you will probably want to try another approach named
    *scripting*. In this case, PyTorch actually parses your Python code directly and
    converts it to TorchScript. This method supports `if` statements and `while` loops
    properly, as long as the conditions are tensors. It also supports `for` loops
    when iterating over tensors. However, it only works on a subset of Python. For
    example, you cannot use global variables, Python generators (`yield`), complex
    list comprehensions, variable length function arguments (`*args` or `**kwargs`),
    or `match` statements. Moreover, types must be fixed (a function cannot return
    an integer in some cases and a float in others), and you can only call other functions
    if they also respect these rules, so no standard library, no third-party libraries,
    etc. (see the documentation for the full list of constraints). This sounds daunting,
    but for most real-world models, these rules are actually not too hard to respect,
    and you can save your model like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Regardless of whether you use tracing or scripting to produce your TorchScript
    model, you can then further optimize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: TorchScript models can only be used for inference, not for training, since the
    TorchScript environment doesn’t support gradient tracking or parameter updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you can save a TorchScript model using its `save()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'And then load it using the `torch.jit.load()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'One important caveat: TorchScript is no longer under active development—bugs
    are fixed but no new features are added. It still works fine and it remains one
    of the best ways to run your PyTorch models in a C++ environment,⁠^([13](ch10.html#id2420))
    but since the release of PyTorch 2.0 in March 2023, the PyTorch team has been
    focusing its efforts on a new set of compilation tools centered around the `torch.compile()`
    function, which you can use very easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting model can now be used normally, and it will automatically be
    compiled and optimized when you use it. This is called Just-In-Time (JIT) compilation,
    as opposed to Ahead-Of-Time (AOT) compilation. Under the hood, `torch.compile()`
    relies on *TorchDynamo* (or *Dynamo* for short) which hooks directly into Python
    bytecode to capture the model’s computation graph at inference time. Having access
    to the bytecode allows Dynamo to efficiently and reliably capture the computation
    graph, properly handling conditionals and loops, while also benefiting from dynamic
    information that can be used to better optimize the model. The actual compilation
    and optimization is performed by default by a backend component named *TorchInductor*,
    which in turn relies on the Triton language to generate highly efficient GPU code
    (Nvidia only), or on the OpenMP API for CPU optimization. PyTorch 2.x offers a
    few other optimization backends, including the XLA backend for Google’s TPU devices:
    just set `device="xla"` when calling `torch.compile()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, you now have all the tools you need to start building and training
    complex and efficient neural networks. I hope you enjoyed this introduction to
    PyTorch! We covered a lot, but the adventure is only beginning: in the next chapter
    we will discuss techniques to train very deep nets. After that, we will dive into
    other popular neural network architectures: convolutional neural networks for
    image processing, recurrent neural networks for sequential data, transformers
    for text (and much more), autoencoders for representation learning, and generative
    adversarial networks and diffusion models to generate data.⁠^([14](ch10.html#id2429))
    Then we will visit reinforcement learning to train autonomous agents, and finally,
    we will learn more about deploying and optimizing your PyTorch models. Let’s go!'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch is similar to NumPy is many ways, but it offers some extra features.
    Can you name the most important ones?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between `torch.exp()` and `torch.exp_()`, or between
    `torch.relu()` and `torch.relu_()`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are two ways to create a new tensor on the GPU?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are three ways to perform tensor computations without using autograd?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Will the following code cause a `RuntimeError`? What if you replace the second
    line with `z = t.cos_().exp()`? And what if you replace it with `z = t.exp().cos_()`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How about the following code, will it cause an error? And what if you replace
    the third line with `w = v.cos_() * v.sin()`? Will `w` have the same value in
    both cases?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Suppose you create a `Linear(100, 200)` module. How many neurons does it have?
    What is the shape of is `weight` and `bias` parameters? What input shape does
    it expect? What output shape does it produce?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the main steps of a PyTorch training loop?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is it recommended to create the optimizer *after* the model is moved to
    the GPU?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What `DataLoader` options should you generally set to speed up training when
    using a GPU?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the main classification losses provided by PyTorch, and when should
    you use each of them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is it important to call `model.train()` before training and `model.eval()`
    before evaluation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between `torch.jit.trace()` and `torch.jit.script()`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use autograd to find the gradient vector of f(*x*, *y*) = sin(*x*² *y*) at the
    point (*x*, *y*) = (1.2, 3.4).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a custom `Dense` module that replicates the functionality of an `nn.Linear`
    module followed by an `nn.ReLU` module. Try implementing it first using the `nn.Linear`
    and `nn.ReLU` modules, and then reimplement it using `nn.Parameter` and the `relu()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Build and train a classification MLP on the CoverType dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the dataset using `sklearn.datasets.fetch_covtype()` and create a custom
    PyTorch `Dataset` for this data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create data loaders for training, validation, and testing.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a custom MLP module to tackle this classification task. You can optionally
    use the custom `Dense` module from the previous exercise.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train this model on the GPU, and try to reach 93% accuracy on the test set.
    For this, you will likely have to perform hyperparameter search to find the right
    number of layers and neurons per layer, a good learning rate and batch size, and
    so on. You can optionally use Optuna for this.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch10.html#id2228-marker)) To be fair, most of TensorFlow’s usability issues
    were fixed in version 2, and Google also launched JAX, which is well designed
    and extremely fast, so PyTorch still has some healthy competition. The good news
    is that the APIs of all these libraries have converged quite a bit, so switching
    from one to the other is much easier than it used to be.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch10.html#id2229-marker)) There are things called tensors in mathematics
    and physics, but ML tensors are simpler: they’re really just multidimensional
    arrays for numerical computations, plus a few extra features.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch10.html#id2249-marker)) CUDA is Nvidia’s proprietary platform to run
    code on its CUDA-compatible GPUs, and cuDNN is a library built on CUDA to accelerate
    various deep neural network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch10.html#id2258-marker)) The `%timeit` magic command only works in Jupyter
    notebooks and Colab, as well as in the iPython shell; in a regular Python shell
    or program, you can use the `timeit.timeit()` function instead.
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch10.html#id2274-marker)) For example, since the derivative of exp(*x*)
    is equal to exp(*x*), it makes a lot of sense to store the output of this operation
    in the computation graph during the forward pass, then use this output during
    the backward pass to get the gradients: no need to store additional data, and
    no need to recompute exp(*x*).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch10.html#id2275-marker)) For example, the derivative of abs(*x*) is –1
    when *x* < 0 and +1 when *x* > 0\. If this operation saved its output in the computation
    graph, the backward pass would be unable to know whether *x* was positive or negative
    (since abs(*x*) is always positive), so it wouldn’t be able to compute the gradients.
    This is why this operation must save its input instead.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch10.html#id2276-marker)) For example, the derivative of floor(*x*) is
    always zero (at least for noninteger inputs), so the `floor()` operation just
    saves the shape of the inputs during the forward pass, then during the backward
    pass it produces gradients of the same shape but full of zeros. For integer inputs,
    autograd also returns zeros instead of NaN.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch10.html#id2281-marker)) Column vectors (shape [*m*, 1]) and row vectors
    (shape [1, *m*]) are often preferred over 1D vectors (shape [*m*]) in machine
    learning, as they avoid ambiguity in some operations, such as matrix multiplication
    or broadcasting, and they make the code more consistent whether there’s just one
    feature or more.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch10.html#id2282-marker)) Just like in NumPy, the `reshape()` method allows
    you to specify –1 for one of the dimensions. This dimension’s size is automatically
    calculated to ensure the new tensor has the same number of cells as the original.
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch10.html#id2337-marker)) The mean of the batch MSEs is equal to the
    overall MSE since all batches have the same size. Well, except the last batch,
    which is often smaller, but this makes very little difference.
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch10.html#id2340-marker)) Heng-Tze Cheng et al., [“Wide & Deep Learning
    for Recommender Systems”](https://homl.info/widedeep), *Proceedings of the First
    Workshop on Deep Learning for Recommender Systems* (2016): 7–10.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch10.html#id2370-marker)) TorchVision includes a `ToTensor` transform
    which does all this, but it’s deprecated so it’s recommended to use this pipeline
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch10.html#id2420-marker)) Another popular option is exporting your PyTorch
    model to the open ONNX standard using `torch.onnx.export()`. The ONNX model can
    then be used for inference in a wide variety of environments.
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch10.html#id2429-marker)) A few extra ANN architectures are presented
    in the online notebook at [*https://homl.info/extra-anns*](https://homl.info/extra-anns).
  prefs: []
  type: TYPE_NORMAL
