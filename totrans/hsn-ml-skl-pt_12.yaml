- en: Chapter 10\. Building Neural Networks with PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章\. 使用 PyTorch 构建神经网络
- en: PyTorch is a powerful open source deep learning library developed by Facebook’s
    AI Research lab (FAIR, now called Meta AI). It is the Python successor of the
    Torch library, originally written in the Lua programming language. With PyTorch,
    you can build all sorts of neural network models and train them at scale using
    GPUs (or other hardware accelerators, as we will see). In many ways it is similar
    to NumPy, except it also supports hardware acceleration and autodiff (see [Chapter 9](ch09.html#ann_chapter)),
    and includes optimizers and ready-to-use neural net components.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是由 Facebook 的 AI 研究实验室（FAIR，现在称为 Meta AI）开发的一个强大的开源深度学习库。它是用 Lua 编程语言编写的
    Torch 库的 Python 后继者。使用 PyTorch，您可以构建各种神经网络模型，并使用 GPU（或我们将会看到的其他硬件加速器）进行大规模训练。在许多方面，它与
    NumPy 类似，但它还支持硬件加速和自动微分（见[第 9 章](ch09.html#ann_chapter)），并包括优化器和现成的神经网络组件。
- en: 'When PyTorch was released in 2016, Google’s TensorFlow library was by far the
    most popular: it was fast, it scaled well, and it could be deployed across many
    platforms. But its programming model was complex and static, making it difficult
    to use and debug. In contrast, PyTorch was designed from the ground up to provide
    a more flexible, Pythonic approach to building neural networks. In particular,
    as you will see, it uses dynamic computation graphs (also known as define-by-run),
    making it intuitive and easy to debug. PyTorch is also beautifully coded and documented,
    and focuses on its core task: making it easy to build and train high-performance
    neural networks. Last but not least, it leans strongly into the open source culture
    and benefits from an enthusiastic and dedicated community, and a rich ecosystem.
    In September 2022, PyTorch’s governance was even transferred to the PyTorch Foundation,
    a subsidiary of the Linux Foundation. All these qualities resonated well with
    researchers: PyTorch quickly became the most used framework in academia, and once
    a majority of deep learning papers were based on PyTorch, a large part of the
    industry was gradually converted as well.⁠^([1](ch10.html#id2228))'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当 PyTorch 在 2016 年发布时，谷歌的 TensorFlow 库无疑是最受欢迎的：它速度快，扩展性好，并且可以在许多平台上部署。但它的编程模型复杂且静态，使得使用和调试变得困难。相比之下，PyTorch
    是从头开始设计的，旨在提供一种更灵活、更 Pythonic 的神经网络构建方法。特别是，正如您将看到的，它使用动态计算图（也称为运行时定义），这使得它直观且易于调试。PyTorch
    还代码优美、文档齐全，专注于其核心任务：使构建和训练高性能神经网络变得容易。最后但同样重要的是，它强烈倾向于开源文化，并受益于一个热情和专注的社区，以及丰富的生态系统。到
    2022 年 9 月，PyTorch 的治理权甚至转移到了 Linux 基金会的子公司 PyTorch 基金会。所有这些品质都与研究人员产生了共鸣：PyTorch
    迅速成为学术界最常用的框架，一旦大多数深度学习论文都基于 PyTorch，那么行业的大部分也逐渐转向了它。⁠^([1](ch10.html#id2228))
- en: In this chapter, you will learn how to train, evaluate, fine-tune, optimize,
    and save neural nets with PyTorch. We will start by getting familiar with the
    core building blocks of PyTorch, namely tensors and autograd, next we will test
    the waters by building and training a simple linear regression model, and then
    we will upgrade this model to a multilayer neural network, first for regression,
    then for classification. Along the way, we will see how to build custom neural
    networks with multiple inputs or outputs. Finally, we will discuss how to automatically
    fine-tune hyperparameters using the Optuna library, and how to optimize and export
    your models. Hop on board, we’re diving into deep learning!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何使用 PyTorch 训练、评估、微调、优化和保存神经网络。我们将从熟悉 PyTorch 的核心构建块开始，即张量和 autograd，然后我们将通过构建和训练一个简单的线性回归模型来试探性地了解情况，接着我们将把这个模型升级为一个多层神经网络，首先是用于回归，然后是用于分类。在这个过程中，我们将看到如何构建具有多个输入或输出的自定义神经网络。最后，我们将讨论如何使用
    Optuna 库自动微调超参数，以及如何优化和导出您的模型。登上船吧，我们将深入探索深度学习！
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Colab runtimes come with a recent version of PyTorch preinstalled. However,
    if you prefer to install it on your own machine, please see the installation instructions
    at [*https://homl.info/install-p*](https://homl.info/install-p): this involves
    installing Python, many libraries, and a GPU driver (if you have one).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Colab 运行时预装了 PyTorch 的最新版本。然而，如果您更喜欢在自己的机器上安装它，请参阅[*https://homl.info/install-p*](https://homl.info/install-p)上的安装说明：这涉及到安装
    Python、许多库以及 GPU 驱动器（如果您有的话）。
- en: PyTorch Fundamentals
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 基础知识
- en: 'The core data structure of PyTorch is the *tensor*.⁠^([2](ch10.html#id2229))
    It’s a multidimensional array with a shape and a data type, used for numerical
    computations. Isn’t that exactly like a NumPy array? Well, yes, it is! But a tensor
    also has two extra features: it can live on a GPU (or other hardware accelerators,
    as we will see), and it supports auto-differentiation. Every neural network we
    will build from now on will input and output tensors (much like Scikit-Learn models
    input and output NumPy arrays). So let’s start by looking at how to create and
    manipulate tensors.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的核心数据结构是 *张量*。⁠^([2](ch10.html#id2229)) 它是一个具有形状和数据类型的多维数组，用于数值计算。这不就是和
    NumPy 数组一模一样吗？嗯，是的！但是张量还有两个额外的特性：它可以存在于 GPU（或我们将会看到的其他硬件加速器）上，并且支持自动微分。从现在开始，我们将构建的每一个神经网络都将输入和输出张量（就像
    Scikit-Learn 模型输入和输出 NumPy 数组一样）。所以，让我们先看看如何创建和操作张量。
- en: PyTorch Tensors
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 张量
- en: 'First, let’s import the PyTorch library:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入 PyTorch 库：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1][PRE2][PRE3] >>> X = torch.tensor([[1.0, 4.0, 7.0], [2.0, 3.0, 6.0]])
    `>>>` `X` `` `tensor([[1., 4., 7.],`  `[2., 3., 6.]])` `` [PRE4][PRE5][PRE6]`
    [PRE7][PRE8][PRE9] >>> X.shape `torch.Size([2, 3])` `>>>` `X``.``dtype` `` `torch.float32`
    `` [PRE10][PRE11][PRE12]``py[PRE13]py` [PRE14]`py You can also run all sorts of
    computations on tensors, and the API is conveniently similar to NumPy’s: for example,
    there’s `torch.abs()`, `torch.cos()`, `torch.exp()`, `torch.max()`, `torch.mean()`,
    `torch.sqrt()`, and so on. PyTorch tensors also have methods for most of these
    operations, so you can write `X.exp()` instead of `torch.exp(X)`. Let’s try a
    few operations:    [PRE15]py`` `tensor([[   2.7183,   54.5981, 1096.6332],`  `[   7.3891,   20.0855,  403.4288]])`
    `>>>` `X``.``mean``()` [PRE16]` [PRE17][PRE18][PRE19]py[PRE20]py` [PRE21] [PRE22][PRE23]``py[PRE24]py``
    [PRE25]`py[PRE26][PRE27][PRE28] [PRE29][PRE30][PRE31][PRE32][PRE33]``  [PRE34][PRE35]``py[PRE36]`py`
    ## Hardware Acceleration    PyTorch tensors can be copied easily to the GPU, assuming
    your machine has a compatible GPU, and you have the required libraries installed.
    On Colab, all you need to do is ensure that you are using a GPU runtime: for this,
    go to the Runtime menu and select “Change runtime type”, then make sure a GPU
    is selected (e.g., an Nvidia T4 GPU). The GPU runtime will automatically have
    the appropriate PyTorch library installed—compiled with GPU support—as well as
    the appropriate GPU drivers and related libraries (e.g., Nvidia’s CUDA and cuDNN
    libraries).⁠^([3](ch10.html#id2249)) If you prefer to run the code on your own
    machine, you will need to ensure that you have all the drivers and libraries required.
    Please follow the instructions at [*https://homl.info/install-p*](https://homl.info/install-p).    PyTorch
    has excellent support for Nvidia GPUs, as well as several other hardware accelerators:    *   Apple’s
    *Metal Performance Shaders* (MPS) to accelerate computations on Apple silicon
    such as the M1, M2, and later chips, as well as some Intel Macs with a compatible
    GPU.           *   AMD Instinct accelerators and AMD Radeon GPUs, through the
    ROCm software stack, or via DirectML on Windows.           *   Intel GPUs and
    CPUs on Linux and Windows via Intel’s oneAPI.           *   Google TPUs via the
    `torch_xla` library.              Let’s check whether PyTorch can access an Nvidia
    GPU or Apple’s MPS, otherwise let’s fall back to the CPU:    [PRE37]py    ######
    Warning    Deep learning generally requires a *lot* of compute power, especially
    once we start diving into computer vision and natural language processing, in
    the following chapters. You will need a reasonably powerful machine, but most
    importantly you will need a hardware accelerator (or several). If you don’t have
    one, you can try using Colab or Kaggle; they offer runtimes with free GPUs. Or
    consider using other cloud services. Otherwise, prepare to be very, very patient.    On
    a Colab GPU runtime, `device` will be equal to `"cuda"`. Now let’s create a tensor
    on that GPU. To do that, one option is to create the tensor on the CPU, then copy
    it to the GPU using the `to()` method:    [PRE38]py   [PRE39] ###### Tip    The
    `cpu()` and `cuda()` methods are short for `to("cpu")` and `to("cuda")`, respectively.    You
    can always tell which device a tensor lives on by looking at its `device` attribute:    [PRE40]   [PRE41]``
    Alternatively, we can create the tensor directly on the GPU using the `device`
    argument:    [PRE42]   [PRE43]` ###### Tip    If you have multiple Nvidia GPUs,
    you can refer to the desired GPU by appending the GPU index: `"cuda:0"` (or just
    `"cuda"`) for GPU #0, `"cuda:1"` for GPU #1, and so on.    Once the tensor is
    on the GPU, we can run operations on it normally, and they will all take place
    on the GPU:    [PRE44]   [PRE45] [PRE46]`py [PRE47]py`` [PRE48]py[PRE49]`` [PRE50]``
    [PRE51]` ## Autograd    PyTorch comes with an efficient implementation of reverse-mode
    auto-differentiation (introduced in [Chapter 9](ch09.html#ann_chapter) and detailed
    in [Appendix A](app01.html#autodiff_appendix)), called *autograd*, which stands
    for automatic gradients. It is quite easy to use. For example, consider a simple
    function, f(*x*) = *x*². Differential calculus tells us that the derivative of
    this function is *f’*(*x*) = 2*x*. If we evaluate f(5) and f''(5), we get 25 and
    10, respectively. Let’s see if PyTorch agrees:    [PRE52]`` `>>>` `f` [PRE53]
    `>>>` `x``.``grad` `` `tensor(10.)` `` [PRE54]` [PRE55]   [PRE56] [PRE57]`py [PRE58]py``
    [PRE59]py[PRE60]``py`` [PRE61]`py  [PRE62]`py[PRE63]py[PRE64]py` [PRE65]`py[PRE66]py[PRE67][PRE68]
    [PRE69] ## Linear Regression Using PyTorch’s High-Level API    PyTorch provides
    an implementation of linear regression in the `torch.nn.Linear` class, so let’s
    use it:    [PRE70]    The `nn.Linear` class (short for `torch.nn.Linear`) is one
    of many *modules* provided by PyTorch. Each module is a subclass of the `nn.Module`
    class. To build a simple linear regression model, a single `nn.Linear` module
    is all you need. However, for most neural networks you will need to assemble many
    modules, as we will see later in this chapter, so you can think of modules as
    math LEGO^® bricks. Many modules contain model parameters. For example, the `nn.Linear`
    module contains a `bias` vector (with one bias term per neuron), and a `weight`
    matrix (with one row per neuron and one column per input dimension, which is the
    transpose of the weight matrix we used earlier and in [Equation 9-2](ch09.html#neural_network_layer_equation)).
    Since our model has a single neuron (because `out_features=1`), the `bias` vector
    contains a single bias term, and the `weight` matrix contains a single row. These
    parameters are accessible directly as attributes of the `nn.Linear` module:    [PRE71]   [PRE72][PRE73]``py[PRE74]``
    [PRE75]`` There’s also a `named_parameters()` method that returns an iterator
    over pairs of parameter names and values.    A module can be called just like
    a regular function. For example, let’s make some predictions for the first two
    instances in the training set (since the model is not trained yet, its parameters
    are random and the predictions are terrible):    [PRE76]   [PRE77]` When we use
    a module as a function, PyTorch internally calls the module’s `forward()` method.
    In the case of the `nn.Linear` module, the `forward()` method computes `X @ self.weight.T
    + self.bias` (where `X` is the input). That’s just what we need for linear regression!    Notice
    that the result contains the `grad_fn` attribute, showing that autograd did its
    job and tracked the computation graph while the model was making its predictions.    ######
    Tip    If you pass a custom function to a module’s `register_forward_hook()` method,
    it will be called automatically every time the module itself is called. This is
    particularly handy for logging or debugging. To remove a hook, just call the `remove()`
    method on the object returned by `register_forward_hook()`. Note that hooks only
    work if you call the model like a function, not if you call its `forward()` method
    directly (which is why you should never do that). You can also register functions
    to run during the backward pass using `register_backward_hook()`.    Now that
    we have our model, we need to create an optimizer to update the model parameters,
    and we must also choose a loss function:    [PRE78]    PyTorch provides a few
    different optimizers (we will discuss them in the next chapter). Here we’re using
    the simple stochastic gradient descent (SGD) optimizer, which can be used for
    SGD, mini-batch GD, or batch gradient descent. To initialize it, we must give
    it the model parameters and the learning rate.    For the loss function, we create
    an instance of the `nn.MSELoss` class: this is also a module, so we can use it
    like a function, giving it the predictions and the targets, and it will compute
    the MSE. The `nn` module contains many other loss functions and other neural net
    tools, as we will see. Next, let’s write a small function to train our model:    [PRE79]    Compare
    this training loop with our earlier training loop: it’s very similar, but we’re
    now using higher-level constructs rather than working directly with tensors and
    autograd. Here are a few things to note:    *   In PyTorch, the loss function
    object is commonly referred to as the *criterion*, to distinguish it from the
    loss value itself (which is computed at each training iteration using the criterion).
    In this example, it’s the `MSELoss` instance.           *   The `optimizer.step()`
    line corresponds to the two lines that updated `b` and `w` in our earlier code.           *   And
    of course the `optimizer.zero_grad()` line corresponds to the two lines that zeroed
    out `b.grad` and `w.grad`. Notice that we don’t need to use `with torch.no_grad()`
    here since this is done automatically by the optimizer, inside the `step()` and
    `zero_grad()` functions.              ###### Note    Most people prefer to call
    `zero_grad()` *before* calling `loss.backward()`, rather than after: this might
    be a bit safer in case the gradients are nonzero when calling the function, but
    in general it makes no difference since gradients are automatically initialized
    to `None`.    Now let’s call this function to train our model!    [PRE80]   [PRE81]
    All good; the model is trained, and you can now use it to make predictions by
    simply calling it like a function (preferably inside a `no_grad()` context, as
    we saw earlier):    [PRE82]py`` `... `    `y_pred` `=` `model``(``X_new``)`  `#
    use the trained model to make predictions` [PRE83]` [PRE84]` [PRE85] [PRE86][PRE87][PRE88][PRE89][PRE90]
    [PRE91]`py` [PRE92] [PRE93][PRE94]``py[PRE95]py[PRE96]py[PRE97]py[PRE98]py[PRE99]py[PRE100]`py[PRE101]py[PRE102]
    `>>>` `valid_mse` `=` `evaluate``(``model``,` `valid_loader``,` `mse``)` [PRE103]
    [PRE104]   [PRE105][PRE106]``py[PRE107]`py` It works fine. But now suppose we
    want to use the RMSE instead of the MSE (as we saw in [Chapter 2](ch02.html#project_chapter),
    it can be easier to interpret). PyTorch does not have a built-in function for
    that, but it’s easy enough to write:    [PRE108]py` `...` [PRE109]py [PRE110]``py[PRE111]``
    [PRE112]`` But wait a second! The RMSE should be equal to the square root of the
    MSE; however, when we compute the square root of the MSE that we found earlier,
    we get a different result:    [PRE113]   [PRE114]` The reason is that instead
    of calculating the RMSE over the whole validation set, we computed it over each
    batch and then computed the mean of all these batch RMSEs. That’s not mathematically
    equivalent to computing the RMSE over the whole validation set. To solve this,
    we can use the MSE as our `metric_fn`, and use the `aggregate_fn` to compute the
    square root of the mean MSE:⁠^([10](ch10.html#id2337))    [PRE115] `...` `` `tensor(0.6388,
    device=''cuda:0'')` `` [PRE116]   [PRE117] [PRE118]`py [PRE119]py`` [PRE120]py[PRE121][PRE122][PRE123][PRE124]py[PRE125]py[PRE126][PRE127][PRE128]``py[PRE129]py
    [PRE130] [PRE131][PRE132]`` `>>>` `X_new` `=` `X_new``[:``3``]``.``to``(``device``)`
    [PRE133]`` `>>>` `with` `torch``.``no_grad``():` [PRE134]` `... `    `y_pred_logits`
    `=` `model``(``X_new``)` [PRE135] `...` [PRE136]`py [PRE137]py`` [PRE138]`py [PRE139]py``
    For each image, the predicted class is the one with the highest logit. In this
    example, all three predictions are correct!    But what if we want the model’s
    estimated probabilities? For this, we need to compute the softmax of the logits
    manually, since the model does not include the softmax activation function on
    the output layer, as we discussed earlier. We could create an `nn.Softmax` module
    and pass it the logits, but we can also just call the `softmax()` function, which
    is just one of many functions you will find in the `torch.nn.functional` module
    (by convention, this module is usually imported as `F`). It doesn’t make much
    difference, it just avoids creating a module instance that we don’t need:    [PRE140]py
    `>>>` `y_proba``.``round``(``decimals``=``3``)` `` `tensor([[0.000, 0.000, 0.000,
    0.000, 0.000, 0.001, 0.000, 0.911, 0.000, 0.088],`  `[0.000, 0.000, 0.004, 0.000,
    0.996, 0.000, 0.000, 0.000, 0.000, 0.000],`  `[0.000, 0.000, 0.625, 0.000, 0.335,
    0.000, 0.039, 0.000, 0.000, 0.000]],`  `device=''cuda:0'')` `` [PRE141]py   [PRE142]py
    [PRE143]`py [PRE144]py`` [PRE145]py[PRE146][PRE147][PRE148][PRE149]py[PRE150]py`
    [PRE151]`py`` [PRE152]`py[PRE153][PRE154][PRE155] [PRE156][PRE157][PRE158][PRE159][PRE160]``
    [PRE161][PRE162][PRE163] [PRE164]`py[PRE165]py[PRE166]py[PRE167]py[PRE168]py[PRE169]py`
    [PRE170]`py[PRE171]'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1][PRE2][PRE3] >>> X = torch.tensor([[1.0, 4.0, 7.0], [2.0, 3.0, 6.0]])
    `>>>` `X` `` `tensor([[1., 4., 7.],`  `[2., 3., 6.]])` `` [PRE4][PRE5][PRE6]`
    [PRE7][PRE8][PRE9] >>> X.shape `torch.Size([2, 3])` `>>>` `X``.``dtype` `` `torch.float32`
    `` [PRE10][PRE11][PRE12]``py[PRE13]py` [PRE14]`py` 你也可以在张量上运行各种计算，API与NumPy的非常相似：例如，有`torch.abs()`、`torch.cos()`、`torch.exp()`、`torch.max()`、`torch.mean()`、`torch.sqrt()`等等。PyTorch张量也有这些操作的方法，所以你可以写`X.exp()`而不是`torch.exp(X)`。让我们尝试几个操作：    [PRE15]py``
    `tensor([[   2.7183,   54.5981, 1096.6332],`  `[   7.3891,   20.0855,  403.4288]])`
    `>>>` `X``.``mean``()` [PRE16]` [PRE17][PRE18][PRE19]py[PRE20]py` [PRE21] [PRE22][PRE23]``py[PRE24]py``
    [PRE25]`py[PRE26][PRE27][PRE28] [PRE29][PRE30][PRE31][PRE32][PRE33]``  [PRE34][PRE35]``py[PRE36]`py`
    ## 硬件加速    假设你的机器有一个兼容的GPU，并且已经安装了所需的库，PyTorch张量可以轻松地复制到GPU上。在Colab上，你只需要确保你正在使用GPU运行时：为此，转到运行时菜单并选择“更改运行时类型”，然后确保已选择GPU（例如，Nvidia
    T4 GPU）。GPU运行时将自动安装适当的PyTorch库——带有GPU支持的编译库——以及适当的GPU驱动程序和相关库（例如，Nvidia的CUDA和cuDNN库）。⁠^([3](ch10.html#id2249))
    如果你更喜欢在自己的机器上运行代码，你需要确保你拥有所有所需的驱动程序和库。请按照[*https://homl.info/install-p*](https://homl.info/install-p)上的说明操作。    PyTorch对Nvidia
    GPU以及几种其他硬件加速器提供了出色的支持：    *   苹果的*Metal Performance Shaders* (MPS)用于加速在M1、M2等芯片以及一些配备兼容GPU的Intel
    Mac上的计算。           *   通过ROCm软件堆栈或Windows上的DirectML，AMD Instinct加速器和AMD Radeon
    GPU。           *   通过Intel的oneAPI在Linux和Windows上的Intel GPU和CPU。           *   通过`torch_xla`库使用Google
    TPUs。              让我们检查PyTorch是否可以访问Nvidia GPU或Apple的MPS，否则就回退到CPU：    [PRE37]py    ######
    警告    深度学习通常需要大量的计算能力，尤其是在我们开始深入研究计算机视觉和自然语言处理时（在接下来的章节中）。你需要一台相当强大的机器，但最重要的是你需要一个硬件加速器（或多个）。如果你没有，你可以尝试使用Colab或Kaggle；它们提供带有免费GPU的运行时。或者考虑使用其他云服务。否则，请做好耐心等待的准备。    在Colab
    GPU运行时，`device`将等于`"cuda"`。现在让我们在那个GPU上创建一个张量。为此，一个选项是在CPU上创建张量，然后使用`to()`方法将其复制到GPU上：    [PRE38]py   [PRE39]
    ###### 提示    `cpu()`和`cuda()`方法分别代表`to("cpu")`和`to("cuda")`。    你可以通过查看张量的`device`属性来始终知道张量位于哪个设备上：    [PRE40]   [PRE41]``
    或者，我们可以使用`device`参数直接在GPU上创建张量：    [PRE42]   [PRE43]` ###### 提示    如果你有多块Nvidia
    GPU，你可以通过附加GPU索引来引用所需的GPU：“`cuda:0`”（或只是“`cuda`”）表示GPU #0，“`cuda:1`”表示GPU #1，依此类推。    一旦张量在GPU上，我们就可以在它上面运行操作，并且它们都会在GPU上执行：    [PRE44]   [PRE45]
    [PRE46]`py [PRE47]py`` [PRE48]py[PRE49]`` [PRE50]`` [PRE51]` ## Autograd    PyTorch附带了一个反向模式自动微分的高效实现（在第9章中介绍，并在附录A中详细介绍），称为*autograd*，代表自动梯度。它非常容易使用。例如，考虑一个简单的函数，f(*x*)
    = *x*²。微分学告诉我们这个函数的导数是*f’*(*x*) = 2*x*。如果我们评估f(5)和f''(5)，我们分别得到25和10。让我们看看PyTorch是否同意：    [PRE52]``
    `>>>` `f` [PRE53] `>>>` `x``.``grad` `` `tensor(10.)` `` [PRE54]` [PRE55]   [PRE56]
    [PRE57]`py [PRE58]py`` [PRE59]py[PRE60]``py`` [PRE61]`py  [PRE62]`py[PRE63]py[PRE64]py`
    [PRE65]`py[PRE66]py[PRE67][PRE68] [PRE69] ## 使用PyTorch高级API进行线性回归    PyTorch在`torch.nn.Linear`类中提供了一个线性回归的实现，所以让我们使用它：    [PRE70]    `nn.Linear`类（简称`torch.nn.Linear`）是PyTorch提供的许多*模块*之一。每个模块都是`nn.Module`类的子类。要构建一个简单的线性回归模型，你只需要一个`nn.Linear`模块。然而，对于大多数神经网络，你将需要组装许多模块，正如我们将在本章后面看到的那样，所以你可以将模块视为数学乐高积木。许多模块包含模型参数。例如，`nn.Linear`模块包含一个`bias`向量（每个神经元一个偏置项），以及一个`weight`矩阵（每行一个神经元，每列一个输入维度，这是我们在前面的[方程式9-2](ch09.html#neural_network_layer_equation)中使用的权重矩阵的转置）。由于我们的模型只有一个神经元（因为`out_features=1`），`bias`向量包含一个偏置项，`weight`矩阵包含一行。这些参数可以直接作为`nn.Linear`模块的属性访问：    [PRE71]   [PRE72][PRE73]``py[PRE74]``
    [PRE75]`` 此外，还有一个`named_parameters()`方法，它返回一个参数名称和值的迭代器。    模块可以像普通函数一样调用。例如，让我们为训练集中的前两个实例进行一些预测（因为模型尚未训练，其参数是随机的，预测非常糟糕）：    [PRE76]   [PRE77]`
    当我们将模块用作函数时，PyTorch内部会调用模块的`forward()`方法。在`nn.Linear`模块的情况下，`forward()`方法计算`X
    @ self.weight.T + self.bias`（其中`X`是输入）。这正是线性回归所需要的！    注意，结果包含`grad_fn`属性，表明autograd在模型进行预测时跟踪了计算图。    ######
    提示    如果你将自定义函数传递给模块的`register_forward_hook()`方法，它将在每次调用模块本身时自动调用。这对于记录或调试特别有用。要删除钩子，只需在`register_forward_hook()`返回的对象上调用`remove()`方法即可。请注意，钩子仅在将模型作为函数调用时才起作用，而不是直接调用其`forward()`方法（这就是为什么你不应该这样做的原因）。你还可以使用`register_backward_hook()`注册在反向传递期间运行的函数。    现在我们有了我们的模型，我们需要创建一个优化器来更新模型参数，并且我们必须选择一个损失函数：    [PRE78]    PyTorch提供了一些不同的优化器（我们将在下一章中讨论它们）。这里我们使用简单的随机梯度下降（SGD）优化器，它可以用于SGD、小批量GD或批量梯度下降。要初始化它，我们必须给它模型参数和学习率。    对于损失函数，我们创建一个`nn.MSELoss`类的实例：这也是一个模块，所以我们可以像函数一样使用它，给它预测和目标，它将计算MSE。`nn`模块包含许多其他损失函数和其他神经网络工具，我们将在后面看到。接下来，让我们编写一个小函数来训练我们的模型：    [PRE79]    将这个训练循环与我们的早期训练循环进行比较：它们非常相似，但我们现在使用的是高级构造，而不是直接与张量和autograd交互。以下是一些需要注意的事项：    *   在PyTorch中，损失函数对象通常被称为*criterion*，以区分它本身（在每次训练迭代中使用*criterion*计算）的损失值。在这个例子中，它是`MSELoss`实例。           *   `optimizer.step()`行对应于我们早期代码中更新`b`和`w`的两行。           *   当然，`optimizer.zero_grad()`行对应于将`b.grad`和`w.grad`清零的两行。请注意，我们不需要在这里使用`with
    torch.no_grad()`，因为这是由优化器在`step()`和`zero_grad()`函数内部自动完成的。              ######
    注意    大多数人更喜欢在调用`loss.backward()`之前调用`zero_grad()`，而不是之后：这可能在函数调用时梯度非零的情况下更安全一些，但通常没有区别，因为梯度自动初始化为`None`。    现在让我们调用这个函数来训练我们的模型！    [PRE80]   [PRE81]
    所有都很好；模型已训练，你现在可以使用它来通过简单地调用它作为函数（最好在`no_grad()`上下文中，就像我们之前看到的那样）进行预测：    [PRE82]py``
    `... `    `y_pred` `=` `model``(``X_new``)`  `# 使用训练好的模型进行预测` [PRE83]` [PRE84]`
    [PRE85] [PRE86][PRE87][PRE88][PRE89][PRE90] [PRE91]`py` [PRE92] [PRE93][PRE94]``py[PRE95]py[PRE96]py[PRE97]py[PRE98]py[PRE99]py[PRE100]`py[PRE101]py[PRE102]
    `>>>` `valid_mse` `=` `evaluate``(``model``,` `valid_loader``,` `mse``)` [PRE103]
    [PRE104]   [PRE105][PRE106]``py[PRE107]`py` 它工作得很好。但现在假设我们想使用RMSE而不是MSE（如我们在[第2章](ch02.html#project_chapter)中看到的，它更容易解释）。PyTorch没有内置的函数来做这个，但写起来很容易：    [PRE108]py`
    `...` [PRE109]py [PRE110]``py[PRE111]`` [PRE112]`` 但等等！RMSE应该等于MSE的平方根；然而，当我们计算我们之前找到的MSE的平方根时，我们得到一个不同的结果：    [PRE113]   [PRE114]`
    原因是，我们不是在整个验证集上计算RMSE，而是对每个批次进行计算，然后计算所有这些批次RMSE的平均值。这从数学上不等同于在整个验证集上计算RMSE。为了解决这个问题，我们可以使用MSE作为我们的`metric_fn`，并使用`aggregate_fn`来计算平均MSE的平方根：⁠^([10](ch10.html#id2337))    [PRE115]
    `...` `` `tensor(0.6388, device=''cuda:0'')` `` [PRE116]   [PRE117] [PRE118]`py
    [PRE119]py`` [PRE120]py[PRE121][PRE122][PRE123][PRE124]py[PRE125]py[PRE126][PRE127][PRE128]``py[PRE129]py
    [PRE130] [PRE131][PRE132]`` `>>>` `X_new` `=` `X_new``[:``3``]``.``to``(``device``)`
    [PRE133]`` `>>>` `with` `torch``.``no_grad``():` [PRE134]` `... `    `y_pred_logits`
    `=` `model``(``X_new``)` [PRE135] `...` [PRE136]`py [PRE137]py`` [PRE138]py``
    [PRE139]py`` 对于每个图像，预测的类别是具有最高logit的类别。在这个例子中，所有三个预测都是正确的！    但如果我们想得到模型的估计概率呢？为此，我们需要手动计算logit的softmax，因为模型在输出层没有包含我们之前讨论的softmax激活函数。我们可以创建一个`nn.Softmax`模块并将logit传递给它，但我们也可以直接调用`softmax()`函数，这是`torch.nn.functional`模块中许多函数之一（按照惯例，此模块通常导入为`F`）。这没有太大区别，只是避免了创建我们不需要的模块实例：    [PRE140]py
    `>>>` `y_proba``.``round``(``decimals``=``3``)` `` `tensor([[0.000, 0.000, 0.000,
    0.000, 0.000, 0.001, 0.000, 0.911, 0.000, 0.088],`  `[0.000, 0.000, 0.004, 0.000,
    0.996, 0.000, 0.000, 0.000, 0.000, 0.000],`  `[0.000, 0.000, 0.625, 0.000, 0.335,
    0.000, 0.039, 0.000, 0.000, 0.000]],`  `device=''cuda:0'')` `` [PRE141]py   [PRE142]py
    [PRE143]`py [PRE144]py`` [PRE145]py[PRE146][PRE147][PRE148][PRE149]py[PRE150]py`
    [PRE151]`py`` [PRE152]`py[PRE153][PRE154][PRE155] [PRE156][PRE157][PRE158][PRE159][PRE160]``
    [PRE161][PRE162][PRE163] [PRE164]`py[PRE165]py[PRE166]py[PRE167]py[PRE168]py[PRE169]py`
    [PRE170]`py[PRE171]'
