- en: Chapter 10\. Building Neural Networks with PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章\. 使用 PyTorch 构建神经网络
- en: PyTorch is a powerful open source deep learning library developed by Facebook’s
    AI Research lab (FAIR, now called Meta AI). It is the Python successor of the
    Torch library, originally written in the Lua programming language. With PyTorch,
    you can build all sorts of neural network models and train them at scale using
    GPUs (or other hardware accelerators, as we will see). In many ways it is similar
    to NumPy, except it also supports hardware acceleration and autodiff (see [Chapter 9](ch09.html#ann_chapter)),
    and includes optimizers and ready-to-use neural net components.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是由 Facebook 的 AI 研究实验室（FAIR，现在称为 Meta AI）开发的一个强大的开源深度学习库。它是用 Lua 编程语言编写的
    Torch 库的 Python 后继者。使用 PyTorch，您可以构建各种神经网络模型，并使用 GPU（或我们将会看到的其他硬件加速器）进行大规模训练。在许多方面，它与
    NumPy 类似，但它还支持硬件加速和自动微分（见[第 9 章](ch09.html#ann_chapter)），并包括优化器和现成的神经网络组件。
- en: 'When PyTorch was released in 2016, Google’s TensorFlow library was by far the
    most popular: it was fast, it scaled well, and it could be deployed across many
    platforms. But its programming model was complex and static, making it difficult
    to use and debug. In contrast, PyTorch was designed from the ground up to provide
    a more flexible, Pythonic approach to building neural networks. In particular,
    as you will see, it uses dynamic computation graphs (also known as define-by-run),
    making it intuitive and easy to debug. PyTorch is also beautifully coded and documented,
    and focuses on its core task: making it easy to build and train high-performance
    neural networks. Last but not least, it leans strongly into the open source culture
    and benefits from an enthusiastic and dedicated community, and a rich ecosystem.
    In September 2022, PyTorch’s governance was even transferred to the PyTorch Foundation,
    a subsidiary of the Linux Foundation. All these qualities resonated well with
    researchers: PyTorch quickly became the most used framework in academia, and once
    a majority of deep learning papers were based on PyTorch, a large part of the
    industry was gradually converted as well.⁠^([1](ch10.html#id2228))'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当 PyTorch 在 2016 年发布时，谷歌的 TensorFlow 库无疑是最受欢迎的：它速度快，扩展性好，并且可以在许多平台上部署。但它的编程模型复杂且静态，使得使用和调试变得困难。相比之下，PyTorch
    是从头开始设计的，旨在提供一种更灵活、更 Pythonic 的神经网络构建方法。特别是，正如您将看到的，它使用动态计算图（也称为运行时定义），这使得它直观且易于调试。PyTorch
    还代码优美、文档齐全，专注于其核心任务：使构建和训练高性能神经网络变得容易。最后但同样重要的是，它强烈倾向于开源文化，并受益于一个热情和专注的社区，以及丰富的生态系统。到
    2022 年 9 月，PyTorch 的治理权甚至转移到了 Linux 基金会的子公司 PyTorch 基金会。所有这些品质都与研究人员产生了共鸣：PyTorch
    迅速成为学术界最常用的框架，一旦大多数深度学习论文都基于 PyTorch，那么行业的大部分也逐渐转向了它。⁠^([1](ch10.html#id2228))
- en: In this chapter, you will learn how to train, evaluate, fine-tune, optimize,
    and save neural nets with PyTorch. We will start by getting familiar with the
    core building blocks of PyTorch, namely tensors and autograd, next we will test
    the waters by building and training a simple linear regression model, and then
    we will upgrade this model to a multilayer neural network, first for regression,
    then for classification. Along the way, we will see how to build custom neural
    networks with multiple inputs or outputs. Finally, we will discuss how to automatically
    fine-tune hyperparameters using the Optuna library, and how to optimize and export
    your models. Hop on board, we’re diving into deep learning!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何使用 PyTorch 训练、评估、微调、优化和保存神经网络。我们将从熟悉 PyTorch 的核心构建块开始，即张量和 autograd，然后我们将通过构建和训练一个简单的线性回归模型来试探性地了解情况，接着我们将把这个模型升级为一个多层神经网络，首先是用于回归，然后是用于分类。在这个过程中，我们将看到如何构建具有多个输入或输出的自定义神经网络。最后，我们将讨论如何使用
    Optuna 库自动微调超参数，以及如何优化和导出您的模型。登上船吧，我们将深入探索深度学习！
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Colab runtimes come with a recent version of PyTorch preinstalled. However,
    if you prefer to install it on your own machine, please see the installation instructions
    at [*https://homl.info/install-p*](https://homl.info/install-p): this involves
    installing Python, many libraries, and a GPU driver (if you have one).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Colab 运行时预装了 PyTorch 的最新版本。然而，如果您更喜欢在自己的机器上安装它，请参阅[*https://homl.info/install-p*](https://homl.info/install-p)上的安装说明：这涉及到安装
    Python、许多库以及 GPU 驱动器（如果您有的话）。
- en: PyTorch Fundamentals
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 基础知识
- en: 'The core data structure of PyTorch is the *tensor*.⁠^([2](ch10.html#id2229))
    It’s a multidimensional array with a shape and a data type, used for numerical
    computations. Isn’t that exactly like a NumPy array? Well, yes, it is! But a tensor
    also has two extra features: it can live on a GPU (or other hardware accelerators,
    as we will see), and it supports auto-differentiation. Every neural network we
    will build from now on will input and output tensors (much like Scikit-Learn models
    input and output NumPy arrays). So let’s start by looking at how to create and
    manipulate tensors.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的核心数据结构是 *张量*。⁠^([2](ch10.html#id2229)) 它是一个具有形状和数据类型的多维数组，用于数值计算。这不就是和
    NumPy 数组一模一样吗？嗯，是的！但是张量还有两个额外的特性：它可以存在于 GPU（或我们将会看到的其他硬件加速器）上，并且支持自动微分。从现在开始，我们将构建的每一个神经网络都将输入和输出张量（就像
    Scikit-Learn 模型输入和输出 NumPy 数组一样）。所以，让我们先看看如何创建和操作张量。
- en: PyTorch Tensors
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 张量
- en: 'First, let’s import the PyTorch library:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入 PyTorch 库：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1][PRE2][PRE3] >>> X = torch.tensor([[1.0, 4.0, 7.0], [2.0, 3.0, 6.0]])
    `>>>` `X` `` `tensor([[1., 4., 7.],`  `[2., 3., 6.]])` `` [PRE4][PRE5][PRE6]`
    [PRE7][PRE8][PRE9] >>> X.shape `torch.Size([2, 3])` `>>>` `X``.``dtype` `` `torch.float32`
    `` [PRE10][PRE11][PRE12]``py[PRE13]py` [PRE14]`py You can also run all sorts of
    computations on tensors, and the API is conveniently similar to NumPy’s: for example,
    there’s `torch.abs()`, `torch.cos()`, `torch.exp()`, `torch.max()`, `torch.mean()`,
    `torch.sqrt()`, and so on. PyTorch tensors also have methods for most of these
    operations, so you can write `X.exp()` instead of `torch.exp(X)`. Let’s try a
    few operations:    [PRE15]py`` `tensor([[   2.7183,   54.5981, 1096.6332],`  `[   7.3891,   20.0855,  403.4288]])`
    `>>>` `X``.``mean``()` [PRE16]` [PRE17][PRE18][PRE19]py[PRE20]py` [PRE21] [PRE22][PRE23]``py[PRE24]py``
    [PRE25]`py[PRE26][PRE27][PRE28] [PRE29][PRE30][PRE31][PRE32][PRE33]``  [PRE34][PRE35]``py[PRE36]`py`
    ## Hardware Acceleration    PyTorch tensors can be copied easily to the GPU, assuming
    your machine has a compatible GPU, and you have the required libraries installed.
    On Colab, all you need to do is ensure that you are using a GPU runtime: for this,
    go to the Runtime menu and select “Change runtime type”, then make sure a GPU
    is selected (e.g., an Nvidia T4 GPU). The GPU runtime will automatically have
    the appropriate PyTorch library installed—compiled with GPU support—as well as
    the appropriate GPU drivers and related libraries (e.g., Nvidia’s CUDA and cuDNN
    libraries).⁠^([3](ch10.html#id2249)) If you prefer to run the code on your own
    machine, you will need to ensure that you have all the drivers and libraries required.
    Please follow the instructions at [*https://homl.info/install-p*](https://homl.info/install-p).    PyTorch
    has excellent support for Nvidia GPUs, as well as several other hardware accelerators:    *   Apple’s
    *Metal Performance Shaders* (MPS) to accelerate computations on Apple silicon
    such as the M1, M2, and later chips, as well as some Intel Macs with a compatible
    GPU.           *   AMD Instinct accelerators and AMD Radeon GPUs, through the
    ROCm software stack, or via DirectML on Windows.           *   Intel GPUs and
    CPUs on Linux and Windows via Intel’s oneAPI.           *   Google TPUs via the
    `torch_xla` library.              Let’s check whether PyTorch can access an Nvidia
    GPU or Apple’s MPS, otherwise let’s fall back to the CPU:    [PRE37]py    ######
    Warning    Deep learning generally requires a *lot* of compute power, especially
    once we start diving into computer vision and natural language processing, in
    the following chapters. You will need a reasonably powerful machine, but most
    importantly you will need a hardware accelerator (or several). If you don’t have
    one, you can try using Colab or Kaggle; they offer runtimes with free GPUs. Or
    consider using other cloud services. Otherwise, prepare to be very, very patient.    On
    a Colab GPU runtime, `device` will be equal to `"cuda"`. Now let’s create a tensor
    on that GPU. To do that, one option is to create the tensor on the CPU, then copy
    it to the GPU using the `to()` method:    [PRE38]py   [PRE39] ###### Tip    The
    `cpu()` and `cuda()` methods are short for `to("cpu")` and `to("cuda")`, respectively.    You
    can always tell which device a tensor lives on by looking at its `device` attribute:    [PRE40]   [PRE41]``
    Alternatively, we can create the tensor directly on the GPU using the `device`
    argument:    [PRE42]   [PRE43]` ###### Tip    If you have multiple Nvidia GPUs,
    you can refer to the desired GPU by appending the GPU index: `"cuda:0"` (or just
    `"cuda"`) for GPU #0, `"cuda:1"` for GPU #1, and so on.    Once the tensor is
    on the GPU, we can run operations on it normally, and they will all take place
    on the GPU:    [PRE44]   [PRE45] [PRE46]`py [PRE47]py`` [PRE48]py[PRE49]`` [PRE50]``
    [PRE51]` ## Autograd    PyTorch comes with an efficient implementation of reverse-mode
    auto-differentiation (introduced in [Chapter 9](ch09.html#ann_chapter) and detailed
    in [Appendix A](app01.html#autodiff_appendix)), called *autograd*, which stands
    for automatic gradients. It is quite easy to use. For example, consider a simple
    function, f(*x*) = *x*². Differential calculus tells us that the derivative of
    this function is *f’*(*x*) = 2*x*. If we evaluate f(5) and f''(5), we get 25 and
    10, respectively. Let’s see if PyTorch agrees:    [PRE52]`` `>>>` `f` [PRE53]
    `>>>` `x``.``grad` `` `tensor(10.)` `` [PRE54]` [PRE55]   [PRE56] [PRE57]`py [PRE58]py``
    [PRE59]py[PRE60]``py`` [PRE61]`py  [PRE62]`py[PRE63]py[PRE64]py` [PRE65]`py[PRE66]py[PRE67][PRE68]
    [PRE69] ## Linear Regression Using PyTorch’s High-Level API    PyTorch provides
    an implementation of linear regression in the `torch.nn.Linear` class, so let’s
    use it:    [PRE70]    The `nn.Linear` class (short for `torch.nn.Linear`) is one
    of many *modules* provided by PyTorch. Each module is a subclass of the `nn.Module`
    class. To build a simple linear regression model, a single `nn.Linear` module
    is all you need. However, for most neural networks you will need to assemble many
    modules, as we will see later in this chapter, so you can think of modules as
    math LEGO^® bricks. Many modules contain model parameters. For example, the `nn.Linear`
    module contains a `bias` vector (with one bias term per neuron), and a `weight`
    matrix (with one row per neuron and one column per input dimension, which is the
    transpose of the weight matrix we used earlier and in [Equation 9-2](ch09.html#neural_network_layer_equation)).
    Since our model has a single neuron (because `out_features=1`), the `bias` vector
    contains a single bias term, and the `weight` matrix contains a single row. These
    parameters are accessible directly as attributes of the `nn.Linear` module:    [PRE71]   [PRE72][PRE73]``py[PRE74]``
    [PRE75]`` There’s also a `named_parameters()` method that returns an iterator
    over pairs of parameter names and values.    A module can be called just like
    a regular function. For example, let’s make some predictions for the first two
    instances in the training set (since the model is not trained yet, its parameters
    are random and the predictions are terrible):    [PRE76]   [PRE77]` When we use
    a module as a function, PyTorch internally calls the module’s `forward()` method.
    In the case of the `nn.Linear` module, the `forward()` method computes `X @ self.weight.T
    + self.bias` (where `X` is the input). That’s just what we need for linear regression!    Notice
    that the result contains the `grad_fn` attribute, showing that autograd did its
    job and tracked the computation graph while the model was making its predictions.    ######
    Tip    If you pass a custom function to a module’s `register_forward_hook()` method,
    it will be called automatically every time the module itself is called. This is
    particularly handy for logging or debugging. To remove a hook, just call the `remove()`
    method on the object returned by `register_forward_hook()`. Note that hooks only
    work if you call the model like a function, not if you call its `forward()` method
    directly (which is why you should never do that). You can also register functions
    to run during the backward pass using `register_backward_hook()`.    Now that
    we have our model, we need to create an optimizer to update the model parameters,
    and we must also choose a loss function:    [PRE78]    PyTorch provides a few
    different optimizers (we will discuss them in the next chapter). Here we’re using
    the simple stochastic gradient descent (SGD) optimizer, which can be used for
    SGD, mini-batch GD, or batch gradient descent. To initialize it, we must give
    it the model parameters and the learning rate.    For the loss function, we create
    an instance of the `nn.MSELoss` class: this is also a module, so we can use it
    like a function, giving it the predictions and the targets, and it will compute
    the MSE. The `nn` module contains many other loss functions and other neural net
    tools, as we will see. Next, let’s write a small function to train our model:    [PRE79]    Compare
    this training loop with our earlier training loop: it’s very similar, but we’re
    now using higher-level constructs rather than working directly with tensors and
    autograd. Here are a few things to note:    *   In PyTorch, the loss function
    object is commonly referred to as the *criterion*, to distinguish it from the
    loss value itself (which is computed at each training iteration using the criterion).
    In this example, it’s the `MSELoss` instance.           *   The `optimizer.step()`
    line corresponds to the two lines that updated `b` and `w` in our earlier code.           *   And
    of course the `optimizer.zero_grad()` line corresponds to the two lines that zeroed
    out `b.grad` and `w.grad`. Notice that we don’t need to use `with torch.no_grad()`
    here since this is done automatically by the optimizer, inside the `step()` and
    `zero_grad()` functions.              ###### Note    Most people prefer to call
    `zero_grad()` *before* calling `loss.backward()`, rather than after: this might
    be a bit safer in case the gradients are nonzero when calling the function, but
    in general it makes no difference since gradients are automatically initialized
    to `None`.    Now let’s call this function to train our model!    [PRE80]   [PRE81]
    All good; the model is trained, and you can now use it to make predictions by
    simply calling it like a function (preferably inside a `no_grad()` context, as
    we saw earlier):    [PRE82]py`` `... `    `y_pred` `=` `model``(``X_new``)`  `#
    use the trained model to make predictions` [PRE83]` [PRE84]` [PRE85] [PRE86][PRE87][PRE88][PRE89][PRE90]
    [PRE91]`py` [PRE92] [PRE93][PRE94]``py[PRE95]py[PRE96]py[PRE97]py[PRE98]py[PRE99]py[PRE100]`py[PRE101]py[PRE102]
    `>>>` `valid_mse` `=` `evaluate``(``model``,` `valid_loader``,` `mse``)` [PRE103]
    [PRE104]   [PRE105][PRE106]``py[PRE107]`py` It works fine. But now suppose we
    want to use the RMSE instead of the MSE (as we saw in [Chapter 2](ch02.html#project_chapter),
    it can be easier to interpret). PyTorch does not have a built-in function for
    that, but it’s easy enough to write:    [PRE108]py` `...` [PRE109]py [PRE110]``py[PRE111]``
    [PRE112]`` But wait a second! The RMSE should be equal to the square root of the
    MSE; however, when we compute the square root of the MSE that we found earlier,
    we get a different result:    [PRE113]   [PRE114]` The reason is that instead
    of calculating the RMSE over the whole validation set, we computed it over each
    batch and then computed the mean of all these batch RMSEs. That’s not mathematically
    equivalent to computing the RMSE over the whole validation set. To solve this,
    we can use the MSE as our `metric_fn`, and use the `aggregate_fn` to compute the
    square root of the mean MSE:⁠^([10](ch10.html#id2337))    [PRE115] `...` `` `tensor(0.6388,
    device=''cuda:0'')` `` [PRE116]   [PRE117] [PRE118]`py [PRE119]py`` [PRE120]py[PRE121][PRE122][PRE123][PRE124]py[PRE125]py[PRE126][PRE127][PRE128]``py[PRE129]py
    [PRE130] [PRE131][PRE132]`` `>>>` `X_new` `=` `X_new``[:``3``]``.``to``(``device``)`
    [PRE133]`` `>>>` `with` `torch``.``no_grad``():` [PRE134]` `... `    `y_pred_logits`
    `=` `model``(``X_new``)` [PRE135] `...` [PRE136]`py [PRE137]py`` [PRE138]`py [PRE139]py``
    For each image, the predicted class is the one with the highest logit. In this
    example, all three predictions are correct!    But what if we want the model’s
    estimated probabilities? For this, we need to compute the softmax of the logits
    manually, since the model does not include the softmax activation function on
    the output layer, as we discussed earlier. We could create an `nn.Softmax` module
    and pass it the logits, but we can also just call the `softmax()` function, which
    is just one of many functions you will find in the `torch.nn.functional` module
    (by convention, this module is usually imported as `F`). It doesn’t make much
    difference, it just avoids creating a module instance that we don’t need:    [PRE140]py
    `>>>` `y_proba``.``round``(``decimals``=``3``)` `` `tensor([[0.000, 0.000, 0.000,
    0.000, 0.000, 0.001, 0.000, 0.911, 0.000, 0.088],`  `[0.000, 0.000, 0.004, 0.000,
    0.996, 0.000, 0.000, 0.000, 0.000, 0.000],`  `[0.000, 0.000, 0.625, 0.000, 0.335,
    0.000, 0.039, 0.000, 0.000, 0.000]],`  `device=''cuda:0'')` `` [PRE141]py   [PRE142]py
    [PRE143]`py [PRE144]py`` [PRE145]py[PRE146][PRE147][PRE148][PRE149]py[PRE150]py`
    [PRE151]`py`` [PRE152]`py[PRE153][PRE154][PRE155] [PRE156][PRE157][PRE158][PRE159][PRE160]``
    [PRE161][PRE162][PRE163] [PRE164]`py[PRE165]py[PRE166]py[PRE167]py[PRE168]py[PRE169]py`
    [PRE170]`py[PRE171]'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
