- en: 14 Building and training a music Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14 构建和训练音乐Transformer
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Representing music with control messages and velocity values
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用控制信息和速度值来表示音乐
- en: Tokenizing music into a sequence of indexes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将音乐标记化为一系列索引
- en: Building and training a music Transformer
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练音乐Transformer
- en: Generating musical events using the trained Transformer
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练好的Transformer生成音乐事件
- en: Converting musical events back to a playable MIDI file
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将音乐事件转换回可播放的MIDI文件
- en: 'Sad that your favorite musician is no longer with us? Sad no more: generative
    AI can bring them back to the stage!'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为你最喜欢的音乐家不再与我们同在而感到难过？不再难过：生成式AI可以将他们带回舞台！
- en: Take, for example, Layered Reality, a London-based company that’s working on
    a project called Elvis Evolution.^([1](#footnote-002)) The goal? To resurrect
    the legendary Elvis Presley using AI. By feeding a vast array of Elvis’ official
    archival material, including video clips, photographs, and music, into a sophisticated
    computer model, this AI Elvis learns to mimic his singing, speaking, dancing,
    and walking with remarkable resemblance. The result? A digital performance that
    captures the essence of the late King himself.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以Layered Reality为例，这是一家位于伦敦的公司，正在开发名为《猫王进化》的项目。[1](#footnote-002) 目标？使用AI复活传奇的猫王艾维斯·普雷斯利。通过将艾维斯的大量官方档案材料，包括视频剪辑、照片和音乐，输入到一个复杂的计算机模型中，这个AI猫王学会了以惊人的相似度模仿他的唱歌、说话、跳舞和走路。结果？一场数字表演，捕捉了已故国王本人的精髓。
- en: The Elvis Evolution project is a shining example of the transformative effect
    of generative AI across various industries. In the previous chapter, you explored
    the use of MuseGAN to create music that could pass as authentic multitrack compositions.
    MuseGAN views a piece of music as a multidimensional object, similar to an image,
    and generates complete music pieces that resemble those in the training dataset.
    Both real and AI-generated music are then evaluated by a critic, which helps refine
    the AI-generated music until it’s indistinguishable from the real thing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 《猫王进化》项目是生成式AI在各个行业产生变革性影响的杰出例子。在前一章中，你探讨了使用MuseGAN创建可以以多轨音乐作品为假的音乐的方法。MuseGAN将一首音乐视为一个多维对象，类似于图像，并生成与训练数据集中的音乐相似的音乐作品。然后，由评论家评估真实和AI生成的音乐，这有助于改进AI生成的音乐，直到它与真实音乐无法区分。
- en: In this chapter, you’ll take a different approach to AI music creation, treating
    it as a sequence of musical events. We’ll apply techniques from text generation,
    as discussed in chapters 11 and 12, to predict the next element in a sequence.
    Specifically, you’ll develop a GPT-style model to predict the next musical event
    based on all previous events in the sequence. GPT-style Transformers are ideal
    for this task because of their scalability and the self-attention mechanism, which
    helps them capture long-range dependencies and understand context. This makes
    them highly effective for sequence prediction and generation across a wide range
    of content, including music. The music Transformer you will create has 20.16 million
    parameters, large enough to capture the long-term relations of different notes
    in music pieces but small enough to be trained in a reasonable amount of time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将采用一种不同的方法来处理AI音乐创作，将其视为一系列音乐事件。我们将应用第11章和第12章中讨论的文本生成技术，来预测序列中的下一个元素。具体来说，你将开发一个类似GPT风格的模型，根据序列中所有先前事件来预测下一个音乐事件。由于GPT风格的Transformer具有可扩展性和自注意力机制，这些机制有助于它们捕捉长距离依赖关系并理解上下文，因此它们非常适合这项任务。你将创建的音乐Transformer具有2016万个参数，足够捕捉音乐作品中不同音符的长期关系，但同时也足够小，可以在合理的时间内进行训练。
- en: We’ll use the Maestro piano music from Google’s Magenta group as our training
    data. You’ll learn how to first convert a musical instrument digital interface
    (MIDI) file into a sequence of music notes, analogous to raw text data in natural
    language processing (NLP). You’ll then break the musical notes down into small
    pieces called musical events, analogous to tokens in NLP. Since neural networks
    can only accept numerical inputs, you’ll map each unique event token to an index.
    With this, the music pieces in the training data are converted into sequences
    of indexes, ready to be fed into neural networks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自谷歌 Magenta 团队的 Maestro 钢琴音乐作为我们的训练数据。你将学习如何首先将音乐乐器数字接口（MIDI）文件转换为音乐音符序列，类似于自然语言处理（NLP）中的原始文本数据。然后你将把音乐音符分解成称为音乐事件的小片段，类似于
    NLP 中的标记。由于神经网络只能接受数值输入，因此你需要将每个独特的事件标记映射到一个索引。有了这个，训练数据中的音乐作品就被转换成了索引序列，准备好输入到神经网络中。
- en: To train the music Transformer to predict the next token based on the current
    token and all previous tokens in the sequence, we’ll create sequences of 2,048
    indexes as inputs (features x). We then shift the sequences one index to the right
    and use them as the outputs (targets y). We feed pairs of (x, y) to the music
    Transformer to train the model. Once trained, we’ll use a short sequence of indexes
    as the prompt and feed it to the music Transformer to predict the next token,
    which is then appended to the prompt to form a new sequence. This new sequence
    is fed back into the model for further predictions, and this process is repeated
    until the sequence reaches a desired length.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练音乐 Transformer 以预测序列中下一个标记，基于当前标记和序列中所有之前的标记，我们将创建长度为 2,048 的索引序列作为输入（特征
    x）。然后我们将序列向右移动一个索引，并使用它们作为输出（目标 y）。我们将 (x, y) 对输入到音乐 Transformer 中以训练模型。一旦训练完成，我们将使用一个短索引序列作为提示并将其输入到音乐
    Transformer 中以预测下一个标记，然后将该标记附加到提示中形成一个新的序列。这个新序列被反馈到模型中进行进一步的预测，这个过程会重复进行，直到序列达到期望的长度。
- en: You’ll see that the trained music Transformer can generate lifelike music that
    mimics the style in the training dataset. Further, unlike the music generated
    in chapter 13, you’ll learn to control the creativity of the music piece. You’ll
    achieve this by scaling the predicted logits with the temperature parameter, just
    as you did in earlier chapters when controlling the creativity of the generated
    text.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到训练好的音乐 Transformer 可以生成模仿训练数据集中风格的逼真音乐。此外，与第 13 章中生成的音乐不同，你将学习如何控制音乐作品的艺术性。你将通过调整预测的
    logits 与温度参数的比例来实现这一点，就像你在前几章中控制生成文本的艺术性时做的那样。
- en: 14.1 Introduction to the music Transformer
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.1 音乐 Transformer 简介
- en: The concept of the music Transformer was introduced in 2018.^([2](#footnote-001))
    This innovative approach extends the Transformer architecture, initially devised
    for NLP tasks, to the field of music generation. As discussed in previous chapters,
    Transformers employ self-attention mechanisms to effectively grasp the context
    and capture long-range dependencies among elements in a sequence.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐 Transformer 的概念于 2018 年提出.^([2](#footnote-001)) 这种创新方法扩展了最初为 NLP 任务设计的 Transformer
    架构，应用于音乐生成领域。正如前几章所讨论的，Transformers 使用自注意力机制来有效地把握上下文，并捕捉序列中元素之间的长距离依赖关系。
- en: In a similar vein, the music Transformer is engineered to generate a sequence
    of musical notes by learning from a vast dataset of existing music. The model
    is trained to predict the next musical event in a sequence based on preceding
    events by understanding the patterns, structures, and relationships between different
    musical elements in the training data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，音乐 Transformer 被设计成通过学习大量现有音乐数据集来生成音乐序列。该模型被训练成根据先前的音乐事件来预测序列中的下一个音乐事件，通过理解训练数据中不同音乐元素之间的模式、结构和关系。
- en: 'A crucial step in training a music Transformer lies in figuring out how to
    represent music as a sequence of unique musical events, akin to tokens in NLP.
    In the previous chapter, you learned to represent a piece of music as a 4D object.
    In this chapter, you will explore an alternative approach to music representation,
    specifically performance-based music representation through control messages and
    velocity values.^([3](#footnote-000)) Based on this, you will convert a piece
    of music into four types of musical events: note-on, note-off, time-shift, and
    velocity.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 训练音乐Transformer的关键步骤在于找出如何将音乐表示为一系列独特的音乐事件，类似于NLP中的标记。在上一章中，你学习了如何将一首音乐表示为4D对象。在本章中，你将探索一种替代的音乐表示方法，即通过控制信息和速度值实现的基于性能的音乐表示.^([3](#footnote-000))
    基于此，你将把一首音乐转换为四种类型的音乐事件：音符开启、音符关闭、时间移动和速度。
- en: Note-on signals the start of a musical note being played, specifying the note’s
    pitch. Note-off indicates the end of a note, telling the instrument to stop playing
    that note. Time-shift represents the amount of time that passes between two musical
    events. Velocity measures the force or speed with which a note is played, with
    higher values corresponding to a stronger, louder sound. Each type of musical
    event has many different values. Each unique event will be mapped to a different
    index, effectively transforming a piece of music into a sequence of indexes. You
    will then apply the GPT models, as discussed in chapters 11 and 12, to create
    a decoder-only music Transformer that predicts the next musical event in the sequence.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 音符开启信号表示一个音符的开始演奏，指定音符的音高。音符关闭表示音符的结束，告诉乐器停止演奏该音符。时间移动表示两个音乐事件之间经过的时间量。速度衡量演奏音符的力量或速度，较高的值对应更强的、更响亮的声音。每种类型的音乐事件都有许多不同的值。每个独特的事件将被映射到不同的索引，有效地将一首音乐转换为一个索引序列。然后，你将应用第11章和第12章中讨论的GPT模型，创建一个仅具有解码器的音乐Transformer，以预测序列中的下一个音乐事件。
- en: In this section, you will begin by learning about performance-based music representation
    through control messages and velocity values. You will then explore how to represent
    music pieces as sequences of musical events. Finally, you will learn the steps
    involved in building and training a Transformer to generate music.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将首先通过控制信息和速度值了解基于性能的音乐表示。然后，你将探索如何将音乐作品表示为一系列音乐事件。最后，你将学习构建和训练Transformer以生成音乐的步骤。
- en: 14.1.1 Performance-based music representation
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.1 基于性能的音乐表示
- en: Performance-based music representation is often achieved using the MIDI format,
    which captures the nuances of a musical performance through control messages and
    velocity values. In MIDI, musical notes are represented by note-on and note-off
    messages, which include information about the pitch and velocity of each note.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于性能的音乐表示通常使用MIDI格式实现，该格式通过控制信息和速度值捕捉音乐表演的细微差别。在MIDI中，音符通过音符开启和音符关闭消息表示，这些消息包含每个音符的音高和速度信息。
- en: As we discussed in chapter 13, the pitch value ranges from 0 to 127, with each
    value corresponding to a semitone in an octave level. For instance, the pitch
    value 60 corresponds to a C4 note, while the pitch value 74 corresponds to a D5
    note. The velocity value, also ranging from 0 to 127, represents the dynamics
    of the note, with higher values indicating louder or more forceful playing. By
    combining these control messages and velocity values, a MIDI sequence can capture
    the expressive details of a live performance, allowing for expressive playback
    through MIDI-compatible instruments and software.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第13章中讨论的，音高值范围从0到127，每个值对应于八度中的一个半音。例如，音高值60对应于C4音符，而音高值74对应于D5音符。速度值，同样范围从0到127，表示音符的动态，较高的值表示更响亮或更有力的演奏。通过结合这些控制信息和速度值，MIDI序列可以捕捉现场表演的表达细节，允许通过兼容MIDI的乐器和软件进行表达性的回放。
- en: To give you a concrete example of how a piece of music can be represented by
    control messages and velocity values, consider the five notes shown in the following
    listing.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你一个具体的例子，说明如何通过控制信息和速度值表示一首音乐，请考虑以下列表中显示的五个音符。
- en: Listing 14.1 Example notes in a performance-based music representation
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.1 基于性能的音乐表示中的示例音符
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: These are the first five notes from a piece of music in the training dataset
    you’ll use in this chapter. The first note has a timestamp of approximately 1.03
    seconds, with a note of pitch value 74 (D5) starting to play at a velocity of
    86\. Looking at the second note, you can infer that after about 0.01 seconds (since
    the timestamp is now 1.04 seconds), a note with a pitch value of 38 starts to
    play at a velocity of 77, and so on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是训练数据集中你将在本章使用的音乐作品中的前五个音符。第一个音符的大致时间戳为1.03秒，音高值为74（D5）的音符以86的速度开始演奏。观察第二个音符，你可以推断出大约0.01秒后（因为时间戳现在是1.04秒），一个音高值为38的音符以77的速度开始演奏，以此类推。
- en: These musical notes are similar to raw text in NLP; we cannot directly feed
    them to a music Transformer to train the model. We first need to “tokenize” the
    notes and then convert the tokens to indexes before feeding them to the model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些音乐符号类似于自然语言处理中的原始文本；我们不能直接将它们输入到音乐Transformer中训练模型。我们首先需要将音符“令牌化”，然后将令牌转换为索引，再输入到模型中。
- en: To tokenize the musical notes, we’ll represent the music using increments of
    0.01 seconds to reduce the number of time steps in the music piece. Additionally,
    we’ll separate control messages from velocity values and treat them as different
    elements of the music piece. Specifically, we’ll represent music using a combination
    of note-on, note-off, time-shift, and velocity events. Once we do that, the preceding
    five musical notes can be represented by the following events (some events are
    omitted for brevity).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了令牌化音乐音符，我们将使用0.01秒的增量来表示音乐，以减少音乐作品中的时间步数。此外，我们将控制消息与速度值分开，并将它们视为音乐作品的独立元素。具体来说，我们将使用音符开启、音符关闭、时间偏移和速度事件来表示音乐。一旦这样做，前五个音符可以表示为以下事件（为了简洁，省略了一些事件）。
- en: Listing 14.2 Tokenized representation of a piece of music
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.2 音乐作品的令牌化表示
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We’ll count time shifts in increments of 0.01 seconds and tokenize time shifts
    from 0.01 seconds to 1 second with 100 different values. Thus, time-shift events
    are tokenized into 1 of 100 unique event tokens: a value of 0 indicates a time
    lapse of 0.01 seconds, 1 indicates a time lapse of 0.02 seconds, and so on, up
    to 99, which indicates a time lapse of 1 second. If a time-shift lasts more than
    1 second, you can use multiple time-shift tokens to indicate it. For example,
    the first two tokens in listing 14.2 are both time-shift tokens, with values 99
    and 2, respectively, indicating time lapses of 1 second and 0.03 seconds. This
    matches the timestamp of the first musical note in listing 14.1: 1.0326 seconds.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以0.01秒的增量计算时间偏移，并将从0.01秒到1秒的时间偏移以100个不同的值进行令牌化。因此，时间偏移事件被令牌化为100个独特的事件令牌：值为0表示0.01秒的时间间隔，值为1表示0.02秒的时间间隔，以此类推，直到99，表示1秒的时间间隔。如果一个时间偏移超过1秒，你可以使用多个时间偏移令牌来表示它。例如，列表14.2中的前两个令牌都是时间偏移令牌，值分别为99和2，分别表示1秒和0.03秒的时间间隔。这与列表14.1中第一个音乐音符的时间戳相匹配：1.0326秒。
- en: Listing 14.2 also shows that velocity is a separate type of musical event. We
    place the value of velocity into 32 equally spaced bins, converting the original
    velocity values, which range from 0 to 127, into 1 of 32 values, ranging from
    0 to 31\. This is why the original velocity value of 86 in the first note in listing
    14.1 is now represented as a velocity event with a value of 21 in listing 14.2
    (the number 86 falls into the 22^(nd) bin, and Python uses zero-based indexing).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.2也显示了速度是音乐事件的一种独立类型。我们将速度值放入32个等间隔的箱子中，将原始的速度值（范围从0到127）转换为32个值中的一个，范围从0到31。这就是为什么列表14.1中第一个音符的原始速度值86现在在列表14.2中表示为速度事件，值为21（因为86落在第22个箱子中，Python使用零基索引）。
- en: Table 14.1 shows the meaning of four types of different tokenized events, their
    value ranges, and the meaning of each event token.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.1显示了四种不同令牌化事件的意义、它们的值范围以及每个事件令牌的意义。
- en: Table 14.1 Meanings of different event tokens
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.1 不同事件令牌的意义
- en: '| Event token type | Event token value range | Meaning of the event tokens
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 事件令牌类型 | 事件令牌值范围 | 事件令牌的意义 |'
- en: '| --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `note_on` | 0–127 | Starting to play at a certain pitch value. For example,
    `note_on` with value 74 means starting to play note D5. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `note_on` | 0–127 | 在某个音高值处开始演奏。例如，值为74的`note_on`表示开始演奏D5音符。|'
- en: '| `note_off` | 0–127 | Releasing a certain note. For example, `note_off` with
    value 60 means to stop playing note C4. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| `note_off` | 0–127 | 释放某个音符。例如，值为60的`note_off`表示停止演奏C4音符。|'
- en: '| `time_shift` | 0–99 | The `time_shift` values are increments of 0.01 seconds.
    For example, 0 indicates 0.01 seconds, 2 indicates 0.03 seconds, and 99 indicates
    1 second. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `time_shift` | 0–99 | `time_shift`值是0.01秒的增量。例如，0表示0.01秒，2表示0.03秒，99表示1秒。|'
- en: '| `velocity` | 0–31 | The original velocity values are placed into 32 bins.
    The bin value is used. For example, an original velocity value of 86 now has a
    tokenized value of 21. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `velocity` | 0–31 | 原始速度值被放入32个箱子中。使用箱子值。例如，原始速度值为86现在有一个标记化值为21。|'
- en: Similar to the approach taken in NLP, we’ll convert each unique token into an
    index so that we can input the data into neural networks. According to table 14.1,
    there are 128 unique note-on event tokens, 128 note-off event tokens, 32 velocity
    event tokens, and 100 time-shift event tokens. This results in a total of 128
    + 128 + 32 + 100 = 388 unique tokens. Consequently, we convert these 388 unique
    tokens into indexes ranging from 0 to 387, based on the mappings provided in table
    14.2.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与NLP中采用的方法类似，我们将每个唯一的标记转换为索引，以便我们可以将数据输入到神经网络中。根据表14.1，有128个唯一的音符开启事件标记，128个音符关闭事件标记，32个速度事件标记和100个时间偏移事件标记。这导致总共有128
    + 128 + 32 + 100 = 388个唯一标记。因此，我们根据表14.2中提供的映射将这些388个唯一标记转换为从0到387的索引。
- en: Table 14.2 Mapping event tokens to indexes and indexes to event tokens
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.2事件标记到索引和索引到事件标记的映射
- en: '| Token type | Index range | Event token to index | Index to event token |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 标记类型 | 索引范围 | 事件标记到索引 | 索引到事件标记 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `note_on` | 0–127 | The value of the `note_on` token. For example, the `note_on`
    token with a value of 74 is assigned an index value of 74. | If the index range
    is 0 to 127, set token type to `note_on` and value to the index value. For example,
    the index value 63 is mapped to a `note_on` token with a value of 63. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `note_on` | 0–127 | `note_on`标记的值。例如，具有74个值的`note_on`标记被分配一个索引值为74。| 如果索引范围是0到127，将标记类型设置为`note_on`并将值设置为索引值。例如，索引值63映射到具有63个值的`note_on`标记。|'
- en: '| `note_off` | 128–255 | 128 plus the value of the `note_off` token. For example,
    the `note_off` token with a value of 60 is assigned an index value of 188 (since
    128 + 60 = 188). | If the index range is 128 to 255, set token type to `note_off`
    and value to index minus 128\. For example, index 180 is mapped to the `note_off`
    token with value 52. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| `note_off` | 128–255 | 128加上`note_off`标记的值。例如，具有60个值的`note_off`标记被分配一个索引值为188（因为128+60=188）。|
    如果索引范围是128到255，将标记类型设置为`note_off`并将值设置为索引减去128。例如，索引180映射到具有52个值的`note_off`标记。|'
- en: '| `time_shift` | 256–355 | 256 plus the value of the `time_shift` token. For
    example, the `time_shift` token with a value of 16 is assigned an index value
    of 272 (since 256 + 16 = 272). | If the index range is 256 to 355, set token type
    to `time_shift` and value to index minus 256\. For example, index 288 is mapped
    to the `time_shift` token with value 32. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| `time_shift` | 256–355 | 256加上`time_shift`标记的值。例如，具有16个值的`time_shift`标记被分配一个索引值为272（因为256+16=272）。|
    如果索引范围是256到355，将标记类型设置为`time_shift`并将值设置为索引减去256。例如，索引288映射到具有32个值的`time_shift`标记。|'
- en: '| `velocity` | 356–387 | 356 plus the value of the velocity token. For example,
    the velocity token with a value of 21 is assigned an index value of 377 (since
    356+21=377). | If the index range is 356 to 387, set token type to `velocity`
    and value to index minus 356\. For example, index 380 is mapped to the `velocity`
    token with value 24. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| `velocity` | 356–387 | 356加上速度标记的值。例如，具有21个值的速度标记被分配一个索引值为377（因为356+21=377）。|
    如果索引范围是356到387，将标记类型设置为`velocity`并将值设置为索引减去356。例如，索引380映射到具有24个值的`velocity`标记。|'
- en: The third column in table 14.2 outlines the conversion of event tokens to indexes.
    Note-on tokens are assigned index values ranging from 0 to 127, where the index
    value corresponds to the pitch number in the token. Note-off tokens are assigned
    index values from 128 to 255, with the index value being 128 plus the pitch number.
    Time-shift tokens are assigned index values from 256 to 355, with the index value
    being 256 plus the time-shift value. Lastly, velocity tokens are assigned index
    values from 356 to 387, with the index value being 356 plus the velocity bin number.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.2概述了事件标记到索引的转换。音符开启标记被分配从0到127的索引值，其中索引值对应于标记中的音高数。音符关闭标记被分配从128到255的索引值，索引值是128加上音高数。时间偏移标记被分配从256到355的索引值，索引值是256加上时间偏移值。最后，速度标记被分配从356到387的索引值，索引值是356加上速度箱子数。
- en: Using this token-to-index mapping, we’ll convert each piece of music into a
    sequence of indexes. We’ll apply this conversion to all music pieces in the training
    dataset and use the resulting sequences to train our music Transformer (the details
    of which will be explained later). Once trained, we’ll use the Transformer to
    generate music in the form of a sequence of indexes. The final step is to convert
    this sequence back into MIDI format so that we can play and enjoy the music on
    a computer.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种标记到索引的映射，我们将每首音乐转换成一系列索引。我们将对此训练数据集中的所有音乐作品应用此转换，并使用生成的序列来训练我们的音乐Transformer（其细节将在后面解释）。一旦训练完成，我们将使用Transformer以序列的形式生成音乐。最后一步是将此序列转换回MIDI格式，以便我们可以在计算机上播放和欣赏音乐。
- en: The last column in table 14.2 provides guidance on converting indexes back to
    event tokens. We first determine the token type based on the range in which the
    index falls. The four ranges in the second column of table 14.2 correspond to
    the four token types in the first column of the table. To obtain the value for
    each token type, we subtract the index value by 0, 128, 256, and 356 for the four
    types of tokens, respectively. These tokenized events are then converted into
    musical notes in MIDI format, ready to be played on a computer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.2的最后一列提供了将索引转换回事件标记的指导。我们首先根据索引所在的范围确定标记类型。表14.2的第二列中的四个范围对应于表的第一列中的四种标记类型。为了获得每种标记类型的值，我们将索引值分别减去0、128、256和356，分别对应四种类型的标记。然后，这些标记化的事件被转换为MIDI格式的音符，准备好在计算机上播放。
- en: 14.1.2 The music Transformer architecture
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.2 音乐Transformer架构
- en: In chapter 9, we built an encoder-decoder Transformer, and in chapters 11 and
    12, we focused on decoder-only Transformers. Unlike language translation tasks
    where the encoder captures the meaning of the source language and passes it to
    the decoder for generating the translation, music generation does not require
    an encoder to understand a different language. Instead, the model generates subsequent
    event tokens based on previous event tokens in the music sequence. Therefore,
    we’ll construct a decoder-only Transformer for our music generation task.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在第9章中，我们构建了一个编码器-解码器Transformer，在第11章和第12章中，我们专注于仅解码器Transformer。与编码器捕获源语言含义并将其传递给解码器以生成翻译的语言翻译任务不同，音乐生成不需要编码器理解不同的语言。相反，模型根据音乐序列中的先前事件标记生成后续事件标记。因此，我们将为我们的音乐生成任务构建一个仅解码器的Transformer。
- en: Our music Transformer, like other Transformer models, utilizes self-attention
    mechanisms to capture the long-range dependencies among different musical events
    in a piece of music, thereby generating coherent and lifelike music. Although
    our music Transformer differs in size from the GPT models we built in chapters
    11 and 12, it shares the same core architecture. It follows the same structural
    design as GPT-2 models but is significantly smaller, making it feasible to train
    without the need for supercomputing facilities.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的音乐Transformer，与其他Transformer模型一样，利用自注意力机制来捕捉音乐作品中不同音乐事件之间的长距离依赖关系，从而生成连贯且逼真的音乐。尽管我们的音乐Transformer在大小上与我们在第11章和第12章中构建的GPT模型不同，但它具有相同的核心架构。它遵循与GPT-2模型相同的结构设计，但尺寸显著更小，这使得在没有超级计算设施的情况下进行训练成为可能。
- en: Specifically, our music Transformer consists of 6 decoder layers with an embedding
    dimension of 512, meaning each token is represented by a 512-value vector after
    word embedding. Instead of using sine and cosine functions for positional encoding
    as in the original 2017 paper “Attention Is All You Need,” we use embedding layers
    to learn the positional encodings for different positions in a sequence. As a
    result, each position in a sequence is also represented by a 512-value vector.
    For calculating causal self-attention, we use 8 parallel attention heads to capture
    different aspects of the meanings of a token in the sequence, giving each attention
    head a dimension of 64 (512/8).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们的音乐Transformer由6个解码器层组成，嵌入维度为512，这意味着每个标记在词嵌入后都由一个512维的向量表示。与原始2017年论文“Attention
    Is All You Need”中使用正弦和余弦函数进行位置编码不同，我们使用嵌入层来学习序列中不同位置的位置编码。因此，序列中的每个位置也由一个512维的向量表示。为了计算因果自注意力，我们使用8个并行注意力头来捕捉序列中标记的不同含义，每个注意力头的维度为64（512/8）。
- en: Compared to the vocabulary size of 50,257 in GPT-2 models, our model has a much
    smaller vocabulary size of 390 (388 different event tokens, plus a token to signify
    the end of a sequence and a token to pad shorter sequences; I’ll explain later
    why padding is needed). This allows us to set the maximum sequence length in our
    music Transformer to 2,048, which is much longer than the maximum sequence length
    of 1,024 in GPT-2 models. This choice is necessary to capture the long-term relations
    of music notes in a sequence. With these hyperparameter values, our music Transformer
    has a size of 20.16 million parameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPT-2模型中50,257的词汇量相比，我们的模型具有更小的词汇量，为390（388个不同的事件标记，加上一个表示序列结束的标记和一个填充较短的序列的标记；我将在后面解释为什么需要填充）。这使得我们可以在音乐Transformer中将最大序列长度设置为2,048，这比GPT-2模型中的最大序列长度1,024长得多。这种选择是必要的，以便捕捉序列中音乐音符的长期关系。具有这些超参数值，我们的音乐Transformer具有1,016万参数。
- en: Figure 14.1 illustrates the architecture of the music Transformer we will create
    in this chapter. It is similar to the architecture of the GPT models you built
    in chapters 11 and 12\. Figure 14.1 also shows the size of the training data as
    it passes through the model during training.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1展示了本章我们将创建的音乐Transformer的架构。它与你在第11章和第12章中构建的GPT模型的架构相似。图14.1还显示了训练过程中数据通过模型时的大小。
- en: The input to the music Transformer we constructed comprises input embeddings,
    as depicted at the bottom of figure 14.1\. The input embedding is the sum of the
    word embedding and positional encoding of the input sequence. This input embedding
    is then passed sequentially through six decoder blocks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的音乐Transformer的输入包括输入嵌入，如图14.1底部所示。输入嵌入是输入序列的词嵌入和位置编码的总和。然后，这个输入嵌入依次通过六个解码器块。
- en: '![](../../OEBPS/Images/CH14_F01_Liu.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH14_F01_Liu.png)'
- en: Figure 14.1 The architecture of a music Transformer. Music files in MIDI formats
    are first converted into sequences of musical events. These events are then tokenized
    and converted into indexes. We organize these indexes into sequences of 2,048
    elements, and each batch contains 2 such sequences. The input sequence first undergoes
    word embedding and positional encoding; the input embedding is the sum of these
    two components. This input embedding is then processed through six decoder layers,
    each utilizing self-attention mechanisms to capture the relationships among different
    musical events in the sequence. After passing through the decoder layers, the
    output undergoes layer normalization to ensure stability in the training process.
    It then passes through a linear layer, resulting in an output size of 390, which
    corresponds to the number of unique tokens in the vocabulary. This final output
    represents the predicted logits for the next musical event in the sequence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1展示了音乐Transformer的架构。MIDI格式的音乐文件首先被转换为音乐事件的序列。这些事件随后被标记化并转换为索引。我们将这些索引组织成2,048个元素的序列，每个批次包含2个这样的序列。输入序列首先进行词嵌入和位置编码；输入嵌入是这两个组件的总和。然后，这个输入嵌入通过六个解码器层进行处理，每个层都利用自注意力机制来捕捉序列中不同音乐事件之间的关系。经过解码器层后，输出经过层归一化以确保训练过程中的稳定性。然后，它通过一个线性层，输出大小为390，这对应于词汇表中的独特标记数量。这个最终输出代表了序列中下一个音乐事件的预测对数几率。
- en: 'As discussed in chapters 11 and 12, each decoder layer consists of two sublayers:
    a causal self-attention layer and a feed-forward network. In addition, we apply
    layer normalization and residual connections to each sublayer to enhance the model’s
    stability and learning capability.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如第11章和第12章所述，每个解码器层由两个子层组成：一个因果自注意力层和一个前馈网络。此外，我们对每个子层应用层归一化和残差连接，以增强模型稳定性和学习能力。
- en: After passing through the decoder layers, the output undergoes layer normalization
    and is then fed into a linear layer. The number of outputs in our model corresponds
    to the number of unique musical event tokens in the vocabulary, which is 390\.
    The output of the model is the logits for the next musical event token.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 经过解码器层后，输出经过层归一化，然后输入到一个线性层。我们模型中的输出数量对应于词汇表中的独特音乐事件标记的数量，即390。模型的输出是下一个音乐事件标记的对数几率。
- en: Later, we will apply the softmax function to these logits to obtain the probability
    distribution over all possible event tokens. The model is designed to predict
    the next event token based on the current token and all previous tokens in the
    music sequence, enabling it to generate coherent and musically sensible sequences.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将应用 softmax 函数到这些 logits 上，以获得所有可能事件标记的概率分布。该模型被设计用来根据当前标记和音乐序列中所有之前的标记来预测下一个事件标记，使其能够生成连贯且音乐上合理的序列。
- en: 14.1.3 Training the music Transformer
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.3 训练音乐 Transformer
- en: Now that we understand how to construct a music Transformer for music generation,
    let’s outline the training process for the music Transformer.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了如何构建用于音乐生成的音乐 Transformer，那么让我们概述一下音乐 Transformer 的训练过程。
- en: The style of the music generated by the model is influenced by the music pieces
    used for training. We’ll use piano performances from Google’s Magenta group to
    train our model. Figure 14.2 illustrates the steps involved in training the Transformer
    for music generation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 模型生成的音乐风格受用于训练的音乐作品的影响。我们将使用 Google 的 Magenta 团队的钢琴表演来训练我们的模型。图 14.2 说明了训练音乐生成
    Transformer 所涉及的步骤。
- en: '![](../../OEBPS/Images/CH14_F02_Liu.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.2 音乐 Transformer 生成音乐的训练过程](../../OEBPS/Images/CH14_F02_Liu.png)'
- en: Figure 14.2 The training process for a music Transformer to generate music
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2 音乐 Transformer 生成音乐的训练过程
- en: Similar to the approach we’ve taken in NLP tasks, the first step in the training
    process for our music Transformer is to convert the raw training data into a numerical
    form so that it can be fed into the model. Specifically, we start by converting
    MIDI files in the training set into sequences of musical notes. We then further
    tokenize these notes by converting them into 1 of 388 unique events/tokens. After
    tokenization, we assign a unique index (i.e., an integer) to each token, converting
    the music pieces in the training set into sequences of integers (see step 1 in
    figure 14.2).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在 NLP 任务中采取的方法类似，我们音乐 Transformer 训练过程中的第一步是将原始训练数据转换为数值形式，以便将其输入到模型中。具体来说，我们首先将训练集中的
    MIDI 文件转换为音乐音符序列。然后，我们将这些音符进一步标记化，通过将它们转换为 388 个独特的事件/标记中的 1 个。标记化后，我们为每个标记分配一个唯一的索引（即一个整数），将训练集中的音乐作品转换为整数序列（见图
    14.2 中的步骤 1）。
- en: Next, we transform the sequence of integers into training data by dividing this
    sequence into sequences of equal length (step 2 in figure 14.2). We allow a maximum
    length of 2,048 indexes in each sequence. The choice of 2,048 enables us to capture
    long-range dependencies among musical events in a music sequence to create lifelike
    music. These sequences form the features (the x variable) of our model. As we
    did in previous chapters when training GPT models to generate text, we slide the
    input sequence window one index to the right and use it as the output in the training
    data (the y variable; step 3 in figure 14.2). Doing so forces our model to predict
    the next music token in a sequence based on the current token and all previous
    tokens in the music sequence.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将整数序列转换为训练数据，通过将此序列划分为等长的序列（见图 14.2 中的步骤 2）。我们允许每个序列中最多有 2,048 个索引。选择
    2,048 允许我们捕捉音乐序列中音乐事件之间的长距离依赖关系，以创建逼真的音乐。这些序列形成我们模型的特征（x 变量）。正如我们在前几章中训练 GPT 模型生成文本时所做的，我们将输入序列窗口向右滑动一个索引，并将其用作训练数据中的输出（y
    变量；见图 14.2 中的步骤 3）。这样做迫使我们的模型根据音乐序列中的当前标记和所有之前的标记来预测序列中的下一个音乐标记。
- en: 'The input and output pairs serve as the training data (x, y) for the music
    Transformer. During training, you will iterate through the training data. In the
    forward passes, you feed the input sequence x through the music Transformer (step
    4). The music Transformer then makes a prediction based on the current parameters
    in the model (step 5). You compute the cross-entropy loss by comparing the predicted
    next tokens with the output obtained from step 3\. In other words, you compare
    the model’s prediction with the ground truth (step 6). Finally, you will adjust
    the parameters in the music Transformer so that in the next iteration, the model’s
    predictions move closer to the actual output, minimizing the cross-entropy loss
    (step 7). The model is essentially performing a multicategory classification problem:
    it’s predicting the next token from all unique music tokens in the vocabulary.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出对作为音乐 Transformer 的训练数据（x, y）。在训练过程中，你将遍历训练数据。在前向传递中，你将输入序列 x 通过音乐 Transformer（步骤
    4）。音乐 Transformer 然后根据模型中的当前参数进行预测（步骤 5）。你通过比较预测的下一个标记与步骤 3 获得的输出来计算交叉熵损失。换句话说，你将模型的预测与真实值（步骤
    6）进行比较。最后，你将调整音乐 Transformer 中的参数，以便在下一个迭代中，模型的预测更接近实际输出，最小化交叉熵损失（步骤 7）。该模型本质上是在执行一个多类别分类问题：它从词汇表中的所有独特音乐标记中预测下一个标记。
- en: You will repeat steps 3 to 7 through many iterations. After each iteration,
    the model parameters are adjusted to improve the prediction of the next token.
    This process will be repeated for 50 epochs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你将通过多次迭代重复步骤 3 到 7。在每次迭代后，模型参数都会调整以改善下一个标记的预测。这个过程将重复进行 50 个 epoch。
- en: To generate a new piece of music with the trained model, we obtain a music piece
    from the test set, tokenize it, and convert it to a long sequence of indexes.
    We’ll use the first, say, 250 indexes as the prompt (200 or 300 will lead to similar
    results). We then ask the trained music Transformer to generate new indexes until
    the sequence reaches a certain length (say, 1,000 indexes). We then convert the
    sequence of indexes back into a MIDI file to be played on your computer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用训练好的模型生成新的音乐作品，我们从测试集中获取一个音乐作品，对其进行标记化，并将其转换为一系列长索引。我们将使用前 250 个索引作为提示（200
    或 300 个索引将产生类似的结果）。然后，我们要求训练好的音乐 Transformer 生成新的索引，直到序列达到一定长度（例如，1,000 个索引）。然后，我们将索引序列转换回
    MIDI 文件，以便在您的计算机上播放。
- en: 14.2 Tokenizing music pieces
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2 音乐作品的标记化
- en: 'Having grasped the structure of the music Transformer and its training methodology,
    we’ll start with the first step: tokenization and indexing of the musical compositions
    in our training dataset.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了音乐 Transformer 的结构和其训练方法之后，我们将从第一步开始：对训练数据集中的音乐作品进行标记化和索引。
- en: We’ll begin with employing a performance-based representation (as explained
    in the first section) to portray music pieces as musical notes, akin to raw text
    in NLP. After that, we’ll divide these musical notes into a series of events,
    similar to tokens in NLP. Each unique event will be assigned a different index.
    Utilizing this mapping, we’ll transform all music pieces in the training dataset
    into sequences of indexes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用基于性能的表示（如第一部分所述）来表示音乐作品为音符，类似于自然语言处理中的原始文本。之后，我们将这些音符划分为一系列事件，类似于自然语言处理中的标记。每个独特的事件将被分配一个不同的索引。利用这个映射，我们将训练数据集中的所有音乐作品转换为索引序列。
- en: Next, we’ll standardize these sequences of indexes into a fixed length, specifically
    sequences with 2,048 indexes, and use them as the feature inputs (x). By shifting
    the window one index to the right, we’ll generate the corresponding output sequences
    (y). We’ll then group pairs of input and output (x, y) into batches, preparing
    them for training the music Transformer later in the chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将这些索引序列标准化为固定长度，具体为 2,048 个索引的序列，并将它们用作特征输入（x）。通过将窗口向右移动一个索引，我们将生成相应的输出序列（y）。然后，我们将输入和输出（x,
    y）对分组成批次，为章节后面的音乐 Transformer 训练做准备。
- en: 'As we’ll require the `pretty_midi` and `music21` libraries for processing MIDI
    files, execute the following line of code in a new cell in the Jupyter Notebook
    application:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要 `pretty_midi` 和 `music21` 库来处理 MIDI 文件，请在 Jupyter Notebook 应用程序的新单元格中执行以下代码行：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 14.2.1 Downloading training data
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.1 下载训练数据
- en: We’ll obtain the piano performances from the MAESTRO dataset, which is made
    available by Google’s Magenta group ([https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip](https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip))
    and download the ZIP file. After downloading, unzip it and move the resulting
    folder, /maestro-v2.0.0/, into the /files/ directory on your computer.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从由谷歌Magenta团队提供的MAESTRO数据集中获取钢琴演奏，该数据集可在[https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip](https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip)获得，并下载ZIP文件。下载后，解压缩它，并将生成的文件夹/maestro-v2.0.0/移动到您计算机上的/files/目录中。
- en: Ensure that the /maestro-v2.0.0/ folder contains 4 files (one of which should
    be named “maestro-v2.0.0.json”) and 10 subfolders. Each subfolder should contain
    more than 100 MIDI files. To familiarize yourself with the sound of the music
    pieces in the training data, try opening some of the MIDI files with your preferred
    music player.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 确保/maestro-v2.0.0/文件夹包含4个文件（其中一个应命名为“maestro-v2.0.0.json”）和10个子文件夹。每个子文件夹应包含超过100个MIDI文件。为了熟悉训练数据中音乐片段的声音，尝试使用您喜欢的音乐播放器打开一些MIDI文件。
- en: 'Next, we’ll split the MIDI files into train, validation, and test subsets.
    To start, create three subfolders within /files/maestro-v2.0.0/:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将MIDI文件分成训练、验证和测试子集。首先，在/files/maestro-v2.0.0/目录下创建三个子文件夹：
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To facilitate the processing of MIDI files, visit Kevin Yang’s GitHub repository
    at [https://github.com/jason9693/midi-neural-processor](https://github.com/jason9693/midi-neural-processor),
    download the processor.py file, and place it in the /utils/ folder on your computer.
    Alternatively, you can obtain the file from the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).
    We’ll use this file as a local module to transform a MIDI file into a sequence
    of indexes and vice versa. This approach allows us to concentrate on developing,
    training, and utilizing a music Transformer without getting bogged down in the
    details of music format conversion. At the same time, I’ll provide a simple example
    of how this process works so that you can convert between a MIDI file and a sequence
    of indexes yourself using the module.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便处理MIDI文件，访问凯文·杨的GitHub仓库[https://github.com/jason9693/midi-neural-processor](https://github.com/jason9693/midi-neural-processor)，下载processor.py文件，并将其放置在您计算机上的/utils/文件夹中。或者，您也可以从本书的GitHub仓库[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)中获取该文件。我们将使用此文件作为本地模块，将MIDI文件转换为一系列索引，反之亦然。这种方法使我们能够专注于开发、训练和利用音乐Transformer，而无需陷入音乐格式转换的细节。同时，我将提供一个简单的示例，说明这个过程是如何工作的，这样您就可以使用该模块自己将MIDI文件和一系列索引之间进行转换。
- en: Additionally, you need to download the ch14util.py file from the book’s GitHub
    repository and place it in the /utils/ directory on your computer. We’ll use the
    ch14util.py file as another local module to define the music Transformer model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还需要从本书的GitHub仓库下载ch14util.py文件，并将其放置在您计算机上的/utils/目录中。我们将使用ch14util.py文件作为另一个本地模块来定义音乐Transformer模型。
- en: The file maestro-v2.0.0.json within the /maestro-v2.0.0/ folder contains the
    names of all MIDI files and their designated subsets (train, validation, or test).
    Based on this information, we’ll categorize the MIDI files into three corresponding
    subfolders.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: /maestro-v2.0.0/文件夹中的maestro-v2.0.0.json文件包含所有MIDI文件及其指定的子集（训练、验证或测试）。基于这些信息，我们将MIDI文件分类到三个相应的子文件夹中。
- en: Listing 14.3 Splitting training data into train, validation, and test subsets
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.3 将训练数据分割为训练、验证和测试子集
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Loads JSON file
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载JSON文件
- en: ② Iterates through all files in the training data
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ② 遍历训练数据中的所有文件
- en: ③ Places a file in train, validation, or test subfolder based on instructions
    in the JSON file
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 根据JSON文件中的说明，将文件放置在训练、验证或测试子文件夹中
- en: 'The JavaScript object notation (JSON) file you downloaded categorizes each
    file in the training dataset into one of three subsets: train, validation, and
    test. After executing the previous code listing, if you explore the /train/, /val/,
    and /test/ folders on your computer, you should find numerous files in each. To
    verify the number of files in each of these three folders, you can perform the
    following checks:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您下载的JavaScript对象表示法（JSON）文件将训练数据集中的每个文件分类到三个子集之一：训练、验证和测试。在执行前面的代码列表后，如果您在计算机上的/train/、/val/和/test/文件夹中查看，您应该在每个文件夹中找到许多文件。为了验证这三个文件夹中每个文件夹的文件数量，您可以执行以下检查：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output from the preceding code block is
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码块输出的结果是
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Results show that there are 967, 137, and 178 pieces of music in the train,
    validation, and test subsets, respectively.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，训练、验证和测试子集中分别有967、137和178首音乐作品。
- en: 14.2.2 Tokenizing MIDI files
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.2 MIDI文件标记化
- en: Next, we’ll represent each MIDI file as a sequence of musical notes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将每个MIDI文件表示为一串音乐音符。
- en: Listing 14.4 Converting a MIDI file to a sequence of music notes
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.4 将MIDI文件转换为音乐音符序列
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Selects a MIDI file from the training dataset
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从训练数据集中选择一个MIDI文件
- en: ② Extracts musical events from the music
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从音乐中提取音乐事件
- en: ③ Places all musical events in the list dnotes
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将所有音乐事件放入列表dnotes中
- en: We have selected one MIDI file from the training dataset and used the processor.py
    local module to convert it into a sequence of musical notes. The output from the
    preceding code listing is
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从训练数据集中选择了一个MIDI文件，并使用processor.py本地模块将其转换为音乐音符序列。前面代码列表的输出如下：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The output displayed here shows the first five musical notes from the MIDI file.
    You might have observed that the time representation in the output is continuous.
    Certain musical notes contain both a `note_on` and a `velocity` attribute, complicating
    the tokenization process due to the vast number of unique musical events resulting
    from the continuous nature of time representation. Additionally, the combination
    of different `note_on` and `velocity` values is large (each can assume 128 distinct
    values, ranging from 0 to 127), leading to an excessively large vocabulary size.
    This, in turn, would render training impractical.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示的输出显示了MIDI文件中的前五个音符。你可能已经注意到输出中的时间表示是连续的。某些音符同时包含`note_on`和`velocity`属性，由于时间表示的连续性，这会导致大量独特的音乐事件，从而复杂化了标记过程。此外，不同的`note_on`和`velocity`值的组合很大（每个都可以假设128个不同的值，范围从0到127），导致词汇表大小过大。这反过来又会使得训练变得不切实际。
- en: 'To mitigate this problem and decrease the vocabulary size, we further convert
    these musical notes into tokenized events:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这个问题并减小词汇表大小，我们进一步将这些音乐音符转换为标记化事件：
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Discretizes time to reduce the number of unique events
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将时间离散化以减少独特事件的数目
- en: ② Converts musical notes to events
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将音乐音符转换为事件
- en: ③ Prints out the first 15 events
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 打印出前15个事件
- en: 'The output is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The music piece is now represented by four types of events: note-on, note-off,
    time-shift, and velocity. Each event type includes different values, resulting
    in a total of 388 unique events, as detailed in table 14.2 earlier. The specifics
    of converting a MIDI file into a sequence of such unique events are not essential
    for constructing and training a music Transformer. Therefore, we will not dive
    deeply into this topic; interested readers can refer to Huang et al (2018) cited
    earlier. All you need to know is how to use the processor.py module to transform
    a MIDI file into a sequence of indexes and vice versa. In the following subsection,
    you’ll learn how to accomplish that.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐作品现在由四种类型的事件表示：音符开启、音符关闭、时间移动和速度。每种事件类型包含不同的值，总共产生388个独特的事件，如前面表格14.2中详细说明。将MIDI文件转换为这种独特事件序列的具体细节对于构建和训练音乐Transformer不是必要的。因此，我们不会深入探讨这个话题；感兴趣的读者可以参考前面提到的Huang等人（2018）的研究。你需要知道的是如何使用processor.py模块将MIDI文件转换为索引序列，反之亦然。在下面的子节中，你将学习如何完成这个任务。
- en: 14.2.3 Preparing the training data
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.3 准备训练数据
- en: We’ve learned how to convert music pieces into tokens and then into indexes.
    The next step involves preparing the training data so that we can utilize it to
    train the music Transformer later in this chapter. To achieve this, we define
    the `create`_`xys()` function shown in the following listing.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何将音乐作品转换为标记，然后转换为索引。下一步涉及准备训练数据，以便我们可以在本章后面利用它来训练音乐Transformer。为了实现这一点，我们定义了以下列表中所示的`create_xys()`函数。
- en: Listing 14.5 Creating training data
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.5 创建训练数据
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Creates (x, y) sequences, with equal lengths of 2,048 indexes and sets index
    399 as the padding index
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建长度为2,048个索引的(x, y)序列，并将索引399设置为填充索引
- en: ② Uses a sequence of up to 2,048 indexes as input
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用最多2,048个索引的序列作为输入
- en: ③ Slides the window one index to the right and uses it as the output
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将窗口向右滑动一个索引，并使用它作为输出
- en: ④ Sets the end index as 388
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 设置结束索引为388
- en: As we’ve seen repeatedly throughout this book, in sequence prediction tasks,
    we use a sequence x as input. We then shift the sequence one position to the right
    to create the output sequence. This approach compels the model to predict the
    next element based on the current element and all preceding elements in the sequence.
    To prepare training data for our music Transformer, we’ll construct pairs (x,
    y), where x is the input and y is the output. Both x and y contain 2,048 indexes—long
    enough to capture the long-term relations of music notes in a sequence but not
    too long to hinder the training process.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在整本书中反复看到的那样，在序列预测任务中，我们使用一个序列x作为输入。然后我们将序列向右移动一个位置以创建输出序列。这种方法迫使模型根据序列中的当前元素和所有前面的元素来预测下一个元素。为了为我们的音乐Transformer准备训练数据，我们将构建(x,
    y)对，其中x是输入，y是输出。x和y都包含2,048个索引——足够长以捕捉序列中音乐音符的长期关系，但又不至于太长而阻碍训练过程。
- en: We’ll iterate through all the music pieces in the training dataset we downloaded.
    If a music piece exceeds 2,048 indexes in length, we’ll use the first 2,048 indexes
    as input x. For the output y, we’ll use indexes from the second position to the
    2,049th position. In the rare case where the music piece is less than or equal
    to 2,048 indexes long, we’ll pad the sequence with index 389 to ensure that both
    x and y are 2,048 indexes long. Additionally, we use index 388 to signal the end
    of the sequence y.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遍历下载的训练数据集中的所有音乐作品。如果一个音乐作品的长度超过2,048个索引，我们将使用前2,048个索引作为输入x。对于输出y，我们将使用从第二个位置到第2,049个位置的索引。在音乐作品长度小于或等于2,048个索引的罕见情况下，我们将使用索引389填充序列，以确保x和y的长度都是2,048个索引。此外，我们使用索引388来表示序列y的结束。
- en: As mentioned in the first section, there are a total of 388 unique event tokens,
    indexed from 0 to 387\. Since we use 388 to signal the end of the y sequence and
    389 to pad sequences, we have a total of 390 unique indexes, ranging from 0 to
    389.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如第一部分所述，总共有388个独特的事件标记，索引从0到387。由于我们使用388来表示y序列的结束，并使用389来填充序列，因此总共有390个独特的索引，范围从0到389。
- en: 'We can now apply the `create_xys()` function to the train subset:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将`create_xys()`函数应用于训练子集：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The output is
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This shows that out of the 967 music pieces in the train subset, only 5 are
    shorter than 2,048 indexes. Their lengths are shown in the previous output.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明在训练子集中的967首音乐作品中，只有5首的长度小于2,048个索引。它们的长度在之前的输出中显示。
- en: 'We also apply the `create_xys()` function to the validation and test subsets:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还把`create_xys()`函数应用于验证和测试子集：
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The output is
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This shows that all music pieces in the validation subset are longer than 2,048
    indexes. Only one music piece in the test subset is shorter than 2,048 indexes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明验证子集中的所有音乐作品长度都超过2,048个索引。测试子集中只有一首音乐作品的长度小于2,048个索引。
- en: 'Let’s print out a file from the validation subset and see what it looks like:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出验证子集中的一份文件，看看它是什么样子：
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The x sequence from the first pair in the validation set has a length of 2,048
    indexes, with values such as 324, 367, and so on. Let’s use the module `processor.py`
    to decode the sequence to a MIDI file so that you can hear what it sounds like:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集第一对中的x序列长度为2,048个索引，具有诸如324、367等值。让我们使用`processor.py`模块来解码序列到一个MIDI文件，这样你就可以听到它的声音：
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `decode_midi()` function converts a sequence of indexes into a MIDI file,
    playable on your computer. After running the preceding code block, open the file
    val1.midi with a music player on your computer to hear what it sounds like.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`decode_midi()`函数将索引序列转换为MIDI文件，可以在你的电脑上播放。在运行前面的代码块后，用电脑上的音乐播放器打开val1.midi文件，听听它的声音。'
- en: Exercise 14.1
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 练习14.1
- en: Use the `decode_midi()` function from the processor.py local module to convert
    the first music piece in the train subset into a MIDI file. Save it as train1.midi
    on your computer. Open it with a music player on your computer and get a sense
    of what type of music we use for training data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`processor.py`本地模块中的`decode_midi()`函数将训练子集中的第一首音乐作品转换为MIDI文件。将其保存为train1.midi到你的电脑上。用电脑上的音乐播放器打开它，感受一下我们用于训练数据类型的音乐。
- en: 'Finally, we create a data loader so that the data are in batches for training:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建一个数据加载器，以便数据以批次的格式进行训练：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To prevent your GPU from running out of memory, we’ll use a batch size of 2,
    as we’ve created very long sequences, each comprising 2,048 indexes. If needed,
    reduce the batch size to one or switch to CPU training.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止您的GPU内存耗尽，我们将使用2个批处理大小，因为我们创建了非常长的序列，每个序列包含2,048个索引。如果需要，可以将批处理大小减少到1或切换到CPU训练。
- en: With that, our training data is prepared. In the next two sections, we’ll construct
    a music Transformer from scratch and then train it using the training data we’ve
    just prepared.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们的训练数据已经准备好了。在接下来的两个部分中，我们将从头开始构建一个音乐Transformer，然后使用我们刚刚准备好的训练数据进行训练。
- en: 14.3 Building a GPT to generate music
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3 构建用于生成音乐的GPT
- en: Now that our training data is prepared, we’ll construct a GPT model from scratch
    for music generation. The architecture of this model will be similar to the GPT-2XL
    model we developed in chapter 11 and the text generator from chapter 12\. However,
    the size of our music Transformer will differ due to the specific hyperparameters
    we select.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了训练数据，我们将从头开始构建一个用于音乐生成的GPT模型。这个模型的架构将与我们在第11章中开发的GPT-2XL模型和第12章中的文本生成器相似。然而，由于我们选择的特定超参数，我们的音乐Transformer的大小将有所不同。
- en: To conserve space, we’ll place the model construction within the local module
    ch14util.py. Our focus here will be on the hyperparameters chosen for the music
    Transformer. Specifically, we’ll decide the values of `n_layer,` the number of
    decoder layers in the model; `n_head`, the number of parallel heads to use to
    calculate causal self-attention; `n_embd,` the embedding dimension; and `block_size,`
    the number of tokens in the input sequence.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省空间，我们将模型构建放在本地模块ch14util.py中。在这里，我们的重点是音乐Transformer选择使用的超参数。具体来说，我们将决定`n_layer`的值，即模型中解码器的层数；`n_head`，用于计算因果自注意力的并行头数；`n_embd`，嵌入维度；以及`block_size`，输入序列中的标记数。
- en: 14.3.1 Hyperparameters in the music Transformer
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.1 音乐Transformer中的超参数
- en: Open the file ch14util.py that you downloaded earlier from the book’s GitHub
    repository. Inside, you’ll find several functions and classes that are identical
    to those defined in chapter 12.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 打开您之前从本书的GitHub仓库下载的文件ch14util.py。在里面，您会发现几个函数和类，它们与第12章中定义的完全相同。
- en: As in all GPT models we have seen in this book, the feed-forward network in
    the decoder block utilizes the Gaussian error linear unit (GELU) activation function.
    Consequently, we define a GELU class in ch14util.py, exactly as we did in chapter
    12.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书中我们看到的所有GPT模型一样，解码器块中的前馈网络使用高斯误差线性单元（GELU）激活函数。因此，我们在ch14util.py中定义了一个GELU类，这与我们在第12章中做的一样。
- en: 'We employ a `Config()` class to store all the hyperparameters used in the music
    Transformer:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个`Config()`类来存储音乐Transformer中使用的所有超参数：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The attributes within the `Config()` class serve as hyperparameters for our
    music Transformer. We assign a value of 6 to the `n_layer` attribute, indicating
    that our music Transformer consists of 6 decoder layers. This is more than the
    number of decoder layers in the GPT model we built in chapter 12\. Each decoder
    layer processes the input sequence and introduces a level of abstraction or representation.
    As the information traverses through more layers, the model is capable of capturing
    more complex patterns and relationships in the data. This depth is crucial for
    our music Transformer to comprehend and generate intricate music pieces.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`Config()`类中的属性作为我们音乐Transformer的超参数。我们将`n_layer`属性的值设置为6，表示我们的音乐Transformer由6个解码器层组成。这比我们在第12章中构建的GPT模型中的解码器层数要多。每个解码器层处理输入序列并引入一个抽象或表示的层次。随着信息穿越更多层，模型能够捕捉到数据中更复杂的模式和关系。这种深度对于我们的音乐Transformer理解并生成复杂的音乐作品至关重要。'
- en: 'The `n_head` attribute is set to 8, signifying that we will divide the query
    Q, key K, and value V vectors into eight parallel heads during the computation
    of causal self-attention. The `n_embd` attribute is set to 512, indicating an
    embedding dimension of 512: each event token will be represented by a vector of
    512 values. The `vocab_size` attribute is determined by the number of unique tokens
    in the vocabulary, which is 390\. As explained earlier, there are 388 unique event
    tokens, and we added 1 token to signify the end of the sequence and another token
    to pad shorter sequences so that all sequences have a length of 2,048\. The `block_size`
    attribute is set to 2,048, indicating that the input sequence contains a maximum
    of 2,048 tokens. We set the dropout rates to 0.1, as in chapters 11 and 12.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_head`属性设置为8，表示在计算因果自注意力时，我们将查询Q、键K和值V向量分为八个并行头。`n_embd`属性设置为512，表示嵌入维度为512：每个事件标记将由一个包含512个值的向量表示。`vocab_size`属性由词汇表中的唯一标记数量确定，为390。如前所述，有388个唯一的事件标记，我们添加了1个标记来表示序列的结束，并添加了另一个标记来填充较短的序列，以便所有序列的长度都为2,048。`block_size`属性设置为2,048，表示输入序列最多包含2,048个标记。我们将dropout比率设置为0.1，与第11章和第12章相同。'
- en: Like all Transformers, our music Transformer employs self-attention mechanisms
    to capture relationships among different elements in a sequence. Consequently,
    we define a `CausalSelfAttention()` class in the local module ch14util, which
    is identical to the `CausalSelfAttention()` class defined in chapter 12\.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有Transformer一样，我们的音乐Transformer使用自注意力机制来捕捉序列中不同元素之间的关系。因此，我们在本地模块ch14util中定义了一个`CausalSelfAttention()`类，它与第12章中定义的`CausalSelfAttention()`类相同。
- en: 14.3.2 Building a music Transformer
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.2 构建音乐Transformer
- en: We combine a feed-forward network with the causal self-attention sublayer to
    form a decoder block (i.e., a decoder layer). We apply layer normalization and
    a residual connection to each sublayer for improved stability and performance.
    To this end, we define a `Block()` class in the local module to create a decoder
    block, which is identical to the `Block()` class we defined in chapter 12.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将前馈网络与因果自注意力子层结合形成一个解码块（即解码层）。我们对每个子层应用层归一化和残差连接以提高稳定性和性能。为此，我们在本地模块中定义了一个`Block()`类来创建解码块，它与我们在第12章中定义的`Block()`类相同。
- en: We then stack six decoder blocks on top of each other to form the main body
    of our music Transformer. To achieve this, we define a `Model()` class in the
    local module. As in all GPT models we have seen in this book, we use learned positional
    encoding by employing the `Embedding()` class in PyTorch, instead of the fixed
    positional encoding in the original 2017 paper “Attention Is All You Need.” Refer
    to chapter 11 on the differences between the two positional encoding methods.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在音乐Transformer的上方堆叠六个解码块，形成其主体。为了实现这一点，我们在本地模块中定义了一个`Model()`类。正如我们在本书中看到的所有GPT模型一样，我们使用通过PyTorch中的`Embedding()`类学习到的位置编码，而不是原始2017年论文“Attention
    Is All You Need”中的固定位置编码。有关两种位置编码方法之间的差异，请参阅第11章。
- en: The input to the model consists of sequences of indexes corresponding to musical
    event tokens in the vocabulary. We pass the input through word embedding and positional
    encoding and add the two to form the input embedding. The input embedding then
    goes through the six decoder layers. After that, we apply layer normalization
    to the output and attach a linear head to it so that the number of outputs is
    390, the size of the vocabulary. The outputs are the logits corresponding to the
    390 tokens in the vocabulary. Later, we’ll apply the softmax activation function
    to the logits to obtain the probability distribution over the unique music tokens
    in the vocabulary when generating music.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输入由对应于词汇表中音乐事件标记的索引序列组成。我们将输入通过词嵌入和位置编码传递，并将两者相加以形成输入嵌入。然后，输入嵌入通过六个解码层。之后，我们对输出应用层归一化，并将其连接到一个线性头，以便输出的数量为390，即词汇表的大小。输出是词汇表中390个标记的logits。稍后，我们将对logits应用softmax激活函数，以获得生成音乐时词汇表中唯一音乐标记的概率分布。
- en: 'Next, we’ll create our music Transformer by instantiating the `Model()` class
    we defined in the local module:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过实例化我们在本地模块中定义的`Model()`类来创建我们的音乐Transformer：
- en: '[PRE21]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The output is
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Our music Transformer consists of 20.16 million parameters, a figure substantially
    smaller than the GPT-2XL, which boasts over 1.5 billion parameters. Nonetheless,
    our music Transformer surpasses the size of the text generator we constructed
    in chapter 12, which contains only 5.12 million parameters. Despite these differences,
    all three models are based on the decoder-only Transformer architecture. The variations
    lie solely in the hyperparameters, such as the embedding dimension, number of
    decoder layers, vocabulary size, and so on.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的音乐 Transformer 由 2016 万个参数组成，这个数字比拥有超过 15 亿个参数的 GPT-2XL 小得多。尽管如此，我们的音乐 Transformer
    的规模超过了我们在第 12 章中构建的仅包含 512 万个参数的文本生成器。尽管存在这些差异，所有三个模型都是基于仅解码器 Transformer 架构。差异仅在于超参数，如嵌入维度、解码器层数、词汇量大小等。
- en: 14.4 Training and using the music Transformer
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4 训练和使用音乐 Transformer
- en: In this section, you’ll train the music Transformer you’ve just constructed
    using the batches of training data we prepared earlier in this chapter. To expedite
    the process, we’ll train the model for 100 epochs and then stop the training process.
    For those interested, you can utilize the validation set to determine when to
    stop training, based on the performance of the model on the validation set, as
    we did in chapter 2.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将使用本章前期准备好的训练数据批次来训练您刚刚构建的音乐 Transformer。为了加快过程，我们将对模型进行100个周期的训练，然后停止训练过程。对于感兴趣的人来说，您可以使用验证集来确定何时停止训练，根据模型在验证集上的性能，就像我们在第2章中所做的那样。
- en: Once the model is trained, we’ll provide it with a prompt in the form of a sequence
    of indexes. We’ll then request the trained music Transformer to generate the next
    index. This new index is appended to the prompt, and the updated prompt is fed
    back into the model for another prediction. This process is repeated iteratively
    until the sequence reaches a certain length.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们将以一系列索引的形式提供给它一个提示。然后，我们将请求训练好的音乐 Transformer 生成下一个索引。这个新的索引被附加到提示中，更新的提示被送回模型进行另一个预测。这个过程会迭代重复，直到序列达到一定的长度。
- en: Unlike the music generated in chapter 13, we can control the creativity of the
    music piece by applying different temperatures.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与第13章中生成的音乐不同，我们可以通过应用不同的温度来控制音乐作品的创造性。
- en: 14.4.1 Training the music Transformer
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4.1 训练音乐 Transformer
- en: 'As always, we’ll use the Adam optimizer for training. Given that our music
    Transformer is essentially executing a multicategory classification task, we’ll
    utilize cross-entropy loss as our loss function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们将使用 Adam 优化器进行训练。鉴于我们的音乐 Transformer 实质上执行的是一个多类别分类任务，我们将使用交叉熵损失作为我们的损失函数：
- en: '[PRE23]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `ignore_index=389` argument in the previous loss function instructs the
    program to disregard index 389 whenever it occurs in the target sequence (i.e.,
    sequence y), as this index is used solely for padding purposes and does not represent
    any specific event token in the music piece.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的损失函数中，`ignore_index=389` 参数指示程序在目标序列（即序列 y）中遇到索引 389 时忽略它，因为这个索引仅用于填充目的，并不代表音乐作品中的任何特定事件标记。
- en: We will then train the model for 100 epochs.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将接着对模型进行100个周期的训练。
- en: Listing 14.6 Training the music Transformer to generate music
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.6 训练音乐 Transformer 生成音乐
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ① Iterates through all batches of training data
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历所有训练数据批次
- en: ② Compares model predictions with actual outputs
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将模型预测与实际输出进行比较
- en: ③ Clips gradient norm to 1
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将梯度范数裁剪到 1
- en: ④ Tweaks model parameters to minimize loss
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 调整模型参数以最小化损失
- en: ⑤ Saves model after training
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 训练后保存模型
- en: During training, we feed all the input sequences x in a batch through the model
    to obtain predictions. We then compare these predictions with the corresponding
    output sequences y in the batch and calculate the cross-entropy loss. After that,
    we adjust the model parameters to minimize this loss. It’s important to note that
    we’ve clipped the gradient norm to 1 to prevent the potential problem of exploding
    gradients.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们将所有输入序列 x 在一个批次中通过模型来获得预测。然后，我们将这些预测与批次中的相应输出序列 y 进行比较，并计算交叉熵损失。之后，我们调整模型参数以最小化这个损失。需要注意的是，我们已经将梯度范数裁剪到
    1，以防止潜在的梯度爆炸问题。
- en: The training process described above takes approximately 3 hours if you have
    a CUDA-enabled GPU. After training, the trained model weights, musicTrans.pth,
    are saved on your computer. Alternatively, you can download the trained weights
    from my website at [https://mng.bz/V2pW](https://mng.bz/V2pW).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有CUDA支持的GPU，上述训练过程大约需要3小时。训练完成后，训练好的模型权重musicTrans.pth将保存在您的计算机上。或者，您可以从我的网站[https://mng.bz/V2pW](https://mng.bz/V2pW)下载训练好的权重。
- en: 14.4.2 Music generation with the trained Transformer
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4.2 使用训练好的Transformer进行音乐生成
- en: Now that we have a trained music Transformer, we can proceed with music generation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了一个音乐Transformer，我们可以进行音乐生成了。
- en: 'Similar to the process in text generation, music generation begins with feeding
    a sequence of indexes (representing event tokens) to the model as a prompt. We’ll
    select a music piece from the test set and use the first 250 musical events as
    the prompt:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与文本生成过程类似，音乐生成始于将一系列索引（代表事件标记）作为提示输入到模型中。我们将从测试集中选择一首音乐作品，并使用前250个音乐事件作为提示：
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We have randomly selected an index (42, in our case) and used it to retrieve
    a song from the test subset. We keep only the first 250 musical events, which
    we’ll later feed to the trained model to predict the next musical events. For
    comparison purposes, we’ll save the prompt as a MIDI file, prompt.midi, in the
    local folder.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随机选择了一个索引（在我们的例子中是42）并使用它从测试子集中检索一首歌曲。我们只保留前250个音乐事件，稍后我们将这些事件输入到训练好的模型中以预测下一个音乐事件。为了比较，我们将提示保存为MIDI文件prompt.midi到本地文件夹中。
- en: Exercise 14.2
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 练习14.2
- en: Use the `decode_midi()` function to convert the first 250 musical events in
    the second music piece in the test set into a MIDI file. Save it as prompt2.midi
    on your computer.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`decode_midi()`函数将测试集中第二首音乐的第一个250个音乐事件转换为MIDI文件。将其保存在您的计算机上的prompt2.midi。
- en: To streamline the music generation process, we’ll define a `sample()` function.
    This function accepts a sequence of indexes as input, representing a short piece
    of music. It then iteratively predicts and appends new indexes to the sequence
    until a specified length, `seq_length`, is achieved. The implementation is shown
    in the following listing.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化音乐生成过程，我们将定义一个`sample()`函数。该函数接受一个索引序列作为输入，代表一小段音乐。然后它迭代地预测并追加新的索引到序列中，直到达到指定的长度`seq_length`。实现方式如下所示。
- en: Listing 14.7 A `sample()` function in music generation
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.7 音乐生成中的`sample()`函数
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ① Generates the new indexes until the sequence reaches a certain length
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成新的索引直到序列达到一定长度
- en: ② Divides the prediction by the temperature and then applies the softmax function
    on logits
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将预测值除以温度，然后在logits上应用softmax函数
- en: ③ Samples from the predicted probability distribution to generate a new index
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 从预测的概率分布中采样以生成新的索引
- en: ④ Outputs the whole sequence
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 输出整个序列
- en: One of the parameters of the `sample()` function is temperature, which regulates
    the creativity of the generated music. Refer to chapter 8 on how this works if
    needed. Since we can adjust the originality and diversity of the generated music
    with the temperature parameter alone, we have omitted `top-K` sampling for simplicity
    in this instance. As we have discussed `top-K` sampling three times earlier in
    this book (in chapters 8, 11, and 12), interested readers can experiment with
    incorporating `top-K` sampling into the `sample()` function here.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample()`函数的一个参数是温度，它调节生成音乐的创造性。如有需要，请参考第8章了解其工作原理。由于我们可以仅通过温度参数调整生成音乐的原创性和多样性，因此在此实例中省略了`top-K`采样以简化过程。正如我们在本书中之前三次讨论过`top-K`采样（在第8、11和12章），感兴趣的读者可以尝试将`top-K`采样结合到`sample()`函数中。'
- en: 'Next, we’ll load the trained weights into the model:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载训练好的权重到模型中：
- en: '[PRE27]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We then call the `sample()` function to generate a piece of music:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们调用`sample()`函数来生成一段音乐：
- en: '[PRE28]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: First, we utilize the `encode_midi()` function from the processor.py module
    to convert the MIDI file, prompt.midi, into a sequence of indexes. We then use
    this sequence as the prompt in the `sample()` function to generate a music piece
    comprising 1,000 indexes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们利用处理器模块中的`encode_midi()`函数将MIDI文件prompt.midi转换为索引序列。然后我们使用这个序列作为`sample()`函数中的提示来生成由1,000个索引组成的音乐作品。
- en: 'Finally, we convert the generated sequence of indexes into the MIDI format:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将生成的索引序列转换为MIDI格式：
- en: '[PRE29]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We employ the `decode_midi()` function in the processor.py module to transform
    the generated sequence of indexes into a MIDI file, musicTrans.midi, on your computer.
    Open both files, prompt.midi and musicTrans.midi, on your computer and listen
    to them. The music from prompt.midi lasts about 10 seconds. The music from musicTrans.midi
    lasts about 40 seconds, with the final 30 seconds being new music generated by
    the music Transformer. The generated music should sound like the music piece on
    my website: [https://mng.bz/x6dg](https://mng.bz/x6dg).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在processor.py模块中使用了`decode_midi()`函数，将生成的索引序列转换成您电脑上的MIDI文件，即musicTrans.midi。在您的电脑上打开这两个文件，prompt.midi和musicTrans.midi，并聆听它们。prompt.midi中的音乐大约持续10秒。musicTrans.midi中的音乐大约持续40秒，最后的30秒是由音乐变换器生成的新音乐。生成的音乐应该听起来像我网站上的音乐作品：[https://mng.bz/x6dg](https://mng.bz/x6dg)。
- en: 'The preceding code block may produce output similar to the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块可能产生类似于以下输出的结果：
- en: '[PRE30]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the generated music, there may be instances where certain notes need to be
    removed. For example, if the generated music piece attempts to turn off note 52,
    but note 52 was never turned on initially, then we cannot turn it off. Therefore,
    we need to remove such notes.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的音乐中，可能会有一些音符需要被移除。例如，如果生成的音乐作品尝试关闭音符52，但音符52最初从未被开启，那么我们就不能关闭它。因此，我们需要移除这样的音符。
- en: Exercise 14.3
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 练习14.3
- en: Generate a piece of music consisting of 1,200 notes using the trained Music
    Transformer model, keeping the temperature parameter at 1\. Use the sequence of
    indexes from the file prompt2.midi you just generated in exercise 14.2 as the
    prompt. Save the generated music in a file named musicTrans2.midi on your computer.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练好的音乐变换器模型生成包含1,200个音符的音乐作品，保持温度参数为1。使用您在14.2练习中生成的prompt2.midi文件中的索引序列作为提示。将生成的音乐保存到您电脑上的名为musicTrans2.midi的文件中。
- en: 'You can increase the creativity of the music by setting the temperature argument
    to a value greater than 1, as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将温度参数设置为大于1的值来提高音乐的创意，如下所示：
- en: '[PRE31]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We set the temperature to 1.5\. The generated music is saved as musicHiTemp.midi
    on your computer. Open the file and listen to the generated music to see if you
    can discern any differences compared to the music in the file musicTrans.midi.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将温度设置为1.5。生成的音乐保存为musicHiTemp.midi文件在您的电脑上。打开该文件并聆听，看看您是否能辨别出与musicTrans.midi文件中的音乐相比有任何差异。
- en: Exercise 14.4
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 练习14.4
- en: Generate a piece of music consisting of 1,000 indexes using the trained Music
    Transformer model, setting the temperature parameter to 0.7\. Use the sequence
    of indexes in the file prompt.midi as the prompt. Save the generated music in
    a file named musicLowTemp.midi on your computer. Open this file to listen to the
    generated music and see if there are any discernible differences between the new
    piece of music and the music in the file musicTrans.midi.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练好的音乐变换器模型生成包含1,000个索引的音乐作品，将温度参数设置为0.7。使用prompt.midi文件中的索引序列作为提示。将生成的音乐保存到您电脑上的名为musicLowTemp.midi的文件中。打开此文件聆听生成的音乐，看看新作品与musicTrans.midi文件中的音乐之间是否有可辨别的差异。
- en: In this chapter, you’ve learned how to construct and train a music Transformer
    from scratch, based on the decoder-only Transformer architecture you used in earlier
    chapters. In the next chapter, you’ll explore diffusion-based models, which are
    at the heart of text-to-image Transformers such as OpenAI’s DALL-E 2 and Google’s
    Imagen.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经学习了如何从头开始构建和训练音乐变换器，基于您在前面章节中使用的仅解码器变换器架构。在下一章中，您将探索基于扩散的模型，这些模型是像OpenAI的DALL-E
    2和Google的Imagen这样的文本到图像变换器的核心。
- en: Summary
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The performance-based representation of music enables us to represent a music
    piece as a sequence of notes, which include control messages and velocity values.
    These notes can be further reduced to four kinds of musical events: note-on, note-off,
    time-shift, and velocity. Each event type can assume various values. Consequently,
    we can transform a music piece into a sequence of tokens and then into indexes.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音乐的性能表示使我们能够将音乐作品表示为一串音符，这些音符包括控制信息和速度值。这些音符可以进一步简化为四种音乐事件：音符开启、音符关闭、时间移动和速度。每种事件类型可以假设各种值。因此，我们可以将音乐作品转换为一串标记，然后转换为索引。
- en: A music Transformer adapts the Transformer architecture, originally designed
    for NLP tasks, for music generation. This model is designed to generate sequences
    of musical notes by learning from a large dataset of existing music. It is trained
    to predict the next note in a sequence based on previous notes, by recognizing
    patterns, structures, and relationships among various musical elements in the
    training data.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音乐Transformer架构是对最初为NLP任务设计的Transformer架构进行适配，用于音乐生成。该模型旨在通过学习大量现有音乐数据集来生成音乐音符序列。它通过识别训练数据中各种音乐元素之间的模式、结构和关系，被训练来根据前面的音符预测序列中的下一个音符。
- en: Just as in text generation, we can use temperature to regulate the creativity
    of the generated music.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如文本生成一样，我们可以使用温度来调节生成音乐的创造力。
- en: '* * *'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([1](#footnote-002-backlink))  Chloe Veltman, March 15, 2024\. “Just because
    your favorite singer is dead doesn’t mean you can’t see them ‘live.’” [https://mng.bz/r1de](https://mng.bz/r1de).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-002-backlink)) Chloe Veltman，2024年3月15日。“仅仅因为你的最爱歌手已经去世，并不意味着你不能看到他们‘现场’。”
    [https://mng.bz/r1de](https://mng.bz/r1de)。
- en: ^([2](#footnote-001-backlink))  Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob
    Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D.
    Hoffman, Monica Dinculescu, and Douglas Eck, 2018, “Music Transformer.” [https://arxiv.org/abs/1809.04281](https://arxiv.org/abs/1809.04281).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](#footnote-001-backlink)) Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit,
    Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman,
    Monica Dinculescu, 和 Douglas Eck，2018年，“Music Transformer。” [https://arxiv.org/abs/1809.04281](https://arxiv.org/abs/1809.04281)。
- en: ^([3](#footnote-000-backlink))  See, for example, Hawthorne et al., 2018, “Enabling
    Factorized Piano Music Modeling and Generation with the MAESTRO Dataset.” [https://arxiv.org/abs/1810.12247](https://arxiv.org/abs/1810.12247).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](#footnote-000-backlink)) 例如，参见Hawthorne等人，2018年，“使用MAESTRO数据集实现分解钢琴音乐建模和生成。”
    [https://arxiv.org/abs/1810.12247](https://arxiv.org/abs/1810.12247)。
