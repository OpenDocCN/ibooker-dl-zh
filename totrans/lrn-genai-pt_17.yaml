- en: 14 Building and training a music Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Representing music with control messages and velocity values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizing music into a sequence of indexes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and training a music Transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating musical events using the trained Transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting musical events back to a playable MIDI file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sad that your favorite musician is no longer with us? Sad no more: generative
    AI can bring them back to the stage!'
  prefs: []
  type: TYPE_NORMAL
- en: Take, for example, Layered Reality, a London-based company that’s working on
    a project called Elvis Evolution.^([1](#footnote-002)) The goal? To resurrect
    the legendary Elvis Presley using AI. By feeding a vast array of Elvis’ official
    archival material, including video clips, photographs, and music, into a sophisticated
    computer model, this AI Elvis learns to mimic his singing, speaking, dancing,
    and walking with remarkable resemblance. The result? A digital performance that
    captures the essence of the late King himself.
  prefs: []
  type: TYPE_NORMAL
- en: The Elvis Evolution project is a shining example of the transformative effect
    of generative AI across various industries. In the previous chapter, you explored
    the use of MuseGAN to create music that could pass as authentic multitrack compositions.
    MuseGAN views a piece of music as a multidimensional object, similar to an image,
    and generates complete music pieces that resemble those in the training dataset.
    Both real and AI-generated music are then evaluated by a critic, which helps refine
    the AI-generated music until it’s indistinguishable from the real thing.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll take a different approach to AI music creation, treating
    it as a sequence of musical events. We’ll apply techniques from text generation,
    as discussed in chapters 11 and 12, to predict the next element in a sequence.
    Specifically, you’ll develop a GPT-style model to predict the next musical event
    based on all previous events in the sequence. GPT-style Transformers are ideal
    for this task because of their scalability and the self-attention mechanism, which
    helps them capture long-range dependencies and understand context. This makes
    them highly effective for sequence prediction and generation across a wide range
    of content, including music. The music Transformer you will create has 20.16 million
    parameters, large enough to capture the long-term relations of different notes
    in music pieces but small enough to be trained in a reasonable amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the Maestro piano music from Google’s Magenta group as our training
    data. You’ll learn how to first convert a musical instrument digital interface
    (MIDI) file into a sequence of music notes, analogous to raw text data in natural
    language processing (NLP). You’ll then break the musical notes down into small
    pieces called musical events, analogous to tokens in NLP. Since neural networks
    can only accept numerical inputs, you’ll map each unique event token to an index.
    With this, the music pieces in the training data are converted into sequences
    of indexes, ready to be fed into neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: To train the music Transformer to predict the next token based on the current
    token and all previous tokens in the sequence, we’ll create sequences of 2,048
    indexes as inputs (features x). We then shift the sequences one index to the right
    and use them as the outputs (targets y). We feed pairs of (x, y) to the music
    Transformer to train the model. Once trained, we’ll use a short sequence of indexes
    as the prompt and feed it to the music Transformer to predict the next token,
    which is then appended to the prompt to form a new sequence. This new sequence
    is fed back into the model for further predictions, and this process is repeated
    until the sequence reaches a desired length.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll see that the trained music Transformer can generate lifelike music that
    mimics the style in the training dataset. Further, unlike the music generated
    in chapter 13, you’ll learn to control the creativity of the music piece. You’ll
    achieve this by scaling the predicted logits with the temperature parameter, just
    as you did in earlier chapters when controlling the creativity of the generated
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1 Introduction to the music Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of the music Transformer was introduced in 2018.^([2](#footnote-001))
    This innovative approach extends the Transformer architecture, initially devised
    for NLP tasks, to the field of music generation. As discussed in previous chapters,
    Transformers employ self-attention mechanisms to effectively grasp the context
    and capture long-range dependencies among elements in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In a similar vein, the music Transformer is engineered to generate a sequence
    of musical notes by learning from a vast dataset of existing music. The model
    is trained to predict the next musical event in a sequence based on preceding
    events by understanding the patterns, structures, and relationships between different
    musical elements in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A crucial step in training a music Transformer lies in figuring out how to
    represent music as a sequence of unique musical events, akin to tokens in NLP.
    In the previous chapter, you learned to represent a piece of music as a 4D object.
    In this chapter, you will explore an alternative approach to music representation,
    specifically performance-based music representation through control messages and
    velocity values.^([3](#footnote-000)) Based on this, you will convert a piece
    of music into four types of musical events: note-on, note-off, time-shift, and
    velocity.'
  prefs: []
  type: TYPE_NORMAL
- en: Note-on signals the start of a musical note being played, specifying the note’s
    pitch. Note-off indicates the end of a note, telling the instrument to stop playing
    that note. Time-shift represents the amount of time that passes between two musical
    events. Velocity measures the force or speed with which a note is played, with
    higher values corresponding to a stronger, louder sound. Each type of musical
    event has many different values. Each unique event will be mapped to a different
    index, effectively transforming a piece of music into a sequence of indexes. You
    will then apply the GPT models, as discussed in chapters 11 and 12, to create
    a decoder-only music Transformer that predicts the next musical event in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will begin by learning about performance-based music representation
    through control messages and velocity values. You will then explore how to represent
    music pieces as sequences of musical events. Finally, you will learn the steps
    involved in building and training a Transformer to generate music.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.1 Performance-based music representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Performance-based music representation is often achieved using the MIDI format,
    which captures the nuances of a musical performance through control messages and
    velocity values. In MIDI, musical notes are represented by note-on and note-off
    messages, which include information about the pitch and velocity of each note.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in chapter 13, the pitch value ranges from 0 to 127, with each
    value corresponding to a semitone in an octave level. For instance, the pitch
    value 60 corresponds to a C4 note, while the pitch value 74 corresponds to a D5
    note. The velocity value, also ranging from 0 to 127, represents the dynamics
    of the note, with higher values indicating louder or more forceful playing. By
    combining these control messages and velocity values, a MIDI sequence can capture
    the expressive details of a live performance, allowing for expressive playback
    through MIDI-compatible instruments and software.
  prefs: []
  type: TYPE_NORMAL
- en: To give you a concrete example of how a piece of music can be represented by
    control messages and velocity values, consider the five notes shown in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.1 Example notes in a performance-based music representation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: These are the first five notes from a piece of music in the training dataset
    you’ll use in this chapter. The first note has a timestamp of approximately 1.03
    seconds, with a note of pitch value 74 (D5) starting to play at a velocity of
    86\. Looking at the second note, you can infer that after about 0.01 seconds (since
    the timestamp is now 1.04 seconds), a note with a pitch value of 38 starts to
    play at a velocity of 77, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: These musical notes are similar to raw text in NLP; we cannot directly feed
    them to a music Transformer to train the model. We first need to “tokenize” the
    notes and then convert the tokens to indexes before feeding them to the model.
  prefs: []
  type: TYPE_NORMAL
- en: To tokenize the musical notes, we’ll represent the music using increments of
    0.01 seconds to reduce the number of time steps in the music piece. Additionally,
    we’ll separate control messages from velocity values and treat them as different
    elements of the music piece. Specifically, we’ll represent music using a combination
    of note-on, note-off, time-shift, and velocity events. Once we do that, the preceding
    five musical notes can be represented by the following events (some events are
    omitted for brevity).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.2 Tokenized representation of a piece of music
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll count time shifts in increments of 0.01 seconds and tokenize time shifts
    from 0.01 seconds to 1 second with 100 different values. Thus, time-shift events
    are tokenized into 1 of 100 unique event tokens: a value of 0 indicates a time
    lapse of 0.01 seconds, 1 indicates a time lapse of 0.02 seconds, and so on, up
    to 99, which indicates a time lapse of 1 second. If a time-shift lasts more than
    1 second, you can use multiple time-shift tokens to indicate it. For example,
    the first two tokens in listing 14.2 are both time-shift tokens, with values 99
    and 2, respectively, indicating time lapses of 1 second and 0.03 seconds. This
    matches the timestamp of the first musical note in listing 14.1: 1.0326 seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.2 also shows that velocity is a separate type of musical event. We
    place the value of velocity into 32 equally spaced bins, converting the original
    velocity values, which range from 0 to 127, into 1 of 32 values, ranging from
    0 to 31\. This is why the original velocity value of 86 in the first note in listing
    14.1 is now represented as a velocity event with a value of 21 in listing 14.2
    (the number 86 falls into the 22^(nd) bin, and Python uses zero-based indexing).
  prefs: []
  type: TYPE_NORMAL
- en: Table 14.1 shows the meaning of four types of different tokenized events, their
    value ranges, and the meaning of each event token.
  prefs: []
  type: TYPE_NORMAL
- en: Table 14.1 Meanings of different event tokens
  prefs: []
  type: TYPE_NORMAL
- en: '| Event token type | Event token value range | Meaning of the event tokens
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `note_on` | 0–127 | Starting to play at a certain pitch value. For example,
    `note_on` with value 74 means starting to play note D5. |'
  prefs: []
  type: TYPE_TB
- en: '| `note_off` | 0–127 | Releasing a certain note. For example, `note_off` with
    value 60 means to stop playing note C4. |'
  prefs: []
  type: TYPE_TB
- en: '| `time_shift` | 0–99 | The `time_shift` values are increments of 0.01 seconds.
    For example, 0 indicates 0.01 seconds, 2 indicates 0.03 seconds, and 99 indicates
    1 second. |'
  prefs: []
  type: TYPE_TB
- en: '| `velocity` | 0–31 | The original velocity values are placed into 32 bins.
    The bin value is used. For example, an original velocity value of 86 now has a
    tokenized value of 21. |'
  prefs: []
  type: TYPE_TB
- en: Similar to the approach taken in NLP, we’ll convert each unique token into an
    index so that we can input the data into neural networks. According to table 14.1,
    there are 128 unique note-on event tokens, 128 note-off event tokens, 32 velocity
    event tokens, and 100 time-shift event tokens. This results in a total of 128
    + 128 + 32 + 100 = 388 unique tokens. Consequently, we convert these 388 unique
    tokens into indexes ranging from 0 to 387, based on the mappings provided in table
    14.2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 14.2 Mapping event tokens to indexes and indexes to event tokens
  prefs: []
  type: TYPE_NORMAL
- en: '| Token type | Index range | Event token to index | Index to event token |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `note_on` | 0–127 | The value of the `note_on` token. For example, the `note_on`
    token with a value of 74 is assigned an index value of 74. | If the index range
    is 0 to 127, set token type to `note_on` and value to the index value. For example,
    the index value 63 is mapped to a `note_on` token with a value of 63. |'
  prefs: []
  type: TYPE_TB
- en: '| `note_off` | 128–255 | 128 plus the value of the `note_off` token. For example,
    the `note_off` token with a value of 60 is assigned an index value of 188 (since
    128 + 60 = 188). | If the index range is 128 to 255, set token type to `note_off`
    and value to index minus 128\. For example, index 180 is mapped to the `note_off`
    token with value 52. |'
  prefs: []
  type: TYPE_TB
- en: '| `time_shift` | 256–355 | 256 plus the value of the `time_shift` token. For
    example, the `time_shift` token with a value of 16 is assigned an index value
    of 272 (since 256 + 16 = 272). | If the index range is 256 to 355, set token type
    to `time_shift` and value to index minus 256\. For example, index 288 is mapped
    to the `time_shift` token with value 32. |'
  prefs: []
  type: TYPE_TB
- en: '| `velocity` | 356–387 | 356 plus the value of the velocity token. For example,
    the velocity token with a value of 21 is assigned an index value of 377 (since
    356+21=377). | If the index range is 356 to 387, set token type to `velocity`
    and value to index minus 356\. For example, index 380 is mapped to the `velocity`
    token with value 24. |'
  prefs: []
  type: TYPE_TB
- en: The third column in table 14.2 outlines the conversion of event tokens to indexes.
    Note-on tokens are assigned index values ranging from 0 to 127, where the index
    value corresponds to the pitch number in the token. Note-off tokens are assigned
    index values from 128 to 255, with the index value being 128 plus the pitch number.
    Time-shift tokens are assigned index values from 256 to 355, with the index value
    being 256 plus the time-shift value. Lastly, velocity tokens are assigned index
    values from 356 to 387, with the index value being 356 plus the velocity bin number.
  prefs: []
  type: TYPE_NORMAL
- en: Using this token-to-index mapping, we’ll convert each piece of music into a
    sequence of indexes. We’ll apply this conversion to all music pieces in the training
    dataset and use the resulting sequences to train our music Transformer (the details
    of which will be explained later). Once trained, we’ll use the Transformer to
    generate music in the form of a sequence of indexes. The final step is to convert
    this sequence back into MIDI format so that we can play and enjoy the music on
    a computer.
  prefs: []
  type: TYPE_NORMAL
- en: The last column in table 14.2 provides guidance on converting indexes back to
    event tokens. We first determine the token type based on the range in which the
    index falls. The four ranges in the second column of table 14.2 correspond to
    the four token types in the first column of the table. To obtain the value for
    each token type, we subtract the index value by 0, 128, 256, and 356 for the four
    types of tokens, respectively. These tokenized events are then converted into
    musical notes in MIDI format, ready to be played on a computer.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.2 The music Transformer architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapter 9, we built an encoder-decoder Transformer, and in chapters 11 and
    12, we focused on decoder-only Transformers. Unlike language translation tasks
    where the encoder captures the meaning of the source language and passes it to
    the decoder for generating the translation, music generation does not require
    an encoder to understand a different language. Instead, the model generates subsequent
    event tokens based on previous event tokens in the music sequence. Therefore,
    we’ll construct a decoder-only Transformer for our music generation task.
  prefs: []
  type: TYPE_NORMAL
- en: Our music Transformer, like other Transformer models, utilizes self-attention
    mechanisms to capture the long-range dependencies among different musical events
    in a piece of music, thereby generating coherent and lifelike music. Although
    our music Transformer differs in size from the GPT models we built in chapters
    11 and 12, it shares the same core architecture. It follows the same structural
    design as GPT-2 models but is significantly smaller, making it feasible to train
    without the need for supercomputing facilities.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, our music Transformer consists of 6 decoder layers with an embedding
    dimension of 512, meaning each token is represented by a 512-value vector after
    word embedding. Instead of using sine and cosine functions for positional encoding
    as in the original 2017 paper “Attention Is All You Need,” we use embedding layers
    to learn the positional encodings for different positions in a sequence. As a
    result, each position in a sequence is also represented by a 512-value vector.
    For calculating causal self-attention, we use 8 parallel attention heads to capture
    different aspects of the meanings of a token in the sequence, giving each attention
    head a dimension of 64 (512/8).
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the vocabulary size of 50,257 in GPT-2 models, our model has a much
    smaller vocabulary size of 390 (388 different event tokens, plus a token to signify
    the end of a sequence and a token to pad shorter sequences; I’ll explain later
    why padding is needed). This allows us to set the maximum sequence length in our
    music Transformer to 2,048, which is much longer than the maximum sequence length
    of 1,024 in GPT-2 models. This choice is necessary to capture the long-term relations
    of music notes in a sequence. With these hyperparameter values, our music Transformer
    has a size of 20.16 million parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 illustrates the architecture of the music Transformer we will create
    in this chapter. It is similar to the architecture of the GPT models you built
    in chapters 11 and 12\. Figure 14.1 also shows the size of the training data as
    it passes through the model during training.
  prefs: []
  type: TYPE_NORMAL
- en: The input to the music Transformer we constructed comprises input embeddings,
    as depicted at the bottom of figure 14.1\. The input embedding is the sum of the
    word embedding and positional encoding of the input sequence. This input embedding
    is then passed sequentially through six decoder blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 The architecture of a music Transformer. Music files in MIDI formats
    are first converted into sequences of musical events. These events are then tokenized
    and converted into indexes. We organize these indexes into sequences of 2,048
    elements, and each batch contains 2 such sequences. The input sequence first undergoes
    word embedding and positional encoding; the input embedding is the sum of these
    two components. This input embedding is then processed through six decoder layers,
    each utilizing self-attention mechanisms to capture the relationships among different
    musical events in the sequence. After passing through the decoder layers, the
    output undergoes layer normalization to ensure stability in the training process.
    It then passes through a linear layer, resulting in an output size of 390, which
    corresponds to the number of unique tokens in the vocabulary. This final output
    represents the predicted logits for the next musical event in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in chapters 11 and 12, each decoder layer consists of two sublayers:
    a causal self-attention layer and a feed-forward network. In addition, we apply
    layer normalization and residual connections to each sublayer to enhance the model’s
    stability and learning capability.'
  prefs: []
  type: TYPE_NORMAL
- en: After passing through the decoder layers, the output undergoes layer normalization
    and is then fed into a linear layer. The number of outputs in our model corresponds
    to the number of unique musical event tokens in the vocabulary, which is 390\.
    The output of the model is the logits for the next musical event token.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we will apply the softmax function to these logits to obtain the probability
    distribution over all possible event tokens. The model is designed to predict
    the next event token based on the current token and all previous tokens in the
    music sequence, enabling it to generate coherent and musically sensible sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.3 Training the music Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we understand how to construct a music Transformer for music generation,
    let’s outline the training process for the music Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: The style of the music generated by the model is influenced by the music pieces
    used for training. We’ll use piano performances from Google’s Magenta group to
    train our model. Figure 14.2 illustrates the steps involved in training the Transformer
    for music generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH14_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 The training process for a music Transformer to generate music
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the approach we’ve taken in NLP tasks, the first step in the training
    process for our music Transformer is to convert the raw training data into a numerical
    form so that it can be fed into the model. Specifically, we start by converting
    MIDI files in the training set into sequences of musical notes. We then further
    tokenize these notes by converting them into 1 of 388 unique events/tokens. After
    tokenization, we assign a unique index (i.e., an integer) to each token, converting
    the music pieces in the training set into sequences of integers (see step 1 in
    figure 14.2).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we transform the sequence of integers into training data by dividing this
    sequence into sequences of equal length (step 2 in figure 14.2). We allow a maximum
    length of 2,048 indexes in each sequence. The choice of 2,048 enables us to capture
    long-range dependencies among musical events in a music sequence to create lifelike
    music. These sequences form the features (the x variable) of our model. As we
    did in previous chapters when training GPT models to generate text, we slide the
    input sequence window one index to the right and use it as the output in the training
    data (the y variable; step 3 in figure 14.2). Doing so forces our model to predict
    the next music token in a sequence based on the current token and all previous
    tokens in the music sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input and output pairs serve as the training data (x, y) for the music
    Transformer. During training, you will iterate through the training data. In the
    forward passes, you feed the input sequence x through the music Transformer (step
    4). The music Transformer then makes a prediction based on the current parameters
    in the model (step 5). You compute the cross-entropy loss by comparing the predicted
    next tokens with the output obtained from step 3\. In other words, you compare
    the model’s prediction with the ground truth (step 6). Finally, you will adjust
    the parameters in the music Transformer so that in the next iteration, the model’s
    predictions move closer to the actual output, minimizing the cross-entropy loss
    (step 7). The model is essentially performing a multicategory classification problem:
    it’s predicting the next token from all unique music tokens in the vocabulary.'
  prefs: []
  type: TYPE_NORMAL
- en: You will repeat steps 3 to 7 through many iterations. After each iteration,
    the model parameters are adjusted to improve the prediction of the next token.
    This process will be repeated for 50 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: To generate a new piece of music with the trained model, we obtain a music piece
    from the test set, tokenize it, and convert it to a long sequence of indexes.
    We’ll use the first, say, 250 indexes as the prompt (200 or 300 will lead to similar
    results). We then ask the trained music Transformer to generate new indexes until
    the sequence reaches a certain length (say, 1,000 indexes). We then convert the
    sequence of indexes back into a MIDI file to be played on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 14.2 Tokenizing music pieces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having grasped the structure of the music Transformer and its training methodology,
    we’ll start with the first step: tokenization and indexing of the musical compositions
    in our training dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll begin with employing a performance-based representation (as explained
    in the first section) to portray music pieces as musical notes, akin to raw text
    in NLP. After that, we’ll divide these musical notes into a series of events,
    similar to tokens in NLP. Each unique event will be assigned a different index.
    Utilizing this mapping, we’ll transform all music pieces in the training dataset
    into sequences of indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll standardize these sequences of indexes into a fixed length, specifically
    sequences with 2,048 indexes, and use them as the feature inputs (x). By shifting
    the window one index to the right, we’ll generate the corresponding output sequences
    (y). We’ll then group pairs of input and output (x, y) into batches, preparing
    them for training the music Transformer later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we’ll require the `pretty_midi` and `music21` libraries for processing MIDI
    files, execute the following line of code in a new cell in the Jupyter Notebook
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 14.2.1 Downloading training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll obtain the piano performances from the MAESTRO dataset, which is made
    available by Google’s Magenta group ([https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip](https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip))
    and download the ZIP file. After downloading, unzip it and move the resulting
    folder, /maestro-v2.0.0/, into the /files/ directory on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that the /maestro-v2.0.0/ folder contains 4 files (one of which should
    be named “maestro-v2.0.0.json”) and 10 subfolders. Each subfolder should contain
    more than 100 MIDI files. To familiarize yourself with the sound of the music
    pieces in the training data, try opening some of the MIDI files with your preferred
    music player.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll split the MIDI files into train, validation, and test subsets.
    To start, create three subfolders within /files/maestro-v2.0.0/:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To facilitate the processing of MIDI files, visit Kevin Yang’s GitHub repository
    at [https://github.com/jason9693/midi-neural-processor](https://github.com/jason9693/midi-neural-processor),
    download the processor.py file, and place it in the /utils/ folder on your computer.
    Alternatively, you can obtain the file from the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).
    We’ll use this file as a local module to transform a MIDI file into a sequence
    of indexes and vice versa. This approach allows us to concentrate on developing,
    training, and utilizing a music Transformer without getting bogged down in the
    details of music format conversion. At the same time, I’ll provide a simple example
    of how this process works so that you can convert between a MIDI file and a sequence
    of indexes yourself using the module.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you need to download the ch14util.py file from the book’s GitHub
    repository and place it in the /utils/ directory on your computer. We’ll use the
    ch14util.py file as another local module to define the music Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: The file maestro-v2.0.0.json within the /maestro-v2.0.0/ folder contains the
    names of all MIDI files and their designated subsets (train, validation, or test).
    Based on this information, we’ll categorize the MIDI files into three corresponding
    subfolders.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.3 Splitting training data into train, validation, and test subsets
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① Loads JSON file
  prefs: []
  type: TYPE_NORMAL
- en: ② Iterates through all files in the training data
  prefs: []
  type: TYPE_NORMAL
- en: ③ Places a file in train, validation, or test subfolder based on instructions
    in the JSON file
  prefs: []
  type: TYPE_NORMAL
- en: 'The JavaScript object notation (JSON) file you downloaded categorizes each
    file in the training dataset into one of three subsets: train, validation, and
    test. After executing the previous code listing, if you explore the /train/, /val/,
    and /test/ folders on your computer, you should find numerous files in each. To
    verify the number of files in each of these three folders, you can perform the
    following checks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output from the preceding code block is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Results show that there are 967, 137, and 178 pieces of music in the train,
    validation, and test subsets, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.2 Tokenizing MIDI files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we’ll represent each MIDI file as a sequence of musical notes.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.4 Converting a MIDI file to a sequence of music notes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ① Selects a MIDI file from the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: ② Extracts musical events from the music
  prefs: []
  type: TYPE_NORMAL
- en: ③ Places all musical events in the list dnotes
  prefs: []
  type: TYPE_NORMAL
- en: We have selected one MIDI file from the training dataset and used the processor.py
    local module to convert it into a sequence of musical notes. The output from the
    preceding code listing is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The output displayed here shows the first five musical notes from the MIDI file.
    You might have observed that the time representation in the output is continuous.
    Certain musical notes contain both a `note_on` and a `velocity` attribute, complicating
    the tokenization process due to the vast number of unique musical events resulting
    from the continuous nature of time representation. Additionally, the combination
    of different `note_on` and `velocity` values is large (each can assume 128 distinct
    values, ranging from 0 to 127), leading to an excessively large vocabulary size.
    This, in turn, would render training impractical.
  prefs: []
  type: TYPE_NORMAL
- en: 'To mitigate this problem and decrease the vocabulary size, we further convert
    these musical notes into tokenized events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ① Discretizes time to reduce the number of unique events
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts musical notes to events
  prefs: []
  type: TYPE_NORMAL
- en: ③ Prints out the first 15 events
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The music piece is now represented by four types of events: note-on, note-off,
    time-shift, and velocity. Each event type includes different values, resulting
    in a total of 388 unique events, as detailed in table 14.2 earlier. The specifics
    of converting a MIDI file into a sequence of such unique events are not essential
    for constructing and training a music Transformer. Therefore, we will not dive
    deeply into this topic; interested readers can refer to Huang et al (2018) cited
    earlier. All you need to know is how to use the processor.py module to transform
    a MIDI file into a sequence of indexes and vice versa. In the following subsection,
    you’ll learn how to accomplish that.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.2.3 Preparing the training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve learned how to convert music pieces into tokens and then into indexes.
    The next step involves preparing the training data so that we can utilize it to
    train the music Transformer later in this chapter. To achieve this, we define
    the `create`_`xys()` function shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.5 Creating training data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates (x, y) sequences, with equal lengths of 2,048 indexes and sets index
    399 as the padding index
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses a sequence of up to 2,048 indexes as input
  prefs: []
  type: TYPE_NORMAL
- en: ③ Slides the window one index to the right and uses it as the output
  prefs: []
  type: TYPE_NORMAL
- en: ④ Sets the end index as 388
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve seen repeatedly throughout this book, in sequence prediction tasks,
    we use a sequence x as input. We then shift the sequence one position to the right
    to create the output sequence. This approach compels the model to predict the
    next element based on the current element and all preceding elements in the sequence.
    To prepare training data for our music Transformer, we’ll construct pairs (x,
    y), where x is the input and y is the output. Both x and y contain 2,048 indexes—long
    enough to capture the long-term relations of music notes in a sequence but not
    too long to hinder the training process.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll iterate through all the music pieces in the training dataset we downloaded.
    If a music piece exceeds 2,048 indexes in length, we’ll use the first 2,048 indexes
    as input x. For the output y, we’ll use indexes from the second position to the
    2,049th position. In the rare case where the music piece is less than or equal
    to 2,048 indexes long, we’ll pad the sequence with index 389 to ensure that both
    x and y are 2,048 indexes long. Additionally, we use index 388 to signal the end
    of the sequence y.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the first section, there are a total of 388 unique event tokens,
    indexed from 0 to 387\. Since we use 388 to signal the end of the y sequence and
    389 to pad sequences, we have a total of 390 unique indexes, ranging from 0 to
    389.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now apply the `create_xys()` function to the train subset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This shows that out of the 967 music pieces in the train subset, only 5 are
    shorter than 2,048 indexes. Their lengths are shown in the previous output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also apply the `create_xys()` function to the validation and test subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This shows that all music pieces in the validation subset are longer than 2,048
    indexes. Only one music piece in the test subset is shorter than 2,048 indexes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s print out a file from the validation subset and see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The x sequence from the first pair in the validation set has a length of 2,048
    indexes, with values such as 324, 367, and so on. Let’s use the module `processor.py`
    to decode the sequence to a MIDI file so that you can hear what it sounds like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `decode_midi()` function converts a sequence of indexes into a MIDI file,
    playable on your computer. After running the preceding code block, open the file
    val1.midi with a music player on your computer to hear what it sounds like.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.1
  prefs: []
  type: TYPE_NORMAL
- en: Use the `decode_midi()` function from the processor.py local module to convert
    the first music piece in the train subset into a MIDI file. Save it as train1.midi
    on your computer. Open it with a music player on your computer and get a sense
    of what type of music we use for training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we create a data loader so that the data are in batches for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: To prevent your GPU from running out of memory, we’ll use a batch size of 2,
    as we’ve created very long sequences, each comprising 2,048 indexes. If needed,
    reduce the batch size to one or switch to CPU training.
  prefs: []
  type: TYPE_NORMAL
- en: With that, our training data is prepared. In the next two sections, we’ll construct
    a music Transformer from scratch and then train it using the training data we’ve
    just prepared.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3 Building a GPT to generate music
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that our training data is prepared, we’ll construct a GPT model from scratch
    for music generation. The architecture of this model will be similar to the GPT-2XL
    model we developed in chapter 11 and the text generator from chapter 12\. However,
    the size of our music Transformer will differ due to the specific hyperparameters
    we select.
  prefs: []
  type: TYPE_NORMAL
- en: To conserve space, we’ll place the model construction within the local module
    ch14util.py. Our focus here will be on the hyperparameters chosen for the music
    Transformer. Specifically, we’ll decide the values of `n_layer,` the number of
    decoder layers in the model; `n_head`, the number of parallel heads to use to
    calculate causal self-attention; `n_embd,` the embedding dimension; and `block_size,`
    the number of tokens in the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3.1 Hyperparameters in the music Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Open the file ch14util.py that you downloaded earlier from the book’s GitHub
    repository. Inside, you’ll find several functions and classes that are identical
    to those defined in chapter 12.
  prefs: []
  type: TYPE_NORMAL
- en: As in all GPT models we have seen in this book, the feed-forward network in
    the decoder block utilizes the Gaussian error linear unit (GELU) activation function.
    Consequently, we define a GELU class in ch14util.py, exactly as we did in chapter
    12.
  prefs: []
  type: TYPE_NORMAL
- en: 'We employ a `Config()` class to store all the hyperparameters used in the music
    Transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The attributes within the `Config()` class serve as hyperparameters for our
    music Transformer. We assign a value of 6 to the `n_layer` attribute, indicating
    that our music Transformer consists of 6 decoder layers. This is more than the
    number of decoder layers in the GPT model we built in chapter 12\. Each decoder
    layer processes the input sequence and introduces a level of abstraction or representation.
    As the information traverses through more layers, the model is capable of capturing
    more complex patterns and relationships in the data. This depth is crucial for
    our music Transformer to comprehend and generate intricate music pieces.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `n_head` attribute is set to 8, signifying that we will divide the query
    Q, key K, and value V vectors into eight parallel heads during the computation
    of causal self-attention. The `n_embd` attribute is set to 512, indicating an
    embedding dimension of 512: each event token will be represented by a vector of
    512 values. The `vocab_size` attribute is determined by the number of unique tokens
    in the vocabulary, which is 390\. As explained earlier, there are 388 unique event
    tokens, and we added 1 token to signify the end of the sequence and another token
    to pad shorter sequences so that all sequences have a length of 2,048\. The `block_size`
    attribute is set to 2,048, indicating that the input sequence contains a maximum
    of 2,048 tokens. We set the dropout rates to 0.1, as in chapters 11 and 12.'
  prefs: []
  type: TYPE_NORMAL
- en: Like all Transformers, our music Transformer employs self-attention mechanisms
    to capture relationships among different elements in a sequence. Consequently,
    we define a `CausalSelfAttention()` class in the local module ch14util, which
    is identical to the `CausalSelfAttention()` class defined in chapter 12\.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3.2 Building a music Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We combine a feed-forward network with the causal self-attention sublayer to
    form a decoder block (i.e., a decoder layer). We apply layer normalization and
    a residual connection to each sublayer for improved stability and performance.
    To this end, we define a `Block()` class in the local module to create a decoder
    block, which is identical to the `Block()` class we defined in chapter 12.
  prefs: []
  type: TYPE_NORMAL
- en: We then stack six decoder blocks on top of each other to form the main body
    of our music Transformer. To achieve this, we define a `Model()` class in the
    local module. As in all GPT models we have seen in this book, we use learned positional
    encoding by employing the `Embedding()` class in PyTorch, instead of the fixed
    positional encoding in the original 2017 paper “Attention Is All You Need.” Refer
    to chapter 11 on the differences between the two positional encoding methods.
  prefs: []
  type: TYPE_NORMAL
- en: The input to the model consists of sequences of indexes corresponding to musical
    event tokens in the vocabulary. We pass the input through word embedding and positional
    encoding and add the two to form the input embedding. The input embedding then
    goes through the six decoder layers. After that, we apply layer normalization
    to the output and attach a linear head to it so that the number of outputs is
    390, the size of the vocabulary. The outputs are the logits corresponding to the
    390 tokens in the vocabulary. Later, we’ll apply the softmax activation function
    to the logits to obtain the probability distribution over the unique music tokens
    in the vocabulary when generating music.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll create our music Transformer by instantiating the `Model()` class
    we defined in the local module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Our music Transformer consists of 20.16 million parameters, a figure substantially
    smaller than the GPT-2XL, which boasts over 1.5 billion parameters. Nonetheless,
    our music Transformer surpasses the size of the text generator we constructed
    in chapter 12, which contains only 5.12 million parameters. Despite these differences,
    all three models are based on the decoder-only Transformer architecture. The variations
    lie solely in the hyperparameters, such as the embedding dimension, number of
    decoder layers, vocabulary size, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 14.4 Training and using the music Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you’ll train the music Transformer you’ve just constructed
    using the batches of training data we prepared earlier in this chapter. To expedite
    the process, we’ll train the model for 100 epochs and then stop the training process.
    For those interested, you can utilize the validation set to determine when to
    stop training, based on the performance of the model on the validation set, as
    we did in chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, we’ll provide it with a prompt in the form of a sequence
    of indexes. We’ll then request the trained music Transformer to generate the next
    index. This new index is appended to the prompt, and the updated prompt is fed
    back into the model for another prediction. This process is repeated iteratively
    until the sequence reaches a certain length.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the music generated in chapter 13, we can control the creativity of the
    music piece by applying different temperatures.
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.1 Training the music Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As always, we’ll use the Adam optimizer for training. Given that our music
    Transformer is essentially executing a multicategory classification task, we’ll
    utilize cross-entropy loss as our loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `ignore_index=389` argument in the previous loss function instructs the
    program to disregard index 389 whenever it occurs in the target sequence (i.e.,
    sequence y), as this index is used solely for padding purposes and does not represent
    any specific event token in the music piece.
  prefs: []
  type: TYPE_NORMAL
- en: We will then train the model for 100 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.6 Training the music Transformer to generate music
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through all batches of training data
  prefs: []
  type: TYPE_NORMAL
- en: ② Compares model predictions with actual outputs
  prefs: []
  type: TYPE_NORMAL
- en: ③ Clips gradient norm to 1
  prefs: []
  type: TYPE_NORMAL
- en: ④ Tweaks model parameters to minimize loss
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Saves model after training
  prefs: []
  type: TYPE_NORMAL
- en: During training, we feed all the input sequences x in a batch through the model
    to obtain predictions. We then compare these predictions with the corresponding
    output sequences y in the batch and calculate the cross-entropy loss. After that,
    we adjust the model parameters to minimize this loss. It’s important to note that
    we’ve clipped the gradient norm to 1 to prevent the potential problem of exploding
    gradients.
  prefs: []
  type: TYPE_NORMAL
- en: The training process described above takes approximately 3 hours if you have
    a CUDA-enabled GPU. After training, the trained model weights, musicTrans.pth,
    are saved on your computer. Alternatively, you can download the trained weights
    from my website at [https://mng.bz/V2pW](https://mng.bz/V2pW).
  prefs: []
  type: TYPE_NORMAL
- en: 14.4.2 Music generation with the trained Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have a trained music Transformer, we can proceed with music generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the process in text generation, music generation begins with feeding
    a sequence of indexes (representing event tokens) to the model as a prompt. We’ll
    select a music piece from the test set and use the first 250 musical events as
    the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We have randomly selected an index (42, in our case) and used it to retrieve
    a song from the test subset. We keep only the first 250 musical events, which
    we’ll later feed to the trained model to predict the next musical events. For
    comparison purposes, we’ll save the prompt as a MIDI file, prompt.midi, in the
    local folder.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.2
  prefs: []
  type: TYPE_NORMAL
- en: Use the `decode_midi()` function to convert the first 250 musical events in
    the second music piece in the test set into a MIDI file. Save it as prompt2.midi
    on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: To streamline the music generation process, we’ll define a `sample()` function.
    This function accepts a sequence of indexes as input, representing a short piece
    of music. It then iteratively predicts and appends new indexes to the sequence
    until a specified length, `seq_length`, is achieved. The implementation is shown
    in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 14.7 A `sample()` function in music generation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: ① Generates the new indexes until the sequence reaches a certain length
  prefs: []
  type: TYPE_NORMAL
- en: ② Divides the prediction by the temperature and then applies the softmax function
    on logits
  prefs: []
  type: TYPE_NORMAL
- en: ③ Samples from the predicted probability distribution to generate a new index
  prefs: []
  type: TYPE_NORMAL
- en: ④ Outputs the whole sequence
  prefs: []
  type: TYPE_NORMAL
- en: One of the parameters of the `sample()` function is temperature, which regulates
    the creativity of the generated music. Refer to chapter 8 on how this works if
    needed. Since we can adjust the originality and diversity of the generated music
    with the temperature parameter alone, we have omitted `top-K` sampling for simplicity
    in this instance. As we have discussed `top-K` sampling three times earlier in
    this book (in chapters 8, 11, and 12), interested readers can experiment with
    incorporating `top-K` sampling into the `sample()` function here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll load the trained weights into the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We then call the `sample()` function to generate a piece of music:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: First, we utilize the `encode_midi()` function from the processor.py module
    to convert the MIDI file, prompt.midi, into a sequence of indexes. We then use
    this sequence as the prompt in the `sample()` function to generate a music piece
    comprising 1,000 indexes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we convert the generated sequence of indexes into the MIDI format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We employ the `decode_midi()` function in the processor.py module to transform
    the generated sequence of indexes into a MIDI file, musicTrans.midi, on your computer.
    Open both files, prompt.midi and musicTrans.midi, on your computer and listen
    to them. The music from prompt.midi lasts about 10 seconds. The music from musicTrans.midi
    lasts about 40 seconds, with the final 30 seconds being new music generated by
    the music Transformer. The generated music should sound like the music piece on
    my website: [https://mng.bz/x6dg](https://mng.bz/x6dg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code block may produce output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the generated music, there may be instances where certain notes need to be
    removed. For example, if the generated music piece attempts to turn off note 52,
    but note 52 was never turned on initially, then we cannot turn it off. Therefore,
    we need to remove such notes.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.3
  prefs: []
  type: TYPE_NORMAL
- en: Generate a piece of music consisting of 1,200 notes using the trained Music
    Transformer model, keeping the temperature parameter at 1\. Use the sequence of
    indexes from the file prompt2.midi you just generated in exercise 14.2 as the
    prompt. Save the generated music in a file named musicTrans2.midi on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can increase the creativity of the music by setting the temperature argument
    to a value greater than 1, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We set the temperature to 1.5\. The generated music is saved as musicHiTemp.midi
    on your computer. Open the file and listen to the generated music to see if you
    can discern any differences compared to the music in the file musicTrans.midi.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.4
  prefs: []
  type: TYPE_NORMAL
- en: Generate a piece of music consisting of 1,000 indexes using the trained Music
    Transformer model, setting the temperature parameter to 0.7\. Use the sequence
    of indexes in the file prompt.midi as the prompt. Save the generated music in
    a file named musicLowTemp.midi on your computer. Open this file to listen to the
    generated music and see if there are any discernible differences between the new
    piece of music and the music in the file musicTrans.midi.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned how to construct and train a music Transformer
    from scratch, based on the decoder-only Transformer architecture you used in earlier
    chapters. In the next chapter, you’ll explore diffusion-based models, which are
    at the heart of text-to-image Transformers such as OpenAI’s DALL-E 2 and Google’s
    Imagen.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The performance-based representation of music enables us to represent a music
    piece as a sequence of notes, which include control messages and velocity values.
    These notes can be further reduced to four kinds of musical events: note-on, note-off,
    time-shift, and velocity. Each event type can assume various values. Consequently,
    we can transform a music piece into a sequence of tokens and then into indexes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A music Transformer adapts the Transformer architecture, originally designed
    for NLP tasks, for music generation. This model is designed to generate sequences
    of musical notes by learning from a large dataset of existing music. It is trained
    to predict the next note in a sequence based on previous notes, by recognizing
    patterns, structures, and relationships among various musical elements in the
    training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just as in text generation, we can use temperature to regulate the creativity
    of the generated music.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](#footnote-002-backlink))  Chloe Veltman, March 15, 2024\. “Just because
    your favorite singer is dead doesn’t mean you can’t see them ‘live.’” [https://mng.bz/r1de](https://mng.bz/r1de).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](#footnote-001-backlink))  Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob
    Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D.
    Hoffman, Monica Dinculescu, and Douglas Eck, 2018, “Music Transformer.” [https://arxiv.org/abs/1809.04281](https://arxiv.org/abs/1809.04281).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](#footnote-000-backlink))  See, for example, Hawthorne et al., 2018, “Enabling
    Factorized Piano Music Modeling and Generation with the MAESTRO Dataset.” [https://arxiv.org/abs/1810.12247](https://arxiv.org/abs/1810.12247).
  prefs: []
  type: TYPE_NORMAL
