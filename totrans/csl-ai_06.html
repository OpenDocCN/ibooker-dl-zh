<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">5</span> </span> <span class="chapter-title-text">Connecting causality and deep learning</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Incorporating deep learning into a causal graphical model</li>
<li class="readable-text" id="p3">Training a causal graphical model with a variational autoencoder</li>
<li class="readable-text" id="p4">Using causal methods to enhance machine learning</li>
</ul>
</div>
<div class="readable-text" id="p5">
<p>The title of this book is <em>Causal AI</em>, but how exactly does causality connect to AI? More specifically, how does causality connect with deep learning, the dominant paradigm in AI? In this chapter, I look at this question from two perspectives:</p>
</div>
<ul>
<li class="readable-text" id="p6"> <em>How to incorporate deep learning into a causal model</em><em> </em>—We’ll look at a causal model of a computer vision problem (section 5.1) and then train the deep causal image model (section 5.2). </li>
<li class="readable-text" id="p7"> <em>How to use causal reasoning to do better deep learning</em><em> </em>—We’ll look at a case study on independence of mechanism and semi-supervised learning (section 5.3.1 and 5.3.2), and we’ll demystify deep learning with causality (section 5.3.3). </li>
</ul>
<div class="readable-text" id="p8">
<p>The term <em>deep learning</em> broadly refers to applications of deep neural networks. It’s a machine learning approach that stacks many nonlinear models together in sequential layers, emulating the connections of neurons in brains. “Deep” refers to stacking many layers to achieve more modeling power, particularly in terms of modeling high-dimensional and nonlinear data, such as visual media and natural language text. Neural nets have been around for a while, but relatively recent advancements in hardware and automatic differentiation have made it possible to scale deep neural networks to extremely large sizes. That scaling is why, in recent years, there have been multiple cases of deep learning outperforming humans on many advanced inference and decision-making tasks, such as image recognition, natural language processing, game playing, medical diagnosis, autonomous driving, and generating lifelike text, images, and video.</p>
</div>
<div class="readable-text intended-text" id="p9">
<p>But asking how deep learning connects to causality can elicit frustrating answers. AI company CEOs and leaders in big tech fuel hype about the power of deep learning models and even claim they can learn the causal structure of the world. On the other hand, some leading researchers claim these models are merely “stochastic parrots” that can echo patterns of correlation that, while nuanced and complex, still fall short of true causal understanding.</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>Our goal in this chapter is to reconcile these perspectives. But skipping ahead, the main takeaway is that deep learning architecture can be integrated into a causal model and we can train the model using deep learning training techniques. But also, we can use causal reasoning to build better deep learning models and improve how we train them.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>We’ll anchor this idea in two case studies:</p>
</div>
<ul>
<li class="readable-text" id="p12"> Building a causal DAG for computer vision using a variational autoencoder </li>
<li class="readable-text" id="p13"> Implementing better semi-supervised learning using independence of mechanism </li>
</ul>
<div class="readable-text" id="p14">
<p>Other examples of the interplay of causality and AI that you’ll see in the rest of the book will build on the intuition we get from these case studies. For example, chapter 9 will illustrate counterfactual reasoning using a variational autoencoder like the one we’ll build in this chapter. In chapter 11, we’ll explore machine learning and probabilistic deep learning approaches for causal effect inference. Chapter 13 will show how to combine large language models and causal reasoning.</p>
</div>
<div class="readable-text intended-text" id="p15">
<p>We’ll start by considering how to incorporate deep learning into a causal model. </p>
</div>
<div class="readable-text" id="p16">
<h2 class="readable-text-h2" id="sigil_toc_id_103"><span class="num-string">5.1</span> A causal model of a computer vision problem</h2>
</div>
<div class="readable-text" id="p17">
<p>Let’s look at a computer vision problem that we can approach with a causal DAG. Recall the MNIST data from chapter 1, composed of images of digits and their labels, illustrated in figure 5.1.</p>
</div>
<div class="browsable-container figure-container" id="p18">
<img alt="figure" height="464" src="../Images/CH05_F01_Ness.png" width="900"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.1</span> MNIST data featuring images of handwritten digits and their digit labels</h5>
</div>
<div class="readable-text intended-text" id="p19">
<p>There is a related dataset called Typeface MNIST (TMNIST) that also features digit images and their digit labels. However, instead of handwritten digits, the images are digits rendered in 2,990 different fonts, illustrated in figure 5.2. For each image, in<span class="aframe-location"/> addition to a digit label, there is a font label. Examples of the font labels include “GrandHotel-Regular,” “KulimPark-Regular,” and “Gorditas-Bold.”<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p20">
<img alt="figure" height="452" src="../Images/CH05_F02_Ness.png" width="900"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.2</span> Examples from the Typeface MNIST, which is composed of typed digits with different typefaces. In addition to a digit label for each digit, there is a label for one of 2,990 different typefaces (fonts).</h5>
</div>
<div class="readable-text" id="p21">
<p>In this analysis, we’ll combine these datasets into one and build a simple deep causal generative model on that data. We’ll simplify the “fonts” label into a sample binary label that indicates “handwritten” for MNIST images and “typed” for the TMNIST images.</p>
</div>
<div class="readable-text intended-text" id="p22">
<p>We have seen how to build a causal generative model on top of a DAG. We factorized the joint distribution into a product of <em>causal Markov kernels</em> representing the conditional probability distributions for each node, conditional on their parents in the DAG. In our previous examples in pgmpy, we fit a conditional probability table for each of these kernels. </p>
</div>
<div class="readable-text intended-text" id="p23">
<p>You can imagine how hard it would be to use a conditional probability table to represent the conditional probability distribution of pixels in an image. But there is nothing stopping us from modeling the causal Markov kernel with a deep neural net, which we know is flexible enough to work with high-dimensional features like pixels. In this section, I’ll demonstrate how to use deep neural nets to model the causal Markov kernels defined by a causal DAG.</p>
</div>
<div class="readable-text" id="p24">
<h3 class="readable-text-h3" id="sigil_toc_id_104"><span class="num-string">5.1.1</span> Leveraging the universal function approximator</h3>
</div>
<div class="readable-text" id="p25">
<p>Deep learning is a highly effective universal function approximator. Let’s imagine there is a function that maps some set of inputs to some set of outputs, but we either don’t know the function or it’s too hard to write down in math or code. Given enough examples of those inputs and outputs, deep learning can approximate that function with high precision. Even if that function is nonlinear and high-dimensional, with enough data, deep learning will learn a good approximation.</p>
</div>
<div class="readable-text intended-text" id="p26">
<p>We regularly work with functions in causal modeling and inference, and sometimes it makes sense to approximate them, so long as the approximations preserve the causal information we care about. For example, the causal Markov property makes us interested in functions that map values of a node’s parents in the causal DAG to values (or probability values) of that node. </p>
</div>
<div class="readable-text intended-text" id="p27">
<p>In this section, we’ll do this mapping between a node and its parents with the variational autoencoder (VAE) framework. We’ll train two deep neural nets in the VAE, one of which maps parent cause variables to a distribution of the outcome variable, and another that maps the outcome variable to a distribution of the cause variables. This example will showcase the use of deep learning when causality is nonlinear and high-dimensional; the effect variable will be an image represented as a high-dimensional array, and the cause variables will represent the contents of the image.</p>
</div>
<div class="readable-text" id="p28">
<h3 class="readable-text-h3" id="sigil_toc_id_105"><span class="num-string">5.1.2</span> Causal abstraction and plate models</h3>
</div>
<div class="readable-text" id="p29">
<p>But what does it mean to build a causal model of an image? Images are comprised of pixels arranged in a grid. As data, we can represent that pixel grid as a matrix of numerical values corresponding to color. In the case of both MNIST and TMNIST, the image is a 28 <span class="regular-symbol">×</span> 28 matrix of grayscale values, as illustrated in figure 5.3. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p30">
<img alt="figure" height="330" src="../Images/CH05_F03_Ness.png" width="540"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.3</span> An MNIST image of “6” (left) and a TMNIST image of “7”. In their raw form, these are 28 <span class="regular-symbol">×</span> 28 matrices of numeric values corresponding to grayscale values.</h5>
</div>
<div class="readable-text" id="p31">
<p>A typical machine learning model looks at this 28 <span class="regular-symbol">×</span> 28 matrix of pixels as 28 <span class="regular-symbol">×</span> 28 = 784 features. The machine learning algorithm learns statistical patterns connecting the pixels to one another and their labels. Based on this fact, one might be tempted to treat each individual pixel as a node in the naive causal DAG, as in figure 5.4, where for visual simplicity I’ve drawn 16 pixels (an arbitrary number) instead of all 784. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p32">
<img alt="figure" height="366" src="../Images/CH05_F04_Ness.png" width="341"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.4</span> What a naive causal DAG might look like for an image represented by a 4 <span class="regular-symbol">×</span> 4 matrix</h5>
</div>
<div class="readable-text intended-text" id="p33">
<p>In figure 5.4, there are edges from the <em>digit</em> and <em>is-handwritten</em> variables to each pixel. Further, there are examples of edges representing possible causal relationships <em>between</em> pixels. Causal edges between pixels imply the color of one pixel is a cause of another. Perhaps most of these relationships are between nodes that are close, with a few far-reaching edges. But how would we know if one pixel causes another? If two pixels are connected, how would we know the direction of causality?</p>
</div>
<div class="readable-text" id="p34">
<h4 class="readable-text-h4 sigil_not_in_toc">Working at the right level of abstraction</h4>
</div>
<div class="readable-text" id="p35">
<p>With these connections among only 16 pixels, the naive DAG in figure 5.4 is already quite unwieldy. It would be much worse with 784 pixels. Aside from the unwieldiness of a DAG, the problem with a pixel-level model is that our causal questions are generally not at the pixel level—we’d probably never ask “what is the causal effect of this pixel on that pixel?” In other words, the pixel is too low a level of abstraction, which is why thinking about causal relationships between individual pixels feels a bit absurd.</p>
</div>
<div class="readable-text intended-text" id="p36">
<p>In applied statistics domains, such as econometrics, social science, public health, and business, our data has variables like per capita income, revenue, location, age, etc. These variables are typically already at the level of abstraction we want to think about when we get the data. But modern machine learning focuses on many perception problems from raw media, such as images, video, text, and sensor data. We don’t generally want to do causal reasoning at the low level of these features. Our causal questions are usually about the high-level abstractions behind these low-level features. We need to model at these higher abstraction levels.</p>
</div>
<div class="readable-text intended-text" id="p37">
<p>Instead of thinking about individual pixels, we’ll think about the entire image. We’ll define a variable <em>X</em> to represent how the image appears; i.e., <em>X</em> is a matrix random variable representing pixels. Figure 5.5 illustrates a causal DAG for the TMNIST case. Simply put, the identity of the digits (0–9) and the font (2,990 possible values) are the causes, and the image is the effect. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p38">
<img alt="figure" height="171" src="../Images/CH05_F05_Ness.png" width="586"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.5</span> A simple causal DAG that represents the implied DGP behind Typeface MNIST</h5>
</div>
<div class="readable-text" id="p39">
<p>In this case, we are using the causal DAG to make an assertion that the label causes the image. That is not always the case, as we’ll discuss in our case study on semi-supervised learning in section 5.3. As with all causal models, it depends on the data generating process (DGP) within a domain.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p40">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Why say that the digit <em>causes</em> the image?</h5>
</div>
<div class="readable-text" id="p41">
<p>Plato’s allegory of the cave describes a group of people who have lived in a cave all their lives, without seeing the world. They face a blank cave wall and watch shadows projected on the wall from objects passing in front of a fire behind them. The shadows are simplified and sometimes distorted representations of the true objects passing in front of the fire. In this case, we can think of the form of the objects as being the cause of the shadow.</p>
</div>
<div class="readable-text" id="p42">
<p>Analogously, the true form of the digit label causes the representation in the image. The MNIST images were written by people, and they have some <em>Platonic ideal</em> of the digit in their head that they want to render onto paper. In the process, that ideal is distorted by motor variation in the hand, the angle of the paper, the friction of the pen on the paper, and other factors—the rendered image is a “shadow” caused by that “ideal.” </p>
</div>
<div class="readable-text" id="p43">
<p>This idea is related to a concept called “vision as inverse graphics” in computer vision (see <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for sources with more information). In causal terms, the takeaway is that when we are analyzing images rendered from raw signals from the environment, and the task is to infer the actual objects or events that resulted in those signals, causality flows from those objects or events to the signals. The inference task is to use the observed signals (shadows on the cave wall) to infer the nature of the causes (objects in front of the fire).</p>
</div>
<div class="readable-text" id="p44">
<p>That said, images can be causes too. For example, if you were modeling how people behave <em>after</em> seeing an image in a mobile app (e.g., whether they “click”, “like”, or “swipe left”), you could model the image as a cause of the behavior.</p>
</div>
</div>
<div class="readable-text" id="p45">
<h4 class="readable-text-h4 sigil_not_in_toc">Plate modeling</h4>
</div>
<div class="readable-text" id="p46">
<p>Modeling 2,990 fonts in our TMNIST data is overkill for our purposes here. Instead, I combined these datasets into one—half from MNIST and half from Typeface MNIST. Along with the “digit” label, I’m just going to have a simple binary label called “is-handwritten”, which is 1 (true) for images of handwritten digits from MNIST and 0 (false) for images of “typed” digits from TMNIST. We can modify our causal DAG to get figure 5.6.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p47">
<img alt="figure" height="141" src="../Images/CH05_F06_Ness.png" width="767"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.6</span> A causal DAG representing the combined MNIST and TMNIST data, where “is-handwritten” is 1 (MNIST images) or 0 (TMNIST images)</h5>
</div>
<div class="readable-text" id="p48">
<p>Plate modeling is a visualizing technique used in probabilistic machine learning that provides an excellent way to visualize the higher-level abstractions while preserving the lower-level dimensional detail. Plate notation is a method of visually representing variables that repeat in a DAG (e.g., <em>X</em><sub>1</sub> to <em>X</em><sub>16</sub> in figure 5.4)—in our case, we have repetition of the pixels. </p>
</div>
<div class="readable-text intended-text" id="p49">
<p>Instead of drawing each of the 784 pixels as an individual node, we use a rectangle or “plate” to group repeating variables into subgraphs. We then write a number on the plate to represent the number of repetitions of the entities on the plate. Plates can nest within one another to indicate repeated entities nested within repeated entities. Each plate gets a letter subscript indexing the elements on that plate. The causal DAG in figure 5.7 represents one image. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p50">
<img alt="figure" height="226" src="../Images/CH05_F07_Ness.png" width="349"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.7</span> A plate model representation of the causal DAG. Plates represent repeating variables, in this case 28 <span class="regular-symbol">×</span> 28 = 784 pixels. <em>X</em><em><sub>j</sub></em> is the <em>j</em><sup>th</sup> pixel.</h5>
</div>
<div class="readable-text intended-text" id="p51">
<p>During training, we’ll have a large set of training images. Next, we’ll modify the DAG to capture all the images in the training data.</p>
</div>
<div class="readable-text" id="p52">
<h2 class="readable-text-h2" id="sigil_toc_id_106"><span class="num-string">5.2</span> Training a neural causal model</h2>
</div>
<div class="readable-text" id="p53">
<p>To train our neural causal model, we need to load and prepare the training data, create the architecture of our model, write a training procedure, and implement some tools for evaluating how well training is progressing. We’ll start by loading and preparing the data.</p>
</div>
<div class="readable-text" id="p54">
<h3 class="readable-text-h3" id="sigil_toc_id_107"><span class="num-string">5.2.1</span> Setting up the training data</h3>
</div>
<div class="readable-text" id="p55">
<p>Our training data has <em>N</em> example images, so we need our plate model to represent all <em>N</em> images in the training data, half handwritten and half typed. We’ll add another plate corresponding to repeating <em>N</em> sets of images and labels, as in figure 5.8.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p56">
<img alt="figure" height="295" src="../Images/CH05_F08_Ness.png" width="413"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.8</span> The causal model with an additional plate for the <em>N</em> images in the data</h5>
</div>
<div class="readable-text" id="p57">
<p>Now we have a causal DAG that illustrates both our desired level of causal abstraction as well as the dimensional information we need to start training the neural nets in the model.</p>
</div>
<div class="readable-text" id="p58">
<p>Let’s first load Pyro and some other libraries and set some hyperparameters. </p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p59">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Setting up your environment</h5>
</div>
<div class="readable-text" id="p60">
<p>This code was written using Python version 3.10.12 and tested in Google Colab. The versions of the main libraries include Pyro (pyro-ppl) version 1.8.4, torch version 2.2.1, torchvision version 0.18.0+cu121, and pandas version 2.0.3. We’ll also use matplotlib for plotting.</p>
</div>
<div class="readable-text" id="p61">
<p>Visit <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for links to a notebook that will load in Google Colab.</p>
</div>
</div>
<div class="readable-text" id="p62">
<p>If GPUs are available on your device, it will be faster to train the neural nets with CUDA (a platform for parallel computing on GPUs). We’ll run a bit of code that lets us toggle it on. If you don’t have GPUs or aren’t sure if you do, leave <code>USE_CUDA</code> set to <code>False</code>.</p>
</div>
<div class="browsable-container listing-container" id="p63">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.1</span> Setting up for GPU training</h5>
<div class="code-area-container">
<pre class="code-area">import torch   
USE_CUDA = False  <span class="aframe-location"/> #1
DEVICE_TYPE = torch.device("cuda" if USE_CUDA else "cpu")</pre>
<div class="code-annotations-overlay-container">
     #1 Use CUDA if it is available.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p64">
<p>First, we’ll make a subclass of the <code>Dataset</code> class (a class for loading and preprocessing data) that will let us combine the MNIST and TMNIST datasets.</p>
</div>
<div class="browsable-container listing-container" id="p65">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.2</span> Combining the data</h5>
<div class="code-area-container">
<pre class="code-area">from torch.utils.data import Dataset

import numpy as np
import pandas as pd
from torchvision import transforms

class CombinedDataset(Dataset):   <span class="aframe-location"/> #1
    def __init__(self, csv_file):
        self.dataset = pd.read_csv(csv_file)

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        images = self.dataset.iloc[idx, 3:]    <span class="aframe-location"/> #2
        images = np.array(images, dtype='float32')/255.   #2
        images = images.reshape(28, 28)    #2
        transform = transforms.ToTensor()     #2
        images = transform(images)     #2
        digits = self.dataset.iloc[idx, 2]    <span class="aframe-location"/> #3
        digits = np.array([digits], dtype='int')     #3
       <span class="aframe-location"/> is_handwritten = self.dataset.iloc[idx, 1]     #4
        is_handwritten = np.array([is_handwritten], dtype='float32')    #4
        return images, digits, is_handwritten   <span class="aframe-location"/> #5</pre>
<div class="code-annotations-overlay-container">
     #1 This class loads and processes a dataset that combines MNIST and Typeface MNIST. The output is a torch.utils.data.Dataset object.
     <br/>#2 Load, normalize, and reshape the images to 28 
     <span class="regular-symbol">×</span> 28 pixels.
     <br/>#3 Get and process the digit labels, 0–9.
     <br/>#4 1 for handwritten digits (MNIST), and 0 for “typed” digits (TMNIST)
     <br/>#5 Return a tuple of the image, the digit label, and the is_handwritten label.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p66">
<p>Next, we’ll use the <code>DataLoader</code> class (which allows for efficient data iteration and batching during training) to load the data from a CSV file in GitHub and split it into training and test sets.</p>
</div>
<div class="browsable-container listing-container" id="p67">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.3</span> Downloading, splitting, and loading the data</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">from torch.utils.data import DataLoader
from torch.utils.data import random_split

def setup_dataloaders(batch_size=64, use_cuda=USE_CUDA):    <span class="aframe-location"/> #1
    combined_dataset = CombinedDataset(
"https://raw.githubusercontent.com/altdeep/causalML/master/datasets
<span class="">↪</span>/combined_mnist_tmnist_data.csv"
    )
    n = len(combined_dataset)    <span class="aframe-location"/> #2
    train_size = int(0.8 * n)    #2
    test_size = n - train_size    #2
    train_dataset, test_dataset = random_split(     #2
        combined_dataset,   #2
        [train_size, test_size],    #2
        generator=torch.Generator().manual_seed(42)   #2
    )     #2
    kwargs = {'num_workers': 1, 'pin_memory': use_cuda} #2
    train_loader = DataLoader(    <span class="aframe-location"/> #3
        train_dataset,     #3
        batch_size=batch_size,     #3
        shuffle=True,     #3
        **kwargs    #3
    )     #3
    test_loader = DataLoader(   #3
        test_dataset,    #3
        batch_size=batch_size,    #3
        shuffle=True,     #3
        **kwargs   #3
    )     #3
    return train_loader, test_loader</pre>
<div class="code-annotations-overlay-container">
     #1 Set up the data loader that loads the data and splits it into training and test sets.
     <br/>#2 Allot 80% of the data to training data and the remaining 20% to test data.
     <br/>#3 Create training and test loaders.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p68">
<p>Next, we’ll set up the full variational autoencoder.</p>
</div>
<div class="readable-text" id="p69">
<h3 class="readable-text-h3" id="sigil_toc_id_108"><span class="num-string">5.2.2</span> Setting up the variational autoencoder</h3>
</div>
<div class="readable-text" id="p70">
<p>The variational autoencoder (VAE) is perhaps the simplest deep probabilistic machine learning modeling approach. In the typical setup for applying VAE to images, we introduce a latent continuous variable <em>Z</em> that has a smaller dimension than the image data. Here, <em>dimensionality</em> refers to the number of elements in a vector representation of the data. For instance, our image is a 28 <span class="regular-symbol">×</span> 28 matrix of pixels, or alternatively a vector with dimension 28 <span class="regular-symbol">×</span> 28 = 784. By having a much smaller dimension than the image dimension, the latent variable <em>Z</em> represents a compressed encoding of the image information. For each image in the dataset, there is a corresponding latent <em>Z</em> value that represents an encoding of that image. This setup is illustrated in figure 5.9.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p71">
<img alt="figure" height="315" src="../Images/CH05_F09_Ness.png" width="514"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.9</span> The causal DAG plate model, extended to include an “encoding” variable <em>Z</em>. During training, the variable is latent, indicated by the dashed line. (After the model is deployed, <em>digit</em> and <em>is-handwritten</em><em> </em>are also latent).</h5>
</div>
<div class="readable-text" id="p72">
<p><em>Z</em> appears as a new parent in the causal DAG, but it’s important to note that the classical VAE framework does not define <em>Z</em> as causal. Now that we are thinking causally, we’ll give <em>Z</em> a causal interpretation. Specifically, as parents of the image node in the DAG, we view <em>digit</em> and <em>is-handwritten</em> as causal drivers of what we see in the image. Yet there are other elements of the image (e.g., the stroke thickness of a handwritten character, or the font of a typed character) that are also causes of what we see in the image. We’ll think of <em>Z</em> as a continuous latent <em>stand-in</em> for all of these other causes of the image that we are not explicitly modeling, like <em>digit</em> and <em>is-handwritten</em>. Examples of these causes include the nuance of the various fonts in the TMNIST labels and all of the variations in the handwritten digits due to different writers and motor movements as they wrote. With that in mind, we can view <em>P</em>(<em>X</em>| <em>digit</em>, <em>is-handwritten</em>, <em>Z</em>) as the causal Markov kernel of <em>X</em>. That said, it is important to remember that the representation we learn for <em>Z</em> is a stand-in for latent causes and is not the same as learning the actual latent causes.</p>
</div>
<div class="readable-text intended-text" id="p73">
<p>The VAE setup will train two deep neural networks: One called an “encoder”, which encodes an image into a value for <em>Z</em>. The other neural network, called the “decoder,” will align with our DAG. The decoder generates an image from the <em>digit</em> label, the <em>is-handwritten</em> label, and a <em>Z</em> value, as in figure 5.10.</p>
</div>
<div class="readable-text intended-text" id="p74">
<p>The decoder acts like a rendering engine; given a <em>Z</em> encoding value and the values for <em>digit</em> and <em>is-handwritten</em>, it renders an image.</p>
</div>
<div class="browsable-container figure-container" id="p75">
<img alt="figure" height="438" src="../Images/CH05_F10_Ness.png" width="802"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.10</span> The decoder neural network generates as output an image <em>X</em> from inputs <em>Z</em> and the labels <em>is-handwritten</em> and <em>digit</em>. As with any neural net, the inputs are processed through one or more “hidden layers.”</h5>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p76">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Key VAE concepts so far</h5>
</div>
<div class="readable-text" id="p77">
<p><em>Variational autoencoder (VAE)</em>—A popular framework in deep generative modeling. We’re using it to model a causal Markov kernel in a causal model.</p>
</div>
<div class="readable-text" id="p78">
<p><em>Decoder</em>—We use the decoder as the model of the causal Markov kernel. It maps the observed causes <em>is-handwritten</em> and <em>digit</em>, and the latent variable <em>Z</em>, to our image outcome variable <em>X</em>. </p>
</div>
</div>
<div class="readable-text" id="p79">
<p>This VAE approach allows us to use a neural net, a la the decoder, to capture the complex and nonlinear relations needed to model the image as an effect caused by <em>digit</em> and <em>is-handwritten</em>. Modeling images would be difficult with the conditional probability tables and other simple parameterizations of causal Markov kernels we’ve discussed previously.</p>
</div>
<div class="readable-text intended-text" id="p80">
<p>First, let’s implement the decoder. We’ll pass in arguments <code>z_dim</code> for the dimension of <em>Z</em> and <code>hidden_dim</code> for the dimension (width) of the hidden layers. We’ll specify these variables when we instantiate the full VAE. The decoder combines the latent vector <em>Z</em> with additional inputs—the variable representing the <em>digit,</em> and <em>is-handwritten</em> (a binary indicator of whether the digit is handwritten). It will produce a 784-dimensional output vector representing an image of size 28 <span class="regular-symbol">×</span> 28 pixels. This output vector contains the parameters for a Bernoulli distribution for each pixel, essentially modeling the likelihood of each pixel being “on.” The class uses two fully connected layers (<code>fc1</code> and <code>fc2</code>), and employs <code>Softplus</code> and <code>Sigmoid</code> “activation functions,” which are the hallmarks of how neural nets emulate neurons. </p>
</div>
<div class="browsable-container listing-container" id="p81">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.4</span> Implement the decoder</h5>
<div class="code-area-container">
<pre class="code-area">from torch import nn

class Decoder(nn.Module):   <span class="aframe-location"/> #1
    def __init__(self, z_dim, hidden_dim):
        super().__init__()
        <span class="aframe-location"/>img_dim = 28 * 28     #2
        <span class="aframe-location"/>digit_dim = 10    #3
        is_handwritten_dim = 1   <span class="aframe-location"/> #4
        self.softplus = nn.Softplus()    <span class="aframe-location"/> #5
        self.sigmoid = nn.Sigmoid()    #5
        encoding_dim = z_dim + digit_dim + is_handwritten_dim  <span class="aframe-location"/>   #6
        self.fc1 = nn.Linear(encoding_dim, hidden_dim)   #6
        self.fc2 = nn.Linear(hidden_dim, img_dim)   <span class="aframe-location"/> #7

    def forward(self, z, digit, is_handwritten):    <span class="aframe-location"/> #8
    <span class="aframe-location"/>    input = torch.cat([z, digit, is_handwritten], dim=1) #9
   <span class="aframe-location"/>     hidden = self.softplus(self.fc1(input))    #10
       <span class="aframe-location"/> img_param = self.sigmoid(self.fc2(hidden))    #11
        return img_param</pre>
<div class="code-annotations-overlay-container">
     #1 A class for the decoder used in the VAE
     <br/>#2 Image is 28 <span class="regular-symbol">×</span> 28 pixels.
     <br/>#3 Digit is one-hot encoded digits 0–9, i.e., a vector of length 10.
     <br/>#4 An indicator for whether the digit is handwritten that has size 1
     <br/>#5 Softplus and sigmoid are nonlinear transforms (activation functions) used in mapping between layers.
     <br/>#6 fc1 is a linear function that maps the Z vector, the digit, and is_handwritten to a linear output, which is passed through a softplus activation function to create a hidden layer-a vector whose length is given by hidden_layer.
     <br/>#7 fc2 linearly maps the hidden layer to an output passed to a sigmoid function. The resulting value is between 0 and 1.
     <br/>#8 Define the forward computation from the latent Z variable value to a generated X variable value.
     <br/>#9 Combine Z and the labels.
     <br/>#10 Compute the hidden layer.
     <br/>#11 Pass the hidden layer to a linear transform and then to a sigmoid transform to output a parameter vector of length 784. Each element of the vector corresponds to a Bernoulli parameter value for an image pixel.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p82">
<p>We use the decoder in the causal model. Our causal DAG acts as the scaffold for a causal probabilistic machine learning model that, with the help of the decoder, defines a joint probability distribution on {<em>is-handwritten</em>, <em>digit</em>, <em>X</em>, <em>Z</em>}, where <em>Z</em> is latent. We can use the model to calculate the likelihood of the training data for a given value of <em>Z</em>.</p>
</div>
<div class="readable-text intended-text" id="p83">
<p>The latent variable <code>z</code>, the digit identity represented as a one-hot vector <code>digit</code>, and a binary indicator <code>is_handwritten</code> are modeled as samples from standard distributions. These variables are then fed into the decoder to produce parameters (<code>img_param</code>) for a Bernoulli distribution representing individual pixel probabilities of an image.</p>
</div>
<div class="readable-text intended-text" id="p84">
<p>Note, using the Bernoulli distribution to model the pixels is a bit of a hack. The pixels are not binary black and white outcomes—they have grayscale values. The line <code>dist.enable_validation(False)</code> lets us cheat by getting Bernoulli log likelihoods for the images given a decoder’s <code>img_param</code> output.</p>
</div>
<div class="readable-text intended-text" id="p85">
<p>The following model code is a class method for a PyTorch neural network module. We’ll see the entire class later.</p>
</div>
<div class="browsable-container listing-container" id="p86">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.5</span> The causal model</h5>
<div class="code-area-container">
<pre class="code-area">import pyro
import pyro.distributions as dist

<span class="aframe-location"/>dist.enable_validation(False)    #1
def model(self, data_size=1):    <span class="aframe-location"/> #2
    pyro.module("decoder", self.decoder)    #2
    options = dict(dtype=torch.float32, device=DEVICE_TYPE)
    z_loc = torch.zeros(data_size, self.z_dim, **options)   <span class="aframe-location"/> #3
    z_scale = torch.ones(data_size, self.z_dim, **options)   #3
    z = pyro.sample("Z", dist.Normal(z_loc, z_scale).to_event(1))    #3
    p_digit = torch.ones(data_size, 10, **options)/10    <span class="aframe-location"/> #4
    digit = pyro.sample(     #4
        "digit",    #4
        dist.OneHotCategorical(p_digit)     #4
    )    #4
    p_is_handwritten = torch.ones(data_size, 1, **options)/2    <span class="aframe-location"/> #5
    is_handwritten = pyro.sample(    #5
        "is_handwritten",     #5
        dist.Bernoulli(p_is_handwritten).to_event(1)     #5
    )     #5
    img_param = self.decoder(z, digit, is_handwritten)   <span class="aframe-location"/>#6
    img = pyro.sample("img", dist.Bernoulli(img_param).to_event(1))  <span class="aframe-location"/> #7
    return img, digit, is_handwritten</pre>
<div class="code-annotations-overlay-container">
     #1 Disabling distribution validation lets Pyro calculate log likelihoods for pixels even though the pixels are not binary values.
     <br/>#2 The model of a single image. Within the method, we register the decoder, a PyTorch module, with Pyro. This lets Pyro know about the parameters inside of the decoder network.
     <br/>#3 We model the joint probability of Z, digit, and is_handwritten, sampling each from canonical distributions. We sample Z from a multivariate normal with location parameter z_loc (all zeros) and scale parameter z_scale (all ones).
     <br/>#4 We also sample the digit from a one-hot categorical distribution. Equal probability is assigned to each digit.
     <br/>#5 We similarly sample the is_handwritten variable from a Bernoulli distribution.
     <br/>#6 The decoder maps digit, is_handwritten, and Z to a probability parameter vector.
     <br/>#7 The parameter vector is passed to the Bernoulli distribution, which models the pixel values in the data. The pixels are not technically Bernoulli binary variables, but we’ll relax this assumption.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p87">
<p>The preceding <code>model</code> method represents the DGP for one image. The <code>training_ model</code> method in the following listing applies that <code>model</code> method to the <em>N</em> images in the training data.</p>
</div>
<div class="browsable-container listing-container" id="p88">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.6</span> Method for applying <code>model</code> to <em>N</em> images in data</h5>
<div class="code-area-container">
<pre class="code-area">def training_model(self, img, digit, is_handwritten, batch_size):    <span class="aframe-location"/> #1
   <span class="aframe-location"/> conditioned_on_data = pyro.condition(     #2
        self.model,
        data={
            "digit": digit,
            "is_handwritten": is_handwritten,
            "img": img
        }
    )
    with pyro.plate("data", batch_size):<span class="aframe-location"/>    #3
        img, digit, is_handwritten = conditioned_on_data(batch_size)
    return img, digit, is_handwritten</pre>
<div class="code-annotations-overlay-container">
     #1 The model represents the DGP for one image. The training_model applies that model to the N images in the training data.
     <br/>#2 Now we condition the model on the evidence in the training data.
     <br/>#3 This context manager represents the N-size plate representing repeating IID examples in the data in figure 5.9. In this case, N is the batch size. It works like a for loop, iterating over each data unit in the batch.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p89">
<p>Our probabilistic machine learning model models the joint distribution of {<em>Z</em>, <em>X</em>, <em>digit</em>, <em>is-handwritten</em>}. But since <em>Z</em> is latent, the model will need to learn <em>P</em>(<em>Z</em>|<em>X</em>, <em>digit</em>, <em>is-handwritten</em>). Given that we use the decoder neural net to go from <em>Z</em> and the labels to <em>X</em>, the distribution of <em>Z</em>, given <em>X</em> and the labels will be complex. We will use <em>variational inference</em>, a technique where we first define an approximating distribution <em>Q</em>(<em>Z</em>|<em>X</em>, <em>digit</em>, <em>is-handwritten</em>), and try to make that distribution as close to <em>P</em>(<em>Z</em>|<em>X</em>, <em>digit</em>, <em>is-handwritten</em>) as we can.</p>
</div>
<div class="readable-text intended-text" id="p90">
<p>The main ingredient of the approximating distribution is the second neural net in the VAE framework, the encoder, illustrated in figure 5.11. The encoder maps an observed image and its labels in the training data to a latent <em>Z</em> variable. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p91">
<img alt="figure" height="467" src="../Images/CH05_F11_Ness.png" width="803"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.11</span> The encoder maps actual images as input to the latent <em>Z</em> variable as output.</h5>
</div>
<div class="readable-text" id="p92">
<p>The encoder does the work of compressing the information in the image into a lower-dimensional encoding.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p93">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Key VAE concepts so far</h5>
</div>
<div class="readable-text" id="p94">
<p><em>Variational autoencoder (VAE)</em>—A popular framework in deep generative modeling. We’re using it to model a causal Markov kernel in our causal model.</p>
</div>
<div class="readable-text" id="p95">
<p><em>Decoder</em>—We use the decoder as the model of the causal Markov kernel. It maps observed causes <em>is-handwritten</em> and <em>digit</em>, and the latent variable <em>Z</em>, to our image outcome variable <em>X</em>.</p>
</div>
<div class="readable-text" id="p96">
<p><em>Encoder</em>—The encoder maps the image, <em>digit</em>, and <em>is-handwritten</em> indicator to the parameters of a distribution where we can draw samples of <em>Z</em>.</p>
</div>
</div>
<div class="readable-text" id="p97">
<p>In the following code, the encoder takes as input an image, a digit label, and the <em>is-handwritten</em> indicator. These inputs are concatenated and passed through a series of fully connected layers with Softplus activation functions. The final output of the encoder consists of two vectors representing the location (<code>z_loc</code>) and scale (<code>z_scale</code>) parameters of the latent space distribution on <em>Z</em>, given observed values for <em>image</em> (<code>img</code>), <em>digit</em> (<code>digit</code>), and <em>is-handwritten</em> (<code>is_handwritten</code>). </p>
</div>
<div class="browsable-container listing-container" id="p98">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.7</span> Implement the encoder</h5>
<div class="code-area-container">
<pre class="code-area">class Encoder(nn.Module):    <span class="aframe-location"/> #1
    def __init__(self, z_dim, hidden_dim):
        super().__init__()
        img_dim = 28 * 28    <span class="aframe-location"/> #2
       <span class="aframe-location"/> digit_dim = 10  #3
        is_handwritten_dim = 1
        self.softplus = nn.Softplus()   <span class="aframe-location"/> #4
        input_dim = img_dim + digit_dim + is_handwritten_dim   <span class="aframe-location"/>  #5
        self.fc1 = nn.Linear(input_dim, hidden_dim)   #5
        self.fc21 = nn.Linear(hidden_dim, z_dim)<span class="aframe-location"/> #6
        self.fc22 = nn.Linear(hidden_dim, z_dim)    #6

    def forward(self, img, digit, is_handwritten):   <span class="aframe-location"/> #7
       <span class="aframe-location"/> input = torch.cat([img, digit, is_handwritten], dim=1)     #8
        hidden = self.softplus(self.fc1(input))   <span class="aframe-location"/> #9
        z_loc = self.fc21(hidden)    <span class="aframe-location"/> #10
        z_scale = torch.exp(self.fc22(hidden))  #10
        return z_loc, z_scale</pre>
<div class="code-annotations-overlay-container">
     #1 The encoder is an instance of a PyTorch module.
     <br/>#2 The input image is 28 
     <span class="regular-symbol">×</span> 28 = 784 pixels.
     <br/>#3 The digit dimension is 10.
     <br/>#4 In the encoder, we’ll only use the softplus transform (activation function).
     <br/>#5 The linear transform fc1 combines with the softplus to map the 784-dimensional pixel vector, 10-dimensional digit label vector, and 2-dimensional is_handwritten vector to the hidden layer.
     <br/>#6 The linear transforms, fc21 and fc22, will combine with the softplus to map the hidden vector to Z’s vector space.
     <br/>#7 Define the reverse computation from an observed X variable value to a latent Z variable value.
     <br/>#8 Combine the image vector, digit label, and is_handwritten label into one input.
     <br/>#9 Map the input to the hidden layer.
     <br/>#10 The VAE framework will sample Z from a normal distribution that approximates P(Z|img, digit, is_handwritten). The final transforms map the hidden layer to a location and scale parameter for that normal distribution.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p99">
<p>The output of the encoder produces the parameters of a distribution on <em>Z</em>. During training, given an image and its labels (<em>is-handwritten</em> and <em>digit</em>), we want to get a good value of <em>Z</em>, so we write a <em>guide function </em>that will use the encoder to sample values of <em>Z</em>.</p>
</div>
<div class="browsable-container listing-container" id="p100">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.8</span> The guide function</h5>
<div class="code-area-container">
<pre class="code-area">def training_guide(self, img, digit, is_handwritten, batch_size):    <span class="aframe-location"/> #1
    pyro.module("encoder", self.encoder)   <span class="aframe-location"/> #2
    options = dict(dtype=torch.float32, device=DEVICE_TYPE)
 <span class="aframe-location"/>   with pyro.plate("data", batch_size):     #3
        z_loc, z_scale = self.encoder(img, digit, is_handwritten)   <span class="aframe-location"/> #4
        normal_dist = dist.Normal(z_loc, z_scale).to_event(1)    #4
        z = pyro.sample("Z", normal_dist)    <span class="aframe-location"/> #5</pre>
<div class="code-annotations-overlay-container">
     #1 training_guide is a method of the VAE that will use the encoder.
     <br/>#2 Register the encoder so Pyro is aware of its weight parameters.
     <br/>#3 This is the same plate context manager for iterating over the batch data that we see in the training_model.
     <br/>#4 Use the encoder to map an image and its labels to parameters of a normal distribution.
     <br/>#5 Sample Z from that normal distribution
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p101">
<p>We combine these elements into one PyTorch neural network module representing the VAE. We’ll initialize the latent dimension of <em>Z</em> to be 50. We’ll set our hidden layer dimension to 400 in both the encoder and decoder. That means that given a dimension of 28 <span class="regular-symbol">×</span> 28 for the image, 1 for the binary <em>is-handwritten</em>, and 10 for the one-hot-encoded <em>digit</em> variable, we’ll take a 28 <span class="regular-symbol">×</span> 28 + 1 + 10 = 795-dimensional feature vector and compress it down to a 400-dimensional hidden layer, and then compress that down to a 50-dimensional location and scale parameter for <em>Z</em>’s multivariate normal (Gaussian) distribution. The decoder takes as input the values of <em>digit</em>, <em>is-handwritten</em>, and <em>Z</em> and maps these to a 400-dimensional hidden layer and to the 28 <span class="regular-symbol">×</span> 28–dimensional image. These architectural choices of latent variable dimension, number of layers, activation functions, and hidden layer dimensions depend on the problem and are typically selected by convention or by experimenting with different values.</p>
</div>
<div class="readable-text intended-text" id="p102">
<p>Now we’ll put these pieces together into the full VAE class. </p>
</div>
<div class="browsable-container listing-container" id="p103">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.9</span> Full VAE class</h5>
<div class="code-area-container">
<pre class="code-area">class VAE(nn.Module):
    def __init__(
        self,
        z_dim=50,   <span class="aframe-location"/> #1
        hidden_dim=400,   <span class="aframe-location"/> #2
        use_cuda=USE_CUDA,
    ):
        super().__init__()
        self.use_cuda = use_cuda
        self.z_dim = z_dim
        self.hidden_dim = hidden_dim
        self.setup_networks()

    def setup_networks(self):    <span class="aframe-location"/> #3
        self.encoder = Encoder(self.z_dim, self.hidden_dim)
        self.decoder = Decoder(self.z_dim, self.hidden_dim)
        if self.use_cuda:
            self.cuda()

    model = model    <span class="aframe-location"/> #4
    training_model = training_model    #4
    training_guide = training_guide    #4</pre>
<div class="code-annotations-overlay-container">
     #1 Set the latent dimension to 50.
     <br/>#2 Set the hidden layers to have a dimension of 400.
     <br/>#3 Set up the encoder and decoder.
     <br/>#4 Add in the methods for model, training_model, and training_guide.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p104">
<p>Having specified the VAE, we can now move on to training.</p>
</div>
<div class="readable-text" id="p105">
<h3 class="readable-text-h3" id="sigil_toc_id_109"><span class="num-string">5.2.3</span> The training procedure</h3>
</div>
<div class="readable-text" id="p106">
<p>We know we have a good generative model when the encoder can encode an image into a latent value of <em>Z</em>, and then decode it into a <em>reconstructed</em> version of the image. We can minimize the <em>reconstruction error</em>—the difference between original and reconstructed images—in the training data.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p107">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">A bit of perspective on the “variational inference” training algorithm</h5>
</div>
<div class="readable-text" id="p108">
<p>In this section, you’ll see a bunch of jargon relating to variational inference, which is the algorithm we’ll use for training. It helps to zoom out and examine why we’re using this algorithm. There are many statistical estimators and algorithms both for fitting neural net weights and other parameters and for causal inference. One of these is variational inference. </p>
</div>
<div class="readable-text" id="p109">
<p>To be clear, variational inference is not a “causal” idea. It is just another probabilistic inference algorithm. In this book, I favor this inference algorithm more than others because it scales well even when variables in the DAG are latent in the training data, and it works with deep neural nets and leverages deep learning frameworks like PyTorch. This opens the door to reasoning causally about richer modalities such as text, images, video, etc., whereas traditional causal inference estimators were developed for numerical data. Further, we can tailor the method to different problems (see the discussion of “commodification of inference” in chapter 1) and leverage domain knowledge during inference (such as by using knowledge of conditional independence in the guide). Finally, the core concepts of variational inference show up across many deep generative modeling approaches (such as latent diffusion models).</p>
</div>
</div>
<div class="readable-text" id="p110">
<p>In practice, solely minimizing reconstruction error leads to overfitting and other issues, so we’ll opt for a probabilistic approach: given an image, we’ll use our guide function to sample a value of <em>Z</em> from <em>P</em>(<em>Z</em>|<em>image</em>, <em>is-handwritten</em>, <em>digi</em><em>t</em>). Then we’ll plug that value into our model’s decoder, and the output parameterizes <em>P</em>(<em>image</em>|<em>is-handwritten</em>, <em>digit</em>, <em>Z</em>). Our probabilistic approach to minimizing reconstruction error optimizes the encoder and decoder such that we’ll maximize the likelihood of <em>Z</em> with respect to <em>P</em>(<em>Z</em>|<em>image</em>, <em>is-handwritten</em>, <em>digit</em>) and the likelihood of the original image with respect to <em>P</em>(<em>image</em>|<em>is-handwritten</em>, <em>digit</em>, <em>Z</em>).</p>
</div>
<div class="readable-text intended-text" id="p111">
<p>But typically we can’t directly sample from or get likelihoods from the distribution <em>P</em>(<em>Z</em>|<em>image</em>, <em>is-handwritten</em>, <em>digit</em>). So, instead, our guide function attempts to approximate it. The guide represents a <em>variational distribution</em>, denoted <em>Q</em>(<em>Z</em>|<em>X</em>, <em>is-handwritten</em>, <em>digit</em>). A change in the weights of the encoder represents a shifting of the variational distribution. Training will optimize the weights of the encoder such that the variational distribution shifts toward <em>P</em>(<em>Z</em>|<em>image</em>, <em>is-handwritten</em>, <em>digit</em>). That training approach is called <em>variational inference</em>, and it works by minimizing the <em>Kullback–Leibler divergence</em> (KL divergence) between the two distributions; KL divergence is a way of quantifying how two distributions differ.</p>
</div>
<div class="readable-text intended-text" id="p112">
<p>Our variational inference procedure optimizes a quantity called <em>ELBO</em>, which means <em>expected lower bound on the log-likelihood of the data</em>. Minimizing negative ELBO loss indirectly minimizes reconstruction error and KL divergence between <em>Q</em>(<em>Z</em>|…) and <em>P</em>(<em>Z</em>|…). Pyro implements ELBO in a utility called <code>Trace_ELBO</code>.</p>
</div>
<div class="readable-text intended-text" id="p113">
<p>Our procedure will use <em>stochastic</em> variational inference (SVI), which simply means doing variational inference with a training procedure that works with randomly selected subsets of the data, or “batches”, rather than the full dataset, which reduces memory use and helps scale to larger data.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p114">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Key VAE concepts so far</h5>
</div>
<div class="readable-text" id="p115">
<p><em>Variational autoencoder (VAE)</em>—A popular framework in deep generative modeling. We’re using it to model a causal Markov kernel in our causal model.</p>
</div>
<div class="readable-text" id="p116">
<p><em>Decoder</em>—We use the decoder as the model of the causal Markov kernel. It maps the observed causes <em>is-handwritten</em> and <em>digit</em>, and the latent variable <em>Z</em>, to our image outcome variable <em>X</em>.</p>
</div>
<div class="readable-text" id="p117">
<p><em>Encoder</em>—The encoder maps the <em>image</em>, <em>digit</em>, and <em>is-handwritten</em> to the parameters of a distribution where we can draw samples of <em>Z</em>.</p>
</div>
<div class="readable-text" id="p118">
<p><em>Guide function</em>—During training, we want values of <em>Z</em> that represent an image, given <em>is-handwritten</em> and <em>digit</em>; i.e., we want to generate <em>Z</em>s from <em>P</em>(<em>Z</em>|<em>image</em>, <em>is-handwritten</em>, <em>digit</em>). But we can’t sample from this distribution directly. So we write a <em>guide function</em> that uses the encoder and convenient canonical distributions like the multivariate normal to sample values of <em>Z</em>.</p>
</div>
<div class="readable-text" id="p119">
<p><em>Variational distribution</em>—The guide function represents a distribution called <em>the variational distribution</em>, denoted <em>Q</em>(<em>Z</em>|<em>image</em>, <em>is-handwritten</em>, <em>digit</em>). During inference, we want to sample from <em>Q</em>(<em>Z</em>|…) in a way that is representative of <em>P</em>(<em>Z</em>|<em>image</em>, <em>is-handwritten</em>, <em>digit</em>).</p>
</div>
<div class="readable-text" id="p120">
<p><em>Variational inference</em>—This is the training procedure that seeks to maximize the closeness between <em>Q</em>(<em>Z</em>|…) and <em>P</em>(<em>Z</em>|…) so sampling from <em>Q</em>(<em>Z</em>|…) produces samples representative of <em>P</em>(<em>Z</em>|…) (e.g., by minimizing KL divergence).</p>
</div>
<div class="readable-text" id="p121">
<p><em>Stochastic variational inference (SVI)</em>—Variational inference where training relies on randomly selected subsets of the data, rather than on the full data, in order to make training faster and more scalable.</p>
</div>
</div>
<div class="readable-text" id="p122">
<p>Before we get started, we’ll make a helper function for plotting images so we can see how we are doing during training.</p>
</div>
<div class="browsable-container listing-container" id="p123">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.10</span> Helper function for plotting images</h5>
<div class="code-area-container">
<pre class="code-area">def plot_image(img, title=None):    <span class="aframe-location"/> #1
    fig = plt.figure()
    plt.imshow(img.cpu(), cmap='Greys_r', interpolation='nearest')
    if title is not None:
        plt.title(title)
    plt.show()</pre>
<div class="code-annotations-overlay-container">
     #1 Helper function for plotting an image
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p124">
<p>Next, we’ll create a <code>reconstruct_img</code> helper function that will <em>reconstruct</em> an image, given its labels, where “reconstruct” means encoding the image into a latent representation and then decoding the latent representation back into an image. We can then compare the original image and its reconstruction to see how well the encoder and decoder have been trained. We’ll create a <code>compare_images</code> function to do that comparison.</p>
</div>
<div class="browsable-container listing-container" id="p125">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.11</span> Define a helper function for reconstructing and viewing the images</h5>
<div class="code-area-container">
<pre class="code-area">import matplotlib.pyplot as plt

def reconstruct_img(vae, img, digit, is_hw, use_cuda=USE_CUDA):    <span class="aframe-location"/> #1
    img = img.reshape(-1, 28 * 28)
    digit = F.one_hot(torch.tensor(digit), 10)
    is_hw = torch.tensor(is_hw).unsqueeze(0)
    if use_cuda:
        img = img.cuda()
        digit = digit.cuda()
        is_hw = is_hw.cuda()
    z_loc, z_scale = vae.encoder(img, digit, is_hw)
    z = dist.Normal(z_loc, z_scale).sample()
    img_expectation = vae.decoder(z, digit, is_hw)
    return img_expectation.squeeze().view(28, 28).detach()

def compare_images(img1, img2):   <span class="aframe-location"/> #2
    fig = plt.figure()
    ax0 = fig.add_subplot(121)
    plt.imshow(img1.cpu(), cmap='Greys_r', interpolation='nearest')
    plt.axis('off')
    plt.title('original')
    ax1 = fig.add_subplot(122)
    plt.imshow(img2.cpu(), cmap='Greys_r', interpolation='nearest')
    plt.axis('off')
    plt.title('reconstruction')
    plt.show()</pre>
<div class="code-annotations-overlay-container">
     #1 Given an input image, this function reconstructs the image by passing it through the encoder and then through the decoder.
     <br/>#2 Plots the two images side by side for comparison
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p126">
<p>Next, we’ll create some helper functions for handling the data. We’ll use <code>get_random_example</code> to grab random images from the dataset. The <code>reshape_data</code> function will convert an image and its labels into input for the encoder. And we’ll use <code>generate_data</code> and <code>generate_coded_data</code> to simulate an image from the model.</p>
</div>
<div class="browsable-container listing-container" id="p127">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.12</span> Data processing helper functions for training</h5>
<div class="code-area-container">
<pre class="code-area">import torch.nn.functional as F

def get_random_example(loader):   <span class="aframe-location"/> #1
    random_idx = np.random.randint(0, len(loader.dataset))    #1
    img, digit, is_handwritten = loader.dataset[random_idx]     #1
    return img.squeeze(), digit, is_handwritten    #1

def reshape_data(img, digit, is_handwritten):    <span class="aframe-location"/> #2
    digit = F.one_hot(digit, 10).squeeze()     #2
    img = img.reshape(-1, 28*28)     #2
    return img, digit, is_handwritten     #2

def generate_coded_data(vae, use_cuda=USE_CUDA):    <span class="aframe-location"/> #3
    z_loc = torch.zeros(1, vae.z_dim)     #3
    z_scale = torch.ones(1, vae.z_dim)     #3
    z = dist.Normal(z_loc, z_scale).to_event(1).sample()     #3
    p_digit = torch.ones(1, 10)/10     #3
    digit = dist.OneHotCategorical(p_digit).sample()     #3
    p_is_handwritten = torch.ones(1, 1)/2     #3
    is_handwritten = dist.Bernoulli(p_is_handwritten).sample()    #3
    if use_cuda:     #3
        z = z.cuda() #3
        digit = digit.cuda() #3
        is_handwritten = is_handwritten.cuda()     #3
    img = vae.decoder(z, digit, is_handwritten)     #3
    return img, digit, is_handwritten    #3

def generate_data(vae, use_cuda=USE_CUDA):    <span class="aframe-location"/> #4
    img, digit, is_handwritten = generate_coded_data(vae, use_cuda)    #4
    img = img.squeeze().view(28, 28).detach()    #4
    digit = torch.argmax(digit, 1)   #4
    is_handwritten = torch.argmax(is_handwritten, 1)     #4
    return img, digit, is_handwritten     #4</pre>
<div class="code-annotations-overlay-container">
     #1 Choose a random example from the dataset.
     <br/>#2 Reshape the data.
     <br/>#3 Generate data that is encoded.
     <br/>#4 Generate (unencoded) data.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p128">
<p>Finally, we can run the training procedure. First, we’ll set up stochastic variational inference. We’ll first set up an instance of the Adam optimizer, which will handle optimization of the parameters in <code>training_guide</code>. Then we’ll pass <code>training_model</code>, <code>training_guide</code>, the optimizer, and the ELBO loss function to the SVI constructor to get an SVI instance.</p>
</div>
<div class="browsable-container listing-container" id="p129">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.13</span> Set up the training procedure</h5>
<div class="code-area-container">
<pre class="code-area">from pyro.infer import SVI, Trace_ELBO
from pyro.optim import Adam

pyro.clear_param_store()    <span class="aframe-location"/> #1
vae = VAE()   <span class="aframe-location"/> #2
train_loader, test_loader = setup_dataloaders(batch_size=256)   <span class="aframe-location"/> #3
svi_adam = Adam({"lr": 1.0e-3})    <span class="aframe-location"/> #4
model = vae.training_model   <span class="aframe-location"/> #5
guide = vae.training_guide    #5
svi = SVI(model, guide, svi_adam, loss=Trace_ELBO())     #5</pre>
<div class="code-annotations-overlay-container">
     #1 Clear any values of the parameters in the guide memory.
     <br/>#2 Initialize the VAE.
     <br/>#3 Load the data.
     <br/>#4 Initialize the optimizer.
     <br/>#5 Initialize the SVI loss calculator. Loss negative “expected lower bound” (ELBO).
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p130">
<p>When training generative models, it is useful to set up a procedure that uses test data to evaluate how well training is progressing. You can include anything you think is useful to monitor during training. Here, I calculate and print the loss function on the test data, just to make sure the test loss is progressively decreasing along with training loss (a flattening of test loss while training loss continues to decrease would indicate overfitting). </p>
</div>
<div class="readable-text intended-text" id="p131">
<p>A more direct way of determining how well our model is training is to generate and view images. In my test evaluation procedure, I produce two visualizations. First, I inspect how well it can reconstruct a random image from the test data. I pass the image through the encoder and then through the decoder, creating a “reconstruction” of the image. Then I plot the original and reconstructed images side by side and compare them visually, looking to see that they are close to identical.</p>
</div>
<div class="readable-text intended-text" id="p132">
<p>Next, I visualize how well it is performing as an overall generative model by generating and plotting an image from scratch. I run this code once each time a certain number of epochs are run.</p>
</div>
<div class="browsable-container listing-container" id="p133">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.14</span> Setting up a test evaluation procedure</h5>
<div class="code-area-container">
<pre class="code-area">def test_epoch(vae, test_loader):
    epoch_loss_test = 0    <span class="aframe-location"/> #1
    for img, digit, is_hw in test_loader:    #1
        batch_size = img.shape[0]    #1
        if USE_CUDA:    #1
            img = img.cuda()    #1
            digit = digit.cuda()     #1
            is_hw = is_hw.cuda()  #1
        img, digit, is_hw = reshape_data(     #1
            img, digit, is_hw     #1
        )   #1
        epoch_loss_test += svi.evaluate_loss(     #1
            img, digit, is_hw, batch_size    #1
        )    #1
    test_size = len(test_loader.dataset)    #1
    avg_loss = epoch_loss_test/test_size     #1
    print("Epoch: {} avg. test loss: {}".format(epoch, avg_loss))     #1
    print("Comparing a random test image to its reconstruction:")   <span class="aframe-location"/> #2
    random_example = get_random_example(test_loader)     #2
    img_r, digit_r, is_hw_r = random_example    #2
    img_recon = reconstruct_img(vae, img_r, digit_r, is_hw_r)     #2
    compare_images(img_r, img_recon)     #2
    print("Generate a random image from the model:")   <span class="aframe-location"/> #3
    img_gen, digit_gen, is_hw_gen = generate_data(vae)    #3
    plot_image(img_gen, "Generated Image")     #3
    print("Intended digit: ", int(digit_gen))    #3
    print("Intended as handwritten: ", bool(is_hw_gen == 1))     #3</pre>
<div class="code-annotations-overlay-container">
     #1 Calculate and print test loss.
     <br/>#2 Compare a random test image to its reconstruction.
     <br/>#3 Generate a random image from the model.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p134">
<p>Now we’ll run the training. For a single epoch, we’ll iteratively get a batch of data from the training data loader and pass it to the step method and run a training step. After a certain number of epochs (a number set by <code>TEST_FREQUENCY</code>), we’ll use our helper functions to compare a random image to its reconstruction, as well as simulate an image from scratch and plot it.</p>
</div>
<div class="browsable-container listing-container" id="p135">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.15</span> Running training and plotting progress</h5>
<div class="code-area-container">
<pre class="code-area">NUM_EPOCHS = 2500
TEST_FREQUENCY = 10

train_loss = []
train_size = len(train_loader.dataset)

for epoch in range(0, NUM_EPOCHS+1):  <span class="aframe-location"/> #1
    loss = 0
    for img, digit, is_handwritten in train_loader:
        batch_size = img.shape[0]
        if USE_CUDA:
            img = img.cuda()
            digit = digit.cuda()
            is_handwritten = is_handwritten.cuda()
        img, digit, is_handwritten = reshape_data(
            img, digit, is_handwritten
        )
        loss += svi.step(   <span class="aframe-location"/> #2
            img, digit, is_handwritten, batch_size     #2
        )     #2
    avg_loss = loss / train_size
    print("Epoch: {} avgs training loss: {}".format(epoch, loss))
    train_loss.append(avg_loss)
    if epoch % TEST_FREQUENCY == 0:   <span class="aframe-location"/> #3
        test_epoch(vae, test_loader)    #3</pre>
<div class="code-annotations-overlay-container">
     #1 Run the training procedure for a certain number of epochs.
     <br/>#2 Run a training step on one batch in one epoch.
     <br/>#3 The test data evaluation procedure runs every 10 epochs.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p136">
<p>Again, see <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for a link to a Jupyter notebook with the full VAE, encoder/decoder, and training code, including a link for running it in Google Colab.</p>
</div>
<div class="readable-text" id="p137">
<h3 class="readable-text-h3" id="sigil_toc_id_110"><span class="num-string">5.2.4</span> Evaluating training</h3>
</div>
<div class="readable-text" id="p138">
<p>At certain points during training, we randomly choose an image and “reconstruct” it by passing the image through the encoder to get a latent value of <em>Z</em>, and passing that value back through the decoder. In one run, the first image I see is a non-handwritten number 6. Figure 5.12 shows this image and its reconstruction.</p>
</div>
<div class="readable-text intended-text" id="p139">
<p>During training, we also simulate random images from the generative model and plot it. Figure 5.13 shows the first simulated image in one run—in this case, the number 3.</p>
</div>
<div class="browsable-container figure-container" id="p140">
<img alt="figure" height="267" src="../Images/CH05_F12_Ness.png" width="414"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.12</span> The first attempt to reconstruct an image during training shows the model has learned something but still has much progress to make.</h5>
</div>
<div class="browsable-container figure-container" id="p141">
<img alt="figure" height="735" src="../Images/CH05_F13_Ness.png" width="750"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.13</span> The first instance of an image generated from the generative model during training</h5>
</div>
<div class="readable-text" id="p142">
<p>But the model learns quickly. By 130 epochs, we get the results in figure 5.14.<span class="aframe-location"/></p>
</div>
<div class="readable-text intended-text" id="p143">
<p>After training is complete, we can see a visualization of loss over training (negative ELBO) in figure 5.15.</p>
</div>
<div class="readable-text intended-text" id="p144">
<p>The code will train the parameters of the encoder that maps images and the labels to the latent variable. It will also train the decoder that maps the latent variable and the labels to the image. That latent variable is a fundamental feature of the VAE, but we should take a closer look at how to interpret the latent variable in causal terms.</p>
</div>
<div class="browsable-container figure-container" id="p145">
<img alt="figure" height="669" src="../Images/CH05_F14_Ness.png" width="411"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.14</span> Reconstructed and randomly generated images from the model after 130 epochs of training look much better.<span class="aframe-location"/></h5>
</div>
<div class="browsable-container figure-container" id="p146">
<img alt="figure" height="698" src="../Images/CH05_F15_Ness.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.15</span> Test loss as training progresses. The <em>x</em>-axis is the epoch.</h5>
</div>
<div class="readable-text" id="p147">
<h3 class="readable-text-h3" id="sigil_toc_id_111"><span class="num-string">5.2.5</span> How should we causally interpret Z?</h3>
</div>
<div class="readable-text" id="p148">
<p>I said we can view <em>Z</em> as a “stand-in” for all the independent latent causes of the object in the image. <em>Z</em> is a representation we learn from the pixels in the images. It is tempting to treat that representation like a higher-level causal abstraction of those latent causes, but it is probably not doing a great job as a causal abstraction. The autoencoder paradigm trains an encoder that can take an image and embed it into a low-dimensional representation <em>Z</em>. It tries to do so in a way that enables it to reconstruct the original image as well as possible. In order to reconstruct the image with little loss, the framework tries to encode as much information from the original image as it can in that lower dimensional representation.</p>
</div>
<div class="readable-text intended-text" id="p149">
<p>A good <em>causal</em> representation, however, shouldn’t try to capture as much information as possible. Rather, it should strive to capture only the <em>causal</em> information in the images and ignore everything else. Indeed, the task of “disentangling” the causal and non-causal factors in <em>Z</em> is generally impossible when <em>Z</em> is unsupervised (meaning we lack labels for <em>Z</em>). However, domain knowledge, interventions, and semi-supervision can help. See <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for references on <em>causal representation learning</em> and <em>disentanglement of causal factors</em>. As we progress through the book, we’ll develop intuition for what the “causal information” in such a representation should look like.</p>
</div>
<div class="readable-text" id="p150">
<h3 class="readable-text-h3" id="sigil_toc_id_112"><span class="num-string">5.2.6</span> Advantages of this causal interpretation</h3>
</div>
<div class="readable-text" id="p151">
<p>There is nothing inherently causal about our VAE’s setup and training procedure; it is typical of a vanilla supervised VAE you’d see in many machine learning settings. The only causal element of our approach was our interpretation. We say that the <em>digit</em> and <em>is-handwritten</em> are causes, and <em>Z</em> is a stand-in for latent causes, and the image is the outcome. Applying the causal Markov property, our causal model factorizes the joint distribution into <em>P</em>(<em>Z</em>), <em>P</em><em> </em>(<em>is-handwritten</em>), <em>P</em>(<em>digit</em><em> </em>), and <em>P</em>(<em>image</em><em> </em>|<em>Z</em>, <em>is-handwritten</em>, <em>digit</em><em> </em>), where the latter factor is the causal Markov kernel of the image.</p>
</div>
<div class="readable-text intended-text" id="p152">
<p>What can we do with this causal interpretation? First, we can use it to improve deep learning and general machine learning workflows and tasks. We’ll see an example of this with <em>semi-supervised learning</em> in the next section.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p153">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Incorporating generative AI in causal models is not limited to VAEs</h5>
</div>
<div class="readable-text" id="p154">
<p>I demonstrated how to use a VAE framework to fit a causal Markov kernel entailed by a causal DAG, but a VAE was just one approach to achieving this end. We could have used another deep probabilistic machine learning framework, such as a generative adversarial network (GAN) or a diffusion model.</p>
</div>
</div>
<div class="readable-text" id="p155">
<p>In this section, we incorporated deep learning into a causal graphical model. Next, we investigate how to use causal ideas to enhance deep learning.</p>
</div>
<div class="readable-text" id="p156">
<h2 class="readable-text-h2" id="sigil_toc_id_113"><span class="num-string">5.3</span> Using causal inference to enhance deep learning</h2>
</div>
<div class="readable-text" id="p157">
<p>We can use causal insights to improve how we set up and train deep learning models. These insights tend to lead to benefits such as improved sample efficiency (i.e., doing more with less data), the ability to do transfer learning (using what a model learned in solving one task to improve performance on another), data fusion (combining different datasets), and enabling more robust predictions.</p>
</div>
<div class="readable-text intended-text" id="p158">
<p>Much of the work of deep learning is trial and error. For example, when training a VAE or other deep learning models, you typically experiment with different approaches (VAE vs. another framework), architectural choices (latent variable and hidden layer dimension, activation functions, number of layers, etc.), and training approaches (choice of loss function, learning rate, optimizer, etc.) before you get a good result. These experiments cost time, effort, and resources. In some cases, causal modeling can help you make better choices about what might work and what is unlikely to work, leading to cost savings. In this section, we’ll look at a particular example of this case in the context of semi-supervised learning.</p>
</div>
<div class="readable-text" id="p159">
<h3 class="readable-text-h3" id="sigil_toc_id_114"><span class="num-string">5.3.1</span> Independence of mechanism as an inductive bias</h3>
</div>
<div class="readable-text" id="p160">
<p>Suppose we had a DAG with two variables: “cause” <em>C</em> and “outcome” <em>O</em>. The DAG is simply <em>C</em> → <em>O</em>. Our causal Markov kernels are <em>P</em>(<em>C</em>) and <em>P</em>(<em>O</em>|<em>C</em>). Recall the idea of <em>independence of mechanism</em> from chapter 3—the causal Markov kernel <em>P</em>(<em>O</em>|<em>C</em>) represents a mechanism of how the cause <em>C</em> drives the outcome <em>O</em>. That mechanism is distinct from other mechanisms in the system, such that changes to those mechanisms have no effect on <em>P</em>(<em>O</em>|<em>C</em>). Thus, knowing about <em>P</em>(<em>O</em>|<em>C</em>) tells you nothing about the distribution of the cause <em>P</em>(<em>C</em>) and vice versa. However, knowing something about the distribution of the outcome <em>P</em>(<em>O</em>) might tell you something about the distribution of the cause given the outcome <em>P</em>(<em>C</em>|<em>O</em>), and vice versa.</p>
</div>
<div class="readable-text intended-text" id="p161">
<p>To illustrate, consider a scenario where <em>C</em> represents sunscreen usage and <em>O</em> indicates whether someone has sunburn. You understand the <em>mechanism</em> by which sunscreen protects against sunburn (UV rays, SPF levels, regular application, the perils of sweat and swimming, etc.), and by extension, the chances of getting sunburn given how one uses sunscreen, captured by <em>P</em>(<em>O</em>|<em>C</em>). However, this understanding of the mechanism doesn’t provide any information about how <em>common</em> sunscreen use is, denoted by <em>P</em>(<em>C</em>).</p>
</div>
<div class="readable-text intended-text" id="p162">
<p>Now, suppose you’re trying to guess whether a sunburned person used sunscreen, i.e., you’re mentally modeling <em>P</em>(<em>C</em>|<em>O</em>). In this case, knowing the prevalence of sunburns, <em>P</em>(<em>O</em>), could help. Consider whether the sunburned individual was a case of someone who did use sunscreen but got a sunburn anyway. That case would be more likely if sunburns were a common problem than if sunburns were rare—if sunburns are common, sunscreen use is probably common, but if sunburns were uncommon, people would be less cautious about prevention.</p>
</div>
<div class="readable-text intended-text" id="p163">
<p>Similarly, suppose <em>C</em> represents study effort and <em>O</em> represents test scores. You know the causal mechanism behind how studying more causes higher test scores, captured by <em>P</em>(<em>O</em>|<em>C</em>). But this doesn’t tell you how common it is for students to study hard, captured by <em>P</em>(<em>C</em>). Suppose a student got a low test score, and you are trying to infer whether they studied hard—you are mentally modeling <em>P</em>(<em>C</em>|<em>O</em>). Again, knowing the typical distribution of test scores <em>P</em>(<em>O</em>) can help. If low scores are rare, students might be complacent, and thus more likely not to study hard. You can use that insight as an <em>inductive bias</em>—a way to constrain your mental model of <em>P</em>(<em>C</em>|<em>O</em>).</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p164">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Causal inductive bias</h5>
</div>
<div class="readable-text" id="p165">
<p>“Inductive bias” refers to the assumptions (explicit or implicit) that lead an inference algorithm to prefer certain inferences or predictions over others. Examples of inductive bias include Occam’s Razor and the assumption in forecasting that trends in the past will continue into the future. </p>
</div>
<div class="readable-text" id="p166">
<p>Modern deep learning relies on using neural network architectures and training objectives to encode inductive bias. For example, “convolutions” and “max pooling” are architectural elements in convolutional neural networks for computer vision that encode an inductive bias called “translation invariance”; i.e., a kitten is still a kitten regardless of whether it appears on the left or right of an image.</p>
</div>
<div class="readable-text" id="p167">
<p>Causal models provide inductive biases in the form of causal assumptions about the DGP (such as a causal DAG). Deep learning can leverage these causal inductive biases to attain better results just as it does with other types of inductive biases. For example, independence of mechanism suggests that knowing <em>P</em>(<em>O</em>) could provide a useful inductive bias in learning <em>P</em>(<em>C</em>|<em>O</em>).</p>
</div>
</div>
<div class="readable-text" id="p168">
<p>Now consider two variables <em>X</em> and <em>Y</em> (which can be vectors) with joint distribution <em>P</em><em> </em>(<em>X</em>, <em>Y</em><em> </em>). We want to design an algorithm that solves a task by learning from data observed from <em>P</em><em> </em>(<em>X</em>, <em>Y</em><em> </em>). The chain rule of probability tells us that <em>P</em>(<em>X</em>=<em>x</em>, <em>Y</em>=<em>y</em>) = <em>P</em><em> </em>(<em>X</em><em> </em>=<em> </em><em>x</em><em> </em>|<em>Y</em><em> </em>=<em> </em><em>y</em>)<em>P</em><em> </em>(<em>Y</em><em> </em>=<em> </em><em>y</em>) = <em>P</em><em> </em>(<em>Y</em><em> </em>=<em> </em><em>y</em><em> </em>|<em>X</em><em> </em>=<em> </em><em>x</em>)<em>P</em><em> </em>(<em>X</em><em> </em>=<em> </em><em>x</em>). So, from that basic probabilistic perspective, modeling the set {<em>P</em><em> </em>(<em>X</em><em> </em>|<em>Y</em><em>  </em>), <em>P</em><em> </em>(<em>Y</em><em>  </em>)} is equivalent to modeling the set {<em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em> </em>), <em>P</em><em> </em>(<em>X</em><em> </em>)}. But consider the cases where either <em>X</em> is a cause of <em>Y</em> or where <em>Y</em> is a cause of <em>X</em>. Under these circumstances, the independence of mechanism gives us an asymmetry between sets {<em>P</em><em> </em>(<em>X</em><em> </em>|<em>Y</em><em>  </em>), <em>P</em>(<em>Y</em><em>  </em>)} and {<em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em> </em>), <em>P</em><em> </em>(<em>X</em><em> </em>)} (specifically, {<em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em> </em>), <em>P</em><em> </em>(<em>X</em><em> </em>)} represents the independent mechanism behind <em>X</em>’s causal influence on <em>Y</em>, and {<em>P</em><em> </em>(<em>X</em><em> </em>|<em>Y</em><em>  </em>), <em>P</em>(<em>Y</em><em>  </em>)} does not) that we can possibly leverage as an inductive bias in these algorithms. Semi-supervised learning is a good example. </p>
</div>
<div class="readable-text" id="p169">
<h3 class="readable-text-h3" id="sigil_toc_id_115"><span class="num-string">5.3.2</span> Case study: Semi-supervised learning</h3>
</div>
<div class="readable-text" id="p170">
<p>Returning to our TMNIST-MNIST VAE-based causal model, suppose we had, in addition to our original data, a large set of images of digits that were unlabeled (i.e., <em>digit</em> and <em>is-handwritten</em> are not observed). Our causal interpretation of our model suggests we can leverage this data during training using semi-supervised learning.</p>
</div>
<div class="readable-text intended-text" id="p171">
<p>Independence of mechanism can help you determine when semi-supervised learning will be effective. In <em>supervised learning</em>, the training data consists of <em>N</em> samples of <em>X</em>, <em>Y</em> pairs; (<em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>), (<em>x</em><sub>2</sub>, <em>y</em><sub>2</sub>), …, (<em>x</em><sub><em>N</em></sub>, <em>y</em><sub><em>N</em></sub>). <em>X</em> is the <em>feature data</em> used to predict the <em>labels</em> <em>Y</em>. The data is “supervised” because every <em>x</em> is paired with a <em>y</em>. We can use these pairs to learn <em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em>  </em>). In <em>unsupervised learning</em>, the data <em>X</em> is unsupervised, meaning we have no labels, no observed value of <em>Y</em>. Our data looks like (<em>x</em><sub>1</sub>), (<em>x</em><sub>2</sub>), …, (<em>x</em><sub><em>N</em></sub>). With this data alone, we can’t directly learn anything about <em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em>  </em>); we can only learn about <em>P</em><em> </em>(<em>X</em><em>  </em>). Semi-supervised learning asks the question, suppose we had a combination of supervised and unsupervised data. Could these two sets of data be combined in a way such that our ability to predict <em>Y</em> was better than if we only used the supervised data? In other words, can learning more about <em>P</em><em> </em>(<em>X</em><em>  </em>) from the unsupervised data somehow augment our learning of <em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em>  </em>) from the supervised data?</p>
</div>
<div class="readable-text intended-text" id="p172">
<p>The semi-supervised question is quite practical. It is common to have abundant unsupervised examples if labeling those examples is costly. For example, suppose you worked at a social media site and were tasked with building an algorithm that classified whether an uploaded image depicted gratuitous violence. The first step is to create supervised data by having humans manually label images as gratuitously violent or not. Not only does this cost many people-hours, but it is mentally stressful for the labelers. A successful semi-supervised approach would mean you could minimize the amount of labeling work you need to do.</p>
</div>
<div class="readable-text intended-text" id="p173">
<p>Our task is to learn a representation of <em>P</em><em> </em>(<em>X</em> ,<em> </em><em>Y</em>) and use it to predict from <em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em>  </em>). For semi-supervised learning to work, the unlabeled values of <em>X</em> must update the representation of <em>P</em><em> </em>(<em>X</em> , <em>Y</em>) in a way that provides information about <em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em>  </em>). However, independence of mechanism means the task of learning <em>P</em><em> </em>(<em>X</em> ,<em> </em><em>Y</em>) decomposes into learning distinct representations of the causal Markov kernels, where the parameter vector of each representation is orthogonal to the others. That parameter modularity (see section 3.2) can block flow of parameter updating information from the unlabeled observations of <em>X</em> to the learned representation of <em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em>  </em>). To illustrate, let's consider two possibilities, one where <em>Y</em> is a cause of <em>X</em>, and one where <em>X</em> is a cause of <em>Y</em>. If <em>Y</em> is a cause of <em>X</em>, such as in our MNIST-TMNIST example (<em>Y</em> is the is-handwritten and digit variables, and <em>X</em> is the image), then our learning task decomposes into learning distinct representations of <em>P</em><em> </em>(<em>X</em><em>  </em>|<em>Y</em><em>  </em>) and P(Y). Unlabeled observations of <em>X</em> can give us a better representation of <em>P</em><em> </em>(<em>X</em><em>  </em>), we can use to flip <em>P</em><em> </em>(<em>X</em><em>  </em>|<em>Y</em><em>  </em>) into <em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em>  </em>) by way of Bayes rule. However, when <em>X</em> is a cause of <em>Y</em>, our learning task decomposes into learning distinct representations of <em>P</em><em> </em>(<em>X</em><em>  </em>) and <em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em>  </em>). That parameter modularity means those unlabeled values of <em>X</em> will help us update <em>P</em><em> </em>(<em>X</em><em>  </em>)’s representation but not that of <em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em>  </em>).<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p174">
<img alt="figure" height="198" src="../Images/CH05_F16_Ness.png" width="300"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.16</span> In causal learning, the features cause the label. In anti-causal learning, the label causes the features.</h5>
</div>
<div class="readable-text intended-text" id="p175">
<p>The case where the feature causes the label is sometimes called <em>causal learning </em>because the direction of the prediction is from the cause to the effect. <em>Anti-causal learning</em> refers to the case when the label causes the feature. The two cases are illustrated in figure 5.16.</p>
</div>
<div class="readable-text intended-text" id="p176">
<p>Independence of mechanism suggests semi-supervised learning can achieve performance gains (relative to a baseline of supervised learning on only the labeled data) only in the anti-causal case. See the chapter notes at www.altdeep.ai/causalAIbook for a more detailed explanation and references. But intuitively, we can see that this mirrors the the sunscreen and sunburn example—knowing the prevalence of sunburns <em>P</em><em> </em>(<em>O</em><em> </em>) helped in learning how to guess sunscreen use when you know if someone has a sunburn <em>P</em><em> </em>(<em>C</em><em> </em>|<em>O</em><em> </em>). In this same anti-causal learning case, having only observations from <em>P</em><em> </em>(<em>X</em><em>  </em>) can still be helpful in learning a good model of <em>P</em><em> </em>(<em>Y</em><em> </em>|<em>X</em><em>  </em>). But in the causal learning case, it would be a waste of effort and resources.</p>
</div>
<div class="readable-text intended-text" id="p177">
<p>In practice, the causal structure between <em>X</em> and <em>Y</em> could be more nuanced and complicated than these simple <em>X</em><em> </em>→<em>Y</em> and <em>X</em><em> </em>←<em>Y</em> cases. For example, there could be unobserved common causes of <em>X</em> and <em>Y</em>. The takeaway here is that when you know something about the causal relationships between the variables in your machine learning problem, you can leverage that knowledge to model more effectively, even if the task is not a causal inference task (e.g., simply predicting <em>Y</em> given <em>X</em><em>  </em>). This could help you avoid spending time and resources on an approach that is not likely to work, as in the semi-supervised case. Or it could enable more efficient, robust, or better performing inferences.</p>
</div>
<div class="readable-text" id="p178">
<h3 class="readable-text-h3" id="sigil_toc_id_116"><span class="num-string">5.3.3</span> Demystifying deep learning with causality </h3>
</div>
<div class="readable-text" id="p179">
<p>Our semi-supervised learning example highlights how a causal perspective can explain when we’d expect semi-supervised learning to work and when to fail. In other words, it somewhat <em>demystifies</em> semi-supervised learning.</p>
</div>
<div class="readable-text intended-text" id="p180">
<p>That mystery around the effectiveness of deep learning methods led AI researcher Ali Rahimi to compare modern machine learning to alchemy.</p>
</div>
<div class="readable-text" id="p181">
<blockquote>
<div>
     Alchemy worked. Alchemists invented metallurgy, ways to dye textiles, modern glass-making processes, and medications. Then again, alchemists also believed they could cure diseases with leeches and transmute base metals into gold.
    </div>
</blockquote>
</div>
<div class="readable-text" id="p182">
<p>In other words, alchemy works, but alchemists lacked an understanding of the underlying scientific principles that made it work when it did. That <em>mystery</em> made it hard to know when it would fail. As a result, alchemists wasted considerable effort on dead ends (philosopher’s stones, immortality elixirs, etc.).</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p183">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Chapter checkpoint</h5>
</div>
<div class="readable-text" id="p184">
<p><em>Incorporating deep learning into a causal model:</em></p>
</div>
<div class="readable-text" id="p185">
<p>✓ A causal model of a computer vision problem</p>
</div>
<div class="readable-text" id="p186">
<p>✓ Training the deep causal image model</p>
</div>
<div class="readable-text" id="p187">
<p><em>Using causal reasoning to enhance machine learning:</em></p>
</div>
<div class="readable-text" id="p188">
<p>✓ Case study on independence of mechanism and semi-supervised learning</p>
</div>
<div class="readable-text" id="p189">
<p>👉 Demystifying deep learning with causality</p>
</div>
</div>
<div class="readable-text" id="p190">
<p>Similarly, deep learning “works” in that it achieves good performance on a wide variety of prediction and inference tasks. But we often have an incomplete understanding of why and when it works. That <em>mystery</em> has led to problems with reproducibility, robustness, and safety. It also leads to irresponsible applications of AI, such as published work that attempts to predict behavior (e.g., criminality) from profile photos. Such efforts are the machine learning analog of the alchemical immortality elixirs that contained toxins like mercury; they don’t work <em>and</em> they cause harm.</p>
</div>
<div class="readable-text intended-text" id="p191">
<p>We often hear about the “superhuman” performance of deep learning. Speaking of superhuman ability, imagine an alternative telling of Superman’s origin story. Imagine if, when Superman made his first public appearance, his superhuman abilities were unreliable? Suppose he demonstrated astounding superhuman feats like flight, super strength, and laser vision, but sometimes his flight ability failed and his super strength faltered. Sometimes his laser vision was dangerously unfocused, resulting in terrible collateral damage. The public would be impressed and hopeful that he could do some good, but unsure if it would be safe to rely on him when the stakes were high.</p>
</div>
<div class="readable-text intended-text" id="p192">
<p>Now imagine that his adoptive Midwestern parents, experts in causal inference, used causal analysis to model the <em>how </em>and<em> why</em> of his powers. Having demystified the mechanisms underlying his superpowers, they were able to engineer a pill that stabilized those powers. The pill wouldn’t so much give Superman new powers; it would just make his existing powers more reliable. The work of developing that pill would get fewer headlines than flight and laser vision, but it would be the difference between merely having superpowers and being Superman.</p>
</div>
<div class="readable-text intended-text" id="p193">
<p>This analogy helps us understand the impact of using causal methods to demystify deep learning and other machine learning methods. Less mystery leads to more robust methods and helps us avoid wasteful or harmful applications.</p>
</div>
<div class="readable-text" id="p194">
<h2 class="readable-text-h2" id="sigil_toc_id_117">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p195"> Deep learning can be used to enhance causal modeling and inference. Causal reasoning can enhance the setup, training, and performance of deep learning models. </li>
<li class="readable-text" id="p196"> Causal models can leverage the ability of deep learning to scale and work with high-dimensional nonlinear relationships. </li>
<li class="readable-text" id="p197"> You can use generative AI frameworks like the variational autoencoder to build a causal generative model on a DAG just as we did with pgmpy. </li>
<li class="readable-text" id="p198"> The decoder maps the outcomes of direct parents (the labels of an image) to the outcomes of the child (the image). </li>
<li class="readable-text" id="p199"> In other words, the decoder gives us a nonlinear high-dimensional representation of the causal Markov kernel for the image. </li>
<li class="readable-text" id="p200"> The encoder maps the image variable and the causes (labels) back to the latent variable <em>Z</em>. </li>
<li class="readable-text" id="p201"> We can view the learned representation of the latent variable as a stand-in for unmodeled causes, but it still lacks the qualities we’d expect from an ideal causal representation. Learning latent causal representations is an active area of research. </li>
<li class="readable-text" id="p202"> Causality often enhances deep learning and other machine learning methods by helping elucidate the underlying principles that make it work. For example, causal analysis shows semi-supervised learning should work in the case of <em>anti-causal learning</em> (when the features are <em>caused by</em> the label) but not in the case of <em>causal learning </em>(when the features cause the label). </li>
<li class="readable-text" id="p203"> Such causal insights can help the modeler avoid spending time, compute, person-hours, and other resources on a given algorithm when it is not likely to work in a given problem setting. </li>
<li class="readable-text" id="p204"> Causal insights can demystify elements of building and training deep learning models, such that they become more robust, efficient, and safe. </li>
</ul>
</div></body></html>