<html><head></head><body><section data-pdf-bookmark="Chapter 5. AI Engineering for High-Risk AI Systems" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_5_ai_engineering_for_high_risk_ai_systems_1748539922576008">
<h1><span class="label">Chapter 5. </span>AI Engineering for High-Risk AI Systems</h1>

<p>This chapter explores the comprehensive requirements for high-risk AI systems and how to meet them proactively by implementing AI engineering practices to ensure compliance with the EU AI Act (see <a data-type="xref" href="#chapter_5_figure_1_1748539922504826">Figure 5-1</a> for a visual of the steps to take to move toward compliance). In the previous chapter, I introduced the foundational steps for compliance: creating an inventory of current and planned AI systems, identifying the risk category of each, and determining whether your organization acts as a provider or deployer of these systems. These steps are essential for understanding which systems fall under the high-risk category and what compliance obligations need to be addressed.</p>

<figure><div class="figure" id="chapter_5_figure_1_1748539922504826"><img src="assets/taie_0501.png"/>
<h6><span class="label">Figure 5-1. </span>This chapter focuses on the requirements for high-risk AI systems and the operationalization of compliance for such systems. See <a data-type="xref" href="ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819">Chapter 1</a> for an explanation of the end-to-end process steps toward EU AI Act compliance.</h6>
</div></figure>

<p>Two guiding questions to consider in this chapter are:</p>

<ul>
	<li>
	<p>To comply with the EU AI Act, what requirements must high-risk AI systems <span class="keep-together">fulfill?</span></p>
	</li>
	<li>
	<p>What processes, structures, and AI engineering practices need to be established to comply with the EU AI Act?</p>
	</li>
</ul>

<p>Let’s start by exploring the requirements for high-risk AI systems, translating them into AI engineering practices, and defining the notion of AI engineering for the EU AI Act in this way.</p>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>As mentioned previously, the author is not a lawyer, and this book does not provide legal advice. The intersection of law and artificial intelligence is a complex subject that requires expertise beyond the scope of AI, data science, and machine learning. Legal considerations surrounding AI systems can be complex and far-reaching. If you have any legal concerns related to the AI systems you are working on, seek professional legal advice from qualified experts in the field.</p>
</div>

<section data-pdf-bookmark="AI Engineering for the EU AI Act" data-type="sect1"><div class="sect1" id="chapter_5_ai_engineering_for_the_eu_ai_act_1748539922576762">
<h1>AI Engineering for the EU AI Act</h1>

<p>The EU AI Act regulates the development, deployment, and use of AI systems within the European Union. It aims to promote trustworthy AI by mitigating risks and protecting fundamental rights. As depicted in <a data-type="xref" href="#chapter_5_figure_2_1748539922504869">Figure 5-2</a>, Articles 9–15 provide requirements for providers of high-risk AI systems. However, these articles do not provide technical guidance for implementing compliance.</p>

<p>This chapter suggests a method for technical implementation of compliance with the EU AI Act by aligning the Act’s articles with the quality model for safety-critical AI systems outlined in the paper <a href="https://oreil.ly/2J89P">“Navigating the EU AI Act: A Methodological Approach to Compliance for Safety-Critical Products”</a> by <span class="keep-together">Jessica</span> Kelly et al., presented at the 2024 IEEE Conference on Artificial Intelligence (CAI). The authors propose an expanded product quality model for AI systems, drawing from established standards such as ISO/IEC 25059. This enhanced model includes additional attributes relevant to the EU AI Act, such as ethical integrity, human oversight, and fairness.</p>

<figure><div class="figure" id="chapter_5_figure_2_1748539922504869"><img src="assets/taie_0502.png"/>
<h6><span class="label">Figure 5-2. </span>EU AI Act Requirements for High-Risk AI Systems (source: <a href="https://oreil.ly/7h6xE"><em>https://oreil.ly/7h6xE</em></a>)</h6>
</div></figure>

<p>Mapping EU AI Act requirements to quality attributes<a contenteditable="false" data-primary="quality attributes (for AI systems)" data-type="indexterm" id="id548"/> for AI systems is a helpful approach for several reasons, including:</p>

<dl>
	<dt>Operationalization of regulatory requirements</dt>
	<dd>
	<p>The EU AI Act provides high-level regulatory guidelines, which can be abstract and challenging to implement directly. By mapping these requirements to specific quality attributes, we create a more concrete, actionable framework for compliance. This helps translate legal language into technical specifications that engineers and data scientists can work with.</p>
	</dd>
	<dt>Integration with existing practices</dt>
	<dd>
	<p>Quality attributes are well-established software and systems engineering concepts. Mapping EU AI Act requirements to these attributes facilitates the integration of regulatory compliance into existing development processes and quality assurance practices. This allows organizations to leverage familiar tools, methodologies, and metrics.</p>
	</dd>
	<dt>Alignment with standards</dt>
	<dd>
	<p>Many quality attributes are defined by international standards (e.g., ISO/IEC 25010). Mapping EU AI Act requirements to these attributes helps align regulatory compliance with established standards, potentially simplifying overall compliance efforts.</p>
	</dd>
</dl>

<p>In short, the mapping of EU AI Act articles to the quality attributes provides technical guidance for achieving compliance. <a data-type="xref" href="#chapter_5_figure_3_1748539922504896">Figure 5-3</a> provides a high-level overview of this mapping.</p>

<figure><div class="figure" id="chapter_5_figure_3_1748539922504896"><img src="assets/taie_0503.png"/>
<h6><span class="label">Figure 5-3. </span>Mapping of EU AI Act requirements to quality attributes for AI systems</h6>
</div></figure>

<section data-pdf-bookmark="Goals" data-type="sect2"><div class="sect2" id="chapter_5_goals_1748539922576832">
<h2>Goals</h2>

<p class="fix_tracking">Before we discuss the method, let’s explore what AI engineering for the EU AI Act involves. This multidisciplinary field focuses on designing, developing, deploying, and maintaining artificial intelligence systems in full compliance with the requirements of the Act. It combines expertise in AI technology, software engineering, data science, law, and ethics in order to achieve the following goals:</p>

<ul>
	<li>
	<p>Implement technical measures ensuring compliance with the Act’s requirements, including risk assessment, transparency, human oversight, and robustness.</p>
	</li>
	<li>
	<p>Develop methodologies for continuous monitoring and auditing of AI systems throughout their lifecycle.</p>
	</li>
	<li>
	<p>Create documentation and traceability mechanisms to demonstrate compliance and facilitate regulatory inspections.</p>
	</li>
	<li>
	<p>Integrate privacy-enhancing technologies and data governance practices aligned with EU data protection laws.</p>
	</li>
	<li>
	<p>Design user interfaces and operational protocols that enable effective human oversight and intervention.</p>
	</li>
	<li>
	<p>Collaborate with legal and policy experts to interpret and apply evolving AI regulations across diverse application domains.</p>
	</li>
</ul>
</div></section>

<section data-pdf-bookmark="Alignment with CRISP-ML(Q) Phases" data-type="sect2"><div class="sect2" id="chapter_5_alignment_with_crisp_ml_q_phases_1748539922576892">
<h2>Alignment with CRISP-ML(Q) Phases</h2>

<p>To establish effective engineering processes, we’ll align the quality attributes of safety-critical (i.e., high-risk) AI systems with the phases of the CRISP-ML(Q) development process model introduced in <a data-type="xref" href="ch02.html#chapter_2_ai_engineering_a_proactive_compliance_catalyst_1748539917637495">Chapter 2</a>.</p>

<p>A core principle of CRISP-ML(Q) is the integration of quality assurance practices into each phase of the machine learning lifecycle. This includes defining system requirements, identifying potential risks, and applying risk mitigation strategies based on established best practices. As such, this model provides a suitable framework for integrating the quality attributes essential to safety-critical AI systems. As discussed previously, these attributes can be mapped directly to regulatory requirements such as those defined in the EU AI Act. Addressing them throughout the development lifecycle supports <em>continuous compliance</em>, rather than treating compliance as a one-time task.</p>

<p>As you already know, CRISP-ML(Q) defines six key phases in the ML development process: business and data understanding, data preparation, modeling, evaluation, deployment, and monitoring and maintenance. It’s an iterative framework, where each phase consists of specific tasks (such as dataset cleaning or model training) and outputs (such as datasets or model artifacts). Many quality attributes relate directly to potential risks. By considering these attributes in each development phase, you can identify and address risks early, when changes are less costly and more feasible to implement. In this chapter, I align the CRISP-ML(Q) phases with the quality attributes linked to EU AI Act requirements to establish AI engineering best practices that support robust implementation and sustained compliance.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="AI Engineering Practices for Achieving Compliance" data-type="sect1"><div class="sect1" id="chapter_5_ai_engineering_practices_for_achieving_compliance_1748539922577134">
<h1>AI Engineering Practices for Achieving Compliance</h1>

<p>The requirements for providers of high-risk AI systems are distributed across several articles in the EU AI Act. For your convenience, I’ve included them here as condensed action points:</p>

<ul>
	<li>
	<p>Establish a risk management system that spans the entire AI system lifecycle.</p>
	</li>
	<li>
	<p>Ensure strong data governance by validating and testing datasets to guarantee their relevance, representativeness, and accuracy in relation to the system’s intended purpose.</p>
	</li>
	<li>
	<p>Create comprehensive technical documentation to demonstrate compliance and provide authorities with the necessary information for assessment.</p>
	</li>
	<li>
	<p>Enable automatic logging capabilities in the AI system to track serious incidents and substantial modifications throughout its lifecycle.</p>
	</li>
	<li>
	<p>Provide detailed instructions for downstream deployers to support compliance.</p>
	</li>
	<li>
	<p>Incorporate human oversight mechanisms into the system’s design for downstream deployers.</p>
	</li>
	<li>
	<p>Ensure the system meets standards for accuracy, robustness, and cybersecurity.</p>
	</li>
	<li>
	<p>Establish a quality management system to support and maintain compliance. The requirements for this system are laid out in <a href="https://oreil.ly/zAKkT">Article 17</a>.</p>
	</li>
</ul>

<p>In the remainder of this chapter we will explore Articles 9–15 of the EU AI Act, which lay out the requirements for providers of high-risk AI systems, and see how these legal requirements translate into actionable AI engineering practices.</p>

<section data-pdf-bookmark="Article 9: Risk Management System" data-type="sect2"><div class="sect2" id="chapter_5_article_9_risk_management_system_1748539922577536">
<h2>Article 9: Risk Management System</h2>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="chapter_5_article_9_1748539922577638">
<h1>Article 9</h1>

<p><a href="https://oreil.ly/an4z8">Article 9 of the EU AI Act</a> introduces a foundational requirement for high-risk AI systems: the establishment of a risk management system. This system is intended to ensure that AI technologies are developed and deployed responsibly, with close attention paid to identifying, assessing, and mitigating potential risks.</p>

<p>As shown in <a data-type="xref" href="#chapter_5_figure_4_1748539922504918">Figure 5-4</a>, key points of Article 9 include:</p>

<dl>
	<dt>Continuous process</dt>
	<dd>
	<p>Risk management is a continuous, iterative process that spans the entire lifecycle of the AI system.</p>
	</dd>
	<dt>Systematic approach</dt>
	<dd>
	<p>It calls for systematic and proactive identification, assessment, and mitigation of risks associated with high-risk AI systems.</p>
	</dd>
	<dt>Risk categories</dt>
	<dd>
	<p>The article outlines various risk categories, including risks to health, safety, and fundamental rights, and potential discriminatory impacts.</p>
	</dd>
	<dt>Testing procedures</dt>
	<dd>
	<p>It mandates establishing appropriate testing procedures to identify and analyze potential risks.</p>
	</dd>
	<dt>Risk mitigation</dt>
	<dd>
	<p>Developers must implement suitable risk management measures, prioritizing elimination or reduction of risks through proper design and development.</p>
	</dd>
	<dt>Remaining risk communication</dt>
	<dd>
	<p>Any remaining risks must be clearly communicated to users.</p>
	</dd>
	<dt>Regular review</dt>
	<dd>
	<p>The risk management process should be regularly reviewed and updated, especially when significant changes occur in the AI system or its usage context.</p>
	</dd>
</dl>

<figure><div class="figure" id="chapter_5_figure_4_1748539922504918"><img src="assets/taie_0504.png"/>
<h6><span class="label">Figure 5-4. </span>Key points of Article 9 of the EU AI Act</h6>
</div></figure>
</div></aside>

<p>To meet the requirements of Article 9 and ensure that high-risk AI systems are developed responsibly, organizations need to implement AI engineering processes that address three quality attributes<a contenteditable="false" data-primary="quality attributes (for AI systems)" data-type="indexterm" id="id549"/> throughout the AI system lifecycle. As identified by the paper I referenced earlier, <a href="https://oreil.ly/2J89P">“Navigating the EU AI Act: A Methodological Approach to Compliance for Safety-Critical Products”</a> by Jessica Kelly et al., these are:</p>

<dl>
	<dt>Risk identification</dt>
	<dd>
	<p>This involves proactively recognizing potential issues that could arise from the AI system’s development, deployment, and operation. Implementing robust risk identification processes ensures that risks are managed before they become critical problems.</p>
	</dd>
	<dt>Testability</dt>
	<dd>
	<p>This ensures that the AI system can be thoroughly evaluated for performance, reliability, and safety before deployment.</p>
	</dd>
	<dt>Value alignment</dt>
	<dd>
	<p>This ensures that the AI system’s objectives and behaviors are consistent with human values, ethical principles, and societal norms.</p>
	</dd>
</dl>

<p>Let’s see how we can translate these quality attributes into AI engineering practices. I’ll use the CRISP-ML(Q) phases as a framework to tackle the complexity of the end-to-end process, considering each phase separately.</p>

<section data-pdf-bookmark="1. Business and data understanding" data-type="sect3"><div class="sect3" id="chapter_5_section_1_business_and_data_understanding_1748539922577710">
<h3>1. Business and data understanding</h3>

<p>The initial phase of the AI project focuses on establishing clear business goals and defining success criteria to ensure the application delivers business value. This involves pinpointing the specific objectives of the AI solution, identifying relevant data sources, and assembling an initial dataset. Practically speaking, conducting an AI system risk assessment during this phase involves three steps: identifying potential risks, assessing the impact and likelihood of those risks, and assigning appropriate mitigation strategies to responsible parties.</p>

<p>Consider conducting workshops with stakeholders to identify possible risks. You might also want to use established AI risk frameworks, such as the IEEE AI Ethics Guidelines. Questions to ask include:</p>

<ul>
	<li>
	<p>What could go wrong with the data (e.g., bias, privacy breaches)?</p>
	</li>
	<li>
	<p>What operational risks could occur (e.g., concept drift, service downtime, poor model performance)?</p>
	</li>
	<li>
	<p>What regulatory risks are relevant (e.g., GDPR or EU AI Act noncompliance)?</p>
	</li>
</ul>

<section class="pagebreak-before" data-pdf-bookmark="Failure Mode and Effects Analysis" data-type="sect4"><div class="sect4" id="chapter_5_failure_mode_and_effects_analysis_1748539922577786">
<h4 class="less_space">Failure Mode and Effects Analysis</h4>

<p>As part of a proactive risk management strategy, you should conduct <em>Failure Mode and Effects Analysis</em> (FMEA) tailored<a contenteditable="false" data-primary="Failure Mode and Effects Analysis (FMEA)" data-type="indexterm" id="fmea-1"/> to AI systems. This systematic process used to identify, assess, and mitigate risks associated with potential failure points in a system is especially valuable in safety-critical environments. It enables teams to anticipate and prioritize risks before they impact users or system performance. Incorporating FMEA into your continuous delivery pipelines and model monitoring workflows during the later stages of CRISP-ML(Q) supports ongoing compliance and minimizes operational risk.</p>

<p>Conducting FMEA for AI systems involves the following steps:</p>

<dl>
	<dt>1. Identify potential failure modes.</dt>
	<dd>
	<p>Consider AI-specific failures such as data quality issues (e.g., missing values, outliers), model underfitting or overfitting, concept drift or data drift, adversarial attacks, bias in predictions, explainability failures, performance degradation over time, resource constraints (e.g., memory, computation time), and integration failures with other systems.</p>
	</dd>
	<dt>2. Determine potential effects.</dt>
	<dd>
	<p>For each failure mode, identify potential consequences, such as incorrect predictions leading to poor decision making, biased outcomes affecting specific groups, privacy breaches, financial losses, reputational damage, and regulatory <span class="keep-together">noncompliance</span>.</p>
	</dd>
	<dt>3. Identify potential causes.</dt>
	<dd>
	<p>For each failure mode, determine possible root causes, such as insufficient or biased training data, poor feature selection, suboptimal hyperparameter tuning, inadequate model architecture, errors in data preprocessing, use of personal data without appropriate consent, and changes in the deployment environment.</p>
	</dd>
	<dt>4. Assess current controls.</dt>
	<dd>
	<p>Evaluate existing measures to prevent or detect each failure mode. These might include data validation checks, model performance monitoring, automated testing procedures, human-in-the-loop oversight, and explainability techniques.</p>
	</dd>
</dl>

<p>Create a template to use for FMEA. <a data-type="xref" href="#chapter_5_table_1_1748539922529349">Table 5-1</a> shows an example of what this might look like and the kind of information it might contain.</p>

<table class="pagebreak-before striped less_space" id="chapter_5_table_1_1748539922529349">
	<caption><span class="label">Table 5-1. </span>Example FMEA specific to an AI system for proactive risk management in alignment with Article 9 of the EU AI Act</caption>
	<thead>
		<tr>
			<th>Failure mode</th>
			<th>Failure cause</th>
			<th>Failure effect</th>
			<th>S</th>
			<th>O</th>
			<th>D</th>
			<th>RPN</th>
			<th>Mitigation plan</th>
			<th>Owner</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Data bias</p>
			</td>
			<td>
			<p>Poor data sampling</p>
			</td>
			<td>
			<p>Biased predictions</p>
			</td>
			<td>
			<p>9</p>
			</td>
			<td>
			<p>8</p>
			</td>
			<td>
			<p>5</p>
			</td>
			<td>
			<p>360</p>
			</td>
			<td>
			<p>Use fairness audits</p>
			</td>
			<td>
			<p>Data team</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Model overfitting</p>
			</td>
			<td>
			<p>Model complexity</p>
			</td>
			<td>
			<p>Poor generalization in production</p>
			</td>
			<td>
			<p>7</p>
			</td>
			<td>
			<p>6</p>
			</td>
			<td>
			<p>7</p>
			</td>
			<td>
			<p>294</p>
			</td>
			<td>
			<p>Conduct regular retraining</p>
			</td>
			<td>
			<p>ML team</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Model drift</p>
			</td>
			<td>
			<p>Evolving user behavior</p>
			</td>
			<td>
			<p>Decreasing prediction quality</p>
			</td>
			<td>
			<p>8</p>
			</td>
			<td>
			<p>7</p>
			</td>
			<td>
			<p>6</p>
			</td>
			<td>
			<p>336</p>
			</td>
			<td>
			<p>Monitor metrics</p>
			</td>
			<td>
			<p>MLOps team</p>
			</td>
		</tr>
	</tbody>
</table>

<p>Let’s walk through each of the columns:</p>

<ul>
	<li>
	<p>The <em>failure modes</em> are the way in which the ML models that are part of the AI system could fail.</p>
	</li>
	<li>
	<p>The <em>failure causes</em> are the root causes that might lead to the failures.</p>
	</li>
	<li>
	<p>The<strong> </strong><em>failure effects</em> are the consequences of the failures.</p>
	</li>
	<li>
	<p>The<strong> </strong><em>severity</em> (<em>S</em>) can be assigned on a scale from 1 to 10, where 1 = minimal impact and 10 = catastrophic failure.</p>
	</li>
	<li>
	<p>The <em>frequency of occurrence</em> (<em>O</em>) can be assigned on a scale from 1 to 10, where <span class="keep-together">1 = unlikely</span> and 10 = very frequent.</p>
	</li>
	<li>
	<p>The<strong> </strong><em>detection rate</em> (<em>D</em>) can be assigned on a scale from 1 to 10, where 1 = easily detectable and 10 = hard to detect.</p>
	</li>
	<li>
	<p>The <em>risk priority number</em> (<em>RPN</em>) is equal to<strong> </strong><em>S * O * D</em>; a higher RPN means a higher priority for mitigation.</p>
	</li>
	<li>
	<p>The <em>mitigation plan</em> is the strategy and concrete steps to mitigate the identified risk.</p>
	</li>
	<li>
	<p>The <em>owner</em> is the team accountable for mitigating the identified risk.</p>
	</li>
</ul>

<p>You can use tools like Jira, Confluence, Google Sheets, or Excel for tracking FMEA<a contenteditable="false" data-primary="Failure Mode and Effects Analysis (FMEA)" data-startref="fmea-1" data-type="indexterm" id="id550"/>.</p>
</div></section>

<section data-pdf-bookmark="Testability and value alignment" data-type="sect4"><div class="sect4" id="chapter_5_testability_and_value_alignment_1748539922577850">
<h4>Testability and value alignment</h4>

<p>In addition to risk identification and mitigation, it’s essential to establish clear, measurable objectives and key performance indicators (KPIs) that the AI system must meet. This includes creating a comprehensive plan outlining how each component and the system as a whole will be tested.</p>

<p>It’s also important to clearly outline the ethical guidelines and principles that the AI system must adhere to, such as fairness and transparency. One practical tool that you can use for this<a contenteditable="false" data-primary="Value Canvas" data-type="indexterm" id="id551"/> is the <a href="https://oreil.ly/c79Xu">Values Canvas</a>, a template for developing ethical AI strategies and documenting existing ethics efforts.</p>

<p>Be sure to involve internal and external stakeholders in these processes, to understand their values, expectations, and needs. Internal stakeholders may include data scientists, MLOps engineers, legal teams, product managers, and executives, while external stakeholders can include regulators, customers, partners, and auditors. To categorize and prioritize stakeholders based on their level of influence and interest in the AI system<a contenteditable="false" data-primary="Power–Interest Grid" data-type="indexterm" id="id552"/>, use the <a href="https://oreil.ly/CGtNw">Power–Interest Grid</a>.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="2. Data preparation" data-type="sect3"><div class="sect3" id="chapter_5_section_2_data_preparation_1748539922577905">
<h3>2. Data preparation</h3>

<p>When conducting risk identification, it’s vital to assess the quality of the data being used. This includes identifying risks related to data quality, such as missing values, inconsistencies, and potential biases. Typical MLOps practices involve automating data validation and profiling using frameworks like Great Expectations or Deequ that can automatically detect issues like these.</p>

<p>Data quality pipelines should also incorporate unit tests to monitor drift and data integrity in real time. Make sure to write unit tests for the data extraction, transformation, and loading (ETL) processes in the data pipelines to ensure any issues with data handling are caught early. To ensure effective testability, implement scripts to automatically validate the integrity and quality of the data.</p>

<p>Regarding value alignment, when curating data, it’s important to ensure that the dataset represents the necessary diversity to prevent biased outcomes. MLOps practices should integrate ethical checks, such as ensuring that no discriminatory proxies are used in feature engineering. To implement ethical checks, integrate tools like SHAP, Fairlearn, Great Expectations, and EvidentlyAI into your MLOps pipelines.</p>

<p>For example, the data pipeline might look like this:</p>

<ol>
	<li>
	<p>Data ingestion</p>
	</li>
	<li>
	<p>Quality checks (Great Expectations)</p>
	</li>
	<li>
	<p>Correlation analysis for proxy detection</p>
	</li>
</ol>

<p>Additionally, privacy preservation techniques such as data minimization or anonymization via advanced techniques like differential privacy should be applied to align with ethical standards.</p>
</div></section>

<section data-pdf-bookmark="3. Modeling" data-type="sect3"><div class="sect3" id="chapter_5_section_3_modeling_1748539922577959">
<h3>3. Modeling</h3>

<p>Risk identification in the modeling phase involves evaluating the risks associated with different modeling approaches, such as overfitting and underfitting.</p>

<p>To incorporate testability into your model code, consider using automated testing frameworks such as pytest for unit tests. Implementing cross-validation techniques can help ensure that your model generalizes well across different datasets and <span class="keep-together">scenarios</span>.</p>

<p>With regard to value alignment, it’s important to select algorithms that align with ethical considerations—for example, you should use interpretable models for high-stakes decisions. Additionally, metrics should be incorporated into the model to assess and address biases in model predictions. Collaborate with stakeholders to determine the values you want to uphold (e.g., fairness, robustness, interpretability) and define specific metrics, such as:</p>

<dl>
	<dt>Fairness</dt>
	<dd>
	<p>Demographic parity, equal opportunity</p>
	</dd>
	<dt>Robustness</dt>
	<dd>
	<p>Tolerance to noisy inputs, resilience to adversarial examples</p>
	</dd>
</dl>

<p>Integrating relevant tools into the pipeline can help ensure continuous alignment with ethical values throughout the product lifecycle and compliance with the requirements of the EU AI Act. Here are some recommendations:</p>

<ul>
	<li>
	<p>MLOps tools: MLflow, Airflow, DVC, Kubeflow, TensorFlow Extended (TFX)</p>
	</li>
	<li>
	<p>Fairness tools: Fairlearn, AI Fairness 360</p>
	</li>
	<li>
	<p>Monitoring tools: EvidentlyAI for drift detection</p>
	</li>
</ul>
</div></section>

<section data-pdf-bookmark="4. Evaluation" data-type="sect3"><div class="sect3" id="chapter_5_section_4_evaluation_1748539922578015">
<h3>4. Evaluation</h3>

<p>Next, let’s consider how the quality attributes that Article 9 maps to can be implemented in the evaluation phase of the CRISP-ML(Q) model.</p>

<section data-pdf-bookmark="Risk identification" data-type="sect4"><div class="sect4" id="chapter_5_risk_identification_1748539922578071">
<h4>Risk identification</h4>

<p>Performing model interpretability analysis during the evaluation phase helps identify unexpected or undesirable model behaviors. You can use tools like SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations), or InterpretML to generate explanations.</p>

<p>When analyzing the results, look for features that are unexpectedly high in importance and counterintuitive feature interactions. Check for proxy variables that might inadvertently introduce bias. Document your findings in a structured report detailing unexpected behaviors and their potential impacts, hypotheses for the causes of these behaviors, and proposed mitigations. Also include key features influencing the model in this report.</p>

<p>To further uncover risks and ensure testability, conduct adversarial testing<a contenteditable="false" data-primary="adversarial testing" data-type="indexterm" id="id553"/> using libraries such as the Adversarial Robustness Toolbox (ART), PyRIT, or CleverHans. Tailor your approach to the model type, deployment mode, and model exposure strategy.</p>

<p class="pagebreak-before">Relevant types of adversarial attacks might include:</p>

<ul>
	<li>
	<p>Evasion attacks (e.g., Fast Gradient Sign Method, Carlini &amp; Wagner)</p>
	</li>
	<li>
	<p>Poisoning attacks (e.g., label flipping, backdoor insertion)</p>
	</li>
	<li>
	<p>Model extraction attacks</p>
	</li>
</ul>

<p>Document your findings regarding the model’s vulnerability to different types of attacks and classify them based on severity and likelihood. You can use this not only for recordkeeping but as a “thinking tool” to develop mitigation strategies such as adversarial training or input preprocessing.</p>
</div></section>

<section data-pdf-bookmark="Testability" data-type="sect4"><div class="sect4" id="chapter_5_testability_1748539922578123">
<h4>Testability</h4>

<p>AI systems are always embedded in larger software systems, which are often legacy systems with complex interdependencies. Therefore, comprehensive test suites are needed to ensure that the AI components integrate reliably and function as expected. These should include:</p>

<ul>
	<li>
	<p><em>Unit tests</em> to validate individual components of the ML pipeline (e.g., data preprocessing, feature engineering)</p>
	</li>
	<li>
	<p><em>Integration tests </em>to ensure that the AI system component interacts correctly with other parts of the system</p>
	</li>
	<li>
	<p><em>System tests</em> to evaluate the AI system in a production-like environment (including performance testing, load testing, and error handling)</p>
	</li>
	<li>
	<p><em>A/B testing </em>in controlled environments to validate performance, demonstrate the business impact of changes to the model, enable incremental rollouts, and detect issues early</p>
	</li>
</ul>

<p>From a compliance perspective, A/B testing can help verify that a new model maintains or improves fairness metrics across different user groups in a real-world setting. It aids in demonstrating due diligence in model deployment, which is crucial for regulatory compliance under the EU AI Act.</p>
</div></section>

<section data-pdf-bookmark="Value alignment" data-type="sect4"><div class="sect4" id="chapter_5_value_alignment_1748539922578175">
<h4>Value alignment</h4>

<p>Be sure to evaluate model performance against ethical guidelines defined earlier, such as fairness, transparency, privacy, and accountability, by using libraries like Fairlearn or AI Fairness 360 to implement fairness metrics.</p>

<p>This comprehensive evaluation helps ensure compliance with Article 9 and promotes the development of responsible and trustworthy AI systems.</p>
</div></section>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="5. Deployment" data-type="sect3"><div class="sect3" id="chapter_5_section_5_deployment_1748539922578231">
<h3 class="less_space">5. Deployment</h3>

<p>The next stage is to deploy your AI system in production. Let’s look at deployment through the lens of the three quality attributes that are relevant to Article 9.</p>

<section data-pdf-bookmark="Risk identification" data-type="sect4"><div class="sect4" id="chapter_5_risk_identification_1748539922578289">
<h4>Risk identification</h4>

<p>You’ll first want to implement vulnerability scanning to identify security risks. The <a href="https://oreil.ly/Ks7Jt">OWASP Machine Learning Security Top 10</a> is a good reference<a contenteditable="false" data-primary="OWASP Machine Learning Security Top 10" data-type="indexterm" id="id554"/> for understanding common security issues, vulnerabilities, and risks associated with machine learning systems. You might also use tools like OWASP ZAP or Nessus to scan for common vulnerabilities such as:</p>

<ul>
	<li>
	<p>Input manipulation attacks (ML01:2023)</p>
	</li>
	<li>
	<p>Data poisoning attacks (ML02:2023)</p>
	</li>
	<li>
	<p>Model inversion attacks (ML03:2023)</p>
	</li>
	<li>
	<p>Membership inference attacks (ML04:2023)</p>
	</li>
	<li>
	<p>Model theft (ML05:2023)</p>
	</li>
	<li>
	<p>AI supply chain attacks (ML06:2023)</p>
	</li>
	<li>
	<p>Transfer learning attacks (ML07:2023)</p>
	</li>
	<li>
	<p>Model skewing (ML08:2023)</p>
	</li>
	<li>
	<p>Output integrity attacks (ML09:2023)</p>
	</li>
	<li>
	<p>Model poisoning (ML10:2023)</p>
	</li>
</ul>

<p>Additionally, the following AI incident and risk trackers<a contenteditable="false" data-primary="AI incident and risk trackers" data-type="indexterm" id="id555"/> can help you stay informed about the current developments and keep your AI security strategy up-to-date:</p>

<ul>
	<li>
	<p><a href="https://oreil.ly/4vrn_">AI Risk Repository</a></p>
	</li>
	<li>
	<p><a href="https://oreil.ly/8dyrm">AI Incident Database</a></p>
	</li>
	<li>
	<p><a href="https://oreil.ly/tVVZ1">OECD AI Incidents and Hazards Monitor</a></p>
	</li>
</ul>

<p>Another important<a contenteditable="false" data-primary="penetration testing" data-type="indexterm" id="id556"/> pre-deployment activity is manual or automated penetration testing. Penetration testing helps identify ML vulnerabilities before malicious actors can exploit them. It’s essential for AI systems handling sensitive data or intellectual property. Tools like Metasploit and Burp Suite can be used to test various attack vectors. Compliance with the EU AI Act requires demonstrable security measures, and regular penetration testing, continuous monitoring, and incident response planning should be part of a comprehensive security strategy for AI systems.</p>

<p>Perform a security-focused code review of the deployment scripts and infrastructure-as-code, and assess operational risks in the production environment. You can evaluate the reliability and scalability of your cloud or on-premises infrastructure using tools like AWS Trusted Advisor or Azure Advisor. To mitigate supply chain risks, analyze all dependencies for known vulnerabilities in the <a href="https://oreil.ly/euMDH">National Vulnerability Database</a>. Tools like OWASP Dependency-Check and GitHub Dependabot are good starting points.</p>

<p>During deployment, you should also plan for disaster recovery. Develop and test backup and restore procedures for data, models, and pipeline artifacts, and implement and validate failover mechanisms for critical components in the ML system.</p>
</div></section>

<section data-pdf-bookmark="Testability" data-type="sect4"><div class="sect4" id="chapter_5_testability_1748539922578343">
<h4>Testability</h4>

<p>Testability in the deployment phase hinges on AI system observability, including implementing continuous monitoring and alerting systems. The initial step is to establish key metrics that offer a holistic view of the AI system. Your metrics categories should include:</p>

<ul>
	<li>
	<p>Model performance metrics (e.g., accuracy, latency)</p>
	</li>
	<li>
	<p>System health metrics (e.g., CPU usage, memory usage)</p>
	</li>
	<li>
	<p>Business metrics (e.g., number of predictions, error rates)</p>
	</li>
</ul>

<p>Once the metrics are defined, set up alerting by defining appropriate thresholds. You can use tools like Prometheus with Alertmanager or CloudWatch Alarms to implement alerting based on predefined conditions.</p>

<p>You might also want to implement API testing, automated regression testing, and data and model validation tests to detect drift. You can use tools like Deepchecks, a holistic open source solution for AI and ML validation.</p>
</div></section>

<section data-pdf-bookmark="Value alignment" data-type="sect4"><div class="sect4" id="chapter_5_value_alignment_1748539922578394">
<h4>Value alignment</h4>

<p>To implement continuous value alignment checks, regularly assess model outputs against defined ethical guidelines (examples of ethical metrics are demographic parity, equal opportunity, and representation balance). A useful way to identify these ethical values is to apply the Foundational Value Finder Framework, as outlined in Olivia Gambelin’s book <a href="https://oreil.ly/hmXoe"><em>Responsible AI</em></a>. It unites the government, industry, and organizational ethical value sets.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="6. Monitoring and maintenance" data-type="sect3"><div class="sect3" id="chapter_5_section_6_monitoring_and_maintenance_1748539922578446">
<h3>6. Monitoring and maintenance</h3>

<p>To implement the quality attributes associated with Article 9 during the monitoring and maintenance phase, several key tasks need to be addressed.</p>

<p>Risk identification involves continuously monitoring for data drift and model performance degradation using automated tools. Update the AI system’s risk register regularly to reflect new insights gathered from operational usage. Risks should be categorized across technical, operational, ethical, and regulatory domains.</p>

<p>To ensure testability, periodic audits of model performance, system behavior, and fairness metrics are important. Introduce chaos engineering techniques, such as <span class="keep-together">simulating</span> data corruption, resource exhaustion, or network failures, to identify system vulnerabilities.</p>

<p>Maintaining value alignment requires regular reviews and updates of ethical guidelines and periodic stakeholder reviews to assess alignment between stakeholder expectations and system behavior.</p>
</div></section>

<section data-pdf-bookmark="Summary" data-type="sect3"><div class="sect3" id="chapter_5_summary_1748539922578502">
<h3>Summary</h3>

<p><a data-type="xref" href="#chapter_5_table_2_1748539922529399">Table 5-2</a> summarizes the AI engineering practices you should implement across the CRISP-ML(Q) phases to support the quality attributes associated with Article 9. Following the steps outlined here will help ensure ongoing compliance with the EU AI Act while maintaining<a contenteditable="false" data-primary="quality attributes (for AI systems)" data-secondary="associated with Article 9: Risk Management System" data-type="indexterm" id="id557"/> system reliability and ethical alignment.</p>

<table id="chapter_5_table_2_1748539922529399">
	<caption><span class="label">Table 5-2. </span>Quality attributes relevant to Article 9 and corresponding AI engineering practices and tools</caption>
	<thead>
		<tr>
			<th>CRISP-ML(Q) phase</th>
			<th>Quality attributes</th>
			<th>AI engineering practices and tools</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td class="subheading" rowspan="2">Business and data understanding</td>
			<td>
			<p>Risk identification</p>
			</td>
			<td>
			<p>Stakeholder interviews, initial risk assessment, creation of risk register, FMEA</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Value alignment</p>
			</td>
			<td>
			<p>Ethical impact assessments aligned with project goals</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="3">Data preparation</td>
			<td>
			<p>Risk identification</p>
			</td>
			<td>
			<p>Data quality assessment, data privacy/security risks</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Testability</p>
			</td>
			<td>
			<p>Data validation tests, versioning/lineage</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Value alignment</p>
			</td>
			<td>
			<p>Ethical checks, fairness audits</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="3">Modeling</td>
			<td>
			<p>Risk identification</p>
			</td>
			<td>
			<p>Model vulnerability assessments, failure modes</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Testability</p>
			</td>
			<td>
			<p>Unit tests, integration tests</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Value alignment</p>
			</td>
			<td>
			<p>Define metrics, ethical guidelines</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="3">Evaluation</td>
			<td>
			<p>Risk identification</p>
			</td>
			<td>
			<p>Model interpretability analysis, adversarial testing</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Testability</p>
			</td>
			<td>
			<p>Comprehensive test suites, A/B testing</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Value alignment</p>
			</td>
			<td>
			<p>Evaluate against ethical guidelines</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="3">Deployment</td>
			<td>
			<p>Risk identification</p>
			</td>
			<td>
			<p>Pre-deployment security audits, operational risks</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Testability</p>
			</td>
			<td>
			<p>Continuous monitoring/alerting, regression testing</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Value alignment</p>
			</td>
			<td>
			<p>Feedback mechanisms</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="3">Monitoring and maintenance</td>
			<td>
			<p>Risk identification</p>
			</td>
			<td>
			<p>Continuous monitoring, risk register updates</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Testability</p>
			</td>
			<td>
			<p>Model/system audits, chaos engineering</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Value alignment</p>
			</td>
			<td>
			<p>Regular reviews and updates of ethical guidelines, stakeholder reviews</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="Checklist for compliance" data-type="sect3"><div class="sect3" id="chapter_5_checklist_for_compliance_1748539922578564">
<h3 class="less_space">Checklist for compliance</h3>

<p>The following is a practical checklist for aligning AI engineering practices with Article 9<a contenteditable="false" data-primary="checklists for compliance (quality attributes)" data-secondary="Article 9: Risk Management System" data-type="indexterm" id="checkl-art-9"/> of the EU AI Act across the CRISP-ML(Q) lifecycle. This framework supports regulatory compliance by integrating risk management, testability, and value alignment throughout each phase of the development process:</p>

<dl>
	<dt>Pre-development (not a part of CRISP-ML(Q))</dt>
	<dd>
	<ul>
		<li>
		<p>Define risk assessment methodologies and tools to identify critical risks based on potential harms (e.g., safety, privacy, bias).</p>
		</li>
		<li>
		<p>Create a risk management plan aligned with EU AI Act requirements.</p>
		</li>
		<li>
		<p>Establish a centralized risk register and documentation strategy.</p>
		</li>
	</ul>
	</dd>
	<dt>Business and data understanding</dt>
	<dd>
	<ul>
		<li>
		<p>Conduct an initial risk identification workshop with key stakeholders.</p>
		</li>
		<li>
		<p>Document potential AI risks in the risk register.</p>
		</li>
		<li>
		<p>Perform an ethical impact assessment.</p>
		</li>
		<li>
		<p>Align project goals with organizational values and EU AI Act compliance requirements.</p>
		</li>
	</ul>
	</dd>
	<dt>Data preparation</dt>
	<dd>
	<ul>
		<li>
		<p>Assess data quality, completeness, and representativeness.</p>
		</li>
		<li>
		<p>Ensure datasets are free from discriminatory bias.</p>
		</li>
		<li>
		<p>Identify and document data-related risks (e.g., bias, privacy, drift).</p>
		</li>
		<li>
		<p>Implement data versioning and lineage tracking.</p>
		</li>
		<li>
		<p>Design and implement data validation tests, incorporating quality checks for completeness and correctness.</p>
		</li>
	</ul>
	</dd>
	<dt>Modeling</dt>
	<dd>
	<ul>
		<li>
		<p>Conduct model vulnerability assessments.</p>
		</li>
		<li>
		<p>Validate the model architecture for explainability and robustness.</p>
		</li>
		<li>
		<p>Implement version control for model code and artifacts.</p>
		</li>
		<li>
		<p>Design and implement unit tests for model components.</p>
		</li>
		<li>
		<p>Document model architecture and design decisions.</p>
		</li>
		<li>
		<p>Implement integration tests for the full ML pipeline.</p>
		</li>
		<li>
		<p>Simulate edge cases and stress-test model behavior.</p>
		</li>
	</ul>
	</dd>
	<dt class="pagebreak-before">Evaluation</dt>
	<dd>
	<ul>
		<li>
		<p>Execute comprehensive test suites (unit, integration, system).</p>
		</li>
		<li>
		<p>Perform adversarial testing and document results (vulnerabilities, mitigation strategies).</p>
		</li>
		<li>
		<p>Evaluate performance with regard to business and ethical goals and the intended purpose of the system; include value alignment metrics such as fairness, explainability, robustness, safety, and accountability.</p>
		</li>
		<li>
		<p>Update the risk register based on evaluation results.</p>
		</li>
		<li>
		<p>Conduct A/B testing in controlled environments.</p>
		</li>
		<li>
		<p>Engage domain experts to validate evaluation metrics.</p>
		</li>
	</ul>
	</dd>
	<dt>Deployment</dt>
	<dd>
	<ul>
		<li>
		<p>Conduct a pre-deployment security audit.</p>
		</li>
		<li>
		<p>Implement a continuous monitoring and alerting system.</p>
		</li>
		<li>
		<p>Set up automated regression testing.</p>
		</li>
		<li>
		<p>Establish feedback mechanisms for end users and stakeholders.</p>
		</li>
		<li>
		<p>Ensure that risk mitigation measures (e.g., rollback mechanisms) are operational.</p>
		</li>
		<li>
		<p>Integrate quality gate checks into the deployment pipeline.</p>
		</li>
	</ul>
	</dd>
	<dt>Monitoring and maintenance</dt>
	<dd>
	<ul>
		<li>
		<p>Implement automated monitoring for data drift, concept drift, and model <span class="keep-together">degradation</span>.</p>
		</li>
		<li>
		<p>Define thresholds and set up alerting for critical metrics.</p>
		</li>
		<li>
		<p>Conduct regular internal audits of model and system performance.</p>
		</li>
		<li>
		<p>Perform periodic chaos engineering tests.</p>
		</li>
		<li>
		<p>Regularly update the risk register based on operational insights and regulatory guidance.</p>
		</li>
		<li>
		<p>Schedule periodic reviews of ethical guidelines and value alignment.</p>
		</li>
	</ul>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Further reading" data-type="sect3"><div class="sect3" id="chapter_5_further_reading_1748539922578618">
<h3>Further reading</h3>

<p>As further reading, I recommend the <a href="https://oreil.ly/ht2NM">NIST AI Risk Management Framework</a> (AI RMF). This is important in the context of Article 9<a contenteditable="false" data-primary="checklists for compliance (quality attributes)" data-secondary="Article 9: Risk Management System" data-startref="checkl-art-9" data-type="indexterm" id="id558"/> because it offers a lifecycle-based approach to identifying, assessing, and mitigating AI risks, in line with the EU AI Act’s requirements for continuous<a contenteditable="false" data-primary="Article 10: Data and Data Governance" data-type="indexterm" id="art-10-ddg-1"/> risk management in high-risk AI systems.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Article 10: Data and Data Governance" data-type="sect2"><div class="sect2" id="chapter_5_article_10_data_and_data_governance_1748539922578693">
<h2>Article 10: Data and Data Governance</h2>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="chapter_5_article_10_1748539922578764">
<h1>Article 10</h1>

<p><a href="https://oreil.ly/v75QK">Article 10 of the EU AI Act</a> mandates high-quality data and sound data governance practices for high-risk AI systems. It aims to ensure that these systems are developed, trained, validated, and tested using data that supports fairness, accuracy, and <span class="keep-together">transparency</span>.</p>

<p>Key points of Article 10 include<a contenteditable="false" data-primary="Article 10: Data and Data Governance" data-secondary="key points of" data-type="indexterm" id="id559"/>:</p>

<dl>
	<dt>Data quality</dt>
	<dd>
	<p>Data used to train, validate, and test AI systems must be relevant, representative, and as free from errors and bias as possible to ensure fair and accurate outcomes.</p>
	</dd>
	<dt>Preprocessing requirements</dt>
	<dd>
	<p>Mandatory preprocessing steps, such as data cleaning, normalization, and anonymization, must be applied to enhance the data’s usability and minimize bias.</p>
	</dd>
	<dt>Data relevance and representativeness</dt>
	<dd>
	<p>To ensure reliable outcomes, data used for training, validation, and testing must reflect the AI system’s intended use cases and operational environment.</p>
	</dd>
	<dt>Bias monitoring</dt>
	<dd>
	<p>Ongoing monitoring is required to detect and correct biases that could impact health, safety, or fundamental rights—particularly those leading to <span class="keep-together">discrimination</span>.</p>
	</dd>
	<dt>Documentation and transparency</dt>
	<dd>
	<p>Comprehensive documentation is required, including clear descriptions of how the data was collected, selected, and processed.</p>
	</dd>
	<dt>Data governance practices</dt>
	<dd>
	<p>Organizations must implement governance measures, including access control, data management policies, and versioning, to ensure continuous compliance with the Act.</p>
	</dd>
	<dt>Periodic review</dt>
	<dd>
	<p>Data governance practices and datasets must be reviewed and updated regularly to maintain alignment with the AI system’s evolving context and address newly identified risks.</p>
	</dd>
</dl>
</div></aside>

<p>As you may recall from <a data-type="xref" href="ch03.html#chapter_3_data_and_ai_governance_and_ai_engineering_1748539918115723">Chapter 3</a>, at its core, data governance ensures high data quality throughout the machine learning lifecycle, supports business objectives, and enables organization-wide data use by focusing on availability, usability, consistency, integrity, security, and compliance. That chapter also outlined data governance <span class="keep-together">processes</span> for each stage of the CRISP-ML(Q) framework, providing a practical approach to managing data responsibly across the development lifecycle that supports the creation of AI systems that are trustworthy, compliant, and sustainable.</p>

<p>Since Article 10 of the EU AI Act provides only a high-level explanation of the requirements regarding data and data governance, let’s break down the quality attributes that support a comprehensive framework for implementing trustworthy AI through MLOps practices. In “Navigating the EU AI Act,” Kelly et al. map Article 10 to 13 quality attributes  for high-risk AI systems. These are:</p>

<dl>
	<dt>Independence</dt>
	<dd>
	<p>The extent to which datasets used for training, validation, and testing are isolated and unaffected by each other, preventing data leakage across sets.</p>
	</dd>
	<dt>Data completeness</dt>
	<dd>
	<p>The extent to which the dataset contains all necessary data points and variables to support accurate predictions, covering the full range of use cases and features relevant to the AI system’s purpose.</p>
	</dd>
	<dt>Currentness</dt>
	<dd>
	<p>The degree to which the data is up-to-date and temporally relevant for its intended use case.</p>
	</dd>
	<dt>Data fairness</dt>
	<dd>
	<p>The absence of bias, prejudice, or favoritism toward individuals or groups in the data, particularly in relation to protected characteristics.</p>
	</dd>
	<dt>Precision</dt>
	<dd>
	<p>The level of exactness or granularity in the data measurements and representations.</p>
	</dd>
	<dt>Representativeness</dt>
	<dd>
	<p>The extent to which the data and its distribution reflects the characteristics of the real-world population or scenario it is intended to model.</p>
	</dd>
	<dt>Consistency</dt>
	<dd>
	<p>The uniformity of data (the degree to which it maintains its format, structure, and values) across the entire dataset and over time.</p>
	</dd>
	<dt>Accuracy</dt>
	<dd>
	<p>The closeness of data values to their true or accepted real-world values.</p>
	</dd>
	<dt>Credibility</dt>
	<dd>
	<p>The trustworthiness of the data sources and collection methods.</p>
	</dd>
	<dt>Temporality</dt>
	<dd>
	<p>The degree to which the temporal characteristics of the data, including timeliness, aging, versioning, and lifecycle management, ensure its ongoing relevance and appropriateness. This is important for understanding how data changes over time and for making decisions based on current information.</p>
	</dd>
	<dt>Confidentiality</dt>
	<dd>
	<p>The protection of sensitive information in accordance with privacy requirements and data protection regulations.</p>
	</dd>
	<dt>Compliance</dt>
	<dd>
	<p>The degree to which data practices conform to applicable legal, ethical, and organizational standards and regulations.</p>
	</dd>
	<dt>Data traceability</dt>
	<dd>
	<p>The ability to track data from its origin through its entire lifecycle, including all transformations, access points, and uses, supporting transparency, accountability, and auditability.</p>
	</dd>
</dl>

<p>The quality attributes associated with Article 10 are distributed throughout the CRISP-ML(Q) lifecycle. <a data-type="xref" href="#chapter_5_table_3_1748539922529427">Table 5-3</a> maps these attributes to each development phase and highlights practical AI engineering practices for implementing them<a contenteditable="false" data-primary="quality attributes (for AI systems)" data-secondary="associated with Article 10: Data and Data Governance" data-type="indexterm" id="id560"/>.</p>

<table id="chapter_5_table_3_1748539922529427">
	<caption><span class="label">Table 5-3. </span>Quality attributes relevant to Article 10 and corresponding AI engineering practices and tools</caption>
	<thead>
		<tr>
			<th>CRISP-ML(Q) phase</th>
			<th>Quality attributes</th>
			<th>AI engineering practices and tools</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td class="subheading" rowspan="5">Business and data understanding</td>
			<td>
			<p>Independence</p>
			</td>
			<td>
			<p>Establish clear data governance policies that outline data acquisition procedures and criteria to ensure data independence.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data completeness</p>
			</td>
			<td>
			<p>Perform thorough data exploration and profiling to identify missing data points, outliers, or insufficient representation and define strategies for addressing them.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Currentness</p>
			</td>
			<td>
			<p>Document data update frequencies and establish refresh procedures to maintain currentness.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data fairness</p>
			</td>
			<td>
			<p>Analyze data for potential biases using fairness metrics and visualization tools; document and mitigate identified biases.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data traceability</p>
			</td>
			<td>
			<p>Implement data versioning and logging practices from the outset to enable traceability of data provenance, transformations, and access across the lifecycle.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="4">Data preparation</td>
			<td>
			<p>Precision</p>
			</td>
			<td>
			<p>Define and enforce data quality rules for input data, including data types, ranges, and formats, and use data validation tools to ensure data precision.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Consistency</p>
			</td>
			<td>
			<p>Apply data cleaning techniques to address inconsistencies, such as duplicate entries and conflicting data formats.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Accuracy</p>
			</td>
			<td>
			<p>Implement data validation checks to identify and correct inaccuracies and errors in the data.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data traceability</p>
			</td>
			<td>
			<p>Continue documenting data manipulation steps, including transformations, feature engineering and data cleaning procedures, to maintain traceability throughout the pipeline.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Modeling</td>
			<td>
			<p>Representativeness</p>
			</td>
			<td>
			<p>Ensure the training data represents the target population or scenario accurately, using sampling techniques to address imbalances and improve representativeness.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data traceability</p>
			</td>
			<td>
			<p>Log and version model training parameters, hyperparameters, datasets, and code to support full traceability of the development process.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Evaluation</td>
			<td>
			<p>Accuracy</p>
			</td>
			<td>
			<p>Evaluate model performance using domain-relevant metrics on a strictly independent hold-out test dataset.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Robustness</p>
			</td>
			<td>
			<p>Conduct robustness testing by introducing variations in input data and assessing model performance under different conditions.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading">Deployment</td>
			<td>
			<p>Confidentiality</p>
			</td>
			<td>
			<p>Implement security measures such as data encryption, authentication, and access control to protect sensitive data during model deployment and prediction serving.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Monitoring and maintenance</td>
			<td>
			<p>Currentness</p>
			</td>
			<td>
			<p>Monitor incoming data for changes in distribution or characteristics that might signal data staleness or concept drift, and define thresholds and triggers for automated model retraining.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Compliance</p>
			</td>
			<td>
			<p>Regularly audit model performance and data usage to verify compliance with relevant regulations and ethical guidelines.</p>
			</td>
		</tr>
	</tbody>
</table>

<section data-pdf-bookmark="Key tools" data-type="sect3"><div class="sect3" id="chapter_5_key_tools_1748539922578817">
<h3>Key tools</h3>

<p>When it comes to ensuring data quality, several key tools and technologies stand out. Among them are Great Expectations, Apache Griffin, Deequ, and TensorFlow Data Validation (TFDV), all of which play crucial roles in maintaining data integrity and reliability.</p>

<p>In terms of monitoring data and machine learning processes, tools like Prometheus, Grafana, MLflow, and Weights &amp; Biases are invaluable. They help track performance and visualize trends effectively.</p>

<p>For compliance purposes, OpenLineage, Atlas, Collibra, and DataHub are go-to tools that assist organizations in managing and safeguarding their data assets, ensuring they adhere to industry regulations and standards.</p>
</div></section>

<section data-pdf-bookmark="Checklist for compliance" data-type="sect3"><div class="sect3" id="chapter_5_checklist_for_compliance_1748539922578916">
<h3>Checklist for compliance</h3>

<p>The following checklist can be used to ensure compliance with Article 10<a contenteditable="false" data-primary="checklists for compliance (quality attributes)" data-secondary="Article 10: Data and Data Governance" data-type="indexterm" id="checkl-art-10"/> throughout the CRISP-ML(Q) lifecycle. The key is to implement these practices systematically, maintain thorough documentation in all phases, and view compliance engineering as an ongoing process:</p>

<dl>
	<dt>Business and data understanding</dt>
	<dd>
	<ul>
		<li>
		<p>Establish a data governance framework aligned with EU AI Act requirements.</p>
		</li>
		<li>
		<p>(Organizational) Define roles and responsibilities for data management.</p>
		</li>
		<li>
		<p>Define and document data quality metrics and standards.</p>
		</li>
		<li>
		<p>Maintain documentation of all data sources, including assessments of their <span class="keep-together">credibility</span>.</p>
		</li>
		<li>
		<p>Verify and record data licensing terms and usage rights for all datasets.</p>
		</li>
		<li>
		<p>Assess data protection requirements.</p>
		</li>
		<li>
		<p>Validate data source reliability.</p>
		</li>
		<li>
		<p>Profile existing datasets.</p>
		</li>
		<li>
		<p>Document known data quality issues.</p>
		</li>
		<li>
		<p>Assess data completeness requirements.</p>
		</li>
		<li>
		<p>Evaluate data representativeness.</p>
		</li>
		<li>
		<p>Check for potential biases in the data.</p>
		</li>
	</ul>
	</dd>
	<dt>Data preparation</dt>
	<dd>
	<ul>
		<li>
		<p>Specify data validation requirements (e.g., schema validation, data type checks, range checks, functional dependency validation, consistency checks).</p>
		</li>
		<li>
		<p>Implement data quality checks (e.g., completeness, accuracy, consistency).</p>
		</li>
		<li>
		<p>Set up data validation pipelines.</p>
		</li>
		<li>
		<p>Implement data cleaning procedures.</p>
		</li>
		<li>
		<p>Implement data lineage tracking.</p>
		</li>
		<li>
		<p>Validate processed data quality.</p>
		</li>
		<li>
		<p>Document transformation rules.</p>
		</li>
		<li>
		<p>Validate transformed data.</p>
		</li>
		<li>
		<p>Implement data anonymization where required.</p>
		</li>
		<li>
		<p>Set up data versioning.</p>
		</li>
		<li>
		<p>Implement privacy-preserving measures.</p>
		</li>
		<li>
		<p>Set up secure data storage.</p>
		</li>
		<li>
		<p>Implement access controls.</p>
		</li>
		<li>
		<p>Document security measures.</p>
		</li>
		<li>
		<p>Set up audit logging.</p>
		</li>
	</ul>
	</dd>
	<dt>Modeling</dt>
	<dd>
	<ul>
		<li>
		<p>Validate training data quality.</p>
		</li>
		<li>
		<p>Document the data splitting methodology.</p>
		</li>
		<li>
		<p>Implement a cross-validation strategy.</p>
		</li>
		<li>
		<p>Track data versions used for training.</p>
		</li>
		<li>
		<p>Track model versions.</p>
		</li>
		<li>
		<p>Track model lineage.</p>
		</li>
		<li>
		<p>Monitor data drift.</p>
		</li>
		<li>
		<p>Implement reproducibility controls.</p>
		</li>
		<li>
		<p>Track experiment results.</p>
		</li>
		<li>
		<p>Implement model validation procedures.</p>
		</li>
		<li>
		<p>Test model robustness.</p>
		</li>
		<li>
		<p>Validate model fairness.</p>
		</li>
	</ul>
	</dd>
	<dt>Evaluation</dt>
	<dd>
	<ul>
		<li>
		<p>Evaluate model performance.</p>
		</li>
		<li>
		<p>Assess model fairness and check for biases.</p>
		</li>
		<li>
		<p>Validate model robustness.</p>
		</li>
		<li>
		<p>Conduct a risk assessment.</p>
		</li>
		<li>
		<p>Validate security measures.</p>
		</li>
	</ul>
	</dd>
	<dt>Deployment</dt>
	<dd>
	<ul>
		<li>
		<p>Establish service level agreements for data quality attributes such as completeness, accuracy, timeliness, and consistency.</p>
		</li>
		<li>
		<p>Integrate quality gates into the deployment pipeline.</p>
		</li>
		<li>
		<p>Configure automated alerting mechanisms.</p>
		</li>
		<li>
		<p>Develop a structured incident response plan.</p>
		</li>
		<li>
		<p>Enable continuous monitoring of model performance in production.</p>
		</li>
		<li>
		<p>Design and test rollback procedures.</p>
		</li>
	</ul>
	</dd>
	<dt>Monitoring and maintenance</dt>
	<dd>
	<ul>
		<li>
		<p>Continuously monitor data quality metrics.</p>
		</li>
		<li>
		<p>Track model performance indicators such as processing or inference time and error rates.</p>
		</li>
		<li>
		<p>Regularly assess fairness metrics across user groups.</p>
		</li>
		<li>
		<p>Monitor system health (e.g., uptime, resource utilization).</p>
		</li>
		<li>
		<p>Track business-critical metrics tied to revenue, customer experience, and <span class="keep-together">compliance</span>.</p>
		</li>
		<li>
		<p>Implement structured update procedures.</p>
		</li>
		<li>
		<p>Systematically log all changes and updates.</p>
		</li>
		<li>
		<p>Maintain an internal audit trail.</p>
		</li>
	</ul>
	</dd>
</dl>

<p>Next, we’ll delve further into the requirements for technical documentation and the importance of keeping logs and records that document<a contenteditable="false" data-primary="Article 11: Technical Documentation" data-type="indexterm" id="art-11-techdoc-1"/> the system’s<a contenteditable="false" data-primary="checklists for compliance (quality attributes)" data-secondary="Article 10: Data and Data Governance" data-startref="checkl-art-10" data-type="indexterm" id="id561"/> performance<a contenteditable="false" data-primary="Article 10: Data and Data Governance" data-startref="art-10-ddg-1" data-type="indexterm" id="id562"/>.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Article 11: Technical Documentation and Article 12: Record-Keeping" data-type="sect2"><div class="sect2" id="chapter_5_article_11_technical_documentation_and_article_12_1748539922579066">
<h2>Article 11: Technical Documentation and Article 12: Record-Keeping</h2>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="chapter_5_article_11_1748539922579183">
<h1>Article 11</h1>

<p><a href="https://oreil.ly/8TJP3">Article 11 of the EU AI Act</a> mandates comprehensive technical documentation for high-risk AI systems, ensuring transparency, traceability, and accountability throughout their lifecycle. This requirement helps stakeholders and regulators verify a system’s safety, compliance, and performance.</p>

<p>Key points of Article 11 include<a contenteditable="false" data-primary="Article 11: Technical Documentation" data-secondary="key points of" data-type="indexterm" id="id563"/>:</p>

<dl>
	<dt>Comprehensive information</dt>
	<dd>
	<p>Developers must prepare detailed documentation covering the design, development, and intended purpose of the AI system.</p>
	</dd>
	<dt>Traceability and reproducibility</dt>
	<dd>
	<p>Documentation must enable the reproduction of results and provide traceability of the AI system’s processes and decisions.</p>
	</dd>
	<dt>Content requirements</dt>
	<dd>
	<p>The documentation should include details on the system’s architecture, algorithms, data sources, and performance metrics.</p>
	</dd>
	<dt>Risk and compliance reporting</dt>
	<dd>
	<p>The documentation must describe the risk management measures applied and how the system complies with the EU AI Act.</p>
	</dd>
	<dt>Availability to authorities</dt>
	<dd>
	<p>Technical documentation must be made available to regulatory authorities upon request to facilitate compliance checks and audits.</p>
	</dd>
	<dt>Maintenance and updates</dt>
	<dd>
	<p>Developers must keep the documentation up-to-date, reflecting all modifications, upgrades, or changes to the AI system.</p>
	</dd>
</dl>
</div></aside>

<p>According to Kelly et al., Article 11 maps to one key quality attribute: traceability. Documenting AI models is a central requirement of the EU AI Act, particularly as a means of assessing legal compliance. Both technical and quality management system documentation are critical components in the conformity assessment process.</p>

<p>For downstream providers incorporating a preexisting AI model into an AI system, a comprehensive understanding of the model and its functionalities is essential, both to facilitate the integration into their offerings and to ensure they fulfill their responsibilities under the EU AI Act and other applicable regulations (<a href="https://oreil.ly/kB5Pj">Recital 101</a>). As such, the creator of the AI model is required to provide comprehensive technical documentation to downstream providers (<a href="https://oreil.ly/Xn8iw">Article 53(1b)</a>).</p>

<p>MLOps and documentation are closely related, as documentation plays a critical role in the lifecycle of machine learning (ML) projects and is essential for operationalizing models effectively. For example, documentation captures details about data preprocessing, feature engineering, model configurations, hyperparameters, and environment settings, enabling others (or the same team at a later time) to reproduce experiments and results reliably.</p>

<p>Furthermore, MLOps involves tracking all stages of the ML workflow—data preparation, model training, testing, deployment, and monitoring. Documenting each stage, including data sources, code versions, and model changes, supports traceability and accountability, which are vital for auditability, especially in regulated sectors. More importantly, robust documentation is essential for meeting regulatory standards, including the data protection and transparency requirements outlined in the EU AI Act.</p>

<p>What’s more, according to DORA’s <a href="https://oreil.ly/sjTX3">2023 Accelerate State of DevOps Report</a>, high-quality documentation has been linked to a 25% increase in team performance.</p>

<section data-pdf-bookmark="Technical documentation requirements" data-type="sect3"><div class="sect3" id="chapter_5_technical_documentation_requirements_1748539922579248">
<h3>Technical documentation requirements</h3>

<p>Article 11 outlines the requirements for the technical documentation of high-risk AI systems, although related obligations are found throughout Articles 8–15.<sup><a data-type="noteref" href="ch05.html#id564" id="id564-marker">1</a></sup> These requirements apply to both datasets and AI systems.</p>

<p>Technical documentation serves two primary purposes: providing instructions for users of the system and demonstrating regulatory compliance to authorities. As such, the intended audience spans technical teams, compliance teams (AI auditors and conformity assessment bodies), and end users. <a data-type="xref" href="#chapter_5_table_4_1748539922529451">Table 5-4</a> outlines the relevant <span class="keep-together">technical</span> information elements under the EU AI Act. It provides an overview of key documentation types, their intended audiences, and the essential details<a contenteditable="false" data-primary="Article 11: Technical Documentation" data-secondary="requirements for" data-type="indexterm" id="id565"/> each should contain.</p>

<table class="striped" id="chapter_5_table_4_1748539922529451">
	<caption><span class="label">Table 5-4. </span>Overview of technical documentation requirements for datasets and AI systems</caption>
	<thead>
		<tr>
			<th>Document type</th>
			<th>Target audience</th>
			<th>Description</th>
			<th>Key information elements</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Data source documentation</p>
			</td>
			<td>
			<p>Compliance teams</p>
			</td>
			<td>
			<p>Documentation of data origins and collection methods</p>
			</td>
			<td>
			<p>Sources, collection methods, and aggregation approaches</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Dataset characteristics</p>
			</td>
			<td>
			<p>All stakeholders</p>
			</td>
			<td>
			<p>Documentation of dataset contents, coverage, and known limitations</p>
			</td>
			<td>
			<p>Data types, scope, and limitations</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data processing documentation</p>
			</td>
			<td>
			<p>Technical teams</p>
			</td>
			<td>
			<p>Documentation of data handling procedures</p>
			</td>
			<td>
			<p>Processing steps, quality controls, and preparation methods</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data quality metrics</p>
			</td>
			<td>
			<p>Compliance teams</p>
			</td>
			<td>
			<p>Documentation of quality assessment procedures</p>
			</td>
			<td>
			<p>Quality metrics, validation results, and completeness checks</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Population coverage analysis</p>
			</td>
			<td>
			<p>All stakeholders</p>
			</td>
			<td>
			<p>Documentation of dataset representativeness</p>
			</td>
			<td>
			<p>Coverage metrics, demographic analysis, and usage context</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Privacy protection documentation</p>
			</td>
			<td>
			<p>Compliance teams</p>
			</td>
			<td>
			<p>Documentation of privacy safeguards</p>
			</td>
			<td>
			<p>Privacy measures, data protection controls</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>System purpose and scope</p>
			</td>
			<td>
			<p>All stakeholders</p>
			</td>
			<td>
			<p>Documentation of intended use and limitations</p>
			</td>
			<td>
			<p>Use cases, constraints, and misuse prevention measures</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Risk assessment documentation</p>
			</td>
			<td>
			<p>All stakeholders</p>
			</td>
			<td>
			<p>Documentation of risk analysis and controls</p>
			</td>
			<td>
			<p>Risk assessment, mitigation measures</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>System operation documentation</p>
			</td>
			<td>
			<p>All stakeholders</p>
			</td>
			<td>
			<p>Documentation of system behavior and oversight</p>
			</td>
			<td>
			<p>Operation procedures, human oversight measures</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Technical architecture documentation</p>
			</td>
			<td>
			<p>Technical teams</p>
			</td>
			<td>
			<p>Documentation of system design and implementation</p>
			</td>
			<td>
			<p>Architecture, components, development process</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Performance documentation</p>
			</td>
			<td>
			<p>All stakeholders</p>
			</td>
			<td>
			<p>Documentation of system capabilities and limitations</p>
			</td>
			<td>
			<p>Accuracy metrics, robustness measures, test results</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Security documentation</p>
			</td>
			<td>
			<p>All stakeholders</p>
			</td>
			<td>
			<p>Documentation of security measures</p>
			</td>
			<td>
			<p>Security controls, vulnerability management</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Maintenance documentation</p>
			</td>
			<td>
			<p>Technical teams</p>
			</td>
			<td>
			<p>Documentation of system lifecycle management</p>
			</td>
			<td>
			<p>Change procedures, version control, updates</p>
			</td>
		</tr>
	</tbody>
</table>

<p>Technical documentation for high-risk AI systems is a legal requirement and must be created prior to placing the systems on the market or putting them into service. It is essential to keep this documentation updated, as it serves to demonstrate compliance to regulatory authorities. Furthermore, it plays a crucial role in conformity assessment procedures, ensuring that all requirements are met. The documentation must be detailed enough to show compliance with all applicable regulations. Ultimately, it serves the needs of both users and regulatory authorities, providing transparency and assurance in the use of high-risk AI systems.</p>
</div></section>

<section data-pdf-bookmark="Managing documentation debt" data-type="sect3"><div class="sect3" id="chapter_5_managing_documentation_debt_1748539922579309">
<h3>Managing documentation debt</h3>

<p><em>Documentation debt</em> refers<a contenteditable="false" data-primary="documentation debt" data-type="indexterm" id="id566"/> to the accumulated costs and risks associated with incomplete, outdated, inconsistent, or missing documentation across the dataset, data processing, and AI system lifecycle. Organizations can minimize this debt by automating documentation generation as part of ML pipeline execution, taking care to capture the following elements:</p>

<ul>
	<li>
	<p>Data quality metrics and validation results</p>
	</li>
	<li>
	<p>Data lineage and transformation steps</p>
	</li>
	<li>
	<p>Feature definitions and characteristics</p>
	</li>
	<li>
	<p>Pipeline metadata, including execution parameters, data statistics and profiling, validation results, transformed datasets, evaluation metrics, and training logs</p>
	</li>
	<li>
	<p>Model versions, hyperparameters, and training outcomes</p>
	</li>
</ul>

<p>Another essential aspect of operationalizing documentation is applying version control to key documentation assets such as feature definitions and schemas, model documentation (such as model cards), ML pipeline configurations and parameters, training datasets, feature stores, data transformations, and preprocessing steps. Documentation versions should be tracked alongside code versions using Git-based version control systems, artifact repositories, model registries, and feature stores with versioning capabilities.</p>

<p>By automating these processes and utilizing appropriate tools and platforms, organizations can establish a robust documentation system that is integrated into their MLOps practices. This reduces manual effort, improves consistency, and helps ensure the AI system’s ongoing compliance with the EU AI Act.</p>
</div></section>

<section data-pdf-bookmark="Existing frameworks for documenting data and AI systems" data-type="sect3"><div class="sect3" id="chapter_5_existing_frameworks_for_documenting_data_and_ai_sy_1748539922579372">
<h3>Existing frameworks for documenting data and AI systems</h3>

<p>Documentation has long been a challenge in software engineering, and several mature approaches have emerged to address it. <a data-type="xref" href="#chapter_5_table_5_1748539922529474">Table 5-5</a> outlines state-of-the-art documentation tools and methods for data and AI systems, highlighting widely adopted practices<a contenteditable="false" data-primary="documentation frameworks" data-type="indexterm" id="id567"/>.</p>

<table class="pagebreak-before less_space striped" id="chapter_5_table_5_1748539922529474">
	<caption><span class="label">Table 5-5. </span>Popular documentation approaches for data and AI systems</caption>
	<thead>
		<tr>
			<th>Approach</th>
			<th>Origin</th>
			<th>Format</th>
			<th>Purpose</th>
			<th>Coverage</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Datasheets for datasets</p>
			</td>
			<td>
			<p>Black in AI, Microsoft, and academia (2018)</p>
			</td>
			<td>
			<p>Questionnaire-based documentation</p>
			</td>
			<td>
			<p>Documents datasets, including collection procedures, intended uses, content, distribution, and maintenance</p>
			</td>
			<td>
			<p>Strong on data provenance, scope, representation, and privacy considerations.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Dataset Nutrition Label</p>
			</td>
			<td>
			<p>Harvard and MIT (2018, updated 2022)</p>
			</td>
			<td>
			<p>Visual template with standardized fields</p>
			</td>
			<td>
			<p>Provides standardized dataset information to drive higher data quality</p>
			</td>
			<td>
			<p>Good for data quality, fairness, and completeness checks.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>AI Factsheets</p>
			</td>
			<td>
			<p>IBM (2019)</p>
			</td>
			<td>
			<p>Customizable questionnaire templates for different stakeholders</p>
			</td>
			<td>
			<p>Documents AI systems comprehensively through their lifecycle</p>
			</td>
			<td>
			<p>Most comprehensive overall coverage of AI system documentation needs. Covers technical, performance, risk, and operational aspects.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Model cards</p>
			</td>
			<td>
			<p>Google (2019)</p>
			</td>
			<td>
			<p>Standardized information sheet</p>
			</td>
			<td>
			<p>Documents model specifications and intended uses</p>
			</td>
			<td>
			<p>Suitable for model performance, limitations, and use cases.</p>

			<p>Focus: Model-specific documentation and transparency.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>OECD AI Classification Framework</p>
			</td>
			<td>
			<p>OECD (2022)</p>
			</td>
			<td>
			<p>Structured questionnaire</p>
			</td>
			<td>
			<p>Standardized AI system classification and documentation</p>
			</td>
			<td>
			<p>Comprehensive AI system classification framework. Institutional backing: International standard-setting organization.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Use case cards</p>
			</td>
			<td>
			<p>European Commission, Joint Research Centre (2024)</p>
			</td>
			<td>
			<p>Information sheet</p>
			</td>
			<td>
			<p>Documents intended use cases of AI systems</p>
			</td>
			<td>
			<p>Provides a high-level overview of use cases without technical details.</p>

			<p>Focus: Risk assessment under the EU AI Act.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>AI Cards framework</p>
			</td>
			<td>
			<p>European Commission, Joint Research Centre (2024)</p>
			</td>
			<td>
			<p>Dual representation: visual human-readable and machine-readable specifications</p>
			</td>
			<td>
			<p>Provides holistic documentation of AI systems and their risks</p>
			</td>
			<td>
			<p>Covers technical specifications, context of use, risk management, and compliance. Recent development specifically for EU AI Act compliance.</p>
			</td>
		</tr>
	</tbody>
</table>

<p>When selecting a documentation framework, it is essential to choose one that aligns with your organization’s specific needs and regulatory obligations. In practice, the most effective approach often involves combining multiple complementary documentation methods to ensure thorough coverage while maintaining usability. AI Factsheets and AI Cards currently offer the most comprehensive support, but organizations may benefit from supplementing them with more specialized tools, such as datasheets for datasets and model cards. Taking a “layered documentation” approach is a useful strategy, where the AI Cards framework is used for high-level system documentation, component-specific details are captured through datasheets and model cards, and greater technical depth is provided by using AI Factsheets.</p>

<p>Implementing machine-readable specifications that integrate with existing metadata management systems is crucial. This enables automated documentation generation, streamlining the process and improving consistency and efficiency. Finally, it’s important to adapt the level of detail in documentation to suit different audiences (technical teams, compliance teams, and end users)<a contenteditable="false" data-primary="Article 11: Technical Documentation" data-startref="art-11-techdoc-1" data-type="indexterm" id="id568"/>.</p>

<p>Now let’s turn to the recordkeeping requirements outlined in Article 12, which closely relate to the technical documentation obligations in Article 11<a contenteditable="false" data-primary="Article 12: Record-Keeping" data-type="indexterm" id="art-12-rec-1"/>.</p>
</div></section>

<section data-pdf-bookmark="Article 12: Recordkeeping requirements for high-risk AI systems" data-type="sect3"><div class="sect3" id="chapter_5_recordkeeping_requirements_for_high_risk_ai_system_1748539922579450">
<h3>Article 12: Recordkeeping requirements for high-risk AI systems</h3>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="chapter_5_article_12_1748539922579520">
<h1>Article 12</h1>

<p><a href="https://oreil.ly/eFkXj">Article 12 of the EU AI Act</a> sets out obligations for recordkeeping to ensure transparency, accountability, and traceability of high-risk AI systems. It emphasizes the need to maintain detailed logs and records to uphold accountability and ensure that high-risk AI systems operate transparently and can be evaluated throughout their lifecycle.</p>

<p>Key points of Article 12 include<a contenteditable="false" data-primary="Article 12: Record-Keeping" data-secondary="key points of" data-type="indexterm" id="id569"/>:</p>

<dl>
	<dt>Comprehensive logging</dt>
	<dd>
	<p>AI systems must maintain detailed logs documenting key operations, system performance, and anomalies.</p>
	</dd>
	<dt>Traceability</dt>
	<dd>
	<p>Logs should provide sufficient detail to trace the AI system’s activities, enabling incident investigation and auditability.</p>
	</dd>
	<dt>Data retention policy</dt>
	<dd>
	<p>Records must be retained for a period appropriate to the AI system’s intended purpose and associated risks.</p>
	</dd>
	<dt>Format and accessibility</dt>
	<dd>
	<p>Records must be stored in a structured and accessible format to ensure they are usable for oversight and accountability purposes.</p>
	</dd>
	<dt>Availability for authorities</dt>
	<dd>
	<p>Developers and operators must make records available to regulatory authorities upon request to assist in compliance checks.</p>
	</dd>
	<dt>Incident reporting support</dt>
	<dd>
	<p>The records must support the investigation of failures, malfunctions, or adverse outcomes and aid with corrective actions.</p>
	</dd>
</dl>
</div></aside>

<p class="pagebreak-before">In “Navigating the EU AI Act,” Kelly et al. map Article 12<a contenteditable="false" data-primary="quality attributes (for AI systems)" data-secondary="associated with Article 12: Record-Keeping" data-type="indexterm" id="qa-art-12"/> to eight quality attributes for high-risk AI systems:</p>

<dl>
	<dt>Operability</dt>
	<dd>
	<p>The ease with which an AI system can be operated, managed, and maintained effectively, including usability, reliability, and the ability to perform functions under stated conditions.</p>
	</dd>
	<dt>Non-repudiation</dt>
	<dd>
	<p>The ability to ensure that actions or events within an AI system cannot be denied after they have occurred, preventing any party from falsely denying <span class="keep-together">responsibility</span>.</p>
	</dd>
	<dt>Traceability</dt>
	<dd>
	<p>The ability to track and document every aspect of the AI system’s data and decision-making processes, including data sources, transformations, model training, and predictions.</p>
	</dd>
	<dt>Self-descriptiveness</dt>
	<dd>
	<p>The AI system’s capacity to explain its structure, functionalities, and behaviors in understandable terms, including comprehensive documentation and the use of explainable AI techniques.</p>
	</dd>
	<dt>Accountability</dt>
	<dd>
	<p>The obligation of individuals or organizations to accept responsibility for the AI system’s actions and outcomes, involving clear role assignments and ensuring accountability for errors or negative impacts.</p>
	</dd>
	<dt>Self-monitoring</dt>
	<dd>
	<p>The AI system’s ability to autonomously observe and assess its performance and behavior, detecting anomalies, errors, or deviations from expected operations without external prompts.</p>
	</dd>
	<dt>User engagement</dt>
	<dd>
	<p>The active participation of users with the AI system, involving designing systems that encourage user interaction, feedback, and collaboration.</p>
	</dd>
	<dt>Monitorability</dt>
	<dd>
	<p>The extent to which the AI system’s operations and performance can be observed and measured in real time or retrospectively, involving tools and processes to track system behavior, performance metrics, and compliance.</p>
	</dd>
</dl>

<p><a data-type="xref" href="#chapter_5_table_6_1748539922529501">Table 5-6</a> maps these quality attributes to the CRISP-ML(Q) phases with specific AI engineering processes and tools.</p>

<table id="chapter_5_table_6_1748539922529501">
	<caption><span class="label">Table 5-6. </span>Quality attributes relevant to Article 12 and corresponding AI engineering practices and tools</caption>
	<thead>
		<tr>
			<th>CRISP-ML(Q) phase</th>
			<th>Quality attributes</th>
			<th>AI engineering practices and tools</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td class="subheading" rowspan="4">Business and data understanding</td>
			<td>
			<p>Accountability</p>
			</td>
			<td>
			<ul>
				<li>Defined roles and responsibilities</li>
				<li>Established governance frameworks</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>User engagement</p>
			</td>
			<td>
			<ul>
				<li>Stakeholder involvement</li>
				<li>Feedback mechanisms (surveys, focus groups)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Self-descriptiveness</p>
			</td>
			<td>
			<ul>
				<li>Comprehensive documentation (data catalog)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Traceability</p>
			</td>
			<td>
			<ul>
				<li>Metadata recording (data sources, collection methods)</li>
				<li>Version control for documentation</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="4">Data preparation</td>
			<td>
			<p>Traceability</p>
			</td>
			<td>
			<ul>
				<li>Data versioning (DVC, LakeFS)</li>
				<li>Data lineage tracking (Apache Atlas, OpenLineage)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Accountability</p>
			</td>
			<td>
			<ul>
				<li>Assigned data stewardship (escalation paths)</li>
				<li>Audit trails (Apache Ranger)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Operability</p>
			</td>
			<td>
			<ul>
				<li>Automated data pipelines (Airflow, Luigi, Prefect)</li>
				<li>Data validation (Great Expectations, TensorFlow Data Validation)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Self-descriptiveness</p>
			</td>
			<td>
			<ul>
				<li>Data documentation (dictionaries, schemas)</li>
				<li>Descriptive metadata</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="5">Modeling</td>
			<td>
			<p>Traceability</p>
			</td>
			<td>
			<ul>
				<li>Model versioning (MLflow, DVC, Git LFS)</li>
				<li>Experiment tracking (Weights &amp; Biases, Neptune.ai, Comet.ml)</li>
				<li>Model documentation (diagrams, explanations)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Self-descriptiveness</p>
			</td>
			<td>
			<ul>
				<li>Explainable AI techniques (SHAP, LIME, Integrated Gradients)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Accountability</p>
			</td>
			<td>
			<ul>
				<li>Assigned modeling responsibilities (peer review)</li>
				<li>Governance policies (performance, ethics)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Operability</p>
			</td>
			<td>
			<ul>
				<li>Automated training pipelines (Kubeflow pipelines, TFX)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Non-repudiation</p>
			</td>
			<td>
			<ul>
				<li>Immutable logs (append-only databases)</li>
				<li>Secure model artifacts (digital signatures, checksums)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="5">Evaluation</td>
			<td>
			<p>Traceability</p>
			</td>
			<td>
			<ul>
				<li>Evaluation logging (metrics, datasets, conditions)</li>
				<li>Version control (evaluation code, documentation)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Accountability</p>
			</td>
			<td>
			<ul>
				<li>Assigned evaluation roles (sign-off procedures)</li>
				<li>Approval processes (performance, ethics)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Non-repudiation</p>
			</td>
			<td>
			<ul>
				<li>Immutable evaluation records (timestamped)</li>
				<li>Digital signatures (evaluation reports)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Self-descriptiveness</p>
			</td>
			<td>
			<ul>
				<li>Evaluation documentation (metrics, visualizations, explainable AI)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Operability</p>
			</td>
			<td>
			<ul>
				<li>Automated evaluation pipelines (CI/CD tools)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="5">Deployment</td>
			<td>
			<p>Operability</p>
			</td>
			<td>
			<ul>
				<li>Continuous deployment (Jenkins, GitLab CI/CD, CircleCI)</li>
				<li>Containerization and orchestration (Docker, Kubernetes)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Non-repudiation</p>
			</td>
			<td>
			<ul>
				<li>Deployment logs (immutable, access-controlled)</li>
				<li>Artifact signatures (cryptographic methods)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Traceability</p>
			</td>
			<td>
			<ul>
				<li>Deployment documentation (architecture diagrams, artifact repositories)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Accountability</p>
			</td>
			<td>
			<ul>
				<li>Deployment approvals (change management systems)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Monitorability</p>
			</td>
			<td>
			<ul>
				<li>Monitoring setup (Prometheus, Grafana, Datadog)</li>
				<li>Logging practices (structured logs)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="6">Monitoring and maintenance</td>
			<td>
			<p>Self-monitoring</p>
			</td>
			<td>
			<ul>
				<li>Automated alerts (Alertmanager, PagerDuty)</li>
				<li>Health checks (APIs, scripts, Kubernetes liveness/readiness probes)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Monitorability</p>
			</td>
			<td>
			<ul>
				<li>Centralized monitoring systems (observability practices)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Operability</p>
			</td>
			<td>
			<ul>
				<li>Log aggregation (Elastic Stack, Splunk)</li>
				<li>Scalable infrastructure (cloud autoscaling)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Accountability</p>
			</td>
			<td>
			<ul>
				<li>Assigned maintenance roles (ticketing systems)</li>
				<li>Incident response plans</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Traceability</p>
			</td>
			<td>
			<ul>
				<li>Maintenance logs (change management logs)</li>
				<li>Performance tracking</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>User engagement</p>
			</td>
			<td>
			<ul>
				<li>Feedback collection (support portals, in-app forms)</li>
				<li>User support (documentation, FAQs, help centers)</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="Interdependence of Articles 11 and 12" data-type="sect3"><div class="sect3" id="chapter_5_interdependence_of_articles_11_and_12_1748539922579579">
<h3>Interdependence of Articles 11 and 12</h3>

<p>Articles <a contenteditable="false" data-primary="Article 12: Record-Keeping" data-startref="art-12-rec-1" data-type="indexterm" id="id570"/>11 and 12 of the EU AI<a contenteditable="false" data-primary="quality attributes (for AI systems)" data-secondary="associated with Article 12: Record-Keeping" data-startref="qa-art-12" data-type="indexterm" id="id571"/> Act are closely connected and interdependent. As you’ve seen, Article 11 outlines comprehensive documentation requirements covering overall system design, technical specifications, development processes, and risk and quality management measures. Article 12 focuses specifically on the automatic recording of events and activities, capturing operational logs, behavioral data, and runtime information. In the context of compliance, technical documentation provides the framework for what needs to be recorded, while recordkeeping provides the operational evidence that the AI system performs in accordance with that documentation. <a data-type="xref" href="#chapter_5_table_7_1748539922529522">Table 5-7</a> shows the correspondence between technical documentation and runtime recordkeeping elements.</p>

<table class="striped" id="chapter_5_table_7_1748539922529522">
	<caption><span class="label">Table 5-7. </span>Correspondence between technical documentation and recordkeeping obligations</caption>
	<thead>
		<tr>
			<th>Article 11: Technical Documentation</th>
			<th>Article 12: Record-Keeping</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>System specifications</p>
			</td>
			<td>
			<p>Runtime behavior logs</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Expected behaviors</p>
			</td>
			<td>
			<p>Actual behaviors</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Risk assessments</p>
			</td>
			<td>
			<p>Incident logs</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Design decisions</p>
			</td>
			<td>
			<p>Operational metrics</p>
			</td>
		</tr>
	</tbody>
</table>

<p>Considering the technical requirements of Articles 11 and 12, one aspect that becomes apparent is that they share a common technical component: the metadata store.</p>
</div></section>

<section data-pdf-bookmark="Data and AI system metadata" data-type="sect3"><div class="sect3" id="chapter_5_data_and_ai_system_metadata_1748539922579653">
<h3>Data and AI system metadata</h3>

<p>Capturing and managing metadata is fundamental to meeting the documentation and recordkeeping obligations under Articles 11 and 12 of the EU AI Act. Metadata provides the foundation for systematic, auditable, and maintainable documentation by enabling traceability, demonstrating compliance, supporting collaboration, and ensuring documentation quality over time.</p>

<p>A robust metadata management system is key to maintaining the level of oversight, quality, and governance required for compliance, particularly as AI systems evolve and scale over time. <a data-type="xref" href="#chapter_5_table_8_1748539922529568">Table 5-8</a> outlines the types of data and AI system metadata required for each of the documentation types listed in <a data-type="xref" href="#chapter_5_table_4_1748539922529451">Table 5-4</a>.</p>

<table id="chapter_5_table_8_1748539922529568">
	<caption><span class="label">Table 5-8. </span>Essential metadata for meeting compliance demands for Articles 9–15 of the EU AI Act</caption>
	<thead>
		<tr>
			<th>Document type</th>
			<th colspan="3">Data and AI system metadata</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Data source documentation</p>
			</td>
			<td>
			<p>Source identification:</p>

			<ul>
				<li>Source name/identifier</li>
				<li>Source type (database, API, sensor data, etc.)</li>
				<li>Source owner/maintainer</li>
				<li>Source access method</li>
				<li>Source version/timestamp</li>
				<li>Data licensing information</li>
				<li>Terms of use</li>
			</ul>
</td>
<td>
			<p>Data collection methods:</p>

			<ul>
				<li>Collection protocol identifier</li>
				<li>Collection time frame (start/end dates)</li>
				<li>Collection frequency</li>
				<li>Collection tools/software used</li>
				<li>Collection validation procedures</li>
				<li>Sampling methodology</li>
				<li>Sample size calculations</li>
			</ul>
</td>
<td>
			<p>Aggregation approaches:</p>

			<ul>
				<li>Aggregation rules</li>
				<li>Merge procedures</li>
				<li>Deduplication methods</li>
				<li>Data harmonization steps</li>
				<li>Source reconciliation procedures</li>
				<li>Data lineage tracking</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Dataset characteristics</p>
			</td>
			<td>
			<p>Content description:</p>

			<ul>
				<li>Data types (numerical, categorical, text, etc.)</li>
				<li>Feature descriptions</li>
				<li>Feature relationships</li>
				<li>Data dictionary</li>
				<li>Schema definition</li>
				<li>Data formats</li>
				<li>Encoding standards</li>
			</ul>
</td>
			<td>
			<p>Coverage information:</p>

			<ul>
				<li>Temporal coverage</li>
				<li>Geographical coverage</li>
				<li>Demographic coverage</li>
				<li>Domain coverage</li>
				<li>Missing data patterns</li>
				<li>Known biases</li>
				<li>Edge cases</li>
			</ul>
</td>
			<td>
			<p>Known limitations:</p>

			<ul>
				<li>Gaps</li>
				<li>Quality issues</li>
				<li>Coverage limitations</li>
				<li>Usage restrictions</li>
				<li>Technical constraints</li>
				<li>Statistical limitations</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data processing documentation</p>
			</td>
			<td>
			<p>Preprocessing steps:</p>

			<ul>
				<li>Cleaning procedures</li>
				<li>Normalization methods</li>
				<li>Feature engineering steps</li>
				<li>Data transformation rules</li>
				<li>Outlier handling</li>
				<li>Missing value treatment</li>
			</ul>
</td>
			<td>
			<p>Quality controls:</p>

			<ul>
				<li>Validation rules</li>
				<li>Data consistency checks</li>
				<li>Integrity constraints</li>
				<li>Quality metrics</li>
				<li>Error handling procedures</li>
				<li>Exception management</li>
			</ul>
			</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Data quality metrics</p>
			</td>
			<td>
			<p>Statistical measures:</p>

			<ul>
				<li>Descriptive statistics</li>
				<li>Distribution analysis</li>
				<li>Correlation metrics</li>
				<li>Data completeness rates</li>
				<li>Error rates</li>
				<li>Confidence intervals</li>
			</ul>
</td>
			<td>
			<p>Validation results:</p>

			<ul>
				<li>Cross-validation metrics</li>
				<li>Test results</li>
				<li>Quality scores</li>
				<li>Performance indicators</li>
				<li>Benchmark comparisons</li>
			</ul>
			</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Population coverage analysis</p>
			</td>
			<td>
			<p>Demographic metrics:</p>

			<ul>
				<li>Population distributions</li>
				<li>Representation ratios</li>
				<li>Coverage gaps</li>
				<li>Bias metrics</li>
				<li>Fairness indicators</li>
			</ul>
</td>
			<td>
			<p>Usage context:</p>

			<ul>
				<li>Target population description</li>
				<li>Application domain</li>
				<li>Use case scenarios</li>
				<li>Environmental conditions</li>
				<li>Operational constraints</li>
			</ul>
			</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Privacy protection documentation</p>
			</td>
			<td>
			<p>Privacy measures:</p>

			<ul>
				<li>Anonymization methods</li>
				<li>Pseudonymization techniques</li>
				<li>Data masking rules</li>
				<li>Access controls</li>
				<li>Consent management</li>
				<li>Deletion requests</li>
				<li>Data retention policies</li>
			</ul>
</td>
			<td>
			<p>Data protection controls:</p>

			<ul>
				<li>Security protocols</li>
				<li>Encryption methods</li>
				<li>Data segregation rules</li>
				<li>Privacy impact assessments</li>
				<li>Compliance checks</li>
			</ul>
			</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>System purpose and scope</p>
			</td>
			<td>
			<p>Definition:</p>

			<ul>
				<li>Primary objectives</li>
				<li>Target users</li>
				<li>Use case descriptions</li>
				<li>Success criteria</li>
				<li>Performance targets</li>
				<li>Business context</li>
			</ul>
</td>
			<td>
			<p>Constraints:</p>

			<ul>
				<li>Operational limitations</li>
				<li>Technical constraints</li>
				<li>Resource requirements</li>
				<li>Environmental requirements</li>
				<li>Regulatory restrictions</li>
			</ul>
</td>
			<td>
			<p>Misuse prevention:</p>

			<ul>
				<li>Usage restrictions</li>
				<li>Warning systems</li>
				<li>Prevention mechanisms</li>
				<li>Detection methods</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Risk assessment documentation</p>
			</td>
			<td>
			<p>Risk analysis:</p>

			<ul>
				<li>Risk categories</li>
				<li>Impact assessments</li>
				<li>Probability metrics</li>
				<li>Severity ratings</li>
				<li>Risk matrices</li>
				<li>Risk evolution tracking</li>
			</ul>
</td>
			<td>
			<p>Control measures:</p>

			<ul>
				<li>Mitigation strategies</li>
				<li>Control mechanisms</li>
				<li>Monitoring procedures</li>
				<li>Response plans</li>
				<li>Recovery procedures</li>
				<li>Review cycles</li>
			</ul>
			</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>System operation documentation</p>
			</td>
			<td>
			<p>Operational procedures:</p>

			<ul>
				<li>Startup procedures</li>
				<li>Shutdown procedures</li>
				<li>Maintenance routines</li>
				<li>Error handling</li>
				<li>Recovery procedures</li>
				<li>Backup protocols</li>
			</ul>
</td>
			<td>
			<p>Human oversight:</p>

			<ul>
				<li>Supervision roles</li>
				<li>Decision authorities</li>
				<li>Intervention protocols</li>
				<li>Override mechanisms</li>
				<li>Audit procedures</li>
				<li>Training requirements</li>
			</ul>
			</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Technical architecture documentation</p>
			</td>
			<td>
			<p>Architecture specifications:</p>

			<ul>
				<li>Component diagrams</li>
				<li>Interface definitions</li>
				<li>Data flows</li>
				<li>Processing pipelines</li>
				<li>Infrastructure requirements</li>
			</ul>
</td>
			<td>
			<p>Dependencies:</p>

			<ul>
				<li>Technology stack</li>
				<li>Libraries and frameworks</li>
				<li>API specifications</li>
				<li>Configuration settings</li>
				<li>Deployment requirements</li>
				<li>Resource specifications</li>
			</ul>
			</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Performance documentation</p>
			</td>
			<td>
			<p>Accuracy metrics:</p>

			<ul>
				<li>Performance measures</li>
				<li>Error rates</li>
				<li>Confidence scores</li>
			</ul>
</td>
			<td>
			<p>Robustness measures:</p>

			<ul>
				<li>Stability metrics</li>
				<li>Reliability scores</li>
				<li>Resilience tests</li>
				<li>Edge case handling</li>
				<li>Failure modes</li>
				<li>Recovery capabilities</li>
			</ul>
			</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Security documentation</p>
			</td>
			<td>
			<p>Security controls:</p>

			<ul>
				<li>Access controls</li>
				<li>Authentication methods</li>
				<li>Authorization rules</li>
				<li>Data protection</li>
				<li>Network security</li>
				<li>Monitoring systems</li>
			</ul>
</td>
			<td>
			<p>Vulnerability management:</p>

			<ul>
				<li>Security assessments</li>
				<li>Threat models</li>
				<li>Patch management</li>
				<li>Incident response</li>
				<li>Recovery procedures</li>
				<li>Audit logs</li>
			</ul>
			</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Maintenance documentation</p>
			</td>
			<td>
			<p>Change management:</p>

			<ul>
				<li>Version control</li>
				<li>Release procedures</li>
				<li>Update protocols</li>
				<li>Testing requirements</li>
				<li>Rollback procedures</li>
				<li>Documentation updates</li>
			</ul>
</td>
			<td>
			<p>Lifecycle management:</p>

			<ul>
				<li>Maintenance schedules</li>
				<li>Support procedures</li>
				<li>Deprecation plans</li>
				<li>Upgrade paths</li>
				<li>End-of-life procedures</li>
				<li>Archive requirements</li>
			</ul>
			</td>
			<td> </td>
		</tr>
	</tbody>
</table>

<p>Metadata systems are often overlooked or deprioritized, but they are key components of any data and ML platform for operationalizing documentation requirements and maintaining compliance with the EU AI Act. A well-designed metadata system provides the foundation for systematic documentation management, supports automation and quality control, and enables organizations to demonstrate compliance to regulatory authorities. To ensure effectiveness, organizations should treat metadata systems as integral to their data and AI governance infrastructure and periodically conduct internal test audits to verify that key information is easily accessible when needed.</p>
</div></section>

<section data-pdf-bookmark="Checklist for compliance" data-type="sect3"><div class="sect3" id="chapter_5_checklist_for_compliance_1748539922579725">
<h3>Checklist for compliance</h3>

<p>The following checklist defines a structured process you can follow to implement the technical documentation and recordkeeping requirements of Articles 11 and 12<a contenteditable="false" data-primary="checklists for compliance (quality attributes)" data-secondary="Article 12: Record-Keeping" data-startref="checkl-art-11-12" data-type="indexterm" id="id572"/> of the EU AI Act. It uses a compressed version of the CRISP-ML(Q) lifecycle to streamline implementation across the business and data understanding, model development, and operational phases:</p>

<dl class="pagebreak-before">
	<dt>Business and data understanding</dt>
	<dd>
	<ul>
		<li>
		<p>Document intended purpose and use cases (business objectives and constraints, expected performance metrics).</p>
		</li>
		<li>
		<p>Establish recordkeeping infrastructure (data collection scope and methods, metadata tracking systems, logging requirements).</p>
		</li>
		<li>
		<p>Document data sources and specifications (data provenance, quality criteria, privacy and security controls, governance procedures).</p>
		</li>
	</ul>
	</dd>
	<dt>Model training and operationalization</dt>
	<dd>
	<ul>
		<li>
		<p>Document model development environment (hardware specifications, software dependencies, tools and versioning).</p>
		</li>
		<li>
		<p>Document training methodology (model architecture, hyperparameters, algorithms, feature engineering steps).</p>
		</li>
		<li>
		<p>Implement pipeline logging (infrastructure usage, execution records, resource utilization).</p>
		</li>
		<li>
		<p>Implement training logs (metadata on training runs, performance metrics, data versioning, feature extraction).</p>
		</li>
	</ul>
	</dd>
	<dt>Model deployment and serving</dt>
	<dd>
	<ul>
		<li>
		<p>Document deployment procedures (testing protocols, validation methods, release criteria).</p>
		</li>
		<li>
		<p>Document deployment architecture (infrastructure design, scaling, security measures).</p>
		</li>
		<li>
		<p>Implement deployment logging (event logs, configuration changes, versioning, access control).</p>
		</li>
		<li>
		<p>Document operational procedures (maintenance protocols, update routines, emergency response plans).</p>
		</li>
		<li>
		<p>Document serving infrastructure (API specifications, performance benchmarks, resource allocations, scaling policies).</p>
		</li>
		<li>
		<p>Implement prediction logging (inference logs, latency, errors, user feedback).</p>
		</li>
		<li>
		<p>Document monitoring strategy (metrics tracked, alerting thresholds, response procedures, maintenance schedules).</p>
		</li>
		<li>
		<p>Document monitoring system (tools used, automated alerts, performance metrics, health checks).</p>
		</li>
	</ul>
	</dd>
</dl>

<p>Teams should adjust this checklist based on their specific high-risk AI system and organizational context, while ensuring all regulatory requirements are addressed.</p>
</div></section>

<section data-pdf-bookmark="Further reading" data-type="sect3"><div class="sect3" id="chapter_5_further_reading_1748539922579780">
<h3>Further reading</h3>

<p>For more in-depth information about the importance of metadata and documentation in software development, see:</p>

<ul>
	<li>
	<p><a href="https://oreil.ly/8IY2w"><em>Crafting Docs for Success: An End-to-End Approach to Developer Documentation</em></a> by Diana Lakatos (Apress)</p>
	</li>
	<li>
	<p><a href="https://oreil.ly/W5xiK"><em>Fundamentals of Metadata Management</em></a> by Ole Olesen-Bagneux (O’Reilly)</p>
	</li>
	<li>
	<p><a href="https://oreil.ly/nlNfU">“ML Lineage for Trustworthy Machine Learning Systems”</a> by Mikko Raatikainen et al<a contenteditable="false" data-primary="Article 13: Transparency and Provision of Information to Deployers" data-seealso="Article 14: Human Oversight" data-type="indexterm" id="art-13"/>.</p>
	</li>
</ul>
</div></section>
</div></section>

<section data-pdf-bookmark="Article 13: Transparency and Provision of Information to Deployers and Article 14: Human Oversight" data-type="sect2"><div class="sect2" id="chapter_5_article_13_transparency_and_provision_of_informat_1748539922579871">
<h2>Article 13: Transparency and Provision of Information to Deployers and Article 14: Human Oversight</h2>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="chapter_5_article_13_1748539922579975">
<h1>Article 13</h1>

<p><a href="https://oreil.ly/E-Jt0">Article 13 of the EU AI Act</a> mandates that deployers of high-risk AI systems receive adequate information to use these systems responsibly. It emphasizes transparency to support informed decision making and regulatory compliance and to ensure that deployers have clarity around their functionality, risks, and limitations. The goal is to equip operators with the information they need to use the systems safely and <span class="keep-together">effectively</span>.</p>

<p>Key points of Article 13 include<a contenteditable="false" data-primary="Article 13: Transparency and Provision of Information to Deployers" data-secondary="key points of" data-type="indexterm" id="id573"/>:</p>

<dl>
	<dt>Clear instructions</dt>
	<dd>
	<p>Developers must provide detailed instructions on the proper setup, installation, and use of the AI system.</p>
	</dd>
	<dt>Operational requirements</dt>
	<dd>
	<p>Documentation should explain specific conditions, limitations, and constraints under which the system is intended to operate.</p>
	</dd>
	<dt>Risk awareness</dt>
	<dd>
	<p>Deployers must be informed about potential risks to health, safety, or fundamental rights and applicable mitigation strategies.</p>
	</dd>
	<dt>Human oversight guidelines</dt>
	<dd>
	<p>Clear instructions must be provided on when and how human operators should intervene during the system’s operation.</p>
	</dd>
	<dt>Performance metrics</dt>
	<dd>
	<p>Information should be provided on expected performance across a range of scenarios and usage conditions.</p>
	</dd>
	<dt>Maintenance and updates</dt>
	<dd>
	<p>Deployers must receive instructions on system maintenance and software updates and versioning, to ensure safe and reliable operation over time.</p>
	</dd>
	<dt>Contact and support information</dt>
	<dd>
	<p>Developers must provide appropriate contact details for technical support and incident reporting.</p>
	</dd>
</dl>
</div></aside>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id574">
<h1>Article 14</h1>

<p><a href="https://oreil.ly/WBf-Q">Article 14 of the EU AI Act</a> sets<a contenteditable="false" data-primary="Article 14: Human Oversight" data-seealso="Article 13: Transparency and Provision of Information to Deployers" data-type="indexterm" id="art-14"/> out requirements for human oversight to help ensure that high-risk AI systems operate safely and ethically. It underscores the importance of human involvement in monitoring system behavior and intervening when necessary to prevent adverse outcomes, thereby promoting accountability, safety, and the ability to respond effectively to unforeseen situations.</p>

<p>Key points of Article 14 include:</p>

<dl>
	<dt>Oversight by design</dt>
	<dd>
	<p>High-risk AI systems must include features that enable effective human oversight during their operation.</p>
	</dd>
	<dt>Prevention of harm</dt>
	<dd>
	<p>Human oversight must be capable of preventing or minimizing risks to health, safety, and fundamental rights.</p>
	</dd>
	<dt>Intervention mechanisms</dt>
	<dd>
	<p>Operators must be able to interrupt or override the system’s actions if it behaves unexpectedly or unsafely.</p>
	</dd>
	<dt>Training for operators</dt>
	<dd>
	<p>Individuals responsible for oversight must be given clear instructions on how and when to intervene.</p>
	</dd>
	<dt>Consideration of operational context</dt>
	<dd>
	<p>Oversight strategies should reflect the specific use case and operating conditions of the system.</p>
	</dd>
	<dt>Managing increasing autonomy</dt>
	<dd>
	<p>In systems that evolve toward greater autonomy, human intervention must remain possible at all times.</p>
	</dd>
	<dt>Continuous monitoring</dt>
	<dd>
	<p>Deployers should regularly assess the system’s performance to determine when human involvement is necessary for safe and reliable operation.</p>
	</dd>
</dl>
</div></aside>

<p>Articles 13 and 14 of the EU AI Act are closely connected, as they both deal with transparency and human oversight requirements for high-risk AI systems. However, they focus on different aspects, as depicted in <a data-type="xref" href="#chapter_5_table_9_1748539922529591">Table 5-9</a>.</p>

<table class="striped" id="chapter_5_table_9_1748539922529591">
	<caption><span class="label">Table 5-9. </span>Similarities and differences between Articles 13 and 14</caption>
	<thead>
		<tr>
			<th>Feature</th>
			<th>Article 13</th>
			<th>Article 14</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Primary focus</p>
			</td>
			<td>
			<p>Transparency and user information</p>
			</td>
			<td>
			<p>Human oversight and control</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Scope</p>
			</td>
			<td>
			<p>System capabilities and limitations</p>
			</td>
			<td>
			<p>Technical design for oversight</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Implementation</p>
			</td>
			<td>
			<p>Documentation and communication</p>
			</td>
			<td>
			<p>Technical measures and procedures</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Key requirements</p>
			</td>
			<td>
			<p>Transparency about AI system outputs and interpretations</p>

			<p>Information about accuracy and performance limits</p>

			<p>Documentation of AI system capabilities</p>
			</td>
			<td>
			<p>Human oversight measures to monitor and control the AI system</p>

			<p>Design for human interpretability and understanding</p>

			<p>Documentation of oversight procedures and assessments</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Relationship to risk management</p>
			</td>
			<td>
			<p>Focus on transparency about potential risks</p>
			</td>
			<td>
			<p>Focus on human oversight as a risk mitigation measure</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Overall goal</p>
			</td>
			<td>
			<p>Ensure users understand the system’s capabilities and limitations</p>
			</td>
			<td>
			<p>Enable effective human monitoring and control of high-risk AI systems</p>
			</td>
		</tr>
	</tbody>
</table>

<section data-pdf-bookmark="Article 13 quality attributes" data-type="sect3"><div class="sect3" id="chapter_5_article_13_quality_attributes_1748539922580041">
<h3>Article 13 quality attributes</h3>

<p>Kelly et al. associate Article 13<a contenteditable="false" data-primary="quality attributes (for AI systems)" data-secondary="associated with Article 13: Transparency and Provision of Information to Deployers" data-type="indexterm" id="qa-art-13"/> with to the following quality attributes, along with self-descriptiveness:</p>

<dl>
	<dt>User engagement</dt>
	<dd>
	<p>The extent to which users are actively involved with and informed about the AI system, such as being able to opt in/out, challenge outputs, or provide feedback.</p>
	</dd>
	<dt>User transparency</dt>
	<dd>
	<p>The degree to which the functionalities, capabilities, and limitations of the AI system are understandably communicated to users.</p>
	</dd>
	<dt>Interpretability</dt>
	<dd>
	<p>The extent to which the reasoning and decision-making processes of the AI system can be understood and explained. An interpretable system allows users and stakeholders to comprehend how inputs are transformed into outputs, supporting trust and accountability.</p>
	</dd>
	<dt>Documentability</dt>
	<dd>
	<p>The quality, completeness, and accessibility of documentation describing the AI system design, development, and operation. This includes information on data sources, model architectures, training processes, evaluation metrics, and system updates, supporting transparency and reproducibility.</p>
	</dd>
	<dt>Appropriateness recognizability</dt>
	<dd>
	<p>The degree to which users can discern that they are interacting with an AI system rather than a human and assess whether it is appropriate for a particular context or task. This ensures that users can identify the system’s limitations and intended use cases, preventing misuse or overreliance.</p>
	</dd>
</dl>

<p><a data-type="xref" href="#chapter_5_table_10_1748539922529615">Table 5-10</a> maps these attributes to the CRISP-ML(Q) phases and outlines core AI engineering practices associated with each phase. This structure helps ensure systematic implementation of the requirements of Article 13 throughout the entire AI system lifecycle.</p>

<table id="chapter_5_table_10_1748539922529615">
	<caption><span class="label">Table 5-10. </span>Quality attributes relevant to Article 13 and corresponding AI engineering practices and tools</caption>
	<thead>
		<tr>
			<th>CRISP-ML(Q) phase</th>
			<th>Quality attributes</th>
			<th>AI engineering practices and tools</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td class="subheading" rowspan="2">Business and data understanding</td>
			<td>
			<p>User engagement</p>
			</td>
			<td>
			<ul>
				<li>Involve users early through interviews, workshops, or cocreation exercises.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Appropriateness recognizability</p>
			</td>
			<td>
			<ul>
				<li>Conduct feasibility studies and risk assessments.</li>
				<li>Design mock interfaces to gather user feedback.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Data preparation</td>
			<td>
			<p>Documentability</p>
			</td>
			<td>
			<ul>
				<li>Maintain data lineage with tools like DataHub or Great Expectations.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>User transparency</p>
			</td>
			<td>
			<ul>
				<li>Annotate datasets clearly and provide metadata descriptions.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Modeling</td>
			<td>
			<p>Interpretability</p>
			</td>
			<td>
			<ul>
				<li>Use interpretable models (like decision trees).</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Documentability</p>
			</td>
			<td>
			<ul>
				<li>Implement versioning for models and scripts using Git and DVC.</li>
				<li>Use techniques like feature attribution (SHAP, LIME).</li>
				<li>Test explanations with users.</li>
				<li>Automate documentation updates using pipelines.</li>
				<li>Create a centralized repository for project documentation.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Evaluation</td>
			<td>
			<p>Appropriateness recognizability</p>
			</td>
			<td>
			<ul>
				<li>Ensure evaluation metrics are domain-appropriate.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>User engagement</p>
			</td>
			<td>
			<ul>
				<li>Test models with representative user groups.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Deployment</td>
			<td>
			<p>User transparency</p>
			</td>
			<td>
			<ul>
				<li>Deploy user-facing documentation alongside APIs or systems.</li>
				<li>Use plain language in user-facing documentation.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Documentability</p>
			</td>
			<td>
			<ul>
				<li>Utilize CI/CD pipelines (e.g., GitHub Actions, MLflow).</li>
				<li>Publish ethical guidelines and disclaimers.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Monitoring and maintenance</td>
			<td>
			<p>User transparency</p>
			</td>
			<td>
			<ul>
				<li>Provide users access to monitoring dashboards.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Interpretability</p>
			</td>
			<td>
			<ul>
				<li>Monitor explainability metrics<a contenteditable="false" data-primary="quality attributes (for AI systems)" data-secondary="associated with Article 13: Transparency and Provision of Information to Deployers" data-startref="qa-art-13" data-type="indexterm" id="id575"/>.</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="Article 14 quality attributes" data-type="sect3"><div class="sect3" id="chapter_5_article_14_quality_attributes_1748539922580113">
<h3 class="less_space">Article 14 quality attributes</h3>

<p>In addition to<a contenteditable="false" data-primary="quality attributes (for AI systems)" data-secondary="associated with Article 14: Human Oversight" data-type="indexterm" id="qa-art-14"/> documentability, value alignment, accountability, and interpretability, Article 14 is associated with the following quality attributes for safety-critical AI <span class="keep-together">systems</span>:</p>

<dl>
	<dt>Learnability</dt>
	<dd>
	<p>The degree to which human operators can understand and learn an AI system’s behavior, outputs, and decision-making processes. Learnability depends on the clarity and comprehensibility of system documentation, interfaces, and training materials; it reflects how quickly human operators can achieve competency in overseeing the system.</p>
	</dd>
	<dt>Fairness</dt>
	<dd>
	<p>The extent to which the system produces outcomes that are unbiased and do not result in unjustified discrimination against any individual or group. Fairness ensures equitable treatment and decision making across different demographic groups.</p>
	</dd>
	<dt>Explainability</dt>
	<dd>
	<p>The extent to which the internal logic and behavior of the system can be understood and interpreted by humans. Explainability enables stakeholders to comprehend how inputs are transformed into outputs.</p>
	</dd>
	<dt>Intervenability</dt>
	<dd>
	<p>The capacity for human operators to influence or override the system’s behavior and decisions, or halt the system if necessary.</p>
	</dd>
	<dt>Monitorability</dt>
	<dd>
	<p>The degree to which the system’s operations and performance can be continuously observed, measured, and analyzed. Monitorability facilitates early detection of issues such as drift, anomalies, and system failures.</p>
	</dd>
	<dt>User error protection</dt>
	<dd>
	<p>The inclusion of design features that help prevent or mitigate errors caused by user interactions. This includes safeguards against incorrect inputs, misinterpretations, and unintended uses of the system.</p>
	</dd>
</dl>

<p><a data-type="xref" href="#chapter_5_table_11_1748539922529637">Table 5-11</a> maps these quality attributes to the relevant AI engineering practices to fulfill the technical requirements of Article 14.</p>

<table id="chapter_5_table_11_1748539922529637">
	<caption><span class="label">Table 5-11. </span>Quality attributes relevant to Article 14 and corresponding AI engineering practices and tools</caption>
	<thead>
		<tr>
			<th>CRISP-ML(Q) phase</th>
			<th>Quality attributes</th>
			<th>AI engineering practices and tools</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td class="subheading" rowspan="2">Business and data understanding</td>
			<td>
			<p>Fairness</p>
			</td>
			<td>
			<ul>
				<li>Assess potential biases in datasets using fairness auditing tools like Aequitas.</li>
				<li>Define fairness metrics (e.g., demographic parity) as project requirements.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Learnability</p>
			</td>
			<td>
			<ul>
				<li>Conduct user research to understand user learning capabilities and needs.</li>
				<li>Define metrics like System Usability Scale (SUS) for usability and ease of learning.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Data preparation</td>
			<td>
			<p>Fairness</p>
			</td>
			<td>
			<ul>
				<li>Ensure representative and balanced datasets by applying techniques such as oversampling or undersampling. Use bias mitigation tools like Fairlearn or IBM’s AI Fairness 360 to evaluate and reduce bias.</li>
				<li>Create a data dictionary to promote transparency about feature definitions.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Monitorability</p>
			</td>
			<td>
			<ul>
				<li>Use metadata tracking tools like DVC to document data transformations, ensuring traceability.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Modeling</td>
			<td>
			<p>Explainability</p>
			</td>
			<td>
			<ul>
				<li>Choose models that balance accuracy with interpretability, such as decision trees or linear models.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Intervenability</p>
			</td>
			<td>
			<ul>
				<li>Introduce model checkpoints during training for human-in-the-loop interventions.</li>
				<li>Integrate tooling like MLflow for easy rollback.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="3">Evaluation</td>
			<td>
			<p>Explainability</p>
			</td>
			<td>
			<ul>
				<li>Utilize tools like SHAP or LIME to evaluate model predictions and make them understandable to nontechnical stakeholders.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Fairness</p>
			</td>
			<td>
			<ul>
				<li>Evaluate the model on the fairness metrics defined earlier.</li>
				<li>Implement subgroup performance tests.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>User error protection</p>
			</td>
			<td>
			<ul>
				<li>Simulate potential user interaction scenarios and evaluate how errors can be mitigated.</li>
				<li>Create fallback strategies.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Deployment</td>
			<td>
			<p>Monitorability</p>
			</td>
			<td>
			<ul>
				<li>Simulate monitoring scenarios such as data drift or model decay.</li>
				<li>Integrate model monitoring tools like Prometheus, Grafana, or AWS SageMaker Model Monitor to track model drift, bias drift, and other performance indicators.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Intervenability</p>
			</td>
			<td>
			<ul>
				<li>Set up mechanisms for human intervention in case anomalies are detected, such as alert systems and override mechanisms.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="2">Monitoring and maintenance</td>
			<td>
			<p>Learnability</p>
			</td>
			<td>
			<ul>
				<li>Provide regular training and clear documentation for users.</li>
				<li>Establish a feedback loop to gather user input continuously.</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Monitorability</p>
			</td>
			<td>
			<ul>
				<li>Conduct regular audits to validate model behavior.</li>
				<li>Implement automated retraining pipelines to address performance drift and ensure compliance<a contenteditable="false" data-primary="quality attributes (for AI systems)" data-secondary="associated with Article 14: Human Oversight" data-startref="qa-art-14" data-type="indexterm" id="id576"/>.</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="Checklist for compliance" data-type="sect3"><div class="sect3" id="chapter_5_checklist_for_compliance_1748539922580183">
<h3>Checklist for compliance</h3>

<p>Since <a contenteditable="false" data-primary="checklists for compliance (quality attributes)" data-secondary="Articles 13 and 14" data-type="indexterm" id="checkl-art-13-14"/>Articles 13 and 14 of the EU AI Act are closely related, as shown in <a data-type="xref" href="#chapter_5_table_9_1748539922529591">Table 5-9</a>, I’ve created a comprehensive joint checklist for integrating AI engineering best practices that align with the quality attributes associated with both of them. It defines a structured process you can follow to implement the requirements of Articles 13 and 14 throughout all phases of the development lifecycle:</p>

<dl>
	<dt>Business and data understanding</dt>
	<dd>
	<ul>
		<li>
		<p>User requirement gathering: Engage end users in defining AI system requirements through interviews, surveys, or workshops.</p>
		</li>
		<li>
		<p>Expectation alignment: Ensure user expectations are documented and understood, along with all stakeholders’ fairness expectations.</p>
		</li>
		<li>
		<p>Transparency requirements: Define user-specific transparency needs.</p>
		</li>
		<li>
		<p>User personas and goals: Create representative user personas to guide the design of user-facing transparency and oversight features.</p>
		</li>
		<li>
		<p>Explainability objectives: Establish clear interpretability goals aligned with user needs.</p>
		</li>
		<li>
		<p>Documentation standards: Set organization-wide standards for capturing system requirements, data sources, design decisions, and rationales.</p>
		</li>
		<li>
		<p>Requirement documentation: Use structured templates to document all information related to system requirements.</p>
		</li>
		<li>
		<p>Scope definition: Clearly define the AI system’s intended use cases and limitations.</p>
		</li>
		<li>
		<p>Use case documentation: Develop detailed descriptions of appropriate and inappropriate use cases to prevent misuse.</p>
		</li>
	</ul>
	</dd>
	<dt>Data preparation</dt>
	<dd>
	<ul>
		<li>
		<p>Data relevance validation: Involve users or domain experts in validating dataset relevance and quality. Analyze the dataset distribution, complete a data bias assessment, and log the results.</p>
		</li>
		<li>
		<p>Fairness-aware preprocessing: Apply preprocessing techniques to address bias or imbalances where necessary, and document decisions and justifications.</p>
		</li>
		<li>
		<p>Data catalog accessibility: Provide users with access to data catalogs, including metadata, schema descriptions, and data quality indicators.</p>
		</li>
		<li>
		<p>Feature explainability: Validate that data features are understandable and meaningful to users.</p>
		</li>
		<li>
		<p>Feature selection documentation: Record the rationale behind feature selection.</p>
		</li>
		<li>
		<p>Data source documentation: Maintain detailed records of data sources, collection methods, and preprocessing steps.</p>
		</li>
		<li>
		<p>Data versioning: Implement a version control system to track changes to the data over time.</p>
		</li>
		<li>
		<p>Contextual data labeling: Label and tag data to indicate the context in which it is appropriate.</p>
		</li>
	</ul>
	</dd>
	<dt>Modeling</dt>
	<dd>
	<ul>
		<li>
		<p>Model selection input: Involve users or domain experts in model selection decisions to ensure they meet users’ needs and expectations.</p>
		</li>
		<li>
		<p>Model architecture disclosure: Provide users with understandable information about the model architecture and its components.</p>
		</li>
		<li>
		<p>Use of interpretable models: Prefer inherently interpretable model types where possible.</p>
		</li>
		<li>
		<p>Explainability techniques: Implement techniques to explain complex or opaque models, if these are used. Validate explainability outputs with SHAP, LIME, or similar tools.</p>
		</li>
		<li>
		<p>Model documentation: Maintain complete documentation of model architecture, hyperparameters, and training processes.</p>
		</li>
		<li>
		<p>Model cards: Create model cards summarizing key information about each model (intended use, ethical considerations, limitations, evaluation results).</p>
		</li>
		<li>
		<p>Human override mechanisms: Implement mechanisms that allow human operators to override or intervene in model decisions when necessary, and document when and how they should be used.</p>
		</li>
	</ul>
	</dd>
	<dt>Evaluation</dt>
	<dd>
	<ul>
		<li>
		<p>User testing sessions: Involve users in testing and evaluating model outputs.</p>
		</li>
		<li>
		<p>Evaluation results sharing: Provide stakeholders with accessible reports on evaluation outcomes.</p>
		</li>
		<li>
		<p>Explain evaluation metrics: Use understandable metrics and explain their implications to users.</p>
		</li>
		<li>
		<p>Evaluation procedures documentation: Thoroughly document evaluation methods and rationales, along with datasets used and results.</p>
		</li>
		<li>
		<p>Experiment tracking: Use tools such as MLflow or Weights &amp; Biases to track experiments and results systematically.</p>
		</li>
		<li>
		<p>Contextual performance analysis: Evaluate model performance across different contexts (user groups, usage scenarios), and document findings.</p>
		</li>
		<li>
		<p>Model applicability testing: Validate that the models used are appropriate for the intended deployment contexts and use cases. Document limitations.</p>
		</li>
		<li>
		<p>Stress testing: Perform stress tests to assess model performance in various <span class="keep-together">scenarios</span>.</p>
		</li>
	</ul>
	</dd>
	<dt class="pagebreak-before">Deployment</dt>
	<dd>
	<ul>
		<li>
		<p>User control options: Implement mechanisms that allow users to control AI interactions (e.g., opt-in/opt-out features, adjustable system behavior settings).</p>
		</li>
		<li>
		<p>AI interaction disclosure: Clearly inform users when they are interacting with an AI system.</p>
		</li>
		<li>
		<p>Information accessibility: Provide easy access to information about how the AI system works.</p>
		</li>
		<li>
		<p>Decision explanations: Offer understandable explanations for AI decisions.</p>
		</li>
		<li>
		<p>Deployment documentation: Record details on deployment configurations, environments, and versions.</p>
		</li>
		<li>
		<p>AI system indicators: Use visual cues or labels to indicate AI-generated content or decisions.</p>
		</li>
		<li>
		<p>Usage limitations display: Clearly communicate the intended use cases and known limitations of the AI system.</p>
		</li>
		<li>
		<p>Alerting systems: Configure alerting mechanisms for data drift, model performance degradation, and other issues.</p>
		</li>
	</ul>
	</dd>
	<dt>Monitoring and maintenance</dt>
	<dd>
	<ul>
		<li>
		<p>Feedback mechanisms: Provide channels for users to submit feedback and report issues.</p>
		</li>
		<li>
		<p>Update notifications: Inform stakeholders about system updates, significant changes, and issues that arise.</p>
		</li>
		<li>
		<p>Continuous explainability: Ensure that explanations remain accurate and relevant over time, and implement real-time explainability pipelines if necessary.</p>
		</li>
		<li>
		<p>Maintenance logs: Keep detailed records of maintenance activities, updates, and system changes.</p>
		</li>
		<li>
		<p>Documentation updates: Regularly revise technical and user-facing documentation to reflect the system’s current state.</p>
		</li>
		<li>
		<p>Context drift monitoring: Monitor for changes in the operating environment that could affect the appropriateness or effectiveness of the system. Conduct regular fairness and performance audits.</p>
		</li>
		<li>
		<p>Misuse alerts: Set up alerting mechanisms for potential misuse or operation outside intended contexts.</p>
		</li>
		<li>
		<p>Feedback loops: Implement mechanisms for integrating user feedback into future system improvements, retraining, or governance decisions.</p>
		</li>
	</ul>
	</dd>
</dl>

<p class="pagebreak-before">This checklist provides a structured way to ensure compliance with the joint requirements of Articles 13 and 14. Again, teams should revise the checklist based on their specific high-risk AI system and organizational context, while ensuring compliance with<a contenteditable="false" data-primary="Article 15: Accuracy, Robustness, and Cybersecurity" data-type="indexterm" id="art-15"/> all <a contenteditable="false" data-primary="checklists for compliance (quality attributes)" data-secondary="Articles 13 and 14" data-startref="checkl-art-13-14" data-type="indexterm" id="id577"/>regulatory<a contenteditable="false" data-primary="Article 13: Transparency and Provision of Information to Deployers" data-startref="art-13" data-type="indexterm" id="id578"/> requirements<a contenteditable="false" data-primary="Article 14: Human Oversight" data-seealso="Article 13: Transparency and Provision of Information to Deployers" data-startref="art-14" data-type="indexterm" id="id579"/>.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Article 15: Accuracy, Robustness, and Cybersecurity" data-type="sect2"><div class="sect2" id="chapter_5_article_15_accuracy_robustness_and_cybersecurit_1748539922580269">
<h2>Article 15: Accuracy, Robustness, and Cybersecurity</h2>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="chapter_5_article_15_1748539922580342">
<h1>Article 15</h1>

<p><a href="https://oreil.ly/7bq1r">Article 15 of the EU AI Act</a> establishes requirements to ensure that high-risk AI systems are accurate, reliable, and secure. It focuses on maintaining the technical integrity of AI systems to prevent malfunctions, reduce vulnerabilities, and guard against misuse. The article highlights the need for resilience, reliability, and safety across the AI system lifecycle, minimizing risks associated with errors, disruptions, or malicious interference.</p>

<p>Key points of Article 15<a contenteditable="false" data-primary="Article 15: Accuracy, Robustness, and Cybersecurity" data-secondary="key points of" data-type="indexterm" id="id580"/> include:</p>

<dl>
	<dt>Accuracy requirements</dt>
	<dd>
	<p>High-risk AI systems must deliver consistent and precise results appropriate to their intended purpose, with a low rate of errors and incorrect outputs.</p>
	</dd>
	<dt>Robustness</dt>
	<dd>
	<p>Systems must be resilient to various conditions, including potential operational disruptions or changes in the input data.</p>
	</dd>
	<dt>Handling adverse scenarios</dt>
	<dd>
	<p>Systems should be designed to maintain functionality or fail gracefully under unexpected or adverse conditions.</p>
	</dd>
	<dt>Cybersecurity measures</dt>
	<dd>
	<p>Developers must implement robust cybersecurity safeguards to protect the system from unauthorized access, tampering, or attacks.</p>
	</dd>
	<dt>Resilience to manipulation</dt>
	<dd>
	<p>AI systems should be designed to be resistant to intentional manipulation or exploitation by bad actors.</p>
	</dd>
	<dt>Ongoing monitoring and testing</dt>
	<dd>
	<p>Accuracy, robustness, and security must be continuously tested, monitored, and improved throughout the system’s lifecycle.</p>
	</dd>
	<dt>Incident response</dt>
	<dd>
	<p>Developers and operators must have protocols in place to detect and address security incidents promptly, minimizing damage and ensuring rapid recovery.</p>
	</dd>
</dl>
</div></aside>

<p>According to Kelly et al., to guarantee the implementation of accurate, reliable, <span class="keep-together">and secure</span> AI systems, the requirements stated in Article 15 can be mapped to the following quality attributes, as well as appropriateness recognizability and self-descriptiveness:</p>

<dl>
	<dt>Functional correctness</dt>
	<dd>
	<p>The degree to which the system produces outputs that accurately reflect its intended functionality and specified behavior for given inputs.</p>
	</dd>
	<dt>Faultlessness</dt>
	<dd>
	<p class="fix_tracking">The extent to which the system operates without defects, malfunctions, or <span class="keep-together">unintended</span> behavior throughout its lifecycle, ensuring reliability and consistency.</p>
	</dd>
	<dt>Robustness</dt>
	<dd>
	<p>The system’s ability to maintain stable performance and functionality under a wide range of conditions, including adversarial inputs, edge cases, or unexpected environmental changes.</p>
	</dd>
	<dt>Functional adaptability</dt>
	<dd>
	<p>The capacity of the system to adapt to changing requirements, environmental conditions, or use contexts without significant performance degradation.</p>
	</dd>
	<dt>Fault tolerance</dt>
	<dd>
	<p>The system’s ability to continue functioning correctly (or in a degraded mode) even when faults or failures occur.</p>
	</dd>
	<dt>Integrity</dt>
	<dd>
	<p>The extent to which the system’s data and operations are protected against <span class="keep-together">unauthorized</span> access or alterations, ensuring trust in its behavior and outputs.</p>
	</dd>
	<dt>Resistance</dt>
	<dd>
	<p>The ability of the system to defend against attacks, tampering, or unauthorized manipulations, maintaining secure and reliable operation.</p>
	</dd>
</dl>

<p><a data-type="xref" href="#chapter_5_table_12_1748539922529659">Table 5-12</a> summarizes the AI engineering practices you should implement throughout the development lifecycle to support these attributes.</p>

<table id="chapter_5_table_12_1748539922529659">
	<caption><span class="label">Table 5-12. </span>Quality attributes relevant to Article 15 and corresponding AI engineering practices and tools</caption>
	<thead>
		<tr>
			<th>CRISP-ML(Q) phase</th>
			<th>Quality attributes</th>
			<th>AI engineering practices and tools</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td class="subheading" rowspan="2">Business and data understanding</td>
			<td>
			<p>Functional correctness</p>
			</td>
			<td>
			<p>Define measurable goals using JIRA.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Self-descriptiveness</p>
			</td>
			<td>
			<p>Document assumptions using Confluence or GitHub wikis.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="3">Data preparation</td>
			<td>
			<p>Faultlessness</p>
			</td>
			<td>
			<p>Implement data validation pipelines with Great Expectations.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Integrity</p>
			</td>
			<td>
			<p>Secure data with encryption (AWS KMS, GCP KMS) and access controls.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Robustness</p>
			</td>
			<td>
			<p>Include data augmentation techniques for edge cases.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="3">Modeling</td>
			<td>
			<p>Robustness</p>
			</td>
			<td>
			<p>Use adversarial training frameworks like CleverHans or IBM ART.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Functional adaptability</p>
			</td>
			<td>
			<p>Utilize AutoML tools (H2O.ai, DataRobot) for adaptability.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Fault tolerance</p>
			</td>
			<td>
			<p>Design models with redundancy (ensemble methods) for fault tolerance.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="3">Evaluation</td>
			<td>
			<p>Functional correctness</p>
			</td>
			<td>
			<p>Use metrics like precision, recall, F1 score, and confusion matrix.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Robustness</p>
			</td>
			<td>
			<p>Evaluate models with stress testing and adversarial scenarios.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Appropriateness recognizability</p>
			</td>
			<td>
			<p>Assess model transparency with explainability tools (SHAP, LIME).</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="3">Deployment</td>
			<td>
			<p>Faultlessness</p>
			</td>
			<td>
			<p>Use CI/CD pipelines (Jenkins, GitHub Actions, ArgoCD) for bug-free deployments.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Integrity</p>
			</td>
			<td>
			<p>Sign model artifacts with secure hash algorithms (SHA-256).</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Resistance</p>
			</td>
			<td>
			<p>Apply NIST Cybersecurity Framework principles: identify vulnerabilities, secure environments, detect threats.</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" rowspan="4">Monitoring and maintenance</td>
			<td>
			<p>Functional correctness</p>
			</td>
			<td>
			<p>Implement real-time performance monitoring with Grafana dashboards.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Self-descriptiveness</p>
			</td>
			<td>
			<p>Log decisions and anomalies with Elastic Stack.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Fault tolerance</p>
			</td>
			<td>
			<p>Design failover mechanisms with Kubernetes health probes.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Resistance</p>
			</td>
			<td>
			<p>Apply active threat detection with AWS GuardDuty or Azure Security Center.</p>
			</td>
		</tr>
	</tbody>
</table>

<section data-pdf-bookmark="Checklist for compliance" data-type="sect3"><div class="sect3" id="chapter_5_checklist_for_compliance_1748539922580405">
<h3>Checklist for compliance</h3>

<p>Building<a contenteditable="false" data-primary="checklists for compliance (quality attributes)" data-secondary="Article 15: Accuracy, Robustness and Cybersecurity" data-type="indexterm" id="checkl-art-15"/> an AI system that complies with Article 15 of the EU AI Act requires a comprehensive approach that addresses the related dimensions of accuracy, robustness, and cybersecurity. The following checklist (which can be adapted according to context) outlines practical steps that can be taken to support compliance throughout the system’s lifecycle. It integrates AI engineering best practices aligned with the quality attributes described in the previous section into each development phase:</p>

<dl>
	<dt>Business and data understanding</dt>
	<dd>
	<ul>
		<li>
		<p>Identify potential data quality issues through profiling and exploratory analysis.</p>
		</li>
		<li>
		<p>Document known data limitations and risks that could impact system reliability and correctness.</p>
		</li>
		<li>
		<p>Ensure training data represents diverse operational scenarios, including potential edge cases.</p>
		</li>
		<li>
		<p>Define robustness requirements based on system objectives and identified risks (e.g., resistance to common attacks such as feature perturbation or injection of adversarial examples, performance degradation thresholds under stress <span class="keep-together">conditions</span>).</p>
		</li>
	</ul>
	</dd>
	<dt>Data preparation</dt>
	<dd>
	<ul>
		<li>
		<p>Validate datasets for accuracy, duplicates, outliers, and completeness using tools such as Great Expectations.</p>
		</li>
		<li>
		<p>Maintain end-to-end data lineage and document all preprocessing steps to support traceability.</p>
		</li>
		<li>
		<p>Apply data augmentation techniques to simulate edge cases and stress-test model generalization.</p>
		</li>
		<li>
		<p>Use adversarial testing to ensure robustness.</p>
		</li>
		<li>
		<p>Encrypt data in transit and at rest.</p>
		</li>
		<li>
		<p>Enforce strict access controls to ensure secure handling of sensitive data.</p>
		</li>
	</ul>
	</dd>
	<dt>Modeling</dt>
	<dd>
	<ul>
		<li>
		<p>Define and track performance metrics to comprehensively assess functional correctness, robustness, and security (e.g., precision, recall, F1 score, mean absolute error, adversarial robustness score, attack success rate, system uptime, and throughput).</p>
		</li>
		<li>
		<p>Implement unit and integration tests for model components.</p>
		</li>
		<li>
		<p>Use exception handling mechanisms in code to manage unexpected inputs or processing errors.</p>
		</li>
		<li>
		<p>Conduct code reviews and apply static code analysis tools (e.g., SonarQube) to detect defects early.</p>
		</li>
		<li>
		<p>Use adversarial training techniques to improve robustness.</p>
		</li>
		<li>
		<p>Use interpretable models or apply explainability tools (e.g., SHAP, LIME) to clarify model behavior.</p>
		</li>
		<li>
		<p>Use metadata management tools (e.g., MLflow, Neptune.ai) to track model <span class="keep-together">lineage</span>.</p>
		</li>
		<li>
		<p>Sign model artifacts with secure hash algorithms (e.g., SHA-256) to ensure their integrity and authenticity.</p>
		</li>
		<li>
		<p>Protect model training environments from unauthorized access.</p>
		</li>
		<li>
		<p>Conduct security assessments and penetration tests during development to identify and address potential vulnerabilities.</p>
		</li>
		<li>
		<p>Evaluate system behavior under adversarial attack scenarios and mitigate high-level threats (e.g., gradient masking).</p>
		</li>
	</ul>
	</dd>
	<dt class="pagebreak-before">Evaluation</dt>
	<dd>
	<ul>
		<li>
		<p>Validate model outputs against ground truth data.</p>
		</li>
		<li>
		<p>Assess the model’s accuracy using holdout datasets and cross-validation <span class="keep-together">techniques</span>.</p>
		</li>
		<li>
		<p>Conduct regression testing to ensure updates do not introduce new errors.</p>
		</li>
		<li>
		<p>Evaluate model performance against adversarial inputs and out-of-distribution data.</p>
		</li>
		<li>
		<p>Perform stress testing to determine performance limits and evaluate system behavior in extreme or unexpected conditions.</p>
		</li>
	</ul>
	</dd>
	<dt>Deployment</dt>
	<dd>
	<ul>
		<li>
		<p>Implement CI/CD pipelines to automate testing and validation prior to deployment.</p>
		</li>
		<li>
		<p>Monitor real-time performance to ensure stability and consistency.</p>
		</li>
		<li>
		<p>Apply redundancy mechanisms to mitigate failures in production.</p>
		</li>
		<li>
		<p>Deploy redundant models or ensemble models for fault tolerance.</p>
		</li>
		<li>
		<p>Secure deployment pipelines using role-based access controls and encrypted communications.</p>
		</li>
		<li>
		<p>Monitor for cybersecurity threats using tools like AWS GuardDuty.</p>
		</li>
		<li>
		<p>Develop and periodically test incident response plans for security breaches and failures.</p>
		</li>
	</ul>
	</dd>
	<dt>Monitoring and maintenance</dt>
	<dd>
	<ul>
		<li>
		<p>Monitor edge case behavior and retrain models as new data becomes available.</p>
		</li>
		<li>
		<p>Periodically reassess the system’s robustness to ensure it can handle evolving operational or environmental conditions.</p>
		</li>
		<li>
		<p>Conduct regular security audits to verify the integrity of deployed systems.</p>
		</li>
		<li>
		<p>Encrypt backups and secure access to sensitive data and systems.</p>
		</li>
		<li>
		<p>Continuously monitor for emerging threats and implement mitigations to maintain system security.</p>
		</li>
		<li>
		<p>Use intrusion detection systems to detect unauthorized access or anomalous activity.</p>
		</li>
	</ul>
	</dd>
</dl>

<p>By integrating these practices into the AI lifecycle, documenting them thoroughly, and continuously evaluating performance against accuracy, robustness, and security requirements, AI engineering teams can support compliance with Article 15. Close collaboration between AI engineers, information security teams, and risk and compliance stakeholders will be key<a contenteditable="false" data-primary="checklists for compliance (quality attributes)" data-secondary="Article 15: Accuracy, Robustness and Cybersecurity" data-startref="checkl-art-15" data-type="indexterm" id="id581"/>.</p>
</div></section>

<section data-pdf-bookmark="Further information" data-type="sect3"><div class="sect3" id="chapter_5_further_information_1748539922580466">
<h3>Further information</h3>

<p>The following frameworks and resources offer practical guidance for implementing the technical requirements of Article 15<a contenteditable="false" data-primary="Article 15: Accuracy, Robustness, and Cybersecurity" data-startref="art-15" data-type="indexterm" id="id582"/> and building trustworthy, reliable, and secure AI systems:</p>

<ul>
	<li>
	<p><a href="https://oreil.ly/2mQel">NIST Cybersecurity Framework</a></p>
	</li>
	<li>
	<p><a href="https://oreil.ly/kTfwZ">NIST Privacy Framework</a></p>
	</li>
	<li>
	<p><a href="https://oreil.ly/dRyWk"><em>Machine Learning for High-Risk Applications</em></a> by Patrick Hall, James Curtis, and Parul Pandey (O’Reilly)</p>
	</li>
</ul>
</div></section>
</div></section>
</div></section>

<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="chapter_5_conclusion_1748539922580548">
<h1>Conclusion</h1>

<p>This chapter has explored the intersection of the EU AI Act and AI engineering, presenting a framework for achieving and demonstrating compliance through robust engineering practices. As you have seen, data and AI governance, monitoring, alerting, and documentation are not merely best practices but foundational elements for compliance with the EU AI Act.</p>

<p>We examined Articles 9 through 15 of the Act here, translating their high-level requirements for high-risk AI systems into actionable technical specifications aligned with the CRISP-ML(Q) lifecycle. By mapping these requirements to relevant quality attributes for safety-critical AI systems, this chapter provides a practical roadmap for organizations to develop, deploy, and manage high-risk AI systems responsibly and transparently. (See <a data-type="xref" href="#chapter_5_table_13_1748539922529689">Table 5-13</a> for the complete mapping.) A key takeaway is the central role of documentation in assessing and demonstrating compliance. A robust metadata management system is also essential, both for meeting the documentation requirements of Article 11 and for ensuring traceability, as mandated by Article 12.</p>

<p>AI governance is an emerging discipline, and its implementation varies across industries. Consequently, achieving compliance with the EU AI Act presents real challenges, and there is a need for an engineering-level approach to meet the Act’s requirements. This chapter, along with the previous ones, represents an initial effort to establish principles for proactive compliance through comprehensive AI engineering practices—practices that serve as enablers of trustworthy AI, which is the core motivation behind the EU AI Act.</p>

<p>The concept of AI engineering for the EU AI Act introduced here blends law, ethics, and MLOps, with an emphasis on continuous compliance through monitoring and documentation. In the next chapter, we will examine the requirements for limited-risk AI systems and explore strategies for proactive compliance through AI engineering in that context.</p>

<table border="1" id="chapter_5_table_13_1748539922529689">
	<caption><span class="label">Table 5-13. </span>Mapping the EU AI Act requirements to quality attributes for high-risk <span class="keep-together">AI systems</span></caption>
	<thead>
		<tr>
			<th>Quality attribute</th>
			<th>Article 9</th>
			<th>Article 10</th>
			<th>Article 11</th>
			<th>Article 12</th>
			<th>Article 13</th>
			<th>Article 14</th>
			<th>Article 15</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Risk identification</p>
			</td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Testability</p>
			</td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Value alignment</p>
			</td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Independence</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Data completeness</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Currentness</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Data fairness</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Precision</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Representativeness</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Consistency</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Accuracy</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Credibility</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Temporality</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Confidentiality</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Compliance</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Data traceability</p>
			</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Traceability</p>
			</td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Operability</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Non-repudiation</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Self-descriptiveness</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td class="center">✓</td>
			<td> </td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td>
			<p>Accountability</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Self-monitoring</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Monitorability</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>User error protection</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>User engagement</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>User transparency</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Interpretability</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td class="center">✓</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Documentability</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td class="center">✓</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Appropriateness recognizability</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td>
			<p>Learnability</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Fairness</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Explainability</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Intervenability</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
			<td> </td>
		</tr>
		<tr>
			<td>
			<p>Functional correctness</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td>
			<p>Faultlessness</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td>
			<p>Robustness</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td>
			<p>Functional adaptability</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td>
			<p>Fault tolerance</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td>
			<p>Integrity</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
		</tr>
		<tr>
			<td>
			<p>Resistance</p>
			</td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td> </td>
			<td class="center">✓</td>
		</tr>
	</tbody>
</table>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id564"><sup><a href="ch05.html#id564-marker">1</a></sup> To learn more about technical documentation, refer to the paper <a href="https://oreil.ly/AoUII">“Documenting High-Risk AI: A European Regulatory Perspective”</a> by Isabelle Hupont et al.</p></div></div></section></body></html>