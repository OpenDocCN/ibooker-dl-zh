- en: 12 Overcoming ranking bias through active learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 通过主动学习克服排名偏差
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Harnessing live user interactions to gather feedback on a deployed LTR model
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用实时用户交互来收集对部署的LTR模型的反馈
- en: A/B testing search relevance solutions with live users
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用实时用户进行A/B测试搜索相关性解决方案
- en: Using active learning to explore potentially relevant results beyond the top
    results
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主动学习来探索可能相关的结果，而不仅仅是顶部结果
- en: Balancing *exploiting* user interactions while *exploring* what else might be
    relevant
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在利用用户交互的同时探索其他可能相关的信息
- en: So far, our learning to rank (LTR) work has taken place in the lab. In previous
    chapters, we built models using automatically constructed training data from user
    clicks. In this chapter, we’ll take our model into the real world for a test drive
    with (simulated) live users!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的学习排名（LTR）工作一直在实验室进行。在前几章中，我们使用用户点击自动构建的训练数据构建了模型。在本章中，我们将把我们的模型带入现实世界，与（模拟的）实时用户进行测试！
- en: 'Recall that we compared an automated LTR system to a self-driving car. Internally,
    the car has an engine: the end-to-end model retraining on historical judgments
    as discussed in chapter 10\. In chapter 11, we compared our model’s training data
    to self-driving car directions: what *should* we optimize to automatically learn
    judgments based on previous interactions with search results? We built training
    data and overcame key biases inherent in click data.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们比较了自动LTR系统与自动驾驶汽车。在内部，汽车有一个引擎：第10章讨论的基于历史判断的端到端模型重新训练。在第11章中，我们将我们的模型训练数据与自动驾驶汽车的方向进行了比较：我们应该优化什么来根据与搜索结果的先前交互自动学习判断？我们构建了训练数据并克服了点击数据中固有的关键偏差。
- en: 'In this chapter, our focus is on moving our ranking models from the lab to
    production. We’ll deploy and monitor our models as they receive user traffic.
    We’ll see where the model does well and understand whether the work in the previous
    two chapters failed or succeeded. This means exploring a new kind of testing to
    validate our model: *A/B testing*. In *A/B testing* we randomly assign live users
    to different models and examine business outcomes (sales, etc.), to see which
    models perform best. You might be familiar with A/B testing in other contexts,
    but here we’ll zero in on the implications for an automated LTR system.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的重点是将我们的排名模型从实验室转移到生产环境中。我们将部署并监控我们的模型，当它们接收用户流量时。我们将看到模型做得好的地方，并了解前两章的工作是否失败或成功。这意味着探索一种新的测试方法来验证我们的模型：*A/B测试*。在*A/B测试*中，我们将随机将实时用户分配到不同的模型，并检查业务结果（销售额等），以查看哪个模型表现最好。你可能在其他环境中熟悉A/B测试，但在这里我们将专注于自动LTR系统的含义。
- en: Live users help us not just validate our system, they also aid in escaping dangerous
    negative feedback loops our models can find themselves in, as shown in figure
    12.1.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 实时用户不仅帮助我们验证系统，还帮助我们摆脱模型可能陷入的危险负面反馈循环，如图12.1所示。
- en: '![figure](../Images/CH12_F01_Grainger.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F01_Grainger.png)'
- en: Figure 12.1 Presentation bias’s negative feedback loop. Users never click on
    what the search engine never returns, so relevance models can never grow beyond
    the current model’s knowledge.
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.1 展示了展示偏差的负面反馈循环。用户永远不会点击搜索引擎从未返回的内容，因此相关性模型永远无法超越当前模型的知识。
- en: 'In figure 12.1, our model can only learn what’s relevant *within the results
    shown to the user*. In other words, we have an unfortunate chicken-and-egg problem:
    the model is trained based on what users deem relevant, but what users deem relevant
    is based on what the model shows them. Good LTR attempts to optimize for results
    with the most positive interaction signals, but users will only click on what’s
    right in front of them. How could LTR possibly get better when the training data
    seems hopelessly biased toward the search engine’s current ranking? This bias
    of implicitly derived training data reflecting back the previously displayed results
    is called *presentation bias*.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在图12.1中，我们的模型只能学习用户展示给他们的相关内容。换句话说，我们面临一个不幸的鸡生蛋问题：模型是根据用户认为相关的数据进行训练的，但用户认为相关的数据是基于模型展示给他们的。好的LTR尝试优化具有最多积极交互信号的结果，但用户只会点击他们面前的东西。当训练数据似乎毫无希望地偏向搜索引擎当前排名时，LTR如何可能变得更好？这种从隐式推导的训练数据中反映出的先前显示结果的偏差被称为*展示偏差*。
- en: After we explore A/B testing, we’ll fight presentation bias for the rest of
    the chapter using active learning. An *active learning* system is one that can
    interactively gather new labeled data from users to answer new questions. In our
    case, our active learning algorithm will determine blind spots leading to ranking
    bias, prompt users to interact with new results exploring those blind spots, and
    utilize the user interactions as new training data to correct the blind spots.
    Much like a self-driving car that has only learned one suboptimal path, we’ll
    have to strategically explore alternative promising paths—in our case, additional
    types of search results—to learn new patterns for what’s relevant to users. In
    figure 12.2, we see the automated LTR loop augmented with this blind-spot exploration.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索A/B测试之后，我们将使用主动学习来对抗本章剩余部分的展示偏差。一个*主动学习*系统是可以交互式地从用户那里收集新的标记数据以回答新问题的系统。在我们的情况下，我们的主动学习算法将确定导致排名偏差的盲点，提示用户与探索这些盲点的新结果进行交互，并将用户交互作为新的训练数据来纠正盲点。就像只有学习了一条次优路径的自动驾驶汽车一样，我们必须战略性地探索其他有希望的路径——在我们的情况下，是额外的搜索结果类型——以学习新的用户相关性模式。在图12.2中，我们看到自动化LTR循环增加了这种盲点探索。
- en: Before we get to this all-important subject, we must first wrap everything we
    learned in chapters 10 and 11 into a few lines of code. Then we’ll be able to
    iterate quickly, exploring A/B testing and overcoming presentation bias.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们到达这个至关重要的主题之前，我们必须首先将第10章和第11章所学的一切封装成几行代码。然后我们才能快速迭代，探索A/B测试和克服展示偏差。
- en: '![figure](../Images/CH12_F02_Grainger.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F02_Grainger.png)'
- en: Figure 12.2 Automated LTR meets live users. To be useful, our automated LTR
    system must overcome presentation bias by exploring yet-to-be-seen results with
    users to expand training data coverage.
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.2 自动化LTR与实时用户相遇。为了有用，我们的自动化LTR系统必须通过探索用户尚未看到的结果来克服展示偏差，以扩大训练数据覆盖范围。
- en: 12.1 Our automated LTR engine in a few lines of code
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 几行代码中的我们的自动化LTR引擎
- en: Before we begin to A/B test, let’s gather all our knowledge from chapters 10
    and 11 into a small handful of reusable Python helper functions. First, we’ll
    define a function to let us rebuild training data from raw session clicks using
    a simplified dynamic Bayesian network (SDBN) click model (all of chapter 11).
    Next, we’ll create an equally simple snippet of code to train a model with that
    training data (all of chapter 10). We’ll very quickly sum up these functions before
    diving into A/B testing and overcoming presentation bias in the rest of the chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始进行A/B测试之前，让我们将第10章和第11章的所有知识汇总到几个可重用的Python辅助函数中。首先，我们将定义一个函数，使我们能够使用简化的动态贝叶斯网络（SDBN）点击模型（第11章的全部内容）从原始会话点击中重建训练数据。接下来，我们将创建一个同样简单的代码片段，使用该训练数据（第10章的全部内容）训练模型。在深入A/B测试和本章剩余部分克服展示偏差之前，我们将非常快速地总结这些函数。
- en: 12.1.1 Turning clicks into training data (chapter 11 in one line of code)
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 将点击转换为训练数据（第11章一行代码）
- en: In chapter 11, we turned clicks into training data and explored the SDBN click
    model, which can overcome biases in how users click on search results. We will
    reuse much of the code from chapter 11 as we explore additional biases and automate
    the end-to-end LTR process in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在第11章中，我们将点击转换为训练数据，并探讨了SDBN点击模型，该模型可以克服用户点击搜索结果时的偏差。在探索本章中额外的偏差并自动化端到端LTR过程时，我们将重用第11章的大部分代码。
- en: 'As a reminder, our click model turns raw clicks into training labels or *grades*
    mapping how relevant a document is for a keyword. The raw input we need to build
    the training data includes a query string, the rank of the result as displayed,
    the document in that position, and whether it was clicked. We can see that data
    stored in this dataframe:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，我们的点击模型将原始点击转换为训练标签或*评分*，映射文档与关键词的相关性。我们需要构建训练数据的原始输入包括查询字符串、显示的结果排名、该位置的文档以及是否被点击。我们可以看到存储在这个数据框中的数据：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Given this input, we can wrap all of chapter 11 into a reusable function that
    computes our training data. Recall that we use the term *judgment list* or *judgments*
    to refer to our training data. We can see our judgments computation in the following
    listing.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这个输入，我们可以将第11章的所有内容封装成一个可重用的函数，该函数用于计算我们的训练数据。回想一下，我们使用“判断列表”或“判断”来指代我们的训练数据。我们可以在以下列表中看到我们的判断计算。
- en: Listing 12.1 Generating training data from sessions (chapter 11)
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.1 从会话生成训练数据（第11章）
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Output (truncated):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（截断）：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `generate_training_data` function takes in all the user search `sessions`,
    along with the `prior_weight`, indicating how strong the prior should be weighted
    (defaults to `10`), and the `prior_grade`, specifying the default probability
    of a result’s relevance when we have no evidence (defaults to `0.2`). See section
    11.3.2 for a refresher on how these values influence the SDBN calculation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate_training_data` 函数接收所有用户搜索 `sessions`，以及 `prior_weight`，表示先验权重应该被赋予多强的权重（默认为
    `10`），和 `prior_grade`，指定在没有证据时结果的相关性的默认概率（默认为 `0.2`）。参见第 11.3.2 节，了解这些值如何影响 SDBN
    计算的复习。'
- en: 'Let’s briefly revisit what we learned in chapter 11 by looking at listing 12.1\.
    As you can see in the output, we compute a dataframe where each query-document
    pair has corresponding `clicked` and `examined` counts. Clicks are what they sound
    like: the sum of raw clicks this product received for this query. Recall that
    `examined` corresponds to the number of times the click model thinks the user
    noticed the result.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看列表 12.1 简要回顾一下在第 11 章中学到的内容。如您在输出中看到的那样，我们计算了一个 dataframe，其中每个查询-文档对都有相应的
    `clicked` 和 `examined` 计数。点击就是它们听起来那样：这个产品为这个查询收到的原始点击总和。回想一下，`examined` 对应于点击模型认为用户注意到结果次数的数量。
- en: 'The `grade` and `beta_grade` statistics are the training labels. These correspond
    to the probability that a document is relevant for the query. Recall that `grade`
    simply divides `clicked` by `examined`: the naive, first implementation of the
    SDBN click model. However, we learned in chapter 11 that it would be better to
    account for how much information we have (see section 11.3). We don’t want one
    click with one examine `(1` `/` `1` `=` `1.0)` to be counted as strongly as a
    hundred-clicks with a hundred examines `(100` `/` `100` `=` `1.0)`. For this reason,
    `beta_grade` places a higher weight on results with more information (preferring
    the hundred-clicks example). We’ll therefore use `beta_grade` as opposed to `grade`
    when retraining LTR models.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`grade` 和 `beta_grade` 统计量是训练标签。这些对应于文档对于查询的相关性概率。回想一下，`grade` 简单地将 `clicked`
    除以 `examined`：SDBN 点击模型的原始、首次实现。然而，我们在第 11 章中了解到，最好考虑我们拥有的信息量（参见第 11.3 节）。我们不希望一个点击和一个查看
    `(1 / 1 = 1.0)` 被计为像一百个点击和一百个查看 `(100 / 100 = 1.0)` 那样强烈。因此，`beta_grade` 对信息量更多的结果赋予更高的权重（更倾向于一百个点击的例子）。因此，当重新训练
    LTR 模型时，我们将使用 `beta_grade` 而不是 `grade`。'
- en: This data served as training data for the LTR models we trained in chapter 10\.
    Next, let’s see how we can easily take this training data, train a model, and
    deploy it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这份数据是我们第 10 章中训练的 LTR 模型的训练数据。接下来，让我们看看我们如何轻松地使用这些训练数据，训练一个模型，并将其部署。
- en: 12.1.2 Model training and evaluation in a few function calls
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 几个函数调用中的模型训练和评估
- en: In addition to regenerating training data, we also need to retrain our model
    before deploying it for live users. In this section, we’ll explore the convenience
    functions for our core LTR model training engine. This will set us up to quickly
    experiment with models through the rest of this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了重新生成训练数据外，我们还需要在将其部署给实时用户之前重新训练我们的模型。在本节中，我们将探讨我们核心 LTR 模型训练引擎的便利函数。这将使我们能够在本章的其余部分快速实验模型。
- en: We’ll wrap model training and offline evaluation in a few simple lines.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用几行简单的代码来封装模型训练和离线评估。
- en: Listing 12.2 Training and evaluating the model on a few features
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.2 在几个特征上训练和评估模型
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Evaluation for `ltr_model_variant_1`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`ltr_model_variant_1` 的评估：'
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With the help of listing 12.2, let’s briefly revisit what we learned in chapter
    10\. We define a `feature_set` with two features for LTR: one to search against
    the `long_ description` field, and another to search against the `short_description`
    field. We must choose carefully, hoping to find features that meaningfully predict
    relevance and that can be learned from the training data in listing 12.1\. We
    then split the `training_data` into `train` and `test` sets and use the `train`
    set to train and upload the model.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 12.2 的帮助下，让我们简要回顾一下在第 10 章中学到的内容。我们定义一个 `feature_set`，包含两个 LTR 特征：一个用于与
    `long_description` 字段进行搜索，另一个用于与 `short_description` 字段进行搜索。我们必须仔细选择，希望找到能够有意义地预测相关性并且可以从列表
    12.1 中的训练数据中学习到的特征。然后我们将 `training_data` 分割成 `train` 和 `test` 集合，并使用 `train` 集合来训练和上传模型。
- en: But how do we know if our model successfully learned from the training data?
    Splitting the judgments and excluding the `test` set during model training reserves
    some of the training data for evaluating the trained model. You’re like a professor
    giving the student (here the model) a final exam. You might give students many
    sample problems to study for the test (the `train` set). But to see if students
    truly learned the material, as opposed to just memorizing it, you’d give them
    a final exam with different questions (the `test` set). This helps you evaluate
    whether the student understands what you’ve taught them before sending them off
    into the real world.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何知道我们的模型是否成功地从训练数据中学习到了知识？在模型训练期间将判断分开并排除`测试`集，为评估训练好的模型保留了一些训练数据。你就像是一位教授给学生（这里的模型）进行期末考试。你可能会给学生提供许多样题来准备考试（`训练`集）。但为了看看学生是否真正学到了材料，而不是仅仅记住它，你会给他们一个包含不同问题的期末考试（`测试`集）。这有助于你在将学生送入现实世界之前评估学生是否理解了你之前教给他们的内容。
- en: Of course, success in the classroom does not always equate to success in the
    real world. Graduating our model into the real world, with live users in an A/B
    test, might show it does not perform as well as we hoped!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，课堂上的成功并不总是等同于现实世界的成功。将我们的模型推广到现实世界，在A/B测试中有真实用户的情况下，可能会显示出它的表现并不像我们希望的那样好！
- en: Finally, what is the statistic next to each test query? How do we evaluate the
    students’ success on the test queries? Recall from chapter 10 that we simply used
    precision (the proportion of relevant queries). This statistic sums the top `N`
    grades and divides by `N` (for us `N` = 10), which is effectively the average
    relevance grade. We recommend exploring other statistics for model training and
    evaluation that are biased toward getting the top positions correct, such as Discounted
    Cumulative Gain (DCG), Normalized DGC (NDCG), or Expected Reciprocal Rank (ERR).
    For our purposes, we’ll stay with the simpler precision statistic.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，每个测试查询旁边的统计数字是什么？我们如何评估学生在测试查询上的成功？回想一下第10章，我们只是使用了精确度（相关查询的比例）。这个统计数字将前`N`个评分相加，然后除以`N`（对我们来说`N`
    = 10），这实际上是平均相关性评分。我们建议探索其他统计指标，用于模型训练和评估，这些指标倾向于正确获取前几位，例如折现累积收益（DCG）、归一化折现累积收益（NDCG）或期望倒数排名（ERR）。对于我们来说，我们将继续使用更简单的精确度统计指标。
- en: Just judging by the relevance metrics for our test queries in listing 12.2,
    our model does quite poorly in offline testing. By improving offline metrics,
    we should see a significant improvement with live users in an A/B test.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 仅从12.2列表中测试查询的相关性指标来判断，我们的模型在离线测试中表现相当糟糕。通过提高离线指标，我们应该在A/B测试中的真实用户中看到显著的改进。
- en: 12.2 A/B testing a new model
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 对新模型进行A/B测试
- en: In this section, we’ll simulate running an A/B test and compare listing 12.2’s
    model to a model that seems to perform better in the lab. We’ll reflect on the
    results of the A/B test, setting us up to complete the automated LTR feedback
    loop we introduced in chapter 11\. We’ll finish by reflecting on what didn’t go
    so well, spending the remainder of the chapter on adding “active learning”, a
    crucial, missing piece to our automated LTR feedback loop.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将模拟运行A/B测试，并将12.2列表的模型与在实验室中似乎表现更好的模型进行比较。我们将反思A/B测试的结果，这为我们完成了在第11章中引入的自动LTR反馈循环奠定了基础。我们将通过反思哪些地方做得不好来结束，本章剩余部分将专注于添加“主动学习”，这是我们自动LTR反馈循环中缺失的关键部分。
- en: 12.2.1 Taking a better model out for a test drive
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 对更好的模型进行测试
- en: Our original LTR model hasn’t performed very well, as we saw in the output of
    listing 12.2\. In this section, we’ll train a new model, and once it looks promising,
    we’ll deploy it in an A/B test against the model we trained in listing 12.2.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们原始的LTR模型表现并不理想，正如我们在12.2列表的输出中所见。在本节中，我们将训练一个新的模型，一旦它看起来有希望，我们就会将其部署在与12.2列表中训练的模型进行的A/B测试中。
- en: Let’s look at the following improved model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下改进后的模型。
- en: Listing 12.3 A new model improved by changing the features
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.3 通过改变特征改进的新模型
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Evaluation for `ltr_model_variant_2`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对`ltr_model_variant_2`的评估：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the preceding listing, we define a `feature_set` containing three features:
    `name_fuzzy`, which performs a fuzzy search against the `name` field, `name_bigram`,
    which performs a two-word phrase search on the `name` field, and `short_description_
    bigram`, which performs a two-word phrase search against the `short_description`
    field. Like before, this model is trained, deployed, and evaluated. Notice the
    output of listing 12.3—on the same set of test queries, our model seems to perform
    much better. This seems promising! Indeed, we’ve chosen a set of features that
    seems to capture the text-matching aspects of relevance better.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的列表中，我们定义了一个包含三个特征的`feature_set`：`name_fuzzy`，它对`name`字段执行模糊搜索；`name_bigram`，它在`name`字段上执行双词短语搜索；以及`short_description_bigram`，它在`short_description`字段上执行双词短语搜索。像之前一样，这个模型被训练、部署和评估。注意列表12.3的输出——在相同的测试查询集上，我们的模型似乎表现更好。这看起来很有希望！实际上，我们选择了一组似乎能更好地捕捉相关性文本匹配方面的特征。
- en: The astute reader might notice we’ve kept the test queries the same as in listing
    12.2\. We’ve intentionally done this for clarity. It’s good enough to teach you
    fundamental AI-powered search skills. In real life, however, we would want a truly
    random test/train split to better evaluate the model’s performance. We might even
    take things further, performing *cross-validation*—the resampling and training
    of many models on different test/train dataset splits to ensure the models generalize
    well without overfitting to the training data. If you’d like to dive deeper into
    offline model evaluation, we recommend a more general machine learning book, such
    as *Machine Learning Bootcamp* by Alexey Grigorev (Manning, 2021).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明的读者可能会注意到，我们保持了与列表12.2相同的测试查询。我们有意这样做是为了清晰起见。这足以教授你基本的AI搜索技能。然而，在现实生活中，我们希望有一个真正的随机测试/训练数据集分割，以更好地评估模型的表现。我们甚至可以更进一步，进行交叉验证——在不同的测试/训练数据集分割上对多个模型进行重采样和训练，以确保模型能够很好地泛化，而不会过度拟合训练数据。如果你想要深入了解离线模型评估，我们推荐一本更通用的机器学习书籍，例如Alexey
    Grigorev的《Machine Learning Bootcamp》（Manning，2021年）。
- en: Perhaps your search team feels the model trained in listing 12.3 has promise
    and is good enough to deploy to production. The team’s hopes are up, so let’s
    see what happens when we deploy to production for further evaluation with live
    users.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你的搜索团队认为列表12.3中训练的模型有潜力，并且足够好以部署到生产环境中。团队的希望很高，那么让我们看看当我们将模型部署到生产环境中进行进一步的用户评估时会发生什么。
- en: 12.2.2 Defining an A/B test in the context of automated LTR
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 在自动LTR的上下文中定义A/B测试
- en: 'By the end of chapter 11, we had developed an end-to-end LTR retraining process:
    we could take incoming user signals to generate a click model, use the click model
    to generate judgments, use the judgments to train an LTR model, and then deploy
    the LTR model to production to gather more signals to restart the process. With
    this LTR retraining loop set up, we can easily deploy promising new ranking models.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 到第11章结束时，我们已经开发了一个端到端的LTR重新训练过程：我们可以将用户信号输入生成点击模型，使用点击模型生成判断，使用判断来训练LTR模型，然后将LTR模型部署到生产环境中以收集更多信号以重启该过程。通过设置这个LTR重新训练循环，我们可以轻松部署有潜力的新排名模型。
- en: We haven’t actually deployed our LTR models to production yet, though. We’ve
    only developed the theoretical models. How do we know whether what we built in
    the lab performs well in the real world? It’s quite a different thing to handle
    live, real-world scenarios.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们实际上还没有将我们的LTR模型部署到生产环境中。我们只开发了理论模型。我们如何知道我们在实验室中构建的东西在现实世界中表现良好？处理真实的现实世界场景是相当不同的事情。
- en: In this section, we’ll explore the results of A/B testing with (simulated) live
    users. Because this is a book with a codebase you’re running locally, it’s unfortunately
    not possible for us to have real users hitting our application. Therefore, the
    “live” user traffic we’ll use will just be simulated from within our codebase.
    For our purposes, this traffic simulation is similar enough to live user interactions
    to successfully demonstrate the active learning process.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨与（模拟的）真实用户进行的A/B测试的结果。因为这是一本你本地运行的代码库的书，很遗憾我们无法让真实用户点击我们的应用程序。因此，我们将使用我们代码库内部模拟的“实时”用户流量。就我们的目的而言，这种流量模拟与真实用户交互足够相似，可以成功地展示主动学习过程。
- en: We’ll see how the A/B test serves as the ultimate arbiter of our automated LTR
    system’s success. It will enable us to correct problems in our offline automated
    LTR model training so the feedback loop can progressively grow more reliable.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到A/B测试如何成为我们自动LTR系统成功的最终仲裁者。它将使我们能够纠正离线自动LTR模型训练中的问题，以便反馈循环可以逐步变得更加可靠。
- en: You may know about A/B tests, but here we’ll demonstrate how they factor into
    an automated LTR system. As illustrated in figure 12.3, an *A/B test* randomly
    assigns users to two *variants*. Each *variant* contains a distinct set of application
    features. This might include anything from different button colors to new relevance
    ranking algorithms. Because users are randomly assigned to the variants, we can
    more reliably infer which variant performs best on chosen business outcomes, such
    as sales, time spent on the app, user retention, or whatever else the business
    might choose to prioritize.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经了解A/B测试，但在这里我们将演示它们如何影响自动LTR系统。如图12.3所示，一个*A/B测试*随机将用户分配到两个*变体*。每个*变体*包含一组独特的应用程序功能。这可能包括从不同的按钮颜色到新的相关性排名算法等任何内容。由于用户是随机分配到变体的，我们可以更可靠地推断哪个变体在选定的业务结果上表现最佳，例如销售、在应用程序上花费的时间、用户保留率，或者企业可能选择优先考虑的其他任何事情。
- en: When running an A/B test, you will usually make one of the variants a *control
    group* representing the current default algorithm. Having a control allows you
    to measure the improvement of other models. It’s also common to perform multivariate
    testing, whereby multiple variants or combinations of variants are tested simultaneously.
    More advanced testing strategies can be implemented, like multi-arm bandit testing,
    where the test continually shifts live traffic toward the currently best-performing
    variants, or signals-based backtesting, where you use historical data to simulate
    the A/B test to predict the best variant offline before even showing results to
    live users.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行A/B测试时，你通常会指定一个变体作为*对照组*，代表当前的默认算法。拥有一个对照组可以让你衡量其他模型的改进。同时进行多变量测试也很常见，其中同时测试多个变体或变体的组合。可以实施更高级的测试策略，如多臂老虎机测试，其中测试不断将实时流量转移到当前表现最佳的变体，或者基于信号的回溯测试，其中你使用历史数据来模拟A/B测试，以预测在向实时用户展示结果之前，离线最佳变体。
- en: '![figure](../Images/CH12_F03_Grainger.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F03_Grainger.png)'
- en: Figure 12.3 A search A/B test. Search users are randomly assigned to two relevance
    algorithms (here two LTR models) with outcomes tracked.
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.3 搜索A/B测试。搜索用户被随机分配到两个相关性算法（此处为两个LTR模型），并跟踪结果。
- en: 12.2.3 Graduating the better model into an A/B test
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.3 将更好的模型毕业到A/B测试
- en: 'Next, we’ll deploy our promising new model, `ltr_model_variant_2` from listing
    12.3, into an A/B test. We’ll then explore the implications of the test results.
    Hopes are high, and your team thinks this model might knock the socks off the
    competition: the poorly performing `ltr_model_variant_1` from listing 12.2.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将部署我们的新颖模型`ltr_model_variant_2`，如列表12.3所示，进入A/B测试。然后我们将探讨测试结果的影响。期望很高，你的团队认为这个模型可能会让竞争对手大吃一惊：列表12.2中表现不佳的`ltr_model_variant_1`。
- en: In this section, we’ll simulate an A/B test, assigning 1,000 users randomly
    to each model. In our case, these simulated users have specific items they want
    to buy. If they see those items, they’ll make a purchase and leave our store happy.
    If they don’t, they might browse around, but they’ll most likely leave without
    making a purchase. Our search team, of course, doesn’t know what users hope to
    buy—this information is hidden from us. We only see a stream of clicks and purchases,
    which, as we’ll see, is heavily influenced by presentation bias.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将模拟一个A/B测试，将1,000名用户随机分配到每个模型。在我们的案例中，这些模拟用户有他们想要购买的具体商品。如果他们看到这些商品，他们就会进行购买并愉快地离开我们的商店。如果他们看不到，他们可能会四处浏览，但很可能会不购买就离开。当然，我们的搜索团队不知道用户希望购买什么——这些信息对我们来说是隐藏的。我们只看到点击和购买流，正如我们将看到的，这些受到展示偏差的严重影响。
- en: 'In listing 12.4, we have a population of users seeking the newest Transformers
    movies by searching for `transformers dvd`. We’ll stay focused on this single
    query during our discussion. Of course, with a real A/B test, we’d look over the
    full query set, and the user population wouldn’t be this static. By zeroing in
    on one query, though, we can more concretely understand the implications of our
    A/B test for automated LTR. For a deeper overview of good A/B testing experimentation,
    we recommend the book *Experimentation for Engineers: From A/B testing to Bayesian
    optimization* by David Sweet (Manning, 2023).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 12.4 中，我们有一个用户群体，他们通过搜索 `transformers dvd` 来寻找最新的 Transformers 电影。在我们的讨论中，我们将专注于这个单一查询。当然，在真实的
    A/B 测试中，我们会查看完整的查询集，用户群体不会这么静态。然而，通过专注于一个查询，我们可以更具体地理解我们的 A/B 测试对自动化 LTR 的影响。对于更深入的
    A/B 测试实验概述，我们推荐 David Sweet（Manning，2023）所著的《工程师的实验：从 A/B 测试到贝叶斯优化》一书。
- en: For each run of the `a_b_test` function in listing 12.4, a model is assigned
    at random. Then the `simulate_live_user_session` function simulates a user searching
    with the query and selected model, scanning the results, possibly clicking and
    making a purchase. Unbeknownst to us, our user population has hidden preferences
    behind their queries, which are simulated in `simulate_live_user_session`. We
    run `a_b_test` 1,000 times, collecting the purchases made by users that use each
    model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于列表 12.4 中 `a_b_test` 函数的每次运行，都会随机分配一个模型。然后，`simulate_live_user_session` 函数模拟用户使用查询和选定的模型进行搜索，浏览结果，可能点击并购买。我们不知道的是，我们的用户群体在他们的查询背后有隐藏的偏好，这些偏好由
    `simulate_live_user_session` 模拟。我们运行 `a_b_test` 1,000 次，收集使用每个模型的用户的购买情况。
- en: Listing 12.4 Simulated A/B test for the query `transformers dvd`
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.4 对查询 `transformers dvd` 的模拟 A/B 测试
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Randomly assigns each user to model a or b'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 随机将每个用户分配给模型 a 或 b'
- en: '#2 Simulates a user’s searching and purchasing behavior'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 模拟用户的搜索和购买行为'
- en: '#3 Simulates the number_of_users being tested'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 模拟测试的用户数量'
- en: '#4 Counts the total number of purchases made by each model'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 统计每个模型的总购买数量'
- en: 'Output:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we see in the output of listing 12.4, `ltr_model_variant_2` (our golden student),
    actually performs *worse* in this A/B test! How can this be? What could have gone
    wrong for it to have such good offline test metric performance but poor outcomes
    in the real world?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如列表 12.4 的输出所示，`ltr_model_variant_2`（我们的黄金学生）在这个 A/B 测试中实际上表现得更差！这怎么可能呢？它为什么会有如此好的离线测试指标性能，但在现实世界中表现不佳？
- en: For the rest of this chapter, we’ll dive into what’s happening and attempt to
    address the problem. Thus, you’ll learn how live users can increase the accuracy
    of your automated LTR system, allowing you to retrain with confidence!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将深入探讨正在发生的事情，并尝试解决问题。因此，你将了解活用户如何提高你自动化 LTR 系统的准确性，让你有信心重新训练！
- en: '12.2.4 When “good” models go bad: What we can learn from a failed A/B test'
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.4 当“好”的模型变坏：我们可以从失败的 A/B 测试中学到什么
- en: As we saw in listing 12.4, a lot can change when our model enters the real world.
    In this section, we’ll reflect on the implications of the A/B test we just ran
    to see what next steps would be appropriate.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在列表 12.4 中所看到的，当我们的模型进入现实世界时，很多东西都可能发生变化。在本节中，我们将反思我们刚刚进行的 A/B 测试的影响，以确定下一步适当的行动。
- en: 'When a model performs well in the lab but fails an A/B test, it means that
    while we may have built a “correct” LTR model, we built it to the wrong specification.
    We need to correct problems with the training data itself: the judgments generated
    from our click model.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型在实验室表现良好但 A/B 测试失败时，这意味着我们可能已经构建了一个“正确”的 LTR 模型，但我们构建的是错误的规范。我们需要纠正训练数据本身的问题：来自我们点击模型的判断。
- en: 'But how might problems creep into our click-model-based judgments? We saw two
    problems in chapter 11: *position bias* and *confidence bias*. Depending on your
    goals, UX, and domain, additional biases can creep in. In e-commerce, users might
    be enticed to click an item that’s on sale, skewing the data toward those items.
    In a research setting, one article might provide a richer summary in the search
    results than another. Some biases blur the line between “bias” and actual relevance
    for that domain. A product with a missing image, for example, might get fewer
    clicks. It might be technically identical to another “relevant” product with an
    image, but, to users, a product missing an image seems less trustworthy and thus
    won’t be clicked. Is that a bias or simply an actual indicator of relevance for
    this domain, where product trustworthiness is a factor?'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们的基于点击模型的判断可能会出现哪些问题？我们在第11章中看到了两个问题：*位置偏差*和*置信度偏差*。根据你的目标、用户体验和领域，可能会出现其他偏差。在电子商务中，用户可能会被吸引点击打折的商品，使数据偏向这些商品。在研究环境中，一篇文章可能在搜索结果中提供比另一篇文章更丰富的摘要。一些偏差模糊了“偏差”和该领域实际相关性之间的界限。例如，一个缺少图像的产品可能会得到更少的点击。它可能在技术上与另一个“相关”的产品相同，但用户可能会觉得缺少图像的产品不太可信，因此不会点击。这是偏差还是这个领域实际相关性的一个简单指标，其中产品可信度是一个因素？
- en: To make better judgments, should clicks be ignored or discounted, and should
    we instead use other behavioral signals? Perhaps follow-on actions after clicking,
    such as clicking a “like” button, adding an item to a cart, or clicking a “read
    more” button, ought to be included? Perhaps we should ignore “cheap” or accidental
    clicks when the user immediately hits the back button after clicking?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做出更好的判断，我们应该忽略或降低点击的重要性，而应该使用其他行为信号吗？也许应该包括点击后的后续行动，比如点击“喜欢”按钮、将商品添加到购物车或点击“阅读更多”按钮？也许我们应该忽略用户在点击后立即点击后退按钮的“便宜”或意外点击？
- en: Considering post-click actions can be valuable. However, we must ask how strongly
    search ranking influences events like a purchase or add-to-cart, or whether they
    are attributable to other factors. For example, a lack of purchases could indicate
    a problem with a product display page, or with a complex checkout process, not
    just with the search result’s relevance for a specific query.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑点击后的行为可能是有价值的。然而，我们必须问搜索排名对事件如购买或添加到购物车的影响有多强，或者它们是否归因于其他因素。例如，缺乏购买可能表明产品展示页面存在问题，或者复杂的结账流程存在问题，而不仅仅是特定查询的搜索结果的相关性。
- en: We might use an outcome like total purchases, in aggregate over all queries
    for each test group, to evaluate an A/B test. As long as all other variables in
    the app remain unchanged, except the ranking algorithm, then we know any significant
    difference in purchases across test groups must be caused by the one thing we
    changed. However, in the specific query-to-document relationship, causality gets
    complicated. Any single product may have very few purchases (many people view
    a $2,000 television, but very few buy). The data may simply lack enough quantity
    to know whether the purchase is exclusively related to a product’s specific relevance
    for a query.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会使用像总购买量这样的结果，在所有测试组的所有查询中汇总，来评估A/B测试。只要应用中的所有其他变量保持不变，除了排名算法，那么我们知道测试组之间购买量的任何显著差异必须是由我们改变的那一件事引起的。然而，在特定的查询到文档关系中，因果关系变得复杂。任何单一产品可能只有很少的购买（许多人看过2000美元的电视，但很少有人购买）。数据可能简单地缺乏足够的数量来知道购买是否仅与产品对查询的特定相关性有关。
- en: Accounting for all the variations in search UX, domains, and behaviors would
    fill many books and still fall short. The search space constantly evolves, with
    new ways of interacting with search results coming in and out of fashion. For
    most scenarios, using clicks and standard click models will suffice. Clicks in
    search UIs have been heavily studied. Still, arriving at good judgments is both
    an art and science; you may find a slight modification to a click model that accounts
    for extra signals is important to your domain and may provide tremendous gains
    in how your model performs in an A/B test. You can spend as much time perfecting
    your click model as you do developing your search engine.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到搜索用户体验、领域和行为的所有变化，可能需要填满许多书籍，但仍会不足。搜索空间不断演变，新的与搜索结果交互的方式不断兴起和衰落。对于大多数情况，使用点击和标准点击模型就足够了。搜索界面中的点击已经被深入研究。然而，得出良好的判断既是艺术也是科学；你可能会发现对点击模型进行轻微修改，以考虑额外的信号对你的领域很重要，并且可能会在A/B测试中显著提高你的模型性能。你可以在完善你的点击模型上花费的时间，与你开发搜索引擎所花费的时间一样多。
- en: 'However, there is one universally pernicious training data problem that challenges
    all click models: presentation bias. *Presentation bias* occurs when our models
    can’t learn what’s relevant from user clicks because the results never show up
    to be clicked in the first place. We’ll dive into this difficult problem next
    and learn how to overcome this bias by simultaneously optimizing both for what
    our models have already learned and for what they still need to learn.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个普遍有害的训练数据问题挑战了所有点击模型：展示偏差。*展示偏差*发生在我们的模型无法从用户点击中学习相关内容，因为相关结果从未显示出来以便被点击。接下来，我们将深入研究这个难题，并学习如何通过同时优化模型已经学习到的内容和它们还需要学习的内容来克服这种偏差。
- en: '12.3 Overcoming presentation bias: Knowing when to explore vs. exploit'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 克服展示偏差：知道何时探索与何时利用
- en: 'Regardless of whether we use clicks or more complex signals, users never interact
    with what they can’t see. In other words, underneath automated LTR is a chicken
    and egg problem: if the relevant result is never returned by the original, poorly
    tuned system, how could any click-based machine learning system learn that the
    result is relevant?'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们使用点击还是更复杂的信号，用户永远不会与那些他们看不到的东西互动。换句话说，在自动化的LTR（学习到排名）背后，存在一个“先有鸡还是先有蛋”的问题：如果相关结果从未被原始的、调优不良的系统返回，那么基于点击的机器学习系统如何能够学会该结果是相关的呢？
- en: In this section, you’ll learn about one machine learning technique that selects
    documents to explore *despite those results having no click data*. This final
    missing piece of our automated LTR system helps us not just build models optimized
    for the training data, but actively participates in its own learning to grow the
    breadth of the available training data. We call a system that participates in
    its own learning an *active learning* system.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解一种机器学习技术，该技术选择文档进行探索，尽管这些结果没有点击数据。我们自动化的LTR系统这个最后的缺失部分不仅帮助我们构建针对训练数据的优化模型，而且积极参与其自身的学习，以扩大可用训练数据的范围。我们将参与自身学习的系统称为*主动学习*系统。
- en: Figure 12.4 demonstrates presentation bias. The items on the right side of the
    figure could feasibly be relevant for our query, but, without traffic, we have
    no data to know either way. It would be nice to send some traffic to these results
    to learn whether users find them relevant.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4展示了展示偏差。图右侧的项目可能对我们的查询具有可行性，但，没有流量，我们无法知道任何一方面。如果能够向这些结果发送一些流量，以了解用户是否认为它们是相关的，那将是很不错的。
- en: '![figure](../Images/CH12_F04_Grainger.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F04_Grainger.png)'
- en: Figure 12.4 Presentation bias (versus unexplored results) for the query `dryer`
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.4 查询`dryer`的展示偏差（与未探索的结果）
- en: To overcome presentation bias, we must carefully balance *exploiting* our model’s
    current, hard-earned knowledge and *exploring* beyond that knowledge. This is
    the *explore versus exploit* tradeoff. Exploring lets us gain knowledge, growing
    the coverage of our click model to new and different types of documents. However,
    if we always explore, we will never take advantage of our knowledge. When we *exploit*,
    we optimize for what we currently know performs well. Exploiting corresponds to
    our current LTR model that aligns with our training data. Knowing how to systematically
    balance exploring and exploiting is key, and it’s something we’ll discuss in the
    next few sections using a machine learning tool (a Gaussian process) built for
    this purpose.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服展示偏差，我们必须仔细平衡*利用*我们模型当前辛苦获得的知识和*探索*超出这些知识。这是*探索与利用*的权衡。探索让我们获得知识，扩大我们的点击模型覆盖范围，涵盖新的和不同类型的文档。然而，如果我们总是探索，我们就永远不会利用我们的知识。当我们*利用*时，我们优化的是我们目前知道表现良好的内容。利用对应于我们的当前LTR模型，该模型与我们的训练数据一致。知道如何系统地平衡探索和利用是关键，我们将在接下来的几节中讨论这一点，我们将使用为此目的构建的机器学习工具（高斯过程）进行讨论。
- en: 12.3.1 Presentation bias in the RetroTech training data
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.1 RetroTech训练数据中的展示偏差
- en: 'Let’s first analyze the current training data to get the lay of the land. What
    kinds of search results does the training data lack? Where is our knowledge incomplete?
    Another way of saying “presentation bias” is that there are potentially relevant
    search results excluded from the training data: blind spots we must detect and
    fight against. Once we’ve defined those blind spots, we can better correct them.
    This will set us up to retrain with a more robust model.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先分析当前的训练数据，以了解情况。训练数据缺少哪些搜索结果？我们的知识在哪些方面不完整？另一种说法是“呈现偏差”，即可能相关的搜索结果被排除在训练数据之外：我们必须检测并对抗的盲点。一旦我们定义了这些盲点，我们就可以更好地纠正它们。这将为我们重新训练一个更稳健的模型奠定基础。
- en: 'In listing 12.5, we create a new feature set named `explore_feature_set`, in
    which we have three simple features: `long_description_match`, `short_description_match`,
    and `name_match`, telling us whether a given field match occurs or not. These
    correspond to features our model has already learned. In addition, we’ve added
    a `has_ promotion` feature. This feature becomes a `1.0` if the product is on
    sale and is being promoted through marketing channels. We haven’t explored this
    feature before; perhaps it’s a blind spot?'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表12.5中，我们创建了一个名为`explore_feature_set`的新特征集，其中包含三个简单的特征：`long_description_match`、`short_description_match`和`name_match`，告诉我们是否发生了给定的字段匹配。这些对应于我们的模型已经学习到的特征。此外，我们还添加了一个`has_promotion`特征。如果产品正在促销并通过营销渠道推广，则该特征变为`1.0`。我们之前还没有探索这个特征；也许它是一个盲点？
- en: Listing 12.5 Analyzing missing types of documents
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.5 分析缺失的文档类型
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Features corresponding to fields already used to train the LTR model.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 与已经用于训练LTR模型的字段相对应的特征。'
- en: '#2 New feature we’re exploring for blind spot: promotions'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们正在探索的盲点新特征：促销'
- en: '#3 Builds SDBN judgments from the current raw sessions'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从当前原始会话构建SDBN判断'
- en: '#4 Logs the feature values and returns the SDBN judgments joined with the feature
    values'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 记录特征值并返回与特征值结合的SDBN判断'
- en: '#5 Examines the properties of the current “transformers dvd” training data'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 检查当前“transformers dvd”训练数据的属性'
- en: 'Output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can see some gaps in our training data’s knowledge in the output of listing
    12.5:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在列表12.5的输出中看到训练数据知识的一些差距：
- en: Every item includes a name match.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个商品都包含一个名称匹配。
- en: No promotions (`has_promotion=0`) are present.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有促销（`has_promotion=0`）存在。
- en: There’s a range of `long_description_match` and `short_description_match` values.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在一系列`long_description_match`和`short_description_match`的值。
- en: Intuitively, if we want to expand our knowledge, we should show users searching
    for `transformers dvd` something completely outside the box from what’s in the
    output of listing 12.5\. That would mean showing users a promoted item, and possibly
    one with no name match. In other words, we need to get search out of its own echo
    chamber by explicitly diversifying what we show to the user, moving away from
    what’s in the training data. The only question is, how much of a risk are we willing
    to take to improve our knowledge? We don’t want to blanket the search results
    with random products just to broaden our training data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，如果我们想扩展我们的知识，我们应该向搜索`transformers dvd`的用户展示一些完全不同于列表12.5输出的内容。这意味着向用户展示一个促销商品，可能是一个没有名称匹配的商品。换句话说，我们需要通过明确地多样化我们向用户展示的内容，远离训练数据中的内容，来让搜索跳出自己的回音室。唯一的问题是，我们愿意承担多大的风险来改善我们的知识？我们不想用随机产品覆盖搜索结果，只是为了拓宽我们的训练数据。
- en: 'What we’ve done so far has not been systematic: we’ve only analyzed a single
    query to see what was missing. How might we automate this? Next up, we’ll discuss
    one method for automating exploration using a tool called a Gaussian process.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止所做的工作还没有系统化：我们只分析了一个查询，以查看缺少了什么。我们如何自动化这个过程？接下来，我们将讨论一种使用称为高斯过程的工具来自动化探索的方法。
- en: '12.3.2 Beyond the ad hoc: Thoughtfully exploring with a Gaussian process'
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.2 超越临时性：用高斯过程深思熟虑地探索
- en: A *Gaussian process* is a statistical model that makes predictions and provides
    a probability distribution capturing the certainty of that prediction. In this
    section, we’ll use a Gaussian process to select areas for exploration. Later in
    this chapter, we’ll create a more robust way of finding gaps in our data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*高斯过程*是一种统计模型，它做出预测并提供一个概率分布，捕捉该预测的确定性。在本节中，我们将使用高斯过程来选择探索的区域。在本章的后面部分，我们将创建一种更稳健的方法来寻找数据中的差距。'
- en: 'Gaussian process by example: Exploring a new river by exploiting existing knowledge'
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 以实例为例：通过利用现有知识探索一条新河流
- en: To get an intuition for Gaussian processes, let’s use a concrete example of
    real-life exploration. Working through this example will enable you to think more
    intuitively about how we might, mathematically, make explore versus exploit tradeoffs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对高斯过程有一个直观的了解，让我们用一个现实生活中的探索具体例子。通过这个例子，你将能够更直观地思考我们如何可能在数学上进行探索与利用的权衡。
- en: Imagine you’re a scientist planning to survey a rarely explored river deep in
    the wilderness. As you plan your trip, you have only spotty river depth observations
    from past expeditions to know when it’s safe to travel. For example, one observation
    shows the river is two meters deep in April; another time in August it’s one meter
    deep. You’d like to pick a date for your expedition that optimizes for ideal river
    conditions (i.e., not monsoon season, but also not during a dry spell). However,
    you’re also a scientist—you’d like to make observations during yet-unobserved
    times of the year to increase your knowledge of the river. Figure 12.5 shows river
    depth measurements made throughout the year.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一位科学家，计划对荒野深处的一条很少被探索的河流进行考察。在你规划旅行时，你只有来自过去探险的零星河流深度观测数据，知道何时旅行是安全的。例如，一次观测显示四月河流深度为两米；另一次在八月，深度为一米。你希望为你的探险选择一个日期，以优化理想的河流条件（即不是雨季，也不是干旱期）。然而，你也是一个科学家——你希望在一年中尚未观测到的时间进行观测，以增加你对河流的了解。图12.5显示了全年进行的河流深度测量。
- en: '![figure](../Images/CH12_F05_Grainger.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F05_Grainger.png)'
- en: Figure 12.5 Exploring a river, with uncertainty of the river’s depth increasing
    as we get further away from past observations. How would we pick a time of year
    that is both safe and maximally increases our knowledge of the river’s depth?
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.5 探索一条河流，随着我们远离过去的观测点，河流深度的不确定性逐渐增加。我们如何选择一个既安全又能最大限度地增加我们对河流深度了解的年份？
- en: 'How might we choose a date for the expedition? If you observed a river depth
    of two meters on April 14th, you would guess that the April 15th depth would be
    very close to two meters. Traveling during that time might be pleasant: you trust
    the river wouldn’t be excessively flooded. However, you wouldn’t gain much knowledge
    about the river. What about trying to go several months away from this observation,
    like January? January would be too far from April to understand the river’s likely
    depth. We might travel during a treacherous time of the year, but we’d almost
    certainly gain new knowledge—perhaps far more than we bargained for! With so little
    data to go on, there may be too much risk exploring at this time of the year.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何选择探险的日期呢？如果你在4月14日观测到河流深度为两米，你会猜测4月15日的深度将非常接近两米。在那个时间旅行可能会很愉快：你相信河流不会过度泛滥。然而，你不会从河流中获得很多知识。尝试在离这个观测几个月的1月去怎么样？1月离4月太远，无法了解河流的预期深度。我们可能会在一年中危险的时间旅行，但我们几乎肯定会获得新的知识——可能比我们预期的要多得多！由于数据如此之少，在这个时间旅行可能会有太多的风险。
- en: In figure 12.5 we see an educated guess at the river level, based on an expected
    correlation between adjacent dates (April 15th and 14th should be very close).
    Our level of certainty decreases as we move away from direct observations, represented
    by the widening gray zone.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在图12.5中，我们看到基于相邻日期之间预期相关性的河流水平面的合理猜测（4月15日和14日应该非常接近）。随着我们远离直接观测，我们的确定性降低，由扩大的灰色区域表示。
- en: Figure 12.5 illustrates a Gaussian process. It mathematically captures predictions,
    along with our uncertainty in each prediction. How does this relate to relevance
    ranking? Just as nearby dates have similar river levels, similar search results
    would be similar in their relevance. Consider our explore features from listing
    12.5\. Those with strong name matches for `transformers` `dvd`, that are not promoted,
    and with no short/long description matches would likely have similar relevance
    grades—all moderately relevant. As we move away from these well-trodden examples—perhaps
    adding promoted items—we grow less certain about our educated guesses. If we go
    very far, way outside the box, such as a search result with no name match, that
    is promoted, but that has strong short/long description field matches, our uncertainty
    grows very high. Just like the scientist considering a trip in January, we have
    almost no ability to make a good guess about whether those results could be relevant.
    It might involve too much risk to show those results to users.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 展示了一个高斯过程。它从数学上捕捉预测，以及我们对每个预测的不确定性。这与相关性排名有何关联？正如附近的日期有相似的河流水位，相似搜索结果在相关性上也会相似。考虑列表
    12.5 中的探索特征。那些与 `transformers` `dvd` 有强烈名称匹配，未被推广，且没有简短/长描述匹配的，可能具有相似的相关性等级——所有都是适度相关。当我们远离这些常见的例子——也许添加推广项目——我们对我们的有根据的猜测变得越来越不确定。如果我们走得很远，远远超出框外，比如一个没有名称匹配但被推广的搜索结果，并且有强烈的简短/长描述字段匹配，我们的不确定性会非常高。就像考虑一月份旅行的科学家一样，我们几乎没有能力做出关于这些结果是否相关的良好猜测。向用户展示这些结果可能涉及太多的风险。
- en: We can use Gaussian processes to balance exploiting existing relevance knowledge
    with riskier exploration to gain knowledge. Gaussian processes use incomplete
    information to make careful tradeoffs in the likely quality and the knowledge
    gained. For example, we can trade off ideal river conditions or a likely relevant
    search result against learning more about river conditions or about the relevance
    of a new kind of search result. We can carefully choose how far away from known,
    safe search results we want to venture to gain new knowledge.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用高斯过程来平衡利用现有的相关性知识与风险较高的探索以获得知识。高斯过程使用不完整的信息在可能的质量和获得的知识之间做出谨慎的权衡。例如，我们可以权衡理想的河流条件或可能的相关搜索结果，以换取更多关于河流条件或关于新类型搜索结果的相关性了解。我们可以谨慎地选择我们想要探索多远，以获得新的知识。
- en: In our `transformers` `dvd` case, what kind of search result would have a high
    upside, would likely also be relevant/safe to explore, but also would maximally
    increase our knowledge? Let’s train a Gaussian process and find out!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 `transformers` `dvd` 案例中，哪种搜索结果可能具有高收益，可能也是相关/安全的探索，同时也会最大限度地增加我们的知识？让我们训练一个高斯过程并找出答案！
- en: Training and analyzing a Gaussian process
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练和分析高斯过程
- en: Let’s get hands-on to see how a Gaussian process works. We’ll train a Gaussian
    process on the `transformers dvd` query. We’ll then use it to generate the best
    exploration candidates. You’ll see how we can score those exploration candidates
    to maximally reduce our risk and increase the likelihood we’ll gain knowledge.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们动手看看高斯过程是如何工作的。我们将在 `transformers dvd` 查询上训练一个高斯过程。然后我们将使用它来生成最佳的探索候选者。您将看到我们如何评分这些探索候选者，以最大限度地减少我们的风险并增加我们获得知识的可能性。
- en: In the following listing, we train a Gaussian process using the `GaussianProcessRegressor`
    (aka `gpr`) from `sklearn`. This code creates a Gaussian process that attempts
    to predict the relevance `grade` as a function of the `explore_feature_set` features
    we logged.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们使用 `sklearn` 中的 `GaussianProcessRegressor`（也称为 `gpr`）来训练高斯过程。此代码创建了一个尝试预测
    `explore_feature_set` 特征作为函数的相关性 `grade` 的高斯过程。
- en: Listing 12.6 Training a `GaussianProcessRegressor` on our training data
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.6 在我们的训练数据上训练 `GaussianProcessRegressor`
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Uses the features logged from the explore_feature_set in listing 12.5'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用来自列表 12.5 中 explore_feature_set 记录的特征'
- en: '#2 Predicts relevance grades'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 预测相关性等级'
- en: '#3 Creates and trains the gpr model'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 创建并训练 gpr 模型'
- en: Once we’ve trained a `GaussianProcessRegressor`, we can use it to make predictions.
    Remember that a `GaussianProcessRegressor` not only predicts a value, it also
    returns the probability distribution of that prediction. This helps us gauge the
    model’s certainty.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了一个 `GaussianProcessRegressor`，我们就可以用它来做出预测。记住，`GaussianProcessRegressor`
    不仅预测一个值，它还返回该预测的概率分布。这有助于我们衡量模型的确定性。
- en: In listing 12.7, we generate candidate feature values we’d like to possibly
    explore. With our river exploration example, these values correspond to possible
    exploration dates for our scientist’s expedition. In our case, as each feature
    can either be `0` or `1`, we look at each possible feature value as a candidate.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表12.7中，我们生成了我们想要探索的可能特征值。在我们的河流探索示例中，这些值对应于我们科学家探险的可能探索日期。在我们的情况下，由于每个特征可以是`0`或`1`，我们查看每个可能的特征值作为候选。
- en: Listing 12.7 Predicting a set of candidates to explore
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.7 预测一组探索候选者
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Generates a candidate matrix with a possible value of 0 or 1 for each feature
    we want to explore'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为我们想要探索的每个特征生成一个可能的值为0或1的候选矩阵'
- en: '#2 Predicts grade and standard deviation for those candidates based on the
    gpr probability distribution'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 根据gpr概率分布预测候选者的等级和标准差'
- en: '#3 Stores the predicted grade and standard deviation from the gpr'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 存储从gpr预测的等级和标准差'
- en: 'Output:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the output of listing 12.7, we see a `predicted_grade`—the educated guess
    from `gpr` on the relevance of that example. We also have `prediction_stddev`,
    which captures the gray band in figure 12.5—how much uncertainty is in the prediction.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表12.7的输出中，我们看到一个`predicted_grade`——`gpr`对示例相关性的合理猜测。我们还有`prediction_stddev`，它捕捉了图12.5中的灰色带——预测中的不确定性有多大。
- en: We note in the output of listing 12.7 that the standard deviation is near 0
    for the first four products with `name_match=1`. In other words, the `gpr` tends
    to have more confidence when `name_match=1`. We see after these observations that
    the standard deviation dramatically increases, as we lack a great deal of knowledge
    beyond these initial name-matching examples.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在列表12.7的输出中注意到，对于前四个`name_match=1`的产品，标准差接近0。换句话说，当`name_match=1`时，`gpr`往往更有信心。在这些观察之后，我们看到标准差急剧增加，因为我们缺乏这些初始名称匹配示例之外的大量知识。
- en: The output begins to show the presentation biases we intuitively detected in
    listing 12.5\. We find a tremendous amount of knowledge about the importance of
    name matches, but little knowledge about other cases. Which case would be worth
    exploring with live users and also minimizes the risk that we’ll show users something
    completely strange in the search results?
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输出开始显示出我们在列表12.5中直观检测到的展示偏差。我们发现关于名称匹配重要性的知识非常多，但关于其他情况的知识却很少。哪个案例值得与真实用户探索，同时还能最小化我们向用户展示搜索结果中完全陌生内容的风险？
- en: In listing 12.8, we generate and score exploration candidates using an algorithm
    called *expected improvement*, which predicts the candidates with the highest
    potential upside. We’ll only cover the basic details of this algorithm, so we
    recommend the article “Exploring Bayesian Optimization” by Agnihotri and Batra
    if you’d like to learn more ([https://distill.pub/2020/bayesian-optimization](https://distill.pub/2020/bayesian-optimization)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表12.8中，我们使用一种称为“预期改进”的算法生成和评分探索候选者，该算法预测具有最高潜在收益的候选者。如果您想了解更多信息，我们建议阅读Agnihotri和Batra的文章“探索贝叶斯优化”（[https://distill.pub/2020/bayesian-optimization](https://distill.pub/2020/bayesian-optimization)）。
- en: Listing 12.8 Calculating expected improvement for exploration
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.8 计算探索的预期改进
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Is the predicted grade likely to be above or below the typical grade?'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 预测等级可能高于或低于典型等级吗？'
- en: '#2 Probability we’ll improve over the mean, considering the amount of uncertainty
    in the prediction'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 考虑预测中的不确定性，我们改善平均值的概率'
- en: '#3 How much there is to gain, given the probability and improvement and the
    magnitude of the improvement.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 在概率、改进和改进幅度给定的情况下，可以获得的收益有多少。'
- en: '#4 Sorts to show the best exploration candidates'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 对候选者进行排序，以显示最佳的探索候选者'
- en: 'Output:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The higher the expected improvement, the higher the predicted upside for the
    exploration candidate. But how does the algorithm quantify this potential upside?
    It’s either the case that we know to a high degree of certainty there’s an upside
    (standard deviation is low and the predicted grade is high) or we know that there’s
    a high degree of uncertainty but the predicted grade is still high enough to take
    a gamble. We can see this in the following code from listing 12.8:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 预期改进越高，探索候选者的预测收益就越高。但算法如何量化这种潜在收益呢？要么是我们高度确信存在收益（标准差低且预测等级高），要么是我们知道存在高度不确定性，但预测等级仍然足够高，可以冒险。我们可以在列表12.8的以下代码中看到这一点：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'An opportunity that is more of a sure thing is covered by this first expression:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第一个表达式覆盖了一个更有把握的机会：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Meanwhile, an unknown opportunity with wide variability is covered by this
    second expression (after the `+`):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，这个第二个表达式（在`+`之后）覆盖了一个具有广泛变异性的未知机会：
- en: '[PRE18]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the first expression, you’ll notice that the opportunity (how much we expect
    to gain) times the probability that improvement happens corresponds to feeling
    confident in a good outcome. On the other hand, the second expression depends
    much more on the standard deviation. The higher the standard deviation *and* opportunity,
    the more likely it will be selected.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个表达式中，你会注意到机会（我们期望获得多少）乘以改进发生的概率对应于对良好结果的信心。另一方面，第二个表达式更多地依赖于标准差。标准差和机会越高，被选中的可能性就越大。
- en: 'We can calibrate our risk tolerance with a parameter called `theta`: the higher
    this value, the more we prefer candidates with a higher standard deviation. A
    high `theta` causes `opportunity` to diminish toward 0\. This biases scoring to
    the second expression—the unknown, higher standard deviation cases.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个名为`theta`的参数来校准我们的风险容忍度：这个值越高，我们越偏好标准差更高的候选者。高`theta`会导致`opportunity`向0减少。这会使评分偏向第二个表达式——未知、标准差更高的案例。
- en: If we set `theta` too high, our `gpr` selects candidates to learn about without
    considering whether they might be useful to the user. If `theta` is too low, we
    don’t explore new candidates very much and instead will be biased toward existing
    knowledge. A high `theta` is analogous to a scientist taking a high degree of
    risk (like exploring in January) in figure 12.5, while a very low `theta` corresponds
    to traveling during a risk-averse time (like mid-April). Because we’re using this
    algorithm to augment an existing LTR system, we chose `theta` of `0.6` (a bit
    high) to give us more knowledge.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将`theta`设置得太高，我们的`gpr`在选择学习候选者时不会考虑它们是否可能对用户有用。如果`theta`太低，我们不会探索很多新的候选者，而会偏向于现有知识。高`theta`类似于图12.5中科学家在1月份承担高风险（如探索）的情况，而非常低的`theta`则对应于在风险规避时期（如4月中旬）旅行。因为我们使用这个算法来增强现有的LTR系统，所以我们选择了`theta`为`0.6`（略高）以获得更多的知识。
- en: 'In the output of listing 12.8, we see that `gpr` confirms our earlier ad hoc
    analysis: we should show users items with promotions. These products will more
    likely yield greater knowledge, with possibly a high upside from the gamble.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表12.8的输出中，我们看到`gpr`证实了我们的早期临时分析：我们应该向用户展示具有推广的商品。这些产品更有可能产生更多的知识，可能从赌博中获得高额回报。
- en: Now that we have identified the kind of products we should explore, let’s gather
    products from the search engine to show users. The following listing shows how
    we might go about selecting products for exploration that we can later intersperse
    into the existing model’s search results.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了我们应该探索的产品类型，让我们从搜索引擎中收集产品以向用户展示。以下列表显示了我们可以如何选择用于探索的产品，这些产品我们可以在以后将其穿插到现有模型的搜索结果中。
- en: Listing 12.9 Selecting a product to explore from the search engine
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.9 从搜索引擎中选择要探索的产品
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Explores according to the provided explore vector and selects a random doc
    from that group'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 根据提供的探索向量进行探索，并从该组中随机选择一个文档'
- en: '#2 Extracts the best exploration candidate features based on expected_improvement'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 根据expected_improvement提取最佳的探索候选者特征'
- en: '#3 Searches for a candidate matching the criteria'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 搜索符合标准的候选者'
- en: 'Output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE20]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In listing 12.9, we take the best exploration candidate—promoted items—and issue
    a query to fetch documents with those characteristics. We omit the lower-level
    translation here of converting the candidate into a query (the `explore_query`
    function), but you can imagine that if `has_promotion=1.0` in the candidate, that
    we would issue a query searching for any item with a promotion (`has_promotion=true`),
    and so on for other features.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表12.9中，我们选取了最佳的探索候选者——推广的商品，并发出查询以获取具有这些特性的文档。我们在这里省略了将候选者转换为查询（`explore_query`函数）的较低级翻译，但你可以想象，如果候选者中的`has_promotion=1.0`，那么我们会发出一个搜索任何具有推广（`has_promotion=true`）的商品的查询，以及其他特征的类似情况。
- en: 'We see in the output of listing 12.9 that for the query `transformers dvd`,
    the randomly selected promoted product for exploration is 826663114164\. This
    corresponds to *Transformers: The Complete Series [25th Anniversary Matrix of
    Leadership Edition] [16 Discs] - DVD*. Interesting!'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在列表12.9的输出中看到，对于查询`transformers dvd`，随机选择的用于探索的推广产品是826663114164。这对应于*Transformers:
    The Complete Series [25th Anniversary Matrix of Leadership Edition] [16 Discs]
    - DVD*。有趣！'
- en: What should we do with this document? This comes down to a design decision.
    A common choice is to slot it into the third position of the results, as this
    allows the user to still see the previously most relevant results first, but also
    ensures that the explore result gets high visibility. Note our 826663114164 document
    in the third slot (`rank=2.0`).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何处理这份文档？这归结为一个设计决策。一个常见的做法是将它放入结果列表的第三个位置，这样用户仍然可以首先看到之前最相关的结果，同时也确保了探索结果获得高可见度。注意我们的
    826663114164 文档位于第三个位置（`rank=2.0`）。
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We’ve simulated many similar exploration sessions in the accompanying notebook.
    Since we don’t have actual live users hitting our app, the simulation code is
    implemented just for demonstration purposes so we can integrate explore candidates
    for active learning within our automated LTR system. In a real production environment,
    you would be showing results to real users instead of simulating user sessions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在附带的笔记本中模拟了许多类似的探索会话。由于我们没有实际的用户在使用我们的应用，模拟代码仅用于演示目的，以便我们可以在自动化的 LTR 系统中集成探索候选者进行主动学习。在实际的生产环境中，你会向真实用户展示结果，而不是模拟用户会话。
- en: 'Each session adds a random exploration candidate based on listing 12.9, simulates
    whether the added exploration result was clicked, and appends it to a new set
    of sessions: `sessions_with_exploration`. Recall that these sessions serve as
    the input we need to compute LTR training data (the SDBN-based judgments from
    chapter 11 that are generated in listing 12.1).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 每个会话都会根据列表 12.9 添加一个随机的探索候选者，模拟添加的探索结果是否被点击，并将其附加到新的会话集合中：`sessions_with_exploration`。回想一下，这些会话作为我们需要计算
    LTR 训练数据（第 11 章中生成的基于 SDBN 的判断，如列表 12.1 所示）的输入。
- en: Finally, we have the data needed to rerun our automated LTR training loop. We’ll
    see what happens with these examples added to our training data and how we can
    fit this exploration into the overall automated LTR algorithm.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有了重新运行我们的自动化 LTR 训练循环所需的数据。我们将看到这些示例添加到我们的训练数据中会发生什么，以及我们如何将这次探索整合到整体自动化
    LTR 算法中。
- en: 12.3.3 Examining the outcome of our explorations
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.3 检查我们探索的结果
- en: We’ve explored by showing some outside-the-box search results to (simulated)
    live users. We now have new sessions appended to the original session data stored
    in the `sessions_with_exploration` dataframe. In this section, we’ll run the session
    data through our automated LTR functions to regenerate training data and train
    a model. We’ll then run this new model in an A/B test to see the results.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过向（模拟的）真实用户展示一些非传统的搜索结果进行了探索。现在，我们有了新的会话附加到存储在 `sessions_with_exploration`
    数据框中的原始会话数据。在本节中，我们将运行会话数据通过我们的自动化 LTR 函数以再生训练数据并训练模型。然后，我们将运行这个新模型进行 A/B 测试以查看结果。
- en: As you’ll recall, our automated LTR helpers can regenerate training data using
    the `generate_training_data` function. We do this in listing 12.10, but this time
    with our augmented sessions that include exploration data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所回忆的，我们的自动化 LTR 辅助工具可以使用 `generate_training_data` 函数再生训练数据。我们在列表 12.10 中这样做，但这次使用的是包含探索数据的增强会话。
- en: Listing 12.10 Regenerating SDBN judgments from new sessions
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.10 从新会话中再生 SDBN 判断
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Output:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We see in listing 12.10’s output that a new product has been included. Note
    specifically the addition of 826663114164, *Transformers: The Complete Series
    [25th Anni- versary Matrix of Leadership Edition] [16 Discs] - DVD*. Interestingly,
    this movie has `has_promotion=true`, meaning it was one of the newly selected
    explore candidates from the previous section:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在列表 12.10 的输出中看到一个新的产品被包含在内。特别注意的是，826663114164 的添加，*Transformers: The Complete
    Series [25th Anniversary Matrix of Leadership Edition] [16 Discs] - DVD*。有趣的是，这部电影有
    `has_promotion=true`，这意味着它是上一节中从新选择的探索候选者之一：'
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It seems users are drawn to promoted products, so let’s move our `has_promotion`
    feature from the explore feature set to our main model and retrain to see the
    effect. In the following listing, we train a model with this new feature added
    to the mix to see the effect.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来用户被促销产品所吸引，所以让我们将 `has_promotion` 特征从探索特征集移动到我们的主模型中，并重新训练以查看效果。在下面的列表中，我们将训练一个包含这个新特征的模型以观察其效果。
- en: Listing 12.11 Rebuilding model using updated judgments
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.11 使用更新后的判断重建模型
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 Adding has_promotion to the feature set we’re training our model with'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将 has_promotion 添加到我们训练模型时使用的特征集'
- en: 'Evaluation for `ltr_model_variant_3`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对 `ltr_model_variant_3` 的评估：
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Wow! When comparing listing 12.11 to the earlier output from listing 12.3, we
    see that adding a promoted product to the training data creates a significant
    improvement in most cases in our offline test evaluation. The precision of `transformers
    dvd`, in particular, jumped significantly! If we issue a search for `transformers
    dvd`, we see this reflected in our data.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！当将列表12.11与列表12.3的早期输出进行比较时，我们看到在大多数情况下，将推广产品添加到训练数据中在我们的离线测试评估中产生了显著的改进。特别是`transformers
    dvd`的精确度显著提高！如果我们发出对`transformers dvd`的搜索，我们会在数据中看到这一点。
- en: Listing 12.12 Searching for `transformers dvd` using the latest model
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.12 使用最新模型搜索`transformers dvd`
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Output:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE28]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: However, we know great-looking test results don’t always translate to the real
    world. What happens when we rerun the A/B test from listing 12.4? If you recall,
    we created an `a_b_test` function that randomly selects a model for a user’s search.
    If the results contained an item the user secretly wanted to purchase, a purchase
    would likely occur. If we use this function to resimulate an A/B test, we see
    our new model appears to have hit the jackpot!
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们知道看起来很好的测试结果并不总是能转化为现实世界。当我们重新运行列表12.4中的A/B测试时会发生什么？如果你还记得，我们创建了一个`a_b_test`函数，该函数随机为用户的搜索选择一个模型。如果结果包含用户秘密想要购买的项目，那么购买很可能会发生。如果我们使用这个函数来重新模拟A/B测试，我们看到我们的新模型似乎已经中了大奖！
- en: Listing 12.13 Rerunning A/B test on new `ltr_model_variant_3` model
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.13 在新的`ltr_model_variant_3`模型上重新运行A/B测试
- en: '[PRE29]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Output:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE30]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now we see that the new model (`ltr_model_variant_3`) has significantly outperformed
    the old model (`ltr_model_variant_1`) in the A/B test. We now know that not only
    did our exploration help us find a theoretical gap in the training data, but that
    when we tested the new model in a real-world scenario for our target query (`transformers`
    `dvd`), it performed significantly better than the old “exploit-only” model. While
    we focused on a particular query in this chapter, the same process can be applied
    across many queries and exploration candidates to continue to automatically refine
    your LTR model with active learning.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们看到新模型（`ltr_model_variant_3`）在A/B测试中显著优于旧模型（`ltr_model_variant_1`）。我们现在知道，我们的探索不仅帮助我们发现了训练数据中的理论差距，而且当我们在针对目标查询（`transformers`
    `dvd`）的真实世界场景中测试新模型时，它的表现比旧的“仅利用”模型要好得多。虽然我们在这章中关注了一个特定的查询，但同样的过程可以应用于许多查询和探索候选者，以继续使用主动学习自动优化你的LTR模型。
- en: We’ve now implemented an automated LTR system that not only relearns from the
    latest user signals, but that uses active learning to explore what else might
    be relevant to live users and then collects the corresponding signals measuring
    their feedback. This active learning process helps remove blind spots in the training
    data on an ongoing basis.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经实施了一个自动化的LTR系统，它不仅从最新的用户信号中重新学习，而且还使用主动学习来探索可能对活跃用户相关的其他内容，然后收集相应的信号来衡量他们的反馈。这个主动学习过程有助于持续地消除训练数据中的盲点。
- en: '12.4 Exploit, explore, gather, rinse, repeat: A robust automated LTR loop'
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 利用、探索、收集、清洗、重复：一个健壮的自动化LTR循环
- en: With the final pieces in place, we see how exploring new features helps us to
    overcome presentation bias. Feature exploration and training data exploration
    go hand-in-hand, as we learn our presentation biases by understanding the features
    we lack and may need to engineer into search. In this chapter, we used a simple
    example with “promotions”, but what other, more complex, features might show blind
    spots in your training data? In this section, let’s conclude by augmenting our
    automated LTR algorithm from chapter 11 to include not just training a model with
    previous click-model-based training data, but also this new active learning approach
    to explore beyond the training data’s current scope.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有最终部分就绪时，我们看到探索新特性如何帮助我们克服展示偏差。特性探索和训练数据探索是相辅相成的，因为我们通过理解我们所缺乏的特性以及可能需要将其工程化到搜索中的特性，来学习我们的展示偏差。在本章中，我们使用了一个简单的“促销”例子，但还有哪些更复杂的功能可能会在你的训练数据中显示出盲点？在本节中，让我们通过增强第11章中的自动化LTR算法来结束，不仅包括使用基于先前点击模型训练数据的模型训练，还包括探索训练数据当前范围之外的新主动学习方法。
- en: 'Our new auto-exploring automated LTR algorithm can be summarized in these three
    main steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们新的自动探索自动化LTR算法可以总结为以下三个主要步骤：
- en: '*Exploit*—Use known features and train the LTR model for ranking using existing
    training data.'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*利用*—使用已知特性，并使用现有训练数据训练LTR模型以进行排名。'
- en: '*Explore*—Generate hypothesized, “explore” features to eliminate training data
    blind spots.'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*探索*—生成假设的、“探索”特性，以消除训练数据盲点。'
- en: '*Gather*—With a deployed model and a trained `gpr` model, show explore/exploit
    search results, and gather clicks to build judgments.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*收集*——使用部署的模型和训练好的`gpr`模型，展示探索/利用搜索结果，并收集点击以建立判断。'
- en: We can summarize the past three chapters by combining them into listing 12.14,
    shown next. This listing puts all the pieces together (with some internals omitted).
    Our main decision points in this algorithm are the features used to explore and
    exploit. We can also go under the hood to change the chosen click model, the LTR
    model architecture, and our risk tolerance (the `theta` parameter).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将它们合并到列表12.14中，总结过去三章的内容，如以下所示。此列表将所有部分组合在一起（省略了一些内部细节）。我们在这个算法中的主要决策点是用于探索和利用的特征。我们还可以深入了解，更改选择的点击模型、LTR模型架构以及我们的风险容忍度（`theta`参数）。
- en: Listing 12.14 Summarizing the fully automated LTR algorithm
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.14 总结完全自动化的LTR算法
- en: '[PRE31]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#1 Returns the model once once per day'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 每天返回一次模型'
- en: '#2 Trains the LTR model on known good features and current training data'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在已知良好特征和当前训练数据上训练LTR模型'
- en: '#3 Collects the new sessions and repeats the process'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 收集新会话并重复该过程'
- en: '#4 Evaluates the current explore variant and promotes it if it’s better than
    the current explore model'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 评估当前探索变体，如果它比当前探索模型更好，则提升它'
- en: '#5 Hypothesizes new features to explore for blind spots'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 假设新的特征以探索盲点'
- en: '#6 Collects user signals until it is time to retrain the model'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 收集用户信号，直到重新训练模型的时间'
- en: '#7 Collects the new sessions and repeats the process'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 收集新会话并重复该过程'
- en: In this loop, we capture a better automated LTR process. We actively learn our
    training data’s blind spots by theorizing the features that might be behind them.
    In letting the loop run, we can observe its performance and decide when to promote
    “explore” features to the full production “exploit” feature set. As we retire
    old click data, we can also note when old features no longer matter and when new
    features become important due to trends and seasonality. Our implementation uses
    a hand-tuned exploit-and-explore feature set to keep our search relevance team
    in control of feature engineering, but you could certainly write an algorithm
    to generate new features or use deep learning to discover latent features or use
    some other approach based on existing content attributes.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个循环中，我们捕捉到一个更好的自动LTR过程。我们通过理论化可能隐藏在背后的特征来主动学习我们的训练数据的盲点。在让循环运行的同时，我们可以观察其性能并决定何时将“探索”特征提升到完整的生产“利用”特征集。当我们淘汰旧的点击数据时，我们还可以注意旧特征何时不再重要，以及何时由于趋势和季节性，新特征变得重要。我们的实现使用手动调优的利用和探索特征集来保持我们的搜索相关性团队对特征工程的控制，但你当然可以编写一个生成新特征的算法，或使用深度学习来发现潜在特征，或使用基于现有内容属性的某些其他方法。
- en: Taken together, these algorithms provide a robust mechanism for approaching
    an ideal ranking, considering a full spectrum of options that could be shown to
    users. They let you choose new features to investigate blind spots, arriving at
    a relevance algorithm that maximizes what users prefer to see in their search
    results.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些算法提供了一种稳健的机制，以全面考虑可能向用户展示的选项范围，从而接近理想的排名。它们让您可以选择新的特征来调查盲点，达到一个最大化用户在搜索结果中希望看到的关联算法。
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Performing well in an offline test shows that our features can approximate the
    training data. However, that’s no guarantee of success. An A/B test can show us
    situations where the training data itself was misleading.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在离线测试中表现良好表明我们的特征可以近似训练数据。然而，这并不能保证成功。A/B测试可以向我们展示训练数据本身可能具有误导性的情况。
- en: Training data must be monitored for biases and carefully corrected.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据必须监控偏差并仔细纠正。
- en: Presentation bias is one of search’s most pernicious relevance problems. Presentation
    bias happens when our models can’t learn what’s relevant from user clicks because
    the results never show up to be clicked in the first place.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示偏差是搜索中最有害的相关性问题之一。展示偏差发生在我们的模型无法从用户点击中学习相关内容，因为结果从未显示出来供点击。
- en: We can overcome presentation bias by making the automated LTR process an active
    participant in finding blind spots in the training data. Models that do this participate
    in active learning.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过使自动LTR过程成为寻找训练数据盲点的积极参与者来克服展示偏差。执行此操作的模型参与主动学习。
- en: A Gaussian process is one way to select promising opportunities for exploration.
    Using a set of features, we can find what’s missing in the training and select
    new items to show to users based on which items are likely to provide the most
    useful new data points for continued learning. We can experiment with different
    ways of describing the data via features to find new and interesting blind spots
    and areas of investigation.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯过程是选择有前景的探索机会的一种方法。通过一组特征，我们可以找出训练中的缺失部分，并根据哪些项目可能为持续学习提供最有用的新数据点来选择新的项目展示给用户。我们可以通过不同的方式通过特征来描述数据，以寻找新的有趣盲点和调查领域。
- en: When we put together exploiting existing knowledge with exploring blind spots,
    we have a more robust, automated LTR system—reflected intelligence that can automatically
    explore and exploit features with little internal maintenance.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们将利用现有知识与探索盲点相结合时，我们便拥有了一个更稳健、自动化的LTR系统——反映智能，它可以自动探索和利用特征，且几乎不需要内部维护。
