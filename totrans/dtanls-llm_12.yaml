- en: 10 Software frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building applications with LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving complex tasks with agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying data with LlamaIndex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to now, we’ve mostly been using OpenAI’s Python library to interact with
    language models. This library offers basic functionality for sending prompts and
    retrieving answers from GPT and other OpenAI models (as well as options for tuning
    and fine-tuning). The libraries from other providers, such as Anthropic and Cohere,
    offer similar functionality. As long as your data-analysis tasks are simple, this
    is probably all you need. However, what if your data analysis requires a complex
    multistep pipeline, possibly integrating many different data formats?
  prefs: []
  type: TYPE_NORMAL
- en: 'At that point, you may want to switch to a more powerful software framework.
    Several higher-level frameworks for building complex applications on top of language
    models are currently emerging. In this chapter, we’ll discuss two of the most
    popular contenders: LangChain and LlamaIndex. The former is a general framework
    for building applications using large language models. What’s more, it comes with
    various useful built-in components that implement popular use cases for language
    models. LlamaIndex, on the other hand, specifically supports use cases where language
    models need to interact with large data sets.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the hang of it, we’ll first write a simple text-classification pipeline
    using LangChain. Then we’ll explore some of the advanced features of LangChain.
    More precisely, we’ll see how LangChain supports *agents* on top of language models.
    Creating an agent means putting the language model itself into the driver’s seat,
    giving it lots of freedom on how to accomplish a given task while using a collection
    of tools provided by the user. We will use such agents to solve complex data-analysis
    tasks independently, using a mix of tools to access different data sources. Next,
    we’ll see how LlamaIndex easily ingests large amounts of data in diverse data
    formats and makes them usable for language models. Internally, it uses cheap language
    models to map data snippets and analysis tasks to vector representations, after
    which it maps tasks to data based on the similarity between those vectors. Finally,
    we’ll compare the two frameworks and discuss tradeoffs between those frameworks
    and the libraries offered by OpenAI and other language model providers.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to create a complex application based on language models, you should
    probably check out LangChain. The framework launched in October 2022 and has been
    gaining popularity quickly (leading to the creation of a corresponding startup
    in April 2023). At the time of writing, LangChain is still developing rapidly.
    Be sure to run the code in this section with the right LangChain version (because
    future versions may change the interfaces).
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, LangChain relates to language models (Lang) and chains.
    In LangChain terminology, a *chain* is simply a sequence of steps. Each step may
    correspond to the invocation of a language model, a data-processing step, or the
    invocation of an arbitrary tool. The important point here is that we no longer
    assume that a single call to a language model will solve our problem (which was
    the case for most of the scenarios we have discussed in this book). Instead, we
    assume that we need a complex network of connected components. That’s the scenario
    where LangChain shines!
  prefs: []
  type: TYPE_NORMAL
- en: 'To use LangChain, you first need to install it. Go to a terminal, and run the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As we mentioned, you need to install the right LangChain version if you want
    to run the following code samples! LangChain is currently changing so quickly
    that the code may not work with a different version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond the LangChain core, you may want to install libraries that support language
    models from specific providers. In the following sections, we’ll be using OpenAI’s
    models. Run the following command in the terminal (and, again, make sure to use
    the version specified):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Support for other providers, such as Anthropic and Cohere, is equally available.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, that’s it! After running these commands, you’re ready to run the sample
    projects discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Classifying reviews with LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first projects we did was text analysis with language models. Remember
    chapter 4? We used language models to classify reviews based on the underlying
    sentiment (is this a recommendation or a warning?). We’ll do the same here; we
    will just use LangChain in our code. Comparing the LangChain code with the original
    should give you a first impression of how LangChain can help simplify building
    applications with language models.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will create a chain to classify text documents. A LangChain chain may involve
    many steps, each implemented by invoking a language model or a generic Python
    function (e.g., to parse the results of language model invocations into a standardized
    format). The term *chain* is actually slightly misleading. Although you may imagine
    a chain as a sequence of consecutive steps, the chains in LangChain are much more
    powerful. For instance, they may involve parallel steps as well as conditional
    execution. However, for the simple text-classification application, we won’t need
    such advanced features. Instead, we will restrict ourselves to a simple chain
    with just a few steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our chain will integrate several standard components offered by LangChain.
    The first component in our chain is a prompt template. As in chapter 4, this template
    describes the classification task and the expected output format. You may wonder
    what has changed compared to the previous code version. After all, we have been
    discussing prompt templates all along. The difference is that LangChain introduces
    a dedicated class to represent prompt templates. This class offers various convenience
    functions for prompt templates: for example, for creating and instantiating them.
    At the same time, LangChain offers a hub allowing users to upload and download
    prompt templates (as well as many other components). In our simple scenario, we
    won’t need any of these advanced features. Instead, we just need to instantiate
    our prompt template by passing a single parameter (the text to classify) as input.'
  prefs: []
  type: TYPE_NORMAL
- en: The second step in our chain is a language model. Again, we have been using
    language models throughout this book, but LangChain adds several helpful functions
    on top of the language model object. For instance, it is easy to automatically
    log all language model invocations, and LangChain offers convenience functions
    for different invocation scenarios (e.g., batch and stream processing). Again,
    we won’t use those advanced features here. Instead, we will pass the prompt (the
    first step in our chain) to the language model to generate a reply.
  prefs: []
  type: TYPE_NORMAL
- en: The third step in our chain is a parser, extracting the answer string from the
    reply generated by the language model. You may remember from chapter 3 that OpenAI’s
    language models generate detailed replies, integrating one or multiple answers
    as well as various types of metadata (e.g., information about token usage). The
    parser automatically extracts the answer string we’re looking for from the result
    object (which works for OpenAI models as well as for all other providers). The
    result of the pipeline is a single token indicating whether the input review is
    a recommendation. Figure [10.1](#fig__LCclassification) illustrates the three
    steps of this pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F01_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 Components in our LangChain classification chain
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 10.2.2 Creating a classification chain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Time to implement our chain in Python! First we need a prompt template. We
    use the same template as in chapter 4, but this time, we use LangChain’s `ChatPromptTemplate`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Text placeholder'
  prefs: []
  type: TYPE_NORMAL
- en: You may notice the reference to chat models (after all, the class we’re instantiating
    is called `ChatPromptTemplate`). As discussed in chapter 3, chat models process
    a history of prior messages rather than a single input message. Many of the most
    recently released models are chat models. In LangChain, chat models require a
    specialized prompt template (which instantiates into a sequence of messages rather
    than a single text). This is the type of template we’re creating here. The template
    is the same as we used in chapter 4\. It contains a placeholder (**1**) for the
    input text to classify. We generally use curly braces (`{}`) to mark placeholders
    in prompt templates; they are replaced with concrete values when instantiating
    the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we need a language model to process prompts. The following code instantiates
    the GPT-4o model from OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ChatOpenAI` class covers all chat models by OpenAI. It is imported from
    the `langchain_openai` package, featuring functionality to support the use of
    OpenAI models in LangChain. Other providers, such as Anthropic and Cohere, have
    their own associated packages offering comparable functionality for their models
    (note that you need to install those packages separately via pip). The parameters
    in the constructor of `ChatOpenAI` may seem familiar: we choose the model (`gpt-4o`),
    set `temperature` to 0 (to reduce the degree of randomness in the output), and
    limit the maximum number of output tokens to one (because both possible class
    labels, `Positive` and `Negative`, consist of a single token).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, we need to extract the answer string from the (more detailed) reply
    of our language model. That’s easy to do with `StrOutputParser`. LangChain output
    parsers implement a wide range of transformations on the output of a model invocation.
    In this case, we only need a very simple transformation, extracting the desired
    answer string. The following piece of code creates a corresponding parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we’ll put all the components together in a chain. To do so, we can
    use the LangChain Expression Language (LCEL). If you’re a Linux user, the following
    syntax should look familiar to you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To use the output of an operation as input for the following step, we connect
    them with the pipe symbol (`|). The command creates a chain that connects the
    previously mentioned components. In addition, it specifies the input that the
    chain expects. In our case, the prompt template has a placeholder for the text
    to classify.`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6] inputs = [''This movie is great!'', ''This movie is bad!''] outputs
    = chain.batch(inputs) [PRE7] from langchain_openai import ChatOpenAI from langchain_core.prompts.chat
    import ChatPromptTemplate from langchain_core.output_parsers.string import StrOutputParser
    from langchain_core.runnables.passthrough import RunnablePassthrough  import argparse
    import pandas as pd  def create_chain():                         #1     """ Creates
    chain for text classification.          Returns:         a chain for text classification.     """     prompt
    = ChatPromptTemplate.from_template(  #2         ''{text}\n''         ''Is the
    sentiment positive or negative?\n''         ''Answer ("Positive"/"Negative")\n'')     llm
    = ChatOpenAI(              #3         model=''gpt-4o'', temperature=0,          max_tokens=1)     parser
    = StrOutputParser()  #4      #5     chain = ({''text'':RunnablePassthrough()}
    | prompt | llm | parser)     return chain  if __name__ == ''__main__'':          parser
    = argparse.ArgumentParser()     parser.add_argument(''file_path'', type=str, help=''Path
    to input file'')     args = parser.parse_args()          df = pd.read_csv(args.file_path)  #6      chain
    = create_chain()            #7     results = chain.batch(list(df[''text'']))  #8      df[''class'']
    = results  #9     df.to_csv(''result.csv'') [PRE8] python listing1.py reviews.csv
    [PRE9] pip install langchainhub==0.1.15 [PRE10] from langchain import hub prompt
    = hub.pull(''hwchase17/react'') print(prompt.template) [PRE11]  #1 Answer the
    following questions as best you can.  You have access to the following tools:  {tools}  #2  Use
    the following format:  #3  Question: the input question you must answer Thought:
    you should always think about what to do Action: the action to take, should be
    one of [{tool_names}] Action Input: the input to the action Observation: the result
    of the action ... (this Thought/Action/Action Input/Observation can repeat N times)
    Thought: I now know the final answer Final Answer: the final answer to the original
    input question  Begin!  Question: {input}      #4 Thought:{agent_scratchpad}  #5
    [PRE12] from langchain_openai import ChatOpenAI llm = ChatOpenAI(     temperature=0,
    model=''gpt-4o'') [PRE13] from langchain_community.utilities.sql_database import
    SQLDatabase db = SQLDatabase.from_uri(f''sqlite:///{dbpath}'') [PRE14] pip install
    google-search-results==2.4.2 [PRE15] from langchain.agents.load_tools import load_tools
    extra_tools = load_tools(     [''serpapi''], serpapi_api_key=serpaikey, llm=llm)
    [PRE16] from langchain_community.agent_toolkits.sql.base import create_sql_agent
    agent = create_sql_agent(     llm=llm, db=db, verbose=True,     agent_type=''openai-tools'',     extra_tools=extra_tools)
    [PRE17] agent.invoke({''input'':task}) [PRE18] import argparse  from langchain.agents.load_tools
    import load_tools from langchain_community.utilities.sql_database import SQLDatabase
    from langchain_community.agent_toolkits.sql.base import create_sql_agent from
    langchain_openai import ChatOpenAI  if __name__ == ''__main__'':      parser =
    argparse.ArgumentParser()     parser.add_argument(''serpaikey'', type=str, help=''SERP
    API access key'')     parser.add_argument(''dbpath'', type=str, help=''Path to
    SQLite database'')     parser.add_argument(''question'', type=str, help=''A question
    to answer'')     args = parser.parse_args()          llm = ChatOpenAI(              #1         temperature=0,
    model=''gpt-4o'')      #2     db = SQLDatabase.from_uri(f''sqlite:///{args.dbpath}'')     extra_tools
    = load_tools(                         #3         [''serpapi''], serpapi_api_key=args.serpaikey,
    llm=llm)     agent = create_sql_agent(     #4         llm=llm, db=db, verbose=True,         agent_type=''openai-tools'',         extra_tools=extra_tools)     agent.invoke({''input'':args.question})  #5
    [PRE19] python listing2.py [SerpAPI key] games.db ↪ ''What was the most sold game
    in 2016, and how is it played?'' [PRE20] [1m> Entering new SQL Agent Executor
    chain...[0m [32;1m[1;3m   #1 Invoking: `sql_db_list_tables` with `{''tool_input'':
    "}` [0m[38;5;200m[1;3mgames[0m[32;1m[1;3m  #2 Invoking: `sql_db_schema` with `{''table_names'':
    ''games''}`  [0m[33;1m[1;3m CREATE TABLE games (     rank INTEGER,      name TEXT,      platform
    TEXT,      year INTEGER,      genre TEXT,      publisher TEXT,      americasales
    NUMERIC,      eusales NUMERIC,      japansales NUMERIC,      othersales NUMERIC,      globalsales
    NUMERIC )  /* 3 rows from the games table: rank    name    platform    year    genre    publisher     americasales    eusales    japansales    othersales    globalsales
    1    Wii Sports    Wii    2006    Sports    Nintendo    41.4900000000     29.0200000000    3.7700000000    8.4600000000    82.7400000000
    2    Super Mario Bros.   NES   1985   Platform   Nintendo   29.0800000000     3.5800000000    6.8100000000    0.7700000000    40.2400000000
    3    Mario Kart Wii    Wii    2008    Racing    Nintendo    15.8500000000     12.8800000000    3.7900000000    3.3100000000    35.8200000000
    */[0m[32;1m[1;3m  #3 Invoking: `sql_db_query_checker` with `{''query'': ''SELECT
    name  FROM games WHERE year = 2016 ORDER BY globalsales DESC LIMIT 1''}` responded:
    The games table contains the information we need.  I will query for the game with
    the highest global sales in 2016. [0m[36;1m[1;3mSELECT name FROM games WHERE year
    = 2016  ORDER BY globalsales DESC LIMIT 1[0m[32;1m[1;3m  #4 Invoking: `sql_db_query`
    with `{''query'': ''SELECT name  FROM games WHERE year = 2016 ORDER BY globalsales
    DESC LIMIT 1''}` [0m[36;1m[1;3m[(''FIFA 17'',)][0m[32;1m[1;3m  #5 Invoking: `Search`
    with `How to play FIFA 17` [0m[33;1m[1;3m["A Beginner''s Guide To Complete FIFA
    17 Domination ... The  main steps you should take are to jump right in with a
    quick play game. ... EA Sports FIFA ...", ''Play FIFA 17 up to 5 days before launch
    for a full  10 hours when you join EA Access on Xbox One and Origin Access on
    PC.'',  "1\. Shield the ball in 360 degrees · 2\. Use Driven Shots and Driven
    Headers ·  3\. Use set piece upgrades to score with style · 4\. Make Fifa 17''s
    ...",  ''Play FIFA 17 as much as you want with EA Access or Origin Access  for
    only $4.99 per month. Now available in The Vault.'',  ''Cautiously Start An Online
    Match. Score Early After Some Self-Proclaimed  Beautiful Build Up Play. Concede
    4 Goals In A Row And Convince ...'',  ''FIFA 17 TUTORIALS & ULTIMATE TEAM ➞  Twitter:
    https://twitter.com/KrasiFIFA ➞  Instagram: http://instagram.com/KrasiFIFA How
    I record my ...'',  "Draft mode is another way to play FIFA Ultimate Team,  giving
    you the ability to play with Players you don''t own.  You''ll have the opportunity
    to draft a random ..."]  #6 [0m[32;1m[1;3mThe most sold game in 2016 was FIFA
    17\.   To play FIFA 17, you can follow these steps:  1\. Jump right in with a
    quick play game. 2\. Shield the ball in 360 degrees. 3\. Use Driven Shots and
    Driven Headers. 4\. Use set piece upgrades to score with style. 5\. Start an online
    match cautiously. 6\. Score early after some self-proclaimed beautiful build-up
    play. 7\. Draft mode is another way to play FIFA Ultimate Team,  giving you the
    ability to play with players you don''t own.  You''ll have the opportunity to
    draft a random team.  Remember, practice makes perfect![0m  [1m> Finished chain.[0m
    [PRE21] SELECT name FROM games WHERE year = 2016 ORDER BY globalsales DESC LIMIT
    1 [PRE22] import argparse  from langchain.agents.load_tools import load_tools
    from langchain.tools import tool from langchain_community.utilities.sql_database
    import SQLDatabase from langchain_community.agent_toolkits.sql.base import create_sql_agent
    from langchain_openai import ChatOpenAI from typing import Union  @tool  #1  #2
    def convert_currency(USD_amount: float, currency: str)->Union[float, str]:      #3     """
    Converts an amount in US dollars to another currency.          Args:         USD_amount:
    amount in US dollars.         currency: name of target currency (e.g., "Yen").          Returns:         input
    amount in target currency.     """     conversion_factors = {         ''Euro'':0.93,
    ''Yen'':151.28, ''Yun'':0.14,          ''Pound'':1.26, ''Won'':0.00074, ''Rupee'':0.012}          if
    currency not in conversion_factors:          #4         error_message = (             f''Unknown
    currency: {currency}!''             f''Use one of {conversion_factors.keys()}'')         return
    error_message       #5     conversion_factor = conversion_factors[currency]     converted_amount
    = USD_amount * conversion_factor     return converted_amount  if __name__ == ''__main__'':      parser
    = argparse.ArgumentParser()     parser.add_argument(''serpaikey'', type=str, help=''SERP
    API access key'')     parser.add_argument(''dbpath'', type=str, help=''Path to
    SQLite database'')     parser.add_argument(''question'', type=str, help=''A question
    to answer'')     args = parser.parse_args()          llm = ChatOpenAI(         temperature=0,
    model=''gpt-4o'')     db = SQLDatabase.from_uri(f''sqlite:///{args.dbpath}'')     extra_tools
    = load_tools(         [''serpapi''], serpapi_api_key=args.serpaikey, llm=llm)      #6     extra_tools.append(convert_currency)      agent
    = create_sql_agent(         llm=llm, db=db, verbose=True,         agent_type=''openai-tools'',         extra_tools=extra_tools)     agent.invoke({''input'':args.question})
    [PRE23] python listing3.py [SerpAPI key] games.db  ''What revenue was generated
    by computer games in 2015?  How much is it in Yen?'' [PRE24] [1m> Entering new
    SQL Agent Executor chain...[0m [32;1m[1;3m  #1 Invoking: `sql_db_list_tables`
    with `''tool_input'': "` [0m[38;5;200m[1;3mgames[0m[32;1m[1;3m  #2 Invoking: `sql_db_schema`
    with `''table_names'': ''games''` [0m[33;1m[1;3m CREATE TABLE games (     rank
    INTEGER,      name TEXT,      platform TEXT,      year INTEGER,      genre TEXT,      publisher
    TEXT,      americasales NUMERIC,      eusales NUMERIC,      japansales NUMERIC,      othersales
    NUMERIC,      globalsales NUMERIC )  /* 3 rows from the games table: rank    name    platform    year    genre    publisher    americasales
    eusales    japansales    othersales    globalsales 1    Wii Sports    Wii    2006    Sports    Nintendo    41.4900000000     29.0200000000    3.7700000000    8.4600000000    82.7400000000
    2    Super Mario Bros.   NES   1985   Platform   Nintendo   29.0800000000     3.5800000000    6.8100000000    0.7700000000    40.2400000000
    3    Mario Kart Wii    Wii    2008    Racing    Nintendo    15.8500000000     12.8800000000    3.7900000000    3.3100000000    35.8200000000
    */[0m[32;1m[1;3m  #3 Invoking: `sql_db_query_checker` with `{''query'': ''SELECT
    SUM(globalsales) as  total_revenue FROM games WHERE year = 2015''}` responded:
    The "games" table contains the information we need.  The "globalsales" column
    represents the global revenue generated by  each game. We can sum this column
    for the games released in 2015 to  get the total revenue. Let''s write and check
    the SQL query.  [0m[36;1m[1;3mSELECT SUM(globalsales) as total_revenue  FROM games
    WHERE year = 2015[0m[32;1m[1;3m  #4 Invoking: `sql_db_query` with `{''query'':
    ''SELECT SUM(globalsales) as  total_revenue FROM games WHERE year = 2015''}`  [0m[36;1m[1;3m[(264.43999999999795,)][0m[32;1m[1;3m  #5
    Invoking: `convert_currency` with `{''USD_amount'':  264.43999999999795, ''currency'':
    ''Yen''}`  [0m[38;5;200m[1;3m40004.48319999969[0m[32;1m[1;3m  #6 The total revenue
    generated by computer games in 2015  was approximately $264.44 million.  In Japanese
    Yen, this is approximately ¥40,004,483,200.[0m  [1m> Finished chain.[0m [PRE25]
    pip install llama-index==0.10.25 [PRE26] pip install torch==2.1.2 pip install
    transformers==4.36.0 pip install python-pptx==0.6.23 pip install Pillow==10.2.0
    [PRE27] import argparse import openai  from llama_index.core import VectorStoreIndex,
    SimpleDirectoryReader  if __name__ == ''__main__'':      #1     parser = argparse.ArgumentParser()     parser.add_argument(''datadir'',
    type=str, help=''Path to data directory'')     parser.add_argument(''question'',
    type=str, help=''A question to answer'')     args = parser.parse_args()      #2     documents
    = SimpleDirectoryReader(args.datadir).load_data()      #3     index = VectorStoreIndex.from_documents(documents)      #4     engine
    = index.as_query_engine()      #5     answer = engine.query(args.question)     print(answer)
    [PRE28] python listing4.py bananareports ''How much did the Plantain unit make
    ↪ in 2023?'' [PRE29] The Plantain unit made 30 million USD in 2023. [PRE30]`'
  prefs: []
  type: TYPE_NORMAL
