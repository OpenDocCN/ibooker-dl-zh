- en: 8 Text generation with recurrent neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 使用循环神经网络进行文本生成
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The idea behind RNNs and why they can handle sequential data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN背后的理念以及为什么它们可以处理序列数据
- en: Character tokenization, word tokenization, and subword tokenization
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符标记化、单词标记化和子词标记化
- en: How word embedding works
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入的工作原理
- en: Building and training an RNN to generate text
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练RNN以生成文本
- en: Using temperature and top-K sampling to control the creativeness of text generation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用温度和top-K采样来控制文本生成的创造性
- en: So far in this book, we have discussed how to generate shapes, numbers, and
    images. Starting from this chapter, we’ll focus mainly on text generation. Generating
    text is often considered the holy grail of generative AI for several compelling
    reasons. Human language is incredibly complex and nuanced. It involves understanding
    not only grammar and vocabulary but also context, tone, and cultural references.
    Successfully generating coherent and contextually appropriate text is a significant
    challenge that requires deep understanding and processing of language.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们已经讨论了如何生成形状、数字和图像。从本章开始，我们将主要关注文本生成。生成文本通常被认为是生成式AI的圣杯，有以下几个令人信服的原因。人类语言极其复杂和微妙。它不仅涉及理解语法和词汇，还包括上下文、语气和文化参考。成功生成连贯且上下文适当的文本是一个重大的挑战，需要深入理解和处理语言。
- en: As humans, we primarily communicate through language. AI that can generate human-like
    text can interact more naturally with users, making technology more accessible
    and user-friendly. Text generation has many applications, from automating customer
    service responses to creating entire articles, scripting for games and movies,
    aiding in creative writing, and even building personal assistants. The potential
    effect across industries is enormous.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们主要通过语言进行沟通。能够生成类似人类文本的AI可以更自然地与用户互动，使技术更加易于访问和用户友好。文本生成有许多应用，从自动化客户服务响应到创建完整的文章、游戏和电影的脚本，帮助创意写作，甚至构建个人助理。其对各个行业的潜在影响是巨大的。
- en: 'In this chapter, we’ll make our first attempt at building and training models
    to generate text. You’ll learn to tackle three main challenges in modeling text
    generation. First, text is sequential data, consisting of data points organized
    in a specific sequence, where each point is successively ordered to reflect the
    inherent order and interdependencies within the data. Predicting outcomes for
    sequences is challenging due to their sensitive ordering. Altering the sequence
    of elements changes their meaning. Second, text exhibits long-range dependencies:
    the meaning of a certain part of the text depends on elements that appeared much
    earlier in the text (e.g., 100 words ago). Understanding and modeling these long-range
    dependencies is essential for generating coherent text. Lastly, human language
    is ambiguous and context dependent. Training a model to understand nuances, sarcasm,
    idioms, and cultural references to generate contextually accurate text is challenging.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将尝试构建和训练模型以生成文本。你将学习如何应对文本生成中的三个主要挑战。首先，文本是序列数据，由按特定顺序组织的数据点组成，其中每个点按顺序排列以反映数据内部固有的顺序和相互依赖关系。由于序列的敏感顺序，预测序列的结果具有挑战性。改变元素的顺序会改变其含义。其次，文本表现出长距离依赖性：文本的某个部分的意义取决于文本中较早出现（例如，100个词之前）的元素。理解和建模这些长距离依赖关系对于生成连贯的文本至关重要。最后，人类语言是模糊和上下文相关的。训练一个模型来理解细微差别、讽刺、习语和文化参考以生成上下文准确的文本具有挑战性。
- en: 'You’ll explore a specific neural network designed for handling sequential data,
    such as text or time series: the recurrent neural network (RNN). Traditional neural
    networks, such as feedforward neural networks or fully connected networks, treat
    each input independently. This means that the network processes each input separately,
    without considering any relationship or order between different inputs. In contrast,
    RNNs are specifically designed to handle sequential data. In an RNN, the output
    at a given time step depends not only on the current input but also on previous
    inputs. This allows RNNs to maintain a form of memory, capturing information from
    previous time steps to influence the processing of the current input.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你将探索一种专门设计用于处理顺序数据（如文本或时间序列）的特定神经网络：循环神经网络（RNN）。传统的神经网络，如前馈神经网络或全连接网络，独立地处理每个输入。这意味着网络分别处理每个输入，不考虑不同输入之间的任何关系或顺序。相比之下，RNNs是专门设计来处理顺序数据的。在RNN中，给定时间步的输出不仅取决于当前输入，还取决于之前的输入。这允许RNNs保持一种记忆形式，从之前的时间步捕获信息以影响当前输入的处理。
- en: This sequential processing makes RNNs suitable for tasks where the order of
    the inputs matters, such as language modeling, where the goal is to predict the
    next word in a sentence based on previous words. We’ll focus on one variant of
    RNN, long short-term memory (LSTM) networks, which can recognize both short-term
    and long-term data patterns in sequential data like text. LSTM models use a hidden
    state to capture information in previous time steps. Therefore, a trained LSTM
    model can produce coherent text based on the context.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种顺序处理使得循环神经网络（RNNs）适用于那些输入顺序很重要的任务，例如语言建模，其目标是根据前面的单词预测句子中的下一个单词。我们将重点关注RNN的一种变体，即长短期记忆（LSTM）网络，它可以在文本等顺序数据中识别短期和长期数据模式。LSTM模型使用一个隐藏状态来捕捉之前时间步的信息。因此，一个训练好的LSTM模型可以根据上下文生成连贯的文本。
- en: The style of the generated text depends on the training data. Additionally,
    as we plan to train a model from scratch for text generation, the length of the
    training text is a crucial factor. It needs to be sufficiently extensive for the
    model to effectively learn and mimic a particular writing style yet concise enough
    to avoid excessive computational demands during training. As a result, we’ll use
    the text from the novel *Anna Karenina*, which appears to be of the right length
    for our purposes, to train an LSTM model. Since neural networks like an LSTM cannot
    accept text as input directly, you’ll learn to break down text into tokens (individual
    words in this chapter but can be parts of words, as you’ll see in later chapters),
    a process known as *tokenization*. You’ll then create a dictionary to map each
    unique token into an integer (i.e., an index). Based on this dictionary, you’ll
    convert the text into a long sequence of integers, ready to be fed into a neural
    network.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本风格取决于训练数据。此外，由于我们计划从头开始训练一个用于文本生成的模型，训练文本的长度是一个关键因素。它需要足够广泛，以便模型能够有效地学习和模仿特定的写作风格，同时又要足够简洁，以避免在训练期间产生过度的计算需求。因此，我们将使用小说《安娜·卡列尼娜》中的文本进行训练，这似乎符合我们的目的，用于训练LSTM模型。由于像LSTM这样的神经网络不能直接接受文本作为输入，你将学习如何将文本分解成标记（在本章中是单个单词，但在后面的章节中可以是单词的一部分），这个过程称为*标记化*。然后，你将创建一个字典，将每个唯一的标记映射到一个整数（即索引）。基于这个字典，你将把文本转换成一个长序列的整数，以便输入到神经网络中。
- en: 'You’ll use sequences of indexes of a certain length as the input to train the
    LSTM model. You shift the sequence of inputs by one token to the right and use
    it as the output: you are effectively training the model to predict the next token
    in a sentence. This is the so-called *sequence-to-sequence* prediction problem
    in natural language processing (NLP), and you’ll see it again in later chapters.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用一定长度的索引序列作为训练LSTM模型的输入。你将输入序列向右移动一个标记，并使用它作为输出：你实际上是在训练模型预测句子中的下一个标记。这是自然语言处理（NLP）中所谓的*序列到序列*预测问题，你将在后面的章节中再次看到它。
- en: 'Once the LSTM is trained, you’ll use it to generate text one token at a time
    based on previous tokens in the sequence as follows: you feed a prompt (part of
    a sentence such as “Anna and the”) to the trained model. The model then predicts
    the most likely next token and appends the selected token to your prompt. The
    updated prompt serves again as the input, and the model is used once more to predict
    the next token. The iterative process continues until the prompt reaches a certain
    length. This approach is similar to the mechanism employed by more advanced generative
    models like ChatGPT (though ChatGPT is not an LSTM). You’ll witness the trained
    LSTM model generating grammatically correct and coherent text, with a style matching
    that of the original novel.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦LSTM（长短期记忆网络）被训练，你将用它一次生成一个标记的文本，基于序列中的前一个标记，如下所示：你将一个提示（如“Anna and the”句子的一部分）输入到训练好的模型中。然后模型预测最可能的下一个标记，并将选定的标记附加到提示上。更新后的提示再次作为输入，模型再次被用来预测下一个标记。这个过程迭代进行，直到提示达到一定的长度。这种方法类似于更高级的生成模型（如ChatGPT）所采用的机制（尽管ChatGPT不是一个LSTM）。你将见证训练好的LSTM模型生成语法正确且连贯的文本，其风格与原始小说相匹配。
- en: Finally, you also learn how to control the creativeness of the generated text
    using temperature and top-K sampling. Temperature controls the randomness of the
    predictions of the trained model. A high temperature makes the generated text
    more creative while a low temperature makes the text more confident and predictable.
    Top-K sampling is a method where you select the next token from the top K most
    probable tokens, rather than selecting from the entire vocabulary. A small value
    of K leads to the selection of highly likely tokens in each step, and this, in
    turn, makes the generated text less creative and more coherent.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你还将学习如何通过温度和top-K采样来控制生成文本的创造性。温度控制训练模型的预测随机性。高温度使得生成的文本更具创造性，而低温度则使文本更加自信和可预测。top-K采样是一种方法，其中你从最可能的K个标记中选择下一个标记，而不是从整个词汇表中选择。K值较小会导致在每一步选择高度可能的标记，这反过来又使得生成的文本不那么具有创造性，而更加连贯。
- en: The primary goal of this chapter is not necessarily to generate the most coherent
    text possible, which, as mentioned earlier, presents substantial challenges. Instead,
    our objective is to demonstrate the limitations of RNNs, thereby setting the stage
    for the introduction of Transformers in subsequent chapters. More importantly,
    this chapter establishes the basic principles of text generation, including tokenization,
    word embedding, sequence prediction, temperature settings, and top-K sampling.
    Consequently, in later chapters, you will have a solid understanding of the fundamentals
    of NLP. This foundation will allow us to concentrate on other, more advanced aspects
    of NLP, such as how the attention mechanism functions and the architecture of
    Transformers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要目标并不是生成尽可能连贯的文本，正如之前提到的，这提出了巨大的挑战。相反，我们的目标是展示RNNs的局限性，从而为后续章节中介绍Transformers做准备。更重要的是，本章建立了文本生成的基本原则，包括标记化、词嵌入、序列预测、温度设置和top-K采样。因此，在后续章节中，你将牢固地理解NLP的基础知识。这个基础将使我们能够专注于NLP的其他更高级方面，例如注意力机制的工作原理和Transformers的架构。
- en: 8.1 Introduction to RNNs
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 RNNs简介
- en: At the beginning of this chapter, we touched upon the complexities involved
    in generating text, particularly when aiming for coherence and contextual relevance.
    This section dives deeper into these challenges and explores the architecture
    of RNNs. We’ll explain why RNNs are suitable for the task and their limitations
    (which are the reasons they have been overtaken by Transformers).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们提到了生成文本所涉及的复杂性，尤其是在追求连贯性和上下文相关性时。本节将进一步深入探讨这些挑战，并探讨RNNs的架构。我们将解释为什么RNNs适合这项任务以及它们的局限性（这是它们被Transformers取代的原因）。
- en: RNNs are specifically designed to handle sequential data, making them capable
    of text generation, a task inherently sequential in nature. They utilize a form
    of memory, known as hidden states, to capture and retain information from earlier
    parts of the sequence. This capability is crucial for maintaining context and
    understanding dependencies as the sequence progresses.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs（循环神经网络）是专门设计来处理序列数据的，这使得它们能够胜任文本生成任务，这项任务在本质上具有序列性。它们利用一种称为隐藏状态的记忆形式，来捕捉和保留序列早期部分的信息。这种能力对于保持上下文和随着序列的进展理解依赖关系至关重要。
- en: In this chapter, we will specifically utilize LSTM networks, advanced versions
    of RNNs, for text generation, using their advanced capabilities to tackle the
    challenges in this task.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将特别使用LSTM网络，这是RNNs的高级版本，用于文本生成，利用其高级功能来应对这项任务中的挑战。
- en: 8.1.1 Challenges in generating text
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 文本生成的挑战
- en: Text represents a quintessential example of *sequential data*, which is defined
    as any dataset where the order of elements is critical. This structuring implies
    that the positioning of individual elements relative to each other holds significant
    meaning, often conveying essential information for understanding the data. Examples
    of sequential data include time series (like stock prices), textual content (such
    as sentences), and musical compositions (a succession of notes).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 文本代表了典型的**序列数据**，它被定义为任何数据集，其中元素的顺序至关重要。这种结构意味着各个元素之间的相对位置具有重大意义，通常传达了理解数据所必需的重要信息。序列数据的例子包括时间序列（如股价）、文本内容（如句子）和音乐作品（音符的连续序列）。
- en: This book primarily zeroes in on text generation, although it also ventures
    into music generation in chapters 13 and 14\. The process of generating text is
    fraught with complexities. A primary challenge lies in modeling the sequence of
    words within sentences, where altering the order can drastically change the meaning.
    For instance, in the sentence “Kentucky defeated Vanderbilt in last night’s football
    game,” swapping ‘Kentucky’ and ‘Vanderbilt’ entirely reverses the sentence’s implication,
    despite using the same words. Furthermore, as mentioned in the introduction, text
    generation encounters challenges in handling long-range dependencies and dealing
    with the problem of ambiguity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本书主要关注文本生成，尽管在第13章和第14章中也涉及音乐生成。文本生成的过程充满了复杂性。一个主要的挑战在于对句子中单词序列的建模，改变顺序可能会极大地改变句子的含义。例如，在句子“肯塔基在昨晚的足球比赛中击败了范德比尔特”中，将“肯塔基”和“范德比尔特”完全互换位置，尽管使用了相同的单词，却完全颠倒了句子的含义。此外，正如引言中提到的，文本生成在处理长距离依赖关系和解决歧义问题上也面临着挑战。
- en: In this chapter, we will explore one approach to tackle these challenges—namely,
    by using RNNs. While this method isn’t flawless, it lays the groundwork for more
    advanced techniques you’ll encounter in later chapters. This approach will provide
    insight into managing word order, addressing long-range dependencies, and navigating
    the inherent ambiguity in text, equipping you with fundamental skills in text
    generation. The journey through this chapter serves as a stepping stone to more
    sophisticated methods and deeper understanding in the subsequent parts of the
    book. Along the way, you’ll acquire many valuable skills in NLP, such as text
    tokenization, word embedding, and sequence-to-sequence predictions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨解决这些挑战的一种方法——即使用RNNs（循环神经网络）。虽然这种方法并不完美，但它为你在后续章节中遇到的更高级技术奠定了基础。这种方法将帮助你了解如何管理词序、解决长距离依赖关系，以及处理文本中的固有歧义，为你提供文本生成的基本技能。通过本章的学习，你将为书中更复杂的方法和深入的理解打下坚实的基础。在这个过程中，你将获得许多在自然语言处理（NLP）领域非常有价值的技术，例如文本分词、词嵌入和序列到序列的预测。
- en: 8.1.2 How do RNNs work?
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 RNNs是如何工作的？
- en: RNNs are a specialized form of artificial neural network designed to recognize
    patterns in sequences of data, such as text, music, or stock prices. Unlike traditional
    neural networks, which process inputs independently, RNNs have loops in them,
    allowing information to persist.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs是一种专门的人工神经网络形式，旨在识别数据序列中的模式，如文本、音乐或股价。与处理输入独立的传统神经网络不同，RNNs内部有循环，允许信息持续存在。
- en: One of the challenges in generating text is how to predict the next word based
    on all previous words so that the prediction captures both the long-range dependencies
    and contextual meaning. RNNs take input not just as a standalone item but as a
    sequence (like words in a sentence, for example). At each time step, the prediction
    is based on not only the current input but also all previous inputs in the form
    of a summary through a hidden state. Let’s consider the phrase “a frog has four
    legs” as an example. In the first time step, we use the word “a” to predict the
    second word “frog.” In the second time step, we predict the next word using both
    “a” and “frog.” By the time we predict the last word, we need to use all four
    previous words “a frog has four.”
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成文本的挑战之一是如何根据所有前面的单词预测下一个单词，以便预测能够捕捉到长距离依赖关系和上下文意义。RNNs的输入不仅是一个独立的项，而是一个序列（例如句子中的单词）。在每一个时间步，预测不仅基于当前输入，还基于通过隐藏状态总结的所有先前输入。以短语“一只青蛙有四条腿”为例。在第一个时间步，我们使用单词“一个”来预测第二个单词“青蛙”。在第二个时间步，我们使用“一个”和“青蛙”来预测下一个单词。当我们预测最后一个单词时，我们需要使用所有四个先前单词“一个青蛙有四条”。
- en: A key feature of RNNs is the so-called hidden state, which captures information
    in all previous elements in a sequence. This feature is crucial for the network’s
    ability to process and generate sequential data effectively. The functioning of
    RNNs and this sequential processing is depicted in figure 8.1, which illustrates
    how a layer of recurrent neurons unfolds over time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs的一个关键特性是所谓的隐藏状态，它捕捉了序列中所有先前元素的信息。这一特性对于网络有效处理和生成序列数据至关重要。RNNs的功能和这种序列处理在图8.1中得到了展示，该图说明了循环神经元层是如何随时间展开的。
- en: '![](../../OEBPS/Images/CH08_F01_Liu.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH08_F01_Liu.png)'
- en: Figure 8.1 How a layer of recurrent neurons unfolds through time. When a recurrent
    neural network makes a prediction on sequential data, it takes the hidden state
    from the previous time step, h(t – 1), along with the input at the current time
    step, x(t), and generates the output, y(t), and the updated hidden state, h(t).
    The hidden state at time step t captures the information in all previous time
    steps, x(0), x(1), …, x(t).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 展示了通过时间展开的循环神经元层。当一个循环神经网络对序列数据进行预测时，它从上一个时间步的隐藏状态h(t – 1)以及当前时间步的输入x(t)中获取信息，并生成输出y(t)以及更新的隐藏状态h(t)。时间步t的隐藏状态捕捉了所有先前时间步的信息，x(0)，x(1)，…，x(t)。
- en: The hidden state in RNNs plays a pivotal role in capturing information across
    all time steps. This allows RNNs to make predictions that are informed not just
    by the current input, x(t), but also by the accumulated knowledge from all previous
    inputs, x(0), x(1), …, x(t – 1). This attribute makes RNNs capable of understanding
    temporal dependencies. They can grasp the context from an input sequence, which
    is indispensable for tasks like language modeling, where the preceding words in
    a sentence set the stage for predicting the next word.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs中的隐藏状态在捕捉所有时间步的信息方面发挥着关键作用。这使得RNNs能够做出不仅由当前输入x(t)而且由所有先前输入的累积知识x(0)，x(1)，…，x(t
    – 1)所指导的预测。这一属性使RNNs能够理解时间依赖关系。它们能够从输入序列中把握上下文，这对于语言建模等任务至关重要，在这些任务中，句子中的前一个单词为预测下一个单词设定了场景。
- en: However, RNNs are not without their drawbacks. Though standard RNNs are capable
    of handling short-term dependencies, they struggle with longer-range dependencies
    within text. This difficulty stems from the vanishing gradient problem, which
    occurs in long sequences where the gradients (essential for training the network)
    diminish, hindering the model’s ability to learn relationships over longer distances.
    To mitigate this, advanced versions of RNNs, such as LSTM networks, have been
    developed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RNNs并非没有缺点。尽管标准RNNs能够处理短期依赖关系，但在文本中的长距离依赖关系上却显得力不从心。这种困难源于梯度消失问题，在长序列中，梯度（对于训练网络至关重要）会减小，阻碍模型学习长距离关系的能力。为了减轻这一问题，已经开发出了RNNs的高级版本，例如LSTM网络。
- en: 'LSTM networks were introduced by Hochreiter and Schmidhuber in 1997.^([1](#footnote-000))
    An LSTM network is composed of LSTM units (or cells), each of which has a more
    complex structure than a standard RNN neuron. The cell state is the key innovation
    of LSTMs: it acts as a kind of conveyor belt, running straight down the entire
    chain of LSTM units. It has the ability to carry relevant information through
    the network. The ability to add or remove information to the cell state allows
    LSTMs to capture long-term dependencies and remember information for long periods.
    This makes them more effective for tasks like language modeling and text generation.
    In this chapter, we will harness the LSTM model to undertake a project on text
    generation, aiming to mimic the style of the novel *Anna Karenina*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 网络是由 Hochreiter 和 Schmidhuber 在 1997 年提出的.^([1](#footnote-000)) LSTM 网络由
    LSTM 单元（或细胞）组成，每个单元的结构都比标准 RNN 神经元更复杂。细胞状态是 LSTM 的关键创新：它充当一种传送带，沿着整个 LSTM 单元链直接运行。它具有在网络中携带相关信息的能力。向细胞状态添加或删除信息的能力使
    LSTM 能够捕捉长期依赖关系并长时间记住信息。这使得它们在语言建模和文本生成等任务上更加有效。在本章中，我们将利用 LSTM 模型进行文本生成项目，旨在模仿小说
    *安娜·卡列尼娜* 的风格。
- en: However, it’s noteworthy that even advanced RNN variants like LSTMs encounter
    hurdles in capturing extremely long-range dependencies in sequence data. We will
    discuss these challenges and provide solutions in the next chapter, continuing
    our exploration of sophisticated models for effective sequence data processing
    and generation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，即使是像 LSTM 这样的高级 RNN 变体在捕捉序列数据中的极长距离依赖关系时也会遇到障碍。我们将在下一章讨论这些挑战并提供解决方案，继续探索用于有效序列数据处理和生成的复杂模型。
- en: 8.1.3 Steps in training a LSTM model
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 训练 LSTM 模型的步骤
- en: Next, we’ll discuss the steps involved in training an LSTM model to generate
    text. This overview aims to provide a foundational understanding of the training
    process before embarking on the project.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论训练 LSTM 模型生成文本所涉及的步骤。这个概述旨在在开始项目之前，提供一个对训练过程的坚实基础理解。
- en: The choice of text for training depends on the desired output. A lengthy novel
    serves as a good starting point. Its extensive content enables the model to learn
    and replicate a specific writing style effectively. An ample amount of text data
    enhances the model’s proficiency in this style. At the same time, novels are generally
    not excessively long, which helps in managing the training time. For our LSTM
    model training, we’ll utilize the text from *Anna Karenina*, aligning with our
    previously outlined training data criteria.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练文本的选择取决于期望的输出。一部篇幅较长的小说是一个良好的起点。其丰富的内容使模型能够有效地学习和复制特定的写作风格。大量的文本数据增强了模型在此风格上的熟练度。同时，小说通常不会过长，这有助于管理训练时间。对于我们
    LSTM 模型的训练，我们将利用来自 *安娜·卡列尼娜* 的文本，符合我们之前概述的训练数据标准。
- en: Similar to other deep neural networks, LSTM models cannot process raw text directly.
    Instead, we’ll convert the text into numerical form. This begins by breaking down
    the text into smaller pieces, a process known as tokenization, where each piece
    is a token. Tokens can be entire words, punctuation marks (like an exclamation
    mark or a comma), or special characters (such as & or %). For this chapter, each
    of these elements will be treated as separate tokens. Although this method of
    tokenization may not be the most efficient, it is easy to implement since all
    we need is to map words to tokens. We will use subword tokenization in subsequent
    chapters where some infrequent words are broken into smaller pieces such as syllables.
    Following tokenization, we assign a unique integer to each token, creating a numerical
    representation of the text as a sequence of integers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他深度神经网络类似，LSTM 模型不能直接处理原始文本。相反，我们将文本转换为数值形式。这始于将文本分解成更小的片段，这个过程称为分词，其中每个片段都是一个标记。标记可以是完整的单词、标点符号（如感叹号或逗号），或特殊字符（如
    & 或 %）。在本章中，这些元素都将被视为单独的标记。尽管这种分词方法可能不是最有效的，但它易于实现，因为我们只需要将单词映射到标记。在后续章节中，我们将使用子词分词，其中一些不常见的单词被分解成更小的片段，如音节。在分词之后，我们为每个标记分配一个唯一的整数，创建文本的数值表示，即整数序列。
- en: To prepare the training data, we divide this long sequence into shorter sequences
    of equal length. For our project, we’ll use sequences comprising 100 integers
    each. These sequences form the features (the *x* variable) of our model. We then
    generate the output *y* by shifting the input sequence one token to the right.
    This setup enables the LSTM model to predict the next token in a sequence. The
    pairs of input and output serve as the training data. Our model includes LSTM
    layers to understand long-term patterns in the text and an embedding layer to
    grasp semantic meanings.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备训练数据，我们将这个长序列划分为长度相等的更短序列。在我们的项目中，我们将使用由100个整数组成的序列。这些序列构成了我们模型的特征（即*x*变量）。然后，我们通过将输入序列向右移动一个标记来生成输出*y*。这种设置使得LSTM模型能够根据序列中的先前标记预测下一个标记。输入和输出的配对作为训练数据。我们的模型包括LSTM层来理解文本中的长期模式，以及一个嵌入层来把握语义含义。
- en: Let’s revisit the example of predicting the sentence “a frog has four legs”
    that we mentioned earlier. Figure 8.2 is a diagram of how the training of the
    LSTM model works.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下之前提到的预测句子“a frog has four legs”的例子。图8.2展示了LSTM模型训练的工作原理。
- en: '![](../../OEBPS/Images/CH08_F02_Liu.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH08_F02_Liu.png)'
- en: Figure 8.2 An example of how an LSTM model is trained. We first break down the
    training text into tokens and assign a unique integer to each token, creating
    a numerical representation of the text as a sequence of indexes. We then divide
    this long sequence into shorter sequences of equal length. These sequences form
    the features (the x variable) of our model. We then generate the output y by shifting
    the input sequence one token to the right. This setup enables the LSTM model to
    predict the next token based on previous tokens in the sequence.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2展示了如何训练LSTM模型的一个示例。我们首先将训练文本分解为标记，并为每个标记分配一个唯一的整数，从而将文本作为索引序列的数值表示。然后，我们将这个长序列划分为长度相等的更短序列。这些序列构成了我们模型的特征（即x变量）。然后，我们通过将输入序列向右移动一个标记来生成输出y。这种设置使得LSTM模型能够根据序列中的先前标记预测下一个标记。
- en: In the first time step, the model uses the word “a” to predict the word “frog.”
    Since there’s no preceding word for “a,” we initialize the hidden state with zeros.
    The LSTM model receives both the index for “a” and this initial hidden state as
    input and outputs the predicted next word along with an updated hidden state,
    h0\. In the subsequent time step, the word “frog” and the updated state h0 are
    used to predict “has” and generate a new hidden state, h1\. This sequence of predicting
    the next word and updating the hidden state continues until the model forecasts
    the final word in the sentence, “legs.”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个时间步，模型使用单词“a”来预测单词“frog”。由于“a”没有前面的单词，我们用零初始化隐藏状态。LSTM模型接收“a”的索引和这个初始隐藏状态作为输入，并输出预测的下一个单词以及更新的隐藏状态h0。在随后的时间步，使用单词“frog”和更新的状态h0来预测“has”并生成新的隐藏状态h1。预测下一个单词和更新隐藏状态的这一序列持续进行，直到模型预测出句子中的最后一个单词，“legs”。
- en: The predictions are then compared to the actual next word in the sentence. Since
    the model is effectively predicting the next token out of all possible tokens
    in the vocabulary, there is a multicategory classification problem. We tweak the
    model parameters in each iteration to minimize the cross-entropy loss so that
    in the next iteration, the model predictions move closer to actual outputs in
    the training data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将预测结果与句子中的实际下一个单词进行比较。由于模型实际上是在预测词汇表中所有可能的标记中的下一个标记，因此存在一个多类别分类问题。我们在每次迭代中调整模型参数，以最小化交叉熵损失，从而使模型在下一个迭代中的预测结果更接近训练数据中的实际输出。
- en: Once the model is trained, generating text begins with a seed sequence input
    into the model. The model predicts the next token, which is then appended to your
    sequence. This iterative process of prediction and sequence updating is repeated
    to generate text for as long as desired.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，生成文本的过程从将种子序列输入模型开始。模型预测下一个标记，然后将该标记附加到序列中。这种预测和序列更新的迭代过程重复进行，直到生成所需的文本长度。
- en: 8.2 Fundamentals of NLP
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 自然语言处理基础
- en: Deep learning models, including the LSTM models we discussed earlier and Transformers,
    which you’ll learn in later chapters, cannot process raw text directly because
    they are designed to work with numerical data, typically in the form of vectors
    or matrices. The processing and learning capabilities of neural networks are based
    on mathematical operations like addition, multiplication, and activation functions,
    which require numerical input. Consequently, it’s essential first to break down
    text into smaller, more manageable elements known as tokens. These tokens can
    range from individual characters and words to subword units.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型，包括我们之前讨论的 LSTM 模型以及你将在后续章节中学习的 Transformer，不能直接处理原始文本，因为它们被设计成与数值数据一起工作，通常是向量或矩阵的形式。神经网络的处理和学习能力基于数学运算，如加法、乘法和激活函数，这些运算需要数值输入。因此，首先将文本分解成更小、更易于管理的元素，即标记，是至关重要的。这些标记可以是从单个字符和单词到子词单元。
- en: The next crucial step in NLP tasks is transforming these tokens into numerical
    representations. This conversion is necessary for feeding them into deep neural
    networks, which is a fundamental part of training our models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 任务中的下一个关键步骤是将这些标记转换为数值表示。这种转换对于将它们输入深度神经网络是必要的，这是训练我们模型的基本部分。
- en: In this section, we’ll discuss different tokenization methods, along with their
    advantages and drawbacks. Additionally, you’ll gain insights into the process
    of converting tokens into dense vector representations—a method known as word
    embedding. This technique is crucial for capturing the meaning of language in
    a format that deep learning models can effectively utilize.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论不同的分词方法，以及它们的优缺点。此外，你还将深入了解将标记转换为密集向量表示的过程——这种方法称为词嵌入。这项技术对于捕捉语言的意义，使其以深度学习模型能够有效利用的格式至关重要。
- en: 8.2.1 Different tokenization methods
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 不同的分词方法
- en: Tokenization involves dividing text into smaller parts, known as tokens, which
    can be in the form of words, characters, symbols, or other significant units.
    The primary goal of tokenization is to streamline the process of text data analysis
    and processing.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 分词涉及将文本划分为更小的部分，称为标记（tokens），这些标记可以是单词、字符、符号或其他有意义的单元。分词的主要目标是简化文本数据分析和处理的过程。
- en: 'Broadly speaking, there are three approaches to tokenization. The first is
    character tokenization, where the text is divided into its constituent characters.
    This method is used in languages with complex morphological structures, such as
    Turkish or Finnish, in which the meaning of words can change significantly with
    slight variations in characters. Take the English phrase “It is unbelievably good!”
    as an example; it’s broken down into individual characters as follows: `[''I'',
    ''t'', '' '', ''i'', ''s'', '' '', ''u'', ''n'', ''b'', ''e'', ''l'', ''i'', ''e'',
    ''v'', ''a'', ''b'', ''l'', ''y'', '' '', ''g'', ''o'', ''o'', ''d'', ''!'']`.
    A key advantage of character tokenization is the limited number of unique tokens.
    This limitation significantly reduces the parameters in deep learning models,
    leading to faster and more efficient training. However, the major drawback is
    that individual characters often lack significant meaning, making it challenging
    for machine learning models to derive meaningful insights from a sequence of characters.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义上讲，分词有三种方法。第一种是字符分词，其中文本被划分为其构成字符。这种方法用于具有复杂形态结构的语言，如土耳其语或芬兰语，在这些语言中，单词的意义可能会因字符的微小变化而显著改变。以英语短语“它好得令人难以置信！”为例；它被分解为以下单个字符：`['I',
    't', ' ', 'i', 's', ' ', 'u', 'n', 'b', 'e', 'l', 'i', 'e', 'v', 'a', 'b', 'l',
    'y', ' ', 'g', 'o', 'o', 'd', '!']`。字符分词的一个关键优势是唯一标记的数量有限。这种限制显著减少了深度学习模型中的参数数量，从而实现了更快、更有效的训练。然而，主要的缺点是单个字符通常缺乏显著的意义，这使得机器学习模型难以从字符序列中提取有意义的见解。
- en: Exercise 8.1
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 8.1
- en: Use character tokenization to divide the phrase “Hi, there!” into individual
    tokens.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用字符分词将短语“Hi, there!”划分为单个标记。
- en: 'The second approach is word tokenization, where the text is split into individual
    words and punctuation marks. It is used often in situations where the number of
    unique words is not too large. For instance, the same phrase “It is unbelievably
    good!” becomes five tokens: `[''It'', ''is'', ''unbelievably'', ''good'', ''!'']`.
    The main advantage of this method is that each word inherently carries semantic
    meaning, making it more straightforward for models to interpret the text. The
    downside, however, lies in the substantial increase in unique tokens, which increases
    the number of parameters in deep learning models. This increase can lead to slower
    and less efficient training processes.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是词元化，其中文本被分割成单个单词和标点符号。它常用于唯一单词数量不是太多的情况下。例如，同样的短语“它好得令人难以置信！”变成了五个标记：`['It',
    'is', 'unbelievably', 'good', '!']`。这种方法的主要优点是每个单词本身携带语义意义，这使得模型更容易解释文本。然而，缺点是独特标记的数量大幅增加，这增加了深度学习模型中的参数数量。这种增加可能导致训练过程变慢和效率降低。
- en: Exercise 8.2
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.2
- en: Use word tokenization to break down the phrase “Hi, how are you?” into individual
    tokens.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词元化将短语“Hi, how are you?”分解成单个标记。
- en: The third approach is subword tokenization. This method, a key concept in NLP,
    breaks text into smaller, meaningful components called subwords. For instance,
    the phrase “It is unbelievably good!” would be divided into tokens like `['It',
    'is', 'un', 'believ', 'ably', 'good', '!']`. Most advanced language models, including
    ChatGPT, use subword tokenization, and you’ll use this method in the next few
    chapters. Subword tokenization strikes a balance between the more traditional
    tokenization techniques that typically split text into either individual words
    or characters. Word-based tokenization, while capturing more meaning, leads to
    a vast vocabulary. Conversely, character-based tokenization results in a smaller
    vocabulary, but each token carries less semantic value.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法是子词元化。这种方法是NLP中的一个关键概念，将文本分解成更小、更有意义的组件，称为子词。例如，短语“它好得令人难以置信！”将被分解成如`['It',
    'is', 'un', 'believ', 'ably', 'good', '!']`这样的标记。大多数高级语言模型，包括ChatGPT，都使用子词元化，你将在接下来的几章中使用这种方法。子词元化在更传统的词元化技术之间取得了平衡，这些技术通常将文本分割成单个单词或字符。基于单词的词元化虽然能捕捉更多意义，但会导致词汇量巨大。相反，基于字符的词元化会导致词汇量较小，但每个标记的语义价值较低。
- en: Subword tokenization effectively mitigates these problems by keeping frequently
    used words whole in the vocabulary while dividing less common or more complex
    words into subcomponents. This technique is particularly advantageous for languages
    with large vocabularies or those exhibiting a high degree of word form variation.
    By adopting subword tokenization, the overall vocabulary size is substantially
    reduced. This reduction enhances the efficiency and effectiveness of language
    processing tasks, especially when dealing with a wide range of linguistic structures.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 子词元化通过在词汇表中保留常用单词的完整性，同时将不太常见或更复杂的单词分解成子组件，有效地缓解了这些问题。这种技术在词汇量大的语言或表现出高度词形变化的语言中特别有利。通过采用子词元化，整体词汇量大幅减少。这种减少提高了语言处理任务的效率和有效性，尤其是在处理广泛的语结构时。
- en: In this chapter, we will focus on word tokenization, as it offers a straightforward
    foundation for beginners. As we progress to later chapters, our attention will
    shift to subword tokenization, utilizing models that have already been trained
    with this technique. This approach allows us to concentrate on more advanced topics,
    such as understanding the Transformer architecture and exploring the inner workings
    of the attention mechanism.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注词元化，因为它为初学者提供了一个直接的起点。随着我们进入后面的章节，我们的注意力将转向子词元化，使用已经通过这种技术训练过的模型。这种方法使我们能够专注于更高级的主题，例如理解Transformer架构和探索注意力机制的内部工作原理。
- en: 8.2.2 Word embedding
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 词嵌入
- en: Word embedding is a method that transforms tokens into compact vector representations,
    capturing their semantic information and interrelationships. This technique is
    vital in NLP, especially since deep neural networks, including models like LSTM
    and Transformers, require numerical input.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是一种将标记转换为紧凑的向量表示的方法，捕捉它们的语义信息和相互关系。这项技术在NLP中至关重要，尤其是在深度神经网络，包括LSTM和Transformer等模型，需要数值输入的情况下。
- en: Traditionally, tokens are converted into numbers using one-hot encoding before
    being fed into NLP models. In one-hot encoding, each token is represented by a
    vector where only one element is ‘1’, and the rest are ‘0’s. For example, in this
    chapter, there are 12,778 unique word-based tokens in the text for the novel *Anna
    Karenina*. Each token is represented by a vector of 12,778 dimensions. Consequently,
    a phrase like “happy families are all alike” is represented as a 5 × 12,778 matrix,
    where 5 represents the number of tokens. This representation, however, is highly
    inefficient due to its large dimensionality, leading to an increased number of
    parameters, which can hinder training speed and efficiency.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Traditionally, tokens are converted into numbers using one-hot encoding before
    being fed into NLP models. In one-hot encoding, each token is represented by a
    vector where only one element is ‘1’, and the rest are ‘0’s. For example, in this
    chapter, there are 12,778 unique word-based tokens in the text for the novel *Anna
    Karenina*. Each token is represented by a vector of 12,778 dimensions. Consequently,
    a phrase like “happy families are all alike” is represented as a 5 × 12,778 matrix,
    where 5 represents the number of tokens. This representation, however, is highly
    inefficient due to its large dimensionality, leading to an increased number of
    parameters, which can hinder training speed and efficiency.
- en: LSTMs, Transformers, and other advanced NLP models address this inefficiency
    through word embedding. Instead of bulky one-hot vectors, word embedding uses
    continuous, lower-dimensional vectors (e.g., 128-value vectors we use in this
    chapter). As a result, the phrase “happy families are all alike” is represented
    by a more compact 5 × 128 matrix after word embedding. This streamlined representation
    drastically reduces the model’s complexity and enhances training efficiency.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: LSTMs, Transformers, and other advanced NLP models address this inefficiency
    through word embedding. Instead of bulky one-hot vectors, word embedding uses
    continuous, lower-dimensional vectors (e.g., 128-value vectors we use in this
    chapter). As a result, the phrase “happy families are all alike” is represented
    by a more compact 5 × 128 matrix after word embedding. This streamlined representation
    drastically reduces the model’s complexity and enhances training efficiency.
- en: Word embedding not only reduces word complexity by condensing it into a lower-dimensional
    space but also effectively captures the context and the nuanced semantic relationships
    between words, a feature that simpler representations like one-hot encoding lack,
    for the following reasons. In one-hot encoding, all tokens have the same distance
    from each other in vector space. However, in word embeddings, tokens with similar
    meanings are represented by vectors close to each other in the embedding space.
    Word embeddings are learned from the text in the training data; the resulting
    vectors capture contextual information. Tokens that appear in similar contexts
    will have similar embeddings, even if they are not explicitly related.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Word embedding not only reduces word complexity by condensing it into a lower-dimensional
    space but also effectively captures the context and the nuanced semantic relationships
    between words, a feature that simpler representations like one-hot encoding lack,
    for the following reasons. In one-hot encoding, all tokens have the same distance
    from each other in vector space. However, in word embeddings, tokens with similar
    meanings are represented by vectors close to each other in the embedding space.
    Word embeddings are learned from the text in the training data; the resulting
    vectors capture contextual information. Tokens that appear in similar contexts
    will have similar embeddings, even if they are not explicitly related.
- en: Word embedding in NLP
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Word embedding in NLP
- en: Word embeddings are a powerful method for representing tokens in NLP that offer
    significant advantages over traditional one-hot encoding in capturing context
    and semantic relationships between words.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Word embeddings are a powerful method for representing tokens in NLP that offer
    significant advantages over traditional one-hot encoding in capturing context
    and semantic relationships between words.
- en: One-hot encoding represents tokens as sparse vectors with a dimension equal
    to the size of the vocabulary, where each token is represented by a vector with
    all zeros except for a single one at the index corresponding to the token. In
    contrast, word embeddings represent tokens as dense vectors with much lower dimensions
    (e.g., 128 dimensions in this chapter and 256 dimensions in chapter 12). This
    dense representation is more efficient and can capture more information.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: One-hot encoding represents tokens as sparse vectors with a dimension equal
    to the size of the vocabulary, where each token is represented by a vector with
    all zeros except for a single one at the index corresponding to the token. In
    contrast, word embeddings represent tokens as dense vectors with much lower dimensions
    (e.g., 128 dimensions in this chapter and 256 dimensions in chapter 12). This
    dense representation is more efficient and can capture more information.
- en: Specifically, in one-hot encoding, all tokens have the same distance from each
    other in the vector space, meaning there is no notion of similarity between tokens.
    However, in word embeddings, similar tokens are represented by vectors that are
    close to each other in the embedding space. For example, the words “king” and
    “queen” would have similar embeddings, reflecting their semantic relationship.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在一维编码中，所有标记在向量空间中彼此距离相同，这意味着标记之间没有相似性的概念。然而，在词嵌入中，相似的标记由在嵌入空间中彼此靠近的向量表示。例如，“king”（国王）和“queen”（王后）会有相似的嵌入，反映了它们语义上的关系。
- en: Word embeddings are learned from the text in the training data. The embedding
    process uses the context in which tokens appear to learn their embeddings, meaning
    that the resulting vectors capture contextual information. Tokens that appear
    in similar contexts will have similar embeddings, even if they are not explicitly
    related.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是从训练数据中的文本学习得到的。嵌入过程使用标记出现的上下文来学习它们的嵌入，这意味着生成的向量捕捉了上下文信息。出现在相似上下文中的标记将具有相似的嵌入，即使它们没有明确的相关性。
- en: Overall, word embeddings provide a more nuanced and efficient representation
    of words that captures semantic relationships and contextual information, making
    them more suitable for NLP tasks compared to one-hot encoding.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，词嵌入提供了对词语更细腻和高效的表示，它捕捉了语义关系和上下文信息，这使得它们比一维编码更适合自然语言处理任务。
- en: In practical terms, particularly in frameworks like PyTorch, word embedding
    is implemented by passing indexes through a linear layer, which compresses them
    into a lower-dimensional space. That is, when you pass an index to the `nn.Embedding()`
    layer, it looks up the corresponding row in the embedding matrix and returns the
    embedding vector for that index, avoiding the need to create potentially very
    large one-hot vectors. The weights of this embedding layer are not predefined
    but are learned during the training process. This learning aspect enables the
    model to refine its understanding of word semantics based on the training data,
    leading to a more nuanced and context-aware representation of language in the
    neural network. This approach significantly enhances the model’s ability to process
    and interpret language data efficiently and meaningfully.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，尤其是在 PyTorch 等框架中，词嵌入是通过通过一个线性层传递索引来实现的，它将这些索引压缩到一个低维空间。也就是说，当你向 `nn.Embedding()`
    层传递一个索引时，它会查找嵌入矩阵中对应的行，并返回该索引的嵌入向量，从而避免了创建可能非常大的一个热向量。这个嵌入层的权重不是预先定义的，而是在训练过程中学习的。这一学习特性使得模型能够根据训练数据细化其对词义的理解，从而在神经网络中实现更细腻和上下文感知的语言表示。这种方法显著提高了模型处理和解释语言数据的有效性和意义。
- en: 8.3 Preparing data to train the LSTM model
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 准备数据以训练 LSTM 模型
- en: In this section, we’ll process text data and get it ready for training. We’ll
    first break text down into individual tokens. Our next step involves creating
    a dictionary that assigns each token an index, essentially mapping them to integers.
    After this setup, we will organize these tokens into batches of training data,
    which will be crucial for training an LSTM model in the subsequent section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将处理文本数据并为其训练做好准备。我们首先将文本分解成单个标记。接下来的步骤是创建一个字典，将每个标记分配一个索引，本质上是将它们映射到整数。完成这些设置后，我们将这些标记组织成训练数据的批次，这对于在下一节训练
    LSTM 模型至关重要。
- en: We’ll walk through the tokenization process in a detailed, step-by-step manner,
    ensuring you gain a thorough understanding of how tokenization functions. We’ll
    use word tokenization, owing to its simplicity in dividing text into words, as
    opposed to the more complex subword tokenization that demands a nuanced grasp
    of linguistic structure. In later chapters, we’ll employ pretrained tokenizers
    for subword tokenization using more sophisticated methods. This will allow us
    to focus on advanced topics, such as the attention mechanism and the Transformer
    architecture, without getting bogged down in the initial stages of text processing.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以详细、分步骤的方式介绍标记化过程，确保你彻底理解标记化是如何工作的。我们将使用词标记化，因为它在将文本分割成单词方面简单，而不是更复杂的子词标记化，后者需要细微地掌握语言结构。在后面的章节中，我们将使用更复杂的方法来使用预训练的子词标记化器。这将使我们能够专注于高级主题，如注意力机制和
    Transformer 架构，而不会在文本处理的初始阶段陷入困境。
- en: 8.3.1 Downloading and cleaning up the text
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 下载和清理文本
- en: 'We’ll use the text from the novel *Anna Karenina* to train our model. Go to
    [https://mng.bz/znmX](https://mng.bz/znmX) to download the text file and save
    it as anna.txt in the folder /files/ on your computer. After that, open the file
    and delete everything after line 39888, which says, `"END OF THIS PROJECT GUTENBERG
    EBOOK ANNA KARENINA`.`"` Or you can simply download the file anna.txt from the
    book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用小说《安娜·卡列尼娜》的文本来训练我们的模型。请访问[https://mng.bz/znmX](https://mng.bz/znmX)下载文本文件，并将其保存为电脑上文件夹/files/中的anna.txt。之后，打开文件并删除第39888行之后的所有内容，该行内容为`"END
    OF THIS PROJECT GUTENBERG EBOOK ANNA KARENINA`.`"`。或者，您可以直接从书籍的GitHub仓库下载anna.txt文件：[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)。
- en: 'First, we load up the data and print out some passages to get a feeling about
    the dataset:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载数据并打印出一些段落，以了解数据集：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The output is
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see, line breaks (represented by \n) are considered part of the
    text. Therefore, we should replace these line breaks with spaces so they are not
    in the vocabulary. Additionally, converting all words to lowercase is helpful
    in our setting, as it ensures words like “The” and “the” are recognized as the
    same token. This step is vital for reducing the variety of unique tokens, thereby
    making the training process more efficient. Furthermore, punctuation marks should
    be spaced apart from the words they follow. Without this separation, combinations
    like “way.” and “way” would be erroneously treated as different tokens. To address
    these problems, we’ll clean up the text:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，行断（用\n表示）被视为文本的一部分。因此，我们应该将这些行断替换为空格，这样它们就不会出现在词汇表中。此外，将所有单词转换为小写在我们的设置中很有帮助，因为它确保像“The”和“the”这样的单词被视为相同的标记。这一步对于减少独特标记的多样性至关重要，从而使得训练过程更加高效。此外，标点符号应与它们后面的单词保持一定的距离。如果没有这种分隔，像“way.”和“way”这样的组合会被错误地视为不同的标记。为了解决这些问题，我们将清理文本：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Replaces line break with a space
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ① 替换行断为空格
- en: ② Replaces a hyphen with a space
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ② 替换连字符为空格
- en: ③ Adds a space around punctuation marks and special characters
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在标点符号和特殊字符周围添加空格
- en: 'Next, we obtain unique tokens:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们获取独特标记：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The list `words` contains all the unique tokens in the text, with the most frequent
    one appearing first, and the least frequent one last. The output from the preceding
    code block is
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列表`words`包含文本中所有的独特标记，最频繁出现的标记排在第一位，最不频繁的标记排在最后。前一个代码块输出的结果是
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding output shows the most frequent 10 tokens. The comma (`,`) and
    the period (`.`) are the most and the second most frequent tokens, respectively.
    The word “the” is the third most frequent token, and so on.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出显示了最频繁的10个标记。逗号（`,`）是最频繁的标记，句号（`.`）是第二频繁的标记。单词“the”是第三频繁的标记，以此类推。
- en: 'We now create two dictionaries: one mapping tokens to indexes and the other
    mapping indexes to tokens.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建两个字典：一个将标记映射到索引，另一个将索引映射到标记。
- en: Listing 8.1 Dictionaries to map tokens to indexes and indexes to tokens
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.1 将标记映射到索引和索引映射到标记的字典
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① The length of text (how many tokens in the text)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ① 文本长度（文本中有多少个标记）
- en: ② The length of unique tokens
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ② 独特标记的长度
- en: ③ Maps tokens to indexes
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将标记映射到索引
- en: ④ Maps indexes to tokens
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将索引映射到标记
- en: The output from the preceding code block is
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码块输出的结果是
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The text for the novel *Anna Karenina* has a total of 437,098 tokens. There
    are 12,778 unique tokens. The dictionary `word_to_int` assigns an index to each
    unique token. For example, the comma (`,`) is assigned an index of 0, and the
    period (`.`) is assigned an index of 1\. The dictionary `int_to_word` translates
    an index back to a token. For example, index 2 is translated back to the token
    “`the`”. Index 4 is translated back to the token “`and`”, and so on.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 小说《安娜·卡列尼娜》的文本总共有437,098个标记。其中，有12,778个独特的标记。字典`word_to_int`为每个独特的标记分配一个索引。例如，逗号（`,`）被分配了索引0，而句号（`.`）被分配了索引1。字典`int_to_word`将索引转换回标记。例如，索引2转换回标记“`the`”。索引4转换回标记“`and`”，以此类推。
- en: 'Finally, we convert the whole text to indexes:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将整个文本转换为索引：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output is
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We convert all tokens in the text into the corresponding indexes and save them
    in a list `wordidx`. The preceding output shows the first 20 tokens in the text,
    as well as the corresponding indexes. For example, the first token in the text
    is `chapter`, with an index value of 208\.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将文本中的所有标记转换为相应的索引，并将它们保存在列表`wordidx`中。前一个输出显示了文本中的前20个标记以及相应的索引。例如，文本中的第一个标记是`chapter`，其索引值为208。
- en: Exercise 8.3
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.3
- en: Find out the index value of the token `anna` in the dictionary `word_to_int`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 查找字典`word_to_int`中标记`anna`的索引值。
- en: 8.3.2 Creating batches of training data
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 创建训练数据批次
- en: Next, we create pairs of (x, y) for training purposes. Each x is a sequence
    with 100 indexes. There is nothing magical about the number 100, and you can easily
    change it to 90 or 110 and have similar results. Setting the number too large
    may slow down training, while setting the number too small may lead to the model’s
    failure to capture long-range dependencies. We then slide the window right by
    one token and use it as the target y. Shifting the sequence by one token to the
    right and using it as the output during sequence generation is a common technique
    in training language models, including Transformers. The code block in the following
    listing creates the training data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建用于训练的(x, y)对。每个x是一个包含100个索引的序列。100这个数字并没有什么神奇之处，你可以轻松地将其更改为90或110，并得到相似的结果。设置数字太大可能会减慢训练速度，而设置数字太小可能会导致模型无法捕捉到长距离依赖关系。然后我们将窗口向右滑动一个标记，并将其用作目标y。在序列生成期间，将序列向右移动一个标记并用作输出是训练语言模型（包括Transformers）中的常见技术。以下列表中的代码块创建了训练数据。
- en: Listing 8.2 Creating training data
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.2 创建训练数据
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Each input contains 100 indexes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ① 每个输入包含100个索引。
- en: ② Starting from the first token in text, slides to the right one at a time
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从文本的第一个标记开始，每次向右滑动一个标记
- en: ③ Defines the input x
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 定义输入x
- en: ④ Shifts the input x to the right by one token and uses it as the output y
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将输入x向右移动一个标记，并将其用作输出y
- en: By shifting the sequence one token to the right and using it as output, the
    model is trained to predict the next token given the previous tokens. For instance,
    if the input sequence is `"how are you"`, then the shifted sequence would be `"are
    you today"`. During training, the model learns to predict `'are'` after seeing
    `'how'`, `'you'` after seeing `'are'`, and so on. This helps the model learn the
    probability distribution of the next token in a sequence. You’ll see this practice
    again and again later in this book.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将序列向右移动一个标记并将其用作输出，模型被训练来根据前面的标记预测下一个标记。例如，如果输入序列是 `"how are you"`，则移动后的序列将是
    `"are you today"`。在训练过程中，模型学会在看到 `'how'` 后预测 `'are'`，在看到 `'are'` 后预测 `'you'`，依此类推。这有助于模型学习序列中下一个标记的概率分布。你将在本书后面的内容中再次看到这种做法。
- en: 'We’ll create batches of data for training, with 32 pairs of (x, y) in each
    batch:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为训练创建数据批次，每个批次包含32对(x, y)。
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We now have the training dataset. Next, we’ll create an LSTM model and train
    it using the data we just processed.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了训练数据集。接下来，我们将创建一个LSTM模型，并使用我们刚刚处理的数据来训练它。
- en: 8.4 Building and training the LSTM model
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 构建和训练LSTM模型
- en: In this section, you’ll begin by constructing an LSTM model using PyTorch’s
    built-in LSTM layer. This model will start with a word embedding layer, which
    transforms each index into a dense vector of 128 dimensions. Your training data
    will pass through this embedding layer before being fed into the LSTM layer. This
    LSTM layer is designed to process elements of a sequence in a sequential manner.
    Following the LSTM layer, the data will proceed to a linear layer, which has an
    output size matching the size of your vocabulary. The outputs generated by the
    LSTM model are essentially logits, serving as inputs for the softmax function
    to compute probabilities.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将开始使用PyTorch的内置LSTM层构建一个LSTM模型。这个模型将从词嵌入层开始，将每个索引转换为一个128维的密集向量。你的训练数据将通过这个嵌入层，然后输入到LSTM层。这个LSTM层被设计成按顺序处理序列的元素。在LSTM层之后，数据将进入一个线性层，其输出大小与你的词汇表大小相匹配。LSTM模型生成的输出本质上是logits，作为softmax函数的输入来计算概率。
- en: Once you have built the LSTM model, the next step will involve using your training
    data to train this model. This training phase is crucial to refine the model’s
    ability to understand and generate patterns consistent with the data it has been
    fed.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你构建了LSTM模型，下一步将涉及使用你的训练数据来训练这个模型。这个训练阶段对于提高模型理解和生成与提供的数据一致的模式的能力至关重要。
- en: 8.4.1 Building an LSTM model
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 构建LSTM模型
- en: In listing 8.3, we define a `WordLSTM()` class to serve as our LSTM model to
    be trained to generate text in the style of *Anna Karenina*. The class is defined
    as shown in the following listing.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表8.3中，我们定义了一个`WordLSTM()`类，作为我们的LSTM模型，用于训练以生成《安娜·卡列尼娜》风格的文本。该类定义如下所示。
- en: Listing 8.3 Defining the `WordLSTM()` class
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.3 定义`WordLSTM()`类
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Training data first goes through an embedding layer.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ① 训练数据首先通过嵌入层。
- en: ② Creates an LSTM layer with the PyTorch LSTM() class
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用 PyTorch LSTM() 类创建一个 LSTM 层
- en: ③ In each time step, the LSTM layer uses the previous token and the hidden state
    to predict the next token and the next hidden state.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在每个时间步中，LSTM层使用前一个标记和隐藏状态来预测下一个标记和下一个隐藏状态。
- en: ④ Initiates the hidden state for the first token in the input sequence
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 初始化输入序列中第一个标记的隐藏状态
- en: 'The `WordLSTM()` class defined previously has three layers: the word embedding
    layer, the LSTM layer, and a final linear layer. We set the value of the argument
    `n_layers` to 3, which means the LSTM layer stacks three LSTMs together to form
    a stacked LSTM, with the last two LSTMs taking the output from the previous LSTM
    as input. The `init_hidden()` method fills the hidden state with zeros when the
    model uses the first element in the sequence to make predictions. In each time
    step, the input is the current token and the previous hidden state while the output
    is the next token and the next hidden state.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 之前定义的 `WordLSTM()` 类有三个层：词嵌入层、LSTM层和最终的线性层。我们将 `n_layers` 参数的值设置为 3，这意味着LSTM层将三个LSTM堆叠在一起形成一个堆叠的LSTM，最后两个LSTM将前一个LSTM的输出作为输入。`init_hidden()`
    方法在模型使用序列的第一个元素进行预测时将隐藏状态填充为零。在每个时间步中，输入是当前标记和前一个隐藏状态，输出是下一个标记和下一个隐藏状态。
- en: How the `torch.nn.Embedding()` class works
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Embedding()` 类的工作原理'
- en: The `torch.nn.Embedding()` class in PyTorch is used to create an embedding layer
    in a neural network. An embedding layer is a trainable lookup table that maps
    integer indexes to dense, continuous vector representations (embeddings).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 中的 `torch.nn.Embedding()` 类用于在神经网络中创建嵌入层。嵌入层是一个可训练的查找表，它将整数索引映射到密集的、连续的向量表示（嵌入）。
- en: 'When you create an instance of `torch.nn.Embedding()`, you need to specify
    two main parameters: num_embeddings, the size of the vocabulary (total number
    of unique tokens), and embedding_dim, the size of each embedding vector (the dimensionality
    of the output embeddings).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建 `torch.nn.Embedding()` 的实例时，你需要指定两个主要参数：num_embeddings，词汇表的大小（唯一标记的总数），以及
    embedding_dim，每个嵌入向量的大小（输出嵌入的维度性）。
- en: Internally, the class creates a matrix (or lookup table) of shape (num_embeddings,
    embedding_dim) where each row corresponds to the embedding vector for a particular
    index. Initially, these embeddings are randomly initialized but are learned and
    updated during training through backpropagation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，该类创建一个形状为 (num_embeddings, embedding_dim) 的矩阵（或查找表），其中每一行对应于特定索引的嵌入向量。最初，这些嵌入是随机初始化的，但在训练过程中通过反向传播进行学习和更新。
- en: When you pass a tensor of indexes to the embedding layer (during the forward
    pass of the network), it looks up the corresponding embedding vectors in the lookup
    table and returns them. More information about the class is provided by PyTorch
    at [https://mng.bz/n0Zd](https://mng.bz/n0Zd).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将索引张量传递给嵌入层（在网络的前向传递过程中）时，它会在查找表中查找相应的嵌入向量并返回它们。关于该类的更多信息可在 PyTorch [https://mng.bz/n0Zd](https://mng.bz/n0Zd)
    获取。
- en: 'We create an instance of the `WordLSTM()` class and use it as our LSTM model,
    as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建 `WordLSTM()` 类的实例，并将其用作我们的LSTM模型，如下所示：
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When the LSTM model is created, the weights are randomly initialized. When we
    use pairs of (x, y) to train the model, LSTM learns to predict the next token
    based on all previous tokens in the sequence by adjusting the model parameters.
    As we have illustrated in figure 8.2, LSTM learns to predict the next token and
    the next hidden state based on the current token and the current hidden state,
    which is a summary of the information in all previous tokens.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建LSTM模型时，权重是随机初始化的。当我们使用 (x, y) 对来训练模型时，LSTM通过调整模型参数学习根据序列中的所有先前标记预测下一个标记。正如我们在图
    8.2 中所展示的，LSTM学习根据当前标记和当前隐藏状态预测下一个标记和下一个隐藏状态，这是所有先前标记信息的总结。
- en: 'We use the Adam optimizer with a learning rate of 0.0001\. The loss function
    is the cross-entropy loss since this is essentially a multicategory classification
    problem: the model is trying to predict the next token from a dictionary with
    12,778 choices:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用学习率为 0.0001 的 Adam 优化器。损失函数是交叉熵损失，因为这是一个多类别分类问题：模型试图从包含 12,778 个选择的字典中预测下一个标记：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that the LSTM model is built, we’ll train the model with the batches of
    training data we prepared before.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在LSTM模型已经构建，我们将使用之前准备好的训练数据批次来训练模型。
- en: 8.4.2 Training the LSTM model
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 训练 LSTM 模型
- en: During each training epoch, we go through all data batches of data (x, y) in
    the training set. The LSTM model receives the input sequence, x, and generates
    a predicted output sequence, *ŷ*. This prediction is compared with the actual
    output sequence, y, to compute the cross-entropy loss since we essentially conduct
    a multicategory classification here. We then tweak the model’s parameters to reduce
    this loss, as we did in chapter 2 when classifying clothing items.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练周期中，我们遍历训练集中的所有数据批次(x, y)。LSTM模型接收输入序列x，并生成一个预测输出序列 *ŷ*。这个预测与实际输出序列y进行比较，以计算交叉熵损失，因为我们实际上在这里进行多类别分类。然后我们调整模型的参数以减少这个损失，就像我们在第2章中分类服装物品时做的那样。
- en: Though we could divide our data into training and validation sets, training
    the model until no further improvements are seen on the validation set (as we
    have done in chapter 2), our primary aim here is to grasp how LSTM models function,
    not necessarily to achieve the best parameter tuning. Therefore, we’ll train the
    model for 50 epochs.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以将我们的数据分为训练集和验证集，直到在验证集上不再看到进一步的改进（就像我们在第2章中所做的那样）来训练模型，但我们的主要目标是掌握LSTM模型的工作原理，而不一定是实现最佳参数调整。因此，我们将模型训练50个周期。
- en: Listing 8.4 Training the LSTM model to generate text
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.4 训练LSTM模型以生成文本
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Iterates through all batches of (x,y) in the training data
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历训练数据中的所有(x,y)批次
- en: ② Uses the model to predict the output sequence
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用模型来预测输出序列
- en: ③ Compares the predictions with the actual output and calculates the loss
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将预测结果与实际输出进行比较，并计算损失
- en: ④ Tweaks model parameters to minimize loss
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 调整模型参数以最小化损失
- en: In the preceding code listing, `sh` and `sc` together form the hidden state.
    In particular, the cell state `sc` acts as a conveyor belt, carrying information
    over many time steps, with information added or removed in each time step. The
    component `sh` is the output of the LSTM cell at a given time step. It contains
    information about the current input and is used to pass information to the next
    LSTM cell in the sequence.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码列表中，`sh` 和 `sc` 一起构成了隐藏状态。特别是，细胞状态 `sc` 作为传送带，在许多时间步中携带信息，每个时间步都会添加或删除信息。`sh`
    是LSTM单元在给定时间步的输出。它包含有关当前输入的信息，并用于将信息传递给序列中的下一个LSTM单元。
- en: 'If you have a CUDA-enabled GPU, this training takes about 6 hours. If you use
    CPU only, it may take a day or two, depending on your hardware. Or you can download
    the pretrained weights from my website: [https://mng.bz/vJZa](https://mng.bz/vJZa).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有CUDA支持的GPU，这次训练大约需要6个小时。如果您只使用CPU，可能需要一两天，具体取决于您的硬件。或者您可以从我的网站下载预训练的权重：[https://mng.bz/vJZa](https://mng.bz/vJZa)。
- en: 'Next, we save the trained model weights in the local folder:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练好的模型权重保存在本地文件夹中：
- en: '[PRE15]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The dictionary `word_to_int` is also saved on your computer, which is a practical
    step ensuring that you can generate text using the trained model without needing
    to repeat the tokenization process.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 字典 `word_to_int` 也保存在您的计算机上，这是一个实用的步骤，确保您可以在不重复分词过程的情况下，使用训练好的模型生成文本。
- en: 8.5 Generating text with the trained LSTM model
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 使用训练好的LSTM模型生成文本
- en: Now that you have a trained LSTM model, you’ll learn how to use it to generate
    text in this section. The goal is to see if the trained model can generate grammatically
    correct and coherent text by iteratively predicting the next token based on previous
    tokens. You’ll also learn to use temperature and top-K sampling to control the
    creativeness of the generated text.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经有一个训练好的LSTM模型，您将在这个部分学习如何使用它来生成文本。目标是查看训练好的模型是否可以通过迭代预测下一个标记来生成语法正确且连贯的文本。您还将学习如何使用温度和top-K采样来控制生成文本的创造性。
- en: When generating text with the trained LSTM model, we start with a prompt as
    the initial input to the model. We use the trained model to predict the most likely
    next token. After appending the next token to the prompt, we feed the new sequence
    to the trained model to predict the next token again. We repeat this process until
    the sequence reaches a certain length.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用训练好的LSTM模型生成文本时，我们以提示作为模型的初始输入。我们使用训练好的模型来预测最可能的下一个标记。将下一个标记附加到提示后，我们将新的序列输入到训练好的模型中，再次预测下一个标记。我们重复这个过程，直到序列达到一定的长度。
- en: 8.5.1 Generating text by predicting the next token
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.1 通过预测下一个标记来生成文本
- en: 'First, we load the trained model weights and the dictionary `word_to_int` from
    the local folder:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从本地文件夹中加载训练好的模型权重和字典 `word_to_int`：
- en: '[PRE16]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The file `word_to_int.p` is also available in the book’s GitHub repository.
    We switch the positions of keys and values in the dictionary `word_to_int` to
    create the dictionary `int_to_word`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 书籍的GitHub仓库中也提供了`word_to_int.p`文件。我们将字典`word_to_int`中的键和值的顺序进行交换，以创建字典`int_to_word`。
- en: 'To generate text with the trained LSTM model, we need a prompt as the starting
    point of the generated text. We’ll set the default prompt to “Anna and the.” An
    easy way to determine when to stop is to limit the generated text to a certain
    length, say 200 tokens: once the desired length is reached, we ask the model to
    stop generating.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用训练好的LSTM模型生成文本，我们需要一个提示作为生成文本的起点。我们将默认提示设置为“Anna and the”。一个简单的方法是限制生成的文本长度，例如200个标记：一旦达到所需长度，我们就要求模型停止生成。
- en: The following listing defines a `sample()` function to generate text based on
    a prompt.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表定义了一个`sample()`函数，用于根据提示生成文本。
- en: Listing 8.5 A `sample()` function to generate text
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5：用于生成文本的`sample()`函数
- en: '[PRE17]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① Determines how many tokens need to be generated
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ① 确定需要生成多少个标记
- en: ② The input is the current sequence; trims it if it’s longer than 100 tokens
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ② 输入是当前序列；如果它超过100个标记，则对其进行修剪
- en: ③ Makes a prediction using the trained model
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用训练模型进行预测
- en: ④ Selects the next token based on predicted probabilities
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 根据预测概率选择下一个标记
- en: ⑤ Appends the predicted next token to the sequence and repeats
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将预测的下一个标记添加到序列中并重复
- en: The function `sample()` takes three arguments. The first is the trained LSTM
    model you will be using. The second is the starting prompt for text generation,
    which can be a phrase of any length, in quotes. The third parameter specifies
    the length of the text to be generated, measured in tokens, with a default value
    of 200 tokens.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample()`函数接受三个参数。第一个是您将要使用的训练好的LSTM模型。第二个是文本生成的起始提示，可以是任何长度的短语，用引号括起来。第三个参数指定要生成的文本长度，以标记为单位，默认值为200个标记。'
- en: Within the function, we first deduct the number of tokens in the prompt from
    the total desired length to determine the number of tokens that need to be generated.
    When generating the next token, we consider the current sequence’s length. If
    it’s under 100 tokens, we input the entire sequence into the model; if it’s over
    100 tokens, only the last 100 tokens of the sequence are used as input. This input
    is then fed into the trained LSTM model to predict the subsequent token, which
    we then add to the current sequence. We continue this process until the sequence
    reaches the desired length.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数内部，我们首先从总期望长度中减去提示中的标记数量，以确定需要生成的标记数量。在生成下一个标记时，我们考虑当前序列的长度。如果它少于100个标记，我们将整个序列输入到模型中；如果它超过100个标记，则只使用序列的最后100个标记作为输入。然后将这个输入送入训练好的LSTM模型来预测下一个标记，我们将这个预测的标记添加到当前序列中。我们继续这个过程，直到序列达到所需长度。
- en: When generating the next token, the model employs the random.choice(len(logits),
    p = p) method from NumPy. Here, the method’s first parameter indicates the range
    of choices, which in this case is len(logits) = 12778\. This signifies that the
    model will randomly select an integer from 0 to 12,777, with each integer corresponding
    to a different token in the vocabulary. The second parameter, p, is an array containing
    12,778 elements where each element denotes the probability of selecting a corresponding
    token from the vocabulary. Tokens with a higher probability in this array are
    more likely to be chosen.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当生成下一个标记时，模型使用NumPy中的random.choice(len(logits), p = p)方法。在这里，该方法的第一参数表示选择范围，在这个例子中是len(logits)
    = 12778。这意味着模型将随机选择一个从0到12,777的整数，每个整数对应词汇表中的一个不同标记。第二个参数p是一个包含12,778个元素的数组，其中每个元素表示从词汇表中选择相应标记的概率。在这个数组中，概率较高的标记更有可能被选中。
- en: 'Let’s generate a passage with the model using “Anna and the prince” as the
    prompt (make sure you put a space before punctuation marks when you use your own
    prompt):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用“Anna and the prince”作为提示（确保在使用自己的提示时在标点符号前加上空格）来生成一段文本：
- en: '[PRE18]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, I fixed the random seed number to 42 in both PyTorch and NumPy in case
    you want to reproduce results. The generated passage is
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将PyTorch和NumPy中的随机种子数固定为42，以防你想要重现结果。生成的段落是
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You may have noticed that the text generated is entirely in lowercase. This
    is because, during the text processing stage, we converted all uppercase letters
    to lowercase to minimize the number of unique tokens.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到生成的文本完全是小写的。这是因为，在文本处理阶段，我们将所有大写字母转换为小写，以最小化唯一标记的数量。
- en: The text generated from 6 hours of training is quite impressive! Most of the
    sentences adhere to grammatical norms. While it may not match the level of sophistication
    seen in text generated by advanced systems like ChatGPT, it’s a significant achievement.
    With skills acquired in this exercise, you are ready to train more advanced text
    generation models in later chapters.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 经过6小时的训练生成的文本相当令人印象深刻！大多数句子都遵循语法规范。虽然它可能无法与ChatGPT等高级系统生成的文本的复杂程度相媲美，但这仍然是一个重大的成就。通过在这个练习中获得的能力，你将准备好在后面的章节中训练更高级的文本生成模型。
- en: 8.5.2 Temperature and top-K sampling in text generation
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.2 文本生成中的温度和top-K采样
- en: The creativity of the generated text can be controlled by using techniques like
    temperature and top-K sampling.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用温度和top-K采样等技术来控制生成文本的创造力。
- en: Temperature adjusts the distribution of probabilities assigned to each potential
    token before selecting the next one. It effectively scales the logits, which are
    the inputs to the softmax function calculating these probabilities, by the value
    of the temperature. Logits are the outputs of the LSTM model prior to the application
    of the softmax function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 温度调整在选择下一个标记之前分配给每个潜在标记的概率分布。它通过温度的值有效地缩放logits，即计算这些概率的softmax函数的输入。logits是应用softmax函数之前的LSTM模型的输出。
- en: In the `sample()` function we just defined, we didn’t adjust the logits, implying
    a default temperature of 1\. A lower temperature (below 1; e.g., 0.8) results
    in fewer variations, making the model more deterministic and conservative, favoring
    more likely choices. Conversely, a higher temperature (above 1; e.g., 1.5) makes
    it more likely to choose improbable words in text generation, leading to more
    varied and inventive outputs. However, this could also make the text less coherent
    or relevant, as the model might opt for less probable words.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚定义的`sample()`函数中，我们没有调整logits，这意味着默认温度为1。较低的温度（低于1；例如，0.8）会导致更少的变体，使模型更加确定性和保守，更倾向于更可能的选择。相反，较高的温度（高于1；例如，1.5）使得在文本生成中选择不可能的单词的可能性更大，导致更加多样化和有创造性的输出。然而，这也可能使文本变得不那么连贯或相关，因为模型可能会选择不太可能的单词。
- en: Top-K sampling is another method to influence the output. This approach involves
    selecting the next word from the top K most probable options as predicted by the
    model. The probability distribution is truncated to include only the top K words.
    With a small K value, such as 5, the model’s choices are limited to a few highly
    probable words, resulting in more predictable and coherent but potentially less
    diverse and interesting outputs. In the `sample()` function we defined earlier,
    we did not apply top-K sampling, so the value of K was effectively the size of
    the vocabulary (12,778 in our case).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Top-K采样是另一种影响输出的方法。这种方法涉及从模型预测的前K个最可能选项中选择下一个单词。概率分布被截断，只包括前K个单词。当K值较小时，例如5，模型的选项仅限于几个高度可能的单词，导致输出更加可预测和连贯，但可能不那么多样化和有趣。在我们之前定义的`sample()`函数中，我们没有应用top-K采样，因此K的值实际上是词汇表的大小（在我们的例子中为12,778）。
- en: 'Next, we introduce a new function, `generate()`, for text generation. This
    function is similar to the `sample()` function but includes two additional parameters:
    `temperature` and `top_k`, allowing for more control over the creativity and randomness
    of the generated text. The function `generate()` is defined in the following listing.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍一个新的函数`generate()`，用于文本生成。这个函数与`sample()`函数类似，但包括两个额外的参数：`temperature`和`top_k`，允许对生成文本的创造性和随机性有更多的控制。`generate()`函数的定义如下所示。
- en: Listing 8.6 Generating text with temperature and top-K sampling
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6 使用温度和top-K采样生成文本
- en: '[PRE20]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① Scales the logits with temperature
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用温度缩放logits
- en: ② Keeps only the K most probable candidates
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ② 仅保留K个最可能的候选词
- en: ③ Selects the next token from the top K candidates
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 从前K个候选词中选择下一个标记
- en: 'Compared to the `sample()` function, the new function `generate()` has two
    more optional arguments: `top_k` and `temperature`. By default, `top_k` is set
    to `None,` and `temperature` is set to 1\. Therefore, if you call the `generate()`
    function without specifying these two arguments, the output will be the same as
    what you would get from the function `sample()`.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与`sample()`函数相比，新的函数`generate()`有两个额外的可选参数：`top_k`和`temperature`。默认情况下，`top_k`设置为`None`，而`temperature`设置为1。因此，如果你在调用`generate()`函数时没有指定这两个参数，输出将与从`sample()`函数获得的输出相同。
- en: 'Let’s illustrate the variations in generated text by focusing on the creation
    of a single token. For this purpose, we’ll use “I ’ m not going to see” as the
    prompt (note the space before the apostrophe, as we previously have done in the
    chapter). We call the `generate()` function 10 times, setting its length argument
    to be one more than the prompt’s length. This approach ensures that the function
    appends only one extra token to the prompt:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过关注单个标记的创建来展示生成文本的变化。为此，我们将使用“我不打算去看”作为提示（注意撇号前的空格，就像我们在本章中之前所做的那样）。我们调用`generate()`函数10次，将其长度参数设置为比提示长度多一个。这种方法确保函数只向提示添加一个额外的标记：
- en: '[PRE21]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The output is
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE22]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With the default setting of top_k = None and temperature = 1, there is some
    degree of repetition in the output. For example, the word “you” was repeated three
    times. There are a total of six unique tokens.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在默认的`top_k = None`和`temperature = 1`设置下，输出中存在一定程度的重复。例如，“you”这个词重复了三次。总共有六个独特的标记。
- en: However, the functionality of `generate()` expands when you adjust these two
    arguments. For instance, setting a low temperature, like 0.5, and a small `top_k`
    value, such as 3, results in generated text that is more predictable and less
    creative.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你调整这两个参数时，`generate()`函数的功能会扩展。例如，设置一个低的温度，如0.5，以及一个小的`top_k`值，例如3，会导致生成的文本更可预测且更缺乏创意。
- en: 'Let’s repeat the single token example. This time, we set the temperature to
    0.5 and `top_k` value to 3:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重复单个标记的示例。这次，我们将温度设置为0.5，`top_k`值设置为3：
- en: '[PRE23]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The output is
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE24]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output has fewer variations: there are only 3 unique tokens from 10 attempts,
    “you,” “the,” and “her.”'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的变化较少：在10次尝试中，只有3个独特的标记，“you”、“the”和“her”。
- en: 'Let’s see this in action by using “Anna and the prince” as our starting prompt
    when we set the temperature to 0.5 and `top_k` value to 3:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将“安娜和王子”作为起始提示，在设置温度为0.5和`top_k`值为3时来观察这一效果：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The output is
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Exercise 8.4
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.4
- en: Generate text by setting temperature to 0.6 and `top_k` to 10 and using “Anna
    and the nurse” as the starting prompt. Set the random seed number to 0 in both
    PyTorch and NumPy.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置温度为0.6和`top_k`为10，并使用“安娜和护士”作为起始提示来生成文本。在PyTorch和NumPy中都设置随机种子数为0。
- en: 'Conversely, opting for a higher `temperature` value, like 1.5, coupled with
    a higher `top_k` value, for instance, `None` (enabling selection from the entire
    pool of 12,778 tokens), leads to outputs that are more creative and less predictable.
    This is demonstrated next, in the single token example. This time, we set the
    temperature to 2 and `top_k` value to `None`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，选择一个更高的`温度`值，例如1.5，以及一个更高的`top_k`值，例如`None`（允许从12,778个标记的全集中进行选择），会导致更富有创造性和更不可预测的输出。这将在下面的单个标记示例中演示。这次，我们将温度设置为2，`top_k`值设置为`None`：
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The output is
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE28]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output has almost no repetition: there are 9 unique tokens from 10 attempts;
    only the word “it” was repeated.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 输出几乎没有重复：在10次尝试中有9个独特的标记；只有“it”这个词重复了。
- en: 'Let’s again use “Anna and the prince” as the initial prompt but set the temperature
    to 2 and top_k value to None and see what happens:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次使用“安娜和王子”作为初始提示，但将温度设置为2，`top_k`值设置为`None`，看看会发生什么：
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The generated text is
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本是
- en: '[PRE30]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The output generated is not repetitive, although it lacks coherence in many
    places.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本没有重复，尽管在许多地方缺乏连贯性。
- en: Exercise 8.5
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.5
- en: Generate text by setting temperature to 2 and `top_k` to 10000 and using “Anna
    and the nurse” as the starting prompt. Set the random seed number to 0 in both
    PyTorch and NumPy.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置温度为2和`top_k`为10000，并使用“安娜和护士”作为起始提示来生成文本。在PyTorch和NumPy中都设置随机种子数为0。
- en: In this chapter, you have acquired foundational skills in NLP, including word-level
    tokenization, word embedding, and sequence prediction. Through these exercises,
    you’ve learned to construct a language model based on word-level tokenization
    and have trained it using LSTM for text generation. Moving forward, the next few
    chapters will introduce you to training Transformers, the type of models used
    in systems like ChatGPT. This will provide you with a more in-depth understanding
    of advanced text generation techniques.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经掌握了NLP的基础技能，包括词级分词、词嵌入和序列预测。通过这些练习，你学会了基于词级分词构建语言模型，并使用LSTM进行文本生成训练。接下来，接下来的几章将向你介绍训练Transformers，这是ChatGPT等系统中使用的模型类型。这将为你提供更深入的高级文本生成技术理解。
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: RNNs are a specialized form of artificial neural network designed to recognize
    patterns in sequences of data, such as text, music, or stock prices. Unlike traditional
    neural networks, which process inputs independently, RNNs have loops in them,
    allowing information to persist. LSTM networks are improved versions of RNNs.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNNs（循环神经网络）是一种专门的人工神经网络形式，旨在识别数据序列中的模式，如文本、音乐或股价。与传统神经网络不同，后者独立处理输入，RNNs在其内部有循环，允许信息持续存在。LSTM网络是RNNs的改进版本。
- en: There are three approaches to tokenization. The first is character tokenization,
    where the text is divided into its constituent characters. The second approach
    is word tokenization, where the text is split into individual words. The third
    approach is subword tokenization, which breaks words into smaller, meaningful
    components called subwords.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有三种分词方法。第一种是字符分词，将文本分割成其构成字符。第二种方法是词分词，将文本分割成单个单词。第三种方法是子词分词，它将单词分解成更小的、有意义的组件，称为子词。
- en: Word embedding is a method that transforms words into compact vector representations,
    capturing their semantic information and interrelationships. This technique is
    vital in NLP, especially since deep neural networks, including models like LSTM
    and Transformers, require numerical input.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入是一种将单词转换成紧凑向量表示的方法，捕捉其语义信息和相互关系。这项技术在NLP中至关重要，尤其是在深度神经网络，包括LSTM和Transformers等模型，需要数值输入的情况下。
- en: Temperature is a parameter that influences the behavior of text generation models.
    It controls the randomness of the predictions by scaling the logits (the inputs
    to the softmax function for probability calculation) before applying softmax.
    Low temperature makes the model more conservative in its predictions but also
    more repetitive. At higher temperatures, the model becomes less repetitive and
    more innovative, increasing the diversity of the generated text.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温度是影响文本生成模型行为的一个参数。它通过在应用softmax之前缩放logits（概率计算的softmax函数的输入）来控制预测的随机性。低温使模型在预测上更加保守但也更加重复。在较高温度下，模型变得不那么重复，更具创新性，增加了生成文本的多样性。
- en: Top-K sampling is another way to influence the behavior of text generation models.
    It involves selecting the next word from the K most likely candidates, as determined
    by the model. The probability distribution is truncated to keep only the top K
    words. Small values of K make the output more predictable and coherent but potentially
    less diverse and interesting.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Top-K采样是另一种影响文本生成模型行为的方法。它涉及从模型确定的K个最可能的候选词中选择下一个词。概率分布被截断，只保留前K个词。K的值较小会使输出更加可预测和连贯，但可能不那么多样化和有趣。
- en: '* * *'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '^([1](#footnote-000-backlink))  Sepp Hochreiter and Jurgen Schmidhuber, 1997,
    “Long Short-Term Memory,” *Neural Computation* 9(8): 1735-1780.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-000-backlink))  Sepp Hochreiter 和 Jurgen Schmidhuber，1997年，“长短期记忆”，*神经计算*
    9(8)：1735-1780。
