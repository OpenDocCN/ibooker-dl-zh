- en: 8 Text generation with recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind RNNs and why they can handle sequential data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Character tokenization, word tokenization, and subword tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How word embedding works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and training an RNN to generate text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using temperature and top-K sampling to control the creativeness of text generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far in this book, we have discussed how to generate shapes, numbers, and
    images. Starting from this chapter, we’ll focus mainly on text generation. Generating
    text is often considered the holy grail of generative AI for several compelling
    reasons. Human language is incredibly complex and nuanced. It involves understanding
    not only grammar and vocabulary but also context, tone, and cultural references.
    Successfully generating coherent and contextually appropriate text is a significant
    challenge that requires deep understanding and processing of language.
  prefs: []
  type: TYPE_NORMAL
- en: As humans, we primarily communicate through language. AI that can generate human-like
    text can interact more naturally with users, making technology more accessible
    and user-friendly. Text generation has many applications, from automating customer
    service responses to creating entire articles, scripting for games and movies,
    aiding in creative writing, and even building personal assistants. The potential
    effect across industries is enormous.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll make our first attempt at building and training models
    to generate text. You’ll learn to tackle three main challenges in modeling text
    generation. First, text is sequential data, consisting of data points organized
    in a specific sequence, where each point is successively ordered to reflect the
    inherent order and interdependencies within the data. Predicting outcomes for
    sequences is challenging due to their sensitive ordering. Altering the sequence
    of elements changes their meaning. Second, text exhibits long-range dependencies:
    the meaning of a certain part of the text depends on elements that appeared much
    earlier in the text (e.g., 100 words ago). Understanding and modeling these long-range
    dependencies is essential for generating coherent text. Lastly, human language
    is ambiguous and context dependent. Training a model to understand nuances, sarcasm,
    idioms, and cultural references to generate contextually accurate text is challenging.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll explore a specific neural network designed for handling sequential data,
    such as text or time series: the recurrent neural network (RNN). Traditional neural
    networks, such as feedforward neural networks or fully connected networks, treat
    each input independently. This means that the network processes each input separately,
    without considering any relationship or order between different inputs. In contrast,
    RNNs are specifically designed to handle sequential data. In an RNN, the output
    at a given time step depends not only on the current input but also on previous
    inputs. This allows RNNs to maintain a form of memory, capturing information from
    previous time steps to influence the processing of the current input.'
  prefs: []
  type: TYPE_NORMAL
- en: This sequential processing makes RNNs suitable for tasks where the order of
    the inputs matters, such as language modeling, where the goal is to predict the
    next word in a sentence based on previous words. We’ll focus on one variant of
    RNN, long short-term memory (LSTM) networks, which can recognize both short-term
    and long-term data patterns in sequential data like text. LSTM models use a hidden
    state to capture information in previous time steps. Therefore, a trained LSTM
    model can produce coherent text based on the context.
  prefs: []
  type: TYPE_NORMAL
- en: The style of the generated text depends on the training data. Additionally,
    as we plan to train a model from scratch for text generation, the length of the
    training text is a crucial factor. It needs to be sufficiently extensive for the
    model to effectively learn and mimic a particular writing style yet concise enough
    to avoid excessive computational demands during training. As a result, we’ll use
    the text from the novel *Anna Karenina*, which appears to be of the right length
    for our purposes, to train an LSTM model. Since neural networks like an LSTM cannot
    accept text as input directly, you’ll learn to break down text into tokens (individual
    words in this chapter but can be parts of words, as you’ll see in later chapters),
    a process known as *tokenization*. You’ll then create a dictionary to map each
    unique token into an integer (i.e., an index). Based on this dictionary, you’ll
    convert the text into a long sequence of integers, ready to be fed into a neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll use sequences of indexes of a certain length as the input to train the
    LSTM model. You shift the sequence of inputs by one token to the right and use
    it as the output: you are effectively training the model to predict the next token
    in a sentence. This is the so-called *sequence-to-sequence* prediction problem
    in natural language processing (NLP), and you’ll see it again in later chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the LSTM is trained, you’ll use it to generate text one token at a time
    based on previous tokens in the sequence as follows: you feed a prompt (part of
    a sentence such as “Anna and the”) to the trained model. The model then predicts
    the most likely next token and appends the selected token to your prompt. The
    updated prompt serves again as the input, and the model is used once more to predict
    the next token. The iterative process continues until the prompt reaches a certain
    length. This approach is similar to the mechanism employed by more advanced generative
    models like ChatGPT (though ChatGPT is not an LSTM). You’ll witness the trained
    LSTM model generating grammatically correct and coherent text, with a style matching
    that of the original novel.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you also learn how to control the creativeness of the generated text
    using temperature and top-K sampling. Temperature controls the randomness of the
    predictions of the trained model. A high temperature makes the generated text
    more creative while a low temperature makes the text more confident and predictable.
    Top-K sampling is a method where you select the next token from the top K most
    probable tokens, rather than selecting from the entire vocabulary. A small value
    of K leads to the selection of highly likely tokens in each step, and this, in
    turn, makes the generated text less creative and more coherent.
  prefs: []
  type: TYPE_NORMAL
- en: The primary goal of this chapter is not necessarily to generate the most coherent
    text possible, which, as mentioned earlier, presents substantial challenges. Instead,
    our objective is to demonstrate the limitations of RNNs, thereby setting the stage
    for the introduction of Transformers in subsequent chapters. More importantly,
    this chapter establishes the basic principles of text generation, including tokenization,
    word embedding, sequence prediction, temperature settings, and top-K sampling.
    Consequently, in later chapters, you will have a solid understanding of the fundamentals
    of NLP. This foundation will allow us to concentrate on other, more advanced aspects
    of NLP, such as how the attention mechanism functions and the architecture of
    Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Introduction to RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we touched upon the complexities involved
    in generating text, particularly when aiming for coherence and contextual relevance.
    This section dives deeper into these challenges and explores the architecture
    of RNNs. We’ll explain why RNNs are suitable for the task and their limitations
    (which are the reasons they have been overtaken by Transformers).
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are specifically designed to handle sequential data, making them capable
    of text generation, a task inherently sequential in nature. They utilize a form
    of memory, known as hidden states, to capture and retain information from earlier
    parts of the sequence. This capability is crucial for maintaining context and
    understanding dependencies as the sequence progresses.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will specifically utilize LSTM networks, advanced versions
    of RNNs, for text generation, using their advanced capabilities to tackle the
    challenges in this task.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Challenges in generating text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Text represents a quintessential example of *sequential data*, which is defined
    as any dataset where the order of elements is critical. This structuring implies
    that the positioning of individual elements relative to each other holds significant
    meaning, often conveying essential information for understanding the data. Examples
    of sequential data include time series (like stock prices), textual content (such
    as sentences), and musical compositions (a succession of notes).
  prefs: []
  type: TYPE_NORMAL
- en: This book primarily zeroes in on text generation, although it also ventures
    into music generation in chapters 13 and 14\. The process of generating text is
    fraught with complexities. A primary challenge lies in modeling the sequence of
    words within sentences, where altering the order can drastically change the meaning.
    For instance, in the sentence “Kentucky defeated Vanderbilt in last night’s football
    game,” swapping ‘Kentucky’ and ‘Vanderbilt’ entirely reverses the sentence’s implication,
    despite using the same words. Furthermore, as mentioned in the introduction, text
    generation encounters challenges in handling long-range dependencies and dealing
    with the problem of ambiguity.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore one approach to tackle these challenges—namely,
    by using RNNs. While this method isn’t flawless, it lays the groundwork for more
    advanced techniques you’ll encounter in later chapters. This approach will provide
    insight into managing word order, addressing long-range dependencies, and navigating
    the inherent ambiguity in text, equipping you with fundamental skills in text
    generation. The journey through this chapter serves as a stepping stone to more
    sophisticated methods and deeper understanding in the subsequent parts of the
    book. Along the way, you’ll acquire many valuable skills in NLP, such as text
    tokenization, word embedding, and sequence-to-sequence predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 How do RNNs work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RNNs are a specialized form of artificial neural network designed to recognize
    patterns in sequences of data, such as text, music, or stock prices. Unlike traditional
    neural networks, which process inputs independently, RNNs have loops in them,
    allowing information to persist.
  prefs: []
  type: TYPE_NORMAL
- en: One of the challenges in generating text is how to predict the next word based
    on all previous words so that the prediction captures both the long-range dependencies
    and contextual meaning. RNNs take input not just as a standalone item but as a
    sequence (like words in a sentence, for example). At each time step, the prediction
    is based on not only the current input but also all previous inputs in the form
    of a summary through a hidden state. Let’s consider the phrase “a frog has four
    legs” as an example. In the first time step, we use the word “a” to predict the
    second word “frog.” In the second time step, we predict the next word using both
    “a” and “frog.” By the time we predict the last word, we need to use all four
    previous words “a frog has four.”
  prefs: []
  type: TYPE_NORMAL
- en: A key feature of RNNs is the so-called hidden state, which captures information
    in all previous elements in a sequence. This feature is crucial for the network’s
    ability to process and generate sequential data effectively. The functioning of
    RNNs and this sequential processing is depicted in figure 8.1, which illustrates
    how a layer of recurrent neurons unfolds over time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 How a layer of recurrent neurons unfolds through time. When a recurrent
    neural network makes a prediction on sequential data, it takes the hidden state
    from the previous time step, h(t – 1), along with the input at the current time
    step, x(t), and generates the output, y(t), and the updated hidden state, h(t).
    The hidden state at time step t captures the information in all previous time
    steps, x(0), x(1), …, x(t).
  prefs: []
  type: TYPE_NORMAL
- en: The hidden state in RNNs plays a pivotal role in capturing information across
    all time steps. This allows RNNs to make predictions that are informed not just
    by the current input, x(t), but also by the accumulated knowledge from all previous
    inputs, x(0), x(1), …, x(t – 1). This attribute makes RNNs capable of understanding
    temporal dependencies. They can grasp the context from an input sequence, which
    is indispensable for tasks like language modeling, where the preceding words in
    a sentence set the stage for predicting the next word.
  prefs: []
  type: TYPE_NORMAL
- en: However, RNNs are not without their drawbacks. Though standard RNNs are capable
    of handling short-term dependencies, they struggle with longer-range dependencies
    within text. This difficulty stems from the vanishing gradient problem, which
    occurs in long sequences where the gradients (essential for training the network)
    diminish, hindering the model’s ability to learn relationships over longer distances.
    To mitigate this, advanced versions of RNNs, such as LSTM networks, have been
    developed.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM networks were introduced by Hochreiter and Schmidhuber in 1997.^([1](#footnote-000))
    An LSTM network is composed of LSTM units (or cells), each of which has a more
    complex structure than a standard RNN neuron. The cell state is the key innovation
    of LSTMs: it acts as a kind of conveyor belt, running straight down the entire
    chain of LSTM units. It has the ability to carry relevant information through
    the network. The ability to add or remove information to the cell state allows
    LSTMs to capture long-term dependencies and remember information for long periods.
    This makes them more effective for tasks like language modeling and text generation.
    In this chapter, we will harness the LSTM model to undertake a project on text
    generation, aiming to mimic the style of the novel *Anna Karenina*.'
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s noteworthy that even advanced RNN variants like LSTMs encounter
    hurdles in capturing extremely long-range dependencies in sequence data. We will
    discuss these challenges and provide solutions in the next chapter, continuing
    our exploration of sophisticated models for effective sequence data processing
    and generation.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.3 Steps in training a LSTM model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we’ll discuss the steps involved in training an LSTM model to generate
    text. This overview aims to provide a foundational understanding of the training
    process before embarking on the project.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of text for training depends on the desired output. A lengthy novel
    serves as a good starting point. Its extensive content enables the model to learn
    and replicate a specific writing style effectively. An ample amount of text data
    enhances the model’s proficiency in this style. At the same time, novels are generally
    not excessively long, which helps in managing the training time. For our LSTM
    model training, we’ll utilize the text from *Anna Karenina*, aligning with our
    previously outlined training data criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to other deep neural networks, LSTM models cannot process raw text directly.
    Instead, we’ll convert the text into numerical form. This begins by breaking down
    the text into smaller pieces, a process known as tokenization, where each piece
    is a token. Tokens can be entire words, punctuation marks (like an exclamation
    mark or a comma), or special characters (such as & or %). For this chapter, each
    of these elements will be treated as separate tokens. Although this method of
    tokenization may not be the most efficient, it is easy to implement since all
    we need is to map words to tokens. We will use subword tokenization in subsequent
    chapters where some infrequent words are broken into smaller pieces such as syllables.
    Following tokenization, we assign a unique integer to each token, creating a numerical
    representation of the text as a sequence of integers.
  prefs: []
  type: TYPE_NORMAL
- en: To prepare the training data, we divide this long sequence into shorter sequences
    of equal length. For our project, we’ll use sequences comprising 100 integers
    each. These sequences form the features (the *x* variable) of our model. We then
    generate the output *y* by shifting the input sequence one token to the right.
    This setup enables the LSTM model to predict the next token in a sequence. The
    pairs of input and output serve as the training data. Our model includes LSTM
    layers to understand long-term patterns in the text and an embedding layer to
    grasp semantic meanings.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s revisit the example of predicting the sentence “a frog has four legs”
    that we mentioned earlier. Figure 8.2 is a diagram of how the training of the
    LSTM model works.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 An example of how an LSTM model is trained. We first break down the
    training text into tokens and assign a unique integer to each token, creating
    a numerical representation of the text as a sequence of indexes. We then divide
    this long sequence into shorter sequences of equal length. These sequences form
    the features (the x variable) of our model. We then generate the output y by shifting
    the input sequence one token to the right. This setup enables the LSTM model to
    predict the next token based on previous tokens in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In the first time step, the model uses the word “a” to predict the word “frog.”
    Since there’s no preceding word for “a,” we initialize the hidden state with zeros.
    The LSTM model receives both the index for “a” and this initial hidden state as
    input and outputs the predicted next word along with an updated hidden state,
    h0\. In the subsequent time step, the word “frog” and the updated state h0 are
    used to predict “has” and generate a new hidden state, h1\. This sequence of predicting
    the next word and updating the hidden state continues until the model forecasts
    the final word in the sentence, “legs.”
  prefs: []
  type: TYPE_NORMAL
- en: The predictions are then compared to the actual next word in the sentence. Since
    the model is effectively predicting the next token out of all possible tokens
    in the vocabulary, there is a multicategory classification problem. We tweak the
    model parameters in each iteration to minimize the cross-entropy loss so that
    in the next iteration, the model predictions move closer to actual outputs in
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, generating text begins with a seed sequence input
    into the model. The model predicts the next token, which is then appended to your
    sequence. This iterative process of prediction and sequence updating is repeated
    to generate text for as long as desired.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Fundamentals of NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning models, including the LSTM models we discussed earlier and Transformers,
    which you’ll learn in later chapters, cannot process raw text directly because
    they are designed to work with numerical data, typically in the form of vectors
    or matrices. The processing and learning capabilities of neural networks are based
    on mathematical operations like addition, multiplication, and activation functions,
    which require numerical input. Consequently, it’s essential first to break down
    text into smaller, more manageable elements known as tokens. These tokens can
    range from individual characters and words to subword units.
  prefs: []
  type: TYPE_NORMAL
- en: The next crucial step in NLP tasks is transforming these tokens into numerical
    representations. This conversion is necessary for feeding them into deep neural
    networks, which is a fundamental part of training our models.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll discuss different tokenization methods, along with their
    advantages and drawbacks. Additionally, you’ll gain insights into the process
    of converting tokens into dense vector representations—a method known as word
    embedding. This technique is crucial for capturing the meaning of language in
    a format that deep learning models can effectively utilize.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Different tokenization methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenization involves dividing text into smaller parts, known as tokens, which
    can be in the form of words, characters, symbols, or other significant units.
    The primary goal of tokenization is to streamline the process of text data analysis
    and processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Broadly speaking, there are three approaches to tokenization. The first is
    character tokenization, where the text is divided into its constituent characters.
    This method is used in languages with complex morphological structures, such as
    Turkish or Finnish, in which the meaning of words can change significantly with
    slight variations in characters. Take the English phrase “It is unbelievably good!”
    as an example; it’s broken down into individual characters as follows: `[''I'',
    ''t'', '' '', ''i'', ''s'', '' '', ''u'', ''n'', ''b'', ''e'', ''l'', ''i'', ''e'',
    ''v'', ''a'', ''b'', ''l'', ''y'', '' '', ''g'', ''o'', ''o'', ''d'', ''!'']`.
    A key advantage of character tokenization is the limited number of unique tokens.
    This limitation significantly reduces the parameters in deep learning models,
    leading to faster and more efficient training. However, the major drawback is
    that individual characters often lack significant meaning, making it challenging
    for machine learning models to derive meaningful insights from a sequence of characters.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 8.1
  prefs: []
  type: TYPE_NORMAL
- en: Use character tokenization to divide the phrase “Hi, there!” into individual
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second approach is word tokenization, where the text is split into individual
    words and punctuation marks. It is used often in situations where the number of
    unique words is not too large. For instance, the same phrase “It is unbelievably
    good!” becomes five tokens: `[''It'', ''is'', ''unbelievably'', ''good'', ''!'']`.
    The main advantage of this method is that each word inherently carries semantic
    meaning, making it more straightforward for models to interpret the text. The
    downside, however, lies in the substantial increase in unique tokens, which increases
    the number of parameters in deep learning models. This increase can lead to slower
    and less efficient training processes.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 8.2
  prefs: []
  type: TYPE_NORMAL
- en: Use word tokenization to break down the phrase “Hi, how are you?” into individual
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The third approach is subword tokenization. This method, a key concept in NLP,
    breaks text into smaller, meaningful components called subwords. For instance,
    the phrase “It is unbelievably good!” would be divided into tokens like `['It',
    'is', 'un', 'believ', 'ably', 'good', '!']`. Most advanced language models, including
    ChatGPT, use subword tokenization, and you’ll use this method in the next few
    chapters. Subword tokenization strikes a balance between the more traditional
    tokenization techniques that typically split text into either individual words
    or characters. Word-based tokenization, while capturing more meaning, leads to
    a vast vocabulary. Conversely, character-based tokenization results in a smaller
    vocabulary, but each token carries less semantic value.
  prefs: []
  type: TYPE_NORMAL
- en: Subword tokenization effectively mitigates these problems by keeping frequently
    used words whole in the vocabulary while dividing less common or more complex
    words into subcomponents. This technique is particularly advantageous for languages
    with large vocabularies or those exhibiting a high degree of word form variation.
    By adopting subword tokenization, the overall vocabulary size is substantially
    reduced. This reduction enhances the efficiency and effectiveness of language
    processing tasks, especially when dealing with a wide range of linguistic structures.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on word tokenization, as it offers a straightforward
    foundation for beginners. As we progress to later chapters, our attention will
    shift to subword tokenization, utilizing models that have already been trained
    with this technique. This approach allows us to concentrate on more advanced topics,
    such as understanding the Transformer architecture and exploring the inner workings
    of the attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Word embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Word embedding is a method that transforms tokens into compact vector representations,
    capturing their semantic information and interrelationships. This technique is
    vital in NLP, especially since deep neural networks, including models like LSTM
    and Transformers, require numerical input.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, tokens are converted into numbers using one-hot encoding before
    being fed into NLP models. In one-hot encoding, each token is represented by a
    vector where only one element is ‘1’, and the rest are ‘0’s. For example, in this
    chapter, there are 12,778 unique word-based tokens in the text for the novel *Anna
    Karenina*. Each token is represented by a vector of 12,778 dimensions. Consequently,
    a phrase like “happy families are all alike” is represented as a 5 × 12,778 matrix,
    where 5 represents the number of tokens. This representation, however, is highly
    inefficient due to its large dimensionality, leading to an increased number of
    parameters, which can hinder training speed and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs, Transformers, and other advanced NLP models address this inefficiency
    through word embedding. Instead of bulky one-hot vectors, word embedding uses
    continuous, lower-dimensional vectors (e.g., 128-value vectors we use in this
    chapter). As a result, the phrase “happy families are all alike” is represented
    by a more compact 5 × 128 matrix after word embedding. This streamlined representation
    drastically reduces the model’s complexity and enhances training efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding not only reduces word complexity by condensing it into a lower-dimensional
    space but also effectively captures the context and the nuanced semantic relationships
    between words, a feature that simpler representations like one-hot encoding lack,
    for the following reasons. In one-hot encoding, all tokens have the same distance
    from each other in vector space. However, in word embeddings, tokens with similar
    meanings are represented by vectors close to each other in the embedding space.
    Word embeddings are learned from the text in the training data; the resulting
    vectors capture contextual information. Tokens that appear in similar contexts
    will have similar embeddings, even if they are not explicitly related.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding in NLP
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are a powerful method for representing tokens in NLP that offer
    significant advantages over traditional one-hot encoding in capturing context
    and semantic relationships between words.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding represents tokens as sparse vectors with a dimension equal
    to the size of the vocabulary, where each token is represented by a vector with
    all zeros except for a single one at the index corresponding to the token. In
    contrast, word embeddings represent tokens as dense vectors with much lower dimensions
    (e.g., 128 dimensions in this chapter and 256 dimensions in chapter 12). This
    dense representation is more efficient and can capture more information.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, in one-hot encoding, all tokens have the same distance from each
    other in the vector space, meaning there is no notion of similarity between tokens.
    However, in word embeddings, similar tokens are represented by vectors that are
    close to each other in the embedding space. For example, the words “king” and
    “queen” would have similar embeddings, reflecting their semantic relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are learned from the text in the training data. The embedding
    process uses the context in which tokens appear to learn their embeddings, meaning
    that the resulting vectors capture contextual information. Tokens that appear
    in similar contexts will have similar embeddings, even if they are not explicitly
    related.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, word embeddings provide a more nuanced and efficient representation
    of words that captures semantic relationships and contextual information, making
    them more suitable for NLP tasks compared to one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, particularly in frameworks like PyTorch, word embedding
    is implemented by passing indexes through a linear layer, which compresses them
    into a lower-dimensional space. That is, when you pass an index to the `nn.Embedding()`
    layer, it looks up the corresponding row in the embedding matrix and returns the
    embedding vector for that index, avoiding the need to create potentially very
    large one-hot vectors. The weights of this embedding layer are not predefined
    but are learned during the training process. This learning aspect enables the
    model to refine its understanding of word semantics based on the training data,
    leading to a more nuanced and context-aware representation of language in the
    neural network. This approach significantly enhances the model’s ability to process
    and interpret language data efficiently and meaningfully.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Preparing data to train the LSTM model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll process text data and get it ready for training. We’ll
    first break text down into individual tokens. Our next step involves creating
    a dictionary that assigns each token an index, essentially mapping them to integers.
    After this setup, we will organize these tokens into batches of training data,
    which will be crucial for training an LSTM model in the subsequent section.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll walk through the tokenization process in a detailed, step-by-step manner,
    ensuring you gain a thorough understanding of how tokenization functions. We’ll
    use word tokenization, owing to its simplicity in dividing text into words, as
    opposed to the more complex subword tokenization that demands a nuanced grasp
    of linguistic structure. In later chapters, we’ll employ pretrained tokenizers
    for subword tokenization using more sophisticated methods. This will allow us
    to focus on advanced topics, such as the attention mechanism and the Transformer
    architecture, without getting bogged down in the initial stages of text processing.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Downloading and cleaning up the text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll use the text from the novel *Anna Karenina* to train our model. Go to
    [https://mng.bz/znmX](https://mng.bz/znmX) to download the text file and save
    it as anna.txt in the folder /files/ on your computer. After that, open the file
    and delete everything after line 39888, which says, `"END OF THIS PROJECT GUTENBERG
    EBOOK ANNA KARENINA`.`"` Or you can simply download the file anna.txt from the
    book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load up the data and print out some passages to get a feeling about
    the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, line breaks (represented by \n) are considered part of the
    text. Therefore, we should replace these line breaks with spaces so they are not
    in the vocabulary. Additionally, converting all words to lowercase is helpful
    in our setting, as it ensures words like “The” and “the” are recognized as the
    same token. This step is vital for reducing the variety of unique tokens, thereby
    making the training process more efficient. Furthermore, punctuation marks should
    be spaced apart from the words they follow. Without this separation, combinations
    like “way.” and “way” would be erroneously treated as different tokens. To address
    these problems, we’ll clean up the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① Replaces line break with a space
  prefs: []
  type: TYPE_NORMAL
- en: ② Replaces a hyphen with a space
  prefs: []
  type: TYPE_NORMAL
- en: ③ Adds a space around punctuation marks and special characters
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we obtain unique tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The list `words` contains all the unique tokens in the text, with the most frequent
    one appearing first, and the least frequent one last. The output from the preceding
    code block is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output shows the most frequent 10 tokens. The comma (`,`) and
    the period (`.`) are the most and the second most frequent tokens, respectively.
    The word “the” is the third most frequent token, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now create two dictionaries: one mapping tokens to indexes and the other
    mapping indexes to tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1 Dictionaries to map tokens to indexes and indexes to tokens
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① The length of text (how many tokens in the text)
  prefs: []
  type: TYPE_NORMAL
- en: ② The length of unique tokens
  prefs: []
  type: TYPE_NORMAL
- en: ③ Maps tokens to indexes
  prefs: []
  type: TYPE_NORMAL
- en: ④ Maps indexes to tokens
  prefs: []
  type: TYPE_NORMAL
- en: The output from the preceding code block is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The text for the novel *Anna Karenina* has a total of 437,098 tokens. There
    are 12,778 unique tokens. The dictionary `word_to_int` assigns an index to each
    unique token. For example, the comma (`,`) is assigned an index of 0, and the
    period (`.`) is assigned an index of 1\. The dictionary `int_to_word` translates
    an index back to a token. For example, index 2 is translated back to the token
    “`the`”. Index 4 is translated back to the token “`and`”, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we convert the whole text to indexes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We convert all tokens in the text into the corresponding indexes and save them
    in a list `wordidx`. The preceding output shows the first 20 tokens in the text,
    as well as the corresponding indexes. For example, the first token in the text
    is `chapter`, with an index value of 208\.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 8.3
  prefs: []
  type: TYPE_NORMAL
- en: Find out the index value of the token `anna` in the dictionary `word_to_int`.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 Creating batches of training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we create pairs of (x, y) for training purposes. Each x is a sequence
    with 100 indexes. There is nothing magical about the number 100, and you can easily
    change it to 90 or 110 and have similar results. Setting the number too large
    may slow down training, while setting the number too small may lead to the model’s
    failure to capture long-range dependencies. We then slide the window right by
    one token and use it as the target y. Shifting the sequence by one token to the
    right and using it as the output during sequence generation is a common technique
    in training language models, including Transformers. The code block in the following
    listing creates the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.2 Creating training data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ① Each input contains 100 indexes.
  prefs: []
  type: TYPE_NORMAL
- en: ② Starting from the first token in text, slides to the right one at a time
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines the input x
  prefs: []
  type: TYPE_NORMAL
- en: ④ Shifts the input x to the right by one token and uses it as the output y
  prefs: []
  type: TYPE_NORMAL
- en: By shifting the sequence one token to the right and using it as output, the
    model is trained to predict the next token given the previous tokens. For instance,
    if the input sequence is `"how are you"`, then the shifted sequence would be `"are
    you today"`. During training, the model learns to predict `'are'` after seeing
    `'how'`, `'you'` after seeing `'are'`, and so on. This helps the model learn the
    probability distribution of the next token in a sequence. You’ll see this practice
    again and again later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll create batches of data for training, with 32 pairs of (x, y) in each
    batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We now have the training dataset. Next, we’ll create an LSTM model and train
    it using the data we just processed.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Building and training the LSTM model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you’ll begin by constructing an LSTM model using PyTorch’s
    built-in LSTM layer. This model will start with a word embedding layer, which
    transforms each index into a dense vector of 128 dimensions. Your training data
    will pass through this embedding layer before being fed into the LSTM layer. This
    LSTM layer is designed to process elements of a sequence in a sequential manner.
    Following the LSTM layer, the data will proceed to a linear layer, which has an
    output size matching the size of your vocabulary. The outputs generated by the
    LSTM model are essentially logits, serving as inputs for the softmax function
    to compute probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have built the LSTM model, the next step will involve using your training
    data to train this model. This training phase is crucial to refine the model’s
    ability to understand and generate patterns consistent with the data it has been
    fed.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 Building an LSTM model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In listing 8.3, we define a `WordLSTM()` class to serve as our LSTM model to
    be trained to generate text in the style of *Anna Karenina*. The class is defined
    as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.3 Defining the `WordLSTM()` class
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Training data first goes through an embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates an LSTM layer with the PyTorch LSTM() class
  prefs: []
  type: TYPE_NORMAL
- en: ③ In each time step, the LSTM layer uses the previous token and the hidden state
    to predict the next token and the next hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Initiates the hidden state for the first token in the input sequence
  prefs: []
  type: TYPE_NORMAL
- en: 'The `WordLSTM()` class defined previously has three layers: the word embedding
    layer, the LSTM layer, and a final linear layer. We set the value of the argument
    `n_layers` to 3, which means the LSTM layer stacks three LSTMs together to form
    a stacked LSTM, with the last two LSTMs taking the output from the previous LSTM
    as input. The `init_hidden()` method fills the hidden state with zeros when the
    model uses the first element in the sequence to make predictions. In each time
    step, the input is the current token and the previous hidden state while the output
    is the next token and the next hidden state.'
  prefs: []
  type: TYPE_NORMAL
- en: How the `torch.nn.Embedding()` class works
  prefs: []
  type: TYPE_NORMAL
- en: The `torch.nn.Embedding()` class in PyTorch is used to create an embedding layer
    in a neural network. An embedding layer is a trainable lookup table that maps
    integer indexes to dense, continuous vector representations (embeddings).
  prefs: []
  type: TYPE_NORMAL
- en: 'When you create an instance of `torch.nn.Embedding()`, you need to specify
    two main parameters: num_embeddings, the size of the vocabulary (total number
    of unique tokens), and embedding_dim, the size of each embedding vector (the dimensionality
    of the output embeddings).'
  prefs: []
  type: TYPE_NORMAL
- en: Internally, the class creates a matrix (or lookup table) of shape (num_embeddings,
    embedding_dim) where each row corresponds to the embedding vector for a particular
    index. Initially, these embeddings are randomly initialized but are learned and
    updated during training through backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: When you pass a tensor of indexes to the embedding layer (during the forward
    pass of the network), it looks up the corresponding embedding vectors in the lookup
    table and returns them. More information about the class is provided by PyTorch
    at [https://mng.bz/n0Zd](https://mng.bz/n0Zd).
  prefs: []
  type: TYPE_NORMAL
- en: 'We create an instance of the `WordLSTM()` class and use it as our LSTM model,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When the LSTM model is created, the weights are randomly initialized. When we
    use pairs of (x, y) to train the model, LSTM learns to predict the next token
    based on all previous tokens in the sequence by adjusting the model parameters.
    As we have illustrated in figure 8.2, LSTM learns to predict the next token and
    the next hidden state based on the current token and the current hidden state,
    which is a summary of the information in all previous tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the Adam optimizer with a learning rate of 0.0001\. The loss function
    is the cross-entropy loss since this is essentially a multicategory classification
    problem: the model is trying to predict the next token from a dictionary with
    12,778 choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now that the LSTM model is built, we’ll train the model with the batches of
    training data we prepared before.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 Training the LSTM model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During each training epoch, we go through all data batches of data (x, y) in
    the training set. The LSTM model receives the input sequence, x, and generates
    a predicted output sequence, *ŷ*. This prediction is compared with the actual
    output sequence, y, to compute the cross-entropy loss since we essentially conduct
    a multicategory classification here. We then tweak the model’s parameters to reduce
    this loss, as we did in chapter 2 when classifying clothing items.
  prefs: []
  type: TYPE_NORMAL
- en: Though we could divide our data into training and validation sets, training
    the model until no further improvements are seen on the validation set (as we
    have done in chapter 2), our primary aim here is to grasp how LSTM models function,
    not necessarily to achieve the best parameter tuning. Therefore, we’ll train the
    model for 50 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.4 Training the LSTM model to generate text
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through all batches of (x,y) in the training data
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses the model to predict the output sequence
  prefs: []
  type: TYPE_NORMAL
- en: ③ Compares the predictions with the actual output and calculates the loss
  prefs: []
  type: TYPE_NORMAL
- en: ④ Tweaks model parameters to minimize loss
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code listing, `sh` and `sc` together form the hidden state.
    In particular, the cell state `sc` acts as a conveyor belt, carrying information
    over many time steps, with information added or removed in each time step. The
    component `sh` is the output of the LSTM cell at a given time step. It contains
    information about the current input and is used to pass information to the next
    LSTM cell in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have a CUDA-enabled GPU, this training takes about 6 hours. If you use
    CPU only, it may take a day or two, depending on your hardware. Or you can download
    the pretrained weights from my website: [https://mng.bz/vJZa](https://mng.bz/vJZa).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we save the trained model weights in the local folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The dictionary `word_to_int` is also saved on your computer, which is a practical
    step ensuring that you can generate text using the trained model without needing
    to repeat the tokenization process.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Generating text with the trained LSTM model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have a trained LSTM model, you’ll learn how to use it to generate
    text in this section. The goal is to see if the trained model can generate grammatically
    correct and coherent text by iteratively predicting the next token based on previous
    tokens. You’ll also learn to use temperature and top-K sampling to control the
    creativeness of the generated text.
  prefs: []
  type: TYPE_NORMAL
- en: When generating text with the trained LSTM model, we start with a prompt as
    the initial input to the model. We use the trained model to predict the most likely
    next token. After appending the next token to the prompt, we feed the new sequence
    to the trained model to predict the next token again. We repeat this process until
    the sequence reaches a certain length.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.1 Generating text by predicting the next token
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we load the trained model weights and the dictionary `word_to_int` from
    the local folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The file `word_to_int.p` is also available in the book’s GitHub repository.
    We switch the positions of keys and values in the dictionary `word_to_int` to
    create the dictionary `int_to_word`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate text with the trained LSTM model, we need a prompt as the starting
    point of the generated text. We’ll set the default prompt to “Anna and the.” An
    easy way to determine when to stop is to limit the generated text to a certain
    length, say 200 tokens: once the desired length is reached, we ask the model to
    stop generating.'
  prefs: []
  type: TYPE_NORMAL
- en: The following listing defines a `sample()` function to generate text based on
    a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.5 A `sample()` function to generate text
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: ① Determines how many tokens need to be generated
  prefs: []
  type: TYPE_NORMAL
- en: ② The input is the current sequence; trims it if it’s longer than 100 tokens
  prefs: []
  type: TYPE_NORMAL
- en: ③ Makes a prediction using the trained model
  prefs: []
  type: TYPE_NORMAL
- en: ④ Selects the next token based on predicted probabilities
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Appends the predicted next token to the sequence and repeats
  prefs: []
  type: TYPE_NORMAL
- en: The function `sample()` takes three arguments. The first is the trained LSTM
    model you will be using. The second is the starting prompt for text generation,
    which can be a phrase of any length, in quotes. The third parameter specifies
    the length of the text to be generated, measured in tokens, with a default value
    of 200 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Within the function, we first deduct the number of tokens in the prompt from
    the total desired length to determine the number of tokens that need to be generated.
    When generating the next token, we consider the current sequence’s length. If
    it’s under 100 tokens, we input the entire sequence into the model; if it’s over
    100 tokens, only the last 100 tokens of the sequence are used as input. This input
    is then fed into the trained LSTM model to predict the subsequent token, which
    we then add to the current sequence. We continue this process until the sequence
    reaches the desired length.
  prefs: []
  type: TYPE_NORMAL
- en: When generating the next token, the model employs the random.choice(len(logits),
    p = p) method from NumPy. Here, the method’s first parameter indicates the range
    of choices, which in this case is len(logits) = 12778\. This signifies that the
    model will randomly select an integer from 0 to 12,777, with each integer corresponding
    to a different token in the vocabulary. The second parameter, p, is an array containing
    12,778 elements where each element denotes the probability of selecting a corresponding
    token from the vocabulary. Tokens with a higher probability in this array are
    more likely to be chosen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate a passage with the model using “Anna and the prince” as the
    prompt (make sure you put a space before punctuation marks when you use your own
    prompt):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, I fixed the random seed number to 42 in both PyTorch and NumPy in case
    you want to reproduce results. The generated passage is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You may have noticed that the text generated is entirely in lowercase. This
    is because, during the text processing stage, we converted all uppercase letters
    to lowercase to minimize the number of unique tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The text generated from 6 hours of training is quite impressive! Most of the
    sentences adhere to grammatical norms. While it may not match the level of sophistication
    seen in text generated by advanced systems like ChatGPT, it’s a significant achievement.
    With skills acquired in this exercise, you are ready to train more advanced text
    generation models in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.2 Temperature and top-K sampling in text generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The creativity of the generated text can be controlled by using techniques like
    temperature and top-K sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature adjusts the distribution of probabilities assigned to each potential
    token before selecting the next one. It effectively scales the logits, which are
    the inputs to the softmax function calculating these probabilities, by the value
    of the temperature. Logits are the outputs of the LSTM model prior to the application
    of the softmax function.
  prefs: []
  type: TYPE_NORMAL
- en: In the `sample()` function we just defined, we didn’t adjust the logits, implying
    a default temperature of 1\. A lower temperature (below 1; e.g., 0.8) results
    in fewer variations, making the model more deterministic and conservative, favoring
    more likely choices. Conversely, a higher temperature (above 1; e.g., 1.5) makes
    it more likely to choose improbable words in text generation, leading to more
    varied and inventive outputs. However, this could also make the text less coherent
    or relevant, as the model might opt for less probable words.
  prefs: []
  type: TYPE_NORMAL
- en: Top-K sampling is another method to influence the output. This approach involves
    selecting the next word from the top K most probable options as predicted by the
    model. The probability distribution is truncated to include only the top K words.
    With a small K value, such as 5, the model’s choices are limited to a few highly
    probable words, resulting in more predictable and coherent but potentially less
    diverse and interesting outputs. In the `sample()` function we defined earlier,
    we did not apply top-K sampling, so the value of K was effectively the size of
    the vocabulary (12,778 in our case).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we introduce a new function, `generate()`, for text generation. This
    function is similar to the `sample()` function but includes two additional parameters:
    `temperature` and `top_k`, allowing for more control over the creativity and randomness
    of the generated text. The function `generate()` is defined in the following listing.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.6 Generating text with temperature and top-K sampling
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: ① Scales the logits with temperature
  prefs: []
  type: TYPE_NORMAL
- en: ② Keeps only the K most probable candidates
  prefs: []
  type: TYPE_NORMAL
- en: ③ Selects the next token from the top K candidates
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to the `sample()` function, the new function `generate()` has two
    more optional arguments: `top_k` and `temperature`. By default, `top_k` is set
    to `None,` and `temperature` is set to 1\. Therefore, if you call the `generate()`
    function without specifying these two arguments, the output will be the same as
    what you would get from the function `sample()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s illustrate the variations in generated text by focusing on the creation
    of a single token. For this purpose, we’ll use “I ’ m not going to see” as the
    prompt (note the space before the apostrophe, as we previously have done in the
    chapter). We call the `generate()` function 10 times, setting its length argument
    to be one more than the prompt’s length. This approach ensures that the function
    appends only one extra token to the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: With the default setting of top_k = None and temperature = 1, there is some
    degree of repetition in the output. For example, the word “you” was repeated three
    times. There are a total of six unique tokens.
  prefs: []
  type: TYPE_NORMAL
- en: However, the functionality of `generate()` expands when you adjust these two
    arguments. For instance, setting a low temperature, like 0.5, and a small `top_k`
    value, such as 3, results in generated text that is more predictable and less
    creative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s repeat the single token example. This time, we set the temperature to
    0.5 and `top_k` value to 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output has fewer variations: there are only 3 unique tokens from 10 attempts,
    “you,” “the,” and “her.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see this in action by using “Anna and the prince” as our starting prompt
    when we set the temperature to 0.5 and `top_k` value to 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 8.4
  prefs: []
  type: TYPE_NORMAL
- en: Generate text by setting temperature to 0.6 and `top_k` to 10 and using “Anna
    and the nurse” as the starting prompt. Set the random seed number to 0 in both
    PyTorch and NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversely, opting for a higher `temperature` value, like 1.5, coupled with
    a higher `top_k` value, for instance, `None` (enabling selection from the entire
    pool of 12,778 tokens), leads to outputs that are more creative and less predictable.
    This is demonstrated next, in the single token example. This time, we set the
    temperature to 2 and `top_k` value to `None`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output has almost no repetition: there are 9 unique tokens from 10 attempts;
    only the word “it” was repeated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s again use “Anna and the prince” as the initial prompt but set the temperature
    to 2 and top_k value to None and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The generated text is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The output generated is not repetitive, although it lacks coherence in many
    places.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 8.5
  prefs: []
  type: TYPE_NORMAL
- en: Generate text by setting temperature to 2 and `top_k` to 10000 and using “Anna
    and the nurse” as the starting prompt. Set the random seed number to 0 in both
    PyTorch and NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you have acquired foundational skills in NLP, including word-level
    tokenization, word embedding, and sequence prediction. Through these exercises,
    you’ve learned to construct a language model based on word-level tokenization
    and have trained it using LSTM for text generation. Moving forward, the next few
    chapters will introduce you to training Transformers, the type of models used
    in systems like ChatGPT. This will provide you with a more in-depth understanding
    of advanced text generation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RNNs are a specialized form of artificial neural network designed to recognize
    patterns in sequences of data, such as text, music, or stock prices. Unlike traditional
    neural networks, which process inputs independently, RNNs have loops in them,
    allowing information to persist. LSTM networks are improved versions of RNNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are three approaches to tokenization. The first is character tokenization,
    where the text is divided into its constituent characters. The second approach
    is word tokenization, where the text is split into individual words. The third
    approach is subword tokenization, which breaks words into smaller, meaningful
    components called subwords.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word embedding is a method that transforms words into compact vector representations,
    capturing their semantic information and interrelationships. This technique is
    vital in NLP, especially since deep neural networks, including models like LSTM
    and Transformers, require numerical input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature is a parameter that influences the behavior of text generation models.
    It controls the randomness of the predictions by scaling the logits (the inputs
    to the softmax function for probability calculation) before applying softmax.
    Low temperature makes the model more conservative in its predictions but also
    more repetitive. At higher temperatures, the model becomes less repetitive and
    more innovative, increasing the diversity of the generated text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top-K sampling is another way to influence the behavior of text generation models.
    It involves selecting the next word from the K most likely candidates, as determined
    by the model. The probability distribution is truncated to keep only the top K
    words. Small values of K make the output more predictable and coherent but potentially
    less diverse and interesting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](#footnote-000-backlink))  Sepp Hochreiter and Jurgen Schmidhuber, 1997,
    “Long Short-Term Memory,” *Neural Computation* 9(8): 1735-1780.'
  prefs: []
  type: TYPE_NORMAL
