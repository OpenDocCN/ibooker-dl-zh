["```py\nclick_weight = 1  #1\nadd_to_cart_weight = 0  #1\npurchase_weight = 0  #1\n\nsignals_collection = engine.get_collection(\"signals\")\n\nmixed_signal_types_aggregation = f\"\"\"\nSELECT user, product,\n  (click_boost + add_to_cart_boost + purchase_boost) AS rating\nFROM (\n  SELECT user, product,\n    SUM(click) AS click_boost,\n    SUM(add_to_cart) AS add_to_cart_boost,\n    SUM(purchase) AS purchase_boost\n  FROM (\n    SELECT s.user, s.target AS product,\n      IF(s.type = 'click', {click_weight}, 0) AS click,\n      IF(s.type = 'add-to-cart', {add_to_cart_weight}, 0) AS add_to_cart,\n      IF(s.type = 'purchase', {purchase_weight}, 0) AS purchase\n    FROM signals s\n    WHERE (s.type != 'query')) AS raw_signals\n  GROUP BY user, product) AS per_type_boosts\"\"\"\n\nsignals_agg_collection = \\ #2\n  aggregate_signals(signals_collection, \"user_product_implicit_preferences\",  #2\n                    mixed_signal_types_aggregation)  #2\n```", "```py\ncreate_view_from_collection(signals_agg_collection,\n                           \"user_product_implicit_preferences\")\n\ntop_product_count_for_recs = 50000  #1\nuser_preference_query = f\"\"\"\nSELECT user, product, rating  #2\nFROM user_product_implicit_preferences\nWHERE product IN (\n  SELECT product FROM (\n    SELECT product, COUNT(user) user_count\n    FROM user_product_implicit_preferences\n    GROUP BY product\n    ORDER BY user_count DESC  #3\n    LIMIT {top_product_count_for_recs}  #3\n  ) AS top_products)\nORDER BY rating DESC\"\"\"\n\nuser_prefs = spark.sql(user_preference_query)\n```", "```py\ndef order_preferences(prefs):\n  return prefs.orderBy(col(\"userIndex\").asc(),\n                       col(\"rating\").desc(),\n                       col(\"product\").asc())\n\ndef strings_to_indexes(ratings, user_indexer,\n                       product_indexer):\n  transformed = product_indexer.transform(  #1\n    user_indexer.transform(ratings))  #1\n  return order_preferences(transformed)\n\ndef indexes_to_strings(ratings, user_indexer,\n                       product_indexer):\n  user_converter = IndexToString(inputCol=\"userIndex\",  #2\n                                     outputCol=\"user\",  #2\n                           labels=user_indexer.labels)  #2\n  product_converter = IndexToString(inputCol=\"productIndex\",  #2\n                                        outputCol=\"product\",  #2\n                              labels=product_indexer.labels)  #2\n  converted = user_converter.transform(  #3\n    product_converter.transform(ratings)) #3\n  return order_preferences(converted)\n\nuser_indexer = StringIndexer(inputCol=\"user\", #4\n       outputCol=\"userIndex\").fit(user_prefs)  #4\nproduct_indexer = StringIndexer(inputCol=\"product\",  #5\n          outputCol=\"productIndex\").fit(user_prefs)  #5\n\nindexed_prefs = strings_to_indexes(user_prefs, user_indexer, product_indexer)\nindexed_prefs.show(10)\n```", "```py\n+-------+------------+------+---------+------------+\n|   user|     product|rating|userIndex|productIndex|\n+-------+------------+------+---------+------------+\n|u159789|008888345435|     1|      0.0|      5073.0|\n|u159789|014633196870|     1|      0.0|      4525.0|\n|u159789|018713571687|     1|      0.0|     10355.0|\n|u159789|024543718710|     1|      0.0|       263.0|\n|u159789|025192979620|     1|      0.0|     12289.0|\n|u159789|025193102324|     1|      0.0|      9650.0|\n|u159789|085391163121|     1|      0.0|      9196.0|\n|u159789|720616236029|     1|      0.0|      2781.0|\n|u159789|801213001996|     1|      0.0|     28736.0|\n|u159789|813985010007|     1|      0.0|      5819.0|\n+-------+------------+------+---------+------------+\nonly showing top 10 rows\n```", "```py\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\n\nals = ALS(maxIter=3, rank=10, regParam=0.15, implicitPrefs=True,\n          userCol=\"userIndex\", itemCol=\"productIndex\", ratingCol=\"rating\",\n          coldStartStrategy=\"drop\", seed=0)\n\n(training_data, test_data) = \\  #1\n  user_prefs.randomSplit([0.95, 0.05], 0)  #1\n\ntraining_data = strings_to_indexes(training_data, user_indexer, product_indexer)\ntest_data = strings_to_indexes(test_data, user_indexer, product_indexer)\n\nmodel = als.fit(training_data) #2\npredictions = model.transform(test_data)  #3\nevaluator = RegressionEvaluator(metricName=\"rmse\",  #3\n                                labelCol=\"rating\",  #3\n                       predictionCol=\"prediction\")  #3\nrmse = evaluator.evaluate(predictions)  #3\nprint(f\"Root-mean-square error = {rmse}\")  #3\n```", "```py\nRoot-mean-square error = 1.0007877733299877\n```", "```py\nindexed_user_recs = model.recommendForAllUsers(10) \\\n                         .orderBy(col(\"userIndex\").asc())\nindexed_user_recs.show(5, truncate=64)\n```", "```py\n+---------+----------------------------------------------------------------+\n|userIndex|                                                 recommendations|\n+---------+----------------------------------------------------------------+\n|        0|[{6, 0.022541389}, {13, 0.015104328}, {36, 0.010634022}, {20,...|\n|        1|[{13, 0.009001873}, {3, 0.007981183}, {23, 0.0050935573}, {31...|\n|        2|[{9, 0.06319133}, {17, 0.04681776}, {3, 0.041046627}, {14, 0....|\n|        3|[{17, 0.0145240165}, {14, 0.01413305}, {12, 0.012459144}, {39...|\n|        4|[{14, 0.006752351}, {4, 0.004651022}, {10, 0.004487163}, {17,...|\n+---------+----------------------------------------------------------------+\nonly showing top 5 rows\n```", "```py\ncolumn_exploder = explode(\"recommendations\").alias(\"productIndex_rating\")\nuser_item_recs = indexed_user_recs.select(\"userIndex\", column_exploder) \\\n                      .select(\"userIndex\", col(\"productIndex_rating.*\"))\nuser_item_recs = indexes_to_strings(user_item_recs, user_indexer,\n                                    product_indexer)\nuser_item_recs = user_item_recs.select(\"user\", \"product\",\n                                       col(\"rating\").alias(\"boost\"))\n```", "```py\nrecs_collection = engine.create_collection(\"user_item_recommendations\")\nrecs_collection.write(user_item_recs)\n```", "```py\ndef signals_request(user_id):\n  return {\"query\": \"*\",\n          \"return_fields\": [\"signal_time\", \"type\", \"target\"],\n          \"order_by\": [(\"signal_time\", \"asc\")],\n          \"filters\": [(\"user\", user_id)]}\n\nuser_id = \"u478462\" #1\nsignals_collection = engine.get_collection(\"signals\")\n\nrequest = signals_request(user_id)\nprevious_signals = signals_collection.search(**request)[\"docs\"]\nprint_interaction_history(user_id, previous_signals)Previous Product Interactions for User: u478462\n+-----------+-----------+------------+-------------------------------------+\n|signal_time|       type|      target|                                 name|\n+-----------+-----------+------------+-------------------------------------+\n|05/20 06:05|      query|       apple|                                apple|\n|05/20 07:05|      click|885909457588|Apple® - iPad® 2 with Wi-Fi - 16GB...|\n|05/20 07:05|add-to-cart|885909457588|Apple® - iPad® 2 with Wi-Fi - 16GB...|\n|05/20 07:05|   purchase|885909457588|Apple® - iPad® 2 with Wi-Fi - 16GB...|\n|05/25 06:05|      query|     macbook|                              macbook|\n|05/25 07:05|      click|885909464043|Apple® - MacBook® Air - Intel® Cor...|\n+-----------+-----------+------------+-------------------------------------+\n```", "```py\ndef get_query_time_boosts(user, boosts_collection):\n  request = {\"query\": \"*\",\n             \"return_fields\": [\"product\", \"boost\"],\n             \"filters\": [(\"user\", user)] if user else [],\n             \"limit\": 10,\n             \"order_by\": [(\"boost\", \"desc\")]}\n\n  response = boosts_collection.search(**request)\n  signals_boosts = response[\"docs\"]\n  return \" \".join(f'\"{b[\"product\"]}\"^{b[\"boost\"] * 100}'\n                  for b in signals_boosts)\n\ndef search_for_products(query, signals_boosts):\n  request = product_search_request(query if query else \"*\")  #1\n  if signals_boosts:\n    request[\"query_boosts\"] = (\"upc\", signals_boosts)\n  return products_collection.search(**request)\n\nuser = \"u478462\"\nboosts = get_query_time_boosts(user, recs_collection)\nresponse = search_for_products(\"\", boosts)  #2\n\nprint(f\"Boost Query:\\n{boosts}\")\ndisplay_product_search(\"\", response[\"docs\"])\n```", "```py\nquery = \"tablet\"\nresponse = search_for_products(query, None)  #1\nprint(f\"Non-personalized Query\")\ndisplay_product_search(query, response[\"docs\"])\n\nresponse = search_for_products(query, boosts)  #2\nprint(f\"Personalized Query\")\ndisplay_product_search(query, response[\"docs\"])\n```", "```py\nquery = \"SELECT DISTINCT name, string(upc), short_description FROM products\"\nspark.sql(query).createOrReplaceTempView(\"products_samples\")\nproduct_names = dataframe.select(\"name\").rdd.flatMap(lambda x: x).collect()\nproduct_ids = dataframe.select(\"upc\").rdd.flatMap(lambda x: x).collect()\n```", "```py\nfrom sentence_transformers import SentenceTransformer  #1\ntransformer = SentenceTransformer(\"all-mpnet-base-v2\")  #1\n...  #2\n\ndef get_embeddings(texts, model, cache_name, ignore_cache=False):\n  ...  #2\n    embeddings = model.encode(texts)  #3\n  ... #2\n  return embeddings\n\nproduct_embeddings = get_embeddings(product_names,\n transformer, cache_name=\"all_product_embeddings\")\n```", "```py\ndef get_clusters(data, algorithm, args):\n  return algorithm(**args).fit(data)\n\ndef assign_clusters(labels, product_names):\n  clusters = defaultdict(lambda:[], {})\n  for i in range(len(labels)):\n    clusters[labels[i]].append(product_names[i])\n  return clusters\n\nargs = {\"n_clusters\": 100, \"n_init\": 10, \"random_state\": 0} #1\nalgo = get_clusters(product_embeddings, cluster.KMeans, args)  #1\nlabels = algo.predict(product_embeddings)\nclusters = assign_clusters(labels, product_names)  #2\n```", "```py\nimport collections, numpy as np, matplotlib.pyplot as plt\nfrom adjustText import adjust_text\nfrom sklearn.decomposition import PCA\n\nplt.figure(figsize=(15, 15))\npca = PCA(100, svd_solver=\"full\")  #1\ncenters = algo.cluster_centers_  #1\nplot_data = pca.fit_transform(centers)  #1\n\npoints = []\nfor i, cluster_name in enumerate(plot_data):  #2\n  plt.scatter(plot_data[i,0], plot_data[i, 1],  #2\n              s=30, color=\"k\")   #2\n  label = f\"{i}_{\"_\".join(top_words(clusters[i], 2))}\"  #3\n  points.append(plt.text(plot_data[i, 0],  #4\n                         plot_data[i, 1],  #4\n                         label, size=12))  #4\nadjust_text(points, arrowprops=dict(arrowstyle=\"-\",  #5\n                          color=\"gray\", alpha=0.3))  #5\nplt.show()\n```", "```py\nimport sentence_transformers, heapq\n\ndef get_top_labels_centers(query, centers, n=2):  #1\n  query_embedding = transformer.encode([query], convert_to_tensor=False)\n  similarities = sentence_transformers.util.cos_sim(\n                            query_embedding, centers)\n  sim = similarities.tolist()[0]\n  return [sim.index(i) for i in heapq.nlargest(n, sim)]\n\ndef get_query_cluster(query):  #2\n  query_embedding = transformer.encode([query], convert_to_tensor=False)\n  return algo.predict(query_embedding)\n\ndef get_cluster_description(cluser_num):\n  return \"_\".join(top_words(clusters[cluser_num], 5))\n\nquery = \"microwave\"\nkmeans_predict = get_query_cluster(query)[0]  #3\nprint(\"K-means Predicted Cluster:\")\nprint(f\"    {kmeans_predict} ({get_cluster_description(kmeans_predict)})\")\n\nclosest_sim = get_top_labels_centers(query, centers, 1)[0] #4\nprint(f\"\\nCosine Predicted Cluster:\")\nprint(f\"    {closest_sim} ({get_cluster_description(closest_sim)})\")\n\nknn_cosine_similarity = get_top_labels_centers(query, centers, 5)  #5\nprint(f\"\\nKNN Cosine Predicted Clusters: {knn_cosine_similarity}\")\nfor n in knn_cosine_similarity:\n  print(f\"    {n} ({get_cluster_description(n)})\")\n```", "```py\nK-means Predicted Cluster:\n  44 (Microwave_Cu._Ft._Stainless-Steel_Oven)\n\nCosine Predicted Cluster:\n  44 (Microwave_Cu._Ft._Stainless-Steel_Oven)\n\nKNN Cosine Predicted Clusters: [44, 52, 5, 83, 6]\n  44 (Microwave_Cu._Ft._Stainless-Steel_Oven)\n  52 (Stainless-Steel_30\"_Black_Range_Cooktop)\n  5 (KitchenAid_Black_White_Stand_Mixer)\n  83 (Black_Coffeemaker_Maker_Coffee_Stainless-Steel)\n  6 (Range_30\"_Self-Cleaning_Freestanding_Stainless-Steel)\n```", "```py\ndef top_clusters_for_embedding(embedding, n=2):\n  similarities = sentence_transformers.util.cos_sim(embedding, centers)\n  sim = similarities.tolist()[0]\n  return [sim.index(i) for i in heapq.nlargest(n, sim)]\n\ndef get_user_embeddings(products=[]):  #1\n  values = []\n  embeddings = get_indexed_product_embeddings()\n  for p in products:\n    values.append([embeddings[p],\n                  top_clusters_for_embedding(embeddings[p], 1)[0]])\n  return pandas.DataFrame(data=numpy.array(values), index=products,\n                          columns=[\"embedding\", \"cluster\"])\n\ndef get_personalization_vector(query=None,  #2\n                            user_items=[],\n                           query_weight=1,  #3\n                   user_items_weights=[]):  #3\n  query_embedding = transformer.encode(query) if query else None\n\n  if len(user_items) > 0 and len(user_items_weights) == 0:  #4\n    user_items_weights = numpy.full(shape=len(user_items),\n                           fill_value=1 / len(user_items))\n\n  embeddings = []\n  embedding_weights = []\n  for weight in user_items_weights:  #3\n    embedding_weights.append(weight) #3 \n  for embedding in user_items:\n    embeddings.append(embedding)\n  if query_embedding.any():\n    embedding_weights.append(query_weight)  #3\n    embeddings.append(query_embedding)\n\n  return numpy.average(embeddings, weights=numpy.array(embedding_weights),\n                       axis=0).astype(\"double\") if len(embeddings) else None\n```", "```py\nproduct_interests = [\"7610465823828\", #hello kitty water bottle\n                     \"36725569478\"]   #stainless steel electric range\n\nuser_embeddings = get_user_embeddings(product_interests)\nquery = \"microwave\"\n\nunfiltered_personalization_vector =  #1\n  get_personalization_vector(query=query, #1\n  user_items=user_embeddings['embedding'].to_numpy())  #1\nprint(\"\\nPersonalization Vector (No Cluster Guardrails):\")\nprint(format_vector(unfiltered_personalization_vector))\n\nquery_clusters = get_top_labels_centers(query,  #2\n                                 centers, n=5)  #2\nprint(\"\\nQuery Clusters ('microwave'):\\n\" + str(query_clusters))\n\nclustered = user_embeddings.cluster.isin(query_clusters)  #3\nproducts_in_cluster = user_embeddings[clustered]  #3\nprint(\"\\nProducts Filtered to Query Clusters:\\n\" + str(products_in_cluster))\n\nfiltered_personalization_vector = get_personalization_vector(query=query,  #4\n  user_items=filtered['embedding'].to_numpy())  #4\nprint(\"\\nFiltered Personalization Vector (With Cluster Guardrails):\")\nprint(format_vector(filtered_personalization_vector))\n```", "```py\nProducts Interactions for Personalization:\nproduct        embedding                                           cluster\n7610465823828  [0.06417941, 0.04178553, -0.0017139615, -0.020...   1\n36725569478    [0.0055417763, -0.024302201, -0.024139373, -0....   6\n\nPersonalization Vector (No Cluster Guardrails):\n[0.016, -0.006, -0.02, -0.032, -0.016, 0.008, -0.0, 0.017, 0.011, 0.007 ...]\n\nQuery Clusters ('microwave'):\n[44, 52, 5, 83, 6]\n\nProducts Filtered to Query Clusters:\nproduct      embedding                                             cluster\n36725569478  [0.0055417763, -0.024302201, -0.024139373, -0....     6\n\nFiltered Personalization Vector (With Cluster Guardrails):\n[0.002, -0.023, -0.026, -0.037, -0.025, 0.002, -0.009, 0.007, 0.033, -0 ...]\n```", "```py\ndef rerank_with_personalization(docs,  #1\n              personalization_vector):  #1\n  embeddings = get_indexed_product_embeddings()\n  result_embeddings = numpy.array(\n    [embeddings[docs[x][\"upc\"]]\n     for x in range(len(docs))]).astype(float)\n    similarities = sentence_transformers.util.cos_sim(\n             personalization_vector, result_embeddings).tolist()[0]\n  reranked = [similarities.index(i)\n              for i in heapq.nlargest(len(similarities), similarities)]\n  reranked, _ = zip(*sorted(enumerate(similarities),\n                            key=itemgetter(1), reverse=True))\n  return [docs[i] for i in reranked]\n\nquery = \"microwave\"\nrequest = product_search_request(query, {\"limit\": 100})\n\nresponse = products_collection.search(**request)\ndocs = response[\"docs\"]\nprint(\"No Personalization:\")\ndisplay_product_search(query, docs[0:4]) #2\n\nprint(\"Global Personalization (No Category Guardrails):\")\nreranked_seach_results_no_guardrails = \\  #3\n  rerank_with_personalization(docs,  #3\n    unfiltered_personalization_vector)  #3\ndisplay_product_search(query, reranked_seach_results_no_guardrails[0:4])\n\nprint(\"Contextual Personalization (with Category Guardrails):\")\nreranked_seach_results_with_guardrails = \\  #4\n  rerank_with_personalization(docs,  #4\n    filtered_personalization_vector)  #4\ndisplay_product_search(query, reranked_seach_results_with_guardrails[0:4])\n```"]