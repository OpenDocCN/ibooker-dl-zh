["```py\nimport numpy as np\n\ndef my_generator(my_array):\n    i = 0\n    while True:\n        yield my_array[i:i+2, :] # output two elements at a time\n        i += 1\n```", "```py\ntest_array = np.array([[10.0, 2.0],\n                       [15, 6.0],\n                       [3.2, -1.5],\n                       [-3, -2]], np.float32)\n```", "```py\noutput = my_generator(test_array)\n```", "```py\nnext(output)\n```", "```py\narray([[10.,  2.],\n       [15.,  6.]], dtype=float32)\n```", "```py\narray([[15\\. ,  6\\. ],\n       [ 3.2, -1.5]], dtype=float32)\n```", "```py\narray([[ 3.2, -1.5],\n       [-3\\. , -2\\. ]], dtype=float32)\n```", "```py\narray([[-3., -2.]], dtype=float32)\n```", "```py\narray([], shape=(0, 2), dtype=float32)\n```", "```py\n['Pregnancies', 'Glucose', 'BloodPressure',\n 'SkinThickness', 'Insulin', 'BMI',\n 'DiabetesPedigree', 'Age', 'Outcome']\n```", "```py\nimport csv\nimport pandas as pd\n\nfile_path = 'working_data/'\nfile_name = 'pima-indians-diabetes.data.csv'\n\ncol_name = ['Pregnancies', 'Glucose', 'BloodPressure',\n            'SkinThickness', 'Insulin', 'BMI',\n            'DiabetesPedigree', 'Age', 'Outcome']\npd.read_csv(file_path + file_name, names = col_name)\n```", "```py\nimport csv\nfile_path = 'working_data/'\nfile_name = 'pima-indians-diabetes.data.csv'\n\nwith open(file_path + file_name, newline='\\n') as csvfile:\n    f = csv.reader(csvfile, delimiter=',')\n    for row in f:\n        print(','.join(row))\n```", "```py\nf = csv.reader(csvfile, delimiter=',')\n```", "```py\nfor row in f:\n        print(','.join(row))\n```", "```py\ndef stream_file(file_handle):\n    holder = []\n    for row in file_handle:\n        holder.append(row.rstrip(\"\\n\"))\n        yield holder\n        holder = []\n\nwith open(file_path + file_name, newline = '\\n') as handle:\n    for part in stream_file(handle):\n        print(part)\n```", "```py\n{\n   \"id\": 1,\n   \"name\": {\n      \"first\": \"Dan\",\n      \"last\": \"Jones\"\n   },\n   \"rating\": [\n      8,\n      7,\n      9\n   ]\n},\n```", "```py\nwc -l owid-covid-data.csv\n```", "```py\n32788 owid-covid-data.csv\n```", "```py\nhead -3 owid-covid-data.csv\niso_code,continent,location,date,total_cases,new_cases,\ntotal_deaths,new_deaths,total_cases_per_million,\nnew_cases_per_million,total_deaths_per_million,\nnew_deaths_per_million,new_tests,total_tests,\ntotal_tests_per_thousand,new_tests_per_thousand,\nnew_tests_smoothed,new_tests_smoothed_per_thousand,tests_units,\nstringency_index,population,population_density,median_age,\naged_65_older,aged_70_older,gdp_per_capita,extreme_poverty,\ncardiovasc_death_rate,diabetes_prevalence,female_smokers,\nmale_smokers,handwashing_facilities,hospital_beds_per_thousand,\nlife_expectancy\nAFG,Asia,Afghanistan,2019-12-31,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n0.0,,,,,,,,,38928341.0,\n54.422,18.6,2.581,1.337,1803.987,,597.029,9.59,,,37.746,0.5,64.8\n\n```", "```py\ncat owid-covid-data.csv| parallel --header : --pipe -N330 \n'cat >owid-covid-data-\npart00{#}.csv'\n```", "```py\nbrew install parallel\n```", "```py\n-rw-r--r--  1 mbp16  staff    54026 Jul 26 16:45 \nowid-covid-data-part0096.csv\n-rw-r--r--  1 mbp16  staff    54246 Jul 26 16:45 \nowid-covid-data-part0097.csv\n-rw-r--r--  1 mbp16  staff    51278 Jul 26 16:45 \nowid-covid-data-part0098.csv\n-rw-r--r--  1 mbp16  staff    62622 Jul 26 16:45 \nowid-covid-data-part0099.csv\n-rw-r--r--  1 mbp16  staff    15320 Jul 26 16:45 \nowid-covid-data-part00100.csv\n```", "```py\nimport tensorflow as tf\n\nbase_pattern = 'dataset'\nfile_pattern = 'owid-covid-data-part*'\nfiles = tf.io.gfile.glob(base_pattern + '/' + file_pattern)\n```", "```py\n['dataset/owid-covid-data-part0091.csv',\n 'dataset/owid-covid-data-part0085.csv',\n 'dataset/owid-covid-data-part0052.csv',\n 'dataset/owid-covid-data-part0046.csv',\n 'dataset/owid-covid-data-part0047.csv',\n\u2026]\n```", "```py\ncsv_dataset = tf.data.experimental.make_csv_dataset(files,\n              header = True,\n              batch_size = 5,\n              label_name = 'new_deaths',\n              num_epochs = 1,\n              ignore_errors = True)\n```", "```py\nfor features, target in csv_dataset.take(1):\n    print(\"'Target': {}\".format(target))\n    print(\"'Features:'\")\n    for k, v in features.items():\n        print(\"  {!r:20s}: {}\".format(k, v))\n```", "```py\n'Target': [ 0\\.  0\\. 16\\.  0\\.  0.]\n'Features:'\n  'iso_code'          : [b'SWZ' b'ESP' b'ECU' b'ISL' b'FRO']\n  'continent'         : \n[b'Africa' b'Europe' b'South America' b'Europe' b'Europe']\n  'location'          : \n[b'Swaziland' b'Spain' b'Ecuador' b'Iceland' b'Faeroe Islands']\n  'date'              : \n[b'2020-04-04' b'2020-02-07' b'2020-07-13' b'2020-04-01' \n  b'2020-06-11']\n  'total_cases'       : [9.000e+00 1.000e+00 6.787e+04 \n1.135e+03 1.870e+02]\n  'new_cases'         : [  0\\.   0\\. 661\\.  49\\.   0.]\n  'total_deaths'      : [0.000e+00 0.000e+00 5.047e+03 \n2.000e+00 0.000e+00]\n  'total_cases_per_million': \n              [7.758000e+00 2.100000e-02 3.846838e+03 \n3.326007e+03 3.826870e+03]\n  'new_cases_per_million': [  0\\.      0\\.     37.465 \n143.59    0\\.   ]\n  'total_deaths_per_million': [  0\\.      0\\.    286.061   \n5.861   0\\.   ]\n  'new_deaths_per_million': \n[0\\.    0\\.    0.907 0\\.    0\\.   ]\n  'new_tests'         : \n[b'' b'' b'1331.0' b'1414.0' b'']\n  'total_tests'       : \n[b'' b'' b'140602.0' b'20889.0' b'']\n  'total_tests_per_thousand': \n[b'' b'' b'7.969' b'61.213' b'']\n  'new_tests_per_thousand': \n[b'' b'' b'0.075' b'4.144' b'']\n  'new_tests_smoothed': \n[b'' b'' b'1986.0' b'1188.0' b'']\n  'new_tests_smoothed_per_thousand': \n[b'' b'' b'0.113' b'3.481' b'']\n  'tests_units'       : \n[b'' b'' b'units unclear' b'tests performed' b'']\n  'stringency_index'  : \n[89.81 11.11 82.41 53.7   0\\.  ]\n  'population'        : \n[ 1160164\\. 46754784\\. 17643060\\.   341250\\.    48865.]\n  'population_density': \n[79.492 93.105 66.939  3.404 35.308]\n  'median_age'        : \n[21.5 45.5 28.1 37.3  0\\. ]\n  'aged_65_older'     : \n[ 3.163 19.436  7.104 14.431  0\\.   ]\n  'aged_70_older'     :\n[ 1.845 13.799  4.458  9.207  0\\.   ]\n  'gdp_per_capita'    : \n[ 7738.975 34272.36  10581.936 46482.957     0\\.   ]\n  'extreme_poverty'   : [b'' b'1.0' b'3.6' b'0.2' b'']\n  'cardiovasc_death_rate': \n[333.436  99.403 140.448 117.992   0\\.   ]\n  'diabetes_prevalence': [3.94 7.17 5.55 5.31 0\\.  ]\n  'female_smokers'    : \n[b'1.7' b'27.4' b'2.0' b'14.3' b'']\n  'male_smokers'      : \n[b'16.5' b'31.4' b'12.3' b'15.2' b'']\n  'handwashing_facilities': \n[24.097  0\\.    80.635  0\\.     0\\.   ]\n  'hospital_beds_per_thousand': \n[2.1  2.97 1.5  2.91 0\\.  ]\n  'life_expectancy'   : \n[60.19 83.56 77.01 82.99 80.67]\n```", "```py\nfeatures, label = next(iter(csv_dataset))\n```", "```py\n<tf.Tensor: shape=(5,), dtype=float32, \nnumpy=array([ 0.,  0.,  1., 33., 29.], dtype=float32)>\n```", "```py\nfeatures, label = next(iter(csv_dataset))\n```", "```py\n<tf.Tensor: shape=(5,), dtype=float32, \nnumpy=array([ 7., 15.,  1.,  0.,  6.], dtype=float32)>\n```", "```py\n<PROJECT_NAME>\n       train\n           class_1\n                <FILENAME>.jpg\n                <FILENAME>.jpg\n                \u2026\n           class_n\n                <FILENAME>.jpg\n                <FILENAME>.jpg\n                \u2026\n       validation\n           class_1\n               <FILENAME>.jpg\n               <FILENAME>.jpg\n               \u2026\n           class_n\n               <FILENAME>.jpg\n               <FILENAME>.jpg\n       test\n           class_1\n               <FILENAME>.jpg\n               <FILENAME>.jpg\n               \u2026\n           class_n\n               <FILENAME>.jpg\n               <FILENAME>.jpg\n               \u2026\n```", "```py\nimport tensorflow as tf\n\ndata_dir = tf.keras.utils.get_file(\n    'flower_photos',\n'https://storage.googleapis.com/download.tensorflow.org/\nexample_images/flower_photos.tgz', untar=True)\n```", "```py\n'/Users/XXXXX/.keras/datasets/flower_photos'\n```", "```py\n-rw-r-----    1 mbp16  staff  418049 Feb  8  2016 LICENSE.txt\ndrwx------  801 mbp16  staff   25632 Feb 10  2016 tulips\ndrwx------  701 mbp16  staff   22432 Feb 10  2016 sunflowers\ndrwx------  643 mbp16  staff   20576 Feb 10  2016 roses\ndrwx------  900 mbp16  staff   28800 Feb 10  2016 dandelion\ndrwx------  635 mbp16  staff   20320 Feb 10  2016 daisy\n```", "```py\n    train_datagen = tf.keras.preprocessing.image.\n        ImageDataGenerator(\n        rescale = 1./255, \n        validation_split = 0.20)\n    ```", "```py\n    datagen_kwargs = dict(rescale=1./255, \n                          validation_split=0.20)\n\n    train_datagen = tf.keras.preprocessing.image.\n        ImageDataGenerator(**datagen_kwargs)\n    ```", "```py\n    IMAGE_SIZE = (224, 224) # Image height and width \n    BATCH_SIZE = 32             \n    dataflow_kwargs = dict(target_size=IMAGE_SIZE, \n                          batch_size=BATCH_SIZE, \n                          interpolation=\"bilinear\")\n\n    train_generator = train_datagen.flow_from_directory(\n    data_dir, subset=\"training\", shuffle=True, \n    **dataflow_kwargs)\n    ```", "```py\n    labels_idx = (train_generator.class_indices)\n    idx_labels = dict((v,k) for k,v in labels_idx.items())\n    ```", "```py\n    {0: 'daisy', 1: 'dandelion', 2: 'roses', \n      3: 'sunflowers', 4: 'tulips'}\n    ```", "```py\n    for image_batch, labels_batch in train_generator:\n      print(image_batch.shape)\n      print(labels_batch.shape)\n      break\n    ```", "```py\n    (32, 224, 224, 3)\n    (32, 5)\n    ```", "```py\nvalid_datagen = train_datagen\n\nvalid_generator = valid_datagen.flow_from_directory(\n    data_dir, subset=\"validation\", shuffle=False, \n    **dataflow_kwargs)\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimage_batch, label_batch = next(iter(train_generator))\n\nfig, axes = plt.subplots(8, 4, figsize=(10, 20))\naxes = axes.flatten()\nfor img, lbl, ax in zip(image_batch, label_batch, axes):\n    ax.imshow(img)\n    label_ = np.argmax(lbl)\n    label = idx_labels[label_]\n    ax.set_title(label)\n    ax.axis('off')\nplt.show()\n```", "```py\nimage_batch, label_batch = next(iter(train_generator))\n```", "```py\nfig, axes = plt.subplots(8, 4, figsize=(10, 20))\n```", "```py\naxes = axes.flatten()\nfor img, lbl, ax in zip(image_batch, label_batch, axes):\n    ax.imshow(img)\n    label_ = np.argmax(lbl)\n    label = idx_labels[label_]\n    ax.set_title(label)\n    ax.axis('off')\nplt.show()\n```"]