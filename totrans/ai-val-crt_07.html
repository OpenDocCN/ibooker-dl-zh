<html><head></head><body><section data-pdf-bookmark="Chapter 7. Where This Technology Is Headed&#x2014;One Model Will Not Rule Them All!" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch07_where_this_technology_is_headed_one_model_will_not_1740182051667482">
<h1><span class="label">Chapter 7. </span>Where This Technology Is Headed—<span class="keep-together">One Model Will <em>Not</em> Rule Them All!</span></h1>

<p>Can you place this mantra?</p>

<blockquote>
<p>“One Ring to rule them all,</p>

<p>One Ring to find them,</p>

<p>One Ring to bring them all,</p>

<p>and in the darkness bind them.”</p>
</blockquote>

<p>If you’re a true Tolkienite nerd, your elf ears likely perked up; otherwise, we’ll tell you it’s the basis of the story for J.R.R. Tolkien’s iconic <em>Lord of the Rings</em> and this One Ring inscription gives its wearer the ability to control everything. (Purists will note it wasn’t the inscription that bestowed the power and then go on about Sauron, but we’ll leave it there; like we said, nerds.) Total domination. Putting all the evil aside, one question looms (likely due to the fanfare around ChatGPT that introduced the world to GenAI): will one single LLM rule them all?</p>

<p>Spoiler alert: we don’t think so at all. Not even close. As you learned earlier in this book, there are almost 1.5 million (it’s likely more by the time you read this book) models<a contenteditable="false" data-primary="models" data-type="indexterm" id="xi_models711137"/> on Hugging Face alone. We’re also certain (assuming you’ve read the book linearly so far) that you can easily articulate the difference between Value Users and Value Creators, and you understand AI ethics and data lineage. In short, you understand why one model can’t possibly rule them all...but we’re going to pull a more complete answer to the <em>why</em> for you here. It starts with the fact that even in the AI labs pushing out the highest-performing frontier models, we are seeing shifts from innovating on a single model performing a task, to empowering a system of models and techniques to work together and complete a task. In this chapter, we want to draw your attention to what’s been going on in the marketplace, and to which trends and technological innovations are powering the future of GenAI. From the rapid innovations that are happening at the small model size, to intra- and inter-model routing, to exciting advancements in agentic systems, we believe there will never be one model to rule them all.</p>

<section data-pdf-bookmark="The Bigger the Better, Right? Perhaps at the Start, But That Was a Long Time Ago" data-type="sect1"><div class="sect1" id="ch07_the_bigger_the_better_right_perhaps_at_the_start_1740182051667698">
<h1>The Bigger the Better, Right? Perhaps at the Start, <span class="keep-together">But That Was a Long Time Ago</span></h1>

<p>Keeping with our theme in this book that while tech years age like dog years (1:7), GenAI years are like mouse years (1:30), that makes 2018 over 2 centuries old in GenAI years—that’s a long time ago! What happened in 2018? OpenAI<a contenteditable="false" data-primary="OpenAI" data-type="indexterm" id="id902"/> released <a href="https://oreil.ly/IBZTG">GPT-1</a> with a mere 117 million parameters.</p>

<p>As a part of their quest<a contenteditable="false" data-primary="artificial general intelligence (AGI)" data-type="indexterm" id="id903"/><a contenteditable="false" data-primary="AGI (artificial general intelligence)" data-type="indexterm" id="id904"/> toward <em>artificial general intelligence (AGI)</em>, OpenAI has built successively more capable GPT versions (some into the trillions of parameters) that can perform more tasks with each successive release.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>AGI shouldn’t be confused with GenAI<a contenteditable="false" data-primary="generative AI (GenAI)" data-secondary="versus AGI" data-secondary-sortas="AGI" data-type="indexterm" id="id905"/>. GenAI is a tool. AGI is a goal of evolving that tool to the extent that its capabilities match human cognitive abilities, or even surpasses them, across a wide range of tasks. We’re not there yet, perhaps never will be, or perhaps it’ll arrive sooner than we expected. But when it comes to AGI, think about LLMs demonstrating and exceeding humanlike intelligence.</p>
</div>

<p>Initially, it seemed that the main vehicle for driving model performance<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="historical perspective" data-type="indexterm" id="xi_largelanguagemodelsLLMshistoricalperspective71984"/> improvements was simply increasing a model’s size. As shown in <a data-type="xref" href="#table-7-1">Table 7-1</a>, between GPT-1 and GPT-3, the models released by OpenAI increased by more than 10,000 times in size! After GPT-3, OpenAI stopped publishing model sizes all together, but GPT-4 and the GPT-4o models were rumored<sup><a data-type="noteref" href="ch07.html#id906" id="id906-marker">1</a></sup> at one point to total over one trillion parameters! And as these models have gotten larger, they have also gotten more expensive. Small models normally cost less than $0.25 for 1 million output tokens (or “free” if you can get it on your laptop with frameworks like Ollama). In contrast, big models are pricier. For example, last we looked, OpenAI’s o1 costs were about $60 for the same amount of output.<sup><a data-type="noteref" href="ch07.html#id907" id="id907-marker">2</a></sup> Whatever the price you’re paying (prices in this space are changing as fast as the technology, mostly in a good way), high performance small models have a lot of business sense to them.</p>

<p><a data-type="xref" href="#table-7-1">Table 7-1</a> shows that as the GPT family of models has grown, the world has witnessed significant improvements in the capabilities that these models could achieve.</p>

<table class="striped" id="table-7-1">
	<caption><span class="label">Table 7-1. </span>OpenAI’s GPT family over time</caption>
	<thead>
		<tr>
			<th>OpenAI model name</th>
			<th>Parameters</th>
			<th>Interesting things to note</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>GPT-1</td>
			<td>117 million</td>
			<td>This is the “original.” It was better than some previous technologies, but turned out to be just the start of something that was going to be big.</td>
		</tr>
		<tr>
			<td>GPT-2</td>
			<td>~1 billion</td>
			<td>This model started to make some interesting completions and prove that there was a different horizon for natural language processing (NLP). It was nowhere close to what you first experienced with ChatGPT and beyond, but it got some press in the news for writing a story about unicorns.<sup><a data-type="noteref" href="ch07.html#id908" id="id908-marker">a</a></sup></td>
		</tr>
		<tr>
			<td>GPT-3<br/>
			GPT-3.5<br/>
			GPT-3.5 Turbo</td>
			<td>~175 billion</td>
			<td>GPT-3.5 was the initial model behind ChatGPT’s debut. Two big changes occurred compared to GPT-2. It was designed to follow instructions (versus simply predicting the next most likely word in a sentence), <em>and</em> they put a user interface on it. Enough said. GPT-3.5 was also released as a more efficient, lightweight version called “Turbo.”</td>
		</tr>
		<tr>
			<td>GPT-4<br/>
			GPT-4 turbo<br/>
			GPT-4o<br/>
			GPT-4o mini<br/>
			GPT-4.5</td>
			<td>OpenAI stopped publishing parameter numbers after GPT-3 (which was noted<br/>
			to have 175 billion parameters). Various blogs suggest GPT-4 has ~1.8 trillion parameters.</td>
			<td>Their fourth generation of models delivered more power and multimodal capabilities. At the time of publishing, GPT-4o was considered OpenAI’s “flagship” model, and GPT-4.5 just came out. GPT-5 wasn’t out when we went to print, but many are suggesting to expect it sometime in the middle of 2025.</td>
		</tr>
		<tr>
			<td>OpenAI o1<br/>
			<span class="keep-together">OpenAI o3 mini</span></td>
			<td>(See above.)</td>
			<td>Considered a separate project and not a part of the core GPT family, these reasoning models were trained to produce long chains of thought before responding, enabling them to solve more complex tasks. This capability is expected to be merged into GPT-5.</td>
		</tr>
	</tbody>
<tbody><tr class="footnotes"><td colspan="3"><p data-type="footnote" id="id908"><sup><a href="ch07.html#id908-marker">a</a></sup> See the story on <a href="https://oreil.ly/1JZ6W">OpenAI’s site</a>. </p></td></tr></tbody></table>

<p>This begs the question, do you need all that capacity for your business? Even OpenAI has started creating smaller, more efficient versions of their models. For each major model release, there has been a pairwise release of a more efficient and more cost-effective alternative. GPT-3.5, meet GPT-3.5 Turbo; GPT-4o, meet GPT-4o mini.</p>

<p>The latest reasoning model OpenAI released at the time this book was published was OpenAI o3 mini. While OpenAI originally committed to releasing OpenAI o3, they have since pressed pause, and announced instead that GPT-5 will introduce an AI system that brings together the best of OpenAI o3 and the GPT model series, with Sam Altman sharing the goal of “simplifying our product offerings” and “to return to magic unified intelligence.”<sup><a data-type="noteref" href="ch07.html#id909" id="id909-marker">3</a></sup></p>

<p>To sum up this section, even in the frontier AI labs that were made famous by innovating through scale, we are seeing innovations and road maps centered around bringing multiple models together, working as a system to drive “unified intelligence.”</p>

<p>And despite the common belief that bigger is always better when it comes to model size, there are many exciting innovations enabling small yet powerful LLMs. So much so that the term small language models (SLMs)<sup><a data-type="noteref" href="ch07.html#id910" id="id910-marker">4</a></sup> has emerged. There is no precise definition, but SLMs usually refer to LLMs that are normally fewer than 13 billion parameters in size. In some scenarios, SLMs have met the performance of LLMs 100+ billion parameters in size<a contenteditable="false" data-primary="" data-startref="xi_largelanguagemodelsLLMshistoricalperspective71984" data-type="indexterm" id="id911"/>.</p>
</div></section>

<section data-pdf-bookmark="The Rise of the Small Language Model" data-type="sect1"><div class="sect1" id="ch07_the_rise_of_the_small_language_model_1740182051667784">
<h1>The Rise of the Small Language Model</h1>

<p>Perhaps the simplest way to describe the phenomena that is SLMs<a contenteditable="false" data-primary="small language models (SLMs)" data-type="indexterm" id="xi_smalllanguagemodelsSLMs76475"/> is that model providers are getting better at training<a contenteditable="false" data-primary="training a model" data-secondary="improvements leading to SLMs" data-type="indexterm" id="xi_trainingamodelimprovementsleadingtoSLMs764130"/><a contenteditable="false" data-primary="models" data-secondary="training" data-type="indexterm" id="xi_modelstraining764130"/>. Case in point, when some of our research teams first got their hands on Llama-2-70B back in July 2023, they were amazed at what it could do. Just a little over a year later, they were able to achieve <em>the same, if not better,</em> performance using just a 2B parameter version of Granite, according to Hugging Face’s Open LLM v2 Leaderboard (see <a data-type="xref" href="#ch07_figure_1_1740182051640230">Figure 7-1</a>).</p>

<figure><div class="figure" id="ch07_figure_1_1740182051640230"><img alt="A screenshot of a graph  AI-generated content may be incorrect." src="assets/aivc_0701.png"/>
<h6><span class="label">Figure 7-1. </span>A snapshot of model performance, taken from Hugging Face’s Open LLM v2 Leaderboard in Feb 2024</h6>
</div></figure>

<p>This, again, is just part of the natural benefit of progressing up the learning curve of anything; as sure as our electric vehicles (EVs) go farther and charge faster, we’re getting more pixels and camera lenses on our phone every other year, and our TVs are getting thinner, providers are gaining more experience training models, and new innovations are making them more efficient.</p>

<p>In the next couple of sections, we want to share with you some of the promising strategies behind the rise of highly competitive SLMs, specifically data curation and model distillation.</p>

<p>It is no coincidence that both of these strategies center around the data used to train and fine-tune LLMs. It surprises many we talk to that, more often than not, advancements that are slimming down model size stem more from innovative strategies with training data than technical innovations in the model’s architecture itself. Please don’t misunderstand what we’re trying to tell you here. Innovations in architecture are definitely occurring. In fact, we cover some exciting architecture advancements in this very chapter! But, when we look at the warp speed of how SLMs have risen to prominence (and they did so within a year of the November 2022 release of ChatGPT), the contributing factor is clear: <em>data reigns supreme!</em> And we go into detail on these data-based trends because in <a data-type="xref" href="ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518">Chapter 8</a>, we will show you how the same techniques that model providers are using today to create SLMs can be used by your company to differentiate and create value with enterprise data<a contenteditable="false" data-primary="" data-startref="xi_trainingamodelimprovementsleadingtoSLMs764130" data-type="indexterm" id="id912"/><a contenteditable="false" data-primary="" data-startref="xi_modelstraining764130" data-type="indexterm" id="id913"/>.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>So here’s the deal: you’ve got data<a contenteditable="false" data-primary="proprietary data" data-secondary="as SLM advantage" data-type="indexterm" id="id914"/>. That data you have access to isn’t part of these LLMs at all. Why? Because it’s your corporate data. We can assure you that many LLM providers want it. In fact, the reason 99% of corporate data isn’t scraped and sucked into an LLM is because you didn’t post it on the internet. So, you have some choices to make that we talked about earlier in this book, and we will go deep into them in the next chapter. Where will you sit on the data value exchange continuum we talked about in <a data-type="xref" href="ch02.html#ch02_oh_to_be_an_ai_value_creator_1740182046162988">Chapter 2</a>? Are you planning to give it away and let others create disproportionate amounts of value from your data, essentially <em>making your data</em><em> THEIR</em><em> competitive advantage</em> OR are you going to <em>make your data</em><em> YOUR</em><em> competitive advantage</em>? That’s what this book is all about. And this and the next chapter help you see that through.</p>
</div>

<section data-pdf-bookmark="Data Curation Results in AI Salvation" data-type="sect2"><div class="sect2" id="ch07_data_curation_results_in_ai_salvation_1740182051667868">
<h2>Data Curation Results in AI Salvation</h2>

<p>OK, we admit it, you likely know this one<a contenteditable="false" data-primary="small language models (SLMs)" data-secondary="data curation results" data-type="indexterm" id="xi_smalllanguagemodelsSLMsdatacurationresults77755"/><a contenteditable="false" data-primary="data curation" data-type="indexterm" id="xi_datacuration77755"/><a contenteditable="false" data-primary="curation of data" data-type="indexterm" id="xi_curationofdata77755"/>. You don’t even have to have a machine learning background to assert that curating a large quantity of high-quality<a contenteditable="false" data-primary="data quality" data-type="indexterm" id="id915"/><a contenteditable="false" data-primary="high-quality data" data-type="indexterm" id="id916"/><a contenteditable="false" data-primary="quality of data" data-type="indexterm" id="id917"/> training data can have huge impacts on a model’s performance (or any analytics project for that matter).</p>

<p>But an emphasis on data curation is a huge part of why SLMs have become so performant, and it goes directly against the initial philosophy of the early LLM bakes: take as much messy, uncleaned, and unstructured data as possible and repurpose it to power an LLM. As it turns out, a compromise is in order when it comes to LLMs for business. Transformer technology made it possible to take large quantities of relatively messy data to create an LLM, but the higher quality the data, the higher quality the model. Ask yourself if you have large volumes of high-quality data that is specialized for business that you care about. Of course you do! Now you are ready to cook with gas because quantity, quality, and specialization are the three key data curation ingredients that have helped lead to the rise of SLMs.</p>

<section data-pdf-bookmark="Data quantity" data-type="sect3"><div class="sect3" id="ch07_data_quantity_1740182051667932">
<h3>Data quantity</h3>

<p>How much data is optimal for a given model<a contenteditable="false" data-primary="data quantity, optimal for model size" data-type="indexterm" id="xi_dataquantityoptimalformodelsize78158"/> size? This has been a subject of much study by the AI research community because, as you can imagine, there are very high environmental and pocketbook costs associated with training an LLM. For this reason, early model providers’ initial focus was trying to optimize performance while minimizing their own up-front costs for model training. A key part of this optimization was defining how many tokens (recall, this is essentially a piece of a word, a whole word, or even a punctuation mark) of language data should be introduced to a model for each additional parameter added to the overall size of the model they were training. These ratios—often referred<a contenteditable="false" data-primary="scaling laws" data-type="indexterm" id="id918"/> to as <em>scaling laws</em> in scientific literature—define how much data you need to scale up a model in size.</p>

<p>In their 2020<a contenteditable="false" data-primary="OpenAI" data-secondary="scaling laws" data-type="indexterm" id="id919"/> paper,<sup><a data-type="noteref" href="ch07.html#id920" id="id920-marker">5</a></sup> a team of OpenAI researchers posited that ~2 tokens of text should be used in training for every 1 parameter of an LLM. This 2:1 ratio became known<a contenteditable="false" data-primary="Kaplan’s scaling law" data-type="indexterm" id="id921"/> as <em>Kaplan’s scaling law </em>(we’re guessing “Kaplan et al.’s scaling law” didn’t have a good ring to it) and was subsequently used to train models like GPT-3 and BLOOM (both models are 175 billion parameters in size and were trained on 300–350 billion tokens of text). In 2022, Google’s DeepMind<a contenteditable="false" data-primary="DeepMind" data-type="indexterm" id="id922"/> published<sup><a data-type="noteref" href="ch07.html#id923" id="id923-marker">6</a></sup> an alternate view on optimal scaling ratios called the Chinchilla scaling law<a contenteditable="false" data-primary="Chinchilla scaling law" data-type="indexterm" id="id924"/>. (This law is also known as Hoffman’s scaling law<a contenteditable="false" data-primary="Hoffman’s scaling law" data-type="indexterm" id="id925"/>, named after the lead researcher; Chinchilla was a family of models published by DeepMind.) DeepMind’s researchers believed that OpenAI drastically underestimated the amount of data needed to optimally train an LLM...they felt the optimal scaling ratio to get <em>the best model performance for a given compute budget</em> was 20:1 as opposed to the ~2:1 ratio. They went on to build a 70 billion parameter Chinchilla LLM using this scaling law. How did it do? At a mere 70 billion parameters, Chinchilla performed much better than larger models like GPT-3 (175 billion <span class="keep-together">parameters</span>). Looking back, we think Chinchilla was kind of like the SLM “OG” (as the kids say—it’s slang for original). This model is still quite big, but it isn’t a huge triple-digit billion parameter model, or bigger.</p>

<p>The research community’s initial goal focused on defining scaling laws to optimize the fixed up-front training costs for their models. But what about the recurring marginal costs across the rest of the model’s lifecycle? A super large model will be more expensive to host and inference. And guess who gets to incur those costs? That’s right, you! To reduce these costs, you need to reduce model size. To reduce model size while maintaining performance, you need to train on more (high quality) data.</p>

<p>And this is <em>exactly why</em> SLMs are capturing so much attention. Since inference and hosting costs are directly passed to model consumers, there was a bit of a delayed reaction. But as GenAI turned from a curiosity to a deployed technology, model providers have started optimizing their training setup to be as inference-efficient as possible, not merely training-efficient.</p>

<p>To create inference-efficient models<a contenteditable="false" data-primary="inference-efficient models" data-type="indexterm" id="id926"/>, it can be cost-effective to train a model on a higher data ratio than what even the Chinchilla scaling law had in mind. At the time this book went to print, the scientific community had not converged upon a universal scaling law for inference-optimal models (and perhaps never will), but there are compelling industry examples of very performant SLMs that are trained on much larger amounts of data than the doctrines of Chinchilla or Kaplan would suggest (we show some of these scaling laws over time in <a data-type="xref" href="#ch07_table_2_1740182051649140">Table 7-2</a>).</p>

<p>In February of 2023, Meta<a contenteditable="false" data-primary="Llama" data-type="indexterm" id="id927"/><a contenteditable="false" data-primary="open source AI" data-secondary="Llama’s move to open source" data-type="indexterm" id="id928"/><a contenteditable="false" data-primary="Meta" data-secondary="Llama" data-type="indexterm" id="id929"/> open sourced its Llama 2 model series, trained on about 2 trillion tokens of training data (at the time, this was considered a massive amount of data). In the Llama 2 series, the 7 billion sized model had almost a 300:1 scaling ratio! By August of 2024, with the release of Llama 3, Meta doubled (well, actually octupled) down and released its Llama3.1-8B model. This model, trained on over 15 trillion tokens has almost a 2,000:1 data density ratio and boasts even higher performance than the Llama 2 series.<sup><a data-type="noteref" href="ch07.html#id930" id="id930-marker">7</a></sup> Sensing a trend? Meta kept its SLM pretty much the same size, but improved performance significantly, just by training on more data!</p>

<table id="ch07_table_2_1740182051649140">
	<caption><span class="label">Table 7-2. </span>Scaling laws over time</caption>
	<thead>
		<tr>
			<th>Date</th>
			<th>Number training tokens/parameter</th>
			<th>Scaling law</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>1/23/20</td>
			<td>1.7</td>
			<td>Kaplan</td>
		</tr>
		<tr>
			<td>3/29/22</td>
			<td>20</td>
			<td>Chinchilla</td>
		</tr>
		<tr>
			<td>2/1/23</td>
			<td>286</td>
			<td>Llama-2-7B</td>
		</tr>
		<tr>
			<td>8/1/23</td>
			<td>1875</td>
			<td>Llama-3.1-8B</td>
		</tr>
	</tbody>
</table>

<p>In fact, in the technical paper accompanying that release, “The Llama 3 Herd of Models,” Meta cited that its 405B parameter flagship model, also trained on ~15 trillion tokens, is “approximately compute optimal” from a training perspective, but that its smaller models were trained “for much longer than is compute-optimal. The resulting models perform better than compute-optimal models at the same inference budget.”<sup><a data-type="noteref" href="ch07.html#id931" id="id931-marker">8</a></sup> Quite simply, while these smaller models were more expensive to train (trained for longer on more data), they are far more efficient to run at inference time. The result? Today, the Llama 3 models are some of the most popular open source models available, and we expect that when it arrives sometime in 2025, Llama 4 will be just as popular.</p>

<p>Bringing this back to SLMs: with data ratios that require over hundreds of tokens of data for every parameter in a model, inference-optimized models and SLMs start to mean the same thing. It is near impossible to have a big, inference-optimized LLM. Given data acquisition costs and the amount of data available in the world, these data ratios are simply too expensive to support training inference-optimal LLMs that are hundreds of billions of parameters in size. We just don’t have enough data.</p>

<p>There is a real question of when will we hit the data ceiling? Today’s models are trained on upward of 15 trillion tokens, but to get there, model providers have basically had to plumb the entirety of the internet. And, as you will see in the next section, we don’t need large quantities of <em>any</em> data, we need volumes of <em>very high-quality</em> data, which is even more difficult to obtain.<a contenteditable="false" data-primary="" data-startref="xi_dataquantityoptimalformodelsize78158" data-type="indexterm" id="id932"/></p>
</div></section>

<section data-pdf-bookmark="Data quality" data-type="sect3"><div class="sect3" id="ch07_data_quality_1740182051667992">
<h3>Data quality</h3>

<p>Can you imagine<a contenteditable="false" data-primary="data quality" data-type="indexterm" id="xi_dataquality712431"/><a contenteditable="false" data-primary="high-quality data" data-type="indexterm" id="xi_highqualitydata712431"/><a contenteditable="false" data-primary="quality of data" data-type="indexterm" id="xi_qualityofdata712431"/> the song “Cecelia” without Garfunkel and just Simon? And could Hall &amp; Oates have put anyone’s “Kiss on My List” if they didn’t start that song’s opening with a 1980s combination of keyboards and a cheesy mustache that sublimely screamed, “I got the romance covered? You just press the play button?” (Yes, younger readers...back then we had to press an actual clunky physical button.) And although we’re dating ourselves musically, it’s not only difficult to understand how great these songs could have been without the partnerships, it’s just as difficult to isolate the impact of data quantity from the impact of data quality in an LLM. Quality and data and great high-performing efficient models go together...just like Simon &amp; Garfunkel and Hall &amp; Oates.</p>

<p>Now, if you believe that the internet has only trustworthy data, that internet data has no bias, profanity, hate, lies, or anger...none of that, then you can probably stop reading this book. That belief is akin to eating a gallon of ice cream a day and wondering how your jeans shrank when you only wash them in cold water. When it comes to GenAI, the adage still applies: garbage in, garbage out! The reality <em>still</em> holds that the more you can do to curate the data used to train your model (both in terms of securing large quantities of it and with with high-quality labeled examples), the more performance you can pack into your model. And while there are some techniques around improving your model’s performance after it is trained—like retrieval-augmented generation (RAG) and more, these techniques all benefit from a high-quality data starting point (more on that in a bit).</p>

<p>Microsoft<a contenteditable="false" data-primary="Phi-2 SLM" data-type="indexterm" id="id933"/><a contenteditable="false" data-primary="Microsoft" data-secondary="Phi-2 SLM" data-type="indexterm" id="id934"/> publicly credits data quality playing a critical role for enabling its (at the time) state-of-the-art (SOTA) Phi-2 2.7 billion parameter SLM that in some benchmarks outperformed larger models 25 times its size. But you could tell Microsoft had sniffed out this path forward before Phi-2 because it introduced its predecessor (Phi-1) to the world through a research publication<sup><a data-type="noteref" href="ch07.html#id935" id="id935-marker">9</a></sup> titled “Textbooks Are All You Need.” In this paper, Microsoft described how “high-quality data can even improve SOTA LLMs while dramatically reducing the dataset size and training compute.” And in the same way humans learn better from clearly laid-out textbooks, Microsoft’s findings support that textbook-quality training data that is “clear, self-contained, instructive, and balanced” results in better-performing LLMs that demonstrate better scaling laws; and of course, this enabled LLMs with the scale and performance of Phi-2 to become (at the time) SOTA. At the time of publishing this book, Microsoft had just released their fourth iteration of this SLM: Phi-4. Similarly to Phi-1 and Phi-2, Microsoft cited “improved data” (among other training advancements) as a core driver to <a href="https://oreil.ly/he3Nf">Phi-4</a> achieving strong performance relative to its size.</p>

<p>Though we talked about this earlier in the book, it’s so important we thought we’d repeat it here because high quality data is critical to SLMs. While many model providers are transparent about the amount of data used to train an LLM, <em>very few</em> providers are transparent about the actual sources of data<a contenteditable="false" data-primary="sources of data, lack of transparency from LLM providers" data-type="indexterm" id="id936"/> that were used to train <em>their</em> LLM. In fact, if you asked the most popular LLM providers what data they used to train their model, they either won’t be able to tell you or tell you it’s none of your business, to which you should reply, <em>“But this is my business!”</em></p>

<p>The bottom line is that the highest quality datasets are long textbooks or other nonfiction books written and copyrighted by humans—not mid-starred or higher Reddit posts and other free-form information sources. High-quality data artifacts aren’t generic snapshots of web content put on public sites that automated crawlers can collect. The ugly truth behind many popular LLMs is that their inclusion of many of the best quality datasets<a contenteditable="false" data-primary="Books3 dataset, copyright issue" data-type="indexterm" id="id937"/> (such as the Books3<sup><a data-type="noteref" href="ch07.html#id938" id="id938-marker">10</a></sup> corpus we first introduced you to in <span class="keep-together"><a data-type="xref" href="ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635">Chapter 5</a></span>) is unfortunately only available for use in model training because they were <span class="keep-together">pirated</span> and posted without author permission. Again, some of our own previous hard work was vacuumed into the inner bowels of multiple LLMs for all to take advantage of and others to profit from. We didn’t get a choice. We weren’t even asked; it just happened. And while we’re not filing suit (it’s not like we wrote some catchy bestseller titled <em>50 Shades of Big Data</em><em> </em>that flew off the shelves and Hollywood wanted to make into a movie), there are a lot of people whose livelihoods and business differentiation were “stolen” to make the LLM you’ve also likely used. This all goes back to the value exchange discussion we had in <a data-type="xref" href="ch02.html#ch02_how_do_you_consume_ai_be_ye_a_value_creator_or_a_1740182046163502">“How Do You Consume AI: Be Ye a Value Creator or a Value User?”</a>.</p>

<p>Only transparent<a contenteditable="false" data-primary="transparency" data-secondary="in data collection" data-secondary-sortas="data collection" data-type="indexterm" id="id939"/> data collection and curation policies can ensure that the LLMs you’re evaluating for your business did not benefit from unethically sourced data. The takeaway? When evaluating SLMs, where data curation is critical for driving performance (and putting aside the legal ramifications), having a heightened awareness of how the data behind the model was sourced is crucial. Ask questions. Demand answers<a contenteditable="false" data-primary="" data-startref="xi_dataquality712431" data-type="indexterm" id="id940"/><a contenteditable="false" data-primary="" data-startref="xi_highqualitydata712431" data-type="indexterm" id="id941"/><a contenteditable="false" data-primary="" data-startref="xi_qualityofdata712431" data-type="indexterm" id="id942"/>.</p>
</div></section>

<section data-pdf-bookmark="Domain specialization" data-type="sect3"><div class="sect3" id="ch07_domain_specialization_1740182051668049">
<h3>Domain specialization</h3>

<p>Being the weekend athlete you are, you find yourself back at home<a contenteditable="false" data-primary="domain specialization, SLMs" data-type="indexterm" id="xi_domainspecializationSLMs713381"/> with an ankle giving you mixed signals—it’s either auditioning for a spot on the soon-to-be-a-hit reality show, “So You Think You Broke Your Ankle,” or it’s just being dramatic with a sprain. Either way, it’s demanding ice and attention. Now it’s up to you to figure out what’s going on. To make this determination, do you ask the smartest person you know, or do you ask a doctor? (Don’t be cheeky...we know some of you just said aloud, “The smartest person I know <em>is</em> a doctor.”) While the smartest person you know might have amazing talents that span poetry, chemistry, philosophy, and more, you’re far better off asking a doctor, even better if they specialize in orthopedics. That doctor’s poetry skills be damned; when the question at hand is specialized in nature (your potentially fractured ankle), it is more important to ask a specialized expert than a general expert.</p>

<p>As it turns out, the same holds true for SLMs. And as you’ve likely figured out by now (because it’s a section in this chapter), there’s increasing evidence that smaller, specialized models<a contenteditable="false" data-primary="specialized models" data-type="indexterm" id="id943"/> can meet or beat larger general-purpose LLMs when evaluated on <em>specialized</em> tasks. And when we say a specialized model, what we really mean is a model that is trained on a significant amount of <em>domain-specific</em> data. For example, in late 2022, a team from Stanford<a contenteditable="false" data-primary="BioMedLM" data-type="indexterm" id="id944"/> announced <a href="https://oreil.ly/QrqbE">BioMedLM</a>,<sup><a data-type="noteref" href="ch07.html#id945" id="id945-marker">11</a></sup> a 2.7 billion parameter model trained on biomedical literature data. When evaluated on United States Medical Licensing Examination (USMLE) questions, a fine-tuned version of BioMedLM outperformed a similarly fine-tuned unspecialized model of the same size (GPT Neo) by 17%. When evaluated against an untuned model that was 44 times bigger (Meta’s Galactica 120B model), BioMedLM outperformed it by almost 6%. But the critical point is whether or not Galactica was good for the task at hand; unlike BioMedLM, Galactica’s size made fine-tuning it cost prohibitive. At just 2.7 billion parameters, the tiny BioMedLM LLM demonstrated it could maintain a specialized advantage while also allowing further customization for fine-tuning. This is a very early example of the impact of domain specialization in GenAI, but these examples have kicked off a huge area of research and application of specializing models on targeted use cases.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Despite seemingly performing well on the medical-based benchmark in Stanford’s tests, Meta’s Galactica<a contenteditable="false" data-primary="Galactica" data-type="indexterm" id="id946"/><a contenteditable="false" data-primary="Meta" data-secondary="Galactica" data-type="indexterm" id="id947"/> (specifically designed to help scientists) was launched into the scientific community with a big bang—until it came crashing down with a thud and was taken offline just three days after its general availability. Public experimentation brought to light many examples of bias, toxicity, and hallucinations that led to scientific nonsense. This is why it’s important to fully appreciate what we discussed in <a data-type="xref" href="ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635">Chapter 5</a>.</p>
</div>

<p class="fix_tracking">Specialization can be especially important for “low resource” domains, areas where there isn’t a lot of data. For example, in <a data-type="xref" href="ch04.html#ch04_the_use_case_chapter_1740182047877425">Chapter 4</a> we told you how the IBM Z (mainframe) runs most of the world’s transactions. In the parlance of LLMs, something classified<a contenteditable="false" data-primary="low-resource data domains" data-type="indexterm" id="id948"/> as <em>low resource </em>are those domains with very little data available for training AI systems. As you can imagine, COBOL is considered a <em>low-resource</em> language, as there is very little public domain enterprise-worthy COBOL data today, especially when compared to Python, SQL, and other popular coding languages (yes, lots of business logic is coded in SQL). But there’s a lot of COBOL out there running businesses—the most critical parts. In fact, Reuters estimates<sup><a data-type="noteref" href="ch07.html#id949" id="id949-marker">12</a></sup> that today there are over 230 billion lines of COBOL code—supporting over $3 trillion of commerce—actively running in enterprises.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p class="fix_tracking">For clarity, the IBM Z supports modern application development tool sets and methodologies like fully automated continuous integration/continuous deployment (CI/CD) pipelines using Jenkins and Zowe, Kafka streams, node.js, Kubernetes, Ansible, Terraform, and more. But there is a lot of critical business logic built a long time ago that was written in COBOL that is deemed mission <span class="keep-together">critical</span>.</p>
</div>

<p>For all those code-assist LLMs that scraped code repositories to build a code-tuned LLM, guess how much COBOL is available for use? For example, one popular dataset for training code-assist LLMs is GitHub Codespaces—it contains 1 terabyte of code from 32 different languages. But COBOL is not covered. Why not? Remember earlier in this book how critical your data is and how today’s LLMs aren’t built on enterprise data. Now think back to those transactions running on IBM Z (credit cards, ATMs, airlines). Do you think that code is just sitting there ready to be scraped by the world? Of course not! So how could an LLM help in this scenario?</p>

<p>Back in 2023, IBM Research trained a 20 billion parameter code model (called granite.20b.cobol) that specializes in COBOL. To specialize a model specifically on COBOL<a contenteditable="false" data-primary="COBOL" data-type="indexterm" id="id950"/>, the IBM Research team held aside separately acquired COBOL data, trained a general-purpose code model first, and then specialized that model by training it further on a dataset that was highly concentrated with high-quality curated COBOL data (this is just like your proprietary data waiting to be put to work). The end result? The COBOL-focused SLM model <a href="https://oreil.ly/H4Sgk">significantly outperformed ChatGPT for COBOL completions on the CodeNet benchmark datasets</a>.</p>

<p>The takeaway? Purpose-built foundation models with quality at their core means better performance and more efficiency. This concept will become hugely important in <a data-type="xref" href="ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518">Chapter 8</a> as we discuss how you can specialize pretrained models using your enterprise data<a contenteditable="false" data-primary="" data-startref="xi_domainspecializationSLMs713381" data-type="indexterm" id="id951"/>.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Think About This When It Comes to Data Curation" data-type="sect2"><div class="sect2" id="ch07_think_about_this_when_it_comes_to_data_curation_1740182051668110">
<h2>Think About This When It Comes to Data Curation</h2>

<p>Beyond the ethical considerations for data curation, understanding and appreciating data scaling laws<a contenteditable="false" data-primary="scaling laws" data-type="indexterm" id="id952"/> and the impact of data quality and domain specialization on performance can help you find more cost-efficient SLM alternatives to bigger, less optimally trained, expensive-to-inference monster LLMs. As suggested before, older LLMs tend to be less data dense and, therefore, less inference efficient because they were trained back when the Kaplan and Chinchilla scaling laws first came out.</p>

<p>And while data quantity is most relevant for those training a model from scratch, for anyone trying to customize already trained models, as we cover in <a data-type="xref" href="ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518">Chapter 8</a>, the lessons on data quality and domain specialization still apply<a contenteditable="false" data-primary="" data-startref="xi_smalllanguagemodelsSLMsdatacurationresults77755" data-type="indexterm" id="id953"/><a contenteditable="false" data-primary="" data-startref="xi_datacuration77755" data-type="indexterm" id="id954"/><a contenteditable="false" data-primary="" data-startref="xi_curationofdata77755" data-type="indexterm" id="id955"/>.</p>
</div></section>

<section data-pdf-bookmark="Model Distillation—Using AI to Improve AI" data-type="sect2"><div class="sect2" id="ch07_model_distillation_using_ai_to_improve_ai_1740182051668177">
<h2>Model Distillation—Using AI to Improve AI</h2>

<p>Let’s talk about the second major technological innovation that is driving SLMs: model distillation<a contenteditable="false" data-primary="small language models (SLMs)" data-secondary="model distillation" data-type="indexterm" id="xi_smalllanguagemodelsSLMsmodeldistillation7161113"/><a contenteditable="false" data-primary="model distillation" data-type="indexterm" id="xi_modeldistillation7161113"/><a contenteditable="false" data-primary="distillation, model" data-type="indexterm" id="xi_distillationmodel7161113"/>. Model distillation is often used when you want the accuracy of a large neural network but need something more practical for real-time applications or devices with limited computational power. It’s really another technique to pack <span class="keep-together">big-model</span> performance into a small form factor; and while at first blush it might seem like a bit of a hack, it is actually an incredibly powerful tool. Model distillation <span class="keep-together">is where</span> a large frontier (big, expensive, state-of-the-art) model, such as Llama-3.1-405B, can instruct a smaller model, such as Llama3.1-8B, teaching it to behave like the bigger model.</p>

<p>A great example of this is a would be trying to replicate Tootsie Tomanetz’s BBQ mastery. This 85-year old custodian by day and pitmaster by night is the legend behind the famous Hill Country BBQ (Texas).</p>

<p>She’ll outright tell you that if she gave you the recipe, you still couldn’t recreate what she does. We’ve all been there—trying to capture the magic of a grandparent’s cooking, only to realize it’s more than just ingredients; it’s a lifetime of love and technique.</p>

<p>For example, when asked what the right temperature was to start a beef brisket cook, she notes she has no idea...she just puts her hand on the smoker and goes by feel. (That reminded us of one of our grandmothers who used her finger as a pincushion.) But we’re willing to bet that if we could spend a week with Tootsie and pepper her (no pun intended) with nonstop questions, we could eventually learn how to make a pretty close to award-winning beef brisket. We surely wouldn’t know all the she knows. For example, we wouldn’t know how she makes her incredible sauces. But if you gave us another week of nonstop questions, we would likely be able to figure something pretty good there too. Next up, the chicken.</p>

<p>Essentially, model distillation is like extracting all the essential knowledge from a heavyweight model into a more lightweight version, so you get similar performance but with less complexity.</p>

<p>In a lot of ways, model distillation is just a new, cheaper way to create training data<a contenteditable="false" data-primary="training data, model distillation to create" data-type="indexterm" id="id956"/>. As LLMs become better and better at different tasks, they become powerful tools for generating training data that used to need to be defined by hand by an army of data annotators. To perform distillation, research scientists leverage a teacher model (the large all-knowing one) to generate a large amount of synthetic data<a contenteditable="false" data-primary="synthetic data" data-type="indexterm" id="id957"/> that exemplifies a target set of behaviors the teacher model knows how to perform (like the cooking skill in our example). This synthetic data is often conversational in nature, representing question-answer (QA) pairs, or multiturn conversations. The synthetic data is then used to fine-tune the smaller (student) model, thereby imbuing the behavior patterns of the larger model into the smaller model. And while it may first appear this technique is only surface level, getting the small model to mimic the larger model’s performance has been shown to be incredibly powerful. In fact, back in 2023, in an early example of model distillation, researchers from the Large Model Systems (LSMYS)<a contenteditable="false" data-primary="Large Model Systems (LSMYS)" data-type="indexterm" id="id958"/><a contenteditable="false" data-primary="LSMYS (Large Model Systems)" data-type="indexterm" id="id959"/> Organization distilled ChatGPT down into a 13 billion parameter model called Vicuna. Vicuna’s performance shocked the community when they first published their work. LSMYS reported<sup><a data-type="noteref" href="ch07.html#id960" id="id960-marker">13</a></sup> that their distilled ChatGPT model “achieves more than 90% quality [referring to its responses] of OpenAI ChatGPT.”</p>

<p>The open source community, including Stanford and LSMYS, were some of the first innovators leveraging this technique and have now become “victims” of their own success. Model distillation has gotten so popular (and competitively threatening) that most frontier model providers (like OpenAI, Google, Anthropic, among others) have written restrictions into their model’s usage terms and conditions stating that their models cannot be used to improve the performance of other competitive models.</p>

<p class="fix_tracking">While this limits the commercial viability of models distilled by the open source community, it is gangbusters for LLM providers with access to large models that make for perfect caffeine-infused teachers. For example, through its partnership with OpenAI, Microsoft released <a href="https://oreil.ly/pLXEK">Orca and Orca-2</a>, highly competitive SLMs<a contenteditable="false" data-primary="Orca and Orca-2 (Microsoft/OpenAI)" data-type="indexterm" id="id961"/><a contenteditable="false" data-primary="Microsoft" data-secondary="Orca and Orca-2" data-type="indexterm" id="id962"/><a contenteditable="false" data-primary="OpenAI" data-secondary="Orca and Orca-2" data-type="indexterm" id="id963"/> that benefit from distillations of GPT-4. And Google’s Gemini Nano and Gemini Pro<a contenteditable="false" data-primary="Gemini Nano and Gemini Pro (Google)" data-type="indexterm" id="id964"/><a contenteditable="false" data-primary="Google Gemini Nano and Gemini Pro" data-type="indexterm" id="id965"/> are Google’s distilled version of its larger <a href="https://oreil.ly/mzSFf">Gemini models</a>.</p>

<p>As this technique continues to improve, due consideration is needed on whether super-large models will ever be used for anything other than teaching smaller, faster, and more cost-efficient distilled models. For example, when NVIDIA released its 340 billion parameter<a contenteditable="false" data-primary="Nemotron-4-340B-Instruct" data-type="indexterm" id="id966"/><a contenteditable="false" data-primary="NVIDIA Nemotron-4-340B-Instruct" data-type="indexterm" id="id967"/> model, Nemotron-4-340B-Instruct, the primary use case highlighted on the model card was to “create training data that helps researchers and developers build their own LLMs” (aka model distillation).<sup><a data-type="noteref" href="ch07.html#id968" id="id968-marker">14</a></sup> Hosting a 340 billion parameter model for running live inference could be incredibly expensive. You better have a pretty high value use case to justify that deployment. <em>But</em>, using the model once to generate synthetic training data for a smaller model is a much more palatable one-time fixed cost that enables the deployment of a cheaper, smaller, and more performant model.</p>

<p>And while closed frontier model providers currently have a “competitive moat” for their SLMs thanks to their teaching models, we think there is huge potential for <span class="keep-together">disruption</span>. Very large open source models, like Nemotron-4-340B-Instruct, Llama-3.1-405B, and most recently DeepSeek-R1, are proving to be powerful teacher models, eroding this advantage.</p>

<p>As noted earlier in this book, a Chinese AI lab, DeepSeek<a contenteditable="false" data-primary="DeepSeek" data-type="indexterm" id="xi_DeepSeek716855"/>, open sourced its family of large 671 billion parameter Mixture of Experts (MoE) style LLMs<a contenteditable="false" data-primary="Mixture of Experts (MoE) architecture" data-type="indexterm" id="id969"/><a contenteditable="false" data-primary="MoE (Mixture of Experts) architecture" data-type="indexterm" id="id970"/>, including the now famous DeepSeek-R1<a contenteditable="false" data-primary="DeepSeek-R1" data-type="indexterm" id="id971"/> model. The DeepSeek model release is fascinating from a number of different dimensions, the least of which was how, in response to the release, NVIDIA’s market cap dropped by $600 billion in one day<sup><a data-type="noteref" href="ch07.html#id972" id="id972-marker">15</a></sup> as spectators around the world were amazed at the performance and reasoning capabilities delivered by a Chinese AI lab for a reported training cost (which wasn’t fully understood by the press and those that reacted to it) of $5.6 million.</p>

<p class="pagebreak-before">There is a lot to unpack here, particularly around the reported training costs, some of which we are going to discuss toward the end of this chapter as we cover the MoE architecture. But a large part of the DeepSeek release is actually an important story about the role of teacher models and model distillation—specifically, the collection of smaller “DeepSeek-Distill” reasoning models that were released alongside the much larger DeepSeek-R1 reasoning model<a contenteditable="false" data-primary="reasoning models" data-type="indexterm" id="id973"/><a contenteditable="false" data-primary="models" data-secondary="reasoning" data-type="indexterm" id="id974"/>.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="ch07_chain_of_thought_reasoning_1740182051668239">
<h1>Chain-of-Thought Reasoning</h1>

<p>A <em>reasoning model</em> is a new technique<a contenteditable="false" data-primary="chain-of-thought reasoning" data-type="indexterm" id="xi_chainofthoughtreasoning717261"/> used by model developers where a model is trained to take a quick (although it doesn’t have to be) “think about it” pause before returning an answer. During this pause, it will first generate some internal thoughts about how it should approach responding to a prompt. Then, once it has evaluated everything fully, it will reply with a final answer.</p>

<p>For example, in one quick test using the version of DeepSeek-R1 hosted on deepinfra.com, we asked a very basic question: “What weighs more, a pound of lead or a kilogram of feathers?”</p>

<p>DeepSeek-R1 then replied with:</p>

<blockquote>
<p><em>&lt;think&gt; </em></p>

<p><em>Okay, so the question is, “What weighs more, a pound of lead or a kilogram of feathers?” Hmm, let me think. Maybe it’s a trick question? I know pounds and kilograms are units of weight, but they belong to different systems. A pound is from the imperial system, and a kilogram is metric.</em></p>

<p><em>First, I need to compare them on the same scale. I remember that 1 kilogram is equal to about 2.2 pounds. So, if I have a kilogram of feathers, that’s already more than a pound. Wait, but the lead is given in pounds and the feathers in kilograms. So if a kilogram is 2.2 pounds, then a kilogram of feathers would weigh 2.2 pounds, right? Whereas the lead is just 1 pound. So the feathers would actually weigh more. But maybe people think that lead is heavier because it’s denser. But when you measure them by weight, a kilogram is more than a pound. So a kilogram of feathers would be heavier than a pound of lead. Let me check that again. If I convert the kilogram to pounds, 1 kg equals approximately 2.20462 pounds, so the feathers weigh over 2 pounds, while the lead is only 1 pound. So yes, the kilogram of feathers weighs more. So the answer must be the feathers. But I want to make sure there’s no trick in the question. They’re both measured in weight units, so if it’s a pound versus a kilogram, the kilogram is larger. Some people might confuse pounds and kilograms as being similar because they are both units, but actually, a kilogram is a bigger unit. So yeah, the feathers weigh more. </em></p>

<p><em>&lt;/think&gt;</em></p>

<p>The kilogram of feathers weighs more than the pound of lead.</p>

<p>Explanation:</p>

<p>1 kilogram (kg) is a unit of mass in the metric system, equivalent to 2.20462 pounds (lbs).</p>

<p>1 pound (lb) is smaller, part of the imperial system.</p>

<p>Thus, 1 kg of feathers (~2.2 lbs) is over twice as heavy as 1 lb of lead. Density (how tightly packed the mass is) might make lead feel heavier for the same volume, but weight depends on mass, not density. When comparing equal weights, the larger unit (kilogram) always outweighs the smaller one (pound).</p>

<p>Answer: A kilogram of feathers weighs more.</p>
</blockquote>

<p>Note how the internal chain-of-thought tokens are wrapped in the <em>&lt;think&gt;...&lt;/think&gt;</em> tags. And while this example’s level of internal, up-front reasoning is most definitely overkill for this simple riddle, when this technique is applied to much more complicated tasks—for example, to support coding—it can have a huge impact on <span class="keep-together">performance</span>.</p>

<p>Ultimately, chain-of-thought reasoning is an example of leveraging<a contenteditable="false" data-primary="inference-time compute" data-type="indexterm" id="id975"/> “inference-time compute” (more on this in <a data-type="xref" href="ch09.html#ch09_generative_computing_a_new_style_of_computing_1740182052619664">Chapter 9</a>) to improve a model’s response. It costs more to inference because this type of model needs to generate chain-of-thought reasoning tokens, in addition to the answer, but the model’s response ultimately improves. OpenAI’s o1 and o3 models as well as DeepSeek’s R1, R1-Zero, and R1-Distill models all leverage this type of technique and are therefore considered reasoning models<a contenteditable="false" data-primary="" data-startref="xi_chainofthoughtreasoning717261" data-type="indexterm" id="id976"/>.</p>
</div></aside>

<p>In order for DeepSeek to build efficient SLMs with reasoning capabilities, they first used R1 to generate a large volume (800k samples) of examples of reasoning in math and code domains.<sup><a data-type="noteref" href="ch07.html#id977" id="id977-marker">16</a></sup> Then they took that dataset and fine-tuned a set of open, third-party models produced by Meta (Llama) and Alibaba Cloud (Qwen), whose sizes ranged from 1.5 billion to 70 billion parameters, et voilà! A series of small DeepSeek-R1-Distill models with advanced math and code reasoning capabilities was born.</p>

<p>DeepSeek’s success in distilling reasoning capabilities into small models has inspired the open source community. Within days of the DeepSeek-R1 and DeepSeek-R1-Distill models being released, the open source community created distillation pipelines so that anyone could perform a similar distillation process using the SLM of their choice.<sup><a data-type="noteref" href="ch07.html#id978" id="id978-marker">17</a></sup> Similarly, in less than one month, over 400 DeepSeek-based distillation datasets were posted to Hugging Face so that others can easily leverage DeepSeek’s outputs in their model development pipelines!<sup><a data-type="noteref" href="ch07.html#id979" id="id979-marker">18</a></sup></p>

<p class="pagebreak-before">In many ways, improving the open source community’s ability to create powerful, distilled models may be one of the biggest long-term impacts of the DeepSeek release—this is why we saw DeepSeek as more of an iterative open source AI event than a disruptive event. At the time of its release, DeepSeek-R1 was the most powerful teacher model available for open source model distillation. No doubt, its release also potentially puts pressure on other large, proprietary model providers to release open source versions of their models.</p>

<p>Of course, you can easily see why large model incumbents might “fear” this process. Think about it. For a few thousand dollars (and a lot of AI expertise), a company could create its own proprietary distilled high-quality model that fuses its own data with frontier LLM performance. And, once trained, they could basically run these distilled SLMs for free. With the billions poured into big investment bets on anything GenAI, that’s bound to make a lot of investors nervous.</p>

<p>It is important to note that distillation is not just limited to big teacher models improving much smaller student models. In fact, at the time this book was <span class="keep-together">being written</span>, OpenAI publicly disclosed that it was exploring whether DeepSeek illegally distilled OpenAI model IP into the large, 671-billion–parameter DeepSeek-R1 model.<sup><a data-type="noteref" href="ch07.html#id980" id="id980-marker">19</a></sup> Irony aside (more on that in the next chapter), this situation underscores the gravity of model distillation and the important role this technique will play moving forward in the future of AI development<a contenteditable="false" data-primary="" data-startref="xi_DeepSeek716855" data-type="indexterm" id="id981"/>.</p>
</div></section>

<section data-pdf-bookmark="Think About This When It Comes to Model Distillation" data-type="sect2"><div class="sect2" id="ch07_think_about_this_when_it_comes_to_model_distillati_1740182051668302">
<h2>Think About This When It Comes to Model Distillation</h2>

<p>When considering models that have benefited from distillation for your use case, the most important consideration (as alluded to before) is the terms and conditions under which this model is eligible to be used, <em>especially</em> in the case of open source models. You need legal involved here because a distilled model could potentially inherit contractual terms from the teacher model and the base model that was tuned. For example, under the Meta Llama 3<a contenteditable="false" data-primary="Llama" data-type="indexterm" id="id982"/><a contenteditable="false" data-primary="Meta" data-secondary="Llama" data-type="indexterm" id="id983"/> Community License Agreement, all models distilled from a Llama 3 model have specific naming requirements (the model’s name needs to start with “Llama 3”), and they need to be licensed under the same Llama 3 license.<sup><a data-type="noteref" href="ch07.html#id984" id="id984-marker">20</a></sup> In extreme cases, the model could potentially have been distilled from a teacher model in violation of the terms of that model’s provider, as OpenAI is investigating with DeepSeek-R1. This is yet another reason why transparency of data sources remains critical so that consumers can do their own due diligence on whether a model is suitable for use.</p>

<p>Finally, it is critical that you understand the limitations of the teacher model and strategy that was used to do the actual distillation. To demonstrate what we mean, let’s take a look back at teacher model and distillation strategy of those DeepSeek-R1<a contenteditable="false" data-primary="DeepSeek-R1" data-type="indexterm" id="id985"/>-Distill models:</p>

<dl>
	<dt>Teacher model</dt>
	<dd>
	<p><em>DeepSeek-R1.</em> As discussed above, this model demonstrates SOTA reasoning capabilities, but it also has a number of significant safety issues. A team from Cisco and the University of Pennsylvania found that DeepSeek-R1 “exhibited a 100% attack success rate, meaning it failed to block a single harmful prompt” in their automated jail-breaking attacks.<sup><a data-type="noteref" href="ch07.html#id986" id="id986-marker">21</a></sup> Further, when asked factual questions for information about Tiananmen Square, the model declines (depending on where it is hosted) to respond. If asked, for example, “Do I need a passport to go to Taiwan?”, the model will immediately reply with: “According to the official policy of the Chinese government, Taiwan is an inalienable part of China’s territory” and “the Chinese government consistently upholds the One-China Principle and opposes any form of ‘Taiwan independence’ separatist activities.”<sup><a data-type="noteref" href="ch07.html#id987" id="id987-marker">22</a></sup></p>

	<p>What is the same about children holds true for teacher models and students: <em>The apple doesn’t fall far from the tree</em>. DeepSeek-R1 is likely to pass along these same safety concerns and political principles along to the student models, so think carefully before running to deploy in production.</p>
	</dd>
	<dt>Distillation strategy</dt>
	<dd>
	<p><em>Generate targeted supervised fine tuning (SFT) data for code and math reasoning tasks.</em><em> </em>DeepSeek took a very targeted and intentional approach in its distillation pipeline, focusing on code and math reasoning tasks to the exclusion of all else. This makes sense, if you only ever plan on using the distilled models for code and reasoning tasks. But a study from IBM Research found that these distilled models have sacrificed all ability to perform as a general-purpose model, failing at even basic instruction-following tasks.<sup><a data-type="noteref" href="ch07.html#id988" id="id988-marker">23</a></sup></p>
	</dd>
</dl>

<p>We dive into this further in <a data-type="xref" href="ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518">Chapter 8</a>, but when taking advantage of model distillation, it is critical that your teacher model meets your requirements for both safety and performance and that the distillation approach you choose is aligned to your envisioned use of the model<a contenteditable="false" data-primary="" data-startref="xi_smalllanguagemodelsSLMs76475" data-type="indexterm" id="id989"/><a contenteditable="false" data-primary="" data-startref="xi_smalllanguagemodelsSLMsmodeldistillation7161113" data-type="indexterm" id="id990"/><a contenteditable="false" data-primary="" data-startref="xi_modeldistillation7161113" data-type="indexterm" id="id991"/><a contenteditable="false" data-primary="" data-startref="xi_distillationmodel7161113" data-type="indexterm" id="id992"/>.</p>
</div></section>

</div></section>

<section data-pdf-bookmark="Where Are We Going Next? Small Language Models...Assemble!" data-type="sect1"><div class="sect1" id="ch07_where_are_we_going_next_small_language_models_a_1740182051668366">
<h1>Where Are We Going Next? Small Language Models...Assemble!</h1>

<p>As you can see, SLMs<a contenteditable="false" data-primary="small language models (SLMs)" data-secondary="assembling" data-type="indexterm" id="xi_smalllanguagemodelsSLMsassembling722332"/> clearly have many advantages, but one of the most exciting applications for leveraging them is not as a standalone specialist, but rather, as a system of models working together to do something amazing. It’s kind of like a bunch of tiny ants teaming up and marching off with an entire hamburger patty from your picnic, living the dream and pulling off what seems like the impossible.</p>

<p>At the time of writing, several key advances are coming from the AI research world. These advances demonstrate that by combining their powers, small models working together can sometimes outperform any given large model and do so at a fraction of the compute cost. And while these SLMs could operate independently (with good results), they can become even more impactful when orchestrated to perform in concert (yes, using AI). AI helping AI. This more systems-based approach to models performing tasks can happen externally to the model, using tools like model routing. Or, through architectures like MoE, a system of models with routing between experts that occur intrinsically within the model. Let’s get into both of these topics next.</p>

<section data-pdf-bookmark="Model Routing" data-type="sect2"><div class="sect2" id="ch07_model_routing_1740182051668430">
<h2>Model Routing</h2>

<p>On average, a bigger language model<a contenteditable="false" data-primary="model routing, SLMs" data-type="indexterm" id="xi_modelroutingSLMs722749"/><a contenteditable="false" data-primary="routing of models, SLMs" data-type="indexterm" id="xi_routingofmodelsSLMs722749"/> is going to perform better than a smaller language model on a given task. But, as you learned in this chapter, SLMs can operate as specialized experts that can outperform a big LLM if the task at hand is specialized in nature (like in the COBOL example). But even without intentional domain specialization, there can be unexpected variability in model performance across the many tasks you’re likely to send to your AI. This could be the case for many reasons: a model’s architecture, nuances in training data, parameter settings, data preparation, data sourcing, its alignment strategy...all of this (and more) could predispose any given smaller model to perform better on a task, independent of model size. The problem around the benefits of SLMs is that their performance advantages can be unpredictable, particularly if you don’t know what data they were trained on, making it difficult to predict which SLM you should use for your task.</p>

<p>Of course, you could run every data point through every SLM you have to try and figure out which one(s) will work best. Don’t get us wrong—usually, putting the work in for something great is a good thing—but for this, you want something different. If you could somehow predict up front whether a smaller model would be suitable for your use case’s task list, then you could use that smaller model instead and save your company the extra inference and latency costs that might accompany a big oversized LLM for your needs. Quite simply, you’d optimize the usage of the big LLM to when you actually need it, instead of making the most expensive option the default or only choice.</p>

<p>We do this all the time in our travels. Typically, we’re living the Uber X life—budget travel. But Uber Black (although it leaves us with some explaining to do to our auditors) is the go-to on a tight schedule because it’s there in minutes, they aren’t going to stop for gas on the way, and they won’t accept your ride while they finish another—not to mention the chewing gum is individually wrapped, not stuck to the floor. Now apply that logic to your AI: use the expensive option <em>only</em> when you truly need it.</p>

<p>A group of researchers at the MIT-IBM Watson AI Lab were looking for answers to the question, “Can a bunch of smaller models outperform a large model?” Even back in 2023, when SLMs were just getting started, one paper<sup><a data-type="noteref" href="ch07.html#id993" id="id993-marker">24</a></sup> proposed an approach where a model-routing algorithm sits as an orchestrator, directing inference requests to whichever model the router predicts would be best for a given task.</p>

<p>In this deployment pattern, you could have an ecosystem of models—some are small and specialized, some are larger—to maximize the chances that a model router can find the optimal model to support a given task while defraying your costs every time the router selects a smaller model. <a data-type="xref" href="#fig-7-3">Figure 7-2</a> shows this.</p>

<figure><div class="figure" id="fig-7-3"><img alt="A diagram of a router  AI-generated content may be incorrect." src="assets/aivc_0702.png"/>
<h6><span class="label">Figure 7-2. </span>An AI router that understands the capabilities of models in its library directs a given inference request to the best model able to perform the task at hand</h6>
</div></figure>

<p>In <a data-type="xref" href="#fig-7-3">Figure 7-2</a>, you can see a new inference request for a given input comes into the ecosystem (new data point). A router (trained on benchmark data) understands what model can best perform the task at hand and routes the work to it. You can see the benefits here, right? Every time the router pushes a task to a smaller model (our example has a library of three tier-sized models: small, medium, and large), you save money, reduce latency, and help the environment.</p>

<p>This begs the question, how does this model router know which model in the library will perform best? There are different approaches. The MIT-IBM team took an approach that leveraged predefined (HELM<sup><a data-type="noteref" href="ch07.html#id994" id="id994-marker">25</a></sup>) benchmark data<a contenteditable="false" data-primary="benchmark data" data-type="indexterm" id="xi_benchmarkdata7240449"/> for each model in order to first train the AI router on the different types of tasks each model could perform satisfactorily (note that this approach could also work with any set of relevant benchmarks defined by a user).</p>

<p>As it turns out, training the AI router is a fairly trivial task. At its core, the router is just a classification model. Given a representative task, the router classifies whether the model will perform satisfactorily or not. Once trained, the router then compares the similarity between any new task and the known benchmarks. If a new task is similar to a benchmark task that a specific model has proven to perform well at, then the router is more confident that this specific model will perform well on that new task, too. For example, if a specific model was really good at Q&amp;A’ing medical questions about your broken or sprained ankle, it will probably be pretty good at your broken or sprained wrist you got fishing last week (seriously, take it easy).</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If the benchmarks you’re using are very dissimilar to the tasks being routed to the models, you could also update the router’s logic by giving it a small amount of labeled data that represent the tasks you’re trying to run so that the router can get updated knowledge on model performance for that specific task. The router can then use that information to route future requests (the same ones or similar) to the most appropriate model in your library.</p>
</div>

<p>To demonstrate the performance of the model router, the MIT-IBM team ran an experiment using a library comprised of over a dozen models that ranged from 3 billion to 70 billion parameters in size (so there was a great representation of small, medium, and large models, despite what our example in <a data-type="xref" href="#fig-7-3">Figure 7-2</a> shows). The team evaluated<sup><a data-type="noteref" href="ch07.html#id995" id="id995-marker">26</a></sup> a bunch of different tasks that make up Stanford’s HELM evaluation benchmark. The first pass was <em>without a router</em> to determine which model in the library could perform the tasks in the HELM benchmark the best.</p>

<p>It shouldn’t be too surprising to find out which model won. As we said before: <em>on average</em>, a large model should perform better than individual smaller models for all the tasks. And, as shown in <a data-type="xref" href="#ch07_figure_3_1740182051640296">Figure 7-3</a>, that was indeed the case for this test. The largest model in the library (Llama-2-70B) achieved 68% accuracy (higher is better). And just like that, Llama-2-70B became the baseline for which we could compare how our AI-powered model router would do with a mixed-model approach. It’s important to understand this, so at the risk of repeating ourselves, we’ll say it more explicitly: this benchmark is not <em>measuring </em>the accuracy of the model router; it is measuring the accuracy of the models that the router selects<a contenteditable="false" data-primary="" data-startref="xi_benchmarkdata7240449" data-type="indexterm" id="id996"/>. Quite simply, this means that if you used the Llama-2-70B model for every task in the HELM benchmark, you would get an average performance of 68%.</p>

<figure><div class="figure" id="ch07_figure_3_1740182051640296"><img alt="A graph with numbers and a number of marks  Description automatically generated with medium confidence" src="assets/aivc_0703.png"/>
<h6><span class="label">Figure 7-3. </span>No router used: on average, the large model performed the best, at around 68% performance (higher is better)</h6>
</div></figure>

<p>Now it’s time to unleash the router! <a data-type="xref" href="#ch07_figure_4_1740182051640324">Figure 7-4</a> shows what happens when we allowed the router to send various tasks to different models in the library. Remember, the entire library <em>did not</em> have a single model over 70 billion parameters. Basically, the router (with its ability to route a task to a library of small, medium, and large models) <em>outperformed </em>the large model on its own! Specifically, the overall performance was about 72% when the router could access the library of models, compared to 68% when using one big LLM alone. But there is more to the story in <a data-type="xref" href="#ch07_figure_3_1740182051640296">Figure 7-3</a>; to tell it, you need to focus on the vertical bar graph within the results.</p>

<p>When the router was in play, only 56% of tasks were routed to the big Llama-2-70B model. The rest of the tasks got routed to the smaller, more efficient, and obviously higher-performing models for the tasks routed to them (a mixture of medium and small models).</p>

<p>The takeaway? Using a model router, we observed improved <em>overall</em> accuracy and efficiency. Remember, every time a task gets routed to a smaller model, it’s more efficient to run it. Lower costs. Better performance. Lower environmental impact. What’s not to love? But like any good leader who challenges their teams for their best, one question remained: can you do better?</p>

<figure><div class="figure" id="ch07_figure_4_1740182051640324"><img alt="A graph of a task  Description automatically generated with medium confidence" src="assets/aivc_0704.png"/>
<h6><span class="label">Figure 7-4. </span>Using a router to route to our SLM and LLM library for the tasks at hand resulted in better performance</h6>
</div></figure>

<p>To answer that question, the research team started with a hypothesis: what if the model library was limited to <em>only models that were equal to or less than 13 billion parameters in size</em>? These are true SLMs—that sweet spot of SLMs that we talked about earlier.</p>

<p><a data-type="xref" href="#ch07_figure_5_1740182051640346">Figure 7-5</a> shows the answer to this question, and it’s worth some extra commentary.</p>

<figure><div class="figure" id="ch07_figure_5_1740182051640346"><img alt="A graph of a number of tasks  Description automatically generated with medium confidence" src="assets/aivc_0705.png"/>
<h6><span class="label">Figure 7-5. </span>Limiting the model library to 13 billion parameters delivers impressive <span class="keep-together">benefits</span></h6>
</div></figure>

<p class="pagebreak-before">The obvious takeaway<a contenteditable="false" data-primary="small language models (SLMs)" data-secondary="model routing" data-type="indexterm" id="id997"/> from <a data-type="xref" href="#ch07_figure_5_1740182051640346">Figure 7-5</a> is that the results of the router with an SLM-only library (70%) aren’t as good as the larger library comprised of all 15 large, medium, and small models, including the 70 billion LLM (72%). But some things caught our eye right off the bat and should have you throttling up your attention span (we know, we’re deep into the chapter) from “somewhat curious” to “we have your full attention.”</p>

<p>First, while the library of all models (up to and including the 70 billion one) performed better, the SLM-only library (models 13 billion parameters and under) outperformed the baseline (the big 70 billion LLM on its own): 70% versus 68%. Second, the SLMs don’t need the biggest most expensive and scarce GPUs to run them. That means you get more deployment options. And of course, giving up only 2% performance over the best result, and gaining 2% performance over the baseline, gives you even lower overall costs (both money and environmental)!</p>
</div></section>

<section data-pdf-bookmark="Think About This When It Comes to Model Routing" data-type="sect2"><div class="sect2" id="ch07_think_about_this_when_it_comes_to_model_routing_1740182051668490">
<h2>Think About This When It Comes to Model Routing</h2>

<p>The appeal of model routing isn’t just maximizing performance at lower overall cost. There’s a second important benefit: having the ability to, before the inference (a priori), predict model performance on a task across different models of different sizes. Why is this important? As a leader who understands this technique, you can make more informed decisions about the cost-benefit trade-off between different models and the suitability of any given task for automation.</p>

<p>For example, if an automation task you are preparing as a GenAI use case is very complicated—and the only models predicted to perform well are the very large, expensive ones—then you might decide that automating that task doesn’t result in large enough cost savings to justify using a model of that size. On the other hand, perhaps you have a low-value task that wasn’t giving a strong signal on your GenAI use case radar, but it’s predicted to be easily automated with a fairly small model. Suddenly, you’re economically incentivized to shift it left and automate that task. When you think about that whole flip of +AI to AI+ mindset we discussed in <a data-type="xref" href="ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974">Chapter 1</a>, where you suddenly see your business as discrete pieces of workflows and business logic, we think model routing can really help here. How so? Those discrete pieces of logic likely aren’t going to need a super large model, so they can be leveraged for the mundane rote shift tasks that are bound to be discovered during this process. We envision a near-future world of LLMOps, driven by model routers, where performance and cost savings are dynamically monitored, and a router actively sends workloads to different models to maintain a desired cost per performance balance defined by an operator<a contenteditable="false" data-primary="" data-startref="xi_modelroutingSLMs722749" data-type="indexterm" id="id998"/><a contenteditable="false" data-primary="" data-startref="xi_routingofmodelsSLMs722749" data-type="indexterm" id="id999"/>.</p>
</div></section>


<section data-pdf-bookmark="Mixture of Experts (MoE) Architecture" data-type="sect2"><div class="sect2" id="ch07_mixture_of_experts_moe_architecture_1740182051668545">
<h2>Mixture of Experts (MoE) Architecture</h2>

<p>Now that we have talked about how groups of models<a contenteditable="false" data-primary="Mixture of Experts (MoE) architecture" data-type="indexterm" id="xi_MixtureofExpertsMoEarchitecture727464"/><a contenteditable="false" data-primary="MoE (Mixture of Experts) architecture" data-type="indexterm" id="xi_MoEMixtureofExpertsarchitecture727464"/> of various strengths and expertise can work together through an external model router, let’s take this idea one step further and talk about how this same concept can be applied internally within a model, using a relatively new type of LLM architecture: Mixture of Experts (MoE).</p>

<p>Think of LLM architectures as the technical strategy that a researcher uses to encode all of the training data into parameters for their model. Almost all modern LLMs trained today are trained<a contenteditable="false" data-primary="transformers" data-type="indexterm" id="id1000"/> using a “transformer” type of architecture (which we talk about in <a data-type="xref" href="ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518">Chapter 8</a>). Since its initial release, many types of transformer architectures have emerged. The most popular is the “dense” style of transformer models, used by many model providers like Meta with its Llama model families. However, more recently, new, more efficient types of transformer architectures, like MoE, have started to gain popularity, and that’s what we cover in this section.</p>

<p>In an MoE-based model, buckets of parameters<a contenteditable="false" data-primary="parameters" data-type="indexterm" id="id1001"/>, referred to as “experts,” are trained to operate fairly independently of one another. These experts can either be specialized by the model developer, or can be generalists in nature. Leveraging the same intuition we covered in the previous section, only a subset of the experts are used at inference time, making these models <em>wicked</em> fast (can you tell one of the authors is a Bostonian?). This is because the inference cost is now approximately reduced to the size of the experts being run, not the entire size of the model. How does the model know which regions of the model to “activate” for a given request? You guessed it. A model router, but <em>this</em> time the model router is internal to the model, not something that can be used independently with other models like in the previous section.</p>

<p>There are some important gotchas with MoE inference efficiency. If you are running inference in large batch jobs, as is common for production workloads, this efficiency advantage goes down because you will need to load more and more of the experts into memory depending on all the samples that are batched. But if you are experimenting locally, or running things in a single batch, or batching across very homogenous data that will always use the same experts, these MoE models can be quite inference-efficient.</p>

<p>In January of 2025, the MoE architecture got broad attention when DeepSeek released its 671 billion MoE model. But DeepSeek wasn’t the first to release an MoE model. The French AI Lab, Mistral AI<a contenteditable="false" data-primary="Mistral AI" data-type="indexterm" id="id1002"/>, made headlines with the release of one of the first high-performing MoE models: Mixtral 8x7B<a contenteditable="false" data-primary="Mixtral 8x7B" data-type="indexterm" id="id1003"/> (we think the name is great, Mistral + mixture) all the way back in December of 2023.</p>

<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="ch07_a_quick_moe_nomenclature_primer_1740182051668603">
<h1>A Quick Primer on MoE Nomenclature</h1>

<p>As the name implies, Mixture of Experts refers to subsetted groups of parameters that have been trained to behave as independent experts working together. (It’s a common misconception that MoE models have multiple expert models within them. That isn’t true [for now]. These experts are parameter regions of the <em>same</em> model.) When you see [A]x[B] in an MoE model name, this is a nomenclature often used to tell you how many experts, and of what size, are available in the model. This means that Mixtral 8x7B has 8 different experts, all of which are 7 billion parameters available at their disposal. At inference time, Mixtral will select the two best experts (according to its internal model router) for the task at hand and use those to run inference. Similarly, Mixtral 8x22B has 8 different experts, all of which are 22 billion parameters in size. Now these names might imply that the total size of the 8x7B model is 8 × 7 = 56 billion parameters. That’s actually not quite the case, because some sharing of parameters happens between the experts; for example, the true model size of Mixtral-8x7B is approximately 47 billion parameters.</p>

<p>A second model nomenclature has started to emerge that focuses less on the number of experts available in the model and more on the total number of parameters that will be run (or activate) at inference time. Granite-3.0-1B-A800M<a contenteditable="false" data-primary="Granite" data-type="indexterm" id="id1004"/><a contenteditable="false" data-primary="IBM" data-secondary="Granite" data-type="indexterm" id="id1005"/> and Qwen1.5-MoE-A2.7B<a contenteditable="false" data-primary="Qwen1.5-MoE-A2.7B" data-type="indexterm" id="id1006"/><a contenteditable="false" data-primary="IBM" data-secondary="Qwen1.5-MoE-A2.7B" data-type="indexterm" id="id1007"/> both follow this style of nomenclature. In these names, the “A” refers to activated parameters. This means that the Granite-3.0-1B-A800M MoE has 800 million parameters worth of collective experts that are activated at inference time. We think this notation is a bit more useful because the number of activated parameters will help predict your latency when you run the model. If Mixtral 8x7B had used this notation, it might have looked something like, Mixtral-47B-A14B because Mixtral 8x7B activates 2 of the 7 billion parameter experts at inference time.</p>

<p>DeepSeek-R1 is also an MoE-style model, but DeepSeek chose to go with a simpler name. If you read their paper, DeepSeek-R1 (and the other related models in the family) all have 671 billion total parameters, with 37 billion parameters activated at inference time.<sup><a data-type="noteref" href="ch07.html#id1008" id="id1008-marker">27</a></sup></p>
</div></aside>

<p>MoE models are more efficient to run at inference time, but they are also more economical to train. DeepSeek brought this point home when it published that it was able to train its base model, DeepSeek-V3-Base (which was later post-trained to create DeepSeek-R1), for $5.6M.<a contenteditable="false" data-primary="DeepSeek-V3" data-type="indexterm" id="id1009"/> But there are a couple of important things to note when interpreting this staggeringly low reported training cost.</p>

<p>First, just as any lawyer will tell you, make sure you read the fine print! When DeepSeek reported its training cost in the DeepSeek-V3 Technical Report, it included a very important caveat: “Note that the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data.”<sup><a data-type="noteref" href="ch07.html#id1010" id="id1010-marker">28</a></sup></p>

<p>What does this translate to in plain speak? Well, to train LLMs, there is a lot of brute-force trial and error that is required in order to optimize performance. That means for any one model that is released, there might be hundreds or thousands of smaller models that are trained in advance, testing out different data mixture efficacies, searching through different hyperparameter settings, etc. These development costs can easily be 10 times or more compared to the final, one-and-done training cost of the model. So, while what DeepSeek did is still impressive, the true training costs of its models were probably far less earth-shattering than some of the press coverage may have let on.</p>
</div></section>

<section data-pdf-bookmark="Think About This When It Comes to MoEs" data-type="sect2"><div class="sect2" id="ch07_think_about_this_when_it_comes_to_moes_1740182051668657">
<h2>Think About This When It Comes to MoEs</h2>

<p>Research and innovation with MoE-style models is still evolving. As DeepSeek showed, the world is getting better and better at training MoE models more efficiently and innovating on how to bring experts together. At the end of the day, we are most bullish on this architecture because its more efficient training costs will allow for more rapid iteration, hopefully continuing to drive innovation in this space.</p>

<p>We see a significant innovation runway for MoEs with respect to configurable inference efficiency<a contenteditable="false" data-primary="inference-efficient models" data-type="indexterm" id="id1011"/>. Today, Mixtral is designed to call two experts at inference time. To enable cost-efficient inferencing in the future, we envision this technology evolving to dynamically change the number of experts called at inference time, allowing users to quickly adjust their cost/performance trade-off for a given task and use case. This is like the model routing use case in the previous section, where more complicated tasks could call for the justified use of a bigger more expensive model. In our crystal ball, we see MoE models operating in the same manner where complicated tasks could call for using more experts at inference time (perhaps all eight and not just the two in our running example).</p>

<p>No matter where this technology evolves, it’s all about the flexibility for model consumers that makes it so exciting. When you reduce your dependency on one large model and harness the power of smaller models (or regions of a model) working together, you have opportunities to tailor model expertise for your use cases all the way to optimizing the cost-performance trade-off to best meet the needs of your business. And now you know why one model couldn’t possibly rule them all<a contenteditable="false" data-primary="" data-startref="xi_smalllanguagemodelsSLMsassembling722332" data-type="indexterm" id="id1012"/><a contenteditable="false" data-primary="" data-startref="xi_MixtureofExpertsMoEarchitecture727464" data-type="indexterm" id="id1013"/><a contenteditable="false" data-primary="" data-startref="xi_MoEMixtureofExpertsarchitecture727464" data-type="indexterm" id="id1014"/>.</p>
</div></section>

</div></section>

<section data-pdf-bookmark="Agentic Systems" data-type="sect1"><div class="sect1" id="ch07_agentic_systems_1740182051668722">
<h1>Agentic Systems</h1>

<p>We’ve given you some high-level details about agents<a contenteditable="false" data-primary="agentic systems" data-type="indexterm" id="xi_agenticsystems729764"/> throughout this book. In the final section of this chapter, it’s time to delve into them a little deeper. When we talk about agents, we often are referring to an implementation of an LLM where a user provides a goal-oriented instruction, and then the LLM independently comes up with a series of tasks (and subtasks) to achieve that goal. It then iterates over those tasks, often leveraging tools and reflection loops to complete each task. An agent can even be comprised of multiple different LLMs, each performing one of those tasks. Because a complex task is broken down into smaller, simpler-to-accomplish steps, the door is often opened for smaller models to tackle simpler tasks in tandem with larger models performing the more difficult tasks (like coming up with the list of tasks that need to be done to achieve the goal in the first place). And often, there is some sort of model routing happening behind the scenes where an LLM is selecting another LLM to outsource a subtask to, based on a catalogue of LLMs to choose from.</p>

<p>While many things agents do today can be done manually and in a static manner, agents deliver productivity<a contenteditable="false" data-primary="productivity" data-secondary="agentic role in" data-type="indexterm" id="id1015"/><a contenteditable="false" data-primary="shifting left" data-secondary="agentic role in" data-type="indexterm" id="id1016"/> breakthroughs by further shifting left more of the work, which saves time and boosts efficiency. For example, if you headed up a clinical trial, you could use an LLM to identify suitable trial candidates, but then you’d have to manually manage visit scheduling and coordination (tasks like sending reminders, rescheduling meetings, and automatically reminding everyone in the trial about key dates or requirements, such as a morning fast). With agents, you shift more of the work left because not only can an agent come up with a great start toward the perfect clinical trial profile, but they can even help come up with a proposed set of compliance reminders and even schedule sample collections with calendar invites for participants! What’s more, agentic systems are not stuck in time, and they can adapt in real time.</p>

<p>Imagine attaching an agent to a supply chain management problem—you now have AI with the ability to understand a weather event and optimize a plan (understanding road closures and such) to get much-needed product into stores. And as you will find out, agents can even learn along the way. Quite simply, the dynamic nature of agents helps a company get more work shifted from +AI to AI+ and keeps them agile. This space keeps changing, so you’re going to want to follow it closely.</p>

<p>Now think back to what you learned in <a data-type="xref" href="ch04.html#ch04_the_use_case_chapter_1740182047877425">Chapter 4</a> about LLMs with a RAG pattern. That was one way of not just making your enterprise data available to an LLM, but also how to provide the LLM with updated information. In this pattern, a larger system injects information from an external source (like a database) directly into the prompt before runtime. This was also the basis of the “talk to a document” use case in <a data-type="xref" href="ch04.html#ch04_the_use_case_chapter_1740182047877425">Chapter 4</a>. With the introduction of agents, AI gets even more powerful and can handle more complex tasks because they have the ability to call tools (this process is referred to as tool calling) outside of the LLM to assist them with their work.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p><em>Tool calling</em> is the term<a contenteditable="false" data-primary="tool calling" data-type="indexterm" id="id1017"/> referred to when LLMs are given the ability to interact with external tools, apps, and other systems—all to enhance their functionality. For example, an agent’s LLM might perform a tool call to get the weather for a particular location to help finish a task or reach out to a calculator to perform certain types of calculations for precision or even to offload the work from the LLM. Simply put, tool calling extends LLMs with capabilities beyond generating text, images, and the other things they are known for that we’ve covered in this book.</p>
</div>

<p>Perhaps the best way to appreciate the power of agents is to reflect on how you typically work with an AI-powered chatbot<a contenteditable="false" data-primary="prompts" data-secondary="versus agents reasoning" data-secondary-sortas="agents reasoning" data-type="indexterm" id="id1018"/> today. The flow looks something like this:</p>

<ul class="simplelist">
	<li>
	<p>human prompt → LLM response → human prompt → LLM response → ...</p>
	</li>
</ul>

<p>In this traditional system, your prompt might go back and forth in the simple manner shown above, but it can trigger multiple calls that operate in the backend, unseen by you, before a response is provided back. For example, a RAG pattern appends data to a prompt from a data source that was connected to this flow by an administrator. But even when enhanced in this manner, the information that is available to the LLM supporting a RAG-based chatbot is also predetermined by its creator (like through a connection to a vector database like Chroma). In this nonagentic architectural pattern, the LLM involved <em>is not </em>given the ability to work “behind the scenes” on its <span class="keep-together">own—it</span> interacts with you on a continual basis as you go back and forth and back and forth, trying to complete your task.</p>

<p>In contrast, agentic implementations provide LLMs with more freedom and power. In this architectural pattern, LLMs are allowed to reason about what information is needed to perform a task that helps achieve a goal, like, “Put together a plan to increase the net promoter score (NPS) for my car dealership’s service center.” The LLMs part of this pattern are provided with access to tools (more on this in a bit) that can be called on the backend to obtain up-to-date information, optimize workflows, create subtasks to tackle the challenge piece by piece, and even call some scripting language (like VBScript) to create some PowerPoint charts of what it finds! This is all done autonomously by the agent (or agents) to achieve the complex goal. An agentic workflow might look like:</p>

<ul class="simplelist">
	<li>
	<p>human prompt → primary LLM response (hidden to user) → primary LLM tool call (hidden to user) → LLM response (hidden to user, shown to secondary LLM) → secondary LLM response (hidden to user, provided back to primary LLM) → primary LLM response (shown to user) → human prompt → ...</p>
	</li>
</ul>

<p>As an end user chatting with an agentic system, you might feel as if you are just querying one big, multifunctional super LLM behind the scenes. But the reality is you’re likely working with a system of bigger and smaller models working together behind the scenes in order to efficiently solve your objective. (Like we said, you can use multiple LLMs in an agentic workflow. This should really give you a feel for just how significant of a role SLMs can play in this domain.)</p>

<p>AI agents can encompass a wide range of functionality beyond language, including decision making, problem solving, interacting with external environments, and executing actions. And these agents can be deployed in various applications to solve complex tasks in enterprise contexts, from software design and IT automation to code-generation tools and conversational assistants. We like to think of agents as digital interns with lots of ambition. Arm them with goals, tools, and tasks, and their smarts <em>may</em> often surprise you—but like we said earlier, AI isn’t magic.</p>

<section data-pdf-bookmark="What’s Your Reaction to This Agent in Action?" data-type="sect2"><div class="sect2" id="id267">
<h2>What’s Your Reaction to This Agent in Action?</h2>

<p>AI agents are systems-based implementations of LLMs that leverage planning, reasoning, and tool calling to solve problems and interact with external environments. Behind the scenes, there might be a single LLM handling all the work, multiple instances of the same LLM working on a task, or a combination of different LLMs. A good agentic framework will let you mix and match different LLM providers, which includes fine-tuned models that you might have customized with your own data. For example, you might pull Anthropic’s Claude Sonnet for desktop controls but augment that with a Granite-based model enhanced with your business data—the two of them might work in concert to figure out an event and fill in a form. Very cool!</p>

<p><a data-type="xref" href="#ch07_figure_6_1740182051640368">Figure 7-6</a> gives you some insights into an agent that we tasked with writing a blog about the impacts of inflation on Canadian housing prices in 2024 and then come up with some social media postings to reference our blog.</p>

<p>We set up several agents that are invoked from our task. One of the agents took on the persona of a Lead Market Analyst. We won’t detail this for each agent, but this particular agent’s <em>goal</em> was to conduct real-time analysis of financial news on our topic of interest to help guide content creation. We also gave this agent a <em>backstory</em>, which made it take on the persona of a market analyst from a reputable firm who dissects market trends to pass on to our agentic writers. We gave this information to the agent framework in YAML files.</p>

<p>Notice in <a data-type="xref" href="#ch07_figure_6_1740182051640368">Figure 7-6</a> that our Lead Market Analyst agent literally tells us how it will get started by searching the internet for articles related to the topic involved in its task.</p>

<figure><div class="figure" id="ch07_figure_6_1740182051640368"><img alt="A screenshot of a computer  Description automatically generated" src="assets/aivc_0706.png"/>
<h6><span class="label">Figure 7-6. </span>Our agentic workflow thinking about some of the steps it needs to do to write our blog and point to that blog on social media</h6>
</div></figure>

<p>As shown in <a data-type="xref" href="#ch07_figure_7_1740182051640388">Figure 7-7</a>, if the task is complicated, the agent might make multiple internal loops of tool calls and internal reasoning before returning a final answer. In this case, the agent has finished finding its sources and now starts to look at the data it’s collected. Notice how it has access to tools to help it.</p>

<figure><div class="figure" id="ch07_figure_7_1740182051640388"><img alt="A screenshot of a computer  Description automatically generated" src="assets/aivc_0707.png"/>
<h6><span class="label">Figure 7-7. </span>The agent starts to look at the contents of the information it found</h6>
</div></figure>

<p>Finally, this particular agent finishes its work and returns the findings shown in <a data-type="xref" href="#ch07_figure_8_1740182051640408">Figure 7-8</a>. It seems evident that our agentic workflow has the source information and summary points that will make for a great blog posting!</p>

<figure><div class="figure" id="ch07_figure_8_1740182051640408"><img alt="A close-up of a text  Description automatically generated" src="assets/aivc_0708.png"/>
<h6><span class="label">Figure 7-8. </span>The key points to make in our blog posting</h6>
</div></figure>

<p>Ultimately, there is a lot more flexibility added to this flow, giving the models powering your agents the ability to plan out tasks, research external information, and more.</p>

<p>As we alluded to earlier, we built multiple agents on the backend, each specialists in different tasks, and we put them all to work on this objective. One agent has the persona of a content creator; another is a creative director, another is a social media guru, and finally, one is a math guru. We suggest that as you go create your own digital employees in your agentic workflows, look to the very job postings you might make for such jobs. In there will reside all kinds of backstory skills you want these digital employees to be able to do. When all was said and done, our agents wrote us the (presumably; we of course looked at the data it collected) well-researched blog that is shown in <a data-type="xref" href="#ch07_figure_9_1740182051640427">Figure 7-9</a>.</p>

<figure><div class="figure" id="ch07_figure_9_1740182051640427"><img alt="A screenshot of a white text  Description automatically generated" src="assets/aivc_0709.png"/>
<h6><span class="label">Figure 7-9. </span>The start of our final blog</h6>
</div></figure>

<p class="pagebreak-before">Finally, look at the social media outreach messages our agentic workflow came up with (see <a data-type="xref" href="#ch07_figure_10_1740182051640455">Figure 7-10</a>) to amplify our article.</p>

<figure><div class="figure" id="ch07_figure_10_1740182051640455"><img alt="A screenshot of a white paper with black text  Description automatically generated" src="assets/aivc_0710.png"/>
<h6><span class="label">Figure 7-10. </span>The agentic workflow didn’t just write our blog; it also composed social media outreach messages to direct traffic to our blog posting</h6>
</div></figure>

<p>We’ll admit we got a touch lazy looking back at the output in <a data-type="xref" href="#ch07_figure_10_1740182051640455">Figure 7-10</a>. How so? We gave the same skills to our social media writer agent for posting on all platforms. Looking back, we should have given this agent broader skills and knowledge so it knew how to better mix tone and style depending on the social media outlet. After all, X (Twitter) is limited to 240 characters, so our agent worked hard to keep all of the postings it generated short (which could have been part of our assigned goal, but wasn’t). As another example, Instagram posts could be a lot less formal than LinkedIn. Notice how in <a data-type="xref" href="#ch07_figure_10_1740182051640455">Figure 7-10</a> the agent used emojis for the X post, which are more commonplace because of its limits than on LinkedIn.</p>

<p>There was a lot of other cool stuff going on behind the scenes than we could show you here. For example, our agents had their own version of the revered <em>Who Wants to Be a Millionaire?</em> game show’s Phone-a-Friend lifeline—only these friends were website crawlers, searchers, and scrapers, pieces of Python code (we used its Pydantic library to parse the data, among other libraries), and other digital labor agents—the best part is they never put you on hold or say, “Sorry bros, you stumped me!”</p>

<p>Do we think Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch07_figure_9_1740182051640427">7-9</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch07_figure_10_1740182051640455">7-10</a> were better than a human? That wasn’t the point...because we think that handing a human this information would give them a productivity boost if their job was to perform these very tasks. We shifted-left the work! Now bring the human element to make it really land.</p>
</div></section>

<section data-pdf-bookmark="A Little More on Agents" data-type="sect2"><div class="sect2" id="ch07_a_little_more_on_agents_1740182051668880">
<h2>A Little More on Agents</h2>

<p>In an agentic system<a contenteditable="false" data-primary="agentic systems" data-secondary="grounding context" data-type="indexterm" id="xi_agenticsystemsgroundingcontext735834"/>, an agent often has access to more advanced forms of grounding context, like memory buffers that store information from past work and tasks it was asked to perform. An agent’s ability to store past interactions in memory and plan future actions encourages a personalized experience and comprehensive responses. But it gets better—these agents learn over time. For example, if there is a certain style you want a report written in, or a sauciness level for an Instagram post versus one on LinkedIn, agent memory can persist these preferences, and that’s a great example of a more personalized experience and comprehensive response. In our example above, had we further instructed our agent not to make the blog posting too chunky with too many short sections, it would learn that preference. Contrast this with a traditional RAG chatbot type setting where a model starts fresh each time.</p>

<p>Although AI agents are autonomous in their decision-making processes<a contenteditable="false" data-primary="agentic systems" data-secondary="autonomy of agents in" data-type="indexterm" id="id1019"/>, as we alluded to earlier, they require goals and environments defined by humans.<sup><a data-type="noteref" href="ch07.html#id1020" id="id1020-marker">29</a></sup> There are four main influences on autonomous agent behavior:</p>

<ul>
	<li>
	<p>The team that designs and trains (or more likely, uses or fine-tunes) the underlying LLM(s) used in the agentic workflow. As you’ve learned about in this book, it’s more likely that you use an LLM to support your agents someone else built, and depending on the task it needs to perform, you may have steered it to your business.</p>
	</li>
	<li>
	<p>The team of engineers that build the agentic AI system. These are the folks who are defining the tools to which the system will have access.</p>
	</li>
	<li>
	<p>The team of developers that configure the agent and provide the user with access to it and the tools. These folks work in conjunction with the business to help create the agentic persona.</p>
	</li>
	<li>
	<p>The user who prompts the AI agent with specific goals and tasks.</p>
	</li>
</ul>

<p>As you saw in the example earlier, given a user’s goals and the agent’s available tools, the agentic workflow created a plan that included tasks and subtasks to accomplish the complex goal it was handed. If this were a simple task (like writing a form letter), planning wouldn’t be a necessary step. Instead, the agent could iteratively reflect on its responses and improve them without planning its next steps. That was not the case with our blog posting. Recall in Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch07_figure_6_1740182051640368">7-6</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch07_figure_8_1740182051640408">7-8</a> that our agent’s logic showed us some insights into its reasoning and planning for how to solve the task we gave it (there was a lot more thinking, reasoning, and planning we didn’t show you).</p>

<p>AI agents base their actions on the information they perceive. Often, AI agents do not have the full knowledge base needed for tackling all subtasks within a complex goal. For example, our agents didn’t have knowledge on the impact of inflation on housing. To remedy this, our agents used their available tools (in our example, an agent went out and searched the web for information). These tools can include external datasets, web searches, APIs, and even other agents. After the needed information was retrieved using these tools, our agent updated its knowledge base. This means that each step of the way, an agent can reassess its plan of action and self-correct.</p>

<p>While our previous example showcased writing, imagine something even more complex, such as planning your next vacation. You task an AI agent with predicting which week in the next year would likely have the best weather for a surfing trip in Hawaii. Since the LLM model at the core of the agent does not specialize in weather patterns, that agent would gather information from an external database (versus a web search) comprised of daily weather reports for Hawaii over the past several years. Despite acquiring this new information, the agent still can’t determine the optimal weather conditions for surfing, so the next subtask is created. For this subtask, the agent communicates with an external agent that specializes in surfing. Let’s say that in doing so, the agent learns that high tides and sunny weather with little to no rain provide the best surfing conditions—not just sunny skies. The agent then combines the information it has learned from its tools to identify those best patterns to put some “maika’i loa” (awesome in Hawaiian) into your surfing vacation. It comes back with a prediction on what weeks in the year are likely to have high tides, sunny weather, and a low chance of rain. These findings are then presented to you, or perhaps the agent even goes on to book your trip<a contenteditable="false" data-primary="" data-startref="xi_agenticsystemsgroundingcontext735834" data-type="indexterm" id="id1021"/>.</p>
</div></section>

<section data-pdf-bookmark="How Agents Are Built" data-type="sect2"><div class="sect2" id="ch07_how_agents_are_built_1740182051668937">
<h2>How Agents Are Built</h2>

<p>At their heart, agents are system-based implementations<a contenteditable="false" data-primary="agentic systems" data-secondary="building agents" data-type="indexterm" id="xi_agenticsystemsbuildingagents738071"/> of an LLM<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="agents as implementations of" data-type="indexterm" id="id1022"/>. In this implementation, you will have an LLM with a set of operating instructions on how to plan and how to make external tool calls (be that a web search or a prompt to another LLM, etc.), embedded within a broader system that performs key, non-GenAI activities, such as:</p>

<ul>
	<li>
	<p>Parsing an LLM’s output, searching for tool call invocations that the LLM will trigger</p>
	</li>
	<li>
	<p>Processing an external API based on the identified tool call</p>
	</li>
	<li>
	<p>Processing a tool response and injecting it directly back into the LLM’s conversation history with the proper formatting (like converting JSON to written text or Markdown)</p>
	</li>
	<li>
	<p>Handling advanced memory functions, such as conversation history manipulation and storage of key artifacts in LLM-accessible memory</p>
	</li>
</ul>

<p>As you can see, this is a complicated system that the LLM operates in, often resulting in complex, multipage prompts summarizing the operating instructions for an agent (or group of them).</p>

<p>While there is not one standard prompt for instructing AI agents, several paradigms<a contenteditable="false" data-primary="agent architectures" data-type="indexterm" id="id1023"/>, also known as <em>agent architectures</em>, have emerged for solving multistep problems and determining how to trigger planning, tool usage, and memory within an LLM <span class="keep-together">workflow</span>.</p>

<section data-pdf-bookmark="ReAct (Reasoning and Action)" data-type="sect3"><div class="sect3" id="ch07_react_reasoning_and_action_1740182051669004">
<h3>ReAct (Reasoning and Action)</h3>

<p>This is the agent architecture we used in our blog example. It lets users instruct their agents<a contenteditable="false" data-primary="ReAct (Reasoning and Action) method" data-type="indexterm" id="id1024"/> to “think” and plan after each action taken...and with each tool response to decide which tool to use next. These think-act-observe loops are used to solve problems step-by-step and iteratively improve upon responses.</p>

<p>Through the prompt structure, agents can be instructed to reason slowly and display each “thought”<sup><a data-type="noteref" href="ch07.html#id1025" id="id1025-marker">30</a></sup> (you saw this in our blog example). An agent’s verbal reasoning gives insight into how responses are formulated. In this framework, agents continuously update their context with new reasoning. This can be interpreted as a form of chain of thought (CoT) prompting<a contenteditable="false" data-primary="chain-of-thought (CoT) prompting" data-type="indexterm" id="id1026"/>.</p>
</div></section>

<section data-pdf-bookmark="ReWOO (Reasoning WithOut Observation)" data-type="sect3"><div class="sect3" id="ch07_rewoo_reasoning_without_observation_1740182051669063">
<h3>ReWOO (Reasoning WithOut Observation)</h3>

<p>The ReWOO method<a contenteditable="false" data-primary="ReWOO (Reasoning WithOut Observation ) method" data-type="indexterm" id="id1027"/>, unlike ReAct, does all the planning up front. This can be desirable from a human-centered perspective since the user can confirm the plan before it is executed. This is important because at some point someone has to pay to spin up the resources to run all of this—it’s not a bad approach to know what’s going to happen (and how) before you pay for it.</p>

<p>The ReWOO workflow is made up of three modules. In the planning module, an agent anticipates its next steps given a user’s prompt. The next stage entails collecting the outputs produced by calling these tools. Finally, an agent pairs the initial plan with the tool outputs to formulate a response<a contenteditable="false" data-primary="" data-startref="xi_agenticsystemsbuildingagents738071" data-type="indexterm" id="id1028"/>. This planning ahead can greatly reduce token usage and computational complexity as well as the repercussions of intermediate tool failure.<sup><a data-type="noteref" href="ch07.html#id1029" id="id1029-marker">31</a></sup></p>
</div></section>
</div></section>

<section data-pdf-bookmark="Risks and Limitations of Agentic Systems" data-type="sect2"><div class="sect2" id="ch07_risks_and_limitations_of_agentic_systems_1740182051669119">
<h2>Risks and Limitations of Agentic Systems</h2>

<p>Agentic systems<a contenteditable="false" data-primary="agentic systems" data-secondary="risks and limitations" data-type="indexterm" id="xi_agenticsystemsrisksandlimitations741031"/> have all of the same risks and limitations as GenAI, particularly concerns of bias, hallucinations, jailbreaking, etc. In addition to these common issues, there are specific limitations and risks with agentic systems that we want you to understand when considering an agentic deployment—and that’s why we wrote this section:</p>

<dl>
	<dt>Computational complexity and infinite feedback loops</dt>
	<dd>
	<p>Because AI agents often leverage multiple inference calls<a contenteditable="false" data-primary="infinite feedback loops, agentic systems" data-type="indexterm" id="id1030"/> to respond to a single prompt, they can become very computationally expensive, particularly for simple NLP tasks. It may be more efficient and cost-effective to run a standard LLM workflow, without bringing in a broader agentic system.</p>

	<p>In addition, agents that are unable to create a comprehensive plan, or reflect on their findings, may find themselves repeatedly calling the same tools, invoking infinite feedback loops. If agents are left unattended and get into an infinite feedback loop that runs inference on a large LLM, you could be looking at a very expensive bill! We’ve literally seen this happen. When we first started experimenting with this technology, we asked an agent to find the world’s best tzatziki recipe. We had hoped it would go out and find some winner lists and use some logic to compare them (like number of hits on the website or how popular the domain was). In the end, our agent got lost in a sea of contradictory food blogs and recommendations of lots of garlic (because every AI knows where the tzatziki magic happens) and no real “Opa!” in the lackluster grand finale.</p>
	</dd>
	<dt>Control and observability</dt>
	<dd>
	<p>The flexibility that allows agents to robustly handle new tasks and solve problems is only possible because of the slackening control<a contenteditable="false" data-primary="observability for agentic systems" data-type="indexterm" id="id1031"/> imposed on the system. It becomes critical, therefore, to monitor and understand an agent’s decision-making process and actions in agentic workflows. Depending on how an agent is implemented, the full internal workings and decision-making flows are not always transparent, potentially leading to unintended consequences. For instance, a model may adapt in unforeseen ways, leading to behaviors that are not aligned with your original objectives or your values.</p>

	<p>This lack of control and observability can result in some of the undesirable outcomes you learned about in <a data-type="xref" href="ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635">Chapter 5</a>; for example, biased or discriminatory actions, which can have severe consequences in high-stakes applications like healthcare, finance, or education. As you go down this path, we want to remind you how essential it is to develop requirements for transparent and explainable LLMs, allowing for real-time monitoring and corrective actions to mitigate these risks.</p>
	</dd>
	<dt>Security and complex permissions</dt>
	<dd>
	<p>There are a multitude of potential security and safety<a contenteditable="false" data-primary="security and privacy" data-secondary="challenges for agents" data-type="indexterm" id="id1032"/> challenges that need to be solved before any custom-built (and perhaps the off-the-shelf ones you buy) agents can be safely deployed in complex enterprise environments. For example, if an HR agent designed for acting upon an employee’s request has access to an HR database that includes sensitive details for all employees, data security measures should be put in place to make sure that agent doesn’t accidentally divulge (or have access to, for that matter) sensitive information about other employees to the end user. Quite simply, this requires fine-grained access controls (FGACs) and role-based access controls (RBACs), adherence to personally identifiable information (PII) transfer protocols, principle of least privileges assignments, an identity fabric, and more. Similarly, in multiagent systems, communication protocols need to be established for how agents with access to different sensitive information types can work together without leaking sensitive content and adhering to data transit regulations that require encryption<a contenteditable="false" data-primary="" data-startref="xi_agenticsystemsrisksandlimitations741031" data-type="indexterm" id="id1033"/>.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Three Tips to Get You Started: Our Agentic Best Practices" data-type="sect2"><div class="sect2" id="ch07_four_tips_to_get_you_started_our_agentic_best_pra_1740182051669179">
<h2>Three Tips to Get You Started: Our Agentic Best Practices</h2>

<p>Whenever you come across anything new<a contenteditable="false" data-primary="agentic systems" data-secondary="best practices" data-type="indexterm" id="xi_agenticsystemsbestpractices743053"/>, it’s always best to get some tips to help you get started. We created this section with extra help from some IBMers like Anna Gutowska, whose day-to-day job is literally training agentic systems that are smart enough to do incredible things, but not so wild that they start doing crazy things. If you pay attention to these tips, you’ll be living your best agentic life—a trusted one.</p>

<section data-pdf-bookmark="1. Activity logs" data-type="sect3"><div class="sect3" id="ch07_section_1_activity_logs_1740182051669237">
<h3>1. Activity logs</h3>

<p>To better understand and debug agent behavior after the fact<a contenteditable="false" data-primary="activity logs for agents" data-type="indexterm" id="id1034"/>, developers can provide users with access to a log of agent actions. These actions can include the use of external tools and describe the individual steps taken to reach the goal. This transparency gives users insights into an agent’s iterative decision-making process and provides the opportunity to discover errors and build trust.</p>
</div></section>

<section data-pdf-bookmark="2. Interruption and runtime observability" data-type="sect3"><div class="sect3" id="ch07_section_2_interruption_and_runtime_observability_1740182051669292">
<h3>2. Interruption and runtime observability</h3>

<p>Prevent AI agents from running<a contenteditable="false" data-primary="runtime observability for agents" data-type="indexterm" id="id1035"/> for overly long periods to avoid cases of unintended infinite feedback loops, changes in access to certain tools, or malfunctioning due to design flaws. One way to accomplish this is by implementing interruptability, where a human user (or an external resource manager like Turbonomic) can stop a pointless (or endless) workflow. To make interruptability more powerful, you also need to layer in observability to your agentic system so that you can monitor where an agent is in its workflow, and if something goes wrong, quickly find the what and how.</p>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="3. Human supervision" data-type="sect3"><div class="sect3" id="ch07_section_4_human_supervision_1740182051669403">
<h3 class="less_space">3. Human supervision</h3>

<p>To assist in the learning process for AI agents<a contenteditable="false" data-primary="supervision by humans for agents" data-type="indexterm" id="id1036"/>, especially in their early stages in a new environment, it can be helpful to provide occasional human feedback. This allows your agents to compare their performance to the expected standard and adjust accordingly. This form of feedback is helpful in improving any agent’s adaptability to user preferences.<sup><a data-type="noteref" href="ch07.html#id1037" id="id1037-marker">32</a></sup><sup> </sup></p>

<p>For example, you can set up your framework such that every time your agent finishes a task, it stops and asks for some feedback<a contenteditable="false" data-primary="feedback interruption for controlling agents" data-type="indexterm" id="id1038"/>—this gives you an opportunity to tell it what’s missing or what it could do better. <a data-type="xref" href="#ch07_figure_10_1740182051640455">Figure 7-10</a> was an example of where this would have been a great thing to do. Upon reading the social media posts, we could have shaped it to better suit our style and the audiences using those outlets. We could then push this feedback into an LLM that would extract these tips for each task and put them into the memory of our agents to reference in the future. Once again, we could use AI here to help AI by creating a “judge” AI model to look at the output work for our LLM and see whether it’s up to snuff.</p>

<p>Apart from this, it is a best practice to require human approval before an AI agent takes highly impactful actions. For instance, actions ranging from sending mass emails to financial trading should require human confirmation.<sup><a data-type="noteref" href="ch07.html#id1039" id="id1039-marker">33</a></sup> Some level of human monitoring is recommended for such high-risk domains<a contenteditable="false" data-primary="" data-startref="xi_agenticsystemsbestpractices743053" data-type="indexterm" id="id1040"/>.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Think About This When It Comes to AI Agents" data-type="sect2"><div class="sect2" id="ch07_think_about_this_when_it_comes_to_ai_agents_1740182051669460">
<h2>Think About This When It Comes to AI Agents</h2>

<p>AI agents and agentic systems are becoming popular for a reason: they are able to significantly improve AI’s performance and robustness on complex tasks. The ability to plan and reason for a task, bring in the latest up-to-date information via tool calls, and break down complex problems into smaller, more tractable components also opens the door for smaller, open source models to take on more challenging assignments. At the end of the day, this technology is still evolving, and while it improves model performance and productivity, ensure you spend time thinking about safety, security, and cost before leveraging AI agents for production tasks<a contenteditable="false" data-primary="" data-startref="xi_models711137" data-type="indexterm" id="id1041"/><a contenteditable="false" data-primary="" data-startref="xi_agenticsystems729764" data-type="indexterm" id="id1042"/>.</p>
</div></section>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="Wrapping It Up" data-type="sect1"><div class="sect1" id="ch07_wrapping_it_up_1740182051669515">
<h1 class="less_space">Wrapping It Up</h1>

<p>There are a lot of exciting things happening in the world of AI, and we hope we’ve convinced you that whether it is the advent of SLMs or the efficacy of more systems-based approaches to AI, including model routing, MoE, or agentic systems, “one model will not rule them all.” In the next chapter, we are going to cover how enterprises can extend these systems-based implementations of LLMs by specializing SLMs on enterprise data.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id906"><sup><a href="ch07.html#id906-marker">1</a></sup> Maximilian Schreiner, “GPT-4 Architecture, Datasets, Costs and More Leaked,” <em>The Decoder</em>, July 11, 2023, <a href="https://oreil.ly/6sD6g"><em>https://oreil.ly/6sD6g</em></a>. </p><p data-type="footnote" id="id907"><sup><a href="ch07.html#id907-marker">2</a></sup> See OpenAI’s API pricing <a href="https://oreil.ly/d21qP">online</a>. </p><p data-type="footnote" id="id909"><sup><a href="ch07.html#id909-marker">3</a></sup> See the update at <a href="https://oreil.ly/jCoxe"><em>https://oreil.ly/jCoxe</em></a>. </p><p data-type="footnote" id="id910"><sup><a href="ch07.html#id910-marker">4</a></sup> Muddu Sudhakar, “Small Language Models (SLMs): The Next Frontier for the Enterprise,” <em>Forbes</em>, <a href="https://oreil.ly/slTCo"><em>https://oreil.ly/slTCo</em></a>. </p><p data-type="footnote" id="id920"><sup><a href="ch07.html#id920-marker">5</a></sup> Jared Kaplan et al., “Scaling Laws for Neural Language Models,” preprint, arXiv, January 23, 2020, arXiv:2001.08361 (2020). <a href="https://arxiv.org/abs/2001.08361"><em>https://arxiv.org/abs/2001.08361</em></a>. </p><p data-type="footnote" id="id923"><sup><a href="ch07.html#id923-marker">6</a></sup> Jordan, Hoffmann et al., “Training Compute-Optimal Large Language Models,” preprint, arXiv, March 29, 2022, <a href="https://arxiv.org/abs/2203.15556"><em>https://arxiv.org/abs/2203.15556</em></a>. </p><p data-type="footnote" id="id930"><sup><a href="ch07.html#id930-marker">7</a></sup> Aaron Grattafiori et al., “The Llama 3 Herd of Models,” preprint, arXiv, November 23, 2024, <a href="https://arxiv.org/abs/2407.21783"><em>https://arxiv.org/abs/2407.21783</em></a>. </p><p data-type="footnote" id="id931"><sup><a href="ch07.html#id931-marker">8</a></sup> Aaron Grattafiori et al., “The Llama 3 Herd of Models,” preprint, arXiv, November 23, 2024, <a href="https://arxiv.org/abs/2407.21783"><em>https://arxiv.org/abs/2407.21783</em></a>. </p><p data-type="footnote" id="id935"><sup><a href="ch07.html#id935-marker">9</a></sup> Suriya Gunasekar et al., “Textbooks Are All You Need,” arXiv, October 2, 2023, <a href="https://arxiv.org/pdf/2306.11644"><em>https://arxiv.org/pdf/2306.11644</em></a>. </p><p data-type="footnote" id="id938"><sup><a href="ch07.html#id938-marker">10</a></sup> Kate Knibbs, “The Battle Over Books3 Is Just the Beginning,” <em>Wired</em>, September 4, 2023, <a href="https://oreil.ly/58JTr"><em>https://oreil.ly/58JTr</em></a>. </p><p data-type="footnote" id="id945"><sup><a href="ch07.html#id945-marker">11</a></sup> Previously known as PubMedGTP. </p><p data-type="footnote" id="id949"><sup><a href="ch07.html#id949-marker">12</a></sup> Reuters Graphics, “COBOL Blues,” <a href="https://oreil.ly/lM-8U"><em>https://oreil.ly/lM-8U</em></a>. </p><p data-type="footnote" id="id960"><sup><a href="ch07.html#id960-marker">13</a></sup> <em>The Vicuna Team</em> (blog), “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality,” LMSYS, March 30, 2023, <a href="https://oreil.ly/qRHD4"><em>https://oreil.ly/qRHD4</em></a>. </p><p data-type="footnote" id="id968"><sup><a href="ch07.html#id968-marker">14</a></sup> “Nemotron-4-340B-Instruct,” Hugging Face, <a href="https://oreil.ly/5Mh3Y"><em>https://oreil.ly/5Mh3Y</em></a>. </p><p data-type="footnote" id="id972"><sup><a href="ch07.html#id972-marker">15</a></sup> Samanatha Subin, “Nvidia Sheds Almost $600 Billion in Market Cap, Biggest One-Day Loss in U.S. History,” CNBC, January 27, 2025, <a href="https://oreil.ly/vWA0q"><em>https://oreil.ly/vWA0q</em></a>. </p><p data-type="footnote" id="id977"><sup><a href="ch07.html#id977-marker">16</a></sup> DeepSeek-AI, “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,” arXiv, January 22, 2025, <a href="https://arxiv.org/pdf/2501.12948"><em>https://arxiv.org/pdf/2501.12948</em></a>. </p><p data-type="footnote" id="id978"><sup><a href="ch07.html#id978-marker">17</a></sup> See the data on <a href="https://oreil.ly/GOLDy">Hugging Face’s website</a>. </p><p data-type="footnote" id="id979"><sup><a href="ch07.html#id979-marker">18</a></sup> See the datasets on the <a href="https://oreil.ly/rVZKY">Hugging Face website</a>. </p><p data-type="footnote" id="id980"><sup><a href="ch07.html#id980-marker">19</a></sup> Cade Metz, “OpenAI Says DeepSeek May Have Improperly Harvested Its Data,” <em>The New York Times</em>, January 29, 2025, <a href="https://oreil.ly/7xn_C"><em>https://oreil.ly/7xn_C</em></a>. </p><p data-type="footnote" id="id984"><sup><a href="ch07.html#id984-marker">20</a></sup> See the licensing agreement on <a href="https://oreil.ly/6prL2">Llama’s website</a>. </p><p data-type="footnote" id="id986"><sup><a href="ch07.html#id986-marker">21</a></sup> <em>Cisco Blogs</em>, “Evaluating Security Risk in DeepSeek and Other Frontier Reasoning Models,” by Paul Kassianik and Amin Karbasi, posted January 31, 2025, <a href="https://oreil.ly/gy5Xp"><em>https://oreil.ly/gy5Xp</em></a>. </p><p data-type="footnote" id="id987"><sup><a href="ch07.html#id987-marker">22</a></sup> Tested using deepinfra.com’s hosted version of DeepSeek-R1. </p><p data-type="footnote" id="id988"><sup><a href="ch07.html#id988-marker">23</a></sup> See the posting on <a href="https://oreil.ly/UrcIZ">IBM’s website</a>. Even more important, these distilled models failed miserably on safety evaluations. It is difficult to know exactly why these models are deficient in general performance and safety, but a potential hypothesis is that by focusing exclusively on code and math during distillation, safety and general performance were left to the wayside.</p><p data-type="footnote" id="id993"><sup><a href="ch07.html#id993-marker">24</a></sup> Tal Shnitzer et al., “Large Language Model Routing with Benchmark Datasets,” preprint, arXiv, September 27, 2023, <a href="https://arxiv.org/abs/2309.15789"><em>https://arxiv.org/abs/2309.15789</em></a>. </p><p data-type="footnote" id="id994"><sup><a href="ch07.html#id994-marker">25</a></sup> Percy Liang et al., “Holistic Evaluation of Language Models,” preprint, arXiv, October 1, 2023, <a href="https://arxiv.org/abs/2211.09110"><em>https://arxiv.org/abs/2211.09110</em></a>. </p><p data-type="footnote" id="id995"><sup><a href="ch07.html#id995-marker">26</a></sup> Tal Shnitzer et al., “Large Language Model Routing with Benchmark Datasets,” preprint, September 27, 2023, arXiv, <a href="https://arxiv.org/abs/2309.15789"><em>https://arxiv.org/abs/2309.15789</em></a>. </p><p data-type="footnote" id="id1008"><sup><a href="ch07.html#id1008-marker">27</a></sup> DeepSeek-AI, “DeepSeek-V3 Technical Report,” preprint, arXiv, February 18, 2025, <a href="https://arxiv.org/html/2412.19437v1"><em>https://arxiv.org/html/2412.19437v1</em></a>. </p><p data-type="footnote" id="id1010"><sup><a href="ch07.html#id1010-marker">28</a></sup> Ibid.</p><p data-type="footnote" id="id1020"><sup><a href="ch07.html#id1020-marker">29</a></sup> Alan Chan et al., “Visibility into AI Agents,” arXiv, updated May 17, 2024, <a href="https://arxiv.org/abs/2401.13138"><em>https://arxiv.org/abs/2401.13138</em></a>. </p><p data-type="footnote" id="id1025"><sup><a href="ch07.html#id1025-marker">30</a></sup> Gautier Dagan et al., “Dynamic Planning with a LLM,” preprint, arXiv, August 11, 2023, <a href="https://arxiv.org/abs/2308.06391"><em>https://arxiv.org/abs/2308.06391</em></a>. </p><p data-type="footnote" id="id1029"><sup><a href="ch07.html#id1029-marker">31</a></sup> Bienfeng Xu et al., “ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,” preprint, arXiv, May 23, 2023, arXiv. <a href="https://arxiv.org/abs/2305.18323"><em>https://arxiv.org/abs/2305.18323</em></a>. </p><p data-type="footnote" id="id1037"><sup><a href="ch07.html#id1037-marker">32</a></sup> Bienfeng Xu et al., “ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,” preprint, arXiv, May 23, 2023, <a href="https://arxiv.org/abs/2305.18323"><em>https://arxiv.org/abs/2305.18323</em></a>. </p><p data-type="footnote" id="id1039"><sup><a href="ch07.html#id1039-marker">33</a></sup> Veselka Sasheva Petrova-Dimitrova, “Classifications of Intelligence Agents and Their Applications,” <em>Fundamental Sciences and Applications</em> 28, no. 1 (2022). </p></div></div></section></body></html>