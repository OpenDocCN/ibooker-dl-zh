- en: Chapter 5\. Responsible AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we focused on building a model and preparing it for deployment.
    Before going live, though, there are additional important considerations to cover.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) are not just powerful from a technical standpoint
    but also from a societal standpoint. They’re being used to amplify spam, aid scammers,
    and even generate plausible propaganda at scale. They are trained on massive corpuses
    of data, which include material we want to draw from as well as all kinds of bias,
    racism, sexism, and potentially illegal or harmful material (for example, early
    versions of ChatGPT cheerfully explained to users how to build bombs). Because
    of this, there has been increased emphasis on and scrutiny of ethical and responsible
    training and use of LLMs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Data Safety and Transparency
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like any other machine learning model, training data is of utmost importance
    to LLMs. Unlike other machine learning models, however, LLMs generate text, which
    creates a new attack vector for bad actors. One way to combat this and help prevent
    unethical data usage is to publish clear information about how an LLM was trained,
    what data it used, and who developed it. This information makes it possible for
    users, researchers, and regulators to effectively scrutinize the model’s behavior
    and to report or manage instances of harm or misuse. There are several platforms
    and frameworks available to help achieve this, such as UNESCO’s [Global AI Ethics
    and Governance Observatory](https://oreil.ly/HSfmk), with more being developed
    and iterated on by government groups, private think tanks, organizations building
    AI, and academic labs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Because of the wide variety of possible outputs of an LLM, there are various
    outputs that a model could generate that could be a security vulnerability (like
    leaking personally identifiable information [PII]), a safety or ethical violation
    (such as generating racist or sexist content), or harmful information (like how
    to commit suicide or to build a bomb). These topics are challenging to consider,
    and the open-ended nature of LLMs multiplies the challenge, but it is important
    to consider them due to the wide deployment and novel capabilities of this technology.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: As AI becomes more regulated, transparency will be key to ensuring compliance
    with laws and standards related to data privacy, use, and security. Transparent
    data practices help companies demonstrate that they are following their legal
    obligations such as HIPAA for health data in the US, GDPR in the EU, and censoring
    PII. The regulatory frameworks around the world are in a constant state of flux,
    so the UN Trade and Development’s page for [data privacy laws around the world](https://oreil.ly/gCUls)
    is a good resource to consult for the latest views on the global regulatory landscape.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate safety, one popular tool currently available is LM-Eval. LM-Eval
    allows you to test generative AI models across a wide range of task-specific benchmarks
    and also to build your own benchmarks to test your trained model against. This
    tool is compatible with Kubernetes via the TrustyAI Kubernetes Operator, and is
    installed by default in [Red Hat OpenShift AI](https://oreil.ly/4s0C_). Check
    out the TrustyAI website to learn more about using [LM-Eval in Kubernetes](https://oreil.ly/ZeZyV)
    and see how [LM-Eval fits in with the rest of your cluster](https://oreil.ly/dsTgo).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: AI Guardrails
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI guardrails are a new software concept that act as safety mechanisms for LLMs.
    These are solutions that act as middleware between an incoming request and an
    LLM, and add limitations to prevent outputs that would be harmful or go against
    some norm. For example, a security guardrail could detect errant PII in an output
    and remove it before returning it to a user. Guardrails should be able to enforce
    an enterprise’s policies and guidelines, enable contextual understanding, and
    be regularly updated.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: One community building an entire suite of open source AI safety tools is [TrustyAI](https://oreil.ly/hMSM-),
    who also created LM-Eval, mentioned in the previous section. They also provide
    AI guardrail tooling, such as `trustyai-detoxify`, a Python module within the
    larger TrustyAI Python library that provides guardrails around toxic language,
    among other safety tools centered on toxic language. Additionally, they maintain
    [TrustyAI Guardrails](https://oreil.ly/AUOiB), which acts as a server for calling
    detectors (such as a toxic language detector) to help developers implement their
    own guardrails.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM guardrail detectors are tools that ensure LLMs operate within predefined
    ethical, safety, and operational boundaries. These are designed to identify and
    mitigate undesirable outcomes, such as generating harmful or inappropriate content,
    ensuring that AI systems behave responsibly. There are a wide array of detectors
    available today, and you can access many public ones via the [Guardrails AI Hub](https://hub.guardrailsai.com).
    Some broad examples of detectors include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: PII detectors
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: These screen outputs for PII and prevent it from reaching the user.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Hate, abuse, profanity (HAP) detectors
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: These screen outputs for toxic content, bias, or harmful content like hate speech,
    discrimination, or misinformation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Bias detectors
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: These analyze outputs for signs of unfair biases related to race, gender, religion,
    or other attributes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Prompt injection detectors
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: These recognize and counter potential adversarial prompts or attempts to manipulate
    the model into generating harmful content.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: TrustyAI provides a [Kubernetes Operator](https://oreil.ly/kB0fi) that allows
    you to seamlessly integrate it with your cluster, as well as a [custom explainer
    for KServe](https://oreil.ly/FzfM_).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: While these toolkits are not exhaustive, they are currently available resources
    to help address abuses of LLMs, keeping your users and your enterprise safe. To
    dive deeper into mitigating bias and harm throughout the AI development lifecycle,
    consult Aileen Nielsen’s book [*Practical Fairness*](https://www.oreilly.com/library/view/practical-fairness/9781492075721/)
    (O’Reilly, 2020).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些工具包并不全面，但它们是目前可用的资源，有助于解决LLMs的滥用问题，确保您的用户和企业的安全。要深入了解在整个AI开发生命周期中减轻偏见和伤害的方法，请参阅Aileen
    Nielsen的书籍[*《实用公平性》*](https://www.oreilly.com/library/view/practical-fairness/9781492075721/)（O’Reilly，2020年）。
