- en: Chapter 5\. Responsible AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章\. 负责任的AI
- en: Up until now, we focused on building a model and preparing it for deployment.
    Before going live, though, there are additional important considerations to cover.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们专注于构建模型并为其部署做准备。然而，在上线之前，还有一些额外的、重要的考虑事项需要涵盖。
- en: Large language models (LLMs) are not just powerful from a technical standpoint
    but also from a societal standpoint. They’re being used to amplify spam, aid scammers,
    and even generate plausible propaganda at scale. They are trained on massive corpuses
    of data, which include material we want to draw from as well as all kinds of bias,
    racism, sexism, and potentially illegal or harmful material (for example, early
    versions of ChatGPT cheerfully explained to users how to build bombs). Because
    of this, there has been increased emphasis on and scrutiny of ethical and responsible
    training and use of LLMs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）不仅在技术层面上强大，在社会层面上也是如此。它们被用来放大垃圾邮件、帮助骗子，甚至大规模生成看似合理的宣传。它们在庞大的数据集上进行训练，这些数据集包括我们希望从中汲取的内容以及各种偏见、种族主义、性别歧视，以及可能非法或有害的内容（例如，ChatGPT的早期版本愉快地向用户解释如何制造炸弹）。因此，对LLMs的道德和负责任训练及使用的重视和审查有所增加。
- en: Data Safety and Transparency
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据安全和透明度
- en: Like any other machine learning model, training data is of utmost importance
    to LLMs. Unlike other machine learning models, however, LLMs generate text, which
    creates a new attack vector for bad actors. One way to combat this and help prevent
    unethical data usage is to publish clear information about how an LLM was trained,
    what data it used, and who developed it. This information makes it possible for
    users, researchers, and regulators to effectively scrutinize the model’s behavior
    and to report or manage instances of harm or misuse. There are several platforms
    and frameworks available to help achieve this, such as UNESCO’s [Global AI Ethics
    and Governance Observatory](https://oreil.ly/HSfmk), with more being developed
    and iterated on by government groups, private think tanks, organizations building
    AI, and academic labs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他机器学习模型一样，训练数据对于大型语言模型（LLMs）至关重要。然而，与其他机器学习模型不同的是，LLMs生成文本，这为恶意行为者创造了一个新的攻击向量。为了应对这种情况并帮助防止不道德的数据使用，一种方法就是公开关于LLM是如何训练的、它使用了哪些数据以及谁开发了它的明确信息。这些信息使用户、研究人员和监管机构能够有效地审查模型的行为，并报告或管理伤害或滥用事件。有几个平台和框架可用于实现这一目标，例如联合国教科文组织（UNESCO）的[全球人工智能伦理与治理观测站](https://oreil.ly/HSfmk)，还有更多由政府团体、私人智库、构建人工智能的组织和学术实验室开发并迭代。
- en: Because of the wide variety of possible outputs of an LLM, there are various
    outputs that a model could generate that could be a security vulnerability (like
    leaking personally identifiable information [PII]), a safety or ethical violation
    (such as generating racist or sexist content), or harmful information (like how
    to commit suicide or to build a bomb). These topics are challenging to consider,
    and the open-ended nature of LLMs multiplies the challenge, but it is important
    to consider them due to the wide deployment and novel capabilities of this technology.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM可能产生各种输出，因此模型可能生成的输出可能存在安全漏洞（如泄露个人身份信息[PII]）、安全或道德违规（例如生成种族主义或性别歧视内容）或有害信息（例如自杀或制造炸弹的方法）。这些话题很难考虑，而LLM的开放性性质又增加了挑战，但由于这项技术的广泛部署和新型能力，考虑这些问题非常重要。
- en: As AI becomes more regulated, transparency will be key to ensuring compliance
    with laws and standards related to data privacy, use, and security. Transparent
    data practices help companies demonstrate that they are following their legal
    obligations such as HIPAA for health data in the US, GDPR in the EU, and censoring
    PII. The regulatory frameworks around the world are in a constant state of flux,
    so the UN Trade and Development’s page for [data privacy laws around the world](https://oreil.ly/gCUls)
    is a good resource to consult for the latest views on the global regulatory landscape.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能监管的加强，透明度将是确保遵守与数据隐私、使用和安全相关的法律和标准的关键。透明度高的数据实践有助于公司证明它们正在履行其法律义务，例如美国的健康数据HIPAA、欧盟的GDPR以及审查个人身份信息（PII）。全球的监管框架处于不断变化的状态，因此联合国贸易和发展署的[全球数据隐私法律](https://oreil.ly/gCUls)页面是一个咨询最新全球监管格局的好资源。
- en: To evaluate safety, one popular tool currently available is LM-Eval. LM-Eval
    allows you to test generative AI models across a wide range of task-specific benchmarks
    and also to build your own benchmarks to test your trained model against. This
    tool is compatible with Kubernetes via the TrustyAI Kubernetes Operator, and is
    installed by default in [Red Hat OpenShift AI](https://oreil.ly/4s0C_). Check
    out the TrustyAI website to learn more about using [LM-Eval in Kubernetes](https://oreil.ly/ZeZyV)
    and see how [LM-Eval fits in with the rest of your cluster](https://oreil.ly/dsTgo).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估安全性，目前一个流行的工具是LM-Eval。LM-Eval允许您在广泛的特定任务基准上测试生成AI模型，还可以构建自己的基准来测试您的训练模型。此工具通过TrustyAI
    Kubernetes Operator与Kubernetes兼容，并在[Red Hat OpenShift AI](https://oreil.ly/4s0C_)中默认安装。查看TrustyAI网站了解有关在Kubernetes中使用[LM-Eval](https://oreil.ly/ZeZyV)的更多信息，并了解[LM-Eval如何与集群中的其他部分相匹配](https://oreil.ly/dsTgo)。
- en: AI Guardrails
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI安全护栏
- en: AI guardrails are a new software concept that act as safety mechanisms for LLMs.
    These are solutions that act as middleware between an incoming request and an
    LLM, and add limitations to prevent outputs that would be harmful or go against
    some norm. For example, a security guardrail could detect errant PII in an output
    and remove it before returning it to a user. Guardrails should be able to enforce
    an enterprise’s policies and guidelines, enable contextual understanding, and
    be regularly updated.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: AI安全护栏是一个新的软件概念，作为LLM的安全机制。这些解决方案作为入站请求和LLM之间的中间件，并添加限制以防止产生有害或违反某些规范的结果。例如，安全护栏可以检测输出中的错误PII并将其删除，然后再将其返回给用户。安全护栏应该能够执行企业的政策和指南，实现上下文理解，并且需要定期更新。
- en: One community building an entire suite of open source AI safety tools is [TrustyAI](https://oreil.ly/hMSM-),
    who also created LM-Eval, mentioned in the previous section. They also provide
    AI guardrail tooling, such as `trustyai-detoxify`, a Python module within the
    larger TrustyAI Python library that provides guardrails around toxic language,
    among other safety tools centered on toxic language. Additionally, they maintain
    [TrustyAI Guardrails](https://oreil.ly/AUOiB), which acts as a server for calling
    detectors (such as a toxic language detector) to help developers implement their
    own guardrails.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个正在构建整个开源AI安全工具套件的社区是[TrustyAI](https://oreil.ly/hMSM-)，他们还创建了之前提到的LM-Eval。他们还提供了AI安全护栏工具，例如`trustyai-detoxify`，这是TrustyAI
    Python库中的一个Python模块，它提供了围绕有毒语言的护栏，以及其他以有毒语言为中心的安全工具。此外，他们维护[TrustyAI Guardrails](https://oreil.ly/AUOiB)，它作为一个服务器，用于调用检测器（如有毒语言检测器），以帮助开发者实施自己的安全护栏。
- en: 'LLM guardrail detectors are tools that ensure LLMs operate within predefined
    ethical, safety, and operational boundaries. These are designed to identify and
    mitigate undesirable outcomes, such as generating harmful or inappropriate content,
    ensuring that AI systems behave responsibly. There are a wide array of detectors
    available today, and you can access many public ones via the [Guardrails AI Hub](https://hub.guardrailsai.com).
    Some broad examples of detectors include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LLM安全护栏检测器是确保LLM在预定义的道德、安全和操作边界内运行的工具。这些工具旨在识别和减轻不良结果，例如生成有害或不适当的内容，确保AI系统负责任地行为。目前有各种各样的检测器可用，您可以通过[Guardrails
    AI Hub](https://hub.guardrailsai.com)访问许多公共的检测器。一些广泛的检测器示例包括：
- en: PII detectors
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PII检测器
- en: These screen outputs for PII and prevent it from reaching the user.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些屏幕输出用于识别PII并防止其到达用户。
- en: Hate, abuse, profanity (HAP) detectors
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 仇恨、滥用、粗话（HAP）检测器
- en: These screen outputs for toxic content, bias, or harmful content like hate speech,
    discrimination, or misinformation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些屏幕输出用于检测有毒内容、偏见或有害内容，如仇恨言论、歧视或错误信息。
- en: Bias detectors
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见检测器
- en: These analyze outputs for signs of unfair biases related to race, gender, religion,
    or other attributes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分析输出，以寻找与种族、性别、宗教或其他属性相关的公平偏见迹象。
- en: Prompt injection detectors
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入检测器
- en: These recognize and counter potential adversarial prompts or attempts to manipulate
    the model into generating harmful content.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些识别并对抗潜在的对抗性提示或试图操纵模型生成有害内容的行为。
- en: TrustyAI provides a [Kubernetes Operator](https://oreil.ly/kB0fi) that allows
    you to seamlessly integrate it with your cluster, as well as a [custom explainer
    for KServe](https://oreil.ly/FzfM_).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TrustyAI提供了一个[Kubernetes Operator](https://oreil.ly/kB0fi)，允许您无缝地将它与您的集群集成，以及一个[KServe的自定义解释器](https://oreil.ly/FzfM_)。
- en: While these toolkits are not exhaustive, they are currently available resources
    to help address abuses of LLMs, keeping your users and your enterprise safe. To
    dive deeper into mitigating bias and harm throughout the AI development lifecycle,
    consult Aileen Nielsen’s book [*Practical Fairness*](https://www.oreilly.com/library/view/practical-fairness/9781492075721/)
    (O’Reilly, 2020).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些工具包并不全面，但它们是目前可用的资源，有助于解决LLMs的滥用问题，确保您的用户和企业的安全。要深入了解在整个AI开发生命周期中减轻偏见和伤害的方法，请参阅Aileen
    Nielsen的书籍[*《实用公平性》*](https://www.oreilly.com/library/view/practical-fairness/9781492075721/)（O’Reilly，2020年）。
