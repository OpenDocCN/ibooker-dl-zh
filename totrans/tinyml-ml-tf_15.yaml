- en: Chapter 15\. Optimizing Latency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章。优化延迟
- en: Embedded systems don’t have much computing power, which means that the intensive
    calculations needed for neural networks can take longer than on most other platforms.
    Because embedded systems usually operate on streams of sensor data in real time,
    running too slowly can cause a lot of problems. Suppose that you’re trying to
    observe something that might occur only briefly (like a bird being visible in
    a camera’s field of view). If your processing time is too long you might sample
    the sensor too slowly and miss one of these occurrences. Sometimes the quality
    of a prediction is improved by repeated observations of overlapping windows of
    sensor data, in the way the wake-word detection example runs a one-second window
    on audio data for wake-word spotting, but moves the window forward only a hundred
    milliseconds or less each time, averaging the results. In these cases, reducing
    latency lets us improve the overall accuracy. Speeding up the model execution
    might also allow the device to run at a lower CPU frequency, or go to sleep in
    between inferences, which can reduce the overall energy usage.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式系统的计算能力有限，这意味着神经网络所需的密集计算可能比大多数其他平台花费更长的时间。由于嵌入式系统通常实时处理传感器数据流，运行速度过慢可能会导致许多问题。假设您试图观察可能仅在短暂时间内发生的事情（比如相机视野中出现的鸟）。如果处理时间太长，您可能会以太慢的速度采样传感器，错过其中一个事件。有时，通过重复观察重叠的传感器数据窗口，可以改善预测的质量，就像唤醒词检测示例在音频数据上运行一秒钟的窗口来进行唤醒词识别，但每次只将窗口向前移动一百毫秒或更少，对结果进行平均。在这些情况下，减少延迟可以帮助我们提高整体准确性。加快模型执行还可以使设备以更低的CPU频率运行，或在推理之间进入睡眠状态，从而降低整体能源使用量。
- en: Because latency is such an important area for optimization, this chapter focuses
    on some of the different techniques you can use to reduce the time it takes to
    run your model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于延迟是优化的一个重要领域，本章重点介绍了一些不同的技术，可以帮助您减少运行模型所需的时间。
- en: First Make Sure It Matters
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 首先确保它重要
- en: It’s possible that your neural network code is such a small part of your overall
    system latency that speeding it up wouldn’t make a big difference to your product’s
    performance. The simplest way to determine whether this is the case is by commenting
    out the call to [`tflite::MicroInterpreter::Invoke()`](https://oreil.ly/1dLTn)
    in your application code. This is the function that contains all of the inference
    calculations, and it will block until the network has been run, so by removing
    it you can observe what difference it makes to the overall latency. In an ideal
    world you’ll be able to calculate this change with a timer log statement or profiler,
    but as described shortly even just blinking an LED and eye balling the frequency
    difference might be enough to give you a rough idea of what the speed increase
    is. If the difference between running the network inference and not is small,
    there’s not much to gain from optimizing the deep learning part of the code, and
    you should focus on other parts of your application first.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能您的神经网络代码只是整体系统延迟的一小部分，加快它可能对产品的性能没有太大影响。确定是否是这种情况的最简单方法是在应用代码中注释掉对[`tflite::MicroInterpreter::Invoke()`](https://oreil.ly/1dLTn)的调用。这个函数包含了所有的推理计算，并且会阻塞直到网络运行完毕，因此通过移除它，您可以观察它对整体延迟的影响。在理想的情况下，您可以通过计时器日志语句或分析器来计算这种变化，但正如稍后所述，即使只是闪烁LED并粗略估计频率差异，也足以让您对速度增加有一个大致的概念。如果运行网络推理和不运行之间的差异很小，那么从优化代码的深度学习部分中获益不大，您应该首先关注应用的其他部分。
- en: Hardware Changes
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件更改
- en: If you do need to speed up your neural network code, the first question to ask
    is whether you are able to use a more powerful hardware device. This won’t be
    possible for many embedded products, because the decision on which hardware platform
    to use is often made very early on or has been set externally, but because it’s
    the easiest factor to change from a software perspective, it’s worth explicitly
    considering. If you do have a choice, the biggest constraints are usually energy,
    speed, and cost. If you can, trade off energy or cost for speed by switching the
    chip you’re using. You might even get lucky in your research and discover a newer
    platform that gives you more speed without losing either of the other main factors!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您确实需要加快神经网络代码的速度，首先要问的问题是是否能够使用更强大的硬件设备。对于许多嵌入式产品来说，这可能是不可能的，因为通常在很早的时候或者外部已经确定了要使用哪种硬件平台，但因为从软件角度来看这是最容易改变的因素，所以值得明确考虑。如果您有选择的余地，最大的约束通常是能源、速度和成本。如果可以的话，通过更换使用的芯片来权衡能源或成本以换取速度。您甚至可能在研究中幸运地发现一个新平台，它可以在不失去其他两个主要因素的情况下提供更快的速度！
- en: Note
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When neural networks are trained, it’s typical to send a large number of training
    examples at once, in every training step. This allows a lot of calculation optimizations
    that are not possible when only one sample is submitted at once. For example,
    a hundred images and labels might be sent as part of a single training call. This
    collection of training data is called a *batch*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络进行训练时，通常会一次发送大量的训练示例，在每个训练步骤中。这样可以进行许多计算优化，而当一次只提交一个样本时是不可能的。例如，一百张图像和标签可能会作为一个单独的训练调用的一部分发送。这些训练数据的集合称为*批次*。
- en: With embedded systems we’re usually dealing with one group of sensor readings
    at a time, in real time, so we don’t want to wait to gather a larger batch before
    we trigger inference. This “single batch” focus means we can’t benefit from some
    optimizations that make sense on the training side, so the hardware architectures
    that are helpful for the cloud don’t always translate over to our use cases.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入式系统中，我们通常一次处理一组传感器读数，实时处理，因此我们不希望等待收集更大的批次再触发推理。这种“单批次”关注意味着我们无法从一些在训练阶段有意义的优化中获益，因此对云端有帮助的硬件架构并不总是适用于我们的用例。
- en: Model Improvements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型改进
- en: After switching hardware platforms, the easiest place to have a big impact on
    neural network latency is at the architecture level. If you can create a new model
    that is accurate enough but involves fewer calculations, you can speed up inference
    without making any code changes at all. It’s usually possible to trade reduced
    accuracy for increased speed, so if you’re able to start with as accurate a model
    as you can get at the beginning, you’ll have a lot more scope for these trade-offs.
    This means that spending time on improving and expanding your training data can
    be very helpful throughout the development process, even with apparently unrelated
    tasks like latency optimization.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在切换硬件平台后，对神经网络延迟产生重大影响的最简单方法是在架构层面。如果您能够创建一个足够准确但涉及更少计算的新模型，您可以加速推断而无需进行任何代码更改。通常可以通过降低准确性来换取增加速度，因此，如果您能够从一开始就使用尽可能准确的模型开始，那么您将有更多的空间进行这些权衡。这意味着花时间改进和扩展您的训练数据在整个开发过程中可能非常有帮助，即使在看似无关的任务，如延迟优化方面。
- en: When optimizing procedural code, it’s typically a better use of your budget
    to spend time changing the high-level algorithms your code is based on rather
    than rewriting inner loops in assembly. The focus on model architectures is based
    on the same idea; it’s better to eliminate work entirely if you can rather than
    improving the speed at which you do it. What is different in our case is that
    it’s actually a lot easier to swap out machine learning models than it is to switch
    algorithms in traditional code because each model is just a functional black box
    that takes in input data and returns numerical results. After you have a good
    set of data gathered, it should be comparatively easy to replace one model with
    another in the training scripts. You can even experiment with removing individual
    layers from a model that you’re using and observe the effect. Neural networks
    tend to degrade extremely gracefully, so you should feel free to try lots of different
    destructive changes and observe their effect on accuracy and latency.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化过程代码时，通常更好的做法是花时间改变代码基于的高级算法，而不是在汇编中重写内部循环。对模型架构的关注基于同样的想法；如果可以的话，最好是完全消除工作，而不是提高执行工作的速度。在我们的情况下不同的是，交换机器学习模型比在传统代码中切换算法要容易得多，因为每个模型只是一个接受输入数据并返回数值结果的功能黑盒。在收集了一组良好的数据之后，应该相对容易地在训练脚本中用另一个模型替换一个模型。您甚至可以尝试删除您正在使用的模型中的单个层并观察效果。神经网络往往具有非常良好的退化性能，因此您应该随意尝试许多不同的破坏性更改，并观察它们对准确性和延迟的影响。
- en: Estimating Model Latency
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 估算模型延迟
- en: Most neural network models spend the majority of their time running large matrix
    multiplications or very close equivalents. This is because every input value must
    be scaled by a different weight for each output value, so the work involved is
    approximately the number of input values times the number of output values for
    each layer in the network. This is often approximated by talking about the number
    of floating-point operations (or FLOPs) that a network requires for a single inference
    run. Usually a multiply-add operation (which is often a single instruction at
    the machine code level) counts as two FLOPs, and even if you’re performing 8-bit
    or lower quantized calculations you might sometimes see them referred to as FLOPs,
    even though floating-point numbers are no longer involved. The number of FLOPs
    required for a network can be calculated by hand, layer by layer. For example,
    a fully connected layer requires a number of FLOPs equal to the size of the input
    vector, multiplied by the size of the output vector. Thus, if you know those dimensions,
    you can figure out the work involved. You can also usually find FLOP estimates
    in papers that discuss and compare model architectures, like [MobileNet](https://arxiv.org/abs/1905.02244).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数神经网络模型在运行时花费大部分时间在运行大型矩阵乘法或非常接近的等效操作。这是因为每个输入值必须由不同的权重缩放以获得每个输出值，因此，每个网络层的工作量大约等于每个输入值乘以每个输出值的数量。这通常通过讨论网络在单次推断运行中所需的浮点运算数（或FLOPs）来近似。通常，一个乘加操作（通常在机器码级别是一个单指令）计为两个FLOPs，即使您执行8位或更低精度的量化计算，有时也会看到它们被称为FLOPs，尽管不再涉及浮点数。可以通过手动逐层计算网络所需的FLOPs。例如，全连接层所需的FLOPs数量等于输入向量的大小乘以输出向量的大小。因此，如果您知道这些维度，您可以计算出所需的工作量。通常在讨论和比较模型架构的论文中可以找到FLOP的估计值，比如[MobileNet](https://arxiv.org/abs/1905.02244)。
- en: FLOPs are useful as a rough metric for how much time a network will take to
    execute because, all else being equal, a model that involves fewer calculations
    will run faster and in proportion to the difference in FLOPs. For example, you
    could reasonably expect a model that requires 100 million FLOPs to run twice as
    fast as a 200-million-FLOP version. This isn’t entirely true in practice, because
    there are other factors like how well optimized the software is for particular
    layers that will affect the latency, but it’s a good starting point for evaluating
    different network architectures. It’s also useful to help establish what’s realistic
    to expect for your hardware platform. If you’re able to run a 1-million-FLOP model
    in 100 ms on your chip, you’ll be able to make an educated guess that a different
    model requiring 10 million FLOPs will take about a second to calculate.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: FLOPs作为一个粗略的度量单位，用于衡量一个网络执行所需时间的多少，因为其他条件相同，涉及更少计算的模型将以与FLOPs差异成比例的速度运行得更快。例如，您可以合理地期望一个需要1亿FLOPs的模型比2亿FLOP版本运行速度快两倍。在实践中，这并不完全正确，因为还有其他因素，比如软件对特定层的优化程度会影响延迟，但这是评估不同网络架构的一个很好的起点。这也有助于确定对于您的硬件平台可以期望什么是现实的。如果您能在芯片上以100毫秒运行一个100万FLOP模型，那么您可以做出一个合理的猜测，即需要1000万FLOPs的不同模型将需要大约一秒来计算。
- en: How to Speed Up Your Model
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何加速您的模型
- en: Model architecture design is still an active research field, so it’s not easy
    to write a good guide for beginners at this point. The best starting point is
    to find some existing models that have been designed with efficiency in mind and
    then iteratively experiment with changes. Many models have particular parameters
    that we can alter to affect the amount of computation required, such as MobileNet’s
    depthwise channel factor, or the input size expected. In other cases, you might
    look at the FLOPs required for each layer and try to remove particularly slow
    ones or substitute them with faster alternatives (such as depthwise convolution
    instead of plain convolution). If you can, it’s also worth looking at the actual
    latency of each layer when running on-device, instead of estimating it through
    FLOPs. This will require some of the profiling techniques discussed in the sections
    that follow for code optimizations, though.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构设计仍然是一个活跃的研究领域，因此目前很难为初学者撰写一份好的指南。最好的起点是找到一些已经设计为高效的现有模型，然后迭代地尝试进行更改。许多模型具有特定的参数，我们可以改变这些参数以影响所需的计算量，比如MobileNet的深度通道因子，或者期望的输入大小。在其他情况下，您可能会查看每个层所需的FLOPs，并尝试删除特别慢的层或用更快的替代方案替换它们（例如使用深度卷积代替普通卷积）。如果可以的话，最好查看在设备上运行时每个层的实际延迟，而不是通过FLOPs来估计。尽管这将需要一些在接下来的代码优化部分讨论的性能分析技术。
- en: Note
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Designing model architectures is difficult and time-consuming, but there have
    recently been some advances in automating the process, such as [MnasNet](https://arxiv.org/abs/1807.11626),
    using approaches like genetic algorithms to improve network designs. These are
    still not at the point of entirely replacing humans (they often require seeding
    with known good architectures as starting points, and manual rules about what
    search space to use, for example), but it’s likely we’ll see rapid progress in
    this area.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 设计模型架构是困难且耗时的，但最近已经有一些自动化这个过程的进展，比如[MnasNet](https://arxiv.org/abs/1807.11626)，使用遗传算法等方法来改进网络设计。这些方法还没有完全取代人类（它们通常需要以已知的良好架构作为起点，并且需要手动规则来确定使用的搜索空间，例如），但很可能我们将在这个领域看到快速的进展。
- en: There are already services like [AutoML](https://cloud.google.com/automl/) that
    allow users to avoid many of the gritty details of training, and hopefully this
    trend will continue, so you’ll be able to pick the best possible model for your
    data and efficiency trade-offs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有像[AutoML](https://cloud.google.com/automl/)这样的服务，允许用户避开训练的许多细节，希望这种趋势会继续下去，这样您就能够选择最适合您的数据和效率权衡的最佳模型。
- en: Quantization
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化
- en: Running a neural network requires hundreds of thousands or even millions of
    calculations for every prediction. Most programs that perform such complex calculations
    are very sensitive to numerical precision; otherwise, errors build up and give
    a result that’s too inaccurate to use. Deep learning models are different—they
    are able to cope with large losses in numerical precision during intermediate
    calculations and still produce end results that are accurate overall. This property
    seems to be a by-product of their training process, in which the inputs are large
    and full of noise, so the models learn to be robust to insignificant variations
    and focus on the important patterns that matter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 运行神经网络需要进行数十万甚至数百万次计算以进行每次预测。执行这种复杂计算的大多数程序对数值精度非常敏感；否则，错误会累积并导致结果太不准确而无法使用。深度学习模型不同——它们能够在中间计算中承受大量数值精度损失，仍然能够产生整体准确的最终结果。这种特性似乎是它们训练过程的副产品，其中输入很大且充满噪音，因此模型学会了对微不足道的变化具有鲁棒性，并专注于重要的模式。
- en: What this means in practice is that operating with 32-bit floating-point representations
    is almost always more precise than is required for inference. Training is a bit
    more demanding because it requires many small changes to the weights to learn,
    but even there, 16-bit representations are widely used. Most inference applications
    can produce results that are indistinguishable from the floating-point equivalent,
    using just 8 bits to store weights and activation values. This is good news for
    embedded applications given that many of our platforms have strong support for
    the kind of 8-bit multiply-and-accumulate instructions that these models rely
    on, because those same instructions are common in signal-processing algorithms.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这意味着使用32位浮点表示进行操作几乎总是比推断所需的精度更高。训练要求更高一些，因为它需要对权重进行许多小的更改来学习，但即使在那里，16位表示也被广泛使用。大多数推断应用程序可以产生与浮点等效物无法区分的结果，只需使用8位来存储权重和激活值。鉴于我们的许多平台对这些模型依赖的8位乘积累加指令提供了强大的支持，这对于嵌入式应用来说是个好消息，因为这些指令在信号处理算法中很常见。
- en: It isn’t straightforward to convert a model from floating point to 8-bit, though.
    To perform calculations efficiently, the 8-bit values require a linear conversion
    to real numbers. This is easy for weights because we know the range for each layer
    from the trained values, so we can derive the correct scaling factor to perform
    the conversion. It’s trickier for activations, though, because it’s not obvious
    from inspecting the model parameters and architecture what the range of each layer’s
    outputs actually is. If we pick a range that’s too small, some outputs will be
    clipped to the minimum or maximum, but if it’s too large, the precision of the
    outputs will be smaller than it could be, and we’ll risk losing accuracy in the
    overall results.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将模型从浮点转换为8位并不简单。为了有效地执行计算，8位值需要线性转换为实数。这对于权重来说很容易，因为我们知道每个层的范围是从训练值中得出的，因此我们可以推导出正确的缩放因子来执行转换。然而，对于激活来说就比较棘手，因为从检查模型参数和架构中并不明显每个层输出的范围是多少。如果我们选择的范围太小，一些输出将被剪切到最小值或最大值，但如果范围太大，输出的精度将比可能的精度小，我们将面临整体结果精度下降的风险。
- en: 'Quantization is still an active research topic and there are a lot of different
    options, so the TensorFlow team has tried a variety of approaches over the past
    few years. You can see a discussion of some of these experiments in [“Quantizing
    Deep Convolutional Networks for Efficient Inference: A Whitepaper” by Raghuraman
    Krishnamoorthi](https://arxiv.org/pdf/1806.08342.pdf), and the [quantization specification](https://oreil.ly/toF_E)
    covers the recommended approach we now use based on our experience.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 量化仍然是一个活跃的研究课题，有许多不同的选择，因此TensorFlow团队在过去几年中尝试了各种方法。您可以在[Raghuraman Krishnamoorthi的“为高效推理量化深度卷积网络：白皮书”](https://arxiv.org/pdf/1806.08342.pdf)中看到一些这些实验的讨论，而[量化规范](https://oreil.ly/toF_E)则涵盖了我们现在基于经验使用的推荐方法。
- en: We’ve centralized the quantization process so that it happens during the process
    of converting a model from the TensorFlow training environment into a TensorFlow
    Lite graph. We used to recommend a quantization-aware training scheme, but this
    was difficult to use and we found we could produce equivalent results at export
    time, using some additional techniques. The easiest type of quantization to use
    is what’s known as [post-training weight quantization](https://oreil.ly/Tz9D_).
    This is when the weights are quantized down to 8 bits but the activation layers
    remain in floating point. This is useful because it shrinks the model file size
    by 75% and offers some speed benefits. It is the easiest approach to run because
    it doesn’t require any knowledge of the activation layer’s ranges, but it does
    still require fast floating-point hardware that isn’t present on many embedded
    platforms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将量化过程集中在将模型从TensorFlow训练环境转换为TensorFlow Lite图的过程中。我们过去推荐了一种量化感知训练方案，但发现这种方法难以使用，我们发现我们可以在导出时使用一些额外的技术获得等效的结果。最容易使用的量化类型是所谓的[训练后权重量化](https://oreil.ly/Tz9D_)。这是将权重量化为8位，但激活层保持浮点数的情况。这是有用的，因为它将模型文件大小缩小了75%，并提供了一些速度优势。这是最容易运行的方法，因为它不需要任何关于激活层范围的知识，但仍然需要快速浮点硬件，这在许多嵌入式平台上并不存在。
- en: '[Post-training integer quantization](https://oreil.ly/LDw-y) means that a model
    can be executed without any floating-point calculations, which makes it the preferred
    approach for the use cases we cover in this book. The most challenging part about
    using it is that you need to provide some example inputs during the model export
    process, so the ranges of the activation layer outputs can be observed by running
    some typical images, audio, or other data through the graph. As we discussed earlier,
    without estimates of these ranges, it’s not possible to quantize these layers
    accurately. In the past, we’ve used other methods, like recording the ranges during
    training or capturing them during every inference at runtime, but these had disadvantages
    like making training much more complicated or imposing a latency penalty, so this
    is the least-worst approach.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[训练后整数量化](https://oreil.ly/LDw-y)意味着模型可以在没有任何浮点计算的情况下执行，这使得它成为我们在本书中涵盖的用例的首选方法。使用它最具挑战性的部分是，在模型导出过程中需要提供一些示例输入，以便通过运行一些典型图像、音频或其他数据来观察激活层输出的范围。正如我们之前讨论过的，如果没有这些范围的估计，就无法准确地量化这些层。过去，我们使用过其他方法，比如在训练期间记录范围或在运行时捕获范围，但这些方法都有缺点，比如使训练变得更加复杂或施加延迟惩罚，因此这是最不好的方法。'
- en: If you look back at our instructions for exporting the person detector model
    in [Chapter 10](ch10.xhtml#chapter_person_detection_training), you’ll see that
    we provide a `representative_dataset` function to the `converter` object. This
    is a Python function that produces the inputs that the activation range estimation
    process needs, and for the person detector model we load some example images from
    the training dataset. This is something you’ll need to figure out for every model
    you train though, because the expected inputs will change for each application.
    It can also be tough to discern how the inputs are scaled and transformed as part
    of the preprocessing, so creating the function can involve some trial and error.
    We’re hoping to make this process easier in the future.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您回顾一下我们在[第10章](ch10.xhtml#chapter_person_detection_training)中导出人员检测器模型的说明，您会看到我们向`converter`对象提供了一个`representative_dataset`函数。这是一个Python函数，用于生成激活范围估计过程所需的输入，对于人员检测器模型，我们从训练数据集中加载一些示例图像。不过，对于您训练的每个模型，您都需要弄清楚预期输入，因为每个应用程序的预期输入都会发生变化。此外，很难辨别输入在预处理过程中是如何缩放和转换的，因此创建该函数可能需要一些试错。我们希望未来能够简化这个过程。
- en: Running fully quantized models has big latency benefits on almost all platforms,
    but if you’re supporting a new device it’s likely that you’ll need to optimize
    the most computationally intensive operations to take advantage of specialized
    instructions offered by your hardware. A good place to begin if you’re working
    on a convolutional network is the [`Conv2D` operation](https://oreil.ly/NrjSo)
    and the [kernel](https://oreil.ly/V27Q-). You’ll notice that there are `uint8`
    and `int8` versions of many kernels; the `uint8` versions are remnants of an older
    approach to quantization that is no longer used, and all models should now be
    exported using the `int8` path.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在几乎所有平台上运行完全量化的模型都具有很大的延迟优势，但如果您支持一个新设备，您可能需要优化最计算密集的操作，以利用硬件提供的专门指令。如果您正在处理卷积网络，一个很好的起点是[`Conv2D`操作](https://oreil.ly/NrjSo)和[kernel](https://oreil.ly/V27Q-)。您会注意到许多内核有`uint8`和`int8`版本；`uint8`版本是旧的量化方法的残余物，现在不再使用，所有模型现在都应该使用`int8`路径导出。
- en: Product Design
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 产品设计
- en: You might not think of your product design as a way to optimize latency, but
    it’s actually one of the best places to invest your time. The key is to figure
    out whether you can loosen the requirements on your network, either for speed
    or accuracy. For example, you might want to track hand gestures using a camera
    at many frames per second, but if you have a body pose detection model that takes
    a second to run, you might be able to use a much faster optical tracking algorithm
    to follow the identified points at a higher rate, updating it with the more accurate
    but less frequent neural network results when they’re available. As another example,
    you could have a microcontroller delegate advanced speech recognition to a cloud
    API accessed over a network while keeping wake-word detection running on the local
    device. At a broader level, you might be able to relax the accuracy requirements
    of your network by incorporating uncertainty into the user interface. The wake
    words chosen for speech recognition systems tend to be short phrases that contain
    sequences of syllables that are unlikely to show up in regular speech. If you
    have a hand gesture system, maybe you can require every sequence to end with a
    thumbs-up to confirm the commands were intentional?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不会将产品设计视为优化延迟的一种方式，但实际上这是投入时间的最佳地方之一。关键是要弄清楚你是否可以放宽对网络的要求，无论是速度还是准确性。例如，你可能想使用摄像头以每秒多帧的速度跟踪手势，但如果你有一个需要一秒钟才能运行的身体姿势检测模型，你可能可以使用更快的光学跟踪算法以更高的速率跟踪识别的点，当更准确但不太频繁的神经网络结果可用时进行更新。另一个例子，你可以让微控制器将高级语音识别委托给通过网络访问的云API，同时保持唤醒词检测在本地设备上运行。在更广泛的层面上，你可能可以通过将不确定性纳入用户界面来放宽网络的准确性要求。用于语音识别系统的唤醒词通常是包含不太可能出现在正常语音中的音节序列的短语。如果你有一个手势系统，也许你可以要求每个序列以竖起大拇指结束以确认命令是有意的？
- en: The goal is to provide the best overall user experience you can, so anything
    you can do in the rest of the system to be more forgiving of mistakes gives you
    more room to trade off accuracy for speed or other properties you need to improve.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是提供尽可能好的用户体验，因此在系统的其他部分中做任何可以更容忍错误的事情，可以让你有更多的空间来权衡准确性和速度或其他需要改进的属性。
- en: Code Optimizations
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码优化
- en: We’ve positioned this topic pretty late in the chapter because there are other
    approaches to optimizing latency that you should try first, but traditional code
    optimization is an important way to achieve acceptable performance. In particular,
    the TensorFlow Lite for Microcontrollers code has been written to run well across
    a large number of models and systems with as small a binary footprint as possible,
    so there might well be optimizations that apply only to your particular model
    or platform that you can benefit from adding yourself. This is one of the reasons
    we encourage you to delay code optimization as long as possible, though—many of
    these kinds of changes will not be applicable if you change your hardware platform
    or the model architecture you’re using, so having those things nailed down first
    is essential.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个主题放在章节的最后，因为在优化延迟方面有其他方法是你应该首先尝试的，但传统的代码优化是实现可接受性能的重要途径。特别是，TensorFlow
    Lite for Microcontrollers的代码已经被编写成在尽可能小的二进制占用空间下运行良好，因此可能有一些优化仅适用于你特定的模型或平台，你可以从中受益。这也是我们鼓励你尽可能推迟代码优化的原因之一，因为如果你更改硬件平台或使用的模型架构，许多这类改变可能不适用，因此首先确定这些事项是至关重要的。
- en: Performance Profiling
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能分析
- en: The foundation of any code optimization effort is knowing how long different
    parts of your program take to run. This can be surprisingly difficult to figure
    out in the embedded world because you might not even have a simple timer available
    by default, and even if you do, recording and returning the information you need
    can be demanding. Here’s a variety of approaches we’ve used, ranging from the
    easiest to implement to the trickiest.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 任何代码优化工作的基础是知道程序中不同部分运行所需的时间。在嵌入式世界中，这可能会很难确定，因为你可能甚至没有一个简单的默认计时器，即使有，记录和返回所需的信息也可能很困难。以下是我们使用过的各种方法，从最容易实现到最棘手的。
- en: Blinky
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 闪烁
- en: Almost all embedded development boards have at least one LED that you can control
    from your program. If you’re measuring times that are more than about half a second,
    you can try turning on that LED at the start of the code section that you want
    to measure and then disabling it afterward. You’ll probably be able to roughly
    estimate the time taken using an external stopwatch and manually counting how
    many blinks you see in 10 seconds. You can also have two dev boards side by side
    with different versions of the code, and estimate which one is faster by the comparative
    frequency of the flashes.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的嵌入式开发板上都至少有一个LED可以从程序中控制。如果你要测量超过半秒的时间，可以尝试在你想要测量的代码部分开始时点亮LED，然后在之后关闭它。你可以大致估计花费的时间，使用外部秒表并手动计算在10秒内看到多少次闪烁。你也可以将两个开发板并排放置，分别运行不同版本的代码，通过闪烁的频率来估计哪个更快。
- en: Shotgun profiling
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 散弹式性能分析
- en: After you have a rough idea of how long a normal run of your application is
    taking, the simplest way to estimate how long a particular piece of code is taking
    is to comment it out and see how much faster the overall execution takes. This
    has been called *shotgun profiling* by analogy with shotgun debugging, in which
    you remove large chunks of code in order to locate crashes when little other information
    is available. It can be surprisingly effective for neural network debugging because
    there are typically no data-dependent branches in the model execution code, so
    turning any one operation into a no-op by commenting out its internal implementation
    shouldn’t affect the speed of other parts of the model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在大致了解您的应用程序正常运行需要多长时间后，估计特定代码段需要多长时间的最简单方法是将其注释掉，看整体执行速度提高了多少。这被称为*shotgun profiling*，类比于shotgun
    debugging，其中您删除大块代码以定位崩溃，当其他信息很少时。对于神经网络调试来说，这可能会非常有效，因为模型执行代码中通常没有数据相关分支，因此通过注释掉其内部实现将任何一个操作变为无操作不应该影响模型其他部分的速度。
- en: Debug logging
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调试日志
- en: In most cases you should have the ability to output a line of text back to a
    host computer from your embedded development board, so this might seem an ideal
    way to detect when a piece of code is executing. Unfortunately, the act of communicating
    with the development machine can itself be very time-consuming. [Serial Wire Debug
    output](https://oreil.ly/SdsWk) on an Arm Cortex-M chip can take up to 500 ms,
    with a lot of variability in the latency, which makes it useless for a simplistic
    approach to log profiling. Debug logging based on UART connections is usually
    a lot less expensive, but it’s still not ideal.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您应该能够从嵌入式开发板向主机计算机输出一行文本，因此这似乎是检测代码执行时机的理想方式。不幸的是，与开发机器通信本身可能非常耗时。在Arm
    Cortex-M芯片上，[串行线调试输出](https://oreil.ly/SdsWk)可能需要长达500毫秒的时间，延迟变化很大，这使得它对于简单的日志分析方法毫无用处。基于UART连接的调试日志通常成本较低，但仍不理想。
- en: Logic analyzer
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑分析仪
- en: In a similar manner to toggling LEDs but with a lot more precision, you can
    have your code turn GPIO pins on and off and then use an external logic analyzer
    (we’ve used the [Saleae Logic Pro 16](https://oreil.ly/pig8l) in the past) to
    visualize and measure the duration. This requires a bit of wiring, and the equipment
    itself can be expensive, but it gives a very flexible way to investigate your
    program’s latency without requiring any software support beyond the control of
    one or more GPIO pins.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于切换LED但更精确，您可以让您的代码打开和关闭GPIO引脚，然后使用外部逻辑分析仪（我们过去使用过[Saleae Logic Pro 16](https://oreil.ly/pig8l)）来可视化和测量持续时间。这需要一些布线，设备本身可能很昂贵，但它提供了一种非常灵活的方式来调查程序的延迟，而无需任何软件支持超出一个或多个GPIO引脚的控制。
- en: Timer
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计时器
- en: If you have a timer that can give you a consistent current time with enough
    precision, you can record the time at the start and end of the code section you’re
    interested in and output the duration to logs afterward, where any communication
    latency won’t affect the result. We’ve considered requiring a platform-agnostic
    timer interface in TensorFlow Lite for Microcontrollers for exactly this reason,
    but we decided this would add too much of a burden for people porting to different
    platforms, given that setting up timers can be complicated. Unfortunately this
    means that you’ll need to explore how to implement this functionality yourself
    for the chip you’re running on. There’s also the disadvantage that you need to
    add the timer calls around any code that you want to investigate, so it does require
    work and planning to identify the critical sections, and you’ll need to keep recompiling
    and flashing as you explore where the time is going.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个可以提供足够精度的一致当前时间的计时器，您可以记录您感兴趣的代码部分的开始和结束时的时间，并在之后将持续时间输出到日志中，其中任何通信延迟都不会影响结果。出于这个原因，我们考虑在TensorFlow
    Lite for Microcontrollers中需要一个平台无关的计时器接口，但我们认为这会给那些移植到不同平台的人增加太多负担，因为设置计时器可能会很复杂。不幸的是，这意味着您需要探索如何为您正在运行的芯片实现此功能。还有一个缺点是您需要在您想要调查的任何代码周围添加计时器调用，因此需要工作和计划来识别关键部分，并且您需要在探索时间去向的过程中不断重新编译和刷新。
- en: Profiler
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析器
- en: If you’re lucky, you’ll be working with a toolchain and platform that support
    some kind of external profiling tool. These applications will typically use debug
    information from your program to match statistics on execution that they gather
    from running your program on-device. They will then be able to visualize which
    functions are taking the most time, or even which lines of code. This is the fastest
    way to understand where the speed bottlenecks are in your code because you’ll
    be able to rapidly explore and zoom into the functions that matter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您幸运的话，您将使用支持某种外部分析工具的工具链和平台。这些应用程序通常会使用来自您的程序的调试信息，以匹配他们从设备上运行您的程序时收集的执行统计信息。然后，它们将能够可视化哪些函数花费了最多时间，甚至是哪些代码行。这是了解代码中速度瓶颈所在的最快方式，因为您将能够快速探索和放大到重要的函数。
- en: Optimizing Operations
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化操作
- en: After you’ve ensured that you’re using as simple a model as you can and you’ve
    identified which parts of your code are taking the most time, you should then
    look at what you can do to speed them up. Most of the execution time for neural
    networks should be spent inside operation implementations, given that they can
    involve hundreds of thousands or millions of calculations for each layer, so it’s
    likely that you’ve found one or more of these to be the bottleneck.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在确保您使用尽可能简单的模型并确定哪些代码部分花费了最多时间之后，您应该看看如何加快它们的速度。神经网络的大部分执行时间应该花在操作实现内部，因为每个层可能涉及数十万或数百万次计算，因此很可能您已经发现其中一个或多个是瓶颈。
- en: Look for Implementations That Are Already Optimized
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找已经优化的实现
- en: The default implementations of all operations in TensorFlow Lite for Microcontrollers
    are written to be small, understandable, and portable, not fast, so it’s expected
    that you should be able to beat them fairly easily with an approach that uses
    more lines of code or memory. We do have a set of these faster implementations
    in [the *kernels/portable_optimized directory*](https://oreil.ly/fmY8R), using
    the subfolder specialization approach described in [Chapter 13](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers).
    These implementations shouldn’t have any platform dependencies, but they can use
    more memory than the reference versions. Because they’re using subfolder specialization,
    you can just pass in the `TAGS="portable_optimized"` argument to generate a project
    that uses these rather than the defaults.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite for Microcontrollers 中所有操作的默认实现都是为了小巧、易懂和可移植，而不是快速的，因此预期您应该能够通过使用更多代码行或内存的方法轻松击败它们。我们在[*kernels/portable_optimized
    目录*](https://oreil.ly/fmY8R)中有一组更快的实现，使用了[第13章](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers)中描述的子文件夹专业化方法。这些实现不应该有任何平台依赖性，但它们可能使用比参考版本更多的内存。因为它们使用子文件夹专业化，您只需传递`TAGS="portable_optimized"`参数即可生成一个使用这些实现而不是默认实现的项目。
- en: If you’re using a device that has platform-specific implementations—for example
    through a library like CMSIS-NN—and they aren’t automatically being picked when
    you specify your target, you can choose to use these nonportable versions by passing
    in the appropriate tag. You’ll need to explore your platform’s documentation and
    the TensorFlow Lite for Microcontrollers source tree to find what that is, though.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用具有特定于平台的实现的设备，例如通过类似CMSIS-NN的库，并且在指定目标时它们没有自动选择，您可以选择通过传递适当的标签来使用这些非可移植版本。但是，您需要查阅平台的文档和
    TensorFlow Lite for Microcontrollers 源代码树，以找到相应的内容。
- en: Write Your Own Optimized Implementation
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写您自己的优化实现
- en: If you’ve not been able to find an optimized implementation of the operations
    that are taking the most time or the available implementations aren’t fast enough,
    you might want to write your own. The good news is that you should be able to
    narrow the scope to make that work easier. You’ll only be calling the operations
    with a few different input and output sizes and parameters, so you need to focus
    only on making those paths faster rather than the general case. For example, we
    found that the depthwise convolution reference code was taking up most of the
    time for the first version of the speech wake-word example on the SparkFun Edge
    board, and it was overall running too slowly to be usable. When we looked at what
    the code was doing, we saw that the width of the convolution filters was always
    eight, which made it possible to write [some optimized code that exploited that
    pattern](https://oreil.ly/Kbx22). We could load four input values and four weights
    held in bytes at a time by using 32-bit integers to fetch them in parallel.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您找不到正在占用大部分时间的操作的优化实现，或者可用的实现速度不够快，您可能需要自己编写。好消息是，您应该能够缩小范围，使工作更容易。您只需要调用几种不同的输入和输出大小以及参数的操作，因此您只需要专注于使这些路径更快，而不是一般情况。例如，我们发现深度卷积参考代码在
    SparkFun Edge 开发板上的语音唤醒示例的第一个版本中占用了大部分时间，并且整体运行速度太慢，无法使用。当我们查看代码时，我们发现卷积滤波器的宽度始终为八，这使得可以编写[利用该模式的一些优化代码](https://oreil.ly/Kbx22)。我们可以使用32位整数并行获取四个输入值和四个字节中保存的权重。
- en: 'To start the optimization process, create a new directory inside the *kernels*
    root using the subfolder specialization approach described earlier. Copy the reference
    kernel implementation into that subfolder as a starting point for your code. To
    make sure things are building correctly, run the unit test associated with that
    op and make sure it still passes; if you’re passing in the correct tags, it should
    use the new implementation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始优化过程，请使用前面描述的子文件夹专业化方法在*kernels*根目录中创建一个新目录。将参考内核实现复制到该子文件夹中，作为您代码的起点。为确保构建正确，请运行与该操作相关的单元测试，并确保它仍然通过；如果您传递了正确的标签，它应该使用新的实现：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We then recommend adding a new test to the unit test code for your op—one that
    doesn’t check the correctness but just reports the time taken to execute the operation.
    Having a benchmark like this will help you to verify that your changes are improving
    performance in the way you expect. You should have a benchmark for each scenario
    for which you see a speed bottleneck in your profiling, with the same sizes and
    other parameters that the op has at that point in your model (though the weights
    and inputs can be random values, because in most cases the numbers won’t affect
    the execution latency). The benchmark code itself will need to rely on one of
    the profiling methods discussed earlier in the chapter, ideally using a high-precision
    timer to measure duration, but if not at least toggling an LED or logic output.
    If the granularity of your measurement process is too large, you might need to
    execute the operation multiple times in a loop and then divide by the number of
    iterations to capture the real time taken. After you have your benchmark written,
    make a note of the latency before you’ve made any changes and ensure that it roughly
    matches what you saw from profiling your application.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后建议为您的操作添加一个新的测试到单元测试代码中，该测试不检查正确性，只报告执行操作所需的时间。拥有这样的基准测试将帮助您验证您的更改是否按照您的预期提高了性能。对于您在分析中看到速度瓶颈的每种情况，您应该为每种情况都有一个基准测试，具有与模型中该点的操作相同的大小和其他参数（尽管权重和输入可以是随机值，因为在大多数情况下，数字不会影响执行延迟）。基准测试代码本身将需要依赖本章前面讨论的一种性能分析方法，最好使用高精度计时器来测量持续时间，但如果没有，至少切换LED或逻辑输出。如果您的测量过程的粒度太大，您可能需要在循环中多次执行操作，然后除以迭代次数以捕获实际所需的时间。在编写基准测试后，记录在您进行任何更改之前的延迟，并确保它大致与您从分析应用程序中看到的相匹配。
- en: 'With a representative benchmark available, you should now be able to quickly
    iterate on potential optimizations. A good first step is finding the innermost
    loop of the initial implementation. This is the section of code that will be run
    most frequently, so making improvements to it will have a bigger impact than for
    other parts of the algorithm. You should hopefully be able to identify this by
    looking through the code and literally finding the most deeply nested `for`-loop
    (or equivalent), but it’s worth verifying that you have the appropriate section
    by commenting it out and running the benchmark again. If the latency drops dramatically
    (hopefully by 50% or more), you’ve found the right area to focus on. As an example,
    take this code from the [reference implementation of depthwise convolution](https://oreil.ly/8S4kS):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有了代表性的基准测试数据，现在您应该能够快速迭代潜在的优化。一个很好的第一步是找到初始实现的最内部循环。这是代码中将被最频繁运行的部分，因此对其进行改进将比算法的其他部分产生更大的影响。通过查看代码并找到最深度嵌套的`for`循环（或等效部分），您应该能够识别出这一部分，但值得验证您是否有适当的部分，通过将其注释掉并再次运行基准测试。如果延迟显著下降（希望至少降低50%），则您已经找到了需要关注的正确区域。例如，从[深度卷积的参考实现](https://oreil.ly/8S4kS)中获取这段代码：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Just from examining the indentation, it’s possible to identify the correct
    inner loop as this section:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过检查缩进，就可以确定正确的内部循环如下所示：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code is being executed many more times than the other lines in the function
    by virtue of its position in the middle of all the loops, and commenting it out
    will confirm it’s taking the majority of the time. If you’re lucky enough to have
    line-by-line profiling information, this can help you find the exact section,
    too.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码被执行的次数比函数中的其他行要多得多，这是因为它位于所有循环的中间位置，将其注释掉将确认它占用了大部分时间。如果你有逐行分析信息的幸运，这也可以帮助你找到确切的部分。
- en: Now that you’ve found a high-impact area, the goal is to move as much work as
    you can outside of it to less critical sections. For example, there’s an `if`
    statement in the middle, which means a conditional check must be executed on every
    inner loop iteration, but it’s possible to hoist that work outside of this part
    of the code so that the check is executed much less frequently in an outer loop.
    You might also notice that some conditions or calculations aren’t needed for your
    particular model and benchmark. In the speech wake-word model, the dilation factors
    are always 1, so the multiplications involving them can be skipped, saving more
    work. We recommend that you guard these kind of parameter-specific optimizations
    with a check at the top level, though, and fall back to a plain reference implementation
    if the arguments aren’t what the optimization requires. This allows speedups for
    known models, but ensures that if you have ops that don’t meet these criteria,
    they at least work correctly. To make sure that you don’t accidentally break correctness
    it’s worth running the unit tests for the op frequently, too, as you’re making
    changes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经找到了一个高影响区域，目标是尽可能将更多工作移到不太关键的部分。例如，在中间有一个`if`语句，这意味着在每次内部循环迭代时必须执行条件检查，但可以将这部分工作提升到代码的其他部分，以便在外部循环中更少频繁地执行检查。你可能还会注意到一些条件或计算对于你的特定模型和基准测试是不需要的。在语音唤醒词模型中，扩张因子始终为1，因此涉及它们的乘法可以被跳过，节省更多工作。我们建议您在顶层进行这种参数特定的优化检查，并在参数不符合优化要求时退回到普通的参考实现。这可以加速已知模型，但确保如果您有不符合这些标准的操作，它们至少能正常工作。为了确保您不会意外破坏正确性，值得经常运行操作的单元测试，因为您正在进行更改。
- en: It’s beyond the scope of this book to cover all the ways that you can optimize
    numerical processing code, but you can look at the kernels in the [*portable_optimized*](https://oreil.ly/tQkJm)
    folder to see some of the techniques that can be useful.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的范围超出了覆盖所有优化数值处理代码的方式，但您可以查看[*portable_optimized*](https://oreil.ly/tQkJm)文件夹中的内核，看看一些可能有用的技术。
- en: Taking Advantage of Hardware Features
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用硬件特性
- en: So far we’ve been talking only about portable optimizations that aren’t platform-specific.
    This is because restructuring your code to avoid work entirely is usually the
    easiest way to make a big impact. It also simplifies and narrows the focus of
    more specialized optimizations. You might find yourself on a platform like a Cortex-M
    device with [SIMD instructions](https://oreil.ly/MBxf5), which are often a big
    help for the kinds of repetitive calculations that take up most of the time for
    neural network inference. You’ll be tempted to jump straight into using intrinsics
    or even assembly to rewrite your inner loop, but resist! At least check the documentation
    of the vendor-supplied libraries to see whether there’s something suitable already
    written to implement a larger part of the algorithm, because that will hopefully
    be highly optimized already (though it might miss optimizations you can apply
    knowing your op parameters). If you can, try calling an existing function to calculate
    something common like fast Fourier transform, rather than writing your own version.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了不特定于平台的可移植优化。这是因为重构代码以完全避免工作通常是产生重大影响的最简单方法。它还简化了更专门优化的焦点和范围。您可能会发现自己在像Cortex-M设备这样的平台上，具有[SIMD指令](https://oreil.ly/MBxf5)，这些指令通常对神经网络推断中占用大部分时间的重复计算非常有帮助。您可能会诱惑直接使用内部函数或者甚至汇编来重写内部循环，但要抵制！至少要查看供应商提供的库的文档，看看是否已经有适合的内容来实现算法的较大部分，因为那可能已经高度优化了（尽管可能会错过您可以应用的优化，了解您的操作参数）。如果可以的话，尝试调用现有函数来计算一些常见的东西，比如快速傅立叶变换，而不是编写自己的版本。
- en: If you have worked through these stages, it’s time to experiment with the assembly
    level of your platform. Our recommended approach is to begin by replacing individual
    lines of code with their mechanical equivalents in assembly, one line at a time
    so that you can verify correctness as you go without initially worrying about
    a speedup. After you have the necessary code converted, you can experiment with
    fusing operations and other techniques to reduce the latency. One advantage of
    working with embedded systems is that they tend to be simpler in behavior than
    more complex processors without deep instruction pipelines or caches, so it’s
    a lot more feasible to understand potential performance on paper and establish
    potential assembly-level optimizations without too much risk of unexpected side
    effects.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经完成了这些阶段，那么现在是时候尝试您平台的汇编级别了。我们推荐的方法是从逐行将代码替换为其在汇编中的机械等效物开始，一次替换一行，这样您可以在进行过程中验证正确性，而不必一开始就担心加速。在您转换了必要的代码之后，您可以尝试融合操作和其他技术来减少延迟。与更复杂的处理器相比，嵌入式系统的一个优势是它们的行为通常比较简单，没有深层指令流水线或缓存，因此更容易在纸上理解潜在的性能，并建立潜在的汇编级优化，而不会有太多意外副作用的风险。
- en: Accelerators and Coprocessors
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速器和协处理器
- en: As machine learning workloads become more important in the embedded world, we’re
    seeing more systems emerge that offer specialized hardware to speed them up or
    reduce the power they need. There isn’t a clear programming model or standard
    API for them yet, however, so it’s not always clear how to integrate them with
    a software framework. With TensorFlow Lite for Microcontrollers, we want to support
    direct integration with hardware that works in a synchronous way with the main
    processor, but asynchronous components are beyond the scope of the current project.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习工作负载在嵌入式世界中变得更加重要，我们看到越来越多的系统出现，提供专门的硬件来加速或降低它们所需的功耗。然而，目前还没有明确的编程模型或标准API，因此并不总是清楚如何将它们与软件框架集成。通过TensorFlow
    Lite for Microcontrollers，我们希望支持与主处理器同步工作的硬件的直接集成，但异步组件超出了当前项目的范围。
- en: What we mean by synchronous is that the acceleration hardware is tightly coupled
    to the main CPU, sharing a memory space, and that an operator implementation can
    invoke the accelerator very quickly and will block until the result is returned.
    It’s potentially possible that a threading layer above TensorFlow Lite could assign
    work to another thread or process during this blocking, but that’s unlikely to
    be feasible on most current embedded platforms. From a programmer’s perspective,
    this kind of accelerator looks more like the kind of floating-point coprocessor
    that existed on early x86 systems than the alternative model, which is more like
    a GPU. The reason we’re focused on these kinds of synchronous accelerators is
    that they seem to make the most sense for the low-energy systems that we’re targeting,
    and avoiding asynchronous coordination keeps the runtime much simpler.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的同步是指加速硬件与主CPU紧密耦合，共享内存空间，并且操作员实现可以快速调用加速器，并在结果返回之前阻塞。从程序员的角度来看，这种加速器更像是早期x86系统上存在的浮点协处理器，而不是另一种更像GPU的模型。我们专注于这种同步加速器的原因是它们似乎对我们的低能耗系统最有意义，避免异步协调可以使运行时更简单。
- en: Coprocessor-like accelerators need to be very close to the CPU in the system
    architecture to be able to respond with such low latency. The contrasting model
    is that used by modern GPUs, in which there’s a completely separate system with
    its own control logic on the other end of a bus. Programming these kinds of processors
    involves the CPU queuing up a large list of commands that will take a comparatively
    long time to execute and sending them over as soon as a batch is ready, but immediately
    continuing with other work and not waiting for the accelerator to complete. In
    this model any latency in communication between the CPU and accelerator is insignificant,
    because sending the commands is done infrequently and there’s no blocking on the
    result. Accelerators can benefit from this approach because seeing a lot of commands
    at once gives lots of opportunities to rearrange and optimize the work involved
    in a way that’s difficult when tasks are much more fine-grained and need to be
    executed in order. It’s perfect for graphics rendering because the result never
    needs to return to the CPU at all; the rendered display buffer is simply shown
    to the user. It’s been adapted to deep learning training by sending large batches
    of training samples to ensure that there’s a lot of work to be done at once and
    keeping as much as possible on the card, avoiding copies back to the CPU. As embedded
    systems become more complex and take on larger workloads, we might revisit the
    requirements for the framework and support this flow with something like the delegate
    interface in mobile TensorFlow Lite, but that’s outside of our scope for this
    version of the library.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 类似协处理器的加速器需要与系统架构中的CPU非常接近，才能以如此低的延迟响应。相反的模型是现代GPU所使用的模型，其中有一个完全独立的系统，具有自己的控制逻辑，位于总线的另一端。编程这些类型的处理器涉及CPU排队一长串命令，这些命令需要相对较长的时间来执行，并在批处理准备就绪后立即发送，但立即继续其他工作，不等待加速器完成。在这种模型中，CPU和加速器之间的通信延迟是微不足道的，因为发送命令的频率很低，而且没有等待结果。加速器可以从这种方法中受益，因为一次看到很多命令会提供许多重新排列和优化工作的机会，这在任务更加细粒度且需要按顺序执行时很难做到。这对图形渲染非常适用，因为结果根本不需要返回给CPU；渲染的显示缓冲区只需显示给用户。通过向深度学习训练发送大批量的训练样本，可以确保一次有很多工作要做，并尽可能多地保留在卡上，避免将数据复制回CPU。随着嵌入式系统变得更加复杂并承担更大的工作负载，我们可能会重新审视框架的要求，并通过类似移动版TensorFlow
    Lite中的委托接口来支持这种流程，但这超出了我们当前版本库的范围。
- en: Contributing Back to Open Source
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回馈开源
- en: We’re always keen to see contributions to TensorFlow Lite, and after you’ve
    put effort into optimizing some framework code, you might be interested in sharing
    it back to the mainline. A good place to begin is by joining the [SIG Micro](https://oreil.ly/wrtz-)
    mailing list and sending a quick email summarizing the work you’ve done, together
    with a pointer to a fork of the TensorFlow repository with your proposed changes.
    It helps if you include the benchmark you’re using and some inline documentation
    discussing where the optimization will be helpful. The community should be able
    to offer feedback; they’ll be looking for something that’s possible to build on
    top of, that is generally useful, and that can be maintained and tested. We can’t
    wait to see what you come up with, and thanks for considering open-sourcing your
    improvements!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终热衷于看到对TensorFlow Lite的贡献，当您努力优化一些框架代码后，您可能会有兴趣将其分享回主线。一个很好的开始是加入[SIG Micro](https://oreil.ly/wrtz-)邮件列表，并发送一封简短的电子邮件总结您所做的工作，以及指向带有您提议更改的TensorFlow存储库分支的指针。如果您包括您正在使用的基准测试以及一些内联文档讨论优化将有所帮助的地方，那将会很有帮助。社区应该能够提供反馈；他们将寻找可以在其基础上构建的东西，通常是有用的，并且可以维护和测试。我们迫不及待地想看看您的成果，感谢您考虑开源您的改进！
- en: Wrapping Up
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收尾
- en: In this chapter, we covered the most important things you need to know to speed
    up the execution of your model. The fastest code is code that you don’t run at
    all, so the key thing to remember is to shrink what you’re doing at the model
    and algorithm level before you begin optimizing individual functions. You’ll probably
    need to tackle latency issues before you can get your application working on a
    real device and test that it works the way you intend it to. After that, the next
    priority is likely to be ensuring that your device has the lifetime it needs to
    be useful—and that’s where the next chapter, on optimizing energy use, will be
    useful.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了加快模型执行速度所需了解的最重要的事情。最快的代码是根本不运行的代码，所以要记住的关键是在开始优化单个函数之前，在模型和算法级别缩小您正在进行的工作。您可能需要解决延迟问题，然后才能让您的应用程序在真实设备上运行，并测试它是否按照您的意图工作。之后，下一个优先事项可能是确保您的设备具有足够的寿命以便有用——这就是下一章关于优化能源使用的地方将会有用的地方。
