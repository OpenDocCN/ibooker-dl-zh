<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Classification"><div class="chapter" id="classification_chapter">
<h1><span class="label">Chapter 3. </span>Classification</h1>


<p>In<a data-type="indexterm" data-primary="classification" id="xi_classification353_1"/> <a data-type="xref" href="ch01.html#landscape_chapter">Chapter 1</a> I mentioned that the most common supervised learning tasks are regression (predicting values) and classification (predicting classes). In <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a> we explored a regression task, predicting housing values, using various algorithms such as linear regression, decision trees, and random forests (which will be explained in further detail in later chapters). Now we will turn our attention to classification systems.</p>






<section data-type="sect1" data-pdf-bookmark="MNIST"><div class="sect1" id="id58">
<h1>MNIST</h1>

<p>In<a data-type="indexterm" data-primary="classification" data-secondary="MNIST dataset" id="xi_classificationMNISTdataset383_1"/><a data-type="indexterm" data-primary="Fashion MNIST dataset" id="xi_FashionMNISTdataset383_1"/> this chapter we will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. This set has been studied so much that it is often called the “hello world” of machine learning: whenever people come up with a new classification algorithm they are curious to see how it will perform on MNIST, and anyone who learns machine learning tackles this dataset sooner or later.</p>

<p>Scikit-Learn provides many helper functions to download popular datasets. MNIST is one of them. The following code fetches the MNIST dataset from OpenML.org:⁠<sup><a data-type="noteref" id="id1283-marker" href="ch03.html#id1283">1</a></sup></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_openml</code>

<code class="n">mnist</code> <code class="o">=</code> <code class="n">fetch_openml</code><code class="p">(</code><code class="s1">'mnist_784'</code><code class="p">,</code> <code class="n">as_frame</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code></pre>

<p class="pagebreak-before">The <code translate="no">sklearn.datasets</code> package<a data-type="indexterm" data-primary="sklearn" data-secondary="datasets package" id="id1284"/> contains mostly three types of functions: <code translate="no">fetch_*</code> functions such as <code translate="no">fetch_openml()</code> to download real-life datasets, <code translate="no">load_*</code> functions to load small toy datasets bundled with Scikit-Learn (so they don’t need to be downloaded over the internet), and <code translate="no">make_*</code> functions to generate fake datasets, useful for tests. Generated datasets are usually returned as an <code translate="no">(X, y)</code> tuple containing the input data and the targets, both as NumPy arrays. Other datasets are returned as <code translate="no">sklearn.utils.Bunch</code> objects<a data-type="indexterm" data-primary="sklearn" data-secondary="utils.Bunch objects" id="id1285"/>, which are dictionaries whose entries can also be accessed as attributes. They generally contain the following entries:</p>
<dl>
<dt><code translate="no">"DESCR"</code></dt>
<dd>
<p>A description of the dataset</p>
</dd>
<dt><code translate="no">"data"</code></dt>
<dd>
<p>The input data, usually as a 2D NumPy array</p>
</dd>
<dt><code translate="no">"target"</code></dt>
<dd>
<p>The labels, usually as a 1D NumPy array</p>
</dd>
</dl>

<p>The <code translate="no">fetch_openml()</code><a data-type="indexterm" data-primary="fetch_openml()" id="id1286"/> function is a bit unusual since by default it returns the inputs as a Pandas DataFrame and the labels as a Pandas Series (unless the dataset is sparse). But the MNIST dataset contains images, and DataFrames<a data-type="indexterm" data-primary="DataFrame" id="id1287"/> aren’t ideal for that, so it’s preferable to set <code translate="no">as_frame=False</code> to get the data as NumPy arrays instead. Let’s look at these arrays:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">mnist</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">mnist</code><code class="o">.</code><code class="n">target</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X</code><code class="w"/>
<code class="go">array([[0, 0, 0, ..., 0, 0, 0],</code>
<code class="go">       [0, 0, 0, ..., 0, 0, 0],</code>
<code class="go">       [0, 0, 0, ..., 0, 0, 0],</code>
<code class="go">       ...,</code>
<code class="go">       [0, 0, 0, ..., 0, 0, 0],</code>
<code class="go">       [0, 0, 0, ..., 0, 0, 0],</code>
<code class="go">       [0, 0, 0, ..., 0, 0, 0]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="w"/>
<code class="go">(70000, 784)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y</code><code class="w"/>
<code class="go">array(['5', '0', '4', ..., '4', '5', '6'], dtype=object)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y</code><code class="o">.</code><code class="n">shape</code><code class="w"/>
<code class="go">(70000,)</code></pre>

<p>There are 70,000 images, and each image has 784 features. This is because each image is 28 × 28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black). Let’s take a peek at one digit from the dataset (<a data-type="xref" href="#some_digit_plot">Figure 3-1</a>). All we need to do is grab an instance’s feature vector, reshape it to a 28 × 28 array, and display it using Matplotlib’s <code translate="no">imshow()</code> function. We use <code translate="no">cmap="binary"</code> to get a grayscale color map where 0 is white and 255 is black:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>

<code class="k">def</code> <code class="nf">plot_digit</code><code class="p">(</code><code class="n">image_data</code><code class="p">):</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">image_data</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"binary"</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>

<code class="n">some_digit</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="n">plot_digit</code><code class="p">(</code><code class="n">some_digit</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure><div id="some_digit_plot" class="figure">
<img src="assets/hmls_0301.png" alt="A pixelated grayscale image of a handwritten digit resembling the number 5, from the MNIST dataset." width="731" height="730"/>
<h6><span class="label">Figure 3-1. </span>Example of an MNIST image</h6>
</div></figure>

<p>This looks like a 5, and indeed that’s what the label tells us:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="w"/>
<code class="go">'5'</code></pre>

<p>To give you a feel for the complexity of the classification task, <a data-type="xref" href="#more_digits_plot">Figure 3-2</a> shows a few more images from the MNIST dataset. There’s quite a large variety of digit shapes. That said, the images are clean, well centered, not too rotated, and the digits all have roughly the same size: this dataset will not require much preprocessing (real-world datasets aren’t usually that friendly).</p>

<p>But wait! You should always create a test set and set it aside before inspecting the data closely. The MNIST dataset returned by <code translate="no">fetch_openml()</code> is actually already split into a training set (the first 60,000 images) and a test set (the last 10,000 images):<sup><a data-type="noteref" id="id1288-marker" href="ch03.html#id1288">2</a></sup></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">X</code><code class="p">[:</code><code class="mi">60000</code><code class="p">],</code> <code class="n">X</code><code class="p">[</code><code class="mi">60000</code><code class="p">:],</code> <code class="n">y</code><code class="p">[:</code><code class="mi">60000</code><code class="p">],</code> <code class="n">y</code><code class="p">[</code><code class="mi">60000</code><code class="p">:]</code></pre>

<p>The training set is already shuffled for us, which is good because this guarantees that all cross-validation folds<a data-type="indexterm" data-primary="folds" id="id1289"/> will be similar (we don’t want one fold to be missing some digits). Moreover, some learning algorithms are sensitive to the order of the training instances, and they perform poorly if they get many similar instances in a row. Shuffling<a data-type="indexterm" data-primary="shuffling data" id="id1290"/> the dataset ensures that this won’t happen<a data-type="indexterm" data-startref="xi_classificationMNISTdataset383_1" id="id1291"/><a data-type="indexterm" data-startref="xi_FashionMNISTdataset383_1" id="id1292"/>.⁠<sup><a data-type="noteref" id="id1293-marker" href="ch03.html#id1293">3</a></sup></p>

<figure class="width-75"><div id="more_digits_plot" class="figure">
<img src="assets/hmls_0302.png" alt="Handwritten digits from the MNIST dataset, showcasing a variety of numbers used to train machine learning models." width="1439" height="1450"/>
<h6><span class="label">Figure 3-2. </span>Digits from the MNIST dataset</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Training a Binary Classifier"><div class="sect1" id="id59">
<h1>Training a Binary Classifier</h1>

<p>Let’s<a data-type="indexterm" data-primary="binary classifiers" id="id1294"/><a data-type="indexterm" data-primary="classification" data-secondary="binary classifier" id="id1295"/> simplify the problem for now and only try to identify one digit—for example, the number 5. This “5-detector” will be an example of a <em>binary classifier</em>, capable of distinguishing between just two classes, 5 and non-5. First we’ll create the target vectors for this classification task:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">y_train_5</code> <code class="o">=</code> <code class="p">(</code><code class="n">y_train</code> <code class="o">==</code> <code class="s1">'5'</code><code class="p">)</code>  <code class="c1"># True for all 5s, False for all other digits</code>
<code class="n">y_test_5</code> <code class="o">=</code> <code class="p">(</code><code class="n">y_test</code> <code class="o">==</code> <code class="s1">'5'</code><code class="p">)</code></pre>

<p>Now let’s pick a classifier and train it. A good place to start is with a <em>stochastic gradient descent</em> (SGD, or stochastic GD) classifier, using Scikit-Learn’s <code translate="no">SGDClassifier</code><a data-type="indexterm" data-primary="sklearn" data-secondary="linear_model.SGDClassifier" data-see="SGDClassifier" id="id1296"/><a data-type="indexterm" data-primary="SGDClassifier" id="id1297"/><a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="training binary classifier" id="id1298"/> class. This classifier is capable of handling very large datasets efficiently. This is in part because SGD deals with training instances independently, one at a time, which also makes SGD well suited for online learning, as you will see later. Let’s create an <code translate="no">SGDClassifier</code> and train it on the whole training set:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">SGDClassifier</code>

<code class="n">sgd_clf</code> <code class="o">=</code> <code class="n">SGDClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">sgd_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_5</code><code class="p">)</code></pre>

<p>Now we can use it to detect images of the number 5:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">sgd_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([</code><code class="n">some_digit</code><code class="p">])</code><code class="w"/>
<code class="go">array([ True])</code></pre>

<p>The classifier guesses that this image represents a 5 (<code translate="no">True</code>). Looks like it guessed right in this particular case! Now, let’s evaluate this model’s performance.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Performance Measures"><div class="sect1" id="id60">
<h1>Performance Measures</h1>

<p>Evaluating<a data-type="indexterm" data-primary="classification" data-secondary="performance measures" id="xi_classificationperformancemeasures312611_1"/><a data-type="indexterm" data-primary="performance measures" id="xi_performancemeasures312611_1"/> a classifier is often significantly trickier than evaluating a regressor, so we will spend a large part of this chapter on this topic. There are many performance measures available, so grab another coffee and get ready to learn a bunch of new concepts and acronyms!</p>








<section data-type="sect2" data-pdf-bookmark="Measuring Accuracy Using Cross-Validation"><div class="sect2" id="id61">
<h2>Measuring Accuracy Using Cross-Validation</h2>

<p>A<a data-type="indexterm" data-primary="accuracy performance measure" id="xi_accuracyperformancemeasure31292_1"/> good way to evaluate a model is to use cross-validation, just as you did in <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a>. Let’s use the <code translate="no">cross_val_score()</code> function<a data-type="indexterm" data-primary="cross_val_score()" id="id1299"/> to evaluate our <code translate="no">SGDClassifier</code> model, using <em>k</em>-fold cross-validation with three folds. Remember that <em>k</em>-fold cross-validation means splitting the training set into <em>k</em> folds<a data-type="indexterm" data-primary="folds" id="id1300"/><a data-type="indexterm" data-primary="k-fold cross-validation" id="id1301"/> (in this case, three), then training the model <em>k</em> times, holding out a different fold each time for evaluation (see <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a>)<a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.cross_val_score()" id="id1302"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">cross_val_score</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">cross_val_score</code><code class="p">(</code><code class="n">sgd_clf</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_5</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s2">"accuracy"</code><code class="p">)</code><code class="w"/>
<code class="go">array([0.95035, 0.96035, 0.9604 ])</code></pre>

<p>Wow! Above 95% accuracy (ratio of correct predictions) on all cross-validation folds? This looks amazing, doesn’t it? Well, before you get too excited, let’s look at a dummy classifier that just classifies every single image in the most frequent class, which in this case is the negative class (i.e., <em>non</em>-5):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.dummy</code> <code class="kn">import</code> <code class="n">DummyClassifier</code>

<code class="n">dummy_clf</code> <code class="o">=</code> <code class="n">DummyClassifier</code><code class="p">()</code>
<code class="n">dummy_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_5</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="nb">any</code><code class="p">(</code><code class="n">dummy_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train</code><code class="p">)))</code>  <code class="c1"># prints False: no 5s detected</code></pre>

<p>Can you guess this model’s accuracy? Let’s find out:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">cross_val_score</code><code class="p">(</code><code class="n">dummy_clf</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_5</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s2">"accuracy"</code><code class="p">)</code><code class="w"/>
<code class="go">array([0.90965, 0.90965, 0.90965])</code></pre>

<p>That’s right, it has over 90% accuracy! This is simply because only about 10% of the images are 5s, so if you always guess that an image is <em>not</em> a 5, you will be right about 90% of the time. Beats Nostradamus.</p>

<p>This demonstrates why accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with <em>skewed datasets</em> <a data-type="indexterm" data-primary="skewed datasets" id="id1303"/>(i.e., when some classes are much more frequent than others). A much better way to evaluate the performance of a classifier is to look at the <em>confusion matrix</em> (CM).<a data-type="indexterm" data-startref="xi_accuracyperformancemeasure31292_1" id="id1304"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1305">
<h1>Implementing Cross-Validation</h1>
<p>Occasionally<a data-type="indexterm" data-primary="cross-validation" id="id1306"/><a data-type="indexterm" data-primary="performance measures" data-secondary="cross-validation to measure accuracy" id="id1307"/><a data-type="indexterm" data-primary="predictions" data-secondary="cross-validation to measure accuracy" id="id1308"/> you will need more control over the cross-validation process than what Scikit-Learn provides off the shelf. In these cases, you can implement cross-validation yourself. The following code does roughly the same thing as Scikit-Learn’s <code translate="no">cross_val_score()</code><a data-type="indexterm" data-primary="k-fold cross-validation" id="id1309"/> function, and it prints the same result<a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.StratifiedKFold" id="id1310"/><a data-type="indexterm" data-primary="StratifiedKFold" id="id1311"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">StratifiedKFold</code>
<code class="kn">from</code> <code class="nn">sklearn.base</code> <code class="kn">import</code> <code class="n">clone</code>

<code class="n">skfolds</code> <code class="o">=</code> <code class="n">StratifiedKFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>  <code class="c1"># add shuffle=True if the dataset is</code>
                                       <code class="c1"># not already shuffled</code>
<code class="k">for</code> <code class="n">train_index</code><code class="p">,</code> <code class="n">test_index</code> <code class="ow">in</code> <code class="n">skfolds</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_5</code><code class="p">):</code>
    <code class="n">clone_clf</code> <code class="o">=</code> <code class="n">clone</code><code class="p">(</code><code class="n">sgd_clf</code><code class="p">)</code>
    <code class="n">X_train_folds</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[</code><code class="n">train_index</code><code class="p">]</code>
    <code class="n">y_train_folds</code> <code class="o">=</code> <code class="n">y_train_5</code><code class="p">[</code><code class="n">train_index</code><code class="p">]</code>
    <code class="n">X_test_fold</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[</code><code class="n">test_index</code><code class="p">]</code>
    <code class="n">y_test_fold</code> <code class="o">=</code> <code class="n">y_train_5</code><code class="p">[</code><code class="n">test_index</code><code class="p">]</code>

    <code class="n">clone_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_folds</code><code class="p">,</code> <code class="n">y_train_folds</code><code class="p">)</code>
    <code class="n">y_pred</code> <code class="o">=</code> <code class="n">clone_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test_fold</code><code class="p">)</code>
    <code class="n">n_correct</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="n">y_pred</code> <code class="o">==</code> <code class="n">y_test_fold</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">n_correct</code> <code class="o">/</code> <code class="nb">len</code><code class="p">(</code><code class="n">y_pred</code><code class="p">))</code>  <code class="c1"># prints 0.95035, 0.96035, and 0.9604</code></pre>

<p>The <code translate="no">StratifiedKFold</code> class performs stratified sampling<a data-type="indexterm" data-primary="stratified sampling" id="id1312"/> (as explained in <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a>) to produce folds<a data-type="indexterm" data-primary="folds" id="id1313"/> that contain a representative ratio of each class. At each iteration the code creates a clone of the classifier, trains that clone on the training folds, and makes predictions on the test fold. Then it counts the number of correct predictions and outputs the ratio of correct predictions.</p>
</div></aside>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Confusion Matrices"><div class="sect2" id="id62">
<h2>Confusion Matrices</h2>

<p>The<a data-type="indexterm" data-primary="confusion matrix (CM)" id="xi_confusionmatrixCM31914_1"/><a data-type="indexterm" data-primary="performance measures" data-secondary="confusion matrix" id="xi_performancemeasuresconfusionmatrix31914_1"/><a data-type="indexterm" data-primary="predictions" data-secondary="confusion matrix" id="xi_predictionsconfusionmatrix31914_1"/><a data-type="indexterm" data-primary="CM (confusion matrix)" id="xi_CMconfusionmatrix31914_1"/> general idea of a confusion matrix is to count the number of times instances of class A are classified as class B, for all A/B pairs. For example, to know the number of times the classifier confused images of 8s with 0s, you would look at row #8, column #0 of the confusion matrix.</p>

<p>To compute the confusion matrix, you first need to have a set of predictions so that they can be compared to the actual targets. You could make predictions on the test set, but it’s best to keep that untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the <code translate="no">cross_val_predict()</code> function<a data-type="indexterm" data-primary="cross_val_predict()" id="id1314"/><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.cross_val_predict()" id="id1315"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">cross_val_predict</code>

<code class="n">y_train_pred</code> <code class="o">=</code> <code class="n">cross_val_predict</code><code class="p">(</code><code class="n">sgd_clf</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_5</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code></pre>

<p>Just like the <code translate="no">cross_val_score()</code> function, <code translate="no">cross_val_predict()</code> performs <em>k</em>-fold cross-validation, but instead of returning the evaluation scores, it returns the predictions made on each test fold. This means that you get a clean prediction for each instance in the training set (by “clean” I mean “out-of-sample”: the model makes predictions on data that it never saw during training).</p>

<p>Now you are ready to get the confusion matrix using the <code translate="no">confusion_matrix()</code> function. Just pass it the target classes (<code translate="no">y_train_5</code>) and the predicted classes (<code translate="no">y_train_pred</code>)<a data-type="indexterm" data-primary="sklearn" data-secondary="metrics.confusion_matrix()" id="id1316"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">confusion_matrix</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm</code> <code class="o">=</code> <code class="n">confusion_matrix</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_train_pred</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm</code><code class="w"/>
<code class="go">array([[53892,   687],</code>
<code class="go">       [ 1891,  3530]])</code></pre>

<p>Each row in a confusion matrix represents an <em>actual class</em><a data-type="indexterm" data-primary="actual versus predicted class" id="id1317"/>, while each column represents a <em>predicted class</em>.<a data-type="indexterm" data-primary="predicted class" id="id1318"/> The first row of this matrix considers non-5 images (the <em>negative class</em>):<a data-type="indexterm" data-primary="negative class" id="id1319"/> 53,892 of them were correctly classified as non-5s (they are called <em>true negatives</em>),<a data-type="indexterm" data-primary="true negatives, confusion matrix" id="id1320"/> while the remaining 687 were wrongly classified as 5s (<em>false positives</em><a data-type="indexterm" data-primary="false positives, confusion matrix" id="id1321"/>, also called <em>type I errors</em>).<a data-type="indexterm" data-primary="type I errors, confusion matrix" id="id1322"/> The second row considers the images of 5s (the <em>positive class</em>):<a data-type="indexterm" data-primary="positive class" id="id1323"/> 1,891 were wrongly classified as non-5s (<em>false negatives</em><a data-type="indexterm" data-primary="false negatives, confusion matrix" id="id1324"/>, also called <em>type II errors</em>),<a data-type="indexterm" data-primary="type II errors, confusion matrix" id="id1325"/> while the remaining 3,530 were correctly classified as 5s (<em>true positives</em>).<a data-type="indexterm" data-primary="true positives, confusion matrix" id="id1326"/> A perfect classifier would only have true positives and true negatives, so its confusion matrix would have nonzero values only on its main diagonal (top left to bottom right):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_train_perfect_predictions</code> <code class="o">=</code> <code class="n">y_train_5</code>  <code class="c1"># pretend we reached perfection</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">confusion_matrix</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_train_perfect_predictions</code><code class="p">)</code><code class="w"/>
<code class="go">array([[54579,     0],</code>
<code class="go">       [    0,  5421]])</code></pre>

<p>The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive predictions; this is called the <em>precision</em> of the classifier (<a data-type="xref" href="#equation_three_one">Equation 3-1</a>).</p>
<div id="equation_three_one" data-type="equation" class="fifty-percent">
<h5><span class="label">Equation 3-1. </span>Precision</h5>
<math alttext="precision equals StartFraction upper T upper P Over upper T upper P plus upper F upper P EndFraction" display="block">
  <mrow>
    <mtext>precision</mtext>
    <mo>=</mo>
    <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac>
  </mrow>
</math>
</div>

<p><em>TP</em> is the number of true positives, and <em>FP</em> is the number of false positives.</p>

<p>Now consider a model that only makes positive predictions when it’s extremely confident. Let’s push this to the extreme and suppose that it always makes negative predictions, except for a single positive prediction on the instance it’s most confident about. If this one prediction is correct, then the classifier has 100% precision (precision = 1/1 = 100%)<a data-type="indexterm" data-primary="performance measures" data-secondary="precision and recall" id="xi_performancemeasuresprecisionandrecall3241357_1"/><a data-type="indexterm" data-primary="precision and recall, classifier metrics" id="xi_precisionandrecallclassifiermetrics3241357_1"/>. Obviously, such a classifier would not be very useful, since it would ignore all but one positive instance. For this reason, precision is typically used along with another metric named <em>recall</em>,<a data-type="indexterm" data-primary="recall metric" id="id1327"/> also called <em>sensitivity</em><a data-type="indexterm" data-primary="sensitivity metric" id="id1328"/> or the <em>true positive rate</em> (TPR):<a data-type="indexterm" data-primary="TPR (true positive rate)" id="id1329"/><a data-type="indexterm" data-primary="true positive rate (TPR)" id="id1330"/> this is the ratio of positive instances that are correctly detected by the classifier (<a data-type="xref" href="#equation_three_two">Equation 3-2</a>).</p>
<div id="equation_three_two" data-type="equation" class="fifty-percent">
<h5><span class="label">Equation 3-2. </span>Recall</h5>
<math alttext="recall equals StartFraction upper T upper P Over upper T upper P plus upper F upper N EndFraction" display="block">
  <mrow>
    <mtext>recall</mtext>
    <mo>=</mo>
    <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac>
  </mrow>
</math>
</div>

<p><em>FN</em> is, of course, the number of false negatives.</p>

<p>If you are confused about the confusion matrix, <a data-type="xref" href="#confusion_matrix_diagram">Figure 3-3</a> may help.<a data-type="indexterm" data-startref="xi_confusionmatrixCM31914_1" id="id1331"/><a data-type="indexterm" data-startref="xi_performancemeasuresconfusionmatrix31914_1" id="id1332"/><a data-type="indexterm" data-startref="xi_predictionsconfusionmatrix31914_1" id="id1333"/><a data-type="indexterm" data-startref="xi_CMconfusionmatrix31914_1" id="id1334"/></p>

<figure><div id="confusion_matrix_diagram" class="figure">
<img src="assets/hmls_0303.png" alt="Illustrated confusion matrix with examples of true negatives, false positives, false negatives, and true positives, used to explain prediction performance." width="1434" height="766"/>
<h6><span class="label">Figure 3-3. </span>An illustrated confusion matrix showing examples of true negatives (top left), false positives (top right), false negatives (lower left), and true positives (lower right)</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Precision and Recall"><div class="sect2" id="id63">
<h2>Precision and Recall</h2>

<p>Scikit-Learn provides several functions to compute classifier metrics, including  precision and recall:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">precision_score</code><code class="p">,</code> <code class="n">recall_score</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">precision_score</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_train_pred</code><code class="p">)</code>  <code class="c1"># == 3530 / (687 + 3530)</code><code class="w"/>
<code class="go">np.float64(0.8370879772350012)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">recall_score</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_train_pred</code><code class="p">)</code>  <code class="c1"># == 3530 / (1891 + 3530)</code><code class="w"/>
<code class="go">np.float64(0.6511713705958311)</code></pre>

<p>Now our 5-detector does not look as shiny as it did when we looked at its accuracy. When it claims an image represents a 5, it is correct only 83.7% of the time. Moreover, it only detects 65.1% of the 5s.</p>

<p>It is often convenient to combine precision and recall into a single metric called the <em>F<sub>1</sub> score</em>,<a data-type="indexterm" data-primary="F1 score" id="id1335"/> especially when you need a single metric to compare two classifiers. The F<sub>1</sub> score is the <em>harmonic mean</em><a data-type="indexterm" data-primary="harmonic mean" id="id1336"/> of precision and recall (<a data-type="xref" href="#equation_three_three">Equation 3-3</a>). Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F<sub>1</sub> score if both recall and precision are high.</p>
<div data-type="equation" id="equation_three_three">
<h5><span class="label">Equation 3-3. </span>F<sub>1</sub> score</h5>
<math alttext="upper F 1 equals StartStartFraction 2 OverOver StartFraction 1 Over precision EndFraction plus StartFraction 1 Over recall EndFraction EndEndFraction equals 2 times StartFraction precision times recall Over precision plus recall EndFraction equals StartStartFraction upper T upper P OverOver upper T upper P plus StartFraction upper F upper N plus upper F upper P Over 2 EndFraction EndEndFraction">
  <mrow>
    <msub><mi>F</mi> <mn>1</mn> </msub>
    <mo>=</mo>
    <mfrac><mn>2</mn> <mrow><mfrac><mn>1</mn> <mtext>precision</mtext></mfrac><mo>+</mo><mfrac><mn>1</mn> <mtext>recall</mtext></mfrac></mrow></mfrac>
    <mo>=</mo>
    <mn>2</mn>
    <mo>×</mo>
    <mfrac><mrow><mtext>precision</mtext><mspace width="0.166667em"/><mo>×</mo><mspace width="0.166667em"/><mtext>recall</mtext></mrow> <mrow><mtext>precision</mtext><mspace width="0.166667em"/><mo>+</mo><mspace width="0.166667em"/><mtext>recall</mtext></mrow></mfrac>
    <mo>=</mo>
    <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mfrac><mrow><mi>F</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow> <mn>2</mn></mfrac></mrow></mfrac>
  </mrow>
</math>
</div>

<p>To compute the F<sub>1</sub> score, simply call the <code translate="no">f1_score()</code> function<a data-type="indexterm" data-primary="f1_score()" id="id1337"/><a data-type="indexterm" data-primary="sklearn" data-secondary="metrics.f1_score()" id="id1338"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">f1_score</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">f1_score</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_train_pred</code><code class="p">)</code><code class="w"/>
<code class="go">0.7325171197343846</code></pre>

<p>The F<sub>1</sub> score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall. For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a classifier that has a much higher recall but lets a few really bad videos show up in your product (in such cases, you may even want to add a human pipeline to check the classifier’s video selection). On the other hand, suppose you train a classifier to detect shoplifters in surveillance images: it is probably fine if your classifier only has 30% precision as long as it has 99% recall. Sure, the security guards will get a few false alerts, but almost all shoplifters will get caught. Similarly, medical diagnosis usually requires a high recall to avoid missing anything important. False positives can be ruled out by follow-up medical tests.</p>

<p>Unfortunately, you can’t have it both ways: increasing precision reduces recall, and vice versa. This is called the <em>precision/recall trade-off</em>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The Precision/Recall Trade-Off"><div class="sect2" id="id64">
<h2>The Precision/Recall Trade-Off</h2>

<p>To<a data-type="indexterm" data-primary="precision/recall tradeoff" id="xi_precisionrecalltradeoff33003_1"/> understand this trade-off, let’s look at how the <code translate="no">SGDClassifier</code><a data-type="indexterm" data-primary="SGDClassifier" data-secondary="classification decisions" id="id1339"/> makes its classification decisions. For each instance, it computes a score based on a <em>decision function</em><a data-type="indexterm" data-primary="decision function" id="id1340"/>. If that score is greater than a threshold<a data-type="indexterm" data-primary="decision threshold" id="xi_decisionthreshold3300217_1"/>, it assigns the instance to the positive class; otherwise it assigns it to the negative class. <a data-type="xref" href="#decision_threshold_diagram">Figure 3-4</a> shows a few digits positioned from the lowest score on the left to the highest score on the right. Suppose the <em>decision threshold</em> is positioned at the central arrow (between the two 5s): you will find 4 true positives (actual 5s) on the right of that threshold, and 1 false positive (actually a 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). If you raise the threshold (move it to the arrow on the right), the false positive (the 6) becomes a true negative, thereby increasing the precision (up to 100% in this case), but one true positive becomes a false negative, decreasing recall down to 50%. Conversely, lowering the threshold increases recall and reduces precision.</p>

<figure><div id="decision_threshold_diagram" class="figure">
<img src="assets/hmls_0304.png" alt="Diagram illustrating the precision/recall trade-off with different decision thresholds; as the threshold increases, precision generally increases while recall decreases." width="1361" height="465"/>
<h6><span class="label">Figure 3-4. </span>The precision/recall trade-off: images are ranked by their classifier score, and those above the chosen decision threshold are considered positive; the higher the threshold, the lower the recall, but (in general) the higher the precision</h6>
</div></figure>

<p>Instead of calling the classifier’s <code translate="no">predict()</code> method, you can call its <code translate="no">decision_function()</code> method<a data-type="indexterm" data-primary="decision_function()" id="id1341"/>, which returns a score for each instance. You can then use any threshold you want to make predictions based on those scores:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_scores</code> <code class="o">=</code> <code class="n">sgd_clf</code><code class="o">.</code><code class="n">decision_function</code><code class="p">([</code><code class="n">some_digit</code><code class="p">])</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_scores</code><code class="w"/>
<code class="go">array([2164.22030239])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">threshold</code> <code class="o">=</code> <code class="mi">0</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_some_digit_pred</code> <code class="o">=</code> <code class="p">(</code><code class="n">y_scores</code> <code class="o">&gt;</code> <code class="n">threshold</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_some_digit_pred</code><code class="w"/>
<code class="go">array([ True])</code></pre>

<p>The <code translate="no">SGDClassifier</code><a data-type="indexterm" data-primary="SGDClassifier" data-secondary="threshold" id="id1342"/> uses a threshold equal to 0, so the preceding code returns the same result as the <code translate="no">predict()</code> method (i.e., <code translate="no">True</code>). Let’s raise the threshold:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">threshold</code> <code class="o">=</code> <code class="mi">3000</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_some_digit_pred</code> <code class="o">=</code> <code class="p">(</code><code class="n">y_scores</code> <code class="o">&gt;</code> <code class="n">threshold</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_some_digit_pred</code><code class="w"/>
<code class="go">array([False])</code></pre>

<p class="pagebreak-before">This confirms that raising the threshold decreases recall. The image actually represents a 5, and the classifier detects it when the threshold is 0, but it misses it when the threshold is increased to 3,000.</p>

<p>How do you decide which threshold to use? One option is to use the <code translate="no">cross_val_predict()</code><a data-type="indexterm" data-primary="cross_val_predict()" id="id1343"/> <a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.cross_val_predict()" id="id1344"/>function to get the scores of all instances in the training set, but this time specify that you want to return decision scores instead of predictions:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">y_scores</code> <code class="o">=</code> <code class="n">cross_val_predict</code><code class="p">(</code><code class="n">sgd_clf</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_5</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
                             <code class="n">method</code><code class="o">=</code><code class="s2">"decision_function"</code><code class="p">)</code></pre>

<p>With these scores, use the <code translate="no">precision_recall_curve()</code> function<a data-type="indexterm" data-primary="sklearn" data-secondary="metrics.precision_recall_curve()" id="id1345"/> to compute precision and recall for all possible thresholds (the function adds a last precision of 1 and a last recall of 0, corresponding to an infinite threshold):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">precision_recall_curve</code>

<code class="n">precisions</code><code class="p">,</code> <code class="n">recalls</code><code class="p">,</code> <code class="n">thresholds</code> <code class="o">=</code> <code class="n">precision_recall_curve</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_scores</code><code class="p">)</code></pre>

<p>Finally, use Matplotlib to plot precision and recall as functions of the threshold value (<a data-type="xref" href="#precision_recall_vs_threshold_plot">Figure 3-5</a>). Let’s show the threshold of 3,000 we selected:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">thresholds</code><code class="p">,</code> <code class="n">precisions</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">],</code> <code class="s2">"b--"</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Precision"</code><code class="p">,</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">thresholds</code><code class="p">,</code> <code class="n">recalls</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">],</code> <code class="s2">"g-"</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Recall"</code><code class="p">,</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">vlines</code><code class="p">(</code><code class="n">threshold</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="s2">"k"</code><code class="p">,</code> <code class="s2">"dotted"</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"threshold"</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># beautify the figure: add grid, legend, axis, labels, and circles</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure><div id="precision_recall_vs_threshold_plot" class="figure">
<img src="assets/hmls_0305.png" alt="Graph showing precision and recall curves as functions of decision threshold, illustrating their inverse relationship." width="2310" height="1095"/>
<h6><span class="label">Figure 3-5. </span>Precision and recall versus the decision threshold</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You may wonder why the precision curve<a data-type="indexterm" data-primary="precision/recall curve (PR)" id="id1346"/> is bumpier than the recall curve in <a data-type="xref" href="#precision_recall_vs_threshold_plot">Figure 3-5</a>. The reason is that precision may sometimes go down when you raise the threshold (although in general it will go up). To understand why, look back at <a data-type="xref" href="#decision_threshold_diagram">Figure 3-4</a> and notice what happens when you start from the central threshold and move it just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%). On the other hand, recall can only go down when the threshold is increased, which explains why its curve looks smooth.</p>
</div>

<p>At this threshold value, precision is near 90% and recall is around 50%. Another way to select a good precision/recall trade-off is to plot precision directly against recall, as shown in <a data-type="xref" href="#precision_vs_recall_plot">Figure 3-6</a> (the same threshold is shown):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">recalls</code><code class="p">,</code> <code class="n">precisions</code><code class="p">,</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Precision/Recall curve"</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># beautify the figure: add labels, grid, legend, arrow, and text</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure class="width-75"><div id="precision_vs_recall_plot" class="figure">
<img src="assets/hmls_0306.png" alt="Graph showing the precision-recall curve with a marked point where precision drops sharply as recall approaches 80%." width="1700" height="1395"/>
<h6><span class="label">Figure 3-6. </span>Precision versus recall</h6>
</div></figure>

<p>You can see that precision really starts to fall sharply at around 80% recall. You will probably want to select a precision/recall trade-off just before that drop—for example, at around 60% recall. But of course, the choice depends on your project.</p>

<p>Suppose you decide to aim for 90% precision. You could use the first plot to find the threshold you need to use, but that’s not very precise. Alternatively, you can search for the lowest threshold that gives you at least 90% precision. For this, you can use the NumPy array’s <code translate="no">argmax()</code><a data-type="indexterm" data-primary="argmax()" id="id1347"/> method. This returns the first index of the maximum value, which in this case means the first <code translate="no">True</code> value:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">idx_for_90_precision</code> <code class="o">=</code> <code class="p">(</code><code class="n">precisions</code> <code class="o">&gt;=</code> <code class="mf">0.90</code><code class="p">)</code><code class="o">.</code><code class="n">argmax</code><code class="p">()</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">threshold_for_90_precision</code> <code class="o">=</code> <code class="n">thresholds</code><code class="p">[</code><code class="n">idx_for_90_precision</code><code class="p">]</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">threshold_for_90_precision</code><code class="w"/>
<code class="go">np.float64(3370.0194991439557)</code></pre>

<p>To make predictions (on the training set for now), instead of calling the classifier’s <code translate="no">predict()</code> method, you can run this code:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">y_train_pred_90</code> <code class="o">=</code> <code class="p">(</code><code class="n">y_scores</code> <code class="o">&gt;=</code> <code class="n">threshold_for_90_precision</code><code class="p">)</code></pre>

<p>Let’s check these predictions’ precision and recall:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">precision_score</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_train_pred_90</code><code class="p">)</code><code class="w"/>
<code class="go">0.9000345901072293</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">recall_at_90_precision</code> <code class="o">=</code> <code class="n">recall_score</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_train_pred_90</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">recall_at_90_precision</code><code class="w"/>
<code class="go">0.4799852425751706</code></pre>

<p>Great, you have a 90% precision classifier! As you can see, it is fairly easy to create a classifier with virtually any precision you want: just set a high enough threshold, and you’re done. But wait, not so fast: a high-precision classifier is not very useful if its recall is too low! For many applications, 48% recall wouldn’t be great at all.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If someone says, “Let’s reach 99% precision”, you should ask, “At what recall?”</p>
</div>

<p>Since Scikit-Learn 1.5, there are two new classes you can use to more easily adjust the decision threshold:</p>

<ul>
<li>
<p>The <code translate="no">FixedThresholdClassifier</code><a data-type="indexterm" data-primary="FixedThresholdClassifier" id="id1348"/><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.FixedThresholdClassifier" id="id1349"/> class lets you wrap a binary classifier and set the desired threshold manually. If the underlying classifier has a <code translate="no">predict_proba()</code> method, then the threshold should be a value between 0 and 1 (the default is 0.5). Otherwise, it should be a decision score, comparable to the output of the model’s <code translate="no">decision_function()</code> (the default is 0).</p>
</li>
<li>
<p>The <code translate="no">TunedThresholdClassifierCV</code><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.TunedThresholdClassifierCV" id="id1350"/><a data-type="indexterm" data-primary="TunedThresholdClassifierCV" id="id1351"/> class uses <em>k</em>-fold cross-validation to automatically find the optimal threshold for a given metric. By default, it tries to find the threshold that maximizes the model’s <em>balanced accuracy</em>: that’s the average of each class’s recall. However, you can select another metric to optimize for (see the documentation for the full list of options).<a data-type="indexterm" data-startref="xi_decisionthreshold3300217_1" id="id1352"/><a data-type="indexterm" data-startref="xi_performancemeasuresprecisionandrecall3241357_1" id="id1353"/><a data-type="indexterm" data-startref="xi_precisionandrecallclassifiermetrics3241357_1" id="id1354"/><a data-type="indexterm" data-startref="xi_precisionrecalltradeoff33003_1" id="id1355"/></p>
</li>
</ul>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The ROC Curve"><div class="sect2" id="id65">
<h2>The ROC Curve</h2>

<p>The<a data-type="indexterm" data-primary="performance measures" data-secondary="ROC curve" id="xi_performancemeasuresROCcurve34224_1"/><a data-type="indexterm" data-primary="receiver operating characteristic (ROC) curve" id="xi_receiveroperatingcharacteristicROCcurve34224_1"/><a data-type="indexterm" data-primary="ROC (receiver operating characteristic) curve" id="xi_ROCreceiveroperatingcharacteristiccurve34224_1"/> <em>receiver operating characteristic</em> (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the <em>true positive rate</em><a data-type="indexterm" data-primary="TPR (true positive rate)" id="id1356"/><a data-type="indexterm" data-primary="true positive rate (TPR)" id="id1357"/> (another name for recall) against the <em>false positive rate</em> (FPR). The FPR<a data-type="indexterm" data-primary="false positive rate (FPR) or fall-out" id="id1358"/><a data-type="indexterm" data-primary="FPR (false positive rate) or fall-out" id="id1359"/> (also called the <em>fall-out</em>) is the ratio of negative instances that are incorrectly classified as positive. It is equal to 1 – the <em>true negative rate</em> (TNR), <a data-type="indexterm" data-primary="TNR (true negative rate)" id="id1360"/><a data-type="indexterm" data-primary="true negative rate (TNR)" id="id1361"/>which is the ratio of negative instances that are correctly classified as negative. The TNR is also called <em>specificity</em>.<a data-type="indexterm" data-primary="specificity, ROC curve" id="id1362"/> Hence, the ROC curve plots <em>sensitivity</em><a data-type="indexterm" data-primary="sensitivity (recall), ROC curve" id="id1363"/> (recall) versus <span class="keep-together">1 – <em>specificity</em></span>.</p>

<p>To plot the ROC curve, you first use the <code translate="no">roc_curve()</code> function to compute the TPR and FPR for various threshold values<a data-type="indexterm" data-primary="sklearn" data-secondary="metrics.roc_curve()" id="id1364"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">roc_curve</code>

<code class="n">fpr</code><code class="p">,</code> <code class="n">tpr</code><code class="p">,</code> <code class="n">thresholds</code> <code class="o">=</code> <code class="n">roc_curve</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_scores</code><code class="p">)</code></pre>

<p>Then you can plot the FPR against the TPR using Matplotlib. The following code produces the plot in <a data-type="xref" href="#roc_curve_plot">Figure 3-7</a>. To find the point that corresponds to 90% precision, we need to look for the index of the desired threshold. Since thresholds are listed in decreasing order in this case, we use <code translate="no">&lt;=</code> instead of <code translate="no">&gt;=</code> on the first line:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">idx_for_threshold_at_90</code> <code class="o">=</code> <code class="p">(</code><code class="n">thresholds</code> <code class="o">&lt;=</code> <code class="n">threshold_for_90_precision</code><code class="p">)</code><code class="o">.</code><code class="n">argmax</code><code class="p">()</code>
<code class="n">tpr_90</code><code class="p">,</code> <code class="n">fpr_90</code> <code class="o">=</code> <code class="n">tpr</code><code class="p">[</code><code class="n">idx_for_threshold_at_90</code><code class="p">],</code> <code class="n">fpr</code><code class="p">[</code><code class="n">idx_for_threshold_at_90</code><code class="p">]</code>

<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">fpr</code><code class="p">,</code> <code class="n">tpr</code><code class="p">,</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"ROC curve"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="s1">'k:'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Random classifier's ROC curve"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">([</code><code class="n">fpr_90</code><code class="p">],</code> <code class="p">[</code><code class="n">tpr_90</code><code class="p">],</code> <code class="s2">"ko"</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Threshold for 90% precision"</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># beautify the figure: add labels, grid, legend, arrow, and text</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p>Once again there is a trade-off: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).</p>

<p>One way to compare classifiers is to measure the <em>area under the curve</em> (AUC).<a data-type="indexterm" data-primary="area under the curve (AUC)" id="id1365"/><a data-type="indexterm" data-primary="AUC (area under the curve)" id="id1366"/> A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to estimate the ROC AUC:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">roc_auc_score</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">roc_auc_score</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_scores</code><code class="p">)</code><code class="w"/>
<code class="go">np.float64(0.9604938554008616)</code></pre>

<figure class="width-75"><div id="roc_curve_plot" class="figure">
<img src="assets/hmls_0307.png" alt="ROC curve showing the true positive rate versus the false positive rate, with a highlighted point indicating a threshold for 90% precision and 48% recall." width="1706" height="1403"/>
<h6><span class="label">Figure 3-7. </span>A ROC curve plotting the false positive rate against the true positive rate for all possible thresholds; the black circle highlights the chosen ratio (at 90% precision and 48% recall)</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>Since the ROC curve is so similar to the precision/recall (PR) curve<a data-type="indexterm" data-primary="precision/recall curve (PR)" id="id1367"/>, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives. Otherwise, use the ROC curve. For example, looking at the previous ROC curve (and the ROC AUC score), you may think that the classifier is really good. But this is mostly because there are few positives (5s) compared to the negatives (non-5s). In contrast, the PR curve makes it clear that the classifier has room for improvement: the curve could really be closer to the top-right corner (see <a data-type="xref" href="#precision_vs_recall_plot">Figure 3-6</a> again).</p>
</div>

<p>Let’s now create a <code translate="no">RandomForestClassifier</code><a data-type="indexterm" data-primary="RandomForestClassifier" id="xi_RandomForestClassifier346844_1"/><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.RandomForestClassifier" id="xi_ScikitLearnsklearnensembleRandomForestClassifier346844_1"/><a data-type="indexterm" data-primary="SGDClassifier" data-secondary="RandomForest classifier compared to" id="xi_SGDClassifierRandomForestclassifiercomparedto346844_1"/>, whose PR curve and F<sub>1</sub> score we can compare to those of the <code translate="no">SGDClassifier</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>

<code class="n">forest_clf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code></pre>

<p>The <code translate="no">precision_recall_curve()</code> function<a data-type="indexterm" data-primary="sklearn" data-secondary="metrics.precision_recall_curve()" id="id1368"/> expects labels and scores for each instance, so we need to train the random forest classifier and make it assign a score to each instance. But the <code translate="no">RandomForestClassifier</code> class does not have a 
<span class="keep-together"><code translate="no">decision_function()</code></span> method, due to the way it works (we will cover this in <a data-type="xref" href="ch06.html#ensembles_chapter">Chapter 6</a>). Luckily, it has a <code translate="no">predict_proba()</code> method that returns estimated class probabilities for each instance, and we can just use the probability of the positive class as a score, so <code translate="no">precision_recall_curve()</code> will work.<sup><a data-type="noteref" id="id1369-marker" href="ch03.html#id1369">4</a></sup> We can call the <code>cross_val_​predict()</code><a data-type="indexterm" data-primary="cross_val_predict()" id="id1370"/> function<a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.cross_val_predict()" id="id1371"/> to train the <code translate="no">RandomForestClassifier</code> using cross-validation and make it predict class probabilities for every image as 
<span class="keep-together">follows:</span></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">y_probas_forest</code> <code class="o">=</code> <code class="n">cross_val_predict</code><code class="p">(</code><code class="n">forest_clf</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_5</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
                                    <code class="n">method</code><code class="o">=</code><code class="s2">"predict_proba"</code><code class="p">)</code></pre>

<p>Let’s look at the estimated class probabilities for the first two images in the training set:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_probas_forest</code><code class="p">[:</code><code class="mi">2</code><code class="p">]</code><code class="w"/>
<code class="go">array([[0.11, 0.89],</code>
<code class="go">       [0.99, 0.01]])</code></pre>

<p>The model predicts that the first image is positive with 89% probability, and it predicts that the second image is negative with 99% probability. Since each image is either positive or negative, the estimated probabilities in each row add up to 100%.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>These are <em>estimated</em> probabilities, not actual<a data-type="indexterm" data-primary="actual versus estimated probabilities" id="id1372"/><a data-type="indexterm" data-primary="estimated versus actual probabilities" id="id1373"/> probabilities. For example, if you look at all the images that the model classified as positive with an estimated probability between 50% and 60%, roughly 94% of them are actually positive. So, the model’s estimated probabilities were much too low in this case—but models can be overconfident as well. The <code translate="no">CalibratedClassifierCV</code> class from the <code translate="no">sklearn.calibration</code> package can calibrate the estimated probabilities using cross-validation, making them much closer to actual probabilities (see the notebook for a code example). This is important in some scenarios, such as medical diagnosis, financial risk assessment, or fraud detection.</p>
</div>

<p>The second column contains the estimated probabilities for the positive class, so let’s pass them to the <code translate="no">precision_recall_curve()</code> function:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">y_scores_forest</code> <code class="o">=</code> <code class="n">y_probas_forest</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">]</code>
<code class="n">precisions_forest</code><code class="p">,</code> <code class="n">recalls_forest</code><code class="p">,</code> <code class="n">thresholds_forest</code> <code class="o">=</code> <code class="n">precision_recall_curve</code><code class="p">(</code>
    <code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_scores_forest</code><code class="p">)</code></pre>

<p>Now we’re ready to plot the PR curve. It is useful to plot the first PR curve as well to see how they compare (<a data-type="xref" href="#pr_curve_comparison_plot">Figure 3-8</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">recalls_forest</code><code class="p">,</code> <code class="n">precisions_forest</code><code class="p">,</code> <code class="s2">"b-"</code><code class="p">,</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
         <code class="n">label</code><code class="o">=</code><code class="s2">"Random Forest"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">recalls</code><code class="p">,</code> <code class="n">precisions</code><code class="p">,</code> <code class="s2">"--"</code><code class="p">,</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"SGD"</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># beautify the figure: add labels, grid, and legend</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure class="width-75"><div id="pr_curve_comparison_plot" class="figure">
<img src="assets/hmls_0308.png" alt="Precision-recall curves compare the performance of RandomForestClassifier and SGDClassifier, with the RandomForest curve closer to the top-right corner, indicating superior performance." width="1700" height="1395"/>
<h6><span class="label">Figure 3-8. </span>Comparing PR curves: the random forest classifier is superior to the SGD classifier because its PR curve is much closer to the top-right corner, and it has a greater AUC</h6>
</div></figure>

<p>As you can see in <a data-type="xref" href="#pr_curve_comparison_plot">Figure 3-8</a>, the <code translate="no">RandomForestClassifier</code>’s PR curve looks much better than the <code translate="no">SGDClassifier</code>’s: it comes much closer to the top-right corner. Its F<sub>1</sub> score and ROC AUC score are also significantly better:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_train_pred_forest</code> <code class="o">=</code> <code class="n">y_probas_forest</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">]</code> <code class="o">&gt;=</code> <code class="mf">0.5</code>  <code class="c1"># positive proba ≥ 50%</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">f1_score</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_train_pred_forest</code><code class="p">)</code><code class="w"/>
<code class="go">0.9274509803921569</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">roc_auc_score</code><code class="p">(</code><code class="n">y_train_5</code><code class="p">,</code> <code class="n">y_scores_forest</code><code class="p">)</code><code class="w"/>
<code class="go">0.9983436731328145</code></pre>

<p>Try measuring the precision and recall scores: you should find about 99.0% precision and 87.3% recall. Not too bad!<a data-type="indexterm" data-startref="xi_RandomForestClassifier346844_1" id="id1374"/><a data-type="indexterm" data-startref="xi_ScikitLearnsklearnensembleRandomForestClassifier346844_1" id="id1375"/><a data-type="indexterm" data-startref="xi_SGDClassifierRandomForestclassifiercomparedto346844_1" id="id1376"/></p>

<p>You now know how to train binary classifiers, choose the appropriate metric for your task, evaluate your classifiers using cross-validation, select the precision/recall trade-off that fits your needs, and use several metrics and curves to compare various models. You’re ready to try to detect more than just the 5s.<a data-type="indexterm" data-startref="xi_classificationperformancemeasures312611_1" id="id1377"/><a data-type="indexterm" data-startref="xi_performancemeasures312611_1" id="id1378"/><a data-type="indexterm" data-startref="xi_performancemeasuresROCcurve34224_1" id="id1379"/><a data-type="indexterm" data-startref="xi_receiveroperatingcharacteristicROCcurve34224_1" id="id1380"/><a data-type="indexterm" data-startref="xi_ROCreceiveroperatingcharacteristiccurve34224_1" id="id1381"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Multiclass Classification"><div class="sect1" id="id66">
<h1>Multiclass Classification</h1>

<p>Whereas<a data-type="indexterm" data-primary="classification" data-secondary="multiclass" id="xi_classificationmulticlass35418_1"/><a data-type="indexterm" data-primary="multiclass (multinomial) classification" id="xi_multiclassmultinomialclassification35418_1"/><a data-type="indexterm" data-primary="multinomial (multiclass) classification" id="xi_multinomialmulticlassclassification35418_1"/> binary classifiers distinguish between two classes, <em>multiclass classifiers</em> (also called <em>multinomial classifiers</em>) can distinguish between more than two classes.</p>

<p>Some Scikit-Learn classifiers (e.g., <code translate="no">LogisticRegression</code>, <code translate="no">RandomForestClassifier</code>, and <code translate="no">GaussianNB</code>) are capable of handling multiple classes natively. Others are strictly binary classifiers (e.g., <code translate="no">SGDClassifier</code> and <code translate="no">SVC</code>). However, there are various strategies that you can use to perform multiclass classification with multiple binary classifiers.</p>

<p>One way to create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). Then when you want to classify an image, you get the decision score from each classifier for that image and you select the class whose classifier outputs the highest score. This is called the <em>one-versus-the-rest</em> (OvR) strategy, or sometimes <em>one-versus-all</em> (OvA).<a data-type="indexterm" data-primary="one-versus-all (OvA) strategy" id="xi_oneversusallOvAstrategy3545468_1"/><a data-type="indexterm" data-primary="one-versus-the-rest (OvR) strategy" id="xi_oneversustherestOvRstrategy3545468_1"/><a data-type="indexterm" data-primary="OvA (one-versus-all) strategy" id="xi_OvAoneversusallstrategy3545468_1"/><a data-type="indexterm" data-primary="OvR (one-versus-the-rest) strategy" id="xi_OvRoneversusthereststrategy3545468_1"/></p>

<p>Another strategy is to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the <em>one-versus-one</em> (OvO) strategy<a data-type="indexterm" data-primary="one-versus-one (OvO) strategy" id="xi_oneversusoneOvOstrategy3547224_1"/><a data-type="indexterm" data-primary="OvO (one-versus-one) strategy" id="xi_OvOoneversusonestrategy3547224_1"/>. If there are <em>N</em> classes, you need to train <em>N</em> × (<em>N</em> – 1) / 2 classifiers. For the MNIST problem, this means training 45 binary classifiers! When you want to classify an image, you have to run the image through all 45 classifiers and see which class wins the most duels. The main advantage of OvO is that each classifier only needs to be trained on the part of the training set containing the two classes that it must distinguish.</p>

<p>Some algorithms (such as support vector machine classifiers) scale poorly with the size of the training set. For these algorithms OvO is preferred because it is faster to train many classifiers on small training sets than to train few classifiers on large training sets. For most binary classification algorithms, however, OvR is preferred.</p>

<p>Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvR or OvO, depending on the algorithm. Let’s try this with a support vector machine classifier using the <code translate="no">sklearn.svm.SVC</code><a data-type="indexterm" data-primary="classification" data-secondary="support vector machines" id="id1382"/><a data-type="indexterm" data-primary="sklearn" data-secondary="svm.SVC" id="id1383"/><a data-type="indexterm" data-primary="support vector machines (SVMs)" id="id1384"/><a data-type="indexterm" data-primary="SVC" id="id1385"/> class (see the online chapter on SVMs at <a href="https://homl.info" class="bare"><em class="hyperlink">https://homl.info</em></a>). We’ll only train on the first 2,000 images, or else it will take a very long time:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>

<code class="n">svm_clf</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">svm_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:</code><code class="mi">2000</code><code class="p">],</code> <code class="n">y_train</code><code class="p">[:</code><code class="mi">2000</code><code class="p">])</code>  <code class="c1"># y_train, not y_train_5</code></pre>

<p>That was easy! We trained the <code translate="no">SVC</code> using the original target classes from 0 to 9 (<code translate="no">y_train</code>), instead of the 5-versus-the-rest target classes (<code translate="no">y_train_5</code>). Since there are 10 classes (i.e., more than 2), Scikit-Learn used the OvO strategy and trained 45 binary classifiers. Now let’s make a prediction on an image:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">svm_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([</code><code class="n">some_digit</code><code class="p">])</code><code class="w"/>
<code class="go">array(['5'], dtype=object)</code></pre>

<p>That’s correct! This code actually made 45 predictions—one per pair of classes—and it selected the class that won the most duels.⁠<sup><a data-type="noteref" id="id1386-marker" href="ch03.html#id1386">5</a></sup> If you call the <code translate="no">decision_function()</code> method, you will see that it returns 10 scores per instance: one per class. Each class gets a score equal to the number of won duels plus or minus a small tweak (max ±0.33) to break ties, based on the classifier scores:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">some_digit_scores</code> <code class="o">=</code> <code class="n">svm_clf</code><code class="o">.</code><code class="n">decision_function</code><code class="p">([</code><code class="n">some_digit</code><code class="p">])</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">some_digit_scores</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code class="w"/>
<code class="go">array([[ 3.79,  0.73,  6.06,  8.3 , -0.29,  9.3 ,  1.75,  2.77,  7.21,</code>
<code class="go">         4.82]])</code></pre>

<p>The highest score is 9.3, and it’s indeed the one corresponding to class 5:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">class_id</code> <code class="o">=</code> <code class="n">some_digit_scores</code><code class="o">.</code><code class="n">argmax</code><code class="p">()</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">class_id</code><code class="w"/>
<code class="go">np.int64(5)</code></pre>

<p>When a classifier is trained, it stores the list of target classes in its <code translate="no">classes_</code> attribute, ordered by value. In the case of MNIST, the index of each class in the <code translate="no">classes_</code> array conveniently matches the class itself (e.g., the class at index 5 happens to be class <code translate="no">'5'</code>), but in general you won’t be so lucky; you will need to look up the class label like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">svm_clf</code><code class="o">.</code><code class="n">classes_</code><code class="w"/>
<code class="go">array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">svm_clf</code><code class="o">.</code><code class="n">classes_</code><code class="p">[</code><code class="n">class_id</code><code class="p">]</code><code class="w"/>
<code class="go">'5'</code></pre>

<p>If you want to force Scikit-Learn to use one-versus-one or one-versus-the-rest, you can use the <code translate="no">OneVsOneClassifier</code> or <code translate="no">OneVsRestClassifier</code> classes. Simply create an instance and pass a classifier to its constructor (it doesn’t even have to be a binary classifier). For example, this code creates a multiclass classifier using the OvR strategy, based on an <code translate="no">SVC</code><a data-type="indexterm" data-primary="sklearn" data-secondary="multiclass.OneVsOneClassifier" id="id1387"/><a data-type="indexterm" data-primary="sklearn" data-secondary="multiclass.OneVsRestClassifier" id="id1388"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.multiclass</code> <code class="kn">import</code> <code class="n">OneVsRestClassifier</code>

<code class="n">ovr_clf</code> <code class="o">=</code> <code class="n">OneVsRestClassifier</code><code class="p">(</code><code class="n">SVC</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">))</code>
<code class="n">ovr_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:</code><code class="mi">2000</code><code class="p">],</code> <code class="n">y_train</code><code class="p">[:</code><code class="mi">2000</code><code class="p">])</code></pre>

<p>Let’s make a prediction, and check the number of trained classifiers:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">ovr_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([</code><code class="n">some_digit</code><code class="p">])</code><code class="w"/>
<code class="go">array(['5'], dtype='&lt;U1')</code>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">len</code><code class="p">(</code><code class="n">ovr_clf</code><code class="o">.</code><code class="n">estimators_</code><code class="p">)</code><code class="w"/>
<code class="go">10</code></pre>

<p>Training an <code translate="no">SGDClassifier</code><a data-type="indexterm" data-primary="SGDClassifier" data-secondary="multiclass dataset" id="id1389"/> on a multiclass dataset and using it to make predictions is just as easy:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">sgd_clf</code> <code class="o">=</code> <code class="n">SGDClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">sgd_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">sgd_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([</code><code class="n">some_digit</code><code class="p">])</code><code class="w"/>
<code class="go">array(['3'], dtype='&lt;U1')</code></pre>

<p>Oops, that’s incorrect. Prediction errors do happen! This time Scikit-Learn used the OvR strategy under the hood: since there are 10 classes, it trained 10 binary classifiers. The <code translate="no">decision_function()</code> method now returns one value per class. Let’s look at the scores that the SGD classifier assigned to each class:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">sgd_clf</code><code class="o">.</code><code class="n">decision_function</code><code class="p">([</code><code class="n">some_digit</code><code class="p">])</code><code class="o">.</code><code class="n">round</code><code class="p">()</code><code class="w"/>
<code class="go">array([[-31893., -34420.,  -9531.,   1824., -22320.,  -1386., -26189.,</code>
<code class="go">        -16148.,  -4604., -12051.]])</code></pre>

<p>You can see that the classifier is not very confident about its prediction: almost all scores are very negative, while class 3 has a score of +1,824, and class 5 is not too far behind at –1,386. Of course, you’ll want to evaluate this classifier on more than one image. Since there are roughly the same number of images in each class, the accuracy metric is fine. As usual, you can use the <code translate="no">cross_val_score()</code> function to evaluate the model:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">cross_val_score</code><code class="p">(</code><code class="n">sgd_clf</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s2">"accuracy"</code><code class="p">)</code><code class="w"/>
<code class="go">array([0.87365, 0.85835, 0.8689 ])</code></pre>

<p>It gets over 85.8% on all test folds. If you used a random classifier, you would get 10% accuracy, so this is not such a bad score, but you can still do much better. Simply scaling the inputs (as discussed in <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a>) increases accuracy above 89.1%:<a data-type="indexterm" data-startref="xi_classificationmulticlass35418_1" id="id1390"/><a data-type="indexterm" data-startref="xi_multiclassmultinomialclassification35418_1" id="id1391"/><a data-type="indexterm" data-startref="xi_multinomialmulticlassclassification35418_1" id="id1392"/><a data-type="indexterm" data-startref="xi_oneversusallOvAstrategy3545468_1" id="id1393"/><a data-type="indexterm" data-startref="xi_oneversusoneOvOstrategy3547224_1" id="id1394"/><a data-type="indexterm" data-startref="xi_oneversustherestOvRstrategy3545468_1" id="id1395"/><a data-type="indexterm" data-startref="xi_OvAoneversusallstrategy3545468_1" id="id1396"/><a data-type="indexterm" data-startref="xi_OvOoneversusonestrategy3547224_1" id="id1397"/><a data-type="indexterm" data-startref="xi_OvRoneversusthereststrategy3545468_1" id="id1398"/></p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_train_scaled</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="s2">"float64"</code><code class="p">))</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">cross_val_score</code><code class="p">(</code><code class="n">sgd_clf</code><code class="p">,</code> <code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s2">"accuracy"</code><code class="p">)</code><code class="w"/>
<code class="go">array([0.8983, 0.891 , 0.9018])</code></pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Error Analysis"><div class="sect1" id="id67">
<h1>Error Analysis</h1>

<p>If<a data-type="indexterm" data-primary="classification" data-secondary="error analysis" id="xi_classificationerroranalysis36593_1"/><a data-type="indexterm" data-primary="confusion matrix (CM)" id="xi_confusionmatrixCM36593_1"/><a data-type="indexterm" data-primary="error analysis, classification" id="xi_erroranalysisclassification36593_1"/><a data-type="indexterm" data-primary="CM (confusion matrix)" id="xi_CMconfusionmatrix36593_1"/> this were a real project, you would now follow the steps in your machine learning project checklist (see <a href="https://homl.info/checklist" class="bare"><em class="hyperlink">https://homl.info/checklist</em></a>). You’d explore data preparation options, try out multiple models, shortlist the best ones, fine-tune their hyperparameters using <code translate="no">GridSearchCV</code>, and automate as much as possible. Here, we will assume that you have found a promising model and you want to find ways to improve it. One way to do this is to analyze the types of errors it makes.</p>

<p class="pagebreak-before">First, look at the confusion matrix. For this, you first need to make predictions using the <code translate="no">cross_val_predict()</code> function<a data-type="indexterm" data-primary="cross_val_predict()" id="id1399"/><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.cross_val_predict()" id="id1400"/>; then you can pass the labels and predictions to the <code translate="no">confusion_matrix()</code> function<a data-type="indexterm" data-primary="sklearn" data-secondary="metrics.confusion_matrix()" id="id1401"/>, just like you did earlier. However, since there are now 10 classes instead of 2, the confusion matrix will contain quite a lot of numbers, and it may be hard to read.</p>

<p>A colored diagram of the confusion matrix is much easier to analyze. To plot such a diagram, use the<a data-type="indexterm" data-primary="ConfusionMatrixDisplay.from_predictions()" id="id1402"/> <code translate="no">ConfusionMatrixDisplay.from_predictions()</code><a data-type="indexterm" data-primary="from_predictions()" id="id1403"/><a data-type="indexterm" data-primary="sklearn" data-secondary="metrics.ConfusionMatrixDisplay.from_predictions()" id="id1404"/> function like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">ConfusionMatrixDisplay</code>

<code class="n">y_train_pred</code> <code class="o">=</code> <code class="n">cross_val_predict</code><code class="p">(</code><code class="n">sgd_clf</code><code class="p">,</code> <code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="n">ConfusionMatrixDisplay</code><code class="o">.</code><code class="n">from_predictions</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> <code class="n">y_train_pred</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p>This produces the left diagram in <a data-type="xref" href="#confusion_matrix_plot_1">Figure 3-9</a>. This confusion matrix looks pretty good: most images are on the main diagonal, which means that they were classified correctly. Notice that the cell on the diagonal in row #5 and column #5 looks slightly darker than the other digits. This could be because the model made more errors on 5s, or because there are fewer 5s in the dataset than the other digits. That’s why it’s important to normalize the confusion matrix by dividing each value by the total number of images in the corresponding (true) class (i.e., divide by the row’s sum). This can be done simply by setting <code translate="no">normalize="true"</code>. We can also specify the <code translate="no">values_format=".0%"</code> argument to show percentages with no decimals. The following code produces the diagram on the right in <a data-type="xref" href="#confusion_matrix_plot_1">Figure 3-9</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">ConfusionMatrixDisplay</code><code class="o">.</code><code class="n">from_predictions</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> <code class="n">y_train_pred</code><code class="p">,</code>
                                        <code class="n">normalize</code><code class="o">=</code><code class="s2">"true"</code><code class="p">,</code> <code class="n">values_format</code><code class="o">=</code><code class="s2">".0%"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure><div id="confusion_matrix_plot_1" class="figure">
<img src="assets/hmls_0309.png" alt="Confusion matrices displaying model predictions, with the left showing raw counts and the right normalized by row to percentage accuracy for each class." width="2465" height="1108"/>
<h6><span class="label">Figure 3-9. </span>Confusion matrix (left) and the same CM normalized by row (right)</h6>
</div></figure>

<p>Now we can easily see that only 82% of the images of 5s were classified correctly. The most common error the model made with images of 5s was to misclassify them as 8s: this happened for 10% of all 5s. But only 2% of 8s got misclassified as 5s; confusion matrices are generally not symmetrical! If you look carefully, you will notice that many digits have been misclassified as 8s, but this is not immediately obvious from this diagram. If you want to make the errors stand out more, you can try putting zero weight on the correct predictions. The following code does just that and produces the diagram on the left in <a data-type="xref" href="#confusion_matrix_plot_2">Figure 3-10</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">sample_weight</code> <code class="o">=</code> <code class="p">(</code><code class="n">y_train_pred</code> <code class="o">!=</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">ConfusionMatrixDisplay</code><code class="o">.</code><code class="n">from_predictions</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> <code class="n">y_train_pred</code><code class="p">,</code>
                                        <code class="n">sample_weight</code><code class="o">=</code><code class="n">sample_weight</code><code class="p">,</code>
                                        <code class="n">normalize</code><code class="o">=</code><code class="s2">"true"</code><code class="p">,</code> <code class="n">values_format</code><code class="o">=</code><code class="s2">".0%"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure><div id="confusion_matrix_plot_2" class="figure">
<img src="assets/hmls_0310.png" alt="Confusion matrix diagrams showing error rates normalized by row on the left and by column on the right, highlighting common misclassification patterns." width="2456" height="1124"/>
<h6><span class="label">Figure 3-10. </span>Confusion matrix with errors only, normalized by row (left) and by column (right)</h6>
</div></figure>

<p>Now you can see much more clearly the kinds of errors the classifier makes. The column for class 8 is now really bright, which confirms that many images got misclassified as 8s. In fact this is the most common misclassification for almost all classes. But be careful how you interpret the percentages in this diagram: remember that we’ve excluded the correct predictions. For example, the 36% in row #7, column #9 in the left grid does <em>not</em> mean that 36% of all images of 7s were misclassified as 9s. It means that 36% of the <em>errors</em> the model made on images of 7s were misclassifications as 9s. In reality, only 3% of images of 7s were misclassified as 9s, as you can see in the diagram on the right in <a data-type="xref" href="#confusion_matrix_plot_1">Figure 3-9</a>.</p>

<p>It is also possible to normalize the confusion matrix by column rather than by row: if you set <code translate="no">normalize="pred"</code>, you get the diagram on the right in <a data-type="xref" href="#confusion_matrix_plot_2">Figure 3-10</a>. For example, you can see that 56% of misclassified 7s are actually 9s.</p>

<p>Analyzing the confusion matrix often gives you insights into ways to improve your classifier. Looking at these plots, it seems that your efforts should be spent on reducing the false 8s. For example, you could try to gather more training data for digits that look like 8s (but are not) so that the classifier can learn to distinguish them from real 8s. Or you could engineer new features that would help the classifier—for example, writing an algorithm to count the number of closed loops (e.g., 8 has two, 6 has one, 5 has none). Or you could preprocess the images (e.g., using Scikit-Image, Pillow, or OpenCV) to make some patterns, such as closed loops, stand out more.</p>

<p>Analyzing individual errors can also be a good way to gain insights into what your classifier is doing and why it is failing. For example, let’s plot examples of 3s and 5s in a confusion matrix style (<a data-type="xref" href="#error_analysis_digits_plot">Figure 3-11</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">cl_a</code><code class="p">,</code> <code class="n">cl_b</code> <code class="o">=</code> <code class="s1">'3'</code><code class="p">,</code> <code class="s1">'5'</code>
<code class="n">X_aa</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[(</code><code class="n">y_train</code> <code class="o">==</code> <code class="n">cl_a</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">y_train_pred</code> <code class="o">==</code> <code class="n">cl_a</code><code class="p">)]</code>
<code class="n">X_ab</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[(</code><code class="n">y_train</code> <code class="o">==</code> <code class="n">cl_a</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">y_train_pred</code> <code class="o">==</code> <code class="n">cl_b</code><code class="p">)]</code>
<code class="n">X_ba</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[(</code><code class="n">y_train</code> <code class="o">==</code> <code class="n">cl_b</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">y_train_pred</code> <code class="o">==</code> <code class="n">cl_a</code><code class="p">)]</code>
<code class="n">X_bb</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[(</code><code class="n">y_train</code> <code class="o">==</code> <code class="n">cl_b</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">y_train_pred</code> <code class="o">==</code> <code class="n">cl_b</code><code class="p">)]</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># plot all images in X_aa, X_ab, X_ba, X_bb in a confusion matrix style</code></pre>

<figure class="width-65"><div id="error_analysis_digits_plot" class="figure">
<img src="assets/hmls_0311.png" alt="Confusion matrix showing images of handwritten digits 3 and 5, highlighting misclassifications by a simple classifier." width="1398" height="1398"/>
<h6><span class="label">Figure 3-11. </span>Some images of 3s and 5s organized like a confusion matrix</h6>
</div></figure>

<p>As you can see, some of the digits that the classifier gets wrong (i.e., in the bottom-left and top-right blocks) are so badly written that even a human would have trouble classifying them. However, most misclassified images seem like obvious errors to us. It may be hard to understand why the classifier made the mistakes it did, but remember that the human brain is a fantastic pattern recognition system, and our visual system does a lot of complex preprocessing before any information even reaches our consciousness. So, the fact that this task feels simple does not mean that it is. Recall that we used a simple <code translate="no">SGDClassifier</code>,<a data-type="indexterm" data-primary="SGDClassifier" data-secondary="misclassified images" id="id1405"/> which is just a linear model: all it does is assign a weight per class to each pixel, and when it sees a new image it just sums up the weighted pixel intensities to get a score for each class. Since 3s and 5s differ by only a few pixels, this model will easily confuse them.</p>

<p>The main difference between 3s and 5s is the position of the small line that joins the top line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left, the classifier might classify it as a 5, and vice versa. In other words, this classifier is quite sensitive to image shifting and rotation. One way to reduce the 3/5 confusion is to preprocess the images to ensure that they are well centered and not too rotated. However, this may not be easy since it requires predicting the correct rotation of each image. A much simpler approach consists of augmenting the training set with slightly shifted and rotated variants of the training images. This will force the model to learn to be more tolerant to such variations. This is called <em>data augmentation</em> (we’ll cover this in <a data-type="xref" href="ch12.html#cnn_chapter">Chapter 12</a>; also see exercise 2 at the end of this chapter).<a data-type="indexterm" data-startref="xi_classificationerroranalysis36593_1" id="id1406"/><a data-type="indexterm" data-startref="xi_confusionmatrixCM36593_1" id="id1407"/><a data-type="indexterm" data-startref="xi_erroranalysisclassification36593_1" id="id1408"/><a data-type="indexterm" data-startref="xi_CMconfusionmatrix36593_1" id="id1409"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Multilabel Classification"><div class="sect1" id="id68">
<h1>Multilabel Classification</h1>

<p>Until<a data-type="indexterm" data-primary="classification" data-secondary="multilabel" id="xi_classificationmultilabel37356_1"/><a data-type="indexterm" data-primary="multilabel classifiers" id="xi_multilabelclassifiers37356_1"/> now, each instance has always been assigned to just one class. But in some cases you may want your classifier to output multiple classes for each instance. Consider a face-recognition classifier<a data-type="indexterm" data-primary="face-recognition classifier" id="id1410"/>: what should it do if it recognizes several people in the same picture? It should attach one tag per person it recognizes. Say the classifier has been trained to recognize three faces: Alice, Bob, and Charlie. Then when the classifier is shown a picture of Alice and Charlie, it should output <code translate="no">[True, False, True]</code> (meaning “Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple binary tags is called a <em>multilabel classification</em> system.</p>

<p>We won’t go into face recognition just yet, but let’s look at a simpler example, just for illustration purposes:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KNeighborsClassifier</code>

<code class="n">y_train_large</code> <code class="o">=</code> <code class="p">(</code><code class="n">y_train</code> <code class="o">&gt;=</code> <code class="s1">'7'</code><code class="p">)</code>
<code class="n">y_train_odd</code> <code class="o">=</code> <code class="p">(</code><code class="n">y_train</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="s1">'int8'</code><code class="p">)</code> <code class="o">%</code> <code class="mi">2</code> <code class="o">==</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">y_multilabel</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">c_</code><code class="p">[</code><code class="n">y_train_large</code><code class="p">,</code> <code class="n">y_train_odd</code><code class="p">]</code>

<code class="n">knn_clf</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">()</code>
<code class="n">knn_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_multilabel</code><code class="p">)</code></pre>

<p>This code creates a <code translate="no">y_multilabel</code> array containing two target labels for each digit image: the first indicates whether the digit is large (7, 8, or 9), and the second indicates whether it is odd. Then the code creates a <code translate="no">KNeighborsClassifier</code><a data-type="indexterm" data-primary="KNeighborsClassifier" id="id1411"/><a data-type="indexterm" data-primary="sklearn" data-secondary="neighbors.KNeighborsClassifier" id="id1412"/> instance, which supports multilabel classification (not all classifiers do), and trains this model using the multiple targets array. Now you can make a prediction, and notice that it outputs two labels:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">knn_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([</code><code class="n">some_digit</code><code class="p">])</code><code class="w"/>
<code class="go">array([[False,  True]])</code></pre>

<p>And it gets it right! The digit 5 is indeed not large (<code translate="no">False</code>) and odd (<code translate="no">True</code>).</p>

<p>There are many ways to evaluate a multilabel classifier, and selecting the right metric really depends on your project. One approach is to measure the F<sub>1</sub> score for each individual label (or any other binary classifier metric discussed earlier), then simply compute the average score. The following code computes the average F<sub>1</sub> score across all labels<a data-type="indexterm" data-primary="f1_score()" id="id1413"/><a data-type="indexterm" data-primary="sklearn" data-secondary="metrics.f1_score()" id="id1414"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_train_knn_pred</code> <code class="o">=</code> <code class="n">cross_val_predict</code><code class="p">(</code><code class="n">knn_clf</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">y_multilabel</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">f1_score</code><code class="p">(</code><code class="n">y_multilabel</code><code class="p">,</code> <code class="n">y_train_knn_pred</code><code class="p">,</code> <code class="n">average</code><code class="o">=</code><code class="s2">"macro"</code><code class="p">)</code><code class="w"/>
<code class="go">0.976410265560605</code></pre>

<p>This approach assumes that all labels are equally important, which may not be the case. In particular, if you have many more pictures of Alice than of Bob or Charlie, you may want to give more weight to the classifier’s score on pictures of Alice. One simple option is to give each label a weight equal to its <em>support</em> (i.e., the number of instances with that target label). To do this, simply set <code translate="no">average="weighted"</code> when calling the <code translate="no">f1_score()</code> function.⁠<sup><a data-type="noteref" id="id1415-marker" href="ch03.html#id1415">6</a></sup></p>

<p>If you wish to use a classifier that does not natively support multilabel classification, such as <code translate="no">SVC</code>, one possible strategy is to train one model per label. However, this strategy may have a hard time capturing the dependencies between the labels. For example, a large digit (7, 8, or 9) is twice more likely to be odd than even, but the classifier for the “odd” label does not know what the classifier for the “large” label predicted. To solve this issue, the models can be organized in a chain: when a model makes a prediction, it uses the input features plus all the predictions of the models that come before it in the chain.</p>

<p>The good news is that Scikit-Learn has a class called <code translate="no">ClassifierChain</code><a data-type="indexterm" data-primary="ClassifierChain, Scikit-Learn" id="id1416"/><a data-type="indexterm" data-primary="sklearn" data-secondary="multioutput.ClassifierChain" id="id1417"/> that does just that! By default it will use the true labels for training, feeding each model the appropriate labels depending on their position in the chain. But if you set the <code translate="no">cv</code> hyperparameter, it will use cross-validation to get “clean” (out-of-sample) predictions from each trained model for every instance in the training set, and these predictions will then be used to train all the models later in the chain. Note that the order of the classifiers in the chain may affect the final performance. Here’s an example showing how to create and train a <code translate="no">ClassifierChain</code> using the cross-validation strategy. As earlier, we’ll just use the first 2,000 images in the training set to speed things up:<a data-type="indexterm" data-startref="xi_classificationmultilabel37356_1" id="id1418"/><a data-type="indexterm" data-startref="xi_multilabelclassifiers37356_1" id="id1419"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.multioutput</code> <code class="kn">import</code> <code class="n">ClassifierChain</code>

<code class="n">chain_clf</code> <code class="o">=</code> <code class="n">ClassifierChain</code><code class="p">(</code><code class="n">SVC</code><code class="p">(),</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">chain_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:</code><code class="mi">2000</code><code class="p">],</code> <code class="n">y_multilabel</code><code class="p">[:</code><code class="mi">2000</code><code class="p">])</code></pre>

<p>Now we can use this <code translate="no">ClassifierChain</code> to make predictions:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">chain_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([</code><code class="n">some_digit</code><code class="p">])</code><code class="w"/>
<code class="go">array([[0., 1.]])</code></pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Multioutput Classification"><div class="sect1" id="id69">
<h1>Multioutput Classification</h1>

<p>The<a data-type="indexterm" data-primary="classification" data-secondary="multioutput" id="xi_classificationmultioutput37964_1"/><a data-type="indexterm" data-primary="multioutput classifiers" id="xi_multioutputclassifiers37964_1"/> last type of classification task we’ll discuss here is called <em>multioutput–multiclass classification</em> (or just <em>multioutput classification</em>). It is a generalization of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values).</p>

<p>To illustrate this, let’s build a system that removes noise from images. It will take as input a noisy digit image, and it will (hopefully) output a clean digit image, represented as an array of pixel intensities, just like the MNIST images. Notice that the classifier’s output is multilabel (one label per pixel) and each label can have multiple values (pixel intensity ranges from 0 to 255). This is thus an example of a multioutput classification system.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The line between classification and regression<a data-type="indexterm" data-primary="classification" data-secondary="regression" id="id1420"/><a data-type="indexterm" data-primary="regression models" data-secondary="and classification" data-secondary-sortas="classification" id="id1421"/> is sometimes blurry, such as in this example. Arguably, predicting pixel intensity is more akin to regression than to classification. Moreover, multioutput systems are not limited to classification tasks; you could even have a system that outputs multiple labels per instance, including both class labels and value labels.</p>
</div>

<p>Let’s start by creating the training and test sets by taking the MNIST images and adding noise to their pixel intensities, using a random number generator’s <code translate="no">integers()</code> method. The target images will be the original images:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">default_rng</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">noise_train</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">integers</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code> <code class="mi">784</code><code class="p">))</code>
<code class="n">X_train_mod</code> <code class="o">=</code> <code class="n">X_train</code> <code class="o">+</code> <code class="n">noise_train</code>
<code class="n">noise_test</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">integers</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_test</code><code class="p">),</code> <code class="mi">784</code><code class="p">))</code>
<code class="n">X_test_mod</code> <code class="o">=</code> <code class="n">X_test</code> <code class="o">+</code> <code class="n">noise_test</code>
<code class="n">y_train_mod</code> <code class="o">=</code> <code class="n">X_train</code>
<code class="n">y_test_mod</code> <code class="o">=</code> <code class="n">X_test</code></pre>

<p class="pagebreak-before">Let’s take a peek at the first image from the test set (<a data-type="xref" href="#noisy_digit_example_plot">Figure 3-12</a>). Yes, we’re snooping on the test data, so you should be frowning right now.</p>

<figure><div id="noisy_digit_example_plot" class="figure">
<img src="assets/hmls_0312.png" alt="A pixelated, noisy image of the number seven on the left and a clean version on the right, illustrating the effect of noise reduction." width="1531" height="832"/>
<h6><span class="label">Figure 3-12. </span>A noisy image (left) and the target clean image (right)</h6>
</div></figure>

<p>On the left is the noisy input image, and on the right is the clean target image. Now let’s train the classifier and make it clean up this image (<a data-type="xref" href="#cleaned_digit_example_plot">Figure 3-13</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">knn_clf</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">()</code>
<code class="n">knn_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_mod</code><code class="p">,</code> <code class="n">y_train_mod</code><code class="p">)</code>
<code class="n">clean_digit</code> <code class="o">=</code> <code class="n">knn_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([</code><code class="n">X_test_mod</code><code class="p">[</code><code class="mi">0</code><code class="p">]])</code>
<code class="n">plot_digit</code><code class="p">(</code><code class="n">clean_digit</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure class="width-15"><div id="cleaned_digit_example_plot" class="figure">
<img src="assets/hmls_0313.png" alt="A grayscale pixelated image of the digit seven, demonstrating a cleaned-up version after applying a K-Neighbors Classifier." width="594" height="793"/>
<h6><span class="label">Figure 3-13. </span>The cleaned-up image</h6>
</div></figure>

<p>Looks close enough to the target! This concludes our tour of classification. You now know how to select good metrics for classification tasks, pick the appropriate precision/recall trade-off, compare classifiers, and more generally build good classification systems for a variety of tasks. In the next chapters, you’ll learn how all these machine learning models you’ve been using actually work.<a data-type="indexterm" data-startref="xi_classificationmultioutput37964_1" id="id1422"/><a data-type="indexterm" data-startref="xi_multioutputclassifiers37964_1" id="id1423"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id70">
<h1>Exercises</h1>
<ol>
<li>
<p>Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set. Hint: the <code translate="no">KNeighborsClassifier</code><a data-type="indexterm" data-primary="KNeighborsClassifier" id="id1424"/><a data-type="indexterm" data-primary="sklearn" data-secondary="neighbors.KNeighborsClassifier" id="id1425"/> works quite well for this task; you just need to find good hyperparameter values (try a grid search on the <code translate="no">weights</code> and <code translate="no">n_neighbors</code> hyperparameters).</p>
</li>
<li>
<p>Write a function that can shift an MNIST image in any direction (left, right, up, or down) by one pixel.⁠<sup><a data-type="noteref" id="id1426-marker" href="ch03.html#id1426">7</a></sup> Then, for each image in the training set, create four shifted copies (one per direction) and add them to the training set. Finally, train your best model on this expanded training set and measure its accuracy on the test set. You should observe that your model performs even better now! This technique of artificially growing the training set is called <em>data augmentation</em><a data-type="indexterm" data-primary="data augmentation" id="id1427"/> or <em>training set expansion</em>.<a data-type="indexterm" data-primary="training set expansion" id="id1428"/></p>
</li>
<li>
<p>Tackle the Titanic dataset. A great place to start is on <a href="https://kaggle.com/c/titanic">Kaggle</a>. Alternatively, you can download the data from <a href="https://homl.info/titanic.tgz" class="bare"><em class="hyperlink">https://homl.info/titanic.tgz</em></a> and unzip this tarball like you did for the housing data in <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a>. This will give you two CSV files, <em>train.csv</em> and <em>test.csv</em>, which you can load using <code translate="no">pandas.read_csv()</code>. The goal is to train a classifier that can predict the <code translate="no">Survived</code> column based on the other columns.</p>
</li>
<li>
<p>Build a spam classifier (a more challenging exercise):</p>
<ol>
<li>
<p>Download examples of spam and ham from <a href="https://homl.info/spamassassin">Apache SpamAssassin’s public datasets</a>.</p>
</li>
<li>
<p>Unzip the datasets and familiarize yourself with the data format.</p>
</li>
<li>
<p>Split the data into a training set and a test set.</p>
</li>
<li>
<p>Write a data preparation pipeline to convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector that indicates the presence or absence of each possible word. For example, if all emails only ever contain four words, “Hello”, “how”, “are”, “you”, then the email “Hello you Hello Hello you” would be converted into a vector [1, 0, 0, 1] (meaning [“Hello” is present, “how” is absent, “are” is absent, “you” is present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of each word.</p>

<p>You may want to add hyperparameters to your preparation pipeline to control whether to strip off email headers, convert each email to lowercase, remove punctuation, replace all URLs with “URL”, replace all numbers with “NUMBER”, or even perform <em>stemming</em><a data-type="indexterm" data-primary="stemming" id="id1429"/> (i.e., trim off word endings; there are Python libraries available to do this).</p>
</li>
<li>
<p>Finally, try out several classifiers and see if you can build a great spam classifier, with both high recall and high precision.<a data-type="indexterm" data-startref="xi_classification353_1" id="id1430"/></p>
</li>

</ol>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1283"><sup><a href="ch03.html#id1283-marker">1</a></sup> By default Scikit-Learn caches downloaded datasets in a directory called <em>scikit_learn_data</em> in your home directory.</p><p data-type="footnote" id="id1288"><sup><a href="ch03.html#id1288-marker">2</a></sup> Datasets returned by <code translate="no">fetch_openml()</code><a data-type="indexterm" data-primary="fetch_openml()" id="id1431"/> are not always shuffled or split.</p><p data-type="footnote" id="id1293"><sup><a href="ch03.html#id1293-marker">3</a></sup> Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as stock market prices or weather conditions). We will explore this in <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>.</p><p data-type="footnote" id="id1369"><sup><a href="ch03.html#id1369-marker">4</a></sup> Scikit-Learn classifiers always have either a <code translate="no">decision_function()</code> method or a <code translate="no">predict_proba()</code> method, or sometimes both.</p><p data-type="footnote" id="id1386"><sup><a href="ch03.html#id1386-marker">5</a></sup> In case of a tie, the first class is selected, unless you set the <code translate="no">break_ties</code> hyperparameters to <code translate="no">True</code>, in which case ties are broken using the output of the <code translate="no">decision_function()</code>.</p><p data-type="footnote" id="id1415"><sup><a href="ch03.html#id1415-marker">6</a></sup> Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for more details.</p><p data-type="footnote" id="id1426"><sup><a href="ch03.html#id1426-marker">7</a></sup> You can use the <code translate="no">shift()</code> function from the <code translate="no">scipy.ndimage.interpolation</code> module. For example, <code translate="no">shift(image, [2, 1], cval=0)</code> shifts the image two pixels down and one pixel to the right.</p></div></div></section></div></div></body></html>