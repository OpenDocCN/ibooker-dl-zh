["```py\n[5, 3, 2]\n```", "```py\nnormalize(np.array([5,3,2]))\n```", "```py\narray([0.5, 0.3, 0.2])\n```", "```py\nsoftmax(np.array([5,3,2]))\n```", "```py\narray([0.84, 0.11, 0.04])\n```", "```py\nsoftmax_x = softmax(x, axis = 1)\nloss_grad = softmax_x - y\n```", "```py\nclass SoftmaxCrossEntropyLoss(Loss):\n    def __init__(self, eps: float=1e-9)\n        super().__init__()\n        self.eps = eps\n        self.single_output = False\n\n    def _output(self) -> float:\n\n        # applying the softmax function to each row (observation)\n        softmax_preds = softmax(self.prediction, axis=1)\n\n        # clipping the softmax output to prevent numeric instability\n        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n\n        # actual loss computation\nsoftmax_cross_entropy_loss = (\n    -1.0 * self.target * np.log(self.softmax_preds) - \\\n        (1.0 - self.target) * np.log(1 - self.softmax_preds)\n)\n\n        return np.sum(softmax_cross_entropy_loss)\n\n    def _input_grad(self) -> ndarray:\n\n        return self.softmax_preds - self.target\n```", "```py\nX_train, y_train, X_test, y_test = mnist.load()\n```", "```py\nX_train, X_test = X_train - np.mean(X_train), X_test - np.mean(X_train)\nX_train, X_test = X_train / np.std(X_train), X_test / np.std(X_train)\n```", "```py\nmodel = NeuralNetwork(\n    layers=[Dense(neurons=89,\n                  activation=Tanh()),\n            Dense(neurons=10,\n                  activation=Sigmoid())],\n            loss = MeanSquaredError(),\nseed=20190119)\n\noptimizer = SGD(0.1)\n\ntrainer = Trainer(model, optimizer)\ntrainer.fit(X_train, train_labels, X_test, test_labels,\n            epochs = 50,\n            eval_every = 10,\n            seed=20190119,\n            batch_size=60);\n\ncalc_accuracy_model(model, X_test)\n```", "```py\nValidation loss after 10 epochs is 0.611\nValidation loss after 20 epochs is 0.428\nValidation loss after 30 epochs is 0.389\nValidation loss after 40 epochs is 0.374\nValidation loss after 50 epochs is 0.366\n\nThe model validation accuracy is: 72.58%\n```", "```py\nmodel = NeuralNetwork(\n    layers=[Dense(neurons=89,\n                  activation=Tanh()),\n            Dense(neurons=10,\n                  activation=Linear())],\n            loss = SoftmaxCrossEntropy(),\nseed=20190119)\n```", "```py\nValidation loss after 10 epochs is 0.630\nValidation loss after 20 epochs is 0.574\nValidation loss after 30 epochs is 0.549\nValidation loss after 40 epochs is 0.546\nLoss increased after epoch 50, final loss was 0.546, using the model from\nepoch 40\n\nThe model validation accuracy is: 91.01%\n```", "```py\nupdate = self.lr*kwargs['grad']\nkwargs['param'] -= update\n```", "```py\ndef step(self) -> None:\n    '''\n If first iteration: intialize \"velocities\" for each param.\n Otherwise, simply apply _update_rule.\n '''\n    if self.first:\n        # now we will set up velocities on the first iteration\n        self.velocities = [np.zeros_like(param)\n                           for param in self.net.params()]\n        self.first = False\n\n    for (param, param_grad, velocity) in zip(self.net.params(),\n                                             self.net.param_grads(),\n                                             self.velocities):\n        # pass in velocity into the \"_update_rule\" function\n        self._update_rule(param=param,\n                          grad=param_grad,\n                          velocity=velocity)\n\ndef _update_rule(self, **kwargs) -> None:\n        '''\n Update rule for SGD with momentum.\n '''\n        # Update velocity\n        kwargs['velocity'] *= self.momentum\n        kwargs['velocity'] += self.lr * kwargs['grad']\n\n        # Use this to update parameters\n        kwargs['param'] -= kwargs['velocity']\n```", "```py\nValidation loss after 10 epochs is 0.441\nValidation loss after 20 epochs is 0.351\nValidation loss after 30 epochs is 0.345\nValidation loss after 40 epochs is 0.338\nLoss increased after epoch 50, final loss was 0.338, using the model from epoch 40\n\nThe model validation accuracy is: 95.51%\n```", "```py\ndef __init__(self,\n             lr: float = 0.01,\n             final_lr: float = 0,\n             decay_type: str = 'exponential')\n    self.lr = lr\n    self.final_lr = final_lr\n    self.decay_type = decay_type\n```", "```py\nself.optim._setup_decay()\n```", "```py\ndef _setup_decay(self) -> None:\n\n    if not self.decay_type:\n        return\n    elif self.decay_type == 'exponential':\n        self.decay_per_epoch = np.power(self.final_lr / self.lr,\n                                   1.0 / (self.max_epochs-1))\n    elif self.decay_type == 'linear':\n        self.decay_per_epoch = (self.lr - self.final_lr) / (self.max_epochs-1)\n```", "```py\ndef _decay_lr(self) -> None:\n\n    if not self.decay_type:\n        return\n\n    if self.decay_type == 'exponential':\n        self.lr *= self.decay_per_epoch\n\n    elif self.decay_type == 'linear':\n        self.lr -= self.decay_per_epoch\n```", "```py\nif self.optim.final_lr:\n    self.optim._decay_lr()\n```", "```py\noptimizer = SGDMomentum(0.15, momentum=0.9, final_lr=0.05, decay_type='linear')\n```", "```py\nValidation loss after 10 epochs is 0.403\nValidation loss after 20 epochs is 0.343\nValidation loss after 30 epochs is 0.282\nLoss increased after epoch 40, final loss was 0.282, using the model from epoch 30\nThe model validation accuracy is: 95.91%\n```", "```py\noptimizer = SGDMomentum(0.2, momentum=0.9, final_lr=0.05, decay_type='exponential')\n```", "```py\nValidation loss after 10 epochs is 0.461\nValidation loss after 20 epochs is 0.323\nValidation loss after 30 epochs is 0.284\nLoss increased after epoch 40, final loss was 0.284, using the model from epoch 30\nThe model validation accuracy is: 96.06%\n```", "```py\nif self.weight_init == \"glorot\":\n    scale = 2/(num_in + self.neurons)\nelse:\n    scale = 1.0\n```", "```py\nmodel = NeuralNetwork(\n    layers=[Dense(neurons=89,\n                  activation=Tanh(),\n                  weight_init=\"glorot\"),\n            Dense(neurons=10,\n                  activation=Linear(),\n                  weight_init=\"glorot\")],\n            loss = SoftmaxCrossEntropy(),\nseed=20190119)\n```", "```py\nValidation loss after 10 epochs is 0.352\nValidation loss after 20 epochs is 0.280\nValidation loss after 30 epochs is 0.244\nLoss increased after epoch 40, final loss was 0.244, using the model from epoch 30\nThe model validation accuracy is: 96.71%\n```", "```py\nValidation loss after 10 epochs is 0.305\nValidation loss after 20 epochs is 0.264\nValidation loss after 30 epochs is 0.245\nLoss increased after epoch 40, final loss was 0.245, using the model from epoch 30\nThe model validation accuracy is: 96.71%\n```", "```py\nclass Dropout(Operation):\n\n    def __init__(self,\n                 keep_prob: float = 0.8):\n        super().__init__()\n        self.keep_prob = keep_prob\n\n    def _output(self, inference: bool) -> ndarray:\n        if inference:\n            return self.inputs * self.keep_prob\n        else:\n            self.mask = np.random.binomial(1, self.keep_prob,\n                                           size=self.inputs.shape)\n            return self.inputs * self.mask\n\n    def _input_grad(self, output_grad: ndarray) -> ndarray:\n        return output_grad * self.mask\n```", "```py\n    test_preds = self.net.forward(X_test, inference=True)\n    ```", "```py\n    def __init__(self,\n                 neurons: int,\n                 activation: Operation = Linear(),\n                 dropout: float = 1.0,\n                 weight_init: str = \"standard\")\n    ```", "```py\n    if self.dropout < 1.0:\n        self.operations.append(Dropout(self.dropout))\n    ```", "```py\nmnist_soft = NeuralNetwork(\n    layers=[Dense(neurons=89,\n                  activation=Tanh(),\n                  weight_init=\"glorot\",\n                  dropout=0.8),\n            Dense(neurons=10,\n                  activation=Linear(),\n                  weight_init=\"glorot\")],\n            loss = SoftmaxCrossEntropy(),\nseed=20190119)\n```", "```py\nValidation loss after 10 epochs is 0.285\nValidation loss after 20 epochs is 0.232\nValidation loss after 30 epochs is 0.199\nValidation loss after 40 epochs is 0.196\nLoss increased after epoch 50, final loss was 0.196, using the model from epoch 40\nThe model validation accuracy is: 96.95%\n```", "```py\nmodel = NeuralNetwork(\n    layers=[Dense(neurons=178,\n                  activation=Tanh(),\n                  weight_init=\"glorot\",\n                  dropout=0.8),\n            Dense(neurons=46,\n                  activation=Tanh(),\n                  weight_init=\"glorot\",\n                  dropout=0.8),\n            Dense(neurons=10,\n                  activation=Linear(),\n                  weight_init=\"glorot\")],\n            loss = SoftmaxCrossEntropy(),\nseed=20190119)\n```", "```py\nValidation loss after 10 epochs is 0.321\nValidation loss after 20 epochs is 0.268\nValidation loss after 30 epochs is 0.248\nValidation loss after 40 epochs is 0.222\nValidation loss after 50 epochs is 0.217\nValidation loss after 60 epochs is 0.194\nValidation loss after 70 epochs is 0.191\nValidation loss after 80 epochs is 0.190\nValidation loss after 90 epochs is 0.182\nLoss increased after epoch 100, final loss was 0.182, using the model from epoch 90\nThe model validation accuracy is: 97.15%\n```", "```py\nValidation loss after 10 epochs is 0.375\nValidation loss after 20 epochs is 0.305\nValidation loss after 30 epochs is 0.262\nValidation loss after 40 epochs is 0.246\nLoss increased after epoch 50, final loss was 0.246, using the model from epoch 40\nThe model validation accuracy is: 96.52%\n```"]