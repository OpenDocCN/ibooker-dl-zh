- en: 3 Coding Attention Mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Exploring the reasons for using attention mechanisms in neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing a basic self-attention framework and progressing to an enhanced
    self-attention mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a causal attention module that allows LLMs to generate one token
    at a time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masking randomly selected attention weights with dropout to reduce overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking multiple causal attention modules into a multi-head attention module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to prepare the input text for training
    LLMs. This involved splitting text into individual word and subword tokens, which
    can be encoded into vector representations, the so-called embeddings, for the
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will now look at an integral part of the LLM architecture
    itself, attention mechanisms, as illustrated in Figure 3.1.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 A mental model of the three main stages of coding an LLM, pretraining
    the LLM on a general text dataset, and finetuning it on a labeled dataset. This
    chapter focuses on attention mechanisms, which are an integral part of an LLM
    architecture.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image001.png)'
  prefs: []
  type: TYPE_IMG
- en: Attention mechanisms are a comprehensive topic, which is why we are devoting
    a whole chapter to it. We will largely look at these attention mechanisms in isolation
    and focus on them at a mechanistic level. In the next chapter, we will then code
    the remaining parts of the LLM surrounding the self-attention mechanism to see
    it in action and to create a model to generate text.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this chapter, we will implement four different variants of
    attention mechanisms, as illustrated in Figure 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 The figure depicts different attention mechanisms we will code in
    this chapter, starting with a simplified version of self-attention before adding
    the trainable weights. The causal attention mechanism adds a mask to self-attention
    that allows the LLM to generate one word at a time. Finally, multi-head attention
    organizes the attention mechanism into multiple heads, allowing the model to capture
    various aspects of the input data in parallel.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image003.png)'
  prefs: []
  type: TYPE_IMG
- en: These different attention variants shown in Figure 3.2 build on each other,
    and the goal is to arrive at a compact and efficient implementation of multi-head
    attention at the end of this chapter that we can then plug into the LLM architecture
    we will code in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The problem with modeling long sequences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we dive into the *self-attention* mechanism that is at the heart of LLMs
    later in this chapter, what is the problem with architectures without attention
    mechanisms that predate LLMs? Suppose we want to develop a language translation
    model that translates text from one language into another. As shown in Figure
    3.3, we can't simply translate a text word by word due to the grammatical structures
    in the source and target language.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 When translating text from one language to another, such as German
    to English, it's not possible to merely translate word by word. Instead, the translation
    process requires contextual understanding and grammar alignment.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image005.png)'
  prefs: []
  type: TYPE_IMG
- en: To address the issue that we cannot translate text word by word, it is common
    to use a deep neural network with two submodules, a so-called *encoder* and *decoder*.
    The job of the encoder is to first read in and process the entire text, and the
    decoder then produces the translated text.
  prefs: []
  type: TYPE_NORMAL
- en: We already briefly discussed encoder-decoder networks when we introduced the
    transformer architecture in chapter 1 (section 1.4, Using LLMs for different tasks*)*.
    Before the advent of transformers, *recurrent neural networks* (RNNs) were the
    most popular encoder-decoder architecture for language translation.
  prefs: []
  type: TYPE_NORMAL
- en: An RNN is a type of neural network where outputs from previous steps are fed
    as inputs to the current step, making them well-suited for sequential data like
    text. If you are unfamiliar with RNNs, don't worry, you don't need to know the
    detailed workings of RNNs to follow this discussion; our focus here is more on
    the general concept of the encoder-decoder setup.
  prefs: []
  type: TYPE_NORMAL
- en: In an encoder-decoder RNN, the input text is fed into the encoder, which processes
    it sequentially. The encoder updates its hidden state (the internal values at
    the hidden layers) at each step, trying to capture the entire meaning of the input
    sentence in the final hidden state, as illustrated in Figure 3.4\. The decoder
    then takes this final hidden state to start generating the translated sentence,
    one word at a time. It also updates its hidden state at each step, which is supposed
    to carry the context necessary for the next-word prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 Before the advent of transformer models, encoder-decoder RNNs were
    a popular choice for machine translation. The encoder takes a sequence of tokens
    from the source language as input, where a hidden state (an intermediate neural
    network layer) of the encoder encodes a compressed representation of the entire
    input sequence. Then, the decoder uses its current hidden state to begin the translation,
    token by token.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image007.png)'
  prefs: []
  type: TYPE_IMG
- en: While we don't need to know the inner workings of these encoder-decoder RNNs,
    the key idea here is that the encoder part processes the entire input text into
    a hidden state (memory cell). The decoder then takes in this hidden state to produce
    the output. You can think of this hidden state as an embedding vector, a concept
    we discussed in chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: The big issue and limitation of encoder-decoder RNNs is that the RNN can't directly
    access earlier hidden states from the encoder during the decoding phase. Consequently,
    it relies solely on the current hidden state, which encapsulates all relevant
    information. This can lead to a loss of context, especially in complex sentences
    where dependencies might span long distances.
  prefs: []
  type: TYPE_NORMAL
- en: For readers unfamiliar with RNNs, it is not essential to understand or study
    this architecture as we will not be using it in this book. The takeaway message
    of this section is that encoder-decoder RNNs had a shortcoming that motivated
    the design of attention mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Capturing data dependencies with attention mechanisms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before transformer LLMs, it was common to use RNNs for language modeling tasks
    such as language translation, as mentioned previously. RNNs work fine for translating
    short sentences but don't work well for longer texts as they don't have direct
    access to previous words in the input.
  prefs: []
  type: TYPE_NORMAL
- en: One major shortcoming in this approach is that the RNN must remember the entire
    encoded input in a single hidden state before passing it to the decoder, as illustrated
    in Figure 3.4 in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, researchers developed the so-called *Bahdanau attention* mechanism for
    RNNs in 2014 (named after the first author of the respective paper), which modifies
    the encoder-decoder RNN such that the decoder can selectively access different
    parts of the input sequence at each decoding step as illustrated in Figure 3.5.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 Using an attention mechanism, the text-generating decoder part of
    the network can access all input tokens selectively. This means that some input
    tokens are more important than others for generating a given output token. The
    importance is determined by the so-called attention weights, which we will compute
    later. Note that this figure shows the general idea behind attention and does
    not depict the exact implementation of the Bahdanau mechanism, which is an RNN
    method outside this book's scope.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image009.png)'
  prefs: []
  type: TYPE_IMG
- en: Interestingly, only three years later, researchers found that RNN architectures
    are not required for building deep neural networks for natural language processing
    and proposed the original *transformer* architecture (discussed in chapter 1)
    with a self-attention mechanism inspired by the Bahdanau attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention is a mechanism that allows each position in the input sequence
    to attend to all positions in the same sequence when computing the representation
    of a sequence. Self-attention is a key component of contemporary LLMs based on
    the transformer architecture, such as the GPT series.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on coding and understanding this self-attention mechanism
    used in GPT-like models, as illustrated in Figure 3.6\. In the next chapter, we
    will then code the remaining parts of the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 Self-attention is a mechanism in transformers that is used to compute
    more efficient input representations by allowing each position in a sequence to
    interact with and weigh the importance of all other positions within the same
    sequence. In this chapter, we will code this self-attention mechanism from the
    ground up before we code the remaining parts of the GPT-like LLM in the following
    chapter.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image011.png)'
  prefs: []
  type: TYPE_IMG
- en: 3.3 Attending to different parts of the input with self-attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll now delve into the inner workings of the self-attention mechanism and
    learn how to code it from the ground up. Self-attention serves as the cornerstone
    of every LLM based on the transformer architecture. It's worth noting that this
    topic may require a lot of focus and attention (no pun intended), but once you
    grasp its fundamentals, you will have conquered one of the toughest aspects of
    this book and implementing LLMs in general.
  prefs: []
  type: TYPE_NORMAL
- en: The "self" in self-attention
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In self-attention, the "self" refers to the mechanism's ability to compute attention
    weights by relating different positions within a single input sequence. It assesses
    and learns the relationships and dependencies between various parts of the input
    itself, such as words in a sentence or pixels in an image. This is in contrast
    to traditional attention mechanisms, where the focus is on the relationships between
    elements of two different sequences, such as in sequence-to-sequence models where
    the attention might be between an input sequence and an output sequence, such
    as the example depicted in Figure 3.5.
  prefs: []
  type: TYPE_NORMAL
- en: Since self-attention can appear complex, especially if you are encountering
    it for the first time, we will begin by introducing a simplified version of self-attention
    in the next subsection. Afterwards, in section 3.4, we will then implement the
    self-attention mechanism with trainable weights, which is used in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 A simple self-attention mechanism without trainable weights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we implement a simplified variant of self-attention, free from
    any trainable weights, which is summarized in Figure 3.7\. The goal of this section
    is to illustrate a few key concepts in self-attention before adding trainable
    weights next in section 3.4.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 The goal of self-attention is to compute a context vector, for each
    input element, that combines information from all other input elements. In the
    example depicted in this figure, we compute the context vector *z*^((2)). The
    importance or contribution of each input element for computing *z*^((2)) is determined
    by the attention weights *α*[21] to *α*[2T]. When computing *z*^((2)), the attention
    weights are calculated with respect to input element *x*^((2)) and all other inputs.
    The exact computation of these attention weights is discussed later in this section.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image013.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 shows an input sequence, denoted as *x*, consisting of *T* elements
    represented as *x*^((1)) to *x*^((T)). This sequence typically represents text,
    such as a sentence, that has already been transformed into token embeddings, as
    explained in chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider an input text like *"Your journey starts with one step."*
    In this case, each element of the sequence, such as *x*^((1)), corresponds to
    a *d*-dimensional embedding vector representing a specific token, like "Your."
    In Figure 3.7, these input vectors are shown as 3-dimensional embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: In self-attention, our goal is to calculate context vectors *z*^((i)) for each
    element *x*^((i)) in the input sequence. A *context vector* can be interpreted
    as an enriched embedding vector.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this concept, let's focus on the embedding vector of the second
    input element, *x*^((2)) (which corresponds to the token "journey"), and the corresponding
    context vector, *z*^((2)), shown at the bottom of Figure 3.7\. This enhanced context
    vector, *z*^((2)), is an embedding that contains information about *x*^((2)) and
    all other input elements *x*^((1)) to *x*^((T)).
  prefs: []
  type: TYPE_NORMAL
- en: In self-attention, context vectors play a crucial role. Their purpose is to
    create enriched representations of each element in an input sequence (like a sentence)
    by incorporating information from all other elements in the sequence, as illustrated
    in Figure 3.7\. This is essential in LLMs, which need to understand the relationship
    and relevance of words in a sentence to each other. Later, we will add trainable
    weights that help an LLM learn to construct these context vectors so that they
    are relevant for the LLM to generate the next token.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we implement a simplified self-attention mechanism to compute
    these weights and the resulting context vector one step at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following input sentence, which has already been embedded into
    3-dimensional vectors as discussed in chapter 2\. We choose a small embedding
    dimension for illustration purposes to ensure it fits on the page without line
    breaks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first step of implementing self-attention is to compute the intermediate
    values *ω,* referred to as attention scores, as illustrated in Figure 3.8.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 The overall goal of this section is to illustrate the computation
    of the context vector *z*^((2)) using the second input sequence, *x*^((2)) as
    a query. This figure shows the first intermediate step, computing the attention
    scores *ω* between the query *x*^((2)) and all other input elements as a dot product.
    (Note that the numbers in the figure are truncated to one digit after the decimal
    point to reduce visual clutter.)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8 illustrates how we calculate the intermediate attention scores between
    the query token and each input token. We determine these scores by computing the
    dot product of the query, *x*^((2)), with every other input token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The computed attention scores are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Understanding dot products
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A dot product is essentially just a concise way of multiplying two vectors
    element-wise and then summing the products, which we can demonstrate as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The outputs confirms that the sum of the element-wise multiplication gives
    the same results as the dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Beyond viewing the dot product operation as a mathematical tool that combines
    two vectors to yield a scalar value, the dot product is a measure of similarity
    because it quantifies how much two vectors are aligned: a higher dot product indicates
    a greater degree of alignment or similarity between the vectors. In the context
    of self-attention mechanisms, the dot product determines the extent to which elements
    in a sequence attend to each other: the higher the dot product, the higher the
    similarity and attention score between two elements.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, as shown in Figure 3.9, we normalize each of the attention
    scores that we computed previously.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 After computing the attention scores *ω*[21] to *ω*[2T] with respect
    to the input query x^((2)), the next step is to obtain the attention weights *α*[21]
    to *α*[2T] by normalizing the attention scores.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The main goal behind the normalization shown in Figure 3.9 is to obtain attention
    weights that sum up to 1\. This normalization is a convention that is useful for
    interpretation and for maintaining training stability in an LLM. Here''s a straightforward
    method for achieving this normalization step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As the output shows, the attention weights now sum to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In practice, it''s more common and advisable to use the softmax function for
    normalization. This approach is better at managing extreme values and offers more
    favorable gradient properties during training. Below is a basic implementation
    of the softmax function for normalizing the attention scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As the output shows, the softmax function also meets the objective and normalizes
    the attention weights such that they sum to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In addition, the softmax function ensures that the attention weights are always
    positive. This makes the output interpretable as probabilities or relative importance,
    where higher weights indicate greater importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this naive softmax implementation (`softmax_naive`) may encounter
    numerical instability problems, such as overflow and underflow, when dealing with
    large or small input values. Therefore, in practice, it''s advisable to use the
    PyTorch implementation of softmax, which has been extensively optimized for performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we can see that it yields the same results as our previous `softmax_naive`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we computed the normalized attention weights, we are ready for the
    final step illustrated in Figure 3.10: calculating the context vector *z*^((2))
    by multiplying the embedded input tokens, *x*^((i)), with the corresponding attention
    weights and then summing the resulting vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 The final step, after calculating and normalizing the attention
    scores to obtain the attention weights for query *x*^((2)), is to compute the
    context vector *z*^((2)). This context vector is a combination of all input vectors
    *x*^((1)) *to x*^((T)) weighted by the attention weights.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The context vector *z*^((2)) depicted in Figure 3.10 is calculated as a weighted
    sum of all input vectors. This involves multiplying each input vector by its corresponding
    attention weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of this computation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will generalize this procedure for computing context
    vectors to calculate all context vectors simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Computing attention weights for all input tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we computed attention weights and the context vector
    for input 2, as shown in the highlighted row in Figure 3.11\. Now, we are extending
    this computation to calculate attention weights and context vectors for all inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11 The highlighted row shows the attention weights for the second input
    element as a query, as we computed in the previous section. This section generalizes
    the computation to obtain all other attention weights.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image021.png)'
  prefs: []
  type: TYPE_IMG
- en: We follow the same three steps as before, as summarized in Figure 3.12, except
    that we make a few modifications in the code to compute all context vectors instead
    of only the second context vector, *z*^((2)).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image023.png)'
  prefs: []
  type: TYPE_IMG
- en: First, in step 1 as illustrated in Figure 3.12, we add an additional for-loop
    to compute the dot products for all pairs of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting attention scores are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Each element in the preceding tensor represents an attention score between each
    pair of inputs, as illustrated in Figure 3.11\. Note that the values in Figure
    3.11 are normalized, which is why they differ from the unnormalized attention
    scores in the preceding tensor. We will take care of the normalization later.
  prefs: []
  type: TYPE_NORMAL
- en: 'When computing the preceding attention score tensor, we used for-loops in Python.
    However, for-loops are generally slow, and we can achieve the same results using
    matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visually confirm that the results are the same as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In step 2, as illustrated in Figure 3.12, we now normalize each row so that
    the values in each row sum to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following attention weight tensor that matches the values
    shown in Figure 3.10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we move on to step 3, the final step shown in Figure 3.12, let''s briefly
    verify that the rows indeed all sum to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the third and last step, we now use these attention weights to compute all
    context vectors via matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the resulting output tensor, each row contains a 3-dimensional context vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can double-check that the code is correct by comparing the 2nd row with
    the context vector *z*^((2)) that we computed previously in section 3.3.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the result, we can see that the previously calculated `context_vec_2`
    matches the second row in the previous tensor exactly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the code walkthrough of a simple self-attention mechanism. In
    the next section, we will add trainable weights, enabling the LLM to learn from
    data and improve its performance on specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Implementing self-attention with trainable weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we are implementing the self-attention mechanism that is used
    in the original transformer architecture, the GPT models, and most other popular
    LLMs. This self-attention mechanism is also called *scaled dot-product attention*.
    Figure 3.13 provides a mental model illustrating how this self-attention mechanism
    fits into the broader context of implementing an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.13 A mental model illustrating how the self-attention mechanism we
    code in this section fits into the broader context of this book and chapter. In
    the previous section, we coded a simplified attention mechanism to understand
    the basic mechanism behind attention mechanisms. In this section, we add trainable
    weights to this attention mechanism. In the upcoming sections, we will then extend
    this self-attention mechanism by adding a causal mask and multiple heads.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As illustrated in Figure 3.13 the self-attention mechanism with trainable weights
    builds on the previous concepts: we want to compute context vectors as weighted
    sums over the input vectors specific to a certain input element. As you will see,
    there are only slight differences compared to the basic self-attention mechanism
    we coded earlier in section 3.3.'
  prefs: []
  type: TYPE_NORMAL
- en: The most notable difference is the introduction of weight matrices that are
    updated during model training. These trainable weight matrices are crucial so
    that the model (specifically, the attention module inside the model) can learn
    to produce "good" context vectors. (Note that we will train the LLM in chapter
    5.)
  prefs: []
  type: TYPE_NORMAL
- en: We will tackle this self-attention mechanism in the two subsections. First,
    we will code it step-by-step as before. Second, we will organize the code into
    a compact Python class that can be imported into an LLM architecture, which we
    will code in chapter 4.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Computing the attention weights step by step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will implement the self-attention mechanism step by step by introducing the
    three trainable weight matrices *W*[q], *W*[k], and *W*[v]. These three matrices
    are used to project the embedded input tokens, *x*^((i)), into query, key, and
    value vectors as illustrated in Figure 3.14.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.14 In the first step of the self-attention mechanism with trainable
    weight matrices, we compute query (*q*), key (*k*), and value (*v*) vectors for
    input elements *x*. Similar to previous sections, we designate the second input,
    *x*^((2)), as the query input. The query vector *q*^((2)) is obtained via matrix
    multiplication between the input *x*^((2)) and the weight matrix *W*[q]. Similarly,
    we obtain the key and value vectors via matrix multiplication involving the weight
    matrices *W*[k] and *W*[v].
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image027.png)'
  prefs: []
  type: TYPE_IMG
- en: Earlier in section 3.3.1, we defined the second input element *x*^((2)) as the
    query when we computed the simplified attention weights to compute the context
    vector *z*^((2)). Later, in section 3.3.2, we generalized this to compute all
    context vectors *z*^((1)) *... z*^((T)) for the six-word input sentence *"Your
    journey starts with one step."*
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we will start by computing only one context vector, *z*^((2)), for
    illustration purposes. In the next section, we will modify this code to calculate
    all context vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by defining a few variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note that in GPT-like models, the input and output dimensions are usually the
    same, but for illustration purposes, to better follow the computation, we choose
    different input (`d_in=3`) and output (`d_out=2`) dimensions here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we initialize the three weight matrices *W*[q], *W*[k], and *W*[v] that
    are shown in Figure 3.14:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are setting `requires_grad=False` to reduce clutter in the outputs
    for illustration purposes, but if we were to use the weight matrices for model
    training, we would set `requires_grad=True` to update these matrices during model
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute the query, key, and value vectors as shown earlier in Figure
    3.14:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see based on the output for the query, this results in a 2-dimensional
    vector since we set the number of columns of the corresponding weight matrix,
    via `d_out`, to 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Weight parameters vs attention weights
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note that in the weight matrices *W*, the term "weight" is short for "weight
    parameters," the values of a neural network that are optimized during training.
    This is not to be confused with the attention weights. As we already saw in the
    previous section, attention weights determine the extent to which a context vector
    depends on the different parts of the input, i.e., to what extent the network
    focuses on different parts of the input.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, weight parameters are the fundamental, learned coefficients that
    define the network's connections, while attention weights are dynamic, context-specific
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Even though our temporary goal is to only compute the one context vector, *z*^((2)),
    we still require the key and value vectors for all input elements as they are
    involved in computing the attention weights with respect to the query *q*^((2)),
    as illustrated in Figure 3.14.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can obtain all keys and values via matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can tell from the outputs, we successfully projected the 6 input tokens
    from a 3D onto a 2D embedding space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The second step is now to compute the attention scores, as shown in Figure 3.15.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.15 The attention score computation is a dot-product computation similar
    to what we have used in the simplified self-attention mechanism in section 3.3\.
    The new aspect here is that we are not directly computing the dot-product between
    the input elements but using the query and key obtained by transforming the inputs
    via the respective weight matrices.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, let''s compute the attention score *ω*[22]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The results in the following unnormalized attention score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we can generalize this computation to all attention scores via matrix
    multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, as a quick check, the second element in the output matches `attn_score_22`
    we computed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The third step is now going from the attention scores to the attention weights,
    as illustrated in Figure 3.16.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.16 After computing the attention scores *ω*, the next step is to normalize
    these scores using the softmax function to obtain the attention weights *α*.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, as illustrated in Figure 3.16, we compute the attention weights by scaling
    the attention scores and using the softmax function we used earlier.. The difference
    to earlier is that we now scale the attention scores by dividing them by the square
    root of the embedding dimension of the keys, (note that taking the square root
    is mathematically the same as exponentiating by 0.5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting attention weights are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The rationale behind scaled-dot product attention
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The reason for the normalization by the embedding dimension size is to improve
    the training performance by avoiding small gradients. For instance, when scaling
    up the embedding dimension, which is typically greater than thousand for GPT-like
    LLMs, large dot products can result in very small gradients during backpropagation
    due to the softmax function applied to them. As dot products increase, the softmax
    function behaves more like a step function, resulting in gradients nearing zero.
    These small gradients can drastically slow down learning or cause training to
    stagnate.
  prefs: []
  type: TYPE_NORMAL
- en: The scaling by the square root of the embedding dimension is the reason why
    this self-attention mechanism is also called scaled-dot product attention.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the final step is to compute the context vectors, as illustrated in Figure
    3.17.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.17 In the final step of the self-attention computation, we compute
    the context vector by combining all value vectors via the attention weights.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to section 3.3, where we computed the context vector as a weighted
    sum over the input vectors, we now compute the context vector as a weighted sum
    over the value vectors. Here, the attention weights serve as a weighting factor
    that weighs the respective importance of each value vector. Similar to section
    3.3, we can use matrix multiplication to obtain the output in one step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The contents of the resulting vector are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: So far, we only computed a single context vector, *z*^((2)). In the next section,
    we will generalize the code to compute all context vectors in the input sequence,
    *z*^((1)) to *z*^((T)).
  prefs: []
  type: TYPE_NORMAL
- en: Why query, key, and value?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The terms "key," "query," and "value" in the context of attention mechanisms
    are borrowed from the domain of information retrieval and databases, where similar
    concepts are used to store, search, and retrieve information.
  prefs: []
  type: TYPE_NORMAL
- en: A "query" is analogous to a search query in a database. It represents the current
    item (e.g., a word or token in a sentence) the model focuses on or tries to understand.
    The query is used to probe the other parts of the input sequence to determine
    how much attention to pay to them.
  prefs: []
  type: TYPE_NORMAL
- en: The "key" is like a database key used for indexing and searching. In the attention
    mechanism, each item in the input sequence (e.g., each word in a sentence) has
    an associated key. These keys are used to match with the query.
  prefs: []
  type: TYPE_NORMAL
- en: The "value" in this context is similar to the value in a key-value pair in a
    database. It represents the actual content or representation of the input items.
    Once the model determines which keys (and thus which parts of the input) are most
    relevant to the query (the current focus item), it retrieves the corresponding
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Implementing a compact self-attention Python class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous sections, we have gone through a lot of steps to compute the
    self-attention outputs. This was mainly done for illustration purposes so we could
    go through one step at a time. In practice, with the LLM implementation in the
    next chapter in mind, it is helpful to organize this code into a Python class
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 A compact self-attention class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In this PyTorch code, `SelfAttention_v1` is a class derived from `nn.Module`,
    which is a fundamental building block of PyTorch models, which provides necessary
    functionalities for model layer creation and management.
  prefs: []
  type: TYPE_NORMAL
- en: The `__init__` method initializes trainable weight matrices (`W_query`, `W_key`,
    and `W_value`) for queries, keys, and values, each transforming the input dimension
    `d_in` to an output dimension `d_out`.
  prefs: []
  type: TYPE_NORMAL
- en: During the forward pass, using the forward method, we compute the attention
    scores (`attn_scores`) by multiplying queries and keys, normalizing these scores
    using softmax. Finally, we create a context vector by weighting the values with
    these normalized attention scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `inputs` contains six embedding vectors, this result in a matrix storing
    the six context vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As a quick check, notice how the second row (`[0.3061, 0.8210]`) matches the
    contents of `context_vec_2` in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.18 summarizes the self-attention mechanism we just implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.18 In self-attention, we transform the input vectors in the input matrix
    X with the three weight matrices, Wq, Wk, and Wv. Then, we compute the attention
    weight matrix based on the resulting queries (Q) and keys (K). Using the attention
    weights and values (V), we then compute the context vectors (Z). (For visual clarity,
    we focus on a single input text with *n* tokens in this figure, not a batch of
    multiple inputs. Consequently, the 3D input tensor is simplified to a 2D matrix
    in this context. This approach allows for a more straightforward visualization
    and understanding of the processes involved.)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image035.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in Figure 3.18, self-attention involves the trainable weight matrices
    *W*[q]*, W*[k]*,* and *W*[v]. These matrices transform input data into queries,
    keys, and values, which are crucial components of the attention mechanism. As
    the model is exposed to more data during training, it adjusts these trainable
    weights, as we will see in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We can improve the `SelfAttention_v1` implementation further by utilizing PyTorch's
    `nn.Linear` layers, which effectively perform matrix multiplication when the bias
    units are disabled. Additionally, a significant advantage of using `nn.Linear`
    instead of manually implementing `nn.Parameter(torch.rand(...))` is that `nn.Linear`
    has an optimized weight initialization scheme, contributing to more stable and
    effective model training.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 A self-attention class using PyTorch's Linear layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: You can use the `SelfAttention_v2` similar to `SelfAttention_v1:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Note that `SelfAttention_v1` and `SelfAttention_v2` give different outputs because
    they use different initial weights for the weight matrices since `nn.Linear` uses
    a more sophisticated weight initialization scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note that `nn.Linear` in `SelfAttention_v2` uses a different weight initialization
    scheme as `nn.Parameter(torch.rand(d_in, d_out)`) used in `SelfAttention_v1`,
    which causes both mechanisms to produce different results. To check that both
    implementations, `SelfAttention_v1` and `SelfAttention_v2`, are otherwise similar,
    we can transfer the weight matrices from a `SelfAttention_v2` object to a `SelfAttention_v1`,
    such that both objects then produce the same results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your task is to correctly assign the weights from an instance of `SelfAttention_v2`
    to an instance of `SelfAttention_v1`. To do this, you need to understand the relationship
    between the weights in both versions. (Hint: `nn.Linear` stores the weight matrix
    in a transposed form.) After the assignment, you should observe that both instances
    produce the same outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will make enhancements to the self-attention mechanism,
    focusing specifically on incorporating causal and multi-head elements. The causal
    aspect involves modifying the attention mechanism to prevent the model from accessing
    future information in the sequence, which is crucial for tasks like language modeling,
    where each word prediction should only depend on previous words.
  prefs: []
  type: TYPE_NORMAL
- en: The multi-head component involves splitting the attention mechanism into multiple
    "heads." Each head learns different aspects of the data, allowing the model to
    simultaneously attend to information from different representation subspaces at
    different positions. This improves the model's performance in complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Hiding future words with causal attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we modify the standard self-attention mechanism to create a
    *causal attention* mechanism, which is essential for developing an LLM in the
    subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Causal attention, also known as *masked attention*, is a specialized form of
    self-attention. It restricts a model to only consider previous and current inputs
    in a sequence when processing any given token. This is in contrast to the standard
    self-attention mechanism, which allows access to the entire input sequence at
    once.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, when computing attention scores, the causal attention mechanism
    ensures that the model only factors in tokens that occur at or before the current
    token in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this in GPT-like LLMs, for each token processed, we mask out the
    future tokens, which come after the current token in the input text, as illustrated
    in Figure 3.19.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.19 In causal attention, we mask out the attention weights above the
    diagonal such that for a given input, the LLM can't access future tokens when
    computing the context vectors using the attention weights. For example, for the
    word "journey" in the second row, we only keep the attention weights for the words
    before ("Your") and in the current position ("journey").
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image037.png)'
  prefs: []
  type: TYPE_IMG
- en: As illustrated in Figure 3.19, we mask out the attention weights above the diagonal,
    and we normalize the non-masked attention weights, such that the attention weights
    sum to 1 in each row. In the next section, we will implement this masking and
    normalization procedure in code.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 Applying a causal attention mask
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we implement the causal attention mask in code. We start with
    the procedure summarized in Figure 3.20.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.20 One way to obtain the masked attention weight matrix in causal attention
    is to apply the softmax function to the attention scores, zeroing out the elements
    above the diagonal and normalizing the resulting matrix.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image039.png)'
  prefs: []
  type: TYPE_IMG
- en: To implement the steps to apply a causal attention mask to obtain the masked
    attention weights as summarized in Figure 3.20, let's work with the attention
    scores and weights from the previous section to code the causal attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first step illustrated in Figure 3.20, we compute the attention weights
    using the softmax function as we have done in previous sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following attention weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We can implement step 2 in Figure 3.20 using PyTorch''s `tril` function to
    create a mask where the values above the diagonal are zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting mask is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can multiply this mask with the attention weights to zero out the values
    above the diagonal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the elements above the diagonal are successfully zeroed out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The third step in Figure 3.20 is to renormalize the attention weights to sum
    up to 1 again in each row. We can achieve this by dividing each element in each
    row by the sum in each row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is an attention weight matrix where the attention weights above
    the diagonal are zeroed out and where the rows sum to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Information leakage
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When we apply a mask and then renormalize the attention weights, it might initially
    appear that information from future tokens (which we intend to mask) could still
    influence the current token because their values are part of the softmax calculation.
    However, the key insight is that when we renormalize the attention weights after
    masking, what we're essentially doing is recalculating the softmax over a smaller
    subset (since masked positions don't contribute to the softmax value).
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical elegance of softmax is that despite initially including all
    positions in the denominator, after masking and renormalizing, the effect of the
    masked positions is nullified — they don't contribute to the softmax score in
    any meaningful way.
  prefs: []
  type: TYPE_NORMAL
- en: In simpler terms, after masking and renormalization, the distribution of attention
    weights is as if it was calculated only among the unmasked positions to begin
    with. This ensures there's no information leakage from future (or otherwise masked)
    tokens as we intended.
  prefs: []
  type: TYPE_NORMAL
- en: While we could be technically done with implementing causal attention at this
    point, we can take advantage of a mathematical property of the softmax function
    and implement the computation of the masked attention weights more efficiently
    in fewer steps, as shown in Figure 3.21.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.21 A more efficient way to obtain the masked attention weight matrix
    in causal attention is to mask the attention scores with negative infinity values
    before applying the softmax function.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image041.png)'
  prefs: []
  type: TYPE_IMG
- en: The softmax function converts its inputs into a probability distribution. When
    negative infinity values (-∞) are present in a row, the softmax function treats
    them as zero probability. (Mathematically, this is because *e*^-^∞ approaches
    0.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement this more efficient masking "trick" by creating a mask with
    1''s above the diagonal and then replacing these 1''s with negative infinity (`-inf`)
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, all we need to do is apply the softmax function to these masked results,
    and we are done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see based on the output, the values in each row sum to 1, and no
    further normalization is necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We could now use the modified attention weights to compute the context vectors
    via `context_vec = attn_weights @ values`, as in section 3.4\. However, in the
    next section, we first cover another minor tweak to the causal attention mechanism
    that is useful for reducing overfitting when training LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 Masking additional attention weights with dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Dropout* in deep learning is a technique where randomly selected hidden layer
    units are ignored during training, effectively "dropping" them out. This method
    helps prevent overfitting by ensuring that a model does not become overly reliant
    on any specific set of hidden layer units. It''s important to emphasize that dropout
    is only used during training and is disabled afterward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the transformer architecture, including models like GPT, dropout in the
    attention mechanism is typically applied in two specific areas: after calculating
    the attention scores or after applying the attention weights to the value vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will apply the dropout mask after computing the attention weights,
    as illustrated in Figure 3.22, because it's the more common variant in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.22 Using the causal attention mask (upper left), we apply an additional
    dropout mask (upper right) to zero out additional attention weights to reduce
    overfitting during training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image043.png)'
  prefs: []
  type: TYPE_IMG
- en: In the following code example, we use a dropout rate of 50%, which means masking
    out half of the attention weights. (When we train the GPT model in later chapters,
    we will use a lower dropout rate, such as 0.1 or 0.2.)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we apply PyTorch''s dropout implementation first to
    a 6×6 tensor consisting of ones for illustration purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, approximately half of the values are zeroed out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: When applying dropout to an attention weight matrix with a rate of 50%, half
    of the elements in the matrix are randomly set to zero. To compensate for the
    reduction in active elements, the values of the remaining elements in the matrix
    are scaled up by a factor of 1/0.5 =2\. This scaling is crucial to maintain the
    overall balance of the attention weights, ensuring that the average influence
    of the attention mechanism remains consistent during both the training and inference
    phases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s apply dropout to the attention weight matrix itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting attention weight matrix now has additional elements zeroed out
    and the remaining ones rescaled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Note that the resulting dropout outputs may look different depending on your
    operating system; you can read more about this inconsistency [here on the PyTorch
    issue tracker at [https://github.com/pytorch/pytorch/issues/121595](issues.html).
  prefs: []
  type: TYPE_NORMAL
- en: Having gained an understanding of causal attention and dropout masking, we will
    develop a concise Python class in the following section. This class is designed
    to facilitate the efficient application of these two techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.3 Implementing a compact causal attention class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will now incorporate the causal attention and dropout modifications
    into the `SelfAttention` Python class we developed in section 3.4\. This class
    will then serve as a template for developing *multi-head attention* in the upcoming
    section, which is the final attention class we implement in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: But before we begin, one more thing is to ensure that the code can handle batches
    consisting of more than one input so that the `CausalAttention` class supports
    the batch outputs produced by the data loader we implemented in chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, to simulate such batch inputs, we duplicate the input text
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in a 3D tensor consisting of 2 input texts with 6 tokens each,
    where each token is a 3-dimensional embedding vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `CausalAttention` class is similar to the `SelfAttention` class
    we implemented earlier, except that we now added the dropout and causal mask components
    as highlighted in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 A compact causal attention class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: While all added code lines should be familiar from previous sections, we now
    added a `self.register_buffer()` call in the `__init__` method. The use of `register_buffer`
    in PyTorch is not strictly necessary for all use cases but offers several advantages
    here. For instance, when we use the `CausalAttention` class in our LLM, buffers
    are automatically moved to the appropriate device (CPU or GPU) along with our
    model, which will be relevant when training the LLM in future chapters. This means
    we don't need to manually ensure these tensors are on the same device as your
    model parameters, avoiding device mismatch errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `CausalAttention` class as follows, similar to `SelfAttention`
    previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting context vector is a 3D tensor where each token is now represented
    by a 2D embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Figure 3.23 provides a mental model that summarizes what we have accomplished
    so far.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.23 A mental model summarizing the four different attention modules
    we are coding in this chapter. We began with a simplified attention mechanism,
    added trainable weights, and then added a casual attention mask. In the remainder
    of this chapter, we will extend the causal attention mechanism and code multi-head
    attention, which is the final module we will use in the LLM implementation in
    the next chapter.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image045.png)'
  prefs: []
  type: TYPE_IMG
- en: As illustrated in Figure 3.23, in this section, we focused on the concept and
    implementation of causal attention in neural networks. In the next section, we
    will expand on this concept and implement a multi-head attention module that implements
    several of such causal attention mechanisms in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Extending single-head attention to multi-head attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this final section of this chapter, we are extending the previously implemented
    causal attention class over multiple-heads. This is also called *multi-head attention*.
  prefs: []
  type: TYPE_NORMAL
- en: The term "multi-head" refers to dividing the attention mechanism into multiple
    "heads," each operating independently. In this context, a single causal attention
    module can be considered single-head attention, where there is only one set of
    attention weights processing the input sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will tackle this expansion from causal attention
    to multi-head attention. The first subsection will intuitively build a multi-head
    attention module by stacking multiple `CausalAttention` modules for illustration
    purposes. The second subsection will then implement the same multi-head attention
    module in a more complicated but computationally more efficient way.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.1 Stacking multiple single-head attention layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In practical terms, implementing multi-head attention involves creating multiple
    instances of the self-attention mechanism (depicted earlier in Figure 3.18 in
    section 3.4.1), each with its own weights, and then combining their outputs. Using
    multiple instances of the self-attention mechanism can be computationally intensive,
    but it's crucial for the kind of complex pattern recognition that models like
    transformer-based LLMs are known for.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.24 illustrates the structure of a multi-head attention module, which
    consists of multiple single-head attention modules, as previously depicted in
    Figure 3.18, stacked on top of each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.24 The multi-head attention module in this figure depicts two single-head
    attention modules stacked on top of each other. So, instead of using a single
    matrix *W*[v] for computing the value matrices, in a multi-head attention module
    with two heads, we now have two value weight matrices: *W*[v1] and *W*[v2]. The
    same applies to the other weight matrices, *W*[q] and *W*[k]. We obtain two sets
    of context vectors *Z*[1] and *Z*[2] that we can combine into a single context
    vector matrix *Z*.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image047.png)'
  prefs: []
  type: TYPE_IMG
- en: As mentioned before, the main idea behind multi-head attention is to run the
    attention mechanism multiple times (in parallel) with different, learned linear
    projections -- the results of multiplying the input data (like the query, key,
    and value vectors in attention mechanisms) by a weight matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, we can achieve this by implementing a simple `MultiHeadAttentionWrapper`
    class that stacks multiple instances of our previously implemented `CausalAttention`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.4 A wrapper class to implement multi-head attention
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: For example, if we use this MultiHeadAttentionWrapper class with two attention
    heads (via `num_heads=2`) and CausalAttention output dimension `d_out=2`, this
    results in a 4-dimensional context vectors (`d_out*num_heads=4`), as illustrated
    in Figure 3.25.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.25 Using the `MultiHeadAttentionWrapper`, we specified the number of
    attention heads (`num_heads`). If we set `num_heads=2`, as shown in this figure,
    we obtain a tensor with two sets of context vector matrices. In each context vector
    matrix, the rows represent the context vectors corresponding to the tokens, and
    the columns correspond to the embedding dimension specified via `d_out=4`. We
    concatenate these context vector matrices along the column dimension. Since we
    have 2 attention heads and an embedding dimension of 2, the final embedding dimension
    is 2 × 2 = 4.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image049.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To illustrate Figure 3.25 further with a concrete example, we can use the `MultiHeadAttentionWrapper`
    class similar to the `CausalAttention` class before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following tensor representing the context vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The first dimension of the resulting `context_vecs` tensor is 2 since we have
    two input texts (the input texts are duplicated, which is why the context vectors
    are exactly the same for those). The second dimension refers to the 6 tokens in
    each input. The third dimension refers to the 4-dimensional embedding of each
    token.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3.2 Returning 2-dimensional embedding vectors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Change the input arguments for the `MultiHeadAttentionWrapper(..., num_heads=2)`
    call such that the output context vectors are 2-dimensional instead of 4-dimensional
    while keeping the setting `num_heads=2`. Hint: You don''t have to modify the class
    implementation; you just have to change one of the other input arguments.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we implemented a MultiHeadAttentionWrapper that combined multiple
    single-head attention modules. However, note that these are processed sequentially
    via `[head(x) for head in self.heads]` in the forward method. We can improve this
    implementation by processing the heads in parallel. One way to achieve this is
    by computing the outputs for all attention heads simultaneously via matrix multiplication,
    as we will explore in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.2 Implementing multi-head attention with weight splits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we created a `MultiHeadAttentionWrapper` to implement
    multi-head attention by stacking multiple single-head attention modules. This
    was done by instantiating and combining several `CausalAttention` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of maintaining two separate classes, `MultiHeadAttentionWrapper` and
    `CausalAttention`, we can combine both of these concepts into a single `MultiHeadAttention`
    class. Also, in addition to just merging the `MultiHeadAttentionWrapper` with
    the `CausalAttention` code, we will make some other modifications to implement
    multi-head attention more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: In the `MultiHeadAttentionWrapper`, multiple heads are implemented by creating
    a list of `CausalAttention` objects (`self.heads`), each representing a separate
    attention head. The `CausalAttention` class independently performs the attention
    mechanism, and the results from each head are concatenated. In contrast, the following
    `MultiHeadAttention` class integrates the multi-head functionality within a single
    class. It splits the input into multiple heads by reshaping the projected query,
    key, and value tensors and then combines the results from these heads after computing
    attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `MultiHeadAttention` class before we discuss it further:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.5 An efficient multi-head attention class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Even though the reshaping (`.view`) and transposing (`.transpose`) of tensors
    inside the `MultiHeadAttention` class looks very complicated, mathematically,
    the `MultiHeadAttention` class implements the same concept as the `MultiHeadAttentionWrapper`
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: On a big-picture level, in the previous `MultiHeadAttentionWrapper`, we stacked
    multiple single-head attention layers that we combined into a multi-head attention
    layer. The `MultiHeadAttention` class takes an integrated approach. It starts
    with a multi-head layer and then internally splits this layer into individual
    attention heads, as illustrated in Figure 3.26.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.26 In the `MultiheadAttentionWrapper` class with two attention heads,
    we initialized two weight matrices *W*[q1] and *W*[q2] and computed two query
    matrices *Q*[1] and *Q*[2] as illustrated at the top of this figure. In the `MultiheadAttention`
    class, we initialize one larger weight matrix *W*[q] *,* only perform one matrix
    multiplication with the inputs to obtain a query matrix *Q*, and then split the
    query matrix into *Q*[1] and *Q*[2] as shown at the bottom of this figure. We
    do the same for the keys and values, which are not shown to reduce visual clutter.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image051.png)'
  prefs: []
  type: TYPE_IMG
- en: The splitting of the query, key, and value tensors, as depicted in Figure 3.26,
    is achieved through tensor reshaping and transposing operations using PyTorch's
    `.view` and `.transpose` methods. The input is first transformed (via linear layers
    for queries, keys, and values) and then reshaped to represent multiple heads.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key operation is to split the `d_out` dimension into `num_heads` and `head_dim`,
    where `head_dim = d_out / num_heads`. This splitting is then achieved using the
    `.view` method: a tensor of dimensions `(b, num_tokens, d_out)` is reshaped to
    dimension `(b, num_tokens, num_heads, head_dim)`.'
  prefs: []
  type: TYPE_NORMAL
- en: The tensors are then transposed to bring the `num_heads` dimension before the
    `num_tokens` dimension, resulting in a shape of `(b, num_heads, num_tokens, head_dim)`.
    This transposition is crucial for correctly aligning the queries, keys, and values
    across the different heads and performing batched matrix multiplications efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this batched matrix multiplication, suppose we have the following
    example tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we perform a batched matrix multiplication between the tensor itself and
    a view of the tensor where we transposed the last two dimensions, `num_tokens`
    and `head_dim`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the matrix multiplication implementation in PyTorch handles the
    4-dimensional input tensor so that the matrix multiplication is carried out between
    the 2 last dimensions `(num_tokens, head_dim)` and then repeated for the individual
    heads.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the above becomes a more compact way to compute the matrix multiplication
    for each head separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are exactly the same results that we obtained when using the batched
    matrix multiplication `print(a @ a.transpose(2, 3))` earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Continuing with MultiHeadAttention, after computing the attention weights and
    context vectors, the context vectors from all heads are transposed back to the
    shape `(b, num_tokens, num_heads, head_dim)`. These vectors are then reshaped
    (flattened) into the shape `(b, num_tokens, d_out)`, effectively combining the
    outputs from all heads.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we added a so-called output projection layer (`self.out_proj`)
    to `MultiHeadAttention` after combining the heads, which is not present in the
    `CausalAttention` class. This output projection layer is not strictly necessary
    (see the References section in Appendix B for more details), but it is commonly
    used in many LLM architectures, which is why we added it here for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the `MultiHeadAttention` class looks more complicated than the `MultiHeadAttentionWrapper`
    due to the additional reshaping and transposition of tensors, it is more efficient.
    The reason is that we only need one matrix multiplication to compute the keys,
    for instance, `keys = self.W_key(x)` (the same is true for the queries and values).
    In the MultiHeadAttentionWrapper, we needed to repeat this matrix multiplication,
    which is computationally one of the most expensive steps, for each attention head.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `MultiHeadAttention` class can be used similar to the `SelfAttention` and
    `CausalAttention` classes we implemented earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see based on the results, the output dimension is directly controlled
    by the `d_out` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we implemented the `MultiHeadAttention` class that we will
    use in the upcoming sections when implementing and training the LLM itself. Note
    that while the code is fully functional, we used relatively small embedding sizes
    and numbers of attention heads to keep the outputs readable.
  prefs: []
  type: TYPE_NORMAL
- en: For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention
    heads and a context vector embedding size of 768\. The largest GPT-2 model (1.5
    billion parameters) has 25 attention heads and a context vector embedding size
    of 1600\. Note that the embedding sizes of the token inputs and context embeddings
    are the same in GPT models (`d_in = d_out`).
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3.3 Initializing GPT-2 size attention modules
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using the `MultiHeadAttention` class, initialize a multi-head attention module
    that has the same number of attention heads as the smallest GPT-2 model (12 attention
    heads). Also ensure that you use the respective input and output embedding sizes
    similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports
    a context length of 1024 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Attention mechanisms transform input elements into enhanced context vector representations
    that incorporate information about all inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A self-attention mechanism computes the context vector representation as a weighted
    sum over the inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a simplified attention mechanism, the attention weights are computed via
    dot products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dot product is just a concise way of multiplying two vectors element-wise
    and then summing the products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix multiplications, while not strictly required, help us to implement computations
    more efficiently and compactly by replacing nested for-loops.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In self-attention mechanisms that are used in LLMs, also called scaled-dot
    product attention, we include trainable weight matrices to compute intermediate
    transformations of the inputs: queries, values, and keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When working with LLMs that read and generate text from left to right, we add
    a causal attention mask to prevent the LLM from accessing future tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next to causal attention masks to zero out attention weights, we can also add
    a dropout mask to reduce overfitting in LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attention modules in transformer-based LLMs involve multiple instances of
    causal attention, which is called multi-head attention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can create a multi-head attention module by stacking multiple instances of
    causal attention modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A more efficient way of creating multi-head attention modules involves batched
    matrix multiplications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
