- en: 3 Coding Attention Mechanisms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 编码注意力机制
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Exploring the reasons for using attention mechanisms in neural networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索在神经网络中使用注意力机制的原因
- en: Introducing a basic self-attention framework and progressing to an enhanced
    self-attention mechanism
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入一个基本的自注意力框架，并逐步提升到增强型自注意力机制
- en: Implementing a causal attention module that allows LLMs to generate one token
    at a time
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个因果注意力模块，使 LLM 每次生成一个标记
- en: Masking randomly selected attention weights with dropout to reduce overfitting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 dropout 随机选择的注意力权重进行掩蔽以减少过拟合
- en: Stacking multiple causal attention modules into a multi-head attention module
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多个因果注意力模块堆叠成一个多头注意力模块
- en: In the previous chapter, you learned how to prepare the input text for training
    LLMs. This involved splitting text into individual word and subword tokens, which
    can be encoded into vector representations, the so-called embeddings, for the
    LLM.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何为 LLM 准备输入文本。这涉及将文本拆分为单个词和子词标记，这些标记可以编码为向量表示，即所谓的嵌入，供 LLM 使用。
- en: In this chapter, we will now look at an integral part of the LLM architecture
    itself, attention mechanisms, as illustrated in Figure 3.1.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点关注 LLM 架构本身的重要组成部分，即注意力机制，如图 3.1 所示。
- en: Figure 3.1 A mental model of the three main stages of coding an LLM, pretraining
    the LLM on a general text dataset, and finetuning it on a labeled dataset. This
    chapter focuses on attention mechanisms, which are an integral part of an LLM
    architecture.
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.1 编码 LLM 的三个主要阶段的思维模型，在一般文本数据集上预训练 LLM，并在标记数据集上微调。 本章专注于注意力机制，它们是 LLM 架构的重要组成部分。
- en: '![](images/03__image001.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image001.png)'
- en: Attention mechanisms are a comprehensive topic, which is why we are devoting
    a whole chapter to it. We will largely look at these attention mechanisms in isolation
    and focus on them at a mechanistic level. In the next chapter, we will then code
    the remaining parts of the LLM surrounding the self-attention mechanism to see
    it in action and to create a model to generate text.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是一个全面的话题，这就是我们为何要为其专门设立一个章节。我们将主要独立考察这些注意力机制，并在机制层面上进行深入探讨。在下一章中，我们将编码围绕自注意力机制的
    LLM 其余部分，以观察其工作原理并创建生成文本的模型。
- en: Over the course of this chapter, we will implement four different variants of
    attention mechanisms, as illustrated in Figure 3.2.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将实现四种不同的注意力机制变体，如图 3.2 所示。
- en: Figure 3.2 The figure depicts different attention mechanisms we will code in
    this chapter, starting with a simplified version of self-attention before adding
    the trainable weights. The causal attention mechanism adds a mask to self-attention
    that allows the LLM to generate one word at a time. Finally, multi-head attention
    organizes the attention mechanism into multiple heads, allowing the model to capture
    various aspects of the input data in parallel.
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.2 该图描绘了我们将在本章中编码的不同注意力机制，从简化的自注意力版本开始，然后添加可训练权重。因果注意力机制为自注意力添加了一个掩码，使 LLM
    能够每次生成一个单词。最后，多头注意力将注意力机制组织为多个头，允许模型并行捕捉输入数据的各种特征。
- en: '![](images/03__image003.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image003.png)'
- en: These different attention variants shown in Figure 3.2 build on each other,
    and the goal is to arrive at a compact and efficient implementation of multi-head
    attention at the end of this chapter that we can then plug into the LLM architecture
    we will code in the next chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 中显示的这些不同注意力变体相互依赖，目标是最终实现一个紧凑且高效的多头注意力实现，以便在下一章的 LLM 架构中进行插入。
- en: 3.1 The problem with modeling long sequences
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 模型处理长序列的问题
- en: Before we dive into the *self-attention* mechanism that is at the heart of LLMs
    later in this chapter, what is the problem with architectures without attention
    mechanisms that predate LLMs? Suppose we want to develop a language translation
    model that translates text from one language into another. As shown in Figure
    3.3, we can't simply translate a text word by word due to the grammatical structures
    in the source and target language.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨本章核心的 *自注意力* 机制之前，架构中没有注意力机制的问题是什么？假设我们想开发一个语言翻译模型，将文本从一种语言翻译成另一种语言。如图
    3.3 所示，由于源语言和目标语言的语法结构，我们不能简单地逐字翻译文本。
- en: Figure 3.3 When translating text from one language to another, such as German
    to English, it's not possible to merely translate word by word. Instead, the translation
    process requires contextual understanding and grammar alignment.
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.3 在将文本从一种语言翻译成另一种语言时，例如从德语翻译成英语，仅仅逐字翻译是不可能的。相反，翻译过程需要上下文理解和语法对齐。
- en: '![](images/03__image005.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image005.png)'
- en: To address the issue that we cannot translate text word by word, it is common
    to use a deep neural network with two submodules, a so-called *encoder* and *decoder*.
    The job of the encoder is to first read in and process the entire text, and the
    decoder then produces the translated text.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决不能逐字翻译的问题，通常使用一个具有两个子模块的深度神经网络，所谓的*编码器*和*解码器*。编码器的工作是首先读取并处理整个文本，然后解码器生成翻译后的文本。
- en: We already briefly discussed encoder-decoder networks when we introduced the
    transformer architecture in chapter 1 (section 1.4, Using LLMs for different tasks*)*.
    Before the advent of transformers, *recurrent neural networks* (RNNs) were the
    most popular encoder-decoder architecture for language translation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在第一章（第 1.4 节，*使用 LLMs 执行不同任务*）介绍变换器架构时，我们已经简要讨论了编码器-解码器网络。在变换器出现之前，*递归神经网络*（RNN）是语言翻译中最流行的编码器-解码器架构。
- en: An RNN is a type of neural network where outputs from previous steps are fed
    as inputs to the current step, making them well-suited for sequential data like
    text. If you are unfamiliar with RNNs, don't worry, you don't need to know the
    detailed workings of RNNs to follow this discussion; our focus here is more on
    the general concept of the encoder-decoder setup.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 是一种神经网络，其中前一步的输出作为输入反馈给当前步骤，使其非常适合处理像文本这样的序列数据。如果你对 RNN 不熟悉，不用担心，你不需要了解
    RNN 的详细工作原理来跟上这个讨论；我们这里的重点更多是在于编码器-解码器架构的一般概念。
- en: In an encoder-decoder RNN, the input text is fed into the encoder, which processes
    it sequentially. The encoder updates its hidden state (the internal values at
    the hidden layers) at each step, trying to capture the entire meaning of the input
    sentence in the final hidden state, as illustrated in Figure 3.4\. The decoder
    then takes this final hidden state to start generating the translated sentence,
    one word at a time. It also updates its hidden state at each step, which is supposed
    to carry the context necessary for the next-word prediction.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器-解码器 RNN 中，输入文本被送入编码器，编码器按顺序处理它。编码器在每一步更新其隐藏状态（隐藏层的内部值），试图在最终隐藏状态中捕捉输入句子的整个含义，如图
    3.4 所示。然后，解码器利用这个最终隐藏状态开始逐字生成翻译句子。它还在每一步更新其隐藏状态，以便携带下一个单词预测所需的上下文。
- en: Figure 3.4 Before the advent of transformer models, encoder-decoder RNNs were
    a popular choice for machine translation. The encoder takes a sequence of tokens
    from the source language as input, where a hidden state (an intermediate neural
    network layer) of the encoder encodes a compressed representation of the entire
    input sequence. Then, the decoder uses its current hidden state to begin the translation,
    token by token.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.4 在变换器模型出现之前，编码器-解码器 RNN 是机器翻译的热门选择。编码器将来自源语言的一个令牌序列作为输入，其中编码器的隐藏状态（一个中间神经网络层）对整个输入序列进行了压缩表示编码。然后，解码器利用其当前的隐藏状态开始逐个令牌进行翻译。
- en: '![](images/03__image007.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image007.png)'
- en: While we don't need to know the inner workings of these encoder-decoder RNNs,
    the key idea here is that the encoder part processes the entire input text into
    a hidden state (memory cell). The decoder then takes in this hidden state to produce
    the output. You can think of this hidden state as an embedding vector, a concept
    we discussed in chapter 2.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不需要了解这些编码器-解码器 RNN 的内部工作原理，但关键思想是编码器部分将整个输入文本处理成一个隐藏状态（记忆单元）。解码器然后使用这个隐藏状态来生成输出。你可以将这个隐藏状态视为一个嵌入向量，这是我们在第二章讨论的一个概念。
- en: The big issue and limitation of encoder-decoder RNNs is that the RNN can't directly
    access earlier hidden states from the encoder during the decoding phase. Consequently,
    it relies solely on the current hidden state, which encapsulates all relevant
    information. This can lead to a loss of context, especially in complex sentences
    where dependencies might span long distances.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器 RNN 的一个重大问题和局限性是，在解码阶段，RNN 不能直接访问编码器的早期隐藏状态。因此，它完全依赖于当前的隐藏状态，该状态封装了所有相关信息。这可能导致上下文丢失，尤其是在依赖关系可能跨越长距离的复杂句子中。
- en: For readers unfamiliar with RNNs, it is not essential to understand or study
    this architecture as we will not be using it in this book. The takeaway message
    of this section is that encoder-decoder RNNs had a shortcoming that motivated
    the design of attention mechanisms.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不熟悉RNN的读者来说，理解或学习这种架构并非必要，因为本书中不会使用它。本节的主要信息是编码器-解码器RNN存在一个缺陷，这促使了注意力机制的设计。
- en: 3.2 Capturing data dependencies with attention mechanisms
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 使用注意力机制捕捉数据依赖性
- en: Before transformer LLMs, it was common to use RNNs for language modeling tasks
    such as language translation, as mentioned previously. RNNs work fine for translating
    short sentences but don't work well for longer texts as they don't have direct
    access to previous words in the input.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器LLM出现之前，RNN通常用于语言建模任务，如语言翻译，正如之前提到的。RNN在翻译短句时表现良好，但在处理较长文本时效果不佳，因为它们无法直接访问输入中的前一个词。
- en: One major shortcoming in this approach is that the RNN must remember the entire
    encoded input in a single hidden state before passing it to the decoder, as illustrated
    in Figure 3.4 in the previous section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个主要缺陷是，RNN必须在将整个编码输入传递给解码器之前在单个隐藏状态中记住所有内容，如前一节中的图3.4所示。
- en: Hence, researchers developed the so-called *Bahdanau attention* mechanism for
    RNNs in 2014 (named after the first author of the respective paper), which modifies
    the encoder-decoder RNN such that the decoder can selectively access different
    parts of the input sequence at each decoding step as illustrated in Figure 3.5.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，研究人员在2014年为RNN开发了所谓的*Bahdanau注意力*机制（以相关论文的第一作者命名），该机制修改了编码器-解码器RNN，使得解码器可以在每个解码步骤中选择性地访问输入序列的不同部分，如图3.5所示。
- en: Figure 3.5 Using an attention mechanism, the text-generating decoder part of
    the network can access all input tokens selectively. This means that some input
    tokens are more important than others for generating a given output token. The
    importance is determined by the so-called attention weights, which we will compute
    later. Note that this figure shows the general idea behind attention and does
    not depict the exact implementation of the Bahdanau mechanism, which is an RNN
    method outside this book's scope.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5 使用注意力机制，网络的文本生成解码器部分可以选择性地访问所有输入标记。这意味着某些输入标记对于生成给定的输出标记更为重要。重要性由所谓的注意力权重决定，我们稍后将计算这些权重。请注意，该图展示了注意力的总体思路，并未描绘Bahdanau机制的确切实现，该机制是本书范围之外的RNN方法。
- en: '![](images/03__image009.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image009.png)'
- en: Interestingly, only three years later, researchers found that RNN architectures
    are not required for building deep neural networks for natural language processing
    and proposed the original *transformer* architecture (discussed in chapter 1)
    with a self-attention mechanism inspired by the Bahdanau attention mechanism.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，仅仅三年后，研究人员发现构建用于自然语言处理的深度神经网络并不需要RNN架构，并提出了最初的*变换器*架构（在第1章讨论），其自注意力机制受到Bahdanau注意力机制的启发。
- en: Self-attention is a mechanism that allows each position in the input sequence
    to attend to all positions in the same sequence when computing the representation
    of a sequence. Self-attention is a key component of contemporary LLMs based on
    the transformer architecture, such as the GPT series.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是一种机制，使得输入序列中的每个位置在计算序列表示时可以关注同一序列中的所有位置。自注意力是基于变换器架构的当代大型语言模型（LLM）的关键组成部分，如GPT系列。
- en: This chapter focuses on coding and understanding this self-attention mechanism
    used in GPT-like models, as illustrated in Figure 3.6\. In the next chapter, we
    will then code the remaining parts of the LLM.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本章专注于编码和理解在GPT类模型中使用的自注意力机制，如图3.6所示。在下一章中，我们将编码LLM的其余部分。
- en: Figure 3.6 Self-attention is a mechanism in transformers that is used to compute
    more efficient input representations by allowing each position in a sequence to
    interact with and weigh the importance of all other positions within the same
    sequence. In this chapter, we will code this self-attention mechanism from the
    ground up before we code the remaining parts of the GPT-like LLM in the following
    chapter.
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6 自注意力是变换器中的一种机制，用于通过允许序列中的每个位置与同一序列中的所有其他位置互动并权衡其重要性，从而计算更高效的输入表示。在本章中，我们将从头开始编码这种自注意力机制，然后在下一章中编码GPT类LLM的其余部分。
- en: '![](images/03__image011.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image011.png)'
- en: 3.3 Attending to different parts of the input with self-attention
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 用自注意力关注输入的不同部分
- en: We'll now delve into the inner workings of the self-attention mechanism and
    learn how to code it from the ground up. Self-attention serves as the cornerstone
    of every LLM based on the transformer architecture. It's worth noting that this
    topic may require a lot of focus and attention (no pun intended), but once you
    grasp its fundamentals, you will have conquered one of the toughest aspects of
    this book and implementing LLMs in general.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将深入探讨自注意力机制的内部工作原理，并学习如何从头开始编码它。自注意力是每个基于变换器架构的大型语言模型的基石。值得注意的是，这个主题可能需要大量的专注和注意（并不是开玩笑），但一旦你掌握了它的基本原理，你就会征服本书中最困难的方面之一，以及一般实现大型语言模型的挑战。
- en: The "self" in self-attention
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自注意力中的“自”
- en: In self-attention, the "self" refers to the mechanism's ability to compute attention
    weights by relating different positions within a single input sequence. It assesses
    and learns the relationships and dependencies between various parts of the input
    itself, such as words in a sentence or pixels in an image. This is in contrast
    to traditional attention mechanisms, where the focus is on the relationships between
    elements of two different sequences, such as in sequence-to-sequence models where
    the attention might be between an input sequence and an output sequence, such
    as the example depicted in Figure 3.5.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力中，“自”指的是该机制通过关联单个输入序列中的不同位置来计算注意力权重的能力。它评估并学习输入自身不同部分之间的关系和依赖性，例如句子中的单词或图像中的像素。这与传统的注意力机制不同，后者关注两个不同序列中元素之间的关系，例如在序列到序列模型中，注意力可能是在输入序列和输出序列之间，如图3.5所示的例子。
- en: Since self-attention can appear complex, especially if you are encountering
    it for the first time, we will begin by introducing a simplified version of self-attention
    in the next subsection. Afterwards, in section 3.4, we will then implement the
    self-attention mechanism with trainable weights, which is used in LLMs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自注意力看起来可能比较复杂，特别是如果你第一次接触它，我们将在下一小节中介绍自注意力的简化版本。之后，在3.4节中，我们将实现带有可训练权重的自注意力机制，这在大型语言模型中使用。
- en: 3.3.1 A simple self-attention mechanism without trainable weights
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 无可训练权重的简单自注意力机制
- en: In this section, we implement a simplified variant of self-attention, free from
    any trainable weights, which is summarized in Figure 3.7\. The goal of this section
    is to illustrate a few key concepts in self-attention before adding trainable
    weights next in section 3.4.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实现一种不带任何可训练权重的自注意力简化变体，如图3.7所示。本节的目标是说明自注意力中的一些关键概念，然后再在3.4节中添加可训练权重。
- en: Figure 3.7 The goal of self-attention is to compute a context vector, for each
    input element, that combines information from all other input elements. In the
    example depicted in this figure, we compute the context vector *z*^((2)). The
    importance or contribution of each input element for computing *z*^((2)) is determined
    by the attention weights *α*[21] to *α*[2T]. When computing *z*^((2)), the attention
    weights are calculated with respect to input element *x*^((2)) and all other inputs.
    The exact computation of these attention weights is discussed later in this section.
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7 自注意力的目标是为每个输入元素计算一个上下文向量，该向量结合来自所有其他输入元素的信息。在图中所示的示例中，我们计算上下文向量*z*^((2))。每个输入元素在计算*z*^((2))时的重要性或贡献由注意力权重*α*[21]到*α*[2T]决定。在计算*z*^((2))时，注意力权重是相对于输入元素*x*^((2))和所有其他输入计算的。这些注意力权重的确切计算将在本节后面讨论。
- en: '![](images/03__image013.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image013.png)'
- en: Figure 3.7 shows an input sequence, denoted as *x*, consisting of *T* elements
    represented as *x*^((1)) to *x*^((T)). This sequence typically represents text,
    such as a sentence, that has already been transformed into token embeddings, as
    explained in chapter 2.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7显示了一个输入序列，记作*x*，由*T*个元素组成，表示为*x*^((1))到*x*^((T))。该序列通常表示文本，例如句子，已经转换为标记嵌入，如第2章所解释。
- en: For example, consider an input text like *"Your journey starts with one step."*
    In this case, each element of the sequence, such as *x*^((1)), corresponds to
    a *d*-dimensional embedding vector representing a specific token, like "Your."
    In Figure 3.7, these input vectors are shown as 3-dimensional embeddings.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个输入文本，例如*“你的旅程始于一步。”* 在这种情况下，序列中的每个元素，例如*x*^((1))，对应于表示特定标记的*d*维嵌入向量，如“你的。”在图3.7中，这些输入向量显示为3维嵌入。
- en: In self-attention, our goal is to calculate context vectors *z*^((i)) for each
    element *x*^((i)) in the input sequence. A *context vector* can be interpreted
    as an enriched embedding vector.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力中，我们的目标是为输入序列中每个元素 *x*^((i)) 计算上下文向量 *z*^((i))。*上下文向量* 可以解释为一个丰富的嵌入向量。
- en: To illustrate this concept, let's focus on the embedding vector of the second
    input element, *x*^((2)) (which corresponds to the token "journey"), and the corresponding
    context vector, *z*^((2)), shown at the bottom of Figure 3.7\. This enhanced context
    vector, *z*^((2)), is an embedding that contains information about *x*^((2)) and
    all other input elements *x*^((1)) to *x*^((T)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个概念，让我们关注第二个输入元素的嵌入向量，*x*^((2))（对应于标记“journey”），以及相应的上下文向量 *z*^((2))，如图3.7底部所示。这个增强的上下文向量
    *z*^((2)) 是一个嵌入，包含关于 *x*^((2)) 和所有其他输入元素 *x*^((1)) 到 *x*^((T)) 的信息。
- en: In self-attention, context vectors play a crucial role. Their purpose is to
    create enriched representations of each element in an input sequence (like a sentence)
    by incorporating information from all other elements in the sequence, as illustrated
    in Figure 3.7\. This is essential in LLMs, which need to understand the relationship
    and relevance of words in a sentence to each other. Later, we will add trainable
    weights that help an LLM learn to construct these context vectors so that they
    are relevant for the LLM to generate the next token.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力中，上下文向量起着关键作用。它们的目的是通过结合序列中所有其他元素的信息，创建输入序列中每个元素的丰富表示（如一个句子），如图3.7所示。这在大型语言模型（LLMs）中至关重要，因为它们需要理解句子中单词之间的关系和相关性。稍后，我们将添加可训练的权重，以帮助LLM学习构建这些上下文向量，使其对于生成下一个标记是相关的。
- en: In this section, we implement a simplified self-attention mechanism to compute
    these weights and the resulting context vector one step at a time.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实现一个简化的自注意力机制，以一步一步计算这些权重和结果上下文向量。
- en: 'Consider the following input sentence, which has already been embedded into
    3-dimensional vectors as discussed in chapter 2\. We choose a small embedding
    dimension for illustration purposes to ensure it fits on the page without line
    breaks:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下输入句子，该句子已嵌入到三维向量中，如第2章所讨论的。为了确保其适合页面而不换行，我们选择一个较小的嵌入维度进行说明：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first step of implementing self-attention is to compute the intermediate
    values *ω,* referred to as attention scores, as illustrated in Figure 3.8.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 实现自注意力的第一步是计算中间值 *ω*，即注意力分数，如图3.8所示。
- en: Figure 3.8 The overall goal of this section is to illustrate the computation
    of the context vector *z*^((2)) using the second input sequence, *x*^((2)) as
    a query. This figure shows the first intermediate step, computing the attention
    scores *ω* between the query *x*^((2)) and all other input elements as a dot product.
    (Note that the numbers in the figure are truncated to one digit after the decimal
    point to reduce visual clutter.)
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8 本节的总体目标是通过将第二个输入序列 *x*^((2)) 作为查询来说明上下文向量 *z*^((2)) 的计算。此图显示了第一步中间步骤，即计算查询
    *x*^((2)) 与所有其他输入元素之间的注意力分数 *ω* 的点积。（请注意，图中的数字被截断为小数点后一个数字，以减少视觉混乱。）
- en: '![](images/03__image015.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image015.png)'
- en: 'Figure 3.8 illustrates how we calculate the intermediate attention scores between
    the query token and each input token. We determine these scores by computing the
    dot product of the query, *x*^((2)), with every other input token:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 说明了我们如何计算查询标记与每个输入标记之间的中间注意力分数。我们通过计算查询 *x*^((2)) 与每个其他输入标记的点积来确定这些分数：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The computed attention scores are as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出的注意力分数如下：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Understanding dot products
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 理解点积
- en: 'A dot product is essentially just a concise way of multiplying two vectors
    element-wise and then summing the products, which we can demonstrate as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 点积本质上是逐元素相乘并求和的简洁方式，我们可以如下演示：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The outputs confirms that the sum of the element-wise multiplication gives
    the same results as the dot product:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输出确认了逐元素乘法的总和与点积的结果相同：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Beyond viewing the dot product operation as a mathematical tool that combines
    two vectors to yield a scalar value, the dot product is a measure of similarity
    because it quantifies how much two vectors are aligned: a higher dot product indicates
    a greater degree of alignment or similarity between the vectors. In the context
    of self-attention mechanisms, the dot product determines the extent to which elements
    in a sequence attend to each other: the higher the dot product, the higher the
    similarity and attention score between two elements.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将点积运算视为一个将两个向量组合以产生标量值的数学工具外，点积也是相似性的度量，因为它量化了两个向量的对齐程度：较高的点积表示两个向量之间更大的对齐或相似度。在自注意力机制的上下文中，点积决定了序列中元素彼此关注的程度：点积越高，两个元素之间的相似度和注意力分数就越高。
- en: In the next step, as shown in Figure 3.9, we normalize each of the attention
    scores that we computed previously.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，如图3.9所示，我们对之前计算的每个注意力分数进行归一化。
- en: Figure 3.9 After computing the attention scores *ω*[21] to *ω*[2T] with respect
    to the input query x^((2)), the next step is to obtain the attention weights *α*[21]
    to *α*[2T] by normalizing the attention scores.
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9 在针对输入查询x^((2))计算注意力分数*ω*[21]到*ω*[2T]之后，下一步是通过归一化注意力分数来获得注意力权重*α*[21]到*α*[2T]。
- en: '![](images/03__image017.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image017.png)'
- en: 'The main goal behind the normalization shown in Figure 3.9 is to obtain attention
    weights that sum up to 1\. This normalization is a convention that is useful for
    interpretation and for maintaining training stability in an LLM. Here''s a straightforward
    method for achieving this normalization step:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9中所示的归一化的主要目标是获得总和为1的注意力权重。这种归一化是一种有利于解释和维护LLM训练稳定性的惯例。以下是实现这一归一化步骤的简单方法：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As the output shows, the attention weights now sum to 1:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，注意力权重现在的总和为1：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In practice, it''s more common and advisable to use the softmax function for
    normalization. This approach is better at managing extreme values and offers more
    favorable gradient properties during training. Below is a basic implementation
    of the softmax function for normalizing the attention scores:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，更常见且建议使用softmax函数进行归一化。这种方法更好地管理极端值，并在训练期间提供更有利的梯度特性。以下是一个基本的softmax函数实现，用于归一化注意力分数：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As the output shows, the softmax function also meets the objective and normalizes
    the attention weights such that they sum to 1:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，softmax函数也满足目标，并将注意力权重归一化，使其总和为1：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In addition, the softmax function ensures that the attention weights are always
    positive. This makes the output interpretable as probabilities or relative importance,
    where higher weights indicate greater importance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，softmax函数确保注意力权重始终为正。这使得输出可解释为概率或相对重要性，其中更高的权重表示更大的重要性。
- en: 'Note that this naive softmax implementation (`softmax_naive`) may encounter
    numerical instability problems, such as overflow and underflow, when dealing with
    large or small input values. Therefore, in practice, it''s advisable to use the
    PyTorch implementation of softmax, which has been extensively optimized for performance:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种简单的softmax实现(`softmax_naive`)在处理大或小输入值时可能会遇到数值不稳定问题，例如溢出和下溢。因此，在实际操作中，建议使用经过广泛优化的PyTorch
    softmax实现：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this case, we can see that it yields the same results as our previous `softmax_naive`
    function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以看到它产生了与之前的`softmax_naive`函数相同的结果：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we computed the normalized attention weights, we are ready for the
    final step illustrated in Figure 3.10: calculating the context vector *z*^((2))
    by multiplying the embedded input tokens, *x*^((i)), with the corresponding attention
    weights and then summing the resulting vectors.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们计算了归一化的注意力权重，准备好进行图3.10中所示的最后一步：通过将嵌入的输入标记*x*^((i))与相应的注意力权重相乘，然后对结果向量求和，计算上下文向量*z*^((2))。
- en: Figure 3.10 The final step, after calculating and normalizing the attention
    scores to obtain the attention weights for query *x*^((2)), is to compute the
    context vector *z*^((2)). This context vector is a combination of all input vectors
    *x*^((1)) *to x*^((T)) weighted by the attention weights.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10 在计算和归一化注意力分数以获得查询*x*^((2))的注意力权重之后，最后一步是计算上下文向量*z*^((2))。这个上下文向量是所有输入向量*x*^((1))到*x*^((T))的组合，按注意力权重加权。
- en: '![](images/03__image019.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image019.png)'
- en: 'The context vector *z*^((2)) depicted in Figure 3.10 is calculated as a weighted
    sum of all input vectors. This involves multiplying each input vector by its corresponding
    attention weight:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10中描绘的上下文向量*z*^((2))是作为所有输入向量的加权和计算得出的。这涉及将每个输入向量乘以其对应的注意力权重：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The results of this computation are as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此计算的结果如下：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the next section, we will generalize this procedure for computing context
    vectors to calculate all context vectors simultaneously.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将对计算上下文向量的过程进行概括，以便同时计算所有上下文向量。
- en: 3.3.2 Computing attention weights for all input tokens
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 为所有输入令牌计算注意力权重
- en: In the previous section, we computed attention weights and the context vector
    for input 2, as shown in the highlighted row in Figure 3.11\. Now, we are extending
    this computation to calculate attention weights and context vectors for all inputs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们计算了输入2的注意力权重和上下文向量，如图3.11中突出显示的行所示。现在，我们将扩展这一计算，以计算所有输入的注意力权重和上下文向量。
- en: Figure 3.11 The highlighted row shows the attention weights for the second input
    element as a query, as we computed in the previous section. This section generalizes
    the computation to obtain all other attention weights.
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.11 突出显示的行显示了第二个输入元素作为查询的注意力权重，正如我们在前一部分中计算的。这一部分将计算推广到获取所有其他注意力权重。
- en: '![](images/03__image021.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image021.png)'
- en: We follow the same three steps as before, as summarized in Figure 3.12, except
    that we make a few modifications in the code to compute all context vectors instead
    of only the second context vector, *z*^((2)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循与之前相同的三个步骤，如图3.12所总结的，除了在代码中进行了一些修改，以计算所有上下文向量，而不仅仅是第二个上下文向量*z*^((2))。
- en: Figure 3.12
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.12
- en: '![](images/03__image023.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image023.png)'
- en: First, in step 1 as illustrated in Figure 3.12, we add an additional for-loop
    to compute the dot products for all pairs of inputs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如图3.12所示，在步骤1中，我们添加了一个额外的for循环，以计算所有输入对的点积。
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The resulting attention scores are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的注意力得分如下：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Each element in the preceding tensor represents an attention score between each
    pair of inputs, as illustrated in Figure 3.11\. Note that the values in Figure
    3.11 are normalized, which is why they differ from the unnormalized attention
    scores in the preceding tensor. We will take care of the normalization later.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的张量中的每个元素表示每对输入之间的注意力得分，如图3.11所示。请注意，图3.11中的值是经过标准化的，这就是它们与前面张量中未标准化的注意力得分不同的原因。我们稍后会处理标准化。
- en: 'When computing the preceding attention score tensor, we used for-loops in Python.
    However, for-loops are generally slow, and we can achieve the same results using
    matrix multiplication:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算前面的注意力得分张量时，我们使用了Python中的for循环。然而，for循环通常比较慢，我们可以通过矩阵乘法实现相同的结果：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can visually confirm that the results are the same as before:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直观地确认结果与之前相同：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In step 2, as illustrated in Figure 3.12, we now normalize each row so that
    the values in each row sum to 1:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，如图3.12所示，我们现在对每一行进行标准化，使每一行的值之和为1：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This returns the following attention weight tensor that matches the values
    shown in Figure 3.10:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回以下注意力权重张量，其值与图3.10中显示的值一致：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Before we move on to step 3, the final step shown in Figure 3.12, let''s briefly
    verify that the rows indeed all sum to 1:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续到步骤3之前，让我们简单验证一下每一行确实都加和为1：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The result is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the third and last step, we now use these attention weights to compute all
    context vectors via matrix multiplication:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个也是最后一个步骤中，我们现在使用这些注意力权重通过矩阵乘法计算所有上下文向量：
- en: '[PRE21]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the resulting output tensor, each row contains a 3-dimensional context vector:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在结果输出张量中，每一行包含一个三维上下文向量：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can double-check that the code is correct by comparing the 2nd row with
    the context vector *z*^((2)) that we computed previously in section 3.3.1:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将第二行与我们在3.3.1节中之前计算的上下文向量*z*^((2))进行比较，来再次确认代码的正确性：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Based on the result, we can see that the previously calculated `context_vec_2`
    matches the second row in the previous tensor exactly:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果，我们可以看到之前计算的`context_vec_2`与前面张量中的第二行完全一致：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This concludes the code walkthrough of a simple self-attention mechanism. In
    the next section, we will add trainable weights, enabling the LLM to learn from
    data and improve its performance on specific tasks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了简单自注意力机制的代码演示。在下一部分中，我们将添加可训练的权重，使LLM能够从数据中学习，并提高其在特定任务上的表现。
- en: 3.4 Implementing self-attention with trainable weights
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 实现带可训练权重的自注意力机制
- en: In this section, we are implementing the self-attention mechanism that is used
    in the original transformer architecture, the GPT models, and most other popular
    LLMs. This self-attention mechanism is also called *scaled dot-product attention*.
    Figure 3.13 provides a mental model illustrating how this self-attention mechanism
    fits into the broader context of implementing an LLM.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实现的是用于原始变压器架构、GPT模型及大多数其他流行LLM的自注意力机制。这个自注意力机制也称为*缩放点积注意力*。图3.13提供了一个心理模型，说明了这个自注意力机制如何适应实现LLM的更广泛背景。
- en: Figure 3.13 A mental model illustrating how the self-attention mechanism we
    code in this section fits into the broader context of this book and chapter. In
    the previous section, we coded a simplified attention mechanism to understand
    the basic mechanism behind attention mechanisms. In this section, we add trainable
    weights to this attention mechanism. In the upcoming sections, we will then extend
    this self-attention mechanism by adding a causal mask and multiple heads.
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.13 一个心理模型，说明我们在本节中编码的自注意力机制如何适应本书和本章的更广泛背景。在前一节中，我们编码了一个简化的注意力机制，以理解注意力机制背后的基本机制。在本节中，我们为这个注意力机制添加了可训练权重。在接下来的部分中，我们将通过添加因果掩码和多个头来扩展这个自注意力机制。
- en: '![](images/03__image025.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image025.png)'
- en: 'As illustrated in Figure 3.13 the self-attention mechanism with trainable weights
    builds on the previous concepts: we want to compute context vectors as weighted
    sums over the input vectors specific to a certain input element. As you will see,
    there are only slight differences compared to the basic self-attention mechanism
    we coded earlier in section 3.3.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3.13所示，带可训练权重的自注意力机制基于之前的概念：我们希望计算特定输入元素的输入向量的加权和作为上下文向量。正如你将看到的，与我们在3.3节中编码的基本自注意力机制相比，只有微小的差异。
- en: The most notable difference is the introduction of weight matrices that are
    updated during model training. These trainable weight matrices are crucial so
    that the model (specifically, the attention module inside the model) can learn
    to produce "good" context vectors. (Note that we will train the LLM in chapter
    5.)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最显著的区别是引入了在模型训练期间更新的权重矩阵。这些可训练权重矩阵对于模型（特别是模型内部的注意力模块）学习生成“良好”上下文向量至关重要。（请注意，我们将在第五章训练LLM。）
- en: We will tackle this self-attention mechanism in the two subsections. First,
    we will code it step-by-step as before. Second, we will organize the code into
    a compact Python class that can be imported into an LLM architecture, which we
    will code in chapter 4.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在两个小节中深入探讨这个自注意力机制。首先，我们将像之前一样一步步编码。其次，我们将把代码整理成一个紧凑的Python类，以便可以导入到LLM架构中，我们将在第四章中编码。
- en: 3.4.1 Computing the attention weights step by step
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 步骤计算注意力权重
- en: We will implement the self-attention mechanism step by step by introducing the
    three trainable weight matrices *W*[q], *W*[k], and *W*[v]. These three matrices
    are used to project the embedded input tokens, *x*^((i)), into query, key, and
    value vectors as illustrated in Figure 3.14.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步实现自注意力机制，介绍三个可训练权重矩阵*W*[q]、*W*[k]和*W*[v]。这三个矩阵用于将嵌入的输入标记*x*^((i))投影到查询、键和值向量，如图3.14所示。
- en: Figure 3.14 In the first step of the self-attention mechanism with trainable
    weight matrices, we compute query (*q*), key (*k*), and value (*v*) vectors for
    input elements *x*. Similar to previous sections, we designate the second input,
    *x*^((2)), as the query input. The query vector *q*^((2)) is obtained via matrix
    multiplication between the input *x*^((2)) and the weight matrix *W*[q]. Similarly,
    we obtain the key and value vectors via matrix multiplication involving the weight
    matrices *W*[k] and *W*[v].
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.14 在带可训练权重矩阵的自注意力机制的第一步中，我们为输入元素*x*计算查询（*q*）、键（*k*）和值（*v*）向量。与之前的部分类似，我们将第二个输入*x*^((2))指定为查询输入。查询向量*q*^((2))通过输入*x*^((2))与权重矩阵*W*[q]的矩阵乘法获得。类似地，我们通过涉及权重矩阵*W*[k]和*W*[v]的矩阵乘法获得键和值向量。
- en: '![](images/03__image027.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image027.png)'
- en: Earlier in section 3.3.1, we defined the second input element *x*^((2)) as the
    query when we computed the simplified attention weights to compute the context
    vector *z*^((2)). Later, in section 3.3.2, we generalized this to compute all
    context vectors *z*^((1)) *... z*^((T)) for the six-word input sentence *"Your
    journey starts with one step."*
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3.3.1节中，我们将第二个输入元素*x*^((2))定义为计算简化注意力权重以获得上下文向量*z*^((2))时的查询。随后，在第3.3.2节中，我们将其推广为计算六个单词输入句子*“你的旅程始于一步。”*的所有上下文向量*z*^((1))
    *... z*^((T))。
- en: Similarly, we will start by computing only one context vector, *z*^((2)), for
    illustration purposes. In the next section, we will modify this code to calculate
    all context vectors.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，为了说明，我们将首先计算一个上下文向量*z*^((2))。在下一节中，我们将修改此代码以计算所有上下文向量。
- en: 'Let''s begin by defining a few variables:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始定义几个变量：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that in GPT-like models, the input and output dimensions are usually the
    same, but for illustration purposes, to better follow the computation, we choose
    different input (`d_in=3`) and output (`d_out=2`) dimensions here.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在GPT类模型中，输入和输出维度通常是相同的，但为了更好地跟随计算，我们在这里选择不同的输入（`d_in=3`）和输出（`d_out=2`）维度。
- en: 'Next, we initialize the three weight matrices *W*[q], *W*[k], and *W*[v] that
    are shown in Figure 3.14:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化图3.14中显示的三个权重矩阵*W*[q]、*W*[k]和*W*[v]：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that we are setting `requires_grad=False` to reduce clutter in the outputs
    for illustration purposes, but if we were to use the weight matrices for model
    training, we would set `requires_grad=True` to update these matrices during model
    training.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将`requires_grad=False`设置为减少输出中的杂乱，以便于说明，但如果我们使用这些权重矩阵进行模型训练，我们将设置`requires_grad=True`以在模型训练期间更新这些矩阵。
- en: 'Next, we compute the query, key, and value vectors as shown earlier in Figure
    3.14:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算查询、键和值向量，如图3.14所示：
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As we can see based on the output for the query, this results in a 2-dimensional
    vector since we set the number of columns of the corresponding weight matrix,
    via `d_out`, to 2:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 从查询的输出中可以看出，这会产生一个二维向量，因为我们将相应权重矩阵的列数通过`d_out`设置为2：
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Weight parameters vs attention weights
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 权重参数与注意力权重
- en: Note that in the weight matrices *W*, the term "weight" is short for "weight
    parameters," the values of a neural network that are optimized during training.
    This is not to be confused with the attention weights. As we already saw in the
    previous section, attention weights determine the extent to which a context vector
    depends on the different parts of the input, i.e., to what extent the network
    focuses on different parts of the input.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在权重矩阵*W*中，术语“权重”是“权重参数”的缩写，指的是在训练过程中优化的神经网络值。这与注意力权重不同。如我们在前一节中所见，注意力权重决定上下文向量在多大程度上依赖于输入的不同部分，即网络关注输入不同部分的程度。
- en: In summary, weight parameters are the fundamental, learned coefficients that
    define the network's connections, while attention weights are dynamic, context-specific
    values.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，权重参数是定义网络连接的基本学习系数，而注意力权重则是动态的、特定于上下文的值。
- en: Even though our temporary goal is to only compute the one context vector, *z*^((2)),
    we still require the key and value vectors for all input elements as they are
    involved in computing the attention weights with respect to the query *q*^((2)),
    as illustrated in Figure 3.14.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们暂时的目标只是计算一个上下文向量*z*^((2))，但我们仍然需要所有输入元素的键和值向量，因为它们在计算与查询*q*^((2))相关的注意力权重时是必不可少的，如图3.14所示。
- en: 'We can obtain all keys and values via matrix multiplication:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过矩阵乘法获得所有键和值：
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As we can tell from the outputs, we successfully projected the 6 input tokens
    from a 3D onto a 2D embedding space:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，我们成功地将6个输入标记从3D投影到2D嵌入空间：
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The second step is now to compute the attention scores, as shown in Figure 3.15.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是计算注意力得分，如图3.15所示。
- en: Figure 3.15 The attention score computation is a dot-product computation similar
    to what we have used in the simplified self-attention mechanism in section 3.3\.
    The new aspect here is that we are not directly computing the dot-product between
    the input elements but using the query and key obtained by transforming the inputs
    via the respective weight matrices.
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.15 注意力得分计算是一个点积计算，类似于我们在第3.3节中使用的简化自注意力机制。这里的新方面是，我们不是直接计算输入元素之间的点积，而是通过各自的权重矩阵转换输入后使用查询和键。
- en: '![](images/03__image029.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image029.png)'
- en: 'First, let''s compute the attention score *ω*[22]:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们计算注意力分数*ω*[22]：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The results in the following unnormalized attention score:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是未归一化的注意力分数的结果：
- en: '[PRE32]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Again, we can generalize this computation to all attention scores via matrix
    multiplication:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以通过矩阵乘法将此计算推广到所有注意力分数：
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As we can see, as a quick check, the second element in the output matches `attn_score_22`
    we computed previously:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，作为快速检查，输出中的第二个元素与我们之前计算的`attn_score_22`匹配：
- en: '[PRE34]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The third step is now going from the attention scores to the attention weights,
    as illustrated in Figure 3.16.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是从注意力分数转变为注意力权重，如图3.16所示。
- en: Figure 3.16 After computing the attention scores *ω*, the next step is to normalize
    these scores using the softmax function to obtain the attention weights *α*.
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.16 在计算注意力分数*ω*之后，下一步是使用softmax函数对这些分数进行归一化，以获得注意力权重*α*。
- en: '![](images/03__image031.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image031.png)'
- en: 'Next, as illustrated in Figure 3.16, we compute the attention weights by scaling
    the attention scores and using the softmax function we used earlier.. The difference
    to earlier is that we now scale the attention scores by dividing them by the square
    root of the embedding dimension of the keys, (note that taking the square root
    is mathematically the same as exponentiating by 0.5):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如图3.16所示，我们通过缩放注意力分数并使用之前使用的softmax函数来计算注意力权重。与之前的不同之处在于，我们现在通过将注意力分数除以键的嵌入维度的平方根来缩放这些分数（请注意，取平方根在数学上等同于取0.5的指数）：
- en: '[PRE35]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The resulting attention weights are as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的注意力权重如下：
- en: '[PRE36]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The rationale behind scaled-dot product attention
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缩放点积注意力背后的原理
- en: The reason for the normalization by the embedding dimension size is to improve
    the training performance by avoiding small gradients. For instance, when scaling
    up the embedding dimension, which is typically greater than thousand for GPT-like
    LLMs, large dot products can result in very small gradients during backpropagation
    due to the softmax function applied to them. As dot products increase, the softmax
    function behaves more like a step function, resulting in gradients nearing zero.
    These small gradients can drastically slow down learning or cause training to
    stagnate.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通过嵌入维度大小进行归一化的原因是通过避免小梯度来提高训练性能。例如，当增加嵌入维度时，通常对于像GPT这样的LLM来说，这个维度大于千，较大的点积会导致在反向传播过程中由于应用了softmax函数而产生非常小的梯度。随着点积的增加，softmax函数的行为更像是一个阶跃函数，导致梯度接近于零。这些小梯度会显著减慢学习速度或导致训练停滞。
- en: The scaling by the square root of the embedding dimension is the reason why
    this self-attention mechanism is also called scaled-dot product attention.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 通过嵌入维度的平方根进行缩放是该自注意力机制也被称为缩放点积注意力的原因。
- en: Now, the final step is to compute the context vectors, as illustrated in Figure
    3.17.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最后一步是计算上下文向量，如图3.17所示。
- en: Figure 3.17 In the final step of the self-attention computation, we compute
    the context vector by combining all value vectors via the attention weights.
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.17 在自注意力计算的最后一步，我们通过注意力权重组合所有值向量来计算上下文向量。
- en: '![](images/03__image033.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image033.png)'
- en: 'Similar to section 3.3, where we computed the context vector as a weighted
    sum over the input vectors, we now compute the context vector as a weighted sum
    over the value vectors. Here, the attention weights serve as a weighting factor
    that weighs the respective importance of each value vector. Similar to section
    3.3, we can use matrix multiplication to obtain the output in one step:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于第3.3节，在那里我们计算了作为输入向量的加权和的上下文向量，现在我们计算作为值向量的加权和的上下文向量。在这里，注意力权重作为加权因子，权衡每个值向量的相对重要性。类似于第3.3节，我们可以使用矩阵乘法在一步内获得输出：
- en: '[PRE37]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The contents of the resulting vector are as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 结果向量的内容如下：
- en: '[PRE38]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: So far, we only computed a single context vector, *z*^((2)). In the next section,
    we will generalize the code to compute all context vectors in the input sequence,
    *z*^((1)) to *z*^((T)).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只计算了一个上下文向量，*z*^((2))。在下一节中，我们将对代码进行泛化，以计算输入序列中的所有上下文向量，从*z*^((1))到*z*^((T))。
- en: Why query, key, and value?
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么查询、键和值？
- en: The terms "key," "query," and "value" in the context of attention mechanisms
    are borrowed from the domain of information retrieval and databases, where similar
    concepts are used to store, search, and retrieve information.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力机制的上下文中，“键”、“查询”和“值”这几个术语借用自信息检索和数据库领域，在这些领域中使用类似的概念来存储、搜索和检索信息。
- en: A "query" is analogous to a search query in a database. It represents the current
    item (e.g., a word or token in a sentence) the model focuses on or tries to understand.
    The query is used to probe the other parts of the input sequence to determine
    how much attention to pay to them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: “查询”类似于数据库中的搜索查询。它代表模型关注或试图理解的当前项（例如句子中的一个词或标记）。查询用于探测输入序列的其他部分，以确定应给予多少关注。
- en: The "key" is like a database key used for indexing and searching. In the attention
    mechanism, each item in the input sequence (e.g., each word in a sentence) has
    an associated key. These keys are used to match with the query.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: “键”就像用于索引和搜索的数据库键。在注意力机制中，输入序列中的每个项（例如句子中的每个词）都有一个相关联的键。这些键用于与查询匹配。
- en: The "value" in this context is similar to the value in a key-value pair in a
    database. It represents the actual content or representation of the input items.
    Once the model determines which keys (and thus which parts of the input) are most
    relevant to the query (the current focus item), it retrieves the corresponding
    values.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在此上下文中，“值”类似于数据库中键值对的值。它表示输入项的实际内容或表示。一旦模型确定了哪些键（因此哪些输入部分）与查询（当前关注项）最相关，它就会检索相应的值。
- en: 3.4.2 Implementing a compact self-attention Python class
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 实现紧凑的自注意力Python类
- en: 'In the previous sections, we have gone through a lot of steps to compute the
    self-attention outputs. This was mainly done for illustration purposes so we could
    go through one step at a time. In practice, with the LLM implementation in the
    next chapter in mind, it is helpful to organize this code into a Python class
    as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们经历了许多步骤来计算自注意力输出。这主要是为了说明目的，以便我们可以逐步处理。在实际操作中，考虑到下一章的LLM实现，将这段代码组织成一个Python类是很有帮助的：
- en: Listing 3.1 A compact self-attention class
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1 紧凑自注意力类
- en: '[PRE39]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In this PyTorch code, `SelfAttention_v1` is a class derived from `nn.Module`,
    which is a fundamental building block of PyTorch models, which provides necessary
    functionalities for model layer creation and management.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个PyTorch代码中，`SelfAttention_v1`是一个派生自`nn.Module`的类，`nn.Module`是PyTorch模型的基本构建块，提供了模型层创建和管理所需的功能。
- en: The `__init__` method initializes trainable weight matrices (`W_query`, `W_key`,
    and `W_value`) for queries, keys, and values, each transforming the input dimension
    `d_in` to an output dimension `d_out`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__`方法初始化可训练的权重矩阵（`W_query`、`W_key`和`W_value`），用于查询、键和值，每个矩阵将输入维度`d_in`转换为输出维度`d_out`。'
- en: During the forward pass, using the forward method, we compute the attention
    scores (`attn_scores`) by multiplying queries and keys, normalizing these scores
    using softmax. Finally, we create a context vector by weighting the values with
    these normalized attention scores.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播过程中，使用前向方法，通过将查询和键相乘来计算注意力得分（`attn_scores`），并使用softmax对这些得分进行归一化。最后，我们通过用这些归一化的注意力得分对值进行加权来创建上下文向量。
- en: 'We can use this class as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下使用这个类：
- en: '[PRE40]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Since `inputs` contains six embedding vectors, this result in a matrix storing
    the six context vectors:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`inputs`包含六个嵌入向量，因此结果是一个存储六个上下文向量的矩阵：
- en: '[PRE41]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As a quick check, notice how the second row (`[0.3061, 0.8210]`) matches the
    contents of `context_vec_2` in the previous section.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 快速检查一下，注意第二行（`[0.3061, 0.8210]`）与前一部分的`context_vec_2`的内容是如何匹配的。
- en: Figure 3.18 summarizes the self-attention mechanism we just implemented.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18总结了我们刚刚实现的自注意力机制。
- en: Figure 3.18 In self-attention, we transform the input vectors in the input matrix
    X with the three weight matrices, Wq, Wk, and Wv. Then, we compute the attention
    weight matrix based on the resulting queries (Q) and keys (K). Using the attention
    weights and values (V), we then compute the context vectors (Z). (For visual clarity,
    we focus on a single input text with *n* tokens in this figure, not a batch of
    multiple inputs. Consequently, the 3D input tensor is simplified to a 2D matrix
    in this context. This approach allows for a more straightforward visualization
    and understanding of the processes involved.)
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.18 在自注意力中，我们使用三个权重矩阵Wq、Wk和Wv对输入矩阵X中的输入向量进行变换。然后，我们基于生成的查询（Q）和键（K）计算注意力权重矩阵。使用注意力权重和值（V），我们接着计算上下文向量（Z）。
    （为了视觉清晰，本图中我们关注一个包含*n*个标记的单一输入文本，而不是多个输入的批处理。因此，在此上下文中，3D输入张量被简化为2D矩阵。这种方法使得过程的可视化和理解更加简单明了。）
- en: '![](images/03__image035.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image035.png)'
- en: As shown in Figure 3.18, self-attention involves the trainable weight matrices
    *W*[q]*, W*[k]*,* and *W*[v]. These matrices transform input data into queries,
    keys, and values, which are crucial components of the attention mechanism. As
    the model is exposed to more data during training, it adjusts these trainable
    weights, as we will see in upcoming chapters.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 3.18 所示，自注意力涉及可训练的权重矩阵 *W*[q]*, W*[k]*,* 和 *W*[v]。这些矩阵将输入数据转换为查询、键和值，这是注意力机制的关键组成部分。随着模型在训练过程中接触到更多数据，它会调整这些可训练权重，正如我们将在后续章节中看到的那样。
- en: We can improve the `SelfAttention_v1` implementation further by utilizing PyTorch's
    `nn.Linear` layers, which effectively perform matrix multiplication when the bias
    units are disabled. Additionally, a significant advantage of using `nn.Linear`
    instead of manually implementing `nn.Parameter(torch.rand(...))` is that `nn.Linear`
    has an optimized weight initialization scheme, contributing to more stable and
    effective model training.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过利用 PyTorch 的 `nn.Linear` 层进一步改进 `SelfAttention_v1` 的实现，这在禁用偏置单元时有效执行矩阵乘法。此外，使用
    `nn.Linear` 而不是手动实现 `nn.Parameter(torch.rand(...))` 的一个重要优势是，`nn.Linear` 具有优化的权重初始化方案，有助于更稳定和有效的模型训练。
- en: Listing 3.2 A self-attention class using PyTorch's Linear layers
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.2 使用 PyTorch 的 Linear 层的自注意力类
- en: '[PRE42]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: You can use the `SelfAttention_v2` similar to `SelfAttention_v1:`
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像使用 `SelfAttention_v1` 一样使用 `SelfAttention_v2：`
- en: '[PRE43]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为：
- en: '[PRE44]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note that `SelfAttention_v1` and `SelfAttention_v2` give different outputs because
    they use different initial weights for the weight matrices since `nn.Linear` uses
    a more sophisticated weight initialization scheme.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 `SelfAttention_v1` 和 `SelfAttention_v2` 产生不同的输出，因为它们对权重矩阵使用了不同的初始权重，`nn.Linear`
    使用了更复杂的权重初始化方案。
- en: Exercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2
  id: totrans-216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 3.1 比较 SelfAttention_v1 和 SelfAttention_v2
- en: Note that `nn.Linear` in `SelfAttention_v2` uses a different weight initialization
    scheme as `nn.Parameter(torch.rand(d_in, d_out)`) used in `SelfAttention_v1`,
    which causes both mechanisms to produce different results. To check that both
    implementations, `SelfAttention_v1` and `SelfAttention_v2`, are otherwise similar,
    we can transfer the weight matrices from a `SelfAttention_v2` object to a `SelfAttention_v1`,
    such that both objects then produce the same results.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 `SelfAttention_v2` 中的 `nn.Linear` 使用了不同于 `SelfAttention_v1` 中的 `nn.Parameter(torch.rand(d_in,
    d_out)` 的权重初始化方案，这导致两个机制产生不同的结果。为了检查两个实现 `SelfAttention_v1` 和 `SelfAttention_v2`
    在其他方面是否相似，我们可以将 `SelfAttention_v2` 对象的权重矩阵转移到 `SelfAttention_v1`，这样两个对象就会产生相同的结果。
- en: 'Your task is to correctly assign the weights from an instance of `SelfAttention_v2`
    to an instance of `SelfAttention_v1`. To do this, you need to understand the relationship
    between the weights in both versions. (Hint: `nn.Linear` stores the weight matrix
    in a transposed form.) After the assignment, you should observe that both instances
    produce the same outputs.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你的任务是正确地将 `SelfAttention_v2` 实例的权重分配给 `SelfAttention_v1` 实例。为此，你需要理解两个版本之间权重的关系。（提示：`nn.Linear`
    以转置形式存储权重矩阵。）在分配后，你应该观察到两个实例产生相同的输出。
- en: In the next section, we will make enhancements to the self-attention mechanism,
    focusing specifically on incorporating causal and multi-head elements. The causal
    aspect involves modifying the attention mechanism to prevent the model from accessing
    future information in the sequence, which is crucial for tasks like language modeling,
    where each word prediction should only depend on previous words.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将对自注意力机制进行增强，重点是引入因果和多头元素。因果方面涉及修改注意力机制，以防止模型访问序列中的未来信息，这对于语言建模等任务至关重要，因为每个单词的预测应该仅依赖于之前的单词。
- en: The multi-head component involves splitting the attention mechanism into multiple
    "heads." Each head learns different aspects of the data, allowing the model to
    simultaneously attend to information from different representation subspaces at
    different positions. This improves the model's performance in complex tasks.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 多头组件涉及将注意力机制分成多个“头”。每个头学习数据的不同方面，使得模型能够同时关注来自不同表示子空间的不同位置的信息。这提高了模型在复杂任务中的表现。
- en: 3.5 Hiding future words with causal attention
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 用因果注意力隐藏未来单词
- en: In this section, we modify the standard self-attention mechanism to create a
    *causal attention* mechanism, which is essential for developing an LLM in the
    subsequent chapters.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们修改标准自注意力机制，以创建一种 *因果注意力* 机制，这对于后续章节中开发 LLM 至关重要。
- en: Causal attention, also known as *masked attention*, is a specialized form of
    self-attention. It restricts a model to only consider previous and current inputs
    in a sequence when processing any given token. This is in contrast to the standard
    self-attention mechanism, which allows access to the entire input sequence at
    once.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 因果注意力，也称为*掩码注意力*，是一种专门的自注意力形式。它限制模型在处理任何给定标记时仅考虑序列中的前一个和当前输入。这与标准自注意力机制形成对比，后者允许同时访问整个输入序列。
- en: Consequently, when computing attention scores, the causal attention mechanism
    ensures that the model only factors in tokens that occur at or before the current
    token in the sequence.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在计算注意力分数时，因果注意力机制确保模型仅考虑序列中发生在当前标记之前或等于当前标记的标记。
- en: To achieve this in GPT-like LLMs, for each token processed, we mask out the
    future tokens, which come after the current token in the input text, as illustrated
    in Figure 3.19.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT类大语言模型中，为了实现这一点，对于每个处理的标记，我们将未来的标记（即当前标记后面的标记）掩盖，如图3.19所示。
- en: Figure 3.19 In causal attention, we mask out the attention weights above the
    diagonal such that for a given input, the LLM can't access future tokens when
    computing the context vectors using the attention weights. For example, for the
    word "journey" in the second row, we only keep the attention weights for the words
    before ("Your") and in the current position ("journey").
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.19 在因果注意力中，我们掩盖对角线以上的注意力权重，使得对于给定输入，LLM在使用注意力权重计算上下文向量时无法访问未来标记。例如，对于第二行中的“journey”一词，我们只保留“Your”之前的单词和当前位于“journey”的注意力权重。
- en: '![](images/03__image037.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image037.png)'
- en: As illustrated in Figure 3.19, we mask out the attention weights above the diagonal,
    and we normalize the non-masked attention weights, such that the attention weights
    sum to 1 in each row. In the next section, we will implement this masking and
    normalization procedure in code.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3.19所示，我们掩盖对角线以上的注意力权重，并对未掩盖的注意力权重进行归一化，使得每一行的注意力权重之和为1。在下一部分中，我们将用代码实现这一掩盖和归一化程序。
- en: 3.5.1 Applying a causal attention mask
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 应用因果注意力掩码
- en: In this section, we implement the causal attention mask in code. We start with
    the procedure summarized in Figure 3.20.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们在代码中实现因果注意力掩码。我们从图3.20中总结的程序开始。
- en: Figure 3.20 One way to obtain the masked attention weight matrix in causal attention
    is to apply the softmax function to the attention scores, zeroing out the elements
    above the diagonal and normalizing the resulting matrix.
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.20 在因果注意力中，获取掩码注意力权重矩阵的一种方法是对注意力分数应用softmax函数，将对角线以上的元素置为零，并对结果矩阵进行归一化。
- en: '![](images/03__image039.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image039.png)'
- en: To implement the steps to apply a causal attention mask to obtain the masked
    attention weights as summarized in Figure 3.20, let's work with the attention
    scores and weights from the previous section to code the causal attention mechanism.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实施步骤以应用因果注意力掩码，从而获得如图3.20所总结的掩码注意力权重，让我们使用上一部分的注意力分数和权重来编写因果注意力机制的代码。
- en: 'In the first step illustrated in Figure 3.20, we compute the attention weights
    using the softmax function as we have done in previous sections:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.20所示的第一步中，我们使用softmax函数计算注意力权重，正如我们在前面的部分中所做的那样：
- en: '[PRE45]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This results in the following attention weights:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下注意力权重：
- en: '[PRE46]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can implement step 2 in Figure 3.20 using PyTorch''s `tril` function to
    create a mask where the values above the diagonal are zero:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用PyTorch的`tril`函数来实现图3.20中的第2步，以创建一个掩码，其中对角线以上的值为零：
- en: '[PRE47]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The resulting mask is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 结果掩码如下：
- en: '[PRE48]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, we can multiply this mask with the attention weights to zero out the values
    above the diagonal:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这个掩码与注意力权重相乘，以将对角线以上的值置为零：
- en: '[PRE49]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As we can see, the elements above the diagonal are successfully zeroed out:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，对角线以上的元素已成功置为零：
- en: '[PRE50]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The third step in Figure 3.20 is to renormalize the attention weights to sum
    up to 1 again in each row. We can achieve this by dividing each element in each
    row by the sum in each row:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20中的第三步是重新归一化注意力权重，使每一行的总和再次为1。我们可以通过将每一行中的每个元素除以该行的总和来实现这一点：
- en: '[PRE51]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The result is an attention weight matrix where the attention weights above
    the diagonal are zeroed out and where the rows sum to 1:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个注意力权重矩阵，其中对角线以上的注意力权重被置为零，并且每一行的总和为1：
- en: '[PRE52]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Information leakage
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 信息泄漏
- en: When we apply a mask and then renormalize the attention weights, it might initially
    appear that information from future tokens (which we intend to mask) could still
    influence the current token because their values are part of the softmax calculation.
    However, the key insight is that when we renormalize the attention weights after
    masking, what we're essentially doing is recalculating the softmax over a smaller
    subset (since masked positions don't contribute to the softmax value).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们应用掩码并重新归一化注意力权重时，最初可能看起来未来标记（我们打算掩码的）的信息仍然会影响当前标记，因为它们的值是softmax计算的一部分。然而，关键见解是，当我们在掩码后重新归一化注意力权重时，我们实际上是在对一个更小的子集重新计算softmax（因为被掩码位置不贡献于softmax值）。
- en: The mathematical elegance of softmax is that despite initially including all
    positions in the denominator, after masking and renormalizing, the effect of the
    masked positions is nullified — they don't contribute to the softmax score in
    any meaningful way.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: softmax的数学优雅之处在于，尽管最初在分母中包含所有位置，但在掩码和重新归一化后，被掩码位置的影响被消除了——它们并没有以任何有意义的方式贡献于softmax分数。
- en: In simpler terms, after masking and renormalization, the distribution of attention
    weights is as if it was calculated only among the unmasked positions to begin
    with. This ensures there's no information leakage from future (or otherwise masked)
    tokens as we intended.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在掩码和重新归一化之后，注意力权重的分布就像最初仅在未掩码的位置之间计算一样。这确保了我们意图中，未来（或其他被掩码的）标记没有信息泄露。
- en: While we could be technically done with implementing causal attention at this
    point, we can take advantage of a mathematical property of the softmax function
    and implement the computation of the masked attention weights more efficiently
    in fewer steps, as shown in Figure 3.21.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这一点上技术上可以完成因果注意力的实现，但我们可以利用softmax函数的数学特性，以更少的步骤更高效地计算被掩码的注意力权重，如图3.21所示。
- en: Figure 3.21 A more efficient way to obtain the masked attention weight matrix
    in causal attention is to mask the attention scores with negative infinity values
    before applying the softmax function.
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.21 在因果注意力中，更高效地获得被掩码的注意力权重矩阵的方法是，在应用softmax函数之前，用负无穷大值掩码注意力得分。
- en: '![](images/03__image041.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image041.png)'
- en: The softmax function converts its inputs into a probability distribution. When
    negative infinity values (-∞) are present in a row, the softmax function treats
    them as zero probability. (Mathematically, this is because *e*^-^∞ approaches
    0.)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: softmax函数将其输入转换为概率分布。当一行中存在负无穷大值（-∞）时，softmax函数将其视为零概率。（从数学上看，这是因为*e*^-^∞接近0。）
- en: 'We can implement this more efficient masking "trick" by creating a mask with
    1''s above the diagonal and then replacing these 1''s with negative infinity (`-inf`)
    values:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过创建一个在对角线以上为1的掩码，然后用负无穷大（`-inf`）值替换这些1，来实现这种更高效的掩码“技巧”：
- en: '[PRE53]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This results in the following mask:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下掩码：
- en: '[PRE54]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, all we need to do is apply the softmax function to these masked results,
    and we are done:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要做的就是对这些掩码结果应用softmax函数，我们就完成了：
- en: '[PRE55]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'As we can see based on the output, the values in each row sum to 1, and no
    further normalization is necessary:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从输出中看到的，每一行的值之和为1，且无需进一步归一化：
- en: '[PRE56]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We could now use the modified attention weights to compute the context vectors
    via `context_vec = attn_weights @ values`, as in section 3.4\. However, in the
    next section, we first cover another minor tweak to the causal attention mechanism
    that is useful for reducing overfitting when training LLMs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用修改后的注意力权重通过`context_vec = attn_weights @ values`计算上下文向量，如第3.4节所示。然而，在下一节中，我们首先介绍一个对于减少训练LLMs时的过拟合有用的因果注意力机制的其他小调整。
- en: 3.5.2 Masking additional attention weights with dropout
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 使用dropout掩码额外的注意力权重
- en: '*Dropout* in deep learning is a technique where randomly selected hidden layer
    units are ignored during training, effectively "dropping" them out. This method
    helps prevent overfitting by ensuring that a model does not become overly reliant
    on any specific set of hidden layer units. It''s important to emphasize that dropout
    is only used during training and is disabled afterward.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dropout*在深度学习中是一种技术，随机选择的隐藏层单元在训练期间被忽略，实际上是“丢弃”它们。这种方法通过确保模型不会过于依赖任何特定的隐藏层单元来帮助防止过拟合。需要强调的是，dropout仅在训练期间使用，之后会被禁用。'
- en: 'In the transformer architecture, including models like GPT, dropout in the
    attention mechanism is typically applied in two specific areas: after calculating
    the attention scores or after applying the attention weights to the value vectors.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在变压器架构中，包括像 GPT 这样的模型，注意机制中的丢弃通常应用于两个特定区域：在计算注意力分数之后或在将注意力权重应用于值向量之后。
- en: Here, we will apply the dropout mask after computing the attention weights,
    as illustrated in Figure 3.22, because it's the more common variant in practice.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将在计算注意力权重后应用丢弃掩码，如图 3.22 所示，因为这是实践中更常见的变体。
- en: Figure 3.22 Using the causal attention mask (upper left), we apply an additional
    dropout mask (upper right) to zero out additional attention weights to reduce
    overfitting during training.
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.22 使用因果注意力掩码（左上），我们应用额外的丢弃掩码（右上）以置零额外的注意力权重，以减少训练过程中的过拟合。
- en: '![](images/03__image043.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image043.png)'
- en: In the following code example, we use a dropout rate of 50%, which means masking
    out half of the attention weights. (When we train the GPT model in later chapters,
    we will use a lower dropout rate, such as 0.1 or 0.2.)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，我们使用 50% 的丢弃率，这意味着将一半的注意力权重屏蔽掉。（在后面的章节中训练 GPT 模型时，我们将使用更低的丢弃率，如 0.1
    或 0.2。）
- en: 'In the following code, we apply PyTorch''s dropout implementation first to
    a 6×6 tensor consisting of ones for illustration purposes:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们首先将 PyTorch 的丢弃实现应用于一个由 1 组成的 6×6 张量，以便进行说明：
- en: '[PRE57]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'As we can see, approximately half of the values are zeroed out:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，约有一半的值被置为零：
- en: '[PRE58]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: When applying dropout to an attention weight matrix with a rate of 50%, half
    of the elements in the matrix are randomly set to zero. To compensate for the
    reduction in active elements, the values of the remaining elements in the matrix
    are scaled up by a factor of 1/0.5 =2\. This scaling is crucial to maintain the
    overall balance of the attention weights, ensuring that the average influence
    of the attention mechanism remains consistent during both the training and inference
    phases.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在以 50% 的丢弃率对注意力权重矩阵应用丢弃时，矩阵中的一半元素会随机设为零。为了补偿有效元素的减少，矩阵中剩余元素的值会按 1/0.5 = 2 的比例放大。这种缩放对于保持注意力权重的整体平衡至关重要，确保在训练和推理阶段注意机制的平均影响保持一致。
- en: 'Now, let''s apply dropout to the attention weight matrix itself:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对注意力权重矩阵本身应用丢弃：
- en: '[PRE59]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The resulting attention weight matrix now has additional elements zeroed out
    and the remaining ones rescaled:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的注意力权重矩阵现在有额外的元素被置为零，剩余的元素被重新缩放：
- en: '[PRE60]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Note that the resulting dropout outputs may look different depending on your
    operating system; you can read more about this inconsistency [here on the PyTorch
    issue tracker at [https://github.com/pytorch/pytorch/issues/121595](issues.html).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成的丢弃输出可能因操作系统的不同而有所差异；你可以在[这里的 PyTorch 问题追踪器](https://github.com/pytorch/pytorch/issues/121595)上阅读更多关于这种不一致的内容。
- en: Having gained an understanding of causal attention and dropout masking, we will
    develop a concise Python class in the following section. This class is designed
    to facilitate the efficient application of these two techniques.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了因果注意力和丢弃掩码后，我们将在接下来的部分开发一个简洁的 Python 类。该类旨在促进这两种技术的高效应用。
- en: 3.5.3 Implementing a compact causal attention class
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 实现紧凑的因果注意力类
- en: In this section, we will now incorporate the causal attention and dropout modifications
    into the `SelfAttention` Python class we developed in section 3.4\. This class
    will then serve as a template for developing *multi-head attention* in the upcoming
    section, which is the final attention class we implement in this chapter.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把因果注意力和丢弃修改纳入我们在 3.4 节中开发的 `SelfAttention` Python 类。该类将作为在接下来的部分中开发
    *多头注意力* 的模板，这是我们在本章实现的最后一个注意力类。
- en: But before we begin, one more thing is to ensure that the code can handle batches
    consisting of more than one input so that the `CausalAttention` class supports
    the batch outputs produced by the data loader we implemented in chapter 2.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 但在开始之前，还有一件事是确保代码能够处理由多个输入组成的批次，以便 `CausalAttention` 类支持我们在第二章实现的数据加载器生成的批量输出。
- en: 'For simplicity, to simulate such batch inputs, we duplicate the input text
    example:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，为了模拟这样的批量输入，我们重复了输入文本示例：
- en: '[PRE61]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This results in a 3D tensor consisting of 2 input texts with 6 tokens each,
    where each token is a 3-dimensional embedding vector:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生一个由 2 个输入文本（每个文本有 6 个标记）组成的 3D 张量，其中每个标记都是一个 3 维嵌入向量：
- en: '[PRE62]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The following `CausalAttention` class is similar to the `SelfAttention` class
    we implemented earlier, except that we now added the dropout and causal mask components
    as highlighted in the following code:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的`CausalAttention`类与我们之前实现的`SelfAttention`类相似，只是现在我们增加了在下面代码中突出显示的dropout和因果掩码组件：
- en: Listing 3.3 A compact causal attention class
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单3.3 紧凑的因果注意力类
- en: '[PRE63]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: While all added code lines should be familiar from previous sections, we now
    added a `self.register_buffer()` call in the `__init__` method. The use of `register_buffer`
    in PyTorch is not strictly necessary for all use cases but offers several advantages
    here. For instance, when we use the `CausalAttention` class in our LLM, buffers
    are automatically moved to the appropriate device (CPU or GPU) along with our
    model, which will be relevant when training the LLM in future chapters. This means
    we don't need to manually ensure these tensors are on the same device as your
    model parameters, avoiding device mismatch errors.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有添加的代码行在前面的章节中应该都很熟悉，但我们在`__init__`方法中增加了一个`self.register_buffer()`调用。虽然在所有用例中使用`register_buffer`在PyTorch中并不是严格必要的，但在这里提供了几个优势。例如，当我们在我们的LLM中使用`CausalAttention`类时，缓冲区会与我们的模型自动移动到适当的设备（CPU或GPU），这在未来章节中训练LLM时将是相关的。这意味着我们不需要手动确保这些张量与模型参数在同一设备上，避免了设备不匹配的错误。
- en: 'We can use the `CausalAttention` class as follows, similar to `SelfAttention`
    previously:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像以前的`SelfAttention`一样，使用`CausalAttention`类如下：
- en: '[PRE64]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The resulting context vector is a 3D tensor where each token is now represented
    by a 2D embedding:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的上下文向量是一个3D张量，其中每个令牌现在由一个2D嵌入表示：
- en: '[PRE65]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Figure 3.23 provides a mental model that summarizes what we have accomplished
    so far.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.23提供了一个心理模型，概括了我们迄今为止所取得的成就。
- en: Figure 3.23 A mental model summarizing the four different attention modules
    we are coding in this chapter. We began with a simplified attention mechanism,
    added trainable weights, and then added a casual attention mask. In the remainder
    of this chapter, we will extend the causal attention mechanism and code multi-head
    attention, which is the final module we will use in the LLM implementation in
    the next chapter.
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.23是一个心理模型，概述了我们在本章中编码的四种不同的注意力模块。我们从一个简化的注意力机制开始，添加了可训练的权重，然后添加了一个因果注意力掩码。在本章的剩余部分，我们将扩展因果注意力机制并编码多头注意力，这是我们将在下一章的LLM实现中使用的最终模块。
- en: '![](images/03__image045.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image045.png)'
- en: As illustrated in Figure 3.23, in this section, we focused on the concept and
    implementation of causal attention in neural networks. In the next section, we
    will expand on this concept and implement a multi-head attention module that implements
    several of such causal attention mechanisms in parallel.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3.23所示，在本节中，我们重点关注了神经网络中因果注意力的概念和实现。在下一节中，我们将扩展这个概念，并实现一个多头注意力模块，该模块并行实现几种因果注意力机制。
- en: 3.6 Extending single-head attention to multi-head attention
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 将单头注意力扩展为多头注意力
- en: In this final section of this chapter, we are extending the previously implemented
    causal attention class over multiple-heads. This is also called *multi-head attention*.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一节中，我们将之前实现的因果注意力类扩展为多个头部。这也称为*多头注意力*。
- en: The term "multi-head" refers to dividing the attention mechanism into multiple
    "heads," each operating independently. In this context, a single causal attention
    module can be considered single-head attention, where there is only one set of
    attention weights processing the input sequentially.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: “多头”一词是指将注意力机制分成多个“头”，每个头独立运行。在这种情况下，单个因果注意力模块可以被视为单头注意力，其中只有一组注意力权重按顺序处理输入。
- en: In the following subsections, we will tackle this expansion from causal attention
    to multi-head attention. The first subsection will intuitively build a multi-head
    attention module by stacking multiple `CausalAttention` modules for illustration
    purposes. The second subsection will then implement the same multi-head attention
    module in a more complicated but computationally more efficient way.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们将从因果注意力扩展到多头注意力。第一小节将通过堆叠多个`CausalAttention`模块直观地构建一个多头注意力模块。第二小节将以更复杂但计算上更高效的方式实现相同的多头注意力模块。
- en: 3.6.1 Stacking multiple single-head attention layers
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.1 堆叠多个单头注意力层
- en: In practical terms, implementing multi-head attention involves creating multiple
    instances of the self-attention mechanism (depicted earlier in Figure 3.18 in
    section 3.4.1), each with its own weights, and then combining their outputs. Using
    multiple instances of the self-attention mechanism can be computationally intensive,
    but it's crucial for the kind of complex pattern recognition that models like
    transformer-based LLMs are known for.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，实现多头注意力涉及创建多个自注意力机制的实例（如第3.4.1节的图3.18所示），每个实例都有自己的权重，然后将它们的输出结合起来。使用多个自注意力机制的实例可能会在计算上比较密集，但对于像基于变换器的LLM这样的模型所需的复杂模式识别至关重要。
- en: Figure 3.24 illustrates the structure of a multi-head attention module, which
    consists of multiple single-head attention modules, as previously depicted in
    Figure 3.18, stacked on top of each other.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.24展示了多头注意力模块的结构，该模块由多个单头注意力模块组成，如图3.18中所示，叠加在一起。
- en: 'Figure 3.24 The multi-head attention module in this figure depicts two single-head
    attention modules stacked on top of each other. So, instead of using a single
    matrix *W*[v] for computing the value matrices, in a multi-head attention module
    with two heads, we now have two value weight matrices: *W*[v1] and *W*[v2]. The
    same applies to the other weight matrices, *W*[q] and *W*[k]. We obtain two sets
    of context vectors *Z*[1] and *Z*[2] that we can combine into a single context
    vector matrix *Z*.'
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.24中的多头注意力模块描绘了两个单头注意力模块相互叠加。因此，在一个多头注意力模块中，我们不再使用单一矩阵*W*[v]来计算值矩阵，而是有两个值权重矩阵：*W*[v1]和*W*[v2]。其他权重矩阵*W*[q]和*W*[k]同样适用。我们得到两组上下文向量*Z*[1]和*Z*[2]，可以将它们合并为一个单一的上下文向量矩阵*Z*。
- en: '![](images/03__image047.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image047.png)'
- en: As mentioned before, the main idea behind multi-head attention is to run the
    attention mechanism multiple times (in parallel) with different, learned linear
    projections -- the results of multiplying the input data (like the query, key,
    and value vectors in attention mechanisms) by a weight matrix.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，多头注意力的主要思想是以不同的、学习到的线性投影多次（并行）运行注意力机制——这是通过将输入数据（如注意力机制中的查询、键和值向量）乘以权重矩阵的结果。
- en: 'In code, we can achieve this by implementing a simple `MultiHeadAttentionWrapper`
    class that stacks multiple instances of our previously implemented `CausalAttention`
    module:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以通过实现一个简单的`MultiHeadAttentionWrapper`类来完成这项工作，该类堆叠了多个之前实现的`CausalAttention`模块的实例：
- en: Listing 3.4 A wrapper class to implement multi-head attention
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.4 一个实现多头注意力的包装类
- en: '[PRE66]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: For example, if we use this MultiHeadAttentionWrapper class with two attention
    heads (via `num_heads=2`) and CausalAttention output dimension `d_out=2`, this
    results in a 4-dimensional context vectors (`d_out*num_heads=4`), as illustrated
    in Figure 3.25.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们使用具有两个注意力头的MultiHeadAttentionWrapper类（通过`num_heads=2`）和CausalAttention输出维度`d_out=2`，则会产生4维的上下文向量（`d_out*num_heads=4`），如图3.25所示。
- en: Figure 3.25 Using the `MultiHeadAttentionWrapper`, we specified the number of
    attention heads (`num_heads`). If we set `num_heads=2`, as shown in this figure,
    we obtain a tensor with two sets of context vector matrices. In each context vector
    matrix, the rows represent the context vectors corresponding to the tokens, and
    the columns correspond to the embedding dimension specified via `d_out=4`. We
    concatenate these context vector matrices along the column dimension. Since we
    have 2 attention heads and an embedding dimension of 2, the final embedding dimension
    is 2 × 2 = 4.
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.25中使用`MultiHeadAttentionWrapper`时，我们指定了注意力头的数量（`num_heads`）。如果我们设置`num_heads=2`，如图中所示，我们将得到一个具有两组上下文向量矩阵的张量。在每个上下文向量矩阵中，行表示与令牌对应的上下文向量，列对应通过`d_out=4`指定的嵌入维度。我们沿列维度连接这些上下文向量矩阵。由于我们有2个注意力头和2的嵌入维度，最终嵌入维度为2
    × 2 = 4。
- en: '![](images/03__image049.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image049.png)'
- en: 'To illustrate Figure 3.25 further with a concrete example, we can use the `MultiHeadAttentionWrapper`
    class similar to the `CausalAttention` class before:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过具体示例进一步说明图3.25，我们可以使用与之前的`CausalAttention`类类似的`MultiHeadAttentionWrapper`类：
- en: '[PRE67]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This results in the following tensor representing the context vectors:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下张量表示上下文向量：
- en: '[PRE68]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The first dimension of the resulting `context_vecs` tensor is 2 since we have
    two input texts (the input texts are duplicated, which is why the context vectors
    are exactly the same for those). The second dimension refers to the 6 tokens in
    each input. The third dimension refers to the 4-dimensional embedding of each
    token.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 结果`context_vecs`张量的第一维为2，因为我们有两个输入文本（输入文本是重复的，因此这些上下文向量是完全相同的）。第二维指的是每个输入中的6个标记。第三维指的是每个标记的4维嵌入。
- en: Exercise 3.2 Returning 2-dimensional embedding vectors
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习3.2 返回二维嵌入向量
- en: 'Change the input arguments for the `MultiHeadAttentionWrapper(..., num_heads=2)`
    call such that the output context vectors are 2-dimensional instead of 4-dimensional
    while keeping the setting `num_heads=2`. Hint: You don''t have to modify the class
    implementation; you just have to change one of the other input arguments.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 更改`MultiHeadAttentionWrapper(..., num_heads=2)`调用的输入参数，使得输出上下文向量为二维而非四维，同时保持设置`num_heads=2`。提示：你不需要修改类的实现，只需更改其他一个输入参数。
- en: In this section, we implemented a MultiHeadAttentionWrapper that combined multiple
    single-head attention modules. However, note that these are processed sequentially
    via `[head(x) for head in self.heads]` in the forward method. We can improve this
    implementation by processing the heads in parallel. One way to achieve this is
    by computing the outputs for all attention heads simultaneously via matrix multiplication,
    as we will explore in the next section.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实现了一个结合多个单头注意力模块的MultiHeadAttentionWrapper。然而，请注意，这些是在前向方法中通过`[head(x)
    for head in self.heads]`顺序处理的。我们可以通过并行处理头来改进这个实现。实现这一点的一种方法是通过矩阵乘法同时计算所有注意力头的输出，这将在下一节中探讨。
- en: 3.6.2 Implementing multi-head attention with weight splits
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.2 使用权重拆分实现多头注意力
- en: In the previous section, we created a `MultiHeadAttentionWrapper` to implement
    multi-head attention by stacking multiple single-head attention modules. This
    was done by instantiating and combining several `CausalAttention` objects.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们创建了一个`MultiHeadAttentionWrapper`，通过堆叠多个单头注意力模块来实现多头注意力。这是通过实例化和组合多个`CausalAttention`对象完成的。
- en: Instead of maintaining two separate classes, `MultiHeadAttentionWrapper` and
    `CausalAttention`, we can combine both of these concepts into a single `MultiHeadAttention`
    class. Also, in addition to just merging the `MultiHeadAttentionWrapper` with
    the `CausalAttention` code, we will make some other modifications to implement
    multi-head attention more efficiently.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`MultiHeadAttentionWrapper`和`CausalAttention`这两个独立的类合并为一个单一的`MultiHeadAttention`类。此外，除了仅仅将`MultiHeadAttentionWrapper`与`CausalAttention`代码合并外，我们还将进行一些其他修改，以更高效地实现多头注意力。
- en: In the `MultiHeadAttentionWrapper`, multiple heads are implemented by creating
    a list of `CausalAttention` objects (`self.heads`), each representing a separate
    attention head. The `CausalAttention` class independently performs the attention
    mechanism, and the results from each head are concatenated. In contrast, the following
    `MultiHeadAttention` class integrates the multi-head functionality within a single
    class. It splits the input into multiple heads by reshaping the projected query,
    key, and value tensors and then combines the results from these heads after computing
    attention.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在`MultiHeadAttentionWrapper`中，通过创建一个`CausalAttention`对象的列表（`self.heads`）来实现多个注意力头，每个对象代表一个独立的注意力头。`CausalAttention`类独立执行注意力机制，各个头的结果会被连接起来。相比之下，以下的`MultiHeadAttention`类将多头功能整合在一个类中。它通过重塑投影的查询、键和值张量，将输入拆分为多个头，然后在计算注意力后将这些头的结果合并。
- en: 'Let''s take a look at the `MultiHeadAttention` class before we discuss it further:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步讨论之前，让我们先看看`MultiHeadAttention`类：
- en: Listing 3.5 An efficient multi-head attention class
  id: totrans-333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5 一个高效的多头注意力类
- en: '[PRE69]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Even though the reshaping (`.view`) and transposing (`.transpose`) of tensors
    inside the `MultiHeadAttention` class looks very complicated, mathematically,
    the `MultiHeadAttention` class implements the same concept as the `MultiHeadAttentionWrapper`
    earlier.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`MultiHeadAttention`类内部的张量重塑（`.view`）和转置（`.transpose`）看起来非常复杂，但在数学上，`MultiHeadAttention`类实现的概念与前面的`MultiHeadAttentionWrapper`相同。
- en: On a big-picture level, in the previous `MultiHeadAttentionWrapper`, we stacked
    multiple single-head attention layers that we combined into a multi-head attention
    layer. The `MultiHeadAttention` class takes an integrated approach. It starts
    with a multi-head layer and then internally splits this layer into individual
    attention heads, as illustrated in Figure 3.26.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 从大局来看，在之前的`MultiHeadAttentionWrapper`中，我们堆叠了多个单头注意力层，将它们组合成一个多头注意力层。`MultiHeadAttention`类采取了集成的方法。它从一个多头层开始，然后在内部将该层分割成单个注意力头，如图3.26所示。
- en: Figure 3.26 In the `MultiheadAttentionWrapper` class with two attention heads,
    we initialized two weight matrices *W*[q1] and *W*[q2] and computed two query
    matrices *Q*[1] and *Q*[2] as illustrated at the top of this figure. In the `MultiheadAttention`
    class, we initialize one larger weight matrix *W*[q] *,* only perform one matrix
    multiplication with the inputs to obtain a query matrix *Q*, and then split the
    query matrix into *Q*[1] and *Q*[2] as shown at the bottom of this figure. We
    do the same for the keys and values, which are not shown to reduce visual clutter.
  id: totrans-337
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.26 在`MultiheadAttentionWrapper`类中，具有两个注意力头，我们初始化了两个权重矩阵*W*[q1]和*W*[q2]，并计算了两个查询矩阵*Q*[1]和*Q*[2]，如图顶部所示。在`MultiheadAttention`类中，我们初始化一个更大的权重矩阵*W*[q]*，只对输入执行一次矩阵乘法以获得查询矩阵*Q*，然后将查询矩阵分割为*Q*[1]和*Q*[2]，如图底部所示。对于键和值，我们采取相同的做法，未显示以减少视觉杂乱。
- en: '![](images/03__image051.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image051.png)'
- en: The splitting of the query, key, and value tensors, as depicted in Figure 3.26,
    is achieved through tensor reshaping and transposing operations using PyTorch's
    `.view` and `.transpose` methods. The input is first transformed (via linear layers
    for queries, keys, and values) and then reshaped to represent multiple heads.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 查询、键和值张量的分离，如图3.26所示，是通过使用PyTorch的`.view`和`.transpose`方法进行张量重塑和转置操作实现的。输入首先经过线性层变换（用于查询、键和值），然后重塑以表示多个头部。
- en: 'The key operation is to split the `d_out` dimension into `num_heads` and `head_dim`,
    where `head_dim = d_out / num_heads`. This splitting is then achieved using the
    `.view` method: a tensor of dimensions `(b, num_tokens, d_out)` is reshaped to
    dimension `(b, num_tokens, num_heads, head_dim)`.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 关键操作是将`d_out`维度拆分为`num_heads`和`head_dim`，其中`head_dim = d_out / num_heads`。这种拆分通过`.view`方法实现：将维度为`(b,
    num_tokens, d_out)`的张量重塑为维度`(b, num_tokens, num_heads, head_dim)`。
- en: The tensors are then transposed to bring the `num_heads` dimension before the
    `num_tokens` dimension, resulting in a shape of `(b, num_heads, num_tokens, head_dim)`.
    This transposition is crucial for correctly aligning the queries, keys, and values
    across the different heads and performing batched matrix multiplications efficiently.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 然后张量被转置，使得`num_heads`维度在`num_tokens`维度之前，结果形状为`(b, num_heads, num_tokens, head_dim)`。这种转置对于在不同头部之间正确对齐查询、键和值，并高效执行批处理矩阵乘法至关重要。
- en: 'To illustrate this batched matrix multiplication, suppose we have the following
    example tensor:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种批处理矩阵乘法，假设我们有以下示例张量：
- en: '[PRE70]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now, we perform a batched matrix multiplication between the tensor itself and
    a view of the tensor where we transposed the last two dimensions, `num_tokens`
    and `head_dim`:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在张量本身与张量的一个视图之间进行批处理矩阵乘法，其中我们转置了最后两个维度`num_tokens`和`head_dim`：
- en: '[PRE71]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The result is as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE72]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: In this case, the matrix multiplication implementation in PyTorch handles the
    4-dimensional input tensor so that the matrix multiplication is carried out between
    the 2 last dimensions `(num_tokens, head_dim)` and then repeated for the individual
    heads.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，PyTorch中矩阵乘法的实现处理4维输入张量，使得矩阵乘法在最后两个维度`(num_tokens, head_dim)`之间进行，然后对各个头部重复该操作。
- en: 'For instance, the above becomes a more compact way to compute the matrix multiplication
    for each head separately:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，上述变成了一种更紧凑的方式，以便分别计算每个头部的矩阵乘法：
- en: '[PRE73]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The results are exactly the same results that we obtained when using the batched
    matrix multiplication `print(a @ a.transpose(2, 3))` earlier:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果正是我们之前使用批处理矩阵乘法`print(a @ a.transpose(2, 3))`时获得的结果：
- en: '[PRE74]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Continuing with MultiHeadAttention, after computing the attention weights and
    context vectors, the context vectors from all heads are transposed back to the
    shape `(b, num_tokens, num_heads, head_dim)`. These vectors are then reshaped
    (flattened) into the shape `(b, num_tokens, d_out)`, effectively combining the
    outputs from all heads.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用 MultiHeadAttention，在计算注意力权重和上下文向量后，所有头部的上下文向量被转置回形状 `(b, num_tokens, num_heads,
    head_dim)`。然后将这些向量重塑（展平）为形状 `(b, num_tokens, d_out)`，有效地结合了所有头部的输出。
- en: Additionally, we added a so-called output projection layer (`self.out_proj`)
    to `MultiHeadAttention` after combining the heads, which is not present in the
    `CausalAttention` class. This output projection layer is not strictly necessary
    (see the References section in Appendix B for more details), but it is commonly
    used in many LLM architectures, which is why we added it here for completeness.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在结合头部之后，我们在 `MultiHeadAttention` 中添加了一个所谓的输出投影层（`self.out_proj`），而在 `CausalAttention`
    类中没有。这个输出投影层并不是严格必要的（更多详细信息请参见附录 B 的参考部分），但在许多 LLM 架构中常常使用，因此我们在这里添加它以求完整。
- en: Even though the `MultiHeadAttention` class looks more complicated than the `MultiHeadAttentionWrapper`
    due to the additional reshaping and transposition of tensors, it is more efficient.
    The reason is that we only need one matrix multiplication to compute the keys,
    for instance, `keys = self.W_key(x)` (the same is true for the queries and values).
    In the MultiHeadAttentionWrapper, we needed to repeat this matrix multiplication,
    which is computationally one of the most expensive steps, for each attention head.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 `MultiHeadAttention` 类看起来比 `MultiHeadAttentionWrapper` 更复杂，因为它额外进行了张量的重塑和转置，但它更高效。原因是我们只需进行一次矩阵乘法来计算键，例如，`keys
    = self.W_key(x)`（查询和值也是如此）。在 MultiHeadAttentionWrapper 中，我们需要为每个注意力头重复进行这次矩阵乘法，这在计算上是最昂贵的步骤之一。
- en: 'The `MultiHeadAttention` class can be used similar to the `SelfAttention` and
    `CausalAttention` classes we implemented earlier:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultiHeadAttention` 类的使用方式与我们之前实现的 `SelfAttention` 和 `CausalAttention` 类相似：'
- en: '[PRE75]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'As we can see based on the results, the output dimension is directly controlled
    by the `d_out` argument:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看出，输出维度由 `d_out` 参数直接控制：
- en: '[PRE76]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: In this section, we implemented the `MultiHeadAttention` class that we will
    use in the upcoming sections when implementing and training the LLM itself. Note
    that while the code is fully functional, we used relatively small embedding sizes
    and numbers of attention heads to keep the outputs readable.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实现了 `MultiHeadAttention` 类，这将在接下来的部分中用于实现和训练 LLM。请注意，虽然代码功能完全，但我们使用了相对较小的嵌入尺寸和注意力头数量，以保持输出的可读性。
- en: For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention
    heads and a context vector embedding size of 768\. The largest GPT-2 model (1.5
    billion parameters) has 25 attention heads and a context vector embedding size
    of 1600\. Note that the embedding sizes of the token inputs and context embeddings
    are the same in GPT models (`d_in = d_out`).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 作为比较，最小的 GPT-2 模型（1.17 亿参数）具有 12 个注意力头和 768 的上下文向量嵌入大小。最大的 GPT-2 模型（15 亿参数）具有
    25 个注意力头和 1600 的上下文向量嵌入大小。请注意，在 GPT 模型中，标记输入和上下文嵌入的嵌入大小是相同的（`d_in = d_out`）。
- en: Exercise 3.3 Initializing GPT-2 size attention modules
  id: totrans-362
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 3.3 初始化 GPT-2 尺寸的注意力模块
- en: Using the `MultiHeadAttention` class, initialize a multi-head attention module
    that has the same number of attention heads as the smallest GPT-2 model (12 attention
    heads). Also ensure that you use the respective input and output embedding sizes
    similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports
    a context length of 1024 tokens.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `MultiHeadAttention` 类，初始化一个多头注意力模块，该模块的注意力头数量与最小的 GPT-2 模型相同（12 个注意力头）。同时确保使用与
    GPT-2 相似的相应输入和输出嵌入大小（768 维）。请注意，最小的 GPT-2 模型支持的上下文长度为 1024 个标记。
- en: 3.7 Summary
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 摘要
- en: Attention mechanisms transform input elements into enhanced context vector representations
    that incorporate information about all inputs.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制将输入元素转换为增强的上下文向量表示，这些表示包含所有输入的信息。
- en: A self-attention mechanism computes the context vector representation as a weighted
    sum over the inputs.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力机制将上下文向量表示计算为对输入的加权和。
- en: In a simplified attention mechanism, the attention weights are computed via
    dot products.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在简化的注意力机制中，注意力权重通过点积计算。
- en: A dot product is just a concise way of multiplying two vectors element-wise
    and then summing the products.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点积只是将两个向量逐元素相乘然后求和的一种简洁方式。
- en: Matrix multiplications, while not strictly required, help us to implement computations
    more efficiently and compactly by replacing nested for-loops.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵乘法虽然不是严格必要，但通过替换嵌套的 for 循环，它帮助我们更高效和紧凑地实现计算。
- en: 'In self-attention mechanisms that are used in LLMs, also called scaled-dot
    product attention, we include trainable weight matrices to compute intermediate
    transformations of the inputs: queries, values, and keys.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 LLM 中使用的自注意力机制，也称为缩放点积注意力，我们包括可训练的权重矩阵来计算输入的中间变换：查询、值和键。
- en: When working with LLMs that read and generate text from left to right, we add
    a causal attention mask to prevent the LLM from accessing future tokens.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理从左到右读取和生成文本的 LLM 时，我们添加因果注意力掩码以防止 LLM 访问未来的标记。
- en: Next to causal attention masks to zero out attention weights, we can also add
    a dropout mask to reduce overfitting in LLMs.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了使用因果注意力掩码将注意力权重置零外，我们还可以添加一个 dropout 掩码以减少 LLM 中的过拟合。
- en: The attention modules in transformer-based LLMs involve multiple instances of
    causal attention, which is called multi-head attention.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于变换器的 LLM 中的注意力模块涉及多个因果注意力的实例，这称为多头注意力。
- en: We can create a multi-head attention module by stacking multiple instances of
    causal attention modules.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过堆叠多个因果注意力模块的实例来创建一个多头注意力模块。
- en: A more efficient way of creating multi-head attention modules involves batched
    matrix multiplications.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建多头注意力模块的更高效方法涉及批量矩阵乘法。
