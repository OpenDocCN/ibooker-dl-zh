- en: 10 Application architecture for generative AI apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An overview of GenAI application architecture and the emerging GenAI app stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The different layers that make up the GenAI app stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GenAI architecture principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benefits of orchestration frameworks and some of the popular ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model ensemble architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create a strategic framework for a cross-functional AI Center of Excellence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The enterprise architecture landscape continues to change, moving inexorably
    toward more self-directed systems—intelligent, self-managing applications that
    are capable of learning from interactions and adapting in real time. Furthermore,
    increasing digitization fuels the AI digital transformation. This ongoing progression
    underscores a transformative era in enterprise technology, poised to redefine
    the very nature of software development and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, this is more of an ideal. However, most enterprises are still very
    inexperienced with AI-infused applications in general, and generative AI is still
    very much in its early stages. This chapter will explore how enterprise application
    architecture standards and best practices must adapt to the emerging generative
    AI technologies and use cases. The chapter introduces the concept of a *GenAI
    app stack* as a conceptual reference architecture for building generative AI applications,
    and it outlines its main components and how generative AI fits together in the
    broader enterprise architecture. The GenAI app stack is an evolution of cloud
    application architecture, with a shift toward data-centric and AI-driven architectures.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter starts by outlining what the new GenAI app stack entails, covering
    details of each section and, finally, bringing all the concepts together into
    working examples that make it real and usable. As you learn about this stack,
    we’ll consolidate the different aspects of the architecture described in previous
    chapters. One thing to note is that despite representing a big change, generative
    AI does not require a completely new architecture but builds on the existing cloud-based
    distributed architecture. This characteristic allows us to build on existing best
    practices and architecture principles to incorporate new GenAI-related paradigms.
    Let’s start by identifying the updates to enterprise application architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '10.1 Generative AI: Application architecture'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the last few years, enterprise application architecture has witnessed a
    significant evolution, going through several transformative stages to meet the
    escalating demands for business agility, scalability, and intelligence. Initially,
    enterprises operated on monolithic systems, that is, robust but inflexible structures
    with tightly interwoven components, which made changes cumbersome and wide-reaching.
    These systems set the stage for enterprise computing but were not suitable for
    the rapid evolution of business needs.
  prefs: []
  type: TYPE_NORMAL
- en: The proliferation of cloud computing and cloud-native architectures saw the
    rise of containerization and orchestration tools, which simplified the deployment
    and management of applications across diverse environments. Simultaneously, the
    deluge of data led to data-centric architectures that prioritize data processing
    and analytics as key drivers for business operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evolution of enterprise application architecture for generative AI can
    be seen as a shift from traditional software development to data-driven software
    synthesis. In the traditional paradigm, software engineers write code to implement
    specific functionalities and logic, using frameworks and libraries that abstract
    away low-level details. In the generative AI paradigm, software developers provide
    data and high-level specifications and use large language models (LLMs) to generate
    code that meets the desired requirements and constraints. The following two key
    concepts enabled this paradigm shift: Software 2.0 and building on copilots.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Software 2.0
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Software 2.0 is a term coined by Andrej Karpathy [1] to describe the trend of
    replacing handcrafted code with learned neural networks. Software 2.0 uses advances
    in AI, such as natural language processing (NLP), computer vision, and reinforcement
    learning, to create software components that can learn from data, adapt to new
    situations, and interact with humans naturally.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, we have transitioned from writing code and managing explicit instructions
    for a desired goal to a more abstract approach. Developers train models on large
    datasets instead of writing explicit instructions or rules in a programming language.
    Software 2.0 also reduces the need for manual debugging, testing, and maintenance,
    as the neural networks can self-correct and improve over time (see figure 10.1).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F01_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 Software 1.0 versus Software 2.0
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This allows the models to learn the rules or patterns themselves. Algorithms
    and models are crafted to learn from data, make decisions, and improve over time,
    effectively writing the software. This paradigm shift has transformed the role
    of AI from a supportive tool to a fundamental component of system architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 The era of copilots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another key concept that facilitated the evolution of enterprise application
    architecture for generative AI is copilots—a concept originally proposed by Microsoft.
    Copilots are meant to augment humans and human capabilities and creativity. Using
    an airplane analogy, if we are humans, we are the pilots; instead of AI being
    on autopilot where we have no control or say in how it functions, this new AI
    plays the role of copilots that help us take on cognitive load and some of the
    drudgery of work. Still, we remain in charge as the pilot.
  prefs: []
  type: TYPE_NORMAL
- en: The Copilot stack is a framework for building AI applications and copilots that
    use LLMs to understand and generate natural language and code. Copilots are intelligent
    assistants that can help users with complex cognitive tasks such as writing, coding,
    searching, or reasoning. Microsoft has developed a range of copilots for different
    domains and platforms, such as GitHub Copilot, Bing Chat, Dynamics 365 Copilot,
    and Windows Copilot. You can also build your custom Copilot using the Copilot
    stack and tools, such as Azure OpenAI, Copilot Studio, and the Teams AI Library.
    Copilots can also be integrated into existing tools and platforms, such as GitHub,
    Visual Studio Code, and Jupyter Notebook, to enhance the productivity and creativity
    of software developers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copilots are based on the concept of Software 2.0, where they use LLMs to generate
    code from natural language descriptions instead of relying on manually written
    code. However, they should be seen as the GenAI application stack, similar to
    the LAMP stack for web development. LAMP is an acronym for the stack components:
    Linux (operating system); Apache (webserver); MySQL (database); and PHP, Perl,
    or Python (programming language).'
  prefs: []
  type: TYPE_NORMAL
- en: Copilots are a useful model for enterprises to follow when designing their generative
    AI apps enterprise architecture because they offer several advantages (e.g., quicker
    and simpler development, more creativity and testing, and improved cooperation
    and learning, enabling enterprises to try out new concepts and opportunities or
    to create original solutions for difficult problems). Let’s expand on what the
    Copilot stack is to make it more relevant and real in concrete terms.
  prefs: []
  type: TYPE_NORMAL
- en: '10.2 Generative AI: Application stack'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Copilots’ architecture comprises several layers and components that work together
    to provide a seamless and powerful user experience, as outlined in figure 10.2\.
    We will start from the bottom up, examine each layer and component in detail,
    and find out how they interact.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F02_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 GenAI application stack
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The AI infrastructure layer is the foundational layer that powers everything
    and hosts the core AI models and computational resources. It encompasses the hardware,
    software, and services that enable the development and deployment of AI applications
    and are often optimized for AI workloads. This also includes the massively scalable
    distributed high-performance computing (HPC), required for training the base foundational
    models.
  prefs: []
  type: TYPE_NORMAL
- en: The foundational model layer includes the range of supported models, from hosted
    foundation models to the model you train and want to deploy. The hosted foundational
    models are large pretrained models, such as LLMs and others (vision and speech
    models), and the newer small language models (SLMs) that can be used for inference;
    these models can be closed or open. Some of the models can be further adjusted
    for specific tasks or domains. These models are hosted and managed within the
    AI infrastructure layer to ensure high performance and availability. Users can
    select from various hosted foundation models based on their needs and preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The orchestration layer manages the interactions between the various components
    of the architecture, ensuring seamless operation and coordination. It is responsible
    for key functions such as task allocation, resource management, and workflow optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: The response filtering component uses the prompt engineering set of components;
    here, the prompts and responses are analyzed, filtered, and optimized to generate
    safe outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system prompt can also provide additional information or constraints for
    the AI model to follow. The user can express a system prompt via a simple syntax,
    or the system can automatically generate it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grounding is the implementation of retrieval-augmented generation (RAG), and
    it refers to the process of contextualizing the responses generated by the AI
    model. Grounding ensures the outputs are syntactically correct, semantically meaningful,
    and relevant to the given context or domain. We use plugins to get data ingested
    from different enterprise systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The plugin execution layer runs plugins that add more features to the basic
    AI model. Plugins are separate and reusable parts that can do different things,
    such as data processing, formatting, validation, or transformation. This is very
    important for taking in data to make embeddings and employing vector databases
    and indexes when we use RAG in our solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The UX layer is the interface that allows the users to use Copilot. It is easy
    to use and has strong tools for working with the AI features underneath. The exact
    nature of how the UX operates depends on which aspect of the application and workflow
    the Copilot is plugging into. For example, suppose one uses Copilot as part of
    Microsoft Office 365\. The way the UX works in Word differs from that in PowerPoint
    and in other applications, such as GitHub Copilot, as we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, all of this is done with AI safety, a main part of responsible AI,
    ensuring the technology is used ethically and responsibly. AI safety includes
    different methods and rules, which we will explain later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Integrating the GenAI stack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To integrate the GenAI stack into an enterprise application, a strategic and
    technical approach is required. It starts with knowing the requirements and the
    business challenges that can be addressed with GenAI, especially LLMs. This involves
    connecting specific use cases to the capabilities of these AI technologies, focusing
    on areas where they can offer substantial value, such as streamlining complex
    workflows, enhancing data analytics, or easing customer interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the use cases in mind, the next step is to create an integration architecture
    that fits the AI stack within the boundaries of the current enterprise system,
    using a service-oriented architecture (SOA) or a microservices approach for adaptability.
    Establishing secure, scalable, and maintainable APIs is important to facilitating
    communication between the application and AI services, which will be the basis
    for the integration.
  prefs: []
  type: TYPE_NORMAL
- en: The AI infrastructure configuration is an important stage where the organization’s
    policies and the data’s sensitivity determine whether to choose on-premises, cloud,
    or a hybrid method. The infrastructure needs hardware and data storage options
    to meet the use case’s demands. A robust data pipeline is also essential for efficient
    model inference, especially when using RAG.
  prefs: []
  type: TYPE_NORMAL
- en: With GenAI, developers can use existing models from cloud AI services or run
    models on their servers. Developers can build or adjust domain-specific models
    when custom solutions are required, ensuring they are trained on accurate, relevant
    datasets and adding continuous learning methods to enhance the model with new
    data.
  prefs: []
  type: TYPE_NORMAL
- en: To maintain responsible AI (RAI), safety, and adherence to data protection laws,
    response filtering systems are used to prevent the creation of unsuitable content
    and compliance. The user experience is based on this UX design, which allows users
    to interact with the AI stack. The design process is repeated, incorporating user
    feedback to satisfy the enterprise’s needs efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: The system allows for the inclusion of third-party integrations or custom extensions
    through a secure plugin architecture, which can run them without affecting the
    application’s reliability. An orchestration layer handles the interactions between
    different AI components, ensuring the system can adjust to different demand levels.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment is automated to ensure consistent and reliable updates to the AI
    stack, and CI/CD pipelines are established to enable ongoing integration and delivery
    without disrupting existing functionalities. The performance and health of the
    AI stack are continuously monitored, with comprehensive logging and alert systems
    to notify of any problems.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a successful adoption and operation of the AI stack depends on well-documented
    guidelines and training for developers and end users, ensuring they are fully
    prepared to use, troubleshoot, and maintain the new system. Each step in this
    process requires detailed planning, cross-team collaboration, and a deep technical
    understanding to ensure a smooth and effective integration into the enterprise
    architecture. Let’s explore this GenAI stack in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 GenAI architecture principles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When building mission-critical applications, enterprises focus on creating a
    robust, scalable, and secure system. Although the traditional architectural principles
    remain unchanged, key additional architectural aspects for generative AI are outlined
    in figure 10.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F03_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 Generative AI architecture principles
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Many GenAI models are accessed via an API, so the model API integration is an
    architecture principle that helps connect with the GenAI API. The models and APIs
    have different ways of formatting and sending data, as well as limits and quotas
    on how many requests they can handle; thus, it can be helpful to create a layer
    of abstraction that can adjust to changes in each API’s design. This involves
    handling API requests and responses and managing API limits and quotas. It is
    also common to have multiple models used in the same application to choose the
    right model for each situation. Having an abstraction layer can help protect each
    API’s design from changes.
  prefs: []
  type: TYPE_NORMAL
- en: As a principle, scalability and performance help the application deal with elastic
    scale and changing loads as they increase and decrease. This involves selecting
    the appropriate cloud infrastructure, balancing the load, and potentially using
    asynchronous processing to manage intensive tasks. Moreover, the use of containerization
    and microservices architecture can help with both scalability and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Hosting LLMs in an enterprise data center is not a trivial task, as it requires
    careful planning to achieve scalability and performance. You must choose an appropriate
    LLM architecture, comparing open source and proprietary alternatives that align
    with the business goals. A streamlined end-to-end pipeline is crucial for smooth
    operations, using orchestration frameworks for workflow management. The infrastructure
    should be solid for GPU optimization and simplified infrastructure management.
    LLMOps should be applied for best practices in deployment, and continuous monitoring
    for performance tracking should be set up. Scalability should be ensured through
    load balancing and auto-scaling. The data and models should be secured with encryption
    and access controls, and industry regulations should be followed. This comprehensive
    approach ensures that LLMs can serve multiple internal customers efficiently and
    reliably. Of course, it involves significant and continuous investment in capital
    expenditure and technical expertise.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the data’s sensitive nature, it is crucial to implement strong data privacy
    and security measures, which include encrypting data both in transit and at rest,
    managing access controls, and ensuring compliance with regulations such as GDPR
    or HIPAA. In addition, it is important to have a data minimization strategy where
    only necessary data is collected and processed, and security audits and penetration
    testing should be conducted regularly to identify and address vulnerabilities
    proactively. Some cloud providers, such as Azure, offer robust enterprise support
    systems and compliance solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Error handling and monitoring do not constitute a new architecture principle;
    with distributed systems, if you do not plan for failure, you are planning to
    fail. Use effective error handling and monitoring to check the GenAI application’s
    health. This means logging errors, creating alerts for anomalies, and having a
    plan for handling downtime or API limits, including using automatic recovery strategies,
    such as fallback mechanisms, to ensure high availability. Distributed tracing
    is essential for complex, microservice-based architectures to better track problems.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are evolving cost and currency meanings. LLM usage growth can lead to unexpected
    expenses. To control costs, optimize API calls and use caching strategies. Have
    budget alerting and cost forecasting mechanisms to avoid surprises.
  prefs: []
  type: TYPE_NORMAL
- en: The GenAI UX design focuses on how users interact with the GenAI models. This
    would vary depending on the model type; for a language-based use case using an
    LLM, the UX design would be quite different from an image-based use case where
    you would be using Stable Diffusion or DALL-E. This includes designing intuitive
    interfaces, providing helpful prompts, and ensuring the model’s responses align
    with user expectations. In some ways, everything should not be a simple chatbot,
    but it should extend and enhance the experience based on the task and intent.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the ethical, biased, and legal implications of GenAI apps, especially
    when using LLMs. Mitigate biases and prevent harmful stereotypes. Understand legal
    consequences in healthcare, finance, or law. Follow relevant laws and industry
    standards. New regulations are emerging, and chapter 12 will cover more on responsible
    AI use.
  prefs: []
  type: TYPE_NORMAL
- en: '10.2.3 GenAI application architecture: A detailed view'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the high-level architecture diagram and going into more detail, figure
    10.4 illustrates the overall structure of a GenAI app stack. Although we have
    already used many of these elements in the previous chapters, this is our first
    look at it holistically. There are six broad categories forming different components
    that constitute the GenAI app stack.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F04_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 GenAI app stack
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next, we will examine each layer more closely.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The orchestration layer is the central component that integrates various services
    and manages data flow and prompts. It is responsible for scheduling tasks, allocating
    resources, and handling errors resiliently. The prompt management system is a
    critical part of this layer, utilizing AI technology to develop prompts that elicit
    the best possible responses from LLMs. This involves A/B testing and machine learning
    (ML) models to analyze user interactions and optimize prompts for higher engagement
    and accuracy. Orchestration tools such as Kubernetes can manage containerized
    microservices and enable component deployment across cloud providers and on-premises
    environments to improve the system’s robustness and fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Grounding layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This layer is the basis of GenAI applications that deal with getting, storing,
    processing, and delivering data. It must work with different record systems, requiring
    connectors to handle various data formats and protocols. Data pipelines are the
    channels that link to the different source systems to take in data for applying
    RAG and enabling enterprises to use their data. The pipelines can connect to the
    system of records through APIs natively (where supported) or using different plugins.
    Data pipelines should be built for high speed and low delay, with the ability
    to handle batch and stream processing as required. The plugin runtime considers
    different authentication aspects, data refresh configurations, and so forth. Data
    preprocessing is important for changing data into a format that LLMs can use.
    Therefore, this layer includes ML models for creating embeddings and vector databases
    such as Redis, as we saw earlier in the book, or others such as Cosmos DB, Pinecone,
    Milvus, Qdrant, and so forth. Using distributed data-processing frameworks such
    as Apache Spark or Azure Fabric ensures scalability and fault tolerance in data
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Model layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The model layer needs to support a diverse range of models, from frontier general-purpose
    LLMs such as GPT-4 to highly specialized SLMs such as Phi-2 [2] and Orca 2 [3].
    We will learn more about SLMs and see an example of using Phi-2 as a classifier
    later in the chapter. As a result, the model layer should provide a consistent
    interface for accessing different models, regardless of whether they are hosted
    internally or externally. When considering model hosting, it is essential to scale
    models to handle varying loads, which may require technologies such as serverless
    computing to allocate resources dynamically. The model catalogs serve as a registry
    and repository, simplifying the discovery and management of models. This layer
    also encompasses the model-as-a-platform concept, which allows developers to extend
    and customize models, similar to how platforms such as Salesforce enable application
    customization.
  prefs: []
  type: TYPE_NORMAL
- en: Response filtering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This layer is crucial for maintaining trust in GenAI applications by ensuring
    quality assurance and content moderation. It involves using classifiers and NLP
    tools to screen the outputs for accuracy, bias, and appropriateness. Responsible
    AI practices are integrated into this layer, incorporating ethical considerations
    and ensuring compliance with regulations such as GDPR data privacy law. The caching
    system within this layer improves performance and enables quick rollback and suitability
    of outputs. Continuous monitoring and real-time evaluation of outputs ensure the
    AIQ is maintained throughout the application’s lifecycle. Moreover, this layer
    also addresses the ethical implications of GenAI technologies, which includes
    developing frameworks for ethical decision-making, ensuring model transparency,
    and incorporating fairness and inclusivity into the design of AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Additional architecture considerations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'While comprehensive, the architecture outlined earlier does not cover the following
    additional considerations, which are critical for production deployment and understood
    well by most enterprises:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Integrations*—These applications don’t work alone and must connect with the
    rest of the enterprise system, which enables the smooth transfer of data and services
    across internal and external systems. Middleware technologies such as enterprise
    service buses (ESBs) or API gateways are used to handle communication and data
    conversion between different systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security*—Security has always been a concern, and it is the same with GenAI;
    all data in the GenAI ecosystem must be safeguarded from unauthorized access and
    breaches, which requires strong authentication and authorization methods, transit
    and rest encryption, and frequent security audits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Production deployment and scaling*—The focus here is on the strategies for
    deploying GenAI applications across various environments, which includes using
    container orchestration systems for deployment, auto-scaling services to handle
    dynamic loads, and infrastructure as code for repeatable and reliable provisioning
    of resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10.3 Orchestration layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI applications require an orchestrator layer that acts as the backbone
    and is crucial for managing complex tasks and workflows. This is a middle-tier
    and integration layer between the models, enterprise data stores, other components,
    and applications. It coordinates and manages various components and processes
    that enable the generation of content by AI models within an enterprise architecture.
    Ensuring that the workflows involving LLMs are efficient, scalable, and reliable
    for generating content is essential.
  prefs: []
  type: TYPE_NORMAL
- en: The main duties of an orchestrator include managing workflows and service orchestration,
    but they can be expanded to include additional responsibilities. An orchestrator
    consists of several components. Orchestration frameworks simplify the manage-ment
    and interaction with LLMs by abstracting away the complexities of prompt generation,
    resource management, and performance monitoring. They provide a high-level interface
    that enables developers to focus on building their applications without getting
    bogged down in the technical details of LLM interaction. Table 10.1 outlines the
    key responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.1 Orchestrator key responsibilities
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Area | Descriptions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Workflow management  | Orchestrator ensures that the sequence of processes—from
    data ingestion and processing to AI model inference and response delivery—is executed
    in an orderly and efficient manner. This includes state management to coordinate
    dependencies between tasks, error handling, retry mechanisms, and the dynamic
    allocation of resources based on the task load.  |'
  prefs: []
  type: TYPE_TB
- en: '| Service orchestration  | Microservices architecture is typically employed,
    where each service is responsible for a discrete function in the generative AI
    process. Service orchestration is about managing these services to scale, communicate,
    and function seamlessly. In addition, containerization platforms such as Docker
    and orchestration systems such as Kubernetes deploy, manage, and scale the microservices
    across various environments.  |'
  prefs: []
  type: TYPE_TB
- en: '| Data flow coordination  | Ensure that data flows correctly through the system,
    from the initial data sources to the model and back to the end user or application.
    This includes preprocessing inputs, queue management for incoming requests, and
    routing outputs to the correct destinations.  |'
  prefs: []
  type: TYPE_TB
- en: '| Load balancing and auto-scaling  | Load balancers distribute incoming AI
    inference requests across multiple instances to prevent any single instance from
    becoming a bottleneck. Auto-scaling adjusts the number of active instances based
    on the current load, ensuring cost-effective resource use. This also has API management
    components to manage rate limits and implement back-off strategies for production
    workloads.  |'
  prefs: []
  type: TYPE_TB
- en: '| Model versioning and rollback  | Orchestration includes maintaining different
    versions of AI models and managing their deployment. It allows for quick rollback
    to previous versions if a new model exhibits unexpected behavior or poor performance.  |'
  prefs: []
  type: TYPE_TB
- en: '| Managing model context windows  | Orchestrator enhances interactions by efficiently
    managing context windows and token counts. It tracks and dynamically adjusts conversation
    history within the model’s token limits and maintains coherence in responses,
    especially in long or complex exchanges. Best practices include efficient context
    management, handling edge cases, continuous performance monitoring, and incorporating
    user feedback for ongoing improvements.  |'
  prefs: []
  type: TYPE_TB
- en: These different components work together to create a strong orchestration system
    that serves as the foundation for the successful deployment and operation of generative
    AI technology in the enterprise sector. Such orchestration is necessary for the
    intricacy and constant changes of AI-powered applications to avoid inefficiencies,
    mistakes, and system breakdowns.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Benefits of an orchestration framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Orchestrators are essential for managing the complex systems powering generative
    AI apps. These systems involve diverse processes that need careful coordination
    through orchestration tools. Orchestrators simplify workflows and ensure tasks
    are done in order, with dependencies and error-handling rules taken care of. This
    results in a reliable and regular operational flow, where steps for preprocessing,
    computation, and postprocessing are smoothly connected, ensuring data quality
    and consistent output generation.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability is another area where orchestration is vital. As demand fluctuates,
    a system that dynamically adjusts resource allocation, especially for production
    workloads, becomes crucial. An orchestrator can provide this agility using different
    techniques, such as load balancers to distribute workloads evenly and auto-scaling
    features to modulate computing power in real-time. This elasticity meets the load
    requirements and optimizes resource usage, balancing performance and cost efficiency.
    The orchestrators would need to manage this across different models, as well as
    the computational and cost profiles of those models.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrators offer a centralized management and monitoring ability. They constitute
    frameworks that offer dashboards and tools for monitoring LLM usage, identifying
    bottlenecks, and troubleshooting problems. This enhances system reliability by
    monitoring service health, responding to failures, and ensuring minimal downtime.
    Orchestrators can employ automated recovery processes, such as instance restarts
    or replacements, allowing for service continuity.
  prefs: []
  type: TYPE_NORMAL
- en: The default deployment model is a pay-as-you-go method for most cloud-based
    LLM providers. This model is shared with other customers, and incoming requests
    are queued and processed on a first-come, first-served basis. However, for production
    workloads that require a better user experience, Azure OpenAI service offers a
    provisioned throughput units (PTU) feature. This feature allows customers to reserve
    and deploy units of model processing capacity for prompt processing and generating
    completions. Each unit’s minimum PTU deployment, increments, and processing capacity
    vary depending on the model type and version. An orchestrator will manage the
    different deployment endpoints between regular pay-as-you-go and PTUs to ensure
    optimum performance and cost-effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrators play a significant role in increasing productivity and streamlining
    operations, which are achieved in two ways. First, it reduces the need to write
    repetitive code for common tasks such as prompt construction and output processing,
    thus increasing developers’ productivity. Second, it automates the deployment
    and management of services, thus minimizing the possibility of human error. This
    automated process reduces manual overhead and ensures effective compute resource
    utilization, streamlining production operations. We will delve deeper into managing
    operations later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Compliance and governance are essential requirements for any enterprise. An
    orchestrator can assist in enforcing compliance by determining how data is processed,
    stored, and used in the workflow, which ensures that the data complies with the
    enterprise’s data governance policies and privacy regulations. Maintaining trust
    and legal compliance in enterprise operations is crucial and can be achieved through
    adherence to data governance policies and privacy regulations.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 Orchestration frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many people are familiar with orchestrators and orchestration frameworks. While
    frameworks such as Kubernetes, Apache Airflow, and MLflow are effective general
    orchestration tools for software engineering and can support ML operations, they
    are not designed exclusively for generative AI applications. Orchestrating workflows
    for generative AI requires a more intimate understanding of the nuances of these
    complex technologies.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of an orchestration framework for generative AI applications depends
    on the existing technology stack, the complexity of the workflows, and specific
    requirements. Table 10.2 outlines orchestration frameworks tailored to the specific
    needs of generative AI applications. These frameworks can handle traditional computational
    workflows; manage interactions’ state, context, and coherence; and are designed
    to suit the unique requirements of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.2 Orchestration frameworks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Name | Notes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic Kernel  | Semantic Kernel is an OSS framework from Microsoft that
    aims to create a unified framework for semantic search and generative AI. It uses
    pretrained LLMs and graph-based knowledge representations to enable rich and diverse
    natural language interaction.  |'
  prefs: []
  type: TYPE_TB
- en: '| LangChain  | LangChain is a library that chains language models with external
    knowledge and capabilities. It facilitates the orchestration of LLMs such as GPT-4
    with databases, APIs, and other systems to create more comprehensive AI applications.  |'
  prefs: []
  type: TYPE_TB
- en: '| PromptLayer  | PromptLayer is a platform that simplifies the creation, management,
    and deployment of prompts for LLMs. Users can visually edit and test prompts,
    compare models, log requ-ests, and monitor performance. More details can be found
    at [https://promptlayer.com/](https://promptlayer.com/).  |'
  prefs: []
  type: TYPE_TB
- en: '| Rasa  | Rasa is an enterprise conversational AI platform that lets you create
    chat- and voice-based AI assistants to manage various conversations for different
    purposes. In addition to conversation AI, it also offers a generative AI-native
    method for building assistants, with enterprise features such as analytics, security,
    observability, testing, knowledge integration, voice connectors, and so forth.
    More information is available at [https://rasa.com/](https://rasa.com/).  |'
  prefs: []
  type: TYPE_TB
- en: '| YouChat API  | The YOU API is a suite of tools that helps enterprises ground
    the output of LLMs in the most recent, accurate, and relevant information available.
    You can use the YOU API to access web search results, news articles, and RAG for
    LLMs. More details can be found at [https://api.you.com/](https://api.you.com/).  |'
  prefs: []
  type: TYPE_TB
- en: '| Ragna  | Ragna is an open source RAG-based AI orchestration framework that
    allows you to experiment with different aspects of a RAG model—LLMs, vector databases,
    tokenization strategies, and embedding models. It also allows you to create custom
    RAG-based web apps and extensions from different data sources. More details can
    be found at [https://ragna.chat/](https://ragna.chat/).  |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-Index  | LlamaIndex is a cloud-based orchestration framework that enables
    you to connect your data to LLMs and generate natural language responses. It can
    access various LLMs.  |'
  prefs: []
  type: TYPE_TB
- en: '| Hugging Face  | Hugging Face provides a collection of pretrained models for
    various NLP tasks. It can be used with orchestration tools to manage the lifecycle
    of generative AI applications. More details can be found at [https://huggingface.co/](https://huggingface.co/).  |'
  prefs: []
  type: TYPE_TB
- en: 10.3.3 Managing operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An orchestrator plays a crucial role in enhancing the performance and seamless
    integration of generative AI models, such as LLMs, within intricate systems and
    workflows. Its core functionality optimizes operational efficiency and fosters
    a better user experience through sophisticated control mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: The orchestrator is crucial in managing the LLM’s integration into complex workflows,
    such as content creation pipelines. It plans and schedules the LLM’s activation
    to ensure smooth data collection, preprocessing, and text generation, thus simplifying
    the entire process from start to finish. This coordination improves the workflow
    and ensures that the API calls for the generated content are timely and relevant.
  prefs: []
  type: TYPE_NORMAL
- en: The orchestrator’s main role is to balance the load and resources for the LLM’s
    services. It effectively manages requests to avoid overloading or wasting resources.
    Furthermore, it can change computational resources by constantly tracking workload
    and performance metrics. This flexibility ensures the system stays responsive
    and resources are used efficiently, even when demanding changes.
  prefs: []
  type: TYPE_NORMAL
- en: The orchestrator also supervises API interactions, enforcing rate limits and
    controlling secure access, while managing any errors or disruptions that may occur.
    Simultaneously, it handles the essential tasks of data preprocessing and postprocessing.
    This means cleaning, formatting, and transforming data to ensure it is in the
    right state for processing by the LLM and then improving the output to meet set
    quality standards and format requirements.
  prefs: []
  type: TYPE_NORMAL
- en: For workflows requiring sequential processing, the orchestrator ensures that
    outputs from one phase are accurately fed into the next, maintaining the process
    integrity. This is complemented by its role in enforcing security and compliance
    measures, where it filters sensitive information and ensures adherence to legal
    and ethical standards, in addition to conducting audits for accountability and
    quality assurance.
  prefs: []
  type: TYPE_NORMAL
- en: For applications such as chatbots or digital assistants, the orchestrator manages
    user interactions by handling session states and queries, directing them to the
    LLM or other services as needed, which results in a more engaging and responsive
    user experience. Moreover, the orchestrator continuously monitors the LLM performance,
    analyzing response time, accuracy, and throughput to guide optimization efforts.
    It also manages updates to the LLM, ensuring that transitions to newer versions
    or configurations are smooth and minimally disruptive to users.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, an orchestrator can significantly enhance the efficiency, reliability,
    and scalability of an LLM when integrated into complex systems, providing a layer
    of management that coordinates between the LLM and other system components.
  prefs: []
  type: TYPE_NORMAL
- en: Building your own orchestrator framework
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Creating your own generative AI orchestrator for an enterprise can be difficult.
    However, it allows you to customize the framework according to your requirements
    and increases your understanding of the technology. This process demands extensive
    technical knowledge and resources. Unfortunately, no universal boilerplate code
    is available to develop an LLM orchestrator. Before proceeding with this project,
    consider the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Customization*—Tailoring the framework to meet your specific application and
    performance requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Integration with existing systems*—Seamlessly integrating the orchestrator
    with your existing infrastructure and workflows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Control and visibility*—Maintaining complete control over the LLM technology
    and accessing detailed insights into its operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Flexibility and scalability*—Designing the framework to be flexible enough
    to accommodate future changes and scaling to meet growing demands'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to create something entirely new, you need to understand generative
    AI, the different types of LLMs, how to train and fine-tune them, and how to use
    them for various tasks and domains. Additionally, you should know how to gather,
    process, and store data and knowledge that can help improve the quality and diversity
    of the generated outputs.
  prefs: []
  type: TYPE_NORMAL
- en: To apply these concepts in real-world scenarios, you must be able to design
    and implement different generative strategies, such as prompt engineering and
    RAG. These strategies can help control the behavior and output of the LLMs. You
    must also ensure that the generative models and workflows are scalable, secure,
    and reliable. This can be achieved using cloud services, APIs, and UIs. Expertise
    in distributed systems, ML, and software engineering is also required.
  prefs: []
  type: TYPE_NORMAL
- en: Some new frameworks used widely nowadays are Semantic Kernel, LangChain, and
    LlamaIndex. These frameworks enable the use of GenAI models, although they address
    different aspects. We will explore these in more depth.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic kernel
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Semantic Kernel (SK) from Microsoft is an SDK that integrates LLMs with languages
    such as C#, Python, and Java. It simplifies the sometimes-complex process of interfacing
    LLMs with traditional C#, Python, or Java code. With SK, developers can define
    semantic functions that encapsulate specific actions their application is capable
    of, such as database interactions, API calls, or email operations. SK allows these
    functions to be orchestrated seamlessly across mixed programming language environments.
  prefs: []
  type: TYPE_NORMAL
- en: The real power of SK lies in its AI-driven orchestration capabilities. Instead
    of meticulously choreographing the LLM interactions by hand, SK lets developers
    use natural language to state a desired outcome or task. The AI automatically
    determines how to combine the relevant semantic functions to achieve this goal,
    which significantly accelerates development and lowers the skill barrier for using
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: SK can benefit enterprises when building LLM applications by simplifying the
    application process, reducing the cost and complexity of prompt engineering, enabling
    in-context learning and reinforcement learning, and supporting multimodality and
    multilanguage scenarios. SK provides a consistent and unified interface for different
    LLM providers, such as OpenAI, Azure OpenAI, and Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: Combining simplified LLM integration with AI-powered orchestration creates a
    powerful platform for enterprises to use to revolutionize their applications.
    Furthermore, SK makes it feasible to build highly tailored, intelligent customer
    support systems, implement more powerful and semantically nuanced search functionality,
    automate routine workflows, and potentially even aid developers with code generation
    and refactoring tasks. Additional details on SK can be found on their site at
    [https://aka.ms/semantic-kernel](https://aka.ms/semantic-kernel).
  prefs: []
  type: TYPE_NORMAL
- en: We can illustrate this using an example. Continuing with the pet theme from
    the previous chapters, we have some books about dogs, which range from general
    topics to more specific medical advice. These books are scanned and available
    as PDFs and contain confidential business data we want to use for a question–answer
    use case. These PDFs are complex documents that contain text, images, tables,
    and so forth. Given that we cannot use real-world internal information, these
    PDFs represent proprietary internal information for an enterprise that requires
    RAG to handle. Suppose we want to do question–answer use cases with the PDFs we
    have; let’s see how that’s possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to use SK to install the SDK (or the package), which is not
    supported via conda and will require pip instead. Also note there are breaking
    changes with some of the SDKs, and we will want to pin the SK SDK to version 1.2.0\.
    You can install this specific version using `pip install semantic-kernel==1.2.0`.
    After installing the SDK, to get started with SK at a high level, we need to follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an SK instance, and register the AI services you want to use, such as
    OpenAI, Azure OpenAI, or Hugging Face.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create semantic functions that are prompts with input parameters. These functions
    can call your existing code or other semantic functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Call the semantic functions with the appropriate arguments, and await the results.
    The results will be the output of the AI model after executing the prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, we can create a planner to orchestrate multiple semantic functions
    based on the user input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SK example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here is an example of implementing this using the SK. As we saw earlier, SK
    is the core component that enables the processing and understanding of natural
    language text. It’s a framework that provides a unified interface for various
    AI services and memory stores.
  prefs: []
  type: TYPE_NORMAL
- en: Our example is a simple question-answering system that uses the OpenAI API to
    generate embeddings for a collection of PDF documents. Then, we use those embeddings
    to find documents relevant to a user’s query. In our example, it is used for
  prefs: []
  type: TYPE_NORMAL
- en: '*Creating embeddings*—SK provides a simple interface for calling the OpenAI
    service to generate embeddings for the text extracted from PDF documents. As we
    know, these embeddings are numerical representations of the text that capture
    its semantic meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Storing and retrieving information*—We use a vector database (Chroma in our
    example) to store the text and corresponding embeddings. SK calls these persistent
    data stores “memory” and, depending on the provider, has methods for querying
    the stored information based on semantic similarity. As we know, this is used
    to find documents relevant to a user’s query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Text completion*—We also use SK to register an OpenAI text completion service,
    which is used to generate completions for a given piece of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note  We need to specifically use Chroma version 0.4.15, as at the moment,
    there is an incompatibility with version 0.4.16 and higher with SK that hasn’t
    been fixed. To do this, we can use one of the following commands depending on
    whether we are using conda or pip: `conda install chromadb=0.4.15` or `pip install
    chromadb==0.4.15`.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 shows this simple application processing a collection of PDF documents,
    extracting their text, and then using the OpenAI API to generate embeddings for
    each document. These embeddings are then stored in a vector database, which can
    be queried to find documents that are semantically similar to a given input. The
    `load_pdfs` function reads PDF files from a specified directory. It uses the PyPDF2
    library to open each PDF, extract the text from each page, and return a collection
    of those pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 10.1 Q&A over my PDFs: Extracting text from PDFs'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After we have extracted the text from the pages, we use the `populate_db()`
    function to generate embeddings and store them in Chroma, a vector database. This
    function takes an SK object and goes through all the pages of the PDF. Each page
    saves the document’s text using the SK’s memory store. When the `save_information()`
    function is called, it automatically creates embedding to store in the vector
    database, as shown in the next listing. If there is already a Chroma vector database,
    we use that instead of making a new one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 10.2 Q&A over my PDFs: Using SK and populating vector database'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The program’s entry point is the `main()` function, as shown in listing 10.3\.
    It sets up the SK with the OpenAI text completion and embedding services, registers
    a memory store, and loads the vector database. Then, it enters a loop where it
    prompts the user for a question, queries the memory store for relevant documents,
    and prints the text of the most relevant document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 10.3 Q&A over my PDFs: SK using Chroma'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In our example, we use Chroma as the vector database. This is one of the many
    options available when using SK. We can get more details on the list of supported
    vector databases at [https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/](https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/).
    It is also important to note that support between C# and Python is not at parity;
    some vector databases are supported across both, but some are only supported in
    one language.
  prefs: []
  type: TYPE_NORMAL
- en: The SK is the central component for processing and understanding text. It provides
    a unified interface for various AI services and memory stores, simplifying the
    process of building complex NLP applications. Now let’s switch gears and see the
    same example using LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LangChain offers a sophisticated framework designed to streamline the integration
    of LLMs into enterprise applications. This framework abstracts the complexities
    of interfacing with LLMs, allowing developers to incorporate advanced NLP capabilities
    without deep expertise in the field. Its library of modular components enables
    the construction of customized NLP solutions easily, facilitating a more efficient
    development process.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain’s main benefit is its ability to work with different LLMs and other
    natural language AI services. This feature allows enterprises to select the best
    tools for their particular needs, avoiding the drawbacks of being tied to one
    vendor. The framework boosts efficiency by providing easier interfaces and ready-made
    components for quick deployment and supports scalability, thus enabling projects
    to expand smoothly from testing stages to full-fledged applications.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, LangChain helps to lower costs by minimizing the amount of specialized
    development and simplifying interactions with LLMs. Enterprises also gain from
    the strong community and support of the ecosystem around LangChain, which gives
    access to documentation, best practices, and cooperative problem-solving resources.
    This comprehensive approach makes LangChain an attractive option for businesses
    that want to use AI and natural language understanding in their services. It provides
    a way to innovate and enhance offerings through AI-driven solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Following the topic of pets from the previous chapters, in this chapter, we
    have a set of books related to dogs, which cover information ranging from general
    subjects to more specific medical advice. These books are PDFs and contain confidential
    business data that we want to use for a question–answer use case.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 shows how this can be done easily using LangChain. In this case,
    we load all the PDFs from a local folder, read each PDF, split the context into
    2K pieces, create embeddings (using OpenAI), and create a vector index using FAISS
    (Facebook AI Similarity Search). For brevity, we don’t show the code for some
    of the helper functions, such as `load_pdfs()`, as they are the same as the previous
    SK section.
  prefs: []
  type: TYPE_NORMAL
- en: Note  FAISS is a library that allows fast and accurate vector search and clustering
    and can be used for various AI applications. It supports different vector comparisons
    and index types and can run on CPU and GPU. Facebook AI Research developed FAISS,
    and more details are available at [https://ai.meta.com/tools/faiss/](https://ai.meta.com/tools/faiss/).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 Q&A over my PDFs using LangChain
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: On the one hand, LangChain is great and gives enterprises a big jumpstart for
    those just starting with LLMs and GenAI applications. LangChain simplifies the
    process by standardizing interactions with different LLM providers and offering
    tools for prompt creation, complex workflows (chains), and sophisticated AI assistants
    (agents). As an orchestrator, it can easily help us to connect LLMs to existing
    company data and systems, overcome initial hurdles, and quickly begin experimenting
    with LLM-driven applications.
  prefs: []
  type: TYPE_NORMAL
- en: However, LangChain comes with its challenges. Mastering concepts such as prompt
    design and building effective chains and agents has a learning curve. In addition,
    keeping the software and dependencies updated in this rapidly changing field can
    add some complexity. It is also essential to be aware of ethical LLM use, as powerful
    language models always carry the risk of incorrect or undesirable output. Finally,
    for production deployments where scale and performance are important, LangChain
    adds too many layers of abstractions and could end up hurting performance.
  prefs: []
  type: TYPE_NORMAL
- en: LlamaIndex
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LlamaIndex is a data framework that enables LLMs to access and process private
    data sources that are not part of their pretraining corpus. This enhances their
    NLP capabilities and domain-specific knowledge for various use cases, such as
    document Q&A, data-augmented chatbots, and structured analytics. LlamaIndex provides
    data ingestion, indexing, query interface, vector store, and database integration
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main challenges of using LLMs for generative AI applications is the
    integration of different data formats (APIs, PDFs, documents, SQL, etc.) and LLM
    providers (OpenAI, Hugging Face, etc.). LlamaIndex simplifies this process by
    providing a unified interface and modular design, allowing users to easily connect
    their custom data sources to their preferred LLMs. LlamaIndex also supports data
    augmentation, which is the process of generating synthetic from existing data
    to improve the performance and robustness of LLMs
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge of using LLMs for generative AI applications is efficient
    retrieval and scalability of data. LlamaIndex uses vector store and database providers
    to store and index data and optimize query speed and memory usage. LlamaIndex
    also supports various query types, such as natural language, keyword, and vector
    queries, to enable users to access their data conveniently and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 10.5 shows the simplicity of using LlamaIndex to implement a RAG question-and-answer
    use case using the same pet-related books. We employ a built-in function that
    loads and processes all the PDFs from storage (saved in our example’s `data/dog_books`
    folder) and creates a built-in vector index using the OpenAI embeddings. We save
    this locally to save time and can reuse it in the next instance. For us to use
    LlamaIndex, we do need to install a couple of packages—`llama-index` and `llama-index-reader-files`
    as shown: `pip` `install` `llama-index==0.10.9` `llama-index-readers-file`.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.5 An example showing RAG with LlamaIndex
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads environment variables'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Checks whether storage already exists'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Loads only PDFs'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Loads the PDF documents and creates the index'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Saves the index for later use'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Loads an existing index'
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.4 Prompt management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier in the book, we learned that prompt engineering plays a crucial role
    in communicating with LLMs, as it directly affects the output quality. A well-constructed
    prompt can help the LLM to generate accurate and contextually relevant responses.
    For this, you need to have a profound understanding of how LLMs interpret input
    and the ability to create prompts that the model can comprehend effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt management involves designing, testing, and deploying prompts or instructions
    for LLMs to perform various tasks. Prompts also need to work together with the
    evaluations and content moderation as part of the response filtering tier and
    RAI implementations. We will cover this aspect in more detail later in the book.
    As a part of the orchestration layer, prompt management provides a comprehensive
    approach to managing LLMs. This involves three essential components: prompt engineering,
    optimization, and PromptOps.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering encompasses the creation of custom, adaptive, and domain-specific
    prompts tailored to the user’s needs and the context of their queries. This involves
    generating custom prompts for specific tasks, such as summarizing news articles
    by understanding the context and requirements and adapting prompts in real-time
    based on user interactions to better align with their intent. Additionally, it
    includes developing prompts that cater to specialized fields, using the appropriate
    technical language and adhering to field-specific standards.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt optimization focuses on improving the prompts’ effectiveness through
    continuous performance monitoring, data-driven refinements, and efficient resource
    management. This entails tracking metrics such as accuracy and relevance to gauge
    the prompts’ success, refining prompts based on user feedback and response quality
    to enhance clarity, and optimizing prompts to stay within token limits and reduce
    complexity, thus ensuring cost-efficiency and prompt–response generation.
  prefs: []
  type: TYPE_NORMAL
- en: PromptOps involves the operational aspects of managing prompts, including automated
    testing for prompt effectiveness, version control for managing different prompt
    versions and enabling easy rollbacks, integration with other AI system components
    to ensure seamless operation, and scalability and maintenance considerations to
    ensure the prompt management system can handle growing demands and is easy to
    update. This comprehensive approach to prompt management ensures that the AI system
    remains effective, efficient, and adaptable to user needs and technological advancements.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt management (i.e., creating and optimizing prompts for LLMs) can benefit
    from various tools and frameworks being developed constantly. For enterprises
    that want to use LLMs and prompt management tools, assessing the technical features
    and the vendor’s adherence to security, privacy, and compliance with relevant
    regulations (e.g., GDPR, HIPAA) is important. Moreover, enterprises should consider
    the level of support, customization, and ability to integrate with existing systems
    and workflows. Many of these providers offer custom solutions and partnerships
    for businesses, ensuring that using LLMs matches enterprise needs and strategic
    objectives. Prompt flow ([https://github.com/microsoft/promptflow](https://github.com/microsoft/promptflow)),
    a Microsoft OSS tool for prompt management, is one example. We will cover Prompt
    flow in more detail in the book’s next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is Pezzo ([https://github.com/pezzolabs/pezzo](https://github.com/pezzolabs/pezzo)),
    which can help with prompt management. LangChain and SK, which we saw earlier,
    also have some support for prompt management. For more details, see “Prompting
    Frameworks for Large Language Models: A Survey” [4].'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt management is an important process in ensuring the effectiveness of LLM
    applications. It is a dynamic and iterative process that involves designing, testing,
    refining, and customizing prompts for optimal outputs. The architecture of the
    LLM system must be flexible enough to accommodate current and future advances
    in prompt design. It should also provide tools for continuous improvement mechanisms
    to generate high-quality outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Grounding layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The grounding layer is the foundation of GenAI applications that handle data
    acquisition, storage, processing, and delivery. It integrates various data sources
    and formats with connectors, pipelines, plugins, and APIs. In addition, it performs
    data preprocessing, embedding, and vectorization to make the data compatible with
    LLMs. It employs distributed data processing frameworks for scalability and reliability.
    Let’s explore this in a little more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.1 Data integration and preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having reliable data pipelines to combine data from different systems as seamlessly
    as possible is important. These pipelines must be designed to handle various data
    types and sources—from structured SQL database entries to unstructured text, image
    files, and real-time streaming data from IoT deployments. The architecture of
    these pipelines must be compatible with various data formats and protocols, which
    may require the development of custom APIs, middleware for data transformation,
    and scalable ETL (extract, transform, load) processes.
  prefs: []
  type: TYPE_NORMAL
- en: Integration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Integrating a system of record is fundamental to the generative AI application
    architecture. It involves multiple layers of interaction and data management in
    a secure, compliant, and efficient manner, which ensures that real-time data is
    available for the LLM, while maintaining quality. In addition, the integration
    must be scalable and adaptable to changes in the enterprise data ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: The main goal for integration pipelines is to integrate them into various systems
    of records (SoRs) and enable access for the data from those systems to be used
    efficiently by GenAI models. Integrating with SoR is crucial in designing generative
    AI applications. These systems include SaaS platforms, customer relationship management
    (CRM), and enterprise resource planning (ERP) systems. They serve as the data
    backbone for the LLM applications, acting as repositories for the enterprise’s
    structured and unstructured data. This data is essential for using the LLMs as
    a reasoning engine, allowing it to access high-quality, domain-specific information.
  prefs: []
  type: TYPE_NORMAL
- en: This information retrieved via the SoR integration is used for RAG implementation.
    As we saw earlier in the book, it is one of the main ways enterprises can operate
    on their proprietary information. SoR integrations are the key to achieving that.
    The main challenge is not just the integration but also understanding the nature
    of the data, the frequency of change, and the computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: Several tools are available to initiate this process, such as Microsoft Fabric,
    which offers over 145 connectors, Apache NiFi, Informatica, and so forth. These
    tools gather and consolidate data from different sources into a single repository
    that can handle various data formats and prevent data loss during data capturing.
  prefs: []
  type: TYPE_NORMAL
- en: Modern storage solutions such as Amazon S3, Azure Data Lake Storage, or the
    Hadoop Distributed File System (HDFS) offer secure and scalable storage options
    for large amounts of data. When combined with data warehousing technologies such
    as Snowflake, Google BigQuery, or Amazon Redshift, businesses can efficiently
    store, query, and manage their data, making it easier to prepare for AI integration.
  prefs: []
  type: TYPE_NORMAL
- en: Data orchestration tools, such as Apache Airflow, Data Factory in Microsoft
    Fabric, and AWS Glues, offer modern, code-centric methods for constructing and
    executing complex data workflows. These systems allow developers to define data
    pipelines through code, facilitating version control and testing similar to standard
    software development practices. Additionally, they provide scheduling, monitoring,
    and error management features that contribute to the reliability of data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once data has been prepared for AI use, it can be sent to processing engines
    or analytical platforms for further preparation. Apache Spark is a well-known
    platform that can handle large-scale data processing and has several libraries
    covering various computing requirements. Platforms such as Databricks have built
    upon Spark’s capabilities to ease the journey from data preparation to model deployment.
    In addition, architectures must include event-driven mechanisms such as webhooks
    or streaming services to ensure data synchronization and real-time updates.
  prefs: []
  type: TYPE_NORMAL
- en: For data to be useful in informing LLM outcomes, it must first undergo a rigorous
    cleansing and standardization process to ensure its quality. The architectural
    blueprint should include these preprocessing activities, such as deduplication,
    normalization, and error rectification. Integrated data quality tools should automate
    these tasks, providing LLMs with superior datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Data handling requires strict access controls for proper security and compliance,
    which is vital when working with sensitive information and following regulations.
    Data interaction needs strong authentication and authorization protocols. Data
    governance frameworks should specify access rights; furthermore, encryption should
    protect data at rest and in motion. Frequent compliance assessments are crucial
    for ensuring data quality and privacy. Following GDPR, HIPAA, or CCPA regulations
    is also important for ethical and lawful processing of personal data.
  prefs: []
  type: TYPE_NORMAL
- en: A plugin enabling the integration into source systems is not a one-time static
    component of the architecture—it changes and adapts constantly. As businesses
    use or improve their new SoRs, the architecture must be built to allow simple
    integration or movement of data sources. For this, a flexible approach to integration
    is required, where new data sources can be connected with little change to the
    current system.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture should be designed to support different data formats and protocols.
    This ensures that data flows seamlessly from various systems to the LLM. To achieve
    this, custom APIs may need to be developed, middleware may have to be used for
    data transformation, and ETL processes capable of handling large volumes of data
    may have to be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: The data pipeline infrastructure for generative AI is complex and requires careful
    planning to handle the intricacies of enterprise-grade data landscapes. These
    will build on existing ETL and data warehousing investments but must factor in
    the new data types of embeddings. By strategically using a combination of tools
    for data ingestion, processing, storage, orchestration, and ML, enterprises can
    build powerful pipelines that provide their generative AI applications with a
    consistent flow of quality data.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2 Embeddings and vector management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In earlier chapters of the book, we discussed the crucial role of model embeddings
    and representations. This is the stage where the complexity of language is distilled
    into machine-interpretable formats, specifically mathematical vectors. Text is
    transformed by embedding techniques and advanced feature extraction forms that
    result in a vector space representation of text. These vectors are not arbitrary;
    they encapsulate the semantic essence of words, phrases, or entire documents,
    mapping information into a compressed, information-rich, lower-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Codex is a prime example of this process. It can comprehend and generate
    human-readable code, making it a powerful tool for embedding programming and natural
    languages. This is a significant advantage for code generation and automation
    tasks. In contrast, Hugging Face provides an extensive suite of pretrained models
    that are finely tuned for diverse languages and tasks. They can adeptly handle
    embeddings ranging from brief sentences to intricate documents.
  prefs: []
  type: TYPE_NORMAL
- en: These models distinguish themselves by their ability to grasp contextual word
    relationships beyond basic dictionary meanings. By considering the words in their
    vicinity, the generated embeddings provide a nuanced reflection of the word usage
    and connotations within specific contexts. This feature is essential for generative
    AI applications that aim to emulate human-like text production. It fosters outcomes
    that are not only coherent and context-aware but also semantically profound.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in earlier chapters on RAG, various libraries are available for chunking
    data, and some offer auto-chunking capabilities. One such library, called Unstructured
    ([https://github.com/Unstructured-IO/unstructured](https://github.com/Unstructured-IO/unstructured)),
    provides open source libraries and APIs that can create customized preprocessing
    pipelines for labeling, training, or production ML pipelines. The library includes
    modular functions and connectors that form a cohesive system, which makes it easy
    to ingest, preprocess, and adapt data to different platforms. It is also efficient
    at transforming unstructured data into structured outputs.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative solution is using LangChain and SK, which we saw earlier. These
    libraries support common chunking techniques for fixed size, variable size, or
    a combination of both. In addition, you can specify an overlap percentage to duplicate
    a small amount of content in each chunk, which helps preserve context.
  prefs: []
  type: TYPE_NORMAL
- en: After transforming vectors, it is crucial to manage them properly. Vector databases
    specially designed to store indexes and retrieve high-dimensional vector data
    are available. Some such databases include Redis, Azure Cosmos DB, Pinecone, and
    Weaviate, to name a few. These databases help with quick searches within large
    embedding spaces, making it easy to identify similar vectors instantly. For instance,
    a generative AI system can use a vector database to match a user’s query with
    the most semantically related questions and answers and achieve this in a fraction
    of a second.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases feature sophisticated indexing algorithms engineered to deftly
    traverse high-dimensional terrains without falling prey to the “curse of dimensionality”
    [5]. This attribute renders them exceptionally valuable for applications such
    as recommendation engines, semantic search platforms, and personalized content
    curation, where pinpointing relevant content quickly is critical.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases offer more than just speed; they also provide accuracy and
    relevance. Combining these databases allows AI models to respond quickly and precisely
    to user inquiries based on their learned context. Proper index management is crucial,
    including tasks such as index creation, update triggers, refresh rates, complex
    data types, and operational factors (e.g., index size, schema design, and underlying
    compute services). Cloud-based solutions such as Azure AI Search and Pinecone
    can efficiently manage these demands in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of transforming textual data into a format that AI can handle has
    two stages: embedding and vector database management. This conversion is essential
    for generative AI’s intelligence, enabling it to understand and engage with the
    world meaningfully and in a scalable manner. Therefore, carefully choosing embedding
    techniques and vector databases is a technical necessity and a key factor in the
    success of generative AI applications. When choosing LLMs, related vector storage
    and retrieval engines, and embedding models, enterprises must consider the data
    size, origin, change rate, and scalability needs.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Model layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model layer is the foundation of AI cognitive capabilities. It involves
    a set of models, including foundational LLMs that provide general intelligence,
    fine-tuned LLMs specialized for specific tasks or domains, model catalogs hosting
    and managing access to various models, and SLMs that offer lightweight, agile
    alternatives for certain applications.
  prefs: []
  type: TYPE_NORMAL
- en: The significance of this layer lies in its design, as it forms the core processing
    units of the GenAI app stack. It allows a scalable and flexible approach to AI
    deployment and can efficiently address various tasks by differentiating between
    foundational, fine-tuned, and small models. This ensures that the architecture
    can cater to diverse use cases, optimize resource allocation, and maintain high
    performance across different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Small language models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: SLMs such as Phi-3 and Orca 2 are designed to offer advanced language processing
    capabilities with fewer parameters than larger models. Both models are part of
    a broader initiative to make powerful language processing tools more accessible
    and efficient, enabling more extensive research and application possibilities.
    They represent a significant step in the evolution of AI language models, balancing
    capability with computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Phi-3, Phi-2, and Orca 2 are smaller-scale language models developed by Microsoft,
    offering advanced language processing with fewer parameters. Phi-3, which is a
    successor to Phi-2, is a family of models in various sizes (mini, 3.8B; small,
    7B; medium, 14B parameters). Phi-2, with 2.7 billion parameters, is efficient
    and matches larger models in performance, while Orca 2, available in 7- and 13-billion-parameter
    versions, excels in reasoning tasks and can outperform much larger models. Both
    are designed for accessibility and computational efficiency, enabling broader
    research and application in AI language processing.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.1 Model ensemble architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative AI employs a model ensemble, which combines multiple ML models to
    enhance performance and reliability. This approach takes advantage of the individual
    strengths of each model, minimizing their weaknesses. For example, one model may
    be great at generating technical content, while another may be better at creative
    storytelling. By assembling these models, an application can better cater to a
    wider range of user requests with greater accuracy. To create an effective model
    ensemble for generative AI, the architecture should include
  prefs: []
  type: TYPE_NORMAL
- en: '*Model selection*—Criteria for choosing which models to include in the ensemble,
    often based on their performance, the diversity of training data, or their area
    of specialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Routing logic*—Routing logic is the mechanism for determining which model
    to use for a given input or how to combine outputs from multiple models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*API integration*—APIs are the main conduits through which applications interact
    with LLMs. API integration becomes complex when dealing with an ensemble of models
    as interactions with multiple endpoints must be managed. The architecture should
    consider API integration of throttling and rate limits, error handling, and caching
    responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalability and redundancy*—Scalable design accommodates growing user bases
    and spikes in demand. Load balancing and the use of API gateways can help distribute
    traffic effectively. Redundancy is equally critical; thus, having multiple regions
    for model deployments ensures the application remains functional.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Queuing and stream processing*—Queuing and stream processing handle asynchronous
    tasks and manage workloads; message queues and stream processing services can
    be utilized, which ensures that the system is not overwhelmed during peak times
    and that tasks are processed in an orderly way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 10.5 is an example of implementing Phi-2 as a classifier. We use Phi-2,
    which runs locally and fast, to identify the user’s intent when asking a question.
    Continuing with the topic of pets and dogs, we asked Phi-2 the intent of the question
    and whether it had anything to do with dogs. If it was irrelevant to the current
    topic (i.e., dogs), we asked GPT-4 to answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F05_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 Classifier using multiple models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 10.6 shows an example of implementing a simple classifier using a lightweight
    model and then, based on the question’s intent, figuring out which model to call.
    Here, we use Phi-2, a research SML from Microsoft, as a classifier to determine
    whether a question is related to dogs. The Phi-2 model is a transformer-based
    model, trained to understand and generate human-like text. It is used here as
    a first-pass filter to determine the question’s intent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function `check_dog_question()` takes a question as input and constructs
    a prompt to ask the Phi-2 model whether there’s anything about dogs in the question.
    If Phi-2 determines that the question is about dogs, the function returns `True`.
    This could trigger a more expensive GPT-4 model to generate a more detailed response.
    If the question is not about dogs, the function returns `False`, and the more
    expensive model would not have to be used. We need to ensure that the following
    packages are installed before running this code: `pip` `install` `transformers==4.42.4`
    `torch= =2.3.1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 Using Phi-2 as an intent classifier
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The approach employs a small model, such as Phi-2, with much less capability
    for more efficient use of resources, as the more expensive GPT-4 model is used
    only when necessary. This approach can just as easily be expanded to use more
    than one model.
  prefs: []
  type: TYPE_NORMAL
- en: This toy example could be better if we used a more powerful LLM, such as a smaller
    GPT-3 model. Figure 10.6 shows another example of using a fine-tuned GPT-3 as
    a classifier to help understand the user’s goal. This is for an enterprise chatbot
    that can answer questions on both structured and unstructured data. It can answer
    questions about Microsoft’s surface devices based on the user’s persona. There
    is fictitious sales information in a SQL database that a salesperson can chat
    with, and there is unstructured data that can answer technical support questions.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F06_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 Enterprise Q&A bot—High-level overview
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The bot uses a RAG pattern and can answer questions using information from both
    structured and unstructured systems based on the user’s intention. The structured
    data has sales information (with fake data), and the unstructured data is a crawl
    of different forums and official sites related to Surface devices. Listing 10.7
    presents a high-level view of the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The orchestrator uses GPT-3 to implement the intent classifier and can help
    select the best path based on the bot’s question. Then, suitable knowledge sources
    are applied. This complicated workflow shows much of what an orchestrator would
    do in a real-world enterprise situation. The sales data is stored in a SQL database,
    and GPT is also used to build the SQL query against the schema to run, depending
    on the user’s query. What is very interesting is that the LLM is invoked multiple
    times in the flow, first to understand the intent of the question, and then, depending
    on the path, GPT also creates the SQL query to execute. Its results are passed
    to the prompt formulation to invoke the LLM again to create the response for the
    user. This mainly shows that along the flow, we can invoke the right model based
    on the point in time and for what it is needed, factoring in the model capability
    and associated computational constraints and costs.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 Using a fine-tuned GPT-3 model as a classifier
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the classifier, we must use the appropriate prompts to convey
    our purpose and obtain the desired behavior. The sample prompts that match the
    classifier are displayed in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 Classifier meta-prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The link to the full code listing can be found in the book’s GitHub repository
    ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)). It is a fork from one
    of Microsoft’s published samples, found at [https://bit.ly/AOAISearchDemo](https://bit.ly/AOAISearchDemo).
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.2 Model serving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many modern AI applications are hosted on cloud platforms due to their scalability
    and the wide range of services they offer. Integrating with major cloud providers
    such as Microsoft Azure, Amazon Web Services, or Google Cloud Platform enables
    developers to use a secure global network of data centers, ML managed services,
    and tools for application monitoring and management. Therefore, many enterprises
    use one of the LLMs hosted in the cloud, which is exposed via an API. This means
    that the cloud providers that manage the model serve to scale up or down model
    inference. If some models are hosted on-premise, the layer must address model
    operations working with LLMOps.
  prefs: []
  type: TYPE_NORMAL
- en: The model layer architecture should provide a strategic framework for using
    multiple LLMs to create a robust, versatile, and scalable application. This involves
    careful planning around model selection and API management, while ensuring security
    and compliance in data handling. The architecture should be flexible enough to
    adapt to new models and APIs as they become available.
  prefs: []
  type: TYPE_NORMAL
- en: 10.6 Response filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most cases, an application should not share the raw generation from the model
    with the end user; it should go through a processing step to help manage and filter
    any sensitive details—this is where the processing layer helps, and a key responsibility
    of this layer is to manage the LLM output.
  prefs: []
  type: TYPE_NORMAL
- en: The response filtering layer is tasked with quality assurance and content moderation,
    crucial for maintaining trust in GenAI applications. It involves using classifiers
    and NLP tools to screen the outputs for accuracy, bias, and appropriateness.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, LLM output can vary significantly, ranging from simple text
    to complex data structures. Managing these outputs requires a systematic approach
    so they meet the application’s standards and are presented to the user in a useful
    format. These postprocessing steps include a few areas, as shown in figure 10.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F07_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 Response filtering stages
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Content moderation relies on RAI practices to mitigate the potential risks of
    generative AI models, such as biased, offensive, or misleading content, cyber,
    privacy, legal, performance, and intellectual property risks. We need to adopt
    RAI practices to use the power of generative AI. RAI is essential for the output
    processing layer to address both application- and enterprise-level risks, such
    as regulatory and compliance requirements. In addition, RAI can enhance other
    aspects, such as privacy, explainability, and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools and frameworks to start with. For example, Microsoft’s
    InterpretML ([https://interpret.ml/](https://interpret.ml/)) and Fairlearn ([https://fairlearn.org/](https://fairlearn.org/))
    are open source toolkits that help developers explain and improve the fairness
    of ML models. IBM’s AI Fairness 360 is another open source toolkit that helps
    detect and reduce bias in ML models. We’ll examine RAI in more depth later in
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: Output and postprocessing are crucial for ensuring the usability and safety
    of content generated by LLMs. The architecture should provide a robust framework
    for refining and managing outputs, including formatting, content classification,
    validation, and caching. Quality assurance, both automated and user driven, must
    be an integral part of the process to maintain high standards and improve over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter shows how GenAI can be integrated into enterprise applications
    using the new GenAI app stack and associated application architecture. We have
    also discussed the role of the Center of Excellence in facilitating this integration
    and addressing the technical, cultural, and ethical challenges involved. However,
    building an AI solution is only the first step; deploying it for production and
    scale requires different skills and tools. The next chapter will explore what
    it takes to operationalize generative AI solutions and ensure their reliability,
    performance, and security. We will also look at some of best practices and frameworks
    for managing the AI lifecycle and delivering value to the end-users and stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Copilot demonstrates how generative AI architecture can build enterprise applications
    and solutions. It uses a different application stack that works with copilots
    to create the new enterprise architecture stack. This stack is for GenAI apps,
    which use Copilot as a counterpart to the LAMP stack.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GenAI app stack includes four layers that cooperate to make the application
    stack function—the model, orchestration, grounding, and response filtering layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The orchestration layer is one of the critical and foundational components of
    the GenAI stack. It handles and organizes different processes, AI services, and
    platforms to enable a dependable and coherent experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The area of orchestration frameworks is new and evolving, with many changes
    and innovations taking place. Some of the frameworks that are more widely used
    today are SK, LangChain, and LlamaIndex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using plugins than can handle the intricacies of the source systems, their
    protocols, and other details, the grounding layer facilitates data integrations
    and preprocessing for RAG deployments in the enterprises. It also oversees the
    embeddings and the related vector databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model layer offers a platform for using multiple models from various sources—from
    managed and fine-tuned models to BYOM (bring-your-own-model) for enterprises.
    These models can all be accessed through strong APIs that guarantee compliance
    and security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The response filtering layer ensures quality and moderates content, essential
    for building confidence in GenAI applications. Furthermore, it involves using
    classifiers and NLP tools to check the outputs for correctness, fairness, and
    suitability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AI Center of Excellence can help enterprises comprehensively integrate LLMs
    and GenAI into their applications. By addressing technical, cultural, and ethical
    challenges, enterprises can use AI to enhance innovation and competitiveness,
    ensuring lasting success in an increasingly AI-powered world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
