- en: 10 Application architecture for generative AI apps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 生成式AI应用的架构
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: An overview of GenAI application architecture and the emerging GenAI app stack
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GenAI应用架构概述及新兴的GenAI应用堆栈
- en: The different layers that make up the GenAI app stack
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构成GenAI应用堆栈的不同层
- en: GenAI architecture principles
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GenAI架构原则
- en: The benefits of orchestration frameworks and some of the popular ones
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排框架的好处以及一些流行的编排框架
- en: Model ensemble architectures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型集成架构
- en: How to create a strategic framework for a cross-functional AI Center of Excellence
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何为跨职能AI卓越中心创建战略框架
- en: The enterprise architecture landscape continues to change, moving inexorably
    toward more self-directed systems—intelligent, self-managing applications that
    are capable of learning from interactions and adapting in real time. Furthermore,
    increasing digitization fuels the AI digital transformation. This ongoing progression
    underscores a transformative era in enterprise technology, poised to redefine
    the very nature of software development and deployment.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 企业架构领域持续变化，不可避免地朝着更多自我驱动的系统发展——智能、自我管理应用程序能够从交互中学习并实时适应。此外，日益增长的数字化推动了AI数字化转型。这一持续进步凸显了企业技术变革的时代，将重新定义软件开发和部署的本质。
- en: Naturally, this is more of an ideal. However, most enterprises are still very
    inexperienced with AI-infused applications in general, and generative AI is still
    very much in its early stages. This chapter will explore how enterprise application
    architecture standards and best practices must adapt to the emerging generative
    AI technologies and use cases. The chapter introduces the concept of a *GenAI
    app stack* as a conceptual reference architecture for building generative AI applications,
    and it outlines its main components and how generative AI fits together in the
    broader enterprise architecture. The GenAI app stack is an evolution of cloud
    application architecture, with a shift toward data-centric and AI-driven architectures.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，这更多的是一个理想状态。然而，大多数企业在AI增强型应用程序方面仍然非常缺乏经验，生成式AI仍然处于早期阶段。本章将探讨企业应用架构标准和最佳实践必须如何适应新兴的生成式AI技术和用例。本章介绍了*GenAI应用堆栈*的概念性参考架构，用于构建生成式AI应用程序，并概述了其主要组件以及生成式AI如何在更广泛的企业架构中协同工作。GenAI应用堆栈是云应用架构的演变，转向以数据为中心和AI驱动的架构。
- en: This chapter starts by outlining what the new GenAI app stack entails, covering
    details of each section and, finally, bringing all the concepts together into
    working examples that make it real and usable. As you learn about this stack,
    we’ll consolidate the different aspects of the architecture described in previous
    chapters. One thing to note is that despite representing a big change, generative
    AI does not require a completely new architecture but builds on the existing cloud-based
    distributed architecture. This characteristic allows us to build on existing best
    practices and architecture principles to incorporate new GenAI-related paradigms.
    Let’s start by identifying the updates to enterprise application architecture.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先概述了新的GenAI应用堆栈包含的内容，涵盖了每个部分的细节，最后将所有概念整合到实际的工作示例中，使其变得真实和可用。在了解这个堆栈的过程中，我们将巩固前几章中描述的架构的不同方面。需要注意的是，尽管代表了一次重大变化，生成式AI并不需要全新的架构，而是建立在现有的基于云的分布式架构之上。这一特性使我们能够基于现有的最佳实践和架构原则来融入新的GenAI相关范式。让我们首先确定企业应用架构的更新。
- en: '10.1 Generative AI: Application architecture'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 生成式AI：应用架构
- en: Over the last few years, enterprise application architecture has witnessed a
    significant evolution, going through several transformative stages to meet the
    escalating demands for business agility, scalability, and intelligence. Initially,
    enterprises operated on monolithic systems, that is, robust but inflexible structures
    with tightly interwoven components, which made changes cumbersome and wide-reaching.
    These systems set the stage for enterprise computing but were not suitable for
    the rapid evolution of business needs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，企业应用架构经历了显著的演变，经历了几个转型阶段，以满足对业务敏捷性、可扩展性和智能性的日益增长的需求。最初，企业运行在单体系统上，即结构坚固但缺乏灵活性的紧密交织的组件，这使得更改变得繁琐且影响广泛。这些系统为企业计算奠定了基础，但并不适合业务需求的快速演变。
- en: The proliferation of cloud computing and cloud-native architectures saw the
    rise of containerization and orchestration tools, which simplified the deployment
    and management of applications across diverse environments. Simultaneously, the
    deluge of data led to data-centric architectures that prioritize data processing
    and analytics as key drivers for business operations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算和云原生架构的普及导致了容器化和编排工具的兴起，这些工具简化了在不同环境中部署和管理应用程序的过程。同时，数据的涌入导致了以数据处理和分析为中心的架构，这些架构将数据处理和分析作为业务运营的关键驱动因素。
- en: 'The evolution of enterprise application architecture for generative AI can
    be seen as a shift from traditional software development to data-driven software
    synthesis. In the traditional paradigm, software engineers write code to implement
    specific functionalities and logic, using frameworks and libraries that abstract
    away low-level details. In the generative AI paradigm, software developers provide
    data and high-level specifications and use large language models (LLMs) to generate
    code that meets the desired requirements and constraints. The following two key
    concepts enabled this paradigm shift: Software 2.0 and building on copilots.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能的企业应用架构的演变可以看作是从传统软件开发到数据驱动软件合成的转变。在传统范式下，软件工程师编写代码以实现特定的功能性和逻辑，使用框架和库来抽象低级细节。在生成式人工智能范式下，软件开发者提供数据和高级规范，并使用大型语言模型（LLMs）生成满足所需要求和约束的代码。以下两个关键概念实现了这一范式转变：软件
    2.0 和基于共同飞行员。
- en: 10.1.1 Software 2.0
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 软件 2.0
- en: Software 2.0 is a term coined by Andrej Karpathy [1] to describe the trend of
    replacing handcrafted code with learned neural networks. Software 2.0 uses advances
    in AI, such as natural language processing (NLP), computer vision, and reinforcement
    learning, to create software components that can learn from data, adapt to new
    situations, and interact with humans naturally.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 软件 2.0 是安德烈·卡帕西 [1] 提出的一个术语，用来描述用学习到的神经网络取代手工编写的代码的趋势。软件 2.0 利用人工智能的进步，如自然语言处理（NLP）、计算机视觉和强化学习，来创建可以从数据中学习、适应新情况并自然与人类互动的软件组件。
- en: Recently, we have transitioned from writing code and managing explicit instructions
    for a desired goal to a more abstract approach. Developers train models on large
    datasets instead of writing explicit instructions or rules in a programming language.
    Software 2.0 also reduces the need for manual debugging, testing, and maintenance,
    as the neural networks can self-correct and improve over time (see figure 10.1).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我们已从编写代码和管理实现目标的具体指令转变为一种更抽象的方法。开发者在大数据集上训练模型，而不是在编程语言中编写明确的指令或规则。软件 2.0
    还减少了手动调试、测试和维护的需求，因为神经网络可以自我纠正并在时间推移中改进（见图 10.1）。
- en: '![figure](../Images/CH10_F01_Bahree.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F01_Bahree.png)'
- en: Figure 10.1 Software 1.0 versus Software 2.0
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.1 软件 1.0 与软件 2.0 对比
- en: This allows the models to learn the rules or patterns themselves. Algorithms
    and models are crafted to learn from data, make decisions, and improve over time,
    effectively writing the software. This paradigm shift has transformed the role
    of AI from a supportive tool to a fundamental component of system architecture.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得模型能够自己学习规则或模式。算法和模型被设计成从数据中学习、做出决策并在时间推移中改进，从而有效地编写软件。这种范式转变将人工智能的角色从辅助工具转变为系统架构的基本组成部分。
- en: 10.1.2 The era of copilots
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 共同飞行员时代
- en: Another key concept that facilitated the evolution of enterprise application
    architecture for generative AI is copilots—a concept originally proposed by Microsoft.
    Copilots are meant to augment humans and human capabilities and creativity. Using
    an airplane analogy, if we are humans, we are the pilots; instead of AI being
    on autopilot where we have no control or say in how it functions, this new AI
    plays the role of copilots that help us take on cognitive load and some of the
    drudgery of work. Still, we remain in charge as the pilot.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个促进生成式人工智能企业应用架构演变的 key 概念是共同飞行员——这一概念最初由微软提出。共同飞行员旨在增强人类的能力和创造力。用飞机的比喻来说，如果我们是人类，我们就是飞行员；而不是
    AI 在自动驾驶中，我们无法控制或决定其功能，这种新的 AI 扮演共同飞行员的角色，帮助我们承担认知负荷和工作中的一些繁琐任务。然而，我们仍然是飞行员，掌握着控制权。
- en: The Copilot stack is a framework for building AI applications and copilots that
    use LLMs to understand and generate natural language and code. Copilots are intelligent
    assistants that can help users with complex cognitive tasks such as writing, coding,
    searching, or reasoning. Microsoft has developed a range of copilots for different
    domains and platforms, such as GitHub Copilot, Bing Chat, Dynamics 365 Copilot,
    and Windows Copilot. You can also build your custom Copilot using the Copilot
    stack and tools, such as Azure OpenAI, Copilot Studio, and the Teams AI Library.
    Copilots can also be integrated into existing tools and platforms, such as GitHub,
    Visual Studio Code, and Jupyter Notebook, to enhance the productivity and creativity
    of software developers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot堆栈是一个用于构建使用LLMs理解和生成自然语言和代码的AI应用和Copilots的框架。Copilots是智能助手，可以帮助用户完成复杂的认知任务，如写作、编码、搜索或推理。微软已经为不同的领域和平台开发了一系列Copilots，例如GitHub
    Copilot、Bing Chat、Dynamics 365 Copilot和Windows Copilot。您还可以使用Copilot堆栈和工具，如Azure
    OpenAI、Copilot Studio和Teams AI库，构建自己的定制Copilot。Copilots还可以集成到现有的工具和平台中，如GitHub、Visual
    Studio Code和Jupyter Notebook，以增强软件开发者的生产力和创造力。
- en: 'Copilots are based on the concept of Software 2.0, where they use LLMs to generate
    code from natural language descriptions instead of relying on manually written
    code. However, they should be seen as the GenAI application stack, similar to
    the LAMP stack for web development. LAMP is an acronym for the stack components:
    Linux (operating system); Apache (webserver); MySQL (database); and PHP, Perl,
    or Python (programming language).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Copilots基于软件2.0的概念，它们使用LLMs从自然语言描述中生成代码，而不是依赖于手动编写的代码。然而，它们应被视为GenAI应用堆栈，类似于用于Web开发的LAMP堆栈。LAMP是堆栈组件的缩写：Linux（操作系统）；Apache（Web服务器）；MySQL（数据库）；以及PHP、Perl或Python（编程语言）。
- en: Copilots are a useful model for enterprises to follow when designing their generative
    AI apps enterprise architecture because they offer several advantages (e.g., quicker
    and simpler development, more creativity and testing, and improved cooperation
    and learning, enabling enterprises to try out new concepts and opportunities or
    to create original solutions for difficult problems). Let’s expand on what the
    Copilot stack is to make it more relevant and real in concrete terms.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 企业在设计生成式AI应用的企业架构时，可以将Copilots视为一个有用的参考模型，因为它们提供了多个优势（例如，更快、更简单的开发，更多创造性和测试，以及改进的合作和学习，使企业能够尝试新的概念和机会，或为难题创造原创解决方案）。让我们进一步探讨Copilot堆栈，使其在具体方面更具相关性和现实性。
- en: '10.2 Generative AI: Application stack'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 生成式AI：应用堆栈
- en: Copilots’ architecture comprises several layers and components that work together
    to provide a seamless and powerful user experience, as outlined in figure 10.2\.
    We will start from the bottom up, examine each layer and component in detail,
    and find out how they interact.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Copilots的架构由多个层和组件组成，它们协同工作以提供无缝且强大的用户体验，如图10.2所示。我们将从下往上，详细检查每一层和组件，并了解它们是如何相互作用的。
- en: '![figure](../Images/CH10_F02_Bahree.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH10_F02_Bahree.png)'
- en: Figure 10.2 GenAI application stack
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2 GenAI应用堆栈
- en: The AI infrastructure layer is the foundational layer that powers everything
    and hosts the core AI models and computational resources. It encompasses the hardware,
    software, and services that enable the development and deployment of AI applications
    and are often optimized for AI workloads. This also includes the massively scalable
    distributed high-performance computing (HPC), required for training the base foundational
    models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: AI基础设施层是支撑一切的基础层，它托管核心AI模型和计算资源。它包括使AI应用的开发和部署成为可能的硬件、软件和服务，通常针对AI工作负载进行优化。这还包括用于训练基础基础模型的超大规模可扩展分布式高性能计算（HPC）。
- en: The foundational model layer includes the range of supported models, from hosted
    foundation models to the model you train and want to deploy. The hosted foundational
    models are large pretrained models, such as LLMs and others (vision and speech
    models), and the newer small language models (SLMs) that can be used for inference;
    these models can be closed or open. Some of the models can be further adjusted
    for specific tasks or domains. These models are hosted and managed within the
    AI infrastructure layer to ensure high performance and availability. Users can
    select from various hosted foundation models based on their needs and preferences.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型层包括支持的模型范围，从托管基础模型到您训练并希望部署的模型。托管的基础模型是大型预训练模型，如LLMs和其他（视觉和语音模型），以及可以用于推理的新小型语言模型（SLMs）；这些模型可以是封闭的或开放的。一些模型可以进一步调整以适应特定任务或领域。这些模型在AI基础设施层中托管和管理，以确保高性能和可用性。用户可以根据其需求和偏好从各种托管基础模型中进行选择。
- en: 'The orchestration layer manages the interactions between the various components
    of the architecture, ensuring seamless operation and coordination. It is responsible
    for key functions such as task allocation, resource management, and workflow optimization:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 编排层管理架构中各个组件之间的交互，确保无缝运行和协调。它负责关键功能，如任务分配、资源管理和工作流程优化：
- en: The response filtering component uses the prompt engineering set of components;
    here, the prompts and responses are analyzed, filtered, and optimized to generate
    safe outputs.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应过滤组件使用提示工程组件集；在这里，提示和响应被分析、过滤和优化以生成安全的输出。
- en: The system prompt can also provide additional information or constraints for
    the AI model to follow. The user can express a system prompt via a simple syntax,
    or the system can automatically generate it.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统提示也可以为AI模型提供额外的信息或约束。用户可以通过简单的语法表达系统提示，或者系统可以自动生成它。
- en: Grounding is the implementation of retrieval-augmented generation (RAG), and
    it refers to the process of contextualizing the responses generated by the AI
    model. Grounding ensures the outputs are syntactically correct, semantically meaningful,
    and relevant to the given context or domain. We use plugins to get data ingested
    from different enterprise systems.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础是检索增强生成（RAG）的实施，它指的是将AI模型生成的响应进行上下文化的过程。基础确保输出在语法上是正确的、在语义上有意义，并且与给定的上下文或领域相关。我们使用插件从不同的企业系统中获取数据。
- en: The plugin execution layer runs plugins that add more features to the basic
    AI model. Plugins are separate and reusable parts that can do different things,
    such as data processing, formatting, validation, or transformation. This is very
    important for taking in data to make embeddings and employing vector databases
    and indexes when we use RAG in our solutions.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插件执行层运行插件，为基本AI模型添加更多功能。插件是独立且可重用的部分，可以执行不同的任务，例如数据处理、格式化、验证或转换。这对于将数据纳入以制作嵌入以及在我们使用RAG时采用向量数据库和索引非常重要。
- en: The UX layer is the interface that allows the users to use Copilot. It is easy
    to use and has strong tools for working with the AI features underneath. The exact
    nature of how the UX operates depends on which aspect of the application and workflow
    the Copilot is plugging into. For example, suppose one uses Copilot as part of
    Microsoft Office 365\. The way the UX works in Word differs from that in PowerPoint
    and in other applications, such as GitHub Copilot, as we saw earlier.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: UX层是允许用户使用Copilot的界面。它易于使用，并具有强大的工具来处理AI功能。UX的确切操作方式取决于Copilot插入的应用程序和工作流程的哪个方面。例如，假设将Copilot作为Microsoft
    Office 365的一部分使用。Word中的UX工作方式与PowerPoint和其他应用程序（如我们之前看到的GitHub Copilot）不同。
- en: Finally, all of this is done with AI safety, a main part of responsible AI,
    ensuring the technology is used ethically and responsibly. AI safety includes
    different methods and rules, which we will explain later in the book.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，所有这些都是在AI安全性的指导下完成的，这是负责任AI的重要组成部分，确保技术被道德和负责任地使用。AI安全性包括不同的方法和规则，我们将在本书的后续部分进行解释。
- en: 10.2.1 Integrating the GenAI stack
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 集成GenAI堆栈
- en: To integrate the GenAI stack into an enterprise application, a strategic and
    technical approach is required. It starts with knowing the requirements and the
    business challenges that can be addressed with GenAI, especially LLMs. This involves
    connecting specific use cases to the capabilities of these AI technologies, focusing
    on areas where they can offer substantial value, such as streamlining complex
    workflows, enhancing data analytics, or easing customer interactions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要将GenAI堆栈集成到企业应用程序中，需要战略和技术方法。这始于了解GenAI（特别是LLMs）可以解决的要求和业务挑战。这涉及到将特定用例与这些AI技术的功能联系起来，重点关注它们可以提供实质性价值的领域，例如简化复杂的工作流程、增强数据分析或简化客户互动。
- en: Keeping the use cases in mind, the next step is to create an integration architecture
    that fits the AI stack within the boundaries of the current enterprise system,
    using a service-oriented architecture (SOA) or a microservices approach for adaptability.
    Establishing secure, scalable, and maintainable APIs is important to facilitating
    communication between the application and AI services, which will be the basis
    for the integration.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到用例，下一步是创建一个集成架构，该架构在当前企业系统的边界内适合AI堆栈，使用面向服务的架构（SOA）或微服务方法以提高适应性。建立安全、可扩展和可维护的API对于促进应用程序和AI服务之间的通信至关重要，这将是集成的基石。
- en: The AI infrastructure configuration is an important stage where the organization’s
    policies and the data’s sensitivity determine whether to choose on-premises, cloud,
    or a hybrid method. The infrastructure needs hardware and data storage options
    to meet the use case’s demands. A robust data pipeline is also essential for efficient
    model inference, especially when using RAG.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: AI基础设施配置是一个重要阶段，组织的政策和数据敏感性决定了是选择本地化、云服务还是混合方法。基础设施需要硬件和数据存储选项以满足用例的需求。一个强大的数据管道对于高效的模型推理也是必不可少的，尤其是在使用RAG时。
- en: With GenAI, developers can use existing models from cloud AI services or run
    models on their servers. Developers can build or adjust domain-specific models
    when custom solutions are required, ensuring they are trained on accurate, relevant
    datasets and adding continuous learning methods to enhance the model with new
    data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GenAI，开发者可以使用云AI服务中的现有模型或在他们的服务器上运行模型。当需要定制解决方案时，开发者可以构建或调整特定领域的模型，确保它们在准确、相关的数据集上训练，并添加持续学习方法以通过新数据增强模型。
- en: To maintain responsible AI (RAI), safety, and adherence to data protection laws,
    response filtering systems are used to prevent the creation of unsuitable content
    and compliance. The user experience is based on this UX design, which allows users
    to interact with the AI stack. The design process is repeated, incorporating user
    feedback to satisfy the enterprise’s needs efficiently.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了维护负责任的AI（RAI）、安全性和遵守数据保护法律，响应过滤系统被用来防止创建不适当的内容和合规问题。用户体验基于这种UX设计，使用户能够与AI堆栈交互。设计过程会重复进行，吸收用户反馈以高效满足企业的需求。
- en: The system allows for the inclusion of third-party integrations or custom extensions
    through a secure plugin architecture, which can run them without affecting the
    application’s reliability. An orchestration layer handles the interactions between
    different AI components, ensuring the system can adjust to different demand levels.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 系统允许通过安全的插件架构包含第三方集成或自定义扩展，而不会影响应用程序的可靠性。编排层处理不同AI组件之间的交互，确保系统能够适应不同的需求水平。
- en: Deployment is automated to ensure consistent and reliable updates to the AI
    stack, and CI/CD pipelines are established to enable ongoing integration and delivery
    without disrupting existing functionalities. The performance and health of the
    AI stack are continuously monitored, with comprehensive logging and alert systems
    to notify of any problems.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 部署是自动化的，以确保AI堆栈的一致性和可靠性更新，并建立CI/CD管道以实现持续的集成和交付，而不会干扰现有功能。AI堆栈的性能和健康状况会持续监控，并配备全面的日志记录和警报系统，以通知任何问题。
- en: Finally, a successful adoption and operation of the AI stack depends on well-documented
    guidelines and training for developers and end users, ensuring they are fully
    prepared to use, troubleshoot, and maintain the new system. Each step in this
    process requires detailed planning, cross-team collaboration, and a deep technical
    understanding to ensure a smooth and effective integration into the enterprise
    architecture. Let’s explore this GenAI stack in more detail.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，AI堆栈的成功采用和运营取决于为开发人员和终端用户提供的良好文档化的指南和培训，确保他们完全准备好使用、故障排除和维护新系统。此过程中的每一步都需要详细的规划、跨团队合作和深入的技术理解，以确保顺利有效地集成到企业架构中。让我们更详细地探讨这个GenAI堆栈。
- en: 10.2.2 GenAI architecture principles
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 GenAI架构原则
- en: When building mission-critical applications, enterprises focus on creating a
    robust, scalable, and secure system. Although the traditional architectural principles
    remain unchanged, key additional architectural aspects for generative AI are outlined
    in figure 10.3.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建关键任务应用时，企业专注于创建一个强大、可扩展和安全的系统。尽管传统的架构原则保持不变，但图10.3概述了生成式AI的关键附加架构方面。
- en: '![figure](../Images/CH10_F03_Bahree.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F03_Bahree.png)'
- en: Figure 10.3 Generative AI architecture principles
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.3 生成式AI架构原则
- en: Many GenAI models are accessed via an API, so the model API integration is an
    architecture principle that helps connect with the GenAI API. The models and APIs
    have different ways of formatting and sending data, as well as limits and quotas
    on how many requests they can handle; thus, it can be helpful to create a layer
    of abstraction that can adjust to changes in each API’s design. This involves
    handling API requests and responses and managing API limits and quotas. It is
    also common to have multiple models used in the same application to choose the
    right model for each situation. Having an abstraction layer can help protect each
    API’s design from changes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 许多GenAI模型通过API访问，因此模型API集成是一个架构原则，有助于连接到GenAI API。模型和API有不同的数据格式化和发送方式，以及它们可以处理的请求数量和配额的限制；因此，创建一个可以适应每个API设计变化的抽象层可能很有帮助。这涉及到处理API请求和响应以及管理API限制和配额。在同一个应用程序中使用多个模型以选择适合每种情况的正确模型也很常见。拥有一个抽象层可以帮助保护每个API的设计不受变化的影响。
- en: As a principle, scalability and performance help the application deal with elastic
    scale and changing loads as they increase and decrease. This involves selecting
    the appropriate cloud infrastructure, balancing the load, and potentially using
    asynchronous processing to manage intensive tasks. Moreover, the use of containerization
    and microservices architecture can help with both scalability and performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项原则，可扩展性和性能有助于应用程序处理弹性扩展和变化的负载。这涉及到选择合适的云基础设施、平衡负载以及可能使用异步处理来管理密集型任务。此外，使用容器化和微服务架构可以帮助提高可扩展性和性能。
- en: Hosting LLMs in an enterprise data center is not a trivial task, as it requires
    careful planning to achieve scalability and performance. You must choose an appropriate
    LLM architecture, comparing open source and proprietary alternatives that align
    with the business goals. A streamlined end-to-end pipeline is crucial for smooth
    operations, using orchestration frameworks for workflow management. The infrastructure
    should be solid for GPU optimization and simplified infrastructure management.
    LLMOps should be applied for best practices in deployment, and continuous monitoring
    for performance tracking should be set up. Scalability should be ensured through
    load balancing and auto-scaling. The data and models should be secured with encryption
    and access controls, and industry regulations should be followed. This comprehensive
    approach ensures that LLMs can serve multiple internal customers efficiently and
    reliably. Of course, it involves significant and continuous investment in capital
    expenditure and technical expertise.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业数据中心托管LLMs（大型语言模型）不是一个简单任务，因为它需要精心规划以实现可扩展性和性能。您必须选择合适的LLM架构，比较与业务目标一致的开源和专有替代方案。一个简化的端到端管道对于平稳运行至关重要，使用编排框架进行工作流程管理。基础设施应该坚固，以优化GPU并简化基础设施管理。LLMOps（大型语言模型操作）应应用于部署的最佳实践，并应设置持续监控以跟踪性能。应通过负载均衡和自动扩展确保可扩展性。数据和模型应通过加密和访问控制进行保护，并应遵守行业法规。这种全面的方法确保LLMs可以高效可靠地为多个内部客户提供服务。当然，这涉及到资本支出和技术专长的重大和持续投资。
- en: Due to the data’s sensitive nature, it is crucial to implement strong data privacy
    and security measures, which include encrypting data both in transit and at rest,
    managing access controls, and ensuring compliance with regulations such as GDPR
    or HIPAA. In addition, it is important to have a data minimization strategy where
    only necessary data is collected and processed, and security audits and penetration
    testing should be conducted regularly to identify and address vulnerabilities
    proactively. Some cloud providers, such as Azure, offer robust enterprise support
    systems and compliance solutions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据具有敏感性质，实施强大的数据隐私和安全措施至关重要，这包括在传输中和静止状态下加密数据，管理访问控制，并确保符合GDPR或HIPAA等法规。此外，重要的是要有一个数据最小化策略，其中只收集和处理必要的数据，并且应定期进行安全审计和渗透测试，以主动识别和解决漏洞。一些云服务提供商，如Azure，提供强大的企业支持系统和合规解决方案。
- en: Error handling and monitoring do not constitute a new architecture principle;
    with distributed systems, if you do not plan for failure, you are planning to
    fail. Use effective error handling and monitoring to check the GenAI application’s
    health. This means logging errors, creating alerts for anomalies, and having a
    plan for handling downtime or API limits, including using automatic recovery strategies,
    such as fallback mechanisms, to ensure high availability. Distributed tracing
    is essential for complex, microservice-based architectures to better track problems.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 错误处理和监控并不构成新的架构原则；在分布式系统中，如果你没有为故障做计划，那么你就是在计划失败。使用有效的错误处理和监控来检查GenAI应用程序的健康状况。这意味着记录错误，为异常创建警报，并制定处理停机或API限制的计划，包括使用自动恢复策略，如回退机制，以确保高可用性。分布式跟踪对于复杂、基于微服务的架构至关重要，以更好地跟踪问题。
- en: LLMs are evolving cost and currency meanings. LLM usage growth can lead to unexpected
    expenses. To control costs, optimize API calls and use caching strategies. Have
    budget alerting and cost forecasting mechanisms to avoid surprises.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）正在演变成本和货币的含义。LLM使用量的增长可能导致意外的费用。为了控制成本，优化API调用并使用缓存策略。设有预算警报和成本预测机制以避免意外。
- en: The GenAI UX design focuses on how users interact with the GenAI models. This
    would vary depending on the model type; for a language-based use case using an
    LLM, the UX design would be quite different from an image-based use case where
    you would be using Stable Diffusion or DALL-E. This includes designing intuitive
    interfaces, providing helpful prompts, and ensuring the model’s responses align
    with user expectations. In some ways, everything should not be a simple chatbot,
    but it should extend and enhance the experience based on the task and intent.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI用户体验设计侧重于用户如何与GenAI模型交互。这会根据模型类型而有所不同；对于使用LLM的语言型用例，UX设计将与使用Stable Diffusion或DALL-E的图像型用例大不相同。这包括设计直观的界面，提供有用的提示，并确保模型的响应与用户期望一致。在某种程度上，一切都不应只是一个简单的聊天机器人，而应根据任务和意图扩展和增强体验。
- en: Consider the ethical, biased, and legal implications of GenAI apps, especially
    when using LLMs. Mitigate biases and prevent harmful stereotypes. Understand legal
    consequences in healthcare, finance, or law. Follow relevant laws and industry
    standards. New regulations are emerging, and chapter 12 will cover more on responsible
    AI use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑GenAI应用程序的道德、偏见和法律影响，尤其是在使用LLMs时。减轻偏见并防止有害的刻板印象。了解医疗保健、金融或法律中的法律后果。遵循相关法律和行业标准。新的法规正在出现，第12章将涵盖更多关于负责任AI使用的内容。
- en: '10.2.3 GenAI application architecture: A detailed view'
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 GenAI应用程序架构：详细视图
- en: Based on the high-level architecture diagram and going into more detail, figure
    10.4 illustrates the overall structure of a GenAI app stack. Although we have
    already used many of these elements in the previous chapters, this is our first
    look at it holistically. There are six broad categories forming different components
    that constitute the GenAI app stack.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于高级架构图并进一步详细说明，图10.4展示了GenAI应用程序堆栈的整体结构。尽管我们已经在之前的章节中使用了这些元素中的许多，但这是我们第一次从整体上审视它。有六个广泛的类别构成了不同的组件，这些组件构成了GenAI应用程序堆栈。
- en: '![figure](../Images/CH10_F04_Bahree.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F04_Bahree.png)'
- en: Figure 10.4 GenAI app stack
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.4 GenAI应用程序堆栈
- en: Next, we will examine each layer more closely.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将更仔细地检查每一层。
- en: Orchestration layer
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编排层
- en: The orchestration layer is the central component that integrates various services
    and manages data flow and prompts. It is responsible for scheduling tasks, allocating
    resources, and handling errors resiliently. The prompt management system is a
    critical part of this layer, utilizing AI technology to develop prompts that elicit
    the best possible responses from LLMs. This involves A/B testing and machine learning
    (ML) models to analyze user interactions and optimize prompts for higher engagement
    and accuracy. Orchestration tools such as Kubernetes can manage containerized
    microservices and enable component deployment across cloud providers and on-premises
    environments to improve the system’s robustness and fault tolerance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 编排层是集成分服务、管理数据流和提示的中心组件。它负责调度任务、分配资源并弹性处理错误。提示管理系统是这一层的关键部分，利用AI技术开发能够从LLM中获得最佳响应的提示。这涉及到A/B测试和机器学习（ML）模型来分析用户交互并优化提示以提高参与度和准确性。编排工具如Kubernetes可以管理容器化微服务，并允许在云提供商和本地环境中部署组件，以提高系统的鲁棒性和容错性。
- en: Grounding layer
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基础层
- en: This layer is the basis of GenAI applications that deal with getting, storing,
    processing, and delivering data. It must work with different record systems, requiring
    connectors to handle various data formats and protocols. Data pipelines are the
    channels that link to the different source systems to take in data for applying
    RAG and enabling enterprises to use their data. The pipelines can connect to the
    system of records through APIs natively (where supported) or using different plugins.
    Data pipelines should be built for high speed and low delay, with the ability
    to handle batch and stream processing as required. The plugin runtime considers
    different authentication aspects, data refresh configurations, and so forth. Data
    preprocessing is important for changing data into a format that LLMs can use.
    Therefore, this layer includes ML models for creating embeddings and vector databases
    such as Redis, as we saw earlier in the book, or others such as Cosmos DB, Pinecone,
    Milvus, Qdrant, and so forth. Using distributed data-processing frameworks such
    as Apache Spark or Azure Fabric ensures scalability and fault tolerance in data
    processing.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层是处理获取、存储、处理和交付数据的GenAI应用的基础。它必须与不同的记录系统协同工作，需要连接器来处理各种数据格式和协议。数据管道是连接到不同源系统以获取数据以应用RAG并使企业能够使用其数据的通道。管道可以通过API原生连接到记录系统（如果支持）或使用不同的插件。数据管道应构建为高速和低延迟，能够根据需要处理批处理和流处理。插件运行时考虑不同的认证方面、数据刷新配置等。数据预处理对于将数据转换为LLM可以使用的形式非常重要。因此，这一层包括用于创建嵌入和向量数据库（如我们之前在书中看到的Redis）的ML模型，以及其他如Cosmos
    DB、Pinecone、Milvus、Qdrant等。使用Apache Spark或Azure Fabric等分布式数据处理框架确保数据处理的可扩展性和容错性。
- en: Model layer
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型层
- en: The model layer needs to support a diverse range of models, from frontier general-purpose
    LLMs such as GPT-4 to highly specialized SLMs such as Phi-2 [2] and Orca 2 [3].
    We will learn more about SLMs and see an example of using Phi-2 as a classifier
    later in the chapter. As a result, the model layer should provide a consistent
    interface for accessing different models, regardless of whether they are hosted
    internally or externally. When considering model hosting, it is essential to scale
    models to handle varying loads, which may require technologies such as serverless
    computing to allocate resources dynamically. The model catalogs serve as a registry
    and repository, simplifying the discovery and management of models. This layer
    also encompasses the model-as-a-platform concept, which allows developers to extend
    and customize models, similar to how platforms such as Salesforce enable application
    customization.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型层需要支持多样化的模型，从前沿的通用大型语言模型（LLM）如GPT-4到高度专业化的小型语言模型（SLM）如Phi-2 [2] 和Orca 2 [3]。我们将在本章后面学习更多关于SLM的知识，并看到一个使用Phi-2作为分类器的示例。因此，模型层应提供一致的接口来访问不同的模型，无论它们是内部托管还是外部托管。在考虑模型托管时，将模型扩展以处理不同负载是至关重要的，这可能需要如无服务器计算等技术来动态分配资源。模型目录作为注册和存储库，简化了模型的发现和管理。这一层还涵盖了模型即平台的概念，允许开发者扩展和定制模型，类似于Salesforce等平台如何实现应用程序定制。
- en: Response filtering
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 响应过滤
- en: This layer is crucial for maintaining trust in GenAI applications by ensuring
    quality assurance and content moderation. It involves using classifiers and NLP
    tools to screen the outputs for accuracy, bias, and appropriateness. Responsible
    AI practices are integrated into this layer, incorporating ethical considerations
    and ensuring compliance with regulations such as GDPR data privacy law. The caching
    system within this layer improves performance and enables quick rollback and suitability
    of outputs. Continuous monitoring and real-time evaluation of outputs ensure the
    AIQ is maintained throughout the application’s lifecycle. Moreover, this layer
    also addresses the ethical implications of GenAI technologies, which includes
    developing frameworks for ethical decision-making, ensuring model transparency,
    and incorporating fairness and inclusivity into the design of AI systems.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层对于通过确保质量保证和内容审核来维护GenAI应用的信任至关重要。它涉及使用分类器和NLP工具来筛选输出，以确保准确性、偏见和适宜性。负责任的AI实践被整合到这一层中，包括道德考虑并确保符合GDPR数据隐私法等法规。这一层内的缓存系统提高了性能，并使输出快速回滚和适宜性成为可能。对输出的持续监控和实时评估确保在整个应用生命周期中维持AIQ。此外，这一层还处理了GenAI技术的伦理影响，包括制定道德决策框架、确保模型透明度，并将公平性和包容性纳入AI系统的设计。
- en: Additional architecture considerations
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 额外的架构考虑因素
- en: 'While comprehensive, the architecture outlined earlier does not cover the following
    additional considerations, which are critical for production deployment and understood
    well by most enterprises:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面概述的架构很全面，但它没有涵盖以下额外的考虑因素，这些因素对于生产部署至关重要，并且大多数企业都了解这些因素：
- en: '*Integrations*—These applications don’t work alone and must connect with the
    rest of the enterprise system, which enables the smooth transfer of data and services
    across internal and external systems. Middleware technologies such as enterprise
    service buses (ESBs) or API gateways are used to handle communication and data
    conversion between different systems.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集成*—这些应用程序不能独立工作，必须与整个企业系统连接，从而实现数据和服务在内部和外部系统之间的顺畅传输。企业服务总线（ESBs）或API网关等中间件技术被用于处理不同系统之间的通信和数据转换。'
- en: '*Security*—Security has always been a concern, and it is the same with GenAI;
    all data in the GenAI ecosystem must be safeguarded from unauthorized access and
    breaches, which requires strong authentication and authorization methods, transit
    and rest encryption, and frequent security audits.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安全*—安全始终是一个关注点，对于通用人工智能（GenAI）也是如此；GenAI生态系统中的所有数据都必须得到保护，防止未经授权的访问和泄露，这需要强大的身份验证和授权方法、传输和静态加密，以及定期的安全审计。'
- en: '*Production deployment and scaling*—The focus here is on the strategies for
    deploying GenAI applications across various environments, which includes using
    container orchestration systems for deployment, auto-scaling services to handle
    dynamic loads, and infrastructure as code for repeatable and reliable provisioning
    of resources.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生产部署和扩展*—这里的重点是部署GenAI应用到各种环境中的策略，包括使用容器编排系统进行部署、自动扩展服务来处理动态负载，以及使用基础设施即代码来提供可重复和可靠的资源。'
- en: 10.3 Orchestration layer
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 编排层
- en: Generative AI applications require an orchestrator layer that acts as the backbone
    and is crucial for managing complex tasks and workflows. This is a middle-tier
    and integration layer between the models, enterprise data stores, other components,
    and applications. It coordinates and manages various components and processes
    that enable the generation of content by AI models within an enterprise architecture.
    Ensuring that the workflows involving LLMs are efficient, scalable, and reliable
    for generating content is essential.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI应用需要一个编排层，它作为骨干，对于管理复杂任务和工作流程至关重要。这是模型、企业数据存储、其他组件和应用程序之间的中间层和集成层。它协调和管理各种组件和流程，使AI模型在企业架构内生成内容。确保涉及大型语言模型（LLMs）的工作流程在生成内容方面高效、可扩展和可靠是至关重要的。
- en: The main duties of an orchestrator include managing workflows and service orchestration,
    but they can be expanded to include additional responsibilities. An orchestrator
    consists of several components. Orchestration frameworks simplify the manage-ment
    and interaction with LLMs by abstracting away the complexities of prompt generation,
    resource management, and performance monitoring. They provide a high-level interface
    that enables developers to focus on building their applications without getting
    bogged down in the technical details of LLM interaction. Table 10.1 outlines the
    key responsibilities.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 坐标器的主要职责包括管理工作流程和服务编排，但它们可以扩展到包括其他责任。坐标器由几个组件组成。编排框架通过抽象提示生成、资源管理和性能监控的复杂性，简化了与LLMs的管理和交互。它们提供了一个高级接口，使开发者能够专注于构建他们的应用程序，而无需陷入LLMs交互的技术细节。表10.1概述了关键职责。
- en: Table 10.1 Orchestrator key responsibilities
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表10.1 坐标器关键职责
- en: '| Area | Descriptions |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 区域 | 描述 |'
- en: '| --- | --- |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Workflow management  | Orchestrator ensures that the sequence of processes—from
    data ingestion and processing to AI model inference and response delivery—is executed
    in an orderly and efficient manner. This includes state management to coordinate
    dependencies between tasks, error handling, retry mechanisms, and the dynamic
    allocation of resources based on the task load.  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 工作流程管理 | 坐标器确保从数据摄取和处理到AI模型推理和响应交付的过程顺序和高效执行。这包括状态管理以协调任务之间的依赖关系、错误处理、重试机制以及根据任务负载动态分配资源。
    |'
- en: '| Service orchestration  | Microservices architecture is typically employed,
    where each service is responsible for a discrete function in the generative AI
    process. Service orchestration is about managing these services to scale, communicate,
    and function seamlessly. In addition, containerization platforms such as Docker
    and orchestration systems such as Kubernetes deploy, manage, and scale the microservices
    across various environments.  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 服务编排 | 通常采用微服务架构，其中每个服务负责生成AI过程中的一个离散功能。服务编排是关于管理这些服务以进行扩展、通信和无缝运行。此外，Docker等容器化平台和Kubernetes等编排系统在各个环境中部署、管理和扩展微服务。
    |'
- en: '| Data flow coordination  | Ensure that data flows correctly through the system,
    from the initial data sources to the model and back to the end user or application.
    This includes preprocessing inputs, queue management for incoming requests, and
    routing outputs to the correct destinations.  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 数据流协调 | 确保数据正确地通过系统流动，从初始数据源到模型，再返回到最终用户或应用程序。这包括预处理输入、管理入站请求的队列以及将输出路由到正确的目的地。
    |'
- en: '| Load balancing and auto-scaling  | Load balancers distribute incoming AI
    inference requests across multiple instances to prevent any single instance from
    becoming a bottleneck. Auto-scaling adjusts the number of active instances based
    on the current load, ensuring cost-effective resource use. This also has API management
    components to manage rate limits and implement back-off strategies for production
    workloads.  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 负载均衡和自动扩展 | 负载均衡器将入站的AI推理请求分配到多个实例，以防止任何单个实例成为瓶颈。自动扩展根据当前负载调整活动实例的数量，确保资源使用的成本效益。这还包括API管理组件，用于管理速率限制并实施生产工作负载的退避策略。
    |'
- en: '| Model versioning and rollback  | Orchestration includes maintaining different
    versions of AI models and managing their deployment. It allows for quick rollback
    to previous versions if a new model exhibits unexpected behavior or poor performance.  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 模型版本控制和回滚 | 编排包括维护不同版本的AI模型并管理它们的部署。如果新模型表现出意外的行为或性能不佳，它允许快速回滚到之前的版本。 |'
- en: '| Managing model context windows  | Orchestrator enhances interactions by efficiently
    managing context windows and token counts. It tracks and dynamically adjusts conversation
    history within the model’s token limits and maintains coherence in responses,
    especially in long or complex exchanges. Best practices include efficient context
    management, handling edge cases, continuous performance monitoring, and incorporating
    user feedback for ongoing improvements.  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 管理模型上下文窗口 | 坐标器通过高效管理上下文窗口和令牌计数来增强交互。它跟踪并动态调整模型令牌限制内的对话历史，并保持响应的一致性，尤其是在长或复杂的交流中。最佳实践包括高效上下文管理、处理边缘情况、持续性能监控以及结合用户反馈进行持续改进。
    |'
- en: These different components work together to create a strong orchestration system
    that serves as the foundation for the successful deployment and operation of generative
    AI technology in the enterprise sector. Such orchestration is necessary for the
    intricacy and constant changes of AI-powered applications to avoid inefficiencies,
    mistakes, and system breakdowns.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的组件共同工作，创建了一个强大的编排系统，为企业部门中生成式AI技术的成功部署和运营提供基础。这种编排对于AI驱动应用的复杂性和不断变化是必要的，以避免低效、错误和系统故障。
- en: 10.3.1 Benefits of an orchestration framework
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 编排框架的益处
- en: Orchestrators are essential for managing the complex systems powering generative
    AI apps. These systems involve diverse processes that need careful coordination
    through orchestration tools. Orchestrators simplify workflows and ensure tasks
    are done in order, with dependencies and error-handling rules taken care of. This
    results in a reliable and regular operational flow, where steps for preprocessing,
    computation, and postprocessing are smoothly connected, ensuring data quality
    and consistent output generation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 编排器对于管理驱动生成式AI应用的复杂系统至关重要。这些系统涉及多种需要通过编排工具仔细协调的过程。编排器简化工作流程并确保任务按顺序完成，同时处理依赖关系和错误处理规则。这导致了一个可靠且规律的操作流程，其中预处理、计算和后处理的步骤顺利连接，确保数据质量和一致的输出生成。
- en: Scalability is another area where orchestration is vital. As demand fluctuates,
    a system that dynamically adjusts resource allocation, especially for production
    workloads, becomes crucial. An orchestrator can provide this agility using different
    techniques, such as load balancers to distribute workloads evenly and auto-scaling
    features to modulate computing power in real-time. This elasticity meets the load
    requirements and optimizes resource usage, balancing performance and cost efficiency.
    The orchestrators would need to manage this across different models, as well as
    the computational and cost profiles of those models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性是另一个需要编排至关重要的领域。随着需求的波动，一个能够动态调整资源分配的系统，尤其是对于生产工作负载，变得至关重要。编排器可以通过不同的技术提供这种灵活性，例如使用负载均衡器均匀地分配工作负载，以及使用自动扩展功能实时调节计算能力。这种弹性满足了负载需求并优化了资源使用，平衡了性能和成本效率。编排器需要跨不同模型以及这些模型的计算和成本配置文件来管理这一点。
- en: Orchestrators offer a centralized management and monitoring ability. They constitute
    frameworks that offer dashboards and tools for monitoring LLM usage, identifying
    bottlenecks, and troubleshooting problems. This enhances system reliability by
    monitoring service health, responding to failures, and ensuring minimal downtime.
    Orchestrators can employ automated recovery processes, such as instance restarts
    or replacements, allowing for service continuity.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 编排器提供集中式管理和监控能力。它们构成了提供仪表板和工具的框架，用于监控LLM使用情况、识别瓶颈和解决问题。通过监控服务健康、响应故障和确保最小停机时间，这增强了系统可靠性。编排器可以采用自动恢复过程，如实例重启或替换，以实现服务连续性。
- en: The default deployment model is a pay-as-you-go method for most cloud-based
    LLM providers. This model is shared with other customers, and incoming requests
    are queued and processed on a first-come, first-served basis. However, for production
    workloads that require a better user experience, Azure OpenAI service offers a
    provisioned throughput units (PTU) feature. This feature allows customers to reserve
    and deploy units of model processing capacity for prompt processing and generating
    completions. Each unit’s minimum PTU deployment, increments, and processing capacity
    vary depending on the model type and version. An orchestrator will manage the
    different deployment endpoints between regular pay-as-you-go and PTUs to ensure
    optimum performance and cost-effectiveness.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数基于云的LLM提供商，默认部署模型是一种按使用付费的方法。这种模型与其他客户共享，入站请求按先到先服务的原则排队和处理。然而，对于需要更好用户体验的生产工作负载，Azure
    OpenAI服务提供了一种预配吞吐量单位（PTU）功能。此功能允许客户预留和部署用于提示处理和生成完成的模型处理能力单位。每个单位的最低PTU部署、增量以及处理能力取决于模型类型和版本。编排器将管理常规按使用付费和PTU之间的不同部署端点，以确保最佳性能和成本效益。
- en: Orchestrators play a significant role in increasing productivity and streamlining
    operations, which are achieved in two ways. First, it reduces the need to write
    repetitive code for common tasks such as prompt construction and output processing,
    thus increasing developers’ productivity. Second, it automates the deployment
    and management of services, thus minimizing the possibility of human error. This
    automated process reduces manual overhead and ensures effective compute resource
    utilization, streamlining production operations. We will delve deeper into managing
    operations later in the chapter.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 协调器在提高生产力和简化操作中发挥着重要作用，这通过两种方式实现。首先，它减少了编写重复代码的需求，例如提示构造和输出处理，从而提高了开发者的生产力。其次，它自动化了服务的部署和管理，从而最大限度地减少了人为错误的可能性。这个自动化过程减少了人工开销，并确保了有效的计算资源利用，简化了生产操作。我们将在本章后面更深入地探讨管理操作。
- en: Compliance and governance are essential requirements for any enterprise. An
    orchestrator can assist in enforcing compliance by determining how data is processed,
    stored, and used in the workflow, which ensures that the data complies with the
    enterprise’s data governance policies and privacy regulations. Maintaining trust
    and legal compliance in enterprise operations is crucial and can be achieved through
    adherence to data governance policies and privacy regulations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 遵守和治理是企业的重要需求。协调器可以通过确定数据在工作流程中的处理、存储和使用方式来协助执行遵守，从而确保数据符合企业的数据治理政策和隐私法规。在企业的运营中保持信任和遵守法律至关重要，可以通过遵守数据治理政策和隐私法规来实现。
- en: 10.3.2 Orchestration frameworks
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 协调器框架
- en: Many people are familiar with orchestrators and orchestration frameworks. While
    frameworks such as Kubernetes, Apache Airflow, and MLflow are effective general
    orchestration tools for software engineering and can support ML operations, they
    are not designed exclusively for generative AI applications. Orchestrating workflows
    for generative AI requires a more intimate understanding of the nuances of these
    complex technologies.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人熟悉协调器和协调器框架。虽然Kubernetes、Apache Airflow和MLflow等框架是有效的通用协调工具，可以支持机器学习操作，但它们并非专为生成式人工智能应用设计。为生成式人工智能协调工作流程需要更深入地了解这些复杂技术的细微差别。
- en: The choice of an orchestration framework for generative AI applications depends
    on the existing technology stack, the complexity of the workflows, and specific
    requirements. Table 10.2 outlines orchestration frameworks tailored to the specific
    needs of generative AI applications. These frameworks can handle traditional computational
    workflows; manage interactions’ state, context, and coherence; and are designed
    to suit the unique requirements of generative AI.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能应用协调器框架的选择取决于现有的技术堆栈、工作流程的复杂性和具体要求。表10.2概述了针对生成式人工智能应用特定需求的协调器框架。这些框架可以处理传统的计算工作流程；管理交互的状态、上下文和连贯性；并且设计用于满足生成式人工智能的独特要求。
- en: Table 10.2 Orchestration frameworks
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表10.2 协调器框架
- en: '| Name | Notes |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 备注 |'
- en: '| --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Semantic Kernel  | Semantic Kernel is an OSS framework from Microsoft that
    aims to create a unified framework for semantic search and generative AI. It uses
    pretrained LLMs and graph-based knowledge representations to enable rich and diverse
    natural language interaction.  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 语义内核 | 语义内核是微软的一个开源框架，旨在创建一个用于语义搜索和生成式人工智能的统一框架。它使用预训练的LLM和基于图的知识表示来启用丰富多样的自然语言交互。  |'
- en: '| LangChain  | LangChain is a library that chains language models with external
    knowledge and capabilities. It facilitates the orchestration of LLMs such as GPT-4
    with databases, APIs, and other systems to create more comprehensive AI applications.  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| LangChain | LangChain是一个库，它将语言模型与外部知识和能力链在一起。它促进了LLM（如GPT-4）与数据库、API和其他系统的协调，以创建更全面的AI应用。  |'
- en: '| PromptLayer  | PromptLayer is a platform that simplifies the creation, management,
    and deployment of prompts for LLMs. Users can visually edit and test prompts,
    compare models, log requ-ests, and monitor performance. More details can be found
    at [https://promptlayer.com/](https://promptlayer.com/).  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| PromptLayer | PromptLayer是一个平台，它简化了LLM的提示创建、管理和部署。用户可以直观地编辑和测试提示，比较模型，记录请求并监控性能。更多详情请见[https://promptlayer.com/](https://promptlayer.com/)。  |'
- en: '| Rasa  | Rasa is an enterprise conversational AI platform that lets you create
    chat- and voice-based AI assistants to manage various conversations for different
    purposes. In addition to conversation AI, it also offers a generative AI-native
    method for building assistants, with enterprise features such as analytics, security,
    observability, testing, knowledge integration, voice connectors, and so forth.
    More information is available at [https://rasa.com/](https://rasa.com/).  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Rasa  | Rasa 是一个企业级对话人工智能平台，让您能够创建基于聊天和语音的人工智能助手，以管理不同目的的各种对话。除了对话人工智能外，它还提供了一种面向生成式人工智能的构建助手的方法，具有企业功能，如分析、安全、可观察性、测试、知识集成、语音连接器等。更多信息请访问
    [https://rasa.com/](https://rasa.com/)。  |'
- en: '| YouChat API  | The YOU API is a suite of tools that helps enterprises ground
    the output of LLMs in the most recent, accurate, and relevant information available.
    You can use the YOU API to access web search results, news articles, and RAG for
    LLMs. More details can be found at [https://api.you.com/](https://api.you.com/).  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| YouChat API  | YOU API 是一套工具集，帮助企业将大型语言模型（LLMs）的输出建立在最新、最准确和最相关的信息之上。您可以使用
    YOU API 访问网络搜索结果、新闻文章和 RAG 为 LLMs。更多详情请访问 [https://api.you.com/](https://api.you.com/)。  |'
- en: '| Ragna  | Ragna is an open source RAG-based AI orchestration framework that
    allows you to experiment with different aspects of a RAG model—LLMs, vector databases,
    tokenization strategies, and embedding models. It also allows you to create custom
    RAG-based web apps and extensions from different data sources. More details can
    be found at [https://ragna.chat/](https://ragna.chat/).  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Ragna  | Ragna 是一个基于 RAG 的开源人工智能编排框架，允许您实验 RAG 模型的不同方面——LLMs、向量数据库、分词策略和嵌入模型。它还允许您从不同的数据源创建基于
    RAG 的自定义网络应用程序和扩展。更多详情请访问 [https://ragna.chat/](https://ragna.chat/)。  |'
- en: '| Llama-Index  | LlamaIndex is a cloud-based orchestration framework that enables
    you to connect your data to LLMs and generate natural language responses. It can
    access various LLMs.  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Llama-Index  | LlamaIndex 是一个基于云的编排框架，它使您能够将数据连接到大型语言模型（LLMs）并生成自然语言响应。它可以访问各种
    LLMs。  |'
- en: '| Hugging Face  | Hugging Face provides a collection of pretrained models for
    various NLP tasks. It can be used with orchestration tools to manage the lifecycle
    of generative AI applications. More details can be found at [https://huggingface.co/](https://huggingface.co/).  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Hugging Face  | Hugging Face 提供了各种自然语言处理任务的预训练模型集合。它可以与编排工具一起使用，以管理生成式人工智能应用的整个生命周期。更多详情请访问
    [https://huggingface.co/](https://huggingface.co/)。  |'
- en: 10.3.3 Managing operations
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.3 管理操作
- en: An orchestrator plays a crucial role in enhancing the performance and seamless
    integration of generative AI models, such as LLMs, within intricate systems and
    workflows. Its core functionality optimizes operational efficiency and fosters
    a better user experience through sophisticated control mechanisms.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 编排器在增强复杂系统和工作流程中生成式人工智能模型（如 LLMs）的性能和无缝集成方面发挥着关键作用。其核心功能通过复杂的控制机制优化操作效率，并促进更好的用户体验。
- en: The orchestrator is crucial in managing the LLM’s integration into complex workflows,
    such as content creation pipelines. It plans and schedules the LLM’s activation
    to ensure smooth data collection, preprocessing, and text generation, thus simplifying
    the entire process from start to finish. This coordination improves the workflow
    and ensures that the API calls for the generated content are timely and relevant.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 编排器在管理 LLM 集成到复杂工作流程（如内容创建管道）方面至关重要。它规划并安排 LLM 的激活，以确保数据收集、预处理和文本生成的顺畅，从而简化从开始到结束的整个过程。这种协调提高了工作流程，并确保生成内容的
    API 调用及时且相关。
- en: The orchestrator’s main role is to balance the load and resources for the LLM’s
    services. It effectively manages requests to avoid overloading or wasting resources.
    Furthermore, it can change computational resources by constantly tracking workload
    and performance metrics. This flexibility ensures the system stays responsive
    and resources are used efficiently, even when demanding changes.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 编排器的主要作用是平衡 LLM 服务的负载和资源。它有效地管理请求，以避免过载或浪费资源。此外，它可以通过持续跟踪工作负载和性能指标来改变计算资源。这种灵活性确保系统保持响应，资源得到有效利用，即使在需求变化时也是如此。
- en: The orchestrator also supervises API interactions, enforcing rate limits and
    controlling secure access, while managing any errors or disruptions that may occur.
    Simultaneously, it handles the essential tasks of data preprocessing and postprocessing.
    This means cleaning, formatting, and transforming data to ensure it is in the
    right state for processing by the LLM and then improving the output to meet set
    quality standards and format requirements.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 协调器还监督API交互，执行速率限制并控制安全访问，同时管理可能发生的任何错误或中断。同时，它处理数据预处理和后处理的必要任务。这意味着清理、格式化和转换数据，以确保数据处于正确的状态，以便LLM进行处理，然后提高输出质量以满足既定的质量标准和格式要求。
- en: For workflows requiring sequential processing, the orchestrator ensures that
    outputs from one phase are accurately fed into the next, maintaining the process
    integrity. This is complemented by its role in enforcing security and compliance
    measures, where it filters sensitive information and ensures adherence to legal
    and ethical standards, in addition to conducting audits for accountability and
    quality assurance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要顺序处理的流程，协调器确保一个阶段的输出准确无误地输入到下一个阶段，保持流程的完整性。此外，它在执行安全和合规措施方面发挥作用，过滤敏感信息并确保遵守法律和伦理标准，此外还进行审计以确保责任和质量保证。
- en: For applications such as chatbots or digital assistants, the orchestrator manages
    user interactions by handling session states and queries, directing them to the
    LLM or other services as needed, which results in a more engaging and responsive
    user experience. Moreover, the orchestrator continuously monitors the LLM performance,
    analyzing response time, accuracy, and throughput to guide optimization efforts.
    It also manages updates to the LLM, ensuring that transitions to newer versions
    or configurations are smooth and minimally disruptive to users.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聊天机器人或数字助理等应用，协调器通过处理会话状态和查询来管理用户交互，根据需要将它们引导到LLM或其他服务，从而提供更吸引人和响应迅速的用户体验。此外，协调器持续监控LLM性能，分析响应时间、准确性和吞吐量，以指导优化工作。它还管理LLM的更新，确保向新版本或配置的过渡平滑且对用户影响最小。
- en: As we can see, an orchestrator can significantly enhance the efficiency, reliability,
    and scalability of an LLM when integrated into complex systems, providing a layer
    of management that coordinates between the LLM and other system components.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，当集成到复杂系统中时，协调器可以显著提高LLM的效率、可靠性和可扩展性，提供一层管理，协调LLM与其他系统组件之间的交互。
- en: Building your own orchestrator framework
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 构建自己的协调器框架
- en: 'Creating your own generative AI orchestrator for an enterprise can be difficult.
    However, it allows you to customize the framework according to your requirements
    and increases your understanding of the technology. This process demands extensive
    technical knowledge and resources. Unfortunately, no universal boilerplate code
    is available to develop an LLM orchestrator. Before proceeding with this project,
    consider the following factors:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为企业创建自己的生成式AI协调器可能很困难。然而，这允许您根据需求定制框架，并增加对技术的理解。这个过程需要广泛的技术知识和资源。不幸的是，没有通用的模板代码可用于开发LLM协调器。在继续此项目之前，请考虑以下因素：
- en: '*Customization*—Tailoring the framework to meet your specific application and
    performance requirements'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*定制*—根据您的特定应用和性能需求调整框架'
- en: '*Integration with existing systems*—Seamlessly integrating the orchestrator
    with your existing infrastructure and workflows'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与现有系统集成*—无缝地将协调器集成到现有的基础设施和工作流程中'
- en: '*Control and visibility*—Maintaining complete control over the LLM technology
    and accessing detailed insights into its operation'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*控制和可见性*—对LLM技术保持完全控制并获取其操作的详细洞察'
- en: '*Flexibility and scalability*—Designing the framework to be flexible enough
    to accommodate future changes and scaling to meet growing demands'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*灵活性及可扩展性*—设计框架使其足够灵活以适应未来的变化，并能够扩展以满足不断增长的需求'
- en: If you want to create something entirely new, you need to understand generative
    AI, the different types of LLMs, how to train and fine-tune them, and how to use
    them for various tasks and domains. Additionally, you should know how to gather,
    process, and store data and knowledge that can help improve the quality and diversity
    of the generated outputs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想创建全新的东西，您需要了解生成式AI、不同类型的LLM、如何训练和微调它们以及如何将它们用于各种任务和领域。此外，您还应了解如何收集、处理和存储有助于提高生成输出质量和多样性的数据和知识。
- en: To apply these concepts in real-world scenarios, you must be able to design
    and implement different generative strategies, such as prompt engineering and
    RAG. These strategies can help control the behavior and output of the LLMs. You
    must also ensure that the generative models and workflows are scalable, secure,
    and reliable. This can be achieved using cloud services, APIs, and UIs. Expertise
    in distributed systems, ML, and software engineering is also required.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这些概念应用于现实场景，你必须能够设计和实现不同的生成策略，例如提示工程和RAG。这些策略可以帮助控制LLM的行为和输出。你还必须确保生成模型和工作流程是可扩展的、安全的和可靠的。这可以通过使用云服务、API和UI来实现。分布式系统、机器学习和软件工程的专业知识也是必需的。
- en: Some new frameworks used widely nowadays are Semantic Kernel, LangChain, and
    LlamaIndex. These frameworks enable the use of GenAI models, although they address
    different aspects. We will explore these in more depth.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一些现在广泛使用的框架包括语义内核、LangChain和LlamaIndex。这些框架使GenAI模型的使用成为可能，尽管它们针对不同的方面。我们将更深入地探讨这些内容。
- en: Semantic kernel
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语义内核
- en: Semantic Kernel (SK) from Microsoft is an SDK that integrates LLMs with languages
    such as C#, Python, and Java. It simplifies the sometimes-complex process of interfacing
    LLMs with traditional C#, Python, or Java code. With SK, developers can define
    semantic functions that encapsulate specific actions their application is capable
    of, such as database interactions, API calls, or email operations. SK allows these
    functions to be orchestrated seamlessly across mixed programming language environments.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的语义内核（SK）是一个SDK，它将LLM与C#、Python和Java等语言集成。它简化了将LLM与传统的C#、Python或Java代码接口的有时复杂的流程。使用SK，开发者可以定义封装应用程序能够执行的具体操作的语义函数，例如数据库交互、API调用或电子邮件操作。SK允许这些函数在混合编程语言环境中无缝编排。
- en: The real power of SK lies in its AI-driven orchestration capabilities. Instead
    of meticulously choreographing the LLM interactions by hand, SK lets developers
    use natural language to state a desired outcome or task. The AI automatically
    determines how to combine the relevant semantic functions to achieve this goal,
    which significantly accelerates development and lowers the skill barrier for using
    LLMs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: SK的真实力量在于其AI驱动的编排能力。SK允许开发者使用自然语言来表述期望的结果或任务，而不是手动精心编排LLM的交互。AI会自动确定如何组合相关的语义功能以实现这一目标，这显著加快了开发速度并降低了使用LLM的技能门槛。
- en: SK can benefit enterprises when building LLM applications by simplifying the
    application process, reducing the cost and complexity of prompt engineering, enabling
    in-context learning and reinforcement learning, and supporting multimodality and
    multilanguage scenarios. SK provides a consistent and unified interface for different
    LLM providers, such as OpenAI, Azure OpenAI, and Hugging Face.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: SK在构建LLM应用程序时，可以通过简化应用程序流程、降低提示工程的成本和复杂性、实现情境学习和强化学习、支持多模态和多语言场景来为企业带来益处。SK为不同的LLM提供商，如OpenAI、Azure
    OpenAI和Hugging Face，提供了一个一致和统一的接口。
- en: Combining simplified LLM integration with AI-powered orchestration creates a
    powerful platform for enterprises to use to revolutionize their applications.
    Furthermore, SK makes it feasible to build highly tailored, intelligent customer
    support systems, implement more powerful and semantically nuanced search functionality,
    automate routine workflows, and potentially even aid developers with code generation
    and refactoring tasks. Additional details on SK can be found on their site at
    [https://aka.ms/semantic-kernel](https://aka.ms/semantic-kernel).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将简化版的LLM集成与AI驱动的编排相结合，为企业提供了一个强大的平台，用于革命性地改变他们的应用程序。此外，SK使得构建高度定制、智能的客户支持系统、实现更强大和语义细腻的搜索功能、自动化常规工作流程，甚至可能帮助开发者进行代码生成和重构任务成为可能。有关SK的更多详细信息，可以在他们的网站上找到：[https://aka.ms/semantic-kernel](https://aka.ms/semantic-kernel)。
- en: We can illustrate this using an example. Continuing with the pet theme from
    the previous chapters, we have some books about dogs, which range from general
    topics to more specific medical advice. These books are scanned and available
    as PDFs and contain confidential business data we want to use for a question–answer
    use case. These PDFs are complex documents that contain text, images, tables,
    and so forth. Given that we cannot use real-world internal information, these
    PDFs represent proprietary internal information for an enterprise that requires
    RAG to handle. Suppose we want to do question–answer use cases with the PDFs we
    have; let’s see how that’s possible.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个示例来说明这一点。继续使用前几章中的宠物主题，我们有一些关于狗的书，这些书从一般主题到更具体的医疗建议都有。这些书籍被扫描并作为PDF文件提供，其中包含我们想要用于问答用例的机密业务数据。这些PDF文件是复杂的文档，包含文本、图像、表格等等。鉴于我们无法使用现实世界的内部信息，这些PDF文件代表了一个企业专有的内部信息，该企业需要RAG来处理。假设我们想要使用我们拥有的PDF文件进行问答用例；让我们看看这是如何可能的。
- en: 'The first step is to use SK to install the SDK (or the package), which is not
    supported via conda and will require pip instead. Also note there are breaking
    changes with some of the SDKs, and we will want to pin the SK SDK to version 1.2.0\.
    You can install this specific version using `pip install semantic-kernel==1.2.0`.
    After installing the SDK, to get started with SK at a high level, we need to follow
    these steps:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是使用SK安装SDK（或包），这不支持通过conda进行，而需要使用pip。此外，请注意，一些SDK存在破坏性更改，我们希望将SK SDK锁定在版本1.2.0。您可以使用`pip
    install semantic-kernel==1.2.0`来安装这个特定版本。在安装SDK之后，为了在较高层次上开始使用SK，我们需要遵循以下步骤：
- en: Create an SK instance, and register the AI services you want to use, such as
    OpenAI, Azure OpenAI, or Hugging Face.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个SK实例，并注册您想要使用的AI服务，例如OpenAI、Azure OpenAI或Hugging Face。
- en: Create semantic functions that are prompts with input parameters. These functions
    can call your existing code or other semantic functions.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建带有输入参数的语义函数，这些函数可以调用您现有的代码或其他语义函数。
- en: Call the semantic functions with the appropriate arguments, and await the results.
    The results will be the output of the AI model after executing the prompt.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用适当的参数调用语义函数，并等待结果。结果将是执行提示后的AI模型的输出。
- en: Optionally, we can create a planner to orchestrate multiple semantic functions
    based on the user input.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选地，我们可以创建一个规划器，根据用户输入来协调多个语义函数。
- en: SK example
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SK示例
- en: Here is an example of implementing this using the SK. As we saw earlier, SK
    is the core component that enables the processing and understanding of natural
    language text. It’s a framework that provides a unified interface for various
    AI services and memory stores.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用SK实现此功能的示例。正如我们之前看到的，SK是使自然语言文本处理和理解成为可能的核心组件。它是一个提供统一接口的框架，用于各种AI服务和内存存储。
- en: Our example is a simple question-answering system that uses the OpenAI API to
    generate embeddings for a collection of PDF documents. Then, we use those embeddings
    to find documents relevant to a user’s query. In our example, it is used for
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例是一个简单的问答系统，它使用OpenAI API为PDF文档集合生成嵌入。然后，我们使用这些嵌入来找到与用户查询相关的文档。在我们的示例中，它用于
- en: '*Creating embeddings*—SK provides a simple interface for calling the OpenAI
    service to generate embeddings for the text extracted from PDF documents. As we
    know, these embeddings are numerical representations of the text that capture
    its semantic meaning.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创建嵌入*—SK提供了一个简单的接口，用于调用OpenAI服务，为从PDF文档中提取的文本生成嵌入。正如我们所知，这些嵌入是文本的数值表示，能够捕捉其语义意义。'
- en: '*Storing and retrieving information*—We use a vector database (Chroma in our
    example) to store the text and corresponding embeddings. SK calls these persistent
    data stores “memory” and, depending on the provider, has methods for querying
    the stored information based on semantic similarity. As we know, this is used
    to find documents relevant to a user’s query.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*存储和检索信息*—我们使用向量数据库（在我们的示例中是Chroma）来存储文本及其对应的嵌入。SK将这些持久数据存储称为“内存”，并且根据提供商，有基于语义相似性查询存储信息的方法。正如我们所知，这是用于找到与用户查询相关的文档。'
- en: '*Text completion*—We also use SK to register an OpenAI text completion service,
    which is used to generate completions for a given piece of text.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本补全*—我们还使用SK注册一个OpenAI文本补全服务，用于为给定的文本生成补全。'
- en: 'Note  We need to specifically use Chroma version 0.4.15, as at the moment,
    there is an incompatibility with version 0.4.16 and higher with SK that hasn’t
    been fixed. To do this, we can use one of the following commands depending on
    whether we are using conda or pip: `conda install chromadb=0.4.15` or `pip install
    chromadb==0.4.15`.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们需要特别使用Chroma版本0.4.15，因为目前，与SK的0.4.16及以上版本存在不兼容性，尚未修复。为此，我们可以根据是否使用conda或pip使用以下命令之一：`conda
    install chromadb=0.4.15`或`pip install chromadb==0.4.15`。
- en: Listing 10.1 shows this simple application processing a collection of PDF documents,
    extracting their text, and then using the OpenAI API to generate embeddings for
    each document. These embeddings are then stored in a vector database, which can
    be queried to find documents that are semantically similar to a given input. The
    `load_pdfs` function reads PDF files from a specified directory. It uses the PyPDF2
    library to open each PDF, extract the text from each page, and return a collection
    of those pages.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.1展示了这个简单的应用程序处理一组PDF文档，提取它们的文本，然后使用OpenAI API为每个文档生成嵌入。然后，这些嵌入被存储在向量数据库中，可以查询以找到与给定输入在语义上相似的文档。`load_pdfs`函数从指定的目录读取PDF文件。它使用PyPDF2库打开每个PDF，从每一页提取文本，并返回这些页面的集合。
- en: 'Listing 10.1 Q&A over my PDFs: Extracting text from PDFs'
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.1 Q&A过我的PDFs：从PDFs中提取文本
- en: '[PRE0]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After we have extracted the text from the pages, we use the `populate_db()`
    function to generate embeddings and store them in Chroma, a vector database. This
    function takes an SK object and goes through all the pages of the PDF. Each page
    saves the document’s text using the SK’s memory store. When the `save_information()`
    function is called, it automatically creates embedding to store in the vector
    database, as shown in the next listing. If there is already a Chroma vector database,
    we use that instead of making a new one.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们从页面中提取文本后，我们使用`populate_db()`函数生成嵌入并将它们存储在Chroma向量数据库中。此函数接受一个SK对象，遍历PDF的所有页面。每一页使用SK的内存存储保存文档的文本。当调用`save_information()`函数时，它会自动创建嵌入以存储在向量数据库中，如下一列表所示。如果已经存在Chroma向量数据库，我们将使用它而不是创建一个新的。
- en: 'Listing 10.2 Q&A over my PDFs: Using SK and populating vector database'
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.2 Q&A过我的PDFs：使用SK和填充向量数据库
- en: '[PRE1]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The program’s entry point is the `main()` function, as shown in listing 10.3\.
    It sets up the SK with the OpenAI text completion and embedding services, registers
    a memory store, and loads the vector database. Then, it enters a loop where it
    prompts the user for a question, queries the memory store for relevant documents,
    and prints the text of the most relevant document.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的入口点是`main()`函数，如列表10.3所示。它设置SK的OpenAI文本完成和嵌入服务，注册一个内存存储，并加载向量数据库。然后，它进入一个循环，提示用户提问，查询内存存储以获取相关文档，并打印最相关文档的文本。
- en: 'Listing 10.3 Q&A over my PDFs: SK using Chroma'
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.3 Q&A过我的PDFs：SK使用Chroma
- en: '[PRE2]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In our example, we use Chroma as the vector database. This is one of the many
    options available when using SK. We can get more details on the list of supported
    vector databases at [https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/](https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/).
    It is also important to note that support between C# and Python is not at parity;
    some vector databases are supported across both, but some are only supported in
    one language.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用Chroma作为向量数据库。这是在使用SK时众多可选选项之一。我们可以在[https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/](https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/)上找到支持的向量数据库列表的更多详细信息。值得注意的是，C#和Python之间的支持并不完全相同；一些向量数据库在两种语言中都得到支持，但有些只在一种语言中得到支持。
- en: The SK is the central component for processing and understanding text. It provides
    a unified interface for various AI services and memory stores, simplifying the
    process of building complex NLP applications. Now let’s switch gears and see the
    same example using LangChain.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: SK是处理和理解文本的核心组件。它为各种AI服务和内存存储提供了一个统一的接口，简化了构建复杂NLP应用的过程。现在让我们转换一下思路，看看使用LangChain的相同示例。
- en: LangChain
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LangChain
- en: LangChain offers a sophisticated framework designed to streamline the integration
    of LLMs into enterprise applications. This framework abstracts the complexities
    of interfacing with LLMs, allowing developers to incorporate advanced NLP capabilities
    without deep expertise in the field. Its library of modular components enables
    the construction of customized NLP solutions easily, facilitating a more efficient
    development process.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 提供了一个复杂的框架，旨在简化将大型语言模型（LLMs）集成到企业应用中的过程。此框架抽象化了与 LLMs 接口交互的复杂性，使得开发者能够在不深入该领域专业知识的情况下，轻松地整合高级自然语言处理（NLP）功能。其模块化组件库使得构建定制的
    NLP 解决方案变得容易，从而促进了更高效的开发过程。
- en: LangChain’s main benefit is its ability to work with different LLMs and other
    natural language AI services. This feature allows enterprises to select the best
    tools for their particular needs, avoiding the drawbacks of being tied to one
    vendor. The framework boosts efficiency by providing easier interfaces and ready-made
    components for quick deployment and supports scalability, thus enabling projects
    to expand smoothly from testing stages to full-fledged applications.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 的主要优势在于其能够与不同的 LLMs 和其他自然语言 AI 服务协同工作。这一特性允许企业根据特定需求选择最佳工具，避免了与单一供应商绑定的缺点。该框架通过提供更易于使用的接口和现成的组件来提高效率，支持可扩展性，从而使得项目能够从测试阶段顺利扩展到完整的应用程序。
- en: Additionally, LangChain helps to lower costs by minimizing the amount of specialized
    development and simplifying interactions with LLMs. Enterprises also gain from
    the strong community and support of the ecosystem around LangChain, which gives
    access to documentation, best practices, and cooperative problem-solving resources.
    This comprehensive approach makes LangChain an attractive option for businesses
    that want to use AI and natural language understanding in their services. It provides
    a way to innovate and enhance offerings through AI-driven solutions.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LangChain 通过最小化专业开发的工作量并简化与 LLMs 的交互来降低成本。企业还可以从围绕 LangChain 的强大社区和支持中受益，这提供了访问文档、最佳实践和协作解决问题的资源。这种全面的方法使得
    LangChain 成为那些希望在服务中使用人工智能和自然语言理解的企业的一个有吸引力的选择。它提供了一种通过人工智能驱动的解决方案来创新和提升产品的方式。
- en: Following the topic of pets from the previous chapters, in this chapter, we
    have a set of books related to dogs, which cover information ranging from general
    subjects to more specific medical advice. These books are PDFs and contain confidential
    business data that we want to use for a question–answer use case.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章讨论宠物主题的基础上，在本章中，我们有一套关于狗的书籍，这些书籍涵盖了从一般主题到更具体的医疗建议的信息。这些书籍是 PDF 格式，包含我们希望用于问答用例的机密商业数据。
- en: Listing 10.4 shows how this can be done easily using LangChain. In this case,
    we load all the PDFs from a local folder, read each PDF, split the context into
    2K pieces, create embeddings (using OpenAI), and create a vector index using FAISS
    (Facebook AI Similarity Search). For brevity, we don’t show the code for some
    of the helper functions, such as `load_pdfs()`, as they are the same as the previous
    SK section.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.4 展示了如何使用 LangChain 轻松实现这一功能。在这种情况下，我们从本地文件夹中加载所有 PDF 文件，读取每个 PDF，将上下文分割成
    2K 个片段，创建嵌入（使用 OpenAI），并使用 FAISS（Facebook AI Similarity Search）创建向量索引。为了简洁，我们没有展示一些辅助函数的代码，例如
    `load_pdfs()`，因为它们与之前 SK 章节中的代码相同。
- en: Note  FAISS is a library that allows fast and accurate vector search and clustering
    and can be used for various AI applications. It supports different vector comparisons
    and index types and can run on CPU and GPU. Facebook AI Research developed FAISS,
    and more details are available at [https://ai.meta.com/tools/faiss/](https://ai.meta.com/tools/faiss/).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：FAISS 是一个允许快速且精确的向量搜索和聚类的库，可用于各种人工智能应用。它支持不同的向量比较和索引类型，可以在 CPU 和 GPU 上运行。Facebook
    AI Research 开发了 FAISS，更多详情请参阅 [https://ai.meta.com/tools/faiss/](https://ai.meta.com/tools/faiss/).
- en: Listing 10.4 Q&A over my PDFs using LangChain
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.4 使用 LangChain 在我的 PDFs 上进行问答
- en: '[PRE3]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: On the one hand, LangChain is great and gives enterprises a big jumpstart for
    those just starting with LLMs and GenAI applications. LangChain simplifies the
    process by standardizing interactions with different LLM providers and offering
    tools for prompt creation, complex workflows (chains), and sophisticated AI assistants
    (agents). As an orchestrator, it can easily help us to connect LLMs to existing
    company data and systems, overcome initial hurdles, and quickly begin experimenting
    with LLM-driven applications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，LangChain 非常出色，为刚开始使用大型语言模型（LLMs）和通用人工智能（GenAI）应用的企业提供了一个巨大的起点。LangChain
    通过标准化与不同 LLM 提供商的交互，并提供提示创建、复杂工作流程（链）和高级人工智能助手（代理）的工具来简化流程。作为一个协调器，它可以帮助我们轻松地将
    LLMs 连接到现有的公司数据和系统中，克服初始障碍，并快速开始尝试 LLM 驱动的应用。
- en: However, LangChain comes with its challenges. Mastering concepts such as prompt
    design and building effective chains and agents has a learning curve. In addition,
    keeping the software and dependencies updated in this rapidly changing field can
    add some complexity. It is also essential to be aware of ethical LLM use, as powerful
    language models always carry the risk of incorrect or undesirable output. Finally,
    for production deployments where scale and performance are important, LangChain
    adds too many layers of abstractions and could end up hurting performance.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LangChain 也存在挑战。掌握提示设计、构建有效的链和代理等概念需要一定的学习曲线。此外，在这个快速变化的领域中，保持软件和依赖项的更新也会增加一些复杂性。了解道德上使用
    LLM 的重要性也很关键，因为强大的语言模型总是存在输出错误或不希望的结果的风险。最后，对于规模和性能至关重要的生产部署，LangChain 添加了过多的抽象层，可能会损害性能。
- en: LlamaIndex
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LlamaIndex
- en: LlamaIndex is a data framework that enables LLMs to access and process private
    data sources that are not part of their pretraining corpus. This enhances their
    NLP capabilities and domain-specific knowledge for various use cases, such as
    document Q&A, data-augmented chatbots, and structured analytics. LlamaIndex provides
    data ingestion, indexing, query interface, vector store, and database integration
    tools.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: LlamaIndex 是一个数据框架，它使大型语言模型（LLMs）能够访问和处理其预训练语料库之外的私有数据源。这增强了它们在自然语言处理（NLP）能力和特定领域知识方面的能力，适用于各种用例，如文档问答、数据增强聊天机器人和结构化分析。LlamaIndex
    提供数据摄取、索引、查询接口、向量存储和数据库集成工具。
- en: One of the main challenges of using LLMs for generative AI applications is the
    integration of different data formats (APIs, PDFs, documents, SQL, etc.) and LLM
    providers (OpenAI, Hugging Face, etc.). LlamaIndex simplifies this process by
    providing a unified interface and modular design, allowing users to easily connect
    their custom data sources to their preferred LLMs. LlamaIndex also supports data
    augmentation, which is the process of generating synthetic from existing data
    to improve the performance and robustness of LLMs
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLMs 进行生成式人工智能应用的主要挑战之一是整合不同的数据格式（API、PDF、文档、SQL 等）和 LLM 提供商（OpenAI、Hugging
    Face 等）。LlamaIndex 通过提供统一的接口和模块化设计来简化这一过程，使用户能够轻松地将他们的自定义数据源连接到他们首选的 LLMs。LlamaIndex
    还支持数据增强，这是一种从现有数据生成合成数据的过程，以提高 LLMs 的性能和鲁棒性。
- en: Another challenge of using LLMs for generative AI applications is efficient
    retrieval and scalability of data. LlamaIndex uses vector store and database providers
    to store and index data and optimize query speed and memory usage. LlamaIndex
    also supports various query types, such as natural language, keyword, and vector
    queries, to enable users to access their data conveniently and effectively.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLMs 进行生成式人工智能应用的一个挑战是数据的有效检索和可扩展性。LlamaIndex 通过使用向量存储和数据库提供商来存储和索引数据，并优化查询速度和内存使用来解决这个问题。LlamaIndex
    还支持各种查询类型，如自然语言、关键词和向量查询，使用户能够方便有效地访问他们的数据。
- en: 'Listing 10.5 shows the simplicity of using LlamaIndex to implement a RAG question-and-answer
    use case using the same pet-related books. We employ a built-in function that
    loads and processes all the PDFs from storage (saved in our example’s `data/dog_books`
    folder) and creates a built-in vector index using the OpenAI embeddings. We save
    this locally to save time and can reuse it in the next instance. For us to use
    LlamaIndex, we do need to install a couple of packages—`llama-index` and `llama-index-reader-files`
    as shown: `pip` `install` `llama-index==0.10.9` `llama-index-readers-file`.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.5 An example showing RAG with LlamaIndex
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Loads environment variables'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Checks whether storage already exists'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Loads only PDFs'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Loads the PDF documents and creates the index'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Saves the index for later use'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Loads an existing index'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.4 Prompt management
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier in the book, we learned that prompt engineering plays a crucial role
    in communicating with LLMs, as it directly affects the output quality. A well-constructed
    prompt can help the LLM to generate accurate and contextually relevant responses.
    For this, you need to have a profound understanding of how LLMs interpret input
    and the ability to create prompts that the model can comprehend effectively.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt management involves designing, testing, and deploying prompts or instructions
    for LLMs to perform various tasks. Prompts also need to work together with the
    evaluations and content moderation as part of the response filtering tier and
    RAI implementations. We will cover this aspect in more detail later in the book.
    As a part of the orchestration layer, prompt management provides a comprehensive
    approach to managing LLMs. This involves three essential components: prompt engineering,
    optimization, and PromptOps.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering encompasses the creation of custom, adaptive, and domain-specific
    prompts tailored to the user’s needs and the context of their queries. This involves
    generating custom prompts for specific tasks, such as summarizing news articles
    by understanding the context and requirements and adapting prompts in real-time
    based on user interactions to better align with their intent. Additionally, it
    includes developing prompts that cater to specialized fields, using the appropriate
    technical language and adhering to field-specific standards.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Prompt optimization focuses on improving the prompts’ effectiveness through
    continuous performance monitoring, data-driven refinements, and efficient resource
    management. This entails tracking metrics such as accuracy and relevance to gauge
    the prompts’ success, refining prompts based on user feedback and response quality
    to enhance clarity, and optimizing prompts to stay within token limits and reduce
    complexity, thus ensuring cost-efficiency and prompt–response generation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: PromptOps involves the operational aspects of managing prompts, including automated
    testing for prompt effectiveness, version control for managing different prompt
    versions and enabling easy rollbacks, integration with other AI system components
    to ensure seamless operation, and scalability and maintenance considerations to
    ensure the prompt management system can handle growing demands and is easy to
    update. This comprehensive approach to prompt management ensures that the AI system
    remains effective, efficient, and adaptable to user needs and technological advancements.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: PromptOps涉及管理提示的操作方面，包括对提示有效性的自动化测试、不同提示版本的版本控制以实现轻松回滚、与其他AI系统组件的集成以确保无缝运行，以及可扩展性和维护性考虑，以确保提示管理系统可以处理不断增长的需求并易于更新。这种全面的提示管理方法确保AI系统保持有效、高效，并能适应用户需求和科技进步。
- en: Prompt management (i.e., creating and optimizing prompts for LLMs) can benefit
    from various tools and frameworks being developed constantly. For enterprises
    that want to use LLMs and prompt management tools, assessing the technical features
    and the vendor’s adherence to security, privacy, and compliance with relevant
    regulations (e.g., GDPR, HIPAA) is important. Moreover, enterprises should consider
    the level of support, customization, and ability to integrate with existing systems
    and workflows. Many of these providers offer custom solutions and partnerships
    for businesses, ensuring that using LLMs matches enterprise needs and strategic
    objectives. Prompt flow ([https://github.com/microsoft/promptflow](https://github.com/microsoft/promptflow)),
    a Microsoft OSS tool for prompt management, is one example. We will cover Prompt
    flow in more detail in the book’s next chapter.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 提示管理（即为LLMs创建和优化提示）可以从不断发展的各种工具和框架中受益。对于希望使用LLMs和提示管理工具的企业来说，评估技术特性和供应商对安全、隐私以及遵守相关法规（例如，GDPR、HIPAA）的遵守情况很重要。此外，企业应考虑支持水平、定制能力和与现有系统和工作流程的集成能力。许多这些提供商为商业提供定制解决方案和合作伙伴关系，确保使用LLMs符合企业需求和战略目标。Prompt
    flow ([https://github.com/microsoft/promptflow](https://github.com/microsoft/promptflow))，这是一个微软的开源工具，用于提示管理，就是一个例子。我们将在本书的下一章中更详细地介绍Prompt
    flow。
- en: 'Another example is Pezzo ([https://github.com/pezzolabs/pezzo](https://github.com/pezzolabs/pezzo)),
    which can help with prompt management. LangChain and SK, which we saw earlier,
    also have some support for prompt management. For more details, see “Prompting
    Frameworks for Large Language Models: A Survey” [4].'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是Pezzo ([https://github.com/pezzolabs/pezzo](https://github.com/pezzolabs/pezzo))，它可以帮助进行提示管理。我们之前看到的LangChain和SK也提供了一些提示管理的支持。更多详情，请参阅“大型语言模型提示框架综述”
    [4]。
- en: Prompt management is an important process in ensuring the effectiveness of LLM
    applications. It is a dynamic and iterative process that involves designing, testing,
    refining, and customizing prompts for optimal outputs. The architecture of the
    LLM system must be flexible enough to accommodate current and future advances
    in prompt design. It should also provide tools for continuous improvement mechanisms
    to generate high-quality outputs.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 提示管理是确保LLM应用有效性的重要过程。它是一个动态和迭代的流程，涉及设计、测试、改进和定制提示以实现最佳输出。LLM系统的架构必须足够灵活，以适应当前和未来在提示设计方面的进步。它还应提供工具以实现持续改进机制，以生成高质量的输出。
- en: 10.4 Grounding layer
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 地基层
- en: The grounding layer is the foundation of GenAI applications that handle data
    acquisition, storage, processing, and delivery. It integrates various data sources
    and formats with connectors, pipelines, plugins, and APIs. In addition, it performs
    data preprocessing, embedding, and vectorization to make the data compatible with
    LLMs. It employs distributed data processing frameworks for scalability and reliability.
    Let’s explore this in a little more detail.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 地基层是处理数据采集、存储、处理和交付的GenAI应用的基础。它通过连接器、管道、插件和API整合了各种数据源和格式。此外，它执行数据预处理、嵌入和向量化，使数据与LLMs兼容。它采用分布式数据处理框架以实现可扩展性和可靠性。让我们更详细地探讨这一点。
- en: 10.4.1 Data integration and preprocessing
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 数据集成和预处理
- en: Having reliable data pipelines to combine data from different systems as seamlessly
    as possible is important. These pipelines must be designed to handle various data
    types and sources—from structured SQL database entries to unstructured text, image
    files, and real-time streaming data from IoT deployments. The architecture of
    these pipelines must be compatible with various data formats and protocols, which
    may require the development of custom APIs, middleware for data transformation,
    and scalable ETL (extract, transform, load) processes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有可靠的数据管道，尽可能无缝地结合来自不同系统的数据非常重要。这些管道必须设计为处理各种数据类型和来源——从结构化的SQL数据库条目到非结构化文本、图像文件以及来自物联网部署的实时流数据。这些管道的架构必须与各种数据格式和协议兼容，这可能需要开发定制的API、数据转换的中间件和可扩展的ETL（提取、转换、加载）过程。
- en: Integration
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集成
- en: Integrating a system of record is fundamental to the generative AI application
    architecture. It involves multiple layers of interaction and data management in
    a secure, compliant, and efficient manner, which ensures that real-time data is
    available for the LLM, while maintaining quality. In addition, the integration
    must be scalable and adaptable to changes in the enterprise data ecosystem.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 将系统记录集成到生成式AI应用架构中是基本的。它涉及在安全、合规和高效的方式下进行多层交互和数据管理，确保实时数据对LLM可用，同时保持数据质量。此外，集成必须是可扩展的，并能够适应企业数据生态系统的变化。
- en: The main goal for integration pipelines is to integrate them into various systems
    of records (SoRs) and enable access for the data from those systems to be used
    efficiently by GenAI models. Integrating with SoR is crucial in designing generative
    AI applications. These systems include SaaS platforms, customer relationship management
    (CRM), and enterprise resource planning (ERP) systems. They serve as the data
    backbone for the LLM applications, acting as repositories for the enterprise’s
    structured and unstructured data. This data is essential for using the LLMs as
    a reasoning engine, allowing it to access high-quality, domain-specific information.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 集成管道的主要目标是将其集成到各种系统记录（SoRs）中，并使来自这些系统的数据能够被生成AI模型高效地使用。与SoR集成对于设计生成式AI应用至关重要。这些系统包括SaaS平台、客户关系管理（CRM）和企业资源规划（ERP）系统。它们作为LLM应用的数据库，作为企业结构化和非结构化数据的存储库。这些数据对于将LLM用作推理引擎至关重要，允许它访问高质量、特定领域的知识。
- en: This information retrieved via the SoR integration is used for RAG implementation.
    As we saw earlier in the book, it is one of the main ways enterprises can operate
    on their proprietary information. SoR integrations are the key to achieving that.
    The main challenge is not just the integration but also understanding the nature
    of the data, the frequency of change, and the computational cost.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 通过SoR集成检索到的信息用于RAG实现。正如我们在本书前面所看到的，这是企业在其专有信息上操作的主要方式之一。SoR集成是实现这一目标的关键。主要挑战不仅在于集成，还包括理解数据的本质、变化的频率以及计算成本。
- en: Several tools are available to initiate this process, such as Microsoft Fabric,
    which offers over 145 connectors, Apache NiFi, Informatica, and so forth. These
    tools gather and consolidate data from different sources into a single repository
    that can handle various data formats and prevent data loss during data capturing.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种工具可以启动此过程，例如Microsoft Fabric，它提供超过145个连接器，Apache NiFi、Informatica等等。这些工具从不同的来源收集和整合数据到一个单一的存储库中，该存储库可以处理各种数据格式，并在数据捕获过程中防止数据丢失。
- en: Modern storage solutions such as Amazon S3, Azure Data Lake Storage, or the
    Hadoop Distributed File System (HDFS) offer secure and scalable storage options
    for large amounts of data. When combined with data warehousing technologies such
    as Snowflake, Google BigQuery, or Amazon Redshift, businesses can efficiently
    store, query, and manage their data, making it easier to prepare for AI integration.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现代存储解决方案，如Amazon S3、Azure Data Lake Storage或Hadoop分布式文件系统（HDFS），为大量数据提供安全且可扩展的存储选项。当与Snowflake、Google
    BigQuery或Amazon Redshift等数据仓库技术结合使用时，企业可以高效地存储、查询和管理数据，从而更容易为人工智能集成做准备。
- en: Data orchestration tools, such as Apache Airflow, Data Factory in Microsoft
    Fabric, and AWS Glues, offer modern, code-centric methods for constructing and
    executing complex data workflows. These systems allow developers to define data
    pipelines through code, facilitating version control and testing similar to standard
    software development practices. Additionally, they provide scheduling, monitoring,
    and error management features that contribute to the reliability of data pipelines.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 数据编排工具，如Apache Airflow、Microsoft Fabric中的Data Factory和AWS Glues，提供了构建和执行复杂数据工作流的现代、以代码为中心的方法。这些系统允许开发人员通过代码定义数据管道，促进版本控制和测试，类似于标准软件开发实践。此外，它们还提供调度、监控和错误管理功能，有助于提高数据管道的可靠性。
- en: Preprocessing
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预处理
- en: Once data has been prepared for AI use, it can be sent to processing engines
    or analytical platforms for further preparation. Apache Spark is a well-known
    platform that can handle large-scale data processing and has several libraries
    covering various computing requirements. Platforms such as Databricks have built
    upon Spark’s capabilities to ease the journey from data preparation to model deployment.
    In addition, architectures must include event-driven mechanisms such as webhooks
    or streaming services to ensure data synchronization and real-time updates.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据为AI使用做好准备，就可以发送到处理引擎或分析平台进行进一步准备。Apache Spark是一个广为人知的平台，能够处理大规模数据处理，并拥有覆盖各种计算需求的多个库。例如，Databricks这样的平台就是基于Spark的能力构建的，以简化从数据准备到模型部署的过程。此外，架构必须包括事件驱动的机制，如webhooks或流服务，以确保数据同步和实时更新。
- en: For data to be useful in informing LLM outcomes, it must first undergo a rigorous
    cleansing and standardization process to ensure its quality. The architectural
    blueprint should include these preprocessing activities, such as deduplication,
    normalization, and error rectification. Integrated data quality tools should automate
    these tasks, providing LLMs with superior datasets.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据在为LLM结果提供信息时有用，它必须首先经过严格的清洗和标准化过程，以确保其质量。架构蓝图应包括这些预处理活动，如去重、归一化和错误纠正。集成数据质量工具应自动化这些任务，为LLMs提供优质的数据集。
- en: Data handling requires strict access controls for proper security and compliance,
    which is vital when working with sensitive information and following regulations.
    Data interaction needs strong authentication and authorization protocols. Data
    governance frameworks should specify access rights; furthermore, encryption should
    protect data at rest and in motion. Frequent compliance assessments are crucial
    for ensuring data quality and privacy. Following GDPR, HIPAA, or CCPA regulations
    is also important for ethical and lawful processing of personal data.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理需要严格的访问控制，以确保安全和合规，这在处理敏感信息和遵循法规时至关重要。数据交互需要强大的身份验证和授权协议。数据治理框架应指定访问权限；此外，加密应保护静态和动态中的数据。频繁的合规性评估对于确保数据质量和隐私至关重要。遵循GDPR、HIPAA或CCPA法规对于个人数据的道德和法律处理也很重要。
- en: A plugin enabling the integration into source systems is not a one-time static
    component of the architecture—it changes and adapts constantly. As businesses
    use or improve their new SoRs, the architecture must be built to allow simple
    integration or movement of data sources. For this, a flexible approach to integration
    is required, where new data sources can be connected with little change to the
    current system.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 允许集成到源系统的插件不是架构中的一次性静态组件——它不断变化和适应。随着企业使用或改进其新的SoR，架构必须构建以允许简单集成或数据源移动。为此，需要一种灵活的集成方法，其中新数据源可以与现有系统进行少量更改连接。
- en: The architecture should be designed to support different data formats and protocols.
    This ensures that data flows seamlessly from various systems to the LLM. To achieve
    this, custom APIs may need to be developed, middleware may have to be used for
    data transformation, and ETL processes capable of handling large volumes of data
    may have to be implemented.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 架构应设计为支持不同的数据格式和协议。这确保了数据能够无缝地从各种系统流向LLM。为此，可能需要开发定制API，可能需要使用中间件进行数据转换，以及实施能够处理大量数据的ETL流程。
- en: The data pipeline infrastructure for generative AI is complex and requires careful
    planning to handle the intricacies of enterprise-grade data landscapes. These
    will build on existing ETL and data warehousing investments but must factor in
    the new data types of embeddings. By strategically using a combination of tools
    for data ingestion, processing, storage, orchestration, and ML, enterprises can
    build powerful pipelines that provide their generative AI applications with a
    consistent flow of quality data.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2 Embeddings and vector management
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In earlier chapters of the book, we discussed the crucial role of model embeddings
    and representations. This is the stage where the complexity of language is distilled
    into machine-interpretable formats, specifically mathematical vectors. Text is
    transformed by embedding techniques and advanced feature extraction forms that
    result in a vector space representation of text. These vectors are not arbitrary;
    they encapsulate the semantic essence of words, phrases, or entire documents,
    mapping information into a compressed, information-rich, lower-dimensional space.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Codex is a prime example of this process. It can comprehend and generate
    human-readable code, making it a powerful tool for embedding programming and natural
    languages. This is a significant advantage for code generation and automation
    tasks. In contrast, Hugging Face provides an extensive suite of pretrained models
    that are finely tuned for diverse languages and tasks. They can adeptly handle
    embeddings ranging from brief sentences to intricate documents.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: These models distinguish themselves by their ability to grasp contextual word
    relationships beyond basic dictionary meanings. By considering the words in their
    vicinity, the generated embeddings provide a nuanced reflection of the word usage
    and connotations within specific contexts. This feature is essential for generative
    AI applications that aim to emulate human-like text production. It fosters outcomes
    that are not only coherent and context-aware but also semantically profound.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in earlier chapters on RAG, various libraries are available for chunking
    data, and some offer auto-chunking capabilities. One such library, called Unstructured
    ([https://github.com/Unstructured-IO/unstructured](https://github.com/Unstructured-IO/unstructured)),
    provides open source libraries and APIs that can create customized preprocessing
    pipelines for labeling, training, or production ML pipelines. The library includes
    modular functions and connectors that form a cohesive system, which makes it easy
    to ingest, preprocess, and adapt data to different platforms. It is also efficient
    at transforming unstructured data into structured outputs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: An alternative solution is using LangChain and SK, which we saw earlier. These
    libraries support common chunking techniques for fixed size, variable size, or
    a combination of both. In addition, you can specify an overlap percentage to duplicate
    a small amount of content in each chunk, which helps preserve context.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: After transforming vectors, it is crucial to manage them properly. Vector databases
    specially designed to store indexes and retrieve high-dimensional vector data
    are available. Some such databases include Redis, Azure Cosmos DB, Pinecone, and
    Weaviate, to name a few. These databases help with quick searches within large
    embedding spaces, making it easy to identify similar vectors instantly. For instance,
    a generative AI system can use a vector database to match a user’s query with
    the most semantically related questions and answers and achieve this in a fraction
    of a second.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases feature sophisticated indexing algorithms engineered to deftly
    traverse high-dimensional terrains without falling prey to the “curse of dimensionality”
    [5]. This attribute renders them exceptionally valuable for applications such
    as recommendation engines, semantic search platforms, and personalized content
    curation, where pinpointing relevant content quickly is critical.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases offer more than just speed; they also provide accuracy and
    relevance. Combining these databases allows AI models to respond quickly and precisely
    to user inquiries based on their learned context. Proper index management is crucial,
    including tasks such as index creation, update triggers, refresh rates, complex
    data types, and operational factors (e.g., index size, schema design, and underlying
    compute services). Cloud-based solutions such as Azure AI Search and Pinecone
    can efficiently manage these demands in a production environment.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of transforming textual data into a format that AI can handle has
    two stages: embedding and vector database management. This conversion is essential
    for generative AI’s intelligence, enabling it to understand and engage with the
    world meaningfully and in a scalable manner. Therefore, carefully choosing embedding
    techniques and vector databases is a technical necessity and a key factor in the
    success of generative AI applications. When choosing LLMs, related vector storage
    and retrieval engines, and embedding models, enterprises must consider the data
    size, origin, change rate, and scalability needs.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Model layer
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model layer is the foundation of AI cognitive capabilities. It involves
    a set of models, including foundational LLMs that provide general intelligence,
    fine-tuned LLMs specialized for specific tasks or domains, model catalogs hosting
    and managing access to various models, and SLMs that offer lightweight, agile
    alternatives for certain applications.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The significance of this layer lies in its design, as it forms the core processing
    units of the GenAI app stack. It allows a scalable and flexible approach to AI
    deployment and can efficiently address various tasks by differentiating between
    foundational, fine-tuned, and small models. This ensures that the architecture
    can cater to diverse use cases, optimize resource allocation, and maintain high
    performance across different scenarios.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Small language models
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: SLMs such as Phi-3 and Orca 2 are designed to offer advanced language processing
    capabilities with fewer parameters than larger models. Both models are part of
    a broader initiative to make powerful language processing tools more accessible
    and efficient, enabling more extensive research and application possibilities.
    They represent a significant step in the evolution of AI language models, balancing
    capability with computational efficiency.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Phi-3, Phi-2, and Orca 2 are smaller-scale language models developed by Microsoft,
    offering advanced language processing with fewer parameters. Phi-3, which is a
    successor to Phi-2, is a family of models in various sizes (mini, 3.8B; small,
    7B; medium, 14B parameters). Phi-2, with 2.7 billion parameters, is efficient
    and matches larger models in performance, while Orca 2, available in 7- and 13-billion-parameter
    versions, excels in reasoning tasks and can outperform much larger models. Both
    are designed for accessibility and computational efficiency, enabling broader
    research and application in AI language processing.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.1 Model ensemble architecture
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative AI employs a model ensemble, which combines multiple ML models to
    enhance performance and reliability. This approach takes advantage of the individual
    strengths of each model, minimizing their weaknesses. For example, one model may
    be great at generating technical content, while another may be better at creative
    storytelling. By assembling these models, an application can better cater to a
    wider range of user requests with greater accuracy. To create an effective model
    ensemble for generative AI, the architecture should include
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '*Model selection*—Criteria for choosing which models to include in the ensemble,
    often based on their performance, the diversity of training data, or their area
    of specialization.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Routing logic*—Routing logic is the mechanism for determining which model
    to use for a given input or how to combine outputs from multiple models.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*API integration*—APIs are the main conduits through which applications interact
    with LLMs. API integration becomes complex when dealing with an ensemble of models
    as interactions with multiple endpoints must be managed. The architecture should
    consider API integration of throttling and rate limits, error handling, and caching
    responses.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalability and redundancy*—Scalable design accommodates growing user bases
    and spikes in demand. Load balancing and the use of API gateways can help distribute
    traffic effectively. Redundancy is equally critical; thus, having multiple regions
    for model deployments ensures the application remains functional.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Queuing and stream processing*—Queuing and stream processing handle asynchronous
    tasks and manage workloads; message queues and stream processing services can
    be utilized, which ensures that the system is not overwhelmed during peak times
    and that tasks are processed in an orderly way.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 10.5 is an example of implementing Phi-2 as a classifier. We use Phi-2,
    which runs locally and fast, to identify the user’s intent when asking a question.
    Continuing with the topic of pets and dogs, we asked Phi-2 the intent of the question
    and whether it had anything to do with dogs. If it was irrelevant to the current
    topic (i.e., dogs), we asked GPT-4 to answer.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F05_Bahree.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 Classifier using multiple models
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 10.6 shows an example of implementing a simple classifier using a lightweight
    model and then, based on the question’s intent, figuring out which model to call.
    Here, we use Phi-2, a research SML from Microsoft, as a classifier to determine
    whether a question is related to dogs. The Phi-2 model is a transformer-based
    model, trained to understand and generate human-like text. It is used here as
    a first-pass filter to determine the question’s intent.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'The function `check_dog_question()` takes a question as input and constructs
    a prompt to ask the Phi-2 model whether there’s anything about dogs in the question.
    If Phi-2 determines that the question is about dogs, the function returns `True`.
    This could trigger a more expensive GPT-4 model to generate a more detailed response.
    If the question is not about dogs, the function returns `False`, and the more
    expensive model would not have to be used. We need to ensure that the following
    packages are installed before running this code: `pip` `install` `transformers==4.42.4`
    `torch= =2.3.1`.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 Using Phi-2 as an intent classifier
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The approach employs a small model, such as Phi-2, with much less capability
    for more efficient use of resources, as the more expensive GPT-4 model is used
    only when necessary. This approach can just as easily be expanded to use more
    than one model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: This toy example could be better if we used a more powerful LLM, such as a smaller
    GPT-3 model. Figure 10.6 shows another example of using a fine-tuned GPT-3 as
    a classifier to help understand the user’s goal. This is for an enterprise chatbot
    that can answer questions on both structured and unstructured data. It can answer
    questions about Microsoft’s surface devices based on the user’s persona. There
    is fictitious sales information in a SQL database that a salesperson can chat
    with, and there is unstructured data that can answer technical support questions.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F06_Bahree.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 Enterprise Q&A bot—High-level overview
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The bot uses a RAG pattern and can answer questions using information from both
    structured and unstructured systems based on the user’s intention. The structured
    data has sales information (with fake data), and the unstructured data is a crawl
    of different forums and official sites related to Surface devices. Listing 10.7
    presents a high-level view of the architecture.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: The orchestrator uses GPT-3 to implement the intent classifier and can help
    select the best path based on the bot’s question. Then, suitable knowledge sources
    are applied. This complicated workflow shows much of what an orchestrator would
    do in a real-world enterprise situation. The sales data is stored in a SQL database,
    and GPT is also used to build the SQL query against the schema to run, depending
    on the user’s query. What is very interesting is that the LLM is invoked multiple
    times in the flow, first to understand the intent of the question, and then, depending
    on the path, GPT also creates the SQL query to execute. Its results are passed
    to the prompt formulation to invoke the LLM again to create the response for the
    user. This mainly shows that along the flow, we can invoke the right model based
    on the point in time and for what it is needed, factoring in the model capability
    and associated computational constraints and costs.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 Using a fine-tuned GPT-3 model as a classifier
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In addition to the classifier, we must use the appropriate prompts to convey
    our purpose and obtain the desired behavior. The sample prompts that match the
    classifier are displayed in the following listing.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 Classifier meta-prompt
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The link to the full code listing can be found in the book’s GitHub repository
    ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)). It is a fork from one
    of Microsoft’s published samples, found at [https://bit.ly/AOAISearchDemo](https://bit.ly/AOAISearchDemo).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.2 Model serving
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many modern AI applications are hosted on cloud platforms due to their scalability
    and the wide range of services they offer. Integrating with major cloud providers
    such as Microsoft Azure, Amazon Web Services, or Google Cloud Platform enables
    developers to use a secure global network of data centers, ML managed services,
    and tools for application monitoring and management. Therefore, many enterprises
    use one of the LLMs hosted in the cloud, which is exposed via an API. This means
    that the cloud providers that manage the model serve to scale up or down model
    inference. If some models are hosted on-premise, the layer must address model
    operations working with LLMOps.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: The model layer architecture should provide a strategic framework for using
    multiple LLMs to create a robust, versatile, and scalable application. This involves
    careful planning around model selection and API management, while ensuring security
    and compliance in data handling. The architecture should be flexible enough to
    adapt to new models and APIs as they become available.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 10.6 Response filtering
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most cases, an application should not share the raw generation from the model
    with the end user; it should go through a processing step to help manage and filter
    any sensitive details—this is where the processing layer helps, and a key responsibility
    of this layer is to manage the LLM output.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The response filtering layer is tasked with quality assurance and content moderation,
    crucial for maintaining trust in GenAI applications. It involves using classifiers
    and NLP tools to screen the outputs for accuracy, bias, and appropriateness.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, LLM output can vary significantly, ranging from simple text
    to complex data structures. Managing these outputs requires a systematic approach
    so they meet the application’s standards and are presented to the user in a useful
    format. These postprocessing steps include a few areas, as shown in figure 10.7.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F07_Bahree.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 Response filtering stages
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Content moderation relies on RAI practices to mitigate the potential risks of
    generative AI models, such as biased, offensive, or misleading content, cyber,
    privacy, legal, performance, and intellectual property risks. We need to adopt
    RAI practices to use the power of generative AI. RAI is essential for the output
    processing layer to address both application- and enterprise-level risks, such
    as regulatory and compliance requirements. In addition, RAI can enhance other
    aspects, such as privacy, explainability, and fairness.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools and frameworks to start with. For example, Microsoft’s
    InterpretML ([https://interpret.ml/](https://interpret.ml/)) and Fairlearn ([https://fairlearn.org/](https://fairlearn.org/))
    are open source toolkits that help developers explain and improve the fairness
    of ML models. IBM’s AI Fairness 360 is another open source toolkit that helps
    detect and reduce bias in ML models. We’ll examine RAI in more depth later in
    the book.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Output and postprocessing are crucial for ensuring the usability and safety
    of content generated by LLMs. The architecture should provide a robust framework
    for refining and managing outputs, including formatting, content classification,
    validation, and caching. Quality assurance, both automated and user driven, must
    be an integral part of the process to maintain high standards and improve over
    time.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: This chapter shows how GenAI can be integrated into enterprise applications
    using the new GenAI app stack and associated application architecture. We have
    also discussed the role of the Center of Excellence in facilitating this integration
    and addressing the technical, cultural, and ethical challenges involved. However,
    building an AI solution is only the first step; deploying it for production and
    scale requires different skills and tools. The next chapter will explore what
    it takes to operationalize generative AI solutions and ensure their reliability,
    performance, and security. We will also look at some of best practices and frameworks
    for managing the AI lifecycle and delivering value to the end-users and stakeholders.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Copilot demonstrates how generative AI architecture can build enterprise applications
    and solutions. It uses a different application stack that works with copilots
    to create the new enterprise architecture stack. This stack is for GenAI apps,
    which use Copilot as a counterpart to the LAMP stack.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GenAI app stack includes four layers that cooperate to make the application
    stack function—the model, orchestration, grounding, and response filtering layers.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The orchestration layer is one of the critical and foundational components of
    the GenAI stack. It handles and organizes different processes, AI services, and
    platforms to enable a dependable and coherent experience.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The area of orchestration frameworks is new and evolving, with many changes
    and innovations taking place. Some of the frameworks that are more widely used
    today are SK, LangChain, and LlamaIndex.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using plugins than can handle the intricacies of the source systems, their
    protocols, and other details, the grounding layer facilitates data integrations
    and preprocessing for RAG deployments in the enterprises. It also oversees the
    embeddings and the related vector databases.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model layer offers a platform for using multiple models from various sources—from
    managed and fine-tuned models to BYOM (bring-your-own-model) for enterprises.
    These models can all be accessed through strong APIs that guarantee compliance
    and security.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The response filtering layer ensures quality and moderates content, essential
    for building confidence in GenAI applications. Furthermore, it involves using
    classifiers and NLP tools to check the outputs for correctness, fairness, and
    suitability.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AI Center of Excellence can help enterprises comprehensively integrate LLMs
    and GenAI into their applications. By addressing technical, cultural, and ethical
    challenges, enterprises can use AI to enhance innovation and competitiveness,
    ensuring lasting success in an increasingly AI-powered world.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
