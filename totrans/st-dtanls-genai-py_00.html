<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">1</span> </span> <span class="chapter-title-text">Introduction to the use of generative AI in data analytics</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Introducing key limitations of generative AI models</li> 
    <li class="readable-text" id="p3">The role of generative AI in data analytics</li> 
    <li class="readable-text" id="p4">Getting started using LLMs to support data analytics</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>As the dust over the generative AI hype begins to settle and the notes of disappointment mix in with the chorus of praises, it may be a good time to ask yourself a question: “If LLMs aren’t the silver bullet to all world problems, what are they really good for?” Our experience using these amazing tools to improve various processes gave us the answer. They are really good, and we mean <em>really</em> <em>good </em>in supporting improvements for different processes. Throughout this book, we will guide you through our methods for utilizing the enormous potential hidden in the matrices of generative AI to improve your analytics skills without falling victim to the risks inherent in this technology. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p6"> 
   <p><span class="print-book-callout-head">Under the hood</span>  To excel in these goals, you will preferably have in the back of your mind what drives the responses you get to your prompts. However, due to the architecture-agnostic nature of this book and the rapidly changing technology landscape, we have consciously avoided the technological nitty-gritty, focusing instead on process implementation. We encourage you, though, to get a good overview of what’s what. You can learn it from several Manning books, such as <em>The Complete Obsolete Guide to Generative AI</em> by David Clinton or <em>Introduction to Generative AI</em> by Numa Dhamani and Maggie Engler. For the technical details of GPT models, see <em>How GPT Works</em> by Drew Farris, Edward Raff, and Stella Biderman. </p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>In this chapter, you will learn about three important aspects of working with generative AI. As we strongly believe that first things should indeed be first, we’ll start by presenting the inherent limitations of generative AI. We already mentioned the misalignment between the expectations and results of generative AI applications. A good understanding of the unavoidable limitations is critical to avoid disappointments at your work. The second aspect is related to embedding generative AI into the data analytics process. This part of the chapter will help you develop your first intuition about when and how to use generative AI when trying to solve an analytical problem. We will also manage expectations when it comes to automating processes involving generative AI. The last part of the chapter will provide you with knowledge about methods of accessing generative AIs. In the lion’s share of cases, browser-based access to chat would be sufficient, but history teaches us that this may not be an advisable method when working with sensitive data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>The overall goal of this chapter is not to give you encyclopedic knowledge of this technology, but to ensure you have a deep enough understanding to demystify generative AI and allow for a more critical interpretation of its abilities.</p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_6"><span class="num-string">1.1</span> Inherent limitations of generative AI models</h2> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>In the Middle Ages, map edges had the inscription <em>Hic sunt dracones</em> (Latin for “Here be dragons”). You also may find monsters depicted in areas of uncharted waters. Later, the dragons, sirens, and krakens were replaced with depictions of reefs, shoals, and ice fields. We would like you to consider our warnings as the latter, rather than the former. For any endeavor, knowing the dangers to be wary of is at least as important as knowing what benefits to hope for. </p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>The following list outlines the inherent limitations of any generative AI system. Some of them may be reduced or even removed in the future, but reading about a potentially obsolete limitation will cost you less than being unaware of even one that remains valid. So, here are the treacherous waters we should be aware of:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p12"> <em>Generative AIs will always provide an answer (even if it’s a wrong one)</em><em> </em>—Like a child who finished their “why?” phase, or an overpromoted manager with heavy impostor syndrome, generative AIs, when asked a question, know everything and, as their first reaction, are unable to admit the limits of their knowledge. One of the main reasons you really want to read this book is because generative AIs can be convinced otherwise. In chapter 8, we’ll also discuss the model’s sensitivity to input phrasing. Slightly rephrasing a question may result in different answers with varying degrees of quality and relevance. It’s worth noting that, since prompts are usually supplied in natural language, this sensitivity is slightly different than for search engines. The latter only react to clusters of keywords, whereas generative AI may provide a different response based not just on the keywords, but also on the grammatical structure of the prompt, its perceived emotional tone of writing, and the context created by recent exchanges. </li> 
   <li class="readable-text" id="p13"> <em>Some of the answers might be entirely made up</em><em> </em>—There are instances where generative AI provides an answer that appears plausible but is not based on facts or directly linked to the training material. This is because the model sometimes fills in gaps in its knowledge by generating content that aligns with the patterns observed in the training data, even if the information is not accurate or complete. We will cover this problem in chapter 7, when we discuss the phenomenon of AI hallucinations. </li> 
   <li class="readable-text" id="p14"> <em>Inherent sycophancy</em><em> </em>—The larger the model at the base of generative AI, the more likely it is to exercise agreeability over reliability and accuracy. If confronted or questioned about the provided answer, it’s likely to apologize and present the point of view contradicting its previous statement, even if it was correct the first time—truth be damned! Generative models can even make up numbers and falsify references to support the user’s perspective! </li> 
   <li class="readable-text" id="p15"> <em>Inaccurate or outdated information</em><em> </em>—Each generative AI’s knowledge is derived from the content corpus it was trained on. The model may provide outdated or incorrect information depending on the knowledge cutoff. This will be visible in examples in this book, where the model gives answers using API calls or programming language structures from obsolete versions. This is not as severe a limitation as it might initially seem. First, the majority of the concepts covered don’t evolve that quickly, and most people will have a lot of ground to cover in the basics before they reach the need to tap the latest developments. Second, many generative AIs have access to the internet. However, mixing the time-constrained body of knowledge used to train the model with continuous updates may lead to inaccurate results. It’s also worth remembering that some generative AIs only check the internet for the latest information when directly prompted. </li> 
   <li class="readable-text" id="p16"> <em>Input and output limits</em><em> </em>—When using generative AIs, you should be aware that the amount of text they can process (the amount of input they can read as a whole and, on this basis, generate the output) is limited, although it can vary greatly between different models and implementations. The unit of text processed by generative AI is called a <em>token</em>, and it can be a word, part of a word, or a punctuation mark, depending on the tokenization method employed. Tokenization algorithms translate text into tokens with an average of 4 tokens per 3 words, or 0.75 words per token (this value should be relatively stable). At the time of writing this book, models have context windows ranging from several thousand up to millions of tokens, and the race to truly limitless context is full-on. For now, however, the available tools offer a limited number of input (prompt) and output (response) tokens, and you should remember that the context window covers both. You will get no warning if some data falls out of the window (no pun intended) and gets forgotten. Such truncation will usually manifest itself by the model giving responses inconsistent with previous exchanges, showing it has forgotten previous prompts or responses. In section 1.3, we present methods for estimating the number of tokens used. As a way of dealing with this limitation, you may frequently summarize the conversation and its key findings to ensure they are not left out of the context window. </li> 
   <li class="readable-text" id="p17"> <em>Verbosity</em><em> </em>—When you try some prompts, it will quickly become apparent that generative AIs generate overly verbose responses or overuse certain phrases (e.g., they tend to “unwaveringly delve into vast landscapes of the rich tapestry of intricacies of…” anything they encounter). This verbosity can be attributed to biases or patterns in the training data, where longer responses, or responses of a particular structure, might be more common. </li> 
   <li class="readable-text" id="p18"> <em>Biased or inappropriate content</em><em> </em>—Despite efforts to reduce harmful and biased content, generative AIs, especially those fine-tuned on unknown data, may still generate responses that exhibit biases or produce content that could be considered inappropriate. This can result from some biases still being present in the training data, biases that are hidden or purposefully included in the prompts, or a multitude of other overlapping factors. However, the developers of most generative AIs have gone to great lengths to balance the model’s responses. An example can be found in the GPT-4 System Card document (https://cdn.openai.com/papers/gpt-4-system-card.pdf). </li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p19"> 
   <p><span class="print-book-callout-head">He said, she said</span>  You can help generative AI provide correct, or at least useful, answers by providing underlying LLMs with the ability to search additional data sources and require linking answers to sources. You can learn more about that form from <em>Generative AI in Action</em> by Amit Bahree or <em>AI-Powered Search</em> by Trey Grainger, Doug Turnbull, and Max Irwin, both available from Manning Publications.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p20"> 
   <p><span class="print-book-callout-head">A word to the wise</span>  The prompt/response size limit and the verbosity can often lead to incomplete or cut-off responses. When designing a conversation with a generative AI, one option is to ensure that the combined length of the prompt and expected response doesn’t exceed the token limit.</p> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>Awareness of these limitations is crucial when you are interacting with generative AIs or incorporating them into various applications. Continued research and development aim to address these limitations and improve the performance and safety of generative AIs.</p> 
  </div> 
  <div class="readable-text" id="p22"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_7"><span class="num-string">1.2</span> The role of generative AIs in data analytics</h2> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>On groups and forums focused on generative AIs, there are dozens of questions to the effect of “Where can I find a GenAI-based tool that does [very specific task description here]?” Even if a requested tool does not exist yet, it probably will, and soon. And it’s all good. Data warehouses, lakes, lakehouses, meshes, fabrics, and so on replace Excel files, data in emails, and napkins (if not for all intended purposes). Dashboards and self-serve business intelligence (BI) platforms replace manually created reports and PowerPoint presentations (OK, questions about using generative AI to create, modify, or improve PowerPoint slides are the most common). However, beware of silver bullets. Only 0.5% of data collected in data warehouses, lakes, and so on is ever analyzed, while the remaining 99.5% generates costs and big-data hangover to companies that over-eagerly started to collect the data without a data utilization plan. BI platforms, in turn, are filled to the brim with rogue analytics (for example, data slicing and dicing that serves no purpose other than the justification of poor business decisions).</p> 
  </div> 
  <div class="readable-text intended-text" id="p24"> 
   <p>The effectiveness of generative AI use in data analytics will depend on your, the data analyst’s, ability to harness the possibilities and overcome the limitations of this new tool. Generative AIs, like any tool or technology, cannot be expected to do all the work. Let’s look closely at what we are dealing with and at generative AI’s differences from and similarities to other elements of the data analytics flow, namely the analytical process and software.</p> 
  </div> 
  <div class="readable-text intended-text" id="p25"> 
   <p>Until now, we have leaned toward the doom-and-gloom side of things, giving you a lot of warnings about generative AI’s limitations and discouraging you from jumping onto every tool labeled as “GenAI-powered” (we have an internal betting pool regarding when the first toothbrush labeled as such will hit the market). We did this on purpose as we noticed that overinflated expectations are the main blocker to the efficient use of this amazing tool. Now let’s get out of this shadow of doubt and step into the light of the bright generative AI-supported future of data analytics.</p> 
  </div> 
  <div class="readable-text" id="p26"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_8"><span class="num-string">1.2.1</span> Generative AI in the data analytics flow</h3> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>Years and years of working with data has convinced us that its value does not come from the complexity of the utilized technologies. We’ve seen millions of dollars saved with a simple breakdown of costs done by process rather than by organizational unit. We’ve seen millions of dollars lost because an overcomplicated market analysis involving dozens of tools and teams poorly reflected actual customer sentiments. <em>Data analysis is not about transforming raw data into charts. It’s about supporting business decisions using conclusions from relevant business data. </em>Your success in this endeavor will depend on a couple of aspects, of which the available tooling is just one. </p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>Different types of data and different business questions require different analytic pipelines. If you work in a retail company, you most likely seek insights into customer behavior. Your pipeline may begin with the collection and cleaning of data from multiple sources: transaction records, customer feedback, and website interactions. Once the data is scrubbed and standardized, you will process it through algorithms performing customer segmentation, product affinity analysis, and sales forecasting. Working with a healthcare provider, your input data would include patient electronic health records, medical imaging, and sensor data from wearable devices. Your processing would employ algorithms for disease diagnosis, treatment optimization, and patient outcome prediction. If you find yourself in a manufacturing firm, you’ll be integrating data from IoT sensors on the factory floor, quality control checks, and supply chain logistics, with the analytics focused on anomaly detection, predictive maintenance, and supply/demand forecasting. </p> 
  </div> 
  <div class="readable-text intended-text" id="p29"> 
   <p>Irrespective of what you’re analyzing and for whom, the essence of the process remains the same: collect and clean input data, process it using more or less advanced algorithms, and finally, present the results to the desired audience. </p> 
  </div> 
  <div class="readable-text intended-text" id="p30"> 
   <p>The details differ greatly depending on the business area, data sources, analytical methods applied, and expected output format. Each of these topics warrants a book (or six) on how to most effectively perform each of these steps, taking into account both cost and time, using this or that technology stack.</p> 
  </div> 
  <div class="readable-text intended-text" id="p31"> 
   <p>This book does not aim to answer all possible questions about all the possible scenarios you may encounter in your work as a data analyst. We offer you something much, much better. We propose a structured method to effectively use generative AIs’ unbelievable knowledge repository (from Wikipedia to scientific papers, to books and literature, to dialogue data, to the Pile (<a href="https://pile.eleuther.ai/">https://pile.eleuther.ai/</a>), and so on) to prepare an analytical pipeline tailor-made to solve exactly your problem.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p32"> 
   <p><span class="print-book-callout-head">The missing link</span>  The scale of generative AIs’ abilities is only starting to be explored. However, it’s already clear that they can be taught to respond consistently and relevantly on a wide range of topics. They have the ability to drill down into details, summarize, explain, and associate related concepts to an extraordinary degree. These abilities can be used to effectively unblock your own thinking and get you out of your rut. You no longer have to trawl through dozens of random articles trying to find inspiration or pointers. Just ask a question. Even if the answer is imperfect, it may point you to concepts you haven’t thought of before. Use this to expand your horizons.</p> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>In the preface, we identified a question you should ask yourself every time you encounter a new analytical problem: <em>where do I star</em><em>t</em>? Searching for the right input data might not be the worst possible choice, especially if it’s accompanied by an analysis of what input data is actually relevant. Let’s assume you work in a healthcare unit and get a question like “What are the average patient waiting times on Tuesdays?”, or you work in retail and are asked to analyze, “How do our customers use loyalty cards?” Do not be fooled by the simplicity of the former question. They can, in fact, both be tricky. </p> 
  </div> 
  <div class="readable-text intended-text" id="p34"> 
   <p>The flow presented in figure 1.1 will help guide you through the crucial steps in going from the question to decision-enabling conclusions, focusing on getting the most added value from both the human and generative AI along the way, while avoiding the common pitfalls. It can be applied to any analytical task and technology stack you may have. All the examples you’ll encounter in this book will follow this general structure.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p35">  
   <img alt="figure" src="../Images/CH01_F01_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 1.1</span> Recommended generative AI–supported data analysis flow<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p36"> 
   <p>You always need to start with a problem statement. Let’s look at our healthcare example. We were asked a question about waiting times on Tuesdays; however, issues around waiting times on a specific day are more likely to be the symptom of a deeper problem. The question you get asked will not often translate directly into an effective problem statement. We should ask ourselves, “What real problem are we looking at here?” While it’s the job of the final decision maker to define the scope of the requested analysis, the guided questions can get you into a much better starting position for your analysis and, ultimately, provide more value from the analyzed data. </p> 
  </div> 
  <div class="readable-text intended-text" id="p37"> 
   <p>Now, the fun part. Even if the request comes from an area of operations you don’t have experience with, generative AI can help you put the received request into a business context without bothering your stakeholders with unnecessary inquiries! Let’s try a couple of generative AI on our Tuesdays-specific question.</p> 
  </div> 
  <div class="readable-text prompt" id="p38"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>I work in a healthcare unit. I’ve been asked to answer a question: “What is the average waiting time for patients on Tuesdays?” What do you think could be some actual reasons behind such a question?</p> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>The answers are too long to include here, but both OpenAI’s ChatGPTs (3.5 and 4), Google Gemini and Gemini Pro, and Meta’s Llama 2 (13B) provided lists of possible project types where such an analysis could be of importance. The answers generally involve planning and budgeting (including resource allocation and staffing optimization), patient experience and quality of care, operational efficiency, and staff training.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>Depending on your knowledge of current projects in your environment, you may come back with more or less specific questions, which shouldn’t be considered a waste of time. For example, if you ask a stakeholder asking for an analysis a follow-up question like, “Is it connected to our latest focus on increasing patient satisfaction?”, you could get an answer like, “Oh yeah, we’ve often heard complaints about Tuesdays, and we want to try to do something about it.” Once you understand that patient complaints and waiting times are the root issues behind the original question about waiting times on Tuesdays, your final problem statement may look something like this: “What is the distribution of waiting times and its correlation with patient satisfaction?” This question balances specificity with scope, aiming to uncover actionable insights that can directly influence patient care protocols and satisfaction.</p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p>The level of detail you disclose about your project in conversations with your generative AI advisor should be tailored to the confidentiality requirements of your analysis and the specifics of your generative AI setup. It’s advisable to share more freely within a locally managed software environment than on public platforms. This topic, including the associated risks of employing generative AI, is explored further in chapter 8.</p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>You can then try querying generative AI about the best ways to answer the final question.</p> 
  </div> 
  <div class="readable-text prompt" id="p43"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>And how could I approach analysis leading to answering the question: “What is the distribution of waiting times and its correlation with patient satisfaction?”</p> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>Yet again, the answers are quite lengthy and detailed. They offer you seven- to nine-step approaches, which, with additional iterative inquiries, should allow you to construct a robust pipeline. Chapter 3 provides a practical example of using generative AI to develop a detailed analysis design from an extremely vague request (unfortunately, likely to be encountered in the real world). Here, our aim is to show you that the value of generative AIs extends beyond just answering highly specific queries. Their utility isn’t really reliant on the craft of “prompt engineering.” Instead, it depends on your readiness to present the full scope of your problem and to recognize that (as in the case of any meaningful conversation with a well-informed colleague) you’re unlikely to receive a flawless answer on the first attempt. Instead, expect to engage in an iterative process, refining broad concepts to meet your particular requirements.</p> 
  </div> 
  <div class="readable-text intended-text" id="p45"> 
   <p>Inquiring into details of the analysis or code and discussing the received results are common for all the steps of your analytical process on all levels of granularity. You should use them as elements of the flow of the whole project and when going through detailed substeps, such as when cleaning the data or formatting the final charts. Generative AI can help you clarify what you want to achieve at any given moment, determine how to effectively get there, and test if what you got is what you wanted.</p> 
  </div> 
  <div class="readable-text intended-text" id="p46"> 
   <p>As we mentioned, generative AI will not replace analytical tools but help you optimize their use. Let’s have a look at the areas where they shine the brightest.</p> 
  </div> 
  <div class="readable-text" id="p47"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_9"><span class="num-string">1.2.2</span> The complementarity of language models and other data analytics tools</h3> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>On their own, generative AIs are particularly well-suited for tasks involving text data, such as sentiment analysis, text classification, summarization, and question-answering. However, their potential extends beyond text-based tasks. Multimodal AIs like Google’s Gemini or OpenAI’s GPT-4 allow you to upload different types of files, with the latter accepting raw data in formats such as CSV. But as we mentioned before, your success as a data analyst will depend on your ability to utilize generative AIs in a wider analytical environment. Luckily, that’s precisely where you can get excellent support from generative AI. It’s like having an expert on a speed dial!</p> 
  </div> 
  <div class="readable-text intended-text" id="p49"> 
   <p>First, all generative AIs worth their mettle have deep knowledge of most analytical frameworks available on the market. They can help you navigate through a vast array of technologies to extract, process, analyze, and visualize data. Suppose you have data in a bunch of Excel files and are tasked with creating the dashboard in Power BI. Try dropping the following question into the generative AI of your choice:</p> 
  </div> 
  <div class="readable-text prompt" id="p50"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>I have data in a bunch of Excel files and am tasked with creating the dashboard in Power BI. What shall I do? </p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>You will get detailed instructions, including where to click to upload your data, basic options for modeling it, and, again, where to click to prepare a dashboard. And that’s just the first step of the iteration.</p> 
  </div> 
  <div class="readable-text intended-text" id="p52"> 
   <p>Did your company just move from WordPress plugins to Google Analytics, but Google Tag Manager–based event tracking is needed for yesterday?</p> 
  </div> 
  <div class="readable-text prompt" id="p53"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>I’m tasked with enhancing our website’s performance and need to leverage Google Tag Manager (GTM) for tracking various user interactions. Additionally, I must provide detailed reports in Google Analytics 4 (GA4). I’m completely new to the Google environment. Can you please help?</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>You’ll get a good list of options, and you will be able to choose what’s right for your specific case. Again, no “prompt engineering” is needed. Just a good, old cry for help.</p> 
  </div> 
  <div class="readable-text intended-text" id="p55"> 
   <p>Second, if you’re more invested in serious data analytics, language models can generate code in various programming languages, such as Python, R, Scala, or even, for the more adventurous, PHP, Perl, or even Cobol or Intercal. The best-known example of this concept implementation is the GitHub Copilot, with software like Bito, Tabnine, Codeium, and FauxPilot (and many more) following in its footsteps. This capability allows you to obtain ready-to-use code for data processing, analysis, and visualization tasks, saving time and effort. The generated code can vary in size and complexity, from short snippets and single functions serving as a starting point for customizing and refining analyses to whole algorithm implementations and modules, limited only by your imagination and patience to coax the model to spit it out. Unlike raw code snippets downloaded off the internet, generative AI will provide code suited exactly to your needs, and it has the invaluable ability to explain the code, as we’ll see in several examples in this book, and optimize it to your specifications. </p> 
  </div> 
  <div class="readable-text intended-text" id="p56"> 
   <p>This ability to generate and explain code will be the most helpful feature for us throughout the book, but it also comes with the biggest warning, which we’ll repeat in many places and cover in depth in chapter 7. Specifically, never <em>trust</em> the model to spit out entirely correct answers or perfectly working code on the first try. The higher the importance or risk of your project, the more scrupulously you should verify any output through review and testing. In subsequent chapters, you’ll find examples of model-generated code that doesn’t work as expected or that has incorrect explanations attached to it. Caveat emptor!</p> 
  </div> 
  <div class="readable-text intended-text" id="p57"> 
   <p>Finally, once the analysis is performed, language models can help interpret the results by generating natural language summaries and explanations. This feature may help you understand the intricacies of complex analytical results and communicate your findings to a broader audience.</p> 
  </div> 
  <div class="readable-text intended-text" id="p58"> 
   <p>There is an old story of a math professor complaining to his colleague about students: <em>I explained it to them three times, I finally understood it myself, and they still had questions</em>. With generative AI, you can allow yourself to be such a student, shamelessly pestering your advisor with questions until you’re comfortable with the answer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>If you wish to practice and feel audacious, start by pasting the following prompt into multiple generative AIs of your choice.</p> 
  </div> 
  <div class="readable-text prompt" id="p60"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>I got a confidence interval of 0.55-0.9 using the Wilson Score Interval (95% confidence level). How should I precisely communicate what it means for the performed test?</p> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>Statistics should be a very important part of your toolbox. Generative AI can help you avoid situations where stakeholders will evaluate your analyses on this scale: small lies, big lies, and statistics. </p> 
  </div> 
  <div class="readable-text intended-text" id="p62"> 
   <p>After reading the previous two sections, you should now feel excited about adding generative AI to your data analytics practice. We share that feeling every time we accomplish our tasks in a third or a quarter of the time they used to take. But, as always with generative AI, you need to be aware of some limitations.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_10"><span class="num-string">1.2.3</span> Limits of generative AIs’ ability to automate and streamline data analytics processes</h3> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>While you can successfully employ generative AIs in all the applications listed previously and in the subsequent sections of this book (and more), their effectiveness in automating and streamlining data analytics processes has certain limitations. You can incorporate them into the data analytics domain, but their limitations make them an amazing supplement, not a replacement, for a data analyst.</p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Lack of quantitative analysis skills</h4> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>Generative AIs excel at understanding and generating natural language, but they lack the inherent ability to perform complex quantitative analysis (aka math). ChatGPT has already included an add-on running Python code on the fly (utilizing an external environment, not as a native LLM capability), and other generative AIs will probably follow suit, but the success rate of the generated analytical runs, as you’ll see in the following sections, is not something we’d wage the success of our business on. Data analytics processes often require mathematical and statistical methods, such as regression analysis, time-series forecasting, and clustering techniques. While generative AIs can suggest such methods and often offer the relevant code, the code must be thoroughly tested before it is put into the production environment. </p> 
  </div> 
  <div class="readable-text" id="p67"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Limited understanding of domain-specific concepts</h4> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>While generative AIs can generate human-like text based on the context provided, their training data may not include highly specialized domain knowledge. Consequently, their ability to accurately generate insights or recommendations in the context of specific industries or niche subjects may be limited. This problem is enhanced by the already mentioned inherent inability of generative AIs to admit their ignorance. Knowledge limitations differ between generative AI providers and can be somewhat mitigated by providing GenAI with internet access, but you really don’t want to base critical business decisions on an enthusiastic hallucination! </p> 
  </div> 
  <div class="readable-text intended-text" id="p69"> 
   <p>In the case of such specific needs, your best option is to provide the model with more general prompts and refine the answer based on your specialist knowledge.</p> 
  </div> 
  <div class="readable-text" id="p70"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Inability to interact with databases and APIs</h4> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>Your work in data analytics will, more often than not, involve working with databases, APIs, or other data sources to extract, clean, and process data. Generative AIs lack the built-in capability to interact directly with these sources. While it is possible to integrate generative AIs with custom-built solutions to bridge this gap, doing so can be resource-intensive and challenging to implement effectively. As in the previous cases, the model can still be effectively used to guide your analysis and provide solutions or even whole swathes of code, which you can execute independently of the model.</p> 
  </div> 
  <div class="readable-text" id="p72"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Unreliable internet access</h4> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>Some generative AIs, such as Google’s Gemini, are “internet native.” The internet connectivity is a natural part of their operations. For others, not so much. Self-hosted models, which you can download and run on your machine, such as Llama 2, need access to a search engine API and an implementation of so-called multistage reasoning, where in the first answer, the LLM decides what it needs to do to get the proper answer, and in the following steps, the architecture runs the internet search and provides the answer to the LLM, which on that basis can form the final response. ChatGPT 4 implements this in its web-based interface. </p> 
  </div> 
  <div class="readable-text intended-text" id="p74"> 
   <p>It seems complicated, and it is. The connectivity of self-hosted LLMs depends on external APIs and either a lot of code or fast-changing libraries. ChatGPT, in turn, sometimes forgets it can connect to the internet. When it was trained, it couldn’t, and this memory still lingers in its network’s deep layers. If your analysis depends on access to the latest data or news, you need to choose your tools very carefully. </p> 
  </div> 
  <div class="readable-text intended-text" id="p75"> 
   <p>In the following chapters of this book, we will show you how to apply this general-to-specific problem-solving path for the best results.</p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_11"><span class="num-string">1.3</span> Getting started with generative AIs for data analytics</h2> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>There is an old Chinese proverb, “In the forest of algorithms, the path to wisdom has many branches.” Actually, there isn’t—ChatGPT generated it for us. We have tried to convey that, depending on the situation, you have more than one way to access your AI advisor. To utilize generative AI’s potential, you need to get comfortable with conversing with it, as most of the tools built upon it strip its answers of relevant nuance, but you should know your options here. </p> 
  </div> 
  <div class="readable-text" id="p78"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_12"><span class="num-string">1.3.1</span> Web interface</h3> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>In this book, we will mainly use ChatGPT (<a href="https://chat.openai.com">https://chat.openai.com</a>) and Gemini<span class=" link-like"/>(<a href="https://gemini.google.com">https://gemini.google.com</a>) as examples of generative AI. These are readily accessible (it’s more true than you’d like, as this access can be bidirectional—do not paste your confidential data there!), and the underlying language models are in constant, rapid development. There is also a chance that your company will have a self-hosted generative AI based on either of these or some other model, such as the <em>n</em>-th incarnation of Llama or Mixtral, but hopefully it will be accompanied by a proper web interface. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p80"> 
   <p><span class="print-book-callout-head">And the winner is…</span>  <em>At the time of writing this book</em>, GPT 4 was by far the most useful of all the tested generative AIs for any analytics-related tasks. However, this <em>will </em>change (just not with GPT 4o). Keep your eyes open and don’t be shy when it comes to engaging new generative AIs and testing their usefulness. Each model has its own training dataset and an architecture influencing its interpretation of your prompts and the resulting answer. Just remember that the technicalities behind them are irrelevant from your perspective (unless they are related to cost-effectiveness, of course). What interests you, as a data analyst, is the model’s ability to support your process.</p> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>If you need specific models, you can search for one of half a million hosted on the AI-development website, www.huggingface.co. Some of them require downloading, but some can be run there. You can register for free, but extensive model use will require creating your own “Space” and purchasing processing power. </p> 
  </div> 
  <div class="readable-text intended-text" id="p82"> 
   <p>The use of generative AI via web interface is as simple as writing a query and reading the answer. Sometimes, there will also be a button allowing you to upload files to be analyzed. </p> 
  </div> 
  <div class="readable-text" id="p83"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_13"><span class="num-string">1.3.2</span> Beware of tokens</h3> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>If you plan to connect directly to a model, you must understand the critical difference between your and GenAIs’ perceptions of text. As explained earlier, generative AIs break down input text into manageable units known as <em>tokens</em>. This foundational step is critical, as it transforms raw text into a structured form that an AI model can efficiently process and learn from.</p> 
  </div> 
  <div class="readable-text intended-text" id="p85"> 
   <p>Tokenization involves dissecting the input text into a sequence of tokens. This process is not a mere splitting by whitespace; it’s more nuanced, incorporating an understanding of the language’s syntax and semantics. For instance, the word “don’t” may be tokenized into “do” and “n’t” to better capture its meaning and structure. Advanced models leverage subword tokenization schemes to balance the tradeoff between representing common words as single tokens and decomposing less common words into smaller, meaningful components. This approach enables the model to handle a vast vocabulary, including neologisms, with a fixed set of tokens.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p86"> 
   <p><span class="print-book-callout-head">Tokens of Babel</span>  The method of tokenization—how the words are split before processing—is specific for each model. This is a critical point, as using tokenization meant for one model as input for another may case the latter model to interpret the input as a meaningless, or worse, misleading, soup of semi-words misaligned with how the model’s training material has been prepared.</p> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>As we warned you in section 1.1, when the input surpasses the context window, the earliest tokens are truncated, leaving the model with only the most recent tokens within its comprehension horizon. This truncation can lead to a loss of crucial context or information necessary for generating coherent and relevant responses. Have you ever been in a situation where someone overheard just the last couple sentences of a lengthy conversation and offered unsolicited advice? Such a response rarely adds to the conversation. Exceeding the context puts generative AI in a similar position. It’s “deaf” to parts of the conversation exceeding its context window. This is particularly dangerous if you work with a large piece of code!</p> 
  </div> 
  <div class="readable-text intended-text" id="p88"> 
   <p>Several strategies can be employed to navigate the constraints of limited context windows. One common method is chunking the input text into smaller segments that fit within the model’s context window, ensuring that each segment contains enough context to stand on its own for generation tasks. Another approach involves using techniques like sliding windows or iterative refinement, where the model progressively processes text, maintaining as much relevant context as possible across segments. For more complex interactions involving longer texts or conversations, strategies like creating a summary of previous interactions or leveraging external memory mechanisms can help maintain coherence.</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_14"><span class="num-string">1.3.3</span> Accessing and using the API</h3> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>For more advanced use cases and seamless integration with your existing data analysis tools, you can access most of the popular models via their application programming interface (API). These APIs suit various programming languages, including Python, JavaScript, and more. With API access, you can create custom applications, integrate generative AIs into your existing data analysis workflows, and even build GenAI-powered analytics dashboards.</p> 
  </div> 
  <div class="readable-text intended-text" id="p91"> 
   <p>To further simplify the process of integrating generative AIs into your data analysis projects, you can use available SDKs (software development kits) and libraries created by model or third-party developers, one of the most prominent being LangChain (<a href="http://www.langchain.com">www.langchain.com</a>). These resources can save you time and effort when it comes to working with the API, as they provide prebuilt functions and classes that handle common tasks. You can find popular SDKs and libraries for various programming languages on platforms like GitHub. Make sure to check the compatibility and support status before using them in your projects.</p> 
  </div> 
  <div class="readable-text" id="p92"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">An example of programmatic access to ChatGPT</h4> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>You can access different ChatGPT versions through the OpenAI API. This method allows you to programmatically send requests and receive responses, giving you greater control over the AI’s capabilities. Sign up for an API key on the OpenAI website to get started (<a href="https://platform.openai.com/signup">https://platform.openai.com/signup</a>). Then, follow the API documentation to learn how to interact with ChatGPT using your preferred programming language.</p> 
  </div> 
  <div class="readable-text intended-text" id="p94"> 
   <p>To illustrate programmatic access, let’s look at an example of accessing the ChatGPT models from Python. Similar code will be used in some of the discussions in chapter 5 on using ChatGPT directly for data analysis. If you haven’t yet set up the OpenAI API, follow the instructions to install the library and set up an API key at the OpenAI signup page. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p95"> 
   <p><span class="print-book-callout-head">A rose by any other name</span> . . . Throughout this book, we’ll work with Python (in a Jupyter environment or any Unix environment). In chapter 7, we’ll show that generative AIs are also capable of supporting many other programming environments.</p> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>Once you have the OpenAI API, it is recommended that you assign it to the <code>OPENAI_API_KEY</code> variable in the environment, either through your shell setup (depending on your system) or, preferably, through a .env file in your project. You can then use the following simple Python code to interact with ChatGPT.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p97"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 1.1</span> Interacting with ChatGPT through the API</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from openai import OpenAI
prompt_text = """Generate a list of 20 keywords indicating positive 
<span class="">↪</span>sentiment to be used for searching customer reviews in 
<span class="">↪</span>Portuguese."""
client = OpenAI(
    api_key=&lt;&lt;your-API-key-here&gt;&gt;,
)
chat_completion = client.chat.completions.create(
    messages=[
    {"role": "system", "content": "You are a scientific assistant, skilled
    <span class="">↪</span>at explaining science to schoolchildren."},
    {"role": "user", "content": "Explain resonance to first-graders?"},
    {"role": "assistant", "content": "Alright, kids, today we're going to
    <span class="">↪</span>talk about something really cool called resonance! (...)."},
    <em># Shortened from the full model response</em>
    {"role": "user", "content": "Where can we use it?"}
  ],


    model="gpt-4-0125-preview",
    temperature=0.7,   
)
print(chat_completion.choices[0].message.content)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>The preceding code sets up an example exchange to be further completed by the model. It exemplifies how the model can handle multiturn exchanges (for example, multiple question-answer iterations) with context. The main input is the <code>messages</code> array of message objects, where each object consists of two components:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p99"> A <code>role</code>, either <code>system</code>, <code>user</code>, or <code>assistant</code> </li> 
   <li class="readable-text" id="p100"> The content </li> 
  </ul> 
  <div class="readable-text" id="p101"> 
   <p>A system-role message is typically included first, followed by alternating user and assistant messages. The system-role message sets up the background for the behavior of the assistant. The example in listing 1.1 modifies the assistant’s personality to reflect the style of responses we want it to take. This can be omitted, and without any specific tone or audience requirement, the model will reply in its usual helpful but flat language.</p> 
  </div> 
  <div class="readable-text intended-text" id="p102"> 
   <p>The rest of the messages should consist of alternating user and assistant content, providing the model with the exchange context. Initially, you can provide one user message to which the model should respond. In subsequent exchanges, you can build up the message array with the history of prompts and responses to provide the model with a context of the exchange so far, allowing the model to relate better to subsequent prompts. By default, the models have no memory of past requests, and all the relevant information must be supplied as part of the message array in each request.</p> 
  </div> 
  <div class="readable-text intended-text" id="p103"> 
   <p>The <code>model</code> parameter specifies which instance of LLM we want to access. It’s best to refer to the OpenAI models website (<a href="https://platform.openai.com/docs/models/">https://platform.openai.com/docs/models/</a>) for the latest list of available models, as it changes quite frequently. To get started, a good choice would be to experiment with the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p104"> gpt-4—The latest production version of the GPT 4 model </li> 
   <li class="readable-text" id="p105"> gpt-4o—A younger, faster, if less thoughtful brother of GPT 4 </li> 
   <li class="readable-text" id="p106"> gpt-3.5-turbo—Still a very good choice, and it can be more cost-effective in some cases </li> 
   <li class="readable-text" id="p107"> dall-e-3—Optimized for image generation </li> 
   <li class="readable-text" id="p108"> tts-1—Designed to generate natural-sounding speech from text </li> 
   <li class="readable-text" id="p109"> whisper-1—Can recognize speech and transcribe it as text </li> 
  </ul> 
  <div class="readable-text" id="p110"> 
   <p>New models are being developed continuously; the preceding list illustrates the breadth of capabilities already available. Obviously, for the models using images or sound as either input or output, more advanced programming techniques will be required to interact with them.</p> 
  </div> 
  <div class="readable-text intended-text" id="p111"> 
   <p>Listing 1.1 used only one of the plethora of other optional parameters: <code>temperature</code>. As you can guess, this controls randomness in the model responses. Increasing <code>temperature</code> can generate interesting results, but it has a high risk of causing the model to hallucinate. Experimenting with this is very interesting, but please use it cautiously in production environments. We’ll touch more on hallucinations and related risks in chapter 8.</p> 
  </div> 
  <div class="readable-text intended-text" id="p112"> 
   <p>Refer to the latest OpenAI documentation for other up-to-date information and additional parameters.</p> 
  </div> 
  <div class="readable-text" id="p113"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Programmatic access to other OpenAI API-compatible models</h4> 
  </div> 
  <div class="readable-text" id="p114"> 
   <p>Many models are available in the market (over a million models in HuggingFace alone), each with its own interface. However, a lot of them are compatible with OpenAI API de facto standards, such as Meta’s Llama models. </p> 
  </div> 
  <div class="readable-text intended-text" id="p115"> 
   <p>To access these models, you can just replace the OpenAI API call code in the previous example.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p116"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 1.2</span> Code to direct the client to a specific model, e.g., Llama</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">client = OpenAI(
api_key = "&lt;your_llamaapi_token&gt;",
base_url = "https://api.llama-api.com"
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p117"> 
   <p>The base URL parameter specifies the server hosting the model. Obviously, you’ll need to provide a specific API key, such as the Llama API token referenced here, as each provider will require their own user authentication. </p> 
  </div> 
  <div class="readable-text intended-text" id="p118"> 
   <p>Additionally, as you probably noticed, we used the “gpt-4-0125-preview” model in listing 1.1. If you switch to Llama, you’ll need to provide a valid model, such as “llama-13b-chat” or one of the other available Llama variants. The rest of the code can remain unchanged most of the time.</p> 
  </div> 
  <div class="readable-text" id="p119"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Example of programmatic access to Google Vertex AI</h4> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>A slightly different API worth mentioning is the one defined by Google to access its powerful Gemini AI models, as well as Codey, which is optimized for code generation and completion, and Imagen, designed for image generation, editing, captioning, and visual question answering. Given the power of Google in the market, this API may also be a good contender for a standard in the future.</p> 
  </div> 
  <div class="readable-text intended-text" id="p121"> 
   <p>The quickest way to access these models is through the Cloud Shell (<a href="https://cloud.google.com/shell/docs/launching-cloud-shell">https://cloud.google.com/shell/docs/launching-cloud-shell</a>), which is a terminal, or command line, used to access cloud services. Once you activate the shell, you need to install the API.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p122"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 1.3</span> Command to install the Google AI package on Google Cloud</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install "google-cloud-aiplatform&gt;=1.38"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p123"> 
   <p>Then you can use the following script to generate completions from the selected model.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p124"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 1.4</span> Example code to call Vertex AI on Google Cloud</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import vertexai
from vertexai.generative_models import GenerativeModel, ChatSession, 
<span class="">↪</span>HarmCategory, HarmBlockThreshold
<em># Replace PROJECT_ID with the ID of your Google Cloud project.</em>
my_id = "PROJECT_ID"
<em># Replace with your Google Cloud location</em>
my_location = "us-central1"
def generate_text(project_id: str, location: str, prompt: str) -&gt; str:
<em>    # Initialize Vertex AI</em>
    vertexai.init(project=project_id, location=location)
<em>    # Load the model</em>
    model = GenerativeModel("gemini-1.0-pro")
<em>    # Generation config</em>
    config = {"max_output_tokens": 2048, "temperature": 0.4, "top_p": 1,
    <span class="">↪</span>"top_k": 32}
<em>    # Safety config</em>
    safety_config = {
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT:
        <span class="">↪</span>HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
        HarmCategory.HARM_CATEGORY_HARASSMENT:
        <span class="">↪</span>HarmBlockThreshold.BLOCK_LOW_AND_ABOVE
    }
<em>    # Generate content</em>
    responses = model.generate_content(
        [prompt],
        generation_config=config,
        stream=True,
        safety_settings=safety_config,
    )
    text_responses = []
    for response in responses:
        text_responses.append(response.text)
    return "".join(text_responses)
prompt = "What are all the colors in a rainbow?"
print(generate_text(my_id, my_location, prompt))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p>The preceding code follows a similar flow as the OpenAI example. Apart from the project ID and location, which Google uses to authenticate access to the API, we need to specify which model we want to use. In this case, we chose “gemini-1.0-pro”, the basic text-only model. Google’s API also supports multimodal requests, including sound and images both in the input and response. A range of examples is available on the API web pages. We’d need to specify the “gemini-1.0-pro-vision” model for multimodal requests.</p> 
  </div> 
  <div class="readable-text intended-text" id="p126"> 
   <p>Listing 1.4 also shows how to provide the <code>temperature</code> parameter, analogous to the one discussed in the OpenAI example, which is used to control the randomness in the responses.</p> 
  </div> 
  <div class="readable-text intended-text" id="p127"> 
   <p>It’s worth mentioning that Google provides an interface for explicitly setting the safety parameters of the model to block unsafe content, based on a list of defined blocking thresholds (table 1.1). The safety parameters can limit the model when it comes to generating content containing harassment, hate speech, explicit sexuality, or that may otherwise be dangerous. The full list for the newest Google models is provided on Google’s AI website<span class=" link-like"/>(<a href="https://ai.google.dev/gemini-api/docs/safety-settings">https://ai.google.dev/gemini-api/docs/safety-settings</a>).</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p128"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 1.1</span> Blocking thresholds for configuring Google’s model safety parameters</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Threshold name 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  <code>BLOCK_NONE</code> <br/></td> 
      <td>  Always show, regardless of the probability of unsafe content. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>BLOCK_ONLY_HIGH</code> <br/></td> 
      <td>  Block when there is a high probability of unsafe content. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>BLOCK_MEDIUM_AND_ABOVE</code> (default) <br/></td> 
      <td>  Block when there is a medium or high probability of unsafe content. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>BLOCK_LOW_AND_ABOVE</code> <br/></td> 
      <td>  Block when there is a low, medium, or high probability of unsafe content. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>HARM_BLOCK_THRESHOLD_UNSPECIFIED</code> <br/></td> 
      <td>  The threshold is unspecified, so block using the default threshold. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p129"> 
   <p>In listing 1.4, we set very conservative parameters, <code>safety_config</code> <code>=</code> <code>{...}</code>, so the model would apply quite stringent filters to the output. This may result in a lower risk while the model is used, but at the cost of returning less useful responses to some prompts. Chapter 8 will offer a broader discussion of model risk considerations.</p> 
  </div> 
  <div class="readable-text" id="p130"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_15"><span class="num-string">1.3.4</span> Third-party integrations of generative AI models</h3> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>In addition to the methods mentioned in previous sections, you may also find a number of generative AI models integrated into various third-party applications and plugins. These integrations typically focus on specific use cases, such as code generation and completion, data visualization, natural language processing, or predictive analytics. </p> 
  </div> 
  <div class="readable-text intended-text" id="p132"> 
   <p>These are some examples of such integrated models:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p133"> GitHub Copilot, designed to assist with code generation, completion, and explanation, with integrations available for the most common integrated development environments (IDEs), such as VSCode, Visual Studio, and the JetBrains suite of IDEs. </li> 
   <li class="readable-text" id="p134"> Packages within the RStudio IDE, like <em>air</em>, provide integration of LLM models into this popular R and Python environment. </li> 
  </ul> 
  <div class="readable-text" id="p135"> 
   <p>The advantage of such integrations is that they usually have direct access to the data or code inside the host environment and are able to directly insert and modify the code, which saves the user the effort of copying each code snippet from the IDE to the model chat window and back again.</p> 
  </div> 
  <div class="readable-text" id="p136"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_16"><span class="num-string">1.3.5</span> Running LLMs locally</h3> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>Running any downloadable model on your personal computer isn’t rocket science. You don’t need NASA-grade equipment either; a decent PC with enough RAM should suffice (with a definition of “decent” being somewhat dynamic). A solid GPU would speed things up, but that is not an absolute requirement. You will need some familiarity with the command line and a couple of libraries to bridge the gap between ambition and reality.</p> 
  </div> 
  <div class="readable-text intended-text" id="p138"> 
   <p>Let’s look at how we can use Python to connect to the Llama 2 model. Implementing it as advertised on Meta’s page is a bit daunting, so we’ll cheat a little. First, we’ll use a quantized model to save on RAM requirements. In essence, quantization of the model means “shaving off,” or rounding, the model weights, sacrificing a little accuracy in exchange for computational efficiency. We’ll also utilize models transitioned into an easy-to-use GGUF file format. If you requested and got a proper license from Meta, you can download the Llama of your choice from the HuggingFace portal. </p> 
  </div> 
  <div class="readable-text intended-text" id="p139"> 
   <p>Two libraries we propose to make your task easier are LangChain and llama-cpp-python. Overall, the environment setup is as simple as presented in listings 1.1 and 1.2. It’s worth mentioning that LangChain could also be used to streamline connecting to ChatGPT or Gemini as well.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p140"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 1.5</span> Command to install the LangChain and Llama libraries</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install langchain llama-cpp-python</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p141"> 
   <p>A word of fair warning. The llama-cpp-python library has some non-negotiable requirements, especially when installed in a Windows environment. However, as it’s a fast-developing tool, check the library site for the latest details. </p> 
  </div> 
  <div class="readable-text intended-text" id="p142"> 
   <p>Now that we have our model downloaded and the proper libraries installed, all we need to do is send a prompt to a model and capture the result. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p143"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 1.6</span> Example of sending a prompt to Llama</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_community.llms import LlamaCpp

llm = LlamaCpp(model_path='path_to_your_gguf_model_file')
response = llm.invoke('Your prompt!')
print(response)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p144"> 
   <p>And that’s it! Depending on your hardware and the chosen model size, you should get your answer in anywhere from a few seconds to a few minutes.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p145"> 
   <p><span class="print-book-callout-head">Context is not only about tokens</span>  The minimalistic implementation we have presented here does not have any conversation memory. It’s a shoot-and-forget method of getting a specific answer to a specific question. Implementing chat memory using LangChain is, however, relatively simple, and you should not be afraid to check the website for current instructions. Funnily enough, it even has an LLM-powered chatbot to answer your questions about the library.</p> 
  </div> 
  <div class="readable-text" id="p146"> 
   <p>There are many options for both model instantiation and prompt development, which you may find useful. You’ll find detailed instructions on the LangChain library website (www.langchain.com). We will not dig into them deeper, as this simple setup is sufficient for our purposes: asking Llama a question and getting an answer.</p> 
  </div> 
  <div class="readable-text" id="p147"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_17"><span class="num-string">1.3.6</span> Best practices and tips for successful generative AI implementation</h3> 
  </div> 
  <div class="readable-text" id="p148"> 
   <p>Although this book won’t cover advanced topics related to the direct integration of generative AI into applications using APIs, we encourage you to follow some best practices and consider the following tips to successfully integrate generative AIs with your data analytics solutions:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p149"> <em>Define clear objectives</em><em> </em>—Start by clearly identifying the goals and expectations of integrating generative AI into your data analytics solution. Determine the tasks you want generative AI to perform, such as data preprocessing, generating insights, or creating visualizations, and tailor your integration accordingly. </li> 
   <li class="readable-text" id="p150"> <em>Familiarize yourself with the API or SDK you are planning to use</em><em> </em>—Thoroughly read and understand the OpenAI API documentation, including details about the API’s or SDK’s features, limitations, and best practices. This knowledge will help you design efficient and reliable interactions between generative AI and your data analytics tools. </li> 
   <li class="readable-text" id="p151"> <em>Use appropriate data formats</em><em> </em>—Ensure that you are using compatible data formats when sending requests to and receiving responses from generative AI. Transform your data, if necessary, to ensure seamless integration and prevent data loss or misinterpretation. </li> 
   <li class="readable-text" id="p152"> <em>Monitor usage and costs</em><em> </em>—Keep track of your API usage to prevent unexpected costs, especially when working with large datasets or complex analytics tasks. Implement rate limiting, caching, or other optimizations to manage your API calls and stay within your plan’s limits. </li> 
   <li class="readable-text" id="p153"> <em>Handle errors and timeouts</em><em> </em>—Implement proper error handling and retry mechanisms to deal with potential issues, such as timeouts or rate-limit errors. This will help ensure the stability and reliability of your integration. </li> 
   <li class="readable-text" id="p154"> <em>Simplify and optimize your prompts</em><em> </em>—Craft your prompts carefully to obtain the most accurate and relevant results from generative AI. Use clear, concise language, and provide enough context to help the AI understand your requirements. You may need to experiment with different prompt structures to find the best approach for your specific use case. </li> 
   <li class="readable-text" id="p155"> <em>Maintain the prompt cheatsheet after optimizing (and verifying the output)</em><em> </em>—Regularly update and refine your prompt notes as you continue to work with generative AI. As your application evolves and your understanding of AI capabilities deepens, your cheatsheet should evolve to include new findings, common errors to avoid, and updated best practices. </li> 
   <li class="readable-text" id="p156"> <em>Provide source material and context</em><em> </em>—Alongside the problem or question, give the model context data examples or explicitly ask it to follow a certain reasoning path. You can also suggest steps in the prompt for the model to follow in its reasoning or ask it to explain certain solution steps. All of this will ensure a greater probability of a correct response and increased transparency regarding how it was generated. </li> 
   <li class="readable-text" id="p157"> <em>Evaluate the AI’s output</em><em> </em>—Generative AI’s output may not always be accurate or relevant. Always double-check the results provided by the AI, and consider implementing human review or validation processes, especially for critical or high-stakes decisions. </li> 
   <li class="readable-text" id="p158"> <em>Test and iterate</em><em> </em>—Before fully integrating generative AI into your data analytics solution, thoroughly test its performance with various tasks and datasets. This will help you identify any issues, limitations, or inaccuracies. Continuously iterate on your prompts, data formats, and integration methods to improve the overall effectiveness of the AI in your analytics workflows. </li> 
   <li class="readable-text" id="p159"> <em>Ensure data security and privacy</em><em> </em>—When working with sensitive data, make sure you comply with data protection regulations and follow security best practices. If you’re working with cloud-based generative AI, encrypt data when transmitting it to and from the provider, and consider using data anonymization techniques to protect the privacy of your users. </li> 
   <li class="readable-text" id="p160"> <em>Stay updated on generative AI developments</em><em> </em>—Keep track of updates and new features developed in the field, as these may impact your integration or offer additional capabilities. Regularly review the API documentation, and subscribe to relevant newsletters or forums to stay informed about any changes or improvements to generative AI. </li> 
   <li class="readable-text" id="p161"> <em>Leverage community resources</em><em> </em>—Take advantage of resources provided by the AI community, such as sample code, tutorials, and forums. These resources can help you learn from others’ experiences and discover best practices for integrating generative AIs with various data analytics solutions. </li> 
  </ul> 
  <div class="readable-text" id="p162"> 
   <p>By following these best practices and tips, you can successfully integrate generative AI into your data analytics workflows and harness its full potential to enhance your decision-making, automate tasks, and uncover valuable insights.</p> 
  </div> 
  <div class="readable-text intended-text" id="p163"> 
   <p>Hopefully, after this introduction, generative AI no longer appears to be a mysterious and, possibly, useless invention. Subsequent chapters will demonstrate specific exchanges between a human and a generative AI and will show us using the responses in all aspects of data analytical work. We’ll also comment on the shortcomings and pitfalls that need to be looked out for to make this cooperation between humans and AI as painless and productive as possible.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p164"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Things to ask generative AI</h5> 
   </div> 
   <ul> 
    <li class="readable-text" id="p165"> What are your limitations? </li> 
    <li class="readable-text" id="p166"> What is the knowledge base you’ve been built upon? </li> 
    <li class="readable-text" id="p167"> What is the latest version of &lt;insert favorite analytics tool&gt; that you know about? </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p168"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_18">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p169"> Generative AI and derivative tools have taken great strides in recent years and can be used as invaluable support in many fields, including data analytics. </li> 
   <li class="readable-text" id="p170"> Despite the progress, these tools won’t (yet!) replace a competent data analyst, and there are many limitations that users should be aware of. </li> 
   <li class="readable-text" id="p171"> At the same time, we encourage you to take full advantage of the immense possibilities of supporting your data analytical work with these language models, which can be done safely by following a few common-sense guidelines. </li> 
   <li class="readable-text" id="p172"> The easiest way to access generative AIs is via their web interfaces, although APIs and SDKs can be used in more advanced applications. </li> 
  </ul>
 </body></html>