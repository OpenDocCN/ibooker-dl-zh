<html><head></head><body>
  <div class="readable-text" id="p1"> &#13;
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">5</span> </span> <span class="chapter-title-text">Clustering</span></h1> &#13;
  </div> &#13;
  <div class="introduction-summary"> &#13;
   <h3 class="introduction-header">This chapter covers</h3> &#13;
   <ul> &#13;
    <li class="readable-text" id="p2">Spectral clustering</li> &#13;
    <li class="readable-text" id="p3">Fuzzy clustering</li> &#13;
    <li class="readable-text" id="p4">Gaussian mixture models clustering</li> &#13;
   </ul> &#13;
  </div> &#13;
  <div class="readable-text" id="p5"> &#13;
   <blockquote>&#13;
    <div>&#13;
     Out of complexity, find simplicity. &#13;
     <div class=" quote-cite">&#13;
       —Einstein &#13;
     </div>&#13;
    </div>&#13;
   </blockquote> &#13;
  </div> &#13;
  <div class="readable-text" id="p6"> &#13;
   <p>Sometimes life is very simple, and sometimes we experience quite complex situations. We sail through both situations and change our approach as needed. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p7"> &#13;
   <p>In part 1, we covered the fundamentals to prepare you for the journey ahead. We are now in part 2, which is slightly more complex than part 1. Part 3 will be more advanced than the first two parts. So please give careful attention to the coming chapters, as the skills and knowledge gained here will prepare you for the later chapters in the book.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p8"> &#13;
   <p>Before starting this chapter, we should refresh our memory on what we covered in chapter 2. We studied clustering algorithms in part 1 of the book. In chapter 2, we learned that clustering is an unsupervised learning technique where we wish to group the data points by discovering interesting patterns in the datasets. We went through the meaning of clustering solutions and different categories of clustering algorithms and looked at a case study. In that chapter, we explored k-means clustering, hierarchical clustering, and DBSCAN clustering in depth. We went through the mathematical background, process, and Python implementation and the pros and cons of each algorithm. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p9"> &#13;
   <p>You may often encounter datasets that do not conform to a simple shape and form. Moreover, we have to find the best fit before making a choice of the final algorithm we wish to implement. Here we might need help with more complex clustering algorithms—the topic of this chapter. In this chapter, we are going to again study three such complex clustering algorithms: spectral clustering, fuzzy clustering, and Gaussian mixture models (GMM) clustering. As always, Python implementation will follow the mathematical and theoretical concepts. This chapter is slightly heavy on mathematical concepts. There is no need to be an advanced student of mathematics, but it is sometimes important to understand how the algorithms work in the background. At the same time, you will be surprised to find that Python implementation of such algorithms is not tedious. This chapter does not have a case study.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p10"> &#13;
   <p>Welcome to the fifth chapter, and all the very best!</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p11"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">5.1</span> Technical toolkit</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p12"> &#13;
   <p>We will continue to use the same version of Python and Jupyter Notebook as we have used so far. The codes and datasets used in this chapter have been checked in at GitHub (<a href="https://mng.bz/6epo">https://mng.bz/6epo</a>). </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p13"> &#13;
   <p>We are going to use the regular Python libraries we have used so far: <code>numpy</code>, <code>pandas</code>, <code>sklearn</code>, <code>seaborn</code>,<span class="code-char"> </span><code>matplotlib</code>, etc. You need to install two other Python libraries in this chapter: <code>skfuzzy</code> and <code>network</code>. Using libraries, we can implement the algorithms very quickly. Otherwise, coding these algorithms is quite a time-consuming and painstaking task. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p14"> &#13;
   <p>Let’s get started with a refresh of clustering!</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p15"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">5.2</span> Clustering: A brief recap</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p16"> &#13;
   <p>Recall from chapter 2, clustering is used to group similar objects or data points. It is an unsupervised learning technique where we intend to find natural grouping in the data, as shown in figure 5.1.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p17"> &#13;
   <p>Here, we can observe that on the left side, we have ungrouped data, and on the right side, the data points have been grouped into logical groups. We can also observe that there can be two methodologies to do the grouping or clustering, and both result in different clusters. Clustering as a technique is quite heavily used in business solutions like customer segmentation, market segmentation, etc. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p18"> &#13;
   <p>We learned about k-means and hierarchical and DBSCAN clustering in chapter 2. We also covered various distance measurement techniques and indicators to measure the performance of clustering algorithms. You are advised to revisit the concepts.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p19"> &#13;
   <p>In this chapter, we focus on advanced clustering methods. We start with spectral clustering in the next section. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p20">  &#13;
   <img alt="figure" src="../Images/CH05_F01_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.1</span> Clustering of objected results into natural grouping</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p21"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">5.3</span> Spectral clustering</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p22"> &#13;
   <p>Spectral clustering is one of the unique clustering algorithms, and a lot of research has been done in this field. Revered researchers include Prof. Andrew Yang, Prof. Michael Jordan, Prof. Yair Weiss, Prof. Jianbo Shi, and Prof. Jitendra Malik, to name a few. We provide links to some of their papers at the end of the chapter. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p23"> &#13;
   <p>Spectral clustering works on the affinity and not the absolute location of the data points for clustering. When we consider the absolute location of the points, the similarity is simply based on the distances between the points, whereas affinity considers the similarity between the points. If the affinity is 0 between the points, they are dissimilar, whereas if the affinity is 1, they are very similar. Hence, wherever the data is in complicated shapes (i.e., some kind of special relationship exists between the data points), spectral clustering is the answer. We show a few examples in figure 5.2 where spectral clustering can provide a logical solution. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p24">  &#13;
   <img alt="figure" src="../Images/CH05_F02_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.2</span> Examples of various complex data shapes that can be clustered using spectral clustering</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p25"> &#13;
   <p>For figure 5.2, we could have used other algorithms like k-means clustering too. But they might not be able to do justice to such complicated shapes of data. You can see from figure 5.2 that the various data points are in a certain pattern. Algorithms like k-means clustering utilize the compactness of the data points and are driven by centroids of the respective clusters. In other words, the closeness of the points to each other and compactness toward the cluster center drive the clustering in k-means. On the other hand, in spectral clustering, <em>connectivity</em> is the driving logic. In connectivity, either the data points are immediately close to one another or they are connected in some way. Some examples of such connectivity-based clustering are depicted in figure 5.2. The points in the inner circle belong to one cluster while those in the outer circle belong to another cluster.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p26"> &#13;
   <p>Now look at the first diagram in figure 5.3, where the data points are in a doughnut pattern. There can be data points that follow this doughnut pattern. We need to cluster this data, and it is indeed a complex pattern. Imagine that by using a clustering method, the circles inside a square are made a part of the same cluster, which is shown in the middle diagram in figure 5.3. After all, they are close to each other. But if we look closely, the points are in a circle and in a pattern, and hence, the actual cluster should be as shown in the far right diagram in figure 5.3. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p27">  &#13;
   <img alt="figure" src="../Images/CH05_F03_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.3</span> We can have a complex representation of data points that need to be clustered. Observe the doughnut shape (left). An explanation can be that the dots in a square are a part of the same cluster as what would be based on the distance only, but clearly, they are not part of the same cluster (middle). We have two circles here. The points in the inner circle belong to one cluster, whereas the outer points belong to another cluster (right).</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p28"> &#13;
   <p>The example shown in figure 5.3 depicts the advantages of spectral clustering as opposed to k-means clustering. In the second figure, the dots in red (those in the square in the print book) will be incorrectly clustered into a different cluster, and in the third figure, the correct clustering is shown. Spectral clustering may group the data from the inner circle in a separate cluster. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p29"> &#13;
   <p>As we said earlier, spectral clustering utilizes the connectivity approach. In spectral clustering, data points that are immediately next to each other are identified in a graph. These data points are sometimes referred to as <em>nodes</em>. These data points or nodes are then mapped to a low-dimensional space. A low-dimensional space is one that has a fewer number of input features. During this process, spectral clustering uses eigenvalues, affinity matrix, Laplacian matrix, and degree matrix derived from the dataset. The low-dimensional space can then be segregated into clusters.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p30"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Spectral clustering utilizes the connectivity approach for clustering. It relies on graph theory, wherein we identify clusters of nodes based on the edges connecting them. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p31"> &#13;
   <p>We will study the process in detail. But first, there are a few important mathematical concepts that form the foundation of spectral clustering, which we will cover now.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p32"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">5.3.1</span> Building blocks of spectral clustering</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p33"> &#13;
   <p>We know that the goal of clustering is to group data points that are similar into one cluster and the data points that are not similar into another. One important mathematical concept is similarity graphs, which are a representation of data points. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p34"> &#13;
   <h4 class=" readable-text-h4">Similarity graphs</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p35"> &#13;
   <p>A graph is one of the intuitive methods to represent data points. The first diagram in figure 5.4 shows an example of a graph that is simply a connection between data points represented by the edge. Two data points are connected if the similarity between them is positive or it is above a certain threshold, which is shown in the second diagram. Instead of absolute values for the similarity, we can use weights. So in the second diagram in figure 5.4, as point 1 and 2 are similar compared to points 1 and 3, the connection between points 1 and 2 has a higher weight than points 1 and 3. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p36">  &#13;
   <img alt="figure" src="../Images/CH05_F04_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.4</span> A graph is a simple representation of data points. The points or nodes are connected by edges if they are very similar (left). The weight is higher if the similarity between data points is high; for dissimilar data points, the weight is less (right).</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p37"> &#13;
   <p>So, we can say that, using similarity graphs, we wish to cluster the data points such that the edges of the data points have </p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p38"> Higher weight values and hence are similar to each other and so are in the same cluster </li> &#13;
   <li class="readable-text" id="p39"> Lower values of weight and hence are not similar to each other and so are in different clusters </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p40"> &#13;
   <p>Apart from similarity graphs, we should also know the concept of eigenvalues and eigenvectors, which we covered in detail in the previous chapter. You are advised to refresh your memory on it should you need to.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p41"> &#13;
   <h4 class=" readable-text-h4">Adjacency matrix</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p42"> &#13;
   <p>Have a close look at figure 5.5. We can see those various points from 1 to 5 are connected. We represent the connection in a matrix. That matrix is called an <em>adjacency matrix</em>. In an adjacency matrix, the rows and columns are the respective nodes. The values inside the matrix represent the connection: if the value is 0, that means there is no connection, and if the value is 1, it means there is a connection.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p43">  &#13;
   <img alt="figure" src="../Images/CH05_F05_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.5</span> An adjacency matrix represents the connection between various nodes. There is a connection between node 1 and node 5; hence the value is 1. There is no connection between node 1 and node 4; hence the corresponding value is 0.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p44"> &#13;
   <p>So, for an adjacency matrix, we are only concerned if there is a connection between two data points. With the way that we are defining the edges (as nonoriented), the matrix is always symmetric. This is because if there is a connection from 1 to 2, there must also be a connection from 2 to 1, and if there is no connection between 3 and 1, there is no connection between 1 and 3 either. If we extend the concept of the adjacency matrix, we get a degree matrix, which is our next topic. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p45"> &#13;
   <h4 class=" readable-text-h4">Degree matrix</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p46"> &#13;
   <p>A degree matrix is a diagonal matrix, where the degree of a node along the diagonal is the number of edges connected to it. If we use the same example as previously, we get the degree matrix shown in figure 5.6. Nodes 3 and 5 have three connections each, so they have values of 3 along the diagonal; the other nodes have only two connections each, so they have 2 as the value along the diagonal.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p47">  &#13;
   <img alt="figure" src="../Images/CH05_F06_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.6</span> While an adjacency matrix represents the connection between various nodes, a degree matrix is for the number of connections each node has. It is shown on the diagonal of the matrix. For example, node 5 has three connections and hence has a value of 3 in the adjacency matrix, while node 1 has only two connections and so has a value of 2.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p48"> &#13;
   <p>You might be wondering: Why do we use these matrices? Matrices provide an elegant representation of the data and can clearly depict the relationships between two points. Also, computers can more easily deal with matrix representation than alternative ways for manipulating the graph. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p49"> &#13;
   <p>Now that we have covered both the adjacency matrix and degree matrix, we can move to the Laplacian matrix. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p50"> &#13;
   <h4 class=" readable-text-h4">Laplacian matrix</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p51"> &#13;
   <p>There are quite a few variants of the Laplacian matrix, but if we take the simplest form, it is nothing but a subtraction of the adjacency matrix from the degree matrix—in other words, L = D – A. We can demonstrate it as shown in figure 5.7.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p52">  &#13;
   <img alt="figure" src="../Images/CH05_F07_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.7</span> The Laplacian matrix is quite simple to understand. To get a Laplacian matrix, we can simply subtract an adjacency matrix from the degree matrix as shown in the example here. Here, D represents the degree matrix, A is the adjacency matrix, and L is the Laplacian matrix. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p53"> &#13;
   <p>The Laplacian matrix is an important concept, and we use the eigenvalues of L to develop spectral clustering. Once we get the eigenvalues and eigenvectors, we can define two other values: spectral gap and Fielder value. The very first nonzero eigenvalue is the <em>spectral gap,</em> which defines the density of the graph. The <em>Fielder value</em> is the second eigenvalue; it provides an approximation of the minimum cut required to separate the graph into two components. The corresponding vector for the Fielder value is called the <em>Fielder vector</em>. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p54"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> The Fielder vector has both negative and positive components, and their resultant sum is zero.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p55"> &#13;
   <p>We will use this concept once we study the process of spectral clustering in detail in the next section. We cover one more concept—the affinity matrix—before moving on to the process of spectral clustering.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p56"> &#13;
   <h4 class=" readable-text-h4">Affinity matrix</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p57"> &#13;
   <p>In the adjacency matrix, if we replace the number of connections with the similarity of the weights, we will get the affinity matrix. If the points are completely dissimilar, the affinity will be 0; if they are completely similar, the affinity will be 1. The values in the matrix represent different levels of similarity between data points.  </p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p58"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 5.1</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p59"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p60"> The degree matrix is created by counting the number of connections. True or False? </li> &#13;
    <li class="readable-text" id="p61"> Laplacian is a transpose of the division of degree and adjacency matrix. True or False? </li> &#13;
    <li class="readable-text" id="p62"> Draw a graph on paper and then derive its adjacency and degree matrix. </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p63"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">5.3.2</span> The process of spectral clustering</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p64"> &#13;
   <p>Now we have covered all the building blocks for spectral clustering. At a high level, the various steps can be noted as follows:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p65"> We get the dataset and calculate its degree matrix and adjacency matrix. </li> &#13;
   <li class="readable-text" id="p66"> Using them, we calculate the Laplacian matrix. </li> &#13;
   <li class="readable-text" id="p67"> Then we calculate the first <em>k</em> eigenvectors of the Laplacian matrix. The <em>k</em> eigenvectors are the ones that correspond to the <em>k</em> smallest eigenvalues. </li> &#13;
   <li class="readable-text" id="p68"> The resultant matrix formed is used to cluster the data points in k-dimensional space. </li> &#13;
  </ol> &#13;
  <div class="readable-text print-book-callout" id="p69"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> For more clarity on eigenvalues, the affinity matrix, and the Laplacian matrix, refer to the appendix.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p70"> &#13;
   <p>We cover the process of spectral clustering using an example, as shown in figure 5.8. These steps are generally not done in real-world implementation, as we have packages and libraries to achieve them, but they are covered here to give you an idea of how the algorithm can be developed from scratch and how it works so that you have a better understanding on how to effectively utilize it. For the Python solution, we will use the libraries and packages only. Though it is possible to develop an implementation from scratch, it is not time-efficient to reinvent the wheel.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p71">  &#13;
   <img alt="figure" src="../Images/CH05_F08_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.8</span> Consider the example shown where we have some data points and they are connected. We will perform spectral clustering on this data.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p72"> &#13;
   <p>When we wish to perform the spectral clustering on this data, we follow these steps:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p73"> Create the adjacency matrix and degree matrix. We will leave this step up to you. </li> &#13;
   <li class="readable-text" id="p74"> Create the Laplacian matrix (see figure 5.9).<span class="aframe-location"/> </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p75">  &#13;
   <img alt="figure" src="../Images/CH05_F09_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.9</span> The Laplacian matrix of the data. You are advised to create the degree and adjacency matrix and check the output. </h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p76"><span class="faux-ol-li-counter">3. </span> Create the Fielder vector, as shown in figure 5.10, for the preceding Laplacian matrix. We create the Fielder vector as described in the Laplacian Matrix section. Observe how the sum of the matrix is zero. <span class="aframe-location"/> </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p77">  &#13;
   <img alt="figure" src="../Images/CH05_F10_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.10</span> The Fielder vector is the output for the Laplacian matrix.</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p78"><span class="faux-ol-li-counter">4. </span> We can see that there are a few positive values and a few negative values. Based on the positive or negative values, we can create two distinct clusters. Figure 5.11 illustrates the process of spectral clustering. </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p79">  &#13;
   <img alt="figure" src="../Images/CH05_F11_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.11</span> The two clusters are identified. This is a very simple example to illustrate the process of spectral clustering.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p80"> &#13;
   <p>Spectral clustering is useful for image segmentation, speech analysis, text analytics, entity resolution, etc. The method does not make any assumptions about the shape of the data. Methods like k-means assume that the points are in a spherical form around the center of the cluster, whereas there is no such strong assumption in spectral clustering. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p81"> &#13;
   <p>Another significant difference is that in spectral clustering the data points need not have convex boundaries as compared to other methods where compactness drives clustering. Spectral clustering is sometimes slow since various matrices and their eigenvalues, Laplacians, etc., have to be calculated. With a large dataset, the complexity increases, and hence, spectral clustering can become slow, but it is a fast method when we have a sparse dataset. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p82"> &#13;
   <p>Spectral clustering requires building a matrix that nominally has the size of the number of items in a dataset squared because there is one column and one row for each element. For example, a modest dataset of a few million elements will require a matrix of several trillion elements! Storing that matrix verbatim requires terabytes of RAM and is something that is at the edge of what a very powerful and expensive server could do. There are techniques to mitigate the memory needs (such as not storing every single element separately), but they make working with the matrix more complicated. Moreover, finding the eigenvalues and even one eigenvector of such a large matrix is very time-intense. As such, spectral clustering is a viable approach generally for small datasets.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p83"> &#13;
   <p>We will now proceed to the Python solution of the spectral clustering algorithm.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p84"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">5.4</span> Python implementation of spectral clustering</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p85"> &#13;
   <p>We have covered the details of spectral clustering—it is time to get into the code. For this, we will create an artificial dataset and run a k-means algorithm and then spectral clustering to compare the results. The steps are as follows:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p86"> Import all the necessary libraries. These libraries are standard, except for a few that we will cover. <code>sklearn</code> is one of the most famous and sought-after libraries, and from <code>sklearn</code> we import <code>SpectralClustering</code>, <code>make_blobs</code>, and <code>make_circles</code>: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p87"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from sklearn.cluster import SpectralClustering&#13;
from sklearn.datasets import make_blobs&#13;
import matplotlib.pyplot as plt&#13;
from sklearn.datasets import make_circles&#13;
from numpy import random&#13;
import numpy as np&#13;
from sklearn.cluster import SpectralClustering, KMeans&#13;
from sklearn.metrics import pairwise_distances&#13;
from matplotlib import pyplot as plt&#13;
import networkx as nx&#13;
import seaborn as sns</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p88"><span class="faux-ol-li-counter">2. </span> Curate a dataset. We will use the <code>make_circles</code> method. Here, we take 2,000 samples and represent them in a circle. The output is as follows (see figure 5.12):  </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p89"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">data, clusters = make_circles(n_samples=2000, noise=.01, factor=.3, random_state=5)&#13;
plt.scatter(data[:,0], data[:,1]) <span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p90">  &#13;
   <img alt="figure" src="../Images/CH05_F12_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.12</span> Curating a dataset using the <code>make_circles</code> method</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p91"><span class="faux-ol-li-counter">3. </span> Test this dataset with k-means clustering. The two colors show two different clusters, which overlap each other. The print version of the book will not show the colors, but the output of the Python code will. The same output is available in the GitHub repository (see figure 5.13): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p92"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">kmeans = KMeans(init='k-means++', n_clusters=2)&#13;
km_clustering = kmeans.fit(data)&#13;
plt.scatter(data[:,0], data[:,1], c=km_clustering.labels_, cmap='prism',&#13;
alpha=0.5, edgecolors='g')<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p93">  &#13;
   <img alt="figure" src="../Images/CH05_F13_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.13</span> Testing the dataset with k-means clustering </h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p94"><span class="faux-ol-li-counter">4. </span> Run the same data with spectral clustering. We find that the two clusters are being handled separately here (see figure 5.14): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p95"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">spectral = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', random_state=5)&#13;
sc_clustering = spectral.fit(data)&#13;
plt.scatter(data[:,0], data[:,1], c=sc_clustering.labels_, cmap='prism', alpha=0.5, edgecolors='g')<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p96">  &#13;
   <img alt="figure" src="../Images/CH05_F14_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.14</span> The two clusters are being handled separately when using spectral clustering.</h5>&#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p97"> &#13;
   <p>We can observe here that the same dataset is handled differently by the two algorithms. Spectral clustering handles the dataset arguably better, as the circles that are separate are depicted separately. </p> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p98"><span class="faux-ol-li-counter">5. </span> Simulate various cases by changing the values in the dataset and running the algorithms. Observe the different outputs for comparison. </li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p99"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">5.5</span> Fuzzy clustering</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p100"> &#13;
   <p>So far we have covered quite a few clustering algorithms. Did you wonder why a data point should belong to only one cluster? Why can’t a data point belong to more than one cluster? Have a look at figure 5.15: the red points in the right image (shown with an x in the print version) can belong to more than one cluster.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p101">  &#13;
   <img alt="figure" src="../Images/CH05_F15_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.15</span> The figure on the left represents all the data points. The red points (those with an x in the print version) can belong to more than one cluster. In fact, we can allocate more than one cluster to each point. A probability score can be given for a point to belong to a particular cluster. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p102"> &#13;
   <p>We know that clustering is used to group items in cohesive groups based on their similarities. The items that are similar are in one cluster, whereas the items that are dissimilar are in different clusters. The idea of clustering is to ensure the items in the same cluster are similar. When the items can be only in one cluster, it is called <em>hard clustering.</em> K-means clustering is a classic example of hard clustering. But if we reflect on figure 5.15, we can observe that an item can belong to more than one cluster. This is called <em>soft clustering.</em>  </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p103"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> It is computationally cheaper to create fuzzy boundaries than to create hard clusters.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p104"> &#13;
   <p>In fuzzy clustering, an item can be assigned to more than one cluster. The items that are closer to the center of a cluster will have a stronger belongingness to that cluster as compared to the points that are at the edge of the cluster. This is referred to as <em>membership</em>. It employs the least-square algorithm to find the most optimal location of an item. The optimal location that we derive from the least-square algorithm will be the probability space between two or more clusters. We will examine this concept in detail later. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p105"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">5.5.1</span> Types of fuzzy clustering</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p106"> &#13;
   <p>Fuzzy clustering can be further divided into classical fuzzy algorithms and shape-based fuzzy algorithms. See figure 5.16. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p107">  &#13;
   <img alt="figure" src="../Images/CH05_F16_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.16</span> Fuzzy algorithms can be divided into the classical fuzzy algorithm and the shape-based fuzzy algorithm.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p108"> &#13;
   <p>We will cover the fuzzy c-means (FCM) algorithm in detail next, but first we will review the rest of the algorithms briefly:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p109"> The Gustafson-Kessel algorithm, sometimes called the GK algorithm, works by associating an item with a cluster and a matrix. GK results in elliptical clusters, and to modify as per varied structures in the datasets, GK uses the covariance matrix. It allows the algorithm to capture the elliptical properties of the cluster. GK can result in narrower clusters, and wherever the number of items is higher, those areas can be thinner.  </li> &#13;
   <li class="readable-text" id="p110"> The Gath-Geva algorithm is not based on an objective function. The clusters can result in any shape, because it is a fuzzification of statistical estimators.  </li> &#13;
   <li class="readable-text" id="p111"> The shape-based clustering algorithms are self-explanatory as per their names. A circular fuzzy clustering algorithm will result in circular-shaped clusters and so on. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p112"> &#13;
   <p>The FCM algorithm is the most popular fuzzy clustering algorithm. It was initially developed in 1973 by J.C. Dunn, and it has been improved multiple times. It is quite similar to k-means clustering.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p113"> &#13;
   <p>Refer to figure 5.17. In the first part of the figure (left), we have some items or data points. These data points can be a part of a clustering dataset like customer transactions, etc. In the second part of the figure (middle), we create a cluster for these data points. While this cluster is created, membership grades are allocated to each of the data points. These membership grades suggest the degree or the level to which a data point belongs to a cluster. We will shortly examine the mathematical function to calculate these values. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p114"> &#13;
   <p><span class="print-book-callout-head">TIP</span>  Do not get confused by the degree and the probabilities. If we sum these degrees, we may not get 1, as these values are normalized between 0 and 1 for all the items. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p115"> &#13;
   <p>In the third part of the figure (right), we can see that point 1 is closer to the cluster center and thus belongs to the cluster to a higher degree than point 2, which is closer to the boundary or the edge of the cluster. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p116">  &#13;
   <img alt="figure" src="../Images/CH05_F17_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.17</span> Data points that can be clustered (left). The data points can be grouped into two clusters. For the first cluster, the cluster centroid is represented using a + sign (middle). Point 1 is much closer to the cluster center as compared to point 2. So we can conclude that point 1 belongs to this cluster to a higher degree than cluster 2.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p117"> &#13;
   <p>We will now venture into the technical details of the algorithm. This can get a little mathematically heavy.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p118"> &#13;
   <p>Consider we have a set of <em>n</em> items (equation 5.1):</p> &#13;
  </div> &#13;
  <div class="browsable-container equation-container" id="p119"> &#13;
   <h5 class=" browsable-container-h5">(5.1)</h5> &#13;
   <p><em> x</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>, <em>x</em><sub>4</sub>, <em>x</em><sub>5</sub>, . . ., <em>x</em><sub><em>n</em></sub>}</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p120"> &#13;
   <p>We apply the FCM algorithm to these items. These <em>n</em> items are clustered into <em>c</em> fuzzy clusters based on some criteria. Let’s say that we will get from the algorithm a list of <em>c</em> cluster centers (equation 5.2):</p> &#13;
  </div> &#13;
  <div class="browsable-container equation-container" id="p121"> &#13;
   <h5 class=" browsable-container-h5">(5.2)</h5> &#13;
   <p><em> c</em> = {<em>c</em><sub>1</sub>, <em>c</em><sub>2</sub>, <em>c</em><sub>3</sub>, <em>c</em><sub>4</sub>, <em>c</em><sub>5</sub>, . . ., <em>c</em><sub><em>c</em></sub>} </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p122"> &#13;
   <p>The algorithm also returns a partition matrix, which can be defined as equation 5.3: </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p123">  &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-2x.png"/> &#13;
   <h5 class=" figure-container-h5">(5.3)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p124"> &#13;
   <p>Here, each of the elements in <em>w</em><sub><em>i</em></sub><sub>,</sub><sub><em>j</em></sub> is the degree to which each of the elements in <em>X</em> belong to cluster <em>c</em><sub><em>j</em></sub>. This is the purpose of the partition matrix. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p125"> &#13;
   <p>Mathematically, we can get <em>w</em><sub><em>i</em></sub><sub>,</sub><sub><em>j</em></sub><sub> </sub>as shown in equation 5.4. The proof of the equation is beyond the scope of this book.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p126">  &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-3x.png"/> &#13;
   <h5 class=" figure-container-h5">(5.4)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p127"> &#13;
   <p>The algorithm generates centroids for the clusters too. The centroid of a cluster is the mean of all the points in that cluster, and the mean is weighted by their respective degrees of belonging to that cluster. If we represent it mathematically, we can write it like in equation 5.5:</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p128">  &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-4x.png"/> &#13;
   <h5 class=" figure-container-h5">(5.5)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p129"> &#13;
   <p>In equations 5.4 and 5.5, we have a very important term: <em>m</em>. <em>m</em> is the hyperparameter used to control the fuzziness of the clusters. The values of <em>m</em> <span class="regular-symbol">≥</span> 1 and can be kept as 2 (a typically used value). </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p130"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> The higher the value of <em>m</em>, the fuzzier the clusters. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p131"> &#13;
   <p>We now examine the step-by-step process in the FCM algorithm:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p132"> Start as we start in k-means clustering by choosing the number of clusters we wish to have in the output. </li> &#13;
   <li class="readable-text" id="p133"> Allocate the weights randomly to each of the data points. </li> &#13;
   <li class="readable-text" id="p134"> The algorithm iterates until it has converged. Recall how the k-means algorithm converges, wherein we initiate the process by randomly allocating the centroids of clusters. And then iteratively we refine the centroids for each of the clusters until we get convergence. This is how k-means works. For FCM, we will utilize a similar process albeit with slight differences. We have added a membership value <em>w</em><sub><em>i</em></sub><sub>,</sub><sub><em>j</em></sub> and <em>m</em>. </li> &#13;
   <li class="readable-text" id="p135"> For FCM, for the algorithm to converge we calculate the centroid for each of the clusters as per equation 5.6: </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p136">  &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-4x.png"/> &#13;
   <h5 class=" figure-container-h5">(5.6)</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p137"><span class="faux-ol-li-counter">5. </span> For each of the data points, we also calculate its respective coefficient for being in that particular cluster. We will use equation 5.4. </li> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p138"><span class="faux-ol-li-counter">6. </span> Now we should iterate until the FCM algorithm has converged. The cost function that we wish to minimize is given by equation 5.7: </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p139">  &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-5x.png"/> &#13;
   <h5 class=" figure-container-h5">(5.7)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p140"> &#13;
   <p>Once this function has been minimized, we can conclude that the FCM algorithm has converged. In other words, we can stop the process as the algorithm has finished processing.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p141"> &#13;
   <p>This is a good time to compare this with the k-means algorithm. In k-means, we have a strict objective function that will allow only one cluster membership, while for FCM clustering, we can get different clustering membership based on the probability scores. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p142"> &#13;
   <p>FCM is very useful for business cases where the boundary between clusters is not clear and stringent. Consider the field of bioinformatics, wherein a gene can belong to more than one cluster of genes. Another example is when we have overlapping datasets like in the fields of the marketing analytics or image segmentation where we might have a lot of complex, overlapping, and confusing datasets. FCM can give comparatively more robust results than k-means. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p143"> &#13;
   <p>We will now proceed to the Python solution of FCM clustering using the libraries.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p144"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 5.2</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p145"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p146"> Fuzzy clustering allows us to create overlapping clusters. True or False? </li> &#13;
    <li class="readable-text" id="p147"> A data point can belong to one and only one cluster. True or False? </li> &#13;
    <li class="readable-text" id="p148"> If the value of <em>m</em> is lower, we get clusters with more precise boundaries. True or False? </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p149"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">5.5.2</span> Python implementation of FCM</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p150"> &#13;
   <p>We have covered the process of FCM. We will now work on the Python implementation of FCM by following these steps: </p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p151"> Import the necessary libraries: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p152"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import skfuzzy as fuzz&#13;
import pandas as pd&#13;
import numpy as np&#13;
import matplotlib.pyplot as plt&#13;
import seaborn as sns&#13;
%matplotlib inline</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p153"><span class="faux-ol-li-counter">2. </span> Declare a color palette, which will be used later for color coding the clusters: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p154"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">color_pallete = ['r','m','y','c', 'brown', 'orange','m','k', &#13;
'gray','purple','seagreen']</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p155"><span class="faux-ol-li-counter">3. </span> Define the cluster centers: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p156"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">cluster_centers = [[1, 1],&#13;
           [2, 4],&#13;
           [5, 8]]</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p157"><span class="faux-ol-li-counter">4. </span> Assign the weights: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p158"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">sigmas = [[0.5, 0.6],&#13;
          [0.4, 0.5],&#13;
          [0.1, 0.6]]</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p159"><span class="faux-ol-li-counter">5. </span> Set the seed and then loop through the cluster centers: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p160"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">np.random.seed(5)  &#13;
&#13;
xpts = np.zeros(1)&#13;
ypts = np.zeros(1)&#13;
labels = np.zeros(1)&#13;
for i, ((xmu, ymu), (xsigma, ysigma)) in enumerate(zip(cluster_centers, &#13;
sigmas)):&#13;
    xpts = np.hstack((xpts, np.random.standard_normal(500) * xsigma + xmu))&#13;
    ypts = np.hstack((ypts, np.random.standard_normal(500) * ysigma + ymu))&#13;
    labels = np.hstack((labels, np.ones(500) * i))</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p161"><span class="faux-ol-li-counter">6. </span> We will represent the data points first. See figure 5.18: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p162"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">fig0, ax0 = plt.subplots()&#13;
for label in range(5):&#13;
    ax0.plot(xpts[labels == label], ypts[labels == label], '.')&#13;
ax0.set_title('Data set having 500 points.')&#13;
plt.show()<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p163">  &#13;
   <img alt="figure" src="../Images/CH05_F18_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.18</span> Representation of the data points</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p164"><span class="faux-ol-li-counter">7. </span> Iterate different outputs with different values of cluster values and FPC (see figure 5.19):<span class="aframe-location"/> </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p165">  &#13;
   <img alt="figure" src="../Images/CH05_F19_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.19</span> The output of the FCM algorithm</h5>&#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p166"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">fig1, axes1 = plt.subplots(3, 3, figsize=(10, 10))&#13;
alldata = np.vstack((xpts, ypts))&#13;
fpcs = []&#13;
&#13;
for ncenters, ax in enumerate(axes1.reshape(-1), 2):&#13;
    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(&#13;
        alldata, ncenters, 2, error=0.005, maxiter=1000, init=None)&#13;
&#13;
    # Store fpc values for later&#13;
    fpcs.append(fpc)&#13;
&#13;
    # Plot assigned clusters, for each data point in training set&#13;
    cluster_membership = np.argmax(u, axis=0)&#13;
    for j in range(ncenters):&#13;
        ax.plot(xpts[cluster_membership == j],&#13;
                ypts[cluster_membership == j], '.', color=colors[j])&#13;
&#13;
    # Mark the center of each fuzzy cluster&#13;
    for pt in cntr:&#13;
        ax.plot(pt[0], pt[1], 'rs')&#13;
&#13;
    ax.set_title('cluster_centers = {0}; FPC = {1:.2f}'.format(ncenters, &#13;
fpc), size=12)&#13;
    ax.axis('off')&#13;
&#13;
fig1.tight_layout()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p167"> &#13;
   <p>Observe the output of the code, where for the same datasets you can see the different clusters with different positions of the centers. To appreciate the colors, you will have to run the code. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p168"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">5.6</span> Gaussian mixture model</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p169"> &#13;
   <p>Next, we continue our discussion of soft clustering. Recall we introduced the GMM at the start of the chapter. Now we will study the concept and see the Python implementation of it. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p170"> &#13;
   <p>First, let’s get an understanding of the <em>Gaussian distribution</em> or what is sometimes called <em>normal distribution</em>. You might recognize it as a bell curve; it usually refers to the same thing. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p171"> &#13;
   <p>In figure 5.20, observe that the distribution where the <em>µ</em> (mean) is 0 and <em class="obliqued">σ</em><sup>2</sup> (standard deviation) is 1. It is a perfect normal distribution curve. Compare the distribution in different curves here. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p172">  &#13;
   <img alt="figure" src="../Images/CH05_F20_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.20</span> A Gaussian distribution is one of the most famous distributions. Observe how the values of mean and standard deviation are changed and their effect on the corresponding curve. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p173"> &#13;
   <p>The mathematical expression for Gaussian distribution is</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p174">  &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-6x.png"/> &#13;
   <br/> &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-7x.png"/> &#13;
   <h5 class=" figure-container-h5">(5.8)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p175"> &#13;
   <p>The equation is also called the probability density function. In figure 5.20, observe the shape of the probability distribution where the <em>µ</em> is 0 and <em class="obliqued">σ</em><sup>2</sup> is 1. It is a perfect normal distribution curve. Compare the distribution in different curves in figure 5.20 where, by changing the values of the mean and standard distribution, we get different graphs. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p176"> &#13;
   <p>You might be wondering why we are using Gaussian distribution here. There is a very famous statistical theorem called the <em>central limit theorem</em>. The theorem states that if the variability of the data is due to a large number of unrelated causes, then the distribution can be approximated by a Gaussian curve. Also, the approximation becomes more and more accurate the more data is collected; that is, the more data we collect, the more Gaussian the distribution. This normal distribution can be observed across all walks of life and in chemistry, physics, mathematics, biology, or any other branch of science. That is the beauty of Gaussian distribution. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p177"> &#13;
   <p>The plot shown in figure 5.20 is 2D. We can have multidimensional Gaussian distribution too. In the case of a multidimensional Gaussian distribution, we will get a 3D figure as shown in figure 5.21. Our input was a scalar in 1D. Now, instead of scalar, our input is a vector; the mean is also a vector and represents the center of the data. Hence, the mean has the same dimensionality as the input data. The variance is now the covariance matrix <span class="regular-symbol">∑</span>. This matrix not only tells us the variance in the inputs but also comments on the relationship between different variables—for example, how the values of <em>y</em> are affected if the value of <em>x</em> is changed. Have a look at figure 5.21. We can understand the relationship between the <em>x</em> and <em>y</em> variables here. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p178">  &#13;
   <img alt="figure" src="../Images/CH05_F21_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.21</span> 3D representation of a Gaussian distribution </h5>&#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p179"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Covariance plays a significant role here. K-means does not consider the covariance of a dataset, which is used in the GMM model.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p180"> &#13;
   <p>Let’s examine the process of GMM clustering. Imagine we have a dataset with <em>n</em> items. When we use GMM clustering, we do not find the clusters using the centroid method; instead, we fit a set of <em>k</em> Gaussian distributions to the dataset at hand. In other words, we have <em>k</em> clusters. We should determine the parameters for each of these Gaussian distributions, which are mean, variance, and weight of a cluster. Once the parameters for each of the distributions are determined, then we can find the respective probability for each of the <em>n</em> items to belong to <em>k</em> clusters.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p181"> &#13;
   <p>Mathematically, we can calculate the probability as shown in equation 5.9. The equation is used so we know that a particular point <em>x</em> is a linear combination of <em>k</em> Gaussians. The term <em class="obliqued">f</em><sub><em>j</em></sub> is used to represent the strength of the Gaussian, and it can be seen in the second equation that the sum of such strength is equal to 1.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p182">  &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-8x.png"/> &#13;
   <br/> &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-9x.png"/> &#13;
   <h5 class=" figure-container-h5">(5.9)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p183"> &#13;
   <p>For spectral clustering, we must identify the values of <em class="obliqued">f</em>, <span class="regular-symbol">∑</span>, and <em>µ</em>. As you can imagine, getting the values of these parameters can be tricky. It is indeed a slightly complex process called the expectation-maximization (EM) technique, which we will cover next. This section is quite heavy on mathematical concepts and is optional. It is recommended for readers interested in understanding the deeper workings of the techniques. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p184"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">5.6.1</span> EM technique</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p185"> &#13;
   <p>EM is a statistical method to determine the correct parameters for a model. There are quite a few techniques that are popular; maximum likelihood estimation might be the most famous. But at the same time, there could be a few challenges with maximum likelihood. The dataset might have missing values or, in other words, be incomplete. Or it is possible that a point in the dataset is generated by two different Gaussian distributions. Hence, it will be very difficult to determine which distribution generated that data point. Here, EM can be helpful. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p186"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> K-means uses only mean while GMM utilizes both mean and variance of the data.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p187"> &#13;
   <p>The variables that are generated in the process are called <em>latent variables</em>. Since we do not know the exact values of these latent variables, EM first estimates their optimum values using the current data. Once this is done, then the model parameters are estimated. Using these model parameters, the latent variables are again determined. And, using these new latent variables, new model parameters are derived. The process continues until a good enough set of latent values and model parameters are achieved that fit the data well. Let’s study that in more detail now. We will use the same example as in the last section. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p188"> &#13;
   <p>Imagine we have a dataset with <em>n</em> items. As mentioned, when we use GMM clustering, we do not find the clusters using the centroid method; instead, we fit a set of <em>k</em> Gaussian distributions to the dataset at hand. In other words, we have <em>k</em> clusters. We determine the parameters for each of these Gaussian distributions (mean, variance, and weight). Let’s say that mean is <em>µ</em><sub>1</sub>, <em>µ</em><sub>2</sub>, <em>µ</em><sub>3</sub>, <em>µ</em><sub>4</sub>…. <em>µ</em><sub><em>k</em></sub> and covariance is <span class="regular-symbol">∑</span><sub>1</sub>, <span class="regular-symbol">∑</span><sub>2</sub>, <span class="regular-symbol">∑</span><sub>3</sub>, <span class="regular-symbol">∑</span><sub>4</sub>…. <span class="regular-symbol">∑</span><sub><em>k</em></sub>. We can also have one more parameter to represent the density or strength of the distribution, and it can be represented by <em class="obliqued">f</em>. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p189"> &#13;
   <p>We start with the expectation, or the E step. In this step, each data point is assigned to a cluster probabilistically. So, for each point, we calculate its probability of belonging to a cluster; if this value is high, the point is in the correct cluster; otherwise, the point is in the wrong cluster. In other words, we calculate the probability that each data point is generated by each of the <em>k</em> Gaussians. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p190"> &#13;
   <p><span class="print-book-callout-head">NOTE</span>  Since we are calculating probabilities, these are called soft assignments. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p191"> &#13;
   <p>The probability is calculated using the formula in equation 5.10. If we look closely, the numerator is the probability, and then we normalize by the denominator. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p192">  &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-10x.png"/> &#13;
   <h5 class=" figure-container-h5">(5.10)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p193"> &#13;
   <p>In the expectation step, for a data point <em>x</em><sub><em>i</em></sub><sub>,</sub><sub><em>j</em></sub>, where <em>i</em> is the row and <em>j</em> is the column, we are getting a matrix where rows are represented by the data points and columns are their respective Gaussian values. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p194"> &#13;
   <p>When the expectation step is finished, we will perform the maximization or the M step. In this step, we will update the values of <em>µ</em>, <span class="regular-symbol">∑</span>, and <em class="obliqued">f</em> using the formula in equation 5.7. Recall, in k-means clustering, we simply take the mean of the data points and move ahead. We do something similar here albeit use the probability or the expectation we calculated in the last step. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p195"> &#13;
   <p>The three values can be calculated using the equations below. Equation 5.7 is the calculation of the covariances <span class="regular-symbol">∑</span><sub><em>j</em></sub>, of all the points, which is then weighted by the probability of that point being generated by Gaussian <em>j</em> as shown in equation 5.11. The mathematical proofs are beyond the scope of this book.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p196">  &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-11x.png"/> &#13;
   <h5 class=" figure-container-h5">(5.11)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p197"> &#13;
   <p>The mean <em>µ</em><sub><em>j</em></sub>, is determined by equation 5.12. Here, we determine the mean for all the points, weighted by the probability of that point being generated by Gaussian <em>j</em>. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p198">  &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-12x.png"/> &#13;
   <h5 class=" figure-container-h5">(5.12)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p199"> &#13;
   <p>Similarly, the density or the strength is calculated by equation 5.13, where we add all the probabilities for each point to be generated by Gaussian <em>j</em> and then divide by the total number of points <em>N</em>. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p200">  &#13;
   <img alt="figure" src="../Images/verdhan-ch5-eqs-13x.png"/> &#13;
   <h5 class=" figure-container-h5">(5.13)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p201"> &#13;
   <p>Based on these values, new values for <span class="regular-symbol">∑</span>, <em>µ</em>, and <em class="obliqued">f</em> are derived, and the process continues until the model converges. We stop when we can maximize the log-likelihood function. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p202"> &#13;
   <p>It is a complex mathematical process. We have covered it to give you an in-depth understanding of what happens in the background of the statistical algorithm. The Python implementation is much more straightforward than the mathematical concept.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p203"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 5.3</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p204"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p205"> Gaussian distribution has a mean equal to 1 and a standard deviation equal to 0. True or False? </li> &#13;
    <li class="readable-text" id="p206"> GMM models do not consider the covariance of the data. True or False? </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p207"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">5.6.2</span> Python implementation of GMM</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p208"> &#13;
   <p>We will first import the data, and then we will compare the results using k-means and GMM. We follow these steps:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p209"> Import all the libraries and the dataset: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p210"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import pandas as pd&#13;
data = pd.read_csv('vehicle.csv')&#13;
import matplotlib.pyplot as plt</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p211"><span class="faux-ol-li-counter">2. </span> Drop any NA from the dataset: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p212"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">data = data.dropna()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p213"><span class="faux-ol-li-counter">3. </span> Fit a <code>kmeans</code> algorithm. We are keeping the number of clusters as 5. Please note that we are not saying that this is an ideal number of clusters. The number of clusters is only for illustrative purposes. We declare a variable k-means and then use five clusters. The dataset is fit next: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p214"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from sklearn.cluster import KMeans&#13;
kmeans = KMeans(n_clusters=5)&#13;
kmeans.fit(data)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p215"><span class="faux-ol-li-counter">4. </span> Plot the clusters. First, a prediction is made on the dataset, and then the values are added to the data frame as a new column. The data is then plotted with different colors representing different clusters. The print version of the book will not show the different colors, but the output of the Python code will. The same output is available in the GitHub repository. </li> &#13;
  </ol> &#13;
  <div class="readable-text list-body-item" id="p216"> &#13;
   <p>The output is as follows (see figure 5.22):</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p217"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">pred = kmeans.predict(data)&#13;
frame = pd.DataFrame(data)&#13;
frame['cluster'] = pred&#13;
&#13;
color=['red','blue','orange', 'brown', 'green']&#13;
for k in range(0,5):&#13;
    data = frame[frame["cluster"]==k]&#13;
    plt.scatter(data["compactness"],data["circularity"],c=color[k])&#13;
plt.show()<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p218">  &#13;
   <img alt="figure" src="../Images/CH05_F22_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.22</span> Outcome of plotting the clusters after fitting the <code>kmeans</code> algorithm</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p219"><span class="faux-ol-li-counter">5. </span> Fit a GMM model. Note that the code is the same as the k-means algorithm, only the algorithm’s name has changed from k-means to <code>GaussianMixture</code>: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p220"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from sklearn.mixture import GaussianMixture&#13;
gmm = GaussianMixture(n_components=5)&#13;
gmm.fit(data)&#13;
&#13;
#predictions from gmm&#13;
labels = gmm.predict(data)&#13;
frame = pd.DataFrame(data)&#13;
frame['cluster'] = labels</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p221"><span class="faux-ol-li-counter">6. </span> Plot the results. The output is as follows (figure 5.23): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p222"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">color=['red','blue','orange', 'brown', 'green']&#13;
for k in range(0,5):&#13;
    data = frame[frame["cluster"]==k]&#13;
    plt.scatter(data["compactness"],data["circularity"],c=color[k])&#13;
plt.show()<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p223">  &#13;
   <img alt="figure" src="../Images/CH05_F23_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.23</span> Outcome of plotting the clusters after fitting a GMM algorithm</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p224"><span class="faux-ol-li-counter">7. </span> Run the code with different values of clusters to observe the difference. In the following plots, the left one is k-means with two clusters, while the right is GMM with two clusters. There are a few points that are classified differently in the two clustering approaches. The print version of the book will not show the different colors, but the output of the Python code will. The same output is available in the GitHub repository, too (see figure 5.24). </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p225"> &#13;
   <img alt="figure" src="../Images/CH05_UN01_Verdhan.png"/> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p226">  &#13;
   <img alt="figure" src="../Images/CH05_F24_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 5.24</span> K-means with two clusters (left) and GMM with two clusters (right) </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p227"> &#13;
   <p>Gaussian distribution is one of the most widely used data distributions used. If we compare k-means and the GMM model, we see that k-means does not consider the normal distribution of the data. The relationship of various data points is also not considered in k-means. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p228"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> K-means is a distance-based algorithm; GMM is a distribution-based algorithm.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p229"> &#13;
   <p>In short, it is advantageous to use GMM models for creating clusters, particularly when we have overlapping datasets. It is a useful technique for financial and price modeling, natural language processing-based solutions, etc. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p230"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">5.7</span> Concluding thoughts</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p231"> &#13;
   <p>In this chapter, we have explored three complex clustering algorithms. You might have felt the mathematical concepts were a bit heavy. They are indeed, but they provide a deeper understanding of the process. These algorithms are not necessarily the best ones for every problem. Ideally, in a real-world business problem, we start with classical clustering algorithms (k-means, hierarchical, and DBSCAN). If we do not get acceptable results, we can try the more complex algorithms. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p232"> &#13;
   <p>Many times, a data science problem is equated to the choice of algorithm, which it is not. The algorithm is certainly an important ingredient of the entire solution, but it is not the only one. In real-world datasets, there are a lot of variables, and the amount of data is also quite high. The data has a lot of noise. We should account for all of these factors when we shortlist an algorithm. Algorithm maintenance and refreshing are also considerations. All of these aspects are covered in detail in the last chapter of the book.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p233"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">5.8</span> Practical next steps and suggested readings</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p234"> &#13;
   <p>The following provides suggestions for what to do next and offers some helpful reading:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p235"> In chapter 2, we did clustering using various techniques. Use the datasets from there and perform spectral clustering, GMM, and FCM clustering to compare the results. Datasets provided at the end of chapter 2 can be used for clustering. </li> &#13;
   <li class="readable-text" id="p236"> Get the credit card dataset for clustering from Kaggle (<a href="https://mng.bz/oKwd">https://mng.bz/oKwd</a>) and data from the famous Iris dataset, which we used earlier (<a href="https://www.kaggle.com/uciml/iris">https://www.kaggle.com/uciml/iris</a>). </li> &#13;
   <li class="readable-text" id="p237"> Refer to the book <em>Computational Network Science</em> by Henry Hexmoor to study the mathematical concepts. </li> &#13;
   <li class="readable-text buletless-item" id="p238"> Get spectral clustering papers from the following links and study them: &#13;
    <ul> &#13;
     <li> On spectral clustering: analysis and an algorithm: <a href="https://mng.bz/nRwa">https://mng.bz/nRwa</a> </li> &#13;
     <li> Spectral clustering with eigenvalue selection: <a href="https://mng.bz/vKw7">https://mng.bz/vKw7</a> </li> &#13;
     <li> The mathematics behind spectral clustering and the equivalence to principal component analysis: <a href="https://arxiv.org/pdf/2103.00733v1.pdf">https://arxiv.org/pdf/2103.00733v1.pdf</a> </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text buletless-item" id="p239"> Get GMM papers from the following links and explore them: &#13;
    <ul> &#13;
     <li> “GMM Estimation for High Dimensional Panel Data Models”: <a href="https://mng.bz/4agw">https://mng.bz/4agw</a> </li> &#13;
     <li> “Application of Compound Gaussian Mixture Model in the Data Stream”: <a href="https://ieeexplore.ieee.org/document/5620507">https://ieeexplore.ieee.org/document/5620507</a> </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text buletless-item" id="p240"> Get FCM papers from the following links and study them: &#13;
    <ul> &#13;
     <li> “FCM: The Fuzzy c-Means <em>Clustering</em> Algorithm”: <a href="https://mng.bz/QDXG">https://mng.bz/QDXG</a> </li> &#13;
     <li> A Survey on Fuzzy c-Means Clustering Techniques: <a href="https://www.ijedr.org/papers/IJEDR1704186.pdf">https://www.ijedr.org/papers/IJEDR1704186.pdf</a> </li> &#13;
     <li> “Implementation of Fuzzy C-Means and Possibilistic C-Means Clustering Algorithms, Cluster Tendency Analysis and Cluster Validation”: <a href="https://arxiv.org/pdf/1809.08417.pdf">https://arxiv.org/pdf/1809.08417.pdf</a> </li> &#13;
    </ul> </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p241"> &#13;
   <h2 class=" readable-text-h2">Summary</h2> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p242"> Spectral clustering focuses on data point affinity rather than location for clustering. It works well with complex data shapes where traditional algorithms like k-means may not suffice. </li> &#13;
   <li class="readable-text" id="p243"> Spectral clustering utilizes graph theory and connectivity, relying on eigenvalues, the Laplacian matrix, and the affinity matrix. </li> &#13;
   <li class="readable-text" id="p244"> The process includes calculating degree, adjacency, Laplacian matrices, and the Fielder vector for clustering. </li> &#13;
   <li class="readable-text" id="p245"> K-means clustering uses centroids, whereas spectral clustering’s focus is on connectivity and data point similarities. </li> &#13;
   <li class="readable-text" id="p246"> Spectral clustering can require substantial computational resources due to matrix operations and is suitable for smaller datasets. </li> &#13;
   <li class="readable-text" id="p247"> Fuzzy clustering allows data points to belong to multiple clusters, introducing “membership” for data items. </li> &#13;
   <li class="readable-text" id="p248"> FCM is a key algorithm in fuzzy clustering, utilizing membership degrees and controlling fuzziness through hyperparameter <em>m</em>. </li> &#13;
   <li class="readable-text" id="p249"> GMM employs Gaussian distributions for soft clustering, factoring in dataset covariance. </li> &#13;
   <li class="readable-text" id="p250"> GMM is suitable for overlapping datasets and considers the relationship between data points, unlike k-means. </li> &#13;
   <li class="readable-text" id="p251"> The EM technique is used in GMM to estimate parameters iteratively. </li> &#13;
   <li class="readable-text" id="p252"> GMM models are advantageous for financial modeling, natural language processing, and cases with overlapping data. </li> &#13;
   <li class="readable-text" id="p253"> Fuzzy and GMM are soft clustering methods, allowing detailed membership and probability assignment to data points. </li> &#13;
   <li class="readable-text" id="p254"> Spectral clustering supports applications in image segmentation, speech analysis, and text analytics without assuming data shape constraints. </li> &#13;
  </ul>&#13;
 </body></html>