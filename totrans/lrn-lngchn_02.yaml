- en: 'Chapter 2\. RAG Part I: Indexing Your Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned about the important building blocks used
    to create an LLM application using LangChain. You also built a simple AI chatbot
    consisting of a prompt sent to the model and the output generated by the model.
    But there are major limitations to this simple chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if your use case requires knowledge that the model wasn’t trained on?
    For example, let’s say you want to use AI to ask questions about a company, but
    the information is contained in a private PDF or other type of document. While
    we’ve seen model providers enriching their training datasets to include more and
    more of the world’s public information (no matter what format it is stored in),
    two major limitations continue to exist in LLM’s knowledge corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: Private data
  prefs: []
  type: TYPE_NORMAL
- en: Information that isn’t publicly available is, by definition, not included in
    the training data of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Current events
  prefs: []
  type: TYPE_NORMAL
- en: Training an LLM is a costly and time-consuming process that can span multiple
    years, with data-gathering being one of the first steps. This results in what
    is called the knowledge cutoff, or a date beyond which the LLM has no knowledge
    of real-world events; usually this would be the date the training set was finalized.
    This can be anywhere from a few months to a few years into the past, depending
    on the model in question.
  prefs: []
  type: TYPE_NORMAL
- en: In either case, the model will most likely hallucinate (find misleading or false
    information) and respond with inaccurate information. Adapting the prompt won’t
    resolve the issue either because it relies on the model’s current knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Goal: Picking Relevant Context for LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If the only private/current data you needed for your LLM use case was one to
    two pages of text, this chapter would be a lot shorter: all you’d need to make
    that information available to the LLM is to include that entire text in every
    single prompt you sent to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: The challenge in making data available to LLMs is first and foremost a quantity
    problem. You have more information than can fit in each prompt you send to the
    LLM. Which small subset of your large collection of text do you include each time
    you call the model? Or in other words, how do you pick (with the aid of the model)
    which text is most relevant to answer each question?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter and the next, you’ll learn how to overcome this challenge in
    two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Indexing* your documents, that is, preprocessing them in a way where your
    application can easily find the most relevant ones for each question'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Retrieving* this external data from the index and using it as *context* for
    the LLM to generate an accurate output based on your data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This chapter focuses on indexing, the first step, which involves preprocessing
    your documents into a format that can be understood and searched with LLMs. This
    technique is called *retrieval-augmented generation* (RAG). But before we begin,
    let’s discuss *why* your documents require preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume you would like to use LLMs to analyze the financial performance
    and risks in [Tesla’s 2022 annual report](https://oreil.ly/Bp51E), which is stored
    as text in PDF format. Your goal is to be able to ask a question like “What key
    risks did Tesla face in 2022?” and get a humanlike response based on context from
    the risk factors section of the document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Breaking it down, there are four key steps (shown in [Figure 2-1](#ch02_figure_1_1736545662484110))
    that you’d need to take in order to achieve this goal:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the text from the document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the text into manageable chunks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the text into numbers that computers can understand.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store these number representations of your text somewhere that makes it easy
    and fast to retrieve the relevant sections of your document to answer a given
    question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A diagram of a diagram  Description automatically generated](assets/lelc_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Four key steps to preprocess your documents for LLM usage
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 2-1](#ch02_figure_1_1736545662484110) illustrates the flow of this
    preprocessing and transformation of your documents, a process known as ingestion.
    *Ingestion* is simply the process of converting your documents into numbers that
    computers can understand and analyze, and storing them in a special type of database
    for efficient retrieval. These numbers are formally known as *embeddings*, and
    this special type of database is known as a *vector store.* Let’s look a little
    more closely at what embeddings are and why they’re important, starting with something
    simpler than LLM-powered embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Embeddings: Converting Text to Numbers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Embedding* refers to representing text as a (long) sequence of numbers. This
    is a lossy representation—that is, you can’t recover the original text from these
    number sequences, so you usually store both the original text and this numeric
    representation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, why bother? Because you gain the flexibility and power that comes with
    working with numbers: you can do math on words! Let’s see why that’s exciting.'
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings Before LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Long before LLMs, computer scientists were using embeddings—for instance, to
    enable full-text search capabilities in websites or to classify emails as spam.
    Let’s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take these three sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What a sunny day.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Such bright skies today.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: I haven’t seen a sunny day in weeks.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'List all unique words in them: *what*, *a*, *sunny*, *day*, *such*, *bright*,
    and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each sentence, go word by word and assign the number 0 if not present, 1
    if used once in the sentence, 2 if present twice, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Table 2-1](#ch02_table_1_1736545662489691) shows the result.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Word embeddings for three sentences
  prefs: []
  type: TYPE_NORMAL
- en: '| Word | What a sunny day. | Such bright skies today. | I haven’t seen a sunny
    day in weeks. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *what* | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| *a* | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *sunny* | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *day* | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *such* | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| *bright* | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| *skies* | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| *today* | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| *I* | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *haven’t* | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *seen* | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *in* | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *weeks* | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: In this model, the embedding for *I haven’t seen a sunny day in weeks* is the
    sequence of numbers *0 1 1 1 0 0 0 0 1 1 1 1 1*. This is called the *bag-of-words*
    model, and these embeddings are also called *sparse embeddings* (or sparse vectors—*vector*
    is another word for a sequence of numbers), because a lot of the numbers will
    be 0\. Most English sentences use only a very small subset of all existing English
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can successfully use this model for:'
  prefs: []
  type: TYPE_NORMAL
- en: Keyword search
  prefs: []
  type: TYPE_NORMAL
- en: You can find which documents contain a given word or words.
  prefs: []
  type: TYPE_NORMAL
- en: Classification of documents
  prefs: []
  type: TYPE_NORMAL
- en: You can calculate embeddings for a collection of examples previously labeled
    as email spam or not spam, average them out, and obtain average word frequencies
    for each of the classes (spam or not spam). Then, each new document is compared
    to those averages and classified accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The limitation here is that the model has no awareness of meaning, only of the
    actual words used. For instance, the embeddings for *sunny day* and *bright skies*
    look very different. In fact they have no words in common, even though we know
    they have similar meaning. Or, in the email classification problem, a would-be
    spammer can trick the filter by replacing common “spam words” with their synonyms.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll see how semantic embeddings address this limitation
    by using numbers to represent the meaning of the text, instead of the exact words
    found in the text.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-Based Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re going to skip over all the ML developments that came in between and jump
    straight to LLM-based embeddings. Just know that there was a gradual evolution
    from the simple method outlined in the previous section to the sophisticated method
    described in this one.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of embedding models as an offshoot from the training process of
    LLMs. If you remember from the [Preface](preface01.html#pr01_preface_1736545679069216),
    the LLM training process (learning from vast amounts of written text) enables
    LLMs to complete a prompt (or input) with the most appropriate continuation (output).
    This capability stems from an understanding of the meaning of words and sentences
    in the context of the surrounding text, learned from how words are used together
    in the training texts. This *understanding* of the meaning (or semantics) of the
    prompt can be extracted as a numeric representation (or embedding) of the input
    text, and can be used directly for some very interesting use cases too.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, most embedding models are trained for that purpose alone, following
    somewhat similar architectures and training processes as LLMs, as that is more
    efficient and results in higher-quality embeddings.^([1](ch02.html#id478))
  prefs: []
  type: TYPE_NORMAL
- en: An *embedding model* then is an algorithm that takes a piece of text and outputs
    a numerical representation of its meaning—technically, a long list of floating-point
    (decimal) numbers, usually somewhere between 100 and 2,000 numbers, or *dimensions*.
    These are also called *dense* embeddings, as opposed to the *sparse* embeddings
    of the previous section, as here usually all dimensions are different from 0.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Different models produce different numbers and different sizes of lists. All
    of these are specific to each model; that is, even if the size of the lists matches,
    you cannot compare embeddings from different models. Combining embeddings from
    different models should always be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Embeddings Explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider these three words: *lion*, *pet*, and *dog*. Intuitively, which pair
    of these words share similar characteristics to each other at first glance? The
    obvious answer is *pet* and *dog*. But computers do not have the ability to tap
    into this intuition or nuanced understanding of the English language. In order
    for a computer to differentiate between a lion, pet, or dog, you need to be able
    to translate them into the language of computers, which is numbers*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-2](#ch02_figure_2_1736545662484146) illustrates converting each word
    into hypothetical number representations that retain their meaning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram of circles  Description automatically generated with
    medium confidence](assets/lelc_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Semantic representations of words
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 2-2](#ch02_figure_2_1736545662484146) shows each word alongside its
    corresponding semantic embedding. Note that the numbers themselves have no particular
    meaning, but instead the sequences of numbers for two words (or sentences) that
    are close in meaning should be *closer* than those of unrelated words. As you
    can see, each number is a *floating-point value*, and each of them represents
    a semantic *dimension*. Let’s see what we mean by *closer*:'
  prefs: []
  type: TYPE_NORMAL
- en: If we plot these vectors in a three-dimensional space, it could look like [Figure 2-3](#ch02_figure_3_1736545662484170).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a dog and a dog  Description automatically generated](assets/lelc_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Plot of word vectors in a multidimensional space
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 2-3](#ch02_figure_3_1736545662484170) shows the *pet* and *dog* vectors
    are closer to each other in distance than the *lion* plot. We can also observe
    that the angles between each plot varies depending on how similar they are. For
    example, the words *pet* and *lion* have a wider angle between one another than
    the *pet* and *dog* do, indicating more similarities shared by the latter word
    pairs. The narrower the angle or shorter the distance between two vectors, the
    closer their similarities.'
  prefs: []
  type: TYPE_NORMAL
- en: One effective way to calculate the degree of similarity between two vectors
    in a multidimensional space is called cosine similarity. *Cosine similarity* computes
    the dot product of vectors and divides it by the product of their magnitudes to
    output a number between –1 and 1, where 0 means the vectors share no correlation,
    –1 means they are absolutely dissimilar, and 1 means they are absolutely similar.
    So, in the case of our three words here, the cosine similarity between *pet* and
    *dog* could be 0.75, but between *pet* and *lion* it might be 0.1.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to convert sentences into embeddingsthat capture semantic meaning
    and then perform calculations to find semantic similarities between different
    sentences enables us to get an LLM to find the most relevant documents to answer
    questions about a large body of text like our Tesla PDF document. Now that you
    understand the big picture, let’s revisit the first step (indexing) of preprocessing
    your document.
  prefs: []
  type: TYPE_NORMAL
- en: Converting Your Documents into Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned at the beginning of the chapter, the first step in preprocessing
    your document is to convert it to text. In order to achieve this, you would need
    to build logic to parse and extract the document with minimal loss of quality.
    Fortunately, LangChain provides *document loaders* that handle the parsing logic
    and enable you to “load” data from various sources into a `Document` class that
    consists of text and associated metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider a simple *.txt* file. You can simply import a LangChain
    `TextLoader` class to extract the text, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code block assumes that you have a file named `test.txt` in your
    current directory. Usage of all LangChain document loaders follows a similar pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: Start by picking the loader for your type of document from the long list of
    [integrations](https://oreil.ly/iLJ33).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an instance of the loader in question, along with any parameters to configure
    it, including the location of your documents (usually a filesystem path or web
    address).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the documents by calling `load()`, which returns a list of documents ready
    to pass to the next stage (more on that soon).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aside from *.txt* files, LangChain provides document loaders for other popular
    file types including *.csv*, *.json*, and Markdown, alongside integrations with
    popular platforms such as Slack and Notion.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can use `WebBaseLoader` to load HTML from web URLs and parse
    it to text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the beautifulsoup4 package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case of our Tesla PDF use case, we can utilize LangChain’s `PDFLoader`
    to extract text from the PDF document:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The text has been extracted from the PDF document and stored in the `Document`
    class. But there’s a problem. The loaded document is over 100,000 characters long,
    so it won’t fit into the context window of the vast majority of LLMs or embedding
    models. In order to overcome this limitation, we need to split the `Document`
    into manageable chunks of text that we can later convert into embeddings and semantically
    search, bringing us to the second step (retrieving).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLMs and embedding models are designed with a hard limit on the size of input
    and output tokens they can handle. This limit is usually called *context window*,
    and usually applies to the combination of input and output; that is, if the context
    window is 100 (we’ll talk about units in a second), and your input measures 90,
    the output can be at most of length 10\. Context windows are usually measured
    in number of tokens, for instance 8,192 tokens. Tokens, as mentioned in the [Preface](preface01.html#pr01_preface_1736545679069216),
    are a representation of text as numbers, with each token usually covering between
    three and four characters of English text.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting Your Text into Chunks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At first glance it may seem straightforward to split a large body of text into
    chunks, but keeping *semantically* related (related by meaning) chunks of text
    together is a complex process. To make it easier to split large documents into
    small, but still meaningful, pieces of text, LangChain provides `RecursiveCharacterTextSplitter`,
    which does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a list of separators, in order of importance. By default these are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The paragraph separator: `\n\n`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The line separator: `\n`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The word separator: space character'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To respect the given chunk size, for instance, 1,000 characters, start by splitting
    up paragraphs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For any paragraph longer than the desired chunk size, split by the next separator:
    lines. Continue until all chunks are smaller than the desired length, or there
    are no additional separators to try.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Emit each chunk as a `Document`, with the metadata of the original document
    passed in and additional information about the position in the original document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the documents created by the document loader are split
    into chunks of 1,000 characters each, with some overlap between chunks of 200
    characters to maintain some context. The result is also a list of documents, where
    each document is up to 1,000 characters in length, split along the natural divisions
    of written text—paragraphs, new lines and finally, words. This uses the structure
    of the text to keep each chunk a consistent, readable snippet of text.
  prefs: []
  type: TYPE_NORMAL
- en: '`RecursiveCharacterTextSplitter` can also be used to split code languages and
    Markdown into semantic chunks. This is done by using keywords specific to each
    language as the separators, which ensures, for instance, the body of each function
    is kept in the same chunk, instead of split between several. Usually, as programming
    languages have more structure than written text, there’s less need to use overlap
    between the chunks. LangChain contains separators for a number of popular languages,
    such as Python, JS, Markdown, HTML, and many more. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we’re still using `RecursiveCharacterTextSplitter` as before, but
    now we’re creating an instance of it for a specific language, using the `from_language`
    method. This one accepts the name of the language, and the usual parameters for
    chunk size, and so on. Also notice we are now using the method `create_documents`,
    which accepts a list of strings, rather than the list of documents we had before.
    This method is useful when the text you want to split doesn’t come from a document
    loader, so you have only the raw text strings.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the optional second argument to `create_documents` in order
    to pass a list of metadata to associate with each text string. This metadata list
    should have the same length as the list of strings and will be used to populate
    the metadata field of each `Document` returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an example for Markdown text, using the metadata argument as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]bash'
  prefs: []
  type: TYPE_NORMAL
- en: pip install langchain
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]bash'','
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata={"source": "https://www.langchain.com"}),'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Document(page_content='pip install langchain',
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata={"source": "https://www.langchain.com"}),'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Document(page_content='[PRE17]
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice two things:'
  prefs: []
  type: TYPE_NORMAL
- en: The text is split along the natural stopping points in the Markdown document;
    for instance, the heading goes into one chunk, the line of text under it in a
    separate chunk, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metadata we passed in the second argument is attached to each resulting
    document, which allows you to track, for instance, where the document came from
    and where you can go to see the original.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating Text Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LangChain also has an `Embeddings` class designed to interface with text embedding
    models—including OpenAI, Cohere, and Hugging Face—and generate vector representations
    of text. This class provides two methods: one for embedding documents and one
    for embedding a query. The former takes a list of text strings as input, while
    the latter takes a single text string.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of embedding a document using [OpenAI’s embedding model](https://oreil.ly/9tnzQ):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Notice that you can embed multiple documents at the same time; you should prefer
    this to embedding them one at a time, as it will be more efficient (due to how
    these models are constructed). You get back a list containing multiple lists of
    numbers—each inner list is a vector or embedding, as explained in an earlier section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s see an end-to-end example using the three capabilities we’ve seen
    so far:'
  prefs: []
  type: TYPE_NORMAL
- en: Document loaders, to convert any document to plain text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text splitters, to split each large document into many smaller ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings models, to create a numeric representation of the meaning of each
    split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Once you’ve generated embeddings from your documents, the next step is to store
    them in a special database known as a vector store.
  prefs: []
  type: TYPE_NORMAL
- en: Storing Embeddings in a Vector Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this chapter, we discussed the cosine similarity calculation to measure
    the similarity between vectors in a vector space. A vector store is a database
    designed to store vectors and perform complex calculations, like cosine similarity,
    efficiently and quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional databases that specialize in storing structured data (such
    as JSON documents or data conforming to the schema of a relational database),
    vector stores handle unstructured data, including text and images. Like traditional
    databases, vector stores are capable of performing create, read, update, delete
    (CRUD), and search operations.
  prefs: []
  type: TYPE_NORMAL
- en: Vector stores unlock a wide variety of use cases, including scalable applications
    that utilize AI to answer questions about large documents, as illustrated in [Figure 2-4](#ch02_figure_4_1736545662484192).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a store  Description automatically generated](assets/lelc_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Loading, embedding, storing, and retrieving relevant docs from
    a vector store
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 2-4](#ch02_figure_4_1736545662484192) illustrates how document embeddings
    are inserted into the vector store and how later, when a query is sent, similar
    embeddings are retrieved from the vector store.'
  prefs: []
  type: TYPE_NORMAL
- en: Currently, there is an abundance of vector store providers to choose from, each
    specializing in different capabilities. Your selection should depend on the critical
    requirements of your application, including multitenancy, metadata filtering capabilities,
    performance, cost, and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although vector stores are niche databases built to manage vector data, there
    are a few disadvantages working with them:'
  prefs: []
  type: TYPE_NORMAL
- en: Most vector stores are relatively new and may not stand the test of time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing and optimizing vector stores can present a relatively steep learning
    curve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing a separate database adds complexity to your application and may drain
    valuable resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortunately, vector store capabilities have recently been extended to PostgreSQL
    (a popular open source relational database) via the `pgvector` extension. This
    enables you to use the same database you’re already familiar with and to power
    both your transactional tables (for instance your users table) as well as your
    vector search tables.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Set Up with PGVector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use Postgres and PGVector you’ll need to follow a few setup steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure you have Docker installed on your computer, following the [instructions
    for your operating system](https://oreil.ly/Gn28O).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following command in your terminal; it will launch a Postgres instance
    in your computer running on port 6024:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Open your docker dashboard containers and you should see a green running status
    next to `pgvector-container`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Save the connection string to use in your code; we’ll need it later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Working with Vector Stores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Picking up where we left off in the previous section on embeddings, now let’s
    see an example of loading, splitting, embedding, and storing a document in PGVector:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we reuse the code from the previous sections to first load the documents
    with the loader and then split them into smaller chunks. Then, we instantiate
    the embeddings model we want to use—in this case, OpenAI’s. Note that you could
    use any other embeddings model supported by LangChain here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have a new line of code, which creates a vector store given documents,
    the embeddings model, and a connection string. This will do a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: Establish a connection to the Postgres instance running in your computer (see
    [“Getting Set Up with PGVector”](#ch02_getting_set_up_with_pgvector_1736545662501989).)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run any setup necessary, such as creating tables to hold your documents and
    vectors, if this is the first time you’re running it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create embeddings for each document you passed in, using the model you chose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store the embeddings, the document’s metadata, and the document’s text content
    in Postgres, ready to be searched.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see what it looks like to search documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This method will find the most relevant documents (which you previously indexed),
    by following this process:'
  prefs: []
  type: TYPE_NORMAL
- en: The search query—in this case, the word `query`—will be sent to the embeddings
    model to retrieve its embedding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, it will run a query on Postgres to find the N (in this case 4) previously
    stored embeddings that are most similar to your query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, it will fetch the text content and metadata that relates to each of
    those embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model can now return a list of `Document` sorted by how similar they are
    to the query—the most similar first, the second most similar after, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also add more documents to an existing database. Let’s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `add_documents` method we’re using here will follow a similar process to
    `fromDocuments`:'
  prefs: []
  type: TYPE_NORMAL
- en: Create embeddings for each document you passed in, using the model you chose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store the embeddings, the document’s metadata, and the document’s text content
    in Postgres, ready to be searched.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, we are using the optional `ids` argument to assign identifiers
    to each document, which allows us to update or delete them later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of the delete operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This removes the second document inserted by using its Universally Unique Identifier
    (UUID). Now let’s see how to do this in a more systematic way.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking Changes to Your Documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key challenges with working with vector stores is working with data
    that regularly changes, because changes mean re-indexing. And re-indexing can
    lead to costly recomputations of embeddings and duplications of preexisting content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, LangChain provides an indexing API to make it easy to keep your
    documents in sync with your vector store. The API utilizes a class (`RecordManager`)
    to keep track of document writes into the vector store. When indexing content,
    hashes are computed for each document and the following information is stored
    in `RecordManager`:'
  prefs: []
  type: TYPE_NORMAL
- en: The document hash (hash of both page content and metadata)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source ID (each document should include information in its metadata to determine
    the ultimate source of this document).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the indexing API provides cleanup modes to help you decide how
    to delete existing documents in the vector store. For example, If you’ve made
    changes to how documents are processed before insertion or if source documents
    have changed, you may want to remove any existing documents that come from the
    same source as the new documents being indexed. If some source documents have
    been deleted, you’ll want to delete all existing documents in the vector store
    and replace them with the re-indexed documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The modes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`None` mode does not do any automatic cleanup, allowing the user to manually
    do cleanup of old content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Incremental` and `full` modes delete previous versions of the content if the
    content of the source document or derived documents has changed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Full` mode will additionally delete any documents not included in documents
    currently being indexed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an example of the use of the indexing API with Postgres database set
    up as a record manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: First, you create a record manager, which keeps track of which documents have
    been indexed before. Then you use the `index` function to synchronize your vector
    store with the new list of documents. In this example, we’re using the incremental
    mode, so any documents that have the same ID as previous ones will be replaced
    with the new version.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A basic RAG indexing stage involves naive text splitting and embedding of chunks
    of a given document. However, this basic approach leads to inconsistent retrieval
    results and a relatively high occurrence of hallucinations, especially when the
    data source contains images and tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various strategies to enhance the accuracy and performance of the
    indexing stage. We will cover three of them in the next sections: MultiVectorRetriever,
    RAPTOR, and ColBERT.'
  prefs: []
  type: TYPE_NORMAL
- en: MultiVectorRetriever
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A document that contains a mixture of text and tables cannot be simply split
    by text into chunks and embedded as context: the entire table can be easily lost.
    To solve this problem, we can decouple documents that we want to use for answer
    synthesis, from a reference that we want to use for the retriever. [Figure 2-5](#ch02_figure_5_1736545662484212)
    illustrates how.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot 2024-03-16 at 5.54.55 PM.png](assets/lelc_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. Indexing multiple representations of a single document
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For example, in the case of a document that contains tables, we can first generate
    and embed summaries of table elements, ensuring each summary contains an `id`
    reference to the full raw table. Next, we store the raw referenced tables in a
    separate docstore. Finally, when a user’s query retrieves a table summary, we
    pass the entire referenced raw table as context to the final prompt sent to the
    LLM for answer synthesis. This approach enables us to provide the model with the
    full context of information required to answer the question.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example. First, let’s use the LLM to generate summaries of the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s define the vector store and docstore to store the raw summaries
    and their embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s retrieve the relevant full context document based on a query:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the full implementation in JavaScript:'
  prefs: []
  type: TYPE_NORMAL
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAG systems need to handle lower-level questions that reference specific facts
    found in a single document or higher-level questions that distill ideas that span
    many documents. Handling both types of questions can be a challenge with typical
    k-nearest neighbors (k-NN) retrieval over document chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '*Recursive abstractive processing for tree-organized retrieval* (RAPTOR) is
    an effective strategy that involves creating document summaries that capture higher-level
    concepts, embedding and clustering those documents, and then [summarizing each
    cluster](https://oreil.ly/VdIpJ).^([2](ch02.html#id538)) This is done recursively,
    producing a tree of summaries with increasingly high-level concepts. The summaries
    and initial documents are then indexed together, giving coverage across lower-to-higher-level
    user questions. [Figure 2-6](#ch02_figure_6_1736545662484232) illustrates.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot 2024-03-16 at 6.16.21 PM.png](assets/lelc_0206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. Recursively summarizing documents
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'ColBERT: Optimizing Embeddings'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the challenges of using embeddings models during the indexing stage is
    that they compress text into fixed-length (vector) representations that capture
    the semantic content of the document. Although this compression is useful for
    retrieval, embedding irrelevant or redundant content may lead to hallucinations
    in the final LLM output.
  prefs: []
  type: TYPE_NORMAL
- en: 'One solution to this problem is to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate contextual embeddings for each token in the document and query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate and score similarity between each query token and all document tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sum the maximum similarity score of each query embedding to any of the document
    embeddings to get a score for each document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This results in a granular and effective embedding approach for better retrieval.
    Fortunately, the embedding model called ColBERTembodies the solution to this problem.^([3](ch02.html#id542))
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how we can utilize ColBERT for optimal embedding of our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: By using ColBERT, you can improve the relevancy of retrieved documents used
    as context by the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned how to prepare and preprocess your documents
    for your LLM application using various LangChain’s modules. The document loaders
    enable you to extract text from your data source, the text splitters help you
    split your document into semantically similar chunks, and the embeddings models
    convert your text into vector representations of their meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Separately, vector stores allow you to perform CRUD operations on these embeddings
    alongside complex calculations to compute semantically similar chunks of text.
    Finally, indexing optimization strategies enable your AI app to improve the quality
    of embeddings and perform accurate retrieval of documents that contain semistructured
    data including tables.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580),
    you’ll learn how to efficiently retrieve the most similar chunks of documents
    from your vector store based on your query, provide it as contextthe model can
    see, and then generate an accurate output.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.html#id478-marker)) Arvind Neelakantan et al., [“Text and Code Embeddings
    by Contrastive Pre-Training”](https://oreil.ly/YOVmh), arXiv, January 21, 2022\.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch02.html#id538-marker)) Parth Sarthi et al., [“RAPTOR: Recursive Abstractive
    Processing for Tree-Organized Retrieval”](https://oreil.ly/hS4NB), arXiv, January
    31, 2024\. Paper published at ICLR 2024\.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch02.html#id542-marker)) Keshav Santhanam et al., [“ColBERTv2: Effective
    and Efficient Retrieval via Lightweight Late Interaction”](https://oreil.ly/9spW2),
    arXiv, December 2, 2021.'
  prefs: []
  type: TYPE_NORMAL
