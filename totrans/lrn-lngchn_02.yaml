- en: 'Chapter 2\. RAG Part I: Indexing Your Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：RAG 第一部分：索引你的数据
- en: In the previous chapter, you learned about the important building blocks used
    to create an LLM application using LangChain. You also built a simple AI chatbot
    consisting of a prompt sent to the model and the output generated by the model.
    But there are major limitations to this simple chatbot.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了使用LangChain创建LLM应用程序的重要构建块。你还构建了一个简单的AI聊天机器人，它由发送给模型的提示和模型生成的输出组成。但这个简单的聊天机器人存在重大局限性。
- en: 'What if your use case requires knowledge that the model wasn’t trained on?
    For example, let’s say you want to use AI to ask questions about a company, but
    the information is contained in a private PDF or other type of document. While
    we’ve seen model providers enriching their training datasets to include more and
    more of the world’s public information (no matter what format it is stored in),
    two major limitations continue to exist in LLM’s knowledge corpus:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的用例需要模型未训练过的知识，怎么办？例如，假设你想使用AI来询问一家公司的问题，但这些信息包含在私有的PDF或其他类型的文档中。虽然我们已经看到模型提供商正在丰富他们的训练数据集，以包括越来越多的世界公共信息（无论其存储格式如何），但在LLM的知识库中仍然存在两个主要限制：
- en: Private data
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 私有数据
- en: Information that isn’t publicly available is, by definition, not included in
    the training data of LLMs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，未公开的信息不包括在LLM的训练数据中。
- en: Current events
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的新闻事件
- en: Training an LLM is a costly and time-consuming process that can span multiple
    years, with data-gathering being one of the first steps. This results in what
    is called the knowledge cutoff, or a date beyond which the LLM has no knowledge
    of real-world events; usually this would be the date the training set was finalized.
    This can be anywhere from a few months to a few years into the past, depending
    on the model in question.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个LLM是一个昂贵且耗时的过程，可能需要多年时间，数据收集是第一步之一。这导致了所谓的知识截止点，即LLM对现实世界事件没有知识的一个日期；这通常会是训练集最终确定的日期。这可能是几个月到几年前的日期，具体取决于所讨论的模型。
- en: In either case, the model will most likely hallucinate (find misleading or false
    information) and respond with inaccurate information. Adapting the prompt won’t
    resolve the issue either because it relies on the model’s current knowledge.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，模型很可能会产生幻觉（找到误导性或错误的信息）并给出不准确的信息。调整提示也不会解决这个问题，因为它依赖于模型当前的知识。
- en: 'The Goal: Picking Relevant Context for LLMs'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标：为LLM选择相关上下文
- en: 'If the only private/current data you needed for your LLM use case was one to
    two pages of text, this chapter would be a lot shorter: all you’d need to make
    that information available to the LLM is to include that entire text in every
    single prompt you sent to the model.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要的唯一私有/当前数据只是LLM用例中的一到两页文本，那么本章将会短得多：你只需要将整个文本包含在你发送给模型的每个提示中，就可以使该信息对LLM可用。
- en: The challenge in making data available to LLMs is first and foremost a quantity
    problem. You have more information than can fit in each prompt you send to the
    LLM. Which small subset of your large collection of text do you include each time
    you call the model? Or in other words, how do you pick (with the aid of the model)
    which text is most relevant to answer each question?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使数据对LLM可用性的挑战首先是一个数量问题。你拥有的信息比可以放入你发送给LLM的每个提示中的信息要多。每次调用模型时，你如何在你的大量文本集合中选择一个小的子集？或者换句话说，你如何（在模型的帮助下）选择与每个问题最相关的文本？
- en: 'In this chapter and the next, you’ll learn how to overcome this challenge in
    two steps:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和下一章中，你将学习如何通过两个步骤克服这一挑战：
- en: '*Indexing* your documents, that is, preprocessing them in a way where your
    application can easily find the most relevant ones for each question'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*索引*你的文档，即以你的应用程序可以轻松找到每个问题的最相关文档的方式预处理它们'
- en: '*Retrieving* this external data from the index and using it as *context* for
    the LLM to generate an accurate output based on your data'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*检索*来自索引的外部数据并将其用作LLM的*上下文*，以便根据你的数据生成准确输出的过程'
- en: This chapter focuses on indexing, the first step, which involves preprocessing
    your documents into a format that can be understood and searched with LLMs. This
    technique is called *retrieval-augmented generation* (RAG). But before we begin,
    let’s discuss *why* your documents require preprocessing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍索引，这是第一步，涉及将你的文档预处理成LLM可以理解和搜索的格式。这种技术称为*检索增强生成*（RAG）。但在我们开始之前，让我们讨论一下*为什么*你的文档需要预处理。
- en: Let’s assume you would like to use LLMs to analyze the financial performance
    and risks in [Tesla’s 2022 annual report](https://oreil.ly/Bp51E), which is stored
    as text in PDF format. Your goal is to be able to ask a question like “What key
    risks did Tesla face in 2022?” and get a humanlike response based on context from
    the risk factors section of the document.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你希望使用LLM来分析特斯拉2022年年度报告中的财务表现和风险，该报告存储为PDF格式的文本。你的目标是能够提出像“特斯拉在2022年面临了哪些关键风险？”这样的问题，并基于文档风险因素部分的内容得到一个类似人类的回答。[特斯拉2022年年度报告](https://oreil.ly/Bp51E)。
- en: 'Breaking it down, there are four key steps (shown in [Figure 2-1](#ch02_figure_1_1736545662484110))
    that you’d need to take in order to achieve this goal:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分解来看，有四个关键步骤（如图2-1所示）需要采取以实现这一目标：
- en: Extract the text from the document.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文档中提取文本。
- en: Split the text into manageable chunks.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本分割成可管理的块。
- en: Convert the text into numbers that computers can understand.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本转换为计算机可以理解的数字。
- en: Store these number representations of your text somewhere that makes it easy
    and fast to retrieve the relevant sections of your document to answer a given
    question.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些文本的数字表示存储在某个地方，以便快速检索文档的相关部分以回答给定的问题。
- en: '![A diagram of a diagram  Description automatically generated](assets/lelc_0201.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![一个图表的图表  描述自动生成](assets/lelc_0201.png)'
- en: Figure 2-1\. Four key steps to preprocess your documents for LLM usage
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 预处理文档以供LLM使用的四个关键步骤
- en: '[Figure 2-1](#ch02_figure_1_1736545662484110) illustrates the flow of this
    preprocessing and transformation of your documents, a process known as ingestion.
    *Ingestion* is simply the process of converting your documents into numbers that
    computers can understand and analyze, and storing them in a special type of database
    for efficient retrieval. These numbers are formally known as *embeddings*, and
    this special type of database is known as a *vector store.* Let’s look a little
    more closely at what embeddings are and why they’re important, starting with something
    simpler than LLM-powered embeddings.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-1](#ch02_figure_1_1736545662484110)说明了这种预处理和转换文档的流程，这个过程被称为摄取。*摄取*简单来说就是将你的文档转换为计算机可以理解和分析的数字，并将它们存储在一种特殊类型的数据库中以便高效检索。这些数字正式上被称为*嵌入*，而这种特殊类型的数据库被称为*向量存储器*。让我们更详细地了解一下嵌入是什么以及为什么它们很重要，从比LLM驱动的嵌入更简单的东西开始。'
- en: 'Embeddings: Converting Text to Numbers'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入：将文本转换为数字
- en: '*Embedding* refers to representing text as a (long) sequence of numbers. This
    is a lossy representation—that is, you can’t recover the original text from these
    number sequences, so you usually store both the original text and this numeric
    representation.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌入*指的是将文本表示为（长）数字序列。这是一种有损表示——也就是说，你不能从这些数字序列中恢复原始文本，因此你通常存储原始文本和这种数字表示。'
- en: 'So, why bother? Because you gain the flexibility and power that comes with
    working with numbers: you can do math on words! Let’s see why that’s exciting.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么要费这个功夫呢？因为当你与数字打交道时，你会获得灵活性和力量：你可以在单词上做数学运算！让我们看看为什么这会令人兴奋。
- en: Embeddings Before LLMs
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM出现之前的嵌入
- en: 'Long before LLMs, computer scientists were using embeddings—for instance, to
    enable full-text search capabilities in websites or to classify emails as spam.
    Let’s see an example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM出现之前，计算机科学家就已经在使用嵌入技术了——例如，为了在网站上实现全文搜索功能或将电子邮件分类为垃圾邮件。让我们看看一个例子：
- en: 'Take these three sentences:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取这三个句子：
- en: What a sunny day.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多么晴朗的一天啊。
- en: Such bright skies today.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 今天的天空如此明亮。
- en: I haven’t seen a sunny day in weeks.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好几周没见过晴朗的天气了。
- en: 'List all unique words in them: *what*, *a*, *sunny*, *day*, *such*, *bright*,
    and so on.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出它们中的所有唯一单词：*what*，*a*，*sunny*，*day*，*such*，*bright*，等等。
- en: For each sentence, go word by word and assign the number 0 if not present, 1
    if used once in the sentence, 2 if present twice, and so on.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个句子，逐个单词地检查，如果单词不存在，则分配数字0，如果句子中只使用一次，则分配数字1，如果出现两次，则分配数字2，依此类推。
- en: '[Table 2-1](#ch02_table_1_1736545662489691) shows the result.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-1](#ch02_table_1_1736545662489691)显示了结果。'
- en: Table 2-1\. Word embeddings for three sentences
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1\. 三个句子的词嵌入
- en: '| Word | What a sunny day. | Such bright skies today. | I haven’t seen a sunny
    day in weeks. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Word | What a sunny day. | Such bright skies today. | I haven’t seen a sunny
    day in weeks. |'
- en: '| --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *what* | 1 | 0 | 0 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| *what* | 1 | 0 | 0 |'
- en: '| *a* | 1 | 0 | 1 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| *a* | 1 | 0 | 1 |'
- en: '| *sunny* | 1 | 0 | 1 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| *sunny* | 1 | 0 | 1 |'
- en: '| *day* | 1 | 0 | 1 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| *day* | 1 | 0 | 1 |'
- en: '| *such* | 0 | 1 | 0 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| *such* | 0 | 1 | 0 |'
- en: '| *bright* | 0 | 1 | 0 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| *bright* | 0 | 1 | 0 |'
- en: '| *skies* | 0 | 1 | 0 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| *skies* | 0 | 1 | 0 |'
- en: '| *today* | 0 | 1 | 0 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| *today* | 0 | 1 | 0 |'
- en: '| *I* | 0 | 0 | 1 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| *I* | 0 | 0 | 1 |'
- en: '| *haven’t* | 0 | 0 | 1 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| *haven’t* | 0 | 0 | 1 |'
- en: '| *seen* | 0 | 0 | 1 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| *seen* | 0 | 0 | 1 |'
- en: '| *in* | 0 | 0 | 1 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| *in* | 0 | 0 | 1 |'
- en: '| *weeks* | 0 | 0 | 1 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| *weeks* | 0 | 0 | 1 |'
- en: In this model, the embedding for *I haven’t seen a sunny day in weeks* is the
    sequence of numbers *0 1 1 1 0 0 0 0 1 1 1 1 1*. This is called the *bag-of-words*
    model, and these embeddings are also called *sparse embeddings* (or sparse vectors—*vector*
    is another word for a sequence of numbers), because a lot of the numbers will
    be 0\. Most English sentences use only a very small subset of all existing English
    words.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在此模型中，*我已经好几个星期没见过晴天了*的嵌入是数字序列*0 1 1 1 0 0 0 0 1 1 1 1 1*。这被称为*词袋模型*，这些嵌入也被称为*稀疏嵌入*（或稀疏向量——*向量*是数字序列的另一种说法），因为很多数字将是0。大多数英语句子只使用所有现有英语单词的一小部分。
- en: 'You can successfully use this model for:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用此模型成功进行：
- en: Keyword search
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词搜索
- en: You can find which documents contain a given word or words.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以找到包含给定单词或单词的文档。
- en: Classification of documents
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 文档分类
- en: You can calculate embeddings for a collection of examples previously labeled
    as email spam or not spam, average them out, and obtain average word frequencies
    for each of the classes (spam or not spam). Then, each new document is compared
    to those averages and classified accordingly.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为之前标记为垃圾邮件或非垃圾邮件的示例集合计算嵌入，取平均值，并得到每个类别（垃圾邮件或非垃圾邮件）的平均词频。然后，每个新文档与这些平均值进行比较，并根据结果进行分类。
- en: The limitation here is that the model has no awareness of meaning, only of the
    actual words used. For instance, the embeddings for *sunny day* and *bright skies*
    look very different. In fact they have no words in common, even though we know
    they have similar meaning. Or, in the email classification problem, a would-be
    spammer can trick the filter by replacing common “spam words” with their synonyms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的限制是模型没有对意义的认识，只有实际使用的词语。例如，*晴天*和*晴朗的天空*的嵌入看起来非常不同。事实上，它们没有共同的词语，尽管我们知道它们有相似的意义。或者，在电子邮件分类问题中，一个潜在的垃圾邮件发送者可以通过用同义词替换常见的“垃圾邮件单词”来欺骗过滤器。
- en: In the next section, we’ll see how semantic embeddings address this limitation
    by using numbers to represent the meaning of the text, instead of the exact words
    found in the text.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到语义嵌入如何通过使用数字来表示文本的意义，而不是文本中找到的确切词语，来解决这一限制。
- en: LLM-Based Embeddings
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于LLM的嵌入
- en: We’re going to skip over all the ML developments that came in between and jump
    straight to LLM-based embeddings. Just know that there was a gradual evolution
    from the simple method outlined in the previous section to the sophisticated method
    described in this one.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将跳过所有介于其中的ML（机器学习）发展，直接跳到基于LLM的嵌入。只需知道，从上一节中概述的简单方法到本节中描述的复杂方法，有一个逐步的演变过程。
- en: You can think of embedding models as an offshoot from the training process of
    LLMs. If you remember from the [Preface](preface01.html#pr01_preface_1736545679069216),
    the LLM training process (learning from vast amounts of written text) enables
    LLMs to complete a prompt (or input) with the most appropriate continuation (output).
    This capability stems from an understanding of the meaning of words and sentences
    in the context of the surrounding text, learned from how words are used together
    in the training texts. This *understanding* of the meaning (or semantics) of the
    prompt can be extracted as a numeric representation (or embedding) of the input
    text, and can be used directly for some very interesting use cases too.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将嵌入模型视为LLM（大型语言模型）训练过程的衍生。如果您还记得[前言](preface01.html#pr01_preface_1736545679069216)，LLM的训练过程（从大量书面文本中学习）使LLM能够根据上下文理解词语和句子的意义，从而完成提示（或输入）的最合适的延续（输出）。这种能力源于从训练文本中词语如何共同使用中学习到的对上下文中词语和句子意义的理解。这种对提示（或语义）意义的*理解*可以提取为输入文本的数值表示（或嵌入），并且可以直接用于一些非常有趣的应用场景。
- en: In practice, most embedding models are trained for that purpose alone, following
    somewhat similar architectures and training processes as LLMs, as that is more
    efficient and results in higher-quality embeddings.^([1](ch02.html#id478))
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，大多数嵌入模型都是为了这个目的而训练的，它们遵循与LLM相似的一些架构和训练过程，因为这样更有效率，并且会产生高质量的嵌入。[1](ch02.html#id478)
- en: An *embedding model* then is an algorithm that takes a piece of text and outputs
    a numerical representation of its meaning—technically, a long list of floating-point
    (decimal) numbers, usually somewhere between 100 and 2,000 numbers, or *dimensions*.
    These are also called *dense* embeddings, as opposed to the *sparse* embeddings
    of the previous section, as here usually all dimensions are different from 0.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，*嵌入模型*是一种算法，它接受一段文本并输出其意义的数值表示——技术上讲，是一长串浮点数（小数），通常在100到2000个数字之间，或称为*维度*。这些也被称为*密集*嵌入，与上一节的*稀疏*嵌入相对，因为在这里通常所有维度都不同于0。
- en: Tip
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Different models produce different numbers and different sizes of lists. All
    of these are specific to each model; that is, even if the size of the lists matches,
    you cannot compare embeddings from different models. Combining embeddings from
    different models should always be avoided.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的模型产生不同的数字和不同大小的列表。所有这些都是针对每个模型的特定属性；也就是说，即使列表的大小匹配，也不能比较不同模型的嵌入。应始终避免将不同模型的嵌入组合在一起。
- en: Semantic Embeddings Explained
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义嵌入解释
- en: 'Consider these three words: *lion*, *pet*, and *dog*. Intuitively, which pair
    of these words share similar characteristics to each other at first glance? The
    obvious answer is *pet* and *dog*. But computers do not have the ability to tap
    into this intuition or nuanced understanding of the English language. In order
    for a computer to differentiate between a lion, pet, or dog, you need to be able
    to translate them into the language of computers, which is numbers*.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这三个单词：*狮子*、*宠物*和*狗*。直观上，哪一对单词在第一眼看上去具有相似的特征？显然的答案是*宠物*和*狗*。但是计算机没有能力挖掘这种直觉或对英语语言的细微理解。为了使计算机能够区分狮子、宠物或狗，你需要能够将它们翻译成计算机的语言，即数字*。
- en: '[Figure 2-2](#ch02_figure_2_1736545662484146) illustrates converting each word
    into hypothetical number representations that retain their meaning.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-2](#ch02_figure_2_1736545662484146) 阐述了将每个单词转换成保留其意义的假设性数字表示。'
- en: '![A diagram of a diagram of circles  Description automatically generated with
    medium confidence](assets/lelc_0202.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![一个圆的图的图  描述自动生成，置信度中等](assets/lelc_0202.png)'
- en: Figure 2-2\. Semantic representations of words
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2. 单词的语义表示
- en: '[Figure 2-2](#ch02_figure_2_1736545662484146) shows each word alongside its
    corresponding semantic embedding. Note that the numbers themselves have no particular
    meaning, but instead the sequences of numbers for two words (or sentences) that
    are close in meaning should be *closer* than those of unrelated words. As you
    can see, each number is a *floating-point value*, and each of them represents
    a semantic *dimension*. Let’s see what we mean by *closer*:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-2](#ch02_figure_2_1736545662484146) 显示了每个单词及其相应的语义嵌入。请注意，这些数字本身没有特定的意义，而是两个意义相近的单词（或句子）的数字序列应该比无关单词的序列更*接近*。正如你所看到的，每个数字都是一个*浮点值*，每个都代表一个语义*维度*。让我们看看我们所说的*接近*是什么意思：'
- en: If we plot these vectors in a three-dimensional space, it could look like [Figure 2-3](#ch02_figure_3_1736545662484170).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在三维空间中绘制这些向量，它可能看起来像[图2-3](#ch02_figure_3_1736545662484170)。
- en: '![A diagram of a dog and a dog  Description automatically generated](assets/lelc_0203.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![一个狗和一个狗的图  描述自动生成](assets/lelc_0203.png)'
- en: Figure 2-3\. Plot of word vectors in a multidimensional space
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3. 多维空间中单词向量的图
- en: '[Figure 2-3](#ch02_figure_3_1736545662484170) shows the *pet* and *dog* vectors
    are closer to each other in distance than the *lion* plot. We can also observe
    that the angles between each plot varies depending on how similar they are. For
    example, the words *pet* and *lion* have a wider angle between one another than
    the *pet* and *dog* do, indicating more similarities shared by the latter word
    pairs. The narrower the angle or shorter the distance between two vectors, the
    closer their similarities.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-3](#ch02_figure_3_1736545662484170) 显示了*宠物*和*狗*向量在距离上比*狮子*图更接近。我们还可以观察到，每个图之间的角度根据它们的相似程度而变化。例如，单词*宠物*和*狮子*之间的角度比*宠物*和*狗*之间的角度更宽，这表明后者单词对之间有更多的相似性。两个向量之间的角度越窄或距离越短，它们的相似性就越高。'
- en: One effective way to calculate the degree of similarity between two vectors
    in a multidimensional space is called cosine similarity. *Cosine similarity* computes
    the dot product of vectors and divides it by the product of their magnitudes to
    output a number between –1 and 1, where 0 means the vectors share no correlation,
    –1 means they are absolutely dissimilar, and 1 means they are absolutely similar.
    So, in the case of our three words here, the cosine similarity between *pet* and
    *dog* could be 0.75, but between *pet* and *lion* it might be 0.1.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在多维空间中计算两个向量之间相似度的有效方法称为余弦相似度。*余弦相似度*计算向量的点积，并将其除以它们模长的乘积，输出一个介于 –1 和 1 之间的数字，其中
    0 表示向量没有相关性，–1 表示它们完全不同，1 表示它们完全相同。因此，在我们的三个词中，*pet* 和 *dog* 之间的余弦相似度可能是 0.75，但
    *pet* 和 *lion* 之间的相似度可能只有 0.1。
- en: The ability to convert sentences into embeddingsthat capture semantic meaning
    and then perform calculations to find semantic similarities between different
    sentences enables us to get an LLM to find the most relevant documents to answer
    questions about a large body of text like our Tesla PDF document. Now that you
    understand the big picture, let’s revisit the first step (indexing) of preprocessing
    your document.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将句子转换为捕获语义意义的嵌入，然后执行计算以在不同句子之间找到语义相似性，这使得我们能够让 LLM 找到与我们的特斯拉 PDF 文档等大量文本相关的最相关文档。现在你已经了解了整体情况，让我们回顾一下预处理文档的第一步（索引）。
- en: Converting Your Documents into Text
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将你的文档转换为文本
- en: As mentioned at the beginning of the chapter, the first step in preprocessing
    your document is to convert it to text. In order to achieve this, you would need
    to build logic to parse and extract the document with minimal loss of quality.
    Fortunately, LangChain provides *document loaders* that handle the parsing logic
    and enable you to “load” data from various sources into a `Document` class that
    consists of text and associated metadata.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章开头所述，预处理你的文档的第一步是将它转换为文本。为了实现这一点，你需要构建逻辑来解析和提取文档，同时尽量减少质量损失。幸运的是，LangChain
    提供了处理解析逻辑的 *文档加载器*，使你能够将来自各种来源的数据“加载”到包含文本和相关元数据的 `Document` 类中。
- en: 'For example, consider a simple *.txt* file. You can simply import a LangChain
    `TextLoader` class to extract the text, like this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个简单的 *.txt* 文件。你可以简单地导入 LangChain 的 `TextLoader` 类来提取文本，如下所示：
- en: '*Python*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*JavaScript*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*The output:*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The previous code block assumes that you have a file named `test.txt` in your
    current directory. Usage of all LangChain document loaders follows a similar pattern:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码块假设你在当前目录下有一个名为 `test.txt` 的文件。所有 LangChain 文档加载器的使用遵循类似的模式：
- en: Start by picking the loader for your type of document from the long list of
    [integrations](https://oreil.ly/iLJ33).
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从长长的 [集成](https://oreil.ly/iLJ33) 列表中挑选适合你文档类型的加载器。
- en: Create an instance of the loader in question, along with any parameters to configure
    it, including the location of your documents (usually a filesystem path or web
    address).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个加载器的实例，包括任何配置它的参数，包括你的文档位置（通常是文件系统路径或网址）。
- en: Load the documents by calling `load()`, which returns a list of documents ready
    to pass to the next stage (more on that soon).
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `load()` 加载文档，它返回一个列表，包含准备好传递到下一阶段的文档（更多内容即将揭晓）。
- en: Aside from *.txt* files, LangChain provides document loaders for other popular
    file types including *.csv*, *.json*, and Markdown, alongside integrations with
    popular platforms such as Slack and Notion.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 *.txt* 文件外，LangChain 还提供了其他流行文件类型的文档加载器，包括 *.csv*、*.json* 和 Markdown，以及与
    Slack 和 Notion 等流行平台的集成。
- en: For example, you can use `WebBaseLoader` to load HTML from web URLs and parse
    it to text.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以使用 `WebBaseLoader` 从网络 URL 加载 HTML 并将其解析为文本。
- en: 'Install the beautifulsoup4 package:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 beautifulsoup4 包：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Python*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*JavaScript*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the case of our Tesla PDF use case, we can utilize LangChain’s `PDFLoader`
    to extract text from the PDF document:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们特斯拉 PDF 用例的情况下，我们可以利用 LangChain 的 `PDFLoader` 从 PDF 文档中提取文本：
- en: '*Python*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*JavaScript*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The text has been extracted from the PDF document and stored in the `Document`
    class. But there’s a problem. The loaded document is over 100,000 characters long,
    so it won’t fit into the context window of the vast majority of LLMs or embedding
    models. In order to overcome this limitation, we need to split the `Document`
    into manageable chunks of text that we can later convert into embeddings and semantically
    search, bringing us to the second step (retrieving).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLMs and embedding models are designed with a hard limit on the size of input
    and output tokens they can handle. This limit is usually called *context window*,
    and usually applies to the combination of input and output; that is, if the context
    window is 100 (we’ll talk about units in a second), and your input measures 90,
    the output can be at most of length 10\. Context windows are usually measured
    in number of tokens, for instance 8,192 tokens. Tokens, as mentioned in the [Preface](preface01.html#pr01_preface_1736545679069216),
    are a representation of text as numbers, with each token usually covering between
    three and four characters of English text.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Splitting Your Text into Chunks
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At first glance it may seem straightforward to split a large body of text into
    chunks, but keeping *semantically* related (related by meaning) chunks of text
    together is a complex process. To make it easier to split large documents into
    small, but still meaningful, pieces of text, LangChain provides `RecursiveCharacterTextSplitter`,
    which does the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a list of separators, in order of importance. By default these are:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The paragraph separator: `\n\n`'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The line separator: `\n`'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The word separator: space character'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To respect the given chunk size, for instance, 1,000 characters, start by splitting
    up paragraphs.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For any paragraph longer than the desired chunk size, split by the next separator:
    lines. Continue until all chunks are smaller than the desired length, or there
    are no additional separators to try.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Emit each chunk as a `Document`, with the metadata of the original document
    passed in and additional information about the position in the original document.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s see an example:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*JavaScript*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding code, the documents created by the document loader are split
    into chunks of 1,000 characters each, with some overlap between chunks of 200
    characters to maintain some context. The result is also a list of documents, where
    each document is up to 1,000 characters in length, split along the natural divisions
    of written text—paragraphs, new lines and finally, words. This uses the structure
    of the text to keep each chunk a consistent, readable snippet of text.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '`RecursiveCharacterTextSplitter` can also be used to split code languages and
    Markdown into semantic chunks. This is done by using keywords specific to each
    language as the separators, which ensures, for instance, the body of each function
    is kept in the same chunk, instead of split between several. Usually, as programming
    languages have more structure than written text, there’s less need to use overlap
    between the chunks. LangChain contains separators for a number of popular languages,
    such as Python, JS, Markdown, HTML, and many more. Here’s an example:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`RecursiveCharacterTextSplitter` 也可以用来将代码语言和 Markdown 分割成语义块。这是通过使用每个语言的特定关键字作为分隔符来完成的，这确保了例如每个函数的主体都保持在同一个块中，而不是被分割到几个块中。通常，由于编程语言比书面文本有更多的结构，因此不需要在块之间使用重叠。LangChain
    包含了多种流行语言的分隔符，例如 Python、JS、Markdown、HTML 等。以下是一个示例：'
- en: '*Python*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*JavaScript*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*The output:*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出结果：*'
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice how we’re still using `RecursiveCharacterTextSplitter` as before, but
    now we’re creating an instance of it for a specific language, using the `from_language`
    method. This one accepts the name of the language, and the usual parameters for
    chunk size, and so on. Also notice we are now using the method `create_documents`,
    which accepts a list of strings, rather than the list of documents we had before.
    This method is useful when the text you want to split doesn’t come from a document
    loader, so you have only the raw text strings.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们仍然像之前一样使用 `RecursiveCharacterTextSplitter`，但现在我们使用 `from_language` 方法为其创建一个特定语言的实例。这个方法接受语言的名称以及通常的块大小等参数。同时注意我们现在使用的是
    `create_documents` 方法，它接受一个字符串列表，而不是我们之前拥有的文档列表。当您想要分割的文本不来自文档加载器，而只有原始文本字符串时，此方法非常有用。
- en: You can also use the optional second argument to `create_documents` in order
    to pass a list of metadata to associate with each text string. This metadata list
    should have the same length as the list of strings and will be used to populate
    the metadata field of each `Document` returned.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 `create_documents` 方法的可选第二个参数来传递与每个文本字符串关联的元数据列表。此元数据列表的长度应与字符串列表相同，并将用于填充每个返回的
    `Document` 的元数据字段。
- en: 'Let’s see an example for Markdown text, using the metadata argument as well:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个使用元数据参数的 Markdown 文本示例：
- en: '*Python*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE13]bash'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE13]bash'
- en: pip install langchain
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: pip install langchain
- en: '[PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*JavaScript*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*The output:*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出结果：*'
- en: '[PRE16]bash'','
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE16]bash'''
- en: 'metadata={"source": "https://www.langchain.com"}),'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'metadata={"source": "https://www.langchain.com"}),'
- en: Document(page_content='pip install langchain',
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Document(page_content='pip install langchain',
- en: 'metadata={"source": "https://www.langchain.com"}),'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'metadata={"source": "https://www.langchain.com"}),'
- en: Document(page_content='[PRE17]
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Document(page_content='[PRE17]
- en: 'Notice two things:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意两点：
- en: The text is split along the natural stopping points in the Markdown document;
    for instance, the heading goes into one chunk, the line of text under it in a
    separate chunk, and so on.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本在 Markdown 文档的自然停止点处进行分割；例如，标题进入一个块，其下的文本行在另一个块中，依此类推。
- en: The metadata we passed in the second argument is attached to each resulting
    document, which allows you to track, for instance, where the document came from
    and where you can go to see the original.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在第二个参数中传递的元数据附加到每个生成的文档上，这使得您可以跟踪例如文档的来源以及您可以去哪里查看原始内容。
- en: Generating Text Embeddings
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成文本嵌入
- en: 'LangChain also has an `Embeddings` class designed to interface with text embedding
    models—including OpenAI, Cohere, and Hugging Face—and generate vector representations
    of text. This class provides two methods: one for embedding documents and one
    for embedding a query. The former takes a list of text strings as input, while
    the latter takes a single text string.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 还有一个 `Embeddings` 类，用于与文本嵌入模型（包括 OpenAI、Cohere 和 Hugging Face）接口，并生成文本的向量表示。这个类提供了两种方法：一种用于嵌入文档，另一种用于嵌入查询。前者接受文本字符串列表作为输入，而后者接受单个文本字符串。
- en: 'Here’s an example of embedding a document using [OpenAI’s embedding model](https://oreil.ly/9tnzQ):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用 [OpenAI 的嵌入模型](https://oreil.ly/9tnzQ)嵌入文档的示例：
- en: '*Python*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*JavaScript*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*The output:*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出结果：*'
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Notice that you can embed multiple documents at the same time; you should prefer
    this to embedding them one at a time, as it will be more efficient (due to how
    these models are constructed). You get back a list containing multiple lists of
    numbers—each inner list is a vector or embedding, as explained in an earlier section.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意您可以同时嵌入多个文档；您应该优先选择这种方式，而不是逐个嵌入，因为它将更有效率（因为这些模型是如何构建的）。您会得到一个包含多个数字列表的列表——每个内部列表都是一个向量或嵌入，如前文所述。
- en: 'Now let’s see an end-to-end example using the three capabilities we’ve seen
    so far:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一个端到端的示例，使用我们迄今为止看到的三个功能：
- en: Document loaders, to convert any document to plain text
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档加载器，将任何文档转换为纯文本
- en: Text splitters, to split each large document into many smaller ones
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分割器，将每个大型文档分割成许多较小的文档
- en: Embeddings models, to create a numeric representation of the meaning of each
    split
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入模型，创建每个分割的语义的数值表示
- en: 'Here’s the code:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '*Python*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*JavaScript*'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE22]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Once you’ve generated embeddings from your documents, the next step is to store
    them in a special database known as a vector store.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您从您的文档中生成了嵌入，下一步就是将它们存储在一个称为向量存储的特殊数据库中。
- en: Storing Embeddings in a Vector Store
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在向量存储中存储嵌入
- en: Earlier in this chapter, we discussed the cosine similarity calculation to measure
    the similarity between vectors in a vector space. A vector store is a database
    designed to store vectors and perform complex calculations, like cosine similarity,
    efficiently and quickly.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面，我们讨论了余弦相似度计算，用于衡量向量空间中向量的相似度。向量存储是一个数据库，旨在存储向量并高效快速地执行复杂计算，如余弦相似度。
- en: Unlike traditional databases that specialize in storing structured data (such
    as JSON documents or data conforming to the schema of a relational database),
    vector stores handle unstructured data, including text and images. Like traditional
    databases, vector stores are capable of performing create, read, update, delete
    (CRUD), and search operations.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与专门存储结构化数据（如JSON文档或符合关系数据库模式的schema数据）的传统数据库不同，向量存储处理非结构化数据，包括文本和图像。像传统数据库一样，向量存储能够执行创建、读取、更新、删除（CRUD）和搜索操作。
- en: Vector stores unlock a wide variety of use cases, including scalable applications
    that utilize AI to answer questions about large documents, as illustrated in [Figure 2-4](#ch02_figure_4_1736545662484192).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 向量存储解锁了各种用例，包括可扩展的应用程序，这些应用程序利用AI来回答有关大型文档的问题，如图2-4所示。
- en: '![A diagram of a store  Description automatically generated](assets/lelc_0204.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![存储的示意图  描述由系统自动生成](assets/lelc_0204.png)'
- en: Figure 2-4\. Loading, embedding, storing, and retrieving relevant docs from
    a vector store
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. 从向量存储中加载、嵌入、存储和检索相关文档
- en: '[Figure 2-4](#ch02_figure_4_1736545662484192) illustrates how document embeddings
    are inserted into the vector store and how later, when a query is sent, similar
    embeddings are retrieved from the vector store.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-4](#ch02_figure_4_1736545662484192)说明了文档嵌入如何插入到向量存储中，以及当发送查询时，如何从向量存储中检索相似的嵌入。'
- en: Currently, there is an abundance of vector store providers to choose from, each
    specializing in different capabilities. Your selection should depend on the critical
    requirements of your application, including multitenancy, metadata filtering capabilities,
    performance, cost, and scalability.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，有大量的向量存储提供商可供选择，每个都专注于不同的功能。您的选择应取决于您应用程序的关键需求，包括多租户、元数据过滤能力、性能、成本和可扩展性。
- en: 'Although vector stores are niche databases built to manage vector data, there
    are a few disadvantages working with them:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管向量存储是专为管理向量数据而构建的利基数据库，但使用它们也有一些缺点：
- en: Most vector stores are relatively new and may not stand the test of time.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数向量存储相对较新，可能无法经受时间的考验。
- en: Managing and optimizing vector stores can present a relatively steep learning
    curve.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理和优化向量存储可能具有相对陡峭的学习曲线。
- en: Managing a separate database adds complexity to your application and may drain
    valuable resources.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理单独的数据库会增加您的应用程序的复杂性，并可能耗尽宝贵的资源。
- en: Fortunately, vector store capabilities have recently been extended to PostgreSQL
    (a popular open source relational database) via the `pgvector` extension. This
    enables you to use the same database you’re already familiar with and to power
    both your transactional tables (for instance your users table) as well as your
    vector search tables.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，向量存储功能最近通过`pgvector`扩展扩展到了PostgreSQL（一个流行的开源关系数据库）。这使得你可以使用你已熟悉的相同数据库，并为你的事务表（例如你的用户表）以及你的向量搜索表提供动力。
- en: Getting Set Up with PGVector
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PGVector进行设置
- en: 'To use Postgres and PGVector you’ll need to follow a few setup steps:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Postgres和PGVector，你需要遵循一些设置步骤：
- en: Ensure you have Docker installed on your computer, following the [instructions
    for your operating system](https://oreil.ly/Gn28O).
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你的计算机上已安装Docker，按照[操作系统的说明](https://oreil.ly/Gn28O)进行操作。
- en: 'Run the following command in your terminal; it will launch a Postgres instance
    in your computer running on port 6024:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的终端中运行以下命令；它将在你的计算机上启动一个运行在端口6024的Postgres实例：
- en: '[PRE23]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Open your docker dashboard containers and you should see a green running status
    next to `pgvector-container`.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 打开你的Docker仪表板容器，你应该在`pgvector-container`旁边看到绿色的运行状态。
- en: 'Save the connection string to use in your code; we’ll need it later:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将连接字符串保存到你的代码中使用；我们稍后会用到它：
- en: '[PRE24]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Working with Vector Stores
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与向量存储一起工作
- en: 'Picking up where we left off in the previous section on embeddings, now let’s
    see an example of loading, splitting, embedding, and storing a document in PGVector:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节关于嵌入的设置中继续，现在让我们看看在PGVector中加载、分割、嵌入和存储文档的示例：
- en: '*Python*'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE25]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*JavaScript*'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Notice how we reuse the code from the previous sections to first load the documents
    with the loader and then split them into smaller chunks. Then, we instantiate
    the embeddings model we want to use—in this case, OpenAI’s. Note that you could
    use any other embeddings model supported by LangChain here.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何重复使用前几节中的代码，首先使用加载器加载文档，然后将它们分割成更小的块。然后，我们实例化我们想要使用的嵌入模型——在这个例子中是OpenAI的。请注意，你在这里可以使用LangChain支持的任何其他嵌入模型。
- en: 'Next, we have a new line of code, which creates a vector store given documents,
    the embeddings model, and a connection string. This will do a few things:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一行新的代码，它根据给定的文档、嵌入模型和连接字符串创建一个向量存储。这将执行以下几件事：
- en: Establish a connection to the Postgres instance running in your computer (see
    [“Getting Set Up with PGVector”](#ch02_getting_set_up_with_pgvector_1736545662501989).)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立与你在计算机上运行的Postgres实例的连接（见[“使用PGVector进行设置”](#ch02_getting_set_up_with_pgvector_1736545662501989)。）
- en: Run any setup necessary, such as creating tables to hold your documents and
    vectors, if this is the first time you’re running it.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行任何必要的设置，例如创建用于存储文档和向量的表，如果这是你第一次运行它。
- en: Create embeddings for each document you passed in, using the model you chose.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用你选择的模型为每个传入的文档创建嵌入。
- en: Store the embeddings, the document’s metadata, and the document’s text content
    in Postgres, ready to be searched.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Postgres中存储嵌入、文档的元数据和文档的文本内容，以便进行搜索。
- en: 'Let’s see what it looks like to search documents:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看搜索文档的样子：
- en: '*Python*'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE27]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '*JavaScript*'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This method will find the most relevant documents (which you previously indexed),
    by following this process:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将通过以下过程找到最相关的文档（你之前已索引的文档）：
- en: The search query—in this case, the word `query`—will be sent to the embeddings
    model to retrieve its embedding.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索查询——在这个例子中，是单词`query`——将被发送到嵌入模型以检索其嵌入。
- en: Then, it will run a query on Postgres to find the N (in this case 4) previously
    stored embeddings that are most similar to your query.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，它将在Postgres上运行一个查询，以找到与你的查询最相似的N（在这个例子中是4）先前存储的嵌入。
- en: Finally, it will fetch the text content and metadata that relates to each of
    those embeddings.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，它将检索与每个嵌入相关的文本内容和元数据。
- en: The model can now return a list of `Document` sorted by how similar they are
    to the query—the most similar first, the second most similar after, and so on.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型现在可以返回一个按与查询相似度排序的`Document`列表——最相似的排在第一位，其次是第二相似的，依此类推。
- en: 'You can also add more documents to an existing database. Let’s see an example:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以向现有数据库添加更多文档。让我们看看一个例子：
- en: '*Python*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE29]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '*JavaScript*'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE30]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `add_documents` method we’re using here will follow a similar process to
    `fromDocuments`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的`add_documents`方法将遵循与`fromDocuments`类似的过程：
- en: Create embeddings for each document you passed in, using the model you chose.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用你选择的模型为每个传入的文档创建嵌入。
- en: Store the embeddings, the document’s metadata, and the document’s text content
    in Postgres, ready to be searched.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将嵌入、文档的元数据和文档的文本内容存储在Postgres中，以便进行搜索。
- en: In this example, we are using the optional `ids` argument to assign identifiers
    to each document, which allows us to update or delete them later.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用可选的`ids`参数为每个文档分配标识符，这允许我们稍后更新或删除它们。
- en: 'Here’s an example of the delete operation:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个删除操作的例子：
- en: '*Python*'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE31]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '*JavaScript*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE32]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This removes the second document inserted by using its Universally Unique Identifier
    (UUID). Now let’s see how to do this in a more systematic way.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过使用其通用唯一标识符（UUID）删除了第二个文档。现在让我们看看如何以更系统的方式来做这件事。
- en: Tracking Changes to Your Documents
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪文档更改
- en: One of the key challenges with working with vector stores is working with data
    that regularly changes, because changes mean re-indexing. And re-indexing can
    lead to costly recomputations of embeddings and duplications of preexisting content.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 与向量存储一起工作的一个主要挑战是处理经常变化的数据，因为变化意味着重新索引。重新索引可能导致昂贵的嵌入重新计算和现有内容的重复。
- en: 'Fortunately, LangChain provides an indexing API to make it easy to keep your
    documents in sync with your vector store. The API utilizes a class (`RecordManager`)
    to keep track of document writes into the vector store. When indexing content,
    hashes are computed for each document and the following information is stored
    in `RecordManager`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，LangChain提供了一个索引API，使您能够轻松地将文档与向量存储保持同步。该API使用一个类（`RecordManager`）来跟踪写入向量存储的文档。在索引内容时，为每个文档计算哈希值，并在`RecordManager`中存储以下信息：
- en: The document hash (hash of both page content and metadata)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档哈希（页面内容和元数据的哈希）
- en: Write time
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写入时间
- en: The source ID (each document should include information in its metadata to determine
    the ultimate source of this document).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源ID（每个文档应在其元数据中包含信息，以确定此文档的最终来源）。
- en: In addition, the indexing API provides cleanup modes to help you decide how
    to delete existing documents in the vector store. For example, If you’ve made
    changes to how documents are processed before insertion or if source documents
    have changed, you may want to remove any existing documents that come from the
    same source as the new documents being indexed. If some source documents have
    been deleted, you’ll want to delete all existing documents in the vector store
    and replace them with the re-indexed documents.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，索引API提供了清理模式，以帮助您决定如何删除向量存储中现有的文档。例如，如果您在插入之前更改了文档的处理方式，或者源文档已更改，您可能希望删除任何来自与正在索引的新文档相同源的新文档。如果某些源文档已被删除，您将希望删除向量存储中的所有现有文档，并用重新索引的文档替换它们。
- en: 'The modes are as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 模式如下：
- en: '`None` mode does not do any automatic cleanup, allowing the user to manually
    do cleanup of old content.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`None`模式不会进行任何自动清理，允许用户手动清理旧内容。'
- en: '`Incremental` and `full` modes delete previous versions of the content if the
    content of the source document or derived documents has changed.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Incremental`和`full`模式如果源文档或派生文档的内容已更改，则会删除内容的前一个版本。'
- en: '`Full` mode will additionally delete any documents not included in documents
    currently being indexed.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Full`模式将删除当前索引文档中未包含的任何文档。'
- en: 'Here’s an example of the use of the indexing API with Postgres database set
    up as a record manager:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用索引API的例子，其中Postgres数据库已设置为记录管理器：
- en: '*Python*'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE33]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '*JavaScript*'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE34]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: First, you create a record manager, which keeps track of which documents have
    been indexed before. Then you use the `index` function to synchronize your vector
    store with the new list of documents. In this example, we’re using the incremental
    mode, so any documents that have the same ID as previous ones will be replaced
    with the new version.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您创建一个记录管理器，它跟踪哪些文档之前已被索引。然后您使用`index`函数将您的向量存储与新的文档列表同步。在这个例子中，我们使用增量模式，因此任何具有与之前相同的ID的文档将被新版本替换。
- en: Indexing Optimization
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 索引优化
- en: A basic RAG indexing stage involves naive text splitting and embedding of chunks
    of a given document. However, this basic approach leads to inconsistent retrieval
    results and a relatively high occurrence of hallucinations, especially when the
    data source contains images and tables.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的RAG索引阶段涉及对给定文档的简单文本分割和嵌入。然而，这种基本方法会导致检索结果不一致，以及相对较高的幻觉发生率，尤其是在数据源包含图像和表格时。
- en: 'There are various strategies to enhance the accuracy and performance of the
    indexing stage. We will cover three of them in the next sections: MultiVectorRetriever,
    RAPTOR, and ColBERT.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种策略可以增强索引阶段的准确性和性能。在下一节中，我们将介绍其中的三种：MultiVectorRetriever、RAPTOR和ColBERT。
- en: MultiVectorRetriever
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MultiVectorRetriever
- en: 'A document that contains a mixture of text and tables cannot be simply split
    by text into chunks and embedded as context: the entire table can be easily lost.
    To solve this problem, we can decouple documents that we want to use for answer
    synthesis, from a reference that we want to use for the retriever. [Figure 2-5](#ch02_figure_5_1736545662484212)
    illustrates how.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含文本和表格混合的文档不能简单地通过文本分割成块并嵌入为上下文：整个表格可以轻易丢失。为了解决这个问题，我们可以将用于答案合成的文档与用于检索器的引用解耦。[图2-5](#ch02_figure_5_1736545662484212)说明了这一点。
- en: '![Screenshot 2024-03-16 at 5.54.55 PM.png](assets/lelc_0205.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![2024年3月16日下午5点54分55秒的屏幕截图.png](assets/lelc_0205.png)'
- en: Figure 2-5\. Indexing multiple representations of a single document
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. 索引单个文档的多个表示
- en: For example, in the case of a document that contains tables, we can first generate
    and embed summaries of table elements, ensuring each summary contains an `id`
    reference to the full raw table. Next, we store the raw referenced tables in a
    separate docstore. Finally, when a user’s query retrieves a table summary, we
    pass the entire referenced raw table as context to the final prompt sent to the
    LLM for answer synthesis. This approach enables us to provide the model with the
    full context of information required to answer the question.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个包含表格的文档中，我们首先可以生成和嵌入表格元素的摘要，确保每个摘要都包含一个指向完整原始表格的`id`引用。接下来，我们将原始引用的表格存储在单独的docstore中。最后，当用户的查询检索到表格摘要时，我们将整个引用的原始表格作为上下文传递给最终发送给LLM进行答案合成的提示。这种方法使我们能够向模型提供回答问题所需的全局信息背景。
- en: 'Here’s an example. First, let’s use the LLM to generate summaries of the documents:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子。首先，让我们使用LLM生成文档的摘要：
- en: '*Python*'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE35]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, let’s define the vector store and docstore to store the raw summaries
    and their embeddings:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义向量存储和docstore以存储原始摘要及其嵌入：
- en: '*Python*'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE36]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, let’s retrieve the relevant full context document based on a query:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们根据查询检索相关的完整上下文文档：
- en: '*Python*'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE37]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here’s the full implementation in JavaScript:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是JavaScript中的完整实现：
- en: '*JavaScript*'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE38]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval'
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAPTOR：递归抽象处理用于树状检索
- en: RAG systems need to handle lower-level questions that reference specific facts
    found in a single document or higher-level questions that distill ideas that span
    many documents. Handling both types of questions can be a challenge with typical
    k-nearest neighbors (k-NN) retrieval over document chunks.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统需要处理低级问题，这些问题引用了单个文档中找到的特定事实，或者处理高级问题，这些问题提炼了跨越多个文档的思想。使用典型的基于文档块的最邻近邻居（k-NN）检索处理这两种类型的问题可能是一个挑战。
- en: '*Recursive abstractive processing for tree-organized retrieval* (RAPTOR) is
    an effective strategy that involves creating document summaries that capture higher-level
    concepts, embedding and clustering those documents, and then [summarizing each
    cluster](https://oreil.ly/VdIpJ).^([2](ch02.html#id538)) This is done recursively,
    producing a tree of summaries with increasingly high-level concepts. The summaries
    and initial documents are then indexed together, giving coverage across lower-to-higher-level
    user questions. [Figure 2-6](#ch02_figure_6_1736545662484232) illustrates.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*递归抽象处理用于树状检索*（RAPTOR）是一种有效的策略，它涉及创建捕获高级概念的文档摘要，嵌入和聚类这些文档，然后[总结每个聚类](https://oreil.ly/VdIpJ).^([2](ch02.html#id538))。这是递归进行的，产生了一个具有越来越高级概念的摘要树。然后，摘要和初始文档一起索引，覆盖从低级到高级的用户问题。[图2-6](#ch02_figure_6_1736545662484232)说明了这一点。'
- en: '![Screenshot 2024-03-16 at 6.16.21 PM.png](assets/lelc_0206.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![2024年3月16日下午6点16分21秒的屏幕截图.png](assets/lelc_0206.png)'
- en: Figure 2-6\. Recursively summarizing documents
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6\. 递归总结文档
- en: 'ColBERT: Optimizing Embeddings'
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ColBERT：优化嵌入
- en: One of the challenges of using embeddings models during the indexing stage is
    that they compress text into fixed-length (vector) representations that capture
    the semantic content of the document. Although this compression is useful for
    retrieval, embedding irrelevant or redundant content may lead to hallucinations
    in the final LLM output.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引阶段使用嵌入模型的一个挑战是，它们将文本压缩成固定长度的（向量）表示，这些表示捕获了文档的语义内容。尽管这种压缩对检索很有用，但嵌入无关或冗余的内容可能导致最终LLM输出的幻觉。
- en: 'One solution to this problem is to do the following:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个方案是执行以下操作：
- en: Generate contextual embeddings for each token in the document and query.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为文档和查询中的每个标记生成上下文嵌入。
- en: Calculate and score similarity between each query token and all document tokens.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并评分每个查询标记与所有文档标记之间的相似度。
- en: Sum the maximum similarity score of each query embedding to any of the document
    embeddings to get a score for each document.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个查询嵌入与任何文档嵌入的最大相似度分数相加，以获得每个文档的评分。
- en: This results in a granular and effective embedding approach for better retrieval.
    Fortunately, the embedding model called ColBERTembodies the solution to this problem.^([3](ch02.html#id542))
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一种细粒度和有效的嵌入方法，以实现更好的检索。幸运的是，名为ColBERT的嵌入模型体现了解决这个问题的方案.^([3](ch02.html#id542))
- en: 'Here’s how we can utilize ColBERT for optimal embedding of our data:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何利用ColBERT进行数据最优嵌入的方法：
- en: '*Python*'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE39]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: By using ColBERT, you can improve the relevancy of retrieved documents used
    as context by the LLM.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用ColBERT，你可以提高LLM用作上下文的检索文档的相关性。
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you’ve learned how to prepare and preprocess your documents
    for your LLM application using various LangChain’s modules. The document loaders
    enable you to extract text from your data source, the text splitters help you
    split your document into semantically similar chunks, and the embeddings models
    convert your text into vector representations of their meaning.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用各种LangChain模块准备和预处理你的文档以用于你的LLM应用。文档加载器使你能够从数据源中提取文本，文本分割器帮助你将文档分割成语义上相似的片段，嵌入模型将你的文本转换为它们意义的向量表示。
- en: Separately, vector stores allow you to perform CRUD operations on these embeddings
    alongside complex calculations to compute semantically similar chunks of text.
    Finally, indexing optimization strategies enable your AI app to improve the quality
    of embeddings and perform accurate retrieval of documents that contain semistructured
    data including tables.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，向量存储允许你对这些嵌入执行CRUD操作，同时进行复杂的计算以计算语义上相似的文本片段。最后，索引优化策略使你的AI应用能够提高嵌入的质量并准确检索包含半结构化数据（包括表格）的文档。
- en: In [Chapter 3](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580),
    you’ll learn how to efficiently retrieve the most similar chunks of documents
    from your vector store based on your query, provide it as contextthe model can
    see, and then generate an accurate output.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580)中，你将学习如何根据查询从你的向量存储中高效检索最相似的文档片段，将其作为模型可以看到的上下文，然后生成准确的输出。
- en: ^([1](ch02.html#id478-marker)) Arvind Neelakantan et al., [“Text and Code Embeddings
    by Contrastive Pre-Training”](https://oreil.ly/YOVmh), arXiv, January 21, 2022\.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.html#id478-marker)) Arvind Neelakantan等人，[“通过对比预训练进行文本和代码嵌入”](https://oreil.ly/YOVmh)，arXiv，2022年1月21日。
- en: '^([2](ch02.html#id538-marker)) Parth Sarthi et al., [“RAPTOR: Recursive Abstractive
    Processing for Tree-Organized Retrieval”](https://oreil.ly/hS4NB), arXiv, January
    31, 2024\. Paper published at ICLR 2024\.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.html#id538-marker)) Parth Sarthi等人，[“RAPTOR：用于树状检索的递归抽象处理”](https://oreil.ly/hS4NB)，arXiv，2024年1月31日。该论文在ICLR
    2024上发表。
- en: '^([3](ch02.html#id542-marker)) Keshav Santhanam et al., [“ColBERTv2: Effective
    and Efficient Retrieval via Lightweight Late Interaction”](https://oreil.ly/9spW2),
    arXiv, December 2, 2021.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.html#id542-marker)) Keshav Santhanam等人，[“ColBERTv2：通过轻量级后期交互实现有效和高效的检索”](https://oreil.ly/9spW2)，arXiv，2021年12月2日。
