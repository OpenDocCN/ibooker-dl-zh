["```py\nfrom typing import Annotated, TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI()\n\nclass State(TypedDict):\n    # Messages have the type \"list\". The `add_messages` \n    # function in the annotation defines how this state should \n    # be updated (in this case, it appends new messages to the \n    # list, rather than replacing the previous messages)\n    messages: Annotated[list, add_messages]\n\ndef chatbot(state: State):\n    answer = model.invoke(state[\"messages\"])\n    return {\"messages\": [answer]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"chatbot\", chatbot)\nbuilder.add_edge(START, 'chatbot')\nbuilder.add_edge('chatbot', END)\n\ngraph = builder.compile()\n```", "```py\nimport {\n  StateGraph,\n  Annotation,\n  messagesStateReducer,\n  START, END\n} from '@langchain/langgraph'\nimport {ChatOpenAI} from '@langchain/openai'\n\nconst model = new ChatOpenAI()\n\nconst State = {\n  /**\n * The State defines three things:\n * 1\\. The structure of the graph's state (which \"channels\" are available to \n * read/write)\n * 2\\. The default values for the state's channels\n * 3\\. The reducers for the state's channels. Reducers are functions that \n * determine how to apply updates to the state. Below, new messages are \n * appended to the messages array.\n */\n  messages: Annotation({\n    reducer: messagesStateReducer,\n    default: () => []\n  }),\n}\n\nasync function chatbot(state) {\n  const answer = await model.invoke(state.messages)\n  return {\"messages\": answer}\n}\n\nconst builder = new StateGraph(State)\n  .addNode('chatbot', chatbot)\n  .addEdge(START, 'chatbot')\n  .addEdge('chatbot', END)\n\nconst graph = builder.compile()\n```", "```py\ngraph.get_graph().draw_mermaid_png()\n```", "```py\nawait graph.getGraph().drawMermaidPng()\n```", "```py\ninput = {\"messages\": [HumanMessage('hi!)]} `for` `chunk` `in` `graph``.``stream``(``input``):`\n    `print``(``chunk``)`\n```", "```py`*JavaScript*    ```", "```py    *The output:*    ```", "```py    Notice how the input to the graph was in the same shape as the `State` object we defined earlier; that is, we sent in a list of messages in the `messages` key of a dictionary.    This is the simplest possible architecture for using an LLM, which is not to say that it should never be used. Here are some examples of where you might see it in action in popular products, among many others:    *   AI-powered features such as summarize and translate (such as you can find in Notion, a popular writing software) can be powered by a single LLM call.           *   Simple SQL query generation can be powered by a single LLM call, depending on the UX and target user the developer has in mind.```", "```py`# Architecture #2: Chain    This next architecture extends on all that by using multiple LLM calls, in a predefined sequence (that is, different invocations of the application do the same sequence of LLM calls, albeit with different inputs and results).    Let’s take as an example a text-to-SQL application, which receives as input from the user a natural language description of some calculation to make over a database. We mentioned earlier that this could be achieved with a single LLM call, to generate a SQL query, but we can create a more sophisticated application by making use of multiple LLM calls in sequence. Some authors call this architecture *flow engineering*.^([2](ch05.html#id709))    First let’s describe the flow in words:    1.  One LLM call to generate a SQL query from the natural language query, provided by the user, and a description of the database contents, provided by the developer.           2.  Another LLM call to write an explanation of the query appropriate for a nontechnical user, given the query generated in the previous call. This one could then be used to enable the user to check if the generated query matches his request.              You could also extend this even further (but we won’t do that here) with additional steps to be taken after the preceding two:    3.  Executes the query against the database, which returns a two-dimensional table.           4.  Uses a third LLM call to summarize the query results into a textual answer to the original user question.              And now let’s implement this with LangGraph:    *Python*    ```", "```py    *JavaScript*    ```", "```py    The visual representation of the graph is shown in [Figure 5-3](#ch05_figure_3_1736545670024001).  ![A diagram of a program  Description automatically generated](assets/lelc_0503.png)  ###### Figure 5-3\\. The chain architecture    Here’s an example of inputs and outputs:    *Python*    ```", "```py    *JavaScript*    ```", "```py    *The output:*    ```", "```py    First, the `generate_sql` node is executed, which populates the `sql_query` key in the state (which will be part of the final output) and updates the `messages` key with the new messages. Then the `explain_sql` node runs, taking the SQL query generated in the previous step and populating the `sql_explanation` key in the state. At this point, the graph finishes running, and the output is returned to the caller.    Note also the use of separate input and output schemas when creating the `StateGraph`. This lets you customize which parts of the state are accepted as input from the user and which are returned as the final output. The remaining state keys are used by the graph nodes internally to keep intermediate state and are made available to the user as part of the streaming output produced by `stream()`.    # Architecture #3: Router    This next architecture moves up the autonomy ladder by assigning to LLMs the next of the responsibilities we outlined before: deciding the next step to take. That is, whereas the chain architecture always executes a static sequence of steps (however many), the router architecture is characterized by using an LLM to choose between certain predefined steps.    Let’s use the example of a RAG application with access to multiple indexes of documents from different domains (refer to [Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927) for more on indexing). Usually you can extract better performance from LLMs by avoiding the inclusion of irrelevant information in the prompt. Therefore, in building this application, we should try to pick the right index to use for each query and use only that one. The key development in this architecture is to use an LLM to make this decision, effectively using an LLM to evaluate each incoming query and decide which index it should use for that *particular* query.    ###### Note    Before the advent of LLMs, the usual way of solving this problem would be to build a classifier model using ML techniques and a dataset mapping example user queries to the right index. This could prove quite challenging, as it requires the following:    *   Assembling that dataset by hand           *   Generating enough *features* (quantitative attributes) from each user query to enable training a classifier for the task              LLMs, given their encoding of human language, can effectively serve as this classifier with zero, or very few, examples or additional training.    First, let’s describe the flow in words:    1.  An LLM call to pick which of the available indexes to use, given the user-supplied query, and the developer-supplied description of the indexes           2.  A retrieval step that queries the chosen index for the most relevant documents for the user query           3.  Another LLM call to generate an answer, given the user-supplied query and the list of relevant documents fetched from the index              And now let’s implement it with LangGraph:    *Python*    ```", "```py    *JavaScript*    ```", "```py    The visual representation is shown in [Figure 5-4](#ch05_figure_4_1736545670024020).  ![A diagram of a router  Description automatically generated](assets/lelc_0504.png)  ###### Figure 5-4\\. The router architecture    Notice how this is now starting to become more useful, as it shows the two possible paths through the graph, through `retrieve_medical_records` or through `retrieve_insurance_faqs`, and that for both of those, we first visit the `router` node and finish by visiting the `generate_answer` node. These two possible paths were implemented through the use of a conditional edge, implemented in the function `pick_retriever`, which maps the `domain` picked by the LLM to one of the two nodes mentioned earlier. The conditional edge is shown in [Figure 5-4](#ch05_figure_4_1736545670024020) as dotted lines from the source node to the destination nodes.    And now for example inputs and outputs, this time with streaming output:    *Python*    ```", "```py    *JavaScript*    ```", "```py    *The output* (the actual answer is not shown, since it would depend on your documents):    ```", "```py    This output stream contains the values returned by each node that ran during this execution of the graph. Let’s take it one at a time. The top-level key in each dictionary is the name of the node, and the value for that key is what that node returned:    1.  The `router` node returned an update to `messages` (this would allow us to easily continue this conversation using the memory technique described earlier), and the `domain` the LLM picked for this user’s query, in this case `insurance`.           2.  Then the `pick_retriever` function ran and returned the name of the next node to run, based on the `domain` identified by the LLM call in the previous step.           3.  Then the `retrieve_insurance_faqs` node ran, returning a set of relevant documents from that index. This means that on the drawing of the graph seen earlier, we took the left path, as decided by the LLM.           4.  Finally, the `generate_answer` node ran, which took those documents and the original user query and produced an answer to the question, which was written to the state (along with a final update to the `messages` key).              # Summary    This chapter talked about the key trade-off when building LLM applications: agency versus oversight. The more autonomous an LLM application is, the more it can do—but that raises the need for more mechanisms of control over its actions. We moved on to different cognitive architectures that strike different balances between agency and oversight.    [Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341) talks about the most powerful of the cognitive architectures we’ve seen so far: the agent architecture.    ^([1](ch05.html#id699-marker)) Theodore R. Sumers et al., [“Cognitive Architectures for Language Agents”](https://oreil.ly/cuQnT), arXiv, September 5, 2023, updated March 15, 2024\\.    ^([2](ch05.html#id709-marker)) Tal Ridnik et al., [“Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering”](https://oreil.ly/0wHX4), arXiv, January 16, 2024\\.```"]