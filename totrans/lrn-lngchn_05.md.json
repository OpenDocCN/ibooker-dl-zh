["```py\nfrom typing import Annotated, TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI()\n\nclass State(TypedDict):\n    # Messages have the type \"list\". The `add_messages` \n    # function in the annotation defines how this state should \n    # be updated (in this case, it appends new messages to the \n    # list, rather than replacing the previous messages)\n    messages: Annotated[list, add_messages]\n\ndef chatbot(state: State):\n    answer = model.invoke(state[\"messages\"])\n    return {\"messages\": [answer]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"chatbot\", chatbot)\nbuilder.add_edge(START, 'chatbot')\nbuilder.add_edge('chatbot', END)\n\ngraph = builder.compile()\n```", "```py\nimport {\n  StateGraph,\n  Annotation,\n  messagesStateReducer,\n  START, END\n} from '@langchain/langgraph'\nimport {ChatOpenAI} from '@langchain/openai'\n\nconst model = new ChatOpenAI()\n\nconst State = {\n  /**\n * The State defines three things:\n * 1\\. The structure of the graph's state (which \"channels\" are available to \n * read/write)\n * 2\\. The default values for the state's channels\n * 3\\. The reducers for the state's channels. Reducers are functions that \n * determine how to apply updates to the state. Below, new messages are \n * appended to the messages array.\n */\n  messages: Annotation({\n    reducer: messagesStateReducer,\n    default: () => []\n  }),\n}\n\nasync function chatbot(state) {\n  const answer = await model.invoke(state.messages)\n  return {\"messages\": answer}\n}\n\nconst builder = new StateGraph(State)\n  .addNode('chatbot', chatbot)\n  .addEdge(START, 'chatbot')\n  .addEdge('chatbot', END)\n\nconst graph = builder.compile()\n```", "```py\ngraph.get_graph().draw_mermaid_png()\n```", "```py\nawait graph.getGraph().drawMermaidPng()\n```", "```py\ninput = {\"messages\": [HumanMessage('hi!)]}\nfor chunk in graph.stream(input):\n    print(chunk)\n```", "```py\nconst input = {messages: [new HumanMessage('hi!)]}\nfor await (const chunk of await graph.stream(input)) {\n  console.log(chunk)\n}\n```", "```py\n{ \"chatbot\": { \"messages\": [AIMessage(\"How can I help you?\")] } }\n```", "```py\nfrom typing import Annotated, TypedDict\n\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\n\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.graph.message import add_messages\n\n# useful to generate SQL query\nmodel_low_temp = ChatOpenAI(temperature=0.1)\n# useful to generate natural language outputs\nmodel_high_temp = ChatOpenAI(temperature=0.7)\n\nclass State(TypedDict):\n    # to track conversation history\n    messages: Annotated[list, add_messages]\n    # input\n    user_query: str\n    # output\n    sql_query: str\n    sql_explanation: str\n\nclass Input(TypedDict):\n    user_query: str\n\nclass Output(TypedDict):\n    sql_query: str\n    sql_explanation: str\n\ngenerate_prompt = SystemMessage(\n    \"\"\"You are a helpful data analyst who generates SQL queries for users based \n on their questions.\"\"\"\n)\n\ndef generate_sql(state: State) -> State:\n    user_message = HumanMessage(state[\"user_query\"])\n    messages = [generate_prompt, *state[\"messages\"], user_message]\n    res = model_low_temp.invoke(messages)\n    return {\n        \"sql_query\": res.content,\n        # update conversation history\n        \"messages\": [user_message, res],\n    }\n\nexplain_prompt = SystemMessage(\n    \"You are a helpful data analyst who explains SQL queries to users.\"\n)\n\ndef explain_sql(state: State) -> State:\n    messages = [\n        explain_prompt,\n        # contains user's query and SQL query from prev step\n        *state[\"messages\"],\n    ]\n    res = model_high_temp.invoke(messages)\n    return {\n        \"sql_explanation\": res.content,\n        # update conversation history\n        \"messages\": res,\n    }\n\nbuilder = StateGraph(State, input=Input, output=Output)\nbuilder.add_node(\"generate_sql\", generate_sql)\nbuilder.add_node(\"explain_sql\", explain_sql)\nbuilder.add_edge(START, \"generate_sql\")\nbuilder.add_edge(\"generate_sql\", \"explain_sql\")\nbuilder.add_edge(\"explain_sql\", END)\n\ngraph = builder.compile()\n```", "```py\nimport {\n  HumanMessage,\n  SystemMessage\n} from \"@langchain/core/messages\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport {\n  StateGraph,\n  Annotation,\n  messagesStateReducer,\n  START,\n  END,\n} from \"@langchain/langgraph\";\n\n// useful to generate SQL query\nconst modelLowTemp = new ChatOpenAI({ temperature: 0.1 });\n// useful to generate natural language outputs\nconst modelHighTemp = new ChatOpenAI({ temperature: 0.7 });\n\nconst annotation = Annotation.Root({\n  messages: Annotation({ reducer: messagesStateReducer, default: () => [] }),\n  user_query: Annotation(),\n  sql_query: Annotation(),\n  sql_explanation: Annotation(),\n});\n\nconst generatePrompt = new SystemMessage(\n  `You are a helpful data analyst who generates SQL queries for users based on \n their questions.`\n);\n\nasync function generateSql(state) {\n  const userMessage = new HumanMessage(state.user_query);\n  const messages = [generatePrompt, ...state.messages, userMessage];\n  const res = await modelLowTemp.invoke(messages);\n  return {\n    sql_query: res.content as string,\n    // update conversation history\n    messages: [userMessage, res],\n  };\n}\n\nconst explainPrompt = new SystemMessage(\n  \"You are a helpful data analyst who explains SQL queries to users.\"\n);\n\nasync function explainSql(state) {\n  const messages = [explainPrompt, ...state.messages];\n  const res = await modelHighTemp.invoke(messages);\n  return {\n    sql_explanation: res.content as string,\n    // update conversation history\n    messages: res,\n  };\n}\n\nconst builder = new StateGraph(annotation)\n  .addNode(\"generate_sql\", generateSql)\n  .addNode(\"explain_sql\", explainSql)\n  .addEdge(START, \"generate_sql\")\n  .addEdge(\"generate_sql\", \"explain_sql\")\n  .addEdge(\"explain_sql\", END);\n\nconst graph = builder.compile();\n```", "```py\ngraph.invoke({\n  \"user_query\": \"What is the total sales for each product?\"\n})\n```", "```py\nawait graph.invoke({\n  user_query: \"What is the total sales for each product?\"\n})\n```", "```py\n{\n  \"sql_query\": \"SELECT product_name, SUM(sales_amount) AS total_sales\\nFROM \n      sales\\nGROUP BY product_name;\",\n  \"sql_explanation\": \"This query will retrieve the total sales for each product \n      by summing up the sales_amount column for each product and grouping the\n      results by product_name.\",\n}\n```", "```py\nfrom typing import Annotated, Literal, TypedDict\n\nfrom langchain_core.documents import Document\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.vectorstores.in_memory import InMemoryVectorStore\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.graph.message import add_messages\n\nembeddings = OpenAIEmbeddings()\n# useful to generate SQL query\nmodel_low_temp = ChatOpenAI(temperature=0.1)\n# useful to generate natural language outputs\nmodel_high_temp = ChatOpenAI(temperature=0.7)\n\nclass State(TypedDict):\n    # to track conversation history\n    messages: Annotated[list, add_messages]\n    # input\n    user_query: str\n    # output\n    domain: Literal[\"records\", \"insurance\"]\n    documents: list[Document]\n    answer: str\n\nclass Input(TypedDict):\n    user_query: str\n\nclass Output(TypedDict):\n    documents: list[Document]\n    answer: str\n\n# refer to Chapter 2 on how to fill a vector store with documents\nmedical_records_store = InMemoryVectorStore.from_documents([], embeddings)\nmedical_records_retriever = medical_records_store.as_retriever()\n\ninsurance_faqs_store = InMemoryVectorStore.from_documents([], embeddings)\ninsurance_faqs_retriever = insurance_faqs_store.as_retriever()\n\nrouter_prompt = SystemMessage(\n    \"\"\"You need to decide which domain to route the user query to. You have two \n domains to choose from:\n - records: contains medical records of the patient, such as \n diagnosis, treatment, and prescriptions.\n - insurance: contains frequently asked questions about insurance \n policies, claims, and coverage.\n\nOutput only the domain name.\"\"\"\n)\n\ndef router_node(state: State) -> State:\n    user_message = HumanMessage(state[\"user_query\"])\n    messages = [router_prompt, *state[\"messages\"], user_message]\n    res = model_low_temp.invoke(messages)\n    return {\n        \"domain\": res.content,\n        # update conversation history\n        \"messages\": [user_message, res],\n    }\n\ndef pick_retriever(\n    state: State,\n) -> Literal[\"retrieve_medical_records\", \"retrieve_insurance_faqs\"]:\n    if state[\"domain\"] == \"records\":\n        return \"retrieve_medical_records\"\n    else:\n        return \"retrieve_insurance_faqs\"\n\ndef retrieve_medical_records(state: State) -> State:\n    documents = medical_records_retriever.invoke(state[\"user_query\"])\n    return {\n        \"documents\": documents,\n    }\n\ndef retrieve_insurance_faqs(state: State) -> State:\n    documents = insurance_faqs_retriever.invoke(state[\"user_query\"])\n    return {\n        \"documents\": documents,\n    }\n\nmedical_records_prompt = SystemMessage(\n    \"\"\"You are a helpful medical chatbot who answers questions based on the \n patient's medical records, such as diagnosis, treatment, and \n prescriptions.\"\"\"\n)\n\ninsurance_faqs_prompt = SystemMessage(\n    \"\"\"You are a helpful medical insurance chatbot who answers frequently asked \n questions about insurance policies, claims, and coverage.\"\"\"\n)\n\ndef generate_answer(state: State) -> State:\n    if state[\"domain\"] == \"records\":\n        prompt = medical_records_prompt\n    else:\n        prompt = insurance_faqs_prompt\n    messages = [\n        prompt,\n        *state[\"messages\"],\n        HumanMessage(f\"Documents: {state[\"documents\"]}\"),\n    ]\n    res = model_high_temp.invoke(messages)\n    return {\n        \"answer\": res.content,\n        # update conversation history\n        \"messages\": res,\n    }\n\nbuilder = StateGraph(State, input=Input, output=Output)\nbuilder.add_node(\"router\", router_node)\nbuilder.add_node(\"retrieve_medical_records\", retrieve_medical_records)\nbuilder.add_node(\"retrieve_insurance_faqs\", retrieve_insurance_faqs)\nbuilder.add_node(\"generate_answer\", generate_answer)\nbuilder.add_edge(START, \"router\")\nbuilder.add_conditional_edges(\"router\", pick_retriever)\nbuilder.add_edge(\"retrieve_medical_records\", \"generate_answer\")\nbuilder.add_edge(\"retrieve_insurance_faqs\", \"generate_answer\")\nbuilder.add_edge(\"generate_answer\", END)\n\ngraph = builder.compile()\n```", "```py\nimport {\n  HumanMessage,\n  SystemMessage\n} from \"@langchain/core/messages\";\nimport {\n  ChatOpenAI,\n  OpenAIEmbeddings\n} from \"@langchain/openai\";\nimport {\n  MemoryVectorStore\n} from \"langchain/vectorstores/memory\";\nimport {\n  DocumentInterface\n} from \"@langchain/core/documents\";\nimport {\n  StateGraph,\n  Annotation,\n  messagesStateReducer,\n  START,\n  END,\n} from \"@langchain/langgraph\";\n\nconst embeddings = new OpenAIEmbeddings();\n// useful to generate SQL query\nconst modelLowTemp = new ChatOpenAI({ temperature: 0.1 });\n// useful to generate natural language outputs\nconst modelHighTemp = new ChatOpenAI({ temperature: 0.7 });\n\nconst annotation = Annotation.Root({\n  messages: Annotation({ reducer: messagesStateReducer, default: () => [] }),\n  user_query: Annotation(),\n  domain: Annotation(),\n  documents: Annotation(),\n  answer: Annotation(),\n});\n\n// refer to Chapter 2 on how to fill a vector store with documents\nconst medicalRecordsStore = await MemoryVectorStore.fromDocuments(\n  [],\n  embeddings\n);\nconst medicalRecordsRetriever = medicalRecordsStore.asRetriever();\n\nconst insuranceFaqsStore = await MemoryVectorStore.fromDocuments(\n  [],\n  embeddings\n);\nconst insuranceFaqsRetriever = insuranceFaqsStore.asRetriever();\n\nconst routerPrompt = new SystemMessage(\n  `You need to decide which domain to route the user query to. You have two \n domains to choose from:\n - records: contains medical records of the patient, such as diagnosis, \n treatment, and prescriptions.\n - insurance: contains frequently asked questions about insurance \n policies, claims, and coverage.\n\nOutput only the domain name.`\n);\n\nasync function routerNode(state) {\n  const userMessage = new HumanMessage(state.user_query);\n  const messages = [routerPrompt, ...state.messages, userMessage];\n  const res = await modelLowTemp.invoke(messages);\n  return {\n    domain: res.content as \"records\" | \"insurance\",\n    // update conversation history\n    messages: [userMessage, res],\n  };\n}\n\nfunction pickRetriever(state) {\n  if (state.domain === \"records\") {\n    return \"retrieve_medical_records\";\n  } else {\n    return \"retrieve_insurance_faqs\";\n  }\n}\n\nasync function retrieveMedicalRecords(state) {\n  const documents = await medicalRecordsRetriever.invoke(state.user_query);\n  return {\n    documents,\n  };\n}\n\nasync function retrieveInsuranceFaqs(state) {\n  const documents = await insuranceFaqsRetriever.invoke(state.user_query);\n  return {\n    documents,\n  };\n}\n\nconst medicalRecordsPrompt = new SystemMessage(\n  `You are a helpful medical chatbot who answers questions based on the \n patient's medical records, such as diagnosis, treatment, and \n prescriptions.`\n);\n\nconst insuranceFaqsPrompt = new SystemMessage(\n  `You are a helpful medical insurance chatbot who answers frequently asked \n questions about insurance policies, claims, and coverage.`\n);\n\nasync function generateAnswer(state) {\n  const prompt =\n    state.domain === \"records\" ? medicalRecordsPrompt : insuranceFaqsPrompt;\n  const messages = [\n    prompt,\n    ...state.messages,\n    new HumanMessage(`Documents: ${state.documents}`),\n  ];\n  const res = await modelHighTemp.invoke(messages);\n  return {\n    answer: res.content as string,\n    // update conversation history\n    messages: res,\n  };\n}\n\nconst builder = new StateGraph(annotation)\n  .addNode(\"router\", routerNode)\n  .addNode(\"retrieve_medical_records\", retrieveMedicalRecords)\n  .addNode(\"retrieve_insurance_faqs\", retrieveInsuranceFaqs)\n  .addNode(\"generate_answer\", generateAnswer)\n  .addEdge(START, \"router\")\n  .addConditionalEdges(\"router\", pickRetriever)\n  .addEdge(\"retrieve_medical_records\", \"generate_answer\")\n  .addEdge(\"retrieve_insurance_faqs\", \"generate_answer\")\n  .addEdge(\"generate_answer\", END);\n\nconst graph = builder.compile();\n```", "```py\ninput = {\n    \"user_query\": \"Am I covered for COVID-19 treatment?\"\n}\nfor c in graph.stream(input):\n    print(c)\n```", "```py\nconst input = {\n  user_query: \"Am I covered for COVID-19 treatment?\"\n}\nfor await (const chunk of await graph.stream(input)) {\nconsole.log(chunk)\n}\n```", "```py\n{\n    \"router\": {\n        \"messages\": [\n            HumanMessage(content=\"Am I covered for COVID-19 treatment?\"),\n            AIMessage(content=\"insurance\"),\n        ],\n        \"domain\": \"insurance\",\n    }\n}\n{\n    \"retrieve_insurance_faqs\": {\n        \"documents\": [...]\n    }\n}\n{\n    \"generate_answer\": {\n        \"messages\": AIMessage(\n            content=\"...\",\n        ),\n        \"answer\": \"...\",\n    }\n}\n```"]