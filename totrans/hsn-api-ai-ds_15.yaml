- en: Chapter 12\. Using APIs with Artificial Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More AI means more APIs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Frank Kilcommins, SmartBear
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In technology circles, AI and APIs are sometimes treated as separate specialties.
    But they are closely related, and getting closer all the time. In this chapter,
    you will learn about the ways that AI and APIs overlap, some of the skills you
    should develop, and how to build APIs that are compatible with AI. Then, you will
    set up your Part III portfolio project, which you will use in the remaining chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The Overlap of AI and APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To begin with, APIs are important data sources—along with databases and files—for
    training AI models. Once a model is trained, a REST API is a common method to
    make it available for users. You will train a machine learning model in [Chapter 13](ch13.html#chapter_13),
    and deploy it with a REST API.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, cloud-based AI tools are advanced machine learning models that
    are deployed using APIs. AI tools such as generative AI, natural language processing,
    and others are often cloud hosted and made available as APIs. You will call an
    Anthropic large language model (LLM) through a REST API in [Chapter 14](ch14.html#chapter_14).
  prefs: []
  type: TYPE_NORMAL
- en: An emerging area of overlap between AI and APIs is calling APIs directly from
    *generative AI* applications, which are built using LLMs and interact with users
    via natural language. One type of these applications is called retrieval augmented
    generation (RAG). In a RAG application, the program calls APIs and other data
    sources and then feeds the retrieved information to the LLM along with the user
    prompt. This helps overcome the knowledge gap, in which an LLM only has information
    that it was trained upon.
  prefs: []
  type: TYPE_NORMAL
- en: Another type of generative AI application uses LLMs to determine what API endpoints
    to use, which we will call *agentic* applications in this book. The LLMs make
    this decision by interpreting definitions from OAS files, Python code, or API
    documentation. You will create agentic AI applications that call APIs in [Chapter 14](ch14.html#chapter_14)
    with LangChain and in [Chapter 15](ch15.html#chapter_15) with ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Designing APIs to Use with Generative AI and LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, how should you design an API so that a generative AI application can use
    it, as Doerrfield recommends? This field is changing rapidly, but here are some
    initial tips that apply to LangChain ([Chapter 14](ch14.html#chapter_14)) and
    ChatGPT ([Chapter 15](ch15.html#chapter_15)).
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to consider whether an API or endpoint is appropriate to use
    with an agentic generative AI application without additional safeguards in place.
    The providers of LLMs provide warnings such as that LLMs should not be used “on
    their own in high-risk situations” (Anthropic Claude 3) and that they “may sometimes
    provide inaccurate information” (Google NotebookLM). ChatGPT’s documentation simply
    admonishes the user to “check important info.”
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Researchers and machine learning engineers are exploring additional methods
    to address risks of using LLMs for API calls and performing other business tasks.
    Some potential safeguards include requiring a human to approve tasks recommended
    by LLMs before executing, combining multiple AI agents to review tasks before
    executing, reviewing and filtering inputs and outputs to the models, and reviewing
    logs of the functioning of the system. In addition, foundational practices of
    API management and security are required when LLMs use APIs—just as they are when
    conventional software uses APIs. A key security practice is to restrict the permissions
    provided to systems that include LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For APIs that are appropriate, here are some design tips based on my own experience
    and on Blobr’s [“Is Your API AI-ready? Our Guidelines and Best Practices”](https://oreil.ly/jxllA):'
  prefs: []
  type: TYPE_NORMAL
- en: Limit the size of the data results.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is important for cost and accuracy. From a cost perspective, model providers
    charge for processing *tokens*, which are chunks of text. The size of these tokens
    differs, but the bottom line is the same: the more data a model processes, the
    greater the cost. In addition to the cost, developers using ChatGPT have found
    that it struggles to perform calculations from very large datasets returned by
    APIs. If you are using a model to perform calculations, limiting the size of the
    data results improves its accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a few ways to accomplish this. Rather than returning all fields related
    to an entity, return only the critical fields. If an API returns child records
    in a collection (e.g., `product.orders`), exclude these from the results and make
    them available in a separate endpoint. Add parameters, filters, and pagination
    to narrow down the specific records in the API call.
  prefs: []
  type: TYPE_NORMAL
- en: Make data structures consistent throughout the API.
  prefs: []
  type: TYPE_NORMAL
- en: The more predictable the API is, the more accurately an AI can use it. By re-using
    schemas inside your APIs and defining them in your OAS file, you will help the
    LLM know what to expect in the results. You used Pydantic in the API you created
    in [Part I](part01.html#part_1), which enforced standard schemas and published
    them in your OAS file.
  prefs: []
  type: TYPE_NORMAL
- en: Provide a software development kit (SDK).
  prefs: []
  type: TYPE_NORMAL
- en: Providing an SDK is a way to provide a subset of endpoints and customized API
    calls that are appropriate for an AI application. The SDK can also include detailed
    explanations of the API calls and parameters that assist an LLM in understanding
    its usage. In [Chapter 14](ch14.html#chapter_14), you’ll use the swcpy SDK with
    LangChain and LangGraph.
  prefs: []
  type: TYPE_NORMAL
- en: Customize your OpenAPI Specification (OAS).
  prefs: []
  type: TYPE_NORMAL
- en: Some methods of using generative AI support reading the OAS file to infer API
    endpoints to call. You can create a customized OAS file with AI-appropriate endpoints
    and detailed descriptions of each endpoint and parameter that assist the LLM in
    inferring their meaning. Endpoints in the OAS file should have unique and clear
    operation IDs.
  prefs: []
  type: TYPE_NORMAL
- en: Provide a separate endpoint for summary statistics.
  prefs: []
  type: TYPE_NORMAL
- en: If users ask the AI questions about summary information and counts, the LLM’s
    behavior can be erratic. It may try to perform a scan of every record in the API,
    it may just look at the record identifiers and infer this is the count, or it
    may try something completely different. Providing dedicated endpoints takes some
    of the guesswork out.
  prefs: []
  type: TYPE_NORMAL
- en: Provide a search endpoint that doesn’t rely on a record identifier.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are more comfortable using language than numbers. They like to search based
    on the information that users are likely to ask them.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Artificial Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Artificial Intelligence is technology that enables computers and machines to
    simulate human learning, comprehension, problem solving, decision making, creativity
    and autonomy.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “What Is Artificial Intelligence (AI)?”, Cole Stryker and Eda Kavlakoglu, IBM
    Corporation, 2024
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Aside from that formal definition of AI, an informal definition today is that
    AI is a computer program that can have humanlike conversations and complete humanlike
    tasks. AI includes *expert systems*, which have been around for decades. These
    are complicated rules-based systems that can perform humanlike tasks. Modern AI
    focuses on *machine learning*, in which researchers direct the training of models
    but don’t explicitly program them. Generative AI using LLMs is one major application
    of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-1](#general_ID_diagram_ch12) demonstrates how these terms relate
    to one another.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of AI terminology](assets/haad_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. Diagram of AI terminology
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generative AI and Large Language Models (LLMs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although machine learning is used in many different applications, most of the
    attention from the general public in recent years has been given to generative
    AI. The ability of these applications to generate text, music, and videos based
    on text prompts has led to rapid adoption in applications such as OpenAI’s ChatGPT,
    Microsoft’s Copilot, Google’s Gemini, and many additions to other software applications.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the impressive capabilities of these applications, generative AI also
    has many risks and limitations associated with it. Providers of some popular models
    include warnings about bias, hallucinations, mistakes, and harmful content. These
    are major risks that should be taken seriously by developers who use them. Chapters
    [14](ch14.html#chapter_14) and [15](ch15.html#chapter_15) have more details on
    the risks and limitations of the models demonstrated in those chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Agentic AI Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As Doerrfield mentions, AI agents are at the forefront of AI research and development.
    An *agent* is software that controls application flow using an LLM. The more autonomously
    the LLM controls the system, the more *agentic* the system is.
  prefs: []
  type: TYPE_NORMAL
- en: Creating AI agents using LLMs is a new field, and a variety of different tools
    have been released to create agents or orchestrate multiple agents to perform
    tasks. [Table 12-1](#frameworks_table_ch12) lists several open source frameworks
    for developing agents and LLM-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12-1\. AI agent frameworks
  prefs: []
  type: TYPE_NORMAL
- en: '| Software | Programming languages supported |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Autogen | Python, dotnet |'
  prefs: []
  type: TYPE_TB
- en: '| CrewAI | Python |'
  prefs: []
  type: TYPE_TB
- en: '| LangChain/LangGraph | Python |'
  prefs: []
  type: TYPE_TB
- en: '| LlamaIndex | Python, Typescript |'
  prefs: []
  type: TYPE_TB
- en: '| PydanticAI | Python |'
  prefs: []
  type: TYPE_TB
- en: '| Vercel AI SDK | Typescript |'
  prefs: []
  type: TYPE_TB
- en: You will use LangChain and LangGraph in [Chapter 14](ch14.html#chapter_14) to
    call APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Your Part III Portfolio Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will create a portfolio project that demonstrates your ability to work
    with API and AI. Here is an overview of the work ahead of you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 13](ch13.html#chapter_13): Deploying a machine learning API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 14](ch14.html#chapter_14): Using APIs with LangChain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 15](ch15.html#chapter_15): Using ChatGPT to call your API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these tasks will enable you to showcase your API and AI skills in a
    unique way.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Your GitHub Codespace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will continue to use GitHub Codespaces for all the code you develop in Part
    III. If you didn’t create a GitHub account yet, do that now.
  prefs: []
  type: TYPE_NORMAL
- en: Cloning the Part III Repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of the [Part III](part03.html#part_3) code examples are contained in [this
    book’s GitHub repository](https://github.com/handsonapibook/api-book-part-three).
  prefs: []
  type: TYPE_NORMAL
- en: 'To clone the repository, log in to GitHub and go the [GitHub Import Repository
    page](https://github.com/new/import). Enter the following information in the fields
    on this page:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The URL for your source repository: **`https://github.com/handsonapibook/api-book-part-three`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Your username for your source code repository: Leave blank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Your access token or password for your source code repository: Leave blank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Repository name: **`ai-project`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Public: Select this so that you can share the results of the work you are doing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click Begin Import. The import process will begin, and the message “Preparing
    your new repository” will be displayed. After several minutes, you will receive
    an email notifying you that your import has finished. Follow the link to your
    new cloned repository.
  prefs: []
  type: TYPE_NORMAL
- en: Launching Your GitHub Codespace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In your new repository, click the Code button and select the Codespaces tab.
    Click “Create codespace on main.” You should see a page with the status “Setting
    up your codespace”. Your Codespace window will be opened as the setup continues.
    When the setup completes, your display will look similar to [Figure 12-2](#codespace_setup_complete_ch12).
  prefs: []
  type: TYPE_NORMAL
- en: '![GitHub Codespace for Part 3](assets/haad_1202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. GitHub Codespace for Part III
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Your Codespace is now created with the cloned repository. This is the environment
    you will be using for Part III of this book. Open the [GitHub Codespaces page](https://oreil.ly/nLbqH)
    and scroll down the page to find this new Codespace, click the ellipsis to the
    right of the name, and select Rename. Enter the name **`Part 3 Portfolio project
    codespace`** and click Save. You should see the message “Your codespace *Part
    3 Portfolio project codespace* has been updated.” Click the ellipsis again and
    then click the ribbon next to “Auto-delete codespace” to turn off auto-deletion.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To save space on the page, I have trimmed the directory listing in the terminal
    prompt of my Codespace. You can do this in your Codespace by editing the */home/codespace/.bashrc*
    file in VS Code. Find the `export PROMPT_DIRTRIM` statement and set it to `export
    PROMPT_DIRTRIM=1`. To load the values the first time, execute this terminal command:
    `source ~/.bashrc`.'
  prefs: []
  type: TYPE_NORMAL
- en: Additional Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To view a 100-point scorecard for AI compatibility of APIs, read Blobr’s [“Is
    Your API AI-ready? Our Guidelines and Best Practices”](https://oreil.ly/JLaK1).
  prefs: []
  type: TYPE_NORMAL
- en: 'To see an example of working around the limitations of a GPT, read [“Syntax
    Sunday: Custom API Wrapper for GPTs” by Kade Halabuza](https://oreil.ly/khh0J).'
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about possible futures of AI and APIs, read [“AI + APIs — What
    12 Experts Think The Future Holds” by Peter Schroeder](https://oreil.ly/t2lxH).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the basics of AI and explained how it relates to APIs.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 13](ch13.html#chapter_13), you will create a machine learning model
    and deploy it using FastAPI.
  prefs: []
  type: TYPE_NORMAL
