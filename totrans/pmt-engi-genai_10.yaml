- en: Chapter 10\. Building AI-Powered Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ll apply the five principles of prompting to an end-to-end
    AI workflow for content writing. The service will write blog posts based on the
    user’s responses to interview questions, in the style of the user’s writing. This
    system was first documented on the [Saxifrage blog](https://oreil.ly/saxifrage).
  prefs: []
  type: TYPE_NORMAL
- en: AI Blog Writing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The naive approach to creating a blog writing service using AI would be to prompt
    ChatGPT with `Write a blog post on {blogPostTopic}`. The resulting content would
    be of reasonable quality but wouldn’t likely contain any valuable opinions or
    unique experiences on the topic. The content would also likely be short and generic
    and therefore unlikely to rank on Google.
  prefs: []
  type: TYPE_NORMAL
- en: A more sophisticated approach might be to build up a longer prompt with further
    instructions. Detail on the prescribed writing tone, architecture of the blog
    post, and keywords to include could be added. An example of a common blog post
    [writing prompt](https://oreil.ly/uMfZa) can be seen here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This longer, more sophisticated prompt is likely to result in better quality
    content. However, let’s run through the five principles of prompting as a checklist:'
  prefs: []
  type: TYPE_NORMAL
- en: Direction
  prefs: []
  type: TYPE_NORMAL
- en: There are some instructions provided, such as the tone, using transition words,
    and an active voice. However, the content is still likely to sound like AI, and
    not like the user.
  prefs: []
  type: TYPE_NORMAL
- en: Format
  prefs: []
  type: TYPE_NORMAL
- en: Although there are some mentions of structure, including dictating nine sections
    of two paragraphs, it’s likely these instructions will be ignored. ChatGPT is
    bad at math and is often unable to follow instructions dictating a number of sections
    or words.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: There are no samples of how to do the task given, which is likely to harm the
    reliability of running this prompt across multiple topics or even multiple times
    on the same topic. Even providing one example (a one-shot prompt) could radically
    help improve quality.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs: []
  type: TYPE_NORMAL
- en: This is an example of *blind prompting* (adding instructions to a prompt [without
    testing them](https://oreil.ly/r7sXi)). It’s likely some of these instructions
    make no difference to quality (unnecessarily costing tokens) or might even degrade
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: Division
  prefs: []
  type: TYPE_NORMAL
- en: The entire task is attempted with just one prompt, which is likely to harm performance.
    Without breaking the task into subtasks, it’s hard to understand which part of
    the process is suceeding or failing.
  prefs: []
  type: TYPE_NORMAL
- en: Through this chapter, you’ll create multiple LLM chain components. Each chain
    will be implemented in LangChain to make it more maintainable and to give easy
    logging for monitoring and optimization. The resulting system will help you generate
    *human-sounding* content based on the unique opinions and experiences of the user.
  prefs: []
  type: TYPE_NORMAL
- en: It’s crucial that you first prepare your workspace with the necessary tools.
    Therefore, let’s shift our focus toward topic research and start setting up your
    programming environment.
  prefs: []
  type: TYPE_NORMAL
- en: Topic Research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need to install several Python packages to effectively use LangChain’s
    document loaders, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: google-searchresults
  prefs: []
  type: TYPE_NORMAL
- en: A Python library designed to scrape and process Google search results.
  prefs: []
  type: TYPE_NORMAL
- en: pandas
  prefs: []
  type: TYPE_NORMAL
- en: This offers data structures and operations for manipulating numerical tables
    and time series data.
  prefs: []
  type: TYPE_NORMAL
- en: html2text
  prefs: []
  type: TYPE_NORMAL
- en: This tool converts HTML from files or web pages into markdown (*.md*) files
    or text.
  prefs: []
  type: TYPE_NORMAL
- en: pytest-playwright
  prefs: []
  type: TYPE_NORMAL
- en: This package enables end-to-end testing with Playwright.
  prefs: []
  type: TYPE_NORMAL
- en: chromadb
  prefs: []
  type: TYPE_NORMAL
- en: ChromaDB is an open source vector database.
  prefs: []
  type: TYPE_NORMAL
- en: nest_asyncio
  prefs: []
  type: TYPE_NORMAL
- en: This extends the Python standard `asyncio` to patch and render it compatible
    with Jupyter Notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Installation of these packages can be achieved easily with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, you’ll be using LangChain’s document loaders that require Playwright.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type this command on your terminal: **playwright install**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, you’ll need to choose a `TOPIC` and set environment variables
    for both `SERPAPI_API_KEY` and `STABILITY_API_KEY`. If you’re running the script
    without Jupyter Notebook, then you won’t need to use any of the `nest_asyncio`
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you’ll focus on summarizing web content efficiently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: First, import the required tools and then fetch the web page content related
    to your `TOPIC`. After setting up your `ChatOpenAI` model, you’ll utilize a `text_splitter`
    to manage text chunks. The splitter ensures no snippet is too long, while maintaining
    context with overlap. Then create the `PydanticOutputParser` to handle and structure
    the summaries. By feeding the extracted documents through a dedicated summarization
    function, the LLM produces concise summaries.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to dive deeper into the `create_all_summaries` function, check
    [*custom_summarize_chain.py*](https://oreil.ly/KyKjS).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key points to highlight are that you can *subclass* most classes within
    LangChain. For example, you can overide the default `ChromiumLoader` to be asynchronous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: By subclassing `ChromiumLoader`, you can easily create a custom implementation
    to *asynchronously scrape content* from multiple URLs using the Chrome browser.
    `get_html_content_from_urls` fetches HTML content from a list of URLs, ensuring
    no duplicates and handling potential errors.
  prefs: []
  type: TYPE_NORMAL
- en: Expert Interview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you’ve successfully extracted the summaries from Google for the top
    three results, you’ll conduct an interview with an LLM, generating relevant questions
    to make sure that your article has a unique perspective using an `InterviewChain`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: InterviewChain instantiation
  prefs: []
  type: TYPE_NORMAL
- en: With your topic and obtained summaries in hand, create an instance of `InterviewChain`,
    tailoring it to your data’s unique context.
  prefs: []
  type: TYPE_NORMAL
- en: Generating questions
  prefs: []
  type: TYPE_NORMAL
- en: By simply calling the `interview_chain`, you kickstart the process of generating
    a series of probing questions derived from your summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive Q&A session
  prefs: []
  type: TYPE_NORMAL
- en: Dive into an engaging loop where each derived question is printed, prompting
    you for an answer with `input()`. Your response is then saved back to the Pydantic
    object.
  prefs: []
  type: TYPE_NORMAL
- en: Give Direction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Giving an LLM unique answers provides unique context, and this allows an LLM
    to generate richer, more nuanced responses, ensuring your article offers a fresh
    and in-depth perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the code for `InterviewChain` is in *[expert_interview_chain.py](https://oreil.ly/0d5Hi)*.
    It has two significant components:'
  prefs: []
  type: TYPE_NORMAL
- en: A custom `System` message
  prefs: []
  type: TYPE_NORMAL
- en: 'This prompt includes role prompting, previously generated summaries, the topic,
    and format instructions (for the output parser):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Output parsers
  prefs: []
  type: TYPE_NORMAL
- en: 'Diving deeper into the class, you encounter the `PydanticOutputParser`. This
    parser actively structures the LLMs responses into parsable, Pydantic `InterviewQuestions`
    objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In essence, you’re orchestrating a conversation with the AI and instructing
    it to conceive potent questions that amplify content insights, all the while making
    customization a breeze.
  prefs: []
  type: TYPE_NORMAL
- en: Generate Outline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Including the previous interview and research, you can generate an outline
    for the post with `BlogOutlineGenerator`. The `TOPIC`, `question_answers`, and
    Google `summaries` are passed to provide additional context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s explore the `BlogOutlineGenerator` class in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'A `BlogOutline` Pydantic object is created that contains `title` and `sub_headings`
    keys. Also, the outline chain is set up using LangChain expression language (LCEL)
    that passes the prompt into the chat model and then finally into the output parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: By using a Pydantic output parser, the chain will return a `BlogOutline` Pydantic
    object that will be used in future chains.
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After obtaining a summary, interview questions, and a blog post outline, it’s
    time to start generating the text. The `ContentGenerator` class integrates SEO
    expertise with several LLM techniques, which include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings and retrieval
  prefs: []
  type: TYPE_NORMAL
- en: This efficiently splits and vectorizes original web pages, storing them in the
    Chroma database and retrieving relevent web page text while writing each section.
  prefs: []
  type: TYPE_NORMAL
- en: Custom memory
  prefs: []
  type: TYPE_NORMAL
- en: While crafting each blog section, it uses memory to avoid repeating the same
    information, while also summarizing the conversation if it becomes too long.
  prefs: []
  type: TYPE_NORMAL
- en: Bespoke context
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLM has a mixture of information, including your previous interview insights,
    what has been said before, and snippets of relevant web page text from Google:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: All of the source code is within *[article_generation.py](https://oreil.ly/0IFyI)*,
    but let’s specifically focus on three components that are key to this chain.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `OnlyStoreAIMemory` class is a customized subclass of `ConversationSummary​BufferMemory`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It’s tailored to ensure that the chat messages memory remains concise and relevant
    by *exclusively storing AI-generated messages*.
  prefs: []
  type: TYPE_NORMAL
- en: This deliberate choice bypasses storing retrieved documents that are used within
    the generation step, preventing memory bloat. Furthermore, the memory mechanism
    ensures the AI remains aware of its prior writings, enabling it to offer condensed
    summaries if the accumulated context surpasses set limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `generate_blog_post` function loops through all of the subheadings and
    tries to retrieve as many relevant documents as possible while fitting in the
    current context length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This function, `generate_blog_post`, iterates over each subheading. It attempts
    to fetch up to five relevant documents. If there’s an issue fetching the documents,
    it smartly decreases the number and tries again. If all attempts fail, it gracefully
    defaults to no documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the prompt for generating each section is very context rich:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `section_prompt` elegantly sets the stage by announcing the specific section
    you’re working on, using `{subheading.title}`. But it doesn’t stop there. By feeding
    the LLM with `{relevant_documents}`, it offers background and depth, while explicitly
    cautioning against plagiarism. Moreover, by including insights from your interview
    via `{self.questions_and_answers}`, the prompt ensures that valuable information
    is front and center. Finally, it sets clear expectations on the format, the inclusion
    of certain features, and the topic at hand. This makes the LLM not just a tool
    but an informed coauthor, working diligently alongside you to create content.
  prefs: []
  type: TYPE_NORMAL
- en: Writing Style
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that the article is written, we can go a step further in terms of making
    it sound uniquely human, by rewriting the content in a specific writing style.
    This will go a long way in making the content less detectable as obviously AI
    (though ethically you should still declare any AI assistance) and decreasing the
    amount of time you spend editing the final draft before publishing.
  prefs: []
  type: TYPE_NORMAL
- en: Before rewriting, you need to know what writing style you want to emulate, be
    it your own or someone else’s. One common approach is to ask ChatGPT to summarize
    the writing style of someone who is famous, or at least popular enough in your
    industry to appear in ChatGPT’s training data. Commonly the model will want to
    respond with the name of the author and examples of writing, so adding instructions
    not to and ending the prompt with a bullet point (or an `-` character in this
    case) will give you the format you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Often provided only a single sample of text, ChatGPT can generate a reasonable
    writing style guide, which can then be used for rewriting. Once the writing style
    is defined, elements can be mixed and matched to arrive at a more ideal style.
    The following example takes elements from both Mike Taylor’s writing style and
    Harry Dry’s writing style from the previous example. This is another example of
    meme unbundling, as discussed in [“Meme Unbundling”](ch08.html#vector_databases_08):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In practice this part of the AI writing process is the most difficult to get
    right, and it’s the only one that requires the larger and more expensive GPT-4
    model to get passable results. If this part of the process isn’t right, the user
    can be left doing a lot of manual editing to get the writing in the house style.
    Given the strategic importance of this prompt, it makes sense to do a round of
    [prompt optimization](https://oreil.ly/H3VtJ), trying multiple approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'When optimizing prompts you can run the same prompt multiple times and check
    the average performance against an evaluation metric. As an example, here are
    the results of testing five different prompt approaches against an evaluation
    metric of embedding distance. The lower the score, the closer the embeddings of
    the response were to a reference answer (the text as rewritten manually is in
    the correct style). The prompts tested were as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A
  prefs: []
  type: TYPE_NORMAL
- en: Control—the standard prompt as detailed in the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: B
  prefs: []
  type: TYPE_NORMAL
- en: One-shot writing sample—we provided one sample of text, and asked GPT-4 to describe
    the writing style.
  prefs: []
  type: TYPE_NORMAL
- en: C
  prefs: []
  type: TYPE_NORMAL
- en: Three-shot rewriting example—we gave three samples of the input text to GPT-4
    and the rewritten version and asked it to describe the writing style.
  prefs: []
  type: TYPE_NORMAL
- en: D
  prefs: []
  type: TYPE_NORMAL
- en: Three-shot writing sample—same as previous, except without the input text, only
    the final samples of Mike’s writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'These prompts were [tested in an experiment we ran](https://oreil.ly/vRRYO)
    against three test cases—memetics, skyscraper technique, and value-based pricing—which
    were snippets of text that were first generated by ChatGPT on a topic, for example:
    *explain value-based pricing*. We then manually rewrote the text in the style
    we desired to make reference texts for comparison. The embedding distance was
    calculated by getting the embeddings for the reference text (from OpenAI’s `text-embedding-ada-002`)
    and comparing them to the embeddings for the output from the prompt, using *cosine
    similarity* (a method for calculating the distance between two sets of numbers),
    as detailed in [LangChain’s embedding evaluator](https://oreil.ly/400gJ) ([Figure 10-1](#figure-10-1)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![pega 1001](assets/pega_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Test results from prompt optimization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see from the results in [Figure 10-1](#figure-10-1), some prompts
    work better than others, and some cases are easier for the AI to deliver on. It’s
    important to test across multiple cases, with 10 or more runs per case, to get
    a realistic result for each prompt. Otherwise, the nondeterministic nature of
    the responses might mean you’ll think the performance was better or worse than
    you can actually expect when scaling up usage of a prompt. Here was the final
    resulting prompt that performed best:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate Quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Without testing the writing style, it would be hard to guess which prompting
    strategy would win. With a small amount of testing, you can be more confident
    this is the correct approach. Testing doesn’t have to be highly organized or systematized,
    and the builders of many successful AI products like [GitHub Copilot](https://oreil.ly/vu0IU)
    admit their eval process was haphazard and messy (but it got the job done!).
  prefs: []
  type: TYPE_NORMAL
- en: In this project we’ll use this well-tested example, but you may take this opportunity
    to try to beat this score. The repository with the reference texts and code is
    [publicly available on GitHub](https://oreil.ly/O6RdB), and please feel free to
    contribute to the repository if you find a better approach. One potential path
    to try is fine-tuning, which may get you better results in matching the writing
    style if you have enough samples ([OpenAI recommends at least 50](https://oreil.ly/OMMKi)).
    Even if you don’t perform an A/B test (comparing two versions of a prompt to see
    which one performs better) on this prompt, these results should convince you of
    the value of testing your prompts in general.
  prefs: []
  type: TYPE_NORMAL
- en: Title Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can optimize the content’s title by generating various options, testing
    them through A/B prompts, and gauging their effectiveness with a thumbs-up/thumbs-down
    rating system, as shown in [Figure 10-2](#figure-10-2).
  prefs: []
  type: TYPE_NORMAL
- en: '![pega 1002](assets/pega_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. A simple thumbs-up and thumbs-down rating system
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After evaluating all the prompts, you’ll be able to see which prompt had the
    highest average score and the token usage ([Figure 10-3](#figure-10-3)).
  prefs: []
  type: TYPE_NORMAL
- en: '![pega 1003](assets/pega_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Example A/B test results after manually evaluating a prompt
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you still aren’t getting the level of quality you need from this prompt,
    or the rest of the chain, this is a good time to experiment with a prompt optimization
    framework like [DSPy](https://oreil.ly/dspy). Upon defining an evaluation metric,
    DSPy tests different combinations of instructions and few-shot examples in your
    prompts, selecting the best-performing combination automatically. [See their documentation
    for examples](https://oreil.ly/vercel).
  prefs: []
  type: TYPE_NORMAL
- en: AI Blog Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One thing you can do to make your blog look more professional is to add custom
    illustrations to your blog posts, with a consistent style. At its maximum this
    may mean training a Dreambooth model, as covered in [Chapter 9](ch09.html#advanced_image_09),
    on your brand style guide or a mood board of images with a certain visual consistency
    or aesthetic quality you value. In many cases, however, training a custom model
    is not necessary, because a style can be replicated well using simple prompting.
  prefs: []
  type: TYPE_NORMAL
- en: One popular visual style among business-to-business (B2B) companies, [Corporate
    Memphis](https://oreil.ly/3UHQs), is characterized by its vibrant color palettes,
    bold and asymmetric shapes, and a mix of both organic and geometric forms. This
    style arose as a [costly signaling technique](https://oreil.ly/haoTZ), showing
    that the company could afford to commission custom illustrations from a designer
    and therefore was serious enough to be trusted. You can replicate this style with
    AI, saving yourself the cost of custom illustrations, while benefiting from the
    prior associations formed in consumers’ minds. [Figure 10-4](#figure-10-4) shows
    an example of Corporate Memphis style generated by Stable Diffusion, via the Stability
    AI API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 10-4](#figure-10-4) shows the output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![pega 1004](assets/pega_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-4\. Corporate Memphis: “websites being linked together”'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Give Direction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stable Diffusion is trained on many different styles, including obscure or niche
    styles like Corporate Memphis. If you know the name of a style, often that’s all
    that’s needed to guide the model toward the desired image. You can find a variety
    of art styles within this [visual prompt builder](https://oreil.ly/nxEzu).
  prefs: []
  type: TYPE_NORMAL
- en: In our blog writing project we could ask the user for an idea of what image
    they want to accompany the blog post, but let’s make it easier for them and automate
    this step. You can make an API call to ChatGPT and get back an idea for what could
    go in the image. When you get that response, it can form the basis of your prompt
    to Stability AI, a technique called *meta-prompting*, where one AI model writes
    the prompt for another AI model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Stability AI hosts Stable Diffusion, including the latest models like Stable
    Diffusion XL, in their DreamStudio platform. You can also call them [via API](https://oreil.ly/XD_jQ)
    or via the Stability AI SDK (a library that simplifies the process of making the
    API call). In the following example, we’ll create a function for calling Stability
    AI with our prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 10-5](#figure-10-5) shows the output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![pega 1005](assets/pega_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. A seamless collage or mosaic of diverse cultural elements from
    around the world
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To encapsulate the whole system for image generation, you can bring the call
    to ChatGPT and the resulting call to Stability AI together in one function that
    uses the `outline_result.title`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `create_image` function in *[image_generation_chain.py](https://oreil.ly/cWpXH)*
    utilizes Stable Diffusion to create an image based on a generated title from GPT-4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the high-level process:'
  prefs: []
  type: TYPE_NORMAL
- en: With the `ChatOpenAI` model, you’ll craft an image prompt for your given `title`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the Stability AI API, you’ll send this prompt to generate an image with
    precise styling instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then you’ll decode and save this image locally using a unique filename and return
    its path.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these steps, you’re not just prompting the AI to create textual content,
    but you’re directing it to bring your prompts to life visually.
  prefs: []
  type: TYPE_NORMAL
- en: This system is flexible based on whatever style you decide to use for blog images.
    Parameters can be adjusted as needed, and perhaps this API call can be replaced
    in future with a call to a custom fine-tuned Dreambooth model of your own. In
    the meantime, however, you have a quick and easy way to generate a custom image
    for each blog post, without requiring any further input from the user, in a consistent
    visual style.
  prefs: []
  type: TYPE_NORMAL
- en: User Interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have your script working end to end, you probably want to make
    it a little easier to work with, and maybe even get it into the hands of people
    who can give you feedback. The frontend of many AI tools in production is typically
    built using JavaScript, specifically the [NextJS](https://nextjs.org) framework
    based on React. This is usually paired with a CSS library such as [Tailwind CSS](https://tailwindcss.com),
    which makes rapid prototyping of design elements easier.
  prefs: []
  type: TYPE_NORMAL
- en: However, most of your AI code is likely in Python at this stage, and switching
    programming languages and development environments can be a daunting challenge.
    As well as learning JavaScript, NextJS, and Tailwind, you may also run into a
    series of issues getting a server running for your Python code, and a database
    live for your application and user data, and then integrating all of that with
    a frontend web design.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of spending a lot of time spinning up servers, building databases, and
    adjusting button colors, it might make sense to create a simple prototype frontend
    to get early feedback, before investing too much at this stage in an unproven
    idea. Once you have built and tested a simple interface, you’ll have a better
    understanding of what to build when you do need to get your app production-ready.
  prefs: []
  type: TYPE_NORMAL
- en: For launching simple user interfaces for AI-based prototypes, there are several
    popular open source interfaces, including [gradio](https://www.gradio.app) and
    [Streamlit](https://streamlit.io). Gradio was acquired by HuggingFace and powers
    the web user interface for many interactive demos of open source AI models, famously
    including the [AUTOMATIC1111](https://oreil.ly/GlwJT) Stable Diffusion Web UI.
    You can quickly build a Gradio interface to make it easier to run your code locally,
    as well as sharing the prototype to get feedback.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve created an interface that allows you to automate the entire process within
    two steps. You can get access to the [gradio source code here](https://oreil.ly/HNqVX).
  prefs: []
  type: TYPE_NORMAL
- en: Then run the gradio application by going into the [chapter_10 folder](https://oreil.ly/chapter10)
    within your terminal and running `python3 gradio_code_example.py`. The script
    will ask you to enter a `SERPAPI_API_KEY` and a `STABILITY_API_KEY` in your terminal.
  prefs: []
  type: TYPE_NORMAL
- en: Then you can access the gradio interface as shown in [Figure 10-6](#figure-10-6).
  prefs: []
  type: TYPE_NORMAL
- en: '![pega 1006](assets/pega_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Gradio user interface
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you run gradio, you get an inline interface you can use directly or a URL
    that you can click to open the web interface in your browser. If you run gradio
    with the parameter `share=True`, for example `demo.launch(share=True)`, you get
    a publicly accessible link to share with friends, coworkers, or early users to
    get feedback on your prototype.
  prefs: []
  type: TYPE_NORMAL
- en: After initializing the interface, input a topic by clicking the Summarize and
    Generate Questions button. This will then collect and summarize the Google results
    as well as generate interview questions.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll then need to fill in the answers for each question. Finally, click the
    Generate Blog Post & Image button, which will take all the questions, answers,
    and summaries and will create an entire blog post and image using GPT-4!
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate Quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most valuable evaluation data in AI is human feedback, as it has been the
    key to many AI alignment breakthroughs, including those that power ChatGPT. Asking
    for feedback from users via a user interface, or even building feedback mechanisms
    into your product, helps you identify and fix edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: If you are building for research purposes or want to contribute to the open
    source community, consider sharing your gradio demo on Hugging Face Spaces. Hugging
    Face Spaces allows anyone to host their gradio demos freely, and uploading your
    project only takes a few minutes. New spaces can be created via the [Hugging Face
    website](https://oreil.ly/pSrP3), or done programmatically using the Hugging Face
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You’ve journeyed through the comprehensive world of prompt
    engineering for generative AI. You started with learning the prompt engineering
    principles and explored the historical context of LLMs, gaining awareness of their
    capabilities and the privacy concerns they pose.
  prefs: []
  type: TYPE_NORMAL
- en: You learned how to extract structured data, apply best practices of prompt engineering,
    and familiarize yourself with an LLM package called LangChain. Then you discovered
    vector databases for storing and querying text based on similarity and ventured
    into the world of autonomous agents.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you immersed yourself in image generation techniques using diffusion models,
    learning how to navigate through this latent space. Your journey covered everything
    from format modifiers and art-style replication to inpainting and outpainting
    techniques. Moreover, you explored more advanced usage cases such as prompt expansion,
    meme mapping, and CLIP Interrogator, alongside many others.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you transitioned toward utilizing prompt engineering for content writing.
    You learned about creating a blog writing service that generates posts based on
    user responses, mimicking their writing styles, along with topic research strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this journey not only enriched your knowledge but also equipped you
    with practical skills, setting you up to work professionally in the field of prompt
    engineering.
  prefs: []
  type: TYPE_NORMAL
- en: It’s been our pleasure to guide you through the wide domain of prompt engineering
    for generative AI. Thank you for staying with us to the end of this book. We trust
    it will become a useful tool in all your future work with AI.
  prefs: []
  type: TYPE_NORMAL
- en: We would also greatly appreciate hearing your thoughts about the book, as well
    as any remarkable projects you create using the techniques we’ve discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Please feel free to share your feedback or showcase your work by emailing us
    at [hi@brightpool.dev](mailto:hi@brightpool.dev). Once again, thank you! Your
    curiosity and perseverance are what shapes the future of this exciting field,
    and we can’t wait to see what you contribute.
  prefs: []
  type: TYPE_NORMAL
- en: Happy prompting!
  prefs: []
  type: TYPE_NORMAL
