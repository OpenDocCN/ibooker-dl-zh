- en: Chapter 10\. Building AI-Powered Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ll apply the five principles of prompting to an end-to-end
    AI workflow for content writing. The service will write blog posts based on the
    user’s responses to interview questions, in the style of the user’s writing. This
    system was first documented on the [Saxifrage blog](https://oreil.ly/saxifrage).
  prefs: []
  type: TYPE_NORMAL
- en: AI Blog Writing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The naive approach to creating a blog writing service using AI would be to prompt
    ChatGPT with `Write a blog post on {blogPostTopic}`. The resulting content would
    be of reasonable quality but wouldn’t likely contain any valuable opinions or
    unique experiences on the topic. The content would also likely be short and generic
    and therefore unlikely to rank on Google.
  prefs: []
  type: TYPE_NORMAL
- en: A more sophisticated approach might be to build up a longer prompt with further
    instructions. Detail on the prescribed writing tone, architecture of the blog
    post, and keywords to include could be added. An example of a common blog post
    [writing prompt](https://oreil.ly/uMfZa) can be seen here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This longer, more sophisticated prompt is likely to result in better quality
    content. However, let’s run through the five principles of prompting as a checklist:'
  prefs: []
  type: TYPE_NORMAL
- en: Direction
  prefs: []
  type: TYPE_NORMAL
- en: There are some instructions provided, such as the tone, using transition words,
    and an active voice. However, the content is still likely to sound like AI, and
    not like the user.
  prefs: []
  type: TYPE_NORMAL
- en: Format
  prefs: []
  type: TYPE_NORMAL
- en: Although there are some mentions of structure, including dictating nine sections
    of two paragraphs, it’s likely these instructions will be ignored. ChatGPT is
    bad at math and is often unable to follow instructions dictating a number of sections
    or words.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: There are no samples of how to do the task given, which is likely to harm the
    reliability of running this prompt across multiple topics or even multiple times
    on the same topic. Even providing one example (a one-shot prompt) could radically
    help improve quality.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs: []
  type: TYPE_NORMAL
- en: This is an example of *blind prompting* (adding instructions to a prompt [without
    testing them](https://oreil.ly/r7sXi)). It’s likely some of these instructions
    make no difference to quality (unnecessarily costing tokens) or might even degrade
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: Division
  prefs: []
  type: TYPE_NORMAL
- en: The entire task is attempted with just one prompt, which is likely to harm performance.
    Without breaking the task into subtasks, it’s hard to understand which part of
    the process is suceeding or failing.
  prefs: []
  type: TYPE_NORMAL
- en: Through this chapter, you’ll create multiple LLM chain components. Each chain
    will be implemented in LangChain to make it more maintainable and to give easy
    logging for monitoring and optimization. The resulting system will help you generate
    *human-sounding* content based on the unique opinions and experiences of the user.
  prefs: []
  type: TYPE_NORMAL
- en: It’s crucial that you first prepare your workspace with the necessary tools.
    Therefore, let’s shift our focus toward topic research and start setting up your
    programming environment.
  prefs: []
  type: TYPE_NORMAL
- en: Topic Research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need to install several Python packages to effectively use LangChain’s
    document loaders, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: google-searchresults
  prefs: []
  type: TYPE_NORMAL
- en: A Python library designed to scrape and process Google search results.
  prefs: []
  type: TYPE_NORMAL
- en: pandas
  prefs: []
  type: TYPE_NORMAL
- en: This offers data structures and operations for manipulating numerical tables
    and time series data.
  prefs: []
  type: TYPE_NORMAL
- en: html2text
  prefs: []
  type: TYPE_NORMAL
- en: This tool converts HTML from files or web pages into markdown (*.md*) files
    or text.
  prefs: []
  type: TYPE_NORMAL
- en: pytest-playwright
  prefs: []
  type: TYPE_NORMAL
- en: This package enables end-to-end testing with Playwright.
  prefs: []
  type: TYPE_NORMAL
- en: chromadb
  prefs: []
  type: TYPE_NORMAL
- en: ChromaDB is an open source vector database.
  prefs: []
  type: TYPE_NORMAL
- en: nest_asyncio
  prefs: []
  type: TYPE_NORMAL
- en: This extends the Python standard `asyncio` to patch and render it compatible
    with Jupyter Notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Installation of these packages can be achieved easily with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, you’ll be using LangChain’s document loaders that require Playwright.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type this command on your terminal: **playwright install**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, you’ll need to choose a `TOPIC` and set environment variables
    for both `SERPAPI_API_KEY` and `STABILITY_API_KEY`. If you’re running the script
    without Jupyter Notebook, then you won’t need to use any of the `nest_asyncio`
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you’ll focus on summarizing web content efficiently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: First, import the required tools and then fetch the web page content related
    to your `TOPIC`. After setting up your `ChatOpenAI` model, you’ll utilize a `text_splitter`
    to manage text chunks. The splitter ensures no snippet is too long, while maintaining
    context with overlap. Then create the `PydanticOutputParser` to handle and structure
    the summaries. By feeding the extracted documents through a dedicated summarization
    function, the LLM produces concise summaries.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to dive deeper into the `create_all_summaries` function, check
    [*custom_summarize_chain.py*](https://oreil.ly/KyKjS).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key points to highlight are that you can *subclass* most classes within
    LangChain. For example, you can overide the default `ChromiumLoader` to be asynchronous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: By subclassing `ChromiumLoader`, you can easily create a custom implementation
    to *asynchronously scrape content* from multiple URLs using the Chrome browser.
    `get_html_content_from_urls` fetches HTML content from a list of URLs, ensuring
    no duplicates and handling potential errors.
  prefs: []
  type: TYPE_NORMAL
- en: Expert Interview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you’ve successfully extracted the summaries from Google for the top
    three results, you’ll conduct an interview with an LLM, generating relevant questions
    to make sure that your article has a unique perspective using an `InterviewChain`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: InterviewChain instantiation
  prefs: []
  type: TYPE_NORMAL
- en: With your topic and obtained summaries in hand, create an instance of `InterviewChain`,
    tailoring it to your data’s unique context.
  prefs: []
  type: TYPE_NORMAL
- en: Generating questions
  prefs: []
  type: TYPE_NORMAL
- en: By simply calling the `interview_chain`, you kickstart the process of generating
    a series of probing questions derived from your summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive Q&A session
  prefs: []
  type: TYPE_NORMAL
- en: Dive into an engaging loop where each derived question is printed, prompting
    you for an answer with `input()`. Your response is then saved back to the Pydantic
    object.
  prefs: []
  type: TYPE_NORMAL
- en: Give Direction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Giving an LLM unique answers provides unique context, and this allows an LLM
    to generate richer, more nuanced responses, ensuring your article offers a fresh
    and in-depth perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the code for `InterviewChain` is in *[expert_interview_chain.py](https://oreil.ly/0d5Hi)*.
    It has two significant components:'
  prefs: []
  type: TYPE_NORMAL
- en: A custom `System` message
  prefs: []
  type: TYPE_NORMAL
- en: 'This prompt includes role prompting, previously generated summaries, the topic,
    and format instructions (for the output parser):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7] from expert_interview_chain import InterviewQuestions  # Set up a parser
    + inject instructions into the prompt template: parser = PydanticOutputParser(pydantic_object=InterviewQuestions)
    [PRE8]`  [PRE9]`` # Generate Outline    Including the previous interview and research,
    you can generate an outline for the post with `BlogOutlineGenerator`. The `TOPIC`,
    `question_answers`, and Google `summaries` are passed to provide additional context:    [PRE10]    Let’s
    explore the `BlogOutlineGenerator` class in detail:    [PRE11]`` `---`  `Here
    is the interview which I answered:`         `{interview_questions_and_answers}`
    [PRE12] `"""`          `system_message_prompt` `=`         `SystemMessagePromptTemplate``.``from_template``(``prompt_content``)`          `self``.``chat_prompt`
    `=` `ChatPromptTemplate``.``from_messages``(`         `[``system_message_prompt``])`          `#
    Create an output parser`         `self``.``parser` `=` `PydanticOutputParser``(``pydantic_object``=``BlogOutline``)`          `#
    Set up the chain`         `self``.``outline_chain` `=` `self``.``chat_prompt`
    `|` `ChatOpenAI``()` `|` `self``.``parser`      `def` `generate_outline``(``self``,`
    `summaries``:` `List``[``DocumentSummary``])` `->` `Any``:`         `print``(``"Generating
    the outline...``\n``---"``)`         `result` `=` `self``.``outline_chain``.``invoke``(`             `{``"topic"``:`
    `self``.``topic``,`             `"document_summaries"``:` `[``s``.``dict``()`
    `for` `s` `in` `summaries``],`             `"interview_questions_and_answers"``:`
    `self``.``questions_and_answers``,`             `"format_instructions"``:` `self``.``parser``.``get_format_instructions``(),`             `}`         `)`         `print``(``"Finished
    generating the outline!``\n``---"``)`         `return` `result` [PRE13]` [PRE14]   [PRE15]`A
    `BlogOutline` Pydantic object is created that contains `title` and `sub_headings`
    keys. Also, the outline chain is set up using LangChain expression language (LCEL)
    that passes the prompt into the chat model and then finally into the output parser:    [PRE16]    By
    using a Pydantic output parser, the chain will return a `BlogOutline` Pydantic
    object that will be used in future chains.[PRE17]``  [PRE18]` [PRE19] # Text Generation    After
    obtaining a summary, interview questions, and a blog post outline, it’s time to
    start generating the text. The `ContentGenerator` class integrates SEO expertise
    with several LLM techniques, which include the following:    Embeddings and retrieval      This
    efficiently splits and vectorizes original web pages, storing them in the Chroma
    database and retrieving relevent web page text while writing each section.      Custom
    memory      While crafting each blog section, it uses memory to avoid repeating
    the same information, while also summarizing the conversation if it becomes too
    long.      Bespoke context      The LLM has a mixture of information, including
    your previous interview insights, what has been said before, and snippets of relevant
    web page text from Google:      [PRE20]py    All of the source code is within
    *[article_generation.py](https://oreil.ly/0IFyI)*, but let’s specifically focus
    on three components that are key to this chain.    The `OnlyStoreAIMemory` class
    is a customized subclass of `ConversationSummary​BufferMemory`:    [PRE21]py    It’s
    tailored to ensure that the chat messages memory remains concise and relevant
    by *exclusively storing AI-generated messages*.    This deliberate choice bypasses
    storing retrieved documents that are used within the generation step, preventing
    memory bloat. Furthermore, the memory mechanism ensures the AI remains aware of
    its prior writings, enabling it to offer condensed summaries if the accumulated
    context surpasses set limits.    The `generate_blog_post` function loops through
    all of the subheadings and tries to retrieve as many relevant documents as possible
    while fitting in the current context length:    [PRE22]py    This function, `generate_blog_post`,
    iterates over each subheading. It attempts to fetch up to five relevant documents.
    If there’s an issue fetching the documents, it smartly decreases the number and
    tries again. If all attempts fail, it gracefully defaults to no documents.    Finally,
    the prompt for generating each section is very context rich:    [PRE23]py   [PRE24]py  [PRE25]`py
    [PRE26]``'
  prefs: []
  type: TYPE_NORMAL
