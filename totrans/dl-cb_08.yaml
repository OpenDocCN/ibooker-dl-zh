- en: Chapter 8\. Sequence-to-Sequence Mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we’ll look at using sequence-to-sequence networks to learn transformations
    between pieces of text. This is a relatively new technique with tantalizing possibilities.
    Google claims to have made huge improvements to its Google Translate product using
    this technique; moreover, it has open sourced a version that can learn language
    translations purely based on parallel texts.
  prefs: []
  type: TYPE_NORMAL
- en: We won’t go that far to start with. Instead, we’ll start out with a simple model
    that learns the rules for pluralization in English. After that we’ll extract dialogue
    from 19th-century novels from Project Gutenberg and train a chatbot on them. For
    this last project we’ll have to abandon the safety of Keras running in a notebook
    and will use Google’s open source seq2seq toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following notebooks contain the code relevant for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 8.1 Training a Simple Sequence-to-Sequence Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you train a model to reverse engineer a transformation?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a sequence-to-sequence mapper.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html#text_generation) we saw how we can use recurrent networks
    to “learn” the rules of a sequence. The model learns how to best represent a sequence
    such that it can predict what the next element will be. Sequence-to-sequence mapping
    builds on this, but now the model learns to predict a different sequence based
    on the first one.
  prefs: []
  type: TYPE_NORMAL
- en: We can use this to learn all kinds of transformations. Let’s consider converting
    singular nouns into plural nouns in English. At first sight it might seem that
    this is just a matter of appending an *s* to a word, but when you look more closely
    it turns out that the rules are really quite a bit more complicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is very similar to what we were using in [Chapter 5](ch05.html#text_generation),
    but now it is not just the input that is a sequence, but also the output. This
    is achieved using the `RepeatVector` layer, which allows us to map from the input
    to the output vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Preprocessing of the data happens much as before. We read in the data from the
    file *data/plurals.txt* and vectorize it. One trick to consider is whether to
    reverse the strings in the input. If the input is reversed, then generating the
    output is like unrolling the processing, which might be easier.
  prefs: []
  type: TYPE_NORMAL
- en: It takes the model quite a bit of time to reach a precision in the neighborhood
    of 99%. Most of this time, though, is spent on learning to reproduce the prefixes
    that the singular and plural forms of the words share. In fact, when we check
    the model’s performance when it has reached over 99% precision, we see that most
    of the errors are still in that area.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sequence-to-sequence models are powerful tools that, given enough resources,
    can learn almost any transformation. Learning the rules for going from singular
    to plural in English is just a simple example. These models are essential elements
    of the state-of-the-art machine translation solutions offered by the leading tech
    companies.
  prefs: []
  type: TYPE_NORMAL
- en: Simpler models like the one from this recipe can learn how to add numbers in
    Roman notation or learn to translate between written English and phonetic English,
    which is a useful first step when building a text-to-speech system.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few recipes we’ll see how we can use this technique to train a chatbot
    based on dialogues extracted from 19th-century novels.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Extracting Dialogue from Texts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you acquire a large corpus of dialogues?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parse some texts available from Project Gutenberg and extract all the dialogue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with downloading a set of books from Project Gutenberg. We could
    download all of them, but here we’ll focus on works whose authors were born after
    1835\. This keeps the dialogue somewhat modern. The *data/books.json* document
    contains the relevant references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The books are mostly laid out consistently in ASCII. Paragraphs are separated
    by double newlines, and dialogue almost always uses double quotes. A small fraction
    of books also use single quotes, but we’ll just ignore those, since single quotes
    also occur elsewhere in the texts. We’ll assume a conversation continues as long
    as the text outside of the quotes is less than 100 characters long (as in “Hi,”
    he said, “How are you doing?”):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Processing this over the top 1,000 authors gets us a good set of dialogue data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This takes some time, so we’d better save the results to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in [Chapter 5](ch05.html#text_generation), Project Gutenberg is a
    good source for freely usable texts, as long as we don’t mind that they are a
    little bit older since they have to be out of copyright.
  prefs: []
  type: TYPE_NORMAL
- en: The project was started at a time before concerns around layout and illustrations
    played a role, and therefore all documents are produced in pure ASCII. While this
    isn’t the best format for actual books, it makes parsing relatively easy. Paragraphs
    are separated by double newlines and there’s no mucking around with smart quotes
    or any markup.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Handling an Open Vocabulary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you tokenize a text completely with only a fixed number of tokens?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use subword units for tokenizing.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter we just skipped words that weren’t found in our vocabulary
    of the top 50,000 words. With subword-unit tokenizing, we break up words that
    don’t appear very often into subunits that do. We continue doing so until all
    words and subunits fit our fixed-size vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have the words *working* and *worked*, we could break them
    up into *work-*, *-ed* and *-ing*. These three tokens will most likely overlap
    with others in our vocabulary, so this could reduce the size of our overall vocabulary.
    The algorithm used is straightforward. We split all tokens up into their individual
    letters. At this point each letter is a subword token, and presumably we have
    less than our maximum number of tokens. We then find which pair of subword tokens
    occurs most in our tokens. In English that would typically be (*t*, *h*). We then
    join those subword tokens. This will usually increase the number of subword tokens
    by one, unless one of the items in our pair is now exhausted. We keep doing this
    until we have the desired number of subword and word tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the code is not complicated, it makes sense to use the [open source
    version of this algorithm](https://github.com/rsennrich/subword-nmt). The tokenizing
    is a three-step process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to tokenize our corpus. The default tokenizer just splits
    the text, which means that it keeps all punctuation, usually attached to the previous
    word. We want something more advanced. We want all punctuation stripped except
    for the question mark. We’ll also convert everything to lowercase and replace
    underscores with spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can learn the subword tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we can apply them to any text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The resulting *some_text.bpe.txt* looks like our original corpus, except that
    rare tokens are broken up and end with *@@* indicating the continuation.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tokenizing a text into words is an effective way of reducing the size of a
    document. As we saw in [Chapter 7](ch07.html#suggest_emojis), it also allows us
    to kick-start our learning by loading up pretrained word embeddings. There is
    a drawback, though: larger texts contain so many different words that we can’t
    hope to cover them all. One solution is to just skip the words that are not in
    our vocabulary, or replace them with a fixed `UNKNOWN` token. This doesn’t work
    too badly for sentiment analysis, but for tasks where we want to generate an output
    text it is rather unsatisfactory. Subword-unit tokenizing is a good solution in
    this situation.'
  prefs: []
  type: TYPE_NORMAL
- en: Another option that has in recent times gotten some traction is to train a character-level
    model to produce embeddings for words that are not in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Training a seq2seq Chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to train a deep learning model to reproduce the characteristics of
    a dialogue corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use Google’s seq2seq framework.
  prefs: []
  type: TYPE_NORMAL
- en: The model from [Recipe 8.1](#training-a-simple-s-t-s-model) is capable of learning
    relations between sequences—even fairly complex ones. However, sequence-to-sequence
    models are hard to tune for performance. In early 2017, Google published seq2seq,
    a library specifically developed for this type of application that runs directly
    on TensorFlow. It lets us focus on the model hyperparameters, rather than the
    nitty-gritty of the code.
  prefs: []
  type: TYPE_NORMAL
- en: The seq2seq framework wants its input split up into training, evaluation, and
    development sets. Each set should contain a source and a target file, with matching
    lines defining the input and the output of the model. In our case the source should
    contain the prompt of the dialogue, and the target the answer. The model will
    then try to learn how to convert from prompt to answer, effectively learning how
    to conduct a dialogue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to split our dialogues into (source, target) pairs. For each
    consecutive pair of lines in the dialogues, we extract the first and last sentence
    as a source and target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s shuffle our pairs and split them into our three groups, with the
    `dev` and `test` sets each representing 5% of our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we need to unpack the pairs and put them into the right directory structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Time to train the network. Clone the seq2seq repository and install the dependencies.
    You might want to do this in a separate `virtualenv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s set an environment variable pointing to the data we’ve put together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The seq2seq library contains a number of configuration files that we can mix
    and match in the *example_configs* directory. In this case, we want to train a
    large model with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, even on a system with a capable GPU it will take days and days
    before we get some decent results. The *zoo* folder in the notebook contains a
    pretrained model though, if you can’t wait.
  prefs: []
  type: TYPE_NORMAL
- en: 'The library doesn’t provide a way to run the model interactively. In [Chapter 16](ch16.html#productionizing)
    we’ll look into how we can do this, but for now we can quickly get some results
    by adding our test questions to a file (for example, */tmp/test_questions.txt*)
    and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple conversation works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: With more complex sentences it is a bit hit or miss.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The seq2seq model’s primary use case seems to be automatic translation, although
    it has also been effective for captioning images and summarizing texts. The documentation
    contains a [tutorial](https://google.github.io/seq2seq/nmt/) on how to train a
    model that learns decent English–German translations in weeks or months, depending
    on your hardware. Google claims that making a sequence-to-sequence model central
    to its machine translation efforts has improved the quality dramatically.
  prefs: []
  type: TYPE_NORMAL
- en: One interesting way to think about sequence-to-sequence mapping is to see it
    as an embedding process. For translations, both the source and the target sentence
    are projected into a multidimensional space and the model learns a projection
    such that sentences that mean the same thing end up around the same point in that
    space. This leads to the intriguing possibility of “zero-shot” translations; if
    a model learns to translate between Finnish and English and then later between
    English and Greek and it uses the same semantic space, it can also be used to
    directly translate between Finnish and Greek. This then opens up the possibility
    of “thought vectors,” embeddings for relatively complex ideas that have similar
    properties to the “word vectors” we saw in [Chapter 3](ch03.html#word_embeddings).
  prefs: []
  type: TYPE_NORMAL
