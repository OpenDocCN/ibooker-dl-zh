- en: Chapter 8\. Sequence-to-Sequence Mapping
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。序列到序列映射
- en: In this chapter we’ll look at using sequence-to-sequence networks to learn transformations
    between pieces of text. This is a relatively new technique with tantalizing possibilities.
    Google claims to have made huge improvements to its Google Translate product using
    this technique; moreover, it has open sourced a version that can learn language
    translations purely based on parallel texts.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究使用序列到序列网络来学习文本片段之间的转换。这是一种相对较新的技术，具有诱人的可能性。谷歌声称已经通过这种技术大大改进了其Google翻译产品；此外，它已经开源了一个版本，可以纯粹基于平行文本学习语言翻译。
- en: We won’t go that far to start with. Instead, we’ll start out with a simple model
    that learns the rules for pluralization in English. After that we’ll extract dialogue
    from 19th-century novels from Project Gutenberg and train a chatbot on them. For
    this last project we’ll have to abandon the safety of Keras running in a notebook
    and will use Google’s open source seq2seq toolkit.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会一开始就走得那么远。相反，我们将从一个简单的模型开始，学习英语中复数形式的规则。之后，我们将从古腾堡计划的19世纪小说中提取对话，并在其中训练一个聊天机器人。对于这个最后的项目，我们将不得不放弃在笔记本中运行Keras的安全性，并使用谷歌的开源seq2seq工具包。
- en: 'The following notebooks contain the code relevant for this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下笔记本包含本章相关的代码：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 8.1 Training a Simple Sequence-to-Sequence Model
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.1 训练一个简单的序列到序列模型
- en: Problem
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you train a model to reverse engineer a transformation?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何训练一个模型来逆向工程一个转换？
- en: Solution
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a sequence-to-sequence mapper.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用序列到序列的映射器。
- en: In [Chapter 5](ch05.html#text_generation) we saw how we can use recurrent networks
    to “learn” the rules of a sequence. The model learns how to best represent a sequence
    such that it can predict what the next element will be. Sequence-to-sequence mapping
    builds on this, but now the model learns to predict a different sequence based
    on the first one.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#text_generation)中，我们看到了如何使用循环网络来“学习”序列的规则。模型学习如何最好地表示一个序列，以便能够预测下一个元素是什么。序列到序列映射建立在此基础上，但现在模型学习根据第一个序列预测不同的序列。
- en: We can use this to learn all kinds of transformations. Let’s consider converting
    singular nouns into plural nouns in English. At first sight it might seem that
    this is just a matter of appending an *s* to a word, but when you look more closely
    it turns out that the rules are really quite a bit more complicated.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个来学习各种转换。让我们考虑将英语中的单数名词转换为复数名词。乍一看，这似乎只是在单词后添加一个*s*，但当你仔细观察时，你会发现规则实际上相当复杂。
- en: 'The model is very similar to what we were using in [Chapter 5](ch05.html#text_generation),
    but now it is not just the input that is a sequence, but also the output. This
    is achieved using the `RepeatVector` layer, which allows us to map from the input
    to the output vector:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型与我们在[第5章](ch05.html#text_generation)中使用的模型非常相似，但现在不仅输入是一个序列，输出也是一个序列。这是通过使用`RepeatVector`层实现的，它允许我们从输入映射到输出向量：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Preprocessing of the data happens much as before. We read in the data from the
    file *data/plurals.txt* and vectorize it. One trick to consider is whether to
    reverse the strings in the input. If the input is reversed, then generating the
    output is like unrolling the processing, which might be easier.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的预处理与以前大致相同。我们从文件*data/plurals.txt*中读取数据并将其向量化。一个要考虑的技巧是是否要颠倒输入中的字符串。如果输入被颠倒，那么生成输出就像展开处理，这可能更容易。
- en: It takes the model quite a bit of time to reach a precision in the neighborhood
    of 99%. Most of this time, though, is spent on learning to reproduce the prefixes
    that the singular and plural forms of the words share. In fact, when we check
    the model’s performance when it has reached over 99% precision, we see that most
    of the errors are still in that area.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模型需要相当长的时间才能达到接近99%的精度。然而，大部分时间都花在学习如何重现单词的单数和复数形式共享的前缀上。事实上，当我们检查模型在达到超过99%精度时的性能时，我们会发现大部分错误仍然在这个领域。
- en: Discussion
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Sequence-to-sequence models are powerful tools that, given enough resources,
    can learn almost any transformation. Learning the rules for going from singular
    to plural in English is just a simple example. These models are essential elements
    of the state-of-the-art machine translation solutions offered by the leading tech
    companies.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型是强大的工具，只要有足够的资源，就可以学习几乎任何转换。学习从英语的单数到复数的规则只是一个简单的例子。这些模型是领先科技公司提供的最先进的机器翻译解决方案的基本元素。
- en: Simpler models like the one from this recipe can learn how to add numbers in
    Roman notation or learn to translate between written English and phonetic English,
    which is a useful first step when building a text-to-speech system.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 像这个配方中的简单模型可以学习如何在罗马数字中添加数字，或者学习在书面英语和音标英语之间进行翻译，这在构建文本到语音系统时是一个有用的第一步。
- en: In the next few recipes we’ll see how we can use this technique to train a chatbot
    based on dialogues extracted from 19th-century novels.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个配方中，我们将看到如何使用这种技术基于从19世纪小说中提取的对话来训练一个聊天机器人。
- en: 8.2 Extracting Dialogue from Texts
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.2 从文本中提取对话
- en: Problem
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you acquire a large corpus of dialogues?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何获取大量的对话语料库？
- en: Solution
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Parse some texts available from Project Gutenberg and extract all the dialogue.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 解析一些从古腾堡计划中提供的文本，并提取所有的对话。
- en: 'Let’s start with downloading a set of books from Project Gutenberg. We could
    download all of them, but here we’ll focus on works whose authors were born after
    1835\. This keeps the dialogue somewhat modern. The *data/books.json* document
    contains the relevant references:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从古腾堡计划中下载一套书籍。我们可以下载所有的书籍，但在这里我们将专注于那些作者出生在1835年之后的作品。这样可以使对话保持现代。*data/books.json*文档包含相关的参考资料：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The books are mostly laid out consistently in ASCII. Paragraphs are separated
    by double newlines, and dialogue almost always uses double quotes. A small fraction
    of books also use single quotes, but we’ll just ignore those, since single quotes
    also occur elsewhere in the texts. We’ll assume a conversation continues as long
    as the text outside of the quotes is less than 100 characters long (as in “Hi,”
    he said, “How are you doing?”):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些书大多以ASCII格式一致排版。段落之间用双换行符分隔，对话几乎总是使用双引号。少量书籍也使用单引号，但我们将忽略这些，因为单引号也出现在文本的其他地方。我们假设对话会持续下去，只要引号外的文本长度不超过100个字符（例如，“嗨，”他说，“你好吗？”）：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Processing this over the top 1,000 authors gets us a good set of dialogue data:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 处理前1000位作者的数据可以得到一组良好的对话数据：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This takes some time, so we’d better save the results to a file:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要一些时间，所以最好将结果保存到文件中：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Discussion
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: As we saw in [Chapter 5](ch05.html#text_generation), Project Gutenberg is a
    good source for freely usable texts, as long as we don’t mind that they are a
    little bit older since they have to be out of copyright.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第5章](ch05.html#text_generation)中看到的，Project Gutenberg是一个很好的来源，可以自由使用文本，只要我们不介意它们有点过时，因为它们必须已经过期。
- en: The project was started at a time before concerns around layout and illustrations
    played a role, and therefore all documents are produced in pure ASCII. While this
    isn’t the best format for actual books, it makes parsing relatively easy. Paragraphs
    are separated by double newlines and there’s no mucking around with smart quotes
    or any markup.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目是在关于布局和插图的担忧之前开始的，因此所有文档都是以纯ASCII格式生成的。虽然这不是实际书籍的最佳格式，但它使解析相对容易。段落之间用双换行符分隔，没有智能引号或任何标记。
- en: 8.3 Handling an Open Vocabulary
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.3 处理开放词汇
- en: Problem
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you tokenize a text completely with only a fixed number of tokens?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如何仅使用固定数量的标记完全标记化文本？
- en: Solution
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use subword units for tokenizing.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用子词单元进行标记化。
- en: In the previous chapter we just skipped words that weren’t found in our vocabulary
    of the top 50,000 words. With subword-unit tokenizing, we break up words that
    don’t appear very often into subunits that do. We continue doing so until all
    words and subunits fit our fixed-size vocabulary.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们只是跳过了我们前50,000个单词的词汇表中找不到的单词。通过子词单元标记化，我们将不经常出现的单词分解为更小的子单元，直到所有单词和子单元适合我们的固定大小的词汇表。
- en: For example, if we have the words *working* and *worked*, we could break them
    up into *work-*, *-ed* and *-ing*. These three tokens will most likely overlap
    with others in our vocabulary, so this could reduce the size of our overall vocabulary.
    The algorithm used is straightforward. We split all tokens up into their individual
    letters. At this point each letter is a subword token, and presumably we have
    less than our maximum number of tokens. We then find which pair of subword tokens
    occurs most in our tokens. In English that would typically be (*t*, *h*). We then
    join those subword tokens. This will usually increase the number of subword tokens
    by one, unless one of the items in our pair is now exhausted. We keep doing this
    until we have the desired number of subword and word tokens.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有*working*和*worked*这两个词，我们可以将它们分解为*work-*、*-ed*和*-ing*。这三个标记很可能会与我们词汇表中的其他标记重叠，因此这可能会减少我们整体词汇表的大小。所使用的算法很简单。我们将所有标记分解为它们的各个字母。此时，每个字母都是一个子词标记，而且我们可能少于最大数量的标记。然后找出在我们的标记中最常出现的一对子词标记。在英语中，这通常是（*t*，*h*）。然后将这些子词标记连接起来。这通常会增加一个子词标记的数量，除非我们的一对中的一个项目现在已经用完。我们继续这样做，直到我们有所需数量的子词和单词标记。
- en: Even though the code is not complicated, it makes sense to use the [open source
    version of this algorithm](https://github.com/rsennrich/subword-nmt). The tokenizing
    is a three-step process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代码并不复杂，但使用这个算法的[开源版本](https://github.com/rsennrich/subword-nmt)是有意义的。标记化是一个三步过程。
- en: 'The first step is to tokenize our corpus. The default tokenizer just splits
    the text, which means that it keeps all punctuation, usually attached to the previous
    word. We want something more advanced. We want all punctuation stripped except
    for the question mark. We’ll also convert everything to lowercase and replace
    underscores with spaces:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是对我们的语料库进行标记化。默认的标记器只是分割文本，这意味着它保留所有标点符号，通常附加在前一个单词上。我们想要更高级的东西。我们希望除了问号之外剥离所有标点符号。我们还将所有内容转换为小写，并用空格替换下划线：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we can learn the subword tokens:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以学习子词标记：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And then we can apply them to any text:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将它们应用于任何文本：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The resulting *some_text.bpe.txt* looks like our original corpus, except that
    rare tokens are broken up and end with *@@* indicating the continuation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的*some_text.bpe.txt*看起来像我们的原始语料库，只是罕见的标记被分解并以*@@*结尾表示继续。
- en: Discussion
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Tokenizing a text into words is an effective way of reducing the size of a
    document. As we saw in [Chapter 7](ch07.html#suggest_emojis), it also allows us
    to kick-start our learning by loading up pretrained word embeddings. There is
    a drawback, though: larger texts contain so many different words that we can’t
    hope to cover them all. One solution is to just skip the words that are not in
    our vocabulary, or replace them with a fixed `UNKNOWN` token. This doesn’t work
    too badly for sentiment analysis, but for tasks where we want to generate an output
    text it is rather unsatisfactory. Subword-unit tokenizing is a good solution in
    this situation.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本标记化为单词是减少文档大小的有效方法。正如我们在[第7章](ch07.html#suggest_emojis)中看到的，它还允许我们通过加载预训练的单词嵌入来启动我们的学习。然而，存在一个缺点：较大的文本包含太多不同的单词，我们无法希望覆盖所有单词。一个解决方案是跳过不在我们词汇表中的单词，或用固定的`UNKNOWN`标记替换它们。这对于情感分析来说效果还不错，但对于我们想要生成输出文本的任务来说，这是相当不令人满意的。在这种情况下，子词单元标记化是一个很好的解决方案。
- en: Another option that has in recent times gotten some traction is to train a character-level
    model to produce embeddings for words that are not in the vocabulary.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是最近开始受到关注的选项，即训练一个字符级模型来为词汇表中不存在的单词生成嵌入。
- en: 8.4 Training a seq2seq Chatbot
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.4 训练一个seq2seq聊天机器人
- en: Problem
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to train a deep learning model to reproduce the characteristics of
    a dialogue corpus.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要训练一个深度学习模型来复制对话语料库的特征。
- en: Solution
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Google’s seq2seq framework.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Google的seq2seq框架。
- en: The model from [Recipe 8.1](#training-a-simple-s-t-s-model) is capable of learning
    relations between sequences—even fairly complex ones. However, sequence-to-sequence
    models are hard to tune for performance. In early 2017, Google published seq2seq,
    a library specifically developed for this type of application that runs directly
    on TensorFlow. It lets us focus on the model hyperparameters, rather than the
    nitty-gritty of the code.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[配方8.1](#training-a-simple-s-t-s-model)中的模型能够学习序列之间的关系，甚至是相当复杂的关系。然而，序列到序列模型很难调整性能。在2017年初，Google发布了seq2seq，这是一个专门为这种应用程序开发的库，可以直接在TensorFlow上运行。它让我们专注于模型的超参数，而不是代码的细节。'
- en: The seq2seq framework wants its input split up into training, evaluation, and
    development sets. Each set should contain a source and a target file, with matching
    lines defining the input and the output of the model. In our case the source should
    contain the prompt of the dialogue, and the target the answer. The model will
    then try to learn how to convert from prompt to answer, effectively learning how
    to conduct a dialogue.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq框架希望其输入被分成训练、评估和开发集。每个集合应包含一个源文件和一个目标文件，其中匹配的行定义了模型的输入和输出。在我们的情况下，源应包含对话的提示，目标应包含答案。然后，模型将尝试学习如何从提示转换为答案，有效地学习如何进行对话。
- en: 'The first step is to split our dialogues into (source, target) pairs. For each
    consecutive pair of lines in the dialogues, we extract the first and last sentence
    as a source and target:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将我们的对话分成（源，目标）对。对于对话中的每一对连续的句子，我们提取第一句和最后一句作为源和目标：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let’s shuffle our pairs and split them into our three groups, with the
    `dev` and `test` sets each representing 5% of our data:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们洗牌我们的对，并将它们分成我们的三个组，`dev`和`test`集合各代表我们数据的5%：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next we need to unpack the pairs and put them into the right directory structure:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要解压这些对，并将它们放入正确的目录结构中：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Time to train the network. Clone the seq2seq repository and install the dependencies.
    You might want to do this in a separate `virtualenv`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候训练网络了。克隆seq2seq存储库并安装依赖项。您可能希望在一个单独的`virtualenv`中执行此操作：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let’s set an environment variable pointing to the data we’ve put together:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们设置一个指向我们已经准备好的数据的环境变量：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The seq2seq library contains a number of configuration files that we can mix
    and match in the *example_configs* directory. In this case, we want to train a
    large model with:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq库包含一些配置文件，我们可以在*example_configs*目录中混合匹配。在这种情况下，我们想要训练一个大型模型，其中包括：
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Unfortunately, even on a system with a capable GPU it will take days and days
    before we get some decent results. The *zoo* folder in the notebook contains a
    pretrained model though, if you can’t wait.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，即使在具有强大GPU的系统上，也需要很多天才能获得一些像样的结果。然而，笔记本中的*zoo*文件夹包含一个预训练模型，如果您等不及的话。
- en: 'The library doesn’t provide a way to run the model interactively. In [Chapter 16](ch16.html#productionizing)
    we’ll look into how we can do this, but for now we can quickly get some results
    by adding our test questions to a file (for example, */tmp/test_questions.txt*)
    and running:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该库没有提供一种交互式运行模型的方法。在[第16章](ch16.html#productionizing)中，我们将探讨如何做到这一点，但现在我们可以通过将我们的测试问题添加到一个文件（例如，*/tmp/test_questions.txt*）并运行以下命令来快速获得一些结果：
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'A simple conversation works:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的对话可以运行：
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With more complex sentences it is a bit hit or miss.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的句子，有时会有点碰运气。
- en: Discussion
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The seq2seq model’s primary use case seems to be automatic translation, although
    it has also been effective for captioning images and summarizing texts. The documentation
    contains a [tutorial](https://google.github.io/seq2seq/nmt/) on how to train a
    model that learns decent English–German translations in weeks or months, depending
    on your hardware. Google claims that making a sequence-to-sequence model central
    to its machine translation efforts has improved the quality dramatically.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq模型的主要用途似乎是自动翻译，尽管它也对图像字幕和文本摘要有效。文档中包含了一个[教程](https://google.github.io/seq2seq/nmt/)，介绍如何训练一个在几周或几个月内学会不错的英语-德语翻译的模型，具体取决于您的硬件。Google声称，将序列到序列模型作为其机器翻译工作的核心部分，显著提高了质量。
- en: One interesting way to think about sequence-to-sequence mapping is to see it
    as an embedding process. For translations, both the source and the target sentence
    are projected into a multidimensional space and the model learns a projection
    such that sentences that mean the same thing end up around the same point in that
    space. This leads to the intriguing possibility of “zero-shot” translations; if
    a model learns to translate between Finnish and English and then later between
    English and Greek and it uses the same semantic space, it can also be used to
    directly translate between Finnish and Greek. This then opens up the possibility
    of “thought vectors,” embeddings for relatively complex ideas that have similar
    properties to the “word vectors” we saw in [Chapter 3](ch03.html#word_embeddings).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的思考序列到序列映射的方式是将其视为嵌入过程。对于翻译，源句和目标句都被投影到一个多维空间中，模型学习一个投影，使得意思相同的句子最终在该空间中的同一点附近。这导致了“零翻译”的有趣可能性；如果一个模型学会了在芬兰语和英语之间进行翻译，然后稍后在英语和希腊语之间进行翻译，并且它使用相同的语义空间，它也可以直接在芬兰语和希腊语之间进行翻译。这就打开了“思维向量”的可能性，这是相对复杂的想法的嵌入，具有与我们在[第3章](ch03.html#word_embeddings)中看到的“词向量”类似的属性。
