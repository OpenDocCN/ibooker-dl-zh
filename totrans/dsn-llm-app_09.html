<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Advanced Fine-Tuning Techniques"><div class="chapter" id="ch07">
<h1><span class="label">Chapter 7. </span>Advanced Fine-Tuning Techniques</h1>


<p>In the previous chapter, we presented the canonical way to fine-tune a typical LLM. In the real world, there are a wide variety of motivations for updating an LLM, and similarly there are multiple ways to update it. In this chapter, we will describe several advanced fine-tuning techniques and highlight the scenarios in which each technique would be suitable.</p>

<p>Why would you want to update the parameters of an LLM? We touched upon this in previous chapters but let’s go through it in more detail now:</p>
<dl>
<dt>Domain adaptation</dt>
<dd>
<p>The data that we work with belongs to a specialized domain that the LLM might not have been familiarized with during pre-training. In this case, we would like to update the model by training it on domain-specific data.</p>
</dd>
<dt>Task adaptation</dt>
<dd>
<p>We care about LLM performance on specific downstream tasks. To improve the LLM’s performance on these tasks, we can train it on task-specific data. This can be supervised or unsupervised.</p>
</dd>
<dt>Knowledge updating</dt>
<dd>
<p>We would like to keep the LLM’s knowledge up-to-date by continually training it on new data.</p>
</dd>
<dt>Controllability/steerability</dt>
<dd>
<p>We would like to control the behavior of the LLM, including making it more likely to follow user requests written in natural language, reject certain types of requests, and so on. Techniques to achieve this are collectively called alignment training. We will defer discussion of alignment training to <a data-type="xref" href="ch08.html#ch8">Chapter 8</a>.</p>
</dd>
</dl>

<p>In this chapter, we will learn techniques that can be used to update the LLM for the aforementioned reasons. To this end, the chapter is divided into three sections:</p>
<dl>
<dt>Continual pre-training</dt>
<dd>
<p>Primarily used for domain adaptation and keeping the knowledge of the LLM up-to-date (the latter is also called lifelong-learning).</p>
</dd>
<dt>Parameter-Efficient Fine-Tuning (PEFT)</dt>
<dd>
<p>A set of fine-tuning techniques that make the fine-tuning process more efficient by updating only a small number of model parameters, thus needing less memory and compute.</p>
</dd>
<dt>Model merging/model fusion</dt>
<dd>
<p>An exciting new subfield of LLMs that explores combining the parameters of two or more models. I call this the “dark arts” of NLP, as it is poorly understood but uncannily effective if done the right way.</p>
</dd>
</dl>

<p>Let’s begin with my personal favorite: continual pre-training!</p>






<section data-type="sect1" data-pdf-bookmark="Continual Pre-Training"><div class="sect1" id="id108">
<h1>Continual Pre-Training</h1>

<p>The premise of continual pre-training<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="continual pre-training" id="xi_finetuningmodelscontinualpretraining72838"/><a data-type="indexterm" data-primary="continual pre-training" id="xi_continualpretraining72838"/><a data-type="indexterm" data-primary="pre-training of data" data-secondary="continual pre-training" id="xi_pretrainingofdatacontinualpretraining72838"/> is simple. Take a pre-trained model checkpoint and continue pre-training it with your own data. But why would you want to do that? Here are some scenarios where continual pre-training can help.</p>

<ul>
<li>
<p>You work in a specialized domain like law, finance, or biomedical. In each of these cases, text belonging to these domains differs linguistically and structurally from naturally occurring English text. For example, legal text is characterized by long sentences written in a formal tone, containing jargon specific to the legal domain. Financial text is interspersed with a lot of numbers. Both legal and financial text contain a significant proportion of boilerplate text. Biomedical text contains a lot of scientific terms that are not part of the standard English vocabulary. In all these cases, you would like to pre-train your LLM on domain-specific data so that the LLM is exposed to the nuances and characteristics of domain-specific text. This<a data-type="indexterm" data-primary="domain-adaptive pre-training (DAPT)" id="id1103"/><a data-type="indexterm" data-primary="DAPT (domain-adaptive pre-training)" id="id1104"/> is called <em>domain-adaptive pre-training (DAPT)</em>.</p>
</li>
<li>
<p>Taking DAPT one step further, you can also continue pre-training your model not just on general text from your domain of interest but also on domain text specifically related to your downstream tasks. This<a data-type="indexterm" data-primary="task-adaptive pre-training (TAPT)" id="id1105"/> is called <em>task-adaptive pre-training (TAPT)</em>.</p>
</li>
<li>
<p>Your LLM is a reservoir of knowledge. But this knowledge can become obsolete over time. To keep its knowledge up-to-date, you continue pre-training the model at regular time periods or when new data is available. This<a data-type="indexterm" data-primary="life-long learning" id="id1106"/> is called <em>life-long learning</em>.</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You might be thinking, “If I want a domain-specific LLM, why don’t I just take my domain-specific data and train an LLM from scratch?” Well, you can, but your LLM just won’t be as performant, and the exercise will cost a whole lot more than continual pre-training. LLMs learn a wide variety of linguistic capabilities that might not be able to be learned from domain-specific text alone. Therefore, it is better to take an already pre-trained LLM that was trained on general text and then continue pre-training it with domain-specific text.</p>
</div>

<p>In practice, continual pre-training is a challenging exercise. This is due to the phenomenon of catastrophic forgetting<a data-type="indexterm" data-primary="catastrophic forgetting" id="id1107"/>, where the LLM <em>forgets</em> its previously learned capabilities and knowledge when it continues to be trained on new and different data. We will soon explore various techniques to combat the catastrophic forgetting 
<span class="keep-together">problem</span>.</p>

<p>How does continual pre-training differ from fine-tuning? The differences are mostly cosmetic and terminology-related. Just like pre-training, continual pre-training is self-supervised, while we typically use the term fine-tuning when we use supervised datasets. Continual pre-training uses the same (but not necessarily) learning objective as the one used in the original pre-training setup. Finally, continual pre-training datasets are usually orders of magnitude larger than typical fine-tuning datasets.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1108">
<h1>What’s in a Domain?</h1>
<p>So far, we have used a very intuitive notion<a data-type="indexterm" data-primary="domain" id="id1109"/> of what a <em>domain</em> is, with broad examples like law, finance, and medicine. But we need not restrict ourselves to such a definition. For example, continual pre-training has been used to expose the LLM to new languages, like a primarily English language LLM being continually pre-trained on Telugu data. Continual pre-training has also been used to expose the LLM to text written in a different tone and style, like social media text.</p>

<p>More formally, a domain can be described as text whose representations form an implicit cluster. <a href="https://oreil.ly/FQd-z">Aharoni et al.</a> show that sentence representations of LLMs lend themselves naturally to these clusters.</p>

<p>Once you have identified a domain, you would also like to select text that is most representative of the domain. In the same paper, Aharoni et al. introduced domain-data selection techniques based on sentence representations generated through the LLMs. One way to select data representative of the domain is to use embedding similarity with gold-truth in-domain data. Another way is to fine-tune a domain classifier that is trained on gold-truth<a data-type="indexterm" data-primary="gold truth" id="id1110"/> in-domain data and randomly sampled negative examples.</p>
</div></aside>

<p><a data-type="xref" href="#continual-pt">Figure 7-1</a> depicts the general continual pre-training process.</p>

<figure><div id="continual-pt" class="figure">
<img src="assets/dllm_0701.png" alt="continual-pt" width="600" height="136"/>
<h6><span class="label">Figure 7-1. </span>Illustration of the continual pre-training process</h6>
</div></figure>

<p>This book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a> contains a tutorial for continual pre-training. This setup is no different than fine-tuning, except that the dataset is not labeled (self-supervised training), and the dataset is orders of magnitude larger than typical fine-tuning 
<span class="keep-together">datasets</span>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1111">
<h1>Exercise</h1>
<p>Using the financial documents dataset linked in the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a>, continue pre-training a 3B LLM of your choice for 1 billion tokens. After pre-training, do you notice any degradation of the base model? Pass your model through some of the benchmark tests mentioned in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>, before and after the continual pre-training. Do you notice any difference?</p>
</div></aside>

<p>As mentioned earlier, naive continual pre-training leads to catastrophic forgetting<a data-type="indexterm" data-primary="catastrophic forgetting" id="id1112"/> of capabilities and knowledge learned previously. Several techniques exist to alleviate this issue:</p>
<dl>
<dt>Replay (memory)</dt>
<dd>
<p>Uses training examples from the original pre-training and mixes them with the new training data.</p>
</dd>
<dt>Distillation</dt>
<dd>
<p>Takes an older checkpoint of the model and during training compares the KL-divergence between the older and the current representations and penalizes it.</p>
</dd>
<dt>Regularization</dt>
<dd>
<p>Penalizes large changes to the parameters during continual training.</p>
</dd>
<dt>Parameter expansion</dt>
<dd>
<p>Adds more parameters to the model as continual pre-training is performed. This can be done by increasing either the width or the depth of the model.</p>
</dd>
</dl>

<p class="pagebreak-before">For a more comprehensive set of continual learning techniques, check out <a href="https://oreil.ly/yNa-H">Jin et al.’s paper</a>. In this chapter, we will dive deeper into replay and parameter expansion 
<span class="keep-together">methods</span>.</p>








<section data-type="sect2" data-pdf-bookmark="Replay (Memory)"><div class="sect2" id="id109">
<h2>Replay (Memory)</h2>

<p>Replay-based techniques<a data-type="indexterm" data-primary="replay-based techniques" id="id1113"/> are one of the simplest techniques to alleviate catastrophic forgetting. In this approach, we store pre-training examples from the original dataset and interleave them with the continual training dataset. Thus, the data drift is not so pronounced.</p>

<p>The following formula has worked very well for me: sample from different subsets of the original pre-training datasets and mix them with the continual training dataset. At the start of training, let the proportion of new data be around 25%. Over training steps, this can be slowly increased up to a maximum proportion, like 80%.</p>

<p>If the original pre-training dataset is a monolith and not made up of several smaller datasets, you might need to identify domains yourself so that all domains in the original pre-training set are included.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1114">
<h1>Learning Rate Strategies for Continual Pre-Training</h1>
<p>You can modify the learning rate<a data-type="indexterm" data-primary="learning rate" data-secondary="continual pre-training strategies" id="id1115"/> to further reduce the possibility of catastrophic forgetting. <a href="https://oreil.ly/TskjL">Winata et al.</a> show that lowering the learning rate through time can be effective. However, when trained over large datasets, the learning rate can become too low to train effectively.</p>

<p>If the learning rate is too small, the model retains its existing capabilities but fails to learn from the new dataset effectively. Conversely, if the learning rate is too large, the model learns from the new dataset but at the expense of forgetting its previous capabilities. Thus, the ideal learning rate is a tradeoff between the forgetting you can tolerate versus the new capabilities and knowledge you would like the LLM to absorb.</p>

<p><a href="https://oreil.ly/1e_eG">Gupta et al.</a> show that an effective learning rate schedule<a data-type="indexterm" data-primary="learning rate" data-secondary="schedule and schedulers" id="id1116"/> is to re-warm the learning rate at the start of continual learning to a maximum learning rate and then decay it with a cosine schedule (as shown in <a data-type="xref" href="ch06.html#llm-fine-tuning">Chapter 6</a>), until it reaches a minimum learning rate, after which the learning rate is kept constant. The maximum learning rate is chosen to balance the tradeoff between forgetting old capabilities and learning new capabilities.</p>
</div></aside>
</div></section>








<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Parameter Expansion"><div class="sect2" id="id110">
<h2>Parameter Expansion</h2>

<p>An alternative to the replay approach is to use parameter expansion techniques<a data-type="indexterm" data-primary="parameters" data-secondary="expansion techniques" id="xi_parametersexpansiontechniques710079"/>. The naive way would be to just add a new layer or two on top of the model and train only those parameters during continual pre-training. You can also insert and train domain-specific parameter modules (called adapters)<a data-type="indexterm" data-primary="domain-specific parameter modules" data-seealso="adapters" id="id1117"/><a data-type="indexterm" data-primary="adapters" data-secondary="for parameter expansion" data-secondary-sortas="parameter expansion" id="id1118"/> within existing layers. We will discuss adapter-based approaches in <a data-type="xref" href="#parameter-efficient-fine-tuning">“Parameter-Efficient Fine-Tuning”</a>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1119">
<h1>Leveraging DEMix Layers</h1>
<p>Transformers can be made more modular by composing the model as a mixture of experts, as shown in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>. One way to divide the experts is to assign each expert a single domain. This removes the possibility of catastrophic forgetting when learning new domains because each expert is trained separately without affecting other experts. To implement this, <a href="https://oreil.ly/y70et">Gururangan et al.</a> propose replacing the feedforward layers of the Transformer with domain expert mixture (DEMix) layers<a data-type="indexterm" data-primary="domain expert mixture (DEMix) layers" id="id1120"/><a data-type="indexterm" data-primary="DEMix (domain expert mixture) layers" id="id1121"/><a data-type="indexterm" data-primary="layers" data-secondary="DEMix" id="id1122"/>. A DEMix layer is a feedforward layer<a data-type="indexterm" data-primary="feedforward networks" id="id1123"/> consisting of one or more expert feedforward networks, one for each domain.</p>

<p>During inference time, a routing function dynamically chooses the experts most suited to handle the current input. This allows the model to handle text from previously unseen domains more effectively.</p>

<p>Domain-adaptive pre-training can be performed by training a new expert. The new expert is initialized by finding the closest available existing expert to the new domain and then using its parameters as the initial parameters of the new expert. The expert is then trained using domain-specific data.</p>
</div></aside>

<p>As mentioned earlier, continual pre-training can also be used to facilitate life-long learning, with the model continually being updated with new facts and knowledge. However, currently this may not be the most effective paradigm for new knowledge learning. You are probably better off using RAG for that. We will explore RAG in more detail in <a data-type="xref" href="ch12.html#ch12">Chapter 12</a>.</p>
<div data-type="tip"><h6>Tip</h6>
<p><a href="https://oreil.ly/H38wF">Task-adaptive pre-training (TAPT)</a> is a useful supplement<a data-type="indexterm" data-primary="task-adaptive pre-training (TAPT)" id="id1124"/> to domain-adaptive pre-training<a data-type="indexterm" data-primary="domain-adaptive pre-training (DAPT)" id="id1125"/><a data-type="indexterm" data-primary="DAPT (domain-adaptive pre-training)" id="id1126"/>. TAPT involves continual pre-training of the LLM on a much smaller but more task-specific unsupervised dataset. To prevent catastrophic forgetting, you should perform DAPT first before TAPT, and then subsequently perform any supervised fine-tuning on your downstream tasks. Unsupervised data for TAPT can be selected using similar methods as that used for DAPT: by constructing embeddings of data and selecting data that is clustered with gold-truth sentences.</p>
</div>

<p>In summary, continual pre-training can be very effective in cases where you have a large body of domain-specific text and the domain is very distinctly characterized by a specialized linguistic structure or vocabulary. Continual pre-training can also be used to help adapt the LLM to a new language.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Domain-specific text can contain jargon specific to that domain. One strategy that has worked well for me is to add extra tokens to represent domain-specific jargon.</p>
</div>

<p>Continual pre-training can take a lot of computational resources. Fine-tuning on smaller datasets takes substantially less resources. However, in the era of large language models, it is imperative to do all we can to reduce compute and memory requirements. Therefore, let’s next discuss some parameter-efficient fine-tuning techniques that make the fine-tuning process more accessible in resource-constrained environments<a data-type="indexterm" data-startref="xi_finetuningmodelscontinualpretraining72838" id="id1127"/><a data-type="indexterm" data-startref="xi_parametersexpansiontechniques710079" id="id1128"/><a data-type="indexterm" data-startref="xi_continualpretraining72838" id="id1129"/><a data-type="indexterm" data-startref="xi_pretrainingofdatacontinualpretraining72838" id="id1130"/>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Parameter-Efficient Fine-Tuning"><div class="sect1" id="parameter-efficient-fine-tuning">
<h1>Parameter-Efficient Fine-Tuning</h1>

<p>In PEFT<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="parameter efficient" id="xi_finetuningmodelsparameterefficient713142"/><a data-type="indexterm" data-primary="parameter-efficient fine-tuning (PEFT)" id="xi_parameterefficientfinetuningPEFT713142"/><a data-type="indexterm" data-primary="PEFT (parameter-efficient fine-tuning)" id="xi_PEFTparameterefficientfinetuning713142"/>, instead of updating all the parameters of the model, we update only a small number of parameters. This can vastly bring down compute and storage 
<span class="keep-together">requirements</span>.</p>

<p>We can categorize current PEFT techniques into three types:</p>
<dl>
<dt>Adding new parameters</dt>
<dd>
<p>This involves adding some extra parameters to the LLM and training only them.</p>
</dd>
<dt>Selecting a subset of parameters</dt>
<dd>
<p>This involves choosing to update only a small subset of parameters of the LLM, either by selecting the subset apriori or by learning the appropriate subset.</p>
</dd>
<dt>Low-rank methods</dt>
<dd>
<p>This involves using methods that reduce the number of parameters to train by finding a smaller matrix containing almost the same information as a larger matrix.</p>
</dd>
</dl>

<p>Let’s now go through each of these in detail.</p>








<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Adding New Parameters"><div class="sect2" id="id248">
<h2>Adding New Parameters</h2>

<p>Perhaps your work needs you to fine-tune models for a large number of tasks<a data-type="indexterm" data-primary="parameters" data-secondary="adding new" id="xi_parametersaddingnew714576"/>. Or maybe you need to drive personalization by fine-tuning a model for each user. It will be cumbersome to maintain and deploy so many full copies of fine-tuned models.</p>

<p>One way to avoid updating all the parameters of the model is to add a few extra parameters to the model and train only them.  Instead of storing and deploying full copies of each fine-tuned model, you store only the newly added parameters.</p>

<p>Common ways of adding new parameters for fine-tuning include:</p>
<dl>
<dt>Bottleneck adapters</dt>
<dd>
<p>These are lightweight modules added to the Transformer layers.</p>
</dd>
<dt>Prefix tuning</dt>
<dd>
<p>These are task-specific vectors that are trained and prefixed to the input.</p>
</dd>
<dt>Prompt tuning (soft prompts)</dt>
<dd>
<p>This is similar to prefix tuning but with a simplified training approach.</p>
</dd>
</dl>

<p>Let’s discuss each of these techniques in detail.</p>










<section data-type="sect3" data-pdf-bookmark="Bottleneck adapters"><div class="sect3" id="id112">
<h3>Bottleneck adapters</h3>

<p>Adapters<a data-type="indexterm" data-primary="bottleneck adapters" id="xi_bottleneckadapters71599"/><a data-type="indexterm" data-primary="adapters" data-secondary="bottleneck adapters" id="xi_adaptersbottleneckadapters71599"/> are parameter modules attached to the LLM architecture. Adapters can be integrated into the LLM architecture in a variety of ways, but in Transformers, the common way is to insert them at each layer of the Transformer. To reduce the number of parameters, the width of the adapter module should be much less than the width of the underlying Transformer model. This constitutes a <em>down-projection</em>, also called a bottleneck.</p>

<p>Therefore, a bottleneck adapter sublayer consists of a down-projection matrix<a data-type="indexterm" data-primary="down-projection matrix" id="id1131"/>, an up-projection matrix at the end to project back to the original dimensions, and parameters that can be configured in a variety of ways in the middle. During fine-tuning, only the adapter modules are updated. The original pre-trained model is not updated. Adapters are initialized with a near-identity initialization to ensure smooth training.</p>

<p><a data-type="xref" href="#adapters">Figure 7-2</a> shows where in the Transformer architecture the bottleneck adapters typically are inserted. Note that this is just one possible configuration.</p>

<figure class="less_space pagebreak-before"><div id="adapters" class="figure">
<img src="assets/dllm_0702.png" alt="adapters" width="324" height="800"/>
<h6><span class="label">Figure 7-2. </span>Adapter modules in the Transformer</h6>
</div></figure>

<p>How does this all work in practice? The <a href="https://oreil.ly/z05rI"><em>adapters</em> library</a> comes in handy to facilitate fine-tuning LLMs using these advanced techniques.</p>

<p>Here is how you can start using bottleneck adapters using the adapters library:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">adapters</code> <code class="kn">import</code> <code class="n">DoubleSeqBnConfig</code>
<code class="n">adapter_config</code> <code class="o">=</code> <code class="n">DoubleSeqBnConfig</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">add_adapter</code><code class="p">(</code><code class="s2">"bottleneck_adapter"</code><code class="p">,</code> <code class="n">config</code><code class="o">=</code><code class="n">adapter_config</code><code class="p">)</code></pre>

<p><code>DoubleSeqBnConfig</code> refers to a config natively supported by the library, corresponding to the adapter architecture shown in <a data-type="xref" href="#adapters">Figure 7-2</a>.
But as I mentioned before, you can change the size and shape of the adapters as you wish. To do that, we need to use <code>BnConfig</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">adapters</code> <code class="kn">import</code> <code class="n">BnConfig</code>
<code class="n">adapter_config</code> <code class="o">=</code> <code class="n">BnConfig</code><code class="p">(</code><code class="n">mh_adapter</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">output_adapter</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>

<code class="n">reduction_factor</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">non_linearity</code><code class="o">=</code><code class="s2">"gelu"</code><code class="p">)</code></pre>

<p>Here is what these arguments stand for:</p>
<dl>
<dt><code>mh_adapter</code></dt>
<dd>
<p>Refers to the adapter modules added right after the multi-head attention sublayer of the Transformer.</p>
</dd>
<dt><code>output_adapter</code></dt>
<dd>
<p>Refers to the adapter modules added right after the feedforward network sublayer of the Transformer.</p>
</dd>
<dt><code>reduction_factor</code></dt>
<dd>
<p>Refers to the down-projection factor: by how much should the adapter width be scaled down compared to the Transformer layer width?</p>
</dd>
<dt><code>non_linearity</code></dt>
<dd>
<p>Refers to the activation function being used, like RELU or GELU.</p>
</dd>
</dl>

<p>Refer to the adapters library <a href="https://oreil.ly/n1Pga">documentation</a> for more configuration options. There are so many configuration options available!</p>

<p>While using bottleneck adapters leads to a vast decrease in fine-tuning time and complexity, adding parameters across all layers of the Transformer increases inference latency by a small amount. Typically, the inference time using commonly used adapter configurations is expected to increase by 6%–8%.</p>
<div data-type="tip"><h6>Tip</h6>
<p>It is possible to reduce the inference latency<a data-type="indexterm" data-primary="inference latency" id="id1132"/> by dropping some adapter layers during inference. <a href="https://oreil.ly/GM_1X">Rücklé et al. propose AdapterDrop</a>, a set of methods for dropping adapter modules during training and inference. They propose dropping adapters from the first few layers of the Transformer during inference or pruning the adapters from each layer that is the <a data-type="indexterm" data-startref="xi_bottleneckadapters71599" id="id1133"/><a data-type="indexterm" data-startref="xi_adaptersbottleneckadapters71599" id="id1134"/>least 
<span class="keep-together">activated</span>.</p>
</div>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Prefix-tuning"><div class="sect3" id="id113">
<h3>Prefix-tuning</h3>

<p>One drawback of using adapter-based fine-tuning techniques is that during inference, each batch can support only a single adapter instance, i.e., an adapter fine-tuned for a particular task. Prefix-tuning, in contrast, enables multiple tasks to be run in the same batch.</p>

<p>In prefix-tuning<a data-type="indexterm" data-primary="prefix-tuning" id="xi_prefixtuning721517"/>, we add and train task-specific vectors to the prefix of the input. This vastly reduces the number of parameters we need to fine-tune. Recall that the prompt contains the instruction, the input, and optionally some few-shot examples. The text generated by the LLM is conditioned on the output generated so far, and the prompt. To this, we add additional context that the LLM can attend to, in the form of these prefix vectors. The new tokens prefixed to the input<a data-type="indexterm" data-primary="virtual tokens" id="id1135"/><a data-type="indexterm" data-primary="soft prompts" id="id1136"/><a data-type="indexterm" data-primary="prompting" data-secondary="soft prompts" id="id1137"/> are called <em><em>virtual tokens</em></em> or <em><em>soft prompts</em></em>.</p>

<p><a data-type="xref" href="#prefix-tuning">Figure 7-3</a> shows how prefix-tuning occurs in the Transformer.</p>

<figure><div id="prefix-tuning" class="figure">
<img src="assets/dllm_0703.png" alt="prefix-tuning" width="600" height="373"/>
<h6><span class="label">Figure 7-3. </span>Prefix-tuning</h6>
</div></figure>

<p>As the figure shows, prefix parameters are added at each layer.</p>

<p>Prefix-tuning is much more parameter-efficient than bottleneck adapters<a data-type="indexterm" data-primary="bottleneck adapters" id="id1138"/><a data-type="indexterm" data-primary="adapters" data-secondary="bottleneck adapters" id="id1139"/>, taking up only 0.1% or less of a model’s parameters, as compared to adapters where it is usually 2% or more. However, prefix-tuning is harder to train effectively than adapters. Prefix-tuning also reduces the sequence length of the model in order to accommodate the virtual tokens.</p>

<p>Similar to adapters, initialization is very important for prefix-tuning. The virtual tokens can be initialized by choosing words that are related to the task the model is being fine-tuned for.</p>

<p>Using the adapters library<a data-type="indexterm" data-primary="adapters" data-secondary="prefix-tuning implementation" id="id1140"/>, we can implement prefix-tuning<a data-type="indexterm" data-startref="xi_prefixtuning721517" id="id1141"/>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">adapters</code> <code class="kn">import</code> <code class="n">PrefixTuningConfig</code>
<code class="n">adapter_config</code> <code class="o">=</code> <code class="n">PrefixTuningConfig</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">add_adapter</code><code class="p">(</code><code class="s2">"prefix_tuning"</code><code class="p">,</code> <code class="n">config</code><code class="o">=</code><code class="n">adapter_config</code><code class="p">)</code></pre>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Prompt tuning"><div class="sect3" id="id114">
<h3>Prompt tuning</h3>

<p>Prompt tuning<a data-type="indexterm" data-primary="prompt tuning" id="id1142"/> is a simplified version of prefix-tuning. Unlike prefix tuning, there are no prefix parameters at each layer.</p>

<p><a data-type="xref" href="#prompt-tuning">Figure 7-4</a> shows how prompt-tuning occurs in the Transformer.</p>

<figure><div id="prompt-tuning" class="figure">
<img src="assets/dllm_0704.png" alt="prompt-tuning" width="600" height="128"/>
<h6><span class="label">Figure 7-4. </span>Prompt-tuning</h6>
</div></figure>

<p>The adapters library<a data-type="indexterm" data-primary="adapters" data-secondary="prompt tuning configuration" id="id1143"/> provides a built-in configuration for prompt tuning:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">adapters</code> <code class="kn">import</code> <code class="n">PromptTuningConfig</code>
<code class="n">adapter_config</code> <code class="o">=</code> <code class="n">PromptTuningConfig</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">add_adapter</code><code class="p">(</code><code class="s2">"prompt_tuning"</code><code class="p">,</code> <code class="n">config</code><code class="o">=</code><code class="n">adapter_config</code><code class="p">)</code></pre>

<p>Some relevant configuration parameters for prompt tuning include:</p>
<dl>
<dt><code>prompt_length</code></dt>
<dd>
<p>The length of the prompt tokens; 10–30 is a good start.</p>
</dd>
<dt><code>prompt_init</code></dt>
<dd>
<p>The method for initializing these tokens. They can be initialized either through the embedding of a string or by a random uniform initialization.</p>
</dd>
<dt><code>prompt_init_text</code></dt>
<dd>
<p>If the soft prompt is initialized by string, the text that is used to initialize it. This can be a descriptor of the task at hand.</p>
</dd>
</dl>

<p class="less_space pagebreak-before"><a href="https://oreil.ly/BPpRu">Lester et al.</a>, who introduced prompt-tuning, also leverage it to perform soft prompt ensembling. For soft prompt ensembling, you train several soft prompts for each task. Then, for a given input, you use each of them as a prefix separately and generate the output. You can then use majority voting to select the correct output among the generated ones.</p>

<p>So far, we have seen techniques where new parameters are added to the model for fine-tuning. However, we can implement PEFT by fine-tuning only a small subset of parameters of the model without having to add new parameters. Let’s explore these methods next<a data-type="indexterm" data-startref="xi_parametersaddingnew714576" id="id1144"/>.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Subset Methods"><div class="sect2" id="id115">
<h2>Subset Methods</h2>

<p>A naive way of choosing a subset<a data-type="indexterm" data-primary="subset methods, parameter tuning" id="id1145"/> of parameters to fine-tune on would be to fine-tune only the upper layers of the Transformer and keep everything else frozen. The lower layers of the Transformer are known to be specialized in more fundamental aspects of language like syntax, which we want the LLM to preserve.</p>

<p>Another way is to fine-tune only the bias terms<a data-type="indexterm" data-primary="bias and fairness issues" data-secondary="fine-tuning focus on terms" id="id1146"/> (discussed in <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>) of the Transformer. This was proposed by <a href="https://oreil.ly/SaWoe">Zaken et al.</a>, who show that you can gain almost the same level of performance as that of fully fine-tuning a model by just fine-tuning on the bias terms. The authors observed that this technique is mostly effective when your training data is limited.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1147">
<h1>Does Fine-Tuning Learn New Capabilities?</h1>
<p>This is an important question with heavy implications<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="and learning new capabilities" data-secondary-sortas="learning new capabilities" id="id1148"/>. There is increasing <a href="https://oreil.ly/cZINX">evidence</a> that fine-tuning (the way it is performed today) only exposes already existing capabilities and doesn’t necessarily impart new capabilities.</p>

<p>If this is the case, then one can find a subset of parameters that is more amenable to solving a given downstream task. <a href="https://oreil.ly/yhPdl">Zhao et al.</a> propose using a binary mask that is trained per downstream task. This mask selects parameters that will be retained during inference that are relevant to solving the given downstream task.</p>
</div></aside>

<p>Ultimately, as we have seen here, there are tradeoffs involved in selecting each of these fine-tuning approaches. The ML community is working on developing best practices around this area. In the meanwhile, experimentation is key!</p>

<p>Next, let’s look at another way to update the parameters of an LLM: by merging it with the parameters of another LLM<a data-type="indexterm" data-startref="xi_finetuningmodelsparameterefficient713142" id="id1149"/><a data-type="indexterm" data-startref="xi_parameterefficientfinetuningPEFT713142" id="id1150"/><a data-type="indexterm" data-startref="xi_PEFTparameterefficientfinetuning713142" id="id1151"/>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Combining Multiple Models"><div class="sect1" id="id249">
<h1>Combining Multiple Models</h1>

<p>If you have access to multiple LLMs<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="combining multiple models" id="xi_finetuningmodelscombiningmultiplemodels728836"/>, each of them overlapping in terms of capabilities yet possessing certain unique characteristics, you want to leverage the capabilities of all the models in your downstream tasks in some way. This can be done by a variety of means, including model ensembling and model fusion or merging. This area of LLMs is in its infancy, and more work remains to be done to reap its full benefits. I call it the dark arts of NLP because the theoretical underpinnings of these techniques remain poorly understood. However, I do believe that even with these caveats it merits inclusion in this book, because the practical benefits are already visible. Let’s explore a few of these methods.</p>








<section data-type="sect2" data-pdf-bookmark="Model Ensembling"><div class="sect2" id="id116">
<h2>Model Ensembling</h2>

<p>Different LLMs may possess different but complementary capabilities<a data-type="indexterm" data-primary="models" data-secondary="ensembling of" id="xi_modelsensemblingof729268"/><a data-type="indexterm" data-primary="ensembling approach to multiple models" id="xi_ensemblingapproachtomultiplemodels729268"/>, a byproduct of the difference in their training regimens, training hyperparameters, etc. This is especially true when it comes to open source LLMs, where we have a plethora of models, most of them being trained on largely overlapping datasets, performing very closely to each other in benchmark evaluation metrics. Thus, an ensembling approach might bring forth benefits by allowing complementary capabilities from multiple models to be leveraged to generate better outputs.</p>

<p>In <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>, we discussed how, for generative tasks, it is useful to generate multiple outputs for the same input and select the best one using heuristics. We can extend this principle to multiple models. Each input is passed through <em>n</em> models. Optionally, an initial step can choose the top k models with the most high-quality or relevant outputs. The outputs from these models can be combined and fed through a model (which can be an LLM) to generate the final output.</p>

<p><a href="https://oreil.ly/Sipzu">Jiang et al.</a> present a framework called LLM-Blender<a data-type="indexterm" data-primary="LLM-Blender" id="xi_LLMBlender729690"/> for enabling LLM ensembling. The framework consists of two components:</p>

<ul>
<li>
<p>PairRanker scores the output from two models, thus choosing a winner.</p>
</li>
<li>
<p>GenFuser takes as input the output from k different models to generate the final output.</p>
</li>
</ul>

<p><a data-type="xref" href="#LLMBlender">Figure 7-5</a> shows the workings of the LLM-Blender framework.</p>

<figure><div id="LLMBlender" class="figure">
<img src="assets/dllm_0705.png" alt="LLMBlender" width="600" height="518"/>
<h6><span class="label">Figure 7-5. </span>LLM-Blender</h6>
</div></figure>

<p>Let’s dig deeper into each of these modules.</p>










<section data-type="sect3" data-pdf-bookmark="PairRanker"><div class="sect3" id="id117">
<h3>PairRanker</h3>

<p>Consider you have access to <em>n</em> different models<a data-type="indexterm" data-primary="PairRanker" id="id1152"/>. For a given input, you feed the input to each of these models to generate the outputs. Now, for each pair of outputs, you can combine them with the input and feed them to the PairRanker module. The PairRanker module is trained to provide scores for each of the outputs. If you end up feeding all the pairs of outputs to the PairRanker module, you will then find the output (model) with the highest score. This output can then be taken as the final output.</p>

<p>However, this just selects the best output and doesn’t necessarily combine the capabilities of the different models. For that, the LLM-Blender framework consists of a module called GenFuser.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="GenFuser"><div class="sect3" id="id118">
<h3>GenFuser</h3>

<p>For GenFuser<a data-type="indexterm" data-primary="GenFuser" id="id1153"/>, we take the top k results from the PairRanker scores. We then feed them together to the GenFuser, which generates the final output. The GenFuser in practice is just a fine-tuned LLM that is tuned to accept several candidate inputs and generate an output that combines the characteristics of the different candidates.</p>

<p>Let’s see how this works in practice. We can use the <a href="https://oreil.ly/F2IcX">LLM-Blender library</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">llm_blender</code>
<code class="kn">from</code> <code class="nn">llm_blender.blender.blender_utils</code> <code class="kn">import</code> <code class="n">get_topk_candidates_from_ranks</code>

<code class="n">ensemble</code> <code class="o">=</code> <code class="n">llm_blender</code><code class="o">.</code><code class="n">Blender</code><code class="p">()</code>
<code class="n">ensemble</code><code class="o">.</code><code class="n">loadranker</code><code class="p">(</code><code class="s2">"llm-blender/PairRM"</code><code class="p">)</code>
<code class="n">ensemnle</code><code class="o">.</code><code class="n">loadfuser</code><code class="p">(</code><code class="s2">"llm-blender/gen_fuser_3b"</code><code class="p">)</code>

<code class="n">rank_list</code> <code class="o">=</code> <code class="n">blender</code><code class="o">.</code><code class="n">rank</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">candidate_outputs</code><code class="p">)</code>
<code class="n">top_k</code> <code class="o">=</code> <code class="n">get_topk_candidates_from_ranks</code><code class="p">(</code><code class="n">rank_list</code><code class="p">,</code> <code class="n">candidate_outputs</code><code class="p">,</code> <code class="n">top_k</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>
<code class="n">final_output</code> <code class="o">=</code> <code class="n">ensemble</code><code class="o">.</code><code class="n">fuse</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">top_k</code><code class="p">)</code></pre>

<p>Given an input and a list of <code>candidate_outputs</code> from <em>n</em> different language models, we rank the outputs using the PairRanker and then select the top-k ranked outputs and fuse them to generate the final output.</p>

<p>While ensembling methods can be effective, there is a lot of recent interest in model fusion techniques<a data-type="indexterm" data-startref="xi_modelsensemblingof729268" id="id1154"/><a data-type="indexterm" data-startref="xi_ensemblingapproachtomultiplemodels729268" id="id1155"/><a data-type="indexterm" data-startref="xi_LLMBlender729690" id="id1156"/>.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Model Fusion"><div class="sect2" id="id119">
<h2>Model Fusion</h2>

<p>In this approach, we combine the parameters<a data-type="indexterm" data-primary="models" data-secondary="fusion of" id="xi_modelsfusionof734244"/><a data-type="indexterm" data-primary="parameters" data-secondary="model merging/fusion" id="xi_parametersmodelmergingfusion734244"/><a data-type="indexterm" data-primary="fusion, model" id="xi_fusionmodel734244"/> of multiple models in some way. The idea is that by combining the parameters of multiple models, we might be able to benefit from all the complementary capabilities possessed by each of the individual models, within a single model.</p>

<p>Some of the common methods used in model fusion are:</p>
<dl>
<dt>Averaging</dt>
<dd>
<p>The simplest way to combine multiple models is to average their parameters. Simple averaging has been shown to be quite effective.</p>
</dd>
<dt>Weighted averaging</dt>
<dd>
<p>During averaging, certain models or even certain layers in models can be weighted more.</p>
</dd>
<dt>Interpolation</dt>
<dd>
<p>Each model can be weighted by a factor w1, w2,…wn, with:</p>
</dd>
</dl>

<pre data-type="programlisting" data-code-language="python"><code class="n">w1</code> <code class="o">+</code> <code class="n">w2</code> <code class="o">+</code> <code class="n">w3</code> <code class="o">+...</code><code class="n">wn</code> <code class="o">=</code> <code class="mi">1</code>
<code class="n">w1p1</code> <code class="o">+</code> <code class="n">w2p2</code> <code class="o">+</code> <code class="n">w3p3</code> <code class="o">+...</code><code class="n">wnpn</code></pre>

<p>where p1, p2, p3…​pn are the parameters of models m1, m2, m3…mn.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1157">
<h1>Can Model Fusion Remove Undesirable Model Attributes?</h1>
<p><a href="https://oreil.ly/xiX8u">Zaman et al.</a> have made a very interesting observation: when you fuse models, the shared capabilities of the models are preserved, while the unshared capabilities are usually lost. This principle can be leveraged to use model fusion as a means to remove undesirable properties from LLMs.</p>

<p>The authors show that simple model averaging can reduce gender and racial bias<a data-type="indexterm" data-primary="bias and fairness issues" data-secondary="model fusion and reduction in" id="id1158"/> exhibited by LLMs. They also reduce the propensity of the LLM to leak sensitive information, as model fusion results in the model forgetting information that is not shared. The more the models are fused, the better the forgetting capability.</p>
</div></aside>

<p>One of the benefits in merging multiple models is model reuse. Say you have a base LLM at your organization. It is used by people all across the organization, who take the model and fine-tune it on their own tasks. They then upload the fine-tuned models back. You can then merge the weights of all the models, resulting in a stronger pre-trained model. This model can then be used as a new version of the base model. This process has been coined Collaborative Descent (ColD) Fusion<a data-type="indexterm" data-primary="Collaborative Descent (ColD) Fusion" id="id1159"/><a data-type="indexterm" data-primary="ColD Fusion (Collaborative Descent Fusion)" id="id1160"/> by <a href="https://oreil.ly/LTcdf">Don-Yehiya et al.</a></p>

<p>Why would we want to do this? The idea is that if we want to fine-tune an LLM on a dataset, it would be nice to have a good starting point such that the training is optimal. The hypothesis is that if we already fine-tuned the LLM on another task, the fine-tuned LLM is a better starting point than the base LLM. This is called intertraining<a data-type="indexterm" data-primary="intertraining" id="id1161"/>. This too is a fairly new concept, so proceed with caution.</p>

<p>Instead of merging all the parameters of the model, you can merge only a small portion of them. In fact, we could just merge the adapter modules<a data-type="indexterm" data-startref="xi_modelsfusionof734244" id="id1162"/><a data-type="indexterm" data-startref="xi_parametersmodelmergingfusion734244" id="id1163"/><a data-type="indexterm" data-startref="xi_fusionmodel734244" id="id1164"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Adapter Merging"><div class="sect2" id="id120">
<h2>Adapter Merging</h2>

<p>Earlier in the chapter, we learned about adapters<a data-type="indexterm" data-primary="adapters" data-secondary="merging" id="id1165"/><a data-type="indexterm" data-primary="domain-adaptive models" id="id1166"/><a data-type="indexterm" data-primary="models" data-secondary="domain-adaptive" id="id1167"/>, which can be used for a variety of purposes including domain-adaptive pre-training. While you can train different adapters for different domains, the question remains on how you would treat new domains seen at inference time. One solution would be to average the adapters related to the closest domains and use that for novel domains. This has been shown to work well, by <a href="https://oreil.ly/mKoZ1">Chronopoulou et al.’s AdapterSoup framework</a><a data-type="indexterm" data-primary="AdapterSoup framework" id="id1168"/>.</p>

<p>Another way to combine adapter parameters is in the context of an MoE<a data-type="indexterm" data-primary="mixture of experts (MoE) models" id="id1169"/><a data-type="indexterm" data-primary="AdaMix framework" id="id1170"/><a data-type="indexterm" data-primary="MoE (mixture of experts) models" id="id1171"/> framework, introduced in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>. Recall that in a mixture-of-experts model, the routing function determines which expert(s) will handle the input. <a href="https://oreil.ly/pc7Js">Wang et al.’s AdaMix 
<span class="keep-together">framework</span></a> extends this to adapter modules. Instead of learning only one adapter module per layer, we learn multiple expert modules. During inference, all the adaptation layers are merged.</p>

<p>Model merging is a fascinating subarea of LLMs. Even if you are not using it in your applications, I highly recommend experimenting with it because it doubles as a really neat tool to understand the working of LLMs<a data-type="indexterm" data-startref="xi_finetuningmodelscombiningmultiplemodels728836" id="id1172"/>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id366">
<h1>Summary</h1>

<p>In this chapter, we learned a plethora of advanced fine-tuning techniques, including continual pre-training strategies like experience replay and parameter expansion; parameter-efficient fine-tuning techniques like bottleneck adapters, prefix tuning, prompt tuning, and subset selection; and various types of model merging and ensembling. We also learned the various motivations for updating model weights and the suitability of different methods for each of those situations.</p>

<p>As discussed in the previous and current chapter, fine-tuning is not a panacea and cannot learn new capabilities or necessarily digest new knowledge. In the next chapter, we will discuss limitations of LLMs like poor steerability, hallucinations, and reasoning issues, along with techniques for mitigating them.</p>
</div></section>
</div></section></div>
</div>
</body></html>