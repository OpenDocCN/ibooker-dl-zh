- en: 7 Interventions and causal effects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Case studies of interventions in machine learning engineering contexts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How interventions relate to A/B tests and randomized experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing interventions on causal models with intervention operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a causal model to represent many interventional distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal effects as natural extensions of an intervention distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An intervention is something an agent *does* to cause other things to happen.
    Interventions *change* the data generating process (DGP).
  prefs: []
  type: TYPE_NORMAL
- en: 'Interventions are the most fundamental concept in how we define causality.
    For example, the concept of intervention, written in terms of “manipulation” and
    “varying” a factor, is central to this definition from an influential 1979 textbook
    on experimental design:'
  prefs: []
  type: TYPE_NORMAL
- en: The paradigmatic assertion in causal relationships is that manipulation of a
    cause will result in the manipulation of an effect . . . . Causation implies that
    by varying one factor I can make another vary. [¹](#footnote-211)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Interventions are how we go from correlation to causality. Correlation is symmetric;
    the statements “Amazon’s laptop sales correlate with Amazon’s laptop bag sales”
    and “Amazon’s laptop bag sales correlate with Amazon’s laptop sales” are equivalent.
    But interventions make causality a one-way street: if Amazon recommends the sale
    of laptops, laptop bag sales will increase, but if Amazon promotes the sale of
    laptop bags, we wouldn’t expect people to respond by buying new laptops to fill
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: A model must have a way of reasoning about intervention to be admitted to the
    club of causal models. Any model that lets you reason about how interventions
    change the DGP is, by definition, a causal model.
  prefs: []
  type: TYPE_NORMAL
- en: You are probably already familiar with interventions in the form of experiments,
    such as A/B tests or randomized clinical trials. Such experiments focus on inferring
    causal effects. Put simply, a causal effect is just a comparison of the expected
    results of different interventions (e.g., a treatment and a control, or “A” and
    “B” in an A/B test).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn how to model an intervention and causal effects
    even if, indeed *especially* if, we do not or cannot do the intervention in real
    life. We’ll start this chapter with case studies that motivate modeling interventions.
    All the datasets and notebooks for executing them are available at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook).
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Case studies of interventions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A machine learning model can drive decisions to make “interventions.” Those
    interventions can, in turn, create conditions different from those that occurred
    during model training. This mismatch in training conditions and deployment conditions
    can lead to problems.
  prefs: []
  type: TYPE_NORMAL
- en: '7.1.1 Case study: Predicting the weather vs. business performance'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every day you wake up, look out the window, and guess whether or not it will
    rain. Based on that guess, you decide whether to take an umbrella on your morning
    walk to work. Several times you guess and choose incorrectly; you either take
    an umbrella and it doesn’t rain, making you look like a fop, or you don’t take
    the umbrella, and it rains, making you look wet. You decide to train a machine
    learning model that will take detailed atmospheric readings in the morning and
    produce a prediction of whether or not it will rain. By leveraging machine learning
    to get more accurate predictions, you expect fewer mistakes in deciding whether
    to bring the umbrella.
  prefs: []
  type: TYPE_NORMAL
- en: You start by collecting daily atmospheric readings as features, and record whether
    it rained as labels. After enough days, you have your first block of training
    data. Next, you train the model on that training data and validate its accuracy
    on hold-out data. Finally, you deploy the trained model, meaning that you use
    it daily to decide whether to take or leave your umbrella. As you use the deployed
    model, you continue to log features and labels daily. Eventually, you have enough
    additional data for a second training block, and you retrain your model to benefit
    from both blocks of data, leading to higher accuracy than you had after training
    on just the first block. You continue to iteratively train the model as you collect
    more blocks of data. Figure 7.1 illustrates the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F01_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 Example of a machine learning training workflow where the sensor
    data is the features, weather is the label, and bringing an umbrella is the decision.
    After each training block, the new data is used to update the old model, and a
    new model is deployed. In this case, the decision does not affect future data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now let’s consider a parallel example in business. You are a data scientist
    at a company. Instead of atmospheric readings, you have economic and industry
    data. Instead of predicting whether the day will be rainy, you are predicting
    whether the quarter will end with low revenues. Instead of deciding whether to
    bring an umbrella, you are deciding whether to advertise. Figure 7.2 illustrates
    the workflow, which mirrors the weather example in figure 7.1 exactly; sunny and
    rainy days in figure 7.1 map to good and bad quarters in figure 7.2, and the decision
    to bring or leave the umbrella maps to the decision to advertise or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F02_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 This is a mirror example of the workflow in figure 7.1\. Business
    indicators are the features, quarterly performance is the label, and advertising
    is the decision. In this case, the decision affects future data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Even though the labels and decisions in the two examples mirror one another,
    the causal structure of the business example is fundamentally different; the act
    of bringing an umbrella will not affect the weather in future days, but the act
    of advertising will affect business in future quarters. As a result, training
    block 2 represents a different DGP than training block 1 because revenue in training
    block 2 was affected by advertising. During training, a naive predictive model
    might go so far as to associate signs of a lousy quarter with *high* revenue,
    since, in the past, signs of bad quarters led your company to advertise, which
    consequently boosted revenue.
  prefs: []
  type: TYPE_NORMAL
- en: We deploy machine learning models to drive or automate decisions. Those decisions
    do not impact the data in domains like meteorology, geology, and astronomy. But
    in many, if not most, domains where we want to use machine learning, those model-driven
    decisions are interventions—actions that change the DGP. That can lead to a mismatch
    between the model’s training and deployment conditions, leading to problems in
    the model’s reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Another real-world example of this problem occurs in anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: '7.1.2 Case study: Credit fraud detection'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Anomaly detection seeks to predict when an abnormal event is occurring. One
    example is detecting a fraudulent transaction on a credit card. Credit card companies
    do supervised training of predictive models of fraud using transaction data, where
    attributes of credit card transactions (buying patterns, location, cost of the
    item, etc.) are the features, and whether the customer later reports the transaction
    as fraudulent is the label.
  prefs: []
  type: TYPE_NORMAL
- en: As in the weather and business examples, you train a model on an initial training
    block. After training, you can deploy the algorithm to predict fraud in real time.
    When a transaction is initiated, the algorithm is run, and a prediction is generated.
    If the algorithm predicts fraud, the transaction is rejected.
  prefs: []
  type: TYPE_NORMAL
- en: While this system is in deployment, a second training set is being compiled.
    Some fraud still gets through and is later reported as fraudulent by the customers.
    Those transactions are labeled fraudulent in this new block of data, but the DGP
    has changed from the initial training set. The deployed version 1.0 predictive
    model is rejecting transactions that it predicted were fraudulent, but because
    they were rejected, you don’t know if they were actual cases of fraud. These rejected
    transactions are excluded from the next training set because they lack labels.
  prefs: []
  type: TYPE_NORMAL
- en: If the model is retrained on the second block, it may develop a bias toward
    fraud that slipped past the fraud rejection system and against the cases of fraud
    that were rejected. This bias can become more severe over several iterations.
    This process is analogous to a homicide detective who, over time, does well in
    solving cases involving uncommon weapons but poorly in cases involving guns.
  prefs: []
  type: TYPE_NORMAL
- en: The filtering of fraudulent transactions in deployment is an intervention. In
    practice, anomaly detection algorithms address this problem by accounting for
    interventions in some way.
  prefs: []
  type: TYPE_NORMAL
- en: '7.1.3 Case study: Statistical analysis for an online role-playing game'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose you are a data scientist at an online role-playing game company. Your
    leadership wants to know if side-quest engagement (mini-objectives that are tangential
    to the game’s primary objectives) is a driver of in-game purchases of virtual
    artifacts. If the answer is yes, the company will intervene in the game dynamics
    such that players engage in more side-quests.
  prefs: []
  type: TYPE_NORMAL
- en: 'You do an analysis. You query the database and pull records for a thousand
    players, the first five of which are shown in table 7.1\. This is observational
    data (in contrast to experimental data) because the data is logged observations
    of the natural behavior of players as they log in and play. (The full dataset
    is available in the notebooks for the chapter: [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook).)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.1 Example rows from observational data on *Side-Quest Engagement* and
    *In-Game Purchases*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| User ID | Side-Quest Engagement | In-Game Purchases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 71d44ad5  | high  | 156.77  |'
  prefs: []
  type: TYPE_TB
- en: '| e6397485  | low  | 34.89  |'
  prefs: []
  type: TYPE_TB
- en: '| 87a5eaf7  | high  | 172.86  |'
  prefs: []
  type: TYPE_TB
- en: '| c5d78ca4  | low  | 215.74  |'
  prefs: []
  type: TYPE_TB
- en: '| d3b2a8ed  | high  | 201.07  |'
  prefs: []
  type: TYPE_TB
- en: '| dc85d847  | low  | 12.93  |'
  prefs: []
  type: TYPE_TB
- en: The standard data science analysis would involve running a statistical test
    of the hypothesis that there is a difference between the *In-Game Purchases* of
    players highly engaged in side-quests and those with low *Side-Quest Engagement*.
    The test calculates the mathematical difference between the sample means of *In-Game
    Purchases* for both groups. In statistical terms, this difference estimates an
    *effect size*. The test will examine whether this estimated effect size is significantly
    different from zero.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The code for this chapter was written with Pyro version 1.9.0, pandas version
    2.2.1, and pgmpy version 0.1.25\. Using Pyro’s `render` function to visualize
    a Pyro model as a DAG will require Graphviz. Visit [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for a link to a notebook that contains the code.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll perform this hypothesis test with the pandas library. First, we’ll pull
    the data and get the sample means and standard deviations within each group.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.1 Load *Side-Quest Engagement* vs.*In-Game Purchases* data and summarize
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Load the data from the database query into a pandas DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 For each level of Side-Quest Engagement (“low”, “high”), calculate the sample
    count (number of players), the sample mean In-Game Purchases amount, and the standard
    deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: This produces the summary in table 7.2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.2 Summary statistics from the online game data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Side-Quest Engagement | mean purchases | std | n |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| low  | 73.10  | 75.95  | 518  |'
  prefs: []
  type: TYPE_TB
- en: '| high  | 111.61  | 55.56  | 482  |'
  prefs: []
  type: TYPE_TB
- en: This database query pulled 1,000 players, where 482 of them were highly engaged
    in side-quests and 518 were not. The mean *In-Game Purchases* amount for highly
    engaged players is around $112 for high *Side-Quest Engagement* and $73 for low
    *Side-Quest Engagement*. Generalizing beyond this data, we conclude that players
    who are highly engaged in side-quests spend, on average 112 – 73 = $39 dollars
    more than those who aren’t. We can run a two-sample *Z*-test to make sure this
    difference is significant.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 7.2 Test if effect of engagement on *In-Game Purchases* is statistically
    significant**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 n1 and n2 are the numbers of players in each group (high vs. low engagement).'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 m1 and m2 are the group sample means.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 s1 and s2 are the group standard deviations.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Estimate the standard error of the difference in mean spending by pooling
    (combining) the group standard deviations.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Convert to a z-score, which has a standard norm under the (null) hypothesis
    of no difference in spending across engagement levels.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Test if the z-score is more than 2 standard deviations from 0, which beats
    a 5% significance threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this code shows that the difference in means is significant. Great,
    you did some data science that showed you have a statistically significant effect
    size: *In-Game Purchases* are significantly higher for players who are highly
    engaged in side-quests relative to those who are not. Based on your findings,
    leadership decides to modify the game dynamics to draw players into more side-quests.
    As a result, *In-Game Purchases* *decline*. How could this happen?'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.4 From randomized experiments to interventions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By now you’ve probably recognized that the result from listing 7.2 is a textbook
    example of how correlation doesn’t imply causation. If management wanted to know
    if intervening on game dynamics would lead to an increase in *In-Game Purchases*,
    they should have relied on analysis from a randomized experiment, not simple observational
    data. We’ll use the randomized experiment to build more intuition for a formal
    model of intervention and see how that intervention model could simulate a randomized
    experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.5 From observations to experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose that instead of running an observational study, you run an experiment.
    Rather than pull data from a SQL query, you randomly select a set of 1,000 players
    and randomly assign them to one of two groups of 500\. In one group, the game
    dynamics are modified such that *Side-Quest Engagement* is artificially fixed
    at “low,” and in the other group it is fixed to “high.” We’ll then observe their
    level of *In-Game Purchases*.
  prefs: []
  type: TYPE_NORMAL
- en: This will create experimental data that is the same size and has roughly the
    same split between engaged and unengaged players as the observational data in
    section 7.1.3\. Similarly, we’ll run the same downstream analysis. This will let
    us make an apples-to-apples comparison of using observational versus experimental
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.3 shows examples from the experimental data. You can find links to the
    data in [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.3 Example rows from the experimental data evaluating the effect of *Side-Quest
    Engagement* on *In-Game Purchases*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| User ID | Side-Quest Engagement | In-Game Purchases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2828924d  | low  | 224.39  |'
  prefs: []
  type: TYPE_TB
- en: '| 7e7c2452  | low  | 19.89  |'
  prefs: []
  type: TYPE_TB
- en: '| 3ddf2915  | low  | 221.26  |'
  prefs: []
  type: TYPE_TB
- en: '| 10c3d883  | high  | 93.21  |'
  prefs: []
  type: TYPE_TB
- en: '| c5080957  | high  | 61.82  |'
  prefs: []
  type: TYPE_TB
- en: '| 241c8fcf  | high  | 188.76  |'
  prefs: []
  type: TYPE_TB
- en: Again, we summarize the data with the following code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.3 Load experimental data and summarize
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Load the experimental data from the database query into a pandas DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 For each level of Side-Quest Engagement (“low”, “high”), calculate the sample
    count (number of players), the sample mean in-game purchase amount, and the standard
    deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.4 shows the same summary statistics for the experimental data as table
    7.2 does for the observational data.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.4 Summary statistics from the online game experimental data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Side-Quest Engagement | mean purchases | std | n |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| low  | 92.99  | 51.67  | 500  |'
  prefs: []
  type: TYPE_TB
- en: '| high  | 131.38  | 94.84  | 500  |'
  prefs: []
  type: TYPE_TB
- en: The experiment reflects what happened when the company intervened to increase
    *Side-Quest Engagement*. The sign of the effect size is negative relative to our
    first analysis; we got –38.39, meaning the mean purchases went down $38.39\. When
    we rerun the test of significance in listing 7.4, we see the difference is significant
    for the experimental data, just as it was for the observational data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.4 Conduct significance test on (experimental) difference in mean purchases
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 n1 and n2 are the number of players in each group (high vs low engagement).'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 m1 and m2 are the group sample means.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 s1 and s2 are the group standard deviations.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Estimate the standard error of the difference in mean spend by pooling (combining)
    the group standard deviations.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Convert to a z-score, which has a standard norm under the (null) hypothesis
    of no difference in spend across engagement levels.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Tests if the z-score is more than 2 standard deviations from 0, which beats
    a 5% significance threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: The result shows the difference in group means is again significant. If you
    had reported the results of this experiment instead of the results of the observational
    study, you would have correctly concluded that a policy of encouraging higher
    *Side-Quest Engagement* would lead to a drop in average *In-Game Purchases* (and
    you wouldn’t have recommended doing so).
  prefs: []
  type: TYPE_NORMAL
- en: This experiment had a cost. Many of those 1,000 players who were included in
    the experiment would have spent more on *In-Game Purchases* had they not been
    included in the experiment, and this is especially true for the 500 players assigned
    to the high side-quests group. That amounts to lost revenue that would have been
    realized had you not run the experiment. Moreover, the experiment created a suboptimal
    gaming experience for players who were assigned a level of *Side-Quest Engagement*
    that was different from their preferred level. These players are paying the company
    for a certain experience, and the experiment degraded that experience.
  prefs: []
  type: TYPE_NORMAL
- en: The least ideal outcome is reporting based on our simple two-sample analysis
    of the observational data; this had no cost, but it gave the wrong answer. A better
    outcome is running the experiment and getting the correct answer, though this
    comes at a cost. The ideal outcome is getting the right answer on the observational
    data for free. To do that, we need a causal model.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.6 From experiments to interventions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s see how we can use a causal model to simulate the results of the experiment
    from the observational data. First, let’s assume the causal DAG in figure 7.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F03_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 A simple DAG showing the causal relationship between *Side-Quest
    Engagement* and *In-Game Purchases. Guild Membership*is a common cause of both.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In our online game, many players are members of guilds. Guilds are groups of
    players who pool resources and coordinate their gameplay, such as working together
    on side-quests. Our model assumes that the amount of *In-Game Purchases* a player
    makes also depends on whether they are in a guild; members of the same guild pool
    resources, and many resources are virtual items they must purchase.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you run a modified version of that initial database query. The query
    produces the same exact observational data seen in table 7.1, except this time
    it includes an additional column indicating *Guild Membership*. Again, we see
    six players in table 7.5 (the same six as players shown in table 7.1).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.5 The same observational data as in table 7.1, but with a *Guild Membersh**ip*column
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| User ID | Side-Quest Engagement | Guild Membership | In-Game Purchases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 71d44ad5  | high  | member  | 156.77  |'
  prefs: []
  type: TYPE_TB
- en: '| e6397485  | low  | nonmember  | 34.89  |'
  prefs: []
  type: TYPE_TB
- en: '| 87a5eaf7  | high  | member  | 172.86  |'
  prefs: []
  type: TYPE_TB
- en: '| c5d78ca4  | low  | member  | 215.74  |'
  prefs: []
  type: TYPE_TB
- en: '| d3b2a8ed  | high  | member  | 201.07  |'
  prefs: []
  type: TYPE_TB
- en: '| dc85d847  | low  | nonmember  | 12.93  |'
  prefs: []
  type: TYPE_TB
- en: 'We are going to build a causal graphical model on this observational data using
    Pyro. To do this, we’ll need to model the causal Markov kernels: the probability
    distributions of *Guild Membership*, *Side-Quest Engagement* given *Guild Membership*,
    and *In-Game Purchases* given *Guild Membership* and *Side-Quest Engagement*.
    In our Pyro model, we’ll need to specify some canonical distributions for these
    variables and estimate their parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Estimating parameters and building the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pyro can jointly estimate the parameters of each of our causal Markov kernels
    just as it could the parameters across a complex neural network architecture.
    But it will make our lives easier to estimate the parameters of each kernel one
    at a time using everyday data science analysis, leveraging the concept of *parameter
    modularity* discussed in chapter 2\. Let’s start with *Guild Membership*.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.5 Estimate the probability distribution of *Guild Membership*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Load the data from the database query into a pandas DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calculate the proportions of members vs. nonmembers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This prints out the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: These are the proportions of guild members vs. nonmembers in the data. We can
    use these as estimates of the probability that a player is a member or a nonmember.
    If we took these proportions as is, they would be maximum likelihood estimates
    of the probabilities, but for simplicity, we’ll just put it at 50/50 (the probability
    of being a member is .5).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll do the same for the conditional probability distribution (CPD) of
    *Side-Quest Engagement* level given *Guild Membership*.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.6 Estimate the CPD of *Side-Quest Engagement* given *Guild Membership*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Calculate the probability distribution of Side-Quest Engagement level (“high”
    vs. “low”) given that a player is a member of a guild.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calculate the probability distribution of Side-Quest Engagement level (“high”
    vs. “low”) given that a player is not a member of a guild.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 7.6 prints the following output proportions of *Side-Quest Engagement*
    levels for guild members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following proportions are for non-guild-members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Again, we’ll round these results. Guild members have an 80% chance of being
    highly engaged in side-quests, while nonmembers have only a 20% chance of being
    highly engaged.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, for each combination of *Guild Membership* and *Side-Quest Engagement*,
    we’ll calculate the sample mean and standard deviation of *In-Game Purchases*.
    We’ll use these sample statistics as estimates for mean and location parameters
    in a canonical distribution when we code the causal Markov kernel for *In-Game
    Purchases* in the causal model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 7.7 Calculate purchase stats across levels of engagement and *Guild
    Membership***'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Estimate the sample mean and standard deviation of In-Game Purchases for
    non-guild-members with low Side-Quest Engagement.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Estimate the sample mean and standard deviation of In-Game Purchases for
    non-guild-members with high Side-Quest Engagement.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Estimate the sample mean and standard deviation of In-Game Purchases for
    guild members with low Side-Quest Engagement.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Estimate the sample mean and standard deviation of In-Game Purchases for
    guild members with high Side-Quest Engagement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For non-guild-members with low *Side-Quest Engagement*, we have these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For non-guild-members with high *Side-Quest Engagement*, we have
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For guild members with low *Side-Quest Engagement*, we have
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: For guild members with high *Side-Quest Engagement*, we have
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Finally, in listing 7.8, we use these various statistics as parameter estimates
    in a causal graphical model built in Pyro.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.8 Building a causal model of *In-Game Purchases* in Pyro
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Probability of being a guild member vs. a nonmember is .5\. Using this probability,
    we generate a Guild Membership value (1 for member, 0 for nonmember) from a Bernoulli
    distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We generate a value for Side-Quest Engagement from a Bernoulli distribution
    (1 for high, 0 for low) with a parameter that depends on Guild Membership.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Helper function for calculating parameters for In-Game Purchases'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 We specify the location parameter of a normal distribution on In-Game Purchases
    using the sample means we found in the observational data.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 As with the mean parameters, we specify the scale parameters for a canonical
    distribution on In-Game Purchases using the standard deviations we found in the
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: To confirm that the Pyro model encodes a causal DAG, we can run `pyro.render_
    model(model)`, which produces figure 7.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F04_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 Result of calling `pyro.render_model` with the causal model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Leveraging the parametric flexibility of probabilistic programming
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note the flexibility of our choices for modeling the variables in the Pyro model.
    For example, in modeling the distribution of *In-Game Purchases*, we used the
    normal distribution, but we could have used other distributions. For example,
    *In-Game Purchases* cannot be a negative number, so we could have selected a canonical
    distribution that is only defined for positive numbers, rather than a normal distribution,
    which is defined for negative and positive numbers. This would be especially useful
    for non-guild-members with low *Side-Quest Engagement*, because generation from
    a normal distribution with a mean of 37.95 and a scale parameter of 23.80 will
    have about a 5.5% chance of generating a negative value. However, we’re choosing
    to be a bit lazy and use the normal distribution in this case, since a few negative
    numbers for *In-Game Purchases* won’t have much impact on the results of our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The point is that probabilistic programming tools like Pyro provide us with
    parametric flexibility, unlike tools like pgmpy. It is good practice to leverage
    that flexibility to reflect your assumptions about the DGP.
  prefs: []
  type: TYPE_NORMAL
- en: Pyro’s intervention abstraction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pyro has an abstraction for representing an intervention in `pyro.do`. It takes
    a model and returns a new model that reflects the intervention. Listing 7.9 shows
    how we can use `pyro.do` to change the previous model into one that reflects an
    intervention that sets *Side-Quest Engagement* to “high” and to “low.”
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.9 Representing interventions with `pyro.do`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 An intervention that sets Side-Quest Engagement to 1.0 (i.e., “high”). This
    returns a new model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 An intervention that sets Side-Quest Engagement to 0.0 (i.e., “low”). This
    returns a new model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have two new models: one with an intervention that sets *Side-Quest
    Engagement* to “high” and one that sets it to “low.” If our original model is
    correct, generating 500 examples from each of these new intervened-upon models,
    and combining them into 1000 examples, effectively *simulates* the experiment.
    Remember, we estimated the parameters of this causal model using only the observational
    data illustrated in table 7.4\. If we can train a model on observational data
    and use it to accurately simulate the results of an experiment, that saves us
    from actually having to run the experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.10 uses `int_engaged_model` and `int_unengaged_model` to simulate
    experimental data. We can confirm that the simulation was effective by comparing
    the summary statistics of this simulated data to the summary statistics of the
    actual experimental data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.10 Simulating experimental data with `pyro.do` interventions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Set a random seed for reproducibility.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Simulate 500 rows from each intervention model, and combine them to create
    simulated experimental data.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The simulated data will include a Guild Membership column. We can drop it
    to get simulated data that looks like the original experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Recreate the statistical summaries of In-Game Purchases for each level of
    engagement.'
  prefs: []
  type: TYPE_NORMAL
- en: This code simulates the experiment, providing the summaries in table 7.6\. Again,
    these are sample statistics from a simulated experiment we created by first estimating
    some parameters on observational data, second, building a causal generative model
    with those parameters, and third, using `pyro.do` to simulate the results of an
    intervention. Contrast these with the statistics in table 7.7 that we obtained
    from the *actual* experimental data.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.6 Summary statistics from the simulated experiment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Side-Quest Engagement | count | mean | std |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| high  | 500  | 89.897309  | 52.696709  |'
  prefs: []
  type: TYPE_TB
- en: '| low  | 500  | 130.674021  | 93.921543  |'
  prefs: []
  type: TYPE_TB
- en: Table 7.7 Summary statistics from the actual experiment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Side-Quest Engagement | count | mean | std |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| high  | 500  | 92.99054  | 51.673631  |'
  prefs: []
  type: TYPE_TB
- en: '| low  | 500  | 131.38228  | 94.840705  |'
  prefs: []
  type: TYPE_TB
- en: The two sets of summaries are similar enough that we can say that we’ve successfully
    replicated the experimental results from the observational data.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.7 Recap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s recap. A causal DAG combined with a Pyro abstraction for an intervention
    allowed us to do an analysis on an observational dataset that produced the same
    results as an analysis on an experimental dataset. Had you run this analysis on
    the initial observational data instead of the simple two-sample statistical test,
    you would have provided the correct answer to leadership, and they would not have
    changed the dynamics to increase *Side-Quest Engagement*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this wasn’t a free lunch. This analysis required causal assumptions
    in the form of a causal DAG. Errors in specifying the causal DAG can lead to errors
    in the output of the analysis. But assuming your causal DAG was correct (or close
    enough), it would have saved you the actual costs and opportunity costs of running
    that experiment.
  prefs: []
  type: TYPE_NORMAL
- en: So how exactly does `pyro.do` work? How does it modify the model to represent
    an intervention? We’ll answer these questions with the ideas of *ideal interventions*
    and *intervention operators*.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 The ideal intervention and intervention operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand how our simulated experiment worked, we need a concrete definition
    of intervention. We’ll use a specific definition, called the *ideal intervention*,
    and also known as the *atomic intervention*, *structural intervention*, *surgical
    intervention*, and *independent intervention*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of an ideal intervention breaks down into three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The ideal intervention targets a specific variable or set of variables in the
    DGP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The operation sets those variables to a fixed value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By setting the variable to a fixed value, the intervention blocks the influence
    from the target’s causes, such that the target is now statistically independent
    of its causes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll use the notation do(*X*=*x*) to represent an ideal intervention that sets
    *X* to *x*. Note that we can have interventions on sets of variables, as in do(*X*=*x*,
    *Y*=*y*, *Z*=*z*).
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Intervention operators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A causal model represents relationships in the DGP. The preceding definition
    of the ideal intervention describes how it *changes* the DGP. Now it remains to
    us to define how our causal models will reflect that change.
  prefs: []
  type: TYPE_NORMAL
- en: An *intervention operator* is some way of *changing* our causal model to reflect
    an intervention. One of the first tasks of creating *any* novelcomputational representation
    of causality is to define an intervention operator for the ideal intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Intervention operators can implement ideal interventions, stochastic interventions
    (discussed in section 7.5), and other types of interventions. Unless I indicate
    otherwise, you can assume that “intervention operator” means “intervention operator
    for ideal interventions.”
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, structural causal models and general causal graphical models have
    well-defined intervention operators. We’ll explore those, as well as look at intervention
    operators designed for causal programs like `pyro.do`.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2 Ideal interventions in structural causal models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll start with the structural causal model. Let *M* represent a structural
    causal model of the online game. We’d write *M* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch7-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: '*f*[*G*], *f*[*E*], and *f*[*I*] are the assignment functions for *G* (*Guild
    Membership*), *E* (*Side-Quest Engagement*), and *I* (*In-Game Purchases*), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An ideal intervention do(*E*=“high”) transforms the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch7-eqs-1x.png)'
  prefs: []
  type: TYPE_IMG
- en: The intervention operator for the SCM replaces the intervention target *E*’s
    assignment function with the intervention value “high.”
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have an SCM with a variable (or set of variables) *X*. You want
    to apply an intervention do(*X*=*x*). The intervention operator replaces the intervention
    target’s assignment function with the intervention value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider how this meets the three elements of our definition of an ideal intervention:'
  prefs: []
  type: TYPE_NORMAL
- en: The intervention do(*X*=*x*) only directly affects the assignment function for
    *X*. No other assignment function is affected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intervention explicitly sets *X* to a specific value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the value of *X* is set to a constant, it no longer depends on its direct
    causal parents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '7.2.3 Graph surgery: The ideal intervention in causal DAGs and causal graphical
    models'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we’ll consider how to think graphically about the ideal intervention. First,
    let’s reexamine the online game’s causal DAG in figure 7.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F05_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 The causal DAG for the online game
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: According to our graph, *Guild Membership* is the causal parent of *Side-Quest
    Engagement*. That parent-child relationship determines a causal Markov kernel—the
    conditional probability distribution of *Side-Quest Engagement*, given *Guild
    Membership*. Recall our model of this causal Markov kernel, shown in table 7.8\.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.8 Conditional probability table for causal Markov kernel of *Side-Quest
    Engagement*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*|  | Guild Membership |'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| nonmember | member |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Side-Quest Engagement  | low  | .8  | .2  |'
  prefs: []
  type: TYPE_TB
- en: '| high  | .2  | .8  |*  *Imagine the mechanics of our experiment. Players log
    on, and the digital experimentation platform selects some players for participation
    in the experiment. Some of those players are guild members, and some are not.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a player named Jojo, who is not a guild member, who is logging on.
    Given this information only, he will have a 20% chance of engaging highly in side-quests
    during this session of gameplay, according to our model.
  prefs: []
  type: TYPE_NORMAL
- en: But the experimentation platform selects him for the experiment. It randomly
    assigns him to the high *Side-Quest Engagement* group. Once he is in that group,
    what is the probability that Jojo will engage highly in side-quests? The answer
    is 100%. In experimental terms, what is the probability that someone assigned
    to the treatment group will be exposed to the treatment? 100%. For data scientists
    familiar with the jargon of A/B testing, what is the probability that someone
    assigned to group A will be exposed to variant A? 100%.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, supposing instead of Jojo, the subject was Ngozi, who is a guild member.
    While originally Ngozi had an 80% chance of being highly engaged in side-quests,
    upon being assigned to the high *Side-Quest Engagement* group in the experiment,
    she changes to having a 100% chance of being highly engaged.
  prefs: []
  type: TYPE_NORMAL
- en: We need to rewrite our conditional probability distribution of *Side-Quest Engagement*
    to reflect these new probabilities, as in table 7.9.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.9 Rewriting the conditional probability table of *Side-Quest Engagement*
    to reflect the certainty of engagement level upon being assigned to the high-engagement
    group in the experiment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '|  | Guild Membership |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| nonmember | member |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Side-Quest Engagement  | low  | 0.0  | 0.0  |'
  prefs: []
  type: TYPE_TB
- en: '| high  | 1.0  | 1.0  |'
  prefs: []
  type: TYPE_TB
- en: Now we see that this modified distribution of *Side-Quest Engagement* is the
    same regardless of *Guild Membership*. That is the definition of probabilistic
    independence, so we should simplify this conditional probability table to reflect
    that; we can reduce table 7.9 to table 7.10.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.10 Rewriting the conditional probability table of *Side-Quest Engagement*
    to reflect the fact that engagement level no longer depends on *Guild Membership*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Side-Quest Engagement  | low  | 0.0  |'
  prefs: []
  type: TYPE_TB
- en: '| high  | 1.0  |'
  prefs: []
  type: TYPE_TB
- en: When we simplify the distribution in this way, we have to recall that this is
    a model of a causal Markov kernel, which is defined by the graph. Our initial
    graph says *Side-Quest Engagement* is caused by *Guild Membership*. But it seems
    that after the experiment randomly allocates players either to the high engagement
    or low engagement group, that causal dependency is broken; a player’s engagement
    level is solely determined by the group they are assigned to.
  prefs: []
  type: TYPE_NORMAL
- en: We need an intervention operator that changes our causal graph to reflect this
    broken causal dependency. This intervention operator is called *graph surgery*
    (also known as *graph mutlitation*), and it’s illustrated in figure 7.6\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F06_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 Graph surgery removes an incoming edge to the intervention target
    *Side-Quest Engagement*.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While *Guild Membership* is a cause of *Side-Quest Engagement* in normal settings,
    the experiment’s intervention on *Side-Quest Engagement* broke that variable’s
    causal dependence on *Guild Membership*. Since that causal dependence is gone,
    graph surgery changes the graph to one where the edge from *Guild Membership*
    to *Side-Quest Engagement* is snipped.
  prefs: []
  type: TYPE_NORMAL
- en: In general, suppose you have a causal graph with node *X*. You want to apply
    an intervention do(*X*=*x*). Then you represent that intervention on the causal
    DAG by “surgery” removing all incoming edges to *X*. Graph surgery is available
    in libraries such aspgmpy. For example, here is how we would use pgmpy to apply
    graph surgery to the online gaming DAG.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.11 Graph surgery on a DAG in pgmpy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Build the causal DAG.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The do method in the DAG class applies graph surgery.'
  prefs: []
  type: TYPE_NORMAL
- en: We can now plot both the original DAG and the transformed DAG and compare them.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.12 Plot the transformed DAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Create a dictionary of node positions that we can use to visualize both
    graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Visualize the original graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Visualize the transformed graph.'
  prefs: []
  type: TYPE_NORMAL
- en: These visualizations produce the same DAG as in figure 7.6\.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at the effect of graph surgery on d-separation and its implications
    for conditional independence.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.4 Graph surgery and d-separation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider how graph surgery affects reasoning with d-separation, as in figure
    7.7\. Initially, we have two d-connecting paths between *Side-Quest Engagement*
    and *In-Game Purchases*: one path was the direct cause path, and the other was
    through the common cause of *Guild Membership*. After graph surgery, only the
    direct causal path remains.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F07_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 In the original DAG on the left, there are two d-connected paths
    between *Side-Quest Engagement* and *In-Game Purchases*. These paths equate to
    two sources of statistical dependence between the two variables. After graph surgery,
    only the causal path remains, reflecting causal dependence.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recall that each d-connected path between two variables is a source of statistical
    dependence between those variables. When we represent an intervention with graph
    surgery that removes incoming edges to the intervention target(s), we remove any
    paths to other nodes that go through that variable’s causes. Only outgoing paths
    to other nodes remain. As a result, the remaining paths from that variable reflect
    dependence due to that variable’s causal influence on other variables. The ideal
    intervention removes the causal influence the target variable receives from its
    direct parents. Thus, it removes any dependence on other variables that flows
    through those parents.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.5 Ideal interventions and causal Markov kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph surgery on the causal DAG removes the incoming edges to the target node(s).
    However, for a causal graphical model, we need an intervention operator that changes
    the graph *and* goes one step farther to rewrite the causal Markov kernel of the
    intervention target, as we did when we collapsed table 7.9 into table 7.10.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, our online gaming model has causal Markov kernels {*P*(*G*), *P*(*E*|*G*),
    and *P*(*I*|*E*, *G*)}. In table 7.9 we saw the conditional probability table
    representation of *P*(*E*|*G*) and how an intervention reduced it to table 7.10,
    where 100% of the probability is placed on the outcome “high.”
  prefs: []
  type: TYPE_NORMAL
- en: Generally, the intervention operator for causal graphical models replaces the
    causal Markov kernel(s) of the intervention target(s) with a degenerate distribution,
    meaning a distribution that puts 100% of the probability on one value, namely
    the intervention value.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we combine graph surgery with this replacement of the target node’s causal
    Markov kernel with a degenerate distribution, we have an intervention operator
    on a causal graphical model that meets the three elements of the definition of
    ideal intervention:'
  prefs: []
  type: TYPE_NORMAL
- en: You only remove incoming edges for the nodes targeted by the intervention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 100% of the probability is assigned to a fixed value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing the incoming edges to the intervention target means that the variable
    is no longer causally dependent on its parents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In listing 7.11, graph surgery is implemented in the `do` method in the `DAG`
    class. The `BayesianNetwork` class, our default for building causal graphical
    models, also has a `do` method. Like the `DAG` method, it takes an intervention
    target. At the time of writing, the method does not take an intervention value
    and thus does not satisfy the second element of the definition of ideal intervention.
  prefs: []
  type: TYPE_NORMAL
- en: pgmpy uses objects from subclasses of the `BaseFactor` class (e.g., the `TabularCPD`
    class) to represent causal Markov kernels. The `do` method in the `BayesianNetwork`
    class first does graph surgery and then replaces the factor object representing
    the intervention target’s causal Markov kernel. However, that replacement factor
    object is not degenerate; it does not assign all the probability value to one
    outcome. Rather, it returns an object representing the probability distribution
    of the target variable after marginalizing over its parents in the original unmodified
    graph. Technically, this is an intervention operator for a stochastic intervention,
    which I’ll discuss in section 7.5\. To build an intervention operator for the
    ideal intervention, you need to write additional code to modify the factor to
    assign all probability to the intervention value.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.6 Ideal interventions in a causal program
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall that in listing 7.10 we simulated an experiment where players were assigned
    to high-engagement and low-engagement groups using the `pyro.do` operator. Specifically,
    we called `pyro.do` as in following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.13 Revisiting `pyro.do`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 An intervention that sets Side-Quest Engagement to 1.0 (i.e., “high”). This
    returns a new model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 An intervention that sets Side-Quest Engagement to 0.0 (i.e., “low”). This
    returns a new model.'
  prefs: []
  type: TYPE_NORMAL
- en: What exactly does `pyro.do` *do*? `pyro.do` is Pyro’s intervention operator.
    We saw, by using `pyro.render_model` to generate figure 7.4, that our online gaming
    model in Pyro has an underlying causal DAG, and therefore is a causal graphical
    model.
  prefs: []
  type: TYPE_NORMAL
- en: But a deep probabilistic machine learning framework like Pyro allows you to
    do things that we can’t easily represent with a causal DAG, such as recursion,
    conditional control flow, or having a random number of variables not realized
    until runtime. As an intervention operator, `pyro.do` must work in these cases
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intervention operator in Pyro works by finding calls to `pyro.sample`,
    and replacing those calls with an assignment to the intervention value. For example,
    the online game model had the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This `pyro.sample` call generates a value for *Side-Quest Engagement*. `pyro.do(model,
    {"Side-quest Engagement": tensor(1.)})` *essentially* replaces that line with
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: (I say “essentially” because `pyro.do` does a few other things too, which I’ll
    discuss in chapter 10).
  prefs: []
  type: TYPE_NORMAL
- en: This replacement is much like the replacement of the assignment function in
    the SCM, or a causal Markov kernel with a degenerate kernel in a causal graphical
    model. As an intervention operator, it meets the criteria for an ideal intervention.
    It targets a specific variable, and it assigns it a specific value. It eliminates
    its dependence on its causes by removing *flow dependence* (dependence on results
    of executing preceding statements in the program).
  prefs: []
  type: TYPE_NORMAL
- en: Using a flexible deep probabilistic machine learning tool like Pyro to build
    a causal model allows you to construct causal representations beyond DAGs and
    simple ideal interventions. Doing so puts you in underdeveloped territory in terms
    of theoretical grounding, but it could lead to interesting new applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll consider how interventions affect probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Intervention variables and distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An ideal intervention fixes the random variable it targets, essentially turning
    it into a constant. But the intervention indirectly affects all the random variables
    causally downstream of the target variable. As a result, their probability distributions
    (joint, conditional, or marginal) change from what they were.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 “Do” and counterfactual notation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Causal modeling uses special notation to help reason about how interventions
    affect random variables and their distributions. One common approach is to use
    the *“do”-notation*. Using our online game as an example, *P*(*I*) is the probability
    distribution of *In-Game Purchases* across all players, *P*(*I*|*E*=“high”) is
    the probability distribution of *In-Game Purchases* given players with “high”
    engagement, and *P*(*I*|do(*E*= “high”)) is the probability distribution of *In-Game
    Purchases* given an intervention that sets a player’s engagement level to “high.”
    The second column of table 7.11 illustrates extensions of this notation to joint
    distributions, multiple interventions, and mixing interventions with observations.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.11 Examples of do-notation and counterfactual notation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Literal | Do-notation | Counterfactual notation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| The probability distribution of *In-Game Purchases* across all players  |
    *P*(*I*)  | *P*(*I*)  |'
  prefs: []
  type: TYPE_TB
- en: '| The probability distribution of *In-Game Purchases* for players with “high”
    engagement  | *P*(*I*&#124;*E*=“high”)  | *P*(*I*&#124;*E*=“high”)  |'
  prefs: []
  type: TYPE_TB
- en: '| The probability distribution of *In-Game Purchases* when a player’s engagement
    level is set (by intervention) to “high”  | *P*(*I*&#124;do(*E*= “high”))  | *P*(*I**[E]*[=
    “high”])  |'
  prefs: []
  type: TYPE_TB
- en: '| The joint probability distribution of *In-Game Purchases* and *Guild Membership*
    when engagement is set to “high”  | *P*(*I*, *G*&#124;do(*E*=“high”))  | *P*(*I*[E=“high”],
    *G*[E=“high”])  |'
  prefs: []
  type: TYPE_TB
- en: '| The probability distribution of In-Game Purchases when engagement is set
    to “high” and membership is set to “nonmember”  | *P*(*I*&#124;do(*E*=“high”,
    *G*=“nonmember”))  | *P*(*I**[E]*[= “high”,] *[G]*[=“nonmember”])  |'
  prefs: []
  type: TYPE_TB
- en: '| The probability distribution of *In-Game Purchases* for guild members when
    engagement is set to “high”  | *P*(*I*&#124;do(*E*=“high”), *G*=“member”)  | *P*(*I**[E]*[=][“high”]&#124;*G*=“member”)  |'
  prefs: []
  type: TYPE_TB
- en: An alternative is to use counterfactual notation, which uses subscripts to represent
    a new version of a variable after the system has been exposed to intervention.
    For example, if *I* is a variable that represents *In-Game Purchases*, *I*[*E*][=“high”]
    represents *In-Game Purchases* under an intervention that sets *Side-Quest Engagement*
    to “high.” If *P*(*I*) is the probability distribution of *In-Game Purchases*,
    then *P*(*I*[*E*][=“high”]) is the probability distribution. Again, table 7.11
    contrasts do-notation with counterfactual notation in the third column. Going
    forward, I’ll mostly use counterfactual notation.
  prefs: []
  type: TYPE_NORMAL
- en: From causal language to symbols
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In many cases in statistics and machine learning, notation only serves to add
    formalism and rigor to something just as easily explained in plain language. However,
    notation is important in causality, because it makes a clear distinction between
    when we are talking about something causal and when we are not. It is important
    because making the distinction is harder in plain English. For example, consider
    the following two questions:'
  prefs: []
  type: TYPE_NORMAL
- en: “What would *In-Game Purchases* be for a player who was highly engaged in side-quests?”
  prefs: []
  type: TYPE_NORMAL
- en: “What would *In-Game Purchases* be if a player was highly engaged in side-quests?”
  prefs: []
  type: TYPE_NORMAL
- en: Is it obvious to you that the first question corresponds to *P*(*I*|*E*=“high”)
    and the second to *P*(*I*[*E*][=“high”])? The first question corresponds to a
    subset of players who are highly engaged. The traditional conditional probability
    notation is fine when we want to zoom in on a subset of a distribution or population.
    The second question asks *what if* someone were highly engaged. In the next chapter,
    we’ll see that “what if” hypothetical questions imply an intervention. But because
    of the ambiguity of language, someone could ask one question while really meaning
    the other. The notation gives us an unambiguous way of constructing our causal
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: Again, in chapter 8, we’ll investigate more examples of mapping language to
    counterfactual notation.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 When causal notation reduces to traditional notation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is crucial to recognize when a variable and that same variable under intervention
    are the same. Consider the intervention on engagement, as in figure 7.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F08_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 In the original DAG (left), there are two d-connected paths between
    *Side-Quest Engagement* and *In-Game Purchases*. These paths equate to two sources
    of statistical dependence between the two variables. After graph surgery, only
    the causal path remains, reflecting causal dependence.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Is *P*(*G*|*E*=“high”) (the probability distribution of *Guild Membership* given
    high *Side-Quest Engagement*) the same as *P*(*G*)? No. In graphical terms, *G*
    and *E* are d-connected. In probabilistic terms, we can reason that knowing a
    player’s level of *Side-Quest Engagement* is predictive of whether they are in
    a guild.
  prefs: []
  type: TYPE_NORMAL
- en: But is *P*(*G*[*E*][=“high”]) the same as *P*(*G*)? Yes. *Guild Membership*
    is not affected by the intervention on *Side-Quest Engagement* because it can
    only affect variables causally downstream of *Side-Quest Engagement*. Thus *P*(*G*[*E*][=“high”])
    is equivalent to *P*(*G*).
  prefs: []
  type: TYPE_NORMAL
- en: In general terms, empirically learning a distribution for a variable *Y*[*X*][=][*x*]
    requires doing the intervention do(*X*=*x*) in real life. However, that real-life
    intervention, at best, has a cost and, at worst, is infeasible or impossible.
    So if we can equate *Y*[*X*][=][*x*] to some distribution involving *Y* that we
    can learn from observational data, that’s a win. That’s going from correlation
    to causation. In trivial cases, we can do this by looking at the graph, as we
    did with *G* and *G*[*E*][=“high”]. But usually, we’ll need to do some mathematical
    derivation, either by hand or using algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: This task of deriving equality between variables that are and aren’t subject
    to intervention is called *identification*, and it is the heart of causal inference
    theory. We’ll examine identification at length in chapter 10.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.3 Causal models represent all intervention distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As generative models, the causal models we’ve worked with encode a joint probability
    distribution of components of the DGP. Inference algorithms enable those models
    to represent (e.g., through Monte Carlo sampling) the conditional distribution
    of some subset of those components, given the state of the other components.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now introduced the ideal intervention and how it changes the DGP and,
    consequently, the joint probability distribution of the variables. Figure 7.9
    illustrates how the generative causal model captures the original DGP (and corresponding
    probability distributions) and any new DGP (and corresponding probability distributions)
    created by intervening in the original process.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F09_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 Suppose our DGP has variables *X*, *Y*, and *Z*. A traditional generative
    model (left) uses observations of *X*, *Y*, and *Z* to statistically learn a representation
    of *P*(*X*, *Y*, *Z*). A generative causal model (right) encodes a representation
    *P*(*X*, *Y*, *Z*) and distributions derived by interventions on *X*, *Y*, and
    *Z*. In that way, the generative causal model represents a broad family of distributions.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Consider the statistical implications of this idea. Given data, an ordinary
    generative model learns a representation of the joint probability distribution.
    But a generative causal model learns not only that distribution but any new distribution
    that would be derived by applying some set of ideal interventions. That’s how
    our causal model of the online game was able to reproduce the outcome of an experiment
    from observational data alone.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Interventions and causal effects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common use case for our formal model of an intervention will be to
    model *causal effects*. Now that we’ve defined and formalized interventions, causal
    effects are easy to think about; they are simply comparisons between the outcomes
    of interventions.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.1 Average treatment effects with binary causes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most common causal effect query is the average treatment effect (ATE).
    Here, we’ll focus on the case where we are interested in the causal effect of
    *X* on *Y*, and *X* is binary, meaning it has two outcomes: 1 and 0\. Binary causes
    entail experiments where the cause has a “treatment” value and a “control” value,
    such as “A/B tests.” Using do-notation, the ATE is *E*(*Y*|do(*X*=1)) – *E*(*Y*|do(*X*=0))
    (recall *E*(…) means “the expectation of …”). Using counterfactual notation, the
    ATE is *E*(*Y*[*X*][=][1]) – *E*(*Y*[*X*][=][0]). The advantage of the counterfactual
    notation is that we can collapse this into one expectation term, *E*(*Y*[*X*][=1]
    – *Y*[*X*][=0]).'
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.2 Average treatment effect with categorical causes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the cause is categorical, the ATE requires choosing which levels of the
    cause you want to compare. For example, if *X* has possible outcomes {a, b, c},
    you might select “a” to be a baseline, and work with two ATEs, *E*(*Y*[*X*][=][*b*]
    – *Y*[*X*][=][*a*]) and *E*(*Y*[*X*][=][*c*] – *Y*[*X*][=][*a*]). Alternatively,
    you may choose to work with all pairwise comparisons of levels of *X*, or just
    convert *X* to a binary variable with outcomes “a” and “not a” The choice depends
    on which ATE is most meaningful to you.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.3 Average treatment effect for continuous causes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we want to generalize *E*(*Y*[*X*][=][1] – *Y*[*X*][=][0]) to the case where
    *X* is continuous, we arrive at derivative calculus. For some baseline do(*X*=*x*),
    imagine changing the intervention value *x* by some small amount Δ, i.e., do(*X*=*x*+Δ).
    Taking the difference between the two outcomes, we get *E*(*Y*[*X*][=][*x*][+][Δ]
    – *Y*[*X*][=][*x*]). Then we can ask, what is the rate of change of *E*(*Y*[*X*])
    as we make Δ infinitesimally smaller. This brings us to the definition of the
    derivative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch7-eqs-2x.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that this is a function, rather than a point value; when you plug in a
    value of *x*, you get the rate of change of *Y*[*X*][=][*x*] of the *X* versus
    *Y*[*x*] curve.
  prefs: []
  type: TYPE_NORMAL
- en: As a practical example, consider the case of pharmacology, where we want to
    establish the ATE of a drug dose on a health outcome. The drug dose is continuous,
    and it usually follows a nonlinear S-curve-like shape; we get more effect as we
    increase the dose, but eventually the effect gets diminishing returns at higher
    doses. The derivative gives us the rate of change of the average response for
    a given dose on the dose-response curve.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.4 Conditional average treatment effect
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The conditional average treatment effect (CATE) is an ATE conditioned on other
    covariates. For example, in our online game example, *E*(*I*[*E*][=“high”] – *I*[*E*][=“low”])
    is the ATE on *In-Game Purchases* for *Side-Quest Engagement*. If we wished to
    understand the ATE for guild members, we’d want *E*(*I*[*E*][=“high”] – *I*[*E*][=“low”]|
    *G*=“member”).
  prefs: []
  type: TYPE_NORMAL
- en: In practical settings, it is often important to work with CATEs instead of ATEs,
    because CATEs can have big differences with ATEs and other CATEs with different
    conditions. In other words, CATEs better reflect the *heterogeneity* of treatment
    effects across a population. For example, it is possible that the ATE of a drug
    on a health outcome is positive across the overall population, but the CATE conditioned
    on a specific subpopulation (e.g., people with a certain allergy) could be negative.
    Similarly, in advertising, certain ad copy might drive your customers to purchase
    more on average, but cause some segment of your customers to purchase less. You
    can optimize the return on investment for your ad campaign by understanding the
    CATEs for each segment, or to use CATE-based reasoning to do customer segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Experts often emphasize the importance of measuring *heterogenous treatment
    effects* with CATEs, lest one think a point value estimate of an ATE tells the
    full picture. But in our probabilistic modeling approach, heterogeneity is front
    and center. If we have a causal graphical model and a model of ideal intervention,
    then we can model *P*(*Y*[*X*][=][*x*]). If we can model *P*(*Y*[*X*][=][*x*]),
    then we can model *P*(*Y*[*X*][=][1] – *Y*[*X*][=][0]). We can then use that model
    to inspect all the variation within *P*(*Y*[*X*][=][1] – *Y*[*X*][=][0]), including
    who in the target population falls above or below 0 or some other threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.5 Statistical measures of association and causality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In statistics, an effect size is a value that measures the strength or intensity
    of the relationship between two variables or groups. For example, in our observational
    analysis of the online gaming data, we quantified the relationship between *Side-Quest
    Engagement* *E* and *In-Game Purchases* *I* as *E*(*I*|*E*=“high”) – *E*(*I*|*E*=“low”).
    Our statistical procedure estimated this *true* effect size with a difference
    in sample averages between both groups. We then conducted a hypothesis test. We
    specified a null hypothesis *E*(*I*|*E*=“high”) – *E*(*I*|*E*=“low”) = 0, and
    then tested if this effect size estimate was statistically different from 0 using
    a *p*-value calculated under some null hypothesis distribution (usually a normal
    or t-distribution).
  prefs: []
  type: TYPE_NORMAL
- en: A causal effect is just an *interventional* effect size; in our example, it
    was *E*(*I*|do(*E*=“high”)) – *E*(*I*|do(*E*=“low”)) = *E*(*I*[*E*][=][“high”]
    – *I*[*E*][=][“low”]), which is the ATE. The statistical hypothesis testing procedure
    is the same as before. Indeed, we still need to test if sample-based estimates
    of ATEs and CATEs are statistically significant. When you conduct a statistical
    significance test with data from an experiment with a treatment and control, you
    are testing an estimate of the ATE by definition.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.6 Causality and regression models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose *X* is continuous, but its relationship with *Y*[*X*] is linear. Then
    the ATE *d* *E*(*Y*[*X*][=][*x*])/*dx* is a point value because the derivative
    of a linear function is a constant. Therefore, if you use a linear model of *E*(*Y*[*X*]),
    then the coefficient for *X* in that model corresponds to the ATE for *X*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch7-eqs-3x.png)'
  prefs: []
  type: TYPE_IMG
- en: For this reason, linear regression modeling is a popular approach to modeling
    causal effects (even when people don’t really believe the causal relationship
    is linear).
  prefs: []
  type: TYPE_NORMAL
- en: 'This convenience extends to other generalized linear models. Suppose Poisson
    regression or logistic regression are better models of *E*(*Y*[*X*]) than linear
    regression. These models capture measures of association between two variables
    not as a difference in means, but as ratios. For example, we can read relative
    risk (RR) directly from a Poisson regression model and odds ratios (OR) directly
    from a logistic regression model. In general, these measures of association have
    no causal interpretation, but we give them a causal interpretation once we use
    them with interventional variables. For example, if we are modeling *E*(*Y*[*X*]),
    and *Y*[*X*] is binary, the relative risk and odds ratios are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch7-eqs-4x.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, traditional non-causal ways of quantifying statistical association become
    measures of *causal* association once we use them in an interventional context.
    And when we fit these regression models to data, we can still use all the traditional
    regression methods for significance testing (Wald tests, F-tests, likelihood ratio
    tests, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Stochastic interventions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stochastic interventions are an important generalization of ideal interventions.
    The second rule of the ideal intervention is that the intervention is set to a
    fixed value. In the stochastic intervention, that value is the outcome of a random
    process; i.e., it is itself a random variable. Most texts treat stochastic interventions
    as an advanced topic beyond the scope of an introduction to causal modeling, but
    I make special mention of them as they are important in machine learning, where
    we often seek data-driven automation. Stochastic interventions are important for
    automatic selection of interventions.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.1 Random assignment in an experiment is a stochastic intervention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For example, the digital experimentation platform in our online gaming experiment
    automatically assigned players to high- and low-engagement groups. It did so *randomly*.
    Random assignment is a stochastic intervention; it targets the *Side-Quest Engagement*
    variable and sets its value by digitally flipping a coin.
  prefs: []
  type: TYPE_NORMAL
- en: Note that randomization is more than what we need to arrive at the right answer.
    Indeed, in our simulation of the experiment, there was no randomization, only
    ideal interventions. Those ideal interventions were sufficient to d-separate the
    path*Side-Quest Engagement* ← *Guild Membership* → *In-Game Purchases*, removing
    the statistical dependence that comes from that path. If randomization is not
    necessary to quantify the causal relationship, why is it called “the gold standard
    of causal inference?” The answer is that randomization works when your causal
    DAG is *wrong*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose that when we did the experiment, rather than randomizing
    players into the high versus low *Side-Quest Engagement* group, the digital experimentation
    platform automatically assigned the first 500 players who logged on to the group
    with high *Side-Quest Engagement* and the next 500 players to the group with low
    *Side-Quest Engagement*. This intervention would be sufficient to d-separate the
    path*Side-Quest Engagement* ← *Guild Membership* → *In-Game Purchases*. But what
    if our DAG was wrong, and there are other paths between *Side-Quest Engagement*
    and *In-Game Purchases* through unknown common causes?
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.10 considers what happens when our DAG is wrong—our model is the DAG
    on the right. Consider what would happen if, instead, the true DAG were the DAG
    on the left. For the DAG on the left, the time of day when the player logs on
    drives both the *Side-Quest Engagement* and *In-Game Purchases*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F10_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10 Left: the true causal relationships. Right: your (incorrect) causal
    DAG.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Suppose, for example, people who log on earlier tend not to be logging on with
    friends. They tend to engage more in side-quests because side-quests are amenable
    to solo gameplay. People who plan missions with friends tend to log on later,
    since some friends have real-world appointments during the day. Friends playing
    together focus more on the game’s primary narrative and avoid side-quests. Also,
    players tend to spend more money on *In-Game Purchases* later in the day, corresponding
    to the broader trend of late-day spending in e-commerce.
  prefs: []
  type: TYPE_NORMAL
- en: When we intervene on a player to assign them to one group or another based on
    their login, that intervention value now depends on the time of day, as shown
    in figure 7.11.
  prefs: []
  type: TYPE_NORMAL
- en: 'The left side of figure 7.11 illustrates the result of an intervention on *Side-Quest
    Engagement* that depends on the time of day. As we expected, the intervention
    performs graph surgery, removing the incoming edges to *Side-Quest Engagement*
    *E*: *T*→*E* and *G*→*E*. However, the value set by the intervention is now determined
    by time of day *T*, via a `time_select` function. The `time_select` function assigns
    “high” engagement to every player whose login time is before that of the 501^(st)
    player to log on and “low” for those who logged in after. After graph surgery,
    we add back a new causal edge *T*→*E* whose mechanism is `time_select`. Thus,
    there is still a noncausal statistical association that biases the experiment
    via the d-connected path *I*←*T*→*E*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F11_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 An intervention that sets the level of *Side-Quest Engagement* based
    on login time changes, but doesn’t eliminate, the causal relationship T→*E*. In
    contrast, randomization eliminates incoming edges to *E*.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In contrast, randomization on the right side of figure 7.11 did what we hoped,
    removing all the incoming edges to *E*. It removed the edge from *T*→*E* even
    though our assumed DAG did not know that *T*→*E* existed. Indeed, if there are
    other unknown common causes between *E* and *I*, randomization will remove those
    incoming edges to *E*, as in figure 7.12.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F12_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 Randomization eliminates incoming edges from unknown common causes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The ability of randomization to eliminate statistical bias from common causes
    we failed to account for in our assumptions is why it is considered “the gold
    standard of causal inference.” But to understand stochastic interventions, note
    that both assignment mechanisms: one based on login time and the other using randomization,
    are stochastic interventions. Both set the *Side-Quest Engagement* level of a
    player using a random process; one depends on when someone logs in, and the other
    depends on a coin flip.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.2 Intervention policies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stochastic interventions are closely related to policies in automated decision-making
    domains, such as bandit algorithms and reinforcement learning. In these domains,
    an agent (e.g., a robot, a recommender algorithm) operates in some environment.
    A *policy* is an algorithm that takes as input the state of some variables in
    the environment and returns an action for the agent to execute. If there are elements
    of randomness in the selection of that action, it is a stochastic intervention.
  prefs: []
  type: TYPE_NORMAL
- en: In our previous example, randomization is a *policy* that selects interventions
    at random. But in automated decision-making, most policies choose interventions
    based on the state of other variables in the system, much like the biased experiment
    that intervenes based on the *time of day* variable. Of course, policies in automated
    decision-making are typically trying to optimize some utility function rather
    than bias an experiment. We’ll focus on causality in automated decision-making
    in chapter 12.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Practical considerations in modeling interventions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I’ll close this chapter with some practical considerations for modeling interventions.
    We’ll consider how ideal (and stochastic) interventions allow us to model the
    impossible. Then we’ll make sure we ground that modeling in pragmatism.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6.1 Reasoning about interventions that we can’t do in reality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our online gaming example, we used an intervention operator on a causal model
    to replicate the results of an experiment. I presented a choice between actually
    running an experiment and simulating the experiment. Simulation avoids the costs
    of running the experiment, but running the experiment is more robust to errors
    in causal assumptions, especially with tools like randomization.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are many times when we can’t run an experiment, because doing
    so is either infeasible, unethical, or impossible.
  prefs: []
  type: TYPE_NORMAL
- en: '*Example of an infeasible experiment*—A randomized experiment that tests the
    effect of interest rates on intergenerational wealth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Example of an unethical experiment*—A randomized experiment that tests the
    effect of caffeine on miscarriages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Example of an impossible experiment*—A randomized experiment that tests the
    effects of black hole size on spectroscopic redshift.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In these scenarios, simulation with a causal model is our only choice.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6.2 Refutation and real-world interventions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose your causal model predicts the outcome of an intervention. You then
    do that intervention in the real world, such as with a controlled experiment.
    If your predicted intervention outcome conflicts with your actual intervention
    outcome, your causal model is wrong.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 4, we discussed the concept of validating, or rather, *refuting*,
    a causal model by checking data for evidence of dependence that violates the conditional
    independence implications of the model’s DAG. In chapter 11, we’ll extend refutation
    from the causal DAG all the way to a causal inference of interest (e.g., estimating
    a causal effect). However, comparing predicted and actual intervention outcomes
    gives us a stronger refutation standard than the methods in chapters 4 and 11\.
    The catch, of course, is that doing these real-world interventions must be feasible.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming they are, comparing predicted and real-world intervention outcomes
    provides a nice iterative framework for building a causal model. First, enumerate
    a set of interventions you can apply in the real world. Select one of those interventions,
    use your model to predict its outcome, and then do the intervention in the real
    world. If the outcomes don’t match, update your model so that it does. Repeat
    until you have exhausted your ability to run real-world interventions.
  prefs: []
  type: TYPE_NORMAL
- en: Doing a real-world intervention usually costs resources and time. To save on
    costs, use your causal model to predict all the interventions you can run, rank
    the predicted outcomes according to which are more interesting or surprising,
    and then prioritize running real-world interventions according to this ranking.
    Interesting or surprising intervention predictions are likely a sign your model
    is wrong, so prioritizing them means you’ll make big updates to your model sooner
    and at less cost. And if your model turns out to be right, you will have spent
    less to arrive at some important insights into your DGP.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6.3 “No causation without manipulation”
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea behind “no causation without manipulation” is that one should define
    the variables in the causal model such that the mechanics of *how* one might intervene
    in it is clear. Clarity here means you could run a real-world experiment that
    implemented the intervention, or, if the experiment were infeasible, unethical,
    or impossible, you could at least clearly articulate how the hypothetical experiment
    would work. “No causation without manipulation” is essentially trying to tether
    a causal model’s abstractions to experimental semantics.
  prefs: []
  type: TYPE_NORMAL
- en: For example, proponents of this idea might object to having “race” as a cause
    in a causal model, because the concept of race is nebulous from the standpoint
    of an intervention applied in an experiment—how would you change somebody’s race
    while holding constant everything about that person not caused by their race?
    They would prefer defining the variable in terms precise enough to be theoretically
    intervenable, such as “racial bias of loan officer” or “racial indicators on application
    form.” Of course, we have important questions to ask about fuzzy abstractions
    like “race,” so we don’t want to add so much precision that we can’t generalize
    the results of our analyses in ways that help answer those questions.
  prefs: []
  type: TYPE_NORMAL
- en: One strategy for establishing this tether to experimentation is to include variables
    in our model that we can manipulate in a hypothetical experiment. For example,
    if we are interested in the causal relationship wealth → anxiety, we could add
    a “cash subsidy” variable and cash subsidy → wealth edge. Cash subsidy represents
    direct payments to an individual, which is easier to do in an experiment than
    directly manipulating an individual’s wealth.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6.4 Modeling “non-ideal” interventions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often the types of interventions we use in practical settings can be challenging
    to map to ideal interventions. For example, a biologist might be studying the
    causal relationships between the expression of different genes in a cell, with
    causal relationships like gene A → gene B → gene C. The biologist might want to
    know how a stressor in the cellular environment (e.g., a toxin or hypoxia) affects
    gene expression. The stressor is an intervention; it changes the DGP. However,
    modeling it as an ideal intervention is challenging because it will likely be
    unclear which genes those stressors affect directly or what specific amount of
    gene expression is set by the stressor. A practical solution for these interventions
    is to model them explicitly as root nodes in the causal DAG, such as the hypoxia
    node in figure 7.13.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F13_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 “Hypoxia” is an intervention that has no specific target. Include
    it as a root node with edges to all variables that are possibly affected.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Explicit representation of interventions as part of the DGP is less expressive
    than the ideal (or stochastic) intervention, which captures how an arbitrary intervention
    can *change* the DGP.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An intervention is an action that changes the data generating process (DGP).
    Interventions are fundamental to defining causality and causal models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many, if not most, machine learning–driven decisions are interventions that
    can render the model’s deployment environment different from its training environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to model an intervention allows one to simulate the outcome of experiments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulating experiments with an intervention model can save costs or enable simulated
    experiments when running an actual experiment is infeasible, unethical, or impossible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ideal intervention targets specific variables, fixes them to a specific value,
    and renders the target independent of its causal parents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal effects are simple extensions of intervention distributions. For example,
    the average treatment effect (ATE) of *X* on *Y* is *E*(*Y*[*X*][=1]) – *E*(*Y*[*X*][=0]),
    the difference in means between two intervention distributions for *Y*. Conditional
    average treatment effects (CATEs) are simply differences in conditional expectations
    for intervention distributions on *Y*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochastic interventions are like ideal interventions, but they fix the intervention
    targets at a value determined by some random process. That value could depend
    on the states of other variables in the system. In this way, they are related
    to policies in automated decision-making domains such as bandit algorithms and
    reinforcement learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An intervention operator describes how a causal model is altered to reflect
    an ideal (or stochastic) intervention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intervention operator for a structural causal model replaces the target
    variables assignment function with the intervention value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph surgery is the intervention operator for causal DAGs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intervention operator for causal graphical models applies graph surgery
    and replaces the causal Markov kernel for the target with a degenerate distribution
    that places all probability on the intervention value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal models can use observational data to statistically learn the observational
    distribution and any interventional distribution that can be derived through the
    intervention operator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomization is a stochastic intervention that eliminates causal influence
    on the intervention target from unknown causes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “No causation without manipulation” suggests defining your causal model so that
    interventions are tethered to hypothetical experiments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can model interventions that don’t meet the ideal intervention standard
    as root nodes with outgoing edges to variables they may affect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) D.T. Campbell and T.D. Cook, *Quasi-experimentation:
    Design & Analysis Issues for Field Settings* (Rand McNally, 1979), p36.*'
  prefs: []
  type: TYPE_NORMAL
