# 第12章。世界模型

本章介绍了近年来生成模型最有趣的应用之一，即它们在所谓的世界模型中的使用。

# 介绍

2018年3月，David Ha和Jürgen Schmidhuber发表了他们的“World Models”论文。该论文展示了如何通过在自己生成的梦境环境中进行实验来训练一个模型，而不是在真实环境中。这是一个很好的例子，说明了当与强化学习等其他机器学习技术一起应用时，生成建模如何解决实际问题。

架构的一个关键组件是一个生成模型，它可以构建给定当前状态和动作的下一个可能状态的概率分布。通过随机移动建立对环境基础物理的理解后，模型能够在自己对环境的内部表示中完全自行训练新任务。这种方法导致了在测试时两个任务的世界最佳得分。

在本章中，我们将详细探讨论文中的模型，特别关注一个需要代理学习如何尽可能快地在虚拟赛道上驾驶汽车的任务。虽然我们将使用2D计算机模拟作为我们的环境，但相同的技术也可以应用于在现实环境中测试策略昂贵或不可行的情况。

###### 提示

在本章中，我们将引用“World Models”论文的优秀TensorFlow实现，该实现公开在GitHub上，我鼓励您克隆并运行！

在我们开始探索模型之前，我们需要更仔细地了解强化学习的概念。

# 强化学习

强化学习可以定义如下：

> 强化学习（RL）是一种机器学习领域，旨在训练一个代理在给定环境中以达到特定目标的最佳表现。

虽然判别建模和生成建模都旨在在观察数据集上最小化损失函数，但强化学习旨在最大化给定环境中代理的长期奖励。它通常被描述为机器学习的三大分支之一，与*监督学习*（使用标记数据进行预测）和*无监督学习*（从未标记数据中学习结构）并列。

让我们首先介绍一些与强化学习相关的关键术语：

环境

代理操作的世界。它定义了规则集，这些规则管理游戏状态更新过程和奖励分配，考虑到代理的先前动作和当前游戏状态。例如，如果我们正在教一个强化学习算法下棋，环境将包括规定给定动作（例如，兵的移动`e2e4`）如何影响下一个游戏状态（棋盘上棋子的新位置）的规则，并且还会指定如何评估给定位置是否为将军，并在获胜移动后为获胜玩家分配奖励1。

代理

在环境中采取行动的实体。

游戏状态

代理可能会遇到的特定情况的数据（也称为*状态*）。例如，具有伴随游戏信息的特定棋盘配置，例如哪个玩家将进行下一步移动。

行动

代理可以采取的可行移动。

奖励

环境在采取行动后向代理返回的值。代理的目标是最大化其奖励的长期总和。例如，在国际象棋游戏中，将对手的国王将军的奖励为1，而其他每一步的奖励为0。其他游戏在整个episode中不断授予奖励（例如，在*Space Invaders*游戏中的得分）。

Episode

环境中代理的一次运行；这也被称为*rollout*。

时间步

对于离散事件环境，所有状态、动作和奖励都被标注以显示它们在时间步<math alttext="t"><mi>t</mi></math>的值。

这些概念之间的关系在[图12-1](#rl)中显示。

![](Images/gdl2_1201.png)

###### 图12-1\. 强化学习图表

环境首先使用当前游戏状态<math alttext="s 0"><msub><mi>s</mi> <mn>0</mn></msub></math>进行初始化。在时间步<math alttext="t"><mi>t</mi></math>，代理接收当前游戏状态<math alttext="s Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math>并使用它来决定下一个最佳动作<math alttext="a Subscript t"><msub><mi>a</mi> <mi>t</mi></msub></math>，然后执行。给定这个动作，环境然后计算下一个状态<math alttext="s Subscript t plus 1"><msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>和奖励<math alttext="r Subscript t plus 1"><msub><mi>r</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>，并将它们传递回代理，以便循环再次开始。这个循环会持续直到episode的结束条件满足（例如，经过给定数量的时间步或代理赢得/输掉）。

我们如何设计一个代理来最大化在给定环境中的奖励总和？我们可以构建一个包含一组规则的代理，用于如何响应任何给定的游戏状态。然而，随着环境变得更加复杂，这很快变得不可行，并且永远不允许我们构建一个在特定任务中具有超人能力的代理，因为我们正在硬编码规则。强化学习涉及创建一个代理，通过反复游戏在复杂环境中学习最佳策略。

现在让我们来看一下模拟汽车在赛道上行驶的`CarRacing`环境。

## 赛车环境

`CarRacing`是通过[Gymnasium](https://gymnasium.farama.org)包提供的环境。Gymnasium是一个用于开发强化学习算法的Python库，其中包含几个经典的强化学习环境，如`CartPole`和`Pong`，以及提出更复杂挑战的环境，比如训练代理在不平坦地形上行走或赢得Atari游戏。

# Gymnasium

Gymnasium是OpenAI的Gym库的维护分支——自2021年以来，Gym的进一步开发已转移到Gymnasium。因此，在本书中，我们将Gymnasium环境称为Gym环境。

所有环境都提供了一个*step*方法，通过该方法您可以提交一个给定的动作；环境将返回下一个状态和奖励。通过反复调用代理选择的动作来调用step方法，您可以在环境中玩出一个episode。还有一个*reset*方法，用于将环境恢复到初始状态，以及一个*render*方法，允许您观看您的代理在给定环境中执行。这对于调试和找到代理可以改进的地方非常有用。

让我们看看`CarRacing`环境中游戏状态、动作、奖励和episode是如何定义的：

游戏状态

一个64×64像素的RGB图像，描绘了赛道和汽车的俯视图。

动作

一组三个值：方向盘方向（-1到1）、加速度（0到1）和刹车（0到1）。代理必须在每个时间步设置这三个值。

奖励

每个时间步骤都会受到-0.1的负惩罚，如果访问了新的赛道瓷砖，则会获得1000/<math alttext="upper N"><mi>N</mi></math>的正奖励，其中<math alttext="upper N"><mi>N</mi></math>是构成赛道的瓷砖总数。

剧集

当汽车完成赛道或驶出环境边缘，或者经过了3000个时间步骤时，剧集结束。

这些概念在[图12-2](#car_racing_example)中的游戏状态的图形表示中显示。

![](Images/gdl2_1202.png)

###### 图12-2\. `CarRacing`环境中一个游戏状态的图形表示

# 视角

我们应该想象代理人漂浮在赛道上方，从鸟瞰视角控制汽车，而不是从驾驶员的视角看赛道。

# 世界模型概述

我们现在将对整个世界模型架构和训练过程进行高层概述，然后深入研究每个组件。

## 架构

解决方案由三个不同部分组成，如[图12-3](#world_models_diagram)所示，它们分别进行训练：

V

变分自动编码器（VAE）

M

带有混合密度网络（MDN-RNN）的递归神经网络

C

一个控制器

![](Images/gdl2_1203.png)

###### 图12-3\. 世界模型架构图

### VAE

当您驾驶时做出决策时，您并不会主动分析视野中的每个像素—而是将视觉信息压缩成较少数量的潜在实体，例如道路的直线程度、即将到来的弯道以及您相对于道路的位置，以指导您的下一个动作。

我们在[第3章](ch03.xhtml#chapter_vae)中看到，VAE可以将高维输入图像压缩成一个潜在随机变量，该变量近似遵循标准高斯分布，通过最小化重构误差和KL散度。这确保了潜在空间是连续的，我们能够轻松从中进行采样以生成有意义的新观察。

在汽车赛道示例中，VAE将64×64×3（RGB）输入图像压缩成一个32维正态分布的随机变量，由两个变量`mu`和`logvar`参数化。这里，`logvar`是分布方差的对数。我们可以从该分布中采样以产生代表当前状态的潜在向量`z`。这将传递给网络的下一个部分，MDN-RNN。

### MDN-RNN

当您驾驶时，每个后续观察对您来说并不是完全意外的。如果当前观察表明前方道路左转，您向左转动方向盘，您期望下一个观察显示您仍然与道路保持一致。

如果您没有这种能力，您的汽车可能会在道路上蛇行，因为您无法看到稍微偏离中心会在下一个时间步骤中变得更糟，除非您现在采取措施。

这种前瞻性是MDN-RNN的任务，它试图根据先前的潜在状态和先前的动作来预测下一个潜在状态的分布。

具体来说，MDN-RNN是一个具有256个隐藏单元的LSTM层，后面跟着一个混合密度网络（MDN）输出层，允许下一个潜在状态实际上可以从几个正态分布中的任何一个中抽取。

“世界模型”论文的一位作者David Ha也将相同的技术应用于[手写生成](https://oreil.ly/WmPGp)任务，如[图12-4](#handwriting_MDN)所示，描述了下一个笔尖可能落在几个不同红色区域中的事实。

![](Images/gdl2_1204.png)

###### 图12-4\. 用于手写生成的MDN

在汽车赛道示例中，我们允许下一个观察到的潜在状态的每个元素都可以从五个正态分布中的任何一个中抽取。

### 控制器

到目前为止，我们还没有提到选择动作的事情。这个责任在于控制器。控制器是一个密集连接的神经网络，其中输入是`z`（从VAE编码的分布中采样的当前潜在状态）和RNN的隐藏状态的串联。三个输出神经元对应于三个动作（转向、加速、刹车），并且被缩放以落入适当的范围内。

控制器使用强化学习进行训练，因为没有训练数据集会告诉我们某个动作是*好*还是*坏*。相反，代理通过反复实验自己发现这一点。

正如我们将在本章后面看到的那样，“世界模型”论文的关键在于它展示了如何在代理的环境生成模型中进行强化学习，而不是在Gym环境中。换句话说，它发生在代理对环境行为的*幻想*版本中，而不是真实的环境中。

为了理解三个组件的不同角色以及它们如何共同工作，我们可以想象它们之间的对话：

> *VAE*（查看最新的64×64×3观察）：这看起来像一条笔直的道路，接近一个轻微的左弯，汽车面向道路的方向（`z`）。
> 
> *RNN*：基于那个描述（`z`）和控制器选择在上一个时间步加速的事实（`action`），我将更新我的隐藏状态（`h`），以便下一个观察被预测为仍然是一条笔直的道路，但在视野中有稍微更多的左转。
> 
> *控制器*：基于来自VAE的描述（`z`）和来自RNN的当前隐藏状态（`h`），我的神经网络输出`[0.34, 0.8, 0]`作为下一个动作。

然后，控制器的动作传递给环境，环境返回更新后的观察结果，循环再次开始。

## 训练

训练过程包括五个步骤，按顺序运行，概述如下：

1.  收集随机回滚数据。在这里，代理不关心给定任务，而是简单地使用随机动作探索环境。多个剧集被模拟，每个时间步的观察状态、动作和奖励被存储。这个想法是建立一个关于环境物理工作方式的数据集，然后VAE可以从中学习以有效地捕捉状态作为潜在向量。MDN-RNN随后可以学习潜在向量随时间的演变方式。

1.  训练VAE。使用随机收集的数据，我们在观察图像上训练VAE。

1.  收集数据以训练MDN-RNN。一旦我们有了训练好的VAE，我们使用它将每个收集到的观察编码为`mu`和`logvar`向量，并将其保存在当前动作和奖励旁边。

1.  训练MDN-RNN。我们获取一批批次的剧集，并在每个时间步加载在步骤3生成的`mu`、`logvar`、`action`和`reward`变量。然后我们从`mu`和`logvar`向量中采样一个`z`向量。给定当前的`z`向量、`action`和`reward`，然后训练MDN-RNN来预测随后的`z`向量和`reward`。

1.  训练控制器。通过训练好的VAE和RNN，我们现在可以训练控制器，以输出一个动作，给定当前的`z`和RNN的隐藏状态`h`。控制器使用进化算法CMA-ES作为其优化器。该算法奖励生成导致任务整体得分较高的动作的矩阵权重，以便未来的代际也可能继承这种期望的行为。

让我们现在更详细地看看每个步骤。

# 收集随机回滚数据

第一步是从环境中收集rollout数据，使用一个代理器执行随机动作。这可能看起来很奇怪，因为我们最终希望我们的代理器学会如何采取智能动作，但这一步将提供代理器将用于学习世界运作方式以及其动作（尽管起初是随机的）如何影响随后观察的数据。

我们可以通过启动多个Python进程并行捕获多个episode，每个进程运行环境的单独实例。每个进程将在单独的核心上运行，因此如果您的计算机有很多核心，您可以比只有几个核心时更快地收集数据。

这一步使用的超参数如下：

`parallel_processes`

要运行的并行进程数（例如，如果您的计算机有≥8个核心，则为`8`）

`max_trials`

每个进程应总共运行多少个episode（例如，`125`，因此8个进程将总共创建1,000个episode）

`max_frames`

每个episode的最大时间步数（例如，`300`）

[图12-5](#observations_excerpt)显示了一个episode的第40到59帧的摘录，汽车驶向一个拐角，同时显示了随机选择的动作和奖励。请注意，随着汽车经过新的赛道瓷砖，奖励变为3.22，但其他情况下为-0.1。

![一个episode的第40到59帧](Images/gdl2_1205.png)

###### 图12-5。一个episode的第40到59帧

# 训练VAE

现在我们在收集的数据上构建一个生成模型（VAE）。请记住，VAE的目的是让我们将一个64×64×3的图像折叠成一个正态分布的随机变量`z`，其分布由两个向量`mu`和`logvar`参数化。这两个向量的长度均为32。这一步的超参数如下：

`vae_batch_size`

训练VAE时使用的批量大小（每批次观察数量）（例如，`100`）

`z_size`

潜在`z`向量的长度（因此`mu`和`logvar`变量）（例如，`32`）

`vae_num_epoch`

训练时的epoch数量（例如，`10`）

## VAE架构

正如我们之前看到的，Keras允许我们不仅定义将进行端到端训练的VAE模型，还可以定义额外的子模型，分别定义训练网络的编码器和解码器。例如，当我们想要对特定图像进行编码或解码给定的`z`向量时，这些将非常有用。我们将定义VAE模型和三个子模型，如下所示：

`vae`

这是经过训练的端到端VAE。它接受一个64×64×3的图像作为输入，并输出一个重建的64×64×3的图像。

`encode_mu_logvar`

这接受一个64×64×3的图像作为输入，并输出与该输入对应的`mu`和`logvar`向量。多次通过该模型运行相同的输入图像将每次产生相同的`mu`和`logvar`向量。

`encode`

这接受一个64×64×3的图像作为输入，并输出一个采样的`z`向量。多次通过该模型运行相同的输入图像将每次产生不同的`z`向量，使用计算出的`mu`和`logvar`值来定义采样分布。

`decode`

这接受一个`z`向量作为输入，并返回重建的64×64×3图像。

模型和子模型的图表显示在[图12-6](#car_racing_vae)中。

![《World Models》论文中的VAE架构](Images/gdl2_1206.png)

###### 图12-6。《World Models》论文中的VAE架构

## 探索VAE

现在我们将查看VAE和每个子模型的输出，然后看看VAE如何用于生成全新的赛道观察。

### VAE模型

如果我们将一个观察输入到VAE中，它能够准确重建原始图像，如[图12-7](#vae_full_model)所示。这对于直观检查VAE是否正常工作非常有用。

![VAE模型的输入和输出](Images/gdl2_1207.png)

###### 图12-7。VAE模型的输入和输出

### 编码器模型

如果我们用一个观察来喂`encode_mu_logvar`模型，输出将是描述多元正态分布的生成`mu`和`logvar`向量。`encode`模型进一步采样特定的`z`向量。显示两个编码器模型输出的图表在[图12-8](#vae_encoder_output)中。

![来自编码器模型的输出](Images/gdl2_1208.png)

###### 图12-8。编码器模型的输出

潜变量`z`是从由`mu`和`logvar`定义的高斯分布中采样的，通过从标准高斯中采样，然后缩放和移位采样的向量（[示例12-1](#example-8_2)）。

##### 示例12-1。从由`mu`和`logvar`定义的多元正态分布中采样`z`

```py
eps = tf.random_normal(shape=tf.shape(mu))
sigma = tf.exp(logvar * 0.5)
z = mu + eps * sigma
```

### 解码器模型

`decode`模型接受一个`z`向量作为输入，并重构原始图像。在[图12-9](#vae_decoder)中，我们线性插值`z`的两个维度，以展示每个维度似乎编码轨道的特定方面——在这个例子中，`z[4]`控制了最接近汽车的轨道的左右方向，`z[7]`控制了即将到来的左转的急剧程度。

这表明VAE学习到的潜在空间是连续的，可以用来生成代理以前从未观察过的新轨迹段。

![z的两个维度的线性插值](Images/gdl2_1209.png)

###### 图12-9。`z`的两个维度的线性插值

# 收集数据以训练MDN-RNN

现在我们有了一个经过训练的VAE，我们可以用它来为我们的MDN-RNN生成训练数据。

在这一步中，我们通过`encode_mu_logvar`模型传递所有随机回滚观察，并存储与每个观察相对应的`mu`和`logvar`向量。这些编码数据，以及已经收集的`action`、`reward`和`done`变量，将用于训练MDN-RNN。这个过程在[图12-10](#creating_rnn_training)中显示。

![创建RNN训练数据集](Images/gdl2_1210.png)

###### 图12-10。创建MDN-RNN训练数据集

# 训练MDN-RNN

现在我们可以训练MDN-RNN来预测下一个`z`向量的分布，并在未来一个时间步骤内奖励，给定当前的`z`向量、当前的动作和先前的奖励。然后我们可以使用RNN的内部隐藏状态（可以被视为模型对环境动态的当前理解）作为控制器的输入之一，控制器最终将决定最佳的下一步动作。

这个过程的超参数如下：

`rnn_batch_size`

训练MDN-RNN时使用的批量大小（每批次多少个序列）（例如，`100`）

`rnn_num_steps`

训练的总迭代次数（例如，`4000`）

## MDN-RNN架构

MDN-RNN的架构在[图12-11](#rnn_model)中显示。

![RNN架构](Images/gdl2_1211.png)

###### 图12-11。MDN-RNN架构

MDN-RNN由一个LSTM层（RNN）组成，后面是一个密集连接层（MDN），将LSTM的隐藏状态转换为混合分布的参数。让我们逐步走过网络。

LSTM层的输入是一个长度为36的向量，是从VAE的编码`z`向量（长度为32）、当前动作（长度为3）和先前奖励（长度为1）连接而成的。

LSTM层的输出是一个长度为256的向量，每个LSTM单元在该层中有一个值。这被传递给MDN，MDN只是一个密集连接层，将长度为256的向量转换为长度为481的向量。

为什么是481？[图12-12](#mixture_output)解释了从MDN-RNN的输出组成。混合密度网络的目的是模拟我们的下一个`z`可能从几个可能的分布中以一定概率抽取的事实。在汽车赛车示例中，我们选择了五个正态分布。我们需要多少参数来定义这些分布？对于这5个混合物，我们需要一个`mu`和一个`logvar`（来定义分布）以及被选择的这个混合物的对数概率（`logpi`），对于`z`的每个32个维度。这使得5 × 3 × 32 = 480个参数。额外的一个参数是用于奖励预测。

从混合密度网络的输出

###### 图12-12。混合密度网络的输出

## 从MDN-RNN中抽样

我们可以从MDN输出中抽样，通过以下过程生成下一个`z`和下一个时间步的奖励的预测：

1.  将481维输出向量分割为3个变量（`logpi`、`mu`、`logvar`）和奖励值。

1.  对`logpi`进行指数化和缩放，以便将其解释为5个混合索引上的32个概率分布。

1.  对于`z`的32个维度中的每一个，从由`logpi`创建的分布中抽样（即选择哪个分布应该用于`z`的每个维度）。

1.  获取此分布的相应`mu`和`logvar`的值。

1.  从由所选参数`mu`和`logvar`参数化的正态分布中为`z`的每个维度抽样一个值。

MDN-RNN的损失函数是`z`向量重构损失和奖励损失的总和。`z`向量重构损失是MDN-RNN预测的分布的负对数似然，给定`z`的真实值，奖励损失是预测奖励和真实奖励之间的均方误差。

# 训练控制器

最后一步是使用协方差矩阵适应进化策略（CMA-ES）来训练控制器（输出选择的动作的网络）。

该步骤的超参数如下：

`controller_num_worker`

将以并行方式测试解决方案的工作者数量

`controller_num_worker_trial`

每个工作者在每一代将被给予测试的解决方案数量

`controller_num_episode`

每个解决方案将被测试的情节数量，以计算平均奖励

`controller_eval_steps`

评估当前最佳参数集之间的代数数量

## 控制器架构

控制器的架构非常简单。它是一个没有隐藏层的密集连接神经网络。它将输入向量直接连接到动作向量。

输入向量是当前`z`向量（长度32）和LSTM当前隐藏状态（长度256）的串联，得到长度为288的向量。由于我们将每个输入单元直接连接到3个输出动作单元，所以要调整的权重总数为288 × 3 = 864，再加上3个偏置权重，总共为867。

我们应该如何训练这个网络？请注意，这不是一个监督学习问题——我们不是在尝试*预测*正确的动作。没有正确动作的训练集，因为我们不知道对于环境的给定状态来说最佳动作是什么。这就是将这个问题区分为强化学习问题的原因。我们需要代理通过在环境中进行实验并根据接收到的反馈更新其权重来发现权重的最佳值。

进化策略是解决强化学习问题的流行选择，因为它们简单、高效且可扩展。我们将使用一种特定的策略，称为CMA-ES。

## CMA-ES

进化策略通常遵循以下过程：

1.  创建一组代理并随机初始化每个代理要优化的参数。

1.  循环以下步骤：

    1.  评估环境中的每个代理，返回多个周期的平均奖励。

    1.  繁殖得分最高的代理，以创建种群的新成员。

    1.  为新成员的参数添加随机性。

    1.  通过添加新创建的代理和删除表现不佳的代理来更新种群池。

这类似于动物在自然界中进化的过程 - 因此称为*进化*策略。在这种情况下，“繁殖”简单地意味着结合现有的得分最高的代理，使得下一代更有可能产生高质量的结果，类似于它们的父母。与所有强化学习解决方案一样，需要在贪婪地寻找局部最优解和探索参数空间中未知区域以寻找潜在更好解决方案之间找到平衡。这就是为什么向种群中添加随机性很重要，以确保我们的搜索领域不会太狭窄。

CMA-ES只是进化策略的一种形式。简而言之，它通过维护一个正态分布来采样新代理的参数。在每一代中，它更新分布的均值以最大化从上一个时间步采样高分代理的可能性。同时，它更新分布的协方差矩阵以最大化在给定先前均值的情况下采样高分代理的可能性。它可以被视为一种自然产生的梯度下降形式，但它的优势在于它是无导数的，这意味着我们不需要计算或估计昂贵的梯度。

在[图12-13](#cmaes_step)中展示了算法在一个玩具示例上的一个代的演示。在这里，我们试图找到一个高度非线性函数在二维空间中的最小点 - 图像中红/黑区域的函数值大于图像中白/黄区域的函数值。

![CMA-ES算法的一个代更新](Images/gdl2_1213.png)

###### 图12-13\. CMA-ES算法的一个更新步骤（来源：[Ha, 2017](http://bit.ly/2XufRwq))^([2](ch12.xhtml#idm45387001521648))

步骤如下：

1.  我们从随机生成的2D正态分布开始，并从中采样候选人种群，如[图12-13](#cmaes_step)中的蓝色所示。

1.  然后我们计算每个候选者的函数值，并将最佳25%孤立出来，如[图12-13](#cmaes_step)中的紫色所示 - 我们将这组点称为`P`。

1.  我们将新正态分布的均值设置为`P`中点的均值。这可以被视为繁殖阶段，在这个阶段我们只使用最佳候选者来生成分布的新均值。我们还将新正态分布的协方差矩阵设置为`P`中点的协方差矩阵，但在协方差计算中使用现有的均值而不是`P`中点的当前均值。现有均值与`P`中点的均值之间的差异越大，下一个正态分布的方差就越大。这会自然地在寻找最佳参数的过程中产生*动量*效应。

1.  然后我们可以从具有更新均值和协方差矩阵的新正态分布中采样一个新的候选人种群。

[图12-14](#cmaes)展示了该过程的几代。请看均值向最小值大步移动时协方差如何扩大，但当均值稳定在真实最小值时，协方差如何变窄。

![CMA-ES](Images/gdl2_1214.png)

###### 图12-14\. CMA-ES（来源：[维基百科](https://oreil.ly/FObGZ))

对于汽车赛车任务，我们没有一个明确定义的函数来最大化，而是一个环境，其中要优化的867个参数决定了代理的得分如何。最初，一些参数集将以随机方式生成比其他参数更高的得分，算法将逐渐将正态分布移向在环境中得分最高的那些参数的方向。

## 并行化CMA-ES

CMA-ES的一个巨大优势是它可以很容易地并行化。算法中最耗时的部分是计算给定参数集的得分，因为它需要在环境中模拟具有这些参数的代理。然而，这个过程可以并行化，因为个别模拟之间没有依赖关系。有一个协调器进程，它将要测试的参数集并行发送给许多节点进程。节点将结果返回给协调器，协调器累积结果，然后将该代的整体结果传递给CMA-ES对象。该对象根据[图12-13](#cmaes_step)更新正态分布的均值和协方差矩阵，并为协调器提供一个新的人口进行测试。然后循环重新开始。[图12-15](#cmaes_loop)在图表中解释了这一点。

![并行化CMA-ES](Images/gdl2_1215.png)

###### 图12-15\. 并行化CMA-ES——这里有一个人口规模为八个和四个节点（因此t = 2，每个节点负责的试验次数）

![1](Images/1.png)

协调器向CMA-ES对象(`es`)请求一组要试验的参数。

![2](Images/2.png)

协调器将参数分成可用节点的数量。在这里，每个四个节点进程都会得到两组参数进行试验。

![3](Images/3.png)

节点运行一个工作进程，循环遍历每组参数，并为每组参数运行几集。在这里，我们为每组参数运行三集。

![4](Images/4.png)

每集剧集的奖励被平均以给出每组参数的单个得分。

![5](Images/5.png)

每个节点将其得分列表返回给协调器。

![6](Images/6.png)

协调器将所有得分组合在一起，并将此列表发送给`es`对象。

![7](Images/7.png)

`es`对象使用这个奖励列表来计算新的正态分布，如[图12-13](#cmaes_step)所示。

大约经过200代，训练过程为汽车赛车任务实现了约840的平均奖励分数，如[图12-16](#controller_training_outputs)所示。

![并行化CMA-ES](Images/gdl2_1216.png)

###### 图12-16\. 控制器训练过程的平均剧集奖励，按代数（来源：[Zac Wellmer，“World Models”](https://github.com/zacwellmer/WorldModels)）

# 梦中训练

到目前为止，控制器训练是使用Gym的`CarRacing`环境来实现将模拟从一个状态移动到下一个状态的步骤方法。该函数根据环境的当前状态和选择的动作计算下一个状态和奖励。

注意步骤方法在我们模型中执行的功能与MDN-RNN非常相似。从MDN-RNN中采样输出了下一个`z`和奖励的预测，给出了当前`z`和选择的动作。

事实上，MDN-RNN可以被视为一个独立的环境，但是在`z`空间中运行，而不是在原始图像空间中。令人难以置信的是，这意味着我们实际上可以用MDN-RNN的副本替换真实环境，并在MDN-RNN启发的*梦境*中完全训练控制器，以模拟环境应该如何行为。

换句话说，MDN-RNN已经从原始随机移动数据集中学到了关于真实环境的一般物理知识，因此可以在训练控制器时作为真实环境的代理使用。这是非常了不起的——这意味着代理可以通过*思考*如何在梦境环境中最大化奖励来训练自己学习新任务，而无需在真实世界中测试策略。然后，它可以在第一次尝试任务时表现良好，而实际上从未尝试过这项任务。

接下来是在真实环境和梦境中进行训练的架构比较：真实世界架构显示在[图12-17](#real_world_training)中，梦境训练设置在[图12-18](#dream_training)中说明。

![](Images/gdl2_1217.png)

###### 图12-17。在Gym环境中训练控制器

请注意，在梦境架构中，控制器的训练完全在`z`空间中进行，而无需将`z`向量解码回可识别的轨道图像。当然，我们可以这样做，以便视觉检查代理的性能，但这并不是训练所必需的。

![](Images/gdl2_1218.png)

###### 图12-18。在MDN-RNN梦境环境中训练控制器

在MDN-RNN梦境环境中完全训练代理的一个挑战是过拟合。当代理在梦境环境中找到一种有益的策略，但在真实环境中泛化能力不强时，就会发生这种情况，这是因为MDN-RNN没有完全捕捉到在某些条件下真实环境的行为方式。

原始论文的作者强调了这一挑战，并展示了如何包含一个`温度`参数来控制模型的不确定性可以帮助缓解问题。增加这个参数会放大通过MDN-RNN对`z`进行采样时的方差，导致在梦境环境中训练时出现更多波动。控制器对于遇到已知状态的更安全策略会获得更高的奖励，因此往往更容易泛化到真实环境。然而，增加温度需要平衡，以免使环境变得太波动，以至于控制器无法学习任何策略，因为梦境环境在时间上的演变不够一致。

在原始论文中，作者展示了这种技术成功应用于不同的环境：`DoomTakeCover`，基于电脑游戏*Doom*。[图12-19](#doom-temp)显示了改变`温度`参数如何影响虚拟（梦境）得分和真实环境中的实际得分。

![](Images/gdl2_1219.png)

###### 图12-19。使用温度控制梦境环境波动性（来源：[Ha and Schmidhuber, 2018](https://arxiv.org/abs/1803.10122)）

在真实环境中，最佳温度设置为1.15，在发表时超过了当前Gym领导者的得分1,092。这是一个惊人的成就——请记住，控制器从未在真实环境中尝试过这项任务。它只是在真实环境中随机行动（用于训练VAE和MDN-RNN *梦*模型），然后使用梦境环境来训练控制器。

使用生成世界模型作为强化学习方法的一个关键优势是，在梦境环境中的每一代训练比在真实环境中的训练要快得多。这是因为MDN-RNN对`z`和奖励预测比Gym环境中的`z`和奖励计算更快。

# 总结

在本章中，我们看到了如何在强化学习环境中利用生成模型（VAE）使代理能够通过在自己生成的梦境中测试策略来学习有效策略，而不是在真实环境中进行测试。

VAE被训练来学习环境的潜在表示，然后作为输入传递给一个递归神经网络，该网络在潜在空间内预测未来轨迹。令人惊讶的是，代理可以使用这个生成模型作为伪环境，通过演化方法迭代地测试策略，以便在真实环境中得到良好的泛化。

有关该模型的更多信息，请参阅原始论文作者编写的出色互动解释，可在[在线](https://worldmodels.github.io)获取。

^([1](ch12.xhtml#idm45387001861120-marker)) 大卫·哈和尤尔根·施密德胡伯，“世界模型”，2018年3月27日，[*https://arxiv.org/abs/1803.10122*](https://arxiv.org/abs/1803.10122)。

^([2](ch12.xhtml#idm45387001521648-marker)) 大卫·哈，“演化策略的视觉指南”，2017年10月29日，[*https://blog.otoro.net/2017/10/29/visual-evolution-strategies*](https://blog.otoro.net/2017/10/29/visual-evolution-strategies)。
