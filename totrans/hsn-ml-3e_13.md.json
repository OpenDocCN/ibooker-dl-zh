["```py\nimport tensorflow as tf\n\ndense = tf.keras.layers.Dense(50, activation=\"relu\",\n                              kernel_initializer=\"he_normal\")\n```", "```py\nhe_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\",\n                                                    distribution=\"uniform\")\ndense = tf.keras.layers.Dense(50, activation=\"sigmoid\",\n                              kernel_initializer=he_avg_init)\n```", "```py\nleaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)  # defaults to alpha=0.3\ndense = tf.keras.layers.Dense(50, activation=leaky_relu,\n                              kernel_initializer=\"he_normal\")\n```", "```py\nmodel = tf.keras.models.Sequential([\n    [...]  # more layers\n    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"),  # no activation\n    tf.keras.layers.LeakyReLU(alpha=0.2),  # activation as a separate layer\n    [...]  # more layers\n])\n```", "```py\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28]),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(300, activation=\"relu\",\n                          kernel_initializer=\"he_normal\"),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(100, activation=\"relu\",\n                          kernel_initializer=\"he_normal\"),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\n```", "```py\n>>> model.summary()\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\nflatten (Flatten)            (None, 784)               0\n_________________________________________________________________\nbatch_normalization (BatchNo (None, 784)               3136\n_________________________________________________________________\ndense (Dense)                (None, 300)               235500\n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 300)               1200\n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               30100\n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 100)               400\n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                1010\n=================================================================\nTotal params: 271,346\nTrainable params: 268,978\nNon-trainable params: 2,368\n_________________________________________________________________\n```", "```py\n>>> [(var.name, var.trainable) for var in model.layers[1].variables]\n[('batch_normalization/gamma:0', True),\n ('batch_normalization/beta:0', True),\n ('batch_normalization/moving_mean:0', False),\n ('batch_normalization/moving_variance:0', False)]\n```", "```py\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28]),\n    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Activation(\"relu\"),\n    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Activation(\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\n```", "```py\noptimizer = tf.keras.optimizers.SGD(clipvalue=1.0)\nmodel.compile([...], optimizer=optimizer)\n```", "```py\n[...]  # Assuming model A was already trained and saved to \"my_model_A\"\nmodel_A = tf.keras.models.load_model(\"my_model_A\")\nmodel_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\nmodel_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n```", "```py\nmodel_A_clone = tf.keras.models.clone_model(model_A)\nmodel_A_clone.set_weights(model_A.get_weights())\n```", "```py\nfor layer in model_B_on_A.layers[:-1]:\n    layer.trainable = False\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\nmodel_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n                     metrics=[\"accuracy\"])\n```", "```py\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n                           validation_data=(X_valid_B, y_valid_B))\n\nfor layer in model_B_on_A.layers[:-1]:\n    layer.trainable = True\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\nmodel_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n                     metrics=[\"accuracy\"])\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n                           validation_data=(X_valid_B, y_valid_B))\n```", "```py\n>>> model_B_on_A.evaluate(X_test_B, y_test_B)\n[0.2546142041683197, 0.9384999871253967]\n```", "```py\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n```", "```py\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,\n                                    nesterov=True)\n```", "```py\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n```", "```py\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9,\n                                     beta_2=0.999)\n```", "```py\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)\n```", "```py\ndef exponential_decay_fn(epoch):\n    return 0.01 * 0.1 ** (epoch / 20)\n```", "```py\ndef exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1 ** (epoch / s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n```", "```py\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\nhistory = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])\n```", "```py\ndef exponential_decay_fn(epoch, lr):\n    return lr * 0.1 ** (1 / 20)\n```", "```py\ndef piecewise_constant_fn(epoch):\n    if epoch < 5:\n        return 0.01\n    elif epoch < 15:\n        return 0.005\n    else:\n        return 0.001\n```", "```py\nlr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\nhistory = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])\n```", "```py\nimport math\n\nbatch_size = 32\nn_epochs = 25\nn_steps = n_epochs * math.ceil(len(X_train) / batch_size)\nscheduled_learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.01, decay_steps=n_steps, decay_rate=0.1)\noptimizer = tf.keras.optimizers.SGD(learning_rate=scheduled_learning_rate)\n```", "```py\nlayer = tf.keras.layers.Dense(100, activation=\"relu\",\n                              kernel_initializer=\"he_normal\",\n                              kernel_regularizer=tf.keras.regularizers.l2(0.01))\n```", "```py\nfrom functools import partial\n\nRegularizedDense = partial(tf.keras.layers.Dense,\n                           activation=\"relu\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=tf.keras.regularizers.l2(0.01))\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28]),\n    RegularizedDense(100),\n    RegularizedDense(100),\n    RegularizedDense(10, activation=\"softmax\")\n])\n```", "```py\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28]),\n    tf.keras.layers.Dropout(rate=0.2),\n    tf.keras.layers.Dense(100, activation=\"relu\",\n                          kernel_initializer=\"he_normal\"),\n    tf.keras.layers.Dropout(rate=0.2),\n    tf.keras.layers.Dense(100, activation=\"relu\",\n                          kernel_initializer=\"he_normal\"),\n    tf.keras.layers.Dropout(rate=0.2),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\n[...]  # compile and train the model\n```", "```py\nimport numpy as np\n\ny_probas = np.stack([model(X_test, training=True)\n                     for sample in range(100)])\ny_proba = y_probas.mean(axis=0)\n```", "```py\n>>> model.predict(X_test[:1]).round(3)\narray([[0\\.   , 0\\.   , 0\\.   , 0\\.   , 0\\.   , 0.024, 0\\.   , 0.132, 0\\.   ,\n 0.844]], dtype=float32)\n```", "```py\n>>> y_proba[0].round(3)\narray([0\\.   , 0\\.   , 0\\.   , 0\\.   , 0\\.   , 0.067, 0\\.   , 0.209, 0.001,\n 0.723], dtype=float32)\n```", "```py\n>>> y_std = y_probas.std(axis=0)\n>>> y_std[0].round(3)\narray([0\\.   , 0\\.   , 0\\.   , 0.001, 0\\.   , 0.096, 0\\.   , 0.162, 0.001,\n 0.183], dtype=float32)\n```", "```py\n>>> y_pred = y_proba.argmax(axis=1)\n>>> accuracy = (y_pred == y_test).sum() / len(y_test)\n>>> accuracy\n0.8717\n```", "```py\nclass MCDropout(tf.keras.layers.Dropout):\n    def call(self, inputs, training=False):\n        return super().call(inputs, training=True)\n```", "```py\ndense = tf.keras.layers.Dense(\n    100, activation=\"relu\", kernel_initializer=\"he_normal\",\n    kernel_constraint=tf.keras.constraints.max_norm(1.))\n```"]