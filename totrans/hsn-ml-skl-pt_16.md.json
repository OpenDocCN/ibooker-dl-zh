["```py\nfrom pathlib import Path\nimport urllib.request\n\ndef download_shakespeare_text():\n    path = Path(\"datasets/shakespeare/shakespeare.txt\")\n    if not path.is_file():\n        path.parent.mkdir(parents=True, exist_ok=True)\n        url = \"https://homl.info/shakespeare\"\n        urllib.request.urlretrieve(url, path)\n    return path.read_text()\n\nshakespeare_text = download_shakespeare_text()\n```", "```py\n>>> print(shakespeare_text[:80])\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n```", "```py\n>>> vocab = sorted(set(shakespeare_text.lower()))\n>>> \"\".join(vocab)\n\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\"\n```", "```py\n>>> char_to_id = {char: index for index, char in enumerate(vocab)}\n>>> id_to_char = {index: char for index, char in enumerate(vocab)}\n>>> char_to_id[\"a\"]\n13\n>>> id_to_char[13]\n'a'\n```", "```py\nimport torch\n\ndef encode_text(text):\n    return torch.tensor([char_to_id[char] for char in text.lower()])\n\ndef decode_text(char_ids):\n    return \"\".join([id_to_char[char_id.item()] for char_id in char_ids])\n```", "```py\n>>> encoded = encode_text(\"Hello, world!\")\n>>> encoded\ntensor([20, 17, 24, 24, 27,  6,  1, 35, 27, 30, 24, 16,  2])\n>>> decode_text(encoded)\n'hello, world!'\n```", "```py\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CharDataset(Dataset):\n    def __init__(self, text, window_length):\n        self.encoded_text = encode_text(text)\n        self.window_length = window_length\n\n    def __len__(self):\n        return len(self.encoded_text) - self.window_length\n\n    def __getitem__(self, idx):\n        if idx >= len(self):\n            raise IndexError(\"dataset index out of range\")\n        end = idx + self.window_length\n        window = self.encoded_text[idx : end]\n        target = self.encoded_text[idx + 1 : end + 1]\n        return window, target\n```", "```py\nwindow_length = 50\nbatch_size = 512  # reduce if your GPU cannot handle such a large batch size\ntrain_set = CharDataset(shakespeare_text[:1_000_000], window_length)\nvalid_set = CharDataset(shakespeare_text[1_000_000:1_060_000], window_length)\ntest_set = CharDataset(shakespeare_text[1_060_000:], window_length)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size)\ntest_loader = DataLoader(test_set, batch_size=batch_size)\n```", "```py\n>>> import torch.nn as nn\n>>> torch.manual_seed(42)\n>>> embed = nn.Embedding(5, 3)  # 5 categories × 3D embeddings\n>>> embed(torch.tensor([[3, 2], [0, 2]]))\ntensor([[[ 0.2674,  0.5349,  0.8094],\n [ 2.2082, -0.6380,  0.4617]],\n\n [[ 0.3367,  0.1288,  0.2345],\n [ 2.2082, -0.6380,  0.4617]]], grad_fn=<EmbeddingBackward0>)\n```", "```py\nclass ShakespeareModel(nn.Module):\n    def __init__(self, vocab_size, n_layers=2, embed_dim=10, hidden_dim=128,\n                 dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n                          batch_first=True, dropout=dropout)\n        self.output = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, X):\n        embeddings = self.embed(X)\n        outputs, _states = self.gru(embeddings)\n        return self.output(outputs).permute(0, 2, 1)\n\ntorch.manual_seed(42)\nmodel = ShakespeareModel(len(vocab)).to(device)\n```", "```py\nmodel.eval()  # don't forget to switch the model to evaluation mode!\ntext = \"To be or not to b\"\nencoded_text = encode_text(text).unsqueeze(dim=0).to(device)\nwith torch.no_grad():\n    Y_logits = model(encoded_text)\n    predicted_char_id = Y_logits[0, :, -1].argmax().item()\n    predicted_char = id_to_char[predicted_char_id]  # correctly predicts \"e\"\n```", "```py\n>>> torch.manual_seed(42)\n>>> probs = torch.tensor([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n>>> samples = torch.multinomial(probs, replacement=True, num_samples=8)\n>>> samples\ntensor([[0, 0, 0, 0, 1, 0, 2, 2]])\n```", "```py\nimport torch.nn.functional as F\n\ndef next_char(model, text, temperature=1):\n    encoded_text = encode_text(text).unsqueeze(dim=0).to(device)\n    with torch.no_grad():\n        Y_logits = model(encoded_text)\n        Y_probas = F.softmax(Y_logits[0, :, -1] / temperature, dim=-1)\n        predicted_char_id = torch.multinomial(Y_probas, num_samples=1).item()\n    return id_to_char[predicted_char_id]\n```", "```py\ndef extend_text(model, text, n_chars=80, temperature=1):\n    for _ in range(n_chars):\n        text += next_char(model, text, temperature)\n    return text\n```", "```py\n>>> print(extend_text(model, \"To be or not to b\", temperature=0.01))\nTo be or not to be the state\nand the contrary of the state and the sea,\nthe common people of the\n>>> print(extend_text(model, \"To be or not to b\", temperature=0.4))\nTo be or not to be the better from the cause\nthat thou think you may be so be gone.\n\nromeo:\nthat\n>>> print(extend_text(model, \"To be or not to b\", temperature=100))\nTo be or not to b-c3;m-rkn&:x:uyve:b&hi n;n-h;wt3k\n&cixxh:a!kq$c$ 3 ncq$ ;;wq cp:!xq;yh\n!3\nd!nhi.\n```", "```py\nfrom datasets import load_dataset\n\nimdb_dataset = load_dataset(\"imdb\")\nsplit = imdb_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\nimdb_train_set, imdb_valid_set = split[\"train\"], split[\"test\"]\nimdb_test_set = imdb_dataset[\"test\"]\n```", "```py\n>>> imdb_train_set[1][\"text\"]\n\"'The Rookie' was a wonderful movie about the second chances life holds [...]\"\n>>> imdb_train_set[1][\"label\"]\n1\n>>> imdb_train_set[16][\"text\"]\n\"Lillian Hellman's play, adapted by Dashiell Hammett with help from Hellman,\nbecomes a curious project to come out of gritty Warner Bros. [...] It seems to\ntake forever for this drama to find its focus, [...], it seems a little\npatronizing [...] Lukas has several speeches in the third-act which undoubtedly\nwon him the Academy Award [...] this tasteful, tactful movie [...] It should be\na heady mix, but instead it's rather dry-eyed and inert. ** from ****\"\n>>> imdb_train_set[16][\"label\"]\n0\n```", "```py\nimport tokenizers\n\nbpe_model = tokenizers.models.BPE(unk_token=\"<unk>\")\nbpe_tokenizer = tokenizers.Tokenizer(bpe_model)\nbpe_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\nspecial_tokens = [\"<pad>\", \"<unk>\"]\nbpe_trainer = tokenizers.trainers.BpeTrainer(vocab_size=1000,\n                                             special_tokens=special_tokens)\ntrain_reviews = [review[\"text\"].lower() for review in imdb_train_set]\nbpe_tokenizer.train_from_iterator(train_reviews, bpe_trainer)\n```", "```py\n>>> some_review = \"what an awesome movie! \n\n>>> bpe_encoding = bpe_tokenizer.encode(some_review)\n>>> bpe_encoding\nEncoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets,\n         attention_mask, special_tokens_mask, overflowing])\n\n```", "```py\n>>> bpe_encoding.tokens\n['what', 'an', 'aw', 'es', 'ome', 'movie', '!', '<unk>']\n>>> bpe_token_ids = bpe_encoding.ids\n>>> bpe_token_ids\n[303, 139, 373, 149, 240, 211, 4, 1]\n```", "```py\n>>> bpe_tokenizer.decode(bpe_token_ids)\n'what an aw es ome movie !'\n```", "```py\n>>> bpe_encoding.offsets\n[(0, 4), (5, 7), (8, 10), (10, 12), (12, 15), (16, 21), (21, 22), (23, 24)]\n```", "```py\n>>> bpe_tokenizer.encode_batch(train_reviews[:3])\n[Encoding(num_tokens=281, attributes=[ids, type_ids, tokens, [...]]),\n Encoding(num_tokens=114, attributes=[ids, type_ids, tokens, [...]]),\n Encoding(num_tokens=285, attributes=[ids, type_ids, tokens, [...]]),\n```", "```py\nbpe_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\nbpe_tokenizer.enable_truncation(max_length=500)\n```", "```py\n>>> bpe_encodings = bpe_tokenizer.encode_batch(train_reviews[:3])\n>>> bpe_batch_ids = torch.tensor([encoding.ids for encoding in bpe_encodings])\n>>> bpe_batch_ids\ntensor([[159, 402, 176, 246,  61, [...], 215, 156, 586,   0,   0,   0,   0],\n [ 10, 138, 198, 289, 175, [...],   0,   0,   0,   0,   0,   0,   0],\n [289,  15, 209, 398, 177, [...],  50,  29,  22,  17,  24,  18,  24]])\n```", "```py\n>>> attention_mask = torch.tensor([encoding.attention_mask\n...                                for encoding in bpe_encodings])\n...\n>>> attention_mask\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 1, 1, 1, 0, 0, 0, 0],\n [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 0, 0, 0, 0, 0, 0, 0],\n [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 1, 1, 1, 1, 1, 1, 1]])\n>>> lengths = attention_mask.sum(dim=-1)\n>>> lengths\ntensor([281, 114, 285])\n```", "```py\nimport transformers\n\ngpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\ngpt2_encoding = gpt2_tokenizer(train_reviews[:3], truncation=True,\n                               max_length=500)\n```", "```py\n>>> gpt2_token_ids = gpt2_encoding[\"input_ids\"][0][:10]\n>>> gpt2_token_ids\n[14247, 35030, 1690, 423, 257, 1688, 8046, 13, 484, 1690]\n>>> gpt2_tokenizer.decode(gpt2_token_ids)\n'stage adaptations often have a major fault. they often'\n```", "```py\nbert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_encoding = bert_tokenizer(train_reviews[:3], padding=True,\n                               truncation=True, max_length=500,\n                               return_tensors=\"pt\")\n```", "```py\n>>> bert_encoding[\"input_ids\"]\ntensor([[ 101, 2754,17241, 2411, 2031, [...],  102, 0, 0, 0, [...], 0, 0, 0],\n [ 101, 1005, 1996, 8305, 1005, [...],  102, 0, 0, 0, [...], 0, 0, 0],\n [ 101, 7929, 1010, 2021, 2515, [...], 1012,  1019,  1013,  1019,  102]])\n>>> bert_encoding[\"attention_mask\"]\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 0, 0, 0, 0, 0, 0, 0, 0, 0],\n [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 0, 0, 0, 0, 0, 0, 0, 0, 0],\n [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n```", "```py\nalbert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"albert-base-v2\")\nalbert_encoding = albert_tokenizer(train_reviews[:3], padding=True, [...])\n```", "```py\nhf_tokenizer = transformers.PreTrainedTokenizerFast(\n    tokenizer_object=bpe_tokenizer)\nhf_encodings = hf_tokenizer(train_reviews[:3], padding=True, [...])\n```", "```py\ndef collate_fn(batch, tokenizer=bert_tokenizer):\n    reviews = [review[\"text\"] for review in batch]\n    labels = [[review[\"label\"]] for review in batch]\n    encodings = tokenizer(reviews, padding=True, truncation=True,\n                          max_length=200, return_tensors=\"pt\")\n    labels = torch.tensor(labels, dtype=torch.float32)\n    return encodings, labels\n\nbatch_size = 256\nimdb_train_loader = DataLoader(imdb_train_set, batch_size=batch_size,\n                               collate_fn=collate_fn, shuffle=True)\nimdb_valid_loader = DataLoader(imdb_valid_set, batch_size=batch_size,\n                               collate_fn=collate_fn)\nimdb_test_loader = DataLoader(imdb_test_set, batch_size=batch_size,\n                              collate_fn=collate_fn)\n```", "```py\nclass SentimentAnalysisModel(nn.Module):\n    def __init__(self, vocab_size, n_layers=2, embed_dim=128, hidden_dim=64,\n                 pad_id=0, dropout=0.2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim,\n                                  padding_idx=pad_id)\n        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n                          batch_first=True, dropout=dropout)\n        self.output = nn.Linear(hidden_dim, 1)\n\n    def forward(self, encodings):\n        embeddings = self.embed(encodings[\"input_ids\"])\n        _outputs, hidden_states = self.gru(embeddings)\n        return self.output(hidden_states[-1])\n```", "```py\n>>> from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n>>> sequences = torch.tensor([[1, 2, 0, 0], [5, 6, 7, 8]])\n>>> packed = pack_padded_sequence(sequences, lengths=(2, 4),\n...                               enforce_sorted=False, batch_first=True)\n...\n>>> packed\nPackedSequence(data=tensor([5, 1, 6, 2, 7, 8]), [...])\n>>> padded, lengths = pad_packed_sequence(packed, batch_first=True)\n>>> padded, lengths\n(tensor([[1, 2, 0, 0],\n [5, 6, 7, 8]]),\n tensor([2, 4]))\n```", "```py\nlengths = encodings[\"attention_mask\"].sum(dim=1)\npacked = pack_padded_sequence(embeddings, lengths=lengths.cpu(),\n                              batch_first=True, enforce_sorted=False)\n_outputs, hidden_states = self.gru(packed)\n```", "```py\nself.output = nn.Linear(2 * hidden_dim, 1)\n```", "```py\nn_dims = self.output.in_features\ntop_states = hidden_states[-2:].permute(1, 0, 2).reshape(-1, n_dims)\nreturn self.output(top_states)\n```", "```py\n>>> bert_model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n>>> bert_model.embeddings.word_embeddings\nEmbedding(30522, 768, padding_idx=0)\n```", "```py\nclass SentimentAnalysisModelPreEmbeds(nn.Module):\n    def __init__(self, pretrained_embeddings, n_layers=2, hidden_dim=64,\n                 dropout=0.2):\n        super().__init__()\n        weights = pretrained_embeddings.weight.data\n        self.embed = nn.Embedding.from_pretrained(weights, freeze=True)\n        embed_dim = weights.shape[-1]\n        [...]  # the rest of the model is exactly like earlier\n\nimdb_model_bert_embeds = SentimentAnalysisModelPreEmbeds(\n    bert_model.embeddings.word_embeddings).to(device)\n```", "```py\n>>> bert_encoding = bert_tokenizer(train_reviews[:3], padding=True,\n...                                max_length=200, truncation=True,\n...                                return_tensors=\"pt\")\n...\n>>> bert_output = bert_model(**bert_encoding)\n>>> bert_output.last_hidden_state.shape\ntorch.Size([3, 200, 768])\n```", "```py\nclass SentimentAnalysisModelBert(nn.Module):\n    def __init__(self, n_layers=2, hidden_dim=64, dropout=0.2):\n        super().__init__()\n        self.bert = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n        embed_dim = self.bert.config.hidden_size\n        self.gru = nn.GRU(embed_dim, hidden_dim, [...])\n        self.output = nn.Linear(hidden_dim, 1)\n\n    def forward(self, encodings):\n        contextualized_embeddings = self.bert(**encodings).last_hidden_state\n        lengths = encodings[\"attention_mask\"].sum(dim=1)\n        packed = pack_padded_sequence(contextualized_embeddings, [...])\n        _outputs, hidden_states = self.gru(packed)\n        return self.output(hidden_states[-1])\n```", "```py\ndef forward(self, encodings):\n    bert_output = self.bert(**encodings)\n    return self.output(bert_output.last_hidden_state[:, 0])\n```", "```py\nfrom transformers import BertForSequenceClassification\n\ntorch.manual_seed(42)\nbert_for_binary_clf = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", num_labels=2, dtype=torch.float16).to(device)\n```", "```py\n>>> encoding = bert_tokenizer([\"This was a great movie!\"])\n>>> with torch.no_grad():\n...   output = bert_for_binary_clf(\n...     input_ids=torch.tensor(encoding[\"input_ids\"], device=device),\n...     attention_mask=torch.tensor(encoding[\"attention_mask\"], device=device))\n...\n>>> output.logits\ntensor([[-0.0120,  0.6304]], device='cuda:0', dtype=torch.float16)\n>>> torch.softmax(output.logits, dim=-1)\ntensor([[0.3447, 0.6553]], device='cuda:0', dtype=torch.float16)\n```", "```py\n>>> with torch.no_grad():\n...   output = bert_for_binary_clf(\n...     input_ids=torch.tensor(encoding[\"input_ids\"], device=device),\n...     attention_mask=torch.tensor(encoding[\"attention_mask\"], device=device),\n...     labels=torch.tensor([1], device=device))\n...\n>>> output.loss\ntensor(0.4226, device='cuda:0', dtype=torch.float16)\n```", "```py\ndef tokenize_batch(batch):\n    return bert_tokenizer(batch[\"text\"], truncation=True, max_length=200)\n\ntok_imdb_train_set = imdb_train_set.map(tokenize_batch, batched=True)\ntok_imdb_valid_set = imdb_valid_set.map(tokenize_batch, batched=True)\ntok_imdb_test_set = imdb_test_set.map(tokenize_batch, batched=True)\n```", "```py\ndef compute_accuracy(pred):\n    return {\"accuracy\": (pred.label_ids == pred.predictions.argmax(-1)).mean()}\n```", "```py\nfrom transformers import TrainingArguments\n\ntrain_args = TrainingArguments(\n    output_dir=\"my_imdb_model\", num_train_epochs=2,\n    per_device_train_batch_size=128, per_device_eval_batch_size=128,\n    eval_strategy=\"epoch\", logging_strategy=\"epoch\", save_strategy=\"epoch\",\n    load_best_model_at_end=True, metric_for_best_model=\"accuracy\",\n    report_to=\"none\")\n```", "```py\nfrom transformers import DataCollatorWithPadding, Trainer\n\ntrainer = Trainer(\n    bert_for_binary_clf, train_args, train_dataset=tok_imdb_train_set,\n    eval_dataset=tok_imdb_valid_set, compute_metrics=compute_accuracy,\n    data_collator=DataCollatorWithPadding(bert_tokenizer))\ntrain_output = trainer.train()\n```", "```py\n>>> from transformers import pipeline\n>>> model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n>>> classifier_imdb = pipeline(\"sentiment-analysis\", model=model_name,\n...                            truncation=True, max_length=512)\n...\n>>> classifier_imdb(train_reviews[:10])\n[{'label': 'POSITIVE', 'score': 0.9996108412742615},\n {'label': 'POSITIVE', 'score': 0.9998623132705688},\n [...]\n {'label': 'POSITIVE', 'score': 0.9978922009468079},\n {'label': 'NEGATIVE', 'score': 0.9997020363807678}]\n```", "```py\n>>> model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n>>> classifier_mnli = pipeline(\"text-classification\", model=model_name)\n>>> classifier_mnli([\n...     \"She loves me. [SEP] She loves me not. [SEP]\",\n...     \"Alice just woke up. [SEP] Alice is awake. [SEP]\",\n...     \"I like dogs. [SEP] Everyone likes dogs. [SEP]\"])\n...\n[{'label': 'contradiction', 'score': 0.9717152714729309},\n {'label': 'entailment', 'score': 0.9119168519973755},\n {'label': 'neutral', 'score': 0.9509281516075134}]\n```", "```py\nnmt_original_valid_set, nmt_test_set = load_dataset(\n    path=\"ageron/tatoeba_mt_train\", name=\"eng-spa\",\n    split=[\"validation\", \"test\"])\nsplit = nmt_original_valid_set.train_test_split(train_size=0.8, seed=42)\nnmt_train_set, nmt_valid_set = split[\"train\"], split[\"test\"]\n```", "```py\n>>> nmt_train_set[0]\n{'source_text': 'Tom tried to break up the fight.',\n 'target_text': 'Tom trató de disolver la pelea.',\n 'source_lang': 'eng',\n 'target_lang': 'spa'}\n```", "```py\ndef train_eng_spa():  # a generator function to iterate over all training text\n    for pair in nmt_train_set:\n        yield pair[\"source_text\"]\n        yield pair[\"target_text\"]\n\nmax_length = 256\nvocab_size = 10_000\nnmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"<unk>\")\nnmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\nnmt_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\nnmt_tokenizer.enable_truncation(max_length=max_length)\nnmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\nnmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(\n    vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\nnmt_tokenizer.train_from_iterator(train_eng_spa(), nmt_tokenizer_trainer)\n```", "```py\n>>> nmt_tokenizer.encode(\"I like soccer\").ids\n[43, 401, 4381]\n>>> nmt_tokenizer.encode(\"<s> Me gusta el fútbol\").ids\n[2, 396, 582, 219, 3356]\n```", "```py\nfrom collections import namedtuple\n\nfields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"]\nclass NmtPair(namedtuple(\"NmtPairBase\", fields)):\n    def to(self, device):\n        return NmtPair(self.src_token_ids.to(device), self.src_mask.to(device),\n                       self.tgt_token_ids.to(device), self.tgt_mask.to(device))\n```", "```py\ndef nmt_collate_fn(batch):\n    src_texts = [pair['source_text'] for pair in batch]\n    tgt_texts = [f\"<s> {pair['target_text']} </s>\" for pair in batch]\n    src_encodings = nmt_tokenizer.encode_batch(src_texts)\n    tgt_encodings = nmt_tokenizer.encode_batch(tgt_texts)\n    src_token_ids = torch.tensor([enc.ids for enc in src_encodings])\n    tgt_token_ids = torch.tensor([enc.ids for enc in tgt_encodings])\n    src_mask = torch.tensor([enc.attention_mask for enc in src_encodings])\n    tgt_mask = torch.tensor([enc.attention_mask for enc in tgt_encodings])\n    inputs = NmtPair(src_token_ids, src_mask,\n                     tgt_token_ids[:, :-1], tgt_mask[:, :-1])\n    labels = tgt_token_ids[:, 1:]\n    return inputs, labels\n\nbatch_size = 32\nnmt_train_loader = DataLoader(nmt_train_set, batch_size=batch_size,\n                              collate_fn=nmt_collate_fn, shuffle=True)\nnmt_valid_loader = DataLoader(nmt_valid_set, batch_size=batch_size,\n                              collate_fn=nmt_collate_fn)\nnmt_test_loader = DataLoader(nmt_test_set, batch_size=batch_size,\n                             collate_fn=nmt_collate_fn)\n```", "```py\nclass NmtModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim=512, pad_id=0, hidden_dim=512,\n                 n_layers=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n        self.encoder = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n                              batch_first=True)\n        self.decoder = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n                              batch_first=True)\n        self.output = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, pair):\n        src_embeddings = self.embed(pair.src_token_ids)\n        tgt_embeddings = self.embed(pair.tgt_token_ids)\n        src_lengths = pair.src_mask.sum(dim=1)\n        src_packed = pack_padded_sequence(\n            src_embeddings, lengths=src_lengths.cpu(),\n            batch_first=True, enforce_sorted=False)\n        _, hidden_states = self.encoder(src_packed)\n        outputs, _ = self.decoder(tgt_embeddings, hidden_states)\n        return self.output(outputs).permute(0, 2, 1)\n\ntorch.manual_seed(42)\nvocab_size = nmt_tokenizer.get_vocab_size()\nnmt_model = NmtModel(vocab_size).to(device)\n```", "```py\nxentropy = nn.CrossEntropyLoss(ignore_index=0)  # ignore <pad> tokens\n```", "```py\ndef translate(model, src_text, max_length=20, pad_id=0, eos_id=3):\n    tgt_text = \"\"\n    token_ids = []\n    for index in range(max_length):\n        batch, _ = nmt_collate_fn([{\"source_text\": src_text,\n                                    \"target_text\": tgt_text}])\n        with torch.no_grad():\n            Y_logits = model(batch.to(device))\n            Y_token_ids = Y_logits.argmax(dim=1)  # find the best token IDs\n            next_token_id = Y_token_ids[0, index]  # take the last token ID\n\n        next_token = nmt_tokenizer.id_to_token(next_token_id)\n        tgt_text += \" \" + next_token\n        if next_token_id == eos_id:\n            break\n    return tgt_text\n```", "```py\n>>> nmt_model.eval()\n>>> translate(nmt_model, \"I like soccer.\")\n' Me gusta el fútbol . </s>'\n```", "```py\n>>> longer_text = \"I like to play soccer with my friends.\"\n>>> translate(nmt_model, longer_text)\n' Me gusta jugar con mis amigos . </s>'\n```", "```py\n>>> beam_search(nmt_model, longer_text, beam_width=3)\n' Me gusta jugar al fútbol con mis amigos . </s>'\n```", "```py\n>>> longest_text = \"I like to play soccer with my friends at the beach.\"\n>>> beam_search(nmt_model, longest_text, beam_width=3)\n' Me gusta jugar con jugar con los jug adores de la playa . </s>'\n```", "```py\ndef attention(query, key, value):  # note: dq == dk and Lk == Lv\n    scores = query @ key.transpose(1, 2)  # [B,Lq,dq] @ [B,dk,Lk] = [B, Lq, Lk]\n    weights = torch.softmax(scores, dim=-1)  # [B, Lq, Lk]\n    return weights @ value  # [B, Lq, Lk] @ [B, Lv, dv] = [B, Lq, dv]\n```", "```py\nself.output = nn.Linear(2 * hidden_dim, vocab_size)\n```", "```py\ndef forward(self, pair):\n    src_embeddings = self.embed(pair.src_token_ids)  # same as earlier\n    tgt_embeddings = self.embed(pair.tgt_token_ids)  # same\n    src_lengths = pair.src_mask.sum(dim=1)  # same\n    src_packed = pack_padded_sequence(src_embeddings, [...])  # same\n    encoder_outputs_packed, hidden_states = self.encoder(src_packed)\n    decoder_outputs, _ = self.decoder(tgt_embeddings, hidden_states)  # same\n    encoder_outputs, _ = pad_packed_sequence(encoder_outputs_packed,\n                                                batch_first=True)\n    attn_output = attention(query=decoder_outputs,\n                            key=encoder_outputs, value=encoder_outputs)\n    combined_output = torch.cat((attn_output, decoder_outputs), dim=-1)\n    return self.output(combined_output).permute(0, 2, 1)\n```", "```py\n>>> translate(nmt_attn_model, longest_text)\n' Me gusta jugar fu tbol con mis amigos en la playa . </s>'\n```"]