["```py\nfrom pathlib import Path\nimport urllib.request\n\ndef download_shakespeare_text():\n    path = Path(\"datasets/shakespeare/shakespeare.txt\")\n    if not path.is_file():\n        path.parent.mkdir(parents=True, exist_ok=True)\n        url = \"https://homl.info/shakespeare\"\n        urllib.request.urlretrieve(url, path)\n    return path.read_text()\n\nshakespeare_text = download_shakespeare_text()\n```", "```py\n>>> print(shakespeare_text[:80]) `First Citizen:`\n`Before we proceed any further, hear me speak.`\n\n`All:`\n`Speak, speak.`\n```", "```py```", "````py`` Looks like Shakespeare, all right!    Neural networks work with numbers, not text, so we need a way to encode text into numbers. In general, this is done by splitting the text into *tokens*, such as words or characters, and assigning an integer ID to each possible token. For example, let’s split our text into characters, and assign an ID to each possible character. We first need to find the list of characters used in the text. This will constitute our token *vocabulary*:    ``` >>> vocab = sorted(set(shakespeare_text.lower())) `>>>` `\"\"``.``join``(``vocab``)` `` `\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\"` `` ```py   ````", "````` ```py`````", "``` >>> char_to_id = {char: index for index, char in enumerate(vocab)} `>>>` `id_to_char` `=` `{``index``:` `char` `for` `index``,` `char` `in` `enumerate``(``vocab``)}` ```", "``` `13` `>>>` `id_to_char``[``13``]` `` `'a'` `` ```", "```` ```py   ````", "``` ```", "```py`` ```", "```py` Next, let’s create two helper functions to encode text to tensors of token IDs, and to decode them back to text:    ```", "```py    Let’s try them out:    ```", "```py `tensor([20, 17, 24, 24, 27,  6,  1, 35, 27, 30, 24, 16,  2])` `>>>` `decode_text``(``encoded``)` `` `'hello, world!'` `` ```", "```py   ```", "```py ```", "```py`Next, let’s prepare the dataset. Right now, we have a single, extremely long sequence of characters containing Shakespeare’s works. Just like we did in [Chapter 13](ch13.html#rnn_chapter), we can turn this long sequence into a dataset of windows that we can then use to train a sequence-to-sequence RNN. The targets will be similar to the inputs, but shifted by one time step into the “future”. For example, one sample in the dataset may be a sequence of character IDs representing the text “to be or not to b” (without the final “e”), and the corresponding target—a sequence of character IDs representing the text “o be or not to be” (with the final “e”, but without the leading “t”). Let’s create our dataset class:    ```", "```py    And now let’s create the data loaders, as usual. Since the text is quite large, we can afford to use roughly 90% for training (i.e., one million characters), and just 5% for validation, and 5% for testing (60,000 characters each):    ```", "```py    Each batch will be composed of 512 50-character windows, where each character is represented by its token ID, and where each window comes with its 50-character target window (offset by one character). Note that the training batches are shuffled at each epoch (see [Figure 14-1](#window_dataset_diagram)).  ![Diagram illustrating a batch of input and target windows with a window length of 10, showing the offset relationship between inputs and targets.](assets/hmls_1401.png)  ###### Figure 14-1\\. Each training batch is composed of shuffled windows, along with their shifted targets. In this figure, the window length is 10 instead of 50.    ###### Tip    We set the window length to 50, but you can try tuning it. It’s easier and faster to train RNNs on shorter input sequences, but the RNN will not be able to learn any pattern longer than the window length, so don’t make it too small.    While we could technically feed the token IDs directly to a neural network without any further preprocessing, it wouldn’t work very well. Indeed, as we saw in [Chapter 2](ch02.html#project_chapter), most ML models—including neural networks—assume that similar inputs represent similar things; unfortunately, similar IDs may represent totally unrelated tokens, and conversely, distant IDs may represent similar tokens. The neural net would be biased in a weird way, and it would have great difficulty overcoming this bias during training.    One solution is to use one-hot encoding, since all one-hot vectors are equally distant from one another. However, when the vocabulary is large, one-hot vectors are equally large. In our case, the vocabulary contains just 39 characters, so each character would be represented by a 39-dimensional one-hot vector. That’s still manageable, but if we were dealing with words instead of characters, the vocabulary size could be in the tens of thousands, so one-hot encoding would be out of the question. Luckily, since we are dealing with neural networks, we have a better option: embeddings.```", "```py`` ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "``````py```` ```py``````", "``````py``````", "```py```", "````py``` ````", "`````` ```py``````", "``` >>> import torch.nn as nn `>>>` `torch``.``manual_seed``(``42``)` ```", "``` `>>>` `embed``(``torch``.``tensor``([[``3``,` `2``],` `[``0``,` `2``]]))` `` `tensor([[[ 0.2674,  0.5349,  0.8094],`  `[ 2.2082, -0.6380,  0.4617]],`   `[[ 0.3367,  0.1288,  0.2345],`  `[ 2.2082, -0.6380,  0.4617]]], grad_fn=<EmbeddingBackward0>)` `` ```", "```` ```py   ````", "```py ``As you can see, category 3 gets encoded as the 3D vector `[0.2674, 0.5349, 0.8094]`, category 2 gets encoded (twice) as the 3D vector `[2.2082, -0.6380, 0.4617]`, and category 0 gets encoded as the 3D vector `[0.3367, 0.1288, 0.2345]` (categories 1 and 4 were not used in this example). Since the layer is not trained yet, these encodings are just random.    Note that an embedding layer is mathematically equivalent to one-hot encoding followed by a linear layer (with no bias parameter). For example, if you create a linear layer with `nn.Linear(5, 3, bias=False)` and pass it the one-hot vector `torch.tensor([[0., 0., 0., 1., 0.]])`, you get a vector equal to row #3 of the linear layer’s transposed weight matrix (which acts as an embedding matrix). That’s because all rows in the transposed weight matrix get multiplied by zero, except for row #3 which gets multiplied by 1, so the result is just row #3\\. However, it’s much more efficient to use `nn.Embedding(5, 3)` and pass it `torch.tensor([3])`: this looks up row #3 in the embedding matrix without the need for one-hot encoding, and without all the pointless multiplications by zero.    OK, now that you know about embeddings, you are ready to build the Shakespeare model.`` ```", "```py`  ```", "```py```", "```py```", "``` ```", "````` ## Building and Training the Char-RNN Model    Since our dataset is reasonably large, and modeling language is quite a difficult task, we need more than a simple RNN with a few recurrent neurons. Let’s build and train a model with a two-layer `nn.GRU` module (introduced in [Chapter 13](ch13.html#rnn_chapter)), with 128 units per layer, and a bit of dropout. You can try tweaking the number of layers and units later, if needed:    ```py class ShakespeareModel(nn.Module):     def __init__(self, vocab_size, n_layers=2, embed_dim=10, hidden_dim=128,                  dropout=0.1):         super().__init__()         self.embed = nn.Embedding(vocab_size, embed_dim)         self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,                           batch_first=True, dropout=dropout)         self.output = nn.Linear(hidden_dim, vocab_size)      def forward(self, X):         embeddings = self.embed(X)         outputs, _states = self.gru(embeddings)         return self.output(outputs).permute(0, 2, 1)  torch.manual_seed(42) model = ShakespeareModel(len(vocab)).to(device) ```    Let’s go over this code:    *   We use an `nn.Embedding` layer as the first layer, to encode the character IDs. As we just saw, the `nn.Embedding` layer’s number of input dimensions is the number of categories, so in our case it’s the number of distinct character IDs. The embedding size is a hyperparameter you can tune—we’ll set it to 10 for now. Whereas the inputs of the `nn.Embedding` layer will be integer tensors of shape [*batch size*, *window length*], the outputs of the `nn.Embedding` layer will be float tensors of shape [*batch size*, *window length*, *embedding size*].           *   The `nn.GRU` layer has 10 inputs (i.e., the embedding size), 128 outputs (i.e., the hidden size), two layers, and as usual we must specify `batch_first=True` because otherwise the layer assumes that the batch dimension comes after the time dimension.           *   We use an `nn.Linear` layer for the output layer: it must have 39 units because there are 39 distinct characters in the text, and we want to output a logit for each possible character (at each time step).           *   In the `forward()` method, we just call these layers one by one. Note that the `nn.GRU` layer’s output shape is [*batch size*, *window length*, *hidden size*], and the `nn.Linear` layer’s output shape is [*batch size*, *window length*, *vocabulary size*], but as we saw in [Chapter 13](ch13.html#rnn_chapter), the `nn.CrossEntropyLoss` and `Accuracy` modules that we will use for training both expect the class dimension (i.e., `vocab_size`) to be the second dimension, not the last one. This is why we must permute the last two dimensions of the `nn.Linear` layer’s output. Note that the `nn.GRU` layer also returns the final hidden states, but we ignore them.⁠^([5](ch14.html#id3218))              Now you can now train and evaluate the model as usual, using the `nn.CrossEntropyLoss` and the `Accuracy` metric.    And now let’s use our model to predict the next character in a sentence:    ```py model.eval()  # don't forget to switch the model to evaluation mode! text = \"To be or not to b\" encoded_text = encode_text(text).unsqueeze(dim=0).to(device) with torch.no_grad():     Y_logits = model(encoded_text)     predicted_char_id = Y_logits[0, :, -1].argmax().item()     predicted_char = id_to_char[predicted_char_id]  # correctly predicts \"e\" ```    We first encode the text, add a batch dimension of size 1, and move the tensor to the GPU. Then we call our model and get logits for each time step. We’re only interested in logits for the final time step (hence the –1), and we want to know which token ID has the highest logit, so we use `argmax()`. We then use `item()` to extract the token ID from the tensor. Lastly, we convert the token ID to a character, and that’s our prediction.    The model correctly predicts “e”, great! Now let’s use this model to pretend we’re Shakespeare!    ###### Warning    If you are running this code on Colab with a GPU activated, then training will take a few hours. You can reduce the number of epochs if you don’t want to wait that long, but of course the model’s accuracy will probably be lower. If the Colab session times out, make sure to reconnect quickly, or else the Colab runtime will be destroyed.    ## Generating Fake Shakespearean Text    To generate new text using the char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it to the end of the text, then give the extended text to the model to guess the next letter, and so on. This is called *greedy decoding*. But in practice this often leads to the same words being repeated over and over again. Instead, we can sample the next character randomly, using the model’s estimated probability distribution: if the model estimates a probability *p* for a given token, then this token will be sampled with probability *p*. This process will generate more diverse and interesting text since the most likely token won’t always be sampled. To sample the next token, we can use the `torch.multinomial()` function, which samples random class indices, given a list of class probabilities. For example:    ```py >>> torch.manual_seed(42) `>>>` `probs` `=` `torch``.``tensor``([[``0.5``,` `0.4``,` `0.1``]])`  `# probas = 50%, 40%, and 10%` ```` `>>>` `samples` `=` `torch``.``multinomial``(``probs``,` `replacement``=``True``,` `num_samples``=``8``)` ```py `>>>` `samples` `` `tensor([[0, 0, 0, 0, 1, 0, 2, 2]])` `` ``` ```py` ```   ```py```` ```py``` `````", "```py import torch.nn.functional as F  def next_char(model, text, temperature=1):     encoded_text = encode_text(text).unsqueeze(dim=0).to(device)     with torch.no_grad():         Y_logits = model(encoded_text)         Y_probas = F.softmax(Y_logits[0, :, -1] / temperature, dim=-1)         predicted_char_id = torch.multinomial(Y_probas, num_samples=1).item()     return id_to_char[predicted_char_id] ```", "```py def extend_text(model, text, n_chars=80, temperature=1):     for _ in range(n_chars):         text += next_char(model, text, temperature)     return text ```", "```py >>> print(extend_text(model, \"To be or not to b\", temperature=0.01)) `To be or not to be the state` `and the contrary of the state and the sea,` `the common people of the` `>>>` `print``(``extend_text``(``model``,` `\"To be or not to b\"``,` `temperature``=``0.4``))` ```", "```py ```", "```py` ```", "```py ```", "```py`` ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "``````py```` ```py``````", "``````py``````", "```py```", "````py```  ````", "```````py```````", "```````py```` ```py```````", "```````py```````", "`````` ```py``````", "```````py```````", "````` # Sentiment Analysis Using Hugging Face Libraries    One of the most common applications of NLP is text classification—especially sentiment analysis. If image classification on the MNIST dataset is the “Hello, world!” of computer vision, then sentiment analysis on the IMDb reviews dataset is the “Hello, world!” of natural language processing. The IMDb dataset consists of 50,000 movie reviews in English (25,000 for training, 25,000 for testing) extracted from the famous [Internet Movie Database](https://imdb.com), along with a simple binary target for each review indicating whether it is negative (0) or positive (1). Just like MNIST, the IMDb reviews dataset is popular for good reasons: it is simple enough to be tackled on a laptop in a reasonable amount of time, but challenging enough to be fun and rewarding.    To download the IMDb dataset, we will use the Hugging Face *Datasets* library, which gives easy access to hundreds of thousands of datasets hosted on the Hugging Face Hub. It is preinstalled on Colab; otherwise it can be installed using `pip install datasets`. We’ll use 80% of the original training set for training, and the remaining 20% for validation, using the `train_test_split()` method to split the set:    ```py from datasets import load_dataset  imdb_dataset = load_dataset(\"imdb\") split = imdb_dataset[\"train\"].train_test_split(train_size=0.8, seed=42) imdb_train_set, imdb_valid_set = split[\"train\"], split[\"test\"] imdb_test_set = imdb_dataset[\"test\"] ```    Let’s inspect a couple of reviews:    ```py >>> imdb_train_set[1][\"text\"] `\"'The Rookie' was a wonderful movie about the second chances life holds [...]\"` `>>>` `imdb_train_set``[``1``][``\"label\"``]` ```` `1` `>>>` `imdb_train_set``[``16``][``\"text\"``]` ```py `\"Lillian Hellman's play, adapted by Dashiell Hammett with help from Hellman,` `becomes a curious project to come out of gritty Warner Bros. [...] It seems to` `take forever for this drama to find its focus, [...], it seems a little` `patronizing [...] Lukas has several speeches in the third-act which undoubtedly` `won him the Academy Award [...] this tasteful, tactful movie [...] It should be` `a heady mix, but instead it's rather dry-eyed and inert. ** from ****\"` `>>>` `imdb_train_set``[``16``][``\"label\"``]` `` `0` `` ``` ```py` ```   ```py`````", "```````py```````", "```` ```py````", "```py```", "````py````", "```py```", "``````py``````", "```````py`````` The first review immediately says that it’s a wonderful movie; no need to read any further: it’s clearly positive (label = 1). The second review is much harder to classify: it contains a detailed description of the movie, sprinkled with both positive and negative comments. Luckily, the conclusion is quite clearly negative, making the task much easier (label = 0). Still, it’s not a trivial task.    A simple char-RNN model would struggle; we need a more powerful tokenization technique. So let’s focus on tokenization before we return to sentiment analysis.    ## Tokenization Using the Hugging Face Tokenizers Library    In a [2016 paper](https://homl.info/rarewords),⁠^([8](ch14.html#id3241)) Rico Sennrich et al. from the University of Edinburgh explored several methods to tokenize and detokenize text at the subword level. This way, even if your model encounters a rare word it has never seen before, it can still reasonably guess what it means. For example, even if the model never saw the word “smartest” during training, if it learned the word “smart” and it also learned that the suffix “est” means “the most”, it can infer the meaning of “smartest”. One of the techniques the authors evaluated is *byte pair encoding* (BPE), introduced by Philip Gage in 1994 (initially for data compression). BPE works by splitting the whole training set into individual characters, then at each iteration it finds the most frequent pair of adjacent tokens and adds it to the vocabulary. It repeats this process until the vocabulary reaches the desired size.    The [Hugging Face Tokenizers library](https://homl.info/tokenizers) includes highly efficient implementations of several popular tokenization algorithms, including BPE. It is preinstalled on Colab (or you can install it with `pip install tokenizers`). Here’s how to train a BPE model on the IMDb dataset:    ```py import tokenizers  bpe_model = tokenizers.models.BPE(unk_token=\"<unk>\") bpe_tokenizer = tokenizers.Tokenizer(bpe_model) bpe_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace() special_tokens = [\"<pad>\", \"<unk>\"] bpe_trainer = tokenizers.trainers.BpeTrainer(vocab_size=1000,                                              special_tokens=special_tokens) train_reviews = [review[\"text\"].lower() for review in imdb_train_set] bpe_tokenizer.train_from_iterator(train_reviews, bpe_trainer) ```    Let’s walk through this code:    *   We import the Tokenizers library, and we create a BPE model, specifying an unknown token `\"<unk>\"` which will be used later if we try to tokenize some text containing tokens that the model never saw during training: the unknown tokens will be replaced with the `\"<unk>\"` token.           *   We then create a `Tokenizer` based on the BPE model.           *   The Tokenizers library lets you specify optional preprocessing and post-processing steps, and it also provides common preprocessors and postprocessors. In this example, we use the `Whitespace` preprocessor which splits the text at spaces (and drops the spaces), and also separates groups of letters and groups of nonletters. For example “Hello, world!!!” will be split into [\"Hello”, “,”, “world”, “!!!\"]. The BPE algorithm will then run on these individual chunks, which dramatically speeds up training and improves token quality (at least when the text is in English) by providing reasonable word boundaries.           *   We then define a list of special tokens: a padding token `\"<pad>\"` that will come in handy when we create batches of texts of different lengths, and the unknown token we have already discussed.           *   We create a `BpeTrainer`, specifying the maximum vocabulary size and the list of special tokens. The trainer will add the special tokens at the beginning of the vocabulary, so `\"<pad>\"` will be token 0, and `\"<unk>\"` will be token 1.           *   Next we create a list of all the text in the IMBd training set.           *   Lastly, we train the tokenizer on this list, using the `BpeTrainer`. A few seconds later, the BPE tokenizer is ready to be used!              Now let’s use our BPE tokenizer to tokenize some text:    ```py >>> some_review = \"what an awesome movie!   >>> bpe_encoding = bpe_tokenizer.encode(some_review) >>> bpe_encoding Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets,          attention_mask, special_tokens_mask, overflowing])  ```    The `encode()` method returns an `Encoding` object that contains eight tokens. Let’s look at these tokens and their IDs:    ```py >>> bpe_encoding.tokens `['what', 'an', 'aw', 'es', 'ome', 'movie', '!', '<unk>']` `>>>` `bpe_token_ids` `=` `bpe_encoding``.``ids` ``` `>>>` `bpe_token_ids` `` `[303, 139, 373, 149, 240, 211, 4, 1]` `` ```py ```   ```py```````", "````` ```py`````", "```` ```py````", "```py```", "``` >>> bpe_tokenizer.decode(bpe_token_ids) `'what an aw es ome movie !'` ```", "``````py``````", "```py >>> bpe_encoding.offsets `[(0, 4), (5, 7), (8, 10), (10, 12), (12, 15), (16, 21), (21, 22), (23, 24)]` ```", "```py```", "````py` It’s also possible to encode a whole batch of strings at once. For example, let’s encode the first three reviews of the training set:    ``` >>> bpe_tokenizer.encode_batch(train_reviews[:3]) `[Encoding(num_tokens=281, attributes=[ids, type_ids, tokens, [...]]),`  `Encoding(num_tokens=114, attributes=[ids, type_ids, tokens, [...]]),`  `Encoding(num_tokens=285, attributes=[ids, type_ids, tokens, [...]]),` ```py   ````", "```` If we want to create a single integer tensor containing the token IDs of all three reviews, we must first ensure that they all have the same number of tokens, which is not the case right now. For this, we can ask the tokenizer to pad the shorter reviews with the padding token ID until they are as long as the longest review in the batch. We can also ask the tokenizer to truncate any sequence longer than some maximum length, since RNNs don’t handle very long sequences very well anyway:    ```py bpe_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\") bpe_tokenizer.enable_truncation(max_length=500) ```    Now let’s encode the batch again. This time all sequences will have the same number of tokens, so we can create a tensor containing all the token IDs:    ```py >>> bpe_encodings = bpe_tokenizer.encode_batch(train_reviews[:3]) `>>>` `bpe_batch_ids` `=` `torch``.``tensor``([``encoding``.``ids` `for` `encoding` `in` `bpe_encodings``])` ``` `>>>` `bpe_batch_ids` `` `tensor([[159, 402, 176, 246,  61, [...], 215, 156, 586,   0,   0,   0,   0],`  `[ 10, 138, 198, 289, 175, [...],   0,   0,   0,   0,   0,   0,   0],`  `[289,  15, 209, 398, 177, [...],  50,  29,  22,  17,  24,  18,  24]])` `` ```py ```   ```py````", "```py```", "```py```", "```py >>> attention_mask = torch.tensor([encoding.attention_mask `... `                               `for` `encoding` `in` `bpe_encodings``])` ```", "```py `...` ```", "```py` `tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 1, 1, 1, 0, 0, 0, 0],`  `[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 0, 0, 0, 0, 0, 0, 0],`  `[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 1, 1, 1, 1, 1, 1, 1]])` `>>>` `lengths` `=` `attention_mask``.``sum``(``dim``=-``1``)` ```", "```py ```", "```py`` ```", "```py ```", "```py` ```", "```py ```", "```py```", "```py```", "```py```", "``` ```", "```````py ``````py````` ```py```````", "``````py``````", "``````py``````", "``````py``````", "``````py``````", "```````py````` ```py```````", "```````py```````", "``````py``````", "```````py``` ## Reusing Pretrained Tokenizers    To download a pretrained tokenizer, we will use the [Hugging Face *Transformers library*](https://homl.info/transformerslib). This library provides many popular models for NLP, computer vision, audio processing, and more. Pretrained weights are available for almost all of these models, and the library can automatically download them from the Hugging Face Hub. The models were originally all based on the *Transformer architecture* (which we will discuss in detail in [Chapter 15](ch15.html#transformer_chapter)), hence the name of the library, but other kinds of models are now available as well, such as CNNs. Lastly, each model comes with all the tools it needs, including tokenizers for NLP models: in a single line of code, you can have a fully functional, high-performance model for a given task, as we will see later in this chapter.    For now, let’s just grab the pretrained tokenizer from some NLP model. For example, the following code downloads the pretrained BBPE tokenizer used by the GPT-2 model (a text generation model), and it uses this tokenizer to encode the first 3 IMDb reviews, truncating the encoded sequences if they exceed 500 tokens:    ``` import transformers  gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\") gpt2_encoding = gpt2_tokenizer(train_reviews[:3], truncation=True,                                max_length=500) ```py    Notice that we use the tokenizer object like a function. The result is a dictionary-like object of type `BatchEncoding`. You can get the token IDs using the `\"input_ids\"` key. It returns a Python list of lists of token IDs. For example, let’s look at the first 10 token IDs of the first encoded review, and use the tokenizer to decode them, using its `decode()` method:    ``` >>> gpt2_token_ids = gpt2_encoding[\"input_ids\"][0][:10] `>>>` `gpt2_token_ids` ```py `[14247, 35030, 1690, 423, 257, 1688, 8046, 13, 484, 1690]` `>>>` `gpt2_tokenizer``.``decode``(``gpt2_token_ids``)` `` `'stage adaptations often have a major fault. they often'` `` ``` ```py   ``````py`` ``````py` ``````py If you would prefer to use a pretrained WordPiece tokenizer, you can reuse the tokenizer of any LLM that was pretrained using WordPiece, such as BERT (another popular NLP model, which stands for Bidirectional Encoder Representations from Transformers). This tokenizer has a padding token (unlike the previous tokenizer, since GPT-2 didn’t need it), so we can specify `padding=True` when encoding a batch of reviews: as usual, the shortest texts will be padded to the length of the longest one using the padding token. This allows us to also specify `return_tensors=\"pt\"` to get a PyTorch tensor instead of a Python list of lists of token IDs: very convenient! So let’s encode the first three IMDb reviews:    ``` bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\") bert_encoding = bert_tokenizer(train_reviews[:3], padding=True,                                truncation=True, max_length=500,                                return_tensors=\"pt\") ```py    ###### Tip    The name `\"bert-base-uncased\"` refers to a *model checkpoint*: this particular checkpoint is a case-insensitive BERT model, pretrained on English text. Other checkpoints are available, such as `\"bert-large-cased\"` if you want a larger and case-sensitive BERT model, or `\"bert-base-multilingual-uncased\"` if you want an uncased model pretrained on over 100 languages. For now we are just using the model’s tokenizer.    The resulting token IDs and attention masks are nicely padded tensors:    ``` >>> bert_encoding[\"input_ids\"] `tensor([[ 101, 2754,17241, 2411, 2031, [...],  102, 0, 0, 0, [...], 0, 0, 0],`  `[ 101, 1005, 1996, 8305, 1005, [...],  102, 0, 0, 0, [...], 0, 0, 0],`  `[ 101, 7929, 1010, 2021, 2515, [...], 1012,  1019,  1013,  1019,  102]])` `>>>` `bert_encoding``[``\"attention_mask\"``]` `` `tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 0, 0, 0, 0, 0, 0, 0, 0, 0],`  `[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 0, 0, 0, 0, 0, 0, 0, 0, 0],`  `[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 1, 1, 1, 1, 1, 1, 1, 1, 1]])` `` ```py   ````` ```py`Notice that each token ID sequence starts with token 101 ([CLS]), and ends with token 102 ([SEP]) (ignoring padding tokens). These tokens are needed by the BERT model (as we will see in [Chapter 15](ch15.html#transformer_chapter)), but unless your model needs them too, you can drop them by setting `add_special_tokens=False` when calling the tokenizer.    What about a pretrained Unigram LM tokenizer? Well, many models were trained using Unigram LM, such as ALBERT, T5, or XML-R models, just to name a few. For example:    ``` albert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"albert-base-v2\") albert_encoding = albert_tokenizer(train_reviews[:3], padding=True, [...]) ```py    The Transformers library also provides an object that can wrap your own tokenizer (from the Tokenizers library) and give it the same API as the pretrained tokenizers (from the Transformers library). For example, let’s wrap the BPE tokenizer we trained earlier:    ``` hf_tokenizer = transformers.PreTrainedTokenizerFast(     tokenizer_object=bpe_tokenizer) hf_encodings = hf_tokenizer(train_reviews[:3], padding=True, [...]) ```py    With that, we have all the tokenization tools we need, so let’s go back to sentiment analysis.```` ```py`` ``````py ``````py` ``````py``  ``````py```````", "```````py`` ``````py```````", "```````py` ``````py```````", "```````py ## Building and Training a Sentiment Analysis Model    Our sentiment analysis model must be trained using batches of tokenized reviews. However, the datasets we created did not take care of tokenization. One option would be to update them (e.g., using the `map()` method), but it’s just as simple to handle tokenization in the data loaders. To do this, we can pass a function to the `DataLoader` constructor using its `collate_fn` argument: the data loader will call this function for every batch, passing it a list of dataset samples. Our function will take this batch, tokenize the reviews, truncate and pad them if needed, and return a `BatchEncoding` object containing PyTorch tensors for the token IDs and attention masks, along with another tensor containing the labels. For tokenization, we will simply use the pretrained WordPiece tokenizer we just loaded:    ``` def collate_fn(batch, tokenizer=bert_tokenizer):     reviews = [review[\"text\"] for review in batch]     labels = [[review[\"label\"]] for review in batch]     encodings = tokenizer(reviews, padding=True, truncation=True,                           max_length=200, return_tensors=\"pt\")     labels = torch.tensor(labels, dtype=torch.float32)     return encodings, labels  batch_size = 256 imdb_train_loader = DataLoader(imdb_train_set, batch_size=batch_size,                                collate_fn=collate_fn, shuffle=True) imdb_valid_loader = DataLoader(imdb_valid_set, batch_size=batch_size,                                collate_fn=collate_fn) imdb_test_loader = DataLoader(imdb_test_set, batch_size=batch_size,                               collate_fn=collate_fn) ```py    Now we’re ready to create our sentiment analysis model:    ``` class SentimentAnalysisModel(nn.Module):     def __init__(self, vocab_size, n_layers=2, embed_dim=128, hidden_dim=64,                  pad_id=0, dropout=0.2):         super().__init__()         self.embed = nn.Embedding(vocab_size, embed_dim,                                   padding_idx=pad_id)         self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,                           batch_first=True, dropout=dropout)         self.output = nn.Linear(hidden_dim, 1)      def forward(self, encodings):         embeddings = self.embed(encodings[\"input_ids\"])         _outputs, hidden_states = self.gru(embeddings)         return self.output(hidden_states[-1]) ```py    As you can see, this model is very similar to our Shakespeare model, but with a few important differences:    *   When creating the `nn.Embedding` layer, we set its `padding_idx` argument to our padding ID. This ensures that the padding ID gets embedded as a nontrainable zero vector to reduce the impact of padding tokens on the loss.           *   Since this is a sequence-to-vector model, not a sequence-to-sequence model, we only need the last output of the top GRU layer to make our final prediction (through the output `nn.Linear` layer). We could have used `outputs[:, -1]` instead of `hidden_states[-1]`, as they are equal.           *   The output `nn.Linear` layer has a single output dimension because it’s a binary classification model. The final output will be a 2D tensor with a single column containing one logit per review, positive for positive reviews, and negative for negative reviews.           *   The `forward()` method takes a `BatchEncoding` object as input, containing the token IDs (possibly padded and truncated).              We can then train this model using the `nn.BCEWithLogitsLoss` since this is a binary classification task. It reaches close to 85% accuracy on the validation set, which is reasonably good, although the best models reach human level, slightly above 90% accuracy. It’s probably not possible to go much higher than that; because many reviews are ambiguous, classifying them feels like flipping a coin.    One problem with our model is the fact that we are not fully ignoring the padding tokens. Indeed, if a review ends with many padding tokens, the `nn.GRU` module will have to process them, and by the time it gets through all of them, it might have forgotten what the review was all about. To avoid this, we can use a *packed sequence* instead of a regular tensor. A packed sequence is a special data structure designed to efficiently represent a batch of sequences of variable lengths.⁠^([12](ch14.html#id3281)) You can use the `pack_padded_sequence()` function to convert a tensor containing padded sequences to a packed sequence object, and conversely you can use the `pad_packed_sequence()` function whenever you want to convert a packed sequence object to a padded tensor:    ``` >>> from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence `>>>` `sequences` `=` `torch``.``tensor``([[``1``,` `2``,` `0``,` `0``],` `[``5``,` `6``,` `7``,` `8``]])` ```py````` `>>>` `packed` `=` `pack_padded_sequence``(``sequences``,` `lengths``=``(``2``,` `4``),` ```py```` `... `                              `enforce_sorted``=``False``,` `batch_first``=``True``)` ```py``` `...` ````` `>>>` `packed` ```py` `PackedSequence(data=tensor([5, 1, 6, 2, 7, 8]), [...])` `>>>` `padded``,` `lengths` `=` `pad_packed_sequence``(``packed``,` `batch_first``=``True``)` ``` `>>>` `padded``,` `lengths` `` `(tensor([[1, 2, 0, 0],`  `[5, 6, 7, 8]]),`  `tensor([2, 4]))` `` ```py ```` ```py`` ``````py ``````py` ``````py`` ```   ```py``` ````` ```py`By default, the `pack_padded_sequence()` function assumes that the sequences in the batch are ordered from the longest to the shortest. If this is not the case, you must set `enforce_sorted=False`. Moreover, the function also assumes that the time dimension comes before the batch dimension. If the batch dimension is first, you must set `batch_first=True`.    PyTorch’s recurrent layers support packed sequences: they efficiently process the sequences, stopping at the end of each sequence. So let’s update our sentiment analysis model to use packed sequences. In the `forward()` method, just replace the `self.gru(embeddings)` line with the following code:    ``` lengths = encodings[\"attention_mask\"].sum(dim=1) packed = pack_padded_sequence(embeddings, lengths=lengths.cpu(),                               batch_first=True, enforce_sorted=False) _outputs, hidden_states = self.gru(packed) ```py    This code starts by computing the length of each sequence in the batch, just like we did earlier, then it packs the embeddings tensor and passes the packed sequence to the `nn.GRU` module. With that, the model will properly handle sequences without being bothered by any padding tokens. You don’t actually need to set `padding_idx` anymore when creating the `nn.Embedding` layer, but it doesn’t hurt, and it makes debugging a bit easier, so I prefer to keep it.    Another way to improve our model is to let it look at the review in both directions: left to right, and right to left. Let’s see how this works.    ###### Note    If you pass a packed sequence to an `nn.GRU` module, its outputs will also be a packed sequence, and you will need to convert it back to a padded tensor before you can pass it to the next layers. Luckily, we don’t need these outputs for our sentiment analysis model, only the hidden states.```` ```py`` ``````py  ``````py```````", "`````` ```py``````", "```````py` ``````py```````", "```` ## Bidirectional RNNs    At each time step, a regular recurrent layer only looks at past and present inputs before generating its output. In other words, it is *causal*, meaning it cannot look into the future. This type of RNN makes sense when forecasting time series, or in the decoder of a sequence-to-sequence (seq2seq) model. But for tasks like text classification, or in the encoder of a seq2seq model, it is often preferable to look ahead at the next words before encoding a given word.    For example, consider the phrases “the right arm”, “the right person”, and “the right to speak”: to properly encode the word “right”, you need to look ahead. One solution is to run two recurrent layers on the same inputs, one reading the words from left to right and the other reading them from right to left, then combine their outputs at each time step, typically by concatenating them. This is what a *bidirectional recurrent layer* does (see [Figure 14-4](#bidirectional_rnn_diagram)).  ![Diagram illustrating a bidirectional recurrent layer, showing inputs processed in both directions and outputs combined.](assets/hmls_1404.png)  ###### Figure 14-4\\. A bidirectional recurrent layer    To make our sentiment analysis model bidirectional, we can just set `bidirectional=True` when creating the `nn.GRU` layer (this also works with the `nn.RNN` and `nn.LSTM` modules).    However, once we do that, we must adjust our model a bit. In particular, we must double the input dimension of the output `nn.Linear` layer, since the hidden states will double in size:    ```py self.output = nn.Linear(2 * hidden_dim, 1) ```    We must also concatenate the forward and backward hidden states of the GRU’s top layer before passing the result to the output layer. For this, we can replace the last line of the `forward()` method (i.e., `return self.output(hidden_states[-1])`) with the following code:    ```py n_dims = self.output.in_features top_states = hidden_states[-2:].permute(1, 0, 2).reshape(-1, n_dims) return self.output(top_states) ```    Let’s see how the middle line works:    *   Until now, the shape of the hidden states returned by the `nn.GRU` module was [*number of layers*, *batch size*, *hidden size*], so [2, 256, 64] in our case. But when we set `bidirectional=True`, we doubled the first dimension size, so we now have a shape of [4, 256, 64]: the tensor contains the hidden states for layer 1 forward, layer 1 backward, layer 2 forward, and layer 2 backward. Since we only want the top layer’s hidden states, both forward and backward, we must get `hidden_states[-2:]`.           *   We also need to concatenate the forward and backward states. One way to do this is to permute the first two dimensions of the top hidden states using `permute(1, 0, 2)` to get the shape [256, 2, 64], then reshape the result using `reshape(-1, n_dims)` (where `n_dims` equals 128) to get the desired shape: [256, 2 * 64].              ###### Note    In this model we only use the last hidden states, ignoring the outputs at each time step. If you ever want to use the outputs of a bidirectional module, be aware that its last dimension’s size will be doubled.    You can try training this model, but you will not see any improvement in this case, because the first model actually overfit the training set, and this new version makes it even worse: it reaches over 99% accuracy on the training set, but just 84% on the validation set. To fix this, you could try to regularize the model a bit more, reduce the size of the model, or increase the size of the training set.    But let’s instead try something different: using pretrained embeddings.    ## Reusing Pretrained Embeddings and Language Models    Our model was able to learn useful embeddings for thousands of tokens, based on just 25,000 movie reviews: that’s quite impressive! Imagine how good the embeddings would be if we had billions of reviews to train on. The good news is that we can reuse word embeddings even when they were trained on some other (very) large text corpus, even if it was not composed of movie reviews, and even if they were not trained for sentiment analysis. After all, the word “amazing” generally has the same meaning whether you use it to talk about movies or anything else.    Since we used pretrained tokens for the BERT model, we might as well try using its embedding layer. First, we need to download the pretrained model using the `AutoModel.from_pretrained()` function from the Transformers library, then we can directly access its embeddings layer:    ```py >>> bert_model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\") `>>>` `bert_model``.``embeddings``.``word_embeddings` `` `Embedding(30522, 768, padding_idx=0)` `` ```   ```py````", "```py```", "```py class SentimentAnalysisModelPreEmbeds(nn.Module):     def __init__(self, pretrained_embeddings, n_layers=2, hidden_dim=64,                  dropout=0.2):         super().__init__()         weights = pretrained_embeddings.weight.data         self.embed = nn.Embedding.from_pretrained(weights, freeze=True)         embed_dim = weights.shape[-1]         [...]  # the rest of the model is exactly like earlier  imdb_model_bert_embeds = SentimentAnalysisModelPreEmbeds(     bert_model.embeddings.word_embeddings).to(device) ```", "```py >>> bert_encoding = bert_tokenizer(train_reviews[:3], padding=True, `... `                               `max_length``=``200``,` `truncation``=``True``,` ```", "```py `... `                               `return_tensors``=``\"pt\"``)` ```", "```py` `>>>` `bert_output` `=` `bert_model``(``**``bert_encoding``)` ```", "```py ```", "```py`` ```", "```py ```", "```py```", "````` ```py`BERT’s output includes an attribute named `last_hidden_state`, which contains contextualized embeddings for each token. The word “last” in this case refers to the last layer, not the last time step (BERT is a transformer, not an RNN). This `last_hidden_state` tensor has a shape of [*batch size*, *max sequence length*, *hidden size*]. Let’s use these contextualized embeddings in a sentiment analysis model:    ``` class SentimentAnalysisModelBert(nn.Module):     def __init__(self, n_layers=2, hidden_dim=64, dropout=0.2):         super().__init__()         self.bert = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")         embed_dim = self.bert.config.hidden_size         self.gru = nn.GRU(embed_dim, hidden_dim, [...])         self.output = nn.Linear(hidden_dim, 1)      def forward(self, encodings):         contextualized_embeddings = self.bert(**encodings).last_hidden_state         lengths = encodings[\"attention_mask\"].sum(dim=1)         packed = pack_padded_sequence(contextualized_embeddings, [...])         _outputs, hidden_states = self.gru(packed)         return self.output(hidden_states[-1]) ```py    Note that we don’t need to make the `nn.GRU` module bidirectional since the contextualized embeddings already looked ahead.    If you freeze the BERT model (e.g., using `model.bert.requires_grad_(False)`) and train the rest of the model, you will notice a significant performance boost, reaching over 88% accuracy. Wonderful!    Another option is to use only the contextualized embedding for the very first token, which is the *class token* [CLS]. Indeed, during pretraining, the BERT model had to perform a text classification task based solely on this token’s contextualized embedding (we will discuss BERT pretraining in more detail in [Chapter 15](ch15.html#transformer_chapter)). As a result, it learned to summarize the most important features of the text into this embedding. This simplifies our model quite a bit, since we can get rid of the `nn.GRU` module altogether, and the `forward()` method becomes much shorter:    ``` def forward(self, encodings):     bert_output = self.bert(**encodings)     return self.output(bert_output.last_hidden_state[:, 0]) ```py    In fact, the BERT model contains an extra hidden layer on top of the class embedding, composed of an `nn.Linear` module and an `nn.Tanh` module. This hidden layer is called the *pooler*. To use it, just replace `bert_output.last_hidden_state[:, 0]` with `bert_output.pooler_output`. You may also want to unfreeze the pooler after a few epochs to fine-tune it for the IMDb task.    So we started by reusing only the pretrained tokenizer, then we reused the pretrained embeddings, then most of the pretrained BERT model, and finally the full model, adding only an `nn.Linear` layer on top of the pooler. We can actually go one step further and just use an off-the-shelf class for sentence classification.```` ```py`` `````", "``````py` ``````", "``````py``````", "``` ```", "```py```", "````py`` ````", "```````py` ## Task-Specific Classes    To tackle our binary classification task using BERT, we can use the `BertForSequenceClassification` class provided by the Transformers library. It’s just a BERT model plus a classification head on top. All you need to do to create this model is specify the pretrained BERT checkpoint you want to use, the number of output units for your classification task, and optionally the data type (we’ll use 16-bit floats to fit on small GPUs):    ``` from transformers import BertForSequenceClassification  torch.manual_seed(42) bert_for_binary_clf = BertForSequenceClassification.from_pretrained(     \"bert-base-uncased\", num_labels=2, dtype=torch.float16).to(device) ```py    ###### Tip    The Transformers library contains many task-specific classes based on various pretrained models, such as `BertForQuestionAnswering` or `RobertaForSequenceClassification` (see [Chapter 15](ch15.html#transformer_chapter)). You can also use `AutoModelForSequenceClassification` to let the library pick the right class for you, based on the requested model checkpoint (e.g., if you ask for `\"bert-base-uncased\"`, you will get an instance of `BertForSequenceClassification`). Similar `AutoModelFor[...]` classes are available for other tasks.    Until now we have always used a single output for binary classification, so why did we set `num_labels=2`? Well, for simplicity Hugging Face prefers to treat binary classification exactly like multiclass classification, so this model will output two logits instead of one, and it must be trained using the `nn.CrossEntropyLoss` instead of `nn.BCELoss` or `nn.BCEWithLogitsLoss`. If you want to convert the logits to estimated probabilities, you must use `torch.softmax()` rather than `torch.sigmoid()`.    Let’s call this model on a very positive review:    ``` >>> encoding = bert_tokenizer([\"This was a great movie!\"]) `>>>` `with` `torch``.``no_grad``():` ```py````` `... `  `output` `=` `bert_for_binary_clf``(` ```py```` `... `    `input_ids``=``torch``.``tensor``(``encoding``[``\"input_ids\"``],` `device``=``device``),` ```py``` `... `    `attention_mask``=``torch``.``tensor``(``encoding``[``\"attention_mask\"``],` `device``=``device``))` ````` `...` ```py` `>>>` `output``.``logits` ``` `tensor([[-0.0120,  0.6304]], device='cuda:0', dtype=torch.float16)` `>>>` `torch``.``softmax``(``output``.``logits``,` `dim``=-``1``)` `` `tensor([[0.3447, 0.6553]], device='cuda:0', dtype=torch.float16)` `` ```py ```` ```py`` ``````py ``````py` ``````py`` ```   ```py```````", "``````py``` ``````", "``` >>> with torch.no_grad(): `... `  `output` `=` `bert_for_binary_clf``(` ```", "```` `... `    `input_ids``=``torch``.``tensor``(``encoding``[``\"input_ids\"``],` `device``=``device``),` ```py``` `... `    `attention_mask``=``torch``.``tensor``(``encoding``[``\"attention_mask\"``],` `device``=``device``),` ````", "```py` `...` ```", "```py ```", "```py`` ```", "```py ```", "```py` ```", "```py` ```", "```py ```", "```py```", "```py```", "``` ```", "```````py  ``````py```````", "``````py``````", "```py```", "````py` ## The Trainer API    The Trainer API lets you fine-tune a model on your own dataset with very little boilerplate code. It can save model checkpoints during training, apply early stopping, distribute the computations across GPUs, log metrics, take care of padding, batching, shuffling, and more. Let’s use the Trainer API to train our IMDb model.    The Trainer API works directly with dataset objects, not data loaders, but it expects the datasets to contain tokenized text, not strings, so we must take care of tokenization. We can do this quite simply using the dataset’s `map()` method (this method is implemented by the Datasets library; it’s not available on pure PyTorch datasets):    ``` def tokenize_batch(batch):     return bert_tokenizer(batch[\"text\"], truncation=True, max_length=200)  tok_imdb_train_set = imdb_train_set.map(tokenize_batch, batched=True) tok_imdb_valid_set = imdb_valid_set.map(tokenize_batch, batched=True) tok_imdb_test_set = imdb_test_set.map(tokenize_batch, batched=True) ```py    Since we set `batched=True`, the `map()` method passes batches of reviews to the `tokenize_batch()` method: this is optional, but it significantly speeds up this preprocessing step. The `tokenize_batch()` method tokenizes the given batch of reviews, and the resulting fields are added to each instance by the `map()` method. This includes fields such as `token_ids` and `attention_mask`, which the model expects.    To evaluate our model, we can write a simple function that takes an object with two attributes: `label_ids` and `predictions`:    ``` def compute_accuracy(pred):     return {\"accuracy\": (pred.label_ids == pred.predictions.argmax(-1)).mean()} ```py    ###### Tip    Alternatively, you can use metrics provided by the Hugging Face *Evaluate library*: they are designed to work nicely with the Transformers library. Alternatively, although the Trainer API does not support the streaming metrics from the TorchMetrics library, you can still use them if you wrap them inside a function.    Next, we must specify our training configuration in a `TrainingArguments` object:    ``` from transformers import TrainingArguments  train_args = TrainingArguments(     output_dir=\"my_imdb_model\", num_train_epochs=2,     per_device_train_batch_size=128, per_device_eval_batch_size=128,     eval_strategy=\"epoch\", logging_strategy=\"epoch\", save_strategy=\"epoch\",     load_best_model_at_end=True, metric_for_best_model=\"accuracy\",     report_to=\"none\") ```py    We specify that the logs and model checkpoints must be saved in the `my_imdb_model` directory; training should run for 2 epochs (you can increase this if you want); the batch size is 128 for both training and evaluation (you can tweak this depending on the amount of VRAM you have); we want evaluation, logging, and saving to take place at the end of each epoch; and the best model should be loaded at the end of training based on the validation accuracy. Lastly, the `report_to` argument lets you specify one or more tools that the training code will report logs to, such as TensorBoard or [Weights & Biases (W&B)](https://wandb.ai). This can be useful to visualize the learning curves. For simplicity, I set `report_to=\"none\"` to turn reporting off.    Lastly, we create a `Trainer` object and pass it the model, along with the training arguments, the training and validation sets, the evaluation function, plus a data collator which will take care of padding. Finally, we call the trainer’s `train()` method, and we’re done! The model reaches about 90% accuracy on the validation set after just two epochs:    ``` from transformers import DataCollatorWithPadding, Trainer  trainer = Trainer(     bert_for_binary_clf, train_args, train_dataset=tok_imdb_train_set,     eval_dataset=tok_imdb_valid_set, compute_metrics=compute_accuracy,     data_collator=DataCollatorWithPadding(bert_tokenizer)) train_output = trainer.train() ```py    Great, you now know how to download a pretrained model like BERT and fine-tune it on your own dataset! But what if you don’t have a dataset at all, and you just want to use a pretrained model that was already fine-tuned for sentiment analysis? For this, you can use the *pipelines API*.    ## Hugging Face Pipelines    The Transformers library provides a very convenient API to download and use pretrained pipelines for various tasks. Each pipeline contains a pretrained model along with its corresponding preprocessing and post-processing modules. For example let’s create a sentiment analysis pipeline and run it on the first 10 IMDb reviews in the training set:    ``` >>> from transformers import pipeline `>>>` `model_name` `=` `\"distilbert-base-uncased-finetuned-sst-2-english\"` ```py``` `>>>` `classifier_imdb` `=` `pipeline``(``\"sentiment-analysis\"``,` `model``=``model_name``,` ````", "```py` `...` ```", "```py ```", "```py`` ```", "```py ```", "```py```", "````py ````", "``` ```", "```py`` Well, it could hardly be any easier, could it? Just create a pipeline by specifying the task and the model to use, and a couple of other parameters, depending on the task, and off you go! In this example, each review gets a `\"POSITIVE\"` or `\"NEGATIVE\"` label, along with a score equal to the model’s estimated probability for that label. This particular model actually reaches 88.2% accuracy on the validation set, which is reasonably good. Here a few points to note:    *   If you don’t specify a model, the `pipeline()` function will use the default model for the chosen task. For sentiment analysis, at the time of writing, it’s the model we chose: it’s a DistilBERT model—a scaled down version of BERT—with an uncased tokenizer, trained on the English Wikipedia and a corpus of English books, and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task.           *   The pipeline automatically uses the GPU if you have one. If you have several GPUs, you can specify which one to use by setting the pipeline’s `device` argument to the GPU index.           *   The models from the Transformers library are always in evaluation mode by default (no need to call `eval()`).           *   The score is for the chosen label, not for the positive class. In particular, since this is a binary classification task, the score cannot be lower than 0.5 (or else the model would have picked the other label).              The model we chose is well-suited for general-purpose sentiment analysis, such as movie reviews, but other models are better suited for specific use cases, such as social media posts (e.g., trained on a large dataset of tweets, then fine-tuned on a sentiment analysis dataset). To find the best model for your use case, you can search the list of available models on the [*Hugging Face Hub*](https://huggingface.co/models). However, there are over 80,000 models available in the “text-classification” category alone, so you will need to use the filters to narrow down the options. In particular, start by filtering on the task, and sort by trending or most-liked models. You can also continue to filter by language and dataset, if necessary. Prefer models from reputable sources (e.g., models from users huggingface, facebook, google, cardiffnlp, and so on), and if the model includes executable code, make absolutely sure you trust the user (and if you do, set `trust_remote_code=True` when calling the `pipeline()` function).    There are many text classification tasks other than sentiment analysis. For example, a model fine-tuned on the multi-genre natural language inference (MultiNLI) dataset can classify a pair of texts (each ending with a separation token [SEP]) into three classes: contradiction (if the texts contradict each other), entailment (if the first text entails the second), or neutral otherwise. For example:    ```", "```py```", "```py```", "````` `... `    `\"Alice just woke up. [SEP] Alice is awake. [SEP]\"``,` ```py` `... `    `\"I like dogs. [SEP] Everyone likes dogs. [SEP]\"``])` ``` `...` `` `[{'label': 'contradiction', 'score': 0.9717152714729309},`  `{'label': 'entailment', 'score': 0.9119168519973755},`  `{'label': 'neutral', 'score': 0.9509281516075134}]` `` ```py ```` ```py`` `````", "``````py` ```   ```py `` `Many other NLP tasks are also available via the pipeline API, such as question answering, summarization, sentence similarity, text generation, token classification, translation, and more. And it doesn’t stop there! There are also many computer vision tasks, such as image classification, image segmentation, object detection, image-to-text, text-to-image, depth estimation, and even audio tasks, such as audio classification, speech-to-text, text-to-speech, and so on. Make sure to check out the full list at [*https://huggingface.co/tasks*](https://huggingface.co/tasks).    ###### Warning    Before you download a model, make sure you trust the hosting platform (e.g., the Hugging Face Hub) and the model’s author: the model may contain executable code, which could be malicious. It could also produce biased outputs, or it may have been trained with copyrighted or sensitive private data which might be leaked to your users, or it might even have *poisoned weights* which could make it produce harmful content (e.g., propaganda) only for some types of inputs, otherwise behaving normally.    Time to step back. So far we have looked at text generation using a char-RNN, and sentiment analysis using various subword tokenization methods, pretrained embeddings, and even entire pretrained models. Along the way, we discussed embeddings, tokenizers, and the Hugging Face libraries. In the next section, we will explore another important NLP task: *neural machine translation* (NMT). Specifically, we will build an encoder-decoder model capable of translating English to Spanish, and we will see how to boost its performance using beam search and attention mechanisms. ¡Vamos!` `` ``` ```py````` ```py``````", "```py```", "````py ````", "````` ```py`````", "``````py``````", "``````py``````", "``````py``````", "``````py``````", "``` ```", "```py```", "````py````", "```py```", "````py````", "```py` ```", "```py```", "````py````", "```py```", "````py````", "```py```", "``````py``````", "```````py` ``````py```````", "```````py`` ``````py```````", "```````py``` ``````py```````", "```````py```` ```py```````", "```````py```````", "``````py``````", "```````py`````` ```py```````", "```````py```````", "``` ```", "```py```", "````py````", "```py```", "````py`  ````", "`````` ```py``````", "``````py```` # An Encoder-Decoder Network for Neural Machine Translation    Let’s begin with a relatively simple [sequence-to-sequence NMT model](https://homl.info/nmtmodel)⁠^([15](ch14.html#id3354)) that will translate English text to Spanish (see [Figure 14-5](#machine_translation_diagram)).  ![Diagram of an encoder-decoder network illustrating a sequence-to-sequence model for translating \"I like soccer\" from English to Spanish as \"Me gusta el fútbol.\"](assets/hmls_1405.png)  ###### Figure 14-5\\. A simple machine translation model    In short, the architecture is as follows: English texts are fed as inputs to the encoder, and the decoder outputs the Spanish translations. Note that the Spanish translations are also used as inputs to the decoder during training, but shifted back by one step. In other words, during training the decoder is given as input the token that it *should* have output at the previous step, regardless of what it actually output. This is called *teacher forcing*—a technique that significantly speeds up training and improves the model’s performance. For the very first token, the decoder is given the start-of-sequence (SoS, a.k.a. beginning-of-sequence, BoS) token (`\"<s>\"`), and the decoder is expected to end the text with an end-of-sequence (EoS) token (`\"</s>\"`).    Each token is initially represented by its ID (e.g., `4553` for the token “soccer”). Next, an `nn.Embedding` layer returns the token embedding. These token embeddings are then fed to the encoder and the decoder.    At each step, the decoder’s dense output layer (i.e., an `nn.Linear` layer) outputs a logit score for each token in the output vocabulary (i.e., Spanish). If you pass these logits through the softmax function, you get an estimated probability for each possible token. For example, at the first step the word “Me” may have a probability of 7%, “Yo” may have a probability of 1%, and so on. This is very much like a regular classification task, and indeed we will train the model using the `nn.CrossEntropyLoss`, much like we did in the char-RNN model.    Note that at inference time (after training), you will not have the target text to feed to the decoder. Instead, you need to feed it the word that it has just output at the previous step, as shown in [Figure 14-6](#inference_decoder_diagram) (this will require an embedding lookup that is not shown in the diagram).  ![Diagram showing a sequence of recurrent neural network cells where each cell outputs a token that is fed as input to the next cell during inference.](assets/hmls_1406.png)  ###### Figure 14-6\\. At inference time, the decoder is fed as input the word it just output at the previous time step    ###### Tip    In a [2015 paper](https://homl.info/scheduledsampling),⁠^([16](ch14.html#id3359)) Samy Bengio et al. proposed gradually switching from feeding the decoder the previous *target* token to feeding it the previous *output* token during training.    Let’s build and train this model! First, we need to download a dataset of English/Spanish text pairs. For this, we will use the Datasets library to download English/Spanish pairs from the *Tatoeba Challenge* dataset. The [Tatoeba project](https://tatoeba.org) is a language-learning initiative started in 2006 by Trang Ho, where contributors have created a huge collection of text pairs from many languages. The Tatoeba Challenge dataset was created by researchers from the University of Helsinki to benchmark machine translation systems, using data extracted from the Tatoeba project. The training set is quite large so we will use the validation set as our training set, setting aside 20% for validation. We will also download the test set:    ```py nmt_original_valid_set, nmt_test_set = load_dataset(     path=\"ageron/tatoeba_mt_train\", name=\"eng-spa\",     split=[\"validation\", \"test\"]) split = nmt_original_valid_set.train_test_split(train_size=0.8, seed=42) nmt_train_set, nmt_valid_set = split[\"train\"], split[\"test\"] ```    Each sample in the dataset is a dictionary containing an English text along with its Spanish translation. For example:    ```py >>> nmt_train_set[0] `{'source_text': 'Tom tried to break up the fight.',`  `'target_text': 'Tom trató de disolver la pelea.',`  `'source_lang': 'eng',`  `'target_lang': 'spa'}` ```   ```py````` We will need to tokenize this text. We could use a different tokenizer for English and Spanish, but these two languages have many words in common (e.g., animal, color, hotel, hospital, idea, radio, motor), and many similar subwords (e.g., pre, auto, inter, uni), so it makes sense to use a common tokenizer. Let’s train a BPE tokenizer on all the training text, both English and Spanish:    ```py def train_eng_spa():  # a generator function to iterate over all training text     for pair in nmt_train_set:         yield pair[\"source_text\"]         yield pair[\"target_text\"]  max_length = 256 vocab_size = 10_000 nmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"<unk>\") nmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model) nmt_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\") nmt_tokenizer.enable_truncation(max_length=max_length) nmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace() nmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(     vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]) nmt_tokenizer.train_from_iterator(train_eng_spa(), nmt_tokenizer_trainer) ```    Let’s test this tokenizer:    ```py >>> nmt_tokenizer.encode(\"I like soccer\").ids `[43, 401, 4381]` `>>>` `nmt_tokenizer``.``encode``(``\"<s> Me gusta el fútbol\"``)``.``ids` `` `[2, 396, 582, 219, 3356]` `` ```   ```py```` ```py``` Perfect! Now let’s create a small utility class that will hold tokenized English texts (i.e., the *source* token ID sequences), along with the corresponding tokenized Spanish targets (i.e., the *target* token ID sequences), plus the corresponding attention masks. For this, we can create a `namedtuple` base class (i.e., a tuple with named fields), and extend it to add a `to()` method, which will make it easy to move all these tensors to the GPU:    ``` from collections import namedtuple  fields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"] class NmtPair(namedtuple(\"NmtPairBase\", fields)):     def to(self, device):         return NmtPair(self.src_token_ids.to(device), self.src_mask.to(device),                        self.tgt_token_ids.to(device), self.tgt_mask.to(device)) ```py    Next, let’s create the data loaders:    ``` def nmt_collate_fn(batch):     src_texts = [pair['source_text'] for pair in batch]     tgt_texts = [f\"<s> {pair['target_text']} </s>\" for pair in batch]     src_encodings = nmt_tokenizer.encode_batch(src_texts)     tgt_encodings = nmt_tokenizer.encode_batch(tgt_texts)     src_token_ids = torch.tensor([enc.ids for enc in src_encodings])     tgt_token_ids = torch.tensor([enc.ids for enc in tgt_encodings])     src_mask = torch.tensor([enc.attention_mask for enc in src_encodings])     tgt_mask = torch.tensor([enc.attention_mask for enc in tgt_encodings])     inputs = NmtPair(src_token_ids, src_mask,                      tgt_token_ids[:, :-1], tgt_mask[:, :-1])     labels = tgt_token_ids[:, 1:]     return inputs, labels  batch_size = 32 nmt_train_loader = DataLoader(nmt_train_set, batch_size=batch_size,                               collate_fn=nmt_collate_fn, shuffle=True) nmt_valid_loader = DataLoader(nmt_valid_set, batch_size=batch_size,                               collate_fn=nmt_collate_fn) nmt_test_loader = DataLoader(nmt_test_set, batch_size=batch_size,                              collate_fn=nmt_collate_fn) ```py    The `nmt_collate_fn()` function starts by extracting all the English and Spanish texts from the given batch. In the process, it also adds an SoS token at the start of each Spanish text, as well as an EoS token at the end. It then tokenizes both the English and Spanish texts using our BPE tokenizer. Next, the input sequences and the attention masks are converted to tensors and wrapped in an `NmtPair`. Importantly, the function drops the EoS token from the decoder inputs, and drops the SoS token from the decoder targets. For example, the inputs may contain the token IDs for “<s> Me gusta el fútbol”, while the targets may contain the token IDs for “Me gusta el fútbol </s>”. Lastly, the function returns the inputs (i.e., the `NmtPair`) along with the targets. Then we just create the data loaders as usual.    And now we are ready to build our translation model. It’s just like [Figure 14-5](#machine_translation_diagram), except the encoder and decoder share the same `nn.Embedding` layer, and the encoder and decoder `nn.GRU` modules contain two layers each:    ``` class NmtModel(nn.Module):     def __init__(self, vocab_size, embed_dim=512, pad_id=0, hidden_dim=512,                  n_layers=2):         super().__init__()         self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)         self.encoder = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,                               batch_first=True)         self.decoder = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,                               batch_first=True)         self.output = nn.Linear(hidden_dim, vocab_size)      def forward(self, pair):         src_embeddings = self.embed(pair.src_token_ids)         tgt_embeddings = self.embed(pair.tgt_token_ids)         src_lengths = pair.src_mask.sum(dim=1)         src_packed = pack_padded_sequence(             src_embeddings, lengths=src_lengths.cpu(),             batch_first=True, enforce_sorted=False)         _, hidden_states = self.encoder(src_packed)         outputs, _ = self.decoder(tgt_embeddings, hidden_states)         return self.output(outputs).permute(0, 2, 1)  torch.manual_seed(42) vocab_size = nmt_tokenizer.get_vocab_size() nmt_model = NmtModel(vocab_size).to(device) ```py    Almost everything in this model should look familiar: it’s very similar to our previous models. We create the modules in the constructor, then the `forward()` method embeds the input sequences (both English and Spanish), it packs the English embeddings and passes them through the encoder, then it passes the Spanish embeddings to the decoder, along with the encoder’s last hidden states (across all `nn.GRU` layers). Lastly, the decoder’s outputs are passed through the output `nn.Linear` layer, and the final outputs are permuted to ensure that the class dimension (containing the token logits) is the second dimension, since this is expected by the `nn.CrossEntropyLoss` and the `Accuracy` metric, as we saw earlier.    ###### Note    The most common metric used in NMT is the *bilingual evaluation understudy* (BLEU) score, which compares each translation produced by the model with several good translations produced by humans. It counts the number of *n*-grams (sequences of *n* words) that appear in any of the target translations and adjusts the score to take into account the frequency of the produced *n*-grams in the target translations. It is implemented by TorchMetric’s `BLEUScore` class.    We could have packed the Spanish embeddings, but then the decoder’s outputs would have been packed sequences, which we would have had to pad before we passed them to the output layer. We avoided this complexity because we can just configure the loss to ignore the output tokens when the targets are padding tokens, like this:    ``` xentropy = nn.CrossEntropyLoss(ignore_index=0)  # ignore <pad> tokens ```py    Now you can train this model (e.g., for 10 epochs using a `Nadam` optimizer with `lr = 0.001`), and it will take quite a while. It’s actually not that long when you consider the fact that the model is learning two languages at once!    While it’s training, let’s write a little helper function to translate some English text to Spanish using our model. It will start by calling the model with the English text for the encoder, and a single SoS token for the decoder. The decoder will just output logits for the first token in the translation. Our function will then pick the most likely token (i.e., with the highest logit) and add it to the decoder inputs, then it will call the model again to get the next token. It will repeat this process, adding one token at a time, until the model outputs an EoS token:    ``` def translate(model, src_text, max_length=20, pad_id=0, eos_id=3):     tgt_text = \"\"     token_ids = []     for index in range(max_length):         batch, _ = nmt_collate_fn([{\"source_text\": src_text,                                     \"target_text\": tgt_text}])         with torch.no_grad():             Y_logits = model(batch.to(device))             Y_token_ids = Y_logits.argmax(dim=1)  # find the best token IDs             next_token_id = Y_token_ids[0, index]  # take the last token ID          next_token = nmt_tokenizer.id_to_token(next_token_id)         tgt_text += \" \" + next_token         if next_token_id == eos_id:             break     return tgt_text ```py    ###### Note    This implementation works but it’s not optimized at all. We could run the encoder just once on the English text, and we could also run the decoder just once per time step, instead of running it over the whole growing text at each iteration.    Let’s try translating some text!    ``` >>> nmt_model.eval() `>>>` `translate``(``nmt_model``,` `\"I like soccer.\"``)` `` `' Me gusta el fútbol . </s>'` `` ```py   ````` ```py` Hurray, it works! We just built a model from scratch that can translate English to Spanish.    If you play around with our translation model, you will find that it often works reasonably well on short text, but it really struggles with longer sentences. For example:    ``` >>> longer_text = \"I like to play soccer with my friends.\" `>>>` `translate``(``nmt_model``,` `longer_text``)` `` `' Me gusta jugar con mis amigos . </s>'` `` ```py   ``` ``The translation says “I like to play with my friends”. Oops, there’s no mention of soccer. So how can we improve this model? One way is to increase the training set size and add more `nn.GRU` layers in both the encoder and the decoder. You could also make the encoder bidirectional (but not the decoder, or else it would no longer be causal and it would see the full translation at each time step, instead of just the previous tokens). Another popular technique that can greatly improve the performance of a translation model at inference time is *beam search*.`` ```py ```` ```py`` ``````", "``````py` ``````", "``````py``` ``````", "``````py` # Beam Search    To translate an English text to Spanish, we call our model several times, producing one word at a time. Unfortunately, this means that when the model makes one mistake, it is stuck with it for the rest of the translation, which can cause more errors, making the translation worse and worse. For example, suppose we want to translate “I like soccer”, and the model correctly starts with “Me”, but then predicts “gustan” (plural) instead of “gusta” (singular). This mistake is understandable, since “Me gustan” is the correct way to start translating “I like” in many cases. Once the model has made this mistake, it is stuck with “gustan”. It then reasonably adds “los”, which is the plural for “the”. But since the model never saw “los fútbol” in the training data (soccer is singular, not plural), the model tries to find something reasonable to add, and given the context it adds “jugadores”, which means “the players”. So “I like soccer” gets translated to “I like the players”. One error caused a chain of errors.    How can we give the model a chance to go back and fix mistakes it made earlier? One of the most common solutions is *beam search*: it keeps track of a short list of the *k* most promising output sequences (say, the top three), and at each decoder step it tries to extend each of them by one word, keeping only the *k* most likely sequences. The parameter *k* is called the *beam width*.    For example, suppose you use the model to translate the sentence “I like soccer” using beam search with a beam width of three (see [Figure 14-7](#beam_search_diagram)). At the first decoder step, the model will output an estimated probability for each possible first word in the translated sentence. Suppose the top three words are “Me” (75% estimated probability), “a” (3%), and “como” (1%). That’s our short list so far. Next, we use the model to find the next word for each sentence. For the first sentence (“Me”), perhaps the model outputs a probability of 36% for the word “gustan”, 32% for the word “gusta”, 16% for the word “encanta”, and so on. Note that these are actually *conditional* probabilities, given that the sentence starts with “Me”. For the second sentence (“a”), the model might output a conditional probability of 50% for the word “mi”, and so on. Assuming the vocabulary has 10,000 tokens, we will end up with 10,000 probabilities per sentence.    Next, we compute the probabilities of each of the 30,000 two-token sentences we considered (3 × 10,000). We do this by multiplying the estimated conditional probability of each word by the estimated probability of the sentence it completes. For example, the estimated probability of the sentence “Me” was 75%, while the estimated conditional probability of the word “gustan” (given that the first word is “Me”) was 36%, so the estimated probability of the sentence “Me gustan” is 75% × 36% = 27%. After computing the probabilities of all 30,000 two-word sentences, we keep only the top 3\\. In this example they all start with the word “Me”: “Me gustan” (27%), “Me gusta” (24%), and “Me encanta” (12%). Right now, the sentence “Me gustan” is winning, but “Me gusta” has not been eliminated.  ![Diagram illustrating a beam search process with a beam width of three, showing probabilities for Spanish sentence constructions starting with \"Me\" and progressing through different stages.](assets/hmls_1407.png)  ###### Figure 14-7\\. Beam search, with a beam width of three    Then we repeat the same process: we use the model to predict the next word in each of these three sentences, and we compute the probabilities of all 30,000 three-word sentences we considered. Perhaps the top 3 are now “Me gustan los” (10%), “Me gusta el” (8%), and “Me gusta mucho” (2%). At the next step we may get “Me gusta el fútbol” (6%), “Me gusta mucho el” (1%), and “Me gusta el deporte” (0.2%). Notice that “Me gustan” was eliminated, and the correct translation is now ahead. We boosted our encoder-decoder model’s performance without any extra training, simply by using it more wisely.    The notebook for this chapter contains a very simple `beam_search()` function, if you’re interested, but in general you will probably want to use the implementation provided by the `GenerationMixin` class in the Transformers library. This is where the text generation models from the Transformers library get their `generate()` method: it accepts a `num_beams` argument which you can set to the desired beam width if you want to use beam search. It also provides a `do_sample` argument that will randomly sample the next token using the probability distribution output by the model, just like we did earlier with our char-RNN model. Other generation strategies are also supported and can be combined (see [*https://homl.info/hfgen*](https://homl.info/hfgen) for more details).    With all this, you can get reasonably good translations for fairly short sentences. For example, the following translation is correct:    ``` >>> beam_search(nmt_model, longer_text, beam_width=3) `' Me gusta jugar al fútbol con mis amigos . </s>'` ```py   ```` Unfortunately, this model will still be pretty bad at translating long sentences:    ```py >>> longest_text = \"I like to play soccer with my friends at the beach.\" `>>>` `beam_search``(``nmt_model``,` `longest_text``,` `beam_width``=``3``)` `` `' Me gusta jugar con jugar con los jug adores de la playa . </s>'` `` ```   `` `This translates to “I like to play with play with the players of the beach”. That’s not quite right. Once again, the problem comes from the limited short-term memory of RNNs. *Attention mechanisms* are the game-changing innovation that addressed this problem.` `` ```py`  ``````", "````` ```py` # Attention Mechanisms    Consider the path from the word “soccer” to its translation “fútbol” back in [Figure 14-5](#machine_translation_diagram): it is quite long! This means that a representation of this word (along with all the other words) needs to be carried over many steps before it is actually used. Can’t we make this path shorter?    This was the core idea in a landmark [2014 paper](https://homl.info/attention)⁠^([20](ch14.html#id3394)) by Dzmitry Bahdanau et al., where the authors introduced a technique that allowed the decoder to focus on the appropriate words (as encoded by the encoder) at each time step. For example, at the time step where the decoder needs to output the word “fútbol”, it will focus its attention on the word “soccer”. This means that the path from an input word to its translation is now much shorter, so the short-term memory limitations of RNNs have much less impact. Attention mechanisms revolutionized neural machine translation (and deep learning in general), allowing a significant improvement in the state of the art, especially for long sentences (e.g., over 30 words).    [Figure 14-8](#attention_diagram) shows our encoder-decoder model with an added attention mechanism:    *   On the left, you have the encoder and the decoder (I’ve made the encoder bidirectional in this figure, as it’s generally a good idea).           *   Instead of sending the encoder’s final hidden state to the decoder, as well as the previous target word at each time step (which is still done, but it is not shown in the figure), we now send all of the encoder’s outputs to the decoder as well.           *   Since the decoder cannot deal with all these encoder outputs at once, they need to be aggregated: at each time step, the decoder’s memory cell computes a weighted sum of all the encoder outputs. This determines which words the decoder will focus on at this step.           *   The weight *α*[(*t*,*i*)] is the weight of the *i*^(th) encoder output at the *t*^(th) decoder time step. For example, if the weight *α*[(3,2)] is much larger than the weights *α*[(3,0)] and *α*[(3,1)], then the decoder will pay much more attention to the encoder’s output for word #2 (“soccer”) than to the other two outputs, at least at this time step.           *   The rest of the decoder works just like earlier: at each time step the memory cell receives the inputs we just discussed, plus the hidden state from the previous time step, and finally (although it is not represented in the diagram) it receives the target word from the previous time step (or at inference time, the output from the previous time step).            ![Diagram illustrating a neural machine translation model using an encoder-decoder network with attention, showing how encoder outputs are weighted and aggregated by the alignment model.](assets/hmls_1408.png)  ###### Figure 14-8\\. Neural machine translation using an encoder-decoder network with an attention model    But where do these *α*[(*t*,*i*)] weights come from? Well, they are generated by a small neural network called an *alignment model* (or an *attention layer*), which is trained jointly with the rest of the encoder-decoder model. This alignment model is illustrated on the righthand side of [Figure 14-8](#attention_diagram):    *   It starts with a dense layer (i.e., `nn.Linear`) that takes as input each of the encoder’s outputs, along with the decoder’s previous hidden state (e.g., **h**[(2)]), and outputs a score (or energy) for each encoder output (e.g., *e*[(3,] [2)]). This score measures how well each encoder output is aligned with the decoder’s previous hidden state.                    For example, in [Figure 14-8](#attention_diagram), the model has already output “me gusta el” (meaning “I like”), so it’s now expecting a noun. The word “soccer” is the one that best aligns with the current state, so it gets a high score.           *   Finally, all the scores go through a softmax layer to get a final weight for each encoder output (e.g., *α*[(3,2)]). All the weights for a given decoder time step add up to 1.              This particular attention mechanism is called *Bahdanau attention* (named after the 2014 paper’s first author). Since it concatenates the encoder output with the decoder’s previous hidden state, it is sometimes called *concatenative attention* (or *additive attention*).    In short, the attention mechanism provides a way to focus the attention of the model on part of the inputs. That said, there’s another way to think of this whole process: it acts as a differentiable memory retrieval mechanism. For example, let’s suppose the encoder analyzed the input sentence “I like soccer”, and it managed to understand that the word “I” is the subject, the word “like” is the verb, and the word “soccer” is the noun, so it encoded this information in its outputs for these words. Now suppose the decoder has already translated “I like”, and it thinks that it should translate the noun next. For this, it needs to fetch the noun from the input sentence. This is analogous to a dictionary lookup: it’s as if the encoder had created a dictionary {\"subject”: “I”, “verb”: “like”, “noun”: “soccer\"} and the decoder wanted to look up the value that corresponds to the key “noun”.    However, the model does not have discrete tokens to represent the keys (like “subject”, “verb”, or “noun”); instead, it has vectorized representations of these concepts that it learned during training, so the query it will use for the lookup will not perfectly match any key in the dictionary. One solution is to compute a similarity measure between the query and each key in the dictionary, and then use the softmax function to convert these similarity scores to weights that add up to 1\\. As we just saw, that’s exactly what the attention layer does. If the key that represents the noun is by far the most similar to the query, then that key’s weight will be close to 1\\. Next, the attention layer computes a weighted sum of the corresponding values: if the weight of the “noun” key is close to 1, then the weighted sum will be very close to the representation of the word “soccer”. In short, the decoder queried for a noun and the attention mechanism retrieved it.    In most modern implementations of attention mechanisms, the arguments are named `query`, `key`, and `value`. In our example, the query is the decoder’s hidden states, the key is the encoder’s outputs (this is used to compute the weights), and the value is also the encoder’s outputs (this is used to compute the final weighted sum).    ###### Note    If the input sentence is *n* words long, and assuming the output sentence is about as long, then the attention mechanism will need to compute about *n*² weights. This quadratic computational complexity becomes untractable when the sentences are too long.    Another common attention mechanism, known as *Luong attention* or *multiplicative attention*, was proposed shortly after, in [2015](https://homl.info/luongattention),⁠^([21](ch14.html#id3403)) by Minh-Thang Luong et al. Because the goal of the alignment model is to measure the similarity between one of the encoder’s outputs and the decoder’s previous hidden state, the authors proposed to simply compute the dot product (see [Chapter 4](ch04.html#linear_models_chapter)) of these two vectors, as this is often a fairly good similarity measure, and modern hardware can compute it very efficiently. For this to be possible, both vectors must have the same dimensionality. The dot product gives a score, and all the scores (at a given decoder time step) go through a softmax layer to give the final weights, just like in Bahdanau attention.    Luong et al. also proposed to use the decoder’s hidden state at the current time step rather than at the previous time step (i.e., **h**[(*t*)] rather than **h**[(*t*–1)]) to compute the attention vector (denoted <msub><mover><mi mathvariant=\"bold\">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>). This attention vector is then concatenated with the decoder’s hidden state to form an attentional hidden state, which is then used to predict the next token. This simplifies and speeds up the process by allowing the encoder and decoder to operate independently before attention is applied, rather than interweaving attention into the decoder’s recurrence.    The researchers also proposed a variant of the dot product mechanism where the encoder outputs first go through a fully connected layer (without a bias term) before the dot products are computed. This is called the “general” dot product approach. The researchers compared both dot product approaches with the concatenative attention mechanism (adding a rescaling parameter vector **v**), and they observed that the dot product variants performed better than concatenative attention. For this reason, concatenative attention is much less used now. The equations for these three attention mechanisms are summarized in [Equation 14-2](#attention_mechanisms_equation).    ##### Equation 14-2\\. Attention mechanisms  <msub><mover><mi mathvariant=\"bold\">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo lspace=\"0%\" rspace=\"0%\">,</mo><mi>i</mi><mo>)</mo></mrow></msub><msub><mover><mi mathvariant=\"bold\">y</mi><mo stretchy=\"false\" style=\"math-style:normal;math-depth:0;\">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mtext> with </mtext><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo lspace=\"0%\" rspace=\"0%\">,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo lspace=\"0%\" rspace=\"0%\">,</mo><mi>i</mi><mo>)</mo></mrow></msub></mfenced></mrow><mrow><mstyle displaystyle=\"true\"><munder><mo>∑</mo><mrow><mi>i</mi><mo>'</mo></mrow></munder></mstyle><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo lspace=\"0%\" rspace=\"0%\">,</mo><mi>i</mi><mo>'</mo><mo>)</mo></mrow></msub></mfenced></mrow></mfrac><mtext> and </mtext><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo lspace=\"0%\" rspace=\"0%\">,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfenced open=\"{\" close=\"\"><mtable columnalign=\"left\"><mtr><mtd><msup><msub><mi mathvariant=\"bold\">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><msub><mover><mi mathvariant=\"bold\">y</mi><mo stretchy=\"false\" style=\"math-style:normal;math-depth:0;\">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>d</mi><mi>o</mi><mi>t</mi></mtd></mtr><mtr><mtd><msup><msub><mi mathvariant=\"bold\">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><mi mathvariant=\"bold\">W</mi><msub><mover><mi mathvariant=\"bold\">y</mi><mo stretchy=\"false\" style=\"math-style:normal;math-depth:0;\">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>g</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>l</mi></mtd></mtr><mtr><mtd><msup><mi mathvariant=\"bold\">v</mi><mo>⊺</mo></msup><mi>tanh</mi><mo>(</mo><mi mathvariant=\"bold\">W</mi><mo>[</mo><msub><mi mathvariant=\"bold\">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>;</mo><msub><mover><mi mathvariant=\"bold\">y</mi><mo stretchy=\"false\" style=\"math-style:normal;math-depth:0;\">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>]</mo><mo>)</mo></mtd><mtd><mi>c</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi></mtd></mtr></mtable></mfenced>  Let’s add Luong attention to our encoder-decoder model. Since PyTorch does not include a Luong attention function, we need to write our own. Luckily, it’s pretty short:    ``` def attention(query, key, value):  # note: dq == dk and Lk == Lv     scores = query @ key.transpose(1, 2)  # [B,Lq,dq] @ [B,dk,Lk] = [B, Lq, Lk]     weights = torch.softmax(scores, dim=-1)  # [B, Lq, Lk]     return weights @ value  # [B, Lq, Lk] @ [B, Lv, dv] = [B, Lq, dv] ```py    Just like in [Equation 14-2](#attention_mechanisms_equation), we first compute the attention scores, then we convert them to attention weights using the softmax function, and lastly we compute the attention output by multiplying the attention weights with the value (i.e., the encoder outputs). This implementation efficiently runs all these computations for the whole batch at once. The `query` argument corresponds to **h**[(*t*)] in [Equation 14-2](#attention_mechanisms_equation) (i.e., the decoder’s hidden states), and the `key` argument corresponds to $ModifyingAbove bold y With caret Subscript left-parenthesis i right-parenthesis$ (i.e., the encoder’s outputs), but only for the computation of the attention scores. The `value` argument also corresponds to $ModifyingAbove bold y With caret Subscript left-parenthesis i right-parenthesis$ , but only for the final computation of the weighted sum. The `key` and `value` arguments are generally identical, but there are a few scenarios where they can differ (e.g., some models use compressed keys to save memory and speed up the score computation). The shapes are shown in the comments: `B` is the batch size; `Lq` is the length of the longest query in the batch; `Lk` is the length of the longest key in the batch (note that each value must have the same length as its corresponding key); `dq` is the query’s embedding size, which must be the same as the key’s embedding size `dk`; and `dv` is the value’s embedding size.    ###### Tip    Since all arguments are 3D tensors, we could replace the `@` matrix multiplication operator with the *batch matrix multiplication* function: `torch.bmm()`. This function only works with batches of matrices (i.e., 3D tensors), but it’s optimized for this use case so it runs faster. The result is the same: each matrix in the first tensor gets multiplied by the corresponding matrix in the second tensor.    Now let’s update our NMT model. The constructor needs just one modification—the output layer’s input size must be doubled, since we will concatenate the attention vectors to the decoder outputs:    ``` self.output = nn.Linear(2 * hidden_dim, vocab_size) ```py    Next, let’s add attention to the `forward()` method:    ``` def forward(self, pair):     src_embeddings = self.embed(pair.src_token_ids)  # same as earlier     tgt_embeddings = self.embed(pair.tgt_token_ids)  # same     src_lengths = pair.src_mask.sum(dim=1)  # same     src_packed = pack_padded_sequence(src_embeddings, [...])  # same     encoder_outputs_packed, hidden_states = self.encoder(src_packed)     decoder_outputs, _ = self.decoder(tgt_embeddings, hidden_states)  # same     encoder_outputs, _ = pad_packed_sequence(encoder_outputs_packed,                                                 batch_first=True)     attn_output = attention(query=decoder_outputs,                             key=encoder_outputs, value=encoder_outputs)     combined_output = torch.cat((attn_output, decoder_outputs), dim=-1)     return self.output(combined_output).permute(0, 2, 1) ```py    Let’s go through this code:    *   We compute the English and Spanish embeddings, the English sequence lengths, and we pack the English embeddings, just like earlier.           *   We then run the encoder like earlier, but we no longer ignore its outputs since we will need them for the attention function.           *   Next, we run the decoder, just like earlier.           *   Since the encoder’s inputs are represented as a packed sequence, its outputs are also represented as a packed sequence. Not many operations support packed sequences, so we must convert the encoder’s outputs to a padded tensor using the `pad_packed_sequence()` function.           *   And now we can call our `attention()` function. Note that we pass the decoder outputs instead of the hidden states because the decoder only returns the last hidden states. That’s OK because the `nn.GRU` layer’s outputs are equal to its top-layer hidden states.           *   Lastly, we concatenate the attention output and the decoder outputs along the last dimension, and we pass the result through the output layer. As earlier, we also permute the last two dimensions of the result.              ###### Warning    Our attention mechanism doesn’t ignore padding tokens. The model learns to ignore them during training, but it’s preferable to mask them entirely. We will see how in [Chapter 15](ch15.html#transformer_chapter).    And that’s it! If you train this model, you will find that it now handles much longer sentences. For example:    ``` >>> translate(nmt_attn_model, longest_text) `' Me gusta jugar fu tbol con mis amigos en la playa . </s>'` ```py   `Perfect! We didn’t even have to use beam search. In fact, attention mechanisms turned out to be so powerful that some Google researchers tried getting rid of recurrent layers altogether, only using feedforward layers and attention. Surprisingly, it worked like a charm. This led the researchers to name their paper “Attention is all you need”, introducing the Transformer architecture to the world. This was the start of a huge revolution in NLP and beyond. In the next chapter, we will explore the Transformer architecture and see how it revolutionized deep learning.`  `# Exercises    1.  What are the pros and cons of using a stateful RNN versus a stateless RNN?           2.  Why do people use encoder-decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?           3.  How can you deal with variable-length input sequences? What about variable-length output sequences?           4.  What is beam search, and why would you use it? What tool can you use to implement it?           5.  What is an attention mechanism? How does it help?           6.  When would you need to use sampled softmax?           7.  *Embedded Reber grammars* were used by Hochreiter and Schmidhuber in [their paper](https://homl.info/93) about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE”. Check out Jenny Orr’s [nice introduction](https://homl.info/108) to this topic, then choose a particular embedded Reber grammar (such as the one represented on Orr’s page), and train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t.           8.  Train an encoder-decoder model that can convert a date string from one format to another (e.g., from “April 22, 2019” to “2019-04-22”).              Solutions to these exercises are available at the end of this chapter’s notebook, at [*https://homl.info/colab-p*](https://homl.info/colab-p).    ^([1](ch14.html#id3189-marker)) Alan Turing, “Computing Machinery and Intelligence”, *Mind* 49 (1950): 433–460.    ^([2](ch14.html#id3191-marker)) Of course, the word *chatbot* came much later. Turing called his test the *imitation game*: machine A and human B chat with human interrogator C via text messages; the interrogator asks questions to figure out which one is the machine (A or B). The machine passes the test if it can fool the interrogator, while the human B must try to help the interrogator.    ^([3](ch14.html#id3210-marker)) Tomáš Mikolov et al., “Distributed Representations of Words and Phrases and Their Compositionality”, *Proceedings of the 26th International Conference on Neural Information Processing Systems* 2 (2013): 3111–3119.    ^([4](ch14.html#id3211-marker)) Malvina Nissim et al., “Fair Is Better Than Sensational: Man Is to Doctor as Woman Is to Doctor”, arXiv preprint arXiv:1905.09866 (2019).    ^([5](ch14.html#id3218-marker)) It’s a convention in Python to name unused variables with an underscore prefix.    ^([6](ch14.html#id3232-marker)) Another technique to capture longer patterns is to use a stateful RNN. It’s a bit more complex and not used as much, but if you’re interested I’ve included a section in this chapter’s notebook.    ^([7](ch14.html#id3233-marker)) Alec Radford et al., “Learning to Generate Reviews and Discovering Sentiment”, arXiv preprint arXiv:1704.01444 (2017).    ^([8](ch14.html#id3241-marker)) Rico Sennrich et al., “Neural Machine Translation of Rare Words with Subword Units”, *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics* 1 (2016): 1715–1725.    ^([9](ch14.html#id3256-marker)) Yonghui Wu et al., “Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation”, arXiv preprint arXiv:1609.08144 (2016).    ^([10](ch14.html#id3258-marker)) Taku Kudo, “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates”, arXiv preprint arXiv:1804.10959 (2018).    ^([11](ch14.html#id3263-marker)) Taku Kudo and John Richardson, “SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing”, arXiv preprint arXiv:1808.06226 (2018).    ^([12](ch14.html#id3281-marker)) *Nested tensors* serve a similar purpose and are more convenient to use, but they are still in prototype stage at the time of writing. See [*https://pytorch.org/docs/stable/nested.html*](https://pytorch.org/docs/stable/nested.html) for more details.    ^([13](ch14.html#id3298-marker)) Matthew Peters et al., “Deep Contextualized Word Representations”, *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies* 1 (2018): 2227–2237.    ^([14](ch14.html#id3303-marker)) Jeremy Howard and Sebastian Ruder, “Universal Language Model Fine-Tuning for Text Classification”, *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics* 1 (2018): 328–339.    ^([15](ch14.html#id3354-marker)) Ilya Sutskever et al., “Sequence to Sequence Learning with Neural Networks”, arXiv preprint, arXiv:1409.3215 (2014).    ^([16](ch14.html#id3359-marker)) Samy Bengio et al., “Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks”, arXiv preprint arXiv:1506.03099 (2015).    ^([17](ch14.html#id3376-marker)) Sébastien Jean et al., “On Using Very Large Target Vocabulary for Neural Machine Translation”, *Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing* 1 (2015): 1–10.    ^([18](ch14.html#id3378-marker)) Edouard Grave et al., “Efficient softmax approximation for GPUs”, arXiv preprint arXiv:1609.04309 (2016).    ^([19](ch14.html#id3383-marker)) Ofir Press, Lior Wolf, “Using the Output Embedding to Improve Language Models”, arXiv preprint arXiv:1608.05859 (2016).    ^([20](ch14.html#id3394-marker)) Dzmitry Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate”, arXiv preprint arXiv:1409.0473 (2014).    ^([21](ch14.html#id3403-marker)) Minh-Thang Luong et al., “Effective Approaches to Attention-Based Neural Machine Translation”, *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing* (2015): 1412–1421.` ```` ```py`` `````", "``````py` ``````", "``````py``` ``````", "```` ```py````", "```py` ```", "```py```", "``` ```", "```````py```````", "```````py````` ```py```````", "```````py```````", "`````` ```py``````", "```````py```````", "```````"]