- en: 11 End-to-end model deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The end-to-end model deployment process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintenance of the model postdeployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python codes for each of the steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The journey is the destination.—Dan Eldon
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The path to learning never ends. It takes a lot of courage, patience, and hard
    work to learn something. We have to be persistent, resourceful, and always looking
    for opportunities to learn and excel.
  prefs: []
  type: TYPE_NORMAL
- en: Across all of the chapters so far, you have covered a lot of concepts, techniques,
    and algorithms. In this last chapter of the book, we are going to discuss the
    end-to-end model deployment process. We will cover various aspects ranging from
    a business problem definition, data cleaning, and exploratory data analysis (EDA)
    to model deployment and maintenance. This end-to-end journey is crucial for you
    to appreciate the entire process. We will discuss Python codes at all the relevant
    places.
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to this last chapter, and all the very best!
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 The machine learning modeling process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall in chapter 1 we briefly discussed end-to-end model development. In this
    section, we cover each of the respective steps in detail and the most common problems
    we face with each of them and how to tackle them. It will finally lead to the
    model deployment phase. Figure 11.1 shows the model development process we follow.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F01_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 The complete machine learning modeling process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The steps in the model development process are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Business problem definition
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data discovery and feasibility analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data cleaning and prepreparation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modeling process and business approval
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model documentation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model maintenance and model refresh
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Throughout this chapter, we will cover each of these processes in much more
    detail. These are all relevant to the modeling process.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Business problem definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your business problem definition is the very first step. It is vital that the
    business problem is concise, clear, measurable, and achievable. Many times, in
    practice, the business problem is vaguely defined, such as “decrease the costs
    or increase the revenue,” which often leads to poor results throughout the rest
    of the process. A good business problem is defined clearly with key performance
    indicators (KPIs) and parameters that can be used to measure the effect. A good
    business problem ensures there is no ambiguity, the goal is clear, and we can
    achieve it with the available resources and within the timeframe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most important considerations regarding a business problem are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If a business problem is vaguely defined, it is going to cause problems and
    should be avoided. For example, all of the businesses and various functions would
    want to increase their revenue and profits, reduce costs, optimize various processes,
    and so forth. With a vague business problem, we will not have clarity on the process,
    which will lead to ambiguity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The business purpose should be practically achievable. Unrealistic goals like
    doubling the revenue or halving the cost should not be set. Unrealistic goals
    mean that good results might get rejected, as they do not meet the business targets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The business problem should be measurable if possible. If the business problem
    is only qualitative, then it will be of limited help. We won’t be able to understand
    the real effect of the machine learning model created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scope creep is one of the problems we face sometimes. Scope creep happens when
    at the start of the project, during project building, the scope is changed drastically,
    changing the requirements and time needs of the project without changing resources
    and deadlines accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An effective business problem is defined correctly, completely, and in discussion
    with the business teams. It is concise with measurable KPIs and is achievable
    within a given timeframe.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Business stakeholders and subject matter experts should be involved in
    defining the business problems. They should be a part of the team from the start
    and own the overall process.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few examples of a good business problem are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The marketing team in an organization aims to optimize the various costs and
    maximize the return on investments. They want to identify the optimal combinations
    of marketing efforts (email, calls, TV advertising, and meetings) to increase
    the return on investment by 1.5% in the next six months.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A manufacturing team faces an increase in the number of defects in the last
    three months. The business problem can be to identify all the potential reasons
    for such an increase in defects. The team also wishes to know if there is a trend
    or pattern. The business goal may be to shortlist the most significant reasons
    for defects and reduce them by 2.5% in the next six months.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have described the attributes of a business problem. We now move to the next
    phase, which is data discovery and feasibility.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Data discovery and feasibility analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data discovery phase is one of the most important steps in the entire model
    building process. If there is not enough data, both quantitatively and qualitatively,
    it might be very difficult to create the solution we desire. At the same time,
    having access to this data is of paramount importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'During this process, we also do the feasibility analysis for the project:'
  prefs: []
  type: TYPE_NORMAL
- en: The data is the protagonist. The very first step is the identification of the
    datasets required for the business problem use case and mechanisms for its access
    by all the stakeholders. For this reason, it is advisable that
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dataset is available from servers or clouds and relevant permissions are
    set correctly to the people who need access. The servers can access the data from
    a database such as SQL/MySQL/NoSQL/MongoDB.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the data is in Excel/.csv/text files, it will be useful to make it available
    on the server. In recent times, cloud servers like AWS, Azure, Google Cloud, etc.,
    are used for storing the data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is imperative to check that the dataset is complete and relevant to the business
    problem. The dataset should be representative enough of the business problem at
    hand and capture all the variability in the business. The time and duration of
    the data is another important dimension we should bear in mind. For example, if
    we wish to analyze the business of a telecom operator or a retail company, we
    should have enough data (for at least the last year so that we capture seasonality
    as well) and variables around sales, transactions, discounts, products/services
    purchased, marketing behaviors, historical behaviors, offline/online purchases,
    etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is prudent to plan the data refresh at this stage. After all, once the model
    is built, we will have to maintain it and refresh it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'During this phase, the most common problems we can face are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We might find that there are certain missing values, outliers, etc., in the
    dataset. We will cover that in detail in the next section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must also ensure that correct business rules are applied on the dataset.
    The steps to ensure it are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the relevant dataset for the business problem.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Make some basic analyses like total sales, number of customers, month-wise trends,
    discounts, etc.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Get these KPIs verified by the business stakeholders. If the numbers are wrong,
    the business rules are refined.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Only once the data is correct and the numbers are accurate can we move on to
    the feasibility analysis for the use case. For the feasibility analysis, we do
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Check the data quality in detail. We cover the various aspects in the next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze the data for any patterns, such as seasonality, etc. We also check if
    there are any correlations present among various variables to ensure which variables
    are related to each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check for relationships between the business problem and the dataset. This is
    followed by an exploratory analysis to identify if there is any significant difference
    between various customer groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After this step, we go to the data cleaning, preprocessing, and data preparation
    step. This is one of the most time-consuming steps we have to do.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 Data cleaning and prepreparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last step, we shortlisted the data for the business problem. Now we will
    go to the data cleaning and preprocessing phase of the modeling process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data in its original form might not be usable enough to be fed to the machine
    learning model. We have to create a few additional variables and treat some others.
    In the real business world, the dataset is generally “dirty.” There can be many
    problems that can be present in the data, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical variables (may cause some problem for certain algorithms)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing values, NULL, or not a number (NaN), etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other problems (as described in previous chapters)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s deal with each of these things in turn. The code for this chapter has
    been checked in at [https://mng.bz/vKY7](https://mng.bz/vKY7). You can access
    the code and datasets there. We will now work on how to deal with duplicate values
    in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5 Duplicate values in the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Duplicates are often a problem in datasets. If there are two rows in the dataset
    that are a complete copy of each other, they are duplicates in nature. This problem
    might occur during data-capturing time. The problem with duplicates is that the
    statistics will be affected—for example, by making some events appear to be more
    frequent than they are. When removing duplicates, one needs to pay attention to
    not removing genuine data of events that happened twice—for example, a customer
    purchasing an item twice at two different times or a customer purchasing two identical
    items at the same time versus the transaction of the purchase being recorded twice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps of a simple Python program to remove duplicates
    (see figure 11.2):'
  prefs: []
  type: TYPE_NORMAL
- en: Import `numpy` and `pandas`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a dataframe with some dummy variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the dataframe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There is an inbuilt method: `drop_duplicates()`. Use it to drop the duplicates.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the dataframe and find that the duplicate rows have been dropped.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F02_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 Removing duplicates in a simple Python program
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.6 Categorical variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is treating the categorical variable. Let’s revisit the definition
    of categorical variables. Variables like gender, city, product categories, zip
    codes, etc., are examples of categorical variables. Categorical variables may
    not strictly be a problem in the data, but they can create problems for certain
    algorithms like k-means clustering. Recall that for k-means clustering, the distance
    needs to be calculated between the data points.
  prefs: []
  type: TYPE_NORMAL
- en: In certain datasets, a categorical variable can have nearly all values as the
    same. For example, if the whole dataset is for the UK and a variable is “city,”
    since a significant percentage of the population lives in London, then this variable
    might be of limited benefit. It will not create any variation in the dataset and
    will not be useful. Similarly, a categorical variable like “zip code” can have
    all the values as distinct and will not add much to the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps the most common method to deal with categorical variables is using
    one-hot encoding. In one-hot encoding, as shown in the Python code book, the variable
    gets transformed:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the same dataset we used in the last code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is a built-in method in pandas, `get_dummies()`, which can be used for
    converting categorical variables to numeric ones. See figure 11.3\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F03_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 The output of the code when executed
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.7 Missing values in dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most common challenges in real-world datasets is missing values,
    which might be blank, NULL, NaN, etc. It might be due to a data capturing problem
    or data transformation. Missing values should be treated to ensure a robust solution.
    There can be a few reasons for missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: The values were not recorded properly during data capturing. This can be due
    to faulty equipment or a manual error when recording the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many times, nonmandatory fields are not entered. For example, a customer might
    not enter age while filling out a retail loyalty form.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Survey responses might not be completely filled out—for example, salary details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To mitigate the missing values, there are a few options:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we should check if the data is missing by design and whether it is a
    problem that needs to be addressed. For example, it is possible for a sensor to
    not record any temperature values above a certain pressure range. In that case,
    having missing values of temperature is correct.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should also check if there are any patterns in the missing values with respect
    to the other independent variables and with respect to the target variable. For
    example, in the dataset used in the next example we can deduce that whenever the
    value of temperature is NULL, then the equipment has failed. In such a case, there
    is a clear pattern in this data between temperature and the failed equipment.
    Hence, it will be the wrong step to delete the temperature or treat the temperature
    variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perhaps the easiest approach to deal with missing values is to delete the rows
    that have missing values. Though this is simple and fast, it reduces the size
    of the population and can delete very important pieces of information, as described
    earlier, or, for example, if a person has a legitimate last name that is not available.
    Hence, we should be careful deleting rows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can impute the missing values by the mean, median, or mode values. Mean or
    median are only possible for continuous variables. Mode can be used for both continuous
    and categorical variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also other popular methods for imputing the missing values like using
    k-nearest neighbor and multivariate imputation by chained equation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now use Python to impute missing values. We will use the built-in method
    `SimpleImputer` and impute the missing values with the mean. The second solution
    is for the categorical variables, where the mode is used to replace the missing
    values. See figure 11.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F04_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 The output of the code when executed
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the next solutions, we will use `IterativeImputer` and the k-nearest neighbor
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 11.8 Outliers present in the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Outliers can be a big problem in the data. Consider this: let’s assume that
    average rainfall for a city is 50 cm. But one particular year, due to heavy rains,
    the average rainfall is 100 cm. This data point would be an outlier and will completely
    change the analysis results should it be included. In the example, depending on
    whether the year of heavy precipitation is included or not in the statistical
    analysis, the results (say, of likely insurance claims) would be very different.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, like missing values, outliers may not necessarily be an error. We
    should apply business acumen to infer if the data points are really outliers for
    the problem under study.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can detect outliers in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: If a data point lies beyond the 5th percentile and 95th percentile or 1st percentile
    and 99th percentile, it can be considered an outlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value that is beyond –1.5 × interquartile range (IQR) and +1.5 × IQR can also
    be considered an outlier. Here IQR is given by (value at 75th percentile) – (value
    at 25th percentile).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values beyond one, two, or three standard deviations from the mean can be termed
    outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can create charts and visualize outliers. We can treat outliers by using
    the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: A data point beyond the 5th percentile and 95th percentile can be capped at
    the 5th percentile and 95th percentile, respectively. Or a data point beyond the
    1st percentile and 99th percentile can be capped at the 1st percentile and 99th
    percentile, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacement by mean, median, or mode is also used sometimes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes taking a natural log of the variable reduces the effect of outliers.
    But since a natural log will change the actual values, we should use sound mathematical
    models for the problem under investigation to make sure it’s appropriate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers pose a big challenge to our datasets. They skew the insights we have
    generated from the data. Sometimes this skew is appropriate (e.g., the insurance
    claims of an outlier heavy precipitation year, which the insurance company needs
    to take into account). In any case, it becomes important that we at least highlight
    outliers in the dataset and sometimes modify them.
  prefs: []
  type: TYPE_NORMAL
- en: 11.9 Exploratory data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: EDA is one of the most crucial steps before we start modeling. Using EDA, we
    generate insights that are quite useful for the business. The insights generated
    from the EDA conform to the modeling outputs too.
  prefs: []
  type: TYPE_NORMAL
- en: In EDA, we examine all the variables and understand their patterns, interdependencies,
    relationships, and trends. During the EDA phase, we come to know how the data
    is expected to behave. We uncover insights and recommendations from the data at
    this stage. A strong visualization complements the complete EDA.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  EDA is the key to success; many times, a good EDA can solve the business
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Next we perform a detailed EDA on a dataset using Python. The entire code is
    quite big for a book; hence, the Python notebook has been checked in to the GitHub
    repository ([https://mng.bz/vKY7](https://mng.bz/vKY7)) with full explanations
    and comments.
  prefs: []
  type: TYPE_NORMAL
- en: 11.10 Model development and business approval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already covered the modeling process in detail throughout the book.
    This includes creating the first version of the model and then iterating with
    different hyperparameters and with different algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, we have covered a lot of algorithms on clustering and dimensionality
    reduction methods. We also covered modeling for the text datasets. During the
    model development phase, based on the business problem and dataset at hand, we
    choose the candidate algorithms. We always strive to select the best algorithm
    based on the accuracy measurement parameters we have discussed in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the modeling process is the final algorithm that delivers the
    best output for the business problem at hand. After a model with satisfactory
    performance is found, we should have a discussion with the business stakeholders
    for their final feedback. There might be a few iterations required to further
    improve the model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you have a model that is statistically significant, useful, and approved
    by the business stakeholders. We can move on to the model deployment stage.
  prefs: []
  type: TYPE_NORMAL
- en: 11.11 Model deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A critical stage in the development of AI and machine learning models is model
    deployment. It is the changeover point between the development and production
    environments, where the model is used for real-world business purposes. There
    are many facets to be considered, like infrastructure concerns, deployment methodologies,
    monitoring, and maintenance. We discuss the challenges and recommended steps related
    to model deployment, with a methodical and organized strategy to put the models
    into production.
  prefs: []
  type: TYPE_NORMAL
- en: 11.12 Purpose of model deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Model deployment is a crucial process. The primary reasons for model deployment
    are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment of a model leads to the transformation of insights into actionable
    and practical purposes. The model is used for making predictions, optimizations,
    recommendations, and suggestions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployed models are integrated with the business processes and workflows.
    This facilitates the automation of various processes and business functions based
    on the insights and recommendations made by the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time predictions ensure that the business is responding quickly to the
    ever-changing business conditions. Real-time predictions are particularly useful
    for scenarios like credit card fraud detection in transactions, dynamic pricing,
    etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization and automation are enhanced. Model deployment leads to a decrease
    in the efforts of the employees by automating the business functions. With the
    help of deployed models, hardware use is optimized, business functions and processes
    are made more efficient, and the overall return on investment is increased.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With deployed models, the versioning of the models can be done. This ensures
    that the organization can track changes, perform A/B tests, and even perform rollback
    if required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, the purpose of model deployment is to translate the potential of
    machine learning models into practical applications, making them an integral part
    of business operations and decision-making processes. Deployment enables organizations
    to harness the power of AI and data science to derive value from their models
    in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 11.13 Types of model deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several types of model deployments. Based on the requirements and
    the strategic objectives, we can choose between them. The various types of deployment
    strategies are
  prefs: []
  type: TYPE_NORMAL
- en: '*Batch deployment*—This methodology is used when we have a large dataset that
    has been collected over a period of time and we need to use the machine learning
    model to assess this data and make predictions in an offline mode. Generally,
    the processing is done in large batches. For example, if we want to cluster the
    customers of a retail store based on k-means clustering, we can take their attributes
    for the last two years and generate a corresponding cluster for each customer.
    We can refresh the underlying data after one month, and hence we can reassign
    these clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Real-time deployment*—Consider this: we want to check if the incoming credit
    card transaction is genuine or fraudulent. In such a scenario, we use a real-time
    check. The predictions are generated in real time based on the latest information
    available. Generally, to support real-time predictions, we should employ a multithreaded
    process so that multiple prediction requests can be handled at the same time.
    For example, there can be hundreds of credit card requests made simultaneously,
    which our system needs to classify with very little latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Edge deployment*—Nowadays, people expect smartphones or Internet of Things
    devices to have sophisticated features that are a good fit for a machine learning
    or AI algorithm. In such a scenario, a deployment in the cloud is possible, but
    edge deployment is also used when an internet connection is not available. The
    prerequisite for edge deployment is that the machine learning model should be
    small in size and require less computation to facilitate running it on the devices
    with limited resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Canary deployment*—In canary deployment, we release the model to a subset
    of users before we make a full-scale deployment for all users. This ensures that
    an unstable version is not released to all users as we will get the feedback from
    the test users in the first phase. This is typically done by large companies with
    a huge number of users providing services through the cloud, such as Google or
    Facebook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A/B testing*—A/B testing is not actually a model deployment technique, but
    it can be used as one and that is why it is listed here. In A/B testing, organizations
    want to test how one solution/service/product compares with another. For example,
    if the product team wishes to test which of the two offers delivers better profitability,
    they will use A/B testing. The example of two offers can be “spend $100 and get
    a 15% discount” or “spend $50 and get a 10% discount.” In such a scenario, there
    can be two similar groups of customers that will receive these offers, and we
    will check which one delivers better profitability. In A/B testing deployments,
    two different models (or the same model with different hyperparameters) are tested
    against each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.14 Considerations while deploying the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are quite a few factors we should keep in mind while deploying the model
    to ensure smooth and effective transition from development of the model to deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Accuracy monitoring*—We should constantly monitor the performance of the model
    and improve it if the performance falls below a threshold. We should cover key
    metrics like accuracy, resource utilizations, time, and accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalability*—A solution should be scalable to other departments or brands.
    Even the volume of the data can increase with time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security and compliance*—This is one consideration that cannot be compromised
    at all. Any kind of deployment should be completely secure from any threats and
    fully compliant with the existing best practices, policies, and requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model drift and data drift*—These should be monitored because the overall
    business scenario can change. Customers, their preferences, the market, and the
    overall economy may change. There are events like COVID, war, floods, etc., and
    hence there is a data drift. It results in a model’s performance change too. Hence,
    we should plan for model drift in advance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reproducibility*—Reproducibility of the results is an important factor when
    we deploy the models. We should be able to replicate the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Continuous integration and continuous deployment*—These pipelines are required
    to automate the testing and the deployment process. This reduces the risk of errors
    and ensures smooth deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*User feedback and successive iterations*—These are very important for a successful
    project. While planning for the deployment, we should give due diligence to incorporating
    users’ feedback and the iterations in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Versioning and rollback*—No model is ever final. There are successive iterations
    to it. In the infrastructure, there should be a provision to roll back to the
    previous version if the new version has any problems or if there are reasons based
    on the business requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this, we have covered all the considerations in model deployment. We will
    now deploy a model using Flask. The entire code has been uploaded to the GitHub
    repository ([https://mng.bz/vKY7](https://mng.bz/vKY7)) with full comments and
    explanations.
  prefs: []
  type: TYPE_NORMAL
- en: 11.15 Documentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our model is deployed. Now we ensure that all the code snippets are cleaned,
    are properly commented, and adhere to best practices. The code files should be
    checked in and properly documented. Documentation is often (unfortunately) not
    given enough time, but it is a very important step that should not be ignored.
    Should priorities be set in writing in the documentation, precedence should be
    given to the aspects more likely to change and to those that require understanding
    and interaction with external stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few tools for version controlling of the code. Git is perhaps
    the most common one. It is a very good practice to ensure that all of our code
    is checked in regularly to safeguard ourselves from any potential computer failures.
    For documentation, we do have a lot of options available in the industry, ranging
    from Word to PowerPoint to Confluence pages, depending on the industry we work
    in.
  prefs: []
  type: TYPE_NORMAL
- en: 11.16 Model maintenance and refresh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have covered all the stages of model development and deployment.
    But once a model is put into production, it needs constant monitoring. We must
    ensure that the model is always performing at the desired level of accuracy. To
    achieve this, it is advised to have a dashboard or a monitoring system to gauge
    the performance of the model regularly. In case of nonavailability of such a system,
    a monthly or quarterly check-up of the model can be done.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is deployed, we can do a monthly health check of the model. It
    means that we compare the performance of the model with the expected accuracy.
    If the performance is not good, the model requires a refresh. Even though the
    model might not be deteriorating, it is still a good practice to refresh the model
    on new data points that are constantly created and saved. The model refresh is
    generally based on the business problem as well as the business domain for which
    the model has been built. For example, in the telecom domain, data updates are
    faster as customers use their mobile phones daily. On the other hand, for retail
    apparel, we don’t expect customers to buy clothes every day. Hence, the model
    for the telecom domain can be refreshed weekly or biweekly, while for apparel,
    we can refresh once a quarter or once every six months.
  prefs: []
  type: TYPE_NORMAL
- en: Model refresh is quite an important phenomenon. Our business scenarios are always
    dynamic in nature. The customers’ preferences and lifestyles will change, and
    there’s always some activity being done by the competitor. There are certain scenarios
    that are beyond our control, like war, COVID, etc. Hence, we always should strive
    to adjust our models to the latest scenario in our business.
  prefs: []
  type: TYPE_NORMAL
- en: Model refresh means that we are retraining the model based on the new data points
    we have collected. It ensures that we are capturing the latest trends, backgrounds,
    and emerging relationships in the data, and hence our models are able to predict,
    optimize, and expedite the latest data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this we have completed all the steps to design a machine learning system:
    how to develop it from scratch, how to deploy it, and how to maintain it. It is
    a long process that is quite tedious and requires teamwork.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.17 Concluding thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: End-to-end machine learning development is quite a time-consuming one. From
    scratch to maintenance, it requires a lot of planning, teamwork, business knowledge,
    and effort. In this chapter, we have covered a lot of those steps. There can be
    other possible solutions too, which are dependent on the business domain and the
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: With this we come to the close of this book. We all read and feel that in this
    new age, data is the new oil, new electricity, new power, and new currency. The
    field is rapidly growing and making its effect felt across the globe. The pace
    of enhancements and improvements has opened new job opportunities like data engineers,
    data scientists, visualization experts, machine learning engineers, MLOps, DevOps,
    GenAI experts, and so on, with demand increasing day by day. But there is a dearth
    of professionals who fulfill the rigorous criteria for these job descriptions.
    The need of the hour is to have *data artists* who can marry business objectives
    with analytical problems, envision solutions to solve the dynamic business problems,
    adjust to the ever-changing technical landscape, and yet deliver cost-effective
    business solutions.
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated systems are being created every day. We can see examples
    of self-driving cars, human chatbots, fraud detection systems, facial recognition
    solutions, object-detection solutions, optimization and monitoring solutions,
    etc. The use of GenAI has further enhanced the effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the same time, there are some risks too, which we should be aware of. The
    onus lies on humankind regarding how to harness this power of data. There are
    instances where (if we believe the claims made) AI has been used for rigging election
    results or DeepFake has been used for morphing pictures of people or profiling
    people based on race/color etc. We can use machine learning and AI to spread love
    or hatred–it is our choice. And like the cliché goes: with great power comes great
    responsibility!'
  prefs: []
  type: TYPE_NORMAL
- en: We sincerely hope you enjoyed the book. Congratulations, and all the very best
    for your next steps!
  prefs: []
  type: TYPE_NORMAL
- en: 11.18 Practical next steps and suggested readings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go through these two research papers on model deployment:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paleyes, A., Urma, R-G., and Lawrence, N. D. (2020). Challenges in Deploying
    Machine Learning: a Survey of Case Studies. [https://arxiv.org/abs/2011.09926v2](https://arxiv.org/abs/2011.09926v2)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sculley, D., Holt, G., Golovin, D., et al. (2015). Hidden Technical Debt in
    Machine Learning Systems. [https://mng.bz/4azw](https://mng.bz/4azw)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the datasets we have developed in the last few chapters and perform EDA
    on those datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The journey of learning is ongoing, requiring courage, patience, and diligence;
    understanding the entire process from conceptualization to model deployment is
    essential for mastering machine learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The end-to-end model deployment process involves key steps such as business
    problem definition, data cleaning, and EDA and culminating in model deployment
    and maintenance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The machine learning modeling process includes distinct stages such as business
    problem definition, data discovery and feasibility analysis, data prepreparation,
    EDA, modeling, deployment, documentation, and maintenance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clear and achievable business problem definition is crucial to align goals effectively,
    prevent scope creep, and ensure that KPIs are measurable to assess the model’s
    effect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data discovery involves identifying necessary datasets, ensuring access and
    completeness, and analyzing feasibility, with particular attention to data relevance,
    quality, and representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data cleaning and prepreparation address common problems like duplicates, categorical
    variables, missing data, and outliers, utilizing various techniques to prepare
    the dataset for effective modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EDA is key to understanding data patterns and relationships and generating actionable
    insights, laying the groundwork for successful model development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model development phase uses algorithms suitable for the business problem
    and requires stakeholder collaboration for refinement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model deployment bridges development and production, necessitating considerations
    for infrastructure, real-time applications, scaling, security, and continuous
    integration to optimize model utility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of model deployment include batch, real-time, edge, canary, and A/B testing,
    each offering different advantages based on strategic objectives and application
    contexts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective deployment involves accuracy monitoring, detecting model and data
    drift, securing compliance and data, and ensuring reproducibility and scalability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Postdeployment, thorough documentation and version control are vital for code
    integrity and facilitating future iterations or rollbacks when necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model maintenance involves regular performance checks and refreshes, adapting
    to dynamic business environments and ensuring alignment with evolving data trends.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data-driven solutions have vast potential but also an equally high duty of responsible
    use. We wrap up this book by stressing the importance of ethical application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
