["```py\n>ollama run gemma2:2b\n```", "```py\nollama run llama3.2-vision\n```", "```py\nPlease give me a detailed analysis of what's in this image. `Call` `out` `any` `major` `or` `minor` `features` `and` `tell` `me` `everything` `you` `know` `about` `it``.`\n`Are` `there` `any` `interesting` `and` `fun` `facts``?`\n`Maybe` `even` `estimate` `when` `this` `picture` `was` `taken``.`\n```", "```py```", "``` # Running Ollama as a Server    To put Ollama into server mode, you simply issue the following command:    ```", "```    This will run the Ollama server on port 11434 by default, so you can hit it and ask it to do inference with a `curl` command to test it.    In a separate terminal window, you issue a `curl` command. Here’s an example:    ```", "```   ```", "````` ```py```` Note the `stream` parameter. If you set it to true, you’ll get an active HTTP connection that will send the answer word by word. That will give you a faster time to the first word, which is very suitable for chat applications. And because the answer will appear little by little and usually faster than a person can read, it will make for a better user experience.    On the other hand, if you set the `stream` parameter to `false`, as I have done here, it will take longer to send something back, but when it does, you’ll get everything at once. The time to the last token will probably be about the same as in streaming, but given that there will be no output for a little while, it will feel slower.    The preceding `curl` to Gemma gave me this response:    ```py {\"model\":\"gemma2:2b\",\"created_at\":\"2024-12-09T18:10:05.711484Z\",     \"response\":\"The sky appears blue because ... phenomena! \\n\",     \"done\":true,     \"done_reason\":\"stop\",     \"context\":[106,1645,108,4385,603,573,...,235248,108],     \"total_duration\":7994972625,     \"load_duration\":820325334,     \"prompt_eval_count\":15,     \"prompt_eval_duration\":2599000000,     \"eval_count\":282,     \"eval_duration\":4573000000}% ```    I trimmed the response text and the context for brevity. Ultimately, you’d use the response text in an application, but I wanted to also show you how you can build more robust applications showing you everything else that the model provided.    The `done` parameter demonstrates that the prompt returned successfully. When the value is streaming, this parameter will be set to `false` until it has finished sending the text. That way, you can keep your UI updating the text word by word until the message is complete.    The `done_reason` parameter is useful in checking for errors, particularly when streaming. It will usually contain `stop` for normal completion, but in other circumstances, it might say `length`, which indicates that you’ve hit a token limit; `canceled` if the user cancels the request (by interrupting streaming, for example); or `error`.    The count values are also useful if you want to manage or report on token usage. The `prompt_eval_count` parameter tells you how many tokens were used in your prompt, and in this case, it was 15\\. Similarly, the `eval_count` parameter tells you how many tokens were used in the response, and in this case (of course), it was 282\\. The various duration numbers are in nanoseconds, so in this case, we can see that the total was 0.8 seconds (or more accurately, 820325334 nanoseconds).    If you want to attach a file to your `curl` prompt (for example, to interpret the contents of an image), you can do so by encoding the image to `base64` and passing it in the images array.    So, with the image of Osaka castle I used in [Figure 17-3](#ch17_figure_4_1748550058907880), I could do the following:    ```py curl -X POST \\   -H \"Content-Type: application/json\" \\   -d \"{\\\"model\\\":\\\"llama3.2-vision\\\", \\`\"prompt``\\\"``:``\\\"``What is in this image?``\\\"``,` ```` \\`\"images``\\\"``:[``\\\"``$(cat ./osaka.jpg | base64 | tr -d '``\\n``')``\\\"``],` ```py \\`\"stream``\\\"``:false}\"` \\        `http``:``//``localhost``:``11434``/``api``/``generate` ``` ```py` ```   ```py``` `````", "```py`The key here is to note how the images are sent. They need to be `base64` encoded, where they are turned into a string-like blob that’s easy to put into a JSON payload, instead of uploading the binary image. But you should be careful with code here, because it depends on your system. The code I used—`$(cat ./osaka.jpg | base64 | tr -d '\\n')`—is based on how to do `base64` encoding on a Mac. Different systems may produce different `base64` encodings for images, and they can lead to errors on the backend.    The response, abbreviated for clarity, is this:    ```", "```py    If you’re trying this on your development box, you may notice that it starts up slowly as it loads the model into memory. It can take one to two minutes, but once it’s loaded and warmed up, successive inferences will be quicker.```", "```py`` ```", "```py ```", "```py` ```", "```py``  ```", "```py`` ```", "```py` ```", "```py # Building an App that Uses an Ollama LLM    It’s all very nice to be able to chat with a local model or curl to it like we just saw. But the next step is to consider building applications that use local LLMs, and in particular, building applications that work within your network to keep LLM inference local.    So, for example, consider [Figure 17-4](#ch17_figure_5_1748550058907900), which depicts the architecture of a typical application that uses the API for an LLM like Gemini, GPT, or Claude. In this case, the user has an application that invokes the LLM service via an API over the internet.  ![](assets/aiml_1704.png)  ###### Figure 17-4\\. Accessing an LLM via API over the internet    Another pattern is quite similar: a service provider provides a backend web server, and that backend uses LLM functionality on your behalf. Ultimately, it still “wraps” the LLM on your behalf (see [Figure 17-5](#ch17_figure_6_1748550058907917)).  ![](assets/aiml_1705.png)  ###### Figure 17-5\\. Accessing an LLM via a backend web server    The issue here is that data that could be private to your users and that they share with you (in the blue boxes) gets passed to a third party across the internet (in the green boxes). This can lead to limitations in how your application might be useful to them. Consider scenarios where there’s information that should always be private or where there’s IP that you don’t want to share. In the early days of ChatGPT, a lot of companies banned its use for that reason. The classic case involves source code, where company IP might be sent to a competitor for analysis!    You can mitigate this problem by using a technology like Ollama. In this case, the architecture would change so that instead of the data being passed across the internet, the API and the server for the LLM would both be on your home or company’s network, as in [Figure 17-6](#ch17_figure_7_1748550058907933).  ![](assets/aiml_1706.png)  ###### Figure 17-6\\. Using an LLM in your data center    Now, there are no privacy issues with sharing data with third parties. Additionally, you have control over the model version that is being used, so you don’t have to worry about the API causing regression issues. Look back to Figures [17-4](#ch17_figure_5_1748550058907900) and [17-5](#ch17_figure_6_1748550058907917), and you’ll see that by accessing the LLMs over the internet, you’re taking a strong dependency on a particular version of an LLM. Given that LLMs are not deterministic, this effectively means the prompts that work today may not work tomorrow!    So, with Ollama as a server to LLMs, you can build something like you saw in [Figure 17-6](#ch17_figure_7_1748550058907933). In the development environment and what we’ll be doing in the rest of this chapter, there’s no data center—the Ollama server will just run at the localhost, but changing your code to one that you have access to over the local network will just mean a change of server address.    ## The Scenario    As an example of a simple scenario, let’s build the basis of an app that uses a local llama to do analysis of books. It will allow the user to specify a book (as a text file, for simplicity’s sake), and it will bundle that to a call to an LLM to have it analyze the book. The app will also primarily be driven off a prompt to that backend. Perhaps this type of app could be used by a publishing house to determine how it should give feedback to an author to change a book, or by a book agent to work with the author to help them make the book more sellable. As you can imagine, the book content is valuable IP, and it should not be shared with a backend from a third party for analysis.    So, for example, consider this prompt:    ```", "```py    Sending that to an LLM along with the text should have the desired effect: getting an analysis of the book based on the *artificial understanding* of the contents of the text and the generative abilities of a transformer-based model to create output guided by the prompt.    On this book’s GitHub page, I’ve provided [the full text of a novel](https://oreil.ly/pytorch_ch18) that I wrote several years ago and that I now have the full rights to, so you can try it with a real book like that one if you like. However, depending on your model, the context window size might not be big enough for a complete novel.    Generally, I like to build a simple proof-of-concept as a Python file to see how well it works and test it on my local machine. There are constraints in doing this, but it will at least let us see if the concept is feasible.    ## Building a Python Proof-of-Concept    Now, let’s take a look at a Python script that can perform the analysis for us.    Let’s start with reading the contents of the file:    ```", "```py    We’re going to use Ollama on the backend, so let’s set up details for the request by specifying the URL and the content headers we’ll call:    ```", "```py    Next, we’ll put together the payload that we’re going to post to the backend. This contains the model name (as a parameter called `model`), the prompt, and the stream flag. We don’t want to stream, so we’ll set the stream to `False`.    ```", "```py `\"stream\"``:` `False` `}` ```", "```py   ```", "```py`Note that we’re appending `file_content` to the prompt. At this point, from the preceding code, it’s just a text blob.    For the model parameter, you can use whatever you like. For this experiment, I tried using the very small `Gemma 2b` parameter model by specifying `gemma2:2b` as the model.    Now, we can POST the request to Ollama like this:    ```", "```py    This is fully synchronous in that we post the data and block everything until we get the result. In a real application, you’d likely do that part asynchronously, but I’m keeping it simple for now.    As we get the response back from the server, it will contain JSON fields, and the response field will contain the text, as we saw earlier in this chapter. We’ll return this as a `dict` data type.    Next, we can wrap all this code in a function called `analyze`, with this signature:    ```", "```py    Then, we can easily call it with this:    ```", "```py    Given that Gemma is such a small model, the output it gives is very impressive! My local instance, via Ollama, was able to digest the book and give back a detailed analysis in just a few seconds. Here’s an excerpt (with spoilers if you haven’t read the book yet):    ```", "```py    It’s pretty impressive work by Gemma to give me this analysis! Other than formatting issues (there are a lot of * characters, which Gemma may have added because it’s sci-fi), we have some great content here, and it’s worth looking further into building an app. So, let’s do that next and explore a web-based app that I can upload my files to so that I can get an analysis within a website.```", "```py``  ```", "```py`## Creating a Web App for Ollama    In this scenario, you’ll create a `node.js` app that provides a local website that the user can upload text to, and you’ll get an analysis back in the browser. The results look like those shown in [Figure 17-7](#ch17_figure_8_1748550058907947).  ![](assets/aiml_1707.png)  ###### Figure 17-7\\. The browser-based Gemma analysis tool    If you’re not familiar with `node.js` and how to install it, full instructions are on [the Node website](http://nodejs.org). The architecture of a simple node app is shown in [Figure 17-8](#ch17_figure_9_1748550058907961).  ![](assets/aiml_1708.png)  ###### Figure 17-8\\. The node app directory    In its simplest form, a *node app* is a directory containing a JavaScript file called *app.js* that contains the core application logic, a *package.json* file that gives details of the dependencies, and an *index.html* file that has the template for the app output.    ## The app.js File    This file contains the core logic for the service. A `node.js` app will run on a server by listening to a particular port, and it starts with this code:    ```", "```py    To analyze a book, you can define an endpoint that the end user can post the book to with the `app.post` command in `node.js`:    ```", "```py    This function is asynchronous, and it accepts a file. If the file isn’t present in the upload, an error will return.    If the code continues, then there’s a file present. This code will start a new job (allowing the server to operate multiple processes in parallel) that uploads the file, reads its text into `fileContent`, and then cleans it up:    ```", "```py    The analysis of the novel is a long-running process. In it, the contents are appended to the prompt and uploaded to Ollama, which then passes it to Gemma—which, upon completion, sends us a result. We’ll look at the analysis code in a moment, but the wrapper for this that operates in the background is here:    ```", "```py    It simply calls the `analyzeNovel` method, sending the file content. If the process completes successfully, the `JobId` is updated with `completed`; otherwise, it’s updated with details of the failure. Note that upon a successful completion, the result is passed to `analysisJobs`. Later, when we look at the web client, we’ll see that after uploading the content, it will repeatedly poll the status of the job until it gets either a completion or an error. At that point, it can display the appropriate output.    The `analyzeNovel` function looks very similar to our Python code from earlier. First, we’ll create the request body with the prompt. It contains the `modelID`, the prompt text with the novel text appended, and the `stream` parameter set to false:    ```", "```py    It will then post this to the Ollama backend and wait for the response. When it gets it, it will turn it into a string with `JSON.stringify`:    ```", "```py    If there’s an error, we’ll throw it; otherwise, we’ll read in the JSON payload from the HTTP response and filter out the response field, which contains the text response from the LLM. Note the two uses of the word *response* here. It can be confusing! The *response object* (`response.ok`, `response.status`, or `response.json`) is the HTTP response to the post that you made, while the *response property* (`data.response`) is the field within the `json` that contains the response from the LLM with the generated output:    ```", "```py    ## The Index.html File    In the public folder, a file called *index.html* will render when you call the `node.js` server. By default, this is at `localhost:3000` if you’re running on your dev box. It will contain all the code to render the user interface for the app that we saw in [Figure 17-7](#ch17_figure_8_1748550058907947).    It interfaces with the backend through a form that is submitted via an HTTP-POST. The HTML code for the form looks like this:    ```", "```py    The form is called `uploadForm`, so you can write code to execute when the user hits the submit button on this form like this:    ```", "```py    The magic happens within this code by taking the attached file (as `FormData`) and passing it to the `/analyze` endpoint of the backend, as we defined using `app.post` in the *app.js* file (seen previously):    ```", "```py    Once this is done, the browser will get the `jobID` back from the server and continually poll the server asking for the status of that `jobID`. Once the status of the job is `completed`, the server will output the results if the job succeeded or the error if it didn’t:    ```", "```py    The `setTimeout` code at the bottom ensures that we poll every second, but you could change this to reduce load on your server.    To run this, simply navigate to the directory in your console and type this:    ```", "```py    And that’s it! You can get the fully working code in the downloadable files for this book, and I’ve also included a copy of the novel as a text file so you can try it out for yourself.```", "```py``  ```", "```py ```", "```py ```", "```py` ```", "```py`` ```"]