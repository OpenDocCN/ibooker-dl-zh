<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">8</span> </span> <span class="chapter-title-text">Chatting with your data</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">How bringing your data benefits enterprises</li> 
    <li class="readable-text" id="p3">Installing and using a vector database and vector index</li> 
    <li class="readable-text" id="p4">Planning and retrieving your proprietary data</li> 
    <li class="readable-text" id="p5">Using a vector database to conduct searches</li> 
    <li class="readable-text" id="p6">How to implement an end-to-end chat powered by RAG using a vector database and an LLM</li> 
    <li class="readable-text" id="p7">The benefits of bringing your data and RAG jointly</li> 
    <li class="readable-text" id="p8">How RAG benefits AI safety for enterprises</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>Utilizing large language models (LLMs) for a chat-with-data implementation is a promising strategy uniquely suitable for enterprises seeking to harness the power of generative artificial intelligence (AI) for their specific business requirements. By synergizing the LLM capabilities with enterprise-specific data sources and tools, businesses can forge intelligent and context-aware chatbots that deliver invaluable insights and recommendations to their clientele and stakeholders.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>At a high level, there are two ways to chat with your data using an LLM—one is by employing a retrieval engine as implemented using the retrieval-augmented generation (RAG) pattern, and another is to custom-train the LLM on your data. The latter is more involved and complex and not available to most users.</p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>This chapter builds on the RAG pattern from the last chapter used to enhance LLMs with your data, especially when enterprises want to implement it at the scale for production workloads. When enterprises integrate their data using a RAG pattern with LLMs, they unlock many advantages, enhancing the functionality and applicability of these AI systems in their unique business contexts. The chapter outlines how these are different and, in many cases, better than larger context windows. Let’s start by identifying the advantages enterprises can get when wanting to bring in their data.</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_133"><span class="num-string">8.1</span> Advantages to enterprises using their data</h2> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>In the dynamic realm of business technology, integrating LLMs into enterprise data systems heralds a transformative era of interactive and intuitive processes. As we explored earlier, these cutting-edge AI-driven tools are reshaping how businesses engage with their data, thus opening up unprecedented avenues of efficiency and accessibility.</p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>LLMs have achieved impressive results in various natural language processing (NLP) tasks, such as answering questions, summarization, translation, and dialogue. However, LLMs have limitations and challenges, such as data quality, ethical problems, and scalability. Therefore, many enterprises are interested in implementing a chat with their data implementation using LLMs, which offer several advantages for their business goals.</p> 
  </div> 
  <div class="readable-text intended-text" id="p15"> 
   <p>One of the main advantages of using LLMs for this purpose is that they can provide intelligent and context-aware chatbots that can handle customer queries and concerns with human-like proficiency. LLMs can understand the meaning and intent of the user’s input, generate relevant and coherent responses, and even take action by invoking APIs as needed. This improves customer satisfaction and frees human agents to focus on more complex tasks. Another advantage of using LLMs for chat with data implementation is that they can be customized with enterprise-specific data, which leads to more accurate and relevant AI-generated insights and recommendations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p16"> 
   <p>Finally, using LLMs for chat with data implementation can enable more efficient and effective data analysis. LLMs can generate natural language summaries or explanations of the data analysis results, which can help users understand the key findings and implications. In addition, LLMs can generate interactive charts or graphs highlighting the patterns or trends in the data. These features can enhance the user experience and facilitate data-driven decision-making across the organization.</p> 
  </div> 
  <div class="readable-text" id="p17"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_134"><span class="num-string">8.1.1</span> What about large context windows?</h3> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>The most recent models from OpenAI—for example, the GPT-4 Turbo with a 128K context window and Google’s newest Gemini Pro 1.5 with 1.5 million token content windows—have generated much enthusiasm and interest. However, a bigger context window alone is not enough. Training an LLM on your data has the following benefits over just using an LLM with a larger context window:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p19"> <em>More accurate and informative answers</em>—When chatting with your data, the LLM can access much more information than it would with a larger context window alone. This allows the LLM to provide more accurate and informative answers to your questions. </li> 
   <li class="readable-text" id="p20"> <em>More personalized answers</em>—The LLM can also learn to personalize its answers based on your data. For example, if you chat with an LLM that has been fine-tuned on your customer data, it can learn to provide more relevant answers to your specific customers and their needs. For example, we can use a retrieval engine to index its customer data and then connect the retrieval engine to an LLM. This would allow the company to chat with its customers in a more personalized and informative way. </li> 
   <li class="readable-text" id="p21"> <em>More creative answers</em>—The LLM can also use your data to generate more creative and interesting answers to your questions. For example, if you chat with an LLM that has fine-tuned your product data, the LLM can learn to generate new product ideas or marketing campaigns. </li> 
  </ul> 
  <div class="readable-text" id="p22"> 
   <p>Of course, LLMs with a larger context window have their own benefits, but they can be a double-edged sword with some limitations. Larger context windows allow us to pass in more information in one API call and worry less about chunking up the application. For example, the recently announced GPT-4.5 Turbo has a 128K context window, allowing for approximately 300 pages of text in a single prompt, compared to approximately 75 pages from the earlier GPT-4 32K model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p23"> 
   <p>On the flip side, having a larger context window has its challenges. For example, larger context window LLMs can be more computationally expensive to train and deploy. They can also be more prone to generating hallucinations or incorrect answers, as large context windows increase the complexity and uncertainty of the model’s output. LLMs are trained on large, diverse datasets that may contain incomplete, contradictory, or noisy information. When the model is given a long context window, it must process more information and decide what to generate next, which can lead to errors, inconsistencies, or fabrications in the output, especially if the model relies on heuristics or memorization rather than reasoning or understanding.</p> 
  </div> 
  <div class="readable-text intended-text" id="p24"> 
   <p>In contrast, chatting with your data can be more efficient and less prone to errors, mainly because when chatting with our data, we are grounding on that data and steering the model to use. The LLM can access a wider range of information and learn to personalize its answers based on your data. Ultimately, the best way to choose between a larger context window LLM and chatting with your data will depend on your specific needs and resources.</p> 
  </div> 
  <div class="readable-text" id="p25"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_135"><span class="num-string">8.1.2</span> Building a chat application using our data</h3> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>We will build on the RAG implementation from the last chapter and build a chat application that we can use to chat our data. As we saw before, vector databases are key for enterprises, enabling them to manage, secure, and scale embeddings in a production environment. For many enterprises, vector databases for semantic search use cases solve the performance and security requirements needed for production systems. Figure 8.1 shows the approach at a high level for incorporating LLM on our data.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p27">  
   <img alt="figure" src="../Images/CH08_F01_Bahree.png" width="1009" height="409"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.1</span> Azure OpenAI on your data</h5>
  </div> 
  <div class="readable-text" id="p28"> 
   <p>For example, we will use my blog (<a href="https://blog.desigeek.com">https://blog.desigeek.com</a>) as the proprietary data source. It has posts going back 20 years across various topics and technologies. If for every question a user asks we go back to the blog, load up all the posts, create embeddings, search through those, and then use RAG to answer the question, the process will be very time-consuming and not scalable. In addition, there will be added costs, as we will be using many more tokens on each conversation turn or for the new set of conversations. A better approach would be to set the following four stages we will go through:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p29"> Reading and injecting the information (i.e., retrieval) </li> 
   <li class="readable-text" id="p30"> Creating the embeddings and saving the details to Redis </li> 
   <li class="readable-text" id="p31"> Searches against the saved details for a Q&amp;A implementation using the blog posts (i.e., augmenting) </li> 
   <li class="readable-text" id="p32"> Plugging this into the LLM generation </li> 
  </ul> 
  <div class="readable-text" id="p33"> 
   <p>Let’s start by setting up a vector database.</p> 
  </div> 
  <div class="readable-text" id="p34"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_136"><span class="num-string">8.2</span> Using a vector database</h2> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>As we saw earlier, a vector database has been designed to operate on embedding vectors. For most enterprise use cases, they are a great addition to RAG implementations and allow us to use our data. Many vector databases are available today, and with the increasing popularity of LLMs and generative AI, there is more support for semantic search each day. Let’s see how we can implement this.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>In our learning context, we want something quick and easy to set up and run, mainly to understand the different concepts and steps required to deploy a vector database for embeddings and how to integrate it into our RAG implementation. For this purpose, we will use Redis as a vector database and run it locally in a Docker container.</p> 
  </div> 
  <div class="readable-text intended-text" id="p37"> 
   <p>Redis is an open source, in-memory, key–value data store that can be used as a database, cache, message broker, and more. It supports data structures such as strings, lists, sets, hashes, and streams. Redis is fast, scalable, and reliable, which makes it popular for many use cases that require low latency and high throughput.</p> 
  </div> 
  <div class="readable-text intended-text" id="p38"> 
   <p>Redis expands its core capabilities using the concept of modules. Redis Search is a module that extends Redis with powerful text search and secondary indexing capabilities. It lets you create indexes on your Redis data and query them using a rich query language. You can also use Redis Search for vector similarity search, which enables semantic search based on embeddings.</p> 
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>There are several ways to deploy Redis. For local development, the quickest method is to use the Redis Stack Docker container, which we will use. Redis Stack contains several Redis modules that, for our purpose, can be used together to create a fast, multimodel data store and query engine. More details on the Redis Stack Docker container are available at <a href="https://hub.docker.com/r/redis/redis-stack">https://hub.docker.com/r/redis/redis-stack</a>.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p40"> 
   <p><span class="print-book-callout-head">NOTE </span> The main prerequisite here is that Docker should already be installed and configured for you to use. The details of Docker installations are outside the book’s scope, as there are books dedicated to Docker and its management. If you don’t have Docker installed, please see the documentation for installing Docker Desktop for a more manageable experience or, at a minimum, the Docker engine. More details can be found at <a href="https://docs.docker.com/desktop/">https://docs.docker.com/desktop/</a>.</p> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>In addition to the OpenAI packages, the following prerequisites are needed for us to get Redis running:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p42"> Docker must be installed and running locally. </li> 
   <li class="readable-text" id="p43"> When using conda, the <code>redis-py</code> package can be installed with <code>conda</code> <code>install -c</code> <code>conda-forge</code> <code>redis-py</code>. If we are using pip, then use <code>pip</code> <code>install</code> <code>redis</code>. </li> 
  </ul> 
  <div class="readable-text" id="p44"> 
   <p>We will use a docker-compose file for Docker, as shown in listing 8.1. We have not changed the default ports, but you can configure them as you see fit for your environment. In this example, we pull the latest <code>redis-stack</code> image from the Docker registry and expose two ports—6379 and 8001. We also set up a data volume to persist the information populated in the database. And finally, we set up some initial health checks to check basic things, such as that the service is up and running and reachable at the configured ports. If you change the ports, ensure this is updated in the test as part of the health check.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p45"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.1</span> docker-compose file for <code>redis-stack</code></h5> 
   <div class="code-area-container"> 
    <pre class="code-area">version: '3.7'
services:

  vector-db:
    image: redis/redis-stack:latest
    ports:
      - 6379:6379
      - 8001:8001
    environment:
      - REDISEARCH_ARGS=CONCURRENT_WRITE_MODE
    volumes:
      - vector-db:/var/lib/redis
      - ./redis.conf:/usr/local/etc/redis/redis.conf
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "localhost", "-p", "6379", "ping"]
      interval: 2s
      timeout: 1m30s
      retries: 5
      start_period: 5s

volumes:
  vector-db:</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>For Docker to run, as per convention, we must ensure this file is saved as a <code>docker-compose.yml</code> file. You can start this by entering the following commands from the same location where the file is saved: <code>docker compose up -d</code>. In our example, the container runs via the Docker Desktop GUI, as shown in figure 8.2.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p47">  
   <img alt="figure" src="../Images/CH08_F02_Bahree.png" width="935" height="579"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.2</span> Docker Desktop running Redis container</h5>
  </div> 
  <div class="readable-text" id="p48"> 
   <p>This also includes Redis Insight, a GUI for managing our Redis database. Once the Docker container runs, we can access it locally at <code>http://localhost:8001</code>. If everything is set up correctly, we can see the database and installed modules (figure 8.3).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p49">  
   <img alt="figure" src="../Images/CH08_F03_Bahree.png" width="829" height="607"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.3</span> Redis database with search running locally in a container</h5>
  </div> 
  <div class="readable-text" id="p50"> 
   <p>Now that we have our vector database up and running, let us work through the next step of retrieving the information. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p51"> 
   <p><span class="print-book-callout-head">Note </span> We use Redis as an example in this chapter, as it is relatively easy to run it locally in a container for enterprises to try out the concepts and get a handle on the associated complexities. Given that it runs locally in a container, it also helps alleviate any initial matters about data going into the cloud, which might be a concern, at least in the early days of development. In addition to Redis, a few other vector databases are becoming increasingly popular. Some of the more popular vector databases are Azure AI Search, Pinecone, and Milvus.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p52"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Azure AI Search</h5> 
   </div> 
   <div class="readable-text" id="p53"> 
    <p>Although we are using Redis running locally, enterprises that need to scale to a larger corpus of data, indexes, and product-scale workloads and availability start getting much more complex. For such scenarios, Azure AI Search is a good choice.</p> 
   </div> 
   <div class="readable-text" id="p54"> 
    <p>Azure AI Search is a cloud-based service that provides various features for building search applications. In addition to offering a vector search, which allows you to find and retrieve data objects that are semantically similar to a given query based on their vector embeddings, it also supports hybrid search. Hybrid search combines full-text and vector queries that execute against a search index containing searchable plain text content and generated embeddings. In a single-search request, hybrid queries can use existing functionality, such as filtering, faceting, sorting, scoring profiles, and semantic ranking. The query response provides just one result set, using reciprocal rank fusion (RRF) to determine which matches are included.</p> 
   </div> 
   <div class="readable-text" id="p55"> 
    <p>Azure AI Search offers several benefits over Redis for vector searches with LLMs. It is a fully managed search service that can index and search structured, semi-structured, and unstructured data. Azure AI Search is highly scalable and can easily handle large amounts of data. It supports more robust security features that enterprises require, such as rest and transit encryption, role-based access control (RBAC), and more. You can find more details at <a href="https://learn.microsoft.com/en-us/azure/search/">https://learn.microsoft.com/en-us/azure/search/</a>. </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p56"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_137"><span class="num-string">8.3</span> Planning for retrieving the information</h2> 
  </div> 
  <div class="readable-text" id="p57"> 
   <p>First, we must understand what we are trying to retrieve and index. This helps us formulate the approach and determine which pieces of information are essential and which are redundant and can be ignored. As part of this exercise, we also need to factor in the technical aspects, such as how we connect to the source system and any technical or practical limitations. We must also understand the data format and engineering requirements (including data cleaning and conversions).</p> 
  </div> 
  <div class="readable-text intended-text" id="p58"> 
   <p>Before we get the data from the blog, take a look at the details outlined in table 8.1.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p59"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 8.1</span> Data items for blog posts we are interested in</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Data 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  URL <br/></td> 
      <td>  The URL to the individual blog post <br/></td> 
     </tr> 
     <tr> 
      <td>  Title <br/></td> 
      <td>  Title of the blog post <br/></td> 
     </tr> 
     <tr> 
      <td>  Description <br/></td> 
      <td>  A couple of sentences describing what that specific blog post is about <br/></td> 
     </tr> 
     <tr> 
      <td>  Publish date <br/></td> 
      <td>  Date when the post was published <br/></td> 
     </tr> 
     <tr> 
      <td>  Content <br/></td> 
      <td>  The actual content of the blog post <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>Although we are using a blog post as a source system, it is a holistic example representing most of the RAG aspects and helping us to understand the best practices and how to approach them. We are retrieving the information from a remote system to read the blog posts. This is fundamentally similar to enterprises reading information for various line-of-business systems. Depending on the source system, they read this via APIs, exported files, or connecting to various databases and data sources.</p> 
  </div> 
  <div class="readable-text intended-text" id="p61"> 
   <p>In our example, we will read all the posts using the blog’s RSS feed. RSS stands for really simple syndication, a standard website content distribution method often used to publish changes. The blog can be found at <a href="https://blog.desigeek.com/">https://blog.desigeek.com/</a>, and the corresponding RSS feed is available at <a href="https://blog.desigeek.com/index.xml">https://blog.desigeek.com/index.xml</a>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p62"> 
   <p>First, we assume Redis runs locally in a container, as shown earlier. We will connect to Redis and create a new index called <code>posts</code>. The schema for the index is shown in the next listing and represents the structure of our data that we saw earlier. In addition to the main content of the blog post, we also capture associated metadata that will help us answer questions or understand the context better.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p63"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.2</span> Redis index schema</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">SCHEMA = [
    TagField("url"),
    TextField("title"), 
    TextField("description"),
    TextField("publish_date"),
    TextField("content"),
    VectorField("embedding", "HNSW",
                {"TYPE": "FLOAT32",
                 "DIM": 1536,
                 "DISTANCE_METRIC": "COSINE"}),
   ]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>This schema contains the following types of fields:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p65"> <code>TagField</code>—Used to store tags, which are short, descriptive keywords that can be employed to categorize and organize data. Tags are typically stored as a list of strings, and Redis search supports searching for tags with Boolean operators such as <code>AND</code>, <code>OR</code>, and <code>NOT</code>. </li> 
   <li class="readable-text" id="p66"> <code>TextField</code>—Used to store text data, such as the title, description, and content of a blog post. Redis search supports full-text search on <code>TextField</code>s, meaning you can search for words and phrases in the text. </li> 
   <li class="readable-text" id="p67"> <code>VectorField</code>—Stores vectors’ mathematical representations of data that can be used to perform machine learning tasks, such as image classification and natural language processing. Redis search supports vector similarity search, meaning you can search for vectors similar to a given vector. </li> 
  </ul> 
  <div class="readable-text" id="p68"> 
   <p>Most of the field names are self-explanatory, except the field called <code>"embedding"</code> of the <code>VectorField</code> type, which is used to store high-dimensional vectors. Redis supports two similarity search algorithms, FLAT and HNSW; in our example, we use HNSW. </p> 
  </div> 
  <div class="readable-text intended-text" id="p69"> 
   <p>HSNW stands for <em>hierarchical navigable small world</em>. It’s an algorithm used for nearest neighbor search in multidimensional spaces and is used here as the embedding type. The HNSW algorithm is particularly useful for tasks such as similarity search or clustering in high-dimensional spaces. It is known for its efficiency and accuracy with lower computational overhead. HNSW organizes vectors into a graph structure.</p> 
  </div> 
  <div class="readable-text intended-text" id="p70"> 
   <p>FLAT stands for <em>fast linear approximation transformation</em>. It is a brute-force algorithm and straightforward approach in which all vectors are indexed in a single tree or list structure. Finding the nearest neighbors of a query point is typically a brute-force search implemented by computing the distance from the query point and other indexes. This makes it much more accurate but computationally intensive and slower.</p> 
  </div> 
  <div class="readable-text intended-text" id="p71"> 
   <p>The embeddings are float numbers, as denoted by FLOAT32. We set the dimensions to match the Azure OpenAI models’ 1536 dimensions, which must match the LLM’s architecture. Finally, we use the COSINE distance metric to measure similarity. Redis supports the three types of distance metrics (see table 8.2).</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p72"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 8.2</span> HNSW distance metric options</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         HNSW distance metric 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  EUCLIDEAN <br/></td> 
      <td>  The straight-line distance between two points in Euclidean space. It’s a good choice when all dimensions are similar (e.g., all distances measured in meters). <br/></td> 
     </tr> 
     <tr> 
      <td>  DOTPRODUCT  <br/></td> 
      <td>  Calculates the dot product between two vectors. The dot product is the sum of the products of the corresponding entries of the two sequence numbers. <br/></td> 
     </tr> 
     <tr> 
      <td>  COSINE  <br/></td> 
      <td>  Calculates the cosine of the angle between two vectors. Regardless of their magnitude, it measures how similar the vectors are. This is often used in text analysis, where the direction of the vector (the angle) is more important than the length of the vector. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p73"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">TagField vs. TextField</h5> 
   </div> 
   <div class="readable-text" id="p74"> 
    <p>The <code>URL</code> field is a <code>TagField</code> instead of a <code>TextField</code>. While this might seem odd at first, there is a good reason. With <code>TagField</code>, the entire URL is treated as a single tag. This property is useful if you want to search for documents using the exact URL. However, searching for documents containing certain words in their URL would be useless because the URL is not tokenized.</p> 
   </div> 
   <div class="readable-text" id="p75"> 
    <p>In contrast, if the URL field were defined as a <code>TextField</code>, it would be tokenized, and each part of the URL would be indexed separately. This would be useful if you searched for documents containing certain words in their URL. However, it would not be useful if you wanted to search for documents by exact URL because the URL would be tokenized. </p> 
   </div> 
   <div class="readable-text" id="p76"> 
    <p>In this case, if we ran a search that required tokenization (i.e., searching for documents that contain a certain word in their URL), the search would not return the expected results. Similarly, if you define the <code>URL</code> as a <code>TextField</code> and then try to perform a search that requires exact matching (i.e., searching for documents by exact URL), the search will not return the expected results.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>Now that we understand the data that we need and the associated schema, let’s create the index to begin within Redis. We start by connecting to the Redis database, which, in our case, is running locally on Docker and reachable over port 6379, as shown in listing 8.3.</p> 
  </div> 
  <div class="readable-text intended-text" id="p78"> 
   <p>We need the following environment variables pointing to the server host, the port, and the password to set, respectively:</p> 
  </div> 
  <div class="readable-text intended-text" id="p79"> 
   <p>For Windows, use</p> 
  </div> 
  <div class="browsable-container listing-container" id="p80"> 
   <div class="code-area-container"> 
    <pre class="code-area">setx REDIS_HOST "your-host-details"
setx REDIS_PORT "Port-number-exposed"
setx REDIS_PASSWORD "Password-required-to-connect"</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p81"> 
   <p><span class="print-book-callout-head">Note</span>  You must restart your terminal to read the new variables.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>On Linux/Mac, use</p> 
  </div> 
  <div class="browsable-container listing-container" id="p83"> 
   <div class="code-area-container"> 
    <pre class="code-area">export REDIS_HOST="your-host-details"
export REDIS_PORT="Port-number-exposed"
export REDIS_ PASSWORD="Password-required-to-connect"Bash:
echo export REDIS_HOST="your-host-details" &gt;&gt; /etc/environment &amp;&amp; source /etc/environment
echo export REDIS_PORT="Port-number-exposed" &gt;&gt; /etc/environment &amp;&amp; source /etc/environment
echo export REDIS_ PASSWORD="Password-required-to-connect" &gt;&gt; /etc/environment &amp;&amp; source /etc/environment</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>We first need to establish a connection with the Redis server, which is quite straightforward:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p85"> 
   <div class="code-area-container"> 
    <pre class="code-area"># Connect to the Redis server
conn = redis.Redis(host=redis_host, 
                   port=redis_port,
                   password=redis_password, 
                   encoding='utf-8', 
                   decode_responses=True)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>Because we already have our schema defined, as shown in listing 8.2, creating a vector index is straightforward. We call the function <code>create_index</code> and pass it a name, schema, and optional prefix. Only two indexes are supported—<code>HASH</code> (the default) or <code>JSON</code>—for which we need a separate module. In our case, we will use the default <code>HASH</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p87"> 
   <div class="code-area-container"> 
    <pre class="code-area">conn.ft(index_name).create_index(
     fields=schema,
     definition=IndexDefinition(prefix=["post:"],
                                       index_type=IndexType.HASH))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>Of course, we can delete the index and view its details. The full code for this helper function is shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p89"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.3</span> Redis search index operations</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import redis
from redis.commands.search.field import VectorField, TextField
from redis.commands.search.query import Query
from redis.commands.search.indexDefinition import 
     <span class="">↪</span>IndexDefinition, IndexType
from redis.commands.search.field import TagField

redis_host = os.getenv('REDIS_HOST')          <span class="aframe-location"/> #1
redis_port = os.getenv('REDIS_PORT')          #1
redis_password = os.getenv('REDIS_PASSWORD')  #1

conn = redis.Redis(host=redis_host,          <span class="aframe-location"/> #2
                   port=redis_port,          #2
                   password=redis_password,  #2
                   encoding='utf-8',         #2          
                   decode_responses=True)    #2          

SCHEMA = [
    TagField("url"),
    TextField("title"), 
    TextField("description"),
    TextField("publish_date"),
    TextField("content"),
    VectorField("embedding", "HNSW",
                {"TYPE": "FLOAT32",
                 "DIM": 1536,                      <span class="aframe-location"/> #3
                 "DISTANCE_METRIC": "COSINE"}),
]

def create_index(conn, schema, index_name="posts"):
    try:
        conn.ft(index_name).create_index(
            fields=schema,
            definition=IndexDefinition(prefix=["post:"],
                                       index_type=IndexType.HASH))
    except Exception as e:
        print("Index already exists")

def delete_index(conn, index_name="posts"):           <span class="aframe-location"/> #4
    try:
        conn.execute_command('FT.DROPINDEX', index_name)
    except Exception as e:
        print("Failed to delete index: ", e)

def delete_all_keys_from_index(conn, index_name="posts"):   <span class="aframe-location"/> #5
    try:
        # 1. Retrieve all document IDs from the index.
        # This assumes the total number of documents isn't large. 
        # If it is, you might want to paginate the query.
        result = conn.execute_command('FT.SEARCH',
                                      index_name,
                                      '*', 
                                      'NOCONTENT')

        # 2. Parse the result to get document IDs.
        # Skip the first element which is the total count.
        # Taking every second element starting from the first.
        doc_ids = result[1::2]

        # 3. Delete each document key.
        for doc_id in doc_ids:
            conn.delete(doc_id)

    except Exception as e:
        print("Failed to delete keys: ", e))

def view_index(conn, index_name="posts"):                <span class="aframe-location"/> #6
    try:
        info = conn.execute_command('FT.INFO', index_name)
        for i in range(0, len(info), 2):
            print(f"{info[i]}: {info[i+1]}")
    except Exception as e:
        print("Failed to retrieve index details: ", e)

def main():
    while True:                                      <span class="aframe-location"/> #7
        print("1. View index details")
        print("2. Create index")
        print("3. Delete index")
        print("4. Exit")
        choice = input("Enter your choice: ")

        if choice == '1':
            # Call the function to view index
            view_index(conn)
            pass
        elif choice == '2':
            # Call the function to create index
            create_index(conn, SCHEMA)
        elif choice == '3':
            # Call the function to delete index
            delete_all_keys_from_index(conn)
            delete_index(conn)
        elif choice == '4':
            break
        else:
            print("Invalid choice. Please enter a valid option.")

if __name__ == "__main__":
    main()</pre> 
    <div class="code-annotations-overlay-container">
     #1 Redis connection details
     <br/>#2 Connects to the Redis server
     <br/>#3 Sets the dimensions to match the LLM design
     <br/>#4 Function to delete index
     <br/>#5 Function to delete the keys from the index
     <br/>#6 Function to create an index
     <br/>#7 Function to run the main loop
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>Figure 8.4 shows this code running locally as an example. The index type is HASH, and the keys’ prefix starts with “post.”</p> 
  </div> 
  <div class="browsable-container figure-container" id="p91">  
   <img alt="figure" src="../Images/CH08_F04_Bahree.png" width="1100" height="650"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.4</span> Redis Insight running locally as an example<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p92"> 
   <p>In our case, we already have the index populated, and when we execute this to see the index, we obtain an output similar to the following listing. Note that the output has been truncated for brevity.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p93"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.4</span> Redis search index details</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">index_name: posts
index_options: []
index_definition: ['key_type', 'HASH', 'prefixes', ['post:'], 'default_score', '1']
attributes: [['identifier', 'url', 'attribute', 'url', 'type', 
    <span class="">↪</span>'TAG', 'SEPARATOR', ','], ['identifier', 'title', 'attribute', 
    <span class="">↪</span>'title', 'type', 'TEXT', 'WEIGHT', '1'], ['identifier', 
    <span class="">↪</span>'description', 'attribute', 'description', 'type', 'TEXT', 
    <span class="">↪</span>'WEIGHT', '1'], ['identifier', 'publish_date', 'attribute', 
    <span class="">↪</span>'publish_date', 'type', 'TEXT', 'WEIGHT', '1'], ['identifier', 
    <span class="">↪</span>'embedding', 'attribute', 'embedding', 'type', 'VECTOR']]
num_docs: 1304
max_doc_id: 1304
num_terms: 3047
num_records: 14092
vector_index_sz_mb: 12.586814880371094
total_inverted_index_blocks: 4370
offset_vectors_sz_mb: 0.011086463928222656
doc_table_size_mb: 0.09221076965332031
key_table_size_mb: 0.03916168212890625
total_indexing_time: 708.988
...</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>Figure 8.5 shows the details of one of the index items using Redis Insight—the GUI that allows us to do some basic database management. We can see the fields we identified when setting up the index. The embeddings are a binary representation, so they appear to be gibberish.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p95">  
   <img alt="figure" src="../Images/CH08_F05_Bahree.png" width="1100" height="652"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.5</span> Index details</h5>
  </div> 
  <div class="readable-text" id="p96"> 
   <p>Now that we have an index set, let’s see how we can retrieve the data (i.e., the blog posts), chunk it, populate the vector database, and finally update the index we created.</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_138"><span class="num-string">8.4</span> Retrieving the data</h2> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>At a high level, the process is quite simple. We start loading RSS feeds using the <code>feedparser</code> library; then, we retrieve each blog post found, parse it for the content we are interested in, create the corresponding embedding, and save all the details in Redis. Listing 8.5 shows this flow.</p> 
  </div> 
  <div class="readable-text intended-text" id="p99"> 
   <p>Because each blog post is an HTML page, we use <code>BeautifulSoup</code>, a Python library, to parse the HTML page, allowing us to select the content we need. As shown in listing 8.5, we need to clean up some things and parse the content by matching the style of the blog post and the HTML generated. The search for various attributes and classes (such as <code>post-title</code>, etc.) depends on the shape of the incoming data and the use case we are trying to solve. In this example, the code must be updated if the blog changes its theme or rendering.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p100"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.5</span> Extracting content from HTML</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">    r = requests.get(post.link)
    soup = BeautifulSoup(r.text, 'html.parser')

    # Get the title
    try:    
        article_title = soup.find('h1', {'class': 'post-title'}).text
        article_title = article_title.replace("| Amit Bahree's  
                                <span class="">↪</span>(useless?) insight!", "")
    except AttributeError:
        article_title = ""
    print("\tTitle:" + article_title)

    # get the post description
    try:
        article_desc = soup.find('div', {'class': 'post-description'}).text
    except AttributeError as e:
        #print("Error getting description: ", e)
        article_desc = ""

    # get the publish date
    try:
        temp = soup.find('div', {'class': 'post-meta'}, {'span', 'title'}).text
        match = re.search(r"(\w+\s\d+,\s\d+)", temp)
        if match:
            publish_date = match.group(1)
    except AttributeError:
        publish_date = ""

    # get the article body
    try:
        article_body = soup.find('div', {'class': 'post-content'}).text
    except AttributeError:
        article_body = ""</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>For real-world enterprise use cases, the retriever must be aware of the source system’s content and structure, which can be quite complex and daunting. In most cases, this would need to run through a data pipeline. This data pipeline would help address any data engineering aspects needed—all in the context of the associated use cases. See section 8.4.1 for more details:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p102"> 
   <div class="code-area-container"> 
    <pre class="code-area">    chunks = split_sentences_by_spacy(article, max_tokens=3000, overlap=10)
    print(f"Number of chunks: {len(chunks)}")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>We create a new index hash, adding details of the information we are interested in as embeddings—URL, title, publish date, and blog post. We also correlate the different chunks that are created with the same context.</p> 
  </div> 
  <div class="readable-text intended-text" id="p104"> 
   <p>Note that we only show the key aspect of the code in the next listing, and for severity, we avoid the helper functions we have seen before. The complete code samples are in the book’s GitHub code repository (<a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p105"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.6</span> Retrieving blog posts and saving them in Redis</h5> 
   <div class="code-area-container"> 
    <pre class="code-area"># OpenAI API key
openai.api_key = os.getenv('OPENAI_API_BOOK_KEY')

# Redis connection details
redis_host = os.getenv('REDIS_HOST')
redis_port = os.getenv('REDIS_PORT')
redis_password = os.getenv('REDIS_PASSWORD')

def split_sentences_by_spacy(text, max_tokens, 
                        overlap=0, 
                        model="en_core_web_sm"):
...

# count tokens
def count_tokens(...)
...

def get_embedding(text):
...

# Connect to the Redis server
conn = redis.Redis(...)

SCHEMA = [ ... ]
# URL of the RSS feed to parse
url = https://blog.desigeek.com/index.xml

# Parse the RSS feed with feedparser
print("Parsing RSS feed...")
feed = feedparser.parse(url)

# get number of blog posts in feed
blog_posts = len(feed.entries)
print("Number of blog posts: ", blog_posts)

p = conn.pipeline(transaction=False)
for i, post in enumerate(feed.entries):
    # report progress
    print("Create embedding and save for entry #", i, " of ", blog_posts)

    # Extract the content – using BeautifulSoup
    r = requests.get(post.link)
    soup = BeautifulSoup(r.text, 'html.parser')

    # Get the title
...

    # get the post description
    ...

    # get the publish date
    ...

    # get the article body
    try:
        article_body = soup.find('div', {'class': 'post-content'}).text
    except AttributeError:
        article_body = ""

    # This should be chunked up
    article = article_body

    total_token_count = 0
    chunks = []

    # split the text into chunks by sentences
    chunks = split_sentences_by_spacy(article, max_tokens=3000, overlap=10)
    print(f"Number of chunks: {len(chunks)}")

    for j, chunk in enumerate(tqdm(chunks))
        vector = get_embedding(chunk)
        # convert to numpy array
        vector = np.array(vector).astype(np.float32).tobytes()

        # Create a new hash with the URL and embedding
        post_hash = {
            "url": post.link,
            "title": article_title,
            "description": article_desc,
            "publish_date": publish_date,
            "content": chunk,
            "embedding": vector
        }

        conn.hset(name=f"post:{i}_{j}", mapping=post_hash)

p.execute()
print("Vector upload complete.")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>Once we get the blog post’s content, we need to chunk it up, as discussed in the previous chapter. For this example, we use spaCy to chunk the blog post and also have some overlap between different chunks.</p> 
  </div> 
  <div class="readable-text" id="p107"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_139"><span class="num-string">8.4.1</span> Retriever pipeline best practices</h3> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>When implementing a RAG pattern, it’s crucial to have a deep understanding of the source system’s content and structure. The success of a RAG model hinges on its ability to access and interpret the right data, which necessitates a well-architected data pipeline. This pipeline is not just a conduit for data flow, but a sophisticated framework that ensures data is extracted, transformed, indexed, and stored to align with the model’s requirements and the defined use case.</p> 
  </div> 
  <div class="readable-text intended-text" id="p109"> 
   <p>The first step toward implementing GPTs and LLMs in enterprises is a deep understanding of the source system. This involves thoroughly analyzing the data structure, including entity-relationship diagrams, data types, and data distribution. Data profiling tools can be instrumental in understanding the nature of the content.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p110"> 
   <p><span class="print-book-callout-head">Note</span>  For RAG to work well, it is important to carefully plan the preprocessing one needs to do in the retriever pipeline and not just use everything without considering whether it is better. If not planned well, this will create problems when using search as part of a RAG implementation. </p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <p>The next phase defines the use case, which entails creating a detailed requirement document outlining the problem, potential solutions, expected results, and success metrics. This document should also detail the users’ informational needs and the scenarios in which the RAG model will be applied.</p> 
  </div> 
  <div class="readable-text intended-text" id="p112"> 
   <p>Following this, the focus shifts to data extraction and transformation. This process involves using ETL (extract, transform, load) tools to extract data from the source system and transform it into a format the RAG model can understand. It may involve NLP techniques such as tokenization, stop-word removal, and lemmatization.</p> 
  </div> 
  <div class="readable-text intended-text" id="p113"> 
   <p>Once the data has been transformed, it needs to be indexed for efficient retrieval. Azure AI Search, Elasticsearch, Solr, and Lucene are ideal for this purpose, as they provide full-text search capabilities and can handle large datasets effectively.</p> 
  </div> 
  <div class="readable-text intended-text" id="p114"> 
   <p>Parallel to data indexing, selecting a suitable data storage solution is important. Depending on the specific needs of the data size, speed, and type, this could be a traditional SQL database, a NoSQL database such as Cosmos DB, or a distributed file system such as Hadoop HDFS.</p> 
  </div> 
  <div class="readable-text intended-text" id="p115"> 
   <p>One of the most critical phases is preprocessing planning. This involves careful planning of preprocessing steps, which could involve techniques such as noise removal, normalization, and dimensionality reduction. The goal is to retain information relevant to the use case while reducing the model’s complexity.</p> 
  </div> 
  <div class="readable-text intended-text" id="p116"> 
   <p>The next phase is model integration, which involves using APIs or SDKs provided by the AI model vendor to integrate the RAG model into the application. The retriever must be configured with the correct query parameters, and the generator should be set up with the desired output structure.</p> 
  </div> 
  <div class="readable-text intended-text" id="p117"> 
   <p>Fine-tuning and monitoring are crucial for enhancing the model’s performance and ensuring the system’s health. This involves using a validation dataset for fine-tuning and application performance management (APM) tools for monitoring.</p> 
  </div> 
  <div class="readable-text intended-text" id="p118"> 
   <p>Regarding scalability and reliability, cloud platforms such as AWS, Google Cloud, or Azure should be used to scale the system as needed. Containerization platforms such as Docker and Kubernetes can assist in scaling and managing the application. Redundancy and failover strategies are crucial to ensuring system reliability.</p> 
  </div> 
  <div class="readable-text intended-text" id="p119"> 
   <p>Furthermore, security and compliance cannot be overlooked. Implementing data encryption, user authentication, access control, and regular system audits can ensure data security and compliance with data protection regulations such as GDPR or CCPA.</p> 
  </div> 
  <div class="readable-text intended-text" id="p120"> 
   <p>Before deployment, rigorous testing and validation are imperative to ensure that the pipeline and the RAG model meet the expectations outlined by the use case. Once the system is live, comprehensive documentation and technical training should be provided to the team for effective management, maintenance, and troubleshooting.</p> 
  </div> 
  <div class="readable-text intended-text" id="p121"> 
   <p>Finally, it’s crucial to ensure the quality control of the retrieval corpus, implement measures for information security and privacy, regularly update the retrieval corpus, and efficiently allocate resources. By following these steps, enterprises can effectively build and maintain AI-powered applications.</p> 
  </div> 
  <div class="readable-text" id="p122"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_140"><span class="num-string">8.5</span> Search using Redis</h2> 
  </div> 
  <div class="readable-text" id="p123"> 
   <p>Now that we have the data ingested and the index ready, we can search against it. We create a simple console app that accepts a user’s query, vectorizes it, and searches based on the top three similar posts to return to the user. This is a semantic search. The following listing shows the output generated as an example when we ask about “Longhorn.”</p> 
  </div> 
  <div class="browsable-container listing-container" id="p124"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.7</span> Search results</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">$  python .\search.py
Connected to Redis
Enter your query: Tell me about Longhorn
Vectorizing query...
Searching for similar posts...
Found 3 results:
You probably already heard this, but &lt;strong&gt;Chris Sells&lt;/strong&gt; 
   <span class="">↪</span>has a new column on MSDN called &lt;strong&gt;Longhorn Foghorn&lt;/strong&gt;
, that describes each of the â
&lt;strong&gt;Pillars of Longhorn&lt;/strong&gt;
â - This is something that IMHO developers would understand and 
<span class="">↪</span>appreciate. In the first article he explains the âPillarsâ and then 
<span class="">↪</span>in the next two goes onto build Solitaire. You can download the sample 
<span class="">↪</span>and play with it too.
From OSNews: Microsft has made &lt;em&gt;hard statements about perfomance 
<span class="">↪</span>improvements in Longhorn ...</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p125"> 
   <p><span class="print-book-callout-head">Note </span> Windows Longhorn used to be the codename for the operating system that eventually became Windows Vista.</p> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>Let’s check out the code for implementing the search using Redis. We first take a user query such as “Tell me about Longhorn,” create a vector, and use cosine similarity to obtain a list of comparable results.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p127"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.8</span> Searching using Redis</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def hybrid_search(query_vector, client, top_k=3, hybrid_fields="*"):
    base_query = f"{hybrid_fields}=&gt;
                        [KNN {top_k} 
                        @embedding $vector AS vector_score]" <span class="aframe-location"/> #1
    query = Query(base_query).return_fields(
        "url",                                              <span class="aframe-location"/> #2
        "title",                                            #2
        "publish_date",                                     #2
        "description",                                      #2
        "content",                                          #2
        "vector_score").sort_by("vector_score").dialect(2)  <span class="aframe-location"/> #3
    try:
        results = client.ft("posts").search(
            query, query_params={"vector": query_vector}) <span class="aframe-location"/> #4
    except Exception as e:
        print("Error calling Redis search: ", e)
        return None

    if results.total == 0:
        print("No results found for the given query vector.")
        return None

    return results

# Connect to the Redis server
conn = redis.Redis(...)

query = input("Enter your query: ")     <span class="aframe-location"/> #5

print("Vectorizing query...")
query_vector = get_embedding(query)   <span class="aframe-location"/> #6

query_vector = np.array(query_vector).astype(             <span class="aframe-location"/> #7

                        np.float32).tobytes()
print("Searching for similar posts...")
results = hybrid_search(query_vector, conn)               <span class="aframe-location"/> #8

if results:
    print(f"Found {results.total} results:")
    for i, post in enumerate(results.docs):
        score = 1 - float(post.vector_score)
        print(post.content)
else:
    print("No results found")</pre> 
    <div class="code-annotations-overlay-container">
     #1 A base query that prefilters fields and is implemented as a KNN search
     <br/>#2 Selects the different fields we are interested in searching
     <br/>#3 Sorts by cosine similarity in descending order
     <br/>#4 Executes the query
     <br/>#5 Captures the query from the user
     <br/>#6 Vectorizes the input
     <br/>#7 Converts the vector to a NumPy array
     <br/>#8 Performs the similarity search
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>As the name suggests, the <code>hybrid_search()</code> function does the heavy lifting of running the hybrid search query. A hybrid search query combines multiple types of searches into a single query. This can include combining text-based searches with other types, such as numerical, categorical, or even vector-based searches. Note that the exact search type would depend on the information and the requirement.</p> 
  </div> 
  <div class="readable-text intended-text" id="p129"> 
   <p>In our example, we combine a K-Nearest Neighbors (KNN) search on an embedding vector with other search fields. The KNN search finds the most related items to a given item, in this case, the most similar posts to a given query vector. The query results are sorted by vector score, which means a high to low ordering based on cosine similarity. In other words, the results with the highest similarity are shown first. We also restrict this to the top three items, as depicted by the <code>top_k</code> parameter. </p> 
  </div> 
  <div class="readable-text intended-text" id="p130"> 
   <p>Note that the exact nature of the search and type also depends on the search engine and the data type. For more details on Redis search types and KNN, see the documentation at <a href="https://mng.bz/o0Gp">https://mng.bz/o0Gp</a>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p131"> 
   <p>Now that we have seen the search, let’s combine all the dimensions and integrate them into a chat experience using an LLM.</p> 
  </div> 
  <div class="readable-text" id="p132"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_141"><span class="num-string">8.6</span> An end-to-end chat implementation powered by RAG</h2> 
  </div> 
  <div class="readable-text" id="p133"> 
   <p>Throughout this and the previous chapter, we have discussed and examined all the pieces to help us understand some of the core concepts; now, we can bring it all together and build an end-to-end chat application. In the application, we can ask questions to get details about our data (i.e., the blog posts). Figure 8.6 shows the application flow.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p134">  
   <img alt="figure" src="../Images/CH08_F06_Bahree.png" width="910" height="436"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.6</span> End-to-end chat application</h5>
  </div> 
  <div class="readable-text" id="p135"> 
   <p>The question the user asks first gets converted into embeddings and then searched in Redis using a hybrid search index to find similar chunks, which are returned as search results. As we saw earlier, the blog posts have already been injected into the Redis database and indexed. Once we have the results, we formulate the LLM prompt by combining the original questions and the chunks retrieved to answer from. These are passed into the prompt itself before finally calling the LLM to generate a response.</p> 
  </div> 
  <div class="readable-text intended-text" id="p136"> 
   <p>On the search front, we deployed Redis running locally and created a vector index. We read all the blog posts going back nearly 20 years. We created the relevant chunks for these posts and their corresponding embeddings and populated our vector database. We also implemented a vector search on those embeddings. The only piece left is to integrate all of this into our application and hook it up with an LLM to complete the last stage of our RAG implementation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p137"> 
   <p>Listing 8.9 shows exactly how to do this. Several helper functions, such as <code>get_ search_results()</code>, take the user’s query, call another helper function to search Redis, and return any results found. The actual API call that calls the GPT is in the <code>ask_gpt()</code> function, and it is a <code>ChatCompletion()</code> API, just like we saw earlier.</p> 
  </div> 
  <div class="readable-text intended-text" id="p138"> 
   <p>As with previous examples, we leave out the code’s helper functions and other aspects for brevity. The complete code samples are available in the GitHub code repository accompanying the book (<a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p139"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 8.9</span> End-to-end RAG-powered chat</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">def hybrid_search(query_vector, client, top_k=5, hybrid_fields="*"):
...
    return results

def get_search_results(query:str, max_token=4096, 
                       <span class="">↪</span>debug_message=False) -&gt; str:
    query_vector = get_embedding(query)   <span class="aframe-location"/> #1

    query_vector = np.array(query_vector).astype(
        np.float32).tobytes()   <span class="aframe-location"/> #2

    print("Searching for similar posts...")
    results = hybrid_search(query_vector, conn, top_k=5)  <span class="aframe-location"/> #3

    token_budget = max_token - count_tokens(query)       <span class="aframe-location"/> #4
    if debug_message:
        print(f"Token budget: {token_budget}")

    message = 'Use the blog post below to answer the subsequent 
               <span class="">↪</span>question. If the answer cannot be found in the 
               <span class="">↪</span>articles, write "Sorry, I could not find an answer in 
               <span class="">↪</span>the blog posts."'
    question = f"\n\nQuestion: {query}"

    if results:
        for i, post in enumerate(results.docs):         <span class="aframe-location"/> #5
            next_post = f'\n\nBlog post:\n"""\n{post.content}\n"""'
            new_token_usage = count_tokens(message + question + next_post)
            if new_token_usage &lt; token_budget:
                if debug_message:
                    print(f"Token usage: {new_token_usage}")
                message += next_post
            else:
                break
    else:
        print("No results found")

    return message + question

def ask_gpt(query : str, max_token = 4096, debug_message = False) -&gt; str:
    message = get_search_results(                     <span class="aframe-location"/> #6
        query,
        max_token,
        debug_message=debug_message)

    messages = [                                  <span class="aframe-location"/> #7
        {"role": 
         "system", 
         "content": "You answer questions in summary from the [CA]
                     blog posts."},
        {"role":
          "user",
            "content": message},]

    response = openai.ChatCompletion.create(          <span class="aframe-location"/> #8
        model="gpt-3.5-turbo-16k",
        messages=messages,
        temperature=0.7,
        max_tokens=2000,
        top_p=0.95
    )
    response_message = response["choices"][0]["message"]["content"]
    return response_message

if __name__ == "__main__":
    # Enter a query
    while True:
        query = input("Please enter your query: ")
        print(ask_gpt(query, max_token=15000, debug_message=False))
        print("=="*20)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Vectorizes the query
     <br/>#2 Converts the vector to a numpy array
     <br/>#3 Performs the similarity search
     <br/>#4 Manages token budget
     <br/>#5 Loops through the results while still keeping within the token budget
     <br/>#6 Runs a vector search to get embeddings
     <br/>#7 Sets up the chat completion calls
     <br/>#8 Calls the LLM
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p140"> 
   <p>We can see all this coming together when we run it and chat with the blog. It understands the query, creates embeddings, uses the vector database and the associated vector indexes to retrieve the top five matching results, adds that to the prompt, and uses the LLM to generate the response (figure 8.7).</p> 
  </div> 
  <div class="readable-text intended-text" id="p141"> 
   <p>In the example we have seen thus far, we are responsible for everything—from setting up the Docker containers to deploying Redis and ingesting the data. This is not enough for enterprises to go into production. More system engineering is required, such as setting up various clusters of machines, scaling them up or down as needed, managing Redis, security requirements, overall operations, and so forth. This takes a significant amount of time, effort, cost, and skills that not every organization might have. Another option is to use Azure OpenAI, which can do much of this out of the box and allows organizations a quicker time to market, potentially at a lower cost. Let’s see how Azure OpenAI can achieve the same result but much faster.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p142">  
   <img alt="figure" src="../Images/CH08_F07_Bahree.png" width="868" height="499"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.7</span> Q&amp;A using blog data with GPT-3.5 Turbo<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p143"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_142"><span class="num-string">8.7</span> Using Azure OpenAI on your data</h2> 
  </div> 
  <div class="readable-text" id="p144"> 
   <p>Many enterprises use Azure, and incorporating Azure OpenAI as part of their data strategy represents a pivotal step in employing the power of generative AI for business transformation. Azure OpenAI provides an enterprise-grade platform to integrate advanced AI models such as ChatGPT into your data workflows. </p> 
  </div> 
  <div class="readable-text intended-text" id="p145"> 
   <p>“Azure OpenAI on your data” is the service that enables running these powerful chat models on your data and getting out-of-the-box features that enterprises require for production workloads: scalability, security, refreshes, and integration into others. You can connect your data source using Azure OpenAI Studio (figure 8.8) or the REST API.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p146"> 
   <p><span class="print-book-callout-head">Note </span> Azure AI Studio is a platform that combines capabilities across multiple Azure AI services. It is designed for developers to build generative AI applications on an enterprise-grade platform. You can first interact with a project code via the Azure AI SDK and Azure AI CLI and seamlessly explore, build, test, and deploy using cutting-edge AI tools and ML models.</p> 
  </div> 
  <div class="readable-text" id="p147"> 
   <p>At the core of Azure OpenAI’s appeal is its seamless integration with the broader Azure ecosystem. Connecting these powerful AI models to your data repositories unlocks the potential for more sophisticated data analysis, natural language processing, and predictive insights. This integration is particularly beneficial for enterprises with a significant footprint in Azure, enabling them to enhance their existing infrastructure with minimal disruption. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p148">  
   <img alt="figure" src="../Images/CH08_F08_Bahree.png" width="1034" height="415"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.8</span> Adding your data to Azure OpenAI<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p149"> 
   <p>Azure AI Studio supports multiple options from existing Azure AI Search indexes, Blob storage, Cosmos DB, and so forth. One of these options is a URL, which we will use to ingest blog posts (see figure 8.9). We can also save the RSS feed locally and upload it as a file. One of the advantages of using our own Azure AI Search index is that it does the heavy lifting of keeping the data ingestion up to date from the source systems. This replaces Redis and can be globally distributed to a cloud-scale if required.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p150">  
   <img alt="figure" src="../Images/CH08_F09_Bahree.png" width="1014" height="503"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.9</span> Azure AI Studio: Adding a data source</h5>
  </div> 
  <div class="readable-text" id="p151"> 
   <p>We can configure and set up most things here, including a storage resource where this data will be saved, an Azure AI Search resource, the index details, embedding details, and so forth (see figure 8.10). With a few clicks, all of this is set up and ready for us to use.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p152">  
   <img alt="figure" src="../Images/CH08_F10_Bahree.png" width="890" height="665"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.10</span> Configure details for data ingestion</h5>
  </div> 
  <div class="readable-text" id="p153"> 
   <p>On the information security front, this process is streamlined by Azure’s robust security and compliance framework, ensuring that your data remains protected throughout its interaction with AI models. Azure OpenAI supports two key features on your data: role-based and document-level access controls. This feature, working alongside Azure AI Search security filters, can be used to limit access to only those users who should have access based on their permitted groups and LDAP memberships, which is a critical requirement for many enterprises, especially in regulated industries.</p> 
  </div> 
  <div class="readable-text intended-text" id="p154"> 
   <p>Finally, Azure’s ability to process and analyze large cloud-scale volumes of unstructured data scalability is another significant advantage. For example, OpenAI’s ChatGPT internally uses Azure AI Search, and that workload is 100+ million users per day. Azure’s cloud infrastructure allows for the easy scaling of AI capabilities as your data needs grow. More details on Azure OpenAI can be found at <a href="https://mng.bz/n022">https://mng.bz/n022</a>.</p> 
  </div> 
  <div class="readable-text" id="p155"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_143"><span class="num-string">8.8</span> Benefits of bringing your data using RAG</h2> 
  </div> 
  <div class="readable-text" id="p156"> 
   <p>Enterprises often struggle to extract meaningful insights from unstructured data sources such as emails, customer feedback, or social media interactions. When enterprises integrate their data using RAG in LLMs, they unlock many advantages, enhancing the functionality and applicability of these AI systems in their unique business contexts.</p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>This feature offers distinct advantages over merely expanding the context window of these models. The pattern enhances the relevance and accuracy of LLM outputs and provides strategic benefits that a larger context window alone cannot match. LLMs can analyze this data, interpret it in a human-like manner, and provide actionable insights, all in a fraction of the time it would take using traditional methods.</p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>Integrating RAG with real-time enterprise data ensures that the information retrieved and included in responses is relevant and current, a critical factor in rapidly evolving industries. This customization leads to more precise and applicable answers, which is especially beneficial for sectors with specialized knowledge, such as legal, medical, or technical fields.</p> 
  </div> 
  <div class="readable-text intended-text" id="p159"> 
   <p>The key advantage of using enterprise-specific data in conjunction with RAG models lies in the tailored accuracy and applicability of the model’s responses. LLMs with a larger context window can process more information in a single instance, but they may still lack the depth of knowledge in specialized domains. When enterprises introduce their data, the LLMs can generate responses intricately aligned with the organization’s specific industry, jargon, and operational intricacies. This specificity is crucial for industries where specialized knowledge is paramount and goes beyond the scope of what a larger context window can provide.</p> 
  </div> 
  <div class="readable-text intended-text" id="p160"> 
   <p>While a larger context window allows for a broader range of preexisting information to be considered in the model’s responses, it does not necessarily incorporate the most current or enterprise-specific data. In addition, the larger the context window, the more the model has to process and the slower it is. </p> 
  </div> 
  <div class="readable-text intended-text" id="p161"> 
   <p>Furthermore, integrating proprietary data enhances decision-making processes more effectively than simply expanding the context window. This integration enables LLMs to offer insights and analysis deeply rooted in the enterprise’s historical data and strategic objectives. In contrast, a larger context window might provide broader information but lacks precision and direct relevance to enterprises’ strategic questions and challenges.</p> 
  </div> 
  <div class="readable-text intended-text" id="p162"> 
   <p>Regarding data security and privacy, bringing proprietary data under enterprise control is more manageable than relying on public or generalized data that a larger context window might access. By controlling data inputs, enterprises can more effectively ensure compliance with data privacy regulations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p163"> 
   <p>Implementing RAG with your data offers significant advantages for AI safety in enterprise environments, primarily by enhancing the accuracy and reliability of information. This fusion of generative capabilities of LLMs with a comprehensive corpus of data allows the model to access up-to-date, factual data, crucial for enterprises dealing with time-sensitive and accuracy-critical information. Moreover, by retrieving from a diverse set of sources, RAG can mitigate biases inherent in the training data of LLMs, a vital feature for making unbiased, data-driven decisions. Enterprises can customize the retrieval corpus, ensuring alignment with industry regulations and internal policies. Furthermore, incorporating the latest information and providing sources for generated content offers improved transparency and decision-making support.</p> 
  </div> 
  <div class="readable-text intended-text" id="p164"> 
   <p>While expanding the context window of LLMs offers certain benefits, integrating proprietary data with RAG models provides specificity, current relevance, strategic alignment, personalization, data security, and innovation potential that a mere increase in the context window cannot match. This approach enables enterprises to use LLMs more effectively for their unique business needs and objectives.</p> 
  </div> 
  <div class="readable-text" id="p165"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_144">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p166"> The benefits of integrating proprietary data with RAG models are enhancing AI systems’ specificity, relevance, strategic alignment, personalization, data security, and innovation potential. </li> 
   <li class="readable-text" id="p167"> Using proprietary data over merely expanding the context window of LLMs offers multiple advantages, as the former provides more accurate, relevant, and personalized answers. </li> 
   <li class="readable-text" id="p168"> In a production environment, using a vector database and vector index to manage, secure, and scale embeddings is crucial for performance and cost reasons. </li> 
   <li class="readable-text" id="p169"> The process of retrieving proprietary data, chunking it, creating embeddings, and saving the details in a vector database depends on the shape of the data at hand. It can require significant planning and data engineering effort. </li> 
   <li class="readable-text" id="p170"> Integration of a RAG pattern with a source system is complex, requiring planning, robust engineering, and an understanding of the data structure details. </li> 
   <li class="readable-text" id="p171"> An end-to-end application using RAG, prompt engineering, embeddings, and search can be very powerful for organizations. Still, it is also complex, and if not designed properly, it will slow things down when deploying to production. </li> 
   <li class="readable-text" id="p172"> The chapter highlights how to conduct search using a vector database, retrieving the most similar items to a given item based on their vector embeddings. It also shows how incorporating the vector databases and RAG is key for implementing an end-to-end chat application. </li> 
   <li class="readable-text" id="p173"> “Azure OpenAI on your data” is a PaaS service that enables enterprises to run AI models on their data with out-of-the-box features such as scalability, security, and integration into other Azure services. </li> 
  </ul>
 </div></div></body></html>