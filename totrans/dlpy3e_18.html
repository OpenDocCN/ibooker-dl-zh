<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Best practices for the real world</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Best practices for the real world</h1>
<blockquote>原文：<a href="https://deeplearningwithpython.io/chapters/chapter18_best-practices-for-the-real-world">https://deeplearningwithpython.io/chapters/chapter18_best-practices-for-the-real-world</a></blockquote>


<aside>
<p>This chapter covers
</p>
<ul>
<li>Hyperparameter tuning</li>
<li>Model ensembling</li>
<li>Training Keras models on multiple GPUs or on TPU</li>
<li>Mixed-precision training</li>
<li>Quantization</li>
</ul>
</aside>

<p>You’ve come quite far since the beginning of this book. You can now
train image classification models, image segmentation models, models for
classification or regression on vector data, timeseries forecasting models,
text classification models, sequence-to-sequence models, and even generative
models for text and images. You’ve got all the bases covered.</p>
<p>However, your models so far have all been trained at a small scale
— on small datasets, with a single GPU — and they generally
haven’t reached the best achievable performance on each dataset we’ve looked at.
This book is, after all, an introductory book. If you are to go out into the real world
and achieve state-of-the-art results on brand new problems, there’s still
a bit of a chasm that you’ll need to cross.</p>
<p>This chapter is about bridging that gap and giving you the best practices
you’ll need as you go from machine learning student to a fully fledged machine learning
engineer. We’ll review essential techniques for systematically improving model
performance: hyperparameter tuning and model ensembling. Then we’ll look at how you can
speed up and scale up model training, with multi-GPU and TPU training, mixed precision,
and quantization.</p>
<h2 id="getting-the-most-out-of-your-models">Getting the most out of your models</h2>
<p>Blindly trying out different architecture configurations works well enough
if you just need something that works okay. In this section, we’ll go
beyond “works okay” to “works great and wins machine learning competitions”
via a quick guide to a set of must-know techniques for building state-of-the-art
deep learning models.</p>
<h3 id="hyperparameter-optimization">Hyperparameter optimization</h3>
<p>When building a deep learning model, you have to make many
seemingly arbitrary decisions: How many layers should you stack? How many
units or filters should go in each layer? Should you use <code>relu</code> as an activation,
or a different function? Should you use <code>BatchNormalization</code> after a given
layer? How much dropout should you use? And so on. These architecture-level
parameters are called <em>hyperparameters</em> to distinguish them from the
<em>parameters</em> of a model, which are trained via backpropagation.</p>
<p>In practice, experienced machine learning engineers and researchers build
intuition over time as to what works and what doesn’t when it comes to these
choices — they develop hyperparameter-tuning skills. But there are no formal
rules. If you want to get to the very limit of what can be achieved on a given
task, you can’t be content with such arbitrary choices.
Your initial decisions are almost always suboptimal, even if you have very good
intuition. You can refine your choices by tweaking them by hand and retraining
the model repeatedly — that’s what machine learning engineers and researchers
spend most of their time doing. But it shouldn’t be your job as a human to
fiddle with hyperparameters all day — that is better left to a machine.</p>
<p>Thus, you need to explore the space of possible decisions automatically and
systematically in a principled way. You need to search the architecture space
and find the best-performing ones empirically. That’s what the field of
automatic hyperparameter optimization is about: it’s an entire field of
research, and an important one.</p>
<p>The process of optimizing hyperparameters typically looks like this:</p>
<ol>
<li>Choose a set of hyperparameters (automatically).</li>
<li>Build the corresponding model.</li>
<li>Fit it to your training data, and measure performance on the
validation data.</li>
<li>Choose the next set of hyperparameters to try (automatically).</li>
<li>Repeat.</li>
<li>Eventually, measure performance on your test data.</li>
</ol>
<p>The key to this process is the algorithm that analyzes the relationship between
validation performance and various hyperparameter values to choose the next set of
hyperparameters to evaluate. Many different techniques are possible: Bayesian
optimization, genetic algorithms, simple random search, and so on.</p>
<p>Training the weights of a model is relatively easy: you compute a loss function
on a mini-batch of data and then use backpropagation to move the
weights in the right direction. Updating hyperparameters, on the other hand,
presents unique challenges. Consider that</p>
<ul>
<li>The hyperparameter space is typically made of discrete decisions and thus
isn’t continuous or differentiable. Hence, you typically can’t do gradient
descent in hyperparameter space. Instead, you must rely on gradient-free
optimization techniques, which, naturally, are far less efficient than gradient
descent.</li>
<li>Computing the feedback signal of this optimization process
(does this set of hyperparameters lead to a
high-performing model on this task?) can be extremely expensive: it requires
creating and training a new model from scratch on your dataset.</li>
<li>The feedback signal may be noisy: if a training run performs 0.2% better, is that because
of a better model configuration or because you got lucky with the initial weight values?</li>
</ul>
<p>Thankfully, there’s a tool that makes hyperparameter tuning simpler: KerasTuner.
Let’s check it out.</p>
<h4 id="using-kerastuner">Using KerasTuner</h4>
<p>Let’s start by installing KerasTuner:</p>
<figure>
<pre><code class="language-text">!pip install keras-tuner -q
</code></pre>
</figure>

<p>The key idea that KerasTuner is built upon is to let you
replace hardcoded hyperparameter values, such as <code>units=32</code>, with a range
of possible choices,
such as <code>Int(name="units", min_value=16, max_value=64, step=16)</code>. The
set of such choices in a given model is called the <em>search space</em>
of the hyperparameter tuning process.</p>
<p>To specify a search space, define a model-building function (see the next listing).
It takes an <code>hp</code> argument, from which you can sample hyperparameter ranges,
and it returns a compiled Keras model.</p>
<figure id="listing-18-1">
<pre><code class="language-python">import keras
from keras import layers

def build_model(hp):
    # Sample hyperparameter values from the hp object. After sampling,
    # these values (such as the "units" variable here) are just regular
    # Python constants.
    units = hp.Int(name="units", min_value=16, max_value=64, step=16)
    model = keras.Sequential(
        [
            layers.Dense(units, activation="relu"),
            layers.Dense(10, activation="softmax"),
        ]
    )
    # Different kinds of hyperparameters are available: Int, Float,
    # Boolean, Choice.
    optimizer = hp.Choice(name="optimizer", values=["rmsprop", "adam"])
    model.compile(
        optimizer=optimizer,
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    # The function returns a compiled model.
    return model
</code></pre>
<figcaption>
<a href="#listing-18-1">Listing 18.1</a>: A KerasTuner model-building function
</figcaption>
</figure>

<p>If you want to adopt a more modular and configurable approach to model-building,
you can also subclass the <code>HyperModel</code> class and define a <code>build</code> method.</p>
<figure id="listing-18-2">
<pre><code class="language-python">import keras_tuner as kt

class SimpleMLP(kt.HyperModel):
    # Thanks to the object-oriented approach, we can configure model
    # constants as constructor arguments (instead of hardcoding them in
    # the model-building function).
    def __init__(self, num_classes):
        self.num_classes = num_classes

    # The build method is identical to our prior build_model standalone
    # function.
    def build(self, hp):
        units = hp.Int(name="units", min_value=16, max_value=64, step=16)
        model = keras.Sequential(
            [
                layers.Dense(units, activation="relu"),
                layers.Dense(self.num_classes, activation="softmax"),
            ]
        )
        optimizer = hp.Choice(name="optimizer", values=["rmsprop", "adam"])
        model.compile(
            optimizer=optimizer,
            loss="sparse_categorical_crossentropy",
            metrics=["accuracy"],
        )
        return model

hypermodel = SimpleMLP(num_classes=10)
</code></pre>
<figcaption>
<a href="#listing-18-2">Listing 18.2</a>: A KerasTuner <code>HyperModel</code>
</figcaption>
</figure>

<p>The next step is to define a “tuner.” Schematically, you can think of a tuner as
a <code>for</code> loop, which will repeatedly</p>
<ul>
<li>Pick a set of hyperparameter values</li>
<li>Call the model-building function with these values to create a model</li>
<li>Train the model and record its metrics</li>
</ul>
<p>KerasTuner has several built-in tuners available — <code>RandomSearch</code>, <code>BayesianOptimization</code>, and
<code>Hyperband</code>. Let’s try <code>BayesianOptimization</code>, a tuner that attempts
to make smart predictions for which new hyperparameter values are likely to perform
best given the outcome of previous choices:</p>
<figure>
<pre><code class="language-python">tuner = kt.BayesianOptimization(
    # Specifies the model-building function (or hypermodel instance)
    build_model,
    # Specifies the metric that the tuner will seek to optimize. Always
    # specify validation metrics, since the goal of the search process
    # is to find models that generalize!
    objective="val_accuracy",
    # Maximum number of different model configurations ("trials") to
    # try before ending the search
    max_trials=20,
    # To reduce metrics variance, you can train the same model multiple
    # times and average the results. executions_per_trial is how many
    # training rounds (executions) to run for each model configuration
    # (trial).
    executions_per_trial=2,
    # Where to store search logs
    directory="mnist_kt_test",
    # Whether to overwrite data in the directory to start a new search.
    # Set this to True if you've modified the model-building function
    # or to False to resume a previously started search with the same
    # model-building function.
    overwrite=True,
)
</code></pre>
</figure>

<p>You can display an overview of the search space via <code>search_space_summary()</code>:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; tuner.search_space_summary()</code>
<code class="language-output">Search space summary
Default search space size: 2
units (Int)
{"default": None,
 "conditions": [],
 "min_value": 128,
 "max_value": 1024,
 "step": 128,
 "sampling": None}
optimizer (Choice)
{"default": "rmsprop",
 "conditions": [],
 "values": ["rmsprop", "adam"],
 "ordered": False}</code></pre>
</figure>

<aside>
<p><span class="note-title">Objective maximization and minimization</span></p>
<p>For built-in metrics (like accuracy, in our case),
  the <em>direction</em> of the metric (accuracy should be maximized,
    but a loss should be minimized) is inferred by KerasTuner. However,
    for a custom metric, you should specify it yourself, like this:</p>
<figure>
<pre><code class="language-python">objective = kt.Objective(
    # The metric's name, as found in epoch logs
    name="val_accuracy",
    # The metric's desired direction: "min" or "max"
    direction="max",
)
tuner = kt.BayesianOptimization(
    build_model,
    objective=objective,
    ...
)
</code></pre>
</figure>
</aside>

<p>Finally, let’s launch the search. Don’t forget to pass validation data
and make sure not to use your test set as validation data — otherwise,
you’d quickly start overfitting to your test data, and you wouldn’t be able
to trust your test metrics anymore:</p>
<figure>
<pre><code class="language-python">(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape((-1, 28 * 28)).astype("float32") / 255
x_test = x_test.reshape((-1, 28 * 28)).astype("float32") / 255
# Reserves these for later
x_train_full = x_train[:]
y_train_full = y_train[:]
# Sets aside a validation set
num_val_samples = 10000
x_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:]
y_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:]
callbacks = [
    # Uses a large number of epochs (you don't know in advance how many
    # epochs each model will need) and uses an EarlyStopping callback
    # to stop training when you start overfitting
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=5),
]
# This takes the same arguments as fit() (it simply passes them down to
# fit() for each new model).
tuner.search(
    x_train,
    y_train,
    batch_size=128,
    epochs=100,
    validation_data=(x_val, y_val),
    callbacks=callbacks,
    verbose=2,
)
</code></pre>
</figure>

<p>The previous example will run in just a few minutes since we’re only looking at
a few possible choices and we’re training on MNIST.
However, with a typical search space and dataset, you’ll often find yourself letting the hyperparameter search run overnight
or even over several days. If your search process crashes, you can always restart
it — just specify <code>overwrite=False</code> in the tuner so that it can resume from
the trial logs stored on disk.</p>
<p>Once the search is complete, you can query the best hyperparameter configurations,
which you can use to create high-performing models that you can then retrain.</p>
<figure id="listing-18-3">
<pre><code class="language-python">top_n = 4
# Returns a list of HyperParameters objects, which you can pass to the
# model-building function
best_hps = tuner.get_best_hyperparameters(top_n)
</code></pre>
<figcaption>
<a href="#listing-18-3">Listing 18.3</a>: Querying the best hyperparameter configurations
</figcaption>
</figure>

<p>Usually, when retraining these models,
you may want to include the validation data as part of the training data
since you won’t be making any further hyperparameter changes, and thus you will no longer
be evaluating performance on the validation data. In our example, we’d train
these final models on the totality of the original MNIST training data,
without reserving a validation set.</p>
<p>Before we can train on the full training data, though, there’s one last parameter
we need to settle: the optimal number of epochs to train for. Typically,
you’ll want to train the new models for longer than you did during the search:
using an aggressive <code>patience</code> value in the <code>EarlyStopping</code> callback saves
time during the search, but may lead to underfitted models. Just use the
validation set to find the best epoch:</p>
<figure>
<pre><code class="language-python">def get_best_epoch(hp):
    model = build_model(hp)
    callbacks = [
        keras.callbacks.EarlyStopping(
            # Note the very high patience value.
            monitor="val_loss", mode="min", patience=10
        )
    ]
    history = model.fit(
        x_train,
        y_train,
        validation_data=(x_val, y_val),
        epochs=100,
        batch_size=128,
        callbacks=callbacks,
    )
    val_loss_per_epoch = history.history["val_loss"]
    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1
    print(f"Best epoch: {best_epoch}")
    return best_epoch
</code></pre>
</figure>

<p>And finally, train on the full dataset for just a bit longer than this epoch count,
since you’re training on more data — 20% more, in this case:</p>
<figure>
<pre><code class="language-python">def get_best_trained_model(hp):
    best_epoch = get_best_epoch(hp)
    model = build_model(hp)
    model.fit(
        x_train_full, y_train_full, batch_size=128, epochs=int(best_epoch * 1.2)
    )
    return model

best_models = []
for hp in best_hps:
    model = get_best_trained_model(hp)
    model.evaluate(x_test, y_test)
    best_models.append(model)
</code></pre>
</figure>

<p>If you’re not worried about slightly underperforming, there’s a
shortcut you can take: just use the tuner to reload the top-performing models
with the best weights saved during the hyperparameter search, without
retraining new models from scratch:</p>
<figure>
<pre><code class="language-python">best_models = tuner.get_best_models(top_n)
</code></pre>
</figure>

<aside>
<p>One important issue to keep in mind when doing automatic hyperparameter
optimization at scale is validation-set overfitting. Because you’re updating
hyperparameters based on a signal that is computed using your validation data,
you’re effectively training them on the validation data, and thus, they will
quickly overfit to the validation data. Always keep this in mind.</p>
</aside>

<h4 id="the-art-of-crafting-the-right-search-space">The art of crafting the right search space</h4>
<p>Overall, hyperparameter optimization is a powerful technique that is an
absolute requirement to get to state-of-the-art models on any task or to win
machine learning competitions. Think about it: once upon a time, people
handcrafted the features that went into shallow machine learning models. That
was very suboptimal. Now deep learning automates the task of
hierarchical feature engineering — features are learned using a feedback signal,
not hand-tuned, and that’s the way it should be. In the same way, you
shouldn’t handcraft your model architectures; you should optimize them in a
principled way.</p>
<p>However, doing hyperparameter tuning is not a replacement for being familiar
with model architecture best practices: search spaces grow combinatorially
with the number of choices, so it would be far too expensive to turn everything
into a hyperparameter and let the tuner sort it out.
You need to be smart about designing the right search space.
Hyperparameter tuning is automation, not magic: you use it to automate
experiments that you would otherwise have run by hand, but you still need
to handpick experiment configurations that have the potential to yield good metrics.</p>
<p>The good news: by using hyperparameter tuning, the
configuration decisions you have to make graduate from micro-decisions (What number of
units do I pick for this layer?) to higher-level architecture decisions (Should I
use residual connections throughout this model?). And while micro-decisions
are specific to a certain model and a certain dataset, higher-level decisions
generalize better across different tasks and datasets: for instance,
pretty much every image classification problem can be solved via the same
sort of search space template.</p>
<p>Following this logic, KerasTuner attempts to provide <em>premade search spaces</em>
that are relevant to broad categories of problems — such as image classification.
Just add data, run the search, and get a pretty good model. You can
try the hypermodels <code>kt.applications.HyperXception</code> and <code>kt.applications.HyperResNet</code>,
which are effectively tunable versions of Keras Applications models.</p>
<h3 id="model-ensembling">Model ensembling</h3>
<p>Another powerful technique for obtaining the best
possible results on a task is <em>model ensembling</em>. Ensembling consists of
pooling together the predictions of a set of different models to produce
better predictions. If you look at machine learning competitions — in
particular, on Kaggle — you’ll see that the winners use very large ensembles of
models that inevitably beat any single model, no matter how good.</p>
<p>Ensembling relies on the assumption that different well-performing models trained
independently are likely to be good for different reasons: each model looks
at slightly different aspects of the data to make its predictions, getting
part of the “truth” but not all of it. You may be familiar with the ancient
parable of the blind men and the elephant: a group of blind men come across an
elephant for the first time and try to understand what the elephant is by
touching it. Each man touches a different part of the elephant’s body — just one
part, such as the trunk or a leg. Then the men describe to each other what an
elephant is: “It’s like a snake,” “Like a pillar or a tree,” and so on. The
blind men are essentially machine learning models trying to understand the
manifold of the training data, each from its own perspective, using its own
assumptions (provided by the unique architecture of the model and the unique
random weight initialization). Each of them gets part of the truth of the
data, but not the whole truth. By pooling their perspectives together, you can
get a far more accurate description of the data. The elephant is a combination
of parts: no single blind man gets it quite right, but interviewed
together, they can tell a fairly accurate story.</p>
<p>Let’s use classification as an example. The easiest way to pool the predictions
of a set of classifiers (to <em>ensemble the classifiers</em>) is to average their
predictions at inference time:</p>
<figure>
<pre><code class="language-python"># Uses four different models to compute initial predictions
preds_a = model_a.predict(x_val)
preds_b = model_b.predict(x_val)
preds_c = model_c.predict(x_val)
preds_d = model_d.predict(x_val)
# This new prediction array should be more accurate than any of the
# initial ones.
final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d)
</code></pre>
</figure>

<p>However, this will work only if the classifiers are more or less equally good.
If one of them is significantly worse than the others, the final predictions may not be
as good as the best classifier of the group.</p>
<p>A smarter way to ensemble classifiers is to do a weighted average, where the
weights are learned on the validation data — typically, the better classifiers
are given a higher weight, and the worse classifiers are given a lower weight.
To search for a good set of ensembling weights, you can use random search or a
simple optimization algorithm, such as the Nelder-Mead algorithm:</p>
<figure>
<pre><code class="language-python">preds_a = model_a.predict(x_val)
preds_b = model_b.predict(x_val)
preds_c = model_c.predict(x_val)
preds_d = model_d.predict(x_val)
# These weights (0.5, 0.25, 0.1, 0.15) are assumed to be learned
# empirically.
final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d
</code></pre>
</figure>

<p>There are many possible variants: you can do an average of an exponential of
the predictions, for instance. In general, a simple weighted average with
weights optimized on the validation data provides a very strong baseline.</p>
<p>The key to making ensembling work is the <em>diversity</em> of the set of classifiers.
Diversity is strength. If all the blind men only touched the elephant’s trunk,
they would agree that elephants are like snakes, and they would forever stay
ignorant of the truth of the elephant. Diversity is what makes ensembling
work. In machine learning terms, if all of your models are biased in the same
way, then your ensemble will retain this same bias. If your models are
<em>biased in different ways</em>, the biases will cancel each other out,
and the ensemble will be more robust and more accurate.</p>
<p>For this reason, you should ensemble models that are <em>as good as possible</em>
while being <em>as different as possible</em>. This typically means using very
different architectures or even different brands of machine learning
approaches. One thing that is largely not worth doing is ensembling the same
network trained several times independently, from different random
initializations. If the only difference between your models is their random
initialization and the order in which they were exposed to the training data,
then your ensemble will be low in diversity and will provide only a tiny
improvement over any single model.</p>
<p>One thing I have found to work well in practice — but that doesn’t generalize to
every problem domain — is the use of an ensemble of tree-based methods (such as
random forests or gradient-boosted trees) and deep neural networks. In 2014,
Andrei Kolev and I took fourth place in the Higgs Boson decay
detection challenge on Kaggle (www.kaggle.com/c/higgs-boson) using an ensemble
of various tree models and deep neural networks. Remarkably,
one of the models in the ensemble originated
from a different method than the others (it was a regularized greedy forest)
and had a significantly worse score than the others. Unsurprisingly, it was
assigned a small weight in the ensemble. But to our surprise, it turned out to
improve the overall ensemble by a large factor because it was so different
from every other model: it provided information that the other models didn’t
have access to. That’s precisely the point of ensembling. It’s not so much
about how good your best model is; it’s about the diversity of your set of
candidate models.</p>
<h2 id="scaling-up-model-training-with-multiple-devices">Scaling up model training with multiple devices</h2>
<p>Recall the “loop of progress” concept we introduced in chapter 7:
the <em>quality</em> of your ideas is a function of how many refinement cycles they’ve been through (figure 18.1).
And the speed at which you can iterate on an idea is a function of how fast
you can set up an experiment, how fast you can run that experiment,
and, finally, how well you can analyze the resulting data.</p>
<figure id="figure-18-1">
<img src="../Images/55c7b82f43c4187d22ac7ddef68ddd0d.png" data-original-src="https://deeplearningwithpython.io/images/ch18/the_loop_of_progress.4bb26a08.png"/>
<figcaption>
<a href="#figure-18-1">Figure 18.1</a>: The loop of progress
</figcaption>
</figure>

<p>As you develop your expertise in the Keras API, how fast you can
code up your deep learning experiments will cease to be the bottleneck
of this progress cycle. The next bottleneck will become the speed at which you can
train your models. Fast training infrastructure means that you can get
your results back in 10 or 15 minutes and, hence, that you can go through dozens of
iterations every day. Faster training directly improves the <em>quality</em>
of your deep learning solutions.</p>
<p>In this section, you’ll learn about how to scale up your training runs by using multiple GPUs or TPUs.</p>
<h3 id="multi-gpu-training">Multi-GPU training</h3>
<p>While GPUs are getting more powerful every year,
deep learning models are also getting increasingly larger,
requiring ever more computational resources. Training on a single GPU
puts a hard bound on how fast you can move. The solution? You could simply add
more GPUs and start doing <em>multi-GPU distributed training</em>.</p>
<p>There are two ways to distribute computation across multiple devices:
<em>data parallelism</em> and <em>model parallelism</em>.</p>
<p>With data parallelism, a single model gets replicated
on multiple devices or multiple machines. Each of the model replicas processes
different batches of data, and then they merge their results.</p>
<p>With model parallelism, different parts of a single model run on different
devices, processing a single batch of data together at the same time.
This works best with models that have a naturally parallel architecture,
such as models that feature multiple branches. In practice, model parallelism is only used in the case of models that
are too large to fit on any single device: it isn’t used as a way to speed
up training of regular models but as a way to train larger models.</p>
<p>Then, of course, you can also mix both data parallelism and model parallelism:
a single model can be split across multiple devices (e.g., 4), and that split model can
be replicated across multiple groups of devices (e.g., twice, for a total of 2 * 4 = 8 devices used).</p>
<p>Let’s see how that works in detail.</p>
<h4 id="data-parallelism-replicating-your-model-on-each-gpu">Data parallelism: Replicating your model on each GPU</h4>
<p>Data parallelism is the most common form of distributed training.
It operates on a simple principle: divide and conquer.
Each GPU receives a copy of the entire model, called a <em>replica</em>.
Incoming batches of data are split into <em>N</em>
sub-batches, which are processed by one model replica each, in parallel.
This is why it’s called <em>data parallelism</em>: different samples (data points)
are processed in parallel.
For instance, with two GPUs, a batch of size 128 would be split into two sub-batches of size 64,
which would be processed by two model replicas. Then</p>
<ul>
<li><em>In inference</em> — We would retrieve the predictions for each sub-batch and concatenate them to obtain
the predictions for the full batch.</li>
<li><em>In training</em> — We would retrieve the gradients for each sub-batch, average them, and update
all model replicas based on the gradient average. The state of the model would then be the same as if you
had trained it on the full batch of 128 samples. This is called <em>synchronous</em> training,
since all replicas are kept in sync — their weights have the same value at all times.
Nonsynchronous alternatives exist,
but they are less efficient and aren’t used anymore in practice.</li>
</ul>
<p>Data parallelism is a simple and highly scalable way to train your models faster.
If you get more devices, just increase your batch size, and your training throughput increases accordingly.
It has one limitation, though: it requires your model to be able to fit into one of your devices.
However, it is now common to train foundation models that have tens of billions of parameters, which wouldn’t
fit on any single GPU.</p>
<h4 id="model-parallelism-splitting-your-model-across-multiple-gpus">Model parallelism: Splitting your model across multiple GPUs</h4>
<p>That’s where <em>model parallelism</em> comes in. While data parallelism works by splitting
your batches of data into sub-batches and processing the sub-batches in parallel, model parallelism
works by splitting your model into submodels and running each one on a different device — in parallel.
For instance, consider the following model.</p>
<figure id="listing-18-4">
<pre><code class="language-python">model = keras.Sequential(
    [
        keras.layers.Input(shape=(16000,)),
        keras.layers.Dense(64000, activation="relu"),
        keras.layers.Dense(8000, activation="sigmoid"),
    ]
)
</code></pre>
<figcaption>
<a href="#listing-18-4">Listing 18.4</a>: A large densely connected model
</figcaption>
</figure>

<p>Each sample has 16,000 features and gets classified into 8,000 potentially overlapping categories
by two <code>Dense</code> layers. Those are large layers — the first one has about 1 billion parameters,
and the last one has about 512 million parameters. If you’re working with two small devices,
you won’t be able to use data parallelism, since you can’t fit the model on a single device.
What you can do is <em>split</em> a single instance of the model across multiple devices. This is often called
<em>sharding</em> or <em>partitioning</em> a model.
There are two main ways to split a model across devices: horizontal partitioning and vertical partitioning.</p>
<p>In horizontal partitioning, each device processes different layers of the model.
For example, in the previous model, one GPU would handle the first <code>Dense</code> layer, and the other one would handle the second <code>Dense</code> layer.
The main drawback of this approach is that it can introduce communication overhead.
For example, the output of the first layer needs to be copied to the second device
before it can be processed by the second layer. This can become a bottleneck, especially if the output of the first layer is large
— you’d risk keeping your GPUs idle.</p>
<p>In vertical partitioning, each layer is split across all available devices. Since layers are usually implemented in terms of
<code>matmul</code> or <code>convolution</code> operations, which are highly parallelizable,
this strategy is easy to implement in practice and is almost always the best fit for large models.
For example, in the previous model, you could split the kernel and bias of the first <code>Dense</code> layer into two halves
so that each device only receives a kernel of shape <code>(16000, 32000)</code> (split along its last axis) and a bias of shape
<code>(32000,)</code>. You’d compute <code>matmul(inputs, kernel) + bias</code> with this half-kernel and half-bias for each device,
and you’d merge the two outputs by concatenating them like this:</p>
<figure>
<pre><code class="language-python">half_kernel_0 = kernel[:, :32000]
half_bias_0 = bias[:32000]

half_kernel_1 = kernel[:, 32000:]
half_bias_1 = bias[32000:]

with keras.device("gpu:0"):
    half_output_0 = keras.ops.matmul(inputs, half_kernel_0) + half_bias_0

with keras.device("gpu:1"):
    half_output_1 = keras.ops.matmul(inputs, half_kernel_1) + half_bias_1
</code></pre>
</figure>

<p>In reality, you will want to mix data parallelism and model parallelism. You will split your model across, say, four devices, and you will replicate
that split model across multiple groups of two devices — let’s say two — each processing one sub-batch of data in parallel. You will then have two replicas, each running on four devices, for a total of eight devices used (figure 18.2).</p>
<figure id="figure-18-2">
<img src="../Images/83dcf3712374590e5c60bc5c084d67b9.png" data-original-src="https://deeplearningwithpython.io/images/ch18/data_and_model_parallelism.1d1087a1.png"/>
<figcaption>
<a href="#figure-18-2">Figure 18.2</a>: Distributing a model across eight devices: two model replicas, each handled by a group of four devices
</figcaption>
</figure>

<h3 id="distributed-training-in-practice">Distributed training in practice</h3>
<p>Now let’s see how to implement these concepts in practice.
We will only cover the JAX backend, as it is the most performant
and most scalable of the various Keras backends, by a mile. If you’re doing
any kind of large-scale distributed training and you aren’t using JAX, you’re making a mistake
— and wasting your dollars burning way more compute than you actually need.</p>
<h4 id="getting-your-hands-on-two-or-more-gpus">Getting your hands on two or more GPUs</h4>
<p>First, you need to get access to several GPUs. As of now,
Google Colab only lets you use a single GPU, so you will need to do one of two things:</p>
<ul>
<li>Acquire two to eight GPUs, mount them on a single machine (it will require a beefy power supply),
and install CUDA drivers, cuDNN, etc. For most people, this isn’t the best option.</li>
<li>Rent a multi-GPU virtual machine (VM) on Google Cloud, Azure, or AWS. You’ll be able to use
VM images with pre-installed drivers and software, and you’ll have very little
setup overhead. This is likely the best option for anyone who isn’t training models 24/7.</li>
</ul>
<p>We won’t cover the details of how to spin up multi-GPU cloud VMs because such
instructions would be relatively short-lived, and this information is
readily available online.</p>
<h4 id="using-data-parallelism-with-jax">Using data parallelism with JAX</h4>
<p>Using data parallelism with Keras and JAX is very simple: before building your model, just
add the following line of code:</p>
<figure>
<pre><code class="language-python">keras.distribution.set_distribution(keras.distribution.DataParallel())
</code></pre>
</figure>

<p>That’s it.</p>
<p>If you want more granular control, you can specify which devices you want to use. You can list
available devices via</p>
<figure>
<pre><code class="language-python">keras.distribution.list_devices()
</code></pre>
</figure>

<p>It will return a list of strings — the names of your devices, such as <code>"gpu:0"</code>, <code>"gpu:1"</code>, and so on.
You can then pass these to the <code>DataParallel</code> constructor:</p>
<figure>
<pre><code class="language-python">keras.distribution.set_distribution(
    keras.distribution.DataParallel(["gpu:0", "gpu:1"])
)
</code></pre>
</figure>

<p>In an ideal world, training on <em>N</em> GPUs would result in a speedup of factor <em>N</em>.
In practice, however, distribution introduces some overhead — in particular,
  merging the weight deltas originating from different devices takes some time.
  The effective speedup you get is a function of the number of GPUs used:</p>
<ul>
<li>With two GPUs, the speedup stays close to 2×.</li>
<li>With four, the speedup is around 3.8×.</li>
<li>With eight, it’s around 7.3×.</li>
</ul>
<p>This assumes that you’re using a large-enough global batch size to keep
each GPU utilized at full capacity. If your batch size is too small, the local
batch size won’t be enough to keep your GPUs busy.</p>
<h4 id="using-model-parallelism-with-jax">Using model parallelism with JAX</h4>
<p>Keras also provides powerful tools for fully customizing how you want to do distributed training,
including model parallel training and any mixture of data parallel and model parallel training you can imagine.
Let’s dive in.</p>
<h5 id="the-devicemesh-api">The DeviceMesh API</h5>
<p>First, you need to understand the concept of a <em>device mesh</em>.
A device mesh is simply a grid of devices. Consider this example, with eight GPUs:</p>
<figure>
<pre><code class="language-text">gpu:0   |   gpu:4
--------|---------
gpu:1   |   gpu:5
--------|---------
gpu:2   |   gpu:6
--------|---------
gpu:3   |   gpu:7
</code></pre>
</figure>

<p>The big idea is to separate devices into groups, organized along axes. Typically, one axis will be responsible
for data parallelism, and one axis will be responsible for model parallelism (like in figure 18.2, your devices form a grid,
where the horizontal axis handles data parallelism and the vertical axis handles model parallelism).</p>
<p>A device mesh doesn’t have to be 2D — it could be any shape you want. In practice, however, you will only ever see
1D and 2D meshes.</p>
<p>Let’s make a 2 × 4 device mesh in Keras:</p>
<figure>
<pre><code class="language-python">device_mesh = keras.distribution.DeviceMesh(
    # We assume eight devices, organized as a 2 × 4 grid.
    shape=(2, 4),
    # It's convenient to give your axes meaningful names!
    axis_names=["data", "model"],
)
</code></pre>
</figure>

<p>Mind you, you can also explicitly specify the devices you want to use:</p>
<figure>
<pre><code class="language-python">devices = [f"gpu:{i}" for i in range(8)]
device_mesh = keras.distribution.DeviceMesh(
    shape=(2, 4),
    axis_names=["data", "model"],
    devices=devices,
)
</code></pre>
</figure>

<p>As you may have guessed from the <code>axis_names</code> argument, we intend to use
the devices along axis 0 for data parallelism
and the devices along axis 1 for model parallelism. Since there are two devices along
axis 0 and four along axis 1, we’ll split our model’s computation across four GPUs,
and we’ll make two copies of our split model, running each copy on a different sub-batch of data
in parallel.</p>
<p>Now that we have our mesh, we need to tell
Keras how to split different pieces of computation across our devices.
For that, we’ll use the <code>LayoutMap</code> API.</p>
<h5 id="the-layoutmap-api">The LayoutMap API</h5>
<p>To specify where different bits of computation should take place, we use <em>variables</em>
as our frame of reference. We will split or replicate variables across our devices,
and we will let the compiler move all computation associated with that part of the variable
to the corresponding device.</p>
<p>Consider a variable. Its shape is, let’s say, <code>(32, 64)</code>. There are two things you could do with this variable:</p>
<ul>
<li>You could <em>replicate it</em> (copy it) across an axis of your mesh so each device along that axis sees the same value.</li>
<li>You could <em>shard it</em> (split it) across an axis of your mesh — for instance, you could shard it into four chunks of
shape <code>(32, 16)</code> — so that each device along that axis sees one different chunk.</li>
</ul>
<p>Now, do note that our variable has two dimensions. Importantly, “sharding” or “replicating” are decisions
that you can make independently for each dimension of the variable.</p>
<p>The API you will use to tell Keras about such decisions is the <code>LayoutMap</code> class.
A <code>LayoutMap</code> is similar to a dictionary. It maps model variables
(for instance, the kernel variable of the first dense layer in your model)
to a bit of information about how that variable should be replicated or sharded over a device mesh.
Specifically, it maps a <em>variable path</em>
to a tuple that has as many entries as your variable has dimensions,
where each entry specifies what to do with that variable dimension.
It looks like this:</p>
<figure>
<pre><code class="language-python">{
    # None means "replicate the variable along this dimension."
    "sequential/dense_1/kernel": (None, "model"),
    # "model" means "shard the variable along this dimension across the
    # devices of the model axis of the device mesh."
    "sequential/dense_1/bias": ("model",),
    ...
}
</code></pre>
</figure>

<p>This is the first time you encountered the concept of a <em>variable path</em> — it is simply
a string identifier that looks like <code>"sequential/dense_1/kernel"</code>. It’s a useful way to
refer to a variable without keeping a handle on the actual variable instance.</p>
<p>Here’s how you can print the paths for all variables in a model:</p>
<figure>
<pre><code class="language-python">for v in model.variables:
    print(v.path)
</code></pre>
</figure>

<p>On the example model from listing 18.4, here’s what we get:</p>
<figure>
<pre><code class="language-text">sequential/dense/kernel
sequential/dense/bias
sequential/dense_1/kernel
sequential/dense_1/bias
</code></pre>
</figure>

<p>Now let’s shard and replicate these variables. In the case of a simple model like this one,
your go-to rule of thumb for variable sharding should be as follows:</p>
<ul>
<li>Shard the last dimension of the variable along the <code>"model"</code> mesh axis.</li>
<li>Leave all other dimensions as replicated.</li>
</ul>
<p>Simple enough, right? Like this:</p>
<figure>
<pre><code class="language-python">layout_map = keras.distribution.LayoutMap(device_mesh)
layout_map["sequential/dense/kernel"] = (None, "model")
layout_map["sequential/dense/bias"] = ("model",)
layout_map["sequential/dense_1/kernel"] = (None, "model")
layout_map["sequential/dense_1/bias"] = ("model",)
</code></pre>
</figure>

<p>Finally, we tell Keras to refer to this sharding layout when instantiating the variables
by setting the distribution configuration like this:</p>
<figure>
<pre><code class="language-python">model_parallel = keras.distribution.ModelParallel(
    layout_map=layout_map,
    # This argument tells Keras to use the mesh axis named "data" for
    # data parallelism.
    batch_dim_name="data",
)
keras.distribution.set_distribution(model_parallel)
</code></pre>
</figure>

<p>Once the distribution configuration is set, you can create your model and <code>fit()</code> it.
No other part of your code changes — your model definition code is the same, and your training code is the same.
That’s true whether you’re using built-in APIs like <code>fit()</code> and <code>evaluate()</code> or your own training logic.
Assuming that you have the right <code>LayoutMap</code> for your variables,
the little code snippets you just saw are enough to distribute computation for any large language model training run — it scales
to as many devices as you have available and arbitrary model sizes.</p>
<p>To check how your variables were sharded, you can inspect the <code>variable.value.sharding</code> property, like this:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; model.layers[0].kernel.value.sharding</code>
<code class="language-output">NamedSharding(
    mesh=Mesh("data": 2, "model": 4),
    spec=PartitionSpec(None, "model")
)</code></pre>
</figure>

<p>You can even visualize it via the JAX utility <code>jax.debug.visualize_sharding</code>:</p>
<figure>
<pre><code class="language-python">import jax

value = model.layers[0].kernel.value
jax.debug.visualize_sharding(value.shape, value.sharding)
</code></pre>
</figure>

<aside>
<p><span class="note-title">tf.data performance tips</span></p>
<p>When doing distributed training, always provide your data as a <code>tf.data.Dataset</code> object
to guarantee best performance (passing your data as NumPy arrays also works since
  those get converted to <code>Dataset</code> objects by <code>fit()</code>).
  You should also make sure to use
data prefetching: before passing the dataset to <code>fit()</code>, call
<code>dataset.prefetch(buffer_size)</code>. If you aren’t sure what buffer size to pick,
  try the <code>dataset.prefetch(tf.data.AUTOTUNE)</code> option, which will pick a buffer
  size for you.</p>
</aside>

<h3 id="tpu-training">TPU training</h3>
<p>Beyond just GPUs, there is generally a trend in the deep learning world
toward moving workflows to increasingly specialized hardware designed specifically for
deep learning workflows; such single-purpose chips are known as ASICs (application-specific integrated circuits).
Various companies big and small are working on new chips, but today the most prominent
effort along these lines is Google’s Tensor Processing Unit (TPU),
which is available on Google Cloud and via Google Colab.</p>
<p>Training on TPU does involve jumping through some hoops. But it’s worth the extra work:
TPUs are really, really fast. Training on a TPU v2 (available on Colab) will typically be 15× faster than
training a NVIDIA P100 GPU. For most models,
TPU training ends up being 3× more cost-effective than GPU training on average.</p>
<p>You can actually use TPU v2 for free in Colab. In the Colab menu,
under the Runtime tab, in the Change Runtime Type option,
you’ll notice that you have access to a TPU runtime in addition to the GPU
runtime. For more serious training runs, Google Cloud also makes available
TPU v3 through v5, which are even faster.</p>
<p>When running Keras code with the JAX backend on a TPU-enabled notebook,
you don’t need anything more than calling <code>keras.distribution.set_distribution(distribution)</code>
with a <code>DataParallel</code> or <code>ModelParallel</code> distribution instance to start using
your TPU cores. Make sure to call it before creating your model!</p>
<aside>
<p>Because TPUs can process batches of data extremely quickly,
the speed at which you can read data from Google Cloud Storage (GCS) can easily become a bottleneck.
If your dataset is small enough, you should keep it in the memory of the virtual machine.
You can do so by calling <code>dataset.cache()</code> on your <code>tf.data.Dataset</code> instance.
That way, the data will only be read from GCS once.</p>
</aside>

<h4 id="using-step-fusing-to-improve-tpu-utilization">Using step fusing to improve TPU utilization</h4>
<p>Because a TPU has a lot of compute power available, you need to train with
very large batches to keep the TPU cores busy. For small models, the batch
size required can get extraordinarily large — upward of 10,000 samples per batch.
When working with enormous batches, you should make sure to increase your
optimizer learning rate accordingly: you’re going to be making fewer updates
to your weights, but each update will be more accurate (since the gradients are computed
using more data points); hence, you should move the weights by a greater magnitude
with each update.</p>
<p>There is, however, a simple trick you can use to keep reasonably sized
batches while maintaining full TPU utilization: <em>step fusing</em>. The idea
is to run multiple steps of training during each TPU execution step. Basically,
do more work in between two roundtrips from the virtual machine memory to the TPU. To do this,
simply specify the <code>steps_per_execution</code> argument in <code>compile()</code> — for instance,
<code>steps_per_execution=8</code> to run eight steps of training during each TPU execution.
For small models that are underutilizing the TPU, this can result in a dramatic
speedup:</p>
<figure>
<pre><code class="language-python">model.compile(..., steps_per_execution=8)
</code></pre>
</figure>

<h2 id="speeding-up-training-and-inference-with-lower-precision-computation">Speeding up training and inference with lower-precision computation</h2>
<p>What if I told you there’s a simple technique you could use to speed up training and inference
of almost any model by up to 2×, basically for free? It seems too good to be true,
and yet, such a trick does exist.
To understand how it works, first, we need to take a look at the notion
of “precision” in computer science.</p>
<h4 id="understanding-floating-point-precision">Understanding floating-point precision</h4>
<p>Precision is to numbers what resolution is to images.
Because computers can only process 1s and 0s, any number seen by a computer
has to be encoded as a binary string.
For instance, you may be familiar with <code>uint8</code> integers,
which are integers encoded on eight bits: <code>00000000</code> represents <code>0</code> in <code>uint8</code>, and
<code>11111111</code> represents 255. To represent integers beyond 255, you’d need to add
more bits — eight isn’t enough. Most integers are stored on 32 bits,
with which we can represent signed integers ranging from −2147483648 to 2147483647.</p>
<p>Floating-point numbers are the same. In mathematics, real numbers form a continuous
axis: there’s an infinite number of points in between any two numbers.
You can always zoom in on the axis of reals. In computer science, this isn’t true:
there’s only a finite number of intermediate points between 3 and 4, for instance.
How many? Well, it depends on the <em>precision</em> you’re working with: the number of bits you’re using
to store a number. You can only zoom up to a certain resolution.</p>
<p>There are three levels of precision you’d typically use:</p>
<ul>
<li>Half precision, or <code>float16</code>, where numbers are stored on 16 bits</li>
<li>Single precision, or <code>float32</code>, where numbers are stored on 32 bits</li>
<li>Double precision, or <code>float64</code>, where numbers are stored on 64 bits</li>
</ul>
<p>You could even go up to <code>float8</code>, as you’ll see in a bit.</p>
<aside>
<p><span class="note-title">On floating-point encoding</span></p>
<p>A counterintuitive fact about floating-point numbers is
that representable numbers are not uniformly distributed.
Larger numbers have lower precision: there’s the same number of representable
values between <code>2 ** N</code> and <code>2 ** (N + 1)</code> as there is between 1 and 2,
for any <em>N</em>.</p>
<p>That’s because floating-point numbers are encoded
in three parts — the sign, the significant value (called the <em>mantissa</em>), and
the exponent in the form</p>
<p><code>{sign} * (2 ** ({exponent} - 127)) * 1.{mantissa}</code></p>
<p>For example, figure 18.3 demonstrates how you would encode the closest <code>float32</code> value approximating
Pi:</p>
<figure id="figure-18-3">
<img src="../Images/b827d6a3b2dca9270d7365db741dca4c.png" data-original-src="https://deeplearningwithpython.io/images/ch18/floating_pi.b6d4aaaf.png"/>
<figcaption>
<a href="#figure-18-3">Figure 18.3</a>: The number Pi encoded in single precision via a sign bit, an integer exponent, and an integer mantissa
</figcaption>
</figure>

<p>For this reason, the numerical error incurred when converting a number to
its floating-point representation can vary wildly depending on the exact value
considered, and the error tends to get larger for numbers with a large absolute value.</p>
</aside>

<p>The way to think about the resolution of floating-point numbers is in terms
of the smallest distance between two arbitrary numbers that you’ll be able to safely
process. In single precision, that’s around 1e-7. In double precision, that’s around
1e-16. And in half precision, it’s only 1e-3.</p>
<h4 id="float16-inference">Float16 inference</h4>
<p>Every model you’ve seen in this book so far has used single-precision numbers: it stored
its state as <code>float32</code> weight variables and ran its computations on <code>float32</code> inputs.
That’s enough precision to run the forward and backwards pass of a model
without losing any information — in particular when it comes to small gradient updates
(recall that the typically learning rate is 1e-3, and it’s pretty common to see
weight updates on the order of 1e-6).</p>
<p>Modern GPUs and TPUs feature specialized hardware that
can run 16-bit operations much faster and using less memory
than equivalent 32-bit operations. By using these lower-precision operations
whenever possible, you can speed up training on those devices by a significant
factor. You can set the default floating point precision to <code>float16</code>
in Keras via</p>
<figure>
<pre><code class="language-python">import keras

keras.config.set_dtype_policy("float16")
</code></pre>
</figure>

<p>Note that this should be done before you define your model. Doing this will net you a nice
speedup for model inference, for instance, via <code>model.predict()</code>. You should expect a nearly 2× speed boost on GPU and TPU.</p>
<p>There’s also an alternative to <code>float16</code> that works better on some devices, in particular TPUs: <code>bfloat16</code>.
<code>bfloat16</code> is also a 16-bit precision floating-point type, but it differs from <code>float16</code> in its structure:
it uses 8 exponent bits instead of 5, and 7 mantissa bits instead of 10 (see table 18.1). This means it can
cover a much wider range of values, but it has a lower “resolution” over this range.
Some devices are better optimized for <code>bfloat16</code> compared to <code>float16</code>, so it can be a good idea to try both before settling for the option
that turns out to be the fastest.</p>
<figure id="table-18-1" class="table-figure"><table>
<thead>
<tr>
<th>dtype</th>
<th><code>float16</code></th>
<th><code>bfloat16</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Exponent bits</td>
<td>5</td>
<td>8</td>
</tr>
<tr>
<td>Mantissa bits</td>
<td>10</td>
<td>7</td>
</tr>
<tr>
<td>Sign bits</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table><figcaption>
<a href="#table-18-1">Table 18.1</a>: Difference between <code>float16</code> and <code>bfloat16</code>
</figcaption>
</figure>

<h4 id="mixed-precision-training">Mixed-precision training</h4>
<p>Setting your default float precision to 16 bits is a great way to speed up inference.
Now, when it comes to training, there’s a significant complication.
The gradient descent process wouldn’t run smoothly in <code>float16</code> or <code>bfloa16</code>, since we couldn’t represent
small gradient updates of around 1e-5 or 1e-6, which are quite common.</p>
<p>You can, however, use a hybrid approach: that’s what <em>mixed-precision training</em> is about.
The idea is to use 16-bit computation in places where precision
isn’t an issue, while working with 32-bit values in other places to maintain
numerical stability — in particular, when handling gradients and variable updates.
By maintaining the precision-sensitive parts of
the model in full precision, you can get most of the speed benefits of 16-bit computation
without meaningfully impacting model quality.</p>
<p>You can turn on mixed precision like this:</p>
<figure>
<pre><code class="language-python">import keras

keras.config.set_dtype_policy("mixed_float16")
</code></pre>
</figure>

<p>Typically, most of the forward pass of the model will be done in <code>float16</code>
(with the exception of numerically unstable operations like softmax),
while the weights of the model will be stored and updated in <code>float32</code>. Your <code>float16</code> gradients
will be cast to <code>float32</code> before updating the <code>float32</code> variables.</p>
<p>Keras layers have a <code>variable_dtype</code> and a <code>compute_dtype</code> attribute. By default,
both of these are set to <code>float32</code>. When you turn on mixed precision, the
<code>compute_dtype</code> of most layers switches to <code>float16</code>. As a result, those layer will cast their
inputs to <code>float16</code> and will perform their computation in <code>float16</code>
(using half-precision copies of the weights). However, since their <code>variable_dtype</code>
is still <code>float32</code>, their weights will be able to receive accurate <code>float32</code> updates
from the optimizer, as opposed to half-precision updates.</p>
<p>Some operations may be numerically unstable in <code>float16</code> (in particular,
softmax and crossentropy). If you need to opt out of mixed precision
for a specific layer, just pass the argument <code>dtype="float32"</code> to the
constructor of this layer.</p>
<h4 id="using-loss-scaling-with-mixed-precision">Using loss scaling with mixed precision</h4>
<p>During training, gradients can become very small. When using mixed precision, your gradients remain in <code>float16</code> (same as the forward pass).
As a result, the limited range of representable numbers can cause small gradients to be rounded down to zero.
This prevents the model from learning effectively.</p>
<p>Gradient values are proportional to the loss value, so to encourage gradients to be larger, a
simple trick is to multiply the loss by a large scalar factor. Your gradients will then
be much less likely to get rounded to zero.</p>
<p>Keras makes this easy. If you want to use a fixed loss scaling factor, you can simply pass a <code>loss_scale_factor</code> argument to your optimizer
like this:</p>
<figure>
<pre><code class="language-python">optimizer = keras.optimizers.Adam(learning_rate=1e-3, loss_scale_factor=10)
</code></pre>
</figure>

<p>If you would like for the optimizer to automatically figure out the right scaling factor, you can also use the <code>LossScaleOptimizer</code> wrapper:</p>
<figure>
<pre><code class="language-python">optimizer = keras.optimizers.LossScaleOptimizer(
    keras.optimizers.Adam(learning_rate=1e-3)
)
</code></pre>
</figure>

<p>Using <code>LossScaleOptimizer</code> is usually your best option: the right scaling value can change over the course of training!</p>
<h4 id="beyond-mixed-precision-float8-training">Beyond mixed precision: float8 training</h4>
<p>If running your forward pass in 16-bit precision yields such neat performance benefits, you might want to ask: Could we go even lower?
What about 8-bit precision? Four bits, maybe? Two bits? The answer is, it’s complicated.</p>
<p>Mixed precision training using <code>float16</code> in the forward pass is that last level of precision that “just works” — <code>float16</code> precision
has enough bits to represent all intermediate tensors (except for gradient updates, which is why we use <code>float32</code> for those).
This is no longer true if you go down to <code>float8</code> precision: you are simply losing too much information.
It is still possible to use <code>float8</code> in some computations, but this requires you to make considerable modifications to your forward pass.
You will <em>not</em> be able to simply set your <code>compute_dtype</code> to <code>float8</code> and run.</p>
<p>The Keras framework provides a built-in implementation for <code>float8</code> training. Because it specifically targets Transformer use cases,
it only covers a restricted set of layers: <code>Dense</code>, <code>EinsumDense</code> (the version of <code>Dense</code> that is used by the <code>MultiHeadAttention</code> layer),
and <code>Embedding</code> layers. The way it works is not simple — it keeps track of past activation values to rescale activations at each step so as to utilize the full range of values representable in <code>float8</code>. It also needs to override part of the backward pass to do the same with gradient values.</p>
<p>Importantly, this added overhead has a computational cost. If your model is too small or if your GPU isn’t powerful enough, that cost will exceed the benefits of
doing certain operations in <code>float8</code>, and you will see a slowdown instead of a speedup. <code>float8</code> training is only viable for very large models (typically over 5B parameters) and large, recent GPUs such as the NVDIA H100. <code>float8</code> is rarely used in practice, except in foundation model training runs.</p>
<h3 id="faster-inference-with-quantization">Faster inference with quantization</h3>
<p>Running inference in <code>float16</code> — or even <code>float8</code> — will result in a nice speedup for your models. But there’s also another trick
you can use: <em><code>int8</code> quantization</em>. The big idea is to take an already trained model with weights in <code>float32</code> and convert these weights
to a lower-precision dtype (typically <code>int8</code>) while preserving the numerical correctness of the forward pass as much as possible.</p>
<p>If you want to implement quantization from scratch, the math is simple: the general idea is to scale all <code>matmul</code> input tensors by a certain factor so that their coefficients fit in the range representable with <code>int8</code>, which is <code>[-127, 127]</code> — a total of 256 possible values. After scaling the inputs, you cast them to <code>int8</code> and perform the <code>matmul</code> operation in <code>int8</code> precision, which should be quite a bit faster than <code>float16</code>. Finally, you cast the output back to <code>float32</code>, and you divide it by the product of the input scaling factors. Since <code>matmul</code> is a linear operation, this final unscaling cancels out the initial scaling, and you should get the same output as if you used the original values — any loss of accuracy only comes from the value rounding that happens when you cast the inputs to <code>int8</code>.</p>
<p>Let’s make this concrete with an example. Let’s say you want to perform <code>matmul(x, kernel)</code>, with the following values:</p>
<figure>
<pre><code class="language-python">from keras import ops

x = ops.array([[0.1, 0.9], [1.2, -0.8]])
kernel = ops.array([[-0.1, -2.2], [1.1, 0.7]])
</code></pre>
</figure>

<p>If you were to naively cast these values to <code>int8</code> without scaling first, that would be very destructive — for instance, your <code>x</code> would become <code>[[0, 0], [1, 0]]</code>. So let’s apply the “abs-max” scaling scheme, which spreads out the values of each tensor across the <code>[-127, 127]</code> range:</p>
<figure>
<pre><code class="language-python">def abs_max_quantize(value):
    # Max of absolute value of the tensor
    abs_max = ops.max(ops.abs(value), keepdims=True)
    # Scale is max of int range divided by max of tensor (1e-7 is to
    # avoid dividing by 0).
    scale = ops.divide(127, abs_max + 1e-7)
    # Scales the value
    scaled_value = value * scale
    # Rounding and clipping first is more accurate than directly
    # casting.
    scaled_value = ops.clip(ops.round(scaled_value), -127, 127)
    # Casts to int8
    scaled_value = ops.cast(scaled_value, dtype="int8")
    return scaled_value, scale

int_x, x_scale = abs_max_quantize(x)
int_kernel, kernel_scale = abs_max_quantize(kernel)
</code></pre>
</figure>

<p>Now we can perform a faster <code>matmul</code> and unscale the output:</p>
<figure>
<pre><code class="language-python">int_y = ops.matmul(int_x, int_kernel)
y = ops.cast(int_y, dtype="float32") / (x_scale * kernel_scale)
</code></pre>
</figure>

<p>How accurate is it? Let’s compare our <code>y</code> with the output of the <code>float32</code> <code>matmul</code>:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; y</code>
<code class="language-output">array([[ 0.9843736,  0.3933239],
       [-1.0151455, -3.1965137]])</code>
<code class="language-python">&gt;&gt;&gt; ops.matmul(x, kernel)</code>
<code class="language-output">array([[ 0.98      ,  0.40999997],
       [-1.        , -3.2       ]])</code></pre>
</figure>

<p>Pretty accurate! For a large <code>matmul</code>, doing this will save you a lot of compute, since <code>int8</code> computation can be considerably faster than even <code>float16</code> computation,
and you only had to add fairly fast elementwise ops to the computation graphs — <code>abs</code>, <code>max</code>, <code>clip</code>, <code>cast</code>, <code>divide</code>, <code>multiply</code>.</p>
<p>Now, of course, I don’t expect you to ever implement quantization by hand — that would be tremendously impractical. Similarly to <code>float8</code>, <code>int8</code> quantization
is built directly into specific Keras layers: <code>Dense</code>, <code>EinsumDense</code>, and <code>Embedding</code>. This unlocks <code>int8</code> inference support for any Transformer-based model.
Here’s how to use it with any Keras model that includes such layers:</p>
<figure>
<pre><code class="language-python"># Instantiates a model (or any quantizable layer)
model = ...
# Boom!
model.quantize("int8")
# Now predict() and call() will run (partially) in int8!
predictions = model.predict(...)
</code></pre>
</figure>

<h2 id="summary">Summary</h2>
<ul>
<li>You can use hyperparameter tuning and KerasTuner to automate the tedium out
of finding the best model configuration. But be mindful of validation-set overfitting!</li>
<li>An ensemble of diverse models can often significantly improve the quality of your predictions.</li>
<li>To further scale your workflows, you can use <em>data parallelism</em> to train a model on multiple devices, as
long as the model is small enough to fit on a single device.</li>
<li>For larger models, you can also use <em>model parallelism</em> to split your model’s variables and computation
across several devices.</li>
<li>You can speed up model training on GPUs or TPUs by turning on mixed precision — you’ll generally get
a nice speed boost at virtually no cost.</li>
<li>You can also speed up inference by using <code>float16</code> precision or even <code>int8</code> quantization.</li>
</ul>
    
</body>
</html>