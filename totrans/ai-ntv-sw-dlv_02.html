<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Source Control Management"><div class="chapter" id="chapter_2_source_control_management_1749354010078326">
<h1><span class="label">Chapter 2. </span>Source Control Management</h1>

<p>Imagine a scenario where you and your team are collaborating on a complex software project<a contenteditable="false" data-primary="source control management (SCM)" data-type="indexterm" id="xi_sourcecontrolmanagementSCM23100"/>. Multiple minds are contributing, making revisions and enhancements. Without a clear system for managing changes, you risk overwriting each other’s work and losing track of who updated what and why they made those changes. Without a clear system to tag sets of changes, you are unable to navigate back to a previous stable version of your team’s code should an issue arise. Without defined workflows and structured access control, anyone can change anything at any time, with no oversight. Without controls, your team is unable to determine which code files were used to build a particular release should you need to recreate it.</p>

<p>Next, imagine that several teams have worked for months on a new application and it is now nearing time to deploy to your production environment. Ad hoc fixes and tweaks have been made to various development and QA environments, but those have not been reliably reflected in the production environment. Important production settings have not been repeated in QA environments, and development environments vary widely. Given the increasing complexity of the required environments, spinning up new environments has become a time-consuming and error-prone bottleneck that creates frustration and delay.</p>

<p>These situations are recipes for dysfunction and wasted effort. Source control management (SCM) practices were created to address these very problems. At its core, SCM is about tracking and managing changes made to code and other critical resources like configurations over time.</p>

<p class="pagebreak-before">Today, artificial intelligence<a contenteditable="false" data-primary="AI (artificial intelligence) systems" data-secondary="and SCM" data-secondary-sortas="SCM" data-type="indexterm" id="id359"/> is transforming how we approach SCM. AI can automatically detect risky changes, suggest improvements to code or configurations, and even help resolve merge conflicts by understanding the intent behind modifications. It can identify inconsistencies across environments, recommend corrections, and optimize deployment workflows. AI-powered tools are not just helping teams manage complexity—they’re enabling faster, safer, and more resilient development cycles. As software delivery becomes more distributed and dynamic, AI is becoming an essential partner in making SCM more intelligent, proactive, and efficient.</p>

<section data-type="sect1" data-pdf-bookmark="Introducing Source Control Management"><div class="sect1" id="chapter_2_introducing_source_control_management_1749354010078447">
<h1>Introducing Source Control Management</h1>

<p>The problem of coordinating changes across a team dates back to the early days of programming, and the history of SCM practices is intricately linked to the evolution of computer programming. In this section we’ll explore how SCM has evolved and the critical role AI tools play in modern SCM.</p>

<section data-type="sect2" data-pdf-bookmark="A Short History of Source Control Management"><div class="sect2" id="chapter_2_a_short_history_of_source_control_management_1749354010078512">
<h2>A Short History of Source Control Management</h2>

<p>In the early days of programming<a contenteditable="false" data-primary="source control management (SCM)" data-secondary="history of" data-type="indexterm" id="xi_sourcecontrolmanagementSCMhistoryof21246"/>, programs were relatively simple; they were constrained by limited hardware, and code management was rudimentary. As CPUs became powerful and sophisticated, computation and code became more complex. Code repositories<a contenteditable="false" data-primary="code repositories" data-seealso="GitOps" data-type="indexterm" id="id360"/>, central stores that provide basic SCM functions, first emerged in the 1970s alongside the rise of high-level languages and structured programming methodologies. Tools like Source Code Control System (SCCS)<a contenteditable="false" data-primary="Source Code Control System (SCCS)" data-type="indexterm" id="id361"/><a contenteditable="false" data-primary="SCCS (Source Code Control System)" data-type="indexterm" id="id362"/> offered basic version tracking, allowing developers to revert to previous versions and see the history of changes. These early systems mirrored the shift toward more organized program development.</p>

<p>SCM further evolved in the 1970s with the emergence of more structured software engineering teams. Tools like Revision Control System (RCS)<a contenteditable="false" data-primary="Revision Control System (RCS)" data-type="indexterm" id="id363"/><a contenteditable="false" data-type="indexterm" data-primary="RCS (Revision Control System)" id="id364"/>, introduced in 1982, and <a contenteditable="false" data-primary="CVS (Concurrent Version System)" data-type="indexterm" id="id365"/>Concurrent Versions System (CVS)<a contenteditable="false" data-primary="Concurrent Versions System (CVS)" data-type="indexterm" id="id366"/>, introduced in 1986, added features crucial for collaboration, including branching. This enabled more complex project management and a collaborative culture.</p>

<p>In the early 1990s, IBM Rational ClearCase<a contenteditable="false" data-primary="IBM Rational ClearCase" data-type="indexterm" id="id367"/> emerged as a commercial solution for SCM. It emphasized robust configuration management and process customization, making it suitable for complex software development environments. Subversion (SVN)<a contenteditable="false" data-primary="Subversion (SVN)" data-type="indexterm" id="id368"/>, developed by CollabNet<a contenteditable="false" data-primary="CollabNet" data-type="indexterm" id="id369"/>, is another centralized code repository that gained popularity. SVN 1.0 was released in 2004 to address shortcomings in CVS and provide missing features.</p>

<section data-type="sect3" data-pdf-bookmark="Distributed version control and Git"><div class="sect3" id="chapter_2_distributed_version_control_and_git_1749354010078566">
<h3>Distributed version control and Git</h3>

<p>The rise of Agile methodologies and open source<a contenteditable="false" data-primary="distributed version control" data-type="indexterm" id="xi_distributedversioncontrol21763"/><a contenteditable="false" data-primary="Git" data-type="indexterm" id="xi_Git21763"/> in the early 2000s put new demands on software development. Rapid releases meant that teams required more flexibility and control over increasingly complex codebases. Teams themselves changed, becoming larger and often geographically dispersed. Git was created in 2005 by Linus Torvalds<a contenteditable="false" data-primary="Torvalds, Linus" data-type="indexterm" id="id370"/>, the creator of the Linux kernel. He needed a powerful and efficient system to manage the massive codebase of the Linux project, and existing options fell short.</p>

<p>A version control system (VCS)<a contenteditable="false" data-primary="version control system (VCS)" data-type="indexterm" id="xi_VersionControlSystemVCS21846"/> is the core technology that tracks changes to files over time, forming the foundation of any SCM approach. Unlike most earlier code repositories, Git is a distributed VCS. With a centralized VCS<a contenteditable="false" data-primary="code repositories" data-secondary="centralized versus distributed VCS" data-type="indexterm" id="xi_coderepositoriescentralizedversusdistributedVCS218241"/>, everyone works from a single copy of the codebase stored in a central server (repository). Each developer has their own local copy (working copy) that they can modify. When a developer makes changes and commits them, those changes are immediately uploaded to the central repository, making them visible to everyone else. To see the latest changes from others, developers simply need to update their local copy from the central repository. <a data-type="xref" href="#chapter_2_figure_1_1749354010071222">Figure 2-1</a> shows a centralized VCS.</p>

<figure><div id="chapter_2_figure_1_1749354010071222" class="figure"><img src="assets/ansd_0201.png" width="600" height="407"/>
<h6><span class="label">Figure 2-1. </span><a href="https://oreil.ly/YLeDg">Centralized version control</a></h6>
</div></figure>

<p>Distributed systems take a different approach. Here, each developer has a complete copy of the codebase (including both the repository and their working copy) on their local machine. Changes made by a developer are private to their local copy until they explicitly share them with the team. This is done by “pushing” their changes to the central repository. Similarly, to see updates made by other developers, users need to download (“fetch”) those changes from the central repository into their local copy. <a data-type="xref" href="#chapter_2_figure_2_1749354010071251">Figure 2-2</a> shows a Git distributed VCS.</p>

<figure><div id="chapter_2_figure_2_1749354010071251" class="figure"><img src="assets/ansd_0202.png" width="600" height="618"/>
<h6><span class="label">Figure 2-2. </span>Distributed version control with Git</h6>
</div></figure>

<p>Git’s focus on speed, its distributed nature, and robust branching made it a game-changer in a number of ways:</p>

<dl>
	<dt>Distributed facilitates offline work</dt>
	<dd>
	<p>Git’s decentralized approach facilitates efficient and independent work, as developers can make changes locally without a central server. This also enabled developers to work offline.</p>
	</dd>
	<dt>Flexible branching and merging</dt>
	<dd>
	<p>Git’s branching system is incredibly flexible. Developers can create isolated branches to work on new features or bug fixes without affecting the main codebase. Merging these branches back into the main codebase is a smooth and efficient process. This empowers developers to experiment and iterate more freely.</p>
	</dd>
	<dt>Lightweight and efficient for large codebases</dt>
	<dd>
	<p>Git excels at handling large codebases efficiently. It only stores the differences between code versions, making it faster and requiring less storage space than traditional SCM systems.</p>
	</dd>
	<dt>Nonlinear history aids organizations</dt>
	<dd>
	<p>Unlike some SCM systems that enforce a linear history, Git allows developers to rewrite history through functionalities like rebasing. This flexibility helps maintain a clean and organized codebase.</p>
	</dd>
</dl>

<p>The first widely used hosted Git repositories arrived a few years later. GitHub<a contenteditable="false" data-primary="GitHub" data-type="indexterm" id="id371"/>, the most popular today, was launched in 2008. These platforms are built upon the power of Git, offering a user-friendly web interface, cloud storage for codebases, and collaboration features. This combination transformed Git from a powerful but technical tool to an accessible and social platform for software development, making it a cornerstone of modern software development workflows.</p>

<p>While traditional centralized repositories still have a legacy footprint and are in use in environments with very specific needs, Git is now the predominant choice. A <a href="https://oreil.ly/rLVE0">2022 Stack Overflow survey</a> found that 94% of overall respondents used Git and 98% of those using any source control use Git. For this reason, we will focus our attention on Git repository variations<a contenteditable="false" data-primary="" data-startref="xi_distributedversioncontrol21763" data-type="indexterm" id="id372"/><a contenteditable="false" data-primary="" data-startref="xi_Git21763" data-type="indexterm" id="id373"/><a contenteditable="false" data-primary="" data-startref="xi_VersionControlSystemVCS21846" data-type="indexterm" id="id374"/><a contenteditable="false" data-primary="" data-startref="xi_coderepositoriescentralizedversusdistributedVCS218241" data-type="indexterm" id="id375"/>.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Branching out with Git"><div class="sect3" id="chapter_2_branching_out_with_git_1749354010078616">
<h3>Branching out with Git</h3>

<p>In 2010, Gitflow<strong> </strong>branching<a contenteditable="false" data-primary="branching with Git" data-type="indexterm" id="xi_branchingwithGit25459"/><a contenteditable="false" data-primary="Gitflow" data-type="indexterm" id="xi_Gitflow25459"/> conventions emerged to use branching to provide a clear separation between development, feature creation, and release preparation. <a data-type="xref" href="#chapter_2_figure_3_1749354010071267">Figure 2-3</a> shows a Gitflow workflow.</p>

<p>In the Gitflow workflow:</p>

<ol>
	<li>
	<p>The main<a contenteditable="false" data-primary="main branch, Gitflow" data-type="indexterm" id="id376"/> codebase resides on a branch called “main.” This branch is typically considered stable and should only contain production-ready code.</p>
	</li>
	<li>
	<p>A new “develop” branch<a contenteditable="false" data-primary="develop branch, Gitflow" data-type="indexterm" id="id377"/>, which serves as the continuous integration branch for all development work, is created.</p>
	</li>
	<li>
	<p>Feature development happens on isolated branches (feature/release branches<a contenteditable="false" data-primary="feature/release branches, Gitflow" data-type="indexterm" id="id378"/>) that branch from the develop branch. Developers work on new features and bug fixes on these feature branches. Once a feature is complete and thoroughly tested, it’s merged back into the develop branch.</p>
	</li>
	<li>
	<p>The develop branch acts as an integration point for all completed features. It represents the upcoming release version and is continuously updated with merged feature branches.</p>
	</li>
	<li>
	<p>When it’s time for a release, a release branch<a contenteditable="false" data-primary="release branch" data-type="indexterm" id="id379"/> is created from “develop.” Bug fixes and minor adjustments can be made on this branch. Once finalized, the release branch is merged back into “main” to create the official release. A corresponding tag is created in “main” to mark the release version.</p>
	</li>
</ol>

<figure><div id="chapter_2_figure_3_1749354010071267" class="figure"><img src="assets/ansd_0203.png" width="494" height="800"/>
<h6><span class="label">Figure 2-3. </span><a href="https://oreil.ly/L2ZLg">Gitflow workflow</a></h6>
</div></figure>

<p>Pull requests<a contenteditable="false" data-primary="pull requests (PRs)" data-type="indexterm" id="id380"/>, sometimes abbreviated as PRs, are a core collaboration feature in Git version control used for code review and integration, and are widely used with Gitflow and other branching models. Pull requests provide a structured way for developers to propose changes to a codebase and get them reviewed by others before merging them into the main branch.</p>

<p>Gitflow’s emphasis on planned releases and separate release branches has been challenged by newer Git branching models. Fueled by the growing adoption of continuous integration and continuous delivery<a contenteditable="false" data-primary="CI/CD pipeline" data-secondary="in Gitflow" data-secondary-sortas="Gitflow" data-type="indexterm" id="id381"/>, these models prioritize faster deployments with more frequent updates. Trunk-based development discards the idea of a dedicated development branch altogether. Instead, features are continuously integrated directly into the main branch (often called “trunk” or “main”) after rigorous testing. <a data-type="xref" href="#chapter_2_figure_4_1749354010071282">Figure 2-4</a> shows this pattern.</p>

<figure><div id="chapter_2_figure_4_1749354010071282" class="figure"><img src="assets/ansd_0204.png" width="192" height="800"/>
<h6><span class="label">Figure 2-4. </span>Trunk-based development</h6>
</div></figure>

<p>This streamlined approach allows for quicker feedback loops and faster deployments, aligning well with modern DevOps practices. Pull requests remain essential in these workflows, ensuring code quality through code review before merging changes into the main branch<a contenteditable="false" data-primary="" data-startref="xi_sourcecontrolmanagementSCMhistoryof21246" data-type="indexterm" id="id382"/><a contenteditable="false" data-primary="" data-startref="xi_branchingwithGit25459" data-type="indexterm" id="id383"/><a contenteditable="false" data-primary="" data-startref="xi_Gitflow25459" data-type="indexterm" id="id384"/>.</p>
</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="GitOps and Source Control Management"><div class="sect2" id="chapter_2_gitops_and_source_control_management_1749354010078665">
<h2>GitOps and Source Control Management</h2>

<p>We have seen how code repositories<a contenteditable="false" data-primary="source control management (SCM)" data-secondary="and GitOps" data-secondary-sortas="GitOps" data-type="indexterm" id="id385"/><a contenteditable="false" data-primary="GitOps" data-secondary="SCM" data-type="indexterm" id="id386"/> evolved alongside programming and software development practices to solve the problems we imagined, enabling teams to <span class="keep-together">collaborate</span> effectively in source code. But what about deployment problems? How can we efficiently and systematically produce the environments we need and how can we streamline the deployment of our code into the production environment?</p>

<p>Here is where GitOps comes in. In bringing Dev and Ops together, DevOps emphasizes the importance of automation<a contenteditable="false" data-primary="automation" data-secondary="and GitOps" data-secondary-sortas="GitOps" data-type="indexterm" id="id387"/><a contenteditable="false" data-primary="GitOps" data-secondary="automating Git changes" data-type="indexterm" id="id388"/> in eliminating manual errors and helps ensure consistency across environments. This translates to faster deployments, improved reliability, and reduced risk. GitOps refers to automating the process of provisioning infrastructure, especially in modern container-first, cloud infrastructures. GitOps emphasizes the use of a code repository<a contenteditable="false" data-primary="code repositories" data-secondary="repository as single source of truth" data-type="indexterm" id="id389"/> (usually Git) as the single source of truth for the desired state of the system and leverages automation to continuously reconcile the actual state with the desired state. Resources stored to our repositories can include:</p>

<dl>
	<dt>Infrastructure configuration</dt>
	<dd>
	<p>Files that define components needed for the environment, the type and number of virtual machines (VMs), storage configurations, network settings, and security policies. This can include declarative and imperative configurations and deployment scripts.</p>
	</dd>
	<dt>Environment variables</dt>
	<dd>
	<p>These are essential for storing sensitive information like passwords or API keys that should not be directly embedded in code. Infrastructure as Code (IaC) tools often have mechanisms for managing and referencing environment variables securely.</p>
	</dd>
	<dt>Additional resources</dt>
	<dd>
	<p>Depending on the complexity of the environment, the repository might also store other resources such as container images (through git-lfs) used for application deployment.</p>
	</dd>
</dl>

<p>Using our repository as that single source of truth, we can take advantage of its powerful features. We get detailed version tracking and change histories, and we can manage our infrastructure updates with Git workflows that promote collaboration and oversight like code reviews through pull requests. Well-managed infrastructure automation translates to faster deployments, fewer errors, and reliable environments every time a new one needs to be created. We’ll learn more about using GitOps to deploy in <a data-type="xref" href="ch04.html#chapter_4_deploying_to_test_environments_1749354010445896">Chapter 4</a>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Monorepos and Remote Caching"><div class="sect2" id="chapter_2_monorepos_and_remote_caching_1749354010078712">
<h2>Monorepos and Remote Caching</h2>

<p>We mentioned the importance of microservices<a contenteditable="false" data-primary="microservices architectures" data-type="indexterm" id="xi_microservicesarchitectures211058"/> in <a data-type="xref" href="ch01.html#chapter_1_the_road_to_ai_native_devops_1749354009875299">Chapter 1</a>. Two key practices that enhance productivity in microservices-based systems are the use of monorepos and remote caching.</p>

<p>A monorepo<a contenteditable="false" data-primary="source control management (SCM)" data-secondary="monorepos" data-type="indexterm" id="id390"/><a contenteditable="false" data-primary="monorepo architectures" data-type="indexterm" id="id391"/> (monolithic repository) is a single version-controlled code repository that stores the code for multiple projects or services. In a microservices context, this approach simplifies collaboration, streamlines dependency management, enables atomic updates across services, and reduces versioning conflicts.</p>

<p>Remote caching<strong> </strong>refers to storing build artifacts—such as compiled code<a contenteditable="false" data-primary="source control management (SCM)" data-secondary="remote caching" data-type="indexterm" id="id392"/><a contenteditable="false" data-primary="remote caching" data-type="indexterm" id="id393"/><a contenteditable="false" data-primary="caching" data-type="indexterm" id="id394"/> or test results—on remote servers. Tools like Nx use this technique to significantly speed up development workflows by allowing teams to reuse previously generated outputs instead of rebuilding from scratch, reducing redundant computations.</p>

<p>Together, monorepos and remote caching support faster and more efficient CI/CD pipelines and contribute to improved overall system performance. However, monorepos can introduce complexity as projects scale, and remote caching can raise concerns about vendor lock-in if not thoughtfully implemented<a contenteditable="false" data-primary="" data-startref="xi_microservicesarchitectures211058" data-type="indexterm" id="id395"/>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="AI in Source Control Management"><div class="sect2" id="chapter_2_ai_in_source_control_management_1749354010078765">
<h2>AI in Source Control Management</h2>

<p>AI tools<a contenteditable="false" data-primary="source control management (SCM)" data-secondary="AI in" data-type="indexterm" id="id396"/><a contenteditable="false" data-primary="AI-native software delivery" data-secondary="SCM" data-type="indexterm" id="id397"/> have revolutionized how developers approach coding. GitHub Copilot, Cursor, Harness AI Code Agent<a contenteditable="false" data-primary="GitHub Copilot" data-type="indexterm" id="id398"/><a contenteditable="false" data-primary="Cursor" data-type="indexterm" id="id399"/><a contenteditable="false" data-primary="Harness" data-secondary="AI Code Agent" data-type="indexterm" id="id400"/>, and similar coding assistants/agents act as intelligent pair programmers, offering real-time code suggestions based on project context. These tools can predict and suggest entire lines or blocks of code, significantly speeding up the development process.</p>

<p>Beyond code completion, AI assistants can:</p>

<ul>
	<li>
	<p>Generate boilerplate code structures automatically</p>
	</li>
	<li>
	<p>Suggest different implementation approaches</p>
	</li>
	<li>
	<p>Provide code explanation and documentation</p>
	</li>
	<li>
	<p>Assist with debugging and optimization</p>
	</li>
</ul>

<p>AI-native software delivery starts with an AI-native SCM. The integration of AI with SCM extends beyond just code completion. Within SCMs, AI can analyze repository patterns, identify potential bugs before they reach production, and suggest architectural improvements based on best practices observed across similar projects. This proactive approach significantly reduces technical debt and improves code quality from the earliest stages of development. We will explore some of these themes later in the chapter.</p>

<p>In the following sections we’ll walk through how SCM systems fit into the delivery pipeline. With that understanding, we’ll discuss factors to consider when choosing an SCM that is right for your team. Lastly, we’ll look at characteristics of modern code repositories, including the role of AI, that can simplify your entire software development pipeline.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Source Control Management in the Delivery Pipeline"><div class="sect1" id="chapter_2_source_control_management_in_the_delivery_pipeline_1749354010078816">
<h1>Source Control Management in the Delivery Pipeline</h1>

<p>The core repository<a contenteditable="false" data-primary="source control management (SCM)" data-secondary="in delivery pipeline" data-secondary-sortas="delivery pipeline" data-type="indexterm" id="xi_sourcecontrolmanagementSCMindeliverypipeline213931"/><a contenteditable="false" data-primary="pipelines" data-secondary="SCM" data-type="indexterm" id="xi_pipelinesSCM213931"/><a contenteditable="false" data-primary="core repository, pipeline role of" data-type="indexterm" id="id401"/> is a critical component of the delivery pipeline, anchoring the entire pipeline process. It serves as the single source of truth for the code, ensuring consistency and reliability, and it is the entity that developers interact with continually, initiating integration and delivery activities.</p>

<p class="pagebreak-before"><a data-type="xref" href="#chapter_2_figure_5_1749354010071299">Figure 2-5</a> depicts the relationship of the code repository to continuous integration and delivery.</p>

<figure><div id="chapter_2_figure_5_1749354010071299" class="figure"><img src="assets/ansd_0205.png" width="600" height="186"/>
<h6><span class="label">Figure 2-5. </span>Developer actions against the code repository instigate the CI/CD pipeline</h6>
</div></figure>

<p>Let’s walk through the three main parts of a typical pipeline:</p>

<dl>
	<dt>Code repository</dt>
	<dd>
	<p>Developers work against the code repository, committing changes and opening and closing pull requests.</p>
	</dd>
	<dt>Continuous integration</dt>
	<dd>
	<p>Continuous integration<a contenteditable="false" data-primary="CI/CD pipeline" data-secondary="AI’s role in" data-type="indexterm" id="id402"/> is initiated by specific actions within a code repository. These triggers can be customized, including events such as code commits, the opening or closing of pull requests, or other relevant actions determined by your team’s specific needs and practices. CI gives developers rapid feedback on code changes. By automating builds and tests, CI acts as an early warning system, alerting developers to potential bugs, integration issues, or even style violations. This immediate feedback empowers developers to quickly address problems, preventing them from snowballing into larger, more costly issues down the line. With CI, your codebase stays in a consistently deployable state, ready for the next step in your delivery pipeline.</p>
	</dd>
	<dt>Continuous delivery and deployment</dt>
	<dd>
	<p>Continuous delivery and deployment steps automate the provisioning of infrastructure and the deployment of new code versions to one or more pre-production environments. Various types of tests are typically executed against the app running in pre-production environments. We’ll look at these steps in <a data-type="xref" href="ch04.html#chapter_4_deploying_to_test_environments_1749354010445896">Chapter 4</a>. Finally, automatic or manual decisions gate the final deployment of the software into the production environment. We’ll discuss these steps at length in <a data-type="xref" href="ch08.html#chapter_8_feature_management_and_experimentation_1749354011197288">Chapter 8</a>. By deploying smaller changes frequently, CD streamlines the delivery process, reduces release risk, and enhances the ability to respond to user feedback quickly.</p>
	</dd>
</dl>

<p class="pagebreak-before">Many code repositories build in secret detection<a contenteditable="false" data-primary="secret detection" data-type="indexterm" id="id403"/><a contenteditable="false" data-primary="code repositories" data-secondary="secret detection" data-type="indexterm" id="id404"/> features. Secrets can include the following:</p>

<dl>
	<dt>API keys</dt>
	<dd>
	<p>Unique identifiers<a contenteditable="false" data-primary="API keys" data-type="indexterm" id="id405"/> used to authenticate and authorize access to various web services and APIs</p>
	</dd>
	<dt>Access tokens</dt>
	<dd>
	<p>Temporary credentials<a contenteditable="false" data-primary="access tokens" data-type="indexterm" id="id406"/> that grant specific access rights to an application or resource</p>
	</dd>
	<dt>OAuth tokens</dt>
	<dd>
	<p>Tokens<a contenteditable="false" data-primary="OAuth tokens" data-type="indexterm" id="id407"/> used for delegated authorization, allowing one application to access resources on behalf of a user</p>
	</dd>
	<dt>Private keys</dt>
	<dd>
	<p>Secret keys<a contenteditable="false" data-primary="private keys" data-type="indexterm" id="id408"/> used in asymmetric encryption to decrypt messages or verify digital signatures</p>
	</dd>
	<dt>Usernames and passwords</dt>
	<dd>
	<p>Credentials used for basic authentication<a contenteditable="false" data-primary="usernames and passwords, authentication" data-type="indexterm" id="id409"/> to systems and services</p>
	</dd>
	<dt>Database connection strings</dt>
	<dd>
	<p>Details needed to establish a connection to a database<a contenteditable="false" data-primary="databases" data-secondary="connection strings" data-type="indexterm" id="id410"/>, often including sensitive information like hostnames, usernames, and passwords</p>
	</dd>
	<dt>Cloud service connection strings</dt>
	<dd>
	<p>Strings used to connect to cloud services<a contenteditable="false" data-primary="cloud service connection strings" data-type="indexterm" id="id411"/> like Azure Storage or AWS S3, potentially containing access keys and other secrets</p>
	</dd>
</dl>

<p>Some code repositories will prevent or warn a developer when attempting to commit or merge code with a detected secret. CI processes can play a role in secret detection, preventing them from reaching a production environment. An ideal approach is to leverage both for comprehensive security<a contenteditable="false" data-primary="" data-startref="xi_sourcecontrolmanagementSCMindeliverypipeline213931" data-type="indexterm" id="id412"/><a contenteditable="false" data-primary="" data-startref="xi_pipelinesSCM213931" data-type="indexterm" id="id413"/>.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Code Repository Considerations"><div class="sect1" id="chapter_2_code_repository_considerations_1749354010078869">
<h1>Code Repository Considerations</h1>

<p>Given the importance of SCM to software development, selecting a code repository<a contenteditable="false" data-primary="source control management (SCM)" data-secondary="code repository considerations" data-type="indexterm" id="xi_sourcecontrolmanagementSCMcoderepositoryconsiderations2195114"/><a contenteditable="false" data-primary="code repositories" data-secondary="SCM" data-type="indexterm" id="xi_coderepositoriesSCM2195114"/> is one of the first decisions a team will make. <em>Where will we put the source code?</em> is a question a team will need to answer to even kick off a project.</p>

<p>First and foremost, a repository must support the basic operations and the developer workflows that are critical to your team:</p>

<ul>
	<li class="pagebreak-before less_space">
	<p>Creating, importing, and cloning repositories with support for distributed offline work</p>
	</li>
	<li>
	<p>Branching, merging, and defining branching rules to meet your specific team’s needs (e.g., limiting branch creation/deletion to specific users)</p>
	</li>
	<li>
	<p>Creating, reviewing, and merging pull requests, along with defining pull request policies in line with the governance your team requires (e.g., requiring all changes to be associated with a pull request, prohibiting direct commits, or setting a minimum number of required reviewer approvals)</p>
	</li>
	<li>
	<p>Creating and modifying tags, and defining tag policies (e.g., enforcing tag names to adhere to a specific pattern like semantic versioning)</p>
	</li>
</ul>

<p>While there may be differences in the implementation details, these are expected repository features.</p>

<p>In creating a delivery pipeline, teams typically start with repository choice first; because this is a choice that can have far-reaching effects on the implementation, it is critical to ensure your code repository will support seamless integration within a broader ecosystem. Your code repository should function in an ecosystem that enhances your team’s productivity instead of adding to their workload. In addition, a solution should be cost-effective and provide the transparency your organization requires.</p>

<section data-type="sect2" data-pdf-bookmark="Comprehensive Integrations"><div class="sect2" id="chapter_2_comprehensive_integrations_1749354010078918">
<h2>Comprehensive Integrations</h2>

<p>A well-designed DevOps ecosystem is characterized by easy-to-use tooling and comprehensive integrations with the functions and services that your delivery pipeline requires. This stands in contrast to a piecemeal approach, where developers are burdened with manual integration of many disparate tools, which can lead to issues that are difficult to troubleshoot and security risks. It is also contrasted with overly complex single-platform solutions, often suffering from feature bloat, that are difficult to configure.</p>

<p>An example of streamlined integration is configuration-as-code<a contenteditable="false" data-primary="configuration-as-code" data-type="indexterm" id="id414"/>. This practice allows updates to your delivery pipeline to be versioned and tracked directly within your repository, just like your project code. You can further enhance collaboration and governance by enforcing workflows that require changes to be made through pull requests and approvals, mirroring standard development practices.</p>

<p>Another feature example relates to security/vulnerability scanning<a contenteditable="false" data-primary="security considerations" data-secondary="vulnerability scanning" data-type="indexterm" id="id415"/>. Displaying detected vulnerabilities and suggested remediations in the context of a pull request helps the developer quickly understand and resolve any detected issue.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="AI-Powered Features"><div class="sect2" id="chapter_2_ai_powered_features_1749354010078963">
<h2>AI-Powered Features</h2>

<p>The past few years have seen explosive growth in coding assistants or agents<a contenteditable="false" data-primary="agentic AI" data-secondary="as coding assistants" data-secondary-sortas="coding assistants" data-type="indexterm" id="id416"/><a contenteditable="false" data-primary="AI coding assistants" data-secondary="AI agents as" data-type="indexterm" id="id417"/> that use large language models to improve developer efficiency. These coding assistants help with auto-completing code, generating code suggestions, understanding what a piece of code does, and many other use cases. When AI assistants are integrated with code repositories to have access to the full codebase as context—not just isolated code snippets—they can generate more accurate and relevant suggestions.</p>

<p>MCP<a contenteditable="false" data-primary="Model Context Protocol (MCP)" data-type="indexterm" id="id418"/><a contenteditable="false" data-primary="MCP (Model Context Protocol)" data-type="indexterm" id="id419"/> plays a key role here by providing a universal, standardized way to connect AI models and code assistants with various data sources, including repositories like Harness Code Repository<a contenteditable="false" data-primary="Harness" data-secondary="Code Repository" data-type="indexterm" id="id420"/>, GitHub, and Git<a contenteditable="false" data-primary="GitHub" data-type="indexterm" id="id421"/><a contenteditable="false" data-primary="Git" data-type="indexterm" id="id422"/>. This eliminates the need for custom integrations, reducing development effort and increasing efficiency.</p>

<p>Another powerful application of generative AI (GenAI) in code repositories is semantic search—the ability to search an entire codebase using natural language. Tools like Sourcegraph’s Cody and Harness Code Repository enable developers to ask questions like, “How is authentication implemented and where is this code?” rather than relying on keyword-based searches like “log in” or “authenticate.” This capability is especially valuable for onboarding new team members and helping them quickly understand complex codebases without deep familiarity with project-specific terminology.</p>

<p>Regarding code reviews<a contenteditable="false" data-primary="code reviews" data-type="indexterm" id="id423"/>, tools like DeepCode and Codacy<a contenteditable="false" data-primary="DeepCode" data-type="indexterm" id="id424"/><a contenteditable="false" data-primary="Codacy" data-type="indexterm" id="id425"/> use ML algorithms to review code changes, automatically detecting potential bugs, code smells, and adherence to coding standards more efficiently than manual reviews. Other use cases for AI in SCMs are enhancing security by automatically scanning for vulnerabilities and compliance issues before code is committed and recommending fixes for those issues, summarizing pull requests, and generating software delivery pipelines using SCM as one of the data sources.</p>

<p>It is important to note that with AI systems, results depend heavily on the data used to train the AI models. So, for example, “good” code will result in good code suggestions and reviews, and “bad” code will result in bad code suggestions and reviews.</p>

<p>Measuring the impact of AI is equally important in verifying whether using AI has actually had a positive impact on the developers. Tools such as Harness<a contenteditable="false" data-primary="Harness" data-secondary="Software Engineering Insights" data-type="indexterm" id="id426"/> Software Engineering Insights and others can help with measuring the productivity of developers using different coding assistants and also compare them with the developers that don’t use any coding assistants.</p>

<p>AI-powered SCMs accelerate time-to-market by generating fast and reliable code (especially when well-trained), improving code quality by identifying issues—including security vulnerabilities—at the source, and enhancing team collaboration by elevating the quality and efficiency of code reviews.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Efficiency and Transparency Through Open Source"><div class="sect2" id="chapter_2_efficiency_and_transparency_through_open_source_1749354010079010">
<h2>Efficiency and Transparency Through Open Source</h2>

<p>Whether or not your DevOps tools are open source<a contenteditable="false" data-primary="open source software (OSS)" data-secondary="versus proprietary solutions" data-type="indexterm" id="id427"/> is an important consideration. Open source solutions can be cost-effective for organizations with budget constraints, and the transparency they offer has advantages as well.</p>

<p>Proprietary solutions can often claim to offer reliable uptime and dedicated customer support teams to address any technical issues you encounter. However, there are often subscription fees for enterprise users, which can be a significant cost factor for small teams. Open source codebases are free to use, making them ideal for teams with limited budgets. The open source nature allows for transparency<a contenteditable="false" data-primary="transparency, OSS" data-type="indexterm" id="id428"/> and community-driven development. Developers have access to the source code, enabling customization of the platform to fit specific needs. However, they often have to rely on the community for troubleshooting and support. While valuable, open source may not offer the same level of guaranteed assistance as a commercial provider. In addition, while open source promotes transparency, it also means potential vulnerabilities are publicly visible.</p>

<p>Open core solutions<a contenteditable="false" data-primary="open core solutions" data-type="indexterm" id="id429"/>, like Harness.io<a contenteditable="false" data-primary="Harness.io" data-type="indexterm" id="id430"/> and GitLab<a contenteditable="false" data-primary="GitLab" data-type="indexterm" id="id431"/>, provide a middle ground. They offer a free, feature-limited version, akin to open source.</p>

<p>Lastly, OSS can be put into escrow<a contenteditable="false" data-primary="escrow, moving open source software into" data-type="indexterm" id="id432"/> if needed for regulatory requirements or to ensure continuity of business generally. This provides assurance that in the event the tool provider goes out of business you will still have access to the tools needed to build, test, and monitor your application and to recreate your development, testing, and production environments.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="A Platform Approach"><div class="sect2" id="chapter_2_a_platform_approach_1749354010079055">
<h2>A Platform Approach</h2>

<p>Traditional, piecemeal DevOps toolchains often create data silos and hinder visibility into the entire SDLC. However, a single DevOps platform<a contenteditable="false" data-primary="platform engineering" data-type="indexterm" id="id433"/> offers a compelling solution by providing end-to-end visibility. For example, it enables tracking of every change, from the initial commit in the code repository to the final deployment on production servers. This holistic view helps you to identify bottlenecks, pinpoint potential issues early in the development cycle, and measure the overall effectiveness of your DevOps practices. Furthermore, comprehensive audit trails provide a clear record of all activity, simplifying troubleshooting and ensuring compliance with security regulations.</p>

<p>A unified platform also streamlines governance<a contenteditable="false" data-primary="governance" data-secondary="platform approach" data-type="indexterm" id="id434"/> and unlocks the potential for intelligent automation. Managing governance policies across disparate tools can be cumbersome and error-prone. A single platform allows you to define and enforce policies consistently throughout your entire development pipeline. This ensures code adheres to coding standards, security best practices, and internal guidelines. For example, you can streamline governance by implementing a policy such as <em>scan code before <span class="keep-together">committing</span>, during the CI process, and during the CD process using (your organization’s) approved security scanner.</em> With a unified platform this can be easily implemented as a template that gets reused.</p>

<p>Additionally, with a complete understanding of the deployment context, including infrastructure and configuration details, the platform can offer intelligent code suggestions that optimize performance and efficiency. Imagine an AI-powered assistant that recommends code tweaks based on how the service will be deployed, potentially saving development time and improving code quality.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Access Control, an Example"><div class="sect2" id="chapter_2_access_control_an_example_1749354010079102">
<h2>Access Control, an Example</h2>

<p>As teams assemble a delivery toolchain, it’s common to start with individual point solutions. However, this piecemeal approach can lead to significant operational overhead. In this section, we’ll look at the example of RBAC<a contenteditable="false" data-primary="role-based access control (RBAC)" data-type="indexterm" id="xi_rolebasedaccesscontrolRBAC2244237"/><a contenteditable="false" data-primary="security considerations" data-secondary="RBAC in DevOps pipeline" data-type="indexterm" id="xi_securityconsiderationsRBACinDevOpspipeline2244237"/><a contenteditable="false" data-primary="platform engineering" data-secondary="RBAC in" data-type="indexterm" id="xi_platformengineeringRBACin2244237"/><a contenteditable="false" data-primary="RBAC (role-based access control)" data-type="indexterm" id="xi_RBACrolebasedaccesscontrol2244237"/> to see how a cohesive delivery pipeline can simplify operations and empower development teams.</p>

<p>Most collaboration tools use role-based access to functionality in some form or another. Code repositories will support built-in roles, or will include built-in roles and will allow users to define custom roles. GitHub, for example, defines the roles <em>Read, Triage, Write, Maintain,</em> and <em>Admin</em>. These roles correspond to levels of access; the Read role is recommended for noncode contributors, whereas the Admin role is designed for users who require full access to the project, including sensitive and destructive actions.</p>

<p>These systems use RBAC, a method of managing access to resources within a system that centers on three core elements, namely users, roles, and permissions:</p>

<ul>
	<li>
	<p>Users represent individuals or accounts requiring access.</p>
	</li>
	<li>
	<p>Roles are defined sets of permissions that grant access to specific resources or actions within the system.</p>
	</li>
	<li>
	<p>Permissions are the fundamental units of control, defining what actions a user can take (like reading, editing, or deleting data).</p>
	</li>
</ul>

<p>Users<a contenteditable="false" data-primary="users" data-secondary="in RBAC" data-secondary-sortas="RBAC" data-type="indexterm" id="id435"/> are not directly assigned permissions. Instead, they are assigned one or more roles. Once a user is assigned a role, they inherit all the permissions associated with that role. This approach simplifies access management by eliminating the need to individually assign permissions to every user. Instead, permissions<a contenteditable="false" data-primary="permissions, in RBAC roles" data-type="indexterm" id="id436"/> are defined at the role level, and users are granted access based on the roles they are assigned. <a data-type="xref" href="#chapter_2_figure_6_1749354010071315">Figure 2-6</a> illustrates users assigned to roles and the sets of permissions associated with roles.</p>

<figure><div id="chapter_2_figure_6_1749354010071315" class="figure"><img src="assets/ansd_0206.png" width="600" height="447"/>
<h6><span class="label">Figure 2-6. </span>Users are assigned to roles; permissions are associated with roles</h6>
</div></figure>

<p class="fix_tracking">Using role-based access is a common pattern that reduces administrative and enhances security by enforcing the principle of least privilege—users are granted only the permissions necessary for their job functions. Role-based access also helps with compliance, as it provides clear documentation of who has access to what within the system.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Defining Roles, a Platform Approach"><div class="sect2" id="chapter_2_defining_roles_a_platform_approach_1749354010079149">
<h2>Defining Roles, a Platform Approach</h2>

<p class="fix_tracking">Imagine a DevOps ecosystem<a contenteditable="false" data-primary="platform engineering" data-secondary="defining roles" data-type="indexterm" id="id437"/> consisting of a Git repo, Jenkins, Terraform to manage an AWS infrastructure, Ansible for configuration management, and Datadog to capture performance metrics. In a system like this, constructed of several disparate tools, you might find that you need to define similar roles in each system, and repeatedly add the same. Provisioning a new developer might take several time-consuming steps. Let’s look at how an all-in-one platform handles RBAC using a platform approach.</p>

<p>As an example, the Harness Platform<a contenteditable="false" data-primary="Harness" data-secondary="Platform" data-type="indexterm" id="id438"/> has a three-level hierarchical structure. The three levels, or scopes, are Account, Organization (Org), and Project:</p>

<ul>
	<li>
	<p>Account is the topmost entity. It can exercise control and has visibility over the entire platform.</p>
	</li>
	<li>
	<p>Organization is a unit of control where people and projects from the same business unit can be organized in an independent hierarchy. An organization can have multiple projects.</p>
	</li>
	<li>
	<p>Projects represent the basic unit of collaboration in which users are grouped together to work on the same task.</p>
	</li>
</ul>

<p>Resource Groups<a contenteditable="false" data-primary="Resource Groups, RBAC" data-type="indexterm" id="id439"/> are an RBAC component that define the objects that a user can access. Objects<a contenteditable="false" data-primary="objects, Harness" data-type="indexterm" id="id440"/> are any Harness resource, including projects, pipelines, connectors, secrets, delegates, environments, users, and more. When you assign a Resource Group to a user, the access defined in the Resource Group is granted to the target user. Resource Groups can be defined at any scope.</p>

<p>Roles likewise can be defined at each scope. Roles are applied together to Resource Groups to create a complete set of permissions and access. For example, you can assign the Pipeline Executor role to a Resource Group that only allows access to specific pipelines, rather than all pipelines in the project<a contenteditable="false" data-primary="" data-startref="xi_sourcecontrolmanagementSCMcoderepositoryconsiderations2195114" data-type="indexterm" id="id441"/><a contenteditable="false" data-primary="" data-startref="xi_coderepositoriesSCM2195114" data-type="indexterm" id="id442"/><a contenteditable="false" data-primary="" data-startref="xi_rolebasedaccesscontrolRBAC2244237" data-type="indexterm" id="id443"/><a contenteditable="false" data-primary="" data-startref="xi_securityconsiderationsRBACinDevOpspipeline2244237" data-type="indexterm" id="id444"/><a contenteditable="false" data-primary="" data-startref="xi_platformengineeringRBACin2244237" data-type="indexterm" id="id445"/><a contenteditable="false" data-primary="" data-startref="xi_RBACrolebasedaccesscontrol2244237" data-type="indexterm" id="id446"/>.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="chapter_2_summary_1749354010079193">
<h1>Summary</h1>

<p>In this chapter, we introduced SCM, a cornerstone of modern software development. SCM addresses the challenges of team collaboration and managing changes to codebases over time. It enables teams to collaborate effectively and manage code changes over time.</p>

<p>SCM is essential to DevOps and CI/CD workflows, and its role is expanding with the emergence of AI-native SCM systems. These intelligent systems can generate, review, analyze, and optimize code, transforming how teams write and manage software. By automating routine tasks, enhancing accuracy, and surfacing insights, AI-powered SCM systems accelerate development and improve delivery efficiency<a contenteditable="false" data-primary="" data-startref="xi_sourcecontrolmanagementSCM23100" data-type="indexterm" id="id447"/>.</p>

<p class="fix_tracking">We also discussed the importance of selecting the right code repository and the benefits of a unified DevOps platform for cohesive workflows and stronger governance. With a solid SCM foundation in place, <a data-type="xref" href="ch03.html#chapter_3_the_build_and_pre_deployment_testing_steps_of_cont_1749354010266208">Chapter 3</a> dives into how continuous integration automates builds and unit tests to ensure code quality and development speed.</p>
</div></section>
</div></section></div></div></body></html>