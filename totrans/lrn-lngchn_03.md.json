["```py\nWhich country is the latest winner of the men’s FIFA World Cup?\n```", "```py\nThe most recent FIFA World Cup winner was France, who won the tournament in 2018.\n```", "```py\nWhich country is the latest winner of the men's FIFA World Cup?\n\nSee context below.\n\nThe FIFA World Cup, often called the World Cup, is an international association\nfootball competition among the senior men's national teams of the members of\nthe Fédération Internationale de Football Association (FIFA), the sport's \nglobal governing body. The tournament has been held every four years since the \ninaugural tournament in 1930, with the exception of 1942 and 1946 due to the \nSecond World War. The reigning champions are Argentina, who won their third \ntitle at the 2022 tournament.\n```", "```py\nThe latest winner of the men's FIFA World Cup is Argentina, who won their third\ntitle at the 2022 tournament.\n```", "```py\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_postgres.vectorstores import PGVector\n\n# Load the document, split it into chunks\nraw_documents = TextLoader('./test.txt').load()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n    chunk_overlap=200)\ndocuments = text_splitter.split_documents(raw_documents)\n\n# embed each chunk and insert it into the vector store\nmodel = OpenAIEmbeddings()\nconnection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\ndb = PGVector.from_documents(documents, model, connection=connection)\n```", "```py\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { PGVectorStore } from \"@langchain/community/vectorstores/pgvector\";\n\n// Load the document, split it into chunks\nconst loader = new TextLoader(\"./test.txt\");\nconst raw_docs = await loader.load();\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n});\nconst docs = await splitter.splitDocuments(docs)\n\n// embed each chunk and insert it into the vector store\nconst model = new OpenAIEmbeddings();\nconst db = await PGVectorStore.fromDocuments(docs, model, {\n  postgresConnectionOptions: {\n    connectionString: 'postgresql://langchain:langchain@localhost:6024/langchain'\n  }\n})\n```", "```py\n# create retriever\nretriever `=` db`.`as_retriever()\n\n# fetch relevant documents\ndocs `=` retriever`.`invoke(\"\"\"Who are the key figures in the ancient greek \n history of philosophy?\"\"\")\n```", "```py\n// create retriever\nconst retriever = db.asRetriever()\n\n// fetch relevant documents\nconst docs = await retriever.invoke(`Who are the key figures in the ancient \n greek history of philosophy?`)\n```", "```py\n# create retriever with k=2\nretriever = db.as_retriever(search_kwargs={\"k\": 2})\n\n# fetch the 2 most relevant documents\ndocs = retriever.invoke(\"\"\"Who are the key figures in the ancient greek history \n of philosophy?\"\"\")\n```", "```py\n// create retriever with k=2\nconst retriever = db.asRetriever({k: 2})\n\n// fetch the 2 most relevant documents\nconst docs = await retriever.invoke(`Who are the key figures in the ancient \n greek history of philosophy?`)\n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\nretriever = db.as_retriever()\n\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on \n the following context:\n{context}\n\nQuestion: {question}\n\"\"\")\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\nchain = prompt | llm\n\n# fetch relevant documents \ndocs = retriever.get_relevant_documents(\"\"\"Who are the key figures in the \n ancient greek history of philosophy?\"\"\")\n\n# run\nchain.invoke({\"context\": docs,\"question\": \"\"\"Who are the key figures in the \n ancient greek history of philosophy?\"\"\"})\n```", "```py\nimport {ChatOpenAI} from '@langchain/openai'\nimport {ChatPromptTemplate} from '@langchain/core/prompts'\n\nconst retriever = db.asRetriever()\n\nconst prompt = ChatPromptTemplate.fromTemplate(`Answer the question based only \n on the following context:\n{context}\n\nQuestion: {question}\n`)\n\nconst llm = new ChatOpenAI({temperature: 0, modelName: 'gpt-3.5-turbo'})\n\nconst chain = prompt.pipe(llm)\n\n// fetch relevant documents\nconst docs = await retriever.invoke(`Who are the key figures in the ancient \n greek history of philosophy?`)\n\nawait chain.invoke({context: docs, question: `Who are the key figures in the \n ancient greek history of philosophy?`})\n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import chain\n\nretriever = db.as_retriever()\n\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on \n the following context:\n{context}\n\nQuestion: {question}\n\"\"\")\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n\n@chain\ndef qa(input):\n    # fetch relevant documents \n    docs = retriever.get_relevant_documents(input)\n    # format prompt\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n    # generate answer\n    answer = llm.invoke(formatted)\n    return answer\n\n# run\nqa.invoke(\"Who are the key figures in the ancient greek history of philosophy?\")\n```", "```py\nimport {ChatOpenAI} from '@langchain/openai'\nimport {ChatPromptTemplate} from '@langchain/core/prompts'\nimport {RunnableLambda} from '@langchain/core/runnables'\n\nconst retriever = db.asRetriever()\n\nconst prompt = ChatPromptTemplate.fromTemplate(`Answer the question based only \n on the following context:\n{context}\n\nQuestion: {question}\n`)\n\nconst llm = new ChatOpenAI({temperature: 0, modelName: 'gpt-3.5-turbo'})\n\nconst qa = RunnableLambda.from(async input => {\n  // fetch relevant documents\n  const docs = await retriever.invoke(input)\n  // format prompt\n  const formatted = await prompt.invoke({context: docs, question: input})\n  // generate answer\n  const answer = await llm.invoke(formatted)\n  return answer\n})\n\nawait qa.invoke(`Who are the key figures in the ancient greek history of \n philosophy?`)\n```", "```py\n@chain\ndef qa(input):\n    # fetch relevant documents \n    docs = retriever.get_relevant_documents(input)\n    # format prompt\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n    # generate answer\n    answer = llm.invoke(formatted)\n    return {\"answer\": answer, \"docs\": docs}\n```", "```py\nconst qa = RunnableLambda.from(async input => {\n  // fetch relevant documents\n  const docs = await retriever.invoke(input)\n  // format prompt\n  const formatted = await prompt.invoke({context: docs, question: input})\n  // generate answer\n  const answer = await llm.invoke(formatted)\n  return {answer, docs}\n})\n```", "```py\n@chain\ndef qa(input):\n    # fetch relevant documents \n    docs = retriever.get_relevant_documents(input)\n    # format prompt\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n    # generate answer\n    answer = llm.invoke(formatted)\n    return answer\n\nqa.invoke(\"\"\"Today I woke up and brushed my teeth, then I sat down to read the \n news. But then I forgot the food on the cooker. Who are some key figures in \n the ancient greek history of philosophy?\"\"\")\n```", "```py\nconst qa = RunnableLambda.from(async input => {\n  // fetch relevant documents\n  const docs = await retriever.invoke(input)\n  // format prompt\n  const formatted = await prompt.invoke({context: docs, question: input})\n  // generate answer\n  const answer = await llm.invoke(formatted)\n  return answer\n})\n\nawait qa.invoke(`Today I woke up and brushed my teeth, then I sat down to read \n the news. But then I forgot the food on the cooker. Who are some key figures \n in the ancient greek history of philosophy?`)\n```", "```py\nBased on the given context, there is no information provided.\n```", "```py\nrewrite_prompt = ChatPromptTemplate.from_template(\"\"\"Provide a better search \n query for web search engine to answer the given question, end the queries \n with ’**’. Question: {x} Answer:\"\"\")\n\ndef parse_rewriter_output(message):\n    return message.content.strip('\"').strip(\"**\")\n\nrewriter = rewrite_prompt | llm | parse_rewriter_output\n\n@chain\ndef qa_rrr(input):\n    # rewrite the query\n    new_query = rewriter.invoke(input)\n    # fetch relevant documents \n    docs = retriever.get_relevant_documents(new_query)\n    # format prompt\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n    # generate answer\n    answer = llm.invoke(formatted)\n    return answer\n\n# run\nqa_rrr.invoke(\"\"\"Today I woke up and brushed my teeth, then I sat down to read \n the news. But then I forgot the food on the cooker. Who are some key \n figures in the ancient greek history of philosophy?\"\"\")\n```", "```py\nconst rewritePrompt = ChatPromptTemplate.fromTemplate(`Provide a better search \n query for web search engine to answer the given question, end the queries \n with ’**’. Question: {question} Answer:`)\n\nconst rewriter = rewritePrompt.pipe(llm).pipe(message => {\n  return message.content.replaceAll('\"', '').replaceAll('**')\n})\n\nconst qa = RunnableLambda.from(async input => {\n  const newQuery = await rewriter.invoke({question: input});\n  // fetch relevant documents\n  const docs = await retriever.invoke(newQuery)\n  // format prompt\n  const formatted = await prompt.invoke({context: docs, question: input})\n  // generate answer\n  const answer = await llm.invoke(formatted)\n  return answer\n})\n\nawait qa.invoke(`Today I woke up and brushed my teeth, then I sat down to read \n the news. But then I forgot the food on the cooker. Who are some key \n figures in the ancient greek history of philosophy?`)\n```", "```py\nBased on the given context, some key figures in the ancient greek history of \nphilosophy include: Themistocles (an Athenian statesman), Pythagoras, and Plato.\n```", "```py\nfrom langchain.prompts import ChatPromptTemplate\n\nperspectives_prompt = ChatPromptTemplate.from_template(\"\"\"You are an AI language \n model assistant. Your task is to generate five different versions of the \n given user question to retrieve relevant documents from a vector database. \n By generating multiple perspectives on the user question, your goal is to \n help the user overcome some of the limitations of the distance-based \n similarity search. Provide these alternative questions separated by \n newlines. Original question: {question}\"\"\")\n\ndef parse_queries_output(message):\n    return message.content.split('\\n')\n\nquery_gen = perspectives_prompt | llm | parse_queries_output\n```", "```py\nconst perspectivesPrompt = ChatPromptTemplate.fromTemplate(`You are an AI \n language model assistant. Your task is to generate five different versions \n of the given user question to retrieve relevant documents from a vector \n database. By generating multiple perspectives on the user question, your \n goal is to help the user overcome some of the limitations of the \n distance-based similarity search. Provide these alternative questions \n separated by newlines. Original question: {question}`)\n\nconst queryGen = perspectivesPrompt.pipe(llm).pipe(message => {\n  return message.content.split('\\n')\n})\n```", "```py\ndef get_unique_union(document_lists):\n    # Flatten list of lists, and dedupe them\n    deduped_docs = {\n        doc.page_content: doc\n        for sublist in document_lists for doc in sublist\n    }\n    # return a flat list of unique docs\n    return list(deduped_docs.values())\n\nretrieval_chain = query_gen | retriever.batch | get_unique_union\n```", "```py\nconst retrievalChain = queryGen\n  .pipe(retriever.batch.bind(retriever))\n  .pipe(documentLists => {\n    const dedupedDocs = {}\n    documentLists.flat().forEach(doc => {\n      dedupedDocs[doc.pageContent] = doc\n    })\n    return Object.values(dedupedDocs)\n  })\n```", "```py\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based \n on this context:\n\n{context}\n\nQuestion: {question}\n\"\"\")\n\n@chain\ndef multi_query_qa(input):\n    # fetch relevant documents \n    docs = retrieval_chain.invoke(input)\n    # format prompt\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n    # generate answer\n    answer = llm.invoke(formatted)\n    return answer\n\n# run\nmulti_query_qa.invoke(\"\"\"Who are some key figures in the ancient greek history \n of philosophy?\"\"\")\n```", "```py\nconst prompt = ChatPromptTemplate.fromTemplate(`Answer the following \n question based on this context:\n\n{context}\n\nQuestion: {question}\n`)\n\nconst multiQueryQa = RunnableLambda.from(async input => {\n  // fetch relevant documents\n  const docs = await retrievalChain.invoke(input)\n  // format prompt\n  const formatted = await prompt.invoke({context: docs, question: input})\n  // generate answer\n  const answer = await llm.invoke(formatted)\n  return answer\n})\n\nawait multiQueryQa.invoke(`Who are some key figures in the ancient greek \n history of philosophy?`)\n```", "```py\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt_rag_fusion = ChatPromptTemplate.from_template(\"\"\"You are a helpful \n assistant that generates multiple search queries based on a single input \n query. \\n\n Generate multiple search queries related to: {question} \\n\n Output (4 queries):\"\"\")\n\ndef parse_queries_output(message):\n    return message.content.split('\\n')\n\nllm = ChatOpenAI(temperature=0)\n\nquery_gen = prompt_rag_fusion | llm | parse_queries_output\n```", "```py\nimport {ChatPromptTemplate} from '@langchain/core/prompts';\nimport {ChatOpenAI} from '@langchain/openai';\nimport {RunnableLambda} from '@langchain/core/runnables';\n\nconst perspectivesPrompt = ChatPromptTemplate.fromTemplate(`You are a helpful \n assistant that generates multiple search queries based on a single input \n query. \\n\n Generate multiple search queries related to: {question} \\n\n Output (4 queries):`)\n\nconst queryGen = perspectivesPrompt.pipe(llm).pipe(message => {\n  return message.content.split('\\n')\n})\n```", "```py\ndef reciprocal_rank_fusion(results: list[list], k=60):\n    \"\"\"reciprocal rank fusion on multiple lists of ranked documents \n and an optional parameter k used in the RRF formula\n \"\"\"\n\n    # Initialize a dictionary to hold fused scores for each document\n    # Documents will be keyed by their contents to ensure uniqueness\n    fused_scores = {}\n    documents = {}\n\n    # Iterate through each list of ranked documents\n    for docs in results:\n        # Iterate through each document in the list,\n        # with its rank (position in the list)\n        for rank, doc in enumerate(docs):\n            # Use the document contents as the key for uniqueness\n            doc_str = doc.page_content\n            # If the document hasn't been seen yet,\n            # - initialize score to 0\n            # - save it for later\n            if doc_str not in fused_scores:\n                fused_scores[doc_str] = 0\n                documents[doc_str] = doc\n            # Update the score of the document using the RRF formula:\n            # 1 / (rank + k)\n            fused_scores[doc_str] += 1 / (rank + k)\n\n    # Sort the documents based on their fused scores in descending order \n    # to get the final reranked results\n    reranked_doc_strs = sorted(\n        fused_scores, key=lambda d: fused_scores[d], reverse=True\n    )\n    # retrieve the corresponding doc for each doc_str\n    return [\n        documents[doc_str]\n        for doc_str in reranked_doc_strs\n    ]\n\nretrieval_chain = generate_queries | retriever.batch | reciprocal_rank_fusion\n```", "```py\nfunction reciprocalRankFusion(results, k = 60) {\n  // Initialize a dictionary to hold fused scores for each document\n  // Documents will be keyed by their contents to ensure uniqueness\n  const fusedScores = {}\n  const documents = {}\n\n  results.forEach(docs => {\n    docs.forEach((doc, rank) => {\n      // Use the document contents as the key for uniqueness\n      const key = doc.pageContent\n      // If the document hasn't been seen yet,\n      // - initialize score to 0\n      // - save it for later\n      if (!(key in fusedScores)) {\n        fusedScores[key] = 0\n        documents[key] = 0\n      }\n      // Update the score of the document using the RRF formula:\n      // 1 / (rank + k)\n      fusedScores[key] += 1 / (rank + k)\n    })\n  })\n\n  // Sort the documents based on their fused scores in descending order \n  // to get the final reranked results\n  const sorted = Object.entries(fusedScores).sort((a, b) => b[1] - a[1])\n  // retrieve the corresponding doc for each key\n  return sorted.map(([key]) => documents[key])\n}\n\nconst retrievalChain = queryGen\n  .pipe(retriever.batch.bind(retriever))\n  .pipe(reciprocalRankFusion)\n```", "```py\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based \n on this context:\n\n{context}\n\nQuestion: {question}\n\"\"\")\n\nllm = ChatOpenAI(temperature=0)\n\n@chain\ndef multi_query_qa(input):\n    # fetch relevant documents \n    docs = retrieval_chain.invoke(input)\n    # format prompt\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n    # generate answer\n    answer = llm.invoke(formatted)\n    return answer\n\nmulti_query_qa.invoke(\"\"\"Who are some key figures in the ancient greek history \n of philosophy?\"\"\")\n```", "```py\nconst rewritePrompt = ChatPromptTemplate.fromTemplate(`Answer the following \n question based on this context:\n\n{context}\n\nQuestion: {question}\n`)\n\nconst llm = new ChatOpenAI({temperature: 0})\n\nconst multiQueryQa = RunnableLambda.from(async input => {\n  // fetch relevant documents\n  const docs = await retrievalChain.invoke(input)\n  // format prompt\n  const formatted = await prompt.invoke({context: docs, question: input})\n  // generate answer\n  const answer = await llm.invoke(formatted)\n  return answer\n})\n\nawait multiQueryQa.invoke(`Who are some key figures in the ancient greek \n history of philosophy?`)\n```", "```py\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\nprompt_hyde = ChatPromptTemplate.from_template(\"\"\"Please write a passage to \n answer the question.\\n Question: {question} \\n Passage:\"\"\")\n\ngenerate_doc = (\n    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser() \n)\n```", "```py\nimport {ChatOpenAI} from '@langchain/openai'\nimport {ChatPromptTemplate} from '@langchain/core/prompts'\nimport {RunnableLambda} from '@langchain/core/runnables';\n\nconst prompt = ChatPromptTemplate.fromTemplate(`Please write a passage to \n answer the question\nQuestion: {question}\nPassage:`)\n\nconst llm = new ChatOpenAI({temperature: 0})\n\nconst generateDoc = prompt.pipe(llm).pipe(msg => msg.content)\n```", "```py\nretrieval_chain `=` generate_doc `|` retriever \n```", "```py\nconst retrievalChain = generateDoc.pipe(retriever)\n```", "```py\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based \n on this context:\n\n{context}\n\nQuestion: {question}\n\"\"\")\n\nllm = ChatOpenAI(temperature=0)\n\n@chain\ndef qa(input):\n  # fetch relevant documents from the hyde retrieval chain defined earlier\n  docs = retrieval_chain.invoke(input)\n  # format prompt\n  formatted = prompt.invoke({\"context\": docs, \"question\": input})\n  # generate answer\n  answer = llm.invoke(formatted)\n  return answer\n\nqa.invoke(\"\"\"Who are some key figures in the ancient greek history of \n philosophy?\"\"\")\n```", "```py\nconst prompt = ChatPromptTemplate.fromTemplate(`Answer the following \n question based on this context:\n\n{context}\n\nQuestion: {question}\n`)\n\nconst llm = new ChatOpenAI({temperature: 0})\n\nconst qa = RunnableLambda.from(async input => {\n  // fetch relevant documents from the hyde retrieval chain defined earlier\n  const docs = await retrievalChain.invoke(input)\n  // format prompt\n  const formatted = await prompt.invoke({context: docs, question: input})\n  // generate answer\n  const answer = await llm.invoke(formatted)\n  return answer\n})\n\nawait qa.invoke(`Who are some key figures in the ancient greek history of \n philosophy?`)\n```", "```py\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"python_docs\", \"js_docs\"] = Field(\n        ...,\n        description=\"\"\"Given a user question, choose which datasource would be \n most relevant for answering their question\"\"\",\n    )\n\n# LLM with function call \nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\nstructured_llm = llm.with_structured_output(RouteQuery)\n\n# Prompt \nsystem = \"\"\"You are an expert at routing a user question to the appropriate data \n source.\n\nBased on the programming language the question is referring to, route it to the \n relevant data source.\"\"\"\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\n# Define router \nrouter `=` prompt `|` structured_llm\n```", "```py\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { z } from \"zod\";\n\nconst routeQuery = z.object({\n  datasource: z.enum([\"python_docs\", \"js_docs\"]).describe(`Given a user \n question, choose which datasource would be most relevant for answering \n their question`),\n}).describe(\"Route a user query to the most relevant datasource.\")\n\nconst llm = new ChatOpenAI({model: \"gpt-3.5-turbo\", temperature: 0})\nconst structuredLlm = llm.withStructuredOutput(routeQuery, {name: \"RouteQuery\"})\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  ['system', `You are an expert at routing a user question to the appropriate \n data source.\n\nBased on the programming language the question is referring to, route it to \n the relevant data source.`],\n  ['human', '{question}']\n])\n\nconst router = prompt.pipe(structuredLlm)\n```", "```py\nquestion = \"\"\"Why doesn't the following code work:\n\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\nprompt.invoke(\"french\")\n\"\"\"\n\nresult = router.invoke({\"question\": question})\n\nresult.datasource\n# \"python_docs\"\n```", "```py\nconst question = `Why doesn't the following code work:\n\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\nprompt.invoke(\"french\")\n`\n\nawait router.invoke({ question })\n```", "```py\n{\n    datasource: \"python_docs\"\n}\n```", "```py\ndef choose_route(result):\n    if \"python_docs\" in result.datasource.lower():\n        ### Logic here \n        return \"chain for python_docs\"\n    else:\n        ### Logic here \n        return \"chain for js_docs\"\n\nfull_chain = router | RunnableLambda(choose_route)\n```", "```py\nfunction chooseRoute(result) {\n  if (result.datasource.toLowerCase().includes('python_docs')) {\n    return 'chain for python_docs';\n  } else {\n    return 'chain for js_docs';\n  }\n} \n\nconst fullChain = router.pipe(chooseRoute) \n```", "```py\nfrom langchain.utils.math import cosine_similarity\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import chain\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# Two prompts\nphysics_template = \"\"\"You are a very smart physics professor. You are great at \n answering questions about physics in a concise and easy-to-understand manner. \n When you don't know the answer to a question, you admit that you don't know.\n\nHere is a question:\n{query}\"\"\"\n\nmath_template = \"\"\"You are a very good mathematician. You are great at answering \n math questions. You are so good because you are able to break down hard \n problems into their component parts, answer the component parts, and then \n put them together to answer the broader question.\n\nHere is a question:\n{query}\"\"\"\n\n# Embed prompts\nembeddings = OpenAIEmbeddings()\nprompt_templates = [physics_template, math_template]\nprompt_embeddings = embeddings.embed_documents(prompt_templates)\n\n# Route question to prompt\n@chain\ndef prompt_router(query):\n    # Embed question\n    query_embedding = embeddings.embed_query(query)\n    # Compute similarity\n    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n    # Pick the prompt most similar to the input question\n    most_similar = prompt_templates[similarity.argmax()]\n    return PromptTemplate.from_template(most_similar)\n\nsemantic_router = (\n    prompt_router\n    | ChatOpenAI()\n    | StrOutputParser()\n)\n\nprint(semantic_router.invoke(\"What's a black hole\"))\n```", "```py\nimport {cosineSimilarity} from '@langchain/core/utils/math'\nimport {ChatOpenAI, OpenAIEmbeddings} from '@langchain/openai'\nimport {PromptTemplate} from '@langchain/core/prompts'\nimport {RunnableLambda} from '@langchain/core/runnables';\n\nconst physicsTemplate = `You are a very smart physics professor. You are great \n at answering questions about physics in a concise and easy-to-understand \n manner. When you don't know the answer to a question, you admit that you \n don't know.\n\nHere is a question:\n{query}`\n\nconst mathTemplate = `You are a very good mathematician. You are great at \n answering math questions. You are so good because you are able to break down \n hard problems into their component parts, answer the component parts, and \n then put them together to answer the broader question.\n\nHere is a question:\n{query}`\n\nconst embeddings = new OpenAIEmbeddings()\n\nconst promptTemplates = [physicsTemplate, mathTemplate]\nconst promptEmbeddings = await embeddings.embedDocuments(promptTemplates)\n\nconst promptRouter = RunnableLambda.from(query => {\n  // Embed question\n  const queryEmbedding = await embeddings.embedQuery(query)\n  // Compute similarity\n  const similarities = cosineSimilarity([queryEmbedding], promptEmbeddings)[0]\n  // Pick the prompt most similar to the input question\n  const mostSimilar = similarities[0] > similarities[1] \n    ? promptTemplates[0] \n    : promptTemplates[1]\n  return PromptTemplate.fromTemplate(mostSimilar)\n})\n\nconst semanticRouter = promptRouter.pipe(new ChatOpenAI())\n\nawait semanticRouter.invoke(\"What's a black hole\")\n```", "```py\nfrom langchain.chains.query_constructor.base import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_openai import ChatOpenAI\n\nfields = [\n    AttributeInfo(\n        name=\"genre\",\n        description=\"The genre of the movie\",\n        type=\"string or list[string]\",\n    ),\n    AttributeInfo(\n        name=\"year\",\n        description=\"The year the movie was released\",\n        type=\"integer\",\n    ),\n    AttributeInfo(\n        name=\"director\",\n        description=\"The name of the movie director\",\n        type=\"string\",\n    ),\n    AttributeInfo(\n        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n    ),\n]\ndescription = \"Brief summary of a movie\"\n\nllm = ChatOpenAI(temperature=0)\n\nretriever = SelfQueryRetriever.from_llm(\n    llm, db, description, fields,\n)\n\nprint(retriever.invoke(\n    \"What's a highly rated (above 8.5) science fiction film?\"))\n```", "```py\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { FunctionalTranslator } from \"@langchain/core/structured_query\";\n\n/**\n * First, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, \n * rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst fields = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\nconst description = \"Brief summary of a movie\";\n\nconst llm = new ChatOpenAI();\nconst attributeInfos = fields.map((field) => new AttributeInfo(field.name,  \n  field.description, field.type));\n\nconst selfQueryRetriever = SelfQueryRetriever.fromLLM({\n  llm,\n  db,\n  description,\n  attributeInfo: attributeInfos,\n  /**\n * We need to use a translator that translates the queries into a\n * filter format that the vector store can understand. LangChain provides one \n * here.\n */\n  structuredQueryTranslator: new FunctionalTranslator(),\n});\n\nawait selfQueryRetriever.invoke(\n  \"What's a highly rated (above 8.5) science fiction film?\"\n);\n```", "```py\nfrom langchain_community.tools import QuerySQLDatabaseTool\nfrom langchain_community.utilities import SQLDatabase\nfrom langchain.chains import create_sql_query_chain\nfrom langchain_openai import ChatOpenAI\n\n# replace this with the connection details of your db\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n\n# convert question to sql query\nwrite_query = create_sql_query_chain(llm, db)\n\n# Execute SQL query\nexecute_query = QuerySQLDatabaseTool(db=db)\n\n# combined\nchain = write_query | execute_query\n\n# invoke the chain\nchain.invoke('How many employees are there?');\n```", "```py\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createSqlQueryChain } from \"langchain/chains/sql_db\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { DataSource } from \"typeorm\";\nimport { QuerySqlTool } from \"langchain/tools/sql\";\n\nconst datasource = new DataSource({\n  type: \"sqlite\",\n  database: \"./Chinook.db\", // here should be the details of your database\n});\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\nconst llm = new ChatOpenAI({ model: \"gpt-4\", temperature: 0 });\n\n// convert question to sql query\nconst writeQuery = await createSqlQueryChain({ llm, db, dialect: \"sqlite\" });\n\n// execute query\nconst executeQuery = new QuerySqlTool(db);\n\n// combined\nconst chain = writeQuery.pipe(executeQuery);\n\n// invoke the chain\nawait chain.invoke('How many employees are there?');\n```"]