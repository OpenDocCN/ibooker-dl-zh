- en: The mathematical building blocks of neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的数学基础
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter02_mathematical-building-blocks](https://deeplearningwithpython.io/chapters/chapter02_mathematical-building-blocks)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter02_mathematical-building-blocks](https://deeplearningwithpython.io/chapters/chapter02_mathematical-building-blocks)
- en: 'Understanding deep learning requires familiarity with many simple mathematical
    concepts: *tensors*, *tensor operations*, *differentiation*, *gradient descent*,
    and so on. Our goal in this chapter will be to build up your intuition about these
    notions without getting overly technical. In particular, we’ll steer away from
    mathematical notation, which can introduce unnecessary barriers for those without
    any mathematics background, and isn’t necessary to explain things well. The most
    precise, unambiguous description of a mathematical operation is its executable
    code.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 理解深度学习需要熟悉许多简单的数学概念：*张量*、*张量运算*、*微分*、*梯度下降*等等。我们本章的目标将是建立你对这些概念的理解，而不需要过于技术化。特别是，我们将避免使用数学符号，这可能会为没有数学背景的人设置不必要的障碍，而且对于解释事物来说也不是必要的。数学运算最精确、最明确描述是其可执行代码。
- en: To provide sufficient context for introducing tensors and gradient descent,
    we’ll begin the chapter with a practical example of a neural network. Then we’ll
    go over every new concept that’s been introduced, point by point. Keep in mind
    that these concepts will be essential for you to understand the practical examples
    that will come in the following chapters!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为介绍张量和梯度下降提供足够的背景，我们将以一个神经网络的实用示例开始本章。然后我们将逐一点评介绍的新概念。记住，这些概念对于你理解接下来章节中的实际示例是至关重要的！
- en: After reading this chapter, you’ll have an intuitive understanding of the mathematical
    theory behind deep learning, and you’ll be ready to start diving into modern deep
    learning frameworks, in chapter 3.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读完本章后，你将对深度学习背后的数学理论有一个直观的理解，并且你将准备好开始深入研究第3章中的现代深度学习框架。
- en: A first look at a neural network
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 首次了解神经网络
- en: Let’s look at a concrete example of a neural network that uses the machine learning
    library *Keras* to learn to classify handwritten digits. We will use Keras extensively
    throughout this book. It’s a simple, high-level library that will allow us to
    stay focused on the concepts we would like to cover.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体的例子，这个神经网络使用机器学习库 *Keras* 来学习对手写数字进行分类。我们将在整本书中广泛使用 Keras。这是一个简单、高级的库，将使我们能够专注于我们想要覆盖的概念。
- en: Unless you already have experience with Keras or similar libraries, you won’t
    understand everything about this first example right away. That’s fine. In a few
    sections, we’ll review each element in the example and explain it in detail. So
    don’t worry if some steps seem arbitrary or look like magic to you! We’ve got
    to start somewhere.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你已经具备使用 Keras 或类似库的经验，否则你不会立即理解这个第一个示例的所有内容。这没关系。在接下来的几个部分中，我们将回顾示例中的每个元素，并对其进行详细解释。所以，如果某些步骤看起来很随意或者对你来说像是魔术，请不要担心！我们必须从某个地方开始。
- en: The problem we’re trying to solve here is to classify grayscale images of handwritten
    digits (28 × 28 pixels) into their 10 categories (0 through 9). We’ll use the
    MNIST dataset, a classic in the machine learning community, which has been around
    almost as long as the field itself and has been intensively studied. It’s a set
    of 60,000 training images, plus 10,000 test images, assembled by the National
    Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can
    think of “solving” MNIST as the “Hello World” of deep learning — it’s what you
    do to verify that your algorithms are working as expected. As you become a machine
    learning practitioner, you’ll see MNIST come up over and over again, in scientific
    papers, blog posts, and so on. You can see some MNIST samples in figure 2.1.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图解决的问题是将灰度图像的手写数字（28 × 28 像素）分类到其10个类别（0到9）中。我们将使用 MNIST 数据集，这是机器学习社区中的经典数据集，几乎与该领域一样历史悠久，并且已经被深入研究。它是由美国国家标准与技术研究院（NIST）在20世纪80年代汇编的一套60,000张训练图像和10,000张测试图像。你可以把“解决”MNIST看作是深度学习的“Hello
    World”——这是你用来验证你的算法是否按预期工作的方法。随着你成为机器学习从业者，你会在科学论文、博客文章等地方反复看到MNIST。你可以在图2.1中看到一些MNIST样本。
- en: '![](../Images/b4c5f453ddbc3487d5e5722a8c5d8a57.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4c5f453ddbc3487d5e5722a8c5d8a57.png)'
- en: '[Figure 2.1](#figure-2-1): MNIST sample digits'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.1](#figure-2-1)：MNIST样本数字'
- en: The MNIST dataset comes preloaded in Keras, in the form of a set of four NumPy
    arrays.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集在Keras中预先加载，以一组四个NumPy数组的形式存在。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 2.1](#listing-2-1): Loading the MNIST dataset in Keras'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表2.1](#listing-2-1)：在Keras中加载MNIST数据集'
- en: '`train_images` and `train_labels` form the training set, the data that the
    model will learn from. The model will then be tested on the test set, `test_images`
    and `test_labels`. The images are encoded as NumPy arrays, and the labels are
    an array of digits, ranging from 0 to 9\. The images and labels have a one-to-one
    correspondence.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_images`和`train_labels`构成了训练集，即模型将从中学习的数据。然后，模型将在测试集`test_images`和`test_labels`上进行测试。图像被编码为NumPy数组，标签是一个从0到9的数字数组。图像和标签有一一对应的关系。'
- en: 'Let’s look at the training data:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看训练数据：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And here’s the test data:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是测试数据：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The workflow will be as follows. First, we’ll feed the neural network the training
    data, `train_images` and `train_labels`. The network will then learn to associate
    images and labels. Finally, we’ll ask the network to produce predictions for `test_images`,
    and we’ll verify whether these predictions match the labels from `test_labels`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程将如下进行。首先，我们将训练数据`train_images`和`train_labels`输入神经网络。然后，网络将学会将图像和标签关联起来。最后，我们将要求网络对`test_images`生成预测，并验证这些预测是否与`test_labels`中的标签匹配。
- en: Let’s build the network — again, remember that you aren’t expected to understand
    everything about this example yet.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建网络——再次提醒，你不需要现在就理解这个例子中的所有内容。
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 2.2](#listing-2-2): The network architecture'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表2.2](#listing-2-2)：网络架构'
- en: 'The core building block of neural networks is the *layer*. You can think of
    a layer as a filter for data: some data goes in, and it comes out in a more useful
    form. Specifically, layers extract *representations* out of the data fed into
    them — hopefully, representations that are more meaningful for the problem at
    hand. Most of deep learning consists of chaining together simple layers that will
    implement a form of progressive *data distillation*. A deep learning model is
    like a sieve for data processing, made of a succession of increasingly refined
    data filters — the layers.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的核心构建块是*层*。你可以将层视为数据的过滤器：一些数据进入，以更有用的形式输出。具体来说，层从输入的数据中提取*表示*——希望这些表示对于当前问题更有意义。大多数深度学习都是由将简单的层链接起来以实现一种形式的*数据蒸馏*。深度学习模型就像数据处理的筛子，由一系列越来越精细的数据过滤器——即层组成。
- en: Here, our model consists of a sequence of two `Dense` layers, which are densely
    connected (also called *fully connected*) neural layers. The second (and last)
    layer is a 10-way *`softmax` classification* layer, which means it will return
    an array of 10 probability scores (summing to 1). Each score will be the probability
    that the current digit image belongs to one of our 10 digit classes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的模型由两个`Dense`层组成，这些层是密集连接的（也称为*全连接*）神经网络层。第二个（也是最后一个）层是一个10路*`softmax`分类*层，这意味着它将返回一个包含10个概率分数的数组（总和为1）。每个分数将是当前数字图像属于我们10个数字类别之一的概率。
- en: 'To make the model ready for training, we need to pick three more things, as
    part of the *compilation* step:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型准备好训练，我们需要在*编译*步骤中挑选三个更多的事物：
- en: '*A loss function* — How the model will be able to measure its performance on
    the training data and thus how it will be able to steer itself in the right direction.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失函数*——模型将如何衡量其在训练数据上的性能，从而如何能够引导自己走向正确的方向。'
- en: '*An optimizer* — The mechanism through which the model will update itself based
    on the training data it sees, to improve its performance.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器* — 模型将根据其看到的训练数据更新自己的机制，以改善其性能。'
- en: '*Metrics to monitor during training and testing* — Here, we’ll only care about
    accuracy (the fraction of the images that were correctly classified).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在训练和测试期间要监控的指标* — 在这里，我们只关心准确率（正确分类的图像的比例）。'
- en: The exact purpose of the loss function and the optimizer will be made clear
    throughout the next two chapters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数和优化器的确切目的将在接下来的两章中变得清晰。
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Listing 2.3](#listing-2-3): The compilation step'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表2.3](#listing-2-3)：编译步骤'
- en: Before training, we’ll *preprocess* the data by reshaping it into the shape
    the model expects and scaling it so that all values are in the `[0, 1]` interval.
    Previously, our training images were stored in an array of shape `(60000, 28,
    28)` of type `uint8` with values in the `[0, 255]` interval. We transform it into
    a `float32` array of shape `(60000, 28 * 28)` with values between `0` and `1`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，我们将通过将其重塑成模型期望的形状并缩放到所有值都在 `[0, 1]` 区间内来*预处理*数据。之前，我们的训练图像存储在一个形状为 `(60000,
    28, 28)`、类型为 `uint8` 且值在 `[0, 255]` 区间内的数组中。我们将其转换为一个形状为 `(60000, 28 * 28)`、值在
    `0` 到 `1` 之间的 `float32` 数组。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Listing 2.4](#listing-2-4): Preparing the image data'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 2.4](#listing-2-4)：准备图像数据'
- en: We’re now ready to train the model, which in Keras is done via a call to the
    model’s `fit()` method — we *fit* the model to its training data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好训练模型了，在Keras中，这是通过调用模型的 `fit()` 方法来完成的——我们将模型*拟合*到其训练数据。
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Listing 2.5](#listing-2-5): “Fitting” the model'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 2.5](#listing-2-5)：“拟合”模型'
- en: 'Two quantities are displayed during training: the loss of the model over the
    training data and the accuracy of the model over the training data. We quickly
    reach an accuracy of 0.989 (98.9%) on the training data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中显示了两个量：模型在训练数据上的损失和模型在训练数据上的准确率。我们很快就在训练数据上达到了98.9%（0.989）的准确率。
- en: Now that we have a trained model, we can use it to predict class probabilities
    for *new* digits — images that weren’t part of the training data, like those from
    the test set.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了一个模型，我们可以用它来预测新数字的类别概率——那些不是训练数据一部分的图像，比如测试集中的图像。
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 2.6](#listing-2-6): Using the model to make predictions'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 2.6](#listing-2-6)：使用模型进行预测'
- en: Each number of index `i` in that array corresponds to the probability that digit
    image `test_digits[0]` belong to class `i`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 那个数组中索引 `i` 的每个数字都对应于数字图像 `test_digits[0]` 属于类别 `i` 的概率。
- en: 'This first test digit has the highest probability score (0.99999106, almost
    1) at index 7, so according to our model, it must be a 7:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第一个测试数字在索引7处具有最高的概率分数（0.99999106，几乎为1），因此根据我们的模型，它必须是一个7：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can check that the test label agrees:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查测试标签是否一致：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: On average, how good is our model at classifying such never-before-seen digits?
    Let’s check by computing average accuracy over the entire test set.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 平均来说，我们的模型在分类这些从未见过的数字方面有多好？让我们通过在整个测试集上计算平均准确率来检查。
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Listing 2.7](#listing-2-7): Evaluating the model on new data'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 2.7](#listing-2-7)：在新数据上评估模型'
- en: 'The test set accuracy turns out to be 97.8% — that’s almost double the error
    rate of the training set (at 98.9% accuracy). This gap between training accuracy
    and test accuracy is an example of *overfitting*: the fact that machine learning
    models tend to perform worse on new data than on their training data. Overfitting
    is a central topic in chapter 5.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集的准确率结果是 97.8%——这几乎是训练集错误率（98.9% 准确率）的两倍。训练准确率和测试准确率之间的差距是*过拟合*的一个例子：机器学习模型往往在新数据上的表现不如在训练数据上。过拟合是第5章的核心主题。
- en: This concludes our first example. You just saw how you can build and train a
    neural network to classify handwritten digits in less than 15 lines of Python
    code. In this chapter and the next, we’ll go into detail about every moving piece
    we just previewed and clarify what’s going on behind the scenes. You’ll learn
    about tensors, the data-storing objects going into the model; tensor operations,
    which layers are made of; and gradient descent, which allows your model to learn
    from its training examples.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们的第一个例子。您刚刚看到了如何用不到15行Python代码构建和训练一个神经网络来分类手写数字。在本章和下一章中，我们将详细介绍我们刚刚预览的每个移动部件，并阐明幕后发生的事情。您将了解张量，即进入模型的数据存储对象；张量操作，即哪些层由它们组成；以及梯度下降，它允许您的模型从其训练示例中学习。
- en: Data representations for neural networks
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的表示
- en: In the previous example, we started from data stored in multidimensional NumPy
    arrays, also called *tensors*. In general, all current machine learning systems
    use tensors as their basic data structure. Tensors are fundamental to the field
    — so fundamental that the TensorFlow framework was named after them. So what’s
    a tensor?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个例子中，我们是从存储在多维NumPy数组中的数据开始的，这些数组也被称为*tensors*。一般来说，所有当前的机器学习系统都使用张量作为它们的基本数据结构。张量是该领域的基石——基础到以至于TensorFlow框架就是以它们的名称命名的。那么，什么是张量呢？
- en: 'At its core, a tensor is a container for data — usually numerical data. So
    it’s a container for numbers. You may already be familiar with matrices, which
    are rank-2 tensors: tensors are a generalization of matrices to an arbitrary number
    of dimensions (note that in the context of tensors, a dimension is often called
    an *axis*).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，张量是数据的容器 — 通常是指数值数据。因此，它是一个数字的容器。您可能已经熟悉矩阵，它们是2秩张量：张量是矩阵向任意维数的推广（请注意，在张量的上下文中，维度通常被称为*轴*）。
- en: Going over the details of tensors might seem a bit abstract at first. But it’s
    well worth it — manipulating tensors will be the bread and butter of any machine
    learning code you ever write.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最初了解张量的细节可能有点抽象。但这是值得的 — 操作张量将是您所写的任何机器学习代码的基础。
- en: Scalars (rank-0 tensors)
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标量（0秩张量）
- en: 'A tensor that contains only one number is called a *scalar* (or scalar tensor,
    rank-0 tensor, or 0D tensor). In NumPy, a `float32` or `float64` number is a scalar
    tensor (or scalar array). You can display the number of axes of a NumPy tensor
    via the `ndim` attribute; a scalar tensor has 0 axes (`ndim == 0`). The number
    of axes of a tensor is also called its *rank*. Here’s a NumPy scalar:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 只包含一个数的张量被称为*标量*（或标量张量、0阶张量或0D张量）。在NumPy中，一个`float32`或`float64`数字是一个标量张量（或标量数组）。您可以通过`ndim`属性显示NumPy张量的轴数；标量张量有0个轴（`ndim
    == 0`）。张量的轴数也称为其*秩*。以下是一个NumPy标量：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Vectors (rank-1 tensors)
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量（1秩张量）
- en: 'An array of numbers is called a vector (or rank-1 tensor or 1D tensor). A rank-1
    tensor has exactly one axis. The following is a NumPy vector:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数字数组被称为向量（或1秩张量或1D张量）。1秩张量恰好有一个轴。以下是一个NumPy向量：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This vector has five entries and so is called a *5-dimensional vector*. Don’t
    confuse a 5D vector with a 5D tensor! A 5D vector has only one axis and has five
    dimensions along its axis, whereas a 5D tensor has five axes (and may have any
    number of dimensions along each axis). *Dimensionality* can denote either the
    number of entries along a specific axis (as in the case of our 5D vector) or the
    number of axes in a tensor (such as a 5D tensor), which can be confusing at times.
    In the latter case, it’s technically more correct to talk about a *tensor of rank
    5* (the rank of a tensor being the number of axes), but the ambiguous notation
    *5D tensor* is common regardless.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量有五个条目，因此被称为*5维向量*。不要将5维向量与5维张量混淆！5维向量只有一个轴，其轴上有五个维度，而5维张量有五个轴（并且每个轴上可能有任意数量的维度）。*维度*可以表示特定轴上的条目数（如我们的5维向量的情况）或张量中的轴数（如5维张量），有时可能会造成混淆。在后一种情况下，从技术上讲，更正确地谈论一个*秩为5的张量*（张量的秩是轴的数量），但模糊的符号*5D张量*仍然很常见。
- en: Matrices (rank-2 tensors)
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵（2秩张量）
- en: 'An array of vectors is a *matrix* (or rank-2 tensor or 2D tensor). A matrix
    has two axes (often referred to as *rows* and *columns*). You can visually interpret
    a matrix as a rectangular grid of numbers. This is a NumPy matrix:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数组是一个*矩阵*（或2秩张量或2D张量）。矩阵有两个轴（通常被称为*行*和*列*）。您可以将矩阵可视化为数字的矩形网格。这是一个NumPy矩阵：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The entries from the first axis are called the *rows*, and the entries from
    the second axis are called the *columns*. In the previous example, `[5, 78, 2,
    34, 0]` is the first row of `x`, and `[5, 6, 7]` is the first column.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第一轴的条目称为*行*，第二轴的条目称为*列*。在上一个例子中，`[5, 78, 2, 34, 0]`是`x`的第一行，而`[5, 6, 7]`是第一列。
- en: Rank-3 tensors and higher-rank tensors
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3秩张量和更高秩的张量
- en: 'If you pack such matrices in a new array, you obtain a rank-3 tensor (or 3D
    tensor), which you can visually interpret as a cube of numbers. The following
    is a NumPy rank-3 tensor:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将这些矩阵打包到一个新的数组中，您将获得一个3秩张量（或3D张量），您可以将其可视化为数字的立方体。以下是一个NumPy 3秩张量：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: By packing rank-3 tensors in an array, you can create a rank-4 tensor, and so
    on. In deep learning, you’ll generally manipulate tensors with ranks 0 to 4, although
    you may go up to 5 if you process video data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将3秩张量打包到数组中，您可以创建一个4秩张量，依此类推。在深度学习中，您通常操作秩为0到4的张量，尽管如果您处理视频数据，您可能达到5。
- en: Key attributes
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键属性
- en: 'A tensor is defined by three key attributes:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 张量由三个关键属性定义：
- en: '*Number of axes (rank)* — For instance, a rank-3 tensor has three axes, and
    a matrix has two axes. This is also called the tensor’s `ndim` in Python libraries
    such as NumPy, JAX, TensorFlow, and PyTorch.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*轴数（秩）* — 例如，一个3秩张量有三个轴，而矩阵有两个轴。这也被称为Python库（如NumPy、JAX、TensorFlow和PyTorch）中的张量的`ndim`。'
- en: '*Shape* — This is a tuple of integers that describes how many dimensions the
    tensor has along each axis. For instance, the previous matrix example has shape
    `(3, 5)`, and the rank-3 tensor example has shape `(3, 3, 5)`. A vector has a
    shape with a single element, such as `(5,)`, whereas a scalar has an empty shape,
    `()`.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*形状* — 这是一个整数元组，描述了张量在每个轴上的维度数。例如，前面的矩阵示例的形状是`(3, 5)`，而三阶张量示例的形状是`(3, 3, 5)`。一个向量有一个元素的形状，例如`(5,)`，而一个标量有一个空的形状，`()`。'
- en: '*Data type (usually called `dtype` in Python libraries)* — This is the type
    of the data contained in the tensor; for instance, a tensor’s type could be `float16`,
    `float32`, `float64`, `uint8`, `bool`, and so on. In TensorFlow, you are also
    likely to come across `string` tensors.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据类型（在Python库中通常称为`dtype`）* — 这是张量中包含的数据类型；例如，张量的类型可以是`float16`、`float32`、`float64`、`uint8`、`bool`等等。在TensorFlow中，你也可能会遇到`string`类型的张量。'
- en: 'To make this more concrete, let’s look back at the data we processed in the
    MNIST example. First, we load the MNIST dataset:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点更加具体，让我们回顾一下MNIST示例中处理的数据。首先，我们加载MNIST数据集：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we display the number of axes of the tensor `train_images`, the `ndim`
    attribute:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们显示张量`train_images`的轴数，`ndim`属性：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here’s its shape:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的形状：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And this is its data type, the `dtype` attribute:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它的数据类型，`dtype`属性：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: So what we have here is a rank-3 tensor of 8-bit integers. More precisely, it’s
    an array of 60,000 matrices of 28 × 28 integers. Each such matrix is a grayscale
    image, with coefficients between 0 and 255.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们这里有一个8位整数的三阶张量。更确切地说，它是一个包含60,000个28 × 28整数的矩阵数组。每个这样的矩阵是一个灰度图像，其系数在0到255之间。
- en: Let’s display the fourth digit in this rank-3 tensor, using the library Matplotlib
    (part of the standard scientific Python suite); see figure 2.2.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Matplotlib库（标准科学Python套件的一部分）显示这个三阶张量中的第四个数字；参见图2.2。
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Listing 2.8](#listing-2-8): Displaying the fourth digit'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表2.8](#listing-2-8)：显示第四位数字'
- en: '![](../Images/a8c8edb965fd31d17812646e506c4e03.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8c8edb965fd31d17812646e506c4e03.png)'
- en: '[Figure 2.2](#figure-2-2): The fourth sample in our dataset'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.2](#figure-2-2)：数据集的第四个样本'
- en: 'Naturally, the corresponding label is just the integer 9:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，相应的标签只是整数9：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Manipulating tensors in NumPy
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在NumPy中操作张量
- en: In the previous example, we selected a specific digit alongside the first axis
    using the syntax `train_images[i]`. Selecting specific elements in a tensor is
    called *tensor slicing*. Let’s look at the tensor-slicing operations you can do
    on NumPy arrays.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们使用语法`train_images[i]`在第一个轴上选择了一个特定的数字。在张量中选择特定元素称为*张量切片*。让我们看看可以在NumPy数组上执行的张量切片操作。
- en: 'The following example selects digits #10 to #100 (#100 isn’t included) and
    puts them in an array of shape `(90, 28, 28)`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例选择了数字#10到#100（不包括#100）并将它们放入形状为`(90, 28, 28)`的数组中：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It’s equivalent to this more detailed notation, which specifies a start index
    and stop index for the slice along each tensor axis. Note that `:` is equivalent
    to selecting the entire axis:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这与以下更详细的表示法等价，它为每个张量轴上的切片指定了起始索引和停止索引。注意，`:`等价于选择整个轴：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In general, you may select slices between any two indices along each tensor
    axis. For instance, to select 14 × 14 pixels in the bottom-right corner of all
    images, you would do this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您可以在每个张量轴上的任意两个索引之间选择切片。例如，要选择所有图像右下角的14 × 14像素，您将这样做：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It’s also possible to use negative indices. Much like negative indices in Python
    lists, they indicate a position relative to the end of the current axis. To crop
    the images to patches of 14 × 14 pixels centered in the middle, do this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用负索引。与Python列表中的负索引类似，它们表示相对于当前轴末尾的位置。为了将图像裁剪为中间14 × 14像素的补丁，这样做：
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The notion of data batches
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据批次的观念
- en: In general, the first axis (axis 0, because indexing starts at 0) in all data
    tensors you’ll come across in deep learning will be the *samples axis*. In the
    MNIST example, “samples” are images of digits.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在深度学习中遇到的所有数据张量的第一个轴（轴0，因为索引从0开始）将是*样本轴*。在MNIST示例中，“样本”是数字的图像。
- en: 'In addition, deep learning models don’t process an entire dataset at once;
    rather, they break the data into small “batches,” or groups of samples with a
    fixed size. Concretely, here’s one batch of our MNIST digits, with a batch size
    of 128:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习模型不会一次性处理整个数据集；相反，它们将数据分成小的“批次”，即具有固定大小的样本组。具体来说，这是我们MNIST数字的一个批次，批次大小为128：
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And here’s the next batch:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是下一个批次：
- en: '[PRE26]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'And the `n`th batch:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以及第`n`个批次：
- en: '[PRE27]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: When considering such a batch tensor, the first axis (axis 0) is called the
    *batch axis* (or *batch dimension*). You’ll frequently encounter this term when
    using Keras and other deep learning libraries.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑这样的批量张量时，第一个轴（轴0）被称为*批量轴*（或*批量维度*）。当使用Keras和其他深度学习库时，你经常会遇到这个术语。
- en: Real-world examples of data tensors
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据张量的现实世界示例
- en: 'Let’s make data tensors more concrete with a few examples similar to what you’ll
    encounter later. The data you’ll manipulate will almost always fall into one of
    the following categories:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些与您稍后将要遇到的类似示例来具体说明数据张量。您将要操作的数据几乎总是属于以下类别之一：
- en: '*Vector data* — Rank-2 tensors of shape `(samples, features)`, where each sample
    is a vector of numerical attributes (“features”)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*向量数据* — 形状为`(样本, 特征)`的二阶张量，其中每个样本是一个数值属性（“特征”）的向量'
- en: '*Timeseries data or sequence data* — Rank-3 tensors of shape `(samples, timesteps,
    features)`, where each sample is a sequence (of length `timesteps`) of feature
    vectors'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时序数据或序列数据* — 形状为`(样本, 时间步长, 特征)`的三阶张量，其中每个样本是一个特征向量的序列（长度为`时间步长`）'
- en: '*Images* — Rank-4 tensors of shape `(samples, height, width, channels)`, where
    each sample is a 2D grid of pixels, and each pixel is represented by a vector
    of values (“channels”)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像* — 形状为`(样本, 高度, 宽度, 通道)`的四阶张量，其中每个样本是一个二维像素网格，每个像素由一个值向量（“通道”）表示'
- en: '*Video* — Rank-5 tensors of shape `(samples, frames, height, width, channels)`,
    where each sample is a sequence (of length `frames`) of images'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*视频* — 形状为`(样本, 帧, 高度, 宽度, 通道)`的五阶张量，其中每个样本是一个长度为`帧`的图像序列'
- en: Vector data
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量数据
- en: Vector data is one of the most common cases. In such a dataset, each single
    data point can be encoded as a vector, and thus a batch of data will be encoded
    as a rank-2 tensor (that is, an array of vectors), where the first axis is the
    *samples axis* and the second axis is the *features axis*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据是最常见的案例之一。在这样的数据集中，每个单独的数据点都可以编码为一个向量，因此一批数据将被编码为一个二阶张量（即向量的数组），其中第一个轴是*样本轴*，第二个轴是*特征轴*。
- en: 'Let’s take a look at two examples:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看两个例子：
- en: An actuarial dataset of people, where we consider each person’s age, gender,
    and income. Each person can be characterized as a vector of three values, and
    thus an entire dataset of 100,000 people can be stored in a rank-2 tensor of shape
    `(100000, 3)`.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关于人群的精算数据集，其中我们考虑每个人的年龄、性别和收入。每个人可以表示为一个包含三个值的向量，因此整个包含100,000个人的数据集可以存储在一个形状为`(100000,
    3)`的二阶张量中。
- en: A dataset of text documents, where we represent each document by the counts
    of how many times each word appears in it (out of a dictionary of 20,000 common
    words). Each document can be encoded as a vector of 20,000 values (one count per
    word in the dictionary), and thus an entire dataset of 500 documents can be stored
    in a tensor of shape `(500, 20000)`.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个文本文档数据集，其中我们通过每个文档中每个单词出现的次数（从20,000个常用单词的字典中）来表示每个文档。每个文档可以编码为一个包含20,000个值（字典中每个单词的一个计数）的向量，因此整个包含500个文档的数据集可以存储在一个形状为`(500,
    20000)`的张量中。
- en: Timeseries data or sequence data
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 时序数据或序列数据
- en: Whenever time matters in your data (or the notion of sequence order), it makes
    sense to store it in a rank-3 tensor with an explicit time axis. Each sample can
    be encoded as a sequence of vectors (a rank-2 tensor), and thus a batch of data
    will be encoded as a rank-3 tensor (see figure 2.3).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当时间在你的数据（或序列顺序的概念）中很重要时，将数据存储在具有显式时间轴的三阶张量中是有意义的。每个样本可以编码为一个向量的序列（一个二阶张量），因此一批数据将被编码为一个三阶张量（参见图2.3）。
- en: '![](../Images/ba394dc8634a1794a9159b2860dd6754.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba394dc8634a1794a9159b2860dd6754.png)'
- en: '[Figure 2.3](#figure-2-3): A rank-3 timeseries data tensor'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.3](#figure-2-3)：一个三阶时序数据张量'
- en: 'The time axis is always the second axis (axis of index 1), by convention. Let’s
    look at a few examples:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 时间轴始终是第二个轴（索引为1的轴），这是惯例。让我们看看几个例子：
- en: '*A dataset of stock prices* — Every minute, we store the current price of the
    stock, the highest price in the past minute, and the lowest price in the past
    minute. Thus every minute is encoded as a 3D vector, an entire day of trading
    is encoded as a matrix of shape `(390, 3)` (there are 390 minutes in a trading
    day), and 250 days’ worth of data can be stored in a rank-3 tensor of shape `(250,
    390, 3)`. Here, each sample would be one day’s worth of data.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*股票价格数据集* — 每分钟，我们存储股票的当前价格、过去一分钟内的最高价格和最低价格。因此，每分钟被编码为一个三维向量，整个交易日的编码是一个形状为`(390,
    3)`的矩阵（交易日中有390分钟），250天的数据可以存储在一个形状为`(250, 390, 3)`的三阶张量中。在这里，每个样本代表一天的数据。'
- en: '*A dataset of tweets, where we encode each tweet as a sequence of 280 characters
    out of an alphabet of 128 unique characters* — In this setting, each character
    can be encoded as a binary vector of size 128 (an all-zeros vector except for
    a 1 entry at the index corresponding to the character). Then each tweet can be
    encoded as a rank-2 tensor of shape `(280, 128)`, and a dataset of 1 million tweets
    can be stored in a tensor of shape `(1000000, 280, 128)`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个包含推文的数据库，其中我们将每个推文编码为从128个唯一字符的字母表中选取的280个字符的序列*——在这个设置中，每个字符可以编码为一个大小为128的二进制向量（除了对应字符的索引处的1条记录之外，其余都是全零向量）。然后每个推文可以编码为一个形状为`(280,
    128)`的二阶张量，一个包含100万个推文的数据库可以存储在一个形状为`(1000000, 280, 128)`的张量中。'
- en: Image data
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像数据
- en: 'Images typically have three dimensions: height, width, and color depth. Although
    grayscale images (like our MNIST digits) have only a single color channel and
    could thus be stored in rank-2 tensors, by convention image tensors are always
    rank-3, with a one-dimensional color channel for grayscale images. A batch of
    128 grayscale images of size 256 × 256 could thus be stored in a tensor of shape
    `(128, 256, 256, 1)`, and a batch of 128 color images could be stored in a tensor
    of shape `(128, 256, 256, 3)` (see figure 2.4).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通常具有三个维度：高度、宽度和颜色深度。尽管灰度图像（如我们的MNIST数字）只有一个颜色通道，因此可以存储在二阶张量中，但按照惯例，图像张量总是三阶的，对于灰度图像有一个一维的颜色通道。因此，一个大小为256
    × 256的128个灰度图像批次的张量形状可以是`(128, 256, 256, 1)`，而一个128个彩色图像批次的张量形状可以是`(128, 256,
    256, 3)`（见图2.4）。
- en: '![](../Images/a1ad92e07f0728088785bcc8ad11d7d5.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1ad92e07f0728088785bcc8ad11d7d5.png)'
- en: '[Figure 2.4](#figure-2-4): A rank-4 image data tensor'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.4](#figure-2-4)：四阶图像数据张量'
- en: 'There are two conventions for the shapes of image tensors: the *channels-last*
    convention (which is standard in JAX and TensorFlow, as well as most other deep
    learning tools out there) and the *channels-first* convention (which is standard
    in PyTorch).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图像张量的形状有两种惯例：*通道最后*惯例（在JAX、TensorFlow以及大多数其他深度学习工具中是标准的）和*通道首先*惯例（在PyTorch中是标准的）。
- en: 'The channels-last convention places the color-depth axis at the end: `(samples,
    height, width, color_depth)`. Meanwhile, the channels-first convention places
    the color depth axis right after the batch axis: `(samples, color_depth, height,
    width)`. With the channels-first convention, the previous examples would become
    `(128, 1, 256, 256)` and `(128, 3, 256, 256)`. The Keras API provides support
    for both formats.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通道最后惯例将颜色深度轴放在末尾：`(样本, 高度, 宽度, 颜色深度)`。同时，通道首先惯例将颜色深度轴放在批次轴之后：`(样本, 颜色深度, 高度,
    宽度)`。使用通道首先惯例，前面的例子将变为`(128, 1, 256, 256)`和`(128, 3, 256, 256)`。Keras API提供了对这两种格式的支持。
- en: Video data
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 视频数据
- en: Video data is one of the few types of real-world data for which you’ll need
    rank-5 tensors. A video can be understood as a sequence of frames, each frame
    being a color image. Because each frame can be stored in a rank-3 tensor `(height,
    width, color_depth)`, a sequence of frames can be stored in a rank-4 tensor `(frames,
    height, width, color_depth)`, and thus a batch of different videos can be stored
    in a rank-5 tensor of shape `(samples, frames, height, width, color_depth)`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 视频数据是少数几种需要五阶张量的真实世界数据之一。视频可以被理解为一系列帧，每一帧都是一个彩色图像。因为每一帧可以存储在一个三阶张量中`(高度, 宽度,
    颜色深度)`，所以一系列帧可以存储在一个四阶张量中`(帧, 高度, 宽度, 颜色深度)`，因此不同视频的批次可以存储在一个形状为`(样本, 帧, 高度, 宽度,
    颜色深度)`的五阶张量中。
- en: For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames
    per second would have 240 frames. A batch of four such video clips would be stored
    in a tensor of shape `(4, 240, 144, 256, 3)`. That’s a total of 106,168,320 values!
    If the `dtype` of the tensor was `float32`, then each value would be stored in
    32 bits, so the tensor would represent 425 MB. Heavy! Videos you encounter in
    real life are much lighter because they aren’t stored in `float32` and they’re
    typically compressed by a large factor (such as the MPEG format).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个60秒、每秒4帧的144 × 256 YouTube视频剪辑将包含240帧。四个这样的视频剪辑的批次将存储在一个形状为`(4, 240, 144,
    256, 3)`的张量中。这总共是10,616,8320个值！如果张量的`dtype`是`float32`，那么每个值将占用32位，因此张量将代表425 MB。太重了！你在现实生活中遇到的视频要轻得多，因为它们不是以`float32`存储的，并且通常被压缩了很大的比例（例如MPEG格式）。
- en: 'The gears of neural networks: Tensor operations'
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络齿轮：张量操作
- en: Just like any computer program can be ultimately reduced to a small set of binary
    operations on binary inputs (`AND`, `OR`, `NOR`, and so on), all transformations
    learned by deep neural networks can be reduced to a handful of *tensor operations*
    (or *tensor functions*) applied to tensors of numeric data. For instance, it’s
    possible to add tensors, multiply tensors, and so on.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何计算机程序最终都可以归结为对二进制输入（`AND`、`OR`、`NOR`等）进行的一小套二进制操作一样，深度神经网络学习到的所有变换都可以归结为对数值数据张量应用的一小套*张量操作*（或*张量函数*）。例如，可以添加张量、乘以张量等。
- en: 'In our initial example, we were building our model by stacking `Dense` layers
    on top of each other. A Keras layer instance looks like this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初始示例中，我们通过将`Dense`层堆叠在一起来构建我们的模型。一个Keras层实例看起来像这样：
- en: '[PRE28]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This layer can be interpreted as a function, which takes as input a matrix
    and returns another matrix — a new representation for the input tensor. Specifically,
    the function is as follows (where `W` is a matrix and `b` is a vector, both attributes
    of the layer):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层可以解释为一个函数，它接受一个矩阵作为输入，并返回另一个矩阵——这是输入张量的新表示。具体来说，函数如下（其中`W`是一个矩阵，`b`是一个向量，都是层的属性）：
- en: '[PRE29]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s unpack this. We have three tensor operations here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下。这里我们有三个张量操作：
- en: A tensor product (`matmul`) between the input tensor and a tensor named `W`.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入张量与名为`W`的张量之间的张量积（`matmul`）。
- en: An addition (`+`) between the resulting matrix and a vector `b`.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果矩阵与向量`b`之间的加法（`+`）。
- en: 'A `relu` operation: `relu(x)` is `max(x, 0)`. `"relu"` stands for “REctified
    Linear Unit.”'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`relu`操作：`relu(x)`是`max(x, 0)`。"relu"代表“REctified Linear Unit"。
- en: Element-wise operations
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 元素级操作
- en: 'The `relu` operation and addition are element-wise operations: operations that
    are applied independently to each entry in the tensors being considered. This
    means these operations are highly amenable to massively parallel implementations
    (*vectorized* implementations, a term that comes from the *vector processor* supercomputer
    architecture from the 1970–1990 period). If you want to write a naive Python implementation
    of an element-wise operation, you use a `for` loop, as in this naive implementation
    of an element-wise `relu` operation:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`relu`操作和加法是元素级操作：这些操作独立应用于考虑中的张量中的每个条目。这意味着这些操作非常适合大规模并行实现（矢量化实现，这个术语来自1970-1990期间的矢量处理器超级计算机架构）。如果你想要编写一个元素级操作的原始Python实现，你可以使用一个`for`循环，就像这个元素级`relu`操作的原始实现一样：'
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You could do the same for addition:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以为加法做同样的事情：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: On the same principle, you can do element-wise multiplication, subtraction,
    and so on.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 根据同样的原理，你可以进行元素级乘法、减法等操作。
- en: In practice, when dealing with NumPy arrays, these operations are available
    as well-optimized built-in NumPy functions, which themselves delegate the heavy
    lifting to a Basic Linear Algebra Subprograms (BLAS) implementation. BLAS are
    low-level, highly parallel, efficient tensor-manipulation routines that are typically
    implemented in Fortran or C.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，当处理NumPy数组时，这些操作也是作为优化良好的内置NumPy函数提供的，这些函数本身将繁重的工作委托给基本线性代数子程序（BLAS）实现。BLAS是低级、高度并行、高效的张量操作例程，通常用Fortran或C实现。
- en: 'So, in NumPy, you can do the following element-wise operation, and it will
    be blazing fast:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在NumPy中，你可以执行以下元素级操作，并且它会非常快：
- en: '[PRE32]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let’s actually time the difference:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实际测量一下这个差异：
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This takes 0.02 seconds. Meanwhile, the naive version takes a stunning 2.45
    seconds:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要0.02秒。与此同时，原始版本需要惊人的2.45秒：
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Likewise, when running JAX/TensorFlow/PyTorch code on a GPU, element-wise operations
    are executed via fully vectorized CUDA implementations that can best utilize the
    highly parallel GPU chip architecture.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当在GPU上运行JAX/TensorFlow/PyTorch代码时，元素级操作通过完全矢量化CUDA实现执行，这可以最好地利用高度并行的GPU芯片架构。
- en: Broadcasting
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广播
- en: Our earlier naive implementation of `naive_add` only supports the addition of
    rank-2 tensors with identical shapes. But in the `Dense` layer introduced earlier,
    we added a rank-2 tensor with a vector. What happens with addition when the shapes
    of the two tensors being added differ?
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前对`naive_add`的原始实现只支持相同形状的秩为2的张量相加。但在之前引入的`Dense`层中，我们添加了一个与向量相加的秩为2的张量。当两个要相加的张量的形状不同时，加法会发生什么？
- en: 'When possible, and if there’s no ambiguity, the smaller tensor will be *broadcast*
    to match the shape of the larger tensor. Broadcasting consists of two steps:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当可能时，如果没有歧义，较小的张量将被*广播*以匹配较大张量的形状。广播包括两个步骤：
- en: Axes (called *broadcast axes*) are added to the smaller tensor to match the
    `ndim` of the larger tensor.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轴（称为**广播轴**）被添加到较小的张量中，以匹配较大张量的`ndim`。
- en: The smaller tensor is repeated alongside these new axes to match the full shape
    of the larger tensor.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较小的张量会沿着这些新轴重复，以匹配较大张量的完整形状。
- en: 'Let’s look at a concrete example. Consider `X` with shape `(32, 10)` and `y`
    with shape `(10,)`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体的例子。考虑形状为`(32, 10)`的`X`和形状为`(10,)`的`y`：
- en: '[PRE35]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'First, we add an empty first axis to `y`, whose shape becomes `(1, 10)`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们向`y`添加一个空的第一轴，其形状变为`(1, 10)`：
- en: '[PRE36]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we repeat `y` 32 times alongside this new axis, so that we end up with
    a tensor `Y` with shape `(32, 10)`, where `Y[i, :] == y` for `i` in `range(0,
    32)`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将`y`重复32次在这个新轴上，这样我们最终得到一个形状为`(32, 10)`的张量`Y`，其中`Y[i, :] == y`对于`i`在`range(0,
    32)`中：
- en: '[PRE37]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: At this point, we can add `X` and `Y` because they have the same shape.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以添加`X`和`Y`，因为它们具有相同的形状。
- en: 'In terms of implementation, no new rank-2 tensor is created because that would
    be terribly inefficient. The repetition operation is entirely virtual: it happens
    at the algorithmic level rather than at the memory level. But thinking of the
    vector being repeated 32 times alongside a new axis is a helpful mental model.
    Here’s what a naive implementation would look like:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现方面，没有创建新的秩为2的张量，因为这会非常低效。重复操作完全是虚拟的：它在算法级别发生，而不是在内存级别。但想象向量在新的轴上重复32次是一个有帮助的心智模型。以下是一个简单的实现示例：
- en: '[PRE38]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: With broadcasting, you can generally apply two-tensor element-wise operations
    if one tensor has shape `(a, b, … n, n + 1, … m)` and the other has shape `(n,
    n + 1, … m)`. The broadcasting will then automatically happen for axes `a` through
    `n - 1`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过广播，如果第一个张量的形状为`(a, b, … n, n + 1, … m)`，而第二个张量的形状为`(n, n + 1, … m)`，则通常可以应用两个张量的逐元素操作。广播将自动发生在轴`a`通过`n
    - 1`上。
- en: 'The following example applies the element-wise `maximum` operation to two tensors
    of different shapes via broadcasting:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例通过广播将逐元素`maximum`操作应用于不同形状的两个张量：
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Tensor product
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量积
- en: The *tensor product*, also called *dot product* or *matmul* (short for “matrix
    multiplication”) is one of the most common, most useful tensor operations.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**张量积**，也称为**点积**或**矩阵乘积**（简称“矩阵乘法”），是最常见、最有用的张量运算之一。'
- en: 'In NumPy, a tensor product is done using the `np.matmul` function, and in Keras,
    with the `keras.ops.matmul` function. Its shorthand is the `@` operator in Python:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在NumPy中，张量积是通过`np.matmul`函数实现的，而在Keras中，则是通过`keras.ops.matmul`函数。其简写是Python中的`@`运算符：
- en: '[PRE40]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In mathematical notation, you’d note the operation with a dot (•) (hence the
    name “dot product”):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学符号中，您会注意到操作使用点（•）来表示（因此得名“点积”）：
- en: '[PRE41]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Mathematically, what does the `matmul` operation do? Let’s start with the product
    of two vectors `x` and `y`. It’s computed as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，`matmul`操作做什么？让我们从两个向量`x`和`y`的乘积开始。它被计算如下：
- en: '[PRE42]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: You’ll have noticed that the product between two vectors is a scalar and that
    only vectors with the same number of elements are compatible for this operation.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到两个向量的乘积是一个标量，并且只有具有相同元素数量的向量才能进行此操作兼容。
- en: 'You can also take the product between a matrix `x` and a vector `y`, which
    returns a vector where the coefficients are the products between `y` and the rows
    of `x`. You implement it as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以计算矩阵`x`和向量`y`之间的乘积，这将返回一个向量，其系数是`y`与`x`的行的乘积。您可以这样实现它：
- en: '[PRE43]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You could also reuse the code we wrote previously, which highlights the relationship
    between a matrix-vector product and a vector product:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以重用我们之前编写的代码，这突出了矩阵-向量乘积和向量乘积之间的关系：
- en: '[PRE44]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note that as soon as one of the two tensors has an `ndim` greater than 1, `matmul`
    is no longer *symmetric*, which is to say that `matmul(x, y)` isn’t the same as
    `matmul(y, x)`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一旦两个张量中的任何一个的`ndim`大于1，`matmul`就不再是**对称的**，也就是说`matmul(x, y)`不等于`matmul(y,
    x)`。
- en: 'Of course, a tensor product generalizes to tensors with an arbitrary number
    of axes. The most common applications may be the product between two matrices.
    You can take the product of two matrices `x` and `y` (`matmul(x, y)`) if and only
    if `x.shape[1] == y.shape[0]`. The result is a matrix with shape `(x.shape[0],
    y.shape[1])`, where the coefficients are the vector products between the rows
    of `x` and the columns of `y`. Here’s the naive implementation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，张量乘积可以推广到具有任意数量轴的张量。最常见的应用可能是两个矩阵之间的乘法。如果你可以取两个矩阵`x`和`y`的乘积（`matmul(x, y)`），当且仅当`x.shape[1]
    == y.shape[0]`。结果是形状为`(x.shape[0], y.shape[1])`的矩阵，其中系数是`x`的行和`y`的列之间的向量乘积。以下是原始实现：
- en: '[PRE45]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: To understand vector product shape compatibility, it helps to visualize the
    input and output tensors by aligning them as shown in figure 2.5.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解向量乘积的形状兼容性，通过如图2.5所示的对齐输入和输出张量来可视化它们是有帮助的。
- en: '![](../Images/427de72c442fcb4c5ccaffb0d3494196.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/427de72c442fcb4c5ccaffb0d3494196.png)'
- en: '[Figure 2.5](#figure-2-5): Matrix product box diagram'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.5](#figure-2-5)：矩阵乘法框图'
- en: '`x`, `y`, and `z` are pictured as rectangles (literal boxes of coefficients).
    Because the rows of `x` and the columns of `y` must have the same size, it follows
    that the width of `x` must match the height of `y`. If you go on to develop new
    machine learning algorithms, you’ll likely be drawing such diagrams often.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`x`、`y`和`z`被表示为矩形（系数的实心框）。因为`x`的行和`y`的列必须具有相同的大小，所以`x`的宽度必须与`y`的高度相匹配。如果你继续开发新的机器学习算法，你很可能会经常绘制这样的图表。'
- en: 'More generally, you can take the product between higher-dimensional tensors,
    following the same rules for shape compatibility as outlined earlier for the 2D
    case:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，你可以根据前面为二维情况概述的形状兼容性规则，对高维张量进行乘法运算：
- en: '[PRE46]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: And so on.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。
- en: Tensor reshaping
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量重塑
- en: 'A third type of tensor operation that’s essential to understand is *tensor
    reshaping*. Although it wasn’t used in the `Dense` layers in our first neural
    network example, we used it when we preprocessed the digits data before feeding
    it into our model:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的一种第三类张量操作是*张量重塑*。尽管它在我们第一个神经网络示例中的`Dense`层中没有使用，但我们使用它在我们预处理数字数据并将其输入到我们的模型之前：
- en: '[PRE47]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Reshaping a tensor means rearranging its rows and columns to match a target
    shape. Naturally, the reshaped tensor has the same total number of coefficients
    as the initial tensor. Reshaping is best understood via simple examples:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 重塑张量意味着重新排列其行和列以匹配目标形状。自然地，重塑后的张量与初始张量具有相同数量的系数。重塑最好通过简单的例子来理解：
- en: '[PRE48]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'A special case of reshaping that’s commonly encountered is *transposition*.
    *Transposing* a matrix means exchanging its rows and its columns, so that `x[i,
    :]` becomes `x[:, i]`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的一种重塑特殊情况是*转置*。转置矩阵意味着交换其行和列，使得`x[i, :]`变为`x[:, i]`：
- en: '[PRE49]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Geometric interpretation of tensor operations
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量操作的几何解释
- en: 'Because the contents of the tensors manipulated by tensor operations can be
    interpreted as coordinates of points in some geometric space, all tensor operations
    have a geometric interpretation. For instance, let’s consider addition. We’ll
    start with the following vector:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因为张量操作中操作的张量内容可以解释为某些几何空间中点的坐标，所以所有张量操作都有几何解释。例如，让我们考虑加法。我们将从以下向量开始：
- en: '[PRE50]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: It’s a point in a 2D space (see figure 2.6). It’s common to picture a vector
    as an arrow linking the origin to the point, as shown in figure 2.7.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个二维空间中的点（见图2.6）。通常将向量表示为从原点到点的箭头，如图2.7所示。
- en: '![](../Images/8370de16886742a4b122e13e32778178.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8370de16886742a4b122e13e32778178.png)'
- en: '[Figure 2.6](#figure-2-6): A point in a 2D space'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.6](#figure-2-6)：二维空间中的点'
- en: '![](../Images/74b9e8f5e514269bd15480a4f8304517.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74b9e8f5e514269bd15480a4f8304517.png)'
- en: '[Figure 2.7](#figure-2-7): A point in a 2D space pictured as an arrow'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.7](#figure-2-7)：一个二维空间中的点，以箭头形式表示'
- en: Let’s consider a new point, `B = [1, 0.25]`, which we’ll add to the previous
    one. This is done geometrically by chaining together the vector arrows, with the
    resulting location being the vector representing the sum of the previous two vectors
    (see figure 2.8). As you can see, adding a vector `B` to a vector `A` represents
    the action of copying point `A` in new location, whose distance and direction
    from the original point `A` is determined by the vector `B`. If you apply the
    same vector addition to a group of points in the plane (an “object”), you would
    be creating a copy of the entire object in a new location (see figure 2.9). Tensor
    addition thus represents the action of *translating an object* (moving the object
    without distorting it) by a certain amount in a certain direction.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个新的点`B = [1, 0.25]`，我们将将其添加到前面的点上。这是通过将向量箭头链式连接起来在几何上完成的，结果位置是表示前两个向量之和的向量（见图2.8）。正如你所看到的，将向量`B`添加到向量`A`表示将点`A`复制到新的位置，该位置与原始点`A`的距离和方向由向量`B`确定。如果你将相同的向量加法应用于平面上的点组（一个“对象”），你将创建整个对象在新的位置上的副本（见图2.9）。因此，张量加法表示通过一定量的方向移动对象（不扭曲对象）的动作。
- en: '![](../Images/4a18a2152ec2122893e0f7236c60551d.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a18a2152ec2122893e0f7236c60551d.png)'
- en: '[Figure 2.8](#figure-2-8): Geometric interpretation of the sum of two vectors'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.8](#figure-2-8)：两个向量的几何和的解释'
- en: 'In general, elementary geometric operations, such as translation, rotation,
    scaling, skewing, and so on, can be expressed as tensor operations. Here are a
    few examples:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，基本的几何运算，如平移、旋转、缩放、倾斜等，都可以表示为张量运算。以下是一些例子：
- en: '*Translation* — As you just saw, adding a vector to a point will move this
    point by a fixed amount in a fixed direction. Applied to a set of points (such
    as a 2D object), this is called a “translation” (see figure 2.9).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平移* — 正如你所看到的，向一个点添加一个向量将使该点在固定方向上移动固定距离。应用于一组点（例如一个2D对象），这被称为“平移”（见图2.9）。'
- en: '![](../Images/f15d6633cdde4a5a8da26f7be58dd28e.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f15d6633cdde4a5a8da26f7be58dd28e.png)'
- en: '[Figure 2.9](#figure-2-9): 2D translation as a vector addition'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.9](#figure-2-9)：2D平移作为向量加法'
- en: '*Rotation* — A counterclockwise rotation of a 2D vector by an angle theta (see
    figure 2.10) can be achieved via a product with a 2 × 2 matrix `R = [[cos(theta),
    -sin(theta)], [sin(theta), cos(theta)]]`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*旋转* — 通过与一个2×2矩阵`R = [[cos(theta), -sin(theta)], [sin(theta), cos(theta)]]`相乘，可以实现一个2D向量以角度theta逆时针旋转（见图2.10）。'
- en: '![](../Images/51a9fb59643fffa14559f2c9cc179ae7.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51a9fb59643fffa14559f2c9cc179ae7.png)'
- en: '[Figure 2.10](#figure-2-10): 2D rotation (counterclockwise) as a matrix product'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.10](#figure-2-10)：2D旋转（逆时针）作为矩阵乘积'
- en: '*Scaling* — A vertical and horizontal scaling of the image (see figure 2.11)
    can be achieved via a product with a 2 × 2 matrix `S = [[horizontal_factor, 0],
    [0, vertical_factor]]` (note that such a matrix is called a “diagonal matrix”
    because it only has non-zero coefficients in its “diagonal,” going from the top
    left to the bottom right).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缩放* — 通过与一个2×2矩阵`S = [[horizontal_factor, 0], [0, vertical_factor]]`相乘（注意这样的矩阵被称为“对角矩阵”，因为它只在“对角线”上有非零系数，从左上角到右下角），可以实现图像的垂直和水平缩放（见图2.11）。'
- en: '![](../Images/9d8aa8155ed97d32b0ce2348b521abb4.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d8aa8155ed97d32b0ce2348b521abb4.png)'
- en: '[Figure 2.11](#figure-2-11): 2D scaling as a matrix product'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.11](#figure-2-11)：2D缩放作为矩阵乘积'
- en: '*Linear transform* — A product with an arbitrary matrix implements a linear
    transform. Note that *scaling* and *rotation*, seen previously, are, by definition,
    linear transforms.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*线性变换* — 与任意矩阵相乘实现线性变换。注意，之前看到的*缩放*和*旋转*按定义是线性变换。'
- en: '*Affine transform* — An affine transform (see figure 2.12) is the combination
    of a linear transform (achieved via a matrix product) and a translation (achieved
    via a vector addition). As you have probably recognized, that’s exactly the `y
    = W @ x + b` computation implemented by the `Dense` layer! A `Dense` layer without
    an activation function is an affine layer.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*仿射变换* — 仿射变换（见图2.12）是线性变换（通过矩阵乘法实现）和平移（通过向量加法实现）的组合。你可能已经注意到，这正是`Dense`层中实现的`y
    = W @ x + b`计算！没有激活函数的`Dense`层是一个仿射层。'
- en: '![](../Images/7511f20f34bb90da31011c4deeb7b51d.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7511f20f34bb90da31011c4deeb7b51d.png)'
- en: '[Figure 2.12](#figure-2-12): Affine transform in the plane'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.12](#figure-2-12)：平面上的仿射变换'
- en: '*`Dense` layer with `relu` activation* — An important observation about affine
    transforms is that if you apply many of them repeatedly, you still end up with
    an affine transform (so you could just have applied that one affine transform
    in the first place). Let’s try it with two: `affine2(affine1(x)) = W2 @ (W1 @
    x + b1) + b2 = (W2 @ W1) @ x + (W2 @ b1 + b2)`. That’s an affine transform where
    the linear part is the matrix `W2 @ W1` and the translation part is the vector
    `W2 @ b1 + b2`. As a consequence, a multilayer neural network made entirely of
    `Dense` layers without activations would be equivalent to a single `Dense` layer.
    This “deep” neural network would just be a linear model in disguise! This is why
    we need activation functions, like `relu` (seen in action in figure 2.13). Thanks
    to activation functions, a chain of `Dense` layers can be made to implement very
    complex, nonlinear geometric transformation, resulting in very rich hypothesis
    spaces for your deep neural networks. We cover this idea in more detail in the
    next chapter.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`具有`relu`激活功能的`Dense`层`* — 关于仿射变换的一个重要观察是，如果你反复应用许多次，最终仍然得到一个仿射变换（所以你一开始就可以只应用那个仿射变换）。让我们用两个来试一下：`affine2(affine1(x))
    = W2 @ (W1 @ x + b1) + b2 = (W2 @ W1) @ x + (W2 @ b1 + b2)`。这是一个仿射变换，其中线性部分是矩阵`W2
    @ W1`，平移部分是向量`W2 @ b1 + b2`。因此，一个完全由`Dense`层组成且没有激活的多层神经网络等价于一个单一的`Dense`层。这个“深度”神经网络实际上只是一个伪装的线性模型！这就是为什么我们需要激活函数，比如`relu`（如图2.13所示）。多亏了激活函数，一系列`Dense`层可以被用来实现非常复杂、非线性的几何变换，从而为你的深度神经网络提供非常丰富的假设空间。我们将在下一章更详细地介绍这个想法。'
- en: '![](../Images/590aae2bc7fbd5a8e3f8d8a16535c6d7.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/590aae2bc7fbd5a8e3f8d8a16535c6d7.png)'
- en: '[Figure 2.13](#figure-2-13): Affine transform followed by `relu` activation'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.13](#figure-2-13)：仿射变换后跟`relu`激活'
- en: A geometric interpretation of deep learning
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习的几何解释
- en: You just learned that neural networks consist entirely of chains of tensor operations
    and that all of these tensor operations are just simple geometric transformations
    of the input data. It follows that you can interpret a neural network as a very
    complex geometric transformation in a high-dimensional space, implemented via
    a series of simple steps.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚了解到神经网络完全由张量操作链组成，而且所有这些张量操作都只是输入数据的简单几何变换。因此，你可以将神经网络解释为一个在多维空间中的非常复杂的几何变换，通过一系列简单的步骤实现。
- en: 'In 3D, the following mental image may prove useful. Imagine two sheets of colored
    paper: one red and one blue. Put one on top of the other. Now crumple them together
    into a small ball. That crumpled paper ball is your input data, and each sheet
    of paper is a class of data in a classification problem. What a neural network
    is meant to do is figure out a transformation of the paper ball that would uncrumple
    it to make the two classes cleanly separable again (see figure 2.14). With deep
    learning, this would be implemented as a series of simple transformations of the
    3D space, such as those you could apply on the paper ball with your fingers, one
    movement at a time.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在三维空间中，以下的心智图可能很有用。想象两张彩色纸：一张红色，一张蓝色。将一张放在另一张上面。现在将它们揉成一个小球。这个揉皱的纸球是你的输入数据，而每一张纸都是分类问题中的一个数据类别。神经网络的目的就是找出一个将纸球展开，使两个类别再次清晰可分的变换（见图2.14）。通过深度学习，这将通过一系列简单的三维空间变换来实现，就像你可以用手指对纸球进行的一次次简单变换。
- en: '![](../Images/a89f2451feb6a78d99e009be13583d1b.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/a89f2451feb6a78d99e009be13583d1b.png)'
- en: '[Figure 2.14](#figure-2-14): Uncrumpling a complicated manifold of data'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.14](#figure-2-14)：展开复杂的数据流形'
- en: 'Uncrumpling paper balls is what machine learning is about: finding neat representations
    for complex, highly folded data *manifolds* in high-dimensional spaces (a manifold
    is a continuous surface, like our crumpled sheet of paper). At this point, you
    should have a pretty good intuition as to why deep learning excels at this: it
    takes the approach of incrementally decomposing a complicated geometric transformation
    into a long chain of elementary ones, which is pretty much the strategy a human
    would follow to uncrumple a paper ball. Each layer in a deep network applies a
    transformation that disentangles the data a little — and a deep stack of layers
    makes tractable an extremely complicated disentanglement process.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 将纸团展开是机器学习的主要内容：在多维空间中找到复杂、高度折叠的数据*流形*的整洁表示（流形是一个连续的表面，就像我们皱巴巴的纸张）。到这一点，你应该对为什么深度学习在这方面表现出色有很好的直觉：它采用了一种将复杂几何变换逐步分解为一系列基本变换的方法，这基本上是人类展开纸团所遵循的策略。深度网络中的每一层都应用一种转换，稍微解开数据——而深度堆叠的层使得极其复杂的解开过程变得可行。
- en: 'The engine of neural networks: Gradient-based optimization'
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的引擎：基于梯度的优化
- en: 'As you saw in the previous section, each neural layer from our first model
    example transforms its input data as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在上一节中看到的，我们第一个模型示例中的每个神经网络层都按以下方式转换其输入数据：
- en: '[PRE51]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In this expression, `W` and `b` are tensors that are attributes of the layer.
    They’re called the *weights* or *trainable parameters* of the layer (the `kernel`
    and `bias` attributes, respectively). These weights contain the information learned
    by the model from exposure to training data.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表达式中，`W`和`b`是层的属性张量。它们被称为层的*权重*或*可训练参数*（分别对应`kernel`和`bias`属性）。这些权重包含了模型从训练数据中学习到的信息。
- en: Initially, these weight matrices are filled with small random values (a step
    called *random initialization*). Of course, there’s no reason to expect that `relu(matmul(input,
    W) + b)`, when `W` and `b` are random, will yield any useful representations.
    The resulting representations are meaningless — but they’re a starting point.
    What comes next is to gradually adjust these weights, based on a feedback signal.
    This gradual adjustment, also called *training*, is basically the learning that
    machine learning is all about.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，这些权重矩阵填充了小的随机值（称为*随机初始化*）。当然，没有理由期望当`W`和`b`是随机的时，`relu(matmul(input, W)
    + b)`会产生任何有用的表示。结果表示是没有意义的——但它们是一个起点。接下来要做的是根据反馈信号逐渐调整这些权重。这种逐渐调整，也称为*训练*，基本上是机器学习所涉及的学习过程。
- en: 'This happens within what’s called a *training loop*, which works as follows.
    Repeat these steps in a loop, until the loss seems sufficiently low:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这发生在所谓的*训练循环*中，其工作原理如下。重复以下步骤，直到损失看起来足够低：
- en: Draw a batch of training samples `x` and corresponding targets `y_true`.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取一批训练样本`x`和相应的目标`y_true`。
- en: Run the model on `x` (a step called the *forward pass*) to obtain predictions
    `y_pred`.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`x`上运行模型（称为*前向传递*）以获得预测`y_pred`。
- en: Compute the loss of the model on the batch, a measure of the mismatch between
    `y_pred` and `y_true`.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在批次上的损失，这是`y_pred`和`y_true`之间不匹配的度量。
- en: Update all weights of the model in a way that slightly reduces the loss on this
    batch.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以一种方式更新模型的所有权重，以略微减少这个批次上的损失。
- en: 'You’ll eventually end up with a model that has a very low loss on its training
    data: a low mismatch between predictions `y_pred` and expected targets `y_true`.
    The model has “learned” to map its inputs to correct targets. From afar, it may
    look like magic, but when you reduce it to elementary steps, it turns out to be
    simple.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你将得到一个在训练数据上损失非常低的模型：预测`y_pred`和预期目标`y_true`之间的低不匹配。模型“学习”了将其输入映射到正确目标。从远处看，这可能看起来像魔法，但当你将其简化为基本步骤时，它实际上很简单。
- en: 'Step 1 sounds easy enough — it’s just I/O code. Steps 2 and 3 are merely the
    application of a handful of tensor operations, so you could implement these steps
    purely from what you learned in the previous section. The difficult part is step
    4: updating the model’s weights. Given an individual weight coefficient in the
    model, how can you compute whether the coefficient should be increased or decreased,
    and by how much?'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步听起来很简单——它只是I/O代码。第二步和第三步仅仅是应用一些张量操作，所以你可以纯粹根据上一节学到的知识来实现这些步骤。困难的部分是第四步：更新模型的权重。给定模型中的一个权重系数，你如何计算该系数应该增加还是减少，以及增加或减少多少？
- en: One naive solution would be to freeze all weights in the model except the one
    scalar coefficient being considered and try different values for this coefficient.
    Let’s say the initial value of the coefficient is 0.3. After the forward pass
    on a batch of data, the loss of the model on the batch is 0.5\. If you change
    the coefficient’s value to 0.35 and rerun the forward pass, the loss increases
    to 0.6\. But if you lower the coefficient to 0.25, the loss falls to 0.4\. In
    this case, it seems that updating the coefficient by –0.05 would contribute to
    minimizing the loss. This would have to be repeated for all coefficients in the
    model.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的方法是冻结模型中除考虑的单个标量系数外的所有权重，并尝试这个系数的不同值。假设系数的初始值为0.3。在数据批次的前向传递后，模型在批次上的损失为0.5。如果你将系数的值改为0.35并重新运行前向传递，损失增加到0.6。但如果你将系数降低到0.25，损失下降到0.4。在这种情况下，似乎通过更新系数-0.05可以有助于最小化损失。这需要为模型中的所有系数重复进行。
- en: 'But such an approach would be horribly inefficient because you’d need to compute
    two forward passes (which are expensive) for every individual coefficient (of
    which there are many, usually at least a few thousands and potentially up to billions).
    Thankfully, there’s a much better approach: *gradient descent*.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 但这样的方法将非常低效，因为你需要为每个单独的系数（通常至少有几千个，甚至可能高达数十亿）计算两次前向传递（这是昂贵的）。幸运的是，有一个更好的方法：**梯度下降**。
- en: 'Gradient descent is the optimization technique that powers modern neural networks.
    Here’s the gist of it. All of the functions used in our models (such as `matmul`
    or `+`) transform their input in a smooth and continuous way: if you look at `z
    = x + y`, for instance, a small change in `y` only results in a small change in
    `z`, and if you know the direction of the change in `y`, you can infer the direction
    of the change in `z`. Mathematically, you’d say these functions are *differentiable*.
    If you chain together such functions, the bigger function you obtain is still
    differentiable. In particular, this applies to the function that maps the model’s
    coefficients to the loss of the model on a batch of data: a small change of the
    model’s coefficients results in a small, predictable change of the loss value.
    This enables you to use a mathematical operator called the *gradient* to describe
    how the loss varies as you move the model’s coefficients in different directions.
    If you compute this gradient, you can use it to move the coefficients (all at
    once in a single update, rather than one at a time) in a direction that decreases
    the loss.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是现代神经网络背后的优化技术。以下是它的核心内容。我们模型中使用的所有函数（例如`matmul`或`+`）都以平滑和连续的方式转换它们的输入：例如，如果你看`z
    = x + y`，那么`y`的微小变化只会导致`z`的微小变化，如果你知道`y`变化的方向，你可以推断出`z`变化的方向。从数学上讲，你会说这些函数是**可微**的。如果你将这些函数链在一起，得到的更大的函数仍然是可微的。特别是，这适用于将模型的系数映射到模型在数据批次上的损失的函数：模型系数的微小变化会导致损失值的微小、可预测的变化。这使得你可以使用一个称为**梯度**的数学运算符来描述随着你将模型系数移动到不同方向时损失的变化。如果你计算这个梯度，你可以用它来移动系数（在一次更新中同时移动所有系数，而不是逐个移动）以减少损失。
- en: If you already know what *differentiable* means and what a *gradient* is, you
    can skip the next two sections. Otherwise, the following will help you understand
    these concepts.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经知道什么是**可微**的以及什么是**梯度**，你可以跳过接下来的两节。否则，以下内容将帮助你理解这些概念。
- en: What’s a derivative?
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是导数？
- en: Consider a continuous, smooth function `f(x) = y`, mapping a number `x` to a
    new number `y`. We can use the function in figure 2.15 as an example.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个连续、平滑的函数`f(x) = y`，将一个数字`x`映射到一个新的数字`y`。我们可以用图2.15中的函数作为例子。
- en: '![](../Images/8d3545d0422fdaf9095c76724c8232d6.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8d3545d0422fdaf9095c76724c8232d6.png)'
- en: '[Figure 2.15](#figure-2-15): A continuous, smooth function'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.15](#figure-2-15)：一个连续、平滑的函数'
- en: 'Because the function is *continuous*, a small change in `x` can only result
    in a small change in `y` — that’s the intuition behind *continuity*. Let’s say
    you increase `x` by a small factor `epsilon_x`: this results in a small `epsilon_y`
    change to `y`, as shown in figure 2.16.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 因为函数是**连续**的，`x`的微小变化只能导致`y`的微小变化——这就是**连续性**的直觉。假设你通过一个小的因子`epsilon_x`增加`x`：这将导致`y`的微小`epsilon_y`变化，如图2.16所示。
- en: '![](../Images/01e3045f3bc6976160217e7eaa54dadb.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01e3045f3bc6976160217e7eaa54dadb.png)'
- en: '[Figure 2.16](#figure-2-16): With a continuous function, a small change in
    `x` results in a small change in `y`.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.16](#figure-2-16)：对于连续函数，`x`的微小变化导致`y`的微小变化。'
- en: 'In addition, because the function is *smooth* (its curve doesn’t have any abrupt
    angles), when `epsilon_x` is small enough, around a certain point `p`, it’s possible
    to approximate `f` as a linear function of slope `a`, so that `epsilon_y` becomes
    `a * epsilon_x`:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于函数是*平滑的*（其曲线没有任何突然的角度），当`epsilon_x`足够小的时候，在某个点`p`附近，可以将`f`近似为一个斜率为`a`的线性函数，这样`epsilon_y`就变成了`a
    * epsilon_x`：
- en: '[PRE52]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Obviously, this linear approximation is valid only when `x` is close enough
    to `p`.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种线性近似仅在`x`足够接近`p`时才有效。
- en: The slope `a` is called the *derivative* of `f` in `p`. If `a` is negative,
    it means a small increase in `x` around `p` will result in a decrease of `f(x)`,
    as shown in figure 2.17, and if `a` is positive, a small increase in `x` will
    result in an increase of `f(x)`. Further, the absolute value of `a` (the *magnitude*
    of the derivative) tells you how quickly this increase or decrease will happen.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率`a`被称为`f`在`p`处的*导数*。如果`a`是负数，这意味着在`p`附近的`x`的微小增加将导致`f(x)`的减少，如图2.17所示，如果`a`是正数，则`x`的微小增加将导致`f(x)`的增加。此外，`a`的绝对值（导数的*大小*）告诉你这种增加或减少将发生得多快。
- en: '![](../Images/8065a8a76ddc704f79e261b220833dd0.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8065a8a76ddc704f79e261b220833dd0.png)'
- en: '[Figure 2.17](#figure-2-17): Derivative of `f` in `p`'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.17](#figure-2-17)：`f`在`p`处的导数'
- en: 'For every differentiable function `f(x)` (*differentiable* means “can be derived”:
    for example, smooth, continuous functions can be derived), there exists a derivative
    function `f''(x)` that maps values of `x` to the slope of the local linear approximation
    of `f` in those points. For instance, the derivative of `cos(x)` is `-sin(x)`,
    the derivative of `f(x) = a * x` is `f''(x) = a`, and so on.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个可导函数`f(x)`（可导意味着“可以求导”：例如，平滑、连续的函数可以求导），都存在一个导数函数`f'(x)`，它将`x`的值映射到`f`在这些点的局部线性近似的斜率。例如，`cos(x)`的导数是`-sin(x)`，`f(x)
    = a * x`的导数是`f'(x) = a`，等等。
- en: 'Being able to derive functions is a very powerful tool when it comes to *optimization*,
    the task of finding values of `x` that minimize the value of `f(x)`. If you’re
    trying to update `x` by a factor `epsilon_x` to minimize `f(x)` and you know the
    derivative of `f`, then your job is done: the derivative completely describes
    how `f(x)` evolves as you change `x`. If you want to reduce the value of `f(x)`,
    you just need to move `x` a little in the opposite direction from the derivative.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 能够求导函数是进行*优化*时一个非常强大的工具，优化是指寻找使`f(x)`的值最小化的`x`的值。如果你试图通过一个因子`epsilon_x`更新`x`以最小化`f(x)`，并且你知道`f`的导数，那么你的工作就完成了：导数完全描述了随着`x`的变化`f(x)`如何演变。如果你想减少`f(x)`的值，你只需要将`x`稍微移动到导数的相反方向。
- en: 'Derivative of a tensor operation: The gradient'
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量运算的导数：梯度
- en: 'The function we were just looking at turned a scalar value `x` into another
    scalar value `y`: you could plot it as a curve in a 2D plane. Now, imagine a function
    that turns a tuple of scalars `(x, y)` into a scalar value `z`: that would be
    a vector operation. You could plot it as a 2D *surface* in a 3D space (indexed
    by coordinates `x, y, z`). Likewise, you can imagine functions that take as input
    matrices, functions that take as input rank-3 tensors, etc.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才看到的函数将标量值`x`转换成另一个标量值`y`：你可以在二维平面上将其绘制为曲线。现在，想象一个将标量元组`(x, y)`转换成标量值`z`的函数：这将是一个向量运算。你可以在三维空间（由坐标`x,
    y, z`索引）中将其绘制为二维*表面*。同样，你可以想象输入矩阵的函数，输入秩为3的张量的函数，等等。
- en: The concept of derivation can be applied to any such function, as long as the
    surfaces they describe are continuous and smooth. The derivative of a tensor operation
    (or tensor function) is called a *gradient*. Gradients are just the generalization
    of the concept of derivatives to functions that take tensors as inputs. Remember
    how, for a scalar function, the derivative represents the *local slope* of the
    curve of the function? In just the same way, the gradient of a tensor function
    represents the *curvature* of the multidimensional surface described by the function.
    It characterizes how the output of the function varies when its input parameters
    vary.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 只要描述它们的表面是连续且平滑的，导数的概念就可以应用于任何这样的函数。张量运算（或张量函数）的导数称为*梯度*。梯度只是将导数的概念推广到以张量作为输入的函数。记得对于标量函数，导数代表函数曲线的*局部斜率*吗？同样地，张量函数的梯度代表由函数描述的多维表面的*曲率*。它描述了当输入参数变化时函数输出的变化情况。
- en: Let’s look at an example grounded in machine learning. Consider
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个基于机器学习的例子。考虑
- en: An input vector `x` (a sample in a dataset)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输入向量 `x`（数据集中的样本）。
- en: A matrix `W` (the weights of a model)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个矩阵 `W`（模型的权重）。
- en: A target `y_true` (what the model should learn to associated to `x`)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个目标 `y_true`（模型应该学习与 `x` 关联的内容）。
- en: A loss function `loss` (meant to measure the gap between the model’s current
    predictions and `y_true`).
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个损失函数 `loss`（用来衡量模型当前预测与 `y_true` 之间的差距）。
- en: 'You can use `W` to compute a target candidate `y_pred` and then compute the
    loss, or mismatch, between the target candidate `y_pred` and the target `y_true`:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `W` 来计算一个目标候选 `y_pred`，然后计算目标候选 `y_pred` 与目标 `y_true` 之间的损失或差异：
- en: '[PRE53]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Now, we’d like to use gradients to figure out how to update `W` to make `loss_value`
    smaller. How do we do that?
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想使用梯度来确定如何更新 `W` 以使 `loss_value` 更小。我们该如何做？
- en: 'Given fixed inputs `x` and `y_true`, the previous operations can be interpreted
    as a function mapping values of `W` (the model’s weights) to loss values:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 给定固定的输入 `x` 和 `y_true`，前面的操作可以解释为一个将 `W`（模型的权重）的值映射到损失值的函数：
- en: '[PRE54]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Let’s say the current value of `W` is `W0`. Then the derivative of `f` in the
    point `W0` is a tensor `grad(loss_value, W0)`, with the same shape as `W`, where
    each coefficient `grad(loss_value, W0)[i, j]` indicates the direction and magnitude
    of the change in `loss_value` you observe when modifying `W0[i, j]`. That tensor
    `grad(loss_value, W0)` is the gradient of the function `f(W) = loss_value` in
    `W0`, also called “gradient of `loss_value` with respect to `W` around `W0`.”
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 假设当前 `W` 的值为 `W0`。那么 `f` 在点 `W0` 的导数是一个与 `W` 形状相同的张量 `grad(loss_value, W0)`，其中每个系数
    `grad(loss_value, W0)[i, j]` 指示了当修改 `W0[i, j]` 时观察到的 `loss_value` 变化的方向和大小。这个张量
    `grad(loss_value, W0)` 是函数 `f(W) = loss_value` 在 `W0` 处的梯度，也称为“`loss_value` 关于
    `W` 在 `W0` 附近的梯度。”
- en: Concretely, what does `grad(loss_value, W0)` represent? You saw earlier that
    the derivative of a function `f(x)` of a single coefficient can be interpreted
    as the slope of the curve of `f`. Likewise, `grad(loss_value, W0)` can be interpreted
    as the tensor describing the *curvature* of `loss_value = f(W)` around `W0`. Each
    partial derivative describes the curvature of `f` in a specific direction.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，`grad(loss_value, W0)` 代表什么？你之前看到，一个单系数函数 `f(x)` 的导数可以解释为 `f` 的曲线的斜率。同样，`grad(loss_value,
    W0)` 可以解释为描述 `loss_value = f(W)` 在 `W0` 附近 *曲率* 的张量。每个偏导数描述了 `f` 在特定方向上的曲率。
- en: We just saw how for a function `f(x)`, you can reduce the value of `f(x)` by
    moving `x` a little in the opposite direction from the derivative. In much the
    same way, with a function `f(W)` of a tensor, you can reduce `loss_value = f(W)`
    by moving `W` in the opposite direction from the gradient, such as an update of
    `W1 = W0 - step * grad(f(W0), W0)` where `step` is a small scaling factor. That
    means going against the curvature, which intuitively should put you lower on the
    curve. Note that the scaling factor `step` is needed because `grad(loss_value,
    W0)` only approximates the curvature when you’re close to `W0`, so you don’t want
    to get too far from `W0`.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到，对于一个函数 `f(x)`，你可以通过将 `x` 向导数的相反方向移动一小段距离来减少 `f(x)` 的值。以类似的方式，对于一个张量 `f(W)`
    的函数，你可以通过将 `W` 向梯度相反的方向移动来减少 `loss_value = f(W)`，例如更新 `W1 = W0 - step * grad(f(W0),
    W0)`，其中 `step` 是一个小的缩放因子。这意味着逆着曲率走，直观上应该会让你处于曲线的较低位置。请注意，缩放因子 `step` 是必需的，因为 `grad(loss_value,
    W0)` 只在接近 `W0` 时近似曲率，所以你不想离 `W0` 太远。
- en: Stochastic gradient descent
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机梯度下降。
- en: 'Given a differentiable function, it’s theoretically possible to find its minimum
    analytically: it’s known that a function’s minimum is a point where the derivative
    is 0, so all you have to do is find all the points where the derivative goes to
    0 and check for which of these points the function has the lowest value.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个可微函数，从理论上讲，可以找到它的最小值：已知函数的最小值是导数为0的点，所以你只需要找到所有导数变为0的点，并检查这些点中哪个函数值最低。
- en: Applied to a neural network, that means finding analytically the combination
    of weight values that yields the smallest possible loss function. This can be
    done by solving the equation `grad(f(W), W) = 0` for `W`. This is a polynomial
    equation of `N` variables, where `N` is the number of coefficients in the model.
    Although it would be possible to solve such an equation for `N = 2` or `N = 3`,
    doing so is intractable for real neural networks, where the number of parameters
    is never less than a few thousand and can sometimes be in the billions.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 将此应用于神经网络，意味着通过解析方法找到权重值的组合，以产生最小的损失函数。这可以通过求解方程 `grad(f(W), W) = 0` 对于 `W`
    来实现。这是一个 `N` 个变量的多项式方程，其中 `N` 是模型中的系数数量。虽然对于 `N = 2` 或 `N = 3`，解决这样的方程是可能的，但对于实际神经网络来说，这样做是难以处理的，因为参数的数量永远不会少于几千，有时甚至达到数十亿。
- en: 'Instead, you can use the four-step algorithm outlined at the beginning of this
    section: modify the parameters little by little based on the current loss value
    on a random batch of data. Because you’re dealing with a differentiable function,
    you can compute its gradient, which gives you an efficient way to implement step
    4\. If you update the weights in the opposite direction from the gradient, the
    loss will be a little less every time:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，您可以使用本节开头概述的四个步骤算法：根据当前随机批次数据上的损失值逐渐修改参数。因为您正在处理一个可微分的函数，您可以计算其梯度，这为您实现第
    4 步提供了一个有效的方法。如果您将权重更新为与梯度相反的方向，损失将每次都略有减少：
- en: Draw a batch of training samples `x` and corresponding targets `y_true`.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抽取一批训练样本 `x` 和相应的目标 `y_true`。
- en: Run the model on `x` to obtain predictions `y_pred` (this is called the *forward
    pass*).
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `x` 上运行模型以获得预测 `y_pred`（这被称为 *正向传播*）。
- en: Compute the loss of the model on the batch, a measure of the mismatch between
    `y_pred` and `y_true`.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在批次上的损失，这是 `y_pred` 和 `y_true` 之间差异的度量。
- en: Compute the gradient of the loss with regard to the model’s parameters (this
    is called the *backward pass*).
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失相对于模型参数的梯度（这被称为 *反向传播*）。
- en: Move the parameters a little in the opposite direction from the gradient — for
    example, `W -= learning_rate * gradient` — thus reducing the loss on the batch
    a bit. The *learning rate* (`learning_rate` here) would be a scalar factor modulating
    the “speed” of the gradient descent process.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将参数稍微向梯度的反方向移动——例如，`W -= learning_rate * gradient`——从而略微减少批次的损失。*学习率*（此处为 `learning_rate`）将是一个标量因子，调节梯度下降过程的“速度”。
- en: Easy enough! What we just described is called *mini-batch stochastic gradient
    descent* (mini-batch SGD). The term *stochastic* refers to the fact that each
    batch of data is drawn at random (*stochastic* is a scientific synonym of *random*).
    Figure 2.18 illustrates what happens in 1D, when the model has only one parameter
    and you have only one training sample.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 足够简单！我们刚才描述的被称为 *小批量随机梯度下降*（mini-batch SGD）。术语 *随机* 指的是每个数据批次都是随机抽取的（*随机* 是
    *random* 的科学同义词）。图 2.18 展示了当模型只有一个参数且只有一个训练样本时，在 1D 中会发生什么。
- en: '![](../Images/3f02512a56330ac15723ca25881d1bef.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3f02512a56330ac15723ca25881d1bef.png)'
- en: '[Figure 2.18](#figure-2-18): SGD down a 1D loss curve (one learnable parameter)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.18](#figure-2-18)：SGD 在 1D 损失曲线上下降（一个可学习的参数）'
- en: We can see intuitively that it’s important to pick a reasonable value for the
    `learning_rate` factor. If it’s too small, the descent down the curve will take
    many iterations, and it could get stuck in a local minimum. If `learning_rate`
    is too large, your updates may end up taking you to completely random locations
    on the curve.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直观地看出，选择一个合理的 `learning_rate` 因子值是很重要的。如果它太小，沿着曲线的下降将需要许多迭代，并且可能会陷入局部最小值。如果
    `learning_rate` 太大，您的更新可能会将您带到曲线上的完全随机位置。
- en: Note that a variant of the mini-batch SGD algorithm would be to draw a single
    sample and target at each iteration, rather than drawing a batch of data. This
    would be *true* SGD (as opposed to *mini-batch* SGD). Alternatively, going to
    the opposite extreme, you could run every step on *all* data available, which
    is called *batch gradient descent*. Each update would then be more accurate, but
    far more expensive. The efficient compromise between these two extremes is to
    use mini-batches of reasonable size.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，mini-batch SGD 算法的一个变体是每次迭代抽取一个样本和目标，而不是抽取一批数据。这将被称为 *真实* SGD（与 *小批量* SGD
    相对）。或者，走向相反的极端，您可以在所有可用数据上运行每个步骤，这被称为 *批量梯度下降*。这样，每次更新将更加准确，但成本也更高。在这两个极端之间的有效折衷方案是使用合理大小的小批量。
- en: 'Although figure 2.18 illustrates gradient descent in a 1D parameter space,
    in practice, you’ll use gradient descent in highly dimensional spaces: every weight
    coefficient in a neural network is a free dimension in the space, and there may
    be tens of thousands or even millions of them. To help you build intuition about
    loss surfaces, you can also visualize gradient descent along a 2D loss surface,
    as shown in figure 2.19\. But you can’t possibly visualize what the actual process
    of training a neural network looks like — you can’t represent a 1,000,000-dimensional
    space in a way that makes sense to humans. As such, it’s good to keep in mind
    that the intuitions you develop through these low-dimensional representations
    may not always be accurate in practice. This has historically been a source of
    issues in the world of deep learning research.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图2.18说明了在1D参数空间中的梯度下降，但在实践中，你会在高维空间中使用梯度下降：神经网络中的每个权重系数都是空间中的一个自由维度，可能有成千上万个，甚至更多。为了帮助你建立对损失表面的直观理解，你还可以可视化沿二维损失表面的梯度下降，如图2.19所示。但你不可能可视化神经网络实际训练过程的样子——你不能以对人类有意义的方式表示一个一百万维度的空间。因此，值得注意的是，通过这些低维表示建立起来的直觉在实践上可能并不总是准确的。这在历史上一直是深度学习研究领域的问题来源。
- en: '![](../Images/1c85eb817b782179fdc04f55a0e07216.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/1c85eb817b782179fdc04f55a0e07216.png)'
- en: '[Figure 2.19](#figure-2-19): Gradient descent down a 2D loss surface (two learnable
    parameters)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.19](#figure-2-19)：沿二维损失表面的梯度下降（两个可学习参数）'
- en: 'Additionally, there exist multiple variants of SGD that differ by taking into
    account previous weight updates when computing the next weight update, rather
    than just looking at the current value of the gradients. There is, for instance,
    SGD with momentum, as well as Adagrad, RMSprop, and several others. Such variants
    are known as *optimization methods* or *optimizers*. In particular, the concept
    of *momentum*, which is used in many of these variants, deserves your attention.
    Momentum addresses two issues with SGD: convergence speed and local minima. Consider
    figure 2.20, which shows the curve of a loss as a function of a model parameter.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还存在多种SGD变体，这些变体在计算下一个权重更新时考虑了之前的权重更新，而不仅仅是查看当前梯度的值。例如，有带有动量的SGD，以及Adagrad、RMSprop和几个其他变体。这些变体被称为*优化方法*或*优化器*。特别是，*动量*的概念，它在许多这些变体中使用，值得你的注意。动量解决了SGD的两个问题：收敛速度和局部最小值。考虑图2.20，它显示了损失作为模型参数的函数的曲线。
- en: '![](../Images/df34e7802f1b473d8a0c90c565ff11c2.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/df34e7802f1b473d8a0c90c565ff11c2.png)'
- en: '[Figure 2.20](#figure-2-20): A local minimum and a global minimum'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.20](#figure-2-20)：局部最小值和全局最小值'
- en: 'As you can see, around a certain parameter value, there is a *local minimum*:
    around that point, moving left would result in the loss increasing, but so would
    moving right. If the parameter under consideration were being optimized via SGD
    with a small learning rate, then the optimization process would get stuck at the
    local minimum instead of making its way to the global minimum.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在某个参数值附近，存在一个*局部最小值*：在该点附近，向左移动会导致损失增加，向右移动也是如此。如果考虑的参数通过具有小学习率的SGD进行优化，那么优化过程会卡在局部最小值上，而不是达到全局最小值。
- en: 'You can avoid such issues by using momentum, which draws inspiration from physics.
    A useful mental image here is to think of the optimization process as a small
    ball rolling down the loss curve. If it has enough momentum, the ball won’t get
    stuck in a ravine and will end up at the global minimum. Momentum is implemented
    by moving the ball at each step based not only on the current slope value (current
    acceleration) but also on the current velocity (resulting from past acceleration).
    In practice, this means updating the parameter `w` based not only on the current
    gradient value but also on the previous parameter update, such as in this naive
    implementation:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用动量来避免这些问题，这从物理学中汲取了灵感。在这里的一个有用的心理图像是将优化过程想象成一个沿着损失曲线滚动的小球。如果它有足够的动量，球就不会卡在沟壑中，最终会达到全局最小值。动量通过在每个步骤中不仅基于当前斜率值（当前加速度）而且还基于当前速度（由过去的加速度产生）来移动球来实现。在实践中，这意味着不仅基于当前的梯度值，还基于之前的参数更新来更新参数`w`，例如在这个简单的实现中：
- en: '[PRE55]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Chaining derivatives: The Backpropagation algorithm'
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续求导：反向传播算法
- en: In the previously discussed algorithm, we casually assumed that because a function
    is differentiable, we can easily compute its gradient. But is that true? How can
    we compute the gradient of complex expressions in practice? In our two-layer network
    example, how can we get the gradient of the loss with regard to the weights? That’s
    where the *Backpropagation algorithm* comes in.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前讨论的算法中，我们随意假设由于函数是可微的，我们可以轻松地计算其梯度。但这真的正确吗？我们如何在实践中计算复杂表达式的梯度？在我们的两层网络示例中，我们如何得到关于权重的损失梯度？这就是*反向传播算法*发挥作用的地方。
- en: The chain rule
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 链式法则
- en: 'Backpropagation is a way to use the derivative of simple operations (such as
    addition, `relu`, or tensor product) to easily compute the gradient of arbitrarily
    complex combinations of these atomic operations. Crucially, a neural network consists
    of many tensor operations chained together, each of which has a simple, known
    derivative. For instance, the model from our first example can be expressed as
    a function parameterized by the variables `W1`, `b1`, `W2`, and `b2` (belonging
    to the first and second `Dense` layers, respectively), involving the atomic operations
    `matmul`, `relu`, `softmax`, and `+`, as well as our loss function, `loss`, which
    are all easily differentiable:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是一种使用简单操作（如加法、`relu`或张量积）的导数来轻松计算这些原子操作任意复杂组合的梯度的方法。关键的是，神经网络由许多链式连接的张量操作组成，每个操作都有一个简单且已知的导数。例如，我们第一个示例中的模型可以表示为一个由变量`W1`、`b1`、`W2`和`b2`（分别属于第一和第二`Dense`层）参数化的函数，涉及原子操作`matmul`、`relu`、`softmax`和`+`，以及我们的损失函数`loss`，所有这些都很容易微分：
- en: '[PRE56]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Calculus tells us that such a chain of functions can be derived using the following
    identity, called the *chain rule*. Consider two functions `f` and `g`, as well
    as the composed function `fg` such that `y = fg(x) == f(g(x))`:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分告诉我们，可以使用以下称为*链式法则*的恒等式推导出这样的函数链。考虑两个函数`f`和`g`，以及复合函数`fg`，使得`y = fg(x) ==
    f(g(x))`：
- en: '[PRE57]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Then the chain rule states that `grad(y, x) == grad(y, x1) * grad(x1, x)`.
    This enables you to compute the derivative of `fg` as long as you know the derivatives
    of `f` and `g`. The chain rule is named like this because when you add more intermediate
    functions, it starts looking like a chain:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则指出`grad(y, x) == grad(y, x1) * grad(x1, x)`。这使你能够在知道`f`和`g`的导数的情况下计算`fg`的导数。链式法则之所以得名，是因为当你添加更多的中间函数时，它开始看起来像一条链：
- en: '[PRE58]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Applying the chain rule to the computation of the gradient values of a neural
    network gives rise to an algorithm called *backpropagation*. Let’s see how that
    works, concretely.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 将链式法则应用于计算神经网络梯度值会产生一个称为*反向传播*的算法。让我们具体看看它是如何工作的。
- en: Automatic differentiation with computation graphs
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于计算图的自动微分
- en: A useful way to think about backpropagation is in terms of *computation graphs*.
    A computation graph is the data structure at the heart of the deep learning revolution.
    It’s a directed acyclic graph of operations — in our case, tensor operations.
    For instance, figure 2.21 is the graph representation of our first model.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到反向传播的一个有用方法是将其视为*计算图*。计算图是深度学习革命的核心数据结构。它是一个有向无环图，表示操作——在我们的例子中，是张量操作。例如，图2.21是我们第一个模型的图表示。
- en: '![](../Images/edddc5625b92a1730fb68e9f303cd6a9.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图2.21](../Images/edddc5625b92a1730fb68e9f303cd6a9.png)'
- en: '[Figure 2.21](#figure-2-21): The computation graph representation of our two-layer
    model'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.21](#figure-2-21)：我们两层模型的计算图表示'
- en: 'Computation graphs have been an extremely successful abstraction in computer
    science because they enable us to *treat computation as data*: a computable expression
    is encoded as a machine-readable data structure that can be used as the input
    or output of another program. For instance, you could imagine a program that receives
    a computation graph and returns a new computation graph that implements a large-scale
    distributed version of the same computation — this would mean that you could distribute
    any computation without having to write the distribution logic yourself. Or imagine
    ... a program that receives a computation graph and can automatically generate
    the derivative of the expression it represents. It’s much easier to do these things
    if your computation is expressed as an explicit graph data structure rather than,
    say, lines of ASCII characters in a `.py` file.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图在计算机科学中是一个非常成功的抽象，因为它们使我们能够将计算视为数据：一个可计算的表达式被编码为一种机器可读的数据结构，可以用作另一个程序的输入或输出。例如，你可以想象一个程序，它接收一个计算图并返回一个新的计算图，该图实现了相同计算的大规模分布式版本——这意味着你可以分发任何计算，而无需自己编写分发逻辑。或者想象一个程序，它接收一个计算图并可以自动生成它所表示的表达式的导数。如果你的计算以显式的图数据结构表达，而不是，比如说，`.py`文件中的ASCII字符行，那么做这些事情会容易得多。
- en: 'To explain backpropagation clearly, let’s look at a really basic example of
    a computation graph. We’ll consider a simplified version of the graph in figure
    2.21, where we only have one linear layer and where all variables are scalar,
    shown in figure 2.22\. We’ll take two scalar variables `w`, `b`, a scalar input
    `x`, and apply some operations to them to combine into an output `y`. Finally,
    we’ll apply an absolute value error loss function: `loss_val = abs(y_true - y)`.
    Since we want to update `w` and `b` in a way that would minimize `loss_val`, we
    are interested in computing `grad(loss_val, b)` and `grad(loss_val, w)`.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰地解释反向传播，让我们来看一个计算图的简单示例。我们将考虑图2.21中的简化版本，其中只有一个线性层，并且所有变量都是标量，如图2.22所示。我们将考虑两个标量变量`w`、`b`，一个标量输入`x`，并对它们应用一些操作以组合成一个输出`y`。最后，我们将应用一个绝对值误差损失函数：`loss_val
    = abs(y_true - y)`。由于我们希望以最小化`loss_val`的方式更新`w`和`b`，因此我们感兴趣的是计算`grad(loss_val,
    b)`和`grad(loss_val, w)`。
- en: '![](../Images/9e863c770b4807b5b36699d4e777d49e.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图2.23](../Images/9e863c770b4807b5b36699d4e777d49e.png)'
- en: '[Figure 2.22](#figure-2-22): A basic example of a computation graph'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.22](#figure-2-22)：计算图的基本示例'
- en: Let’s set concrete values for the “input nodes” in the graph — that is, the
    input `x`, the target `y_true`, `w` and `b` (figure 2.23). We propagate these
    values to all nodes in the graph, from top to bottom, until we reach `loss_val`.
    This is the *forward pass*.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在图中为“输入节点”设置具体的值——也就是说，输入`x`、目标`y_true`、`w`和`b`（图2.23）。我们将从上到下传播这些值到图中的所有节点，直到我们达到`loss_val`。这就是*正向传播*。
- en: '![](../Images/27324b2738a199602ed527c7e8e06c56.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![图2.24](../Images/27324b2738a199602ed527c7e8e06c56.png)'
- en: '[Figure 2.23](#figure-2-23): Running a forward pass'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.23](#figure-2-23)：运行正向传播'
- en: 'Now let’s “reverse” the graph: for each edge in the graph going from `A` to
    `B`, we will create an opposite edge from `B` to `A`, and ask, “How much does
    `B` vary when `A` varies?” That is, what is `grad(B, A)`? We’ll annotate each
    inverted edge with this value (figure 2.24). This backward graph represents the
    *backward pass*.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们“反转”图：对于图中从`A`到`B`的每条边，我们将创建一个从`B`到`A`的相反边，并询问，“当`A`变化时，`B`变化多少？”也就是说，`grad(B,
    A)`是多少？我们将用这个值标注每个反转边（图2.24）。这个反向图代表了*反向传播*。
- en: '![](../Images/48e99f7d7c2a6e1f6517e5ee25b92f3e.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图2.23](../Images/48e99f7d7c2a6e1f6517e5ee25b92f3e.png)'
- en: '[Figure 2.24](#figure-2-24): Running a backward pass'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.24](#figure-2-24)：运行反向传播'
- en: We have
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: '`grad(loss_val, x2) = 1` because as `x2` varies by an amount epsilon, `loss_val
    = abs(4 - x2)` varies by the same amount.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(loss_val, x2) = 1`，因为当`x2`变化一个量ε时，`loss_val = abs(4 - x2)`也变化相同的量。'
- en: '`grad(x2, x1) = 1` because as `x1` varies by an amount epsilon, `x2 = x1 +
    b = x1 + 1` varies by the same amount.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(x2, x1) = 1`，因为当`x1`变化一个量ε时，`x2 = x1 + b = x1 + 1`也变化相同的量。'
- en: '`grad(x2, b) = 1` because as `b` varies by an amount epsilon, `x2 = x1 + b
    = 6 + b` varies by the same amount.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(x2, b) = 1`，因为当`b`变化一个量ε时，`x2 = x1 + b = 6 + b`也变化相同的量。'
- en: '`grad(x1, w) = 2` because as `w` varies by an amount epsilon, `x1 = x * w =
    2 * w` varies by `2 * epsilon`.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(x1, w) = 2`，因为当`w`变化一个量ε时，`x1 = x * w = 2 * w`变化`2 * epsilon`。'
- en: What the chain rule says about this backward graph is that you can obtain the
    derivative of a node with respect to another node by *multiplying the derivatives
    for each edge along the path linking the two nodes*. For instance, `grad(loss_val,
    w) = grad(loss_val, x2) * grad(x2, x1) * grad(x1, w)`.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则关于这个反向图的说法是，你可以通过*沿着连接两个节点的路径乘以每个边的导数*来获得一个节点相对于另一个节点的导数。例如，`grad(loss_val,
    w) = grad(loss_val, x2) * grad(x2, x1) * grad(x1, w)`。
- en: '![](../Images/7e87e6dc72a5695f31469a3a176a50f1.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e87e6dc72a5695f31469a3a176a50f1.png)'
- en: '[Figure 2.25](#figure-2-25): Path from `loss_val` to `w` in the backward graph'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.25](#figure-2-25)：反向图中从`loss_val`到`w`的路径'
- en: 'By applying the chain rule to our graph, we obtain what we were looking for:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将链式法则应用于我们的图，我们得到了我们想要的结果：
- en: '`grad(loss_val, w) = 1 * 1 * 2 = 2`'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(loss_val, w) = 1 * 1 * 2 = 2`'
- en: '`grad(loss_val, b) = 1 * 1 = 1`'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(loss_val, b) = 1 * 1 = 1`'
- en: 'And with that, you just saw backpropagation in action! Backpropagation is simply
    the application of the chain rule to a computation graph. There’s nothing more
    to it. Backpropagation starts with the final loss value and works backward from
    the top layers to the bottom layers, computing the contribution that each parameter
    had in the loss value. That’s where the name “backpropagation” comes from: we
    “back propagate” the loss contributions of different nodes in a computation graph.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，你刚刚见证了反向传播的实际应用！反向传播仅仅是将链式法则应用于计算图。除此之外没有其他内容。反向传播从最终的损失值开始，从顶层到底层反向工作，计算每个参数在损失值中的贡献。这就是“反向传播”这个名字的由来：我们在计算图的节点中“反向传播”不同节点的损失贡献。
- en: Nowadays, people implement neural networks in modern frameworks that are capable
    of *automatic differentiation*, such as JAX, TensorFlow, and PyTorch. Automatic
    differentiation is implemented with the kind of computation graph previously presented.
    Automatic differentiation makes it possible to retrieve the gradients of arbitrary
    compositions of differentiable tensor operations without doing any extra work
    besides writing down the forward pass. When I wrote my first neural networks in
    C in the 2000s, I had to write my gradients by hand. Now, thanks to modern automatic
    differentiation tools, you’ll never have to implement backpropagation yourself.
    Consider yourself lucky!
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，人们使用能够进行*自动微分*的现代框架来实现神经网络，例如JAX、TensorFlow和PyTorch。自动微分是通过之前提出的计算图实现的。自动微分使得能够检索任意可微张量操作的任意组合的梯度，而无需进行任何额外的工作，除了写下正向传递。当我2000年代用C语言编写我的第一个神经网络时，我不得不手动编写梯度。现在，多亏了现代自动微分工具，你永远不需要自己实现反向传播。觉得自己很幸运吧！
- en: Looking back at our first example
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾我们的第一个例子
- en: 'You’re nearing the end of this chapter, and you should now have a general understanding
    of what’s going on behind the scenes in a neural network. What was a magical black
    box at the start of the chapter has turned into a clearer picture, as illustrated
    in figure 2.26: the model, composed of layers that are chained together, maps
    the input data to predictions. The loss function then compares these predictions
    to the targets, producing a loss value: a measure of how well the model’s predictions
    match what was expected. The optimizer uses this loss value to update the model’s
    weights.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 你即将结束本章的学习，现在你应该对神经网络背后的工作原理有一个大致的了解。在章节开始时，神经网络是一个神秘的黑色盒子，而现在它已经变成了一个更清晰的图景，如图2.26所示：由层组成的模型将输入数据映射到预测。损失函数随后将这些预测与目标进行比较，产生损失值：衡量模型预测与预期匹配程度的一个指标。优化器使用这个损失值来更新模型的权重。
- en: '![](../Images/dd2bc218f52709e83d164144484ab53c.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd2bc218f52709e83d164144484ab53c.png)'
- en: '[Figure 2.26](#figure-2-26): Relationship between the network, layers, loss
    function, and optimizer'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.26](#figure-2-26)：网络、层、损失函数和优化器之间的关系'
- en: Let’s go back to the first example and review each piece of it in the light
    of what you’ve learned in the previous sections.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到第一个例子，并回顾一下你在前几节中学到的内容。
- en: 'This was the input data:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输入数据：
- en: '[PRE59]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Now you understand that the input images are stored in NumPy tensors, which
    are here formatted as `float32` tensors of shape `(60000, 784)` (training data)
    and `(10000, 784)` (test data), respectively.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了输入图像存储在NumPy张量中，这里格式化为形状为`(60000, 784)`的`float32`张量（训练数据）和形状为`(10000,
    784)`的张量（测试数据）。
- en: 'This was our model:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的模型：
- en: '[PRE60]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Now you understand that this model consists of a chain of two `Dense` layers,
    that each layer applies a few simple tensor operations to the input data, and
    that these operations involve weight tensors. Weight tensors, which are attributes
    of the layers, are where the *knowledge* of the model persists.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经明白这个模型由两个 `Dense` 层组成，每个层都对输入数据应用一些简单的张量操作，并且这些操作涉及到权重张量。权重张量是层的属性，是模型知识的持久化所在。
- en: 'This was the model-compilation step:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型编译步骤：
- en: '[PRE61]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Now you understand that `"sparse_categorical_crossentropy"` is the loss function
    that’s used as a feedback signal for learning the weight tensors, which the training
    phase will attempt to minimize. You also know that this reduction of the loss
    happens via mini-batch stochastic gradient descent. The exact rules governing
    a specific use of gradient descent are defined by the `"adam"` optimizer passed
    as the first argument.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经明白 `"sparse_categorical_crossentropy"` 是用于学习权重张量的损失函数，作为学习的反馈信号，训练阶段将尝试最小化它。你还知道这种损失减少是通过小批量随机梯度下降实现的。具体使用梯度下降的规则由作为第一个参数传递的
    `"adam"` 优化器定义。
- en: 'Finally, this was the training loop:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是训练循环：
- en: '[PRE62]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now you understand what happens when you call `fit`: the model will start to
    iterate on the training data in mini-batches of 128 samples, 5 times over (each
    iteration over all the training data is called an *epoch*). For each batch, the
    model will compute the gradient of the loss with regard to the weights (using
    the Backpropagation algorithm, which derives from the chain rule in calculus)
    and move the weights in the direction that will reduce the value of the loss for
    this batch.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经明白当你调用 `fit` 时会发生什么：模型将开始以 128 个样本的小批量迭代训练数据，总共迭代 5 次（每次遍历所有训练数据称为一个 *epoch*）。对于每个批量，模型将计算损失相对于权重的梯度（使用反向传播算法，该算法源于微积分中的链式法则）并将权重移动到减少该批量损失值的方向。
- en: After these 5 epochs, the model will have performed 2,345 gradient updates (469
    per epoch), and the loss of the model will be sufficiently low that the model
    will be capable of classifying handwritten digits with high accuracy.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这 5 个 epoch 后，模型将执行 2,345 次梯度更新（每个 epoch 469 次），并且模型的损失将足够低，以至于模型能够以高精度对手写数字进行分类。
- en: At this point, you already know most of what there is to know about neural networks.
    Let’s prove it by reimplementing a simplified version of that first example step
    by step, using only low-level operations.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解了关于神经网络的大部分知识。让我们通过逐步重新实现第一个示例的简化版本来证明这一点，只使用低级操作。
- en: Reimplementing our first example from scratch
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从头开始重新实现我们的第一个示例
- en: 'What’s better to demonstrate full, unambiguous understanding than to implement
    everything from scratch? Of course, what “from scratch” means here is relative:
    we won’t reimplement basic tensor operations, and we won’t implement backpropagation.
    But we’ll go to such a low level that each computation step will be made explicit.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 什么比从头开始实现一切更能证明对知识的全面、明确理解呢？当然，“从头开始”在这里是相对的：我们不会重新实现基本的张量操作，也不会实现反向传播。但我们会深入到这样的低级，以至于每个计算步骤都会被明确地表示出来。
- en: Don’t worry if you don’t understand every little detail in this example just
    yet. The next chapter will dive in more detail into the Keras API. For now, just
    try to follow the gist of what’s going on — the intent of this example is to help
    crystallize your understanding of the mathematics of deep learning using a concrete
    implementation. Let’s go!
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在还不完全理解这个例子中的每一个细节，不要担心。下一章将更详细地介绍 Keras API。现在，只需尽量理解正在发生的事情的大致情况——这个例子的目的是通过具体的实现帮助你巩固对深度学习数学的理解。让我们开始吧！
- en: A simple Dense class
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个简单的 Dense 类
- en: 'You’ve learned earlier that the `Dense` layer implements the following input
    transformation, where `W` and `b` are model parameters, and `activation` is an
    element-wise function (usually `relu`):'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 你之前已经了解到 `Dense` 层实现了以下输入转换，其中 `W` 和 `b` 是模型参数，而 `activation` 是逐元素函数（通常是 `relu`）：
- en: '[PRE63]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Let’s implement a simple Python class `NaiveDense` that creates two Keras variables
    `W` and `b`, and exposes a `__call__()` method that applies the previous transformation:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个简单的 Python 类 `NaiveDense`，它创建两个 Keras 变量 `W` 和 `b`，并公开一个 `__call__()`
    方法，该方法应用之前的转换：
- en: '[PRE64]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: A simple Sequential class
  id: totrans-377
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个简单的 Sequential 类
- en: 'Now, let’s create a `NaiveSequential` class to chain these layers. It wraps
    a list of layers and exposes a `__call__()` method that simply calls the underlying
    layers on the inputs, in order. It also features a `weights` property to easily
    keep track of the layers’ parameters:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个`NaiveSequential`类来连接这些层。它包装了一个层列表，并暴露了一个`__call__()`方法，该方法简单地按顺序调用输入的底层层。它还提供了一个`weights`属性，以便轻松跟踪层的参数：
- en: '[PRE65]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Using this `NaiveDense` class and this `NaiveSequential` class, we can create
    a mock Keras model:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个`NaiveDense`类和这个`NaiveSequential`类，我们可以创建一个模拟的Keras模型：
- en: '[PRE66]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: A batch generator
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批量生成器
- en: 'Next, we need a way to iterate over the MNIST data in mini-batches. This is
    easy:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一种方法来迭代MNIST数据的小批量。这很简单：
- en: '[PRE67]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Running one training step
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行一个训练步骤
- en: 'The most difficult part of the process is the “training step”: updating the
    weights of the model after running it on one batch of data. We need to'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 过程中最困难的部分是“训练步骤”：在运行一个数据批次后更新模型的权重。我们需要
- en: Compute the predictions of the model for the images in the batch
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算模型对批次中图像的预测
- en: Compute the loss value for these predictions given the actual labels
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据实际标签计算这些预测的损失值
- en: Compute the gradient of the loss with regard to the model’s weights
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失相对于模型权重的梯度
- en: Move the weights by a small amount in the direction opposite to the gradient
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将权重沿着与梯度相反的方向移动一小步
- en: '[PRE68]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[Listing 2.9](#listing-2-9): A single step of training'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表2.9](#listing-2-9)：训练的单个步骤'
- en: The weight update step
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权重更新步骤
- en: 'As you already know, the purpose of the “weight update” step (represented by
    the `update_weights()` function) is to move the weights by “a bit” in a direction
    that will reduce the loss on this batch. The magnitude of the move is determined
    by the “learning rate,” typically a small quantity. The simplest way to implement
    this `update_weights()` function is to subtract `gradient * learning_rate` from
    each weight:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，“权重更新”步骤（由`update_weights()`函数表示）的目的是将权重“稍微”移动到一个方向，以减少这个批次的损失。移动的幅度由“学习率”决定，通常是一个很小的量。实现这个`update_weights()`函数的最简单方法是从每个权重中减去`gradient
    * learning_rate`：
- en: '[PRE69]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'In practice, you will almost never implement a weight update step like this
    by hand. Instead, you would use an `Optimizer` instance from Keras — like this:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你几乎永远不会手动实现这样的权重更新步骤。相反，你会使用Keras中的一个`Optimizer`实例——就像这样：
- en: '[PRE70]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Gradient computation
  id: totrans-398
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度计算
- en: 'Now, there’s just one thing we’re still missing: gradient computation (represented
    by the `get_gradients_of_loss_wrt_weights()` function in listing 2.9). In the
    previous section, we outlined how we could use the chain rule to obtain the gradients
    of a chain of functions given their individual derivatives, a process known as
    backpropagation. We could reimplement backpropagation from scratch here, but that
    would be rather cumbersome, especially since we’re using a `softmax` operation
    and a crossentropy loss, which have fairly verbose derivatives.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只是还缺少一件事情：梯度计算（由列表2.9中的`get_gradients_of_loss_wrt_weights()`函数表示）。在前一节中，我们概述了如何使用链式法则来获得函数链的梯度，给定它们的各个导数，这个过程称为反向传播。我们可以在这里从头开始重新实现反向传播，但这会很繁琐，尤其是我们正在使用`softmax`操作和交叉熵损失，它们的导数相当冗长。
- en: Instead, we can rely on the automatic differentiation mechanism that’s built
    into one of the low-level frameworks supported by Keras, such as TensorFlow, JAX,
    or PyTorch. For the sake of the example, let’s go with TensorFlow here. You’ll
    learn more about TensorFlow, JAX, and PyTorch in the next chapter.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以依赖Keras支持的低级框架之一内置的自动微分机制，例如TensorFlow、JAX或PyTorch。为了示例的目的，让我们在这里使用TensorFlow。你将在下一章中了解更多关于TensorFlow、JAX和PyTorch的信息。
- en: 'The API through which you can use TensorFlow’s automatic differentiation capabilities
    is the `tf.GradientTape` object. It’s a Python scope that will “record” the tensor
    operations that run inside it, in the form of a computation graph (sometimes called
    a *tape*). This graph can then be used to retrieve the gradient of any scalar
    value with respect to any set of input values:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过`tf.GradientTape`对象使用TensorFlow的自动微分功能。它是一个Python作用域，会“记录”在其中运行的张量操作，以计算图的形式（有时称为*tape*）。然后，这个图可以用来检索任何标量值相对于任何一组输入值的梯度：
- en: '[PRE71]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Let’s rewrite our function `one_training_step()` using the TensorFlow `GradientTape`
    (skipping the need for a separate `get_gradients_of_loss_wrt_weights()` function):'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用TensorFlow的`GradientTape`重写我们的`one_training_step()`函数（跳过需要单独的`get_gradients_of_loss_wrt_weights()`函数）：
- en: '[PRE72]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Now that our per-batch training step is ready, we can move on to implementing
    an entire epoch of training.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了每个批次的训练步骤，我们可以继续实现整个训练周期的实现。
- en: The full training loop
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完整的训练循环
- en: 'An epoch of training simply consists of the repetition of the training step
    for each batch in the training data, and the full training loop is simply the
    repetition of one epoch:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 一个训练周期简单地由重复训练数据集中每个批次的训练步骤组成，完整的训练循环只是重复一个周期：
- en: '[PRE73]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Let’s test-drive it:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试驾一下：
- en: '[PRE74]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Evaluating the model
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'We can evaluate the model by taking the `argmax` of its predictions over the
    test images, and comparing it to the expected labels:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对其测试图像的预测取`argmax`并与其期望的标签进行比较来评估模型：
- en: '[PRE75]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: All done! As you can see, it’s quite a bit of work to do “by hand” what you
    can do in a few lines of Keras code. But because you’ve gone through these steps,
    you should now have a crystal-clear understanding of what goes on inside a neural
    network when you call `fit()`. Having this low-level mental model of what your
    code is doing behind the scenes will make you better able to take advantage of
    the high-level features of the Keras API.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 所有工作都完成了！正如你所见，手动完成你可以在几行Keras代码中完成的事情需要相当多的工作。但是因为你已经经历了这些步骤，你现在应该对当你调用`fit()`时神经网络内部发生的事情有一个清晰的理解。拥有这种低级别的心理模型，将使你更好地利用Keras
    API的高级功能。
- en: Summary
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: '*Tensors* form the foundation of modern machine learning systems. They come
    in various flavors of `dtype`, `rank`, and `shape`.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*张量*是现代机器学习系统的基础。它们有各种`dtype`、`rank`和`shape`的口味。'
- en: You can manipulate numerical tensors via *tensor operations* (such as addition,
    tensor product, or element-wise multiplication), which can be interpreted as encoding
    geometric transformations. In general, everything in deep learning is amenable
    to a geometric interpretation.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过*张量操作*（如加法、张量积或逐元素乘法）来操作数值张量，这些操作可以解释为编码几何变换。在深度学习中，一切都可以用几何解释。
- en: Deep learning models consist of chains of simple tensor operations, parameterized
    by *weights*, which are themselves tensors. The weights of a model are where its
    “knowledge” is stored.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型由一系列简单的张量操作组成，这些操作由*权重*参数化，而权重本身也是张量。模型的重量是其“知识”存储的地方。
- en: '*Learning* means finding a set of values for the model’s weights that minimizes
    a *loss function* for a given set of training data samples and their corresponding
    targets.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习*意味着找到一组模型权重的值，这组值可以最小化给定一组训练数据样本及其对应的目标的*损失函数*。'
- en: Learning happens by drawing random batches of data samples and their targets
    and computing the gradient of the model parameters with respect to the loss on
    the batch. The model parameters are then moved a bit (the magnitude of the move
    is defined by the learning rate) in the opposite direction from the gradient.
    This is called *mini-batch gradient descent*.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是通过随机抽取数据样本及其目标，并计算模型参数相对于批次损失的梯度来发生的。然后，模型参数会朝着梯度的反方向移动一点（移动的大小由学习率定义）。这被称为*小批量梯度下降*。
- en: The entire learning process is made possible by the fact that all tensor operations
    in neural networks are differentiable, and thus it’s possible to apply the chain
    rule of derivation to find the gradient function mapping the current parameters
    and current batch of data to a gradient value. This is called *backpropagation*.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个学习过程之所以成为可能，是因为神经网络中的所有张量操作都是可微分的，因此可以应用导数的链式法则来找到将当前参数和当前批次数据映射到梯度值的梯度函数。这被称为*反向传播*。
- en: 'Two key concepts you’ll see frequently in future chapters are *loss* and *optimizers*.
    These are the two things you need to define before you begin feeding data into
    a model:'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在未来的章节中你将频繁看到的两个关键概念是*损失*和*优化器*。在开始将数据输入模型之前，你需要定义这两者：
- en: The *loss* is the quantity you’ll attempt to minimize during training, so it
    should represent a measure of success for the task you’re trying to solve.
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失*是在训练过程中你将尝试最小化的量，因此它应该代表你试图解决的问题的成功度量。'
- en: 'The *optimizer* specifies the exact way in which the gradient of the loss will
    be used to update parameters: for instance, it could be the RMSProp optimizer,
    SGD with momentum, and so on.'
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器*指定了损失梯度的确切使用方式来更新参数：例如，它可以是RMSProp优化器、带有动量的SGD，等等。'
