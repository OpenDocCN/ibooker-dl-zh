<html><head></head><body><section class="pagenumrestart" data-pdf-bookmark="Chapter 1. AI in the Tableau Platform" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch01_ai_in_the_tableau_platform_1732595607206026">&#13;
<h1><span class="label">Chapter 1. </span>AI in the Tableau Platform</h1>&#13;
&#13;
<p>Artificial intelligence <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="history of" data-type="indexterm" id="id253"/>isn’t new. It’s been around since the famous mathematician Alan <a contenteditable="false" data-primary="Turing, Alan" data-type="indexterm" id="id254"/>Turing first <a contenteditable="false" data-primary="“Can machines think?” (Turing)" data-primary-sortas="Can machines think" data-type="indexterm" id="id255"/>asked, “Can machines think?” in his well known 1950 paper “Computing Machinery and Intelligence.” AI was formalized into an academic area for research, study, and innovation (according to many academics and historians in the field) in 1956. In the beginning, we can imagine that the goals of AI were to replicate or emulate human intelligence and decision making. This may have been best stated by computer scientist <a contenteditable="false" data-primary="McCarthy, John" data-type="indexterm" id="id256"/>John McCarthy, who coined the term <em>artificial intelligence</em>: “Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.” Many would add that computers could (in theory and now in practice) handle complex operations—sometimes much more complex (particularly mathematical ones) than humans could.</p>&#13;
&#13;
<p>Since that time, AI has slowly woven itself into every area of our lives. The idea of being bested by a computer at chess is nothing new. In fact, AI’s place in any sort of strategic game now feels common and expected. More recent and prolific applications of AI would include recommendation engines, such as those determining what to watch next on Netflix or what to buy on Amazon. Voice assistants like Apple’s Siri that can translate your speech into some type of action also qualify as AI. And then there are self-driving vehicles like Google’s Waymo that take the idea of <em>autopilot</em>, a concept that’s been around for roughly 100 years, to a brand-new paradigm.</p>&#13;
&#13;
<p>So much of AI has depended on the capabilities of computers. Early computers were limited in the information and knowledge that they could hold. And similarly, even if they had access to the knowledge, the computational and processing power necessary to access, retrieve, and serve up that information was massive. But in the early 2000s, the pace of innovation with computer hardware caught up with the needs of AI. Multiple processors and multithreading proliferated, smartphones became prominent fixtures in our everyday lives, and cloud computing became the new normal.</p>&#13;
&#13;
<section data-pdf-bookmark="AI in Analytics" data-type="sect1"><div class="sect1" id="ch01_ai_in_analytics_1732595607206227">&#13;
<h1>AI in Analytics</h1>&#13;
&#13;
<p>As <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="in analytics" data-type="indexterm" id="id257"/>advancements in computer hardware became more prominent, the domain of analytics started to see its own advancements. In particular, this meant the inclusion of more advanced methods of mathematical <a contenteditable="false" data-primary="multivariate analysis" data-type="indexterm" id="id258"/>analysis. <em>Multivariate analysis</em> became more pervasive in business applications as more knowledge workers could lean on computers to help with the complex mathematical <a contenteditable="false" data-primary="predictive analytics" data-type="indexterm" id="id259"/>computations. <em>Predictive analytics</em> and the ways in which something could be predicted expanded from the simple linear regression model of old to much more robust and sophisticated methods. Classification <a contenteditable="false" data-primary="classification methods" data-type="indexterm" id="id260"/>methods, <a contenteditable="false" data-primary="k-means clustering" data-type="indexterm" id="id261"/>like <em>k-means clustering</em> (available in Tableau) were possible. Machine <a contenteditable="false" data-primary="machine learning (ML)" data-type="indexterm" id="id262"/><a contenteditable="false" data-primary="ML (machine learning)" data-type="indexterm" id="id263"/>learning (ML), both <em>supervised</em>, <a contenteditable="false" data-primary="supervised machine learning" data-type="indexterm" id="id264"/>where the input and output are provided to the model, <a contenteditable="false" data-primary="unsupervised machine learning" data-type="indexterm" id="id265"/>and <em>unsupervised</em>, where models are given the freedom to derive their own patterns, came into prominence. A computer could now be given a massive amount of data and draw conclusions with varying levels of aid and decision input from humans.</p>&#13;
&#13;
<p>Then <a contenteditable="false" data-primary="NLP (natural language processing)" data-type="indexterm" id="id266"/>came <em>natural language processing</em> (NLP). As mentioned in the Preface, <a contenteditable="false" data-primary="Ask Data" data-type="indexterm" id="id267"/>Ask Data was released by Tableau in 2018, allowing users to ask a question and get an answer, but speech recognition and transcription tools have been living in the business world for much longer. <em>Sentiment analysis</em>—<a contenteditable="false" data-primary="sentiment analysis" data-type="indexterm" id="id268"/>or the process of scoring language to determine whether it is positive, negative, or neutral—started becoming mainstream. Computer languages like Python and R became prominent, and the practice of data science and the emergence of the data scientist reached a fever pitch in the 2010s, along with the term <em>big data</em>.</p>&#13;
&#13;
<p>2018 also heralded the <a contenteditable="false" data-primary="GPT (generative pre-trained transformer) technologies" data-type="indexterm" id="kab201"/>first <em>generative pre-trained transformer</em> (GPT) technologies, a type <a contenteditable="false" data-primary="large language models" data-see="LLMs" data-type="indexterm" id="id269"/><a contenteditable="false" data-primary="LLMs (large language models)" data-seealso="GPT (generative pre-trained transformer) technologies" data-type="indexterm" id="id270"/>of <em>large language model</em> (LLM), by <a contenteditable="false" data-primary="OpenAI" data-type="indexterm" id="id271"/>OpenAI, the company that soon became known for its famous <a contenteditable="false" data-primary="ChatGPT" data-type="indexterm" id="id272"/>ChatGPT chatbot, capable of having coherent and improvised conversations with humans, and its <a contenteditable="false" data-primary="DALL-E" data-type="indexterm" id="id273"/>generative <a contenteditable="false" data-primary="digital art" data-type="indexterm" id="id274"/><a contenteditable="false" data-primary="art, digital" data-type="indexterm" id="id275"/><a contenteditable="false" data-primary="generative digital art" data-type="indexterm" id="id276"/>digital art tool DALL-E. I was introduced to both technologies in 2022. I signed up for the early invitation-only beta of DALL-E and loved creating digital art that would have been previously out of my own reach (like melting bar charts in the style of Salvador Dali, and Darth Vader walking Princess Leia down the aisle as a heartfelt watercolor vignette (see <a data-type="xref" href="#ch01_figure_1_1732595607196098">Figure 1-1</a>). AI-generated sonnets, poems, parodies, and rote business communications started popping up as novelties at my workplace. My then-boss jokingly posted his ChatGPT-created professional biography which included glowing praise of who he was but was also littered with inaccuracies and accolades that he had never received. Generative and creative AI had officially arrived.</p>&#13;
&#13;
<figure><div class="figure" id="ch01_figure_1_1732595607196098"><img src="assets/lait_0101.png"/>&#13;
<h6><span class="label">Figure 1-1. </span>AI-generated art created using DALL-E (left) and MidJourney (right)</h6>&#13;
</div></figure>&#13;
&#13;
<p>The technology and structure underpinning LLMs and GPTs are in full use in both Tableau Pulse<a contenteditable="false" data-primary="Tableau Pulse" data-secondary="LLMs in" data-type="indexterm" id="id277"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="in Tableau Pulse" data-type="indexterm" id="id278"/> and Tableau Agent, the focal points of this book. Both rely heavily on LLMs. In Pulse, an LLM is used to summarize the insights and serve them up in a meaningful way. With <a contenteditable="false" data-primary="Tableau Agent " data-secondary="LLMs in" data-type="indexterm" id="id279"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="in Tableau Agent" data-type="indexterm" id="id280"/>Tableau Agent, you get direct interaction and results based on your input into a chat box. This is a very privileged position for technology to have in our domain, and as such, it’s important for analytics professionals to understand how AI technology works as much as possible. Referring to generative AI as opaque or enigmatic technology isn’t acceptable when you’re serving up information to end users to make business-critical and sometimes life-or-death decisions based on data.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Generative AI Explained" data-type="sect1"><div class="sect1" id="ch01_generative_ai_explained_1732595607206299">&#13;
<h1>Generative AI Explained</h1>&#13;
&#13;
<p>A <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="generative" data-type="indexterm" id="kab002"/>good <a contenteditable="false" data-primary="generative AI" data-type="indexterm" id="kab003"/>way to think about how <a contenteditable="false" data-primary="GPT (generative pre-trained transformer) technologies" data-startref="kab201" data-type="indexterm" id="id281"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="how function" data-type="indexterm" id="kab009"/>LLMs and GPTs function is to start by thinking about words as a set of coordinates, <a contenteditable="false" data-primary="word vectors" data-type="indexterm" id="id282"/>called <em>word vectors</em>. Similar to latitude and longitude, imagine that each word has an elaborate set of coordinates determining where it is located in the universe of words. Unlike the confines of the three dimensions that we experience in the physical world, each word in the universe of words has a very large number of dimensions (300+). Moreover, each word can be represented multiple times in the universe based on what it represents. Good examples of this are <em>homonyms</em>, words that are spelled the same but have different meanings, like the word <em>hide</em>, which can mean an animal skin pelt and can also mean to position something out of sight. It’s easy to imagine that there are two separate coordinates for these two concepts, just as there are (at least) two definitions of <em>hide</em> in a dictionary.</p>&#13;
&#13;
<p class="pagebreak-before">LLMs are trained on copious amounts of text, <a contenteditable="false" data-primary="training data" data-type="indexterm" id="id283"/>called <em>training data</em><em>,</em> which they go through to systematically assign coordinates to each word in the universe. Those coordinates have a natural proximity to other words that are closely related, and you can further imagine that those coordinates can shift slightly with each additional new passage of text received. Eventually, from a mathematical angle, there are diminishing returns on this—similar to limits in calculus: as you approach infinity, the coordinate of a word eventually settles at a final location within the word universe.</p>&#13;
&#13;
<p>Alongside the coordinates of words is the GPT, or transformer, process of the model. I like to explain this process in terms of <em>layers</em>. These layers (for example, GPT-3 from OpenAI has 96 layers) go through the text input called a <em>prompt</em> it <a contenteditable="false" data-primary="prompts" data-type="indexterm" id="id284"/>receives, categorizes the words within the prompt, and eventually aims at comprehending it. All of this is somewhat opaque, because humans haven’t dictated to the models at which layer different actions should be taken. From what has been observed, it appears that most models start by understanding the sentence syntax and role (noun/verb/adjective/pronoun) each word takes on. After those initial layers are done, more contextual layers process the prompt. I try to think of this as what humans innately and effortlessly do. When you start reading a book, you get a grounding in the words, and eventually, once you’ve read enough, your brain starts imagining the whole scene. <a data-type="xref" href="#ch01_figure_2_1732595607196141">Figure 1-2</a> demonstrates how an <a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="processing sentence, example of" data-type="indexterm" id="id285"/>LLM may process a simple sentence through the transformer layers.</p>&#13;
&#13;
<figure><div class="figure" id="ch01_figure_2_1732595607196141"><img src="assets/lait_0102.png"/>&#13;
<h6><span class="label">Figure 1-2. </span>An example of how an LLM processes a sentence</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">After the prompt is processed comes the heart of the model’s generation: a prediction of what should come next. Again, this is not unlike what humans might also innately do on their own. If someone asks you what your favorite ice-cream flavor is, at the very least you are limited by the list of flavors you are aware of (possible predictions). In addition to that list, you rely on your own experiences with each of these flavors, and that eventually leads you to a single answer. Maybe it’s hard to land on one flavor, or you might be mentally scoring each flavor to get to a result. Maybe vanilla is your favorite “safe” flavor because it’s hard to get vanilla wrong, but you experience more pleasure and delight when eating mint chocolate chip. Maybe Ben and Jerry’s Cherry Garcia is your guilty pleasure, but not every ice-cream parlor has that on the menu. It is possible to answer the question differently depending entirely on the question’s <em>context</em>: who is asking, your recent ice-cream experiences, where you are, what you are craving. The models resolve this by relying on interpreting the context of the prompt and what they have learned from the training data. And it’s important to say, the model’s training data is far beyond the scope of what any one human knows, so it quite often has a very broad understanding of the possible answers.</p>&#13;
&#13;
<p>Remember, the whole act of processing the syntax and sentence structure of the prompt through comprehension and context is completely dependent on the information the model has on hand. The predictions it is able to generate are from the information it’s been given, which has been presented in sentences written by humans. So in particular, when we apply LLMs to a narrow field like coding or analytics, the model is relying entirely on what humans have already done. This isn’t to say that there aren’t original outputs (dare I say thoughts) or creativity in the process. In fact, one of the reasons I like <a contenteditable="false" data-primary="generative digital art" data-type="indexterm" id="id286"/><a contenteditable="false" data-primary="digital art" data-type="indexterm" id="id287"/><a contenteditable="false" data-primary="art, digital" data-type="indexterm" id="id288"/>generative AI for creating art is that it is not bounded by practicality. <a contenteditable="false" data-primary="DALL-E" data-type="indexterm" id="id289"/>DALL-E may generate beautiful images that include an extra hand or misspelled word, because the prediction model has resolved to output the “best result.” A human artist would never paint an extra limb unless it was highly intentional, even if it better conveyed the idea behind the overall piece of art.</p>&#13;
&#13;
<p>Given how LLMs function, we must treat their prediction method as a double-edged sword. It can be contaminated by human bias innate in the text, limited by the information it has been given, and may predict an output that is nonsensical or factually inaccurate. It may also provide answers or results that are uniquely different from what a human (or meticulously crafted algorithm) completing the same <a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="how function" data-startref="kab009" data-type="indexterm" id="id290"/>task<a contenteditable="false" data-primary="generative AI" data-startref="kab003" data-type="indexterm" id="id291"/> may<a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="generative" data-startref="kab002" data-type="indexterm" id="id292"/> produce.</p>&#13;
</div></section>&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Risks and Considerations with AI" data-type="sect1"><div class="sect1" id="ch01_risks_and_considerations_with_ai_1732595607206365">&#13;
<h1>Risks and Considerations with AI</h1>&#13;
&#13;
<p>There <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="risks and considerations with" data-type="indexterm" id="id293"/><a contenteditable="false" data-primary="model bias" data-type="indexterm" id="id294"/>are three big areas of risk and consideration I want to discuss:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Model bias</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Model hallucination</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Worker displacement</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>It’s important to discuss these because they will inevitably come up on your organization’s journey to include generative AI tools in your analytics practice. Knowing what these issues are up front arms you with the knowledge your organization needs to make responsible and informed decisions about these tools.</p>&#13;
&#13;
<section data-pdf-bookmark="Model Bias" data-type="sect2"><div class="sect2" id="ch01_model_bias_1732595607206425">&#13;
<h2>Model Bias</h2>&#13;
&#13;
<p>You’ve already learned that a model can be biased based on the information it has been trained on or has access to. But a model can also be biased based on how the algorithm was designed. A relatively easy example to understand is an algorithm that has been tuned to over-rely on information that shows up frequently or more recently. When that is applied to the question <em>Who is the most popular singer?</em>, the result might be <em>Taylor Swift</em> based on the success of her recent <em>Eras</em> tour and how often her music is requested and served up. But looking at the question over a longer span of time could surface a different answer: <em>Frank Sinatra</em>, who released an astounding 59 studio albums over his 50+ year career. You can imagine how that question gets murkier when you’re applying it directly to data or statistical information. The question <em>What has caused the decline in sales?</em> (after looking at a chart of trending sales values) could yield different answers based on whether the model considers only the data points observed in the chart or has access to the entire history of sales.</p>&#13;
&#13;
<p>User interaction is another tricky part which will naturally influence what the model produces. If the model knows that you are the manager of the electronics department or if you’ve asked many questions that tilt toward electronics, it will likely produce an output more directly related to electronics. This could produce an answer that is accurate, but perhaps not the largest or most direct root cause of the example question <em>What has caused the decline in sales?</em> I often see this unfold in my own interactions with <a contenteditable="false" data-primary="ChatGPT" data-type="indexterm" id="id295"/>ChatGPT, where I’ll ask for something in a list format, and from then on it has trouble breaking out of a pattern of listing items, even if my follow-up requests aren’t best resolved with lists. ChatGPT also tends to use similar language or structure to what I am providing, which, again, seems logical (we humans often tend to adopt the same words and style of those around us) but works against getting an unbiased response.</p>&#13;
&#13;
<p>Finally, there are <a contenteditable="false" data-primary="social prejudices" data-type="indexterm" id="id296"/>social prejudices, <a contenteditable="false" data-primary="stereotypes" data-type="indexterm" id="id297"/>stereotypes, and <a contenteditable="false" data-primary="representation biases" data-type="indexterm" id="id298"/>representation biases to contend with. These can appear by a model parroting back the most popular prejudices that have woven their way into the body of human knowledge. A fairly benign example is the word <em>nurse</em>. When you read that word, your mental image of a nurse is likely a woman. But you could extrapolate that act out to something more analytically oriented. For example, you seek help in creating a formula to predict how sales will perform, and the model defaults to recommending a linear regression because of its popularity in analytics. In a nonharmful way, the model may simply be providing the most common way to address your data. However, the model’s response could also be negating a more sound and reasoned method that would be more appropriate for addressing your data, leaving you with a method that may not be the right fit for your situation.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Model Hallucination" data-type="sect2"><div class="sect2" id="ch01_model_hallucination_1732595607206482">&#13;
<h2>Model Hallucination</h2>&#13;
&#13;
<p><em>Model hallucination</em> is <a contenteditable="false" data-primary="model hallucination" data-type="indexterm" id="kab004"/>a<a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="risks and considerations with" data-type="indexterm" id="kab005"/> model’s return of a result that is not factually accurate or not grounded in reality. The example of the extra limb added to a generated image of a human figure mentioned earlier comes to mind, or fake legal cases used as supporting evidence in a new defense, or the production of seemingly valid numerical facts that are totally made up. All these possibilities can be generated by models, and they are especially dangerous when the task at hand is to use underlying supporting data as evidence to draw a conclusion. This is true particularly in data analytics, where there isn’t one defining fact or figure that can accurately capture the nuance and complexity of reality.</p>&#13;
&#13;
<p>You can see model hallucination play out when you ask AI to explain how it arrived at an answer. Since the whole model is opaque technology that provides predictions, the sound reasoning that you may be familiar with receiving from a human is typically not available. You can also see model hallucinations and limitations when it outputs what you know to be a wrong answer and you try to coax the AI into arriving at the right answer. <a data-type="xref" href="#ch01_figure_3_1732595607196167">Figure 1-3</a> shows a conversation I recently had with ChatGPT about the popular TV show <em>Survivor</em> and the phrase its host uses at challenges. In this exchange, the model initially provides a factually incorrect answer, including the word “guys” at the end of the phrase. Only after redirecting the model to consider more information, like that it was updated in a recent season, does it return the factually accurate answer. This isn’t hard to imagine, since there were 40 seasons of Survivor where “guys” was part of the phrasing, likely influencing the model more heavily to include the word “guys” and ignoring more recent history.</p>&#13;
&#13;
<figure><div class="figure" id="ch01_figure_3_1732595607196167"><img src="assets/lait_0103.png"/>&#13;
<h6><span class="label">Figure 1-3. </span>A discussion with ChatGPT about Jeff Probst</h6>&#13;
</div></figure>&#13;
&#13;
<p>Since <a contenteditable="false" data-primary="artificial intelligence" data-see="AI" data-type="indexterm" id="id299"/>the hallucination phenomenon tends to be more prominent in ambiguous situations—when the question itself may be unclear, the training data is insufficient, or extrapolation is required—it’s crucial to remain skeptical. These scenarios are common in the world of analytics, where understanding the reasoning behind insights is essential. Historically, human experience and intuition have been key to overcoming these challenges—an ability AI lacks, making it harder to fully trust its conclusions without <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="risks and considerations with" data-startref="kab005" data-type="indexterm" id="id300"/>deeper <a contenteditable="false" data-primary="model hallucination" data-startref="kab004" data-type="indexterm" id="id301"/>scrutiny.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Worker Displacement" data-type="sect2"><div class="sect2" id="ch01_worker_displacement_1732595607206538">&#13;
<h2>Worker Displacement</h2>&#13;
&#13;
<p>AI holds <a contenteditable="false" data-primary="worker displacement" data-type="indexterm" id="id302"/>the promise, danger, and fear of displacing humans who are doing the work. In data analytics, there is a very real sense that if an end user can interact directly with AI to get the analytics and insights they need, the utility of the analytics professional is displaced. It can be frightening to consider what your actual job is if the AI <span class="keep-together">can—with</span> a few simple text commands—do tasks like building charts, curating data sets, or constructing complex queries that have been your full-time job. One of AI’s major marketing pitches is that AI will improve efficiency, but <em>improved efficiency</em> almost always means that someone will be made redundant in the process.</p>&#13;
&#13;
<p>Although the prospect of AI taking away jobs may seem scary and inevitable, this is an area where data professionals must be loud advocates and have confidence in the body of work they have produced. The charge of data analytics hasn’t changed. The analyst’s responsibility and purpose have always been to ensure that the analytics being produced can be trusted and is useful. If anything, that purpose becomes much more necessary when computers start doing the work unsupervised.</p>&#13;
&#13;
<p>The tasks of data analytics can and will change with this shift. But remember that new tasks will usher in new jobs and responsibilities, much like the story I shared with you in the Preface of how improvements to <a contenteditable="false" data-primary="Tableau Desktop" data-secondary="data professional's responsibilities when using" data-type="indexterm" id="id303"/>Tableau Desktop’s connectivity to multiple databases changed my requests from standalone data sets to requests for direct access to databases. It is still the responsibility of the data professional to manage the analytics, manage the access, manage the information produced, and manage the data products that are available to audiences. And in an applied situation, such as when <a contenteditable="false" data-primary="Tableau Agent" data-secondary="data professional's responsibilities when using" data-type="indexterm" id="id304"/>Tableau Agent starts producing charts automatically, the analyst must still ensure that the chart constructed is accurate. The learned human skills and knowledge to make that discernment don’t disappear over time; they become even more mission critical.</p>&#13;
&#13;
<p>Finally, <a contenteditable="false" data-primary="data analytics" data-secondary="AI and audience for" data-type="indexterm" id="id305"/> a reverse force is working in humans’ favor. One of the most often-heard soundbites in data analytics is how many people are <em>underserved</em> with data and analytics. A disparity exists between the massive amounts of data being collected and the availability and comprehension of said data to workers. So if anything, it would be sound to reason that the audiences for data analytics are bound to grow as tools for easier access become available to them. And if you’re a true zealot like me, obsessed with democratizing availability of data for decision making, it is an exciting time to be the sherpa of this practice to a broader group of people.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Trusting AI" data-type="sect1"><div class="sect1" id="ch01_trusting_ai_1732595607206596">&#13;
<h1>Trusting AI</h1>&#13;
&#13;
<p>This <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="trusting" data-type="indexterm" id="kab006"/>discussion <a contenteditable="false" data-primary="risk mitigation, generative AI and" data-type="indexterm" id="id306"/><a contenteditable="false" data-primary="generative AI" data-type="indexterm" id="id307"/>of the risks associated with <a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="relationship with Tableau Cloud, Einstein Trust Layer and" data-type="indexterm" id="id308"/><a contenteditable="false" data-primary="Tableau Cloud" data-secondary="relationship with Einstein Trust Layer, LMMs and" data-type="indexterm" id="id309"/>LLMs and generative AI leads naturally to a discussion of how Tableau and Salesforce handle trust and risk mitigation. The primary arbiter of trust with these technologies is called the <a contenteditable="false" data-primary="Einstein Trust Layer" data-type="indexterm" id="id310"/><em>Einstein Trust Layer</em>. You can consider this layer as an intermediary between your enterprise data and the LLM models (at the time of writing, Tableau uses OpenAI’s GPT-3.5 Turbo) that generate summaries and respond to your prompts/queries. <a data-type="xref" href="#ch01_figure_4_1732595607196190">Figure 1-4</a> shows the relationships among Tableau Cloud, the Einstein Trust Layer, and LLM models.</p>&#13;
&#13;
<figure><div class="figure" id="ch01_figure_4_1732595607196190"><img src="assets/lait_0104.png"/>&#13;
<h6><span class="label">Figure 1-4. </span>The relationships among Tableau Cloud, Einstein Trust Layer, and LLMs</h6>&#13;
</div></figure>&#13;
&#13;
<p>The key components of the Einstein Trust Layer include the following:</p>&#13;
&#13;
<dl>&#13;
	<dt>Secure data retrieval</dt>&#13;
	<dd>&#13;
	<p>Access <a contenteditable="false" data-primary="secure data retrieval" data-type="indexterm" id="id311"/>controls and permissions are built into the Tableau platform. These controls are managed by administrators and creators and dictate who has access to data sources, dashboards, and Pulse metrics. During the process of interacting with AI, these controls are checked and verified prior to generating a response or serving up an insight.</p>&#13;
	</dd>&#13;
	<dt>Dynamic grounding</dt>&#13;
	<dd>&#13;
	<p>The LLMs <a contenteditable="false" data-primary="dynamic grounding" data-type="indexterm" id="id312"/>are enriched with context related to your business. Additionally, when Pulse metrics are served up, templates are used as guidelines for the prompt response. This process includes very clear instructions to the LLM model on not guessing, using neutral language, and handling the numerical values it receives. It also gives clear instructions on the format of the response, like the inclusion of time period comparisons.</p>&#13;
	</dd>&#13;
	<dt>Data masking</dt>&#13;
	<dd>&#13;
	<p>To <a contenteditable="false" data-primary="data masking" data-type="indexterm" id="id313"/>eliminate passing of protected information, like <em>personally identifiable information</em> (PII), sensitive values or words (such as a customer’s name) are <em>tokenized</em>. In the case of a customer’s name, the name isn’t sent to the model, but instead a substitute token or string of text is used. Upon serving up the result, the token is then translated back to the PII it represented.</p>&#13;
	</dd>&#13;
	<dt>Toxicity detection</dt>&#13;
	<dd>&#13;
	<p>This <a contenteditable="false" data-primary="toxicity detection" data-type="indexterm" id="id314"/>is the process of scoring prompt results or generated summaries for harmful information, which includes language that exhibits hate, violence, identity, or sexual content. Each response is scored, and if it is overly toxic, it will not be served back to the end user.</p>&#13;
	</dd>&#13;
	<dt>Auditing</dt>&#13;
	<dd>&#13;
	<p>An <a contenteditable="false" data-primary="auditing" data-type="indexterm" id="id315"/>audit trail is created and stored in Salesforce’s Data Cloud associated with the prompt or information processed by the LLM. This includes the user who initiated the prompt, the body of the prompt (both masked and unmasked), a categorization of whether PII was found in the prompt, the toxicity score, and any user feedback associated with the output they receive.</p>&#13;
	</dd>&#13;
	<dt>Zero retention</dt>&#13;
	<dd>&#13;
	<p>After <a contenteditable="false" data-primary="zero retention" data-type="indexterm" id="id316"/>the <a contenteditable="false" data-primary="retention" data-type="indexterm" id="id317"/>prompt is resolved, the prompt itself is purged from the system. This ensures that the LLM doesn’t retain the prompt as future information it has “learned” to influence the next response or the overall model.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Beyond the Einstein Trust Layer, all data is encrypted both at rest and in transit. Salesforce uses OpenAI’s GPT models and has agreements in place that dictate OpenAI will not store or retain any information or prompts that are fed into the models. And if you’re deeply curious, all these services are hosted on Amazon Web Services (AWS)<a contenteditable="false" data-primary="Amazon Web Services (AWS)" data-type="indexterm" id="id318"/><a contenteditable="false" data-primary="AWS (Amazon Web Services)" data-type="indexterm" id="id319"/>, which exists globally across many availability regions. For a deeper dive or explanation, watch <a href="https://oreil.ly/zExox">this Salesforce presentation</a> on the <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="trusting" data-startref="kab006" data-type="indexterm" id="id320"/>topic.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Competitors and AI" data-type="sect1"><div class="sect1" id="ch01_competitors_and_ai_1732595607206656">&#13;
<h1>Competitors and AI</h1>&#13;
&#13;
<p>As this <a contenteditable="false" data-primary="competitors to Tableau, AI in" data-type="indexterm" id="kab007"/><a contenteditable="false" data-primary="Tableau" data-secondary="competitors to, AI in" data-type="indexterm" id="kab008"/>chapter comes to a close, I want to touch on how competitors to Tableau are imagining and implementing AI into their business intelligence (BI) platforms. While many options certainly are available for comparison, I’ve chosen to focus on <a contenteditable="false" data-primary="Microsoft’s Power BI" data-see="Power BI" data-type="indexterm" id="id321"/>Microsoft Power BI (PBI) and Google Looker.</p>&#13;
&#13;
<section data-pdf-bookmark="AI in Power BI" data-type="sect2"><div class="sect2" id="ch01_ai_in_power_bi_1732595607206715">&#13;
<h2>AI in Power BI</h2>&#13;
&#13;
<p>Microsoft <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="in Power BI" data-type="indexterm" id="id322"/><a contenteditable="false" data-primary="Power BI, AI in" data-type="indexterm" id="id323"/><a contenteditable="false" data-primary="PBI (Power BI), AI in" data-type="indexterm" id="id324"/>Power BI has three distinct types of AI features:</p>&#13;
&#13;
<dl>&#13;
	<dt>Advanced analytics assisted by AI</dt>&#13;
	<dd>&#13;
	<p>This feature includes baked-in advanced analysis types available to those working in <a contenteditable="false" data-primary="advanced analytics, in Power BI" data-type="indexterm" id="id325"/>PBI without the need to code. Of note is <a contenteditable="false" data-primary="anomaly detection, in Power BI" data-type="indexterm" id="id326"/>anomaly detection, which scans data presented in a visualization or data set and provides statistical results aimed at finding anything out of the ordinary. Two advanced data exploration options, take the form of interactive charts. One is called <a contenteditable="false" data-primary="key influencers,  in Power BI" data-type="indexterm" id="id327"/><em>key influencers</em>, an AI-driven visual aimed at finding primary drivers, and the other is a decomposition tree. The <a contenteditable="false" data-primary="decomposition tree, in Power BI" data-type="indexterm" id="id328"/><em>decomposition tree</em> breaks a metric into the (likely) hierarchical dimensions and categories that exist within a data set. Sentiment analysis and forecasting are also available as no-code solutions.</p>&#13;
	</dd>&#13;
	<dt>AI assistance for analytics creation</dt>&#13;
	<dd>&#13;
	<p>Very similar to Tableau Agent, this is an interactive AI chatbot that can help developers create <a contenteditable="false" data-primary="Data Analysis Expressions (DAX)" data-type="indexterm" id="id329"/><a contenteditable="false" data-primary="DAX (Data Analysis Expressions)" data-type="indexterm" id="id330"/>Data Analysis Expressions (DAX) code, a formula expression language. There are also areas throughout Microsoft Office products, like Teams and SharePoint, which can take suggested starting visualizations or analytical questions and automatically build them in PBI.</p>&#13;
	</dd>&#13;
	<dt>Natural language query (NLQ)</dt>&#13;
	<dd>&#13;
	<p>This is <a contenteditable="false" data-primary="NLQ (natural language querying), in Power BI" data-type="indexterm" id="id331"/>nearly identical to the concept of Ask Data or interacting with Tableau Pulse. Here users can formulate questions that are turned into queries and return visualizations and results. The distinction to remember is that the input is coming from a nontechnical business user.</p>&#13;
	</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="AI in Looker" data-type="sect2"><div class="sect2" id="ch01_ai_in_looker_1732595607206772">&#13;
<h2>AI in Looker</h2>&#13;
&#13;
<p>Google’s <a contenteditable="false" data-primary="Google Looker, AI in" data-type="indexterm" id="id332"/><a contenteditable="false" data-primary="Looker, AI in" data-type="indexterm" id="id333"/>listed and upcoming AI capabilities nearly mirror what I’ve already described for Tableau and Power BI. Of note, they are focusing on the following:</p>&#13;
&#13;
<dl>&#13;
	<dt>Duet AI assistant</dt>&#13;
	<dd>&#13;
	<p>An AI <a contenteditable="false" data-primary="Duet AI assistant, Google Looker" data-type="indexterm" id="id334"/>assistant designed to help analytics creators make visualizations and access data. Notably, this includes methods to translate natural language requests into its own proprietary query language, LookML.</p>&#13;
	</dd>&#13;
	<dt>AI-powered visualizations</dt>&#13;
	<dd>&#13;
	<p>Again, <a contenteditable="false" data-primary="visualizations, in Google Looker" data-type="indexterm" id="id335"/>this is technology designed for a business user to automatically create visualizations based on information and data they have access to.</p>&#13;
	</dd>&#13;
	<dt>Summarized insights</dt>&#13;
	<dd>&#13;
	<p>While <a contenteditable="false" data-primary="insights" data-secondary="in Google Looker" data-type="indexterm" id="id336"/>there aren’t many specifics on how or where this will be implemented, this is fundamentally similar to the insight summaries served up in Tableau Pulse.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>Of note in discussing these competitors are the AI models that are the engines. Microsoft has a large stake in OpenAI and utilizes that company’s models or variations of its models. Google, on the other hand, leans on its own <a contenteditable="false" data-primary="Gemini LLM" data-type="indexterm" id="id337"/>proprietary LLMs—<a contenteditable="false" data-primary="competitors to Tableau, AI in" data-startref="kab007" data-type="indexterm" id="id338"/>namely, <a contenteditable="false" data-primary="Tableau" data-secondary="competitors to, AI in" data-startref="kab008" data-type="indexterm" id="id339"/>Gemini (formerly Bard).</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch01_summary_1732595607206829">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter, you’ve learned about AI in the analytics space. Key applications of AI in analytics include the following:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>More sophisticated analysis methods, like predictive multivariate models and sentiment analysis (scoring of text as positive/negative/neutral)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Natural language processing and querying (NLP and NLQ), which allow users to ask questions in normal (human) language and receive analytical results</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Generative AI that summarizes information and surfaces interesting insights to end users</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>AI assistance that makes building analytics easier</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>To help you understand the latest AI tools in this book, the chapter unpacked how LLMs function:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>They consume large amounts of text and categorize words across a copious number of dimensions.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Prompts and text inputs are processed in layers called <em>transformers</em>. These models tend to first resolve sentence structure and ultimately reach comprehension and context of the input.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Humans control what training data is fed into the models as well as guidelines for processing that information. However, much of what occurs within the LLM is unknown and left to the model.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>It is important to remember that LLMs and generative AI are essentially very sophisticated prediction engines. They rely heavily on the training data they receive. The output is influenced in how the LLM was constructed and whatever instructions it was given. Additionally, there are some inherent risks to be aware of when working with generative AI:</p>&#13;
&#13;
<dl>&#13;
	<dt>Model bias</dt>&#13;
	<dd>&#13;
	<p>An LLM can have bias built in from the training data it receives. The model relies on information that has already been recorded by humans. In particular, there can be biases around societal issues (like diversity), and responses can be heavily influenced by user input.</p>&#13;
	</dd>&#13;
	<dt>Model hallucination</dt>&#13;
	<dd>&#13;
	<p>An LLM could respond with a factually made up or erroneous result. This typically occurs in ambiguous situations or where extrapolation is involved—both of which occur frequently in analytics.</p>&#13;
	</dd>&#13;
	<dt>Worker displacement</dt>&#13;
	<dd>&#13;
	<p>Although it can cause worry to consider how analytics roles may change, the bedrock of analytical skills you hold are necessary to ensure that the AI-derived results are grounded in sound analytical reasoning.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>Tableau and Salesforce aim at creating trust while using AI-powered tools via the Einstein Trust Layer. This layer acts as a go-between for enterprise data and the LLM engines processing prompts. The six facets of this trust layer are as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Secure data retrieval</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Dynamic grounding</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Data masking</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Toxicity detection</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Auditing</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Zero retention</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>And finally, two major competitors of Tableau, Microsoft Power BI and Google Looker, show common themes in the utility of AI in the analytics space, namely: AI assistance in analytics creation, advanced mathematical analyses, summarized insights, and natural language query.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch02.html#ch02_getting_started_with_tableau_pulse_1732595607647464">Chapter 2</a>, I’ll show you how to get started with Tableau Pulse. I’ll break down how to activate it in your Tableau environment, how to build your first metric, and more. By the end of the chapter, you’ll have the confidence and knowledge to start deploying your own metrics for business users to start following. And with the history of Tableau and knowledge of LLMs you received here, you will be able to accurately convey AI’s role and the risks when using it in Tableau.</p>&#13;
</div></section>&#13;
</div></section></body></html>