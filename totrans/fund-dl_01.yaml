- en: Chapter 1\. Fundamentals of Linear Algebra for Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。深度学习的线性代数基础
- en: In this chapter, we cover important prerequisite knowledge that will motivate
    our discussion of deep learning techniques in the main text and the optional sidebars
    at the end of select chapters. Deep learning has recently experienced a renaissance,
    both in academic research and in the industry. It has pushed the limits of machine
    learning by leaps and bounds, revolutionizing fields such as computer vision and
    natural language processing. However, it is important to remember that deep learning
    is, at its core, a culmination of achievements in fields such as calculus, linear
    algebra, and probability. Although there are deeper connections to other fields
    of mathematics, we focus on the three listed here to help us broaden our perspective
    before diving into deep learning. These fields are key to unlocking both the big
    picture of deep learning and the intricate subtleties that make it as exciting
    as it is. In this first chapter on background, we cover the fundamentals of linear
    algebra.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了重要的先决知识，这些知识将激发我们在主文本和选定章节末尾的可选侧栏中讨论深度学习技术。深度学习最近在学术研究和工业领域都经历了一次复兴。它通过飞跃和飞跃地推动了机器学习的极限，革新了计算机视觉和自然语言处理等领域。然而，重要的是要记住，深度学习在其核心上是对诸如微积分、线性代数和概率等领域成就的总结。尽管与其他数学领域有更深入的联系，但我们专注于这里列出的三个领域，以帮助我们在深入研究深度学习之前拓宽我们的视野。这些领域对于揭示深度学习的整体图景和使其变得如此令人兴奋的微妙细节至关重要。在这个关于背景的第一章中，我们涵盖了线性代数的基础知识。
- en: Data Structures and Operations
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据结构和操作
- en: The most important data structure in linear algebra (whenever we reference linear
    algebra in this text, we refer to its applied variety) is arguably the *matrix*,
    a 2D array of numbers where each entry can be indexed via its row and column.
    Think of an Excel spreadsheet, where you have offers from Company X and Company
    Y as two rows, and the columns represent some characteristic of each offer, such
    as starting salary, bonus, or position, as shown in [Table 1-1](#first_table).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，每当我们提到线性代数时，我们指的是其应用领域中最重要的数据结构可能是*矩阵*，一个由数字组成的二维数组，其中每个条目可以通过其行和列进行索引。想象一下Excel电子表格，其中Company
    X和Company Y的报价作为两行，列代表每个报价的某些特征，比如起薪、奖金或职位，如[表1-1](#first_table)所示。
- en: Table 1-1\. Excel spreadsheet
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 表1-1。Excel电子表格
- en: '|   | Company X | Company Y |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '|   | Company X | Company Y |'
- en: '| Salary | $50,000 | $40,000 |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| 薪水 | $50,000 | $40,000 |'
- en: '| Bonus | $5,000 | $7,500 |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| 奖金 | $5,000 | $7,500 |'
- en: '| Position | Engineer | Data Scientist |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 职位 | 工程师 | 数据科学家 |'
- en: The table format is especially suited to keep track of such data, where you
    can index by row and column to find, for example, Company X’s starting position.
    Matrices, similarly, are a multipurpose tool to hold all kinds of data, where
    the data we work in this book is of numerical form. In deep learning, matrices
    are often used to represent both datasets and weights in a neural network. A dataset,
    for example, has many individual data points with any number of associated features.
    A lizard dataset might contain information on length, weight, speed, age, and
    other important attributes. We can represent this intuitively as a matrix or table,
    where each row represents an individual lizard, and each column represents a lizard
    feature, such as age. However, as opposed to [Table 1-1](#first_table), the matrix
    stores only the numbers and assumes that the user has kept track of which rows
    correspond to which data points, which columns correspond to which feature, and
    what the units are for each feature, as you can see in [Figure 1-1](#a_comparison_of_tables_and_matricies).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 表格格式特别适合跟踪这种数据，您可以通过行和列进行索引，以找到例如Company X的起始职位。类似地，矩阵是一个多功能工具，可以保存各种数据，我们在本书中处理的数据是数值形式的。在深度学习中，矩阵经常用于表示神经网络中的数据集和权重。例如，一个数据集有许多具有任意数量相关特征的个体数据点。一个蜥蜴数据集可能包含长度、重量、速度、年龄和其他重要属性的信息。我们可以直观地将其表示为矩阵或表格，其中每行代表一个单独的蜥蜴，每列代表一个蜥蜴特征，比如年龄。然而，与[表1-1](#first_table)不同，矩阵仅存储数字，并假定用户已经跟踪了哪些行对应哪些数据点，哪些列对应哪些特征，以及每个特征的单位是什么，如您可以在[图1-1](#a_comparison_of_tables_and_matricies)中看到的。
- en: '![](Images/fdl2_0101.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0101.png)'
- en: Figure 1-1\. A comparison of tables and matrices
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1。表格和矩阵的比较
- en: On the right side, we have a matrix, where it’s assumed, for example, that the
    age of each lizard is in years, and Komodo Ken weighs a whopping 50 kilograms!
    But why even work with matrices when tables clearly give the user more information?
    Well, in linear algebra and even deep learning, operations such as multiplication
    and addition are done on the tabular data itself, but such operations can only
    be computed efficiently when the data is in solely numerical format.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧，我们有一个矩阵，例如，假设每只蜥蜴的年龄是以年为单位，Komodo Ken体重惊人的50公斤！但是为什么要使用矩阵，当表格明显给用户更多信息？在线性代数甚至深度学习中，诸如乘法和加法之类的操作是在表格数据本身上进行的，但只有当数据仅以数字格式存在时，这些操作才能高效计算。
- en: Much of the work in linear algebra centers on the emergent properties of matrices,
    which are especially interesting when the matrix has certain base attributes,
    and operations on these data structures. *Vectors*, which can be seen as a subset
    type of matrices, are a 1D array of numbers. This data structure can be used to
    represent an individual data point or the weights in a linear regression, for
    example. We cover properties of matrices and vectors as well as operations on
    both.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数中的许多工作集中在矩阵的 emergent 属性上，当矩阵具有某些基本属性和对这些数据结构的操作时，这些属性尤其有趣。*向量*，可以看作是矩阵的一种子类型，是一个由数字组成的一维数组。这种数据结构可以用来表示一个单独的数据点或线性回归中的权重，例如。我们涵盖矩阵和向量的属性以及对两者的操作。
- en: Matrix Operations
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵运算
- en: Matrices can be added, subtracted, and multiplied—there is no division of matrices,
    but there exists a similar concept called *inversion*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵可以相加、相减和相乘——矩阵没有除法，但存在一个类似的概念称为*求逆*。
- en: When indexing a matrix, we use a tuple, where the first index represents the
    row number and the second index represents the column number. To add two matrices
    *A* and *B*, one loops through each index *(i,j)* of the two matrices, sums the
    two entries at the current index, and places that result in the same index *(i,j)*
    of a new matrix *C*, as can be seen in [Figure 1-2](#matrix_addition).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引矩阵时，我们使用一个元组，其中第一个索引表示行号，第二个索引表示列号。要将两个矩阵 *A* 和 *B* 相加，需要循环遍历两个矩阵的每个索引 *(i,j)*，将当前索引处的两个条目相加，并将结果放在新矩阵
    *C* 的相同索引 *(i,j)* 处，如[图1-2](#matrix_addition)所示。
- en: '![](Images/fdl2_0102.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0102.png)'
- en: Figure 1-2\. Matrix addition
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2. 矩阵加法
- en: This algorithm implies that we can’t add two matrices of different shapes, since
    indices that exist in one matrix wouldn’t exist in the other. It also implies
    that the final matrix *C* is of the same shape as *A* and *B*. In addition to
    adding matrices, we can multiply a matrix by a scalar. This involves simply taking
    the scalar and multiplying each of the entries of the matrix by it (the shape
    of the resultant matrix stays constant), as depicted in [Figure 1-3](#scalar_matrix_multiplication).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法意味着我们不能将不同形状的两个矩阵相加，因为一个矩阵中存在的索引在另一个矩阵中不存在。它还意味着最终的矩阵 *C* 与 *A* 和 *B* 的形状相同。除了相加矩阵，我们还可以将矩阵乘以一个标量。这只需要将标量乘以矩阵的每个元素（结果矩阵的形状保持不变），如[图1-3](#scalar_matrix_multiplication)所示。
- en: '![](Images/fdl2_0103.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0103.png)'
- en: Figure 1-3\. Scalar-matrix multiplication
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3. 标量-矩阵乘法
- en: These two operations, addition of matrices and scalar-matrix multiplication,
    lead us directly to matrix subtraction, since computing *A* – *B* is the same
    as computing the matrix addition *A* + (–*B*), and computing *–B* is the product
    of a scalar –1 and the matrix *B.*
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个操作，矩阵相加和标量-矩阵乘法，直接导致矩阵减法，因为计算 *A* - *B* 相当于计算矩阵相加 *A* + (-*B*)，而计算 *-B* 是标量
    -1 与矩阵 *B* 的乘积。
- en: 'Multiplying two matrices starts to get interesting. For reasons beyond the
    scope of this text (motivations in a more theoretical flavor of linear algebra
    where matrices represent linear transformations), we define the matrix product 
    <math alttext="upper A dot upper B"><mrow><mi>A</mi> <mo>·</mo> <mi>B</mi></mrow></math>
     as:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个矩阵相乘开始变得有趣。出于本文本文的范围之外的原因（在矩阵代表线性变换的更理论化的线性代数中的动机），我们将矩阵乘积 <math alttext="upper
    A dot upper B"><mrow><mi>A</mi> <mo>·</mo> <mi>B</mi></mrow></math> 定义为：
- en: Equation 1-1\. Matrix multiplication formula
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程1-1. 矩阵乘法公式
- en: <math alttext="left-parenthesis upper A dot upper B right-parenthesis Subscript
    i comma j Baseline equals sigma-summation Underscript k prime equals 1 Overscript
    k Endscripts upper A Subscript i comma k Sub Superscript prime Subscript Baseline
    upper B Subscript k prime comma j"><mrow><msub><mrow><mo>(</mo><mi>A</mi><mo>·</mo><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><msup><mi>k</mi> <mo>'</mo></msup> <mo>=</mo><mn>1</mn></mrow> <mi>k</mi></munderover>
    <msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><msup><mi>k</mi> <mo>'</mo></msup></mrow></msub>
    <msub><mi>B</mi> <mrow><msup><mi>k</mi> <mo>'</mo></msup> <mo>,</mo><mi>j</mi></mrow></msub></mrow></math>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="left-parenthesis upper A dot upper B right-parenthesis Subscript
    i comma j Baseline equals sigma-summation Underscript k prime equals 1 Overscript
    k Endscripts upper A Subscript i comma k Sub Superscript prime Subscript Baseline
    upper B Subscript k prime comma j"><mrow><msub><mrow><mo>(</mo><mi>A</mi><mo>·</mo><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><msup><mi>k</mi> <mo>'</mo></msup> <mo>=</mo><mn>1</mn></mrow> <mi>k</mi></munderover>
    <msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><msup><mi>k</mi> <mo>'</mo></msup></mrow></msub>
    <msub><mi>B</mi> <mrow><msup><mi>k</mi> <mo>'</mo></msup> <mo>,</mo><mi>j</mi></mrow></msub></mrow></math>
- en: In simpler terms, this means that the value at the index *(i,j) *of  <math alttext="upper
    A dot upper B"><mrow><mi>A</mi> <mo>·</mo> <mi>B</mi></mrow></math>  is the sum
    of the product of the entries in the *i*th row of *A* with those of the *j*th
    column of *B.* [Figure 1-4](#matrix_multiplication) is an example of matrix multiplication.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，这意味着在 <math alttext="upper A dot upper B"><mrow><mi>A</mi> <mo>·</mo> <mi>B</mi></mrow></math>
    的索引 *(i,j)* 处的值是矩阵 *A* 的第 *i* 行与矩阵 *B* 的第 *j* 列的乘积之和。[图1-4](#matrix_multiplication)
    是矩阵乘法的一个例子。
- en: '![](Images/fdl2_0104.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0104.png)'
- en: Figure 1-4\. Matrix multiplication
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4. 矩阵乘法
- en: It follows that the rows of *A *and the columns of *B *must have the same length,
    so two matrices can be multiplied only if the shapes align. We use the term *dimension*
    to formally represent what we have referred to so far as *shape*: i.e., *A *is
    of dimension *m*by *k*, meaning it has *m *rows and *k* columns, and *B *is of
    dimension *k *by *n*. If this weren’t the case, the formula for matrix multiplication
    would give us an indexing error. The dimension of the product is *m*by *n,* signifying
    an entry for every pair of rows in *A *and columns in *B*. This is the computational
    way of thinking about matrix multiplication, and it doesn’t lend itself well to
    theoretical interpretation. We’ll call [Equation 1-1](#mmformula) the *dot product
    interpretation of matrix multiplication,*​ which will make more sense after reading
    [“Vector Operations”](#vector-ops-sect).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由此可知，*A* 的行和 *B* 的列必须具有相同的长度，因此只有当形状对齐时，两个矩阵才能相乘。我们使用术语 *维度* 正式表示我们迄今为止称之为 *形状*
    的内容：即，*A* 的维度为 *m* 行 *k* 列，意味着它有 *m* 行和 *k* 列，而 *B* 的维度为 *k* 行 *n* 列。如果不是这种情况，矩阵乘法的公式将导致索引错误。乘积的维度为
    *m* 行 *n* 列，表示 *A* 的每一对行和 *B* 的每一列都有一个条目。这是关于矩阵乘法的计算思维方式，不太适合理论解释。我们将[方程1-1](#mmformula)称为*矩阵乘法的点积解释*，在阅读完[“向量运算”](#vector-ops-sect)之后会更有意义。
- en: Note that matrix multiplication is not commutative, i.e., <math alttext="upper
    A dot upper B not-equals upper B dot upper A"><mrow><mi>A</mi> <mo>·</mo> <mi>B</mi>
    <mo>≠</mo> <mi>B</mi> <mo>·</mo> <mi>A</mi></mrow></math> . Of course, if we were
    to take a matrix *A* that is 2 by 3 and a matrix *B* that is 3 by 5, for example,
    by the rules of matrix multiplication,  <math alttext="upper B dot upper A"><mrow><mi>B</mi>
    <mo>·</mo> <mi>A</mi></mrow></math> doesn’t exist. However, even if the product
    were defined due to both matrices being *square,* where square means that the
    matrix has an equal number of rows and columns, the two products will not be the
    same (this is an exercise for you to explore on your own). However, matrix multiplication
    is associative, i.e., <math alttext="upper A dot left-parenthesis upper B plus
    upper C right-parenthesis equals upper A dot upper B plus upper A dot upper C"><mrow><mi>A</mi>
    <mo>·</mo> <mo>(</mo> <mi>B</mi> <mo>+</mo> <mi>C</mi> <mo>)</mo> <mo>=</mo> <mi>A</mi>
    <mo>·</mo> <mi>B</mi> <mo>+</mo> <mi>A</mi> <mo>·</mo> <mi>C</mi></mrow></math>
    .
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，矩阵乘法不是可交换的，即，<math alttext="upper A dot upper B not-equals upper B dot upper
    A"><mrow><mi>A</mi> <mo>·</mo> <mi>B</mi> <mo>≠</mo> <mi>B</mi> <mo>·</mo> <mi>A</mi></mrow></math>。当然，如果我们取一个2乘3的矩阵*A*和一个3乘5的矩阵*B*，根据矩阵乘法的规则，例如，<math
    alttext="upper B dot upper A"><mrow><mi>B</mi> <mo>·</mo> <mi>A</mi></mrow></math>不存在。然而，即使由于两个矩阵都是*方阵*，即矩阵的行数和列数相等，乘积被定义了，这两个乘积也不会相同（这是一个让你自己探索的练习）。然而，矩阵乘法是结合的，即，<math
    alttext="upper A dot left-parenthesis upper B plus upper C right-parenthesis equals
    upper A dot upper B plus upper A dot upper C"><mrow><mi>A</mi> <mo>·</mo> <mo>(</mo>
    <mi>B</mi> <mo>+</mo> <mi>C</mi> <mo>)</mo> <mo>=</mo> <mi>A</mi> <mo>·</mo> <mi>B</mi>
    <mo>+</mo> <mi>A</mi> <mo>·</mo> <mi>C</mi></mrow></math>。
- en: 'Let’s delve into matrix multiplication a bit further. After some algebraic
    manipulation, we can see that another way to formulate matrix multiplication is:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步探讨矩阵乘法。经过一些代数运算，我们可以看到另一种表达矩阵乘法的方式是：
- en: <math alttext="left-parenthesis upper A dot upper B right-parenthesis Subscript
    dot comma j Baseline equals upper A dot upper B Subscript dot comma j"><mrow><msub><mrow><mo>(</mo><mi>A</mi><mo>·</mo><mi>B</mi><mo>)</mo></mrow>
    <mrow><mo>·</mo><mo>,</mo><mi>j</mi></mrow></msub> <mo>=</mo> <mi>A</mi> <mo>·</mo>
    <msub><mi>B</mi> <mrow><mo>·</mo><mo>,</mo><mi>j</mi></mrow></msub></mrow></math>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="left-parenthesis upper A dot upper B right-parenthesis Subscript
    dot comma j Baseline equals upper A dot upper B Subscript dot comma j"><mrow><msub><mrow><mo>(</mo><mi>A</mi><mo>·</mo><mi>B</mi><mo>)</mo></mrow>
    <mrow><mo>·</mo><mo>,</mo><mi>j</mi></mrow></msub> <mo>=</mo> <mi>A</mi> <mo>·</mo>
    <msub><mi>B</mi> <mrow><mo>·</mo><mo>,</mo><mi>j</mi></mrow></msub></mrow></math>
- en: This states that the *j*th column of the product  <math alttext="upper A dot
    upper B"><mrow><mi>A</mi> <mo>·</mo> <mi>B</mi></mrow></math>  is the matrix product
    of *A *and the *j*th column of *B*, a vector. We’ll call this *column vector interpretation
    of matrix multiplication, *as can be seen in [Figure 1-5](#matrix_multiplication_another_view).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明乘积<math alttext="upper A dot upper B"><mrow><mi>A</mi> <mo>·</mo> <mi>B</mi></mrow></math>的第*j*列是矩阵*A*和向量*B*的第*j*列的矩阵乘积。我们将这称为*矩阵乘法的列向量解释*，如[图1-5](#matrix_multiplication_another_view)所示。
- en: '![](Images/fdl2_0105.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0105.png)'
- en: 'Figure 1-5\. Matrix multiplication: another view'
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-5. 矩阵乘法：另一种视角
- en: In a later section, we cover matrix-vector multiplication and different ways
    to think about this computation, which leads to more exciting properties regarding
    matrices.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的部分，我们将介绍矩阵-向量乘法以及思考这种计算的不同方式，这将导致关于矩阵更令人兴奋的性质。
- en: One of the most important matrices in linear algebra is the *identity matrix*,
    which is a square matrix with 1s along the main diagonal and 0s in every other
    entry. This matrix is usually denoted as *I.* When computing the product of *I *with
    any other matrix *A, *the result is always *A—*thus its name, the identity matrix.
    Try multiplying a few matrices of your choosing with the appropriate-sized identity
    matrix to see why this is the case.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数中最重要的矩阵之一是*单位矩阵*，它是一个方阵，主对角线上是1，其他位置都是0。这个矩阵通常表示为*I*。当计算*I*与任何其他矩阵*A*的乘积时，结果总是*A*，因此得名为单位矩阵。尝试将您选择的几个矩阵与适当大小的单位矩阵相乘，看看为什么会这样。
- en: As noted at the beginning of the section, there is no such division operation
    for matrices, but there is the concept of inversion. The inverse of matrix *A*
    is matrix *B*, such that *AB = BA = I, *the identity matrix (similar in idea to
    a number’s reciprocal—when dividing by a number on both sides of an equation,
    we can also think of this operation as multiplying both sides by its reciprocal).
    If such a *B* exists, we denote it as  <math alttext="upper A Superscript negative
    1"><msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> . From this
    definition, we know that *A *must be, at the very least, a square matrix since
    we are able to multiply *A *on either side by the same matrix <math alttext="upper
    A Superscript negative 1"><msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    , as you can see in [Figure 1-6](#matrix_inversion). Matrix inversion is deeply
    tied to other properties of matrices that we will discuss soon, which are the
    backbone of fundamental data science techniques. These techniques influenced their
    more complex neural variants, which researchers still use to this day.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节开头所指出的，矩阵没有除法运算，但有求逆的概念。矩阵*A*的逆矩阵是矩阵*B*，使得*AB = BA = I*，即单位矩阵（类似于数字的倒数——当在等式两边除以一个数字时，我们也可以将这个操作看作是将两边乘以它的倒数）。如果这样的*B*存在，我们将其表示为<math
    alttext="upper A Superscript negative 1"><msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>。根据这个定义，我们知道*A*至少必须是一个方阵，因为我们能够将*A*在两边乘以相同的矩阵<math
    alttext="upper A Superscript negative 1"><msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>，如[图1-6](#matrix_inversion)所示。矩阵求逆与我们即将讨论的矩阵的其他性质密切相关，这些性质是基本数据科学技术的基础。这些技术影响了更复杂的神经变体，研究人员至今仍在使用。
- en: '![](Images/fdl2_0106.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0106.png)'
- en: Figure 1-6\. Matrix inversion
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6. 矩阵求逆
- en: When trying to solve an equation such as  <math alttext="upper A x equals b"><mrow><mi>A</mi>
    <mi>x</mi> <mo>=</mo> <mi>b</mi></mrow></math>  for *x, *we would multiply both
    sides on the left by  <math alttext="upper A Superscript negative 1"><msup><mi>A</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>  to get  <math alttext="x equals
    upper A Superscript negative 1 Baseline b"><mrow><mi>x</mi> <mo>=</mo> <msup><mi>A</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi>b</mi></mrow></math>  if *A *is invertible.
    There exists another necessary condition for *A *to be invertible, which we’ll
    discuss later.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试解方程 <math alttext="upper A x equals b"><mrow><mi>A</mi> <mi>x</mi> <mo>=</mo>
    <mi>b</mi></mrow></math> 时，我们会将两边左乘以 <math alttext="upper A Superscript negative
    1"><msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> 来得到 <math
    alttext="x equals upper A Superscript negative 1 Baseline b"><mrow><mi>x</mi>
    <mo>=</mo> <msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi>b</mi></mrow></math>
    如果*A*是可逆的。*A*可逆还有另一个必要条件，我们稍后会讨论。
- en: Vector Operations
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量操作
- en: Vectors can be seen as a subset of matrices, so a lot of the operations follow
    from the properties of addition, subtraction, multiplication, and inversion. However,
    there is some vector-specific terminology we should cover. When a vector is of
    dimension 1 × *n*, we call this vector a *row vector*, and when the vector is
    of dimension *n* × 1, we call it a *column vector*. When taking matrix product
    of a row vector and a column vector, we can see that the result is a single number—we
    call this operation the *dot product.* [Figure 1-7](#dot_product) is an example
    of the dot product of two vectors.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 向量可以看作是矩阵的一个子集，因此很多操作都遵循加法、减法、乘法和求逆的性质。然而，有一些特定于向量的术语我们应该了解。当一个向量的维度为1×*n*时，我们称这个向量为*行向量*，当向量的维度为*n*×1时，我们称其为*列向量*。当取行向量和列向量的矩阵乘积时，结果是一个单个数字，我们称这个操作为*点积*。[图1-7](#dot_product)
    是两个向量的点积的一个示例。
- en: '![](Images/fdl2_0107.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0107.png)'
- en: Figure 1-7\. Dot product
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-7\. 点积
- en: Now the reason for the name dot product interpretation of matrix multiplication
    might make more sense. Looking back at [Equation 1-1](#mmformula), we see that
    every entry in the matrix product  <math alttext="left-parenthesis upper A dot
    upper B right-parenthesis Subscript i comma j"><msub><mrow><mo>(</mo><mi>A</mi><mo>·</mo><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math> is just the dot product
    of the corresponding row <math alttext="upper A Subscript i comma dot"><msub><mi>A</mi>
    <mrow><mi>i</mi><mo>,</mo><mo>·</mo></mrow></msub></math> and the corresponding
    column  <math alttext="upper B Subscript dot comma j"><msub><mi>B</mi> <mrow><mo>·</mo><mo>,</mo><mi>j</mi></mrow></msub></math>
    .
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在矩阵乘法的点积解释的名称可能更有意义了。回顾[方程1-1](#mmformula)，我们看到矩阵乘积 <math alttext="left-parenthesis
    upper A dot upper B right-parenthesis Subscript i comma j"><msub><mrow><mo>(</mo><mi>A</mi><mo>·</mo><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math> 中的每个条目只是对应行 <math alttext="upper
    A Subscript i comma dot"><msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mo>·</mo></mrow></msub></math>
    和对应列 <math alttext="upper B Subscript dot comma j"><msub><mi>B</mi> <mrow><mo>·</mo><mo>,</mo><mi>j</mi></mrow></msub></math>
    的点积。
- en: When the dot product of two vectors is 0, we term the two vectors to be *orthogonal*.
    Orthogonality is a generalization of perpendicularity to any dimension, even those
    far beyond the ones we can imagine. You can check in the 2D case, for example,
    that any two vectors are perpendicular if and only if (also termed *iff*) they
    have a dot product of 0.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个向量的点积为0时，我们称这两个向量为*正交*。正交性是垂直性在任意维度上的推广，甚至超出我们能想象的维度。例如，在二维情况下，您可以检查任意两个向量是否垂直，当且仅当它们的点积为0时。
- en: When we instead take the matrix product of a column vector and a row vector,
    we see that the result is quite surprisingly a matrix! This is termed the *outer
    product*. [Figure 1-8](#outer_product) is the outer product of the same two vectors
    from the dot product example, except their roles as row and column vectors have
    been reversed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们取列向量和行向量的矩阵乘积时，我们会发现结果竟然是一个矩阵！这被称为*外积*。[图1-8](#outer_product) 是与点积示例中相同的两个向量的外积，只是它们作为行向量和列向量的角色已经颠倒。
- en: '![](Images/fdl2_0108.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0108.png)'
- en: Figure 1-8\. Outer product
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-8\. 外积
- en: Matrix-Vector Multiplication
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵-向量乘法
- en: 'When multiplying a matrix *A* and a vector *v*, we can again do this via the
    dot product interpretation of matrix multiplication, as described previously.
    However, if we instead manipulate the expression slightly, we’ll see that another
    way to formulate this product is:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当将矩阵*A*和向量*v*相乘时，我们可以再次通过矩阵乘法的点积解释来进行，如前所述。然而，如果我们稍微调整表达式，我们会发现另一种表达这个乘积的方法是：
- en: <math alttext="upper A v equals sigma-summation Underscript j Endscripts v Subscript
    j Baseline upper A Subscript dot comma j"><mrow><mi>A</mi> <mi>v</mi> <mo>=</mo>
    <munder><mo>∑</mo> <mi>j</mi></munder> <msub><mi>v</mi> <mi>j</mi></msub> <msub><mi>A</mi>
    <mrow><mo>·</mo><mo>,</mo><mi>j</mi></mrow></msub></mrow></math>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A v equals sigma-summation Underscript j Endscripts v Subscript
    j Baseline upper A Subscript dot comma j"><mrow><mi>A</mi> <mi>v</mi> <mo>=</mo>
    <munder><mo>∑</mo> <mi>j</mi></munder> <msub><mi>v</mi> <mi>j</mi></msub> <msub><mi>A</mi>
    <mrow><mo>·</mo><mo>,</mo><mi>j</mi></mrow></msub></mrow></math>
- en: Where each  <math alttext="v Subscript j"><msub><mi>v</mi> <mi>j</mi></msub></math>
     is a constant to be multiplied with its corresponding column of *A.* [Figure 1-9](#matrix_vector_multiplication)
    is an example of this method in action.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 <math alttext="v Subscript j"><msub><mi>v</mi> <mi>j</mi></msub></math> 都是一个常数，将其与对应的*A*列相乘。[图1-9](#matrix_vector_multiplication)
    是这种方法的一个示例。
- en: '![](Images/fdl2_0109.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0109.png)'
- en: Figure 1-9\. Matrix-vector multiplication
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9\. 矩阵-向量乘法
- en: This section introduced matrix and vector operations, which are fundamental
    to understanding the inner workings of a neural network. In the next section,
    we will use our knowledge of matrix and vector operations to concretely define
    some matrix properties, which serve as the basis for important data science and
    deep learning techniques.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了矩阵和向量操作，这些操作对于理解神经网络的内部工作至关重要。在下一节中，我们将利用我们对矩阵和向量操作的知识来明确定义一些矩阵属性，这些属性是重要的数据科学和深度学习技术的基础。
- en: The Fundamental Spaces
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本空间
- en: In this section, we will formally discuss some important matrix properties and
    provide some background knowledge on key algorithms in deep learning, such as
    representation learning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将正式讨论一些重要的矩阵属性，并提供一些关于深度学习中关键算法的背景知识，如表示学习。
- en: The Column Space
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列空间
- en: Consider the set of all possible vectors *v *and their products <math alttext="upper
    A v"><mrow><mi>A</mi> <mi>v</mi></mrow></math> . We term this the *column space* of
    *A, *or *C(A). *The term column space is used because *C(A) *represents all possible
    linear combinations of the columns of *A, *where a linear combination of vectors
    is a sum of constant scalings of each vector. The constant scaling for each column
    vector of *A *is determined by the choice of *v,* as we just saw in the previous
    section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑所有可能向量 *v* 及其乘积 <math alttext="upper A v"><mrow><mi>A</mi> <mi>v</mi></mrow></math>
    的集合。我们称这个集合为 *A* 的*列空间*，或者 *C(A)*。之所以称之为列空间，是因为 *C(A)* 代表了 *A* 的所有列的所有可能线性组合，其中向量的线性组合是每个向量的常数缩放的和。对于
    *A* 的每个列向量，常数缩放由 *v* 的选择确定，正如我们在前一节中所看到的。
- en: The column space is an example of a *vector space,* which is the space defined
    by a list of vectors and all possible linear combinations of this collection.
    Properties for formally defining a vector space pop up directly from this intuition.
    For example, if a set of vectors is a vector space, then the vector that arises
    from multiplying any vector in the space by a scalar must also be in the space.
    In addition, if we were to add any two vectors in the space, the result should
    still be in the space. In both of these operations, the vectors we start with
    are known to be in the vector space, and thus can be formulated as linear combinations
    of the original list. By performing scalar multiplication or addition on the vectors
    in question, we are just computing linear combinations of linear combinations,
    which are still linear combinations, as can be seen in [Figure 1-10](#we_can_see_that_the_sum_or_linear_combination).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 列空间是*向量空间*的一个例子，它是由向量列表和该集合的所有可能线性组合定义的空间。从这种直觉中直接出现了形式上定义向量空间的属性。例如，如果一组向量是一个向量空间，那么通过将空间中的任何向量乘以标量得到的向量也必须在该空间中。此外，如果我们将空间中的任意两个向量相加，结果仍应在该空间中。在这两种操作中，我们开始的向量已知在向量空间中，因此可以被公式化为原始列表的线性组合。通过对所讨论的向量进行标量乘法或加法，我们只是计算线性组合的线性组合，这仍然是线性组合，正如在[图1-10](#we_can_see_that_the_sum_or_linear_combination)中所示。
- en: '![](Images/fdl2_0110.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0110.png)'
- en: Figure 1-10\. The sum, or linear combination, of the two linear combinations
    3a and 2b + 2c is still a linear combination of the original vectors a, b, and
    c
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-10。两个线性组合 3a 和 2b + 2c 的和，仍然是原始向量 a、b 和 c 的线性组合
- en: We term these key properties of vector spaces *closed under scalar multiplication*
    and *closed under addition.* If a set of vectors doesn’t always satisfy either
    of these properties, then the set clearly doesn’t contain all possible linear
    combinations of the original list and is not a vector space.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这些向量空间的关键属性为*标量乘法封闭*和*加法封闭*。如果一组向量不满足这些属性中的任何一个，那么该集合显然不包含原始列表的所有可能线性组合，也不是一个向量空间。
- en: An example of a vector space you’re probably familiar with is  <math alttext="double-struck
    upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math> , or the entire space
    defined by the *x-y-z* coordinate axis. The reason for the notation  <math alttext="double-struck
    upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math> is that each coordinate
    can take on any value in the reals, or  <math alttext="double-struck upper R"><mi>ℝ</mi></math>
    , and there are three coordinates that uniquely define any such vector in this
    space. A collection of vectors that defines this space are the vectors (0,0,1),(0,1,0),(1,0,0),
    the unit vectors of each axis. Any vector (*a,b,c) *in the space can be written
    as *a**(1,0,0) + *b****(0,1,0) + *c****(0,0,1), a linear combination of the collection.
    In the other direction, any possible linear combination of the three vectors represents
    some vector (*a,b,c)* that lies in  <math alttext="double-struck upper R cubed"><msup><mi>ℝ</mi>
    <mn>3</mn></msup></math> .
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能熟悉的一个向量空间的例子是 <math alttext="double-struck upper R cubed"><msup><mi>ℝ</mi>
    <mn>3</mn></msup></math> ，或者由 *x-y-z* 坐标轴定义的整个空间。之所以使用符号 <math alttext="double-struck
    upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math> ，是因为每个坐标可以取任意实数值，或者 <math
    alttext="double-struck upper R"><mi>ℝ</mi></math> ，并且有三个坐标可以唯一定义该空间中的任何向量。定义这个空间的向量集包括
    (0,0,1), (0,1,0), (1,0,0)，即每个轴的单位向量。空间中的任何向量 (*a,b,c) *都可以写成 *a**(1,0,0) + *b****(0,1,0)
    + *c****(0,0,1) 的形式，即集合的线性组合。反过来，三个向量的任何可能线性组合都代表了在 <math alttext="double-struck
    upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math> 中的某个向量 (*a,b,c)*。
- en: Often, there exist matrices *A *for which some columns are linear combinations
    of other columns. For example, imagine if in our lizard dataset from [Figure 1-2](#matrix_addition),
    we had an additional feature for each lizard’s weight, but instead in pounds.
    This is a clear redundancy in the data since this feature is completely determined
    by the feature for weight in kilograms. In other words, the new feature is a linear
    combination of the other features in the data—simply take the column for weight
    in kilograms, multiply it by 2.2, and sum it with all the other columns multiplied
    by zero to get the column for weight in pounds. Logically, if we were to remove
    these sorts of redundancies from *A, *then *C(A)* shouldn’t change. One method
    to do this is to first create a list of all the original column vectors of *A, *where
    order is assigned arbitrarily. When iterating through the list, check to see if
    the current vector is a linear combination of all the vectors that precede it.
    If so, remove this vector from the list and continue. It’s clear that the removed
    vector provided no additional information beyond the ones we’ve already seen.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: The resulting list is called the *basis* of *C(A),* and the length of the basis
    is the *dimension* of *C(A).* We say that the basis of any vector space *spans*
    the space, which means that all of the elements in the vector space can be formulated
    as a linear combination of basis vectors. In addition, the basis vectors are *linearly
    independent*, which means that none of the vectors can be written as a linear
    combination of the others, i.e., no redundancies. Going back to the example where
    we defined vector space, (0,0,1),(0,1,0),(1,0,0) would be a basis for the space 
    <math alttext="double-struck upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>
    , since no vector in the list is a linear combination of the others, and this
    list spans the entire space. And instead, the list (0,0,1),(0,1,0),(1,0,0),(2,5,1) spans
    the entire space, but is not linearly independent because (2,5,1) can be written
    as a linear combination of the first three vectors (we call such a list of vectors
    a *spanning list*, and of course the set of bases for a vector space is a subset
    of the set of spanning lists for the same space).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: As we alluded to in the discussion of our lizard dataset, the basis of the column
    space, given each lizard feature is a column, is a concise representation of the
    information represented in the feature matrix. In the real world, where we often
    have thousands of features (e.g., each pixel in an image), achieving a concise
    representation of our data is quite desirable. Though this is a good start, identifying
    the clear redundancies in our data often isn’t enough, as the randomness and complexity
    that exist in the real world tend to obscure these redundancies. Quantifying relationships
    between features can inform concise data representations, as we discuss at the
    end of this chapter and in [Chapter 9](ch09.xhtml#ch07) on representation learning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: The Null Space
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another key vector space is the *null space *of a matrix *A,* or *N(A)**. *This
    space consists of the vectors *v *such that *Av = 0.* We know that *v = 0*, the
    trivial solution, will always satisfy this property. If only the trivial solution
    is in the null space of a matrix, we call the space trivial. However, it is possible
    that there exist other solutions to this equation depending on the properties
    of *A, *or a nontrivial null space. For a vector *v* to satisfy *Av = 0, v* must
    be orthogonal to each of the rows of *A*, as shown in [Figure 1-11](#the_implication_that_the_dot).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0111.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Figure 1-11\. The implication that the dot product between each row and the
    vector v must be equal to 0
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s assume *A* is of dimension 2 by 3, for example. In our case, *A*’s rows
    cannot span  <math alttext="double-struck upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>
     due to *A *having only two rows (remember from our recent discussion that all
    bases have the same length, and all spanning lists are at least as long as all
    bases, so *A*’s rows can be neither of these). At best, *A*’s rows define a plane
    in the 3D coordinate system. The other two options are that the rows define a
    line or a point. The former occurs when *A* either has two nonzero rows, where
    one is a multiple of the other, or has one zero row and one nonzero row. The latter
    occurs when *A* has two zero rows, or in other words, is the zero matrix.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设*A*是一个2乘3的矩阵，例如。在我们的情况下，*A*的行不能跨越<math alttext="double-struck upper R cubed"><msup><mi>ℝ</mi>
    <mn>3</mn></msup></math>，因为*A*只有两行（请记住，从我们最近的讨论中知道，所有基的长度相同，所有跨度列表至少与所有基的长度相同，所以*A*的行既不可能是这两者）。在最好的情况下，*A*的行在3D坐标系中定义一个平面。另外两种选择是行定义一条线或一个点。前者发生在*A*有两个非零行，其中一个是另一个的倍数，或者有一行为零，一行为非零。后者发生在*A*有两行为零，换句话说，是零矩阵。
- en: 'In the case where *A’s *row space defines a plane (or even a line for that
    matter), all we’d need to do to find a vector in *N(A) *is:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在*A*的行空间定义一个平面（或者甚至一条线）的情况下，我们只需要找到*N(A)*中的一个向量：
- en: Pick any vector *v *that doesn’t lie in *A’s *row space.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择任意不在*A*的行空间中的向量*v*。
- en: Find its projection *v’* onto the row space, where the projection of *v* is
    defined as the vector in the space closest to *v. *Geometrically, the projection
    looks as if we had dropped a line down from the tip of *v* perpendicular to the
    space, and connected a vector from the origin to that point on the space.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到它在行空间上的投影*v'*，其中*v*的投影被定义为空间中离*v*最近的向量。从几何上讲，投影看起来好像我们从*v*的顶端垂直地向下拉了一条线，然后连接了一个从原点到空间上那一点的向量。
- en: Compute *v – v’*, which is orthogonal to the row space and thus, each row vector.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算*v - v'*，这是与行空间正交的向量，因此与每个行向量正交。
- en: '[Figure 1-12](#note_that_v_v_prime) depicts this.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-12](#note_that_v_v_prime)描述了这一点。'
- en: '![](Images/fdl2_0112.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0112.png)'
- en: Figure 1-12\. Finding a vector in N(A)
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-12. 在N(A)中找到一个向量
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that *v – v’* is perpendicular to *R(A)*, the row space of *A*, since *v’*
    was formed by dropping a perpendicular to the plane down from its tip.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意*v - v'*与*A*的行空间*R(A)*垂直，因为*v'*是通过从其顶端垂直向下拉一条线形成的。
- en: 'An important takeaway is that nontrivial solutions to *Av = 0 *exist when the
    rows of *A *do not span  <math alttext="double-struck upper R cubed"><msup><mi>ℝ</mi>
    <mn>3</mn></msup></math> ; more generally, if *A *is of dimension *m* by *n, *nontrivial
    solutions to *Av = 0 *exist when the row vectors do not span  <math alttext="double-struck
    upper R Superscript n"><msup><mi>ℝ</mi> <mi>n</mi></msup></math> *. *The process
    is similar to that shown: pick a vector in  <math alttext="double-struck upper
    R Superscript n"><msup><mi>ℝ</mi> <mi>n</mi></msup></math>  not in the row space,
    find its projection onto the row space, and subtract to get a vector in null space.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的结论是，当*A*的行不跨越<math alttext="double-struck upper R cubed"><msup><mi>ℝ</mi>
    <mn>3</mn></msup></math>时，*Av = 0*的非平凡解存在；更一般地，如果*A*是*m*乘*n*的，*Av = 0*的非平凡解存在于行向量不跨越<math
    alttext="double-struck upper R Superscript n"><msup><mi>ℝ</mi> <mi>n</msup></math>的情况下。这个过程与所示类似：选择一个不在行空间中的向量，找到它在行空间上的投影，然后减去以获得零空间中的一个向量。
- en: 'But we still must show that *N(A)* itself is a vector space. We can easily
    see that any linear combination of nontrivial solutions to *Av = 0* is still a
    solution. For example, given two nontrivial solutions  <math alttext="v 1"><msub><mi>v</mi>
    <mn>1</mn></msub></math>  and  <math alttext="v 2"><msub><mi>v</mi> <mn>2</mn></msub></math>
     and their linear combination  <math alttext="c 1 v 1 plus c 2 v 2"><mrow><msub><mi>c</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>c</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>2</mn></msub></mrow></math> , where <math
    alttext="c 1"><msub><mi>c</mi> <mn>1</mn></msub></math>  and  <math alttext="c
    2"><msub><mi>c</mi> <mn>2</mn></msub></math>  are constants, we see that:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们仍然必须证明*N(A)*本身是一个向量空间。我们很容易看到，*Av = 0*的任何非平凡解的线性组合仍然是一个解。例如，给定两个非平凡解<math
    alttext="v 1"><msub><mi>v</mi> <mn>1</mn></msub></math>和<math alttext="v 2"><msub><mi>v</mi>
    <mn>2</mn></sub></math>及其线性组合<math alttext="c 1 v 1 plus c 2 v 2"><mrow><msub><mi>c</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>c</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>2</mn></sub></mrow></math>，其中<math alttext="c
    1"><msub><mi>c</mi> <mn>1</mn></sub></math>和<math alttext="c 2"><msub><mi>c</mi>
    <mn>2</mn></sub></math>是常数，我们看到：
- en: <math alttext="StartLayout 1st Row  upper A left-parenthesis c 1 v 1 plus c
    2 v 2 right-parenthesis 2nd Row  equals upper A left-parenthesis c 1 v 1 right-parenthesis
    plus upper A left-parenthesis c 2 v 2 right-parenthesis 3rd Row  equals c 1 upper
    A v 1 plus c 2 upper A v 2 4th Row  equals c 1 asterisk 0 plus c 2 asterisk 0
    5th Row  equals 0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>A</mi> <mo>(</mo> <msub><mi>c</mi> <mn>1</mn></msub>
    <msub><mi>v</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>c</mi> <mn>2</mn></msub>
    <msub><mi>v</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <mi>A</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mn>1</mn></msub> <msub><mi>v</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <mi>A</mi> <mrow><mo>(</mo> <msub><mi>c</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>=</mo> <msub><mi>c</mi> <mn>1</mn></msub>
    <mi>A</mi> <msub><mi>v</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>c</mi> <mn>2</mn></msub>
    <mi>A</mi> <msub><mi>v</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <msub><mi>c</mi> <mn>1</mn></msub> <mo>*</mo> <mn>0</mn> <mo>+</mo> <msub><mi>c</mi>
    <mn>2</mn></msub> <mo>*</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <mn>0</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  upper A left-parenthesis c 1 v 1 plus c
    2 v 2 right-parenthesis 2nd Row  equals upper A left-parenthesis c 1 v 1 right-parenthesis
    plus upper A left-parenthesis c 2 v 2 right-parenthesis 3rd Row  equals c 1 upper
    A v 1 plus c 2 upper A v 2 4th Row  equals c 1 asterisk 0 plus c 2 asterisk 0
    5th Row  equals 0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>A</mi> <mo>(</mo> <msub><mi>c</mi> <mn>1</mn></msub>
    <msub><mi>v</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>c</mi> <mn>2</mn></msub>
    <msub><mi>v</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <mi>A</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mn>1</mn></msub> <msub><mi>v</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <mi>A</mi> <mrow><mo>(</mo> <msub><mi>c</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>=</mo> <msub><mi>c</mi> <mn>1</mn></msub>
    <mi>A</mi> <msub><mi>v</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>c</mi> <mn>2</mn></msub>
    <mi>A</mi> <msub><mi>v</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <msub><mi>c</mi> <mn>1</mn></msub> <mo>*</mo> <mn>0</mn> <mo>+</mo> <msub><mi>c</mi>
    <mn>2</mn></msub> <mo>*</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <mn>0</mn></mrow></mtd></mtr></mtable></math>
- en: Where the first equality arises from the associativity of matrix multiplication
    and the second from the fact that <math alttext="c 1"><msub><mi>c</mi> <mn>1</mn></msub></math>
     and  <math alttext="c 2"><msub><mi>c</mi> <mn>2</mn></msub></math>  are constants.
    Note that this logic can be used for any number of nontrivial solutions, not just
    two. Thus, the null space is defined by some collection of vectors that can be
    boiled down to a basis, and contains all possible linear combinations of these
    vectors. These characteristics make the null space a vector space.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个等式来自矩阵乘法的结合律，第二个等式来自<math alttext="c 1"><msub><mi>c</mi> <mn>1</mn></msub></math>和<math
    alttext="c 2"><msub><mi>c</mi> <mn>2</mn></msub></math>是常数的事实。请注意，这种逻辑可以用于任意数量的非平凡解，而不仅仅是两个。因此，零空间由一些向量的集合定义，可以归结为一组基，并包含这些向量的所有可能线性组合。这些特征使零空间成为一个向量空间。
- en: This is all deeply connected to one of the key matrix operations presented,
    the *matrix inverse*. We can think of a matrix’s inverse as undoing the action
    of a matrix upon any other entity. For example, if we were to compute *Av* and
    multiply on the left by  <math alttext="upper A Superscript negative 1"><msup><mi>A</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> , we should be left with our initial *v. *However,
    depending on the properties of *A, *there can exist ambiguities as to how to “undo”
    its action. For example, let’s say *v *was some nonzero vector, but for some reason, *Av
    = 0*. If we were to multiply on the left by  <math alttext="upper A Superscript
    negative 1"><msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> ,
    we’d be left with *v = 0 *instead of our initial *v.* This unfortunately goes
    against the properties of an inverse, and we declare that such a matrix is noninvertible,
    or *singular*. But why does this happen in the first place? This goes back to
    our observation about ambiguities. Because an inverse is supposed to undo the
    action of a matrix, if there are multiple initial vectors that map to the same
    vector via the matrix’s action, trying to undo this action is impossible. Going
    back to our example, we know that nonzero vectors are mapped to 0 by *A* when
    *A* has a nontrivial null space. Thus, any matrix with a nontrivial null space
    is also singular.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们介绍的关键矩阵运算之一，*矩阵逆*密切相关。我们可以将矩阵的逆想象为撤销矩阵对任何其他实体的作用。例如，如果我们计算*Av*并在左侧乘以<math
    alttext="上A上标负1"><msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>，我们应该得到我们的初始*v。*然而，根据*A*的属性，可能存在关于如何“撤销”其作用的歧义。例如，假设*v*是某个非零向量，但由于某种原因，*Av
    = 0*。如果我们在左侧乘以<math alttext="上A上标负1"><msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>，我们将得到*v
    = 0*而不是我们的初始*v。*不幸的是，这违反了逆的属性，我们宣布这样的矩阵是不可逆的，或*奇异*的。但为什么会发生这种情况呢？这归结于我们对歧义的观察。因为逆应该撤销矩阵的作用，如果有多个初始向量通过矩阵的作用映射到相同的向量，尝试撤销这种作用是不可能的。回到我们的例子，我们知道当*A*具有非平凡零空间时，非零向量被映射到0。因此，任何具有非平凡零空间的矩阵也是奇异的。
- en: Next, we will cover eigenvectors and eigenvalues, which puts all of the information
    we’ve learned so far into practice.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将涵盖特征向量和特征值，将迄今为止学到的所有信息付诸实践。
- en: Eigenvectors and Eigenvalues
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征向量和特征值
- en: 'Matrices can act on vectors in many different ways. For most combinations of
    matrices and vectors, plotting the vector and its transformation doesn’t provide
    us with any interesting patterns. However, for certain matrices and specific vectors
    for those matrices, the action of the matrix upon the vector gives us an informative
    and surprising result: the transformation is a scalar multiple of the original.
    We call these vectors *eigenvectors*, and the scalar multiple its corresponding
    *eigenvalue.* In this section, we discuss these very special vectors, relate back
    to the material presented in the previously, and begin the discussion connecting
    the theory of linear algebra with the practice of data science.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵可以以许多不同的方式作用于向量。对于大多数矩阵和向量的组合，绘制向量及其变换并不会给我们提供任何有趣的模式。然而，对于某些矩阵和特定的向量，矩阵对向量的作用给我们提供了一个信息丰富且令人惊讶的结果：变换是原始向量的标量倍数。我们称这些向量为*特征向量*，标量倍数为其对应的*特征值*。在本节中，我们讨论这些非常特殊的向量，与之前提出的材料相关，并开始讨论将线性代数理论与数据科学实践联系起来。
- en: More formally, an eigenvector for a matrix *A *is a nonzero vector *v* such
    that *Av = cv,* where *c *is some constant (including zero, potentially), as shown
    in [Figure 1-13](#we_see_here_that_the_vector).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，对于矩阵*A*，特征向量是一个非零向量*v*，使得*Av = cv*，其中*c*是某个常数（包括零，可能），如[图1-13](#we_see_here_that_the_vector)所示。
- en: '![](Images/fdl2_0113.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0113.png)'
- en: Figure 1-13\. The vector (1,1) is an eigenvector of our matrix, with a corresponding
    eigenvalue of 3
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-13。向量(1,1)是我们矩阵的特征向量，对应的特征值为3
- en: Note
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Note that if we were to pick any random vector, such as (2,5), the transformation
    wouldn’t look as meaningful as it does in [Figure 1-13](#we_see_here_that_the_vector).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们选择任意随机向量，例如(2,5)，变换看起来不像在[图1-13](#we_see_here_that_the_vector)中那样有意义。
- en: Of course, if *A *is a rectangular matrix, it’s impossible for *A *to have any
    eigenvectors. The original vector and its transformation have different sizes,
    and thus transformation couldn’t be a scalar multiple of the original. For this
    reason, we limit our discussion in this section to square matrices.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果*A*是一个矩形矩阵，它不可能有任何特征向量。原始向量及其变换具有不同的大小，因此变换不能是原始向量的标量倍数。因此，出于这个原因，我们在本节中将讨论限制在方阵上。
- en: The simplest example is the identity matrix. Every nonzero vector is an eigenvector
    of the identity matrix since *Iv = v* for all *v,* with each having an eigenvalue
    of 1. Oftentimes, however, the eigenvectors of a matrix won’t be so obvious. How
    do we find these vectors and their corresponding eigenvalues? We know the conditions
    of any potential eigenvector; that is, if *v *is an eigenvector, it must satisfy *Av
    = cv *for some scalar *c:*
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的例子是单位矩阵。单位矩阵的每个非零向量都是特征向量，因为*Iv = v*对于所有*v*都成立，每个向量都有一个特征值为1。然而，通常情况下，矩阵的特征向量不会那么明显。我们如何找到这些向量及其对应的特征值呢？我们知道任何潜在特征向量的条件；也就是说，如果*v*是一个特征向量，它必须满足*Av
    = cv*，其中*c*是某个标量。
- en: <math alttext="upper A v equals c v long left right double arrow upper A v minus
    c v equals 0 long left right double arrow left-parenthesis upper A minus c upper
    I right-parenthesis v equals 0"><mrow><mi>A</mi> <mi>v</mi> <mo>=</mo> <mi>c</mi>
    <mi>v</mi> <mo>⇔</mo> <mi>A</mi> <mi>v</mi> <mo>-</mo> <mi>c</mi> <mi>v</mi> <mo>=</mo>
    <mn>0</mn> <mo>⇔</mo> <mo>(</mo> <mi>A</mi> <mo>-</mo> <mi>c</mi> <mi>I</mi> <mo>)</mo>
    <mi>v</mi> <mo>=</mo> <mn>0</mn></mrow></math>
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A v equals c v long left right double arrow upper A v minus
    c v equals 0 long left right double arrow left-parenthesis upper A minus c upper
    I right-parenthesis v equals 0"><mrow><mi>A</mi> <mi>v</mi> <mo>=</mo> <mi>c</mi>
    <mi>v</mi> <mo>⇔</mo> <mi>A</mi> <mi>v</mi> <mo>-</mo> <mi>c</mi> <mi>v</mi> <mo>=</mo>
    <mn>0</mn> <mo>⇔</mo> <mo>(</mo> <mi>A</mi> <mo>-</mo> <mi>c</mi> <mi>I</mi> <mo>)</mo>
    <mi>v</mi> <mo>=</mo> <mn>0</mn></mrow></math>
- en: The implication here is that if *Av = cv, *then *A – cI *must have a nontrivial
    null space. In the other direction, if we find a *c *such that *A – cI *has a
    nontrivial null space, the nonzero vectors in the null space are eigenvectors
    of *A.* Of course, if *A *itself has a nontrivial null space, then all nonzero *v *in
    the null space satisfy the above implication when *c* is 0\. More generally, however,
    we must find the *c *such that *A – cI *has a nontrivial null space. As established
    previously, checking for a nontrivial null space is equivalent to testing for
    a matrix being singular. For reasons beyond the scope of this text, one way to
    test if  <math alttext="upper A minus c upper I"><mrow><mi>A</mi> <mo>-</mo> <mi>c</mi>
    <mi>I</mi></mrow></math>  for some *c *is a singular matrix is to check whether
    its *determinant *is 0\. We won’t go into too much depth here, but we can think
    about the determinant as a function, or polynomial, that encodes properties of
    the matrix and results in a value of 0 iff the matrix is singular.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的含义是，如果*Av = cv*，那么*A - cI*必须有一个非平凡的零空间。另一方面，如果我们找到一个*c*，使得*A - cI*有一个非平凡的零空间，那么零空间中的非零向量就是*A*的特征向量。当然，如果*A*本身有一个非平凡的零空间，那么当*c*为0时，零空间中的所有非零*v*都满足上述含义。然而，更一般地，我们必须找到一个*c*，使得*A
    - cI*有一个非平凡的零空间。正如之前所建立的，检查非平凡的零空间等同于测试矩阵是否奇异。出于本文范围之外的原因，检查*<math alttext="upper
    A minus c upper I"><mrow><mi>A</mi> <mo>-</mo> <mi>c</mi> <mi>I</mi></mrow></math>*是否为奇异矩阵的一种方法是检查其行列式是否为0。我们不会在这里深入讨论，但我们可以将行列式视为一个函数或多项式，它编码矩阵的属性，并在矩阵奇异时产生值为0。
- en: However, it would be inefficient, and frankly impossible, for us to test every
    possible *c* for a zero determinant. We can instead think of *c *as a variable
    in an equation and solve for itvia the *characteristic polynomial*, which is the
    determinant of the matrix *A – cI* set equal to 0\. The roots of this polynomial
    give us the eigenvalues of *A.* To find their corresponding eigenvectors, we can
    plug each solution for *c *into *A – cI *and then solve for the *v *that make(s) *(A
    – cI)v* = 0.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于我们来说测试每个可能的零行列式的*c*是低效的，而且实际上是不可能的。我们可以将*c*视为方程中的一个变量，通过*特征多项式*解出它，这是矩阵*A
    - cI*的行列式设置为0。这个多项式的根给出了*A*的特征值。要找到它们对应的特征向量，我们可以将每个*c*的解代入*A - cI*，然后解出使*(A -
    cI)v* = 0的*v*。
- en: Note
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Calculating the determinant for any matrix of reasonable size is quite prohibitive
    in terms of computational cost. Although we won’t delve further into this, algorithms
    today use a version of the QR algorithm (named after the QR matrix decomposition) to
    calculate the eigenvalues of a matrix. If you’d like to learn more about these
    and similar such algorithms, we highly recommend lecture notes or books on numerical
    linear algebra.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何合理大小的矩阵计算行列式在计算成本方面是相当禁止的。虽然我们不会进一步深入讨论，但今天的算法使用QR算法的一个版本（以QR矩阵分解命名）来计算矩阵的特征值。如果您想了解更多关于这些和类似算法的信息，我们强烈推荐阅读关于数值线性代数的讲座笔记或书籍。
- en: How does our study of eigenvalues and eigenvectors connect to that of data science?
    Principal component analysis, or PCA, is one of the most famous algorithms in
    data science, and it uses the eigenvectors and eigenvalues of a special matrix
    called the correlation matrix, which represents the quantifiable relationships
    between features alluded to earlier, to perform dimensionality reduction on the
    original data matrix. We will discuss correlation and related concepts in the
    next chapter on probability, and learn more about PCA in [Chapter 8](ch08.xhtml#embedding_and_representing_learning).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对特征值和特征向量的研究如何与数据科学相联系？主成分分析（PCA）是数据科学中最著名的算法之一，它使用一个称为相关矩阵的特殊矩阵的特征向量和特征值，该矩阵代表了前面提到的特征之间的可量化关系，以在原始数据矩阵上执行降维。我们将在下一章讨论概率中讨论相关和相关概念，并在第8章中了解更多关于PCA的内容。
- en: Summary
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we investigated some of the basics of applied linear algebra.
    We learned about the key data structures and operations that rule both applied
    linear algebra and deep learning, and different ways to view these fundamental
    operations. For example, we learned that the dot product view of matrix multiplication
    was important from a computational lens, while the column vector approach led
    us into our discussion on the fundamental spaces quite naturally. We also got
    a peek at some of the surprising hidden properties of matrices, such as eigenvalues
    and eigenvectors, and how these properties are widely utilized in data science
    even to this day. In the next chapter, we will learn about the field of probability,
    which is often used in tandem with linear algebra to build complex, neural models
    used in the world.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了应用线性代数的一些基础知识。我们了解了统治应用线性代数和深度学习的关键数据结构和操作，以及查看这些基本操作的不同方式。例如，我们了解到矩阵乘法的点积视图在计算角度上很重要，而列向量方法自然地引导我们进入对基本空间的讨论。我们还一窥了矩阵的一些令人惊讶的隐藏属性，如特征值和特征向量，以及这些属性如何广泛应用于数据科学，甚至至今。在下一章中，我们将学习概率领域，这经常与线性代数一起用于构建世界中使用的复杂神经模型。
