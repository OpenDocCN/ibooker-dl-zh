<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. End-to-End Machine Learning Project"><div class="chapter" id="project_chapter">
<h1><span class="label">Chapter 2. </span>End-to-End Machine Learning Project</h1>


<p>In<a data-type="indexterm" data-primary="end-to-end ML project" id="xi_endtoendMLprojectexercise253_1"/> this chapter you will work through an example project end to end, pretending to be a recently hired data scientist at a real estate company. This example is fictitious; the goal is to illustrate the main steps of a machine learning project, not to learn anything about the real estate business. Here are the main steps we will walk through:</p>
<ol>
<li>
<p>Look at the big picture.</p>
</li>
<li>
<p>Get the data.</p>
</li>
<li>
<p>Explore and visualize the data to gain insights.</p>
</li>
<li>
<p>Prepare the data for machine learning algorithms.</p>
</li>
<li>
<p>Select a model and train it.</p>
</li>
<li>
<p>Fine-tune your model.</p>
</li>
<li>
<p>Present your solution.</p>
</li>
<li>
<p>Launch, monitor, and maintain your system.</p>
</li>

</ol>






<section data-type="sect1" data-pdf-bookmark="Working with Real Data"><div class="sect1" id="id29">
<h1>Working with Real Data</h1>

<p>When<a data-type="indexterm" data-primary="data" data-secondary="working with real data" id="xi_dataworkingwithrealdata2175_1"/><a data-type="indexterm" data-primary="end-to-end ML project" data-secondary="real data, advantages of working with" id="xi_endtoendMLprojectexerciserealdataadvantagesofworkingwith2175_1"/> you are learning about machine learning, it is best to experiment with real-world data, not artificial datasets. Fortunately, there are thousands of open datasets to choose from, ranging across all sorts of domains. Here are a few popular open data repositories you can use to get data:</p>

<ul>
<li>
<p><a href="https://datasetsearch.research.google.com">Google Datasets Search</a></p>
</li>
<li>
<p><a href="https://huggingface.co/docs/datasets">Hugging Face Datasets</a></p>
</li>
<li>
<p><a href="https://openml.org">OpenML.org</a></p>
</li>
<li>
<p><a href="https://kaggle.com/datasets">Kaggle.com</a></p>
</li>
<li>
<p><a href="https://archive.ics.uci.edu">UC Irvine Machine Learning Repository</a></p>
</li>
<li>
<p><a href="https://snap.stanford.edu/data">Stanford Large Network Dataset Collection</a></p>
</li>
<li>
<p><a href="https://registry.opendata.aws">Amazon‚Äôs AWS datasets</a></p>
</li>
<li>
<p><a href="https://data.gov">U.S. Government‚Äôs Open Data</a></p>
</li>
<li>
<p><a href="https://dataportals.org">DataPortals.org</a></p>
</li>
<li>
<p><a href="https://homl.info/9">Wikipedia‚Äôs list of machine learning datasets</a></p>
</li>
</ul>

<p>In this chapter we‚Äôll use the California Housing Prices dataset<a data-type="indexterm" data-primary="California Housing Prices dataset" id="xi_CaliforniaHousingPricesdataset23264_1"/><a data-type="indexterm" data-primary="end-to-end ML project" data-secondary="building the model" id="xi_endtoendMLprojectexercisebuildingthemodel23264_1"/><a data-type="indexterm" data-primary="housing dataset" id="xi_housingdataset23264_1"/> from the StatLib repository‚Å†<sup><a data-type="noteref" id="id990-marker" href="ch02.html#id990">1</a></sup> (see <a data-type="xref" href="#california_housing_prices_plot">Figure¬†2-1</a>). This dataset is based on data from the 1990 California census. It is not exactly recent (a nice house in the Bay Area was still affordable at the time), but it has many qualities for learning, so we will pretend it is recent data. For teaching purposes I‚Äôve added a categorical attribute and removed a few features.<a data-type="indexterm" data-startref="xi_dataworkingwithrealdata2175_1" id="id991"/><a data-type="indexterm" data-startref="xi_endtoendMLprojectexerciserealdataadvantagesofworkingwith2175_1" id="id992"/></p>

<figure class="width-90"><div id="california_housing_prices_plot" class="figure">
<img src="assets/hmls_0201.png" alt="Map of California displaying housing price data with colored dots representing median house values and dot size indicating population density." width="2640" height="1971"/>
<h6><span class="label">Figure 2-1. </span>California housing prices</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Look at the Big Picture"><div class="sect1" id="id392">
<h1>Look at the Big Picture</h1>

<p>Welcome to the Machine Learning Housing Corporation! Your first task is to use California census data to build a model of housing prices in the state. This data includes metrics such as the population, median income, and median housing price for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). I will call them ‚Äúdistricts‚Äù for short.</p>

<p>Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Since you are a well-organized data scientist, the first thing you should do is pull out your machine learning project checklist<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="project checklist" data-seealso="end-to-end ML project" id="id993"/>. You can start with the one at <a href="https://homl.info/checklist" class="bare"><em class="hyperlink">https://homl.info/checklist</em></a>; it should work reasonably well for most machine learning projects, but make sure to adapt it to your needs. In this chapter we will go through many checklist items, but we will also skip a few, either because they are self-explanatory or because they will be discussed in later chapters.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Frame the Problem"><div class="sect2" id="id30">
<h2>Frame the Problem</h2>

<p>The first question to ask your boss is what exactly the business objective is. Building a model is probably not the end goal. How does the company expect to use and benefit from this model? Knowing the objective is important because it will determine how you frame the problem, which algorithms you will select, which performance measure you will use to evaluate your model, and how much effort you will spend tweaking it.</p>

<p>Your boss answers that your model‚Äôs output (a prediction of a district‚Äôs median housing price) will be essential to determine whether it is worth investing in a given area. More specifically, your model‚Äôs output will be fed to another machine learning system (see <a data-type="xref" href="#house_pricing_pipeline_diagram">Figure¬†2-2</a>), along with some other signals.‚Å†<sup><a data-type="noteref" id="id994-marker" href="ch02.html#id994">2</a></sup> So it‚Äôs important to make our housing price model as accurate as we can.</p>

<p>The next question to ask your boss is what the current solution looks like (if any). The current situation will often give you a reference for performance, as well as insights on how to solve the problem. Your boss answers that the district housing prices are currently estimated manually by experts: a team gathers up-to-date 
<span class="keep-together">information</span> about a district, and when they cannot get the median housing price, they estimate it using complex rules.</p>

<figure><div id="house_pricing_pipeline_diagram" class="figure">
<img src="assets/hmls_0202.png" alt="Diagram showing a machine learning pipeline for real estate, highlighting data flow from district data to district pricing, investment analysis, and investments." width="1120" height="422"/>
<h6><span class="label">Figure 2-2. </span>A machine learning pipeline for real estate investments</h6>
</div></figure>

<p>This is costly and time-consuming, and their estimates are not great; in cases where they manage to find out the actual median housing price, they often realize that their estimates were off by more than 30%. This is why the company thinks that it would be useful to train a model to predict a district‚Äôs median housing price, given other data about that district. The census data looks like a great dataset to exploit for this purpose, since it includes the median housing prices of thousands of districts, as well as other data.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id995">
<h1>Pipelines</h1>
<p>A sequence of data processing components is called a data <em>pipeline</em><a data-type="indexterm" data-primary="data pipeline" id="id996"/><a data-type="indexterm" data-primary="pipelines" id="id997"/>. Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.</p>

<p>Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store. Then, some time later, the next component in the pipeline pulls in this data and spits out its own output. Each component is fairly self-contained: the interface between components is simply the data store. This makes the system simple to grasp (with the help of a data flow graph), and different teams can focus on different components. Moreover, if a component breaks down, the downstream components can often continue to run normally (at least for a while) by just using the last output from the broken component. This makes the architecture quite robust.</p>

<p>On the other hand, a broken component can go unnoticed for some time if proper monitoring is not implemented. The data gets stale and the overall system‚Äôs performance drops.</p>
</div></aside>

<p>With all this information, you are now ready to start designing your system. First, determine what kind of training supervision the model will need: is it a supervised, unsupervised, semi-supervised, self-supervised, or reinforcement learning task? And is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? Before you read on, pause and try to answer these questions for yourself.</p>

<p>Have you found the answers? Let‚Äôs see. This is clearly a typical supervised learning task, since the model can be trained with <em>labeled</em> examples<a data-type="indexterm" data-primary="labels" id="id998"/> (each instance comes with the expected output, i.e., the district‚Äôs median housing price). It is a typical regression task, since the model will be asked to predict a value. More specifically, this is a <em>multiple regression</em><a data-type="indexterm" data-primary="multiple regression" id="id999"/><a data-type="indexterm" data-primary="regression models" data-secondary="multiple regression" id="id1000"/> problem, since the system will use multiple features to make a prediction (the district‚Äôs population, the median income, etc.). It is also 
<span class="keep-together">a <em>univariate regression</em></span> problem<a data-type="indexterm" data-primary="regression models" data-secondary="univariate regression" id="id1001"/><a data-type="indexterm" data-primary="univariate regression" id="id1002"/>, since we are only trying to predict a single value for each district. If we were trying to predict multiple values per district, it would be a <em>multivariate regression</em> problem<a data-type="indexterm" data-primary="multivariate regression" id="id1003"/><a data-type="indexterm" data-primary="regression models" data-secondary="multivariate regression" id="id1004"/>. Finally, there is no continuous flow of data coming into the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If the data were huge, you could either split your batch learning work across multiple servers (using the MapReduce technique) or use an online learning technique.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Select a Performance Measure"><div class="sect2" id="select_a_performance_measure">
<h2>Select a Performance Measure</h2>

<p>Your<a data-type="indexterm" data-primary="performance measures" data-secondary="selecting" id="xi_performancemeasuresselecting2805_1"/> next step is to select a performance measure. A typical performance measure for regression problems is the <em>root mean squared error</em> (RMSE)<a data-type="indexterm" data-primary="root mean square error (RMSE)" id="xi_rootmeansquareerrorRMSE280145_1"/>. It gives an idea of how much error the system typically makes in its predictions, with a higher weight given to large errors. <a data-type="xref" href="#rmse_equation">Equation 2-1</a> shows the mathematical formula to compute the RMSE.</p>
<div id="rmse_equation" data-type="equation">
<h5><span class="label">Equation 2-1. </span>Root mean squared error (RMSE)</h5>
<math alttext="RMSE left-parenthesis bold upper X comma bold y comma h right-parenthesis equals StartRoot StartFraction 1 Over m EndFraction sigma-summation Underscript i equals 1 Overscript m Endscripts left-parenthesis h left-parenthesis bold x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis minus y Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis squared EndRoot" display="block">
  <mrow>
    <mtext>RMSE</mtext>
    <mrow>
      <mo>(</mo>
      <mi>ùêó</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>ùê≤</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>h</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <msqrt>
      <mrow>
        <mfrac><mn>1</mn> <mi>m</mi></mfrac>
        <msubsup><mo>‚àë</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </msubsup>
        <msup><mrow><mo>(</mo><mi>h</mi><mrow><mo>(</mo><msup><mrow><mi>ùê±</mi></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow><mo>-</mo><msup><mrow><mi>y</mi></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mn>2</mn> </msup>
      </mrow>
    </msqrt>
  </mrow>
</math>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="notations">
<h1>Notations</h1>
<p>This<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="notations" id="xi_machinelearningMLnotations2955_1"/><a data-type="indexterm" data-primary="notations" id="xi_notations2955_1"/> equation introduces several very common machine learning notations that I will use throughout this book:</p>

<ul>
<li>
<p><em>m</em> is the number of instances in the dataset you are measuring the RMSE on.</p>

<ul>
<li>
<p>For example, if you are evaluating the RMSE on a validation set of 2,000 districts, then <em>m</em> = 2,000.</p>
</li>
</ul>
</li>
<li>
<p><strong>x</strong><sup>(<em>i</em>)</sup> is a vector of all the feature values (excluding the label) of the <em>i</em><sup>th</sup> instance in the dataset, and <em>y</em><sup>(<em>i</em>)</sup> is its label (the desired output value for that instance). <strong>y</strong> is a vector containing the labels of all the instances in the dataset.</p>

<ul>
<li>
<p>For example, if the first district in the dataset is located at longitude ‚Äì118.29¬∞, latitude 33.91¬∞, and it has 1,416 inhabitants with a median income of $38,372, and the median house value is $156,400 (ignoring other features for now), then:</p>
<div data-type="equation">
<math alttext="bold x Superscript left-parenthesis 1 right-parenthesis Baseline equals Start 4 By 1 Matrix 1st Row  negative 118.29 2nd Row  33.91 3rd Row  1 comma 416 4th Row  38 comma 372 EndMatrix" display="block">
  <mrow>
    <msup><mrow><mi>ùê±</mi></mrow> <mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow> </msup>
    <mo>=</mo>
    <mrow>
      <mo>(</mo>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mo>-</mo>
              <mn>118</mn>
              <mo lspace="0%" rspace="0%">.</mo>
              <mn>29</mn>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mn>33</mn>
              <mo lspace="0%" rspace="0%">.</mo>
              <mn>91</mn>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mn>1</mn>
              <mo lspace="0%" rspace="0%">,</mo>
              <mn>416</mn>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mn>38</mn>
              <mo lspace="0%" rspace="0%">,</mo>
              <mn>372</mn>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>and:</p>
<div data-type="equation">
<math alttext="y Superscript left-parenthesis 1 right-parenthesis Baseline equals 156 comma 400" display="block">
  <mrow>
    <msup><mrow><mi>y</mi></mrow> <mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow> </msup>
    <mo>=</mo>
    <mn>156</mn>
    <mo lspace="0%" rspace="0%">,</mo>
    <mn>400</mn>
  </mrow>
</math>
</div>
</li>
</ul>
</li>
<li>
<p><strong>X</strong> is a matrix containing all the feature values (excluding labels) of all instances in the dataset. There is one row per instance, and the <em>i</em><sup>th</sup> row is equal to the transpose of <strong>x</strong><sup>(<em>i</em>)</sup>, denoted (<strong>x</strong><sup>(<em>i</em>)</sup>)<sup>‚ä∫</sup>.‚Å†<sup><a data-type="noteref" id="id1005-marker" href="ch02.html#id1005">3</a></sup></p>

<ul>
<li>
<p>For example, if the first district is as just described, then the matrix <strong>X</strong> looks like this:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi mathvariant="bold">X</mi>
    <mo>=</mo>
    <mrow><mo>(</mo></mrow>
      <mtable>
        <mtr>
          <mtd>
            <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mo>‚ä∫</mo> </msup>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mo>‚ä∫</mo> </msup>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mo>‚ãÆ</mo>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mn>1999</mn><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mo>‚ä∫</mo> </msup>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mn>2000</mn><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mo>‚ä∫</mo> </msup>
          </mtd>
        </mtr>
      </mtable>
    <mrow><mo>)</mo></mrow>
    <mo>=</mo>
    <mrow><mo>(</mo></mrow>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mo>-</mo>
              <mn>118.29</mn>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mn>33.91</mn>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mn>1,416</mn>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mn>38,372</mn>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mo>‚ãÆ</mo>
          </mtd>
          <mtd>
            <mo>‚ãÆ</mo>
          </mtd>
          <mtd>
            <mo>‚ãÆ</mo>
          </mtd>
          <mtd>
            <mo>‚ãÆ</mo>
          </mtd>
        </mtr>
      </mtable>
    <mrow><mo>)</mo></mrow>
  </mrow>
</math>
</div>
</li>
</ul>
</li>
<li>
<p><em>h</em> is your system‚Äôs prediction function, also called a <em>hypothesis</em>.<a data-type="indexterm" data-primary="hypothesis" id="id1006"/> When your system is given an instance‚Äôs feature vector<a data-type="indexterm" data-primary="feature vectors" id="id1007"/> <strong>x</strong><sup>(<em>i</em>)</sup>, it outputs a predicted value <em>≈∑</em><sup>(<em>i</em>)</sup> = <em>h</em>(<strong>x</strong><sup>(<em>i</em>)</sup>) for that instance (<em>≈∑</em> is pronounced ‚Äúy-hat‚Äù).</p>

<ul>
<li>
<p>For example, if your system predicts that the median housing price in the first district is $158,400, then <em>≈∑</em><sup>(1)</sup> = <em>h</em>(<strong>x</strong><sup>(1)</sup>) = 158,400. The prediction error for this district is <em>≈∑</em><sup>(1)</sup> ‚Äì <em>y</em><sup>(1)</sup> = 2,000.</p>
</li>
</ul>
</li>
<li>
<p>RMSE(<strong>X</strong>,<em>y</em>,<em>h</em>) is the cost function<a data-type="indexterm" data-primary="cost function" id="id1008"/> measured on the set of examples using your hypothesis <em>h</em>.</p>
</li>
</ul>

<p>We use lowercase italic font for scalar values (such as <em>m</em> or <em>y</em><sup>(<em>i</em>)</sup>) and function names (such as <em>h</em>), lowercase bold font for vectors (such as <strong>x</strong><sup>(<em>i</em>)</sup>), and uppercase bold font for matrices (such as <strong>X</strong>).</p>
</div></aside>

<p>Although the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function, especially when there are many outliers in the data, as the RMSE is quite sensitive to them. In that case, you may consider using the <em>mean absolute error</em> <a data-type="indexterm" data-primary="MAE (mean absolute error)" id="id1009"/><a data-type="indexterm" data-primary="mean absolute error (MAE)" id="id1010"/>(MAE, also called the <em>average absolute deviation</em>),<a data-type="indexterm" data-primary="average absolute deviation" id="id1011"/> shown in <a data-type="xref" href="#mae_equation">Equation 2-2</a>:</p>
<div id="mae_equation" data-type="equation">
<h5><span class="label">Equation 2-2. </span>Mean absolute error (MAE)</h5>
<math alttext="MAE left-parenthesis bold upper X comma bold y comma h right-parenthesis equals StartFraction 1 Over m EndFraction sigma-summation Underscript i equals 1 Overscript m Endscripts StartAbsoluteValue h left-parenthesis bold x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis minus y Superscript left-parenthesis i right-parenthesis Baseline EndAbsoluteValue" display="block">
  <mrow>
    <mtext>MAE</mtext>
    <mrow>
      <mo>(</mo>
      <mi>ùêó</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>ùê≤</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>h</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <munderover><mo>‚àë</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover>
    <mrow>
      <mo>|</mo>
      <mi>h</mi>
      <mrow>
        <mo>(</mo>
        <msup><mrow><mi>ùê±</mi></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
        <mo>)</mo>
      </mrow>
      <mo>-</mo>
      <msup><mrow><mi>y</mi></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
      <mo>|</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>Both the RMSE and the MAE are ways to measure the distance between two vectors<a data-type="indexterm" data-primary="vectors, norms for measuring distance" id="id1012"/>: the vector of predictions and the vector of target values. Various distance measures, or <em>norms</em>, are possible:</p>

<ul>
<li>
<p>Computing the root of a sum of squares (RMSE) corresponds to the <em>Euclidean norm</em><a data-type="indexterm" data-primary="Euclidean norm" id="id1013"/>: this is the notion of distance we are all familiar with. It is also called the ‚Ñì<sub>2</sub> <em>norm</em><a data-type="indexterm" data-primary="‚Ñì norms" id="id1014"/><a data-type="indexterm" data-primary="‚Ñì norms" data-primary-sortas="l norms" id="id1015"/>, denoted ‚à• ¬∑ ‚à•<sub>2</sub> (or just ‚à• ¬∑ ‚à•).</p>
</li>
<li>
<p>Computing the sum of absolutes (MAE) corresponds to the ‚Ñì<sub>1</sub> <em>norm</em>, denoted ‚à• ¬∑ ‚à•<sub>1</sub>. This is sometimes called the <em>Manhattan norm</em><a data-type="indexterm" data-primary="Manhattan norm" id="id1016"/> because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.</p>
</li>
<li>
<p>More generally, the ‚Ñì<sub><em>k</em></sub> <em>norm</em> of a vector <strong>v</strong> containing <em>n</em> elements is defined as ‚à•<strong>v</strong>‚à•<sub><em>k</em></sub> = (|<em>v</em><sub>1</sub>|<sup><em>k</em></sup> + |<em>v</em><sub>2</sub>|<sup><em>k</em></sup> + ... + |<em>v</em><sub><em>n</em></sub>|<sup><em>k</em></sup>)<sup>1/<em>k</em></sup>. ‚Ñì<sub>0</sub> gives the number of nonzero elements in the vector, and ‚Ñì<sub>‚àû</sub> gives the maximum absolute value in the vector.</p>
</li>
</ul>

<p>The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred.<a data-type="indexterm" data-startref="xi_machinelearningMLnotations2955_1" id="id1017"/><a data-type="indexterm" data-startref="xi_notations2955_1" id="id1018"/><a data-type="indexterm" data-startref="xi_performancemeasuresselecting2805_1" id="id1019"/><a data-type="indexterm" data-startref="xi_rootmeansquareerrorRMSE280145_1" id="id1020"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Check the Assumptions"><div class="sect2" id="id32">
<h2>Check the Assumptions</h2>

<p>Lastly<a data-type="indexterm" data-primary="assumptions, checking in model building" id="id1021"/>, it is good practice to list and verify the assumptions that have been made so far (by you or others); this can help you catch serious issues early on. For example, the district prices that your system outputs are going to be fed into a downstream machine learning system, and you assume that these prices are going to be used as such. But what if the downstream system converts the prices into categories (e.g., ‚Äúcheap‚Äù, ‚Äúmedium‚Äù, or ‚Äúexpensive‚Äù) and then uses those categories instead of the prices themselves? In this case, getting the price perfectly right is not important at all; your system just needs to get the category right. If that‚Äôs so, then the problem should have been framed as a classification task, not a regression task. You don‚Äôt want to find this out after working on a regression system for months.</p>

<p>Fortunately, after talking with the team in charge of the downstream system, you are confident that they do indeed need the actual prices, not just categories. Great! You‚Äôre all set, the lights are green, and you can start coding now!<a data-type="indexterm" data-startref="xi_CaliforniaHousingPricesdataset23264_1" id="id1022"/><a data-type="indexterm" data-startref="xi_endtoendMLprojectexercisebuildingthemodel23264_1" id="id1023"/><a data-type="indexterm" data-startref="xi_housingdataset23264_1" id="id1024"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Get the Data"><div class="sect1" id="id33">
<h1>Get the Data</h1>

<p>It‚Äôs<a data-type="indexterm" data-primary="end-to-end ML project" data-secondary="getting the data" id="xi_endtoendMLprojectexercisegettingthedata22385_1"/> time to get your hands dirty. Don‚Äôt hesitate to pick up your laptop and walk through the code examples. As I mentioned in the preface, all the code examples in this book are open source and available <a href="https://github.com/ageron/handson-mlp">online</a> as Jupyter<a data-type="indexterm" data-primary="Jupyter" id="id1025"/> notebooks, which are interactive documents containing text, images, and executable code snippets (Python in our case). In this book I will assume you are running these notebooks on Google Colab<a data-type="indexterm" data-primary="Google Colab" id="xi_GoogleColab2238456_1"/><a data-type="indexterm" data-primary="Colab" id="xi_Colab2238456_1"/>, a free service that lets you run any Jupyter notebook directly online, without having to install anything on your machine. If you want to use another online platform (e.g., Kaggle) or if you want to install everything locally on your own machine, please see the instructions on the book‚Äôs GitHub page.</p>








<section data-type="sect2" data-pdf-bookmark="Running the Code Examples Using Google Colab"><div class="sect2" id="id34">
<h2>Running the Code Examples Using Google Colab</h2>

<p>First, open a web browser and visit <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>: this will lead you to Google Colab, and it will display the list of Jupyter notebooks for this book (see <a data-type="xref" href="#google_colab_notebook_list">Figure¬†2-3</a>). You will find one notebook per chapter, plus a few extra notebooks and tutorials for NumPy<a data-type="indexterm" data-primary="NumPy" id="id1026"/>, Matplotlib<a data-type="indexterm" data-primary="Matplotlib library" id="id1027"/>, Pandas<a data-type="indexterm" data-primary="Pandas library" id="id1028"/>, linear algebra, and differential calculus. For example, if you click <em>02_end_to_end_machine_learning_project.ipynb</em>, the notebook from <a data-type="xref" href="#project_chapter">Chapter¬†2</a> will open up in Google Colab (see <a data-type="xref" href="#notebook_in_colab">Figure¬†2-4</a>).</p>

<p>A Jupyter notebook is composed of a list of cells. Each cell contains either executable code or text. Try double-clicking the first text cell (which contains the sentence ‚ÄúWelcome to Machine Learning Housing Corp.!‚Äù). This will open the cell for editing. Notice that Jupyter notebooks use Markdown syntax for formatting (e.g., <code>**bold**</code>, <code>*italics*</code>, <code># Title</code>, <code>[url](link text)</code>, and so on). Try modifying this text, then press Shift-Enter to see the result.</p>

<figure><div id="google_colab_notebook_list" class="figure">
<img src="assets/hmls_0203.png" alt="Google Colab interface showing a list of Jupyter notebooks in the &quot;ageron/handson-mlp&quot; repository on GitHub, with &quot;02_end_to_end_machine_learning_project.ipynb&quot; highlighted." width="1657" height="1206"/>
<h6><span class="label">Figure 2-3. </span>List of notebooks in Google Colab</h6>
</div></figure>

<figure><div id="notebook_in_colab" class="figure">
<img src="assets/hmls_0204.png" alt="Screenshot of a Google Colab notebook showing a section titled &quot;Chapter 2 ‚Äì End-to-end Machine Learning project,&quot; with instructions for editing and running text and code cells." width="1441" height="773"/>
<h6><span class="label">Figure 2-4. </span>Your notebook in Google Colab</h6>
</div></figure>

<p>Next, create a new code cell by selecting Insert ‚Üí ‚ÄúCode cell‚Äù from the menu. Alternatively, you can click the + Code button in the toolbar, or hover your mouse over the bottom of a cell until you see + Code and + Text appear, then click + Code. In the new code cell, type some Python code, such as <code translate="no">print("Hello World")</code>, then press Shift-Enter to run this code (or click the ‚ñ∑ button on the left side of the cell).</p>

<p>If you‚Äôre not logged in to your Google account, you‚Äôll be asked to log in now (if you don‚Äôt already have a Google account, you‚Äôll need to create one). Once you are logged in, when you try to run the code you‚Äôll see a security warning telling you that this notebook was not authored by Google. A malicious person could create a notebook that tries to trick you into entering your Google credentials so they can access your personal data, so before you run a notebook, always make sure you trust its author (or double-check what each code cell will do before running it). Assuming you trust me (or you plan to check every code cell), you can now click ‚ÄúRun anyway‚Äù.</p>

<p>Colab will then allocate a new <em>runtime</em> for you: this is a free virtual machine located on Google‚Äôs servers that contains a bunch of tools and Python libraries, including everything you‚Äôll need for most chapters (in some chapters, you‚Äôll need to run a command to install additional libraries). This will take a few seconds. Next, Colab will automatically connect to this runtime and use it to execute your new code cell. Importantly, the code runs on the runtime, <em>not</em> on your machine. The code‚Äôs output will be displayed under the cell. Congrats, you‚Äôve run some Python code on Colab!</p>
<div data-type="tip"><h6>Tip</h6>
<p>To insert a new code cell, you can also type Ctrl-M (or Cmd-M on macOS) followed by A (to insert above the active cell) or B (to insert below). There are many other keyboard shortcuts available: you can view and edit them by typing Ctrl-M (or Cmd-M) then H. If you choose to run the notebooks on Kaggle or on your own machine using JupyterLab or an IDE such as Visual Studio Code with the Jupyter extension, you will see some minor differences‚Äîruntimes are called <em>kernels</em>, the user interface and keyboard shortcuts are slightly different, etc.‚Äîbut switching from one Jupyter environment to another is not too hard.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Saving Your Code Changes and Your Data"><div class="sect2" id="id35">
<h2>Saving Your Code Changes and Your Data</h2>

<p>You can make changes to a Colab notebook, and they will persist for as long as you keep your browser tab open. But once you close it, the changes will be lost. To avoid this, make sure you save a copy of the notebook to your Google Drive by selecting File ‚Üí ‚ÄúSave a copy in Drive‚Äù. Alternatively, you can download the notebook to your computer by selecting File ‚Üí Download ‚Üí ‚ÄúDownload .ipynb‚Äù. Then you can later visit <a href="https://colab.research.google.com" class="bare"><em class="hyperlink">https://colab.research.google.com</em></a> and open the notebook again (either from Google Drive or by uploading it from your computer).</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Google Colab is meant only for interactive use: you can play around in the notebooks and tweak the code as you like, but you cannot let the notebooks run unattended for a long period of time, or else the runtime<a data-type="indexterm" data-primary="kernels (runtimes)" id="id1029"/> will be shut down and all of its data will be lost.</p>
</div>

<p>If the notebook generates data that you care about, make sure you download this data before the runtime shuts down. To do this, click the Files icon (see step 1 in <a data-type="xref" href="#save_data_google_colab">Figure¬†2-5</a>), find the file you want to download, click the vertical dots next to it (step 2), and click Download (step 3). Alternatively, you can mount your Google Drive on the runtime, allowing the notebook to read and write files directly to Google Drive as if it were a local directory. For this, click the Files icon (step 1), then click the Google Drive icon (circled in <a data-type="xref" href="#save_data_google_colab">Figure¬†2-5</a>) and follow the on-screen instructions.</p>

<figure class="width-65"><div id="save_data_google_colab" class="figure">
<img src="assets/hmls_0205.png" alt="Screenshot of Google Colab interface showing steps to download a file or mount Google Drive, with icons and menu options highlighted." width="1441" height="747"/>
<h6><span class="label">Figure 2-5. </span>Downloading a file from a Google Colab runtime (steps 1 to 3), or mounting your Google Drive (circled icon)</h6>
</div></figure>

<p>By default, your Google Drive will be mounted at <em>/content/drive/MyDrive</em>. If you want to back up a data file, simply copy it to this directory by running <code translate="no">!cp [.keep-together]#/content/my_great_model /content/drive/MyDrive</code>.# Any command starting with a bang (<code translate="no">!</code>) is treated as a shell command, not as Python code: <code translate="no">cp</code> is the Linux shell command to copy a file from one path to another. Note that Colab runtimes run on Linux (specifically, Ubuntu).<a data-type="indexterm" data-startref="xi_GoogleColab2238456_1" id="id1030"/><a data-type="indexterm" data-startref="xi_Colab2238456_1" id="id1031"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The Power and Danger of Interactivity"><div class="sect2" id="id725">
<h2>The Power and Danger of Interactivity</h2>

<p>Jupyter notebooks are interactive, and that‚Äôs a great thing: you can run each cell one by one, stop at any point, insert a cell, play with the code, go back and run the same cell again, etc., and I highly encourage you to do so. If you just run the cells one by one without ever playing around with them, you won‚Äôt learn as fast. However, this flexibility comes at a price: it‚Äôs very easy to run cells in the wrong order, or to forget to run a cell. If this happens, the subsequent code cells are likely to fail. For example, the very first code cell in each notebook contains setup code (such as imports), so make sure you run it first, or else nothing will work.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you ever run into a weird error, try restarting the runtime (by selecting Runtime ‚Üí ‚ÄúRestart runtime‚Äù from the menu) and then run all the cells again from the beginning of the notebook. This often solves the problem. If not, it‚Äôs likely that one of the changes you made broke the notebook: just revert to the original notebook and try again. If it still fails, please file an issue on GitHub.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Book Code Versus Notebook Code"><div class="sect2" id="id726">
<h2>Book Code Versus Notebook Code</h2>

<p>You may sometimes notice some little differences between the code in this book and the code in the notebooks. This may happen for several reasons:</p>

<ul>
<li>
<p>A library may have changed slightly by the time you read these lines, or perhaps despite my best efforts I made an error in the book. Sadly, I cannot magically fix the code in your copy of this book (unless you are reading an electronic copy and you can download the latest version), but I <em>can</em> fix the notebooks. So, if you run into an error after copying code from this book, please look for the fixed code in the notebooks: I will strive to keep them error-free and up-to-date with the latest library versions.</p>
</li>
<li>
<p>The notebooks contain some extra code to beautify the figures (adding labels, setting font sizes, etc.) and to save them in high resolution for this book. You can safely ignore this extra code if you want.</p>
</li>
</ul>

<p>I optimized the code for readability and simplicity: I made it as linear and flat as possible, defining very few functions or classes. The goal is to ensure that the code you are running is generally right in front of you, and not nested within several layers of abstractions that you have to search through. This also makes it easier for you to play with the code. For simplicity, there‚Äôs limited error handling, and I placed some of the least common imports right where they are needed (instead of placing them at the top of the file, as is recommended by the PEP 8 Python style guide). That said, your production code will not be very different: just a bit more modular, and with additional tests and error handling.</p>

<p>OK! Once you‚Äôre comfortable with Colab, you‚Äôre ready to download the data.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Download the Data"><div class="sect2" id="download_the_data">
<h2>Download the Data</h2>

<p>In<a data-type="indexterm" data-primary="data" data-secondary="downloading" id="xi_datadownloading22923_1"/><a data-type="indexterm" data-primary="downloading data" id="xi_downloadingdata22923_1"/> typical environments your data would be available in a relational database or some other common data store, and spread across multiple tables/documents/files. To access it, you would first need to get your credentials and access authorizations and familiarize yourself with the data schema.‚Å†<sup><a data-type="noteref" id="id1032-marker" href="ch02.html#id1032">4</a></sup> In this project, however, things are much simpler: you will just download a single compressed file, <em>housing.tgz</em>, which contains a comma-separated values (CSV) file called <em>housing.csv</em> with all the data.</p>

<p>Rather than manually downloading and decompressing the data, it‚Äôs usually preferable to write a function that does it for you. This is useful in particular if the data changes regularly: you can write a small script that uses the function to fetch the latest data (or you can set up a scheduled job to do that automatically at regular intervals). Automating the process of fetching the data is also useful if you need to install the dataset on multiple machines.</p>

<p>Here is the function to fetch and load the data:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">pathlib</code> <code class="kn">import</code> <code class="n">Path</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
<code class="kn">import</code> <code class="nn">tarfile</code>
<code class="kn">import</code> <code class="nn">urllib.request</code>

<code class="k">def</code> <code class="nf">load_housing_data</code><code class="p">():</code>
    <code class="n">tarball_path</code> <code class="o">=</code> <code class="n">Path</code><code class="p">(</code><code class="s2">"datasets/housing.tgz"</code><code class="p">)</code>
    <code class="k">if</code> <code class="ow">not</code> <code class="n">tarball_path</code><code class="o">.</code><code class="n">is_file</code><code class="p">():</code>
        <code class="n">Path</code><code class="p">(</code><code class="s2">"datasets"</code><code class="p">)</code><code class="o">.</code><code class="n">mkdir</code><code class="p">(</code><code class="n">parents</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">exist_ok</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="n">url</code> <code class="o">=</code> <code class="s2">"https://github.com/ageron/data/raw/main/housing.tgz"</code>
        <code class="n">urllib</code><code class="o">.</code><code class="n">request</code><code class="o">.</code><code class="n">urlretrieve</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">tarball_path</code><code class="p">)</code>
        <code class="k">with</code> <code class="n">tarfile</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">tarball_path</code><code class="p">)</code> <code class="k">as</code> <code class="n">housing_tarball</code><code class="p">:</code>
            <code class="n">housing_tarball</code><code class="o">.</code><code class="n">extractall</code><code class="p">(</code><code class="n">path</code><code class="o">=</code><code class="s2">"datasets"</code><code class="p">,</code> <code class="nb">filter</code><code class="o">=</code><code class="s2">"data"</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">Path</code><code class="p">(</code><code class="s2">"datasets/housing/housing.csv"</code><code class="p">))</code>

<code class="n">housing_full</code> <code class="o">=</code> <code class="n">load_housing_data</code><code class="p">()</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you get an SSL <code translate="no">CERTIFICATE_VERIFY_FAILED</code> error on macOS, then you most likely need to install the <code translate="no">certifi</code> package, as explained at <a href="https://homl.info/sslerror" class="bare"><em class="hyperlink">https://homl.info/sslerror</em></a>.</p>
</div>

<p>When <code translate="no">load_housing_data()</code> is called, it looks for the <em>datasets/housing.tgz</em> file. If it does not find it, it creates the <em>datasets</em> directory inside the current directory (which is <em>/content</em> by default, in Colab), downloads the <em>housing.tgz</em> file from the <em>ageron/data</em> GitHub repository, and extracts its content into the <em>datasets</em> directory; this creates the <em>datasets</em>/<em>housing</em> directory with the <em>housing.csv</em> file inside it. Lastly, the function loads this CSV file into a Pandas DataFrame object containing all the data, and returns it.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you are using Python 3.12 or 3.13, you should add <code translate="no">filter='data'</code> to the <code translate="no">extractall()</code> method‚Äôs arguments: this limits what the extraction algorithm can do and improves security (see the documentation for more details).<a data-type="indexterm" data-startref="xi_datadownloading22923_1" id="id1033"/><a data-type="indexterm" data-startref="xi_downloadingdata22923_1" id="id1034"/></p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Take a Quick Look at the Data Structure"><div class="sect2" id="id37">
<h2>Take a Quick Look at the Data Structure</h2>

<p>You start by looking at the top five rows of data using the DataFrame‚Äôs <code translate="no">head()</code> method (see <a data-type="xref" href="#housing_head_screenshot">Figure¬†2-6</a>).</p>

<figure><div id="housing_head_screenshot" class="figure">
<img src="assets/hmls_0206.png" alt="A screenshot showing the top five rows of a housing dataset, including attributes such as longitude, latitude, housing median age, median income, ocean proximity, and median house value." width="2657" height="782"/>
<h6><span class="label">Figure 2-6. </span>Top five rows in the dataset</h6>
</div></figure>

<p>Each row represents one district. There are 10 attributes (they are not all shown in the screenshot): <code translate="no">longitude</code>, <code translate="no">latitude</code>, <code translate="no">housing_median_age</code>, <code translate="no">total_rooms</code>, <code translate="no">total_bedrooms</code>, <code translate="no">population</code>, <code translate="no">households</code>, <code translate="no">median_income</code>, <code translate="no">median_house_value</code>, and <code translate="no">ocean_proximity</code>.</p>

<p>The <code translate="no">info()</code><a data-type="indexterm" data-primary="info()" id="id1035"/> method is useful to get a quick description of the data, in particular the total number of rows, each attribute‚Äôs type, and the number of non-null values:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">housing_full</code><code class="o">.</code><code class="n">info</code><code class="p">()</code><code class="w"/>
<code class="go">&lt;class 'pandas.core.frame.DataFrame'&gt;</code>
<code class="go">RangeIndex: 20640 entries, 0 to 20639</code>
<code class="go">Data columns (total 10 columns):</code>
<code class="go"> #   Column              Non-Null Count  Dtype</code>
<code class="go">---  ------              --------------  -----</code>
<code class="go"> 0   longitude           20640 non-null  float64</code>
<code class="go"> 1   latitude            20640 non-null  float64</code>
<code class="go"> 2   housing_median_age  20640 non-null  float64</code>
<code class="go"> 3   total_rooms         20640 non-null  float64</code>
<code class="go"> 4   total_bedrooms      20433 non-null  float64</code>
<code class="go"> 5   population          20640 non-null  float64</code>
<code class="go"> 6   households          20640 non-null  float64</code>
<code class="go"> 7   median_income       20640 non-null  float64</code>
<code class="go"> 8   median_house_value  20640 non-null  float64</code>
<code class="go"> 9   ocean_proximity     20640 non-null  object</code>
<code class="go">dtypes: float64(9), object(1)</code>
<code class="go">memory usage: 1.6+ MB</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In this book, when a code example contains a mix of code and outputs, as is the case here, it is formatted like in the Python interpreter for better readability: the code lines are prefixed with <code translate="no">&gt;&gt;&gt;</code> (or <code translate="no">...</code> for indented blocks), and the outputs have no prefix.</p>
</div>

<p>There are 20,640 instances in the dataset, which means that it is fairly small by machine learning standards, but it‚Äôs perfect to get started. You notice that the 
<span class="keep-together"><code translate="no">total_bedrooms</code></span> attribute has only 20,433 non-null values, meaning that 207 districts are missing this feature. You will need to take care of this later.</p>

<p>All attributes<a data-type="indexterm" data-primary="attributes" id="xi_attributes236315_1"/><a data-type="indexterm" data-primary="data structure" id="xi_datastructure236315_1"/> are numerical, except for <code translate="no">ocean_proximity</code>. Its type is <code translate="no">object</code>, so it could hold any kind of Python object. But since you loaded this data from a CSV file, you know that it must be a text attribute. When you looked at the top five rows, you probably noticed that the values in the <code translate="no">ocean_proximity</code> column were repetitive, which means that it is probably a categorical attribute. You can find out what categories exist and how many districts belong to each category by using the <code translate="no">value_counts()</code><a data-type="indexterm" data-primary="value_counts()" id="id1036"/> method:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">housing_full</code><code class="p">[</code><code class="s2">"ocean_proximity"</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code><code class="w"/>
<code class="go">ocean_proximity</code>
<code class="go">&lt;1H OCEAN     9136</code>
<code class="go">INLAND        6551</code>
<code class="go">NEAR OCEAN    2658</code>
<code class="go">NEAR BAY      2290</code>
<code class="go">ISLAND           5</code>
<code class="go">Name: count, dtype: int64</code></pre>

<p>Let‚Äôs look at the other fields. The <code translate="no">describe()</code><a data-type="indexterm" data-primary="describe()" id="id1037"/> method shows a summary of the numerical attributes (<a data-type="xref" href="#housing_describe_screenshot">Figure¬†2-7</a>).</p>

<figure><div id="housing_describe_screenshot" class="figure">
<img src="assets/hmls_0207.png" alt="A table generated by the `describe()` method, displaying statistical summaries for numerical attributes such as count, mean, standard deviation, minimum, maximum, and percentiles." width="2652" height="1047"/>
<h6><span class="label">Figure 2-7. </span>Summary of each numerical attribute</h6>
</div></figure>

<p>The <code translate="no">count</code>, <code translate="no">mean</code>, <code translate="no">min</code>, and <code translate="no">max</code> rows are self-explanatory. Note that the null values are ignored (so, for example, the <code translate="no">count</code> of <code translate="no">total_bedrooms</code> is 20,433, not 20,640). The <code translate="no">std</code> row shows the <em>standard deviation</em>, which measures how dispersed the values are.‚Å†<sup><a data-type="noteref" id="id1038-marker" href="ch02.html#id1038">5</a></sup> The <code translate="no">25%</code>, <code translate="no">50%</code>, and <code translate="no">75%</code> rows show the corresponding <em>percentiles</em><a data-type="indexterm" data-primary="percentiles" id="id1039"/>: a percentile indicates the value below which a given percentage of observations in a group of observations fall. For example, 25% of the districts have a <code translate="no">housing_median_age</code> lower than 18, while 50% are lower than 29, and 75% are lower than 37. These are often called the 25th percentile (or first <em>quartile</em>)<a data-type="indexterm" data-primary="quartiles" id="id1040"/>, the median, and the 75th percentile (or third <span class="keep-together">quartile</span>).</p>

<p>Another quick way to get a feel of the type of data you are dealing with is to plot a histogram<a data-type="indexterm" data-primary="histograms" id="xi_histograms238596_1"/> for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis). You can either plot this one attribute at a time, or you can call the <code translate="no">hist()</code> method on the whole dataset (as shown in the following code example), and it will plot a histogram for each numerical attribute (see <a data-type="xref" href="#attribute_histogram_plots">Figure¬†2-8</a>).</p>

<figure><div id="attribute_histogram_plots" class="figure">
<img src="assets/hmls_0208.png" alt="Histograms displaying the distribution of various numerical attributes such as longitude, latitude, housing median age, total rooms, total bedrooms, population, households, median income, and median house value." width="3461" height="2264"/>
<h6><span class="label">Figure 2-8. </span>A histogram for each numerical attribute</h6>
</div></figure>

<p>The number of value ranges can be adjusted using the <code translate="no">bins</code> argument (try playing with it to see how it affects the histograms):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>

<code class="n">housing_full</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">bins</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code> <code class="mi">8</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p>Looking at these histograms, you notice a few things:</p>

<ul>
<li>
<p>First, the median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes<a data-type="indexterm" data-primary="attributes" data-secondary="preprocessed" id="id1041"/><a data-type="indexterm" data-primary="preprocessed attributes" id="id1042"/> is common in machine learning, and it is not necessarily a problem, but you should try to understand how the data was computed.</p>
</li>
<li>
<p>The housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute<a data-type="indexterm" data-primary="attributes" data-secondary="target" id="id1043"/><a data-type="indexterm" data-primary="target attributes" id="id1044"/> (your labels). Your machine learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system‚Äôs output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have two options:</p>

<ul>
<li>
<p>Collect proper labels for the districts whose labels were capped.</p>
</li>
<li>
<p>Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond $500,000).</p>
</li>
</ul>
</li>
<li>
<p>These attributes have very different scales. We will discuss this later in this chapter when we explore feature scaling.</p>
</li>
<li>
<p>Finally, many histograms are <em>skewed right</em><a data-type="indexterm" data-primary="skewed left/right" id="id1045"/>: they extend much farther to the right of the median than to the left. This may make it a bit harder for some machine learning algorithms to detect patterns. Later, you‚Äôll try transforming these attributes to have more symmetrical and bell-shaped distributions.<a data-type="indexterm" data-startref="xi_histograms238596_1" id="id1046"/></p>
</li>
</ul>

<p>You should now have a better understanding of the kind of data you‚Äôre dealing with.<a data-type="indexterm" data-startref="xi_attributes236315_1" id="id1047"/><a data-type="indexterm" data-startref="xi_datastructure236315_1" id="id1048"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Create a Test Set"><div class="sect2" id="create_a_test_set">
<h2>Create a Test Set</h2>

<p>Before<a data-type="indexterm" data-primary="test set" id="xi_testset24137_1"/> you look at the data any further, you need to create a test set, put it aside, and never look at it. It may seem strange to voluntarily set aside part of the data at this stage. After all, you have only taken a quick glance at the data, and surely you should learn a whole lot more about it before you decide what algorithms to use, right? This is true, but your brain is an amazing pattern detection system, which also means that it is highly prone to overfitting<a data-type="indexterm" data-primary="overfitting of data" id="id1049"/>: if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of machine learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic, and you will launch a system that will not perform as well as expected. This is called <em>data snooping</em> bias<a data-type="indexterm" data-primary="data snooping bias" id="id1050"/><a data-type="indexterm" data-primary="bias" data-secondary="data snooping bias" id="id1051"/>.</p>

<p>Creating a test set is theoretically simple; pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside<a data-type="indexterm" data-primary="permutation()" id="id1052"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="k">def</code> <code class="nf">shuffle_and_split_data</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">test_ratio</code><code class="p">,</code> <code class="n">rng</code><code class="p">):</code>
    <code class="n">shuffled_indices</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">permutation</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">data</code><code class="p">))</code>
    <code class="n">test_set_size</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">data</code><code class="p">)</code> <code class="o">*</code> <code class="n">test_ratio</code><code class="p">)</code>
    <code class="n">test_indices</code> <code class="o">=</code> <code class="n">shuffled_indices</code><code class="p">[:</code><code class="n">test_set_size</code><code class="p">]</code>
    <code class="n">train_indices</code> <code class="o">=</code> <code class="n">shuffled_indices</code><code class="p">[</code><code class="n">test_set_size</code><code class="p">:]</code>
    <code class="k">return</code> <code class="n">data</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">train_indices</code><code class="p">],</code> <code class="n">data</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">test_indices</code><code class="p">]</code></pre>

<p>You<a data-type="indexterm" data-primary="shuffle_and_split_data()" id="id1053"/> can then use this function like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">default_rng</code><code class="p">()</code>  <code class="c1"># default random number generator</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">train_set</code><code class="p">,</code> <code class="n">test_set</code> <code class="o">=</code> <code class="n">shuffle_and_split_data</code><code class="p">(</code><code class="n">housing_full</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">,</code> <code class="n">rng</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">len</code><code class="p">(</code><code class="n">train_set</code><code class="p">)</code><code class="w"/>
<code class="go">16512</code>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">len</code><code class="p">(</code><code class="n">test_set</code><code class="p">)</code><code class="w"/>
<code class="go">4128</code></pre>

<p>Well, this works, but it is not perfect: if you run the program again, it will generate a different test set! Over time, you (or your machine learning algorithms) will get to see the whole dataset, which is what you want to avoid.</p>

<p>One solution is to save the test set on the first run and then load it in subsequent runs. Another option is to set the random number generator‚Äôs seed (e.g., by passing <code translate="no">seed=42</code> to the <code translate="no">default_rng()</code> function)‚Å†<sup><a data-type="noteref" id="id1054-marker" href="ch02.html#id1054">6</a></sup> to ensure it always generates the same sequence of random numbers every time you run the program.</p>

<p>However, both these solutions will break the next time you fetch an updated dataset. To have a stable train/test split even after updating the dataset, a common solution is to use each instance‚Äôs identifier to decide whether it should go in the test set (assuming instances have unique and immutable identifiers). For example, you could compute a hash of each instance‚Äôs identifier and put that instance in the test set if the hash is lower than or equal to 20% of the maximum hash value. This ensures that the test set will remain consistent across multiple runs, even if you refresh the dataset. The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set.</p>

<p>Here is a possible implementation:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">zlib</code> <code class="kn">import</code> <code class="n">crc32</code>

<code class="k">def</code> <code class="nf">is_id_in_test_set</code><code class="p">(</code><code class="n">identifier</code><code class="p">,</code> <code class="n">test_ratio</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">crc32</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">int64</code><code class="p">(</code><code class="n">identifier</code><code class="p">))</code> <code class="o">&lt;</code> <code class="n">test_ratio</code> <code class="o">*</code> <code class="mi">2</code><code class="o">**</code><code class="mi">32</code>

<code class="k">def</code> <code class="nf">split_data_with_id_hash</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">test_ratio</code><code class="p">,</code> <code class="n">id_column</code><code class="p">):</code>
    <code class="n">ids</code> <code class="o">=</code> <code class="n">data</code><code class="p">[</code><code class="n">id_column</code><code class="p">]</code>
    <code class="n">in_test_set</code> <code class="o">=</code> <code class="n">ids</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="k">lambda</code> <code class="n">id_</code><code class="p">:</code> <code class="n">is_id_in_test_set</code><code class="p">(</code><code class="n">id_</code><code class="p">,</code> <code class="n">test_ratio</code><code class="p">))</code>
    <code class="k">return</code> <code class="n">data</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="o">~</code><code class="n">in_test_set</code><code class="p">],</code> <code class="n">data</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">in_test_set</code><code class="p">]</code></pre>

<p>Unfortunately, the housing dataset does not have an identifier column. The simplest solution is to use the row index as the ID:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing_with_id</code> <code class="o">=</code> <code class="n">housing_full</code><code class="o">.</code><code class="n">reset_index</code><code class="p">()</code>  <code class="c1"># adds an `index` column</code>
<code class="n">train_set</code><code class="p">,</code> <code class="n">test_set</code> <code class="o">=</code> <code class="n">split_data_with_id_hash</code><code class="p">(</code><code class="n">housing_with_id</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">,</code> <code class="s2">"index"</code><code class="p">)</code></pre>

<p>If you use the row index as a unique identifier, you need to make sure that new data gets appended to the end of the dataset and that no row ever gets deleted. If this is not possible, then you can try to use the most stable features to build a unique identifier. For example, a district‚Äôs latitude and longitude are guaranteed to be stable for a few million years, so you could combine them into an ID like so:‚Å†<sup><a data-type="noteref" id="id1055-marker" href="ch02.html#id1055">7</a></sup></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing_with_id</code><code class="p">[</code><code class="s2">"id"</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="n">housing_full</code><code class="p">[</code><code class="s2">"longitude"</code><code class="p">]</code> <code class="o">*</code> <code class="mi">1000</code>
                         <code class="o">+</code> <code class="n">housing_full</code><code class="p">[</code><code class="s2">"latitude"</code><code class="p">])</code>
<code class="n">train_set</code><code class="p">,</code> <code class="n">test_set</code> <code class="o">=</code> <code class="n">split_data_with_id_hash</code><code class="p">(</code><code class="n">housing_with_id</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">,</code> <code class="s2">"id"</code><code class="p">)</code></pre>

<p>Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is <code translate="no">train_test_split()</code><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.train_test_split()" id="id1056"/><a data-type="indexterm" data-primary="train_test_split()" id="id1057"/>, which does pretty much the same thing as the <code translate="no">shuffle_and_split_data()</code> function we defined earlier, with a couple of additional features. First, there is a <code translate="no">random_state</code> parameter that allows you to set the random generator seed. Second, you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (this is very useful, for example, if you have a separate DataFrame for labels):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="n">train_set</code><code class="p">,</code> <code class="n">test_set</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">housing_full</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code>
                                       <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code></pre>

<p>So far we have considered purely random sampling methods. This is generally fine if your dataset is large enough (especially relative to the number of attributes), but if it is not, you run the risk of introducing a significant sampling bias<a data-type="indexterm" data-primary="sampling bias" id="id1058"/><a data-type="indexterm" data-primary="bias" data-secondary="sampling bias" id="id1059"/>. When employees at a survey company decide to call 1,000 people to ask them a few questions, they don‚Äôt just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are representative of the whole population, with regard to the questions they want to ask. For example, according to the US Census Bureau, 51.6% of citizens of voting age are female, while 48.4% are male, so a well-conducted survey in the US would try to maintain this ratio in the sample: 516 females and 484 males (at least if it seems likely that the answers may vary across genders). This is called <em>stratified sampling</em><a data-type="indexterm" data-primary="stratified sampling" id="id1060"/>: the population is divided into homogeneous subgroups called <em>strata</em>, and the right number of instances are sampled from each stratum to guarantee that the test set is representative of the overall population. If the people running the survey used purely random sampling, there would be over 10% chance of sampling a skewed test set with less than 49% female or more than 54% female participants. Either way, the survey results would likely be quite biased.</p>

<p>Suppose you‚Äôve chatted with some experts who told you that the median income is a very important attribute to predict median housing prices. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset. Since the median income is a continuous numerical attribute, you first need to create an income category attribute. Let‚Äôs look at the median income histogram more closely (back in <a data-type="xref" href="#attribute_histogram_plots">Figure¬†2-8</a>): most median income values are clustered around 1.5 to 6 (i.e., $15,000‚Äì$60,000), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of a stratum‚Äôs importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. The following code uses the <code translate="no">pd.cut()</code> function to create an income category attribute with five categories (labeled from 1 to 5); category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing_full</code><code class="p">[</code><code class="s2">"income_cat"</code><code class="p">]</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">cut</code><code class="p">(</code><code class="n">housing_full</code><code class="p">[</code><code class="s2">"median_income"</code><code class="p">],</code>
                                    <code class="n">bins</code><code class="o">=</code><code class="p">[</code><code class="mf">0.</code><code class="p">,</code> <code class="mf">1.5</code><code class="p">,</code> <code class="mf">3.0</code><code class="p">,</code> <code class="mf">4.5</code><code class="p">,</code> <code class="mf">6.</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">inf</code><code class="p">],</code>
                                    <code class="n">labels</code><code class="o">=</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">])</code></pre>

<p>These income categories are represented in <a data-type="xref" href="#housing_income_cat_bar_plot">Figure¬†2-9</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">cat_counts</code> <code class="o">=</code> <code class="n">housing_full</code><code class="p">[</code><code class="s2">"income_cat"</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code><code class="o">.</code><code class="n">sort_index</code><code class="p">()</code>
<code class="n">cat_counts</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">rot</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">grid</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Income category"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Number of districts"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p>Now you are ready to do stratified sampling based on the income category. Scikit-Learn provides a number of splitter classes in the <code translate="no">sklearn.model_selection</code> package that implement various strategies to split your dataset into a training set and a test set. Each splitter has a <code translate="no">split()</code> method that returns an iterator over different training/test splits of the same data.</p>

<figure class="width-70"><div id="housing_income_cat_bar_plot" class="figure">
<img src="assets/hmls_0209.png" alt="Bar chart illustrating the distribution of districts across five income categories, with categories two and three having the highest counts." width="1797" height="1314"/>
<h6><span class="label">Figure 2-9. </span>Histogram of income categories</h6>
</div></figure>

<p>To be precise, the <code translate="no">split()</code> method yields the training and test <em>indices</em>, not the data itself. Having multiple splits can be useful if you want to better estimate the performance of your model, as you will see when we discuss cross-validation later in this chapter. For example, the following code generates 10 different stratified splits<a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.StratifiedShuffleSplit" id="id1061"/><a data-type="indexterm" data-primary="StratifiedShuffleSplit" id="id1062"/> of the same dataset:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">StratifiedShuffleSplit</code>

<code class="n">splitter</code> <code class="o">=</code> <code class="n">StratifiedShuffleSplit</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">strat_splits</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">train_index</code><code class="p">,</code> <code class="n">test_index</code> <code class="ow">in</code> <code class="n">splitter</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">housing_full</code><code class="p">,</code>
                                              <code class="n">housing_full</code><code class="p">[</code><code class="s2">"income_cat"</code><code class="p">]):</code>
    <code class="n">strat_train_set_n</code> <code class="o">=</code> <code class="n">housing_full</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">train_index</code><code class="p">]</code>
    <code class="n">strat_test_set_n</code> <code class="o">=</code> <code class="n">housing_full</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">test_index</code><code class="p">]</code>
    <code class="n">strat_splits</code><code class="o">.</code><code class="n">append</code><code class="p">([</code><code class="n">strat_train_set_n</code><code class="p">,</code> <code class="n">strat_test_set_n</code><code class="p">])</code></pre>

<p>For now, you can just use the first split:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">strat_train_set</code><code class="p">,</code> <code class="n">strat_test_set</code> <code class="o">=</code> <code class="n">strat_splits</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>

<p>Or, since stratified sampling is fairly common, there‚Äôs a shorter way to get a single split using the <code translate="no">train_test_split()</code><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.train_test_split()" id="id1063"/><a data-type="indexterm" data-primary="train_test_split()" id="id1064"/> function with the <code translate="no">stratify</code> argument:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">strat_train_set</code><code class="p">,</code> <code class="n">strat_test_set</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">housing_full</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">stratify</code><code class="o">=</code><code class="n">housing_full</code><code class="p">[</code><code class="s2">"income_cat"</code><code class="p">],</code>
    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code></pre>

<p>Let‚Äôs see if this worked as expected. You can start by looking at the income category proportions in the test set:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">strat_test_set</code><code class="p">[</code><code class="s2">"income_cat"</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code> <code class="o">/</code> <code class="nb">len</code><code class="p">(</code><code class="n">strat_test_set</code><code class="p">)</code><code class="w"/>
<code class="go">income_cat</code>
<code class="go">3    0.350533</code>
<code class="go">2    0.318798</code>
<code class="go">4    0.176357</code>
<code class="go">5    0.114341</code>
<code class="go">1    0.039971</code>
<code class="go">Name: count, dtype: float64</code></pre>

<p>With similar code you can measure the income category proportions in the full dataset. <a data-type="xref" href="#compare_sampling_errors_screenshot">Figure¬†2-10</a> compares the income category proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated using purely random sampling. As you can see, the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is skewed.</p>

<figure class="width-80"><div id="compare_sampling_errors_screenshot" class="figure">
<img src="assets/hmls_0210.png" alt="Table comparing income category proportions in the overall dataset, stratified sampling, and random sampling, highlighting lower errors in stratified sampling." width="2247" height="745"/>
<h6><span class="label">Figure 2-10. </span>Sampling bias comparison of stratified versus purely random sampling</h6>
</div></figure>

<p>You won‚Äôt use the <code translate="no">income_cat</code> column again, so you might as well drop it, reverting the data back to its original state:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">set_</code> <code class="ow">in</code> <code class="p">(</code><code class="n">strat_train_set</code><code class="p">,</code> <code class="n">strat_test_set</code><code class="p">):</code>
    <code class="n">set_</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s2">"income_cat"</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>We spent quite a bit of time on test set generation for a good reason: this is an often neglected but critical part of a machine learning project. Moreover, many of these ideas will be useful later when we discuss cross-validation. Now it‚Äôs time to move on to the next stage: exploring the data.<a data-type="indexterm" data-startref="xi_endtoendMLprojectexercisegettingthedata22385_1" id="id1065"/><a data-type="indexterm" data-startref="xi_testset24137_1" id="id1066"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Explore and Visualize the Data to Gain Insights"><div class="sect1" id="id39">
<h1>Explore and Visualize the Data to Gain Insights</h1>

<p>So<a data-type="indexterm" data-primary="end-to-end ML project" data-secondary="discovering and visualizing data" id="xi_endtoendMLprojectexercisediscoveringandvisualizingdata25833_1"/><a data-type="indexterm" data-primary="training set" data-secondary="visualizing data" id="xi_trainingsetvisualizingdata25833_1"/><a data-type="indexterm" data-primary="visualization of data" id="xi_visualizationofdata25833_1"/><a data-type="indexterm" data-primary="visualization of data" data-secondary="end-to-end project" id="xi_visualizationofdataendtoendexercise25833_1"/> <a data-type="indexterm" data-primary="geographic data, visualizing" id="xi_geographicdatavisualizing25834_1"/>far you have only taken a quick glance at the data to get a general understanding of the kind of data you are manipulating. Now the goal is to go into a little more depth.</p>

<p>First, make sure you have put the test set aside and you are only exploring the training set. Also, if the training set is very large, you may want to sample an exploration set, to make manipulations easy and fast during the exploration phase. In this case, the training set is quite small, so you can just work directly on the full set. Since you‚Äôre going to experiment with various transformations of the full training set, you should make a copy of the original so you can revert to it afterwards:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing</code> <code class="o">=</code> <code class="n">strat_train_set</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code></pre>








<section data-type="sect2" data-pdf-bookmark="Visualizing Geographical Data"><div class="sect2" id="id40">
<h2>Visualizing Geographical Data</h2>

<p>Because<a data-type="indexterm" data-startref="xi_trainingsetvisualizingdata25833_1" id="id1067"/> the dataset includes geographical information (latitude and longitude), it is a good idea to create a scatterplot of all the districts to visualize the data (<a data-type="xref" href="#bad_visualization_plot">Figure¬†2-11</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s2">"scatter"</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s2">"longitude"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"latitude"</code><code class="p">,</code> <code class="n">grid</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure class="width-75"><div id="bad_visualization_plot" class="figure">
<img src="assets/hmls_0211.png" alt="Scatter plot displaying geographical data points with longitude on the x-axis and latitude on the y-axis, showing a distribution resembling California." width="1787" height="1314"/>
<h6><span class="label">Figure 2-11. </span>A geographical scatterplot of the data</h6>
</div></figure>

<p>This looks like California all right, but other than that it is hard to see any particular pattern. Setting the <code translate="no">alpha</code> option to <code translate="no">0.2</code> makes it much easier to visualize the places where there is a high density of data points (<a data-type="xref" href="#better_visualization_plot">Figure¬†2-12</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s2">"scatter"</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s2">"longitude"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"latitude"</code><code class="p">,</code> <code class="n">grid</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.2</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p>Now that‚Äôs much better: you can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high-density areas in the Central Valley (in particular, around Sacramento and Fresno).</p>

<p>Our brains are very good at spotting patterns in pictures, but you may need to play around with visualization parameters to make the patterns stand out.</p>

<figure class="width-80"><div id="better_visualization_plot" class="figure">
<img src="assets/hmls_0212.png" alt="Scatter plot showing the distribution of housing locations by latitude and longitude, illustrating high-density areas with darker blue clusters." width="1787" height="1314"/>
<h6><span class="label">Figure 2-12. </span>A better visualization that highlights high-density areas</h6>
</div></figure>

<p>Next, you look at the housing prices (<a data-type="xref" href="#housing_prices_scatterplot">Figure¬†2-13</a>). The radius of each circle represents the district‚Äôs population (option <code translate="no">s</code>), and the color represents the price (option <code translate="no">c</code>). Here you use a predefined color map (option <code translate="no">cmap</code>) called <code translate="no">jet</code>, which ranges from blue (low values) to red (high prices):‚Å†<sup><a data-type="noteref" id="id1068-marker" href="ch02.html#id1068">8</a></sup></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s2">"scatter"</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s2">"longitude"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"latitude"</code><code class="p">,</code> <code class="n">grid</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
             <code class="n">s</code><code class="o">=</code><code class="n">housing</code><code class="p">[</code><code class="s2">"population"</code><code class="p">]</code> <code class="o">/</code> <code class="mi">100</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"population"</code><code class="p">,</code>
             <code class="n">c</code><code class="o">=</code><code class="s2">"median_house_value"</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"jet"</code><code class="p">,</code> <code class="n">colorbar</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
             <code class="n">legend</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">sharex</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">7</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p>This image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density, as you probably knew already. A clustering algorithm should be useful for detecting the main cluster and for adding new features that measure the proximity to the cluster centers. The ocean proximity attribute may be useful as well, although in Northern California the housing prices in coastal districts are not too high, so it is not a simple rule.<a data-type="indexterm" data-startref="xi_geographicdatavisualizing25834_1" id="id1069"/></p>

<figure><div id="housing_prices_scatterplot" class="figure">
<img src="assets/hmls_0213.png" alt="Scatter plot showing California housing prices by location, with red indicating expensive areas and blue indicating cheaper ones; larger circles represent areas with larger population density." width="2706" height="1971"/>
<h6><span class="label">Figure 2-13. </span>California housing prices: red is expensive, blue is cheap, larger circles indicate areas with a larger population</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Look for Correlations"><div class="sect2" id="id41">
<h2>Look for Correlations</h2>

<p>Since the dataset is not too large, you can easily compute the <em>standard correlation coefficient</em><a data-type="indexterm" data-primary="correlation coefficient" id="xi_correlationcoefficient264198_1"/><a data-type="indexterm" data-primary="data" data-secondary="finding correlations in" id="xi_datafindingcorrelationsin264198_1"/><a data-type="indexterm" data-primary="Pandas library" id="xi_Pandas264198_1"/><a data-type="indexterm" data-primary="Pearson‚Äôs r" id="id1070"/><a data-type="indexterm" data-primary="scatter_matrix" id="xi_scatter_matrix264198_1"/><a data-type="indexterm" data-primary="standard correlation coefficient" id="id1071"/> (also called <em>Pearson‚Äôs r</em>) between every pair of numerical attributes using the <code translate="no">corr()</code> method:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">corr_matrix</code> <code class="o">=</code> <code class="n">housing</code><code class="o">.</code><code class="n">corr</code><code class="p">(</code><code class="n">numeric_only</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>Now you can look at how much each attribute correlates with the median house value:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">corr_matrix</code><code class="p">[</code><code class="s2">"median_house_value"</code><code class="p">]</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code><code class="w"/>
<code class="go">median_house_value    1.000000</code>
<code class="go">median_income         0.688380</code>
<code class="go">total_rooms           0.137455</code>
<code class="go">housing_median_age    0.102175</code>
<code class="go">households            0.071426</code>
<code class="go">total_bedrooms        0.054635</code>
<code class="go">population           -0.020153</code>
<code class="go">longitude            -0.050859</code>
<code class="go">latitude             -0.139584</code>
<code class="go">Name: median_house_value, dtype: float64</code></pre>

<p>The correlation coefficient ranges from ‚Äì1 to 1. When it is close to 1, it means that there is a strong positive correlation; for example, the median house value tends to go up when the median income goes up. When the coefficient is close to ‚Äì1, it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north). Finally, coefficients close to 0 mean that there is no linear correlation.</p>

<p>Another way to check for correlation between attributes is to use the Pandas <span class="keep-together"><code>scatter_matrix()</code></span> function, which plots every numerical attribute against every other numerical attribute. Since there are now 9 numerical attributes, you would get 9<sup>2</sup> = 81 plots, which would not fit on a page‚Äîso you decide to focus on a few promising attributes that seem most correlated with the median housing value (<a data-type="xref" href="#scatter_matrix_plot">Figure¬†2-14</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">pandas.plotting</code> <code class="kn">import</code> <code class="n">scatter_matrix</code>

<code class="n">attributes</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"median_house_value"</code><code class="p">,</code> <code class="s2">"median_income"</code><code class="p">,</code> <code class="s2">"total_rooms"</code><code class="p">,</code>
              <code class="s2">"housing_median_age"</code><code class="p">]</code>
<code class="n">scatter_matrix</code><code class="p">(</code><code class="n">housing</code><code class="p">[</code><code class="n">attributes</code><code class="p">],</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code> <code class="mi">8</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure><div id="scatter_matrix_plot" class="figure">
<img src="assets/hmls_0214.png" alt="Scatter matrix displaying pairwise comparisons of median house value, median income, total rooms, and housing median age, with histograms on the diagonals." width="3471" height="2335"/>
<h6><span class="label">Figure 2-14. </span>This scatter matrix plots every numerical attribute against every other numerical attribute, plus a histogram of each numerical attribute‚Äôs values on the main diagonal (top left to bottom right)</h6>
</div></figure>

<p>The main diagonal would be full of straight lines if Pandas plotted each variable against itself, which would not be very useful. So instead, the Pandas displays a histogram of each attribute (other options are available; see the Pandas documentation for more details).</p>

<p>Looking at the correlation scatterplots, it seems like the most promising attribute to predict the median house value is the median income, so you zoom in on that scatterplot (<a data-type="xref" href="#income_vs_house_value_scatterplot">Figure¬†2-15</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s2">"scatter"</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s2">"median_income"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"median_house_value"</code><code class="p">,</code>
             <code class="n">alpha</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">grid</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure class="smallereighty"><div id="income_vs_house_value_scatterplot" class="figure">
<img src="assets/hmls_0215.png" alt="Scatter plot showing the correlation between median income and median house value, highlighting an upward trend and visible price caps at specific values." width="1791" height="1313"/>
<h6><span class="label">Figure 2-15. </span>Median income versus median house value</h6>
</div></figure>

<p>This plot reveals a few things. First, the correlation is indeed quite strong; you can clearly see the upward trend, although the data is noisy. Second, the price cap you noticed earlier is clearly visible as a horizontal line at $500,000. But the plot also reveals other less obvious straight lines: a horizontal line around $450,000, another around $350,000, perhaps one around $280,000, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks.</p>

<p>Note that the correlation coefficient only measures linear correlations (‚Äúas <em>x</em> goes up, <em>y</em> generally goes up/down‚Äù). It may completely miss out on nonlinear relationships (e.g., ‚Äúas <em>x</em> approaches 0, <em>y</em> generally goes up‚Äù). <a data-type="xref" href="#correlation_coefficient_plots">Figure¬†2-16</a> shows a variety of datasets along with their correlation coefficient. Note how all the plots of the bottom row have a correlation coefficient equal to 0, despite the fact that their axes are clearly <em>not</em> independent: these are examples of nonlinear relationships. Also, the second row shows examples where the correlation coefficient is equal to 1 or ‚Äì1; notice that this has nothing to do with the slope. For example, your height in inches has a correlation coefficient of 1 with your height in feet or in nanometers.<a data-type="indexterm" data-startref="xi_correlationcoefficient264198_1" id="id1072"/><a data-type="indexterm" data-startref="xi_datafindingcorrelationsin264198_1" id="id1073"/><a data-type="indexterm" data-startref="xi_Pandas264198_1" id="id1074"/><a data-type="indexterm" data-startref="xi_scatter_matrix264198_1" id="id1075"/></p>

<figure><div id="correlation_coefficient_plots" class="figure">
<img src="assets/hmls_0216.png" alt="Scatter plots of various datasets showing how correlation coefficients can indicate strong linear relationships in some cases but fail to capture nonlinear relationships, with coefficients ranging from -1 to 1." width="2409" height="1049"/>
<h6><span class="label">Figure 2-16. </span>Standard correlation coefficient of various datasets (source: Wikipedia; public domain image)</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Experiment with Attribute Combinations"><div class="sect2" id="id42">
<h2>Experiment with Attribute Combinations</h2>

<p>Hopefully<a data-type="indexterm" data-primary="attributes" data-secondary="combinations of" id="xi_attributescombinationsof271110_1"/> the previous sections gave you an idea of a few ways you can explore the data and gain insights. You identified a few data quirks that you may want to clean up before feeding the data to a machine learning algorithm, and you found interesting correlations between attributes, in particular with the target attribute. You also noticed that some attributes have a skewed-right distribution, so you may want to transform them (e.g., by computing their logarithm or square root). Of course, your mileage will vary considerably with each project, but the general ideas are similar.</p>

<p>One last thing you may want to do before preparing the data for machine learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don‚Äôt know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the total number of rooms. And the population per household also seems like an interesting attribute combination to look at. You create these new attributes as follows:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing</code><code class="p">[</code><code class="s2">"rooms_per_house"</code><code class="p">]</code> <code class="o">=</code> <code class="n">housing</code><code class="p">[</code><code class="s2">"total_rooms"</code><code class="p">]</code> <code class="o">/</code> <code class="n">housing</code><code class="p">[</code><code class="s2">"households"</code><code class="p">]</code>
<code class="n">housing</code><code class="p">[</code><code class="s2">"bedrooms_ratio"</code><code class="p">]</code> <code class="o">=</code> <code class="n">housing</code><code class="p">[</code><code class="s2">"total_bedrooms"</code><code class="p">]</code> <code class="o">/</code> <code class="n">housing</code><code class="p">[</code><code class="s2">"total_rooms"</code><code class="p">]</code>
<code class="n">housing</code><code class="p">[</code><code class="s2">"people_per_house"</code><code class="p">]</code> <code class="o">=</code> <code class="n">housing</code><code class="p">[</code><code class="s2">"population"</code><code class="p">]</code> <code class="o">/</code> <code class="n">housing</code><code class="p">[</code><code class="s2">"households"</code><code class="p">]</code></pre>

<p>And then you look at the correlation matrix again:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">corr_matrix</code> <code class="o">=</code> <code class="n">housing</code><code class="o">.</code><code class="n">corr</code><code class="p">(</code><code class="n">numeric_only</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">corr_matrix</code><code class="p">[</code><code class="s2">"median_house_value"</code><code class="p">]</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code><code class="w"/>
<code class="go">median_house_value    1.000000</code>
<code class="go">median_income         0.688380</code>
<code class="go">rooms_per_house       0.143663</code>
<code class="go">total_rooms           0.137455</code>
<code class="go">housing_median_age    0.102175</code>
<code class="go">households            0.071426</code>
<code class="go">total_bedrooms        0.054635</code>
<code class="go">population           -0.020153</code>
<code class="go">people_per_house     -0.038224</code>
<code class="go">longitude            -0.050859</code>
<code class="go">latitude             -0.139584</code>
<code class="go">bedrooms_ratio       -0.256397</code>
<code class="go">Name: median_house_value, dtype: float64</code></pre>

<p>Hey, not bad! The new <code translate="no">bedrooms_ratio</code> attribute is much more correlated with the median house value than the total number of rooms or bedrooms. It‚Äôs a strong negative correlation, so it looks like houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district‚Äîobviously the larger the houses, the more expensive they are.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>When creating new combined features, make sure they are not too linearly correlated with existing features: <em>collinearity</em> can cause issues with some models, such as linear regression. In particular, avoid simple weighted sums of existing features.</p>
</div>

<p>This round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.<a data-type="indexterm" data-startref="xi_attributescombinationsof271110_1" id="id1076"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Prepare the Data for Machine Learning Algorithms"><div class="sect1" id="id43">
<h1>Prepare the Data for Machine Learning Algorithms</h1>

<p>It‚Äôs<a data-type="indexterm" data-primary="algorithms, preparing data for" id="xi_algorithmspreparingdatafor27505_1"/><a data-type="indexterm" data-primary="data" data-secondary="preparing for ML algorithms" data-see="algorithms" id="id1077"/><a data-type="indexterm" data-primary="end-to-end ML project" data-secondary="preparing data for ML algorithms" data-see="algorithms" id="id1078"/><a data-type="indexterm" data-primary="training set" data-secondary="preparing for ML algorithms" id="xi_trainingsetpreparingforMLalgorithms27505_1"/> time to prepare the data for your machine learning algorithms. Instead of doing this manually, you should write functions for this purpose, for several good reasons:</p>

<ul>
<li>
<p>This will allow you to reproduce these transformations easily on any dataset (e.g., the next time you get a fresh dataset).</p>
</li>
<li>
<p>You will gradually build a library of transformation functions that you can reuse in future projects.</p>
</li>
<li>
<p>You can use these functions in your live system to transform the new data before feeding it to your algorithms.</p>
</li>
<li>
<p>This will make it possible for you to easily try various transformations and see which combination of transformations works best.</p>
</li>
</ul>

<p>But first, revert to a clean training set (by copying <code translate="no">strat_train_set</code> once again). You should also separate the predictors and the labels, since you don‚Äôt necessarily want to apply the same transformations to the predictors and the target values (note that <code translate="no">drop()</code> creates a copy of the data and does not affect <code translate="no">strat_train_set</code>)<a data-type="indexterm" data-primary="strat_train_set" id="id1079"/>:<a data-type="indexterm" data-primary="drop(), Pandas" id="id1080"/><a data-type="indexterm" data-startref="xi_endtoendMLprojectexercisediscoveringandvisualizingdata25833_1" id="id1081"/><a data-type="indexterm" data-startref="xi_trainingsetpreparingforMLalgorithms27505_1" id="id1082"/><a data-type="indexterm" data-startref="xi_visualizationofdata25833_1" id="id1083"/><a data-type="indexterm" data-startref="xi_visualizationofdataendtoendexercise25833_1" id="id1084"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing</code> <code class="o">=</code> <code class="n">strat_train_set</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s2">"median_house_value"</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">housing_labels</code> <code class="o">=</code> <code class="n">strat_train_set</code><code class="p">[</code><code class="s2">"median_house_value"</code><code class="p">]</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code></pre>








<section data-type="sect2" data-pdf-bookmark="Clean the Data"><div class="sect2" id="id44">
<h2>Clean the Data</h2>

<p>Most<a data-type="indexterm" data-primary="algorithms, preparing data for" data-secondary="cleaning the data" id="xi_algorithmspreparingdataforcleaningthedata27665_1"/><a data-type="indexterm" data-primary="data cleaning" id="xi_datacleaning27665_1"/> machine learning algorithms cannot work with missing features, so you‚Äôll need to take care of these. For example, you noticed earlier that the <code translate="no">total_bedrooms</code> attribute has some missing values. You have three options to fix this:</p>
<ol>
<li>
<p>Get rid of the corresponding districts.</p>
</li>
<li>
<p>Get rid of the whole attribute.</p>
</li>
<li>
<p>Set the missing values to some value (zero, the mean, the median, etc.). This is called <em>imputation</em>.<a data-type="indexterm" data-primary="imputation" id="id1085"/></p>
</li>

</ol>

<p>You can accomplish these easily using the Pandas DataFrame<a data-type="indexterm" data-primary="DataFrame" id="id1086"/>‚Äôs <code translate="no">dropna()</code><a data-type="indexterm" data-primary="dropna(), Pandas" id="id1087"/>, <code translate="no">drop()</code>, and <code translate="no">fillna()</code><a data-type="indexterm" data-primary="fillna()" id="id1088"/> methods:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">subset</code><code class="o">=</code><code class="p">[</code><code class="s2">"total_bedrooms"</code><code class="p">],</code> <code class="n">inplace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>  <code class="c1"># option 1</code>

<code class="n">housing</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s2">"total_bedrooms"</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>  <code class="c1"># option 2</code>

<code class="n">median</code> <code class="o">=</code> <code class="n">housing</code><code class="p">[</code><code class="s2">"total_bedrooms"</code><code class="p">]</code><code class="o">.</code><code class="n">median</code><code class="p">()</code>  <code class="c1"># option 3</code>
<code class="n">housing</code><code class="p">[</code><code class="s2">"total_bedrooms"</code><code class="p">]</code> <code class="o">=</code> <code class="n">housing</code><code class="p">[</code><code class="s2">"total_bedrooms"</code><code class="p">]</code><code class="o">.</code><code class="n">fillna</code><code class="p">(</code><code class="n">median</code><code class="p">)</code></pre>

<p>You decide to go for option 3 since it is the least destructive, but instead of the preceding code, you will use a handy Scikit-Learn class: <code translate="no">SimpleImputer</code><a data-type="indexterm" data-primary="sklearn" data-secondary="impute.SimpleImputer" id="id1089"/><a data-type="indexterm" data-primary="SimpleImputer" id="id1090"/>. The benefit is that it will store the median value of each feature: this will make it possible to impute missing values not only on the training set, but also on the validation set, the test set, and any new data fed to the model. To use it, first you need to create a <code translate="no">SimpleImputer</code> instance, specifying that you want to replace each attribute‚Äôs missing values with the median of that attribute:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.impute</code> <code class="kn">import</code> <code class="n">SimpleImputer</code>

<code class="n">imputer</code> <code class="o">=</code> <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s2">"median"</code><code class="p">)</code></pre>

<p>Since the median can only be computed on numerical attributes, you then need to create a copy of the data with only the numerical attributes (this will exclude the text attribute <code translate="no">ocean_proximity</code>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing_num</code> <code class="o">=</code> <code class="n">housing</code><code class="o">.</code><code class="n">select_dtypes</code><code class="p">(</code><code class="n">include</code><code class="o">=</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">number</code><code class="p">])</code></pre>

<p>Now you can fit the <code translate="no">imputer</code> instance to the training data using the <code translate="no">fit()</code><a data-type="indexterm" data-primary="fit(), Scikit-Learn" data-secondary="data cleaning" id="id1091"/> method:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">imputer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">housing_num</code><code class="p">)</code></pre>

<p>The <code translate="no">imputer</code> has simply computed the median of each attribute and stored the result in its <code translate="no">statistics_</code> instance variable. Only the <code translate="no">total_bedrooms</code> attribute had missing values, but you cannot be sure that there won‚Äôt be any missing values in new data after the system goes live, so it is safer to apply the <code translate="no">imputer</code> to all the numerical attributes:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">imputer</code><code class="o">.</code><code class="n">statistics_</code><code class="w"/>
<code class="go">array([-118.51 , 34.26 , 29. , 2125. , 434. , 1167. , 408. , 3.5385])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">housing_num</code><code class="o">.</code><code class="n">median</code><code class="p">()</code><code class="o">.</code><code class="n">values</code><code class="w"/>
<code class="go">array([-118.51 , 34.26 , 29. , 2125. , 434. , 1167. , 408. , 3.5385])</code></pre>

<p>Now you can use this ‚Äútrained‚Äù <code translate="no">imputer</code> to transform the training set by replacing missing values with the learned medians:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">X</code> <code class="o">=</code> <code class="n">imputer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">housing_num</code><code class="p">)</code></pre>

<p>Missing values can also be replaced with the mean value (<code translate="no">strategy="mean"</code>), or with the most frequent value (<code translate="no">strategy="most_frequent"</code>), or with a constant value (<code translate="no">strategy="constant", fill_value=</code>‚Ä¶‚Äã). The last two strategies support non-numerical data.</p>
<div data-type="tip"><h6>Tip</h6>
<p>There are also more powerful imputers available in the <code>sklearn.‚Äãimpute</code> package (both for numerical features only):</p>

<ul>
<li>
<p><code translate="no">KNNImputer</code><a data-type="indexterm" data-primary="KNNImputer" id="id1092"/><a data-type="indexterm" data-primary="sklearn" data-secondary="impute.KNNImputer" id="id1093"/> replaces each missing value with the mean of the <em>k</em>-nearest neighbors‚Äô values for that feature. The distance is based on all the available features.</p>
</li>
<li>
<p><code translate="no">IterativeImputer</code><a data-type="indexterm" data-primary="IterativeImputer" id="id1094"/><a data-type="indexterm" data-primary="sklearn" data-secondary="impute.IterativeImputer" id="id1095"/> trains a regression model per feature to predict the missing values based on all the other available features. It then trains the model again on the updated data, and repeats the process several times, improving the models and the replacement values at each iteration.<a data-type="indexterm" data-primary="estimators, Scikit-Learn" id="id1096"/><a data-type="indexterm" data-primary="fit_transform()" id="id1097"/><a data-type="indexterm" data-primary="predict()" id="id1098"/><a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="design principles" id="xi_ScikitLearndesignprinciples2831290_1"/><a data-type="indexterm" data-primary="score()" id="id1099"/><a data-type="indexterm" data-primary="transform()" id="id1100"/><a data-type="indexterm" data-primary="transformation of data" data-secondary="estimator transformers" id="id1101"/></p>
</li>
</ul>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="scikit_learn_design">
<h1>Scikit-Learn Design</h1>
<p>Scikit-Learn‚Äôs API is remarkably well designed. These are the <a href="https://homl.info/11">main design principles</a>:‚Å†<sup><a data-type="noteref" id="id1102-marker" href="ch02.html#id1102">9</a></sup></p>
<dl>
<dt>Consistency</dt>
<dd>
<p>All objects share a consistent and simple interface:</p>

<dl>
<dt>Estimators</dt>
<dd>
<p>Any object that can estimate some parameters based on a dataset is called an <em>estimator</em> (e.g., a <code>SimpleImputer</code> is an estimator). The estimation itself is performed by the <code>fit()</code> method, and it takes a dataset as a parameter, or two for supervised learning algorithms‚Äîthe second dataset contains the labels. Any other parameter needed to guide the estimation process is considered a hyperparameter (such as a <code>SimpleImputer</code>‚Äôs <code>strategy</code>), and it must be set as an instance variable (generally via a constructor parameter).</p>
</dd>
<dt>Transformers</dt>
<dd>
<p>Some estimators (such as a <code>SimpleImputer</code>) can also transform a dataset; these are called <em>transformers</em>. Once again, the API is simple: the transformation is <span class="keep-together">performed</span> by the <code>transform()</code> method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for a <code>SimpleImputer</code>. All transformers also have a convenience method called <code>fit_transform()</code>, which is equivalent to calling <code>fit()</code> and then <code>transform()</code> (but sometimes <code>fit_transform()</code> is optimized and runs much faster).</p>
</dd>
<dt>Predictors</dt>
<dd>
<p>Finally, some estimators, given a dataset, are capable of making predictions; they are called <em>predictors</em>. For example, the <code>LinearRegression</code> model in the previous chapter was a predictor: given a country‚Äôs GDP per capita, it predicted life satisfaction. A predictor has a <code>predict()</code> method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a <code>score()</code> method that measures the quality of the predictions, given a test set (and the corresponding labels, in the case of supervised learning algorithms).<sup><a data-type="noteref" id="id1103-marker" href="ch02.html#id1103">10</a></sup></p>
</dd>
</dl>
</dd>
<dt>Inspection</dt>
<dd>
<p>All the estimator‚Äôs hyperparameters are accessible directly via public instance variables (e.g., <code>imputer.strategy</code>), and all the estimator‚Äôs learned parameters are accessible via public instance variables with an underscore suffix (e.g., <code>imputer.statistics_</code>).</p>
</dd>
<dt>Nonproliferation of classes</dt>
<dd>
<p>Datasets are represented as NumPy arrays or SciPy sparse matrices, instead of homemade classes. Hyperparameters are just regular Python strings or numbers.</p>
</dd>
<dt>Composition</dt>
<dd>
<p>Existing building blocks are reused as much as possible. For example, it is easy to create a <code>Pipeline</code> estimator from an arbitrary sequence of transformers followed by a final estimator, as you will see.</p>
</dd>
<dt>Sensible defaults</dt>
<dd>
<p>Scikit-Learn provides reasonable default values for most parameters, making it easy to quickly create a baseline working system.</p>
</dd>
</dl>
</div></aside>

<p>Scikit-Learn<a data-type="indexterm" data-startref="xi_ScikitLearndesignprinciples2831290_1" id="id1104"/> transformers output NumPy array<a data-type="indexterm" data-primary="NumPy arrays" id="id1105"/>s (or sometimes SciPy sparse matrices) even when they are fed Pandas DataFrames<a data-type="indexterm" data-primary="DataFrame" id="id1106"/> as input.‚Å†<sup><a data-type="noteref" id="id1107-marker" href="ch02.html#id1107">11</a></sup> So, the output of <code translate="no">imputer.transform(housing_num)</code> is a NumPy array: <code translate="no">X</code> has neither column names nor index. Luckily, it‚Äôs not too hard to wrap <code translate="no">X</code> in a DataFrame and recover the column names and index from <code translate="no">housing_num</code>:<a data-type="indexterm" data-startref="xi_algorithmspreparingdataforcleaningthedata27665_1" id="id1108"/><a data-type="indexterm" data-startref="xi_datacleaning27665_1" id="id1109"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing_tr</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">housing_num</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>
                          <code class="n">index</code><code class="o">=</code><code class="n">housing_num</code><code class="o">.</code><code class="n">index</code><code class="p">)</code></pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Handling Text and Categorical Attributes"><div class="sect2" id="id45">
<h2>Handling Text and Categorical Attributes</h2>

<p>So<a data-type="indexterm" data-primary="algorithms, preparing data for" data-secondary="text and categorical attributes" id="xi_algorithmspreparingdatafortextandcategoricalattributes28953_1"/><a data-type="indexterm" data-primary="attributes" data-secondary="text and categorical" id="xi_attributestextandcategorical28953_1"/> far we have only dealt with numerical attributes, but your data may also contain text attributes<a data-type="indexterm" data-primary="text attributes" id="id1110"/>. In this dataset, there is just one: the <code translate="no">ocean_proximity</code> attribute. Let‚Äôs look at its value for the first few instances:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">housing_cat</code> <code class="o">=</code> <code class="n">housing</code><code class="p">[[</code><code class="s2">"ocean_proximity"</code><code class="p">]]</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">housing_cat</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">8</code><code class="p">)</code><code class="w"/>
<code class="go">      ocean_proximity</code>
<code class="go">13096        NEAR BAY</code>
<code class="go">14973       &lt;1H OCEAN</code>
<code class="go">3785           INLAND</code>
<code class="go">14689          INLAND</code>
<code class="go">20507      NEAR OCEAN</code>
<code class="go">1286           INLAND</code>
<code class="go">18078       &lt;1H OCEAN</code>
<code class="go">4396         NEAR BAY</code></pre>

<p class="pagebreak-before">It‚Äôs not arbitrary text: there are a limited number of possible values, each of which represents a category. So this attribute is a categorical attribute.<a data-type="indexterm" data-primary="attributes" data-secondary="categorical" id="id1111"/><a data-type="indexterm" data-primary="categorical attributes" id="id1112"/> Most machine 
<span class="keep-together">learning</span> algorithms prefer to work with numbers, so let‚Äôs convert these categories from text to numbers. For this, we can use Scikit-Learn‚Äôs <code translate="no">OrdinalEncoder</code> class:<a data-type="indexterm" data-primary="OrdinalEncoder" id="id1113"/><a data-type="indexterm" data-primary="sklearn" data-secondary="preprocessing.OrdinalEncoder" id="id1114"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">OrdinalEncoder</code>

<code class="n">ordinal_encoder</code> <code class="o">=</code> <code class="n">OrdinalEncoder</code><code class="p">()</code>
<code class="n">housing_cat_encoded</code> <code class="o">=</code> <code class="n">ordinal_encoder</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">housing_cat</code><code class="p">)</code></pre>

<p>Here‚Äôs what the first few encoded values in <code translate="no">housing_cat_encoded</code> look like:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">housing_cat_encoded</code><code class="p">[:</code><code class="mi">8</code><code class="p">]</code><code class="w"/>
<code class="go">array([[3.],</code>
<code class="go">       [0.],</code>
<code class="go">       [1.],</code>
<code class="go">       [1.],</code>
<code class="go">       [4.],</code>
<code class="go">       [1.],</code>
<code class="go">       [0.],</code>
<code class="go">       [3.]])</code></pre>

<p>You can get the list of categories using the <code translate="no">categories_</code> instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">ordinal_encoder</code><code class="o">.</code><code class="n">categories_</code><code class="w"/>
<code class="go">[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],</code>
<code class="go">       dtype=object)]</code></pre>

<p>One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as ‚Äúbad‚Äù, ‚Äúaverage‚Äù, ‚Äúgood‚Äù, and ‚Äúexcellent‚Äù), but it is obviously not the case for the <code translate="no">ocean_proximity</code> column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is <code translate="no">"&lt;1H OCEAN"</code> (and 0 otherwise), another attribute equal to 1 when the category is <code translate="no">"INLAND"</code> (and 0 otherwise), and so on. This is called <em>one-hot encoding</em>, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called <em>dummy</em> attributes<a data-type="indexterm" data-primary="dummy attributes" id="id1115"/><a data-type="indexterm" data-primary="attributes" data-secondary="dummy" id="id1116"/>. Scikit-Learn provides a <code translate="no">OneHotEncoder</code> class<a data-type="indexterm" data-primary="OneHotEncoder" id="xi_OneHotEncoder2946881_1"/><a data-type="indexterm" data-primary="one-hot encoding" id="xi_onehotencoding2946881_1"/><a data-type="indexterm" data-primary="sklearn" data-secondary="preprocessing.OneHotEncoder" id="xi_ScikitLearnsklearnpreprocessingOneHotEncoder2946881_1"/> to convert categorical values into one-hot vectors:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">OneHotEncoder</code>

<code class="n">cat_encoder</code> <code class="o">=</code> <code class="n">OneHotEncoder</code><code class="p">()</code>
<code class="n">housing_cat_1hot</code> <code class="o">=</code> <code class="n">cat_encoder</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">housing_cat</code><code class="p">)</code></pre>

<p class="pagebreak-before">By default, the output of a <code translate="no">OneHotEncoder</code> is a SciPy <em>sparse matrix</em><a data-type="indexterm" data-primary="sparse matrix" id="id1117"/>, instead of a NumPy array:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">housing_cat_1hot</code><code class="w"/>
<code class="go">&lt;Compressed Sparse Row sparse matrix of dtype 'float64'</code>
<code class="go"> with 16512 stored elements and shape (16512, 5)&gt;</code></pre>

<p>A sparse matrix is a very efficient representation for matrices that contain mostly zeros. Indeed, internally it only stores the nonzero values and their positions. When a categorical attribute has hundreds or thousands of categories, one-hot encoding it results in a very large matrix full of 0s except for a single 1 per row. In this case, a sparse matrix is exactly what you need: it will save plenty of memory and speed up computations. You can use a sparse matrix mostly like a normal 2D array,‚Å†<sup><a data-type="noteref" id="id1118-marker" href="ch02.html#id1118">12</a></sup> but if you want to convert it to a (dense) NumPy<a data-type="indexterm" data-primary="NumPy arrays" id="id1119"/> array, just call the <code translate="no">toarray()</code> method:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">housing_cat_1hot</code><code class="o">.</code><code class="n">toarray</code><code class="p">()</code><code class="w"/>
<code class="go">array([[0., 0., 0., 1., 0.],</code>
<code class="go">       [1., 0., 0., 0., 0.],</code>
<code class="go">       [0., 1., 0., 0., 0.],</code>
<code class="go">       ...,</code>
<code class="go">       [0., 0., 0., 0., 1.],</code>
<code class="go">       [1., 0., 0., 0., 0.],</code>
<code class="go">       [0., 0., 0., 0., 1.]], shape=(16512, 5))</code></pre>

<p>Alternatively, you can set <code translate="no">sparse_output=False</code> when creating the <code translate="no">OneHotEncoder</code>, in which case the <code translate="no">transform()</code> method will return a regular (dense) NumPy array directly:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">cat_encoder</code> <code class="o">=</code> <code class="n">OneHotEncoder</code><code class="p">(</code><code class="n">sparse_output</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
<code class="n">housing_cat_1hot</code> <code class="o">=</code> <code class="n">cat_encoder</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">housing_cat</code><code class="p">)</code>  <code class="c1"># now a dense array</code></pre>

<p>As with the <code translate="no">OrdinalEncoder</code>, you can get the list of categories using the encoder‚Äôs <code translate="no">categories_</code> instance variable:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">cat_encoder</code><code class="o">.</code><code class="n">categories_</code><code class="w"/>
<code class="go">[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],</code>
<code class="go">       dtype=object)]</code></pre>

<p>Pandas<a data-type="indexterm" data-primary="Pandas library" id="id1120"/> has a function called <code translate="no">get_dummies()</code><a data-type="indexterm" data-primary="get_dummies()" id="id1121"/>, which also converts each categorical feature into a one-hot representation, with one binary feature per category:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">df_test</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code><code class="s2">"ocean_proximity"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"INLAND"</code><code class="p">,</code> <code class="s2">"NEAR BAY"</code><code class="p">]})</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">df_test</code><code class="p">)</code><code class="w"/>
<code class="go">ocean_proximity_INLAND  ocean_proximity_NEAR      BAY</code>
<code class="go">0                       True                      False</code>
<code class="go">1                       False                     True</code></pre>

<p>It looks nice and simple, so why not use it instead of <code translate="no">OneHotEncoder</code>? Well, the advantage of <code translate="no">OneHotEncoder</code> is that it remembers which categories it was trained on. This is very important because once your model is in production, it should be fed exactly the same features as during training: no more, no less. Look what our trained <code translate="no">cat_encoder</code> outputs when we make it transform the same <code translate="no">df_test</code> (using <code translate="no">transform()</code><a data-type="indexterm" data-primary="transform()" id="id1122"/>, not <code translate="no">fit_transform()</code>):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">cat_encoder</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">df_test</code><code class="p">)</code><code class="w"/>
<code class="go">array([[0., 1., 0., 0., 0.],</code>
<code class="go">       [0., 0., 0., 1., 0.]])</code></pre>

<p>See the difference? <code translate="no">get_dummies()</code> saw only two categories, so it output two columns, whereas <code translate="no">OneHotEncoder</code> output one column per learned category, in the right order. Moreover, if you feed <code translate="no">get_dummies()</code> a DataFrame containing an unknown category (e.g., <code translate="no">"&lt;2H OCEAN"</code>), it will happily generate a column for it:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">df_test_unknown</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code><code class="s2">"ocean_proximity"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"&lt;2H OCEAN"</code><code class="p">,</code> <code class="s2">"ISLAND"</code><code class="p">]})</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">df_test_unknown</code><code class="p">)</code><code class="w"/>
<code class="go">   ocean_proximity_&lt;2H OCEAN  ocean_proximity_ISLAND</code>
<code class="go">0                       True                   False</code>
<code class="go">1                      False                    True</code></pre>

<p>But <code translate="no">OneHotEncoder</code> is smarter: it will detect the unknown category and raise an exception. If you prefer, you can set the <code translate="no">handle_unknown</code> hyperparameter to <code translate="no">"ignore"</code>, in which case it will just represent the unknown category with zeros:<a data-type="indexterm" data-startref="xi_OneHotEncoder2946881_1" id="id1123"/><a data-type="indexterm" data-startref="xi_onehotencoding2946881_1" id="id1124"/><a data-type="indexterm" data-startref="xi_ScikitLearnsklearnpreprocessingOneHotEncoder2946881_1" id="id1125"/></p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">cat_encoder</code><code class="o">.</code><code class="n">handle_unknown</code> <code class="o">=</code> <code class="s2">"ignore"</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">cat_encoder</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">df_test_unknown</code><code class="p">)</code><code class="w"/>
<code class="go">array([[0., 0., 0., 0., 0.],</code>
<code class="go">       [0., 0., 1., 0., 0.]])</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If a categorical attribute<a data-type="indexterm" data-primary="attributes" data-secondary="categorical" id="id1126"/><a data-type="indexterm" data-primary="categorical attributes" id="id1127"/> has a large number of possible categories (e.g., country code, profession, species), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance. If this happens, you may want to replace the categorical input with useful numerical features related to the categories: for example, you could replace the <code translate="no">ocean_proximity</code> feature with the distance to the ocean (similarly, a country code could be replaced with the country‚Äôs population and GDP per capita). Alternatively, you can use one of the encoders provided by the <code translate="no">category_encoders</code> package on <a href="https://github.com/scikit-learn-contrib/category_encoders">GitHub</a>. Or, when dealing with neural networks, you can replace each category with a learnable, low-dimensional vector called an <em>embedding</em><a data-type="indexterm" data-primary="embeddings" id="id1128"/> (see <a data-type="xref" href="ch14.html#nlp_chapter">Chapter¬†14</a>). This is an example of <em>representation learning</em><a data-type="indexterm" data-primary="representation learning" data-seealso="autoencoders" id="id1129"/> (we will see more examples in <a data-type="xref" href="ch18.html#autoencoders_chapter">Chapter¬†18</a>).</p>
</div>

<p>When you fit any Scikit-Learn estimator<a data-type="indexterm" data-primary="estimators, Scikit-Learn" id="id1130"/> using a DataFrame, the estimator stores the column names in the <code translate="no">feature_names_in_</code> attribute. Scikit-Learn then ensures that any DataFrame fed to this estimator after that (e.g., to <code translate="no">transform()</code> or <code translate="no">predict()</code>) has the same column names. Transformers also provide a <code translate="no">get_feature_names_out()</code><a data-type="indexterm" data-primary="get_feature_names_out()" id="id1131"/> method that you can use to build a DataFrame around the transformer‚Äôs output:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">cat_encoder</code><code class="o">.</code><code class="n">feature_names_in_</code><code class="w"/>
<code class="go">array(['ocean_proximity'], dtype=object)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">cat_encoder</code><code class="o">.</code><code class="n">get_feature_names_out</code><code class="p">()</code><code class="w"/>
<code class="go">array(['ocean_proximity_&lt;1H OCEAN', 'ocean_proximity_INLAND',</code>
<code class="go">       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',</code>
<code class="go">       'ocean_proximity_NEAR OCEAN'], dtype=object)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">df_output</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">cat_encoder</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">df_test_unknown</code><code class="p">),</code><code class="w"/>
<code class="gp">... </code>                         <code class="n">columns</code><code class="o">=</code><code class="n">cat_encoder</code><code class="o">.</code><code class="n">get_feature_names_out</code><code class="p">(),</code><code class="w"/>
<code class="gp">... </code>                         <code class="n">index</code><code class="o">=</code><code class="n">df_test_unknown</code><code class="o">.</code><code class="n">index</code><code class="p">)</code><code class="w"/>
<code class="gp">...</code><code class="w"/></pre>

<p>This feature helps avoid column mismatches, and it‚Äôs also quite useful when 
<span class="keep-together">debugging.</span><a data-type="indexterm" data-startref="xi_algorithmspreparingdatafortextandcategoricalattributes28953_1" id="id1132"/><a data-type="indexterm" data-startref="xi_attributestextandcategorical28953_1" id="id1133"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Feature Scaling and Transformation"><div class="sect2" id="id46">
<h2>Feature Scaling and Transformation</h2>

<p>One<a data-type="indexterm" data-primary="algorithms, preparing data for" data-secondary="feature scaling and transformation" id="xi_algorithmspreparingdataforfeaturescalingandtransformation210624_1"/><a data-type="indexterm" data-primary="feature scaling" id="xi_featurescaling210624_1"/><a data-type="indexterm" data-primary="transformation of data" data-secondary="and feature scaling" data-secondary-sortas="feature" id="xi_transformationofdataandfeaturescaling210624_1"/> of the most important transformations you need to apply to your data is <em>feature scaling</em>. With few exceptions, machine learning algorithms don‚Äôt perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.</p>

<p>There are two common ways to get all attributes to have the same scale: <em>min-max scaling</em> and <em>standardization</em>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>As with all estimators, it is important to fit the scalers to the training data only: never use <code translate="no">fit()</code><a data-type="indexterm" data-primary="fit(), Scikit-Learn" data-secondary="using only with training set" id="id1134"/><a data-type="indexterm" data-primary="fit_transform()" id="id1135"/> or <code translate="no">fit_transform()</code> for anything else than the training set. Once you have a trained scaler, you can then use it to <code translate="no">transform()</code><a data-type="indexterm" data-primary="transform()" id="id1136"/> any other set, including the validation set, the test set, and new data. Note that while the training set values will always be scaled to the specified range, if new data contains outliers, these may end up scaled outside the range. If you want to avoid this, just set the <code translate="no">clip</code> hyperparameter to <code translate="no">True</code>.</p>
</div>

<p>Min-max scaling<a data-type="indexterm" data-primary="min-max scaling" id="id1137"/><a data-type="indexterm" data-primary="normalization" id="id1138"/><a data-type="indexterm" data-primary="training set" data-secondary="min-max scaling" id="id1139"/> (many people call this <em>normalization</em>) is the simplest: for each attribute, the values are shifted and rescaled so that they end up ranging from 0 to 1. This is performed by subtracting the min value from all values, and dividing the results by the difference between the min and the max. Scikit-Learn provides a transformer called <code translate="no">MinMaxScaler</code><a data-type="indexterm" data-primary="MinMaxScaler" id="id1140"/> for this. It has a <code translate="no">feature_range</code> hyperparameter<a data-type="indexterm" data-primary="hyperparameters" data-secondary="and normalization" data-secondary-sortas="normalization" id="id1141"/> that lets you change the range if, for some reason, you don‚Äôt want 0‚Äì1 (e.g., neural networks work best with zero-mean inputs, so a range of ‚Äì1 to 1 is preferable). It‚Äôs quite easy to use<a data-type="indexterm" data-primary="sklearn" data-secondary="preprocessing.MinMaxScaler" id="id1142"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">MinMaxScaler</code>

<code class="n">min_max_scaler</code> <code class="o">=</code> <code class="n">MinMaxScaler</code><code class="p">(</code><code class="n">feature_range</code><code class="o">=</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>
<code class="n">housing_num_min_max_scaled</code> <code class="o">=</code> <code class="n">min_max_scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">housing_num</code><code class="p">)</code></pre>

<p>Standardization<a data-type="indexterm" data-primary="standardization" id="id1143"/> is different: first it subtracts the mean value (so standardized values have a zero mean), then it divides the result by the standard deviation (so standardized values have a standard deviation equal to 1). Unlike min-max scaling, standardization does not restrict values to a specific range. However, standardization is much less affected by outliers. For example, suppose a district has a median income equal to 100 (by mistake), instead of the usual 0‚Äì15. Min-max scaling to the 0‚Äì1 range would map this outlier down to 1 and it would crush all the other values down to 0‚Äì0.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called <code translate="no">StandardScaler</code> for <span class="keep-together">standardization</span><a data-type="indexterm" data-primary="sklearn" data-secondary="preprocessing.StandardScaler" id="id1144"/><a data-type="indexterm" data-primary="StandardScaler" id="id1145"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="n">std_scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">housing_num_std_scaled</code> <code class="o">=</code> <code class="n">std_scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">housing_num</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If you want to scale a sparse matrix<a data-type="indexterm" data-primary="sparse matrix" id="id1146"/> without converting it to a dense matrix<a data-type="indexterm" data-primary="dense matrix" id="id1147"/> first, you can use a <code translate="no">StandardScaler</code> with its <code translate="no">with_mean</code> hyperparameter set to <code translate="no">False</code>: it will only divide the data by the standard deviation, without subtracting the mean (as this would break sparsity).</p>
</div>

<p>When a feature‚Äôs distribution has a <em>heavy tail</em><a data-type="indexterm" data-primary="heavy tail, feature distribution" id="id1148"/> (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization will squash most values into a small range. Machine learning models generally don‚Äôt like this at all, as you will see in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>. So <em>before</em> you scale the feature, you should first transform it to shrink the heavy tail, and if possible to make the distribution roughly symmetrical. For example, a common way to do this for positive features with a heavy tail to the right is to replace the feature with its square root (or raise the feature to a power between 0 and 1). If the feature has a really long and heavy tail, such as a <em>power law distribution</em><a data-type="indexterm" data-primary="power law distribution" id="id1149"/>, then replacing the feature with its logarithm may help. For example, the <code translate="no">population</code> feature roughly follows a power law: districts with 10,000 inhabitants are only 10 times less frequent than districts with 1,000 inhabitants, not exponentially less frequent. <a data-type="xref" href="#long_tail_plot">Figure¬†2-17</a> shows how much better this feature looks when you compute its log: it‚Äôs very close to a Gaussian distribution<a data-type="indexterm" data-primary="Gaussian distribution" id="id1150"/> (i.e., bell-shaped).</p>

<figure class="smallerseventy"><div id="long_tail_plot" class="figure">
<img src="assets/hmls_0217.png" alt="Two histograms compare the distribution of a population feature: the left graph shows a heavily right-skewed distribution, while the right graph shows a more symmetrical, Gaussian-like distribution after applying a logarithmic transformation." width="2270" height="774"/>
<h6><span class="label">Figure 2-17. </span>Transforming a feature to make it closer to a Gaussian distribution</h6>
</div></figure>

<p>Another approach to handle heavy-tailed features consists in <em>bucketizing</em><a data-type="indexterm" data-primary="bucketizing a feature" id="id1151"/> the feature. This means chopping its distribution into roughly equal-sized buckets, and replacing each feature value with the index of the bucket it belongs to, much like we did to create the <code translate="no">income_cat</code> feature (although we only used it for stratified sampling). For example, you could replace each value with its percentile. Bucketizing with equal-sized buckets results in a feature with an almost uniform distribution, so there‚Äôs no need for further scaling, or you can just divide by the number of buckets to force the values to the 0‚Äì1 range.</p>

<p>When a feature has a multimodal distribution<a data-type="indexterm" data-primary="multimodal distribution" id="id1152"/> (i.e., with two or more clear peaks, called <em>modes</em>)<a data-type="indexterm" data-primary="mode" id="id1153"/>, such as the <code translate="no">housing_median_age</code> feature, it can also be helpful to bucketize it, but this time treating the bucket IDs as categories, rather than as numerical values. This means that the bucket indices must be encoded, for example using a <code translate="no">OneHotEncoder</code> (so you usually don‚Äôt want to use too many buckets). This approach will allow the regression model to more easily learn different rules for different ranges of this feature value. For example, perhaps houses built around 35 years ago have a peculiar style that fell out of fashion, and therefore they‚Äôre cheaper than their age alone would suggest.</p>

<p>Another approach to transforming multimodal distributions is to add a feature for each of the modes (at least the main ones), representing the similarity between the housing median age and that particular mode. The similarity measure is typically computed using a <em>radial basis function</em> (RBF)<a data-type="indexterm" data-primary="radial basis function (RBF)" id="id1154"/>‚Äî<a data-type="indexterm" data-primary="RBF (radial basis function)" id="id1155"/>any function that depends only on the distance between the input value and a fixed point. The most commonly used RBF is the Gaussian RBF, whose output value decays exponentially as the input value moves away from the fixed point. For example, the Gaussian RBF similarity between the housing age <em>x</em> and 35 is given by the equation exp(‚Äì<em>Œ≥</em>(<em>x</em> ‚Äì 35)¬≤). The hyperparameter <em>Œ≥</em> (gamma) determines how quickly the similarity measure decays as <em>x</em> moves away from 35. Using Scikit-Learn‚Äôs <code translate="no">rbf_kernel()</code><a data-type="indexterm" data-primary="rbf_kernel()" id="id1156"/> function, you can create a new Gaussian RBF feature measuring the similarity between the housing median age and 35:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.metrics.pairwise</code> <code class="kn">import</code> <code class="n">rbf_kernel</code>

<code class="n">age_simil_35</code> <code class="o">=</code> <code class="n">rbf_kernel</code><code class="p">(</code><code class="n">housing</code><code class="p">[[</code><code class="s2">"housing_median_age"</code><code class="p">]],</code> <code class="p">[[</code><code class="mi">35</code><code class="p">]],</code> <code class="n">gamma</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code></pre>

<p><a data-type="xref" href="#age_similarity_plot">Figure¬†2-18</a> shows this new feature as a function of the housing median age (solid line). It also shows what the feature would look like if you used a smaller <code translate="no">gamma</code> value. As the chart shows, the new age similarity feature peaks at 35, right around the spike in the housing median age distribution: if this particular age group is well correlated with lower prices, there‚Äôs a good chance that this new feature will help.</p>

<figure class="width-80"><div id="age_similarity_plot" class="figure">
<img src="assets/hmls_0218.png" alt="Histogram and line plot showing the Gaussian RBF feature for age similarity peaking at a housing median age of 35, with gamma values of 0.10 and 0.03." width="1793" height="1314"/>
<h6><span class="label">Figure 2-18. </span>Gaussian RBF feature measuring the similarity between the housing median age and 35</h6>
</div></figure>

<p>So far we‚Äôve only looked at the input features, but the target values may also need to be transformed. For example, if the target distribution<a data-type="indexterm" data-primary="target distribution, transforming" id="id1157"/><a data-type="indexterm" data-primary="training set" data-secondary="transforming data" id="id1158"/> has a heavy tail, you may choose to replace the target with its logarithm. But if you do, the regression model will now predict the <em>log</em> of the median house value, not the median house value itself. You will need to compute the exponential of the model‚Äôs prediction if you want the predicted median house value.</p>

<p>Luckily, most of Scikit-Learn‚Äôs transformers have an <code translate="no">inverse_transform()</code><a data-type="indexterm" data-primary="inverse_transform()" id="id1159"/> method, making it easy to compute the inverse of their transformations. For example, the following code example shows how to scale the labels using a <code translate="no">StandardScaler</code> (just like we did for inputs), then train a simple linear regression model on the resulting scaled labels and use it to make predictions on some new data, which we transform back to the original scale using the trained scaler‚Äôs <code translate="no">inverse_transform()</code> method. Note that we convert the labels from a Pandas Series<a data-type="indexterm" data-primary="Pandas library" id="id1160"/> to a DataFrame<a data-type="indexterm" data-primary="DataFrame" id="id1161"/>, since the <code translate="no">StandardScaler</code> expects 2D inputs. Also, in this example we just train the model on a single raw input feature (median income), for simplicity<a data-type="indexterm" data-primary="LinearRegression" id="id1162"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>

<code class="n">target_scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">scaled_labels</code> <code class="o">=</code> <code class="n">target_scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">housing_labels</code><code class="o">.</code><code class="n">to_frame</code><code class="p">())</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">housing</code><code class="p">[[</code><code class="s2">"median_income"</code><code class="p">]],</code> <code class="n">scaled_labels</code><code class="p">)</code>
<code class="n">some_new_data</code> <code class="o">=</code> <code class="n">housing</code><code class="p">[[</code><code class="s2">"median_income"</code><code class="p">]]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[:</code><code class="mi">5</code><code class="p">]</code>  <code class="c1"># pretend this is new data</code>

<code class="n">scaled_predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">some_new_data</code><code class="p">)</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">target_scaler</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">scaled_predictions</code><code class="p">)</code></pre>

<p>This works fine, but it‚Äôs simpler and less error-prone to use a <code>TransformedTarget‚ÄãRegressor</code><a data-type="indexterm" data-primary="sklearn" data-secondary="compose.TransformedTargetRegressor" id="id1163"/><a data-type="indexterm" data-primary="TransformedTargetRegressor" id="id1164"/>, avoiding potential scaling mismatches. We just need to construct it, giving it the regression model and the label transformer, then fit it on the training set, using the original unscaled labels. It will automatically use the transformer to scale the labels and train the regression model on the resulting scaled labels, just like we did previously. Then, when we want to make a prediction, it will call the regression model‚Äôs <code translate="no">predict()</code> method and use the scaler‚Äôs <code translate="no">inverse_transform()</code> method to produce the prediction<a data-type="indexterm" data-primary="sklearn" data-secondary="linear_model.LinearRegression" id="id1165"/>:<a data-type="indexterm" data-startref="xi_algorithmspreparingdataforfeaturescalingandtransformation210624_1" id="id1166"/><a data-type="indexterm" data-startref="xi_featurescaling210624_1" id="id1167"/><a data-type="indexterm" data-startref="xi_transformationofdataandfeaturescaling210624_1" id="id1168"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.compose</code> <code class="kn">import</code> <code class="n">TransformedTargetRegressor</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">TransformedTargetRegressor</code><code class="p">(</code><code class="n">LinearRegression</code><code class="p">(),</code>
                                   <code class="n">transformer</code><code class="o">=</code><code class="n">StandardScaler</code><code class="p">())</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">housing</code><code class="p">[[</code><code class="s2">"median_income"</code><code class="p">]],</code> <code class="n">housing_labels</code><code class="p">)</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">some_new_data</code><code class="p">)</code></pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Custom Transformers"><div class="sect2" id="id47">
<h2>Custom Transformers</h2>

<p>Although<a data-type="indexterm" data-primary="algorithms, preparing data for" data-secondary="custom transformers" id="xi_algorithmspreparingdataforcustomtransformers211569_1"/><a data-type="indexterm" data-primary="custom transformers" id="xi_customtransformers211569_1"/><a data-type="indexterm" data-primary="transformation of data" data-secondary="custom transformers" id="xi_transformationofdatacustomtransformers211569_1"/> Scikit-Learn provides many useful transformers, you will occasionally need to write your own for tasks such as custom transformations, cleanup operations, or combining specific attributes.</p>

<p>For transformations that don‚Äôt require any training, you can just write a function that takes a NumPy array as input and outputs the transformed array. For example, as discussed in the previous section, it‚Äôs often a good idea to transform features with heavy-tailed distributions by replacing them with their logarithm (assuming the feature is positive and the tail is on the right). Let‚Äôs create a log-transformer<a data-type="indexterm" data-primary="log-transformer" id="id1169"/> and apply it to the <code translate="no">population</code> feature<a data-type="indexterm" data-primary="FunctionTransformer" id="id1170"/><a data-type="indexterm" data-primary="sklearn" data-secondary="preprocessing.FunctionTransformer" id="id1171"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">FunctionTransformer</code>

<code class="n">log_transformer</code> <code class="o">=</code> <code class="n">FunctionTransformer</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">,</code> <code class="n">inverse_func</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">)</code>
<code class="n">log_pop</code> <code class="o">=</code> <code class="n">log_transformer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">housing</code><code class="p">[[</code><code class="s2">"population"</code><code class="p">]])</code></pre>

<p>The <code translate="no">inverse_func</code> argument is optional. It lets you specify an inverse transform function, e.g., if you plan to use your transformer in a <code translate="no">TransformedTargetRegressor</code>.</p>

<p>Your transformation function can take hyperparameters<a data-type="indexterm" data-primary="hyperparameters" data-secondary="in custom transformations" data-secondary-sortas="custom" id="id1172"/> as additional arguments. For example, here‚Äôs how to create a transformer that computes the same Gaussian RBF similarity measure as earlier:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">rbf_transformer</code> <code class="o">=</code> <code class="n">FunctionTransformer</code><code class="p">(</code><code class="n">rbf_kernel</code><code class="p">,</code>
                                      <code class="n">kw_args</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">Y</code><code class="o">=</code><code class="p">[[</code><code class="mf">35.</code><code class="p">]],</code> <code class="n">gamma</code><code class="o">=</code><code class="mf">0.1</code><code class="p">))</code>
<code class="n">age_simil_35</code> <code class="o">=</code> <code class="n">rbf_transformer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">housing</code><code class="p">[[</code><code class="s2">"housing_median_age"</code><code class="p">]])</code></pre>

<p>Note that there‚Äôs no inverse function for the RBF kernel<a data-type="indexterm" data-primary="Gaussian RBF kernel" id="id1173"/>, since there are always two values at a given distance from a fixed point (except at distance 0). Also note that <code translate="no">rbf_kernel()</code><a data-type="indexterm" data-primary="rbf_kernel()" id="id1174"/> does not treat the features separately. If you pass it an array with two features, it will measure the 2D distance (Euclidean) to measure similarity. For example, here‚Äôs how to add a feature that will measure the geographic similarity between each district and San Francisco:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">sf_coords</code> <code class="o">=</code> <code class="mf">37.7749</code><code class="p">,</code> <code class="o">-</code><code class="mf">122.41</code>
<code class="n">sf_transformer</code> <code class="o">=</code> <code class="n">FunctionTransformer</code><code class="p">(</code><code class="n">rbf_kernel</code><code class="p">,</code>
                                     <code class="n">kw_args</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">Y</code><code class="o">=</code><code class="p">[</code><code class="n">sf_coords</code><code class="p">],</code> <code class="n">gamma</code><code class="o">=</code><code class="mf">0.1</code><code class="p">))</code>
<code class="n">sf_simil</code> <code class="o">=</code> <code class="n">sf_transformer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">housing</code><code class="p">[[</code><code class="s2">"latitude"</code><code class="p">,</code> <code class="s2">"longitude"</code><code class="p">]])</code></pre>

<p>Custom transformers are also useful to combine features. For example, here‚Äôs a <code translate="no">FunctionTransformer</code> that computes the ratio between the input features 0 and 1:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">ratio_transformer</code> <code class="o">=</code> <code class="n">FunctionTransformer</code><code class="p">(</code><code class="k">lambda</code> <code class="n">X</code><code class="p">:</code> <code class="n">X</code><code class="p">[:,</code> <code class="p">[</code><code class="mi">0</code><code class="p">]]</code> <code class="o">/</code> <code class="n">X</code><code class="p">[:,</code> <code class="p">[</code><code class="mi">1</code><code class="p">]])</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">ratio_transformer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mf">1.</code><code class="p">,</code> <code class="mf">2.</code><code class="p">],</code> <code class="p">[</code><code class="mf">3.</code><code class="p">,</code> <code class="mf">4.</code><code class="p">]]))</code><code class="w"/>
<code class="go">array([[0.5 ],</code>
<code class="go">       [0.75]])</code></pre>

<p><code translate="no">FunctionTransformer</code> is very handy, but what if you would like your transformer to be trainable, learning some parameters in the <code translate="no">fit()</code><a data-type="indexterm" data-primary="fit(), Scikit-Learn" data-secondary="and custom transformers" data-secondary-sortas="custom" id="id1175"/><a data-type="indexterm" data-primary="fit_transform()" id="id1176"/> method and using them later in the <code translate="no">transform()</code> method? For this, you need to write a custom class.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The rest of this section shows how to define custom transformer classes. In particular, it defines a custom transformer that groups districts into 10 geographical clusters, then measures the distance between each district and the center of each cluster, adding 10 corresponding RBF similarity features to the data. Since defining custom transformer classes is somewhat advanced, please feel free to skip to the next section and come back whenever needed.</p>
</div>

<p>Scikit-Learn relies on duck typing,‚Å†<sup><a data-type="noteref" id="id1177-marker" href="ch02.html#id1177">13</a></sup> so custom transformer classes do not have to inherit from any particular base class. All they need is three methods: <code translate="no">fit()</code> (which must return <code translate="no">self</code>), <code translate="no">transform()</code>, and <code translate="no">fit_transform()</code>. You can get <code translate="no">fit_transform()</code> for free by simply adding <code translate="no">TransformerMixin</code><a data-type="indexterm" data-primary="TransformerMixin" id="id1178"/> as a base class: the default implementation will just call <code translate="no">fit()</code> and then <code translate="no">transform()</code>.<a data-type="indexterm" data-primary="transform()" id="id1179"/> If you add <code translate="no">BaseEstimator</code> as a base class (and avoid using <code translate="no">*args</code> and <code translate="no">**kwargs</code> in your constructor), you will also get two extra methods: <code translate="no">get_params()</code><a data-type="indexterm" data-primary="get_params()" id="id1180"/> and <code translate="no">set_params()</code><a data-type="indexterm" data-primary="set_params()" id="id1181"/>. These will be useful for automatic hyperparameter tuning.</p>

<p>For example, here‚Äôs a custom transformer that acts much like the <code translate="no">StandardScaler</code><a data-type="indexterm" data-primary="BaseEstimator" id="id1182"/><a data-type="indexterm" data-primary="sklearn" data-secondary="base.BaseEstimator" id="id1183"/><a data-type="indexterm" data-primary="sklearn" data-secondary="base.TransformerMixin" id="id1184"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.base</code> <code class="kn">import</code> <code class="n">BaseEstimator</code><code class="p">,</code> <code class="n">TransformerMixin</code>
<code class="kn">from</code> <code class="nn">sklearn.utils.validation</code> <code class="kn">import</code> <code class="n">check_array</code><code class="p">,</code> <code class="n">check_is_fitted</code>

<code class="k">class</code> <code class="nc">StandardScalerClone</code><code class="p">(</code><code class="n">BaseEstimator</code><code class="p">,</code> <code class="n">TransformerMixin</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">with_mean</code><code class="o">=</code><code class="kc">True</code><code class="p">):</code>  <code class="c1"># no *args or **kwargs!</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">with_mean</code> <code class="o">=</code> <code class="n">with_mean</code>

    <code class="k">def</code> <code class="nf">fit</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>  <code class="c1"># y is required even though we don't use it</code>
        <code class="n">X</code> <code class="o">=</code> <code class="n">check_array</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>  <code class="c1"># checks that X is an array with finite float values</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">mean_</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">scale_</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">n_features_in_</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>  <code class="c1"># every estimator stores this in fit()</code>
        <code class="k">return</code> <code class="bp">self</code>  <code class="c1"># always return self!</code>

    <code class="k">def</code> <code class="nf">transform</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">check_is_fitted</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code>  <code class="c1"># looks for learned attributes (with trailing _)</code>
        <code class="n">X</code> <code class="o">=</code> <code class="n">check_array</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="k">assert</code> <code class="bp">self</code><code class="o">.</code><code class="n">n_features_in_</code> <code class="o">==</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
        <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">with_mean</code><code class="p">:</code>
            <code class="n">X</code> <code class="o">=</code> <code class="n">X</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">mean_</code>
        <code class="k">return</code> <code class="n">X</code> <code class="o">/</code> <code class="bp">self</code><code class="o">.</code><code class="n">scale_</code></pre>

<p>Here are a few things to note:</p>

<ul>
<li>
<p>The <code translate="no">sklearn.utils.validation</code><a data-type="indexterm" data-primary="sklearn" data-secondary="utils.validation" id="id1185"/> package contains several functions we can use to validate the inputs. For simplicity, we will skip such tests in the rest of this book, but production code should have them.</p>
</li>
<li>
<p>Scikit-Learn pipelines require the <code translate="no">fit()</code> method to have two arguments <code translate="no">X</code> and <code translate="no">y</code>, which is why we need the <code translate="no">y=None</code> argument even though we don‚Äôt use <code translate="no">y</code>.</p>
</li>
<li>
<p>All Scikit-Learn estimators set <code translate="no">n_features_in_</code> in the <code translate="no">fit()</code> method, and they ensure that the data passed to <code translate="no">transform()</code> or <code translate="no">predict()</code><a data-type="indexterm" data-primary="predict()" id="id1186"/> has this number of features.</p>
</li>
<li>
<p>The <code translate="no">fit()</code> method must return <code translate="no">self</code>.</p>
</li>
<li>
<p>This implementation is not 100% complete: all estimators should set <code>feature_‚Äãnames_in_</code> in the <code translate="no">fit()</code> method when they are passed a DataFrame. Moreover, all transformers should provide a <code translate="no">get_feature_names_out()</code> method, as well as an <code translate="no">inverse_transform()</code><a data-type="indexterm" data-primary="inverse_transform()" id="id1187"/> method when their transformation can be reversed. See the last exercise at the end of this chapter for more details.</p>
</li>
</ul>

<p>A custom transformer can (and often does) use other estimators in its implementation. For example, the following code demonstrates a custom transformer that uses a <code translate="no">KMeans</code> clusterer in the <code translate="no">fit()</code> method to identify the main clusters in the training data, and then uses <code translate="no">rbf_kernel()</code> in the <code translate="no">transform()</code> method to measure how similar each sample is to each cluster center<a data-type="indexterm" data-primary="KMeans" id="id1188"/><a data-type="indexterm" data-primary="sklearn" data-secondary="cluster.KMeans" id="id1189"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>

<code class="k">class</code> <code class="nc">ClusterSimilarity</code><code class="p">(</code><code class="n">BaseEstimator</code><code class="p">,</code> <code class="n">TransformerMixin</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">n_clusters</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">n_clusters</code> <code class="o">=</code> <code class="n">n_clusters</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">gamma</code> <code class="o">=</code> <code class="n">gamma</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">random_state</code> <code class="o">=</code> <code class="n">random_state</code>

    <code class="k">def</code> <code class="nf">fit</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">sample_weight</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">kmeans_</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">n_clusters</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">random_state</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">kmeans_</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">sample_weight</code><code class="o">=</code><code class="n">sample_weight</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code>  <code class="c1"># always return self!</code>

    <code class="k">def</code> <code class="nf">transform</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">rbf_kernel</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">kmeans_</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">gamma</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">get_feature_names_out</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">names</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
        <code class="k">return</code> <code class="p">[</code><code class="sa">f</code><code class="s2">"Cluster </code><code class="si">{</code><code class="n">i</code><code class="si">}</code><code class="s2"> similarity"</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">n_clusters</code><code class="p">)]</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>You can check whether your custom estimator respects Scikit-Learn‚Äôs API by passing an instance to <code translate="no">check_estimator()</code><a data-type="indexterm" data-primary="check_estimator()" id="id1190"/> from the <code translate="no">sklearn.utils.estimator_checks</code> package. For the full API, check out <a href="https://scikit-learn.org/stable/developers" class="bare"><em class="hyperlink">https://scikit-learn.org/stable/developers</em></a>.</p>
</div>

<p>As you will see in <a data-type="xref" href="ch08.html#unsupervised_learning_chapter">Chapter¬†8</a>, <em>k</em>-means<a data-type="indexterm" data-primary="k-means algorithm" id="id1191"/> is a clustering algorithm that locates clusters in the data. For example, we can use it to find the most populated regions in California. How many clusters <em>k</em>-means searches for is controlled by the <code translate="no">n_clusters</code> hyperparameter. The <code translate="no">fit()</code> method of <code translate="no">KMeans</code> supports an optional argument <code translate="no">sample_weight</code>, which lets the user specify the relative weights of the samples. For example, we could pass it the median income if we wanted the clusters to be biased toward wealthy districts. After training, the cluster centers are available via the <code translate="no">cluster_centers_</code> attribute. <em>k</em>-means is a stochastic algorithm, meaning that it relies on randomness to locate the clusters, so if you want reproducible results, you must set the <code translate="no">random_state</code> parameter. As you can see, despite the complexity of the task, the code is fairly straightforward. Now let‚Äôs use this custom transformer:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">cluster_simil</code> <code class="o">=</code> <code class="n">ClusterSimilarity</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mf">1.</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">similarities</code> <code class="o">=</code> <code class="n">cluster_simil</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">housing</code><code class="p">[[</code><code class="s2">"latitude"</code><code class="p">,</code> <code class="s2">"longitude"</code><code class="p">]])</code></pre>

<p>This code creates a <code translate="no">ClusterSimilarity</code> transformer, setting the number of clusters to 10. Then it calls <code translate="no">fit_transform()</code> with the latitude and longitude of every district in the training set (you can try weighting each district by its median income to see how that affects the clusters). The transformer uses <em>k</em>-means to locate the clusters, then measures the Gaussian RBF similarity between each district and all 10 cluster centers. The result is a matrix with one row per district, and one column per cluster. Let‚Äôs look at the first three rows, rounding to two decimal places:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">similarities</code><code class="p">[:</code><code class="mi">3</code><code class="p">]</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code class="w"/>
<code class="go">array([[0.46, 0.  , 0.08, 0.  , 0.  , 0.  , 0.  , 0.98, 0.  , 0.  ],</code>
<code class="go">       [0.  , 0.96, 0.  , 0.03, 0.04, 0.  , 0.  , 0.  , 0.11, 0.35],</code>
<code class="go">       [0.34, 0.  , 0.45, 0.  , 0.  , 0.  , 0.01, 0.73, 0.  , 0.  ]])</code></pre>

<p><a data-type="xref" href="#district_cluster_plot">Figure¬†2-19</a> shows the 10 cluster centers found by <em>k</em>-means. The districts are colored according to their geographic similarity to their closest cluster center. Notice that most clusters are located in highly populated areas.<a data-type="indexterm" data-startref="xi_algorithmspreparingdataforcustomtransformers211569_1" id="id1192"/><a data-type="indexterm" data-startref="xi_customtransformers211569_1" id="id1193"/><a data-type="indexterm" data-startref="xi_transformationofdatacustomtransformers211569_1" id="id1194"/></p>

<figure class="width-85"><div id="district_cluster_plot" class="figure">
<img src="assets/hmls_0219.png" alt="Scatter plot showing geographic clusters based on Gaussian RBF similarity, with highly populated areas highlighted." width="2611" height="1974"/>
<h6><span class="label">Figure 2-19. </span>Gaussian RBF similarity to the nearest cluster center</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Transformation Pipelines"><div class="sect2" id="id48">
<h2>Transformation Pipelines</h2>

<p>As<a data-type="indexterm" data-primary="algorithms, preparing data for" data-secondary="transformation pipelines" id="xi_algorithmspreparingdatafortransformationpipelines213043_1"/><a data-type="indexterm" data-primary="pipelines" id="xi_pipelines213043_1"/><a data-type="indexterm" data-primary="transformation pipelines" id="xi_transformationpipelines213043_1"/> you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the <code translate="no">Pipeline</code> class<a data-type="indexterm" data-primary="Pipeline class" id="id1195"/> to help with such sequences of transformations. Here is a small pipeline for numerical attributes, which will first impute then scale the input features<a data-type="indexterm" data-primary="sklearn" data-secondary="pipeline.Pipeline" id="id1196"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">Pipeline</code>

<code class="n">num_pipeline</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([</code>
    <code class="p">(</code><code class="s2">"impute"</code><code class="p">,</code> <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s2">"median"</code><code class="p">)),</code>
    <code class="p">(</code><code class="s2">"standardize"</code><code class="p">,</code> <code class="n">StandardScaler</code><code class="p">()),</code>
<code class="p">])</code></pre>

<p>The <code translate="no">Pipeline</code> constructor<a data-type="indexterm" data-primary="Pipeline constructor" id="xi_Pipelineconstructor2131827_1"/><a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="Pipeline constructor" id="xi_ScikitLearnPipelineconstructor2131827_1"/> takes a list of name/estimator pairs (2-tuples) defining a sequence of steps. The names can be anything you like, as long as they are unique and don‚Äôt contain double underscores (<code translate="no">__</code>). They will be useful later, when we discuss hyperparameter tuning. The estimators must all be transformers (i.e., they must have a <code translate="no">fit_transform()</code> method), except for the last one, which can be anything: a transformer, a predictor, or any other type of estimator.</p>
<div data-type="tip"><h6>Tip</h6>
<p>In a Jupyter notebook, if you <code translate="no">import</code> <code translate="no">sklearn</code> and run <code>sklearn.‚Äãset_config(display="diagram")</code>, all Scikit-Learn estimators will be rendered as interactive diagrams. This is particularly useful for visualizing pipelines. To visualize <code translate="no">num_pipeline</code>, run a cell with <code translate="no">num_pipeline</code> as the last line. Clicking an estimator will show more details.</p>
</div>

<p>If you don‚Äôt want to have to name the transformers, you can use the convenient <code translate="no">make_pipeline()</code><a data-type="indexterm" data-primary="make_pipeline()" id="id1197"/> function instead; it takes transformers as positional arguments and creates a <code translate="no">Pipeline</code> using the names of the transformers‚Äô classes, in lowercase and without underscores (e.g., <code translate="no">"simpleimputer"</code>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>

<code class="n">num_pipeline</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s2">"median"</code><code class="p">),</code> <code class="n">StandardScaler</code><code class="p">())</code></pre>

<p>If multiple transformers have the same name, an index is appended to their names (e.g., <code translate="no">"foo-1"</code>, <code translate="no">"foo-2"</code>, etc.).</p>

<p>When you call the pipeline‚Äôs <code translate="no">fit()</code> method, it calls <code translate="no">fit_transform()</code> sequentially on all the transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it just calls the <code translate="no">fit()</code> method.</p>

<p>The pipeline exposes the same methods as the final estimator. In this example the last estimator is a <code translate="no">StandardScaler</code>, which is a transformer, so the pipeline also acts like a transformer. If you call the pipeline‚Äôs <code translate="no">transform()</code> method, it will sequentially apply all the transformations to the data. If the last estimator were a predictor instead of a transformer, then the pipeline would have a <code translate="no">predict()</code> method rather than a <code translate="no">transform()</code> method. Calling it would sequentially apply all the transformations to the data and pass the result to the predictor‚Äôs <code translate="no">predict()</code> method<a data-type="indexterm" data-primary="predict()" id="id1198"/>.</p>

<p>Let‚Äôs call the pipeline‚Äôs <code translate="no">fit_transform()</code> method<a data-type="indexterm" data-primary="fit(), Scikit-Learn" data-secondary="and custom transformers" data-secondary-sortas="custom" id="id1199"/><a data-type="indexterm" data-primary="fit_transform()" id="id1200"/> and look at the output‚Äôs first two rows, rounded to two decimal places:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">housing_num_prepared</code> <code class="o">=</code> <code class="n">num_pipeline</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">housing_num</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">housing_num_prepared</code><code class="p">[:</code><code class="mi">2</code><code class="p">]</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code class="w"/>
<code class="go">array([[-1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],</code>
<code class="go">       [ 0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17]])</code></pre>

<p>As you saw earlier, if you want to recover a nice DataFrame, you can use the pipeline‚Äôs <code translate="no">get_feature_names_out()</code><a data-type="indexterm" data-primary="get_feature_names_out()" id="id1201"/> method:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">df_housing_num_prepared</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>
    <code class="n">housing_num_prepared</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">num_pipeline</code><code class="o">.</code><code class="n">get_feature_names_out</code><code class="p">(),</code>
    <code class="n">index</code><code class="o">=</code><code class="n">housing_num</code><code class="o">.</code><code class="n">index</code><code class="p">)</code></pre>

<p>Pipelines support indexing; for example, <code translate="no">pipeline[1]</code> returns the second estimator in the pipeline, and <code translate="no">pipeline[:-1]</code> returns a <code translate="no">Pipeline</code> object containing all but the last estimator. You can also access the estimators via the <code translate="no">steps</code> attribute, which is a list of name/estimator pairs, or via the <code translate="no">named_steps</code> dictionary attribute, which maps the names to the estimators. For example, <code translate="no">num_pipeline["simpleimputer"]</code> returns the estimator named <code translate="no">"simpleimputer"</code>.</p>

<p>So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer capable of handling all columns, applying the appropriate transformations to each column. For this, you can use a <code translate="no">ColumnTransformer</code><a data-type="indexterm" data-primary="ColumnTransformer" id="id1202"/>. For example, the following <code translate="no">ColumnTransformer</code> will apply <code translate="no">num_pipeline</code> (the one we just defined) to the numerical attributes, and <code translate="no">cat_pipeline</code> to the categorical attribute<a data-type="indexterm" data-primary="sklearn" data-secondary="compose.ColumnTransformer" id="id1203"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.compose</code> <code class="kn">import</code> <code class="n">ColumnTransformer</code>

<code class="n">num_attribs</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"longitude"</code><code class="p">,</code> <code class="s2">"latitude"</code><code class="p">,</code> <code class="s2">"housing_median_age"</code><code class="p">,</code> <code class="s2">"total_rooms"</code><code class="p">,</code>
               <code class="s2">"total_bedrooms"</code><code class="p">,</code> <code class="s2">"population"</code><code class="p">,</code> <code class="s2">"households"</code><code class="p">,</code> <code class="s2">"median_income"</code><code class="p">]</code>
<code class="n">cat_attribs</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"ocean_proximity"</code><code class="p">]</code>

<code class="n">cat_pipeline</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code>
    <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s2">"most_frequent"</code><code class="p">),</code>
    <code class="n">OneHotEncoder</code><code class="p">(</code><code class="n">handle_unknown</code><code class="o">=</code><code class="s2">"ignore"</code><code class="p">))</code>

<code class="n">preprocessing</code> <code class="o">=</code> <code class="n">ColumnTransformer</code><code class="p">([</code>
    <code class="p">(</code><code class="s2">"num"</code><code class="p">,</code> <code class="n">num_pipeline</code><code class="p">,</code> <code class="n">num_attribs</code><code class="p">),</code>
    <code class="p">(</code><code class="s2">"cat"</code><code class="p">,</code> <code class="n">cat_pipeline</code><code class="p">,</code> <code class="n">cat_attribs</code><code class="p">),</code>
<code class="p">])</code></pre>

<p>First we import the <code translate="no">ColumnTransformer</code> class, then we define the list of numerical and categorical column names and construct a simple pipeline for categorical attributes. Lastly, we construct a <code translate="no">ColumnTransformer</code>. Its constructor requires a list of triplets (3-tuples), each containing a name (which must be unique and not contain double underscores), a transformer, and a list of names (or indices) of columns that the transformer should be applied to.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Instead of using a transformer, you can specify the string <code translate="no">"drop"</code> if you want the columns to be dropped, or you can specify 
<span class="keep-together"><code translate="no">"passthrough"</code></span> if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the <code translate="no">remainder</code> hyperparameter to any transformer (or to <code translate="no">"passthrough"</code>) if you want these columns to be handled differently.</p>
</div>

<p>Since listing all the column names is not very convenient, Scikit-Learn provides a <code translate="no">make_column_selector</code><a data-type="indexterm" data-primary="make_column_selector()" id="id1204"/> class that you can use to automatically select all the features of a given type, such as numerical or categorical. You can pass a selector to the <code translate="no">ColumnTransformer</code> instead of column names or indices. Moreover, if you don‚Äôt care about naming the transformers, you can use <code translate="no">make_column_transformer()</code><a data-type="indexterm" data-primary="make_column_transformer()" id="id1205"/>, which chooses the names for you, just like <code translate="no">make_pipeline()</code> does. For example, the following code creates the same <code translate="no">ColumnTransformer</code> as earlier, except the transformers are automatically named <code translate="no">"pipeline-1"</code> and <code translate="no">"pipeline-2"</code> instead of <code translate="no">"num"</code> and <code translate="no">"cat"</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.compose</code> <code class="kn">import</code> <code class="n">make_column_selector</code><code class="p">,</code> <code class="n">make_column_transformer</code>

<code class="n">preprocessing</code> <code class="o">=</code> <code class="n">make_column_transformer</code><code class="p">(</code>
    <code class="p">(</code><code class="n">num_pipeline</code><code class="p">,</code> <code class="n">make_column_selector</code><code class="p">(</code><code class="n">dtype_include</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">number</code><code class="p">)),</code>
    <code class="p">(</code><code class="n">cat_pipeline</code><code class="p">,</code> <code class="n">make_column_selector</code><code class="p">(</code><code class="n">dtype_include</code><code class="o">=</code><code class="nb">object</code><code class="p">)),</code>
<code class="p">)</code></pre>

<p>Now we‚Äôre ready to apply this <code translate="no">ColumnTransformer</code> to the housing data:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing_prepared</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">housing</code><code class="p">)</code></pre>

<p>Great! We have a preprocessing pipeline that takes the entire training dataset and applies each transformer to the appropriate columns, then concatenates the transformed columns horizontally (transformers must never change the number of rows). Once again this returns a NumPy array, but you can get the column names using 
<span class="keep-together"><code translate="no">preprocessing.get_feature_names_out()</code></span> and wrap the data in a nice DataFrame as we did before.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <code translate="no">OneHotEncoder</code><a data-type="indexterm" data-primary="OneHotEncoder" id="id1206"/><a data-type="indexterm" data-primary="sklearn" data-secondary="preprocessing.OneHotEncoder" id="id1207"/> returns a sparse matrix<a data-type="indexterm" data-primary="sparse matrix" id="id1208"/> and the <code translate="no">num_pipeline</code> returns a dense matrix<a data-type="indexterm" data-primary="dense matrix" id="id1209"/>. When there is such a mix of sparse and dense matrices, the <code translate="no">ColumnTransformer</code> estimates the density of the final matrix (i.e., the ratio of nonzero cells), and it returns a sparse matrix if the density is lower than a given threshold (by default, <code translate="no">sparse_threshold=0.3</code>). In this example, it returns a dense matrix.</p>
</div>

<p>Your project is going really well and you‚Äôre almost ready to train some models! You now want to create a single pipeline that will perform all the transformations you‚Äôve experimented with up to now. Let‚Äôs recap what the pipeline will do and why:</p>

<ul>
<li>
<p>Missing values in numerical features will be imputed by replacing them with the median, as most ML algorithms don‚Äôt expect missing values. In categorical features, missing values will be replaced by the most frequent category.</p>
</li>
<li>
<p>The categorical feature will be one-hot encoded, as most ML algorithms only accept numerical inputs.</p>
</li>
<li>
<p>A few ratio features will be computed and added: <code translate="no">bedrooms_ratio</code>, <code>rooms_‚Äãper_house</code>, and <code translate="no">people_per_house</code>. Hopefully these will better correlate with the median house value, and thereby help the ML models.</p>
</li>
<li>
<p>A few cluster similarity features will also be added. These will likely be more useful to the model than latitude and longitude.</p>
</li>
<li>
<p>Features with a long tail will be replaced by their logarithm, as most models prefer features with roughly uniform or Gaussian distributions.</p>
</li>
<li>
<p>All numerical features will be standardized, as most ML algorithms prefer when all features have roughly the same scale.</p>
</li>
</ul>

<p>The code that builds the pipeline to do all of this should look familiar to you by now:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">column_ratio</code><code class="p">(</code><code class="n">X</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">X</code><code class="p">[:,</code> <code class="p">[</code><code class="mi">0</code><code class="p">]]</code> <code class="o">/</code> <code class="n">X</code><code class="p">[:,</code> <code class="p">[</code><code class="mi">1</code><code class="p">]]</code>

<code class="k">def</code> <code class="nf">ratio_name</code><code class="p">(</code><code class="n">function_transformer</code><code class="p">,</code> <code class="n">feature_names_in</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">[</code><code class="s2">"ratio"</code><code class="p">]</code>  <code class="c1"># feature names out</code>

<code class="k">def</code> <code class="nf">ratio_pipeline</code><code class="p">():</code>
    <code class="k">return</code> <code class="n">make_pipeline</code><code class="p">(</code>
        <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s2">"median"</code><code class="p">),</code>
        <code class="n">FunctionTransformer</code><code class="p">(</code><code class="n">column_ratio</code><code class="p">,</code> <code class="n">feature_names_out</code><code class="o">=</code><code class="n">ratio_name</code><code class="p">),</code>
        <code class="n">StandardScaler</code><code class="p">())</code>

<code class="n">log_pipeline</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code>
    <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s2">"median"</code><code class="p">),</code>
    <code class="n">FunctionTransformer</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">,</code> <code class="n">feature_names_out</code><code class="o">=</code><code class="s2">"one-to-one"</code><code class="p">),</code>
    <code class="n">StandardScaler</code><code class="p">())</code>
<code class="n">cluster_simil</code> <code class="o">=</code> <code class="n">ClusterSimilarity</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mf">1.</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">default_num_pipeline</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s2">"median"</code><code class="p">),</code>
                                     <code class="n">StandardScaler</code><code class="p">())</code>
<code class="n">preprocessing</code> <code class="o">=</code> <code class="n">ColumnTransformer</code><code class="p">([</code>
        <code class="p">(</code><code class="s2">"bedrooms"</code><code class="p">,</code> <code class="n">ratio_pipeline</code><code class="p">(),</code> <code class="p">[</code><code class="s2">"total_bedrooms"</code><code class="p">,</code> <code class="s2">"total_rooms"</code><code class="p">]),</code>
        <code class="p">(</code><code class="s2">"rooms_per_house"</code><code class="p">,</code> <code class="n">ratio_pipeline</code><code class="p">(),</code> <code class="p">[</code><code class="s2">"total_rooms"</code><code class="p">,</code> <code class="s2">"households"</code><code class="p">]),</code>
        <code class="p">(</code><code class="s2">"people_per_house"</code><code class="p">,</code> <code class="n">ratio_pipeline</code><code class="p">(),</code> <code class="p">[</code><code class="s2">"population"</code><code class="p">,</code> <code class="s2">"households"</code><code class="p">]),</code>
        <code class="p">(</code><code class="s2">"log"</code><code class="p">,</code> <code class="n">log_pipeline</code><code class="p">,</code> <code class="p">[</code><code class="s2">"total_bedrooms"</code><code class="p">,</code> <code class="s2">"total_rooms"</code><code class="p">,</code> <code class="s2">"population"</code><code class="p">,</code>
                               <code class="s2">"households"</code><code class="p">,</code> <code class="s2">"median_income"</code><code class="p">]),</code>
        <code class="p">(</code><code class="s2">"geo"</code><code class="p">,</code> <code class="n">cluster_simil</code><code class="p">,</code> <code class="p">[</code><code class="s2">"latitude"</code><code class="p">,</code> <code class="s2">"longitude"</code><code class="p">]),</code>
        <code class="p">(</code><code class="s2">"cat"</code><code class="p">,</code> <code class="n">cat_pipeline</code><code class="p">,</code> <code class="n">make_column_selector</code><code class="p">(</code><code class="n">dtype_include</code><code class="o">=</code><code class="nb">object</code><code class="p">)),</code>
    <code class="p">],</code>
    <code class="n">remainder</code><code class="o">=</code><code class="n">default_num_pipeline</code><code class="p">)</code>  <code class="c1"># one column remaining: housing_median_age</code></pre>

<p>If you run this <code translate="no">ColumnTransformer</code>, it performs all the transformations and outputs a NumPy array with 24 features<a data-type="indexterm" data-startref="xi_algorithmspreparingdatafor27505_1" id="id1210"/>:<a data-type="indexterm" data-startref="xi_algorithmspreparingdatafortransformationpipelines213043_1" id="id1211"/><a data-type="indexterm" data-startref="xi_Pipelineconstructor2131827_1" id="id1212"/><a data-type="indexterm" data-startref="xi_pipelines213043_1" id="id1213"/><a data-type="indexterm" data-startref="xi_ScikitLearnPipelineconstructor2131827_1" id="id1214"/><a data-type="indexterm" data-startref="xi_transformationpipelines213043_1" id="id1215"/></p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">housing_prepared</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">housing</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">housing_prepared</code><code class="o">.</code><code class="n">shape</code><code class="w"/>
<code class="go">(16512, 24)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">preprocessing</code><code class="o">.</code><code class="n">get_feature_names_out</code><code class="p">()</code><code class="w"/>
<code class="go">array(['bedrooms__ratio', 'rooms_per_house__ratio',</code>
<code class="go">       'people_per_house__ratio', 'log__total_bedrooms',</code>
<code class="go">       'log__total_rooms', 'log__population', 'log__households',</code>
<code class="go">       'log__median_income', 'geo__Cluster 0 similarity', [...],</code>
<code class="go">       'geo__Cluster 9 similarity', 'cat__ocean_proximity_&lt;1H OCEAN',</code>
<code class="go">       'cat__ocean_proximity_INLAND', 'cat__ocean_proximity_ISLAND',</code>
<code class="go">       'cat__ocean_proximity_NEAR BAY', 'cat__ocean_proximity_NEAR OCEAN',</code>
<code class="go">       'remainder__housing_median_age'], dtype=object)</code></pre>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Select and Train a Model"><div class="sect1" id="id393">
<h1>Select and Train a Model</h1>

<p>At<a data-type="indexterm" data-primary="end-to-end ML project" data-secondary="selecting and training a model" id="xi_endtoendMLprojectexerciseselectingandtrainingamodel214693_1"/> last! You framed the problem, you got the data and explored it, you sampled a training set and a test set, and you wrote a preprocessing pipeline to automatically clean up and prepare your data for machine learning algorithms. You are now ready to select and train a machine learning model.</p>








<section data-type="sect2" data-pdf-bookmark="Train and Evaluate on the Training Set"><div class="sect2" id="id49">
<h2>Train and Evaluate on the Training Set</h2>

<p>The<a data-type="indexterm" data-primary="training set" data-secondary="training and evaluating on" id="xi_trainingsettrainingandevaluatingon214724_1"/> good news is that thanks to all these previous steps, things are now going to be easy! You decide to train a very basic linear regression model<a data-type="indexterm" data-primary="linear regression" data-secondary="training set evaluation" id="id1216"/> to get started<a data-type="indexterm" data-primary="LinearRegression" id="id1217"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>

<code class="n">lin_reg</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">preprocessing</code><code class="p">,</code> <code class="n">LinearRegression</code><code class="p">())</code>
<code class="n">lin_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">housing</code><code class="p">,</code> <code class="n">housing_labels</code><code class="p">)</code></pre>

<p>Done! You now have a working linear regression model. You try it out on the training set, looking at the first five predictions and comparing them to the labels:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">housing_predictions</code> <code class="o">=</code> <code class="n">lin_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">housing</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">housing_predictions</code><code class="p">[:</code><code class="mi">5</code><code class="p">]</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="o">-</code><code class="mi">2</code><code class="p">)</code>  <code class="c1"># -2 = rounded to the nearest hundred</code><code class="w"/>
<code class="go">array([246000., 372700., 135700.,  91400., 330900.])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">housing_labels</code><code class="o">.</code><code class="n">iloc</code><code class="p">[:</code><code class="mi">5</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="w"/>
<code class="go">array([458300., 483800., 101700.,  96100., 361800.])</code></pre>

<p>Well, it works, but not always: the first prediction is way off (by over $200,000!), while the other predictions are better: two are off by about 25%, and two are off by less than 10%. Remember that you chose to use the RMSE<a data-type="indexterm" data-primary="root mean square error (RMSE)" id="id1218"/> as your performance measure, so you want to measure this regression model‚Äôs RMSE on the whole training set using Scikit-Learn‚Äôs <code translate="no">root_mean_squared_error()</code><a data-type="indexterm" data-primary="sklearn" data-secondary="metrics.root_mean_squared_error()" id="id1219"/> function<a data-type="indexterm" data-primary="root_mean_squared_error()" id="id1220"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">root_mean_squared_error</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lin_rmse</code> <code class="o">=</code> <code class="n">root_mean_squared_error</code><code class="p">(</code><code class="n">housing_labels</code><code class="p">,</code> <code class="n">housing_predictions</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lin_rmse</code><code class="w"/>
<code class="go">68972.88910758484</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>We‚Äôre not using the <code translate="no">score()</code><a data-type="indexterm" data-primary="score()" id="id1221"/> method here because it returns the <em>R<sup>2</sup> coefficient of determination</em> instead of the RMSE. This coefficient represents the ratio of the variance in the data that the model can explain: the closer to 1 (which is the max value), the better. If the model simply predicts the mean all the time, it does not explain any part of the variance, so the model‚Äôs R<sup>2</sup> score is 0. And if the model does even worse than that, then its R<sup>2</sup> score can be negative, and indeed arbitrarily low.</p>
</div>

<p>This is better than nothing, but clearly not a great score: the <code translate="no">median_housing_values</code> of most districts range between $120,000 and $265,000, so a typical prediction error of $68,973 is really not very satisfying. This is an example of a model underfitting the training data. When this happens it can mean that the features do not provide enough <span class="keep-together">information</span> to make good predictions, or that the model is not powerful enough. As we saw in the previous chapter, the main ways to fix underfitting<a data-type="indexterm" data-primary="data" data-secondary="underfitting of" id="id1222"/><a data-type="indexterm" data-primary="underfitting of data" id="id1223"/> are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, which rules out the last option. You could try to add more features, but first you want to try a more complex model to see how it does.</p>

<p>You decide to try a <code translate="no">DecisionTreeRegressor</code><a data-type="indexterm" data-primary="DecisionTreeRegressor" id="id1224"/><a data-type="indexterm" data-primary="sklearn" data-secondary="tree.DecisionTreeRegressor" id="id1225"/>, as this is a fairly powerful model capable of finding complex nonlinear relationships in the data (decision trees<a data-type="indexterm" data-primary="decision trees" data-secondary="in training the model" data-secondary-sortas="training" id="xi_decisiontreesintrainingthemodel21509159_1"/> are presented in more detail in <a data-type="xref" href="ch05.html#trees_chapter">Chapter¬†5</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeRegressor</code>

<code class="n">tree_reg</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">preprocessing</code><code class="p">,</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">))</code>
<code class="n">tree_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">housing</code><code class="p">,</code> <code class="n">housing_labels</code><code class="p">)</code></pre>

<p>Now that the model is trained, you evaluate it on the training set:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">housing_predictions</code> <code class="o">=</code> <code class="n">tree_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">housing</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tree_rmse</code> <code class="o">=</code> <code class="n">root_mean_squared_error</code><code class="p">(</code><code class="n">housing_labels</code><code class="p">,</code> <code class="n">housing_predictions</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tree_rmse</code><code class="w"/>
<code class="go">0.0</code></pre>

<p>Wait, what!? No error at all? Could this model really be absolutely perfect? Of course, it is much more likely that the model has badly overfit the data. How can you be sure? As you saw earlier, you don‚Äôt want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training and part of it for model validation.<a data-type="indexterm" data-startref="xi_trainingsettrainingandevaluatingon214724_1" id="id1226"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Better Evaluation Using Cross-Validation"><div class="sect2" id="id50">
<h2>Better Evaluation Using Cross-Validation</h2>

<p>One<a data-type="indexterm" data-primary="cross-validation" id="xi_crossvalidation215324_1"/><a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="cross-validation" id="xi_ScikitLearncrossvalidation215324_1"/> way to evaluate the decision tree model would be to use the <code>train_‚Äãtest_split()</code><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.train_test_split()" id="id1227"/><a data-type="indexterm" data-primary="train_test_split()" id="id1228"/> function to split the training set into a smaller training set and a validation set, then train your models against the smaller training set and evaluate them against the validation set. It‚Äôs a bit of effort, but nothing too difficult, and it would work fairly well.</p>

<p>A great alternative is to use Scikit-Learn‚Äôs <em>k-fold cross-validation</em><a data-type="indexterm" data-primary="k-fold cross-validation" id="id1229"/> feature. You split the training set into <em>k</em> nonoverlapping subsets called <em>folds</em><a data-type="indexterm" data-primary="folds" id="id1230"/>, then you train and evaluate your model <em>k</em> times, picking a different fold for evaluation every time (i.e., the validation fold) and using the other <em>k</em> ‚Äì 1 folds for training. This process produces <em>k</em> evaluation scores (see <a data-type="xref" href="#k_fold_cross_validation_diagram">Figure¬†2-20</a>).</p>

<figure><div id="k_fold_cross_validation_diagram" class="figure">
<img src="assets/hmls_0220.png" alt="Diagram illustrating _k_-fold cross-validation with _k_ = 10, displaying different validation folds and corresponding evaluation scores for each split." width="1021" height="535"/>
<h6><span class="label">Figure 2-20. </span><em>k</em>-fold cross-validation, with <em>k</em> = 10</h6>
</div></figure>

<p>Scikit-Learn provides a convenient <code translate="no">cross_val_score()</code><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.cross_val_score()" id="id1231"/> function that does just that, and it returns an array containing the <em>k</em> evaluation scores. For example, let‚Äôs use it to evaluate our tree regressor, using <em>k</em> = 10<a data-type="indexterm" data-primary="cross_val_score()" id="id1232"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">cross_val_score</code>

<code class="n">tree_rmses</code> <code class="o">=</code> <code class="o">-</code><code class="n">cross_val_score</code><code class="p">(</code><code class="n">tree_reg</code><code class="p">,</code> <code class="n">housing</code><code class="p">,</code> <code class="n">housing_labels</code><code class="p">,</code>
                              <code class="n">scoring</code><code class="o">=</code><code class="s2">"neg_root_mean_squared_error"</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Scikit-Learn‚Äôs cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the RMSE. It‚Äôs a negative value, so you need to switch the sign of the output to get the RMSE scores.</p>
</div>

<p>Let‚Äôs look at the results:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">tree_rmses</code><code class="p">)</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code><code class="w"/>
<code class="go">count       10.000000</code>
<code class="go">mean     66573.734600</code>
<code class="go">std       1103.402323</code>
<code class="go">min      64607.896046</code>
<code class="go">25%      66204.731788</code>
<code class="go">50%      66388.272499</code>
<code class="go">75%      66826.257468</code>
<code class="go">max      68532.210664</code>
<code class="go">dtype: float64</code></pre>

<p>Now the decision tree doesn‚Äôt look as good as it did earlier. In fact, it seems to perform almost as poorly as the linear regression model! Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The decision tree has an RMSE of about 66,574, with a standard deviation of about 1,103. You would not have this information if you just used one validation set. But cross-validation comes at the cost of training the model several times, so it is not always feasible.</p>

<p>If you compute the same metric for the linear regression model, you will find that the mean RMSE is 70,003 and the standard deviation is 4,182. So the decision tree model seems to perform very slightly better than the linear model, but the difference is minimal due to severe overfitting. We know there‚Äôs an overfitting problem because the training error is low (actually zero) while the validation error is high.</p>

<p>Let‚Äôs try one last model now: the <code translate="no">RandomForestRegressor</code><a data-type="indexterm" data-primary="RandomForestRegressor" id="xi_RandomForestRegressor2157258_1"/><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.RandomForestRegressor" id="xi_ScikitLearnsklearnensembleRandomForestRegressor2157258_1"/>. As you will see in <a data-type="xref" href="ch06.html#ensembles_chapter">Chapter¬†6</a>, random forests work by training many decision trees on random subsets of the features, then averaging out their predictions. Such models composed of many other models are called <em>ensembles</em><a data-type="indexterm" data-primary="ensemble learning" data-secondary="cross-validation" id="id1233"/>:<a data-type="indexterm" data-primary="ensemble methods" id="id1234"/> if the underlying models are very diverse, then their errors will not be very correlated, and therefore averaging out the predictions will smooth out the errors, reduce overfitting, and improve the overall performance. The code is much the same as earlier:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestRegressor</code>

<code class="n">forest_reg</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">preprocessing</code><code class="p">,</code>
                           <code class="n">RandomForestRegressor</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">))</code>
<code class="n">forest_rmses</code> <code class="o">=</code> <code class="o">-</code><code class="n">cross_val_score</code><code class="p">(</code><code class="n">forest_reg</code><code class="p">,</code> <code class="n">housing</code><code class="p">,</code> <code class="n">housing_labels</code><code class="p">,</code>
                                <code class="n">scoring</code><code class="o">=</code><code class="s2">"neg_root_mean_squared_error"</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code></pre>

<p class="pagebreak-before">Let‚Äôs look at the scores:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">forest_rmses</code><code class="p">)</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code><code class="w"/>
<code class="go">count       10.000000</code>
<code class="go">mean     47038.092799</code>
<code class="go">std       1021.491757</code>
<code class="go">min      45495.976649</code>
<code class="go">25%      46510.418013</code>
<code class="go">50%      47118.719249</code>
<code class="go">75%      47480.519175</code>
<code class="go">max      49140.832210</code>
<code class="go">dtype: float64</code></pre>

<p>Wow, this is much better: random forests really look very promising for this task! However, if you train a <code translate="no">RandomForestRegressor</code> and measure the RMSE on the training set, you will find roughly 17,551: that‚Äôs much lower, meaning that there‚Äôs still quite a lot of overfitting going on. Possible solutions are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. Before you dive much deeper into random forests, however, you should try out many other models from various categories of machine learning algorithms (e.g., several support vector machines with different kernels, and possibly a neural network), without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models.<a data-type="indexterm" data-startref="xi_crossvalidation215324_1" id="id1235"/><a data-type="indexterm" data-startref="xi_decisiontreesintrainingthemodel21509159_1" id="id1236"/><a data-type="indexterm" data-startref="xi_endtoendMLprojectexerciseselectingandtrainingamodel214693_1" id="id1237"/><a data-type="indexterm" data-startref="xi_RandomForestRegressor2157258_1" id="id1238"/><a data-type="indexterm" data-startref="xi_ScikitLearncrossvalidation215324_1" id="id1239"/><a data-type="indexterm" data-startref="xi_ScikitLearnsklearnensembleRandomForestRegressor2157258_1" id="id1240"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Fine-Tune Your Model"><div class="sect1" id="id394">
<h1>Fine-Tune Your Model</h1>

<p>Let‚Äôs<a data-type="indexterm" data-primary="end-to-end ML project" data-secondary="fine-tune your model" id="xi_endtoendMLprojectexercisefinetuneyourmodel216036_1"/> assume that you now have a shortlist of promising models. You now need to fine-tune them. Let‚Äôs look at a few ways you can do that.</p>








<section data-type="sect2" data-pdf-bookmark="Grid Search"><div class="sect2" id="id51">
<h2>Grid Search</h2>

<p>One<a data-type="indexterm" data-primary="grid search" id="xi_gridsearch216064_1"/><a data-type="indexterm" data-primary="GridSearchCV" id="xi_GridSearchCV216064_1"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="tuning of" id="xi_hyperparameterstuningof216064_1"/><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.GridSearchCV" id="xi_ScikitLearnsklearnmodel_selectionGridSearchCV216064_1"/> option would be to fiddle with the hyperparameters<a data-type="indexterm" data-primary="hyperparameters" data-secondary="grid search and" id="xi_hyperparametersgridsearchand2160655_1"/> manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.</p>

<p>Instead, you can use Scikit-Learn‚Äôs <code translate="no">GridSearchCV</code> class to search for you. All you need to do is tell it which hyperparameters you want it to experiment with and what values to try out, and it will use cross-validation to evaluate all the possible 
<span class="keep-together">combinations</span> of hyperparameter values. For example, the following code searches for the best combination of hyperparameter values for the <code translate="no">RandomForestRegressor</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="widows9"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>

<code class="n">full_pipeline</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([</code>
    <code class="p">(</code><code class="s2">"preprocessing"</code><code class="p">,</code> <code class="n">preprocessing</code><code class="p">),</code>
    <code class="p">(</code><code class="s2">"random_forest"</code><code class="p">,</code> <code class="n">RandomForestRegressor</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)),</code>
<code class="p">])</code>
<code class="n">param_grid</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">{</code><code class="s1">'preprocessing__geo__n_clusters'</code><code class="p">:</code> <code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">10</code><code class="p">],</code>
     <code class="s1">'random_forest__max_features'</code><code class="p">:</code> <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">8</code><code class="p">]},</code>
    <code class="p">{</code><code class="s1">'preprocessing__geo__n_clusters'</code><code class="p">:</code> <code class="p">[</code><code class="mi">10</code><code class="p">,</code> <code class="mi">15</code><code class="p">],</code>
     <code class="s1">'random_forest__max_features'</code><code class="p">:</code> <code class="p">[</code><code class="mi">6</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">10</code><code class="p">]},</code>
<code class="p">]</code>
<code class="n">grid_search</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">full_pipeline</code><code class="p">,</code> <code class="n">param_grid</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
                           <code class="n">scoring</code><code class="o">=</code><code class="s1">'neg_root_mean_squared_error'</code><code class="p">)</code>
<code class="n">grid_search</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">housing</code><code class="p">,</code> <code class="n">housing_labels</code><code class="p">)</code></pre>

<p>Notice that you can refer to any hyperparameter of any estimator in a pipeline, even if this estimator is nested deep inside several pipelines and column transformers. For example, when Scikit-Learn sees <code translate="no">"preprocessing__geo__n_clusters"</code>, it splits this string at the double underscores, then it looks for an estimator named 
<span class="keep-together"><code translate="no">"preprocessing"</code></span> in the pipeline and finds the preprocessing <code translate="no">ColumnTransformer</code>. Next, it looks for a transformer named <code translate="no">"geo"</code> inside this <code translate="no">ColumnTransformer</code> and finds the <code translate="no">ClusterSimilarity</code> transformer we used on the latitude and longitude attributes. Then it finds this transformer‚Äôs <code translate="no">n_clusters</code> hyperparameter. Similarly, <code translate="no">random_forest__max_features</code> refers to the <code translate="no">max_features</code> hyperparameter of the estimator named <code translate="no">"random_forest"</code>, which is of course the <code translate="no">RandomForestRegressor</code> model (the <code translate="no">max_features</code> hyperparameter will be explained in <a data-type="xref" href="ch06.html#ensembles_chapter">Chapter¬†6</a>).</p>
<div data-type="tip"><h6>Tip</h6>
<p>Wrapping preprocessing steps in a Scikit-Learn pipeline<a data-type="indexterm" data-primary="pipelines" id="id1241"/> allows you to tune the preprocessing hyperparameters along with the model hyperparameters. This is a good thing since they often interact. For example, perhaps increasing <code translate="no">n_clusters</code> requires increasing <code translate="no">max_features</code> as well. If fitting the pipeline transformers is computationally expensive, you can set the pipeline‚Äôs <code translate="no">memory</code> parameter to the path of a caching directory: when you first fit the pipeline, Scikit-Learn will save the fitted transformers to this directory. If you then fit the pipeline again with the same hyperparameters, Scikit-Learn will just load the cached transformers.</p>
</div>

<p>There are two dictionaries in this <code translate="no">param_grid</code>, so <code translate="no">GridSearchCV</code> will first evaluate all 3 √ó 3 = 9 combinations of <code translate="no">n_clusters</code> and <code translate="no">max_features</code> hyperparameter values specified in the first <code translate="no">dict</code>, then it will try all 2 √ó 3 = 6 combinations of hyperparameter values in the second <code translate="no">dict</code>. So in total the grid search will explore 9 + 6 = 15 combinations of hyperparameter values, and it will train the pipeline 3 times per combination, since we are using 3-fold cross validation. This means there will be a grand total of 15 √ó 3 = 45 rounds of training! It may take a while, but when it is done you can get the best combination of parameters like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">grid_search</code><code class="o">.</code><code class="n">best_params_</code><code class="w"/>
<code class="go">{'preprocessing__geo__n_clusters': 15, 'random_forest__max_features': 6}</code></pre>

<p>In this example, the best model is obtained by setting <code translate="no">n_clusters</code> to 15 and setting <code translate="no">max_features</code> to 6.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Since 15 is the maximum value that was evaluated for <code translate="no">n_clusters</code>, you should probably try searching again with higher values; the score may continue to improve.</p>
</div>

<p>You can access the best estimator using <code translate="no">grid_search.best_estimator_</code>. If <code>Grid‚ÄãSearchCV</code> is initialized with <code translate="no">refit=True</code> (which is the default), then once it finds the best estimator using cross-validation, it retrains it on the whole training set. This is usually a good idea, since feeding it more data will likely improve its <span class="keep-together">performance</span>.</p>

<p>The evaluation scores are available using <code translate="no">grid_search.cv_results_</code>. This is a dictionary, but if you wrap it in a DataFrame you get a nice list of all the test scores for each combination of hyperparameters and for each cross-validation split, as well as the mean test score across all splits:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">cv_res</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">grid_search</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">cv_res</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="s2">"mean_test_score"</code><code class="p">,</code> <code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># change column names to fit on this page, and show rmse = -score</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">cv_res</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>  <code class="c1"># note: the 1st column is the row ID</code><code class="w"/>
<code class="go">    n_clusters  max_features  split0  split1  split2  mean_test_rmse</code>
<code class="go">12          15             6   42725   43708   44335           43590</code>
<code class="go">13          15             8   43486   43820   44900           44069</code>
<code class="go">6           10             4   43798   44036   44961           44265</code>
<code class="go">9           10             6   43710   44163   44967           44280</code>
<code class="go">7           10             6   43710   44163   44967           44280</code></pre>

<p>The mean test RMSE score for the best model is 43,590, which is better than the score you got earlier using the default hyperparameter values (which was 47,038). Congratulations, you have successfully fine-tuned your best model!<a data-type="indexterm" data-startref="xi_gridsearch216064_1" id="id1242"/><a data-type="indexterm" data-startref="xi_GridSearchCV216064_1" id="id1243"/><a data-type="indexterm" data-startref="xi_hyperparametersgridsearchand2160655_1" id="id1244"/><a data-type="indexterm" data-startref="xi_hyperparameterstuningof216064_1" id="id1245"/><a data-type="indexterm" data-startref="xi_ScikitLearnsklearnmodel_selectionGridSearchCV216064_1" id="id1246"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Randomized Search"><div class="sect2" id="id52">
<h2>Randomized Search</h2>

<p>The<a data-type="indexterm" data-primary="randomized search" id="xi_randomizedsearch216664_1"/> grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but <code translate="no">RandomizedSearchCV</code><a data-type="indexterm" data-primary="RandomizedSearchCV" id="id1247"/><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.RandomizedSearchCV" id="id1248"/> is often preferable, especially when the hyperparameter search space is large. This class can be used in much the same way as the <code translate="no">GridSearchCV</code> class, but instead of trying out all possible 
<span class="keep-together">combinations</span> it evaluates a fixed number of combinations, selecting a random value for each hyperparameter at every iteration. This may sound surprising, but this approach has several <span class="keep-together">benefits</span>:</p>

<ul>
<li>
<p>If some of your hyperparameters are continuous (or discrete but with many possible values), and you let randomized search run for, say, 1,000 iterations, then it will explore 1,000 different values for each of these hyperparameters, whereas grid search would only explore the few values you listed for each one.</p>
</li>
<li>
<p>Suppose a hyperparameter does not actually make much difference, but you don‚Äôt know it yet. If it has 10 possible values and you add it to your grid search, then training will take 10 times longer. But if you add it to a random search, it will not make any difference.</p>
</li>
<li>
<p>If there are 6 hyperparameters to explore, each with 10 possible values, then grid search offers no other choice than training the model a million times, whereas random search can always run for any number of iterations you choose.</p>
</li>
</ul>

<p>For each hyperparameter, you must provide either a list of possible values, or a probability distribution:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">RandomizedSearchCV</code>
<code class="kn">from</code> <code class="nn">scipy.stats</code> <code class="kn">import</code> <code class="n">randint</code>

<code class="n">param_distribs</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'preprocessing__geo__n_clusters'</code><code class="p">:</code> <code class="n">randint</code><code class="p">(</code><code class="n">low</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">high</code><code class="o">=</code><code class="mi">50</code><code class="p">),</code>
                  <code class="s1">'random_forest__max_features'</code><code class="p">:</code> <code class="n">randint</code><code class="p">(</code><code class="n">low</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">high</code><code class="o">=</code><code class="mi">20</code><code class="p">)}</code>

<code class="n">rnd_search</code> <code class="o">=</code> <code class="n">RandomizedSearchCV</code><code class="p">(</code>
    <code class="n">full_pipeline</code><code class="p">,</code> <code class="n">param_distributions</code><code class="o">=</code><code class="n">param_distribs</code><code class="p">,</code> <code class="n">n_iter</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
    <code class="n">scoring</code><code class="o">=</code><code class="s1">'neg_root_mean_squared_error'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>

<code class="n">rnd_search</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">housing</code><code class="p">,</code> <code class="n">housing_labels</code><code class="p">)</code></pre>

<p>Scikit-Learn also has <code translate="no">HalvingRandomSearchCV</code><a data-type="indexterm" data-primary="HalvingGridSearchCV" id="id1249"/><a data-type="indexterm" data-primary="HalvingRandomSearchCV" id="id1250"/><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.HalvingGridSearchCV" id="id1251"/><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.HalvingRandomSearchCV" id="id1252"/> and <code translate="no">HalvingGridSearchCV</code> hyperparameter search classes. Their goal is to use the computational resources more efficiently, either to train faster or to explore a larger hyperparameter space. Here‚Äôs how they work: in the first round, many hyperparameter combinations (called ‚Äúcandidates‚Äù) are generated using either the grid approach or the random approach. These candidates are then used to train models that are evaluated using cross-validation, as usual. However, training uses limited resources, which speeds up this first round considerably. By default, ‚Äúlimited resources‚Äù means that the models are trained on a small part of the training set. However, other limitations are possible, such as reducing the number of training iterations if the model has a hyperparameter to set it. Once every candidate has been evaluated, only the best ones go on to the second round, where they are allowed more resources to compete. After several rounds, the final candidates are evaluated using full resources. This may save you some time tuning hyperparameters.<a data-type="indexterm" data-startref="xi_randomizedsearch216664_1" id="id1253"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Ensemble Methods"><div class="sect2" id="id395">
<h2>Ensemble Methods</h2>

<p>Another<a data-type="indexterm" data-primary="ensemble learning" data-secondary="fine-tuning the system" id="id1254"/> way to fine-tune your system is to try to combine the models that perform best. The group (or ‚Äúensemble‚Äù) will often perform better than the best individual model‚Äîjust like random forests perform better than the individual decision trees they rely on‚Äîespecially if the individual models make very different types of errors. For example, you could train and fine-tune a <em>k</em>-nearest neighbors model, then create an ensemble model that just predicts the mean of the random forest prediction and that model‚Äôs prediction. We will cover this topic in more detail in <a data-type="xref" href="ch06.html#ensembles_chapter">Chapter¬†6</a>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Analyzing the Best Models and Their Errors"><div class="sect2" id="id53">
<h2>Analyzing the Best Models and Their Errors</h2>

<p>You<a data-type="indexterm" data-primary="random forests" data-secondary="analysis of models and their errors" id="id1255"/> will often gain good insights on the problem by inspecting the best models. For example, the <code translate="no">RandomForestRegressor</code><a data-type="indexterm" data-primary="RandomForestRegressor" id="id1256"/><a data-type="indexterm" data-primary="sklearn" data-secondary="ensemble.RandomForestRegressor" id="id1257"/> can indicate the relative importance of each attribute for making accurate predictions:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">final_model</code> <code class="o">=</code> <code class="n">rnd_search</code><code class="o">.</code><code class="n">best_estimator_</code>  <code class="c1"># includes preprocessing</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">feature_importances</code> <code class="o">=</code> <code class="n">final_model</code><code class="p">[</code><code class="s2">"random_forest"</code><code class="p">]</code><code class="o">.</code><code class="n">feature_importances_</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">feature_importances</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code class="w"/>
<code class="go">array([0.07, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.19, [...], 0.  , 0.01])</code></pre>

<p>Let‚Äôs sort these importance scores in descending order and display them next to their corresponding attribute names:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="nb">sorted</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">feature_importances</code><code class="p">,</code><code class="w"/>
<code class="gp">... </code>           <code class="n">final_model</code><code class="p">[</code><code class="s2">"preprocessing"</code><code class="p">]</code><code class="o">.</code><code class="n">get_feature_names_out</code><code class="p">()),</code><code class="w"/>
<code class="gp">... </code>       <code class="n">reverse</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="go">[(np.float64(0.18599734460509476), 'log__median_income'),</code>
<code class="go"> (np.float64(0.07338850855844489), 'cat__ocean_proximity_INLAND'),</code>
<code class="go"> (np.float64(0.06556941990883976), 'bedrooms__ratio'),</code>
<code class="go"> (np.float64(0.053648710076725316), 'rooms_per_house__ratio'),</code>
<code class="go"> (np.float64(0.04598870861894749), 'people_per_house__ratio'),</code>
<code class="go"> (np.float64(0.04175269214442519), 'geo__Cluster 30 similarity'),</code>
<code class="go"> (np.float64(0.025976797232869678), 'geo__Cluster 25 similarity'),</code>
<code class="go"> (np.float64(0.023595895886342255), 'geo__Cluster 36 similarity'),</code>
<code class="go"> [...]</code>
<code class="go"> (np.float64(0.0004325970342247361), 'cat__ocean_proximity_NEAR BAY'),</code>
<code class="go"> (np.float64(3.0190221102670295e-05), 'cat__ocean_proximity_ISLAND')]</code></pre>

<p>With this information, you may want to try dropping some of the less useful features (e.g., apparently only one <code translate="no">ocean_proximity</code> category is really useful, so you could try dropping the others).</p>
<div data-type="tip"><h6>Tip</h6>
<p>The <code translate="no">sklearn.feature_selection.SelectFromModel</code><a data-type="indexterm" data-primary="sklearn" data-secondary="feature_selection.SelectFromModel" id="id1258"/><a data-type="indexterm" data-primary="SelectFromModel" id="id1259"/> <a data-type="indexterm" data-primary="feature selection" id="id1260"/>transformer can automatically drop the least useful features for you: when you fit it, it trains a model (typically a random forest), looks at its <code translate="no">feature_importances_</code> attribute, and selects the most useful features. Then when you call <code translate="no">transform()</code>, it drops the other features.</p>
</div>

<p>You should also look at the specific errors that your system makes, then try to understand why it makes them and what could fix the problem: adding extra features or getting rid of uninformative ones, cleaning up outliers, etc.</p>

<p>Now is also a good time to check <em>model fairness</em><a data-type="indexterm" data-primary="model fairness" id="id1261"/>: it should not only work well on average, but also on various categories of districts, whether they‚Äôre rural or urban, rich or poor, northern or southern, minority or not, etc. This requires a detailed <em>bias analysis</em><a data-type="indexterm" data-primary="biases" data-secondary="bias analysis" id="id1262"/>: creating subsets of your validation set for each category, and analyzing your model‚Äôs performance on them. That‚Äôs a lot of work, but it‚Äôs important: if your model performs poorly on a whole category of districts, then it should probably not be deployed until the issue is resolved, or at least it should not be used to make predictions for that category, as it may do more harm than good.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Evaluate Your System on the Test Set"><div class="sect2" id="id54">
<h2>Evaluate Your System on the Test Set</h2>

<p>After tweaking your models for a while, you eventually have a system that performs sufficiently well. You are ready to evaluate the final model on the test set. There is nothing special about this process; just get the predictors and the labels from your test set and run your <code translate="no">final_model</code> to transform the data and make predictions, then evaluate these predictions:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">X_test</code> <code class="o">=</code> <code class="n">strat_test_set</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s2">"median_house_value"</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">strat_test_set</code><code class="p">[</code><code class="s2">"median_house_value"</code><code class="p">]</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>

<code class="n">final_predictions</code> <code class="o">=</code> <code class="n">final_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>

<code class="n">final_rmse</code> <code class="o">=</code> <code class="n">root_mean_squared_error</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">final_predictions</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">final_rmse</code><code class="p">)</code>  <code class="c1"># prints 41445.533268606625</code></pre>

<p>In some cases, such a point estimate of the generalization error will not be quite enough to convince you to launch: what if it is just 0.1% better than the model currently in production? You might want to have an idea of how precise this estimate is. For this, you can compute a 95% <em>confidence interval</em><a data-type="indexterm" data-primary="confidence interval" id="id1263"/> for the generalization error using <code translate="no">scipy.stats.bootstrap()</code>. You get a fairly large interval from 39,521 to 43,702, and your previous point estimate of 41,445 is roughly in the middle of it:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">scipy</code> <code class="kn">import</code> <code class="n">stats</code>

<code class="k">def</code> <code class="nf">rmse</code><code class="p">(</code><code class="n">squared_errors</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">squared_errors</code><code class="p">))</code>

<code class="n">confidence</code> <code class="o">=</code> <code class="mf">0.95</code>
<code class="n">squared_errors</code> <code class="o">=</code> <code class="p">(</code><code class="n">final_predictions</code> <code class="o">-</code> <code class="n">y_test</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code>
<code class="n">boot_result</code> <code class="o">=</code> <code class="n">stats</code><code class="o">.</code><code class="n">bootstrap</code><code class="p">([</code><code class="n">squared_errors</code><code class="p">],</code> <code class="n">rmse</code><code class="p">,</code>
                              <code class="n">confidence_level</code><code class="o">=</code><code class="n">confidence</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">rmse_lower</code><code class="p">,</code> <code class="n">rmse_upper</code> <code class="o">=</code> <code class="n">boot_result</code><code class="o">.</code><code class="n">confidence_interval</code></pre>

<p>If you do a lot of hyperparameter tuning<a data-type="indexterm" data-primary="hyperparameters" data-secondary="tuning of" id="id1264"/>, the performance will usually be slightly worse than what you measured using cross-validation<a data-type="indexterm" data-primary="cross-validation" id="id1265"/>. That‚Äôs because your system ends up fine-tuned to perform well on the validation data and will likely not perform as well on unknown datasets. That‚Äôs not the case in this example since the test RMSE is lower than the validation RMSE, but when it happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data.</p>

<p>Now comes the project prelaunch phase. Presenting your solution effectively is what sets great data scientists apart from good ones. You should create concise reports (Markdown, PDFs, slides), visualize key insights (e.g., using Matplotlib or other tools such as SeaBorn or Tableau), and tailor your message to the audience: technical for peers, high-level for stakeholders. Provide impactful and easy-to-remember statements (e.g., ‚Äúthe median income is the number one predictor of housing prices‚Äù). Highlight what you have learned, what worked and what did not, what assumptions were made, and what your system‚Äôs limitations are.</p>

<p>Your results should be reproducible<a data-type="indexterm" data-primary="reproducibility" id="id1266"/> (as much as possible): make the code accessible to your team (e.g., via GitHub), add a structured <em>README</em> file to guide a technical person through the installation steps. Provide clear notebooks (e.g., Jupyter) with code, explanations, and results, writing clean, well-commented code. Define a <em>requirements.txt</em> or <em>environment.yml</em> file containing all the required libraries along with their precise versions (or create a Docker image). Set seeds for random generators, and remove any other source of variability.</p>

<p>In this California housing example, the final performance of the system is not much better than the experts‚Äô price estimates, which were often off by 30%, but it may still be a good idea to launch it, especially if this frees up some time for the experts so they can work on more interesting and productive tasks.<a data-type="indexterm" data-startref="xi_endtoendMLprojectexercisefinetuneyourmodel216036_1" id="id1267"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Launch, Monitor, and Maintain Your System"><div class="sect1" id="id55">
<h1>Launch, Monitor, and Maintain Your System</h1>

<p>Perfect<a data-type="indexterm" data-primary="end-to-end ML project" data-secondary="launching, monitoring, and maintaining" id="xi_endtoendMLprojectexerciselaunchingmonitoringandmaintaining217738_1"/>, you got approval to launch! You now need to get your solution ready for production (e.g., polish the code, write documentation and tests, and so on). Then you can deploy your model to your production environment. The most basic way to do this is just to save the best model you trained, transfer the file to your production environment, and load it. To save the model, you can use the <code translate="no">joblib</code> library<a data-type="indexterm" data-primary="joblib library" id="xi_jobliblibrary21773411_1"/><a data-type="indexterm" data-primary="sklearn" data-secondary="externals.joblib" id="xi_ScikitLearnsklearnexternalsjoblib21773411_1"/> like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">joblib</code>

<code class="n">joblib</code><code class="o">.</code><code class="n">dump</code><code class="p">(</code><code class="n">final_model</code><code class="p">,</code> <code class="s2">"my_california_housing_model.pkl"</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>It‚Äôs often a good idea to save every model you experiment with so that you can come back easily to any model you want. You may also save the cross-validation scores and perhaps the actual predictions on the validation set. This will allow you to easily compare scores across model types, and compare the types of errors they make.</p>
</div>

<p class="pagebreak-before">Once your model is transferred to production, you can load it and use it. For this you must first import any custom classes and functions the model relies on (which means transferring the code to production), then load the model using <code translate="no">joblib</code> and use it to make predictions:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">joblib</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># import KMeans, BaseEstimator, TransformerMixin, rbf_kernel, etc.</code>

<code class="k">def</code> <code class="nf">column_ratio</code><code class="p">(</code><code class="n">X</code><code class="p">):</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>
<code class="k">def</code> <code class="nf">ratio_name</code><code class="p">(</code><code class="n">function_transformer</code><code class="p">,</code> <code class="n">feature_names_in</code><code class="p">):</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>
<code class="k">class</code> <code class="nc">ClusterSimilarity</code><code class="p">(</code><code class="n">BaseEstimator</code><code class="p">,</code> <code class="n">TransformerMixin</code><code class="p">):</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>

<code class="n">final_model_reloaded</code> <code class="o">=</code> <code class="n">joblib</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"my_california_housing_model.pkl"</code><code class="p">)</code>

<code class="n">new_data</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># some new districts to make predictions for</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">final_model_reloaded</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">new_data</code><code class="p">)</code></pre>

<p>For example, perhaps the model will be used within a website: the user will type in some data about a new district and click the Estimate Price button. This will send a query containing the data to the web server, which will forward it to your web application, and finally your code will simply call the model‚Äôs <code translate="no">predict()</code> method (you want to load the model upon server startup, rather than every time the model is used). Alternatively, you can wrap the model within a dedicated web service that your web application can query through a REST API‚Å†<sup><a data-type="noteref" id="id1268-marker" href="ch02.html#id1268">14</a></sup> (see <a data-type="xref" href="#webservice_model_diagram">Figure¬†2-21</a>). This makes it easier to upgrade your model to new versions without interrupting the main application. It also simplifies scaling, since you can start as many web services as needed and load-balance the requests coming from your web application across these web services. Moreover, it allows your web application to use any programming language, not just Python.</p>

<figure><div id="webservice_model_diagram" class="figure">
<img src="assets/hmls_0221.png" alt="Diagram showing a user interacting with a web app, which sends inputs to a web service hosting a model and receives predictions in return." width="1282" height="153"/>
<h6><span class="label">Figure 2-21. </span>A model deployed as a web service and used by a web application</h6>
</div></figure>

<p>Another popular strategy is to deploy your model to the cloud, for example on Google‚Äôs Vertex AI<a data-type="indexterm" data-primary="Google Vertex AI" id="id1269"/><a data-type="indexterm" data-primary="Vertex AI" id="id1270"/> (formerly Google Cloud AI Platform and Google Cloud ML Engine): just save your model using <code translate="no">joblib</code> and upload it to Google Cloud Storage (GCS), then go to Vertex AI and create a new model version, pointing it to the GCS file.<a data-type="indexterm" data-startref="xi_jobliblibrary21773411_1" id="id1271"/><a data-type="indexterm" data-startref="xi_ScikitLearnsklearnexternalsjoblib21773411_1" id="id1272"/> That‚Äôs it! This gives you a simple web service that takes care of load balancing and scaling for you. It takes JSON requests containing the input data (e.g., of a district) and returns JSON responses containing the predictions. You can then use this web service in your website (or whatever production environment you are using).</p>

<p>But deployment is not the end of the story. You also need to write monitoring code to check your system‚Äôs live performance at regular intervals and trigger alerts when it drops. It may drop very quickly, for example if a component breaks in your infrastructure, but be aware that it could also decay very slowly, which can easily go unnoticed for a long time. This is quite common because of data drift: if the model was trained with last year‚Äôs data, it may not be adapted to today‚Äôs data.</p>

<p>So, you need to monitor your model‚Äôs live performance<a data-type="indexterm" data-primary="performance measures" id="id1273"/>. But how do you do that? Well, it depends. In some cases, the model‚Äôs performance can be inferred from downstream metrics. For example, if your model is part of a recommender system and it suggests products that the users may be interested in, then it‚Äôs easy to monitor the number of recommended products sold each day. If this number drops (compared to nonrecommended products), then the prime suspect is the model. This may be because the data pipeline is broken, or perhaps the model needs to be retrained on fresh data (as we will discuss shortly).</p>

<p>However, you may also need human analysis to assess the model‚Äôs performance. For example, suppose you trained an image classification model (we‚Äôll look at these in <a data-type="xref" href="ch03.html#classification_chapter">Chapter¬†3</a>) to detect various product defects on a production line. How can you get an alert if the model‚Äôs performance drops, before thousands of defective products get shipped to your clients? One solution is to send to human raters a sample of all the pictures that the model classified (especially pictures that the model wasn‚Äôt so sure about). Depending on the task, the raters may need to be experts, or they could be nonspecialists, such as workers on a crowdsourcing platform (e.g., Amazon Mechanical Turk). In some applications they could even be the users themselves, responding, for example, via surveys or repurposed captchas.‚Å†<sup><a data-type="noteref" id="id1274-marker" href="ch02.html#id1274">15</a></sup></p>

<p>Either way, you need to put in place a monitoring system (with or without human raters to evaluate the live model), as well as all the relevant processes to define what to do in case of failures and how to prepare for them. Unfortunately, this can be a lot of work. In fact, it is often much more work than building and training a model.</p>

<p>If the data keeps evolving, you will need to update your datasets and retrain your model regularly. You should probably automate the whole process as much as possible. Here are a few things you can automate:</p>

<ul class="less_space pagebreak-before">
<li>
<p>Collect fresh data regularly and label it (e.g., using human raters).</p>
</li>
<li>
<p>Write a script to train the model and fine-tune the hyperparameters automatically. This script could run automatically, for example every day or every week, depending on your needs.</p>
</li>
<li>
<p>Write another script that will evaluate both the new model and the previous model on the updated test set, and deploy the model to production if the performance has not decreased (if it did, make sure you investigate why). The script should probably test the performance of your model on various subsets of the test set, such as poor or rich districts, rural or urban districts, etc.</p>
</li>
</ul>

<p>You should also make sure you evaluate the model‚Äôs input data quality. Sometimes performance will degrade slightly because of a poor-quality signal (e.g., a malfunctioning sensor sending random values, or another team‚Äôs output becoming stale), but it may take a while before your system‚Äôs performance degrades enough to trigger an alert. If you monitor your model‚Äôs inputs, you may catch this earlier. For example, you could trigger an alert if more and more inputs are missing a feature, or the mean or standard deviation drifts too far from the training set, or a categorical feature starts containing new categories.</p>

<p>Finally, make sure you keep backups of every model you create and have the process and tools in place to roll back to a previous model quickly, in case the new model starts failing badly for some reason. Having backups also makes it possible to easily compare new models with previous ones. Similarly, you should keep backups of every version of your datasets so that you can roll back to a previous dataset if the new one ever gets corrupted (e.g., if the fresh data that gets added to it turns out to be full of outliers). Having backups of your datasets also allows you to evaluate any model against any previous dataset.</p>

<p>As you can see, machine learning involves quite a lot of infrastructure. This is a very broad topic called <em>ML Operations</em> (MLOps)<a data-type="indexterm" data-primary="ML Operations (MLOps)" id="id1275"/>, which deserves its own book. So don‚Äôt be surprised if your first ML project takes a lot of effort and time to build and deploy to production. Fortunately, once all the infrastructure is in place, going from idea to production will be much faster.<a data-type="indexterm" data-startref="xi_endtoendMLprojectexerciselaunchingmonitoringandmaintaining217738_1" id="id1276"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Try It Out!"><div class="sect1" id="id727">
<h1>Try It Out!</h1>

<p>Hopefully this chapter gave you a good idea of what a machine learning project looks like as well as showing you some of the tools you can use to train a great system. As you can see, much of the work is in the data preparation step: building monitoring tools, setting up human evaluation pipelines, and automating regular model training. The machine learning algorithms are important, of course, but it is probably preferable to be comfortable with the overall process and know three or four algorithms well rather than to spend all your time exploring advanced algorithms.</p>

<p>So, if you have not already done so, now is a good time to pick up a laptop, select a dataset that you are interested in, and try to go through the whole process from A to Z. A good place to start is on a competition website such as <a href="https://kaggle.com">Kaggle</a>: you will have a dataset to play with, a clear goal, and people to share the experience with. Have fun!</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id56">
<h1>Exercises</h1>

<p>The following exercises are based on this chapter‚Äôs housing dataset:</p>
<ol>
<li>
<p>Try a support vector machine regressor (<code translate="no">sklearn.svm.SVR</code>) with various hyperparameters, such as <code translate="no">kernel="linear"</code> (with various values for the 
<span class="keep-together"><code translate="no">C</code> hyperparameter)</span> or <code translate="no">kernel="rbf"</code> (with various values for the <code translate="no">C</code> and <code translate="no">gamma</code> hyperparameters). Note that support vector machines don‚Äôt scale well to large datasets, so you should probably train your model on just the first 5,000 instances of the training set and use only 3-fold cross-validation, or else it will take hours. Don‚Äôt worry about what the hyperparameters mean for now; these are explained in the online chapter on SVMs at <a href="https://homl.info/" class="bare"><em class="hyperlink">https://homl.info/</em></a>. How does the best <code translate="no">SVR</code> predictor perform?</p>
</li>
<li>
<p>Try replacing the <code translate="no">GridSearchCV</code> with a <code translate="no">RandomizedSearchCV</code>.</p>
</li>
<li>
<p>Try adding a <code translate="no">SelectFromModel</code> transformer in the preparation pipeline to select only the most important attributes.</p>
</li>
<li>
<p>Try creating a custom transformer that trains a <em>k</em>-nearest neighbors regressor (<code translate="no">sklearn.neighbors.KNeighborsRegressor</code>) in its <code translate="no">fit()</code> method, and outputs the model‚Äôs predictions in its <code translate="no">transform()</code> method. The KNN regressor should use only the latitude and longitude as input and predict the median income. Next, add this new transformer to the preprocessing pipeline. This will add a feature representing the smoothed median income over the nearby districts.</p>
</li>
<li>
<p>Automatically explore some preparation options using <code translate="no">RandomizedSearchCV</code>.</p>
</li>
<li>
<p>Try to implement the <code translate="no">StandardScalerClone</code> class again from scratch, then add support for the <code translate="no">inverse_transform()</code> method: executing <code>scaler.‚Äãinverse_transform(scaler.fit_transform(X))</code> should return an array very close to <code translate="no">X</code>. Then add support for feature names: set <code translate="no">feature_names_in_</code> in the <code translate="no">fit()</code> method if the input is a DataFrame. This attribute should be a NumPy array of column names. Lastly, implement the <code translate="no">get_feature_names_out()</code> method: it should have one optional <code translate="no">input_features=None</code> argument. If passed, the method should check that its length matches <code translate="no">n_features_in_</code>, and it should match <code translate="no">feature_names_in_</code> if it is defined; then <code translate="no">input_features</code> should be returned. If <code translate="no">input_features</code> is <code translate="no">None</code>, then the method should either return 
<span class="keep-together"><code translate="no">feature_names_in_</code></span> if it is defined or <code translate="no">np.array(["x0", "x1", ...])</code> with length <code translate="no">n_features_in_</code> otherwise.</p>
</li>
<li>
<p>Tackle a regression task of your choice by following the process you learned in this chapter. For example, you can try tackling the <a href="https://homl.info/usedcars">Vehicle dataset</a>, where the goal is to predict the selling price of a used car, based on its age, the number of kilometers it has driven, its make and model, and more. Another good dataset to try is the <a href="https://homl.info/bikes">Bike Sharing dataset</a>: the objective is to predict the number of bikes rented within a period of time (column <code translate="no">cnt</code>), based on the day of the week, the time, and the weather conditions.<a data-type="indexterm" data-startref="xi_endtoendMLprojectexercise253_1" id="id1277"/></p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter‚Äôs notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id990"><sup><a href="ch02.html#id990-marker">1</a></sup> The original dataset appeared in R. Kelley Pace and Ronald Barry, ‚ÄúSparse Spatial Autoregressions‚Äù, <em>Statistics &amp; Probability Letters</em> 33, no. 3 (1997): 291‚Äì297.</p><p data-type="footnote" id="id994"><sup><a href="ch02.html#id994-marker">2</a></sup> A piece of information fed to a machine learning system is often called a <em>signal</em>, in reference to Claude Shannon‚Äôs information theory, which he developed at Bell Labs to improve telecommunications. His theory: you want a high signal-to-noise ratio.</p><p data-type="footnote" id="id1005"><sup><a href="ch02.html#id1005-marker">3</a></sup> Recall that the transpose operator<a data-type="indexterm" data-primary="transpose operator" id="id1278"/> flips a column vector into a row vector (and vice versa).</p><p data-type="footnote" id="id1032"><sup><a href="ch02.html#id1032-marker">4</a></sup> You might also need to check legal constraints, such as private fields that should never be copied to unsafe data stores.</p><p data-type="footnote" id="id1038"><sup><a href="ch02.html#id1038-marker">5</a></sup> The standard deviation<a data-type="indexterm" data-primary="standard deviation" id="id1279"/> is generally denoted <em>œÉ</em> (the Greek letter sigma), and it is the square root of the <em>variance</em>, which is the average of the squared deviation from the mean. When a feature has a bell-shaped <em>normal distribution</em> (also called a <em>Gaussian distribution</em>), which is very common, the ‚Äú68-95-99.7‚Äù rule applies: about 68% of the values fall within 1<em>œÉ</em> of the mean, 95% within 2<em>œÉ</em>, and 99.7% within 3<em>œÉ</em>.</p><p data-type="footnote" id="id1054"><sup><a href="ch02.html#id1054-marker">6</a></sup> You will often see people set the random seed to 42. This number has no special property, other than being the Answer to the Ultimate Question of Life, the Universe, and Everything.</p><p data-type="footnote" id="id1055"><sup><a href="ch02.html#id1055-marker">7</a></sup> The location information is actually quite coarse, and as a result many districts will have the exact same ID, so they will end up in the same set (test or train). This introduces some unfortunate sampling bias.</p><p data-type="footnote" id="id1068"><sup><a href="ch02.html#id1068-marker">8</a></sup> If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from the Bay Area down to San Diego (as you might expect). You can add a patch of yellow around Sacramento as well.</p><p data-type="footnote" id="id1102"><sup><a href="ch02.html#id1102-marker">9</a></sup> For more details on the design principles, see Lars Buitinck et al., ‚ÄúAPI Design for Machine Learning Software: Experiences from the Scikit-Learn Project‚Äù, arXiv preprint arXiv:1309.0238 (2013).</p><p data-type="footnote" id="id1103"><sup><a href="ch02.html#id1103-marker">10</a></sup> Some predictors also provide methods to measure the confidence of their predictions.</p><p data-type="footnote" id="id1107"><sup><a href="ch02.html#id1107-marker">11</a></sup> If you run <code translate="no">sklearn.set_config(transform_output="pandas")</code>, all transformers will output Pandas DataFrames when they receive a DataFrame as input: Pandas in, Pandas out.</p><p data-type="footnote" id="id1118"><sup><a href="ch02.html#id1118-marker">12</a></sup> See SciPy‚Äôs documentation for more details.</p><p data-type="footnote" id="id1177"><sup><a href="ch02.html#id1177-marker">13</a></sup> With duck typing<a data-type="indexterm" data-primary="duck typing" id="id1280"/>, an object‚Äôs methods and behavior are what matters, not its type: ‚Äúif it looks like a duck and quacks like a duck, it must be a duck‚Äù.</p><p data-type="footnote" id="id1268"><sup><a href="ch02.html#id1268-marker">14</a></sup> In a nutshell, a REST (or RESTful) API <a data-type="indexterm" data-primary="REST APIs" id="id1281"/>is an HTTP-based API that follows some conventions, such as using standard HTTP verbs to read, update, create, or delete resources (GET, POST, PUT, and DELETE) and using JSON for the inputs and outputs.</p><p data-type="footnote" id="id1274"><sup><a href="ch02.html#id1274-marker">15</a></sup> A captcha<a data-type="indexterm" data-primary="captchas" id="id1282"/> is a test to ensure a user is not a robot. These tests have often been used as a cheap way to label training data.</p></div></div></section></div></div></body></html>