["```py\ntest_data = pd.read_csv(\"../data/benchmark_data.csv\", delimiter=\";\")\n```", "```py\nanswers = []\nground_truths = []\nlatencies = []\ncontexts = []\n\nfor i, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Processing rows\"):\n    ground_truth, _, _ = neo4j_driver.execute_query(row[\"cypher\"])   #1\n    ground_truths.append([str(el.data()) for el in ground_truth])\n    start = datetime.now()\n    try:\n        answer, context = get_answer(row[\"question\"])   #2\n        context = [el['content'] for el in context]\n    except Exception:\n        answer, context = None, []\n    latencies.append((datetime.now() - start).total_seconds())   #3\n    answers.append(answer)\n    contexts.append(context)\n #4\ntest_data['ground_truth'] = [str(el) for el in ground_truths]\ntest_data['answer'] = answers\ntest_data['latency'] = latencies\ntest_data['retrieved_contexts'] = contexts\n```", "```py\ndataset = Dataset.from_pandas(test_data.fillna(\"I don't know\"))   #1\n\nresult = evaluate( #2\n    dataset,\n\n    metrics=[ #3\n        answer_correctness,\n        context_recall,\n        faithfulness,\n    ],\n)\n```", "```py\nfor key in [\"answer_correctness\", \"context_recall\", \"faithfulness\"]:\n    test_data[key] = [el[key] for el in result.scores]\ntest_data\n```"]