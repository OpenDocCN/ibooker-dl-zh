- en: 11 Building a generative pretrained Transformer from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Building a generative pretrained Transformer from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal self-attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting and loading weights from a pretrained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating coherent text with GPT-2, the predecessor of ChatGPT and GPT-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative Pretrained Transformer 2 (GPT-2) is an advanced large language model
    (LLM) developed by OpenAI and announced in February 2019\. It represents a significant
    milestone in the field of natural language processing (NLP) and has paved the
    way for the development of even more sophisticated models, including its successors,
    ChatGPT and GPT-4\.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2, an improvement over its predecessor, GPT-1, was designed to generate
    coherent and contextually relevant text based on a given prompt, demonstrating
    a remarkable ability to mimic human-like text generation across various styles
    and topics. Upon its announcement, OpenAI initially decided not to release to
    the public the most powerful version of GPT-2 (also the one you’ll build from
    scratch in this chapter, with 1.5 billion parameters). The main concern was potential
    misuse, such as generating misleading news articles, impersonating individuals
    online, or automating the production of abusive or fake content. This decision
    sparked a significant debate within the AI and tech communities about the ethics
    of AI development and the balance between innovation and safety.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI later adopted a staggered release strategy, gradually making smaller
    versions of the model available while monitoring the effect and exploring safe
    deployment strategies. Eventually, in November 2019, OpenAI released the full
    model, along with several datasets and a tool to detect model-generated text,
    contributing to discussions on responsible AI usage. Because of this release,
    you’ll learn to extract the pretrained weights from GPT-2 and load them to the
    GPT-2 model that you create.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 is based on the Transformer architecture that we discussed in chapters
    9 and 10\. However, unlike the English-to-French translator you created before,
    GPT-2 is a decoder-only Transformer, meaning there is no encoder stack in the
    model. When translating an English phrase into French, the encoder captures the
    meaning of the English phrase and passes it to the decoder to generate the translation.
    However, in text generation tasks, the model does not need an encoder to understand
    a different language. Instead, it generates text based on the previous tokens
    in the sentence, using only a decoder-only architecture. Like other Transformer
    models, GPT-2 uses self-attention mechanisms to process input data in parallel,
    significantly improving the efficiency and effectiveness of training LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 is pretrained on a large corpus of text data, essentially predicting the
    next word in a sentence given the words that precede it. This training enables
    the model to learn a wide range of language patterns, grammar, and knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn to build GPT-2XL, the largest version of GPT-2,
    from scratch. After that, you’ll learn how to extract the pretrained weights from
    Hugging Face (an AI community that hosts and collaborates on machine learning
    models, datasets, and applications) and load them to your own GPT-2 model. You’ll
    use your GPT-2 to generate text by feeding a prompt to the model. GPT-2 calculates
    the probabilities of possible next tokens and samples from these probabilities.
    It can produce coherent and contextually relevant paragraphs of text based on
    the input prompt it receives. Additionally, as you did in chapter 8, you can control
    the creativeness of the generated text by using `temperature` and `top-K` sampling.
  prefs: []
  type: TYPE_NORMAL
- en: While GPT-2 marks a notable advance in NLP, it’s essential to moderate your
    expectations and recognize its inherent limitations. It’s crucial not to compare
    GPT-2 with ChatGPT or GPT-4 directly, as GPT-2XL has only 1.5 billion parameters
    compared to ChatGPT’s 175 billion and GPT-4’s estimated 1.76 trillion parameters.
    One of the main limitations of GPT-2 is its lack of genuine comprehension of the
    content it generates. The model predicts the next word in a sequence based on
    the probability distribution of words in its training data, which can produce
    syntactically correct and seemingly logical text. However, the model lacks a true
    understanding of the meaning behind the words, leading to potential inaccuracies,
    nonsensical statements, or superficial content.
  prefs: []
  type: TYPE_NORMAL
- en: Another key factor is GPT-2’s limited contextual awareness. While it can maintain
    coherence over short spans of text, it struggles with longer passages, potentially
    resulting in a loss of coherence, contradictions, or irrelevant content. We should
    be cautious not to overestimate the model’s ability to generate long-form content
    that requires sustained attention to context and detail. Therefore, while GPT-2
    represents a significant step forward in NLP, it’s important to approach its generated
    text with a healthy dose of skepticism and set realistic expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 GPT-2 architecture and causal self-attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-2 operates as a solely decoder-based Transformer (it generates text based
    on previous tokens in the sentence without the need for an encoder to understand
    a different language), mirroring the decoder component of the English-to-French
    translator discussed in chapters 9 and 10\. Unlike its bilingual counterpart,
    GPT-2 lacks an encoder and thus does not incorporate encoder-derived inputs in
    its output generation process. The model relies entirely on preceding tokens within
    the sequence to produce its output.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll discuss the architecture of GPT-2\. We will also dive
    into the causal self-attention mechanism, which is the core of the GPT-2 model.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1.1 The architecture of GPT-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GPT-2 comes in four different sizes: small (S), medium (M), large (L), and
    extra-large (XL), each varying in capability. Our primary focus will be on the
    most powerful version, GPT-2XL. The smallest GPT-2 model has around 124 million
    parameters, while the extra-large version has about 1.5 billion parameters. It
    is the most powerful among the GPT-2 models, with the highest number of parameters.
    GPT-2XL can understand complex contexts, generating coherent and nuanced text.'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 consists of many identical decoder blocks. The extra-large version has
    48 decoder blocks, while the other three versions have 12, 24, and 36 decoder
    blocks, respectively. Each of these decoder blocks comprises two distinct sublayers.
    The first sublayer is a causal self-attention layer, which I’ll explain in detail
    soon. The second sublayer is a basic, position-wise, fully connected feed-forward
    network, as we have seen in the encoder and decoder blocks in the English-to-French
    translator. Each sublayer incorporates layer normalization and a residual connection
    to stabilize the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 is a diagram of the architecture of GPT-2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH11_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 The architecture of the GPT-2 model. GPT-2 is a decoder-only Transformer,
    consisting of N identical decoder layers. Each decoder block contains two sublayers.
    The first sublayer is a causal self-attention layer. The second is a feed-forward
    network. Each sublayer uses layer normalization and a residual connection. The
    input is first passed through word embedding and positional encoding, and the
    sum is then passed through the decoder. The output from the decoder goes through
    layer normalization and a linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 first passes indexes for a sequence of tokens through word embedding and
    positional encoding to obtain input embedding (I’ll explain soon how this process
    works). The input embedding is passed through N decoder blocks sequentially. After
    that, the output is passed through layer normalization and a linear layer. The
    number of outputs in GPT-2 is the number of unique tokens in the vocabulary (50,257
    tokens for all GPT-2 versions). The model is designed to predict the next token
    based on all previous tokens in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: To train GPT-2, OpenAI used a dataset called WebText, which was collected automatically
    from the internet. The dataset contained a wide variety of text, including websites
    like Reddit links that were highly upvoted, aiming to cover a broad spectrum of
    human languages and topics. This dataset is estimated to contain about 40GB of
    text.
  prefs: []
  type: TYPE_NORMAL
- en: The training data was broken into sequences of a fixed length (1,024 tokens
    for all GPT-2 versions) and used as inputs. The sequences were shifted to the
    right by one token and used as outputs to the model during training. Since the
    model uses causal self-attention, in which future tokens in a sequence are masked
    (i.e., hidden) during the training process, this is effectively training the model
    to predict the next token based on all previous tokens in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1.2 Word embedding and positional encoding in GPT-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPT-2 uses a subword tokenization method called the Byte Pair Encoder (BPE)
    to break text into individual tokens (whole words or punctuation marks in most
    cases but syllables for uncommon words). These tokens are then mapped into an
    index between 0 and 50,256 since the vocabulary size is 50,257\. GPT-2 transforms
    text in the training data into vector representations that capture its meaning
    through word embedding, similar to what you’ve done in the previous two chapters.
  prefs: []
  type: TYPE_NORMAL
- en: To give you a concrete example, the phrase “this is a prompt” is first converted
    into four tokens through BPE tokenization, `['this', ' is', ' a', ' prompt']`.
    Each token is then represented by a one-hot variable of size 50,257\. The GPT-2
    model passes them through a word embedding layer to compress them into condensed
    vectors with floating point values of a much smaller size, such as a length of
    1,600 in GPT-2XL (the lengths are 768, 1,024, and 1,280, for the other three versions
    of GPT-2, respectively). With word embedding, the phrase “this is a prompt” is
    represented by a matrix with size 4 × 1,600 instead of the original 4 × 50,257.
    Word embedding significantly reduces the number of the model’s parameters and
    makes training more efficient. The left side of figure 11.2 depicts how word embedding
    works.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH11_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 GPT-2 first represents each token in a sequence with a 50,276-value
    one-hot vector. The token representation of the sequence goes through a word embedding
    layer to compress it into an embedding with a dimension of 1,600\. GPT-2 also
    represents each position in a sequence with a 1,024-value one-hot vector. The
    positional representation of the sequence goes through a positional encoding layer
    to compress it into an embedding also with a dimension of 1,600\. The word embedding
    and positional encoding are added together to form the input embedding.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2, like other Transformers, processes input data in parallel, and this inherently
    doesn’t allow it to recognize the sequence order of the input. To address this,
    we need to add positional encodings to the input embeddings. GPT-2 adopts a unique
    approach to positional encoding, diverging from the methodology outlined in the
    seminal 2017 “Attention Is All You Need” paper. Instead, GPT-2’s technique for
    positional encoding parallels that of word embeddings. Given the model’s capacity
    to handle up to 1,024 tokens in an input sequence, each position within the sequence
    is initially denoted by a one-hot vector of the same size. For instance, in the
    sequence “this is a prompt,” the first token is represented by a one-hot vector
    where all elements are zero except for the first, which is set to one. The second
    token follows suit, represented by a vector where all but the second element are
    zero. Consequently, the positional representation for the phrase “this is a prompt”
    manifests as a 4 × 1,024 matrix, as illustrated in the upper right section of
    figure 11.2.
  prefs: []
  type: TYPE_NORMAL
- en: To generate positional encoding, the sequence’s positional representation undergoes
    processing through a linear neural network, which is dimensioned at 1,024 × 1,600.
    The weights within this network are randomly initialized and subsequently refined
    through the training process. As a result, the positional encoding for each token
    in the sequence is a 1,600-value vector, matching the dimension of the word embedding
    vector. A sequence’s input embedding is the sum of its word embedding and positional
    encoding, as depicted at the bottom of figure 11.2\. In the context of the phrase
    “this is a prompt,” both the word embedding and positional encoding are structured
    as 4 × 1,600 matrices. Therefore, the input embedding for “this is a prompt,”
    which is the sum of these two matrices, maintains a dimensionality of 4 × 1,600.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1.3 Causal self-attention in GPT-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Causal self-attention is a crucial mechanism within the GPT-2 model (and broadly
    in the GPT series of models), enabling the model to generate text by conditioning
    on the sequence of previously generated tokens. It’s similar to the masked self-attention
    in the first sublayer of each decoder layer in the English-to-French translator
    we discussed in chapters 9 and 10, though the implementation differs slightly.
  prefs: []
  type: TYPE_NORMAL
- en: Note The concept of “causal” in this context refers to the model’s ability to
    ensure that predictions for a given token can only be influenced by the tokens
    that precede it in the sequence, respecting the causal (time-forward) direction
    of text generation. This is essential for generating coherent and contextually
    relevant text outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention is a mechanism that allows each token in the input sequence to
    attend to all other tokens in the same sequence. In the context of Transformer
    models like GPT-2, self-attention enables the model to weigh the importance of
    other tokens when processing a specific token, thereby capturing the context and
    relationships between words in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure causality, GPT-2’s self-attention mechanism is modified so that any
    given token can only attend to itself and the tokens that have come before it
    in the sequence. This is achieved by masking future tokens (i.e., tokens that
    come after the current token in the sequence) in the attention calculation, ensuring
    that the model cannot “see” or be influenced by future tokens when predicting
    the next token in a sequence. For example, in the phrase “this is a prompt,” the
    mask hides the last three words in the first time step when the model uses the
    word “this” to predict the word “is.” To implement this, positions corresponding
    to future tokens are set to minus infinity when we compute the attention scores.
    After softmax activation, future tokens are allocated zero weights, effectively
    removing them from the attention calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use a concrete example to illustrate exactly how causal self-attention
    works in code. The input embedding for the phrase “this is a prompt” is a 4 ×
    1,600 matrix after word embedding and positional encoding. We then pass this input
    embedding through N decoder layers in GPT-2\. In each decoder layer, it first
    goes through the causal self-attention sublayer as follows. The input embedding
    is passed through three neural networks to create query Q, key K, and value V,
    as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.1 Creating `query`, `key`, and `value` vectors
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates three neural networks
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates an input embedding x
  prefs: []
  type: TYPE_NORMAL
- en: ③ Passes the input embedding the three neural networks to create Q, K, and V
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints out the sizes of Q, K, and V
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create a matrix with size 4 × 1,600, the same size as the input embedding
    for “this is a prompt”. We then pass the input embedding through three neural
    networks, each with a size of 1,600 × 1,600, to obtain query Q, key K, and value
    V. If you run the preceding code block, you’ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The shapes of Q, K, and V are all 4 × 1,600\. Next, instead of using one head,
    we split them into 25 parallel heads. Each head pays attention to different parts
    or aspects of the input, enabling the model to capture a broader range of information
    and form a more detailed and contextual understanding of the input data. As a
    result, we have 25 sets of Q, K, and V:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① Splits Q, K, and V into 25 heads
  prefs: []
  type: TYPE_NORMAL
- en: ② Prints out the size of the multihead Q, K, and V
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the preceding code block, you’ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The shapes of Q, K, and V are now 25 × 4 × 64: this means we have 25 heads;
    each head has a set of query, key, and value, all having a size of 4 × 64.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we calculate the scaled attention scores in each head:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The scaled attention scores are the dot product of Q and K in each head, scaled
    by the square root of the dimension of K, which is 1,600/25 = 64. The scaled attention
    scores form a 4 × 4 matrix in each head, and we print out those in the first head:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The scaled attention scores in the first head are also shown in the bottom left
    table in figure 11.3.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.1
  prefs: []
  type: TYPE_NORMAL
- en: The tensor `scaled_att` contains the scaled attention scores in the 25 heads.
    We have printed out those in the first head previously. How do you print out the
    scaled attention scores in the second head?
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we apply a mask to the scaled attention scores to hide future tokens
    in the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a mask
  prefs: []
  type: TYPE_NORMAL
- en: ② Applies the mask on the scaled attention scores by changing the values to
    –∞ for future tokens
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH11_F03_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 How to calculate masked attention weights in causal self-attention.
    A mask is applied to the scaled attention scores so that values corresponding
    to future tokens (those above the main diagonal in the matrix) become –∞. We then
    apply the softmax function on the masked scaled attention scores and obtain the
    masked attention weights. The masking ensures that predictions for a given token
    can only be influenced by the tokens that precede it in the sequence, not by future
    tokens. This is essential for generating coherent and contextually relevant text
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the preceding code, you’ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The mask is a 4 × 4 matrix as shown at the top of figure 11.3\. The lower half
    of the mask (values below the main diagonal) are 1s while the upper half of the
    mask (values above the main diagonal) are 0s. When this mask is applied to the
    scaled attention scores, the values in the upper half of the matrix become –∞
    (the middle bottom of figure 11.3). This way, when we apply the softmax function
    on the scaled attention scores, the upper half of the attention weights matrix
    is filled with 0s (bottom right of figure 11.3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the attention weights in the first head with the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first row means in the first time step, the token “this” attends only to
    itself and not to any future tokens. Similarly, if you look at the second row,
    the tokens “this is” attend to each other but not to future tokens “a prompt”.
  prefs: []
  type: TYPE_NORMAL
- en: Note The weights in this numerical example are not trained, so don’t take these
    values in attention weights literally. We use them as an example to illustrate
    how causal self-attention works.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.2
  prefs: []
  type: TYPE_NORMAL
- en: We have printed out the attention weights in the first head. How do you print
    out the attention weights in the last (i.e., the 25^(th)) head?
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we calculate the attention vector in each head as the dot product
    of attention weights and the value vector. The attention vectors in the 25 heads
    are then joined together as one single attention vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The final output after causal self-attention is a 4 × 1,600 matrix, the same
    size as the input to the causal self-attention sublayer. The decoder layers are
    designed in such a way that the input and output have the same dimensions, and
    this allows us to stack many decoder layers together to increase the representation
    capacity of the model and to enable hierarchical feature extraction during training.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Building GPT-2XL from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you understand the architecture of GPT-2 and how its core ingredient,
    causal self-attention, functions, let’s create the largest version of GPT-2 from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’ll first learn to use the subword tokenization method in
    GPT-2, the byte pair encoder (BPE) tokenizer, to break text into individual tokens.
    You’ll also learn the GELU activation function used in the feed-forward network
    in GPT-2\. After that, you’ll code in the causal self-attention mechanism and
    combine it with a feed-forward network to form a decoder block. Finally, you’ll
    stack 48 decoder blocks to create the GPT-2XL model. The code in this chapter
    is adapted from the excellent GitHub repository by Andrej Kaparthy ([https://github.com/karpathy/minGPT](https://github.com/karpathy/minGPT)).
    I encourage you to read through the repository if you want to dive deeper into
    how GPT-2 works.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.1 BPE tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPT-2 uses a subword tokenization method called byte pair encoder (BPE), which
    is a data compression technique that has been adapted for use in tokenizing text
    in NLP tasks. It’s particularly well-known for its application in training LLMs,
    such as the GPT series and BERT (Bidirectional Encoder Representations from Transformers).
    The primary goal of BPE is to encode a piece of text into a sequence of tokens
    in a way that balances the vocabulary size and the length of the tokenized text.
  prefs: []
  type: TYPE_NORMAL
- en: BPE operates by iteratively merging the most frequent pair of consecutive characters
    in a dataset into a single new token, subject to certain conditions. This process
    is repeated until a desired vocabulary size is reached or no more merges are beneficial.
    BPE allows for an efficient representation of text, balancing between character-level
    and word-level tokenization. It helps to reduce the vocabulary size without significantly
    increasing the sequence length, which is crucial for the performance of NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the pros and cons of the three types of tokenization methods (character-level,
    word-level, and subword tokenizations) in chapter 8\. Further, you implemented
    a word-level tokenizer from scratch in chapter 8 (and will do so again in chapter
    12). Therefore, in this chapter, we’ll borrow the tokenization method from OpenAI
    directly. The detailed workings of BPE are beyond the scope of this book. All
    you need to know is that it first converts text into subword tokens and then the
    corresponding indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `bpe.py` from Andrej Karpathy’s GitHub repository, [https://mng.bz/861B](https://mng.bz/861B),
    and place the file in the folder /utils/ on your computer. We’ll use the file
    as a local module in this chapter. As Andrej Karpathy explained in his GitHub
    repository, the module is based on OpenAI’s implementation at [https://mng.bz/EOlj](https://mng.bz/EOlj)
    but was mildly modified to make it easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how the module `bpe.py` converts text into tokens and then indexes,
    let’s try an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① The text for an example sentence
  prefs: []
  type: TYPE_NORMAL
- en: ② Instantiates the get_encoder() class from the bpe.py module
  prefs: []
  type: TYPE_NORMAL
- en: ③ Tokenizes the example text and print out the tokens
  prefs: []
  type: TYPE_NORMAL
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The BPE tokenizer splits the example text “This is the original text.” into
    six tokens as shown in the preceding output. Note that the BPE tokenizer doesn’t
    convert uppercase letters to lowercase ones. This leads to more meaningful tokenization
    but also a much larger number of unique tokens. In fact, all versions of GPT-2
    models have a vocabulary size of 50,276, several times larger than the vocabulary
    size in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use the module `bpe.py` to map tokens to indexes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding list contains the six indexes corresponding to the six tokens
    in the example text “This is the original text.”
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also restore the text based on the indexes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: ① Instantiates the BPETokenizer() class from the bpe.py module
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses the tokenizer to restore text based on indexes
  prefs: []
  type: TYPE_NORMAL
- en: The output from the preceding code block is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the BPE tokenizer has restored the example text to its original
    form.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.3
  prefs: []
  type: TYPE_NORMAL
- en: Use the BPE tokenizer to split the phrase “this is a prompt” into tokens. After
    that, map the tokens to indexes. Finally, restore the phrase based on the indexes.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.2 The Gaussian error linear unit activation function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Gaussian error linear unit (GELU) activation function is used in the feed-forward
    sublayers of each decoder block in GPT-2\. GELU provides a blend of linear and
    nonlinear activation properties that have been found to enhance model performance
    in deep learning tasks, particularly NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'GELU offers a nonlinear, smooth curve that allows for more nuanced adjustments
    during training compared to other functions like the rectified linear unit (ReLU).
    This smoothness helps in optimizing the neural network more effectively, as it
    provides a more continuous gradient for backpropagation. To compare GELU with
    ReLU, our go-to activation function, let’s first define a GELU() class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The ReLU function is not differentiable everywhere since it has a kink in it.
    The GELU activation function, in contrast, is differentiable everywhere and provides
    a better learning process. Next we draw a picture of the GELU activation function
    and compare it to ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.2 Comparing two activation functions: GELU and ReLU'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a function to represent ReLU
  prefs: []
  type: TYPE_NORMAL
- en: ② Plots the ReLU activation function in solid lines
  prefs: []
  type: TYPE_NORMAL
- en: ③ Plots the GELU activation function in dashed lines
  prefs: []
  type: TYPE_NORMAL
- en: If you run the preceding code block, you’ll see a graph as shown in figure 11.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH11_F04_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 Comparing the GELU activation function with ReLU. The solid line
    is the ReLU activation function, while the dashed line is the GELU activation
    function. ReLU is not differentiable everywhere since there is a kink in it. GELU,
    in contrast, is differentiable everywhere. This smoothness in GELU helps to optimize
    the neural network more effectively, as it provides a more continuous gradient
    for backpropagation during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the formulation of GELU allows it to model input data distributions
    more effectively. It combines the properties of linear and Gaussian distribution
    modeling, which can be particularly beneficial for the complex, varied data encountered
    in NLP tasks. This capability helps in capturing subtle patterns in language data,
    improving the model’s understanding and generation of text.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.3 Causal self-attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we explained earlier, causal self-attention is the core element in GPT-2
    models. Next, we’ll implement this mechanism from scratch in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: We first specify the hyperparameters in the GPT-2XL model that we’ll build in
    this chapter. To that end, we define a `Config()` class with the values shown
    in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.3 Specifying hyperparameters in GPT-2XL
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a Config() class
  prefs: []
  type: TYPE_NORMAL
- en: ② Places model hyperparameters as attributes in the class
  prefs: []
  type: TYPE_NORMAL
- en: ③ Instantiates the Config() class
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a `Config()` class and create several attributes in it to be used
    as the hyperparameters in the GPT-2XL model. The `n_layer` attribute means the
    GPT-2XL model we construct will have 48 decoder layers (we use the terms “decoder
    block” and “decoder layer” interchangeably). The `n_head` attribute means we’ll
    split Q, K, and V into 25 parallel heads when calculating causal self-attention.
    The `n_embd` attribute means the embedding dimension is 1,600: each token will
    be represented by a 1,600-value vector. The `vocab_size` attribute means there
    are 50,257 unique tokens in the vocabulary. The `block_size` attribute means the
    input sequence to the GPT-2XL model contains at most 1,024 tokens. The dropout
    rates are all set to 0.1\.'
  prefs: []
  type: TYPE_NORMAL
- en: In the last section, I explained in detail how causal self-attention works.
    Next, we define a `CausalSelfAttention()` class to implement it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.4 Implementing causal self-attention
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a mask and registers it as a buffer since it doesn’t need to be updated
  prefs: []
  type: TYPE_NORMAL
- en: ② Passes input embedding through three neural networks to obtain Q, K, and V
  prefs: []
  type: TYPE_NORMAL
- en: ③ Splits Q, K, and V into multiple heads
  prefs: []
  type: TYPE_NORMAL
- en: ④ Calculates masked attention weights in each head
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Concatenates attention vectors in all heads into one single attention vector
  prefs: []
  type: TYPE_NORMAL
- en: 'In PyTorch, `register_buffer` is a method used to register a tensor as a buffer.
    Variables in a buffer are not considered learnable parameters of the model; hence
    they are not updated during backpropagation. In the preceding code block, we have
    created a mask and registered it as a buffer. This has implications for how we
    extract and load model weights later: we’ll omit the masks when retrieving weights
    from GPT-2XL.'
  prefs: []
  type: TYPE_NORMAL
- en: As we explained in the first section, the input embedding is passed through
    three neural networks to obtain query Q, key K, and value V. We then split them
    into 25 heads and calculate masked self-attention in each head. After that, we
    join the 25 attention vectors back into one single attention vector, which is
    the output of the previous `CausalSelfAttention()` class.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.4 Constructing the GPT-2XL model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we add a feed-forward network to the causal self-attention sublayer to
    form a decoder block, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.5 Constructing a decoder block
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: ① Initiates the Block() class
  prefs: []
  type: TYPE_NORMAL
- en: ② The first sublayer in the block is the causal self-attention sublayer, with
    layer normalization and residual connection.
  prefs: []
  type: TYPE_NORMAL
- en: ③ The second sublayer in the block is a feed-forward network, with GELU activation,
    layer normalization, and residual connection.
  prefs: []
  type: TYPE_NORMAL
- en: Every decoder block is composed of two sublayers. The first sublayer is the
    causal self-attention mechanism, with the integration of layer normalization and
    residual connection. The second sublayer within the decoder block is the feed-forward
    network, which incorporates the GELU activation function, alongside layer normalization
    and residual connection.
  prefs: []
  type: TYPE_NORMAL
- en: We stack 48 decoder layers to form the main body of the GPT-2XL model, as shown
    in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.6 Building the GPT-2XL model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: ① Calculates input embedding as the sum of word embedding and positional encoding
  prefs: []
  type: TYPE_NORMAL
- en: ② Passes the input embedding through 48 decoder blocks
  prefs: []
  type: TYPE_NORMAL
- en: ③ Applies layer normalization one more time
  prefs: []
  type: TYPE_NORMAL
- en: ④ Attaches a linear head to the output so the number of outputs equals the number
    of unique tokens
  prefs: []
  type: TYPE_NORMAL
- en: We construct the model in the `GPT2XL()` class as we explained in the first
    section of this chapter. The input to the model consists of sequences of indexes
    corresponding to tokens in the vocabulary. We first pass the input through word
    embedding and positional encoding; we then add the two to form the input embedding.
    The input embedding goes through 48 decoder blocks. After that, we apply layer
    normalization to the output and then attach a linear head to it so that the number
    of outputs is 50,257, the size of the vocabulary. The outputs are the logits corresponding
    to the 50,257 tokens in the vocabulary. Later, we’ll apply the softmax activation
    on the logits to obtain the probability distribution over the unique tokens in
    the vocabulary when generating text.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Since the model size is too large, we didn’t move the model to a GPU. This
    leads to a lower speed in text generation later in the chapter. However, if you
    have access to a CUDA-enabled GPU with large memory (say, above 32GB), you can
    move the model to a GPU for faster text generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll create the GPT-2XL model by instantiating the `GPT2XL()` class
    we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We also count the number of parameters in the main body of the model. The output
    is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output shows that GPT-2XL has more than 1.5 billion parameters.
    Note that the number doesn’t include the parameters in the linear head at the
    end of the model. Depending on what the downstream task is, we can attach different
    heads to the model. Since our focus is on text generation, we have attached a
    linear head to ensure the number of outputs is equal to the number of unique tokens
    in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE In LLMs like GPT-2, ChatGPT, or BERT, an output head refers to the final
    layer of the model that is responsible for producing the actual output based on
    the processed input. This output can vary depending on the downstream task the
    model is performing. In text generation, the output head is often a linear layer
    that transforms the final hidden states into logits for each token in the vocabulary.
    These logits are then passed through a softmax function to generate a probability
    distribution over the vocabulary, which is used to predict the next token in a
    sequence. For classification tasks, the output head typically consists of a linear
    layer followed by a softmax function. The linear layer transforms the final hidden
    states of the model into logits for each class, and the softmax function converts
    these logits into probabilities for each class. The specific architecture of the
    output head can vary depending on the model and the task, but its primary function
    is to map the processed input to the desired output format (e.g., class probabilities,
    token probabilities, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you can print out the GPT-2XL model structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: It shows the detailed blocks and layers in the GPT-2XL model.
  prefs: []
  type: TYPE_NORMAL
- en: And just like that, you have created the GPT-2XL model from scratch!
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Loading up pretrained weights and generating text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though you have just created the GPT-2XL model, it is not trained. Therefore,
    you cannot use it to generate any meaningful text.
  prefs: []
  type: TYPE_NORMAL
- en: Given the sheer number of the model’s parameters, it’s impossible to train the
    model without supercomputing facilities, let alone the amount of data needed to
    train the model. Luckily, the pretrained weights of GPT-2 models, including the
    largest one, GPT-2XL, were released by OpenAI to the public on November 5, 2019
    (see the statement on the OpenAI website, [https://openai.com/research/gpt-2-1-5b-release](https://openai.com/research/gpt-2-1-5b-release),
    as well as a report by an American technology news website, The Verge, [https://mng.bz/NBm7](https://mng.bz/NBm7)).
    We, therefore, will load up the pretrained weights to generate text in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.1 Loading up pretrained parameters in GPT-2XL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll use the `transformers` library developed by the Hugging Face team to extract
    pretrained weights in GPT-2XL.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, run the following line of code in a new cell in this Jupyter Notebook
    to install the `transformers` library on your computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we import the GPT2 model from the `transformers` library and extract
    the pretrained weights in GPT-2XL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: ① Loads the pretrained GPT-2XL model
  prefs: []
  type: TYPE_NORMAL
- en: ② Extracts model weights
  prefs: []
  type: TYPE_NORMAL
- en: ③ Prints out the model structure of the original OpenAI GTP-2XL model
  prefs: []
  type: TYPE_NORMAL
- en: The output from the preceding code block is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: ① OpenAI used a Conv1d layer instead of a linear layer as we did
  prefs: []
  type: TYPE_NORMAL
- en: If you compare this model structure with the one from the previous section,
    you’ll notice that they are the same except that the linear layers are replaced
    with Conv1d layers. As we explained in chapters 9 and 10, in feed-forward networks,
    we treat values in an input as independent elements rather than a sequence. Therefore,
    we often call it a 1D convolutional network. OpenAI checkpoints use a Conv1d module
    in places of the model where we use a linear layer. As a result, we need to transpose
    certain weight matrices when we extract model weights from Hugging Face and place
    them in our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how this works, let’s look at the weights in the first layer
    of the feed-forward network in the first decoder block of the OpenAI GPT-2XL model.
    We can print out its shape as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The weight matrix in the Conv1d layer is a tensor with size (1,600, 6,400).
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we look at the same weight matrix in the model we just constructed,
    its shape is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The output this time is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The weight matrix in the linear layer in our model is a tensor with size (6,400,
    1,600), which is a transposed matrix of the weight matrix in OpenAI GPT-2XL. Therefore,
    we need to transpose the weight matrix in all Conv1d layers in the OpenAI GPT-2XL
    model before we place them in our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we name the parameters in the original OpenAI GPT-2XL model as `keys`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have excluded parameters ending with `attn.masked_bias` in the
    preceding line of code. OpenAI GPT-2 uses them to implement future token masking.
    Since we have created our own masking in the `CausalSelfAttention()` class and
    registered it as a buffer in PyTorch, we don’t need to load parameters ending
    with `attn.masked_bias` from OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: 'We name the parameters in the GPT-2XL model we created from scratch as `sd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll extract the pretrained weights in OpenAI GPT-2XL and place them
    in our own model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: ① Finds out layers that OpenAI uses a Conv1d module instead of a linear module
  prefs: []
  type: TYPE_NORMAL
- en: ② For those layers, we transpose the weight matrix before placing weights in
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Otherwise, simply copies the weights from OpenAI and places them in our model
  prefs: []
  type: TYPE_NORMAL
- en: We extract the OpenAI pretrained weights from Hugging Face and place them in
    our own model. In the process, we make sure that we transpose the weight matrix
    whenever OpenAI checkpoints use a Conv1d module instead of a plain linear module.
  prefs: []
  type: TYPE_NORMAL
- en: Now our model is equipped with pre-trained weights from OpenAI. We can use the
    model to generate coherent text.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.2 Defining a generate() function to produce text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Armed with pretrained weights from the OpenAI GPT-2XL model, we’ll use the GPT2
    model we created from scratch to generate text.
  prefs: []
  type: TYPE_NORMAL
- en: When generating text, we’ll feed a sequence of indexes that correspond to tokens
    in a prompt to the model. The model predicts the index corresponding to the next
    token and attaches the prediction to the end of the sequence to form a new sequence.
    It then uses the new sequence to make predictions again. It keeps doing this until
    the model has generated a fixed number of new tokens or the conversation is over
    (signified by the special token `<|endoftext|>`).
  prefs: []
  type: TYPE_NORMAL
- en: The special token <|endoftext|> in GPTs
  prefs: []
  type: TYPE_NORMAL
- en: GPT models undergo training using text from a diverse range of sources. A unique
    token, `<|endoftext|>`, is employed during this phase to delineate text from different
    origins. In the text generation phase, it’s crucial to halt the conversation upon
    encountering this special token. Failing to do so may trigger the initiation of
    an unrelated new topic, resulting in subsequent generated text that bears no relevance
    to the ongoing discussion.
  prefs: []
  type: TYPE_NORMAL
- en: To that end, we define a `sample()` function to add a certain number of new
    indexes to the current sequence. It takes a sequence of indexes as input to feed
    to the GPT-2XL model. It predicts one index at a time and adds the new index to
    the end of the running sequence. It stops until the specified number of time steps,
    `max_new_tokens`, is reached or when the predicted next token is `<|endoftext|>`,
    which signals the end of the conversation. If we don’t stop, the model will randomly
    start an unrelated topic. The `sample()` function is defined as shown in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.7 Iteratively predicting the next index
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: ① Generates a fixed number of new indexes
  prefs: []
  type: TYPE_NORMAL
- en: ② Predicts the next index using GPT-2XL
  prefs: []
  type: TYPE_NORMAL
- en: ③ If using top-K sampling, sets the logits below the top K choices to –∞
  prefs: []
  type: TYPE_NORMAL
- en: ④ Stops predicting if the next token is <|endoftext|>
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Attaches the new prediction to the end of the sequence
  prefs: []
  type: TYPE_NORMAL
- en: The `sample()` function uses GPT-2XL to add new indexes to a running sequence.
    It incorporates two arguments, `temperature` and `top_k`, to modulate the generated
    output’s novelty, operating in the same manner as described in chapter 8\. The
    function returns a new sequence of indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a `generate()` function to generate text based on a prompt.
    It first converts the text in the prompt to a sequence of indexes. It then feeds
    the sequence to the `sample()` function we just defined to generate a new sequence
    of indexes. Finally, the function `generate()` converts the new sequence of indexes
    back to text.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.8 A function to generate text with GPT-2XL
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: ① If the prompt is empty, uses <|endoftext|> as the prompt
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts prompt into a sequence of indexes
  prefs: []
  type: TYPE_NORMAL
- en: ③ Uses the sample() function to generate new indexes
  prefs: []
  type: TYPE_NORMAL
- en: ④ Converts the new sequence of indexes back to text
  prefs: []
  type: TYPE_NORMAL
- en: 'The `generate()` function bears resemblance to the version we introduced in
    chapter 8 but with a notable distinction: it employs GPT-2XL for prediction purposes,
    moving away from the LSTM model previously utilized. The function accepts a prompt
    as its initial input, transforming this prompt into a series of indexes that are
    then fed into the model to forecast the subsequent index. Upon producing a predetermined
    number of new indexes, the function reverts the entire index sequence back into
    textual form.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.3 Text generation with GPT-2XL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have defined the `generate()` function, we can use it to generate
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, the `generate()` function allows for unconditional text generation,
    which means the prompt is empty. The model will generate text randomly. This can
    be beneficial in creative writing: the generated text can be used as inspiration
    or a starting point for one’s own creative work. Let’s try that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the preceding output is coherent and grammatically correct but
    may not be factually accurate. I did a quick Google search, and the text doesn’t
    seem to be copied from any online source.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.4
  prefs: []
  type: TYPE_NORMAL
- en: Generate text unconditionally by setting the prompt as an empty string, temperature
    to 0.9, maximum number of new tokens to 100, and `top_k` to 40\. Set the random
    seed number to 42 in PyTorch. See what the output is.
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate whether GPT-2XL can produce coherent text based on preceding tokens,
    we will use the prompt “I went to the kitchen and” and generate 10 additional
    tokens after the prompt. We will repeat this process five times to determine if
    the generated text aligns with typical kitchen activities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: These results indicate that the generated text includes activities such as conversing
    with someone, noticing something, and taking beverages, all of which are typical
    kitchen activities. This demonstrates that GPT-2XL can generate text relevant
    to the given context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use “Lexington is the second largest city in the state of Kentucky”
    as the prompt and ask the `generate()` function to add up to 100 new tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, this text is coherent. Even though the generated content may not be
    factually accurate. The GPT-2XL model is, fundamentally, trained to predict the
    next token based on preceding tokens in the sentence. The preceding output shows
    that the model has achieved that goal: the generated text is grammatically correct
    and seemingly logical. It shows the ability to remember the text in the early
    parts of the sequence and generate subsequent words that are relevant to the context.
    For example, while the first sentence discusses the city of Lexington, about 90
    tokens later, the model mentions the music acts from the Lexington area.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, as noted in the introduction, GPT-2 has its limitations. It should
    not be held to the same standard as ChatGPT or GPT-4, given that its size is less
    than 1% of ChatGPT and less than 0.1% of GPT-4\. GPT-3 has 175 billion parameters
    and produces more coherent text than GPT-2, but the pretrained weights are not
    released to the public.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll explore how `temperature` and `top-K` sampling affect the generated
    text from GPT-2XL. We’ll set the `temperature` to 0.9 and `top_k` to 50 and keep
    other arguments the same. Let’s see what the generated text looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The generated text seems more coherent than before. However, the content is
    not factually accurate. It made up many facts about the city of Lexington, Kentucky,
    such as “The population of Lexington was 1,731,947 in the 2011 Census.”
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.5
  prefs: []
  type: TYPE_NORMAL
- en: Generate text by setting the `temperature` to 1.2 and `top_k` to None and using
    “Lexington is the second largest city in the state of Kentucky” as the starting
    prompt. Set the random seed number to 42 in PyTorch and the maximum number of
    new tokens to 100.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you have learned how to build GPT-2, the predecessor of ChatGPT
    and GPT-4, from scratch. After that, you extracted the pretrained weights from
    the GPT-2XL model released by OpenAI and loaded them into your model. You witnessed
    the coherent text generated by the model.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the large size of the GPT-2XL model (1.5 billion parameters), it’s impossible
    to train the model without supercomputing facilities. In the next chapter, you’ll
    create a smaller version of a GPT model, with a similar structure as GPT-2 but
    only about 5.12 million parameters. You’ll train the model with the text from
    Ernest Hemingway’s novels. The trained model will generate coherent text with
    a style matching that of Hemingway!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-2 is an advanced LLM developed by OpenAI and announced in February 2019\.
    It represents a significant milestone in the field of NLP and has paved the way
    for the development of even more sophisticated models, including its successors,
    ChatGPT and GPT-4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-2 is a decoder-only Transformer, meaning there is no encoder stack in the
    model. Like other Transformer models, GPT-2 uses self-attention mechanisms to
    process input data in parallel, significantly improving the efficiency and effectiveness
    of training LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-2 adopts a different approach to positional encoding than the one used in
    the seminal 2017 paper “Attention Is All You Need.” Instead, GPT-2’s technique
    for positional encoding parallels that of word embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GELU activation function is used in the feed-forward sublayers of GPT-2\.
    GELU provides a blend of linear and nonlinear activation properties that have
    been found to enhance model performance in deep learning tasks, particularly in
    NLPs and in training LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can build a GPT-2 model from scratch and load up the pretrained weights released
    by OpenAI. The GPT-2 model you created can generate coherent text just as the
    original OpenAI GPT-2 model does.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
