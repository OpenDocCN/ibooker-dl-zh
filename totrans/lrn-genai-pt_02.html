<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">2 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/>Deep learning with PyTorch</h1>
<p class="co-summary-head">This chapter covers<a id="marker-21"/><a id="idIndexMarker000"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">PyTorch tensors and basic operations</li>
<li class="co-summary-bullet">Preparing data for deep learning in PyTorch</li>
<li class="co-summary-bullet">Building and training deep neural networks with PyTorch</li>
<li class="co-summary-bullet">Conducting binary and<a class="calibre" id="idTextAnchor003"/> multicategory classifications with deep learning</li>
<li class="co-summary-bullet">Creating a validation set to decide training stop points</li>
</ul>
<p class="body">In this book, we’ll use deep neural networks to generate a wide range of content, including text, images, shapes, music, and more. I assume you already have a foundational understanding of machine learning (ML) and, in particular, artificial neural networks. In this chapter, I’ll refresh your memory on essential concepts such as loss functions, activation functions, optimizers, and learning rates, which are crucial for developing and training deep neural networks. If you find any gaps in your understanding of these topics, I strongly encourage you to address them before proceeding with the projects in this book. Appendix B provides a summary of the basic skills and concepts needed, including the architecture and training of artificial neural networks. <a id="idIndexMarker001"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> There are plenty of great ML books out there for you to choose from. Examples include <i class="fm-italics">Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow</i> (2019, O’Reilly) and <i class="fm-italics">Machine Learning, Animated</i> (2023, CRC Press). Both books use TensorFlow to create neural networks. If you prefer a book that uses PyTorch, I recommend <i class="fm-italics">Deep Learning with PyTorch</i> (2020, Manning Publications).</p>
<p class="body">Generative AI models are frequently confronted with the task of either binary or multicategory classification. For instance, in generative adversarial networks (GANs), the discriminator undertakes the essential role of a binary classifier, its purpose being to distinguish between the fake samples created by the generator from real samples from the training set. Similarly, in the context of text generation models, whether in recurrent neural networks or Transformers, the overarching objective is to predict the subsequent character or word from an extensive array of possibilities (essentially a multicategory classification task).<a id="idIndexMarker002"/><a id="marker-22"/></p>
<p class="body">In this chapter, you’ll learn how to use PyTorch to create deep neural networks to perform binary and multicategory classifications so that you become well-versed in deep learning and classification tasks.</p>
<p class="body">Specifically, you’ll engage in an end-to-end deep learning project in PyTorch, on a quest to classify grayscale images of clothing items into different categories such as coats, bags, sneakers, shirts, and so on. The intention is to prepare you for the creation of deep neural networks, capable of performing both binary and multicategory classification tasks in PyTorch. This, in turn, will get you ready for the upcoming chapters, where you use deep neural networks in PyTorch to create various generative models.</p>
<p class="body">To train generative AI models, we harness a diverse range of data formats such as raw text, audio files, image pixels, and arrays of numbers. Deep neural networks created in PyTorch cannot take these forms of data directly as inputs. Instead, we must first convert them into a format that the neural networks understand and accept. Specifically, you’ll convert various forms of raw data into PyTorch tensors (fundamental data structures used to represent and manipulate data) before feeding them to generative AI models. Therefore, in this chapter, you’ll also learn the basics of data types, how to create various forms of PyTorch tensors, and how to use them in deep learning.</p>
<p class="body">Knowing how to perform classification tasks has many practical applications in our society. Classifications are widely used in healthcare for diagnostic purposes, such as identifying whether a patient has a particular disease (e.g., positive or negative for a specific cancer based on medical imaging or test results). They play a vital role in many business tasks (stock recommendations, credit card fraud detection, and so on). Classification tasks are also integral to many systems and services that we use daily such as spam detection and facial recognition.</p>
<h2 class="fm-head" id="heading_id_3">2.1 Data types in PyTorch</h2>
<p class="body">We’ll use datasets from a wide range of sources and formats in this book, and the first step in deep learning is to transform the inputs into arrays of numbers. <a id="idIndexMarker003"/><a id="idIndexMarker004"/></p>
<p class="body">In this section, you’ll learn how PyTorch converts different formats of data into algebraic structures known as <i class="fm-italics">tensors</i>. Tensors can be represented as multidimensional arrays of numbers, similar to NumPy arrays but with several key differences, chief among them the ability of GPU accelerated training. There are different types of tensors depending on their end use, and you’ll learn how to create different types of tensors and when to use each type. We’ll discuss the data structure in PyTorch in this section by using the heights of the 46 U.S. presidents as our running example.<a id="idIndexMarker005"/></p>
<p class="body">Refer to the instructions in appendix A to create a virtual environment and install PyTorch and Jupyter Notebook on your computer. Open the Jupyter Notebook app within the virtual environment and run the following line of code in a new cell:</p>
<pre class="programlisting">!pip install matplotlib</pre>
<p class="body">This command will install the Matplotlib library on your computer, enabling you to plot images in Python.</p>
<h3 class="fm-head1" id="heading_id_4">2.1.1 Creating PyTorch tensors</h3>
<p class="body"><a id="marker-23"/>When training deep neural networks, we feed the models with arrays of numbers as inputs. Depending on what a generative model is trying to create, these numbers have different types. For example, when generating images, the inputs are raw pixels in the form of integers between 0 and 255, but we’ll convert them to floating-point numbers be<a id="idTextAnchor004"/>tween –1 and 1; when generating text, there is a “vocabulary” akin to a dictionary, and the input is a sequence of integers telling you which entry in the dictionary the word corresponds to. <a id="idIndexMarker006"/><a id="idIndexMarker007"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The code for this chapter, as well as other chapters in this book, is available at the book’s GitHub repository: <a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>.</p>
<p class="body">Imagine you want to use PyTorch to calculate the average height of the 46 U.S. presidents. We can first collect the heights of the 46 U.S. presidents in centimeters and store them in a Python list:</p>
<pre class="programlisting">heights = [189, 170, 189, 163, 183, 171, 185,
           168, 173, 183, 173, 173, 175, 178,
           183, 193, 178, 173, 174, 183, 183,
           180, 168, 180, 170, 178, 182, 180,
           183, 178, 182, 188, 175, 179, 183,
           193, 182, 183, 177, 185, 188, 188,
           182, 185, 191, 183]</pre>
<p class="body">The numbers are in chronological order: the first value in the list, 189, indicates that the first U.S. president, George Washington, was 189 centimeters tall. The last value shows that Joe Biden’s height is 183 centimeters. We can convert a Python list into a PyTorch tensor by using the <code class="fm-code-in-text">tensor()</code> method in PyTorch:<a id="idIndexMarker008"/></p>
<pre class="programlisting">import torch
heights_tensor = torch.tensor(heights,      <span class="fm-combinumeral">①</span>
           dtype=torch.float64)             <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Converts a Python list to a PyTorch tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Specifies the data type in the PyTorch tensor</p>
<p class="body">We specify the data type using the <code class="fm-code-in-text">dtype</code> argument in the <code class="fm-code-in-text">tensor()</code> method. The default data type in PyTorch tensors is <code class="fm-code-in-text">float32</code>, a 32-bit floating-point number. In the preceding code cell, we converted the data type to <code class="fm-code-in-text">float64</code>, double-precision floating-point numbers. <code class="fm-code-in-text">float64</code> provides more precise results than <code class="fm-code-in-text">float32</code>, but it takes longer to compute. There is a tradeoff between precision and computational costs. Which data type to use depends on the task at hand. <a id="idIndexMarker009"/><a id="idIndexMarker010"/><a id="idIndexMarker011"/><a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="idIndexMarker014"/></p>
<p class="body"><a id="marker-24"/>Table 2.1 lists different data types and the corresponding PyTorch tensor types. These include integers and floating-point numbers with different precisions. Integers can also be either signed or unsigned.</p>
<p class="fm-table-caption">Table 2.1 Data and tensor types in PyTorch</p>
<table border="1" class="contenttable-1-table" id="table001" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="28%"/>
<col class="contenttable-0-col" span="1" width="40%"/>
<col class="contenttable-0-col" span="1" width="32%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head">PyTorch tensor type</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">dtype argument in <code class="fm-code-in-text">tensor()</code></p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Data type</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">FloatTensor</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">torch.float32 or torch.float</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">32-bit floating point</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">HalfTensor</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">torch.float16 or torch.half</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">16-bit floating point</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">DoubleTensor</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">torch.float64 or torch.double</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">64-bit floating point</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">CharTensor</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">torch.int8</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">8-bit integer (signed)</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">ByteTensor</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">torch.uint8</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">8-bit integer (unsigned)</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">ShortTensor</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">torch.int16 or torch.short</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">16-bit integer (signed)</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">IntTensor</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">torch.int32 or torch.int</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">32-bit integer (signed)</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">LongTensor</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><code class="fm-code-in-text1">torch.int64 or torch.long</code></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">64-bit integer (signed)</p>
</td>
</tr>
</tbody>
</table>
<p class="body">You can create a tensor with a certain data type in one of the two ways. The first way is to use the PyTorch class as specified in the first column of table 2.1. The second way is to use the <code class="fm-code-in-text">torch.tensor()</code> method and specify the data type using the <code class="fm-code-in-text">dtype</code> argument (the value of the argument is listed in the second column of table 2.1). For example, to convert the Python list <code class="fm-code-in-text">[1, 2, 3]</code> into a PyTorch tensor with 32-bit integers in it, you can use two methods in the following listing.<a id="idIndexMarker015"/><a id="idIndexMarker016"/></p>
<p class="fm-code-listing-caption">Listing 2.1 Two ways of specifying tensor types</p>
<pre class="programlisting">t1=torch.IntTensor([1, 2, 3])    <span class="fm-combinumeral">①</span>
t2=torch.tensor([1, 2, 3],
             dtype=torch.int)    <span class="fm-combinumeral">②</span>
print(t1)
print(t2)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Uses torch.IntTensor() to specify the tensor type</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Uses dtype=torch.int to specify the tensor type</p>
<p class="body">This leads to the following output:</p>
<pre class="programlisting">tensor([1, 2, 3], dtype=torch.int32)
tensor([1, 2, 3], dtype=torch.int32)</pre>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 2.1</p>
<p class="fm-sidebar-text">Use two different methods to convert the Python list <code class="fm-code-in-text1">[5, 8, 10]</code> into a PyTorch tensor with 64-bit floating-point numbers in it. Consult the third row in table 2.1 for this question.</p>
</div>
<p class="body">Many times, you need to create a PyTorch tensor with values 0 everywhere. For example, in GANs, we create a tensor of zeros as the labels for fake samples, as you’ll see in chapter 3. The <code class="fm-code-in-text">zeros()</code> method in PyTorch generates a tensor of zeros with a certain shape. In PyTorch, a tensor is an n-dimensional array, and its shape is a tuple representing the size along each of its dimensions. The following lines of code generate a tensor of zeros with two rows and three columns:<a id="idIndexMarker017"/></p>
<pre class="programlisting">tensor1 = torch.zeros(2, 3)
print(tensor1)</pre>
<p class="body">The output is as follows:</p>
<pre class="programlisting">tensor([[0., 0., 0.],
        [0., 0., 0.]])</pre>
<p class="body"><a id="marker-25"/>The tensor has a shape of (2, 3), which means the tensor is a 2D array; there are two elements in the first dimension and three elements in the second dimension. Here, we didn’t specify the data type, and the output has the default data type of <code class="fm-code-in-text">float32</code><i class="fm-italics">.</i> <a id="idIndexMarker018"/></p>
<p class="body">From time to time, you need to create a PyTorch tensor with values 1 everywhere. For example, in GANs, we create a tensor of ones as the labels for real samples. Here we use the <code class="fm-code-in-text">ones()</code> method to create a 3D tensor with values 1 everywhere:<a id="idIndexMarker019"/></p>
<pre class="programlisting">tensor2 = torch.ones(1,4,5)
print(tensor2)</pre>
<p class="body">The output is</p>
<pre class="programlisting">tensor([[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]])</pre>
<p class="body">We have generated a 3D PyTorch tensor. The shape of the tensor is (1, 4, 5).</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exerci<a class="calibre" id="idTextAnchor006"/>se 2.2</p>
<p class="fm-sidebar-text">Create a 3D PyTorch tensor with values 0 in it. Make the shape of the tensor (2, 3, 4).</p>
</div>
<p class="body">You can also use a NumPy array instead of a Python list in the tensor constructor:</p>
<pre class="programlisting">import numpy as np

nparr=np.array(range(10))
pt_tensor=torch.tensor(nparr, dtype=torch.int)
print(pt_tensor)</pre>
<p class="body">The output is<a id="idIndexMarker020"/><a id="idIndexMarker021"/></p>
<pre class="programlisting">tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)</pre>
<h3 class="fm-head1" id="heading_id_5">2.1.2 Index and slice PyTorch tensors</h3>
<p class="body">We use square brackets (<code class="fm-code-in-text">[ ]</code>) to index and slice PyTorch tensors, as we do with Python lists. Indexing and slicing allow us to operate on one or more elements in a tensor, instead of on all elements. To continue our example of the heights of the 46 U.S. presidents, if we want to assess the height of the third president, Thomas Jefferson, we can do the following:<a id="marker-26"/><a id="idIndexMarker022"/><a id="idIndexMarker023"/></p>
<pre class="programlisting">height = heights_tensor[2]
print(height)</pre>
<p class="body">This leads to an output of</p>
<pre class="programlisting">tensor(189., dtype=torch.float64)</pre>
<p class="body">The output shows that the height of Thomas Jefferson was 189 centimeters.</p>
<p class="body">We can use negative indexing to count from the back of the tensor. For example, to find the height of Donald Trump, who is the second to last president in the list, we use index –2:</p>
<pre class="programlisting">height = heights_tensor[-2]
print(height)</pre>
<p class="body">The output is</p>
<pre class="programlisting">tensor(191., dtype=torch.float64)</pre>
<p class="body">The output shows that Trump’s height is 191 centimeters.</p>
<p class="body">What if we want to know the heights of five recent presidents in the tensor <code class="fm-code-in-text">heights_tensor</code>? We can obtain a slice of the tensor:</p>
<pre class="programlisting">five_heights = heights_tensor[-5:]
print(five_heights)</pre>
<p class="body">The colon (<code class="fm-code-in-text">:</code>) is used to separate the starting and end index. If no starting index is provided, the default is 0; if no end index is provided, you include the very last element in the tensor (as we did in the preceding code cell). Negative indexing means you count from the back. The output is</p>
<pre class="programlisting">tensor([188., 182., 185., 191., 183.], dtype=torch.float64)</pre>
<p class="body">The results show that the five recent presidents in the tensor (Clinton, Bush, Obama, Trump, and Biden) are 188, 182, 185, 191, and 183 centimeters tall, respectively.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 2.3</p>
<p class="fm-sidebar-text">Use slicing to obtain the heights of the first five U.S. presidents in the tensor <code class="fm-code-in-text1">heights_tensor</code>.</p>
</div>
<h3 class="fm-head1" id="heading_id_6">2.1.3 PyTorch tensor shapes</h3>
<p class="body">PyTorch tensors have an attribute <i class="fm-italics">shape</i>, which tells us the dimensions of a tensor. It’s important to know the shapes of PyTorch tensors because mismatched shapes will lead to errors when we operate on them. For example, if we want to find out the shape of the tensor <code class="fm-code-in-text">heights_tensor</code>, we can do this:<a id="idIndexMarker024"/><a id="idIndexMarker025"/><a id="marker-27"/></p>
<pre class="programlisting">print(heights_tensor.shape)</pre>
<p class="body">The output is</p>
<pre class="programlisting">torch.Size([46])</pre>
<p class="body">This tells us that <code class="fm-code-in-text">heights_tensor</code> is a 1D tensor with 46 values in it.</p>
<p class="body">You can also change the shape of a PyTorch tensor. To learn how, let’s first convert the heights from centimeters to feet. Since a foot is about 30.48 centimeters, we can accomplish this by dividing the tensor by 30.48:</p>
<pre class="programlisting">heights_in_feet = heights_tensor / 30.48
print(heights_in_feet)</pre>
<p class="body">This leads to the following output (I omitted some values to save space; the complete output is in the book’s GitHub repository):</p>
<pre class="programlisting">tensor([6.2008, 5.5774, 6.2008, 5.3478, 6.0039, 5.6102, 6.0696, …
        6.0039], dtype=torch.float64)</pre>
<p class="body">The new tensor, <code class="fm-code-in-text">heights_in_feet</code>, stores the heights in feet. For example, the last value in the tensor shows that Joe Biden is 6.0039 feet tall.</p>
<p class="body">We can use the <code class="fm-code-in-text">cat()</code> method in PyTorch to concatenate the two tensors:<a id="idIndexMarker026"/></p>
<pre class="programlisting">heights_2_measures = torch.cat(
    [heights_tensor,heights_in_feet], dim=0)
print(heights_2_measures.shape)</pre>
<p class="body">The <code class="fm-code-in-text">dim</code> argument is used in various tensor operations to specify the dimension along which the operation is to be performed. In the preceding code cell, <code class="fm-code-in-text">dim=0</code> means we concatenate the two tensors along the first dimension. This leads to the following output:<a id="idIndexMarker027"/></p>
<pre class="programlisting">torch.Size([92])</pre>
<p class="body">The resulting tensor is 1D with 92 values, with some values in centimeters and others in feet. Therefore, we need to reshape it into two rows and 46 columns so that the first row represents heights in centimeters and the second in feet:</p>
<pre class="programlisting">heights_reshaped = heights_2_measures.reshape(2, 46)</pre>
<p class="body">The new tensor, <code class="fm-code-in-text">heights_reshaped</code>, is 2D with a shape of (2, 46). We can index and slice multidimensional tensors using square brackets as well. For example, to print out the height of Trump in feet, we can do this:</p>
<pre class="programlisting">print(heights_reshaped[1,-2])</pre>
<p class="body">This leads to a result of</p>
<pre class="programlisting">tensor(6.2664, dtype=torch.float64)</pre>
<p class="body">The command <code class="fm-code-in-text">heights_reshaped[1,-2]</code> tells Python to look for the value in the second row and the second to last column, which returns Trump’s height in feet, 6.2664.</p>
<p class="fm-callout"><span class="fm-callout-head">tip</span> The number of indexes needed to refer to scalar values within the tensor is the same as the dimensionality of the tensor. That’s why we used only one index to locate values in the 1D tensor <code class="fm-code-in-text2">heights_tensor</code> but we used two indexes to locate values in the 2D tensor <code class="fm-code-in-text2">heights_reshaped</code>. <a id="idIndexMarker028"/><a id="idIndexMarker029"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 2.4</p>
<p class="fm-sidebar-text">Use indexing to obtain the height of Joe Biden in the tensor <code class="fm-code-in-text1">heights_reshaped</code> in centimeters.</p>
</div>
<h3 class="fm-head1" id="heading_id_7">2.1.4 Mathematical operations on PyTorch tensors</h3>
<p class="body"><a id="marker-28"/>We can conduct mathematical operations on PyTorch tensors by using different methods such as <code class="fm-code-in-text">mean()</code>, <code class="fm-code-in-text">median()</code>, <code class="fm-code-in-text">sum()</code>, <code class="fm-code-in-text">max()</code>, and so on. For example, to find the median height of the 46 presidents in centimeters, we can do this:<a id="idIndexMarker030"/><a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/></p>
<pre class="programlisting">print(torch.median(heights_reshaped[0,:]))</pre>
<p class="body">The code snippet <code class="fm-code-in-text">heights_reshaped[0,:]</code> returns the first row and all columns in the tensor <code class="fm-code-in-text">heights_reshaped</code>. The preceding line of code returns the median value in the first row, and this leads to an output of</p>
<pre class="programlisting">tensor(182., dtype=torch.float64)</pre>
<p class="body">This means the median height of U.S. presidents is 182 centimeters.</p>
<p class="body">To find the average height in both rows, we can use the <code class="fm-code-in-text">dim=1</code> argument in the <code class="fm-code-in-text">mean()</code> method:<a id="idIndexMarker036"/><a id="idIndexMarker037"/></p>
<pre class="programlisting">print(torch.mean(heights_reshaped,dim=1))</pre>
<p class="body">The <code class="fm-code-in-text">dim=1</code> argument indicates that the averages are calculated by collapsing columns (the dimension indexed 1), effectively obtaining averages along the dimension indexed 0 (rows). The output is<a id="idIndexMarker038"/></p>
<pre class="programlisting">tensor([180.0652,   5.9077], dtype=torch.float64)</pre>
<p class="body">The results show that the average values in the two rows are 180.0652 centimeters and 5.9077 feet.</p>
<p class="body">To find out the tallest president, we can do this:</p>
<pre class="programlisting">values, indices = torch.max(heights_reshaped, dim=1)
print(values)
print(indices)</pre>
<p class="body">The output is</p>
<pre class="programlisting">tensor([193.0000,   6.3320], dtype=torch.float64)
tensor([15, 15])</pre>
<p class="body">The <code class="fm-code-in-text">torch.max()</code> method returns two tensors: a tensor <code class="fm-code-in-text">values</code> with the tallest president’s height (in centimeters and in feet), and a tensor <code class="fm-code-in-text">indices</code> with the indexes of the president with the maximum height. The results show that the 16th president (Lincoln) is the tallest, at 193 centimeters, or 6.332 feet.<a id="idIndexMarker039"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 2.5</p>
<p class="fm-sidebar-text">Use the <code class="fm-code-in-text1">torch.min()</code> method to find out the index and height of the shortest U.S. president.<a id="idIndexMarker040"/><a id="idIndexMarker041"/><a id="idIndexMarker042"/></p>
</div>
<h2 class="fm-head" id="heading_id_8">2.2 An end-to-end deep learning project with PyTorch</h2>
<p class="body"><a id="marker-29"/>In the next few sections, you’ll work through an example deep learning project with PyTorch, learning to classify grayscale images of clothing items into 1 of the 10 types. In this section, we’ll first provide a high-level overview of the steps involved. We then discuss how to obtain training data for this project and how to preprocess the data.<a id="idIndexMarker043"/><a id="idIndexMarker044"/></p>
<h3 class="fm-head1" id="heading_id_9">2.2.1 Deep learning in PyTorch: A high-level overview</h3>
<p class="body">Our job in this project is to create and train a deep neural network in PyTorch to classify grayscale images of clothing items. Figure 2.1 provides a diagram of the steps involved. <a id="idIndexMarker045"/><a id="idIndexMarker046"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="363" src="../../OEBPS/Images/CH02_F01_Liu.png" width="840"/></p>
<p class="figurecaption">Figure 2.1 The steps involved in training a deep learning model</p>
</div>
<p class="body">First, we’ll obtain a dataset of grayscale clothing images, as shown on the left of figure 2.1. The images are in raw pixels, and we’ll convert them to PyTorch tensors in the form of float numbers (step 1). Each image comes with a label.</p>
<p class="body">We’ll then create a deep neural network in PyTorch, as shown in the center of figure 2.1. Some neural<a id="idTextAnchor007"/> networks in this book involve convolutional neural networks (CNNs). For this simple classification problem, we’ll use dense layers only for the moment.</p>
<p class="body">We’ll select a loss function for multicategory classification, and cross-entropy loss is commonly used for this task. Cross-entropy loss measures the difference between the predicted probability distribution and the true distribution of the labels. We’ll use the Adam optimizer (a variant of the gradient descent algorithm) to update the network’s weights during training. We set the learning rate to 0.001. The learning rate controls how much the model’s weights are adjusted with respect to the loss gradient during training.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Optimizers in ML</p>
<p class="fm-sidebar-text">Optimizers in ML are algorithms that update model parameters based on gradient information <a id="idTextAnchor008"/>to minimize the loss function. Stochastic Gradient Descent (SGD) is the most fundamental optimizer, utilizing straightforward updates based on the loss gradient. Adam, the most popular optimizer, is known for its efficiency and out-of-the-box performance, a<a id="idTextAnchor009"/>s it combines the strengths of the Adaptive Gr<a id="idTextAnchor010"/>adient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). Despite their differences, all optimizers aim to iteratively adjust parameters to minimize the loss function, each creating a unique optimization path to reach this goal.<a id="idIndexMarker047"/><a id="idIndexMarker048"/><a id="idIndexMarker049"/><a id="idIndexMarker050"/><a id="marker-30"/></p>
</div>
<p class="body">We’ll divide the training data into a train set and a validation set. In ML, we usually use the validation set to provide an unbiased evaluation of the model and to select the best hyperparameters such as the learning rate, number of epochs of training, and so on. The validation set can also be used to avoid overfitting the model in which the model works well in the training set but poorly on unseen data. An epoch is when all the training data is used to train the model once and only once.</p>
<p class="body">During training, you’ll iterate through the training data. During forward passes, you feed images through the network to obtain predictions (step 2) and compute the loss by comparing the predicted labels with the actual labels (step 3; see the right side of figure 2.1). You’ll then backpropagate the gradient through the network to update the weights. This is where the learning happens (step 4), as shown at the bottom of figure 2.1.</p>
<p class="body">You’ll use the validation set to determine when we should stop training. We calculate the loss in the validation set. If the model stops improving after a fixed number of epochs, we consider the model trained. We then evaluate the trained model on the test set to assess its performance in classifying images into different labels.</p>
<p class="body">Now that you have a high-level overview of how deep learning in PyTorch works, let’s dive into the end-to-end project!</p>
<h3 class="fm-head1" id="heading_id_10">2.2.2 Preprocessing data</h3>
<p class="body">We’ll be using the Fashion Modified National Institute of Standards and Technology (MNIST) dataset in this project. Along the way, you’ll learn how to use the <code class="fm-code-in-text">datasets</code> and <code class="fm-code-in-text">transforms</code> packages in the Torchvision library, as well as the <code class="fm-code-in-text">Dataloader</code> packages in PyTorch that will help you for the rest of the book. You’ll use these tools to preprocess data throughout the book. The Torchvision library provides tools for image processing, including popular datasets, model architectures, and common image transformations for deep learning applications. <a id="idIndexMarker051"/><a id="idIndexMarker052"/><a id="idIndexMarker053"/><a id="idIndexMarker054"/><a id="idIndexMarker055"/><a id="marker-31"/></p>
<p class="body">We first import needed libraries and instantiate a <code class="fm-code-in-text">Compose()</code> class in the <code class="fm-code-in-text">transforms</code> package to transform raw images to PyTorch tensors.<a id="idIndexMarker056"/></p>
<p class="fm-code-listing-caption">Listing 2.2 Transforming raw image data to PyTorch tensors</p>
<pre class="programlisting">import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as T
  
torch.manual_seed(42)
transform=T.Compose([             <span class="fm-combinumeral">①</span>
    T.ToTensor(),                 <span class="fm-combinumeral">②</span>
    T.Normalize([0.5],[0.5])])    <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Composes several transforms together</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Converts image pixels to PyTorch tensors</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Normalizes the values to the range [–1, 1]</p>
<p class="body">We use the <code class="fm-code-in-text">manual_seed()</code> method in PyTorch to fix the random state so that results are reproducible. The <i class="fm-italics">transforms</i> package in Torchvision can help create a series of transformations to preprocess images. The <code class="fm-code-in-text">ToTensor()</code> class converts image data (in either Python Imaging Library (PIL) image formats or NumPy arrays) into PyTorch tensors. In particular, the image data are integers ranging from 0 to 255, and the <code class="fm-code-in-text">ToTensor()</code> class converts them to float tensors with values in the range of 0.0 and 1.0.<a id="idIndexMarker057"/><a id="idIndexMarker058"/><a id="idIndexMarker059"/><a id="idIndexMarker060"/></p>
<p class="body">The <code class="fm-code-in-text">Normalize()</code> class normalizes tensor images with mean and standard deviation for <i class="fm-italics">n</i> channels. The Fashion MNIST data are grayscale images of clothing items so there is only one color channel. Later in this book, we’ll deal with images of three different color channels (red, green, and blue). In the preceding code cell, <code class="fm-code-in-text">Normalize([0.5],[0.5])</code> means that we subtract 0.5 from the data and divide the difference by 0.5. The resulting image data range from –1 to 1. Normalizing the input data to the range [–1, 1] allows gradient descent to operate more efficiently by maintaining more uniform step sizes across dimensions. This helps in faster convergence during training, and you’ll do this often in this book. <a id="idIndexMarker061"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The code in listing 2.2 only defines the data transformation process. It doesn’t perform the actual transformation, which happens in the next code cell.</p>
<p class="body">Next, we use the <i class="fm-italics">datasets</i> package in Torchvision to download the dataset to a folder on your computer and perform the transformation:</p>
<pre class="programlisting">train_set=torchvision.datasets.FashionMNIST(    <span class="fm-combinumeral">①</span>
    root=".",                                   <span class="fm-combinumeral">②</span>
    train=True,                                 <span class="fm-combinumeral">③</span>
    download=True,                              <span class="fm-combinumeral">④</span>
    transform=transform)                        <span class="fm-combinumeral">⑤</span>
test_set=torchvision.datasets.FashionMNIST(root=".",
    train=False,download=True,transform=transform)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Which dataset to download</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Where to savethe data</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The training or test dataset</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Whether or not to download the data to your computer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Performs data transformation</p>
<p class="body">You can print out the first sample in the training set:</p>
<pre class="programlisting">print(train_set[0])</pre>
<p class="body"><a id="marker-32"/>The first sample consists of a tensor with 784 values and a label 9. The 784 numbers represent a 28 by 28 grayscale image (28 <span class="cambria">×</span> 28 = 784), and the label 9 means it’s an ankle boot. You may be wondering: How do you know the label 9 indicates an ankle boot? There are 10 different types of clothing items. The labels in the dataset are numbered from 0 to 9. You can search online and find the text labels for the 10 categories (for example, I got the text labels here <a class="url" href="https://github.com/pranay414/Fashion-MNIST-Pytorch">https://github.com/pranay414/Fashion-MNIST-Pytorch</a>). The list <code class="fm-code-in-text">text_labels</code> contains the 10 text labels corresponding to the numerical labels 0 to 9. For example, if an item has a numerical label of 0 in the dataset, the corresponding text label is “t-shirt.” The list <code class="fm-code-in-text">text_labels</code> is defined as follows:</p>
<pre class="programlisting">text_labels=['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
             'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']</pre>
<p class="body">We can plot the data to visualize the clothing items in the dataset.</p>
<p class="fm-code-listing-caption">Listing 2.3 Visualizing the clothing items</p>
<pre class="programlisting">!pip install matplotlib
import matplotlib.pyplot as plt
  
plt.figure(dpi=300,figsize=(8,4))
for i in range(24):
    ax=plt.subplot(3, 8, i + 1)                 <span class="fm-combinumeral">①</span>
    img=train_set[i][0]                         <span class="fm-combinumeral">②</span>
    img=img/2+0.5                               <span class="fm-combinumeral">③</span>
    img=img.reshape(28, 28)                     <span class="fm-combinumeral">④</span>
    plt.imshow(img,
               cmap="binary")
    plt.axis('off')
    plt.title(text_labels[train_set[i][1]],     <span class="fm-combinumeral">⑤</span>
        fontsize=8)
plt.show()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Where to place the image</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Obtains the i-th image from the training data</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Converts the values from [–1,1] to [0,1]</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Reshapes the image to 28 by 28</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Adds text label to each image</p>
<p class="body">The plot in figure 2.2 shows 24 clothing items such as coats, pullovers, sandals, and so on. <a id="idIndexMarker062"/><a id="idIndexMarker063"/><a id="idIndexMarker064"/><a id="idIndexMarker065"/><a id="idIndexMarker066"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="416" src="../../OEBPS/Images/CH02_F02_Liu.png" width="842"/></p>
<p class="figurecaption">Figure 2.2 Grayscale images of clothing items in the Fashion MNIST dataset.</p>
</div>
<p class="body">You’ll learn how to create deep neural networks with PyTorch to perform binary and multicategory classification problems in the next two sections.</p>
<h2 class="fm-head" id="heading_id_11">2.3 Binary classification</h2>
<p class="body">In this section, we’ll first create batches of data for training. We then build a deep neural network in PyTorch for this purpose and train the model using the data. Finally, we’ll use the trained model to make predictions and test how accurate the predictions are. The steps involved with binary and multicategory classifications are similar, with a few notable exceptions that I’ll highlight later. <a id="idIndexMarker067"/><a id="idIndexMarker068"/><a id="marker-33"/></p>
<h3 class="fm-head1" id="heading_id_12">2.3.1 Creating batches</h3>
<p class="body">We’ll create a training set and a test set that contain only two types of clothing items: t-shirts and ankle boots. (Later in this chapter when we discuss multicategory classification, you’ll also learn to create a validation set to determine when to stop training.) The following code cell accomplishes that goal:<a id="idIndexMarker069"/><a id="idIndexMarker070"/></p>
<pre class="programlisting">binary_train_set=[x for x in train_set if x[1] in [0,9]]
binary_test_set=[x for x in test_set if x[1] in [0,9]]</pre>
<p class="body">We only keep samples with numerical labels 0 and 9 to create a binary classification problem with a balanced training set. Next, we create batches for training the deep neural network.</p>
<p class="fm-code-listing-caption">Listing 2.4 Creating batches for training and testing</p>
<pre class="programlisting">batch_size=64
binary_train_loader=torch.utils.data.DataLoader(
    binary_train_set,                                <span class="fm-combinumeral">①</span>
    batch_size=batch_size,                           <span class="fm-combinumeral">②</span>
    shuffle=True)                                    <span class="fm-combinumeral">③</span>
binary_test_loader=torch.utils.data.DataLoader(
    binary_test_set,                                 <span class="fm-combinumeral">④</span>
    batch_size=batch_size,shuffle=True)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates batches for the binary training set</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Number of samples in each batch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Shuffles the observations when batching</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Creates batches for the binary test set</p>
<p class="body">The <code class="fm-code-in-text">DataLoader</code> class in the PyTorch <i class="fm-italics">utils</i> package helps create data iterators in batches. We set the batch size to 64. We created two data loaders in listing 2.4: a training set and a test set for binary classification. We shuffle the observations when creating batches to avoid correlations among the original dataset: the training is more stable if different labels are evenly distributed in the data loader. <a id="idIndexMarker071"/></p>
<h3 class="fm-head1" id="heading_id_13">2.3.2 Building and training a binary classification model</h3>
<p class="body">We’ll first create a binary classification model. We then train the model by using the images of t-shirts and ankle boots. Once it’s trained, we’ll see if the model can tell t-shirts from ankle boots. We use PyTorch to create the following neural network by using the Pytorch <code class="fm-code-in-text">nn.Sequential</code> class (in later chapters, you’ll also learn to use the <code class="fm-code-in-text">nn.Module</code> class to create PyTorch neural networks).<a id="idIndexMarker072"/><a id="idIndexMarker073"/><a id="idIndexMarker074"/><a id="idIndexMarker075"/><a id="marker-34"/></p>
<p class="fm-code-listing-caption">Listing 2.5 Creating a binary classification model</p>
<pre class="programlisting">import torch.nn as nn
  
device="cuda" if torch.cuda.is_available() else "cpu"  <span class="fm-combinumeral">①</span>
  
binary_model=nn.Sequential(                            <span class="fm-combinumeral">②</span>
    nn.Linear(28*28,256),                              <span class="fm-combinumeral">③</span>
    nn.ReLU(),                                         <span class="fm-combinumeral">④</span>
    nn.Linear(256,128),
    nn.ReLU(),
    nn.Linear(128,32),
    nn.ReLU(),
    nn.Linear(32,1),
    nn.Dropout(p=0.25),
    nn.Sigmoid()).to(device)                           <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> PyTorch automatically detects if a CUDA-enabled GPU is available.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a sequential neural network in PyTorch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Numbers of input and output neurons in a linear layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Applies ReLU activation to outputs of the layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Applies sigmoid activation and moves the model to a GPU if available</p>
<p class="body">The <code class="fm-code-in-text">Linear()</code> class in PyTorch creates a linear transformation of the incoming data. This effectively creates a dense layer in the neural network. The input shape is 784 because we’ll later flatten the 2D image to a 1D vector with 28 <span class="cambria">×</span> 28 = 784 values in it. We flatten the 2D image into a 1D tensor because dense layers only take 1D inputs. In later chapters, you’ll see that you don’t need to flatten images when you use convolutional layers. There are three hidden layers in the network, with 256, 128, and 32 neurons in them, respectively. The numbers 256, 128, and 32 are chosen somewhat arbitrarily: changing them to, say, 300, 200, and 50 won’t affect the training process. We apply the rectified linear unit (ReLU) activation function on the three hidden layers. The ReLU activation function decides whether a neuron should be turned on based on the weighted sum. These functions introduce nonlinearity to the output of a neuron so that the network can learn nonlinear relations between inputs and outputs. ReLU is your go-to activation function with very few exceptions, and you’ll encounter a few other activation functions in later chapters. <a id="idIndexMarker076"/><a id="idIndexMarker077"/></p>
<p class="body">The output of the last layer of the model contains a single value, and we use the sigmoid activation function to squeeze the number to the range [0, 1] so that it can be interpreted as the probability that the object is an ankle boot. With the complementary probability, the object is a t-shirt.</p>
<p class="body">Here we set the learning rate and define the optimizer and the loss function:</p>
<pre class="programlisting">lr=0.001
optimizer=torch.optim.Adam(binary_model.parameters(),lr=lr)
loss_fn=nn.BCELoss()</pre>
<p class="body">We set the learning rate to 0.001. What learning rate to set is an empirical question, and the answer comes with experience. It can also be determined by using hyperparameter tuning using a validation set. Most optimizers in PyTorch use a default learning rate of 0.001. The Adam optimizer is a variant of the gradient descent algorithm, which is used to determine how much to adjust the model parameters in each training step. The Adam optimizer was first introduced in 2014 by Diederik Kingma and Jimmy Ba.<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">1</a></sup> In the traditional gradient descent algorithm, only gradients in the current iteration are considered. The Adam optimizer, in contrast, takes into consideration gradients in previous iterations as well. <a id="idIndexMarker078"/><a id="marker-35"/></p>
<p class="body">We use <code class="fm-code-in-text">nn.BCELoss()</code>, which is the binary cross-entropy loss function. Loss functions measure how well an ML model performs. The training of a model involves adjusting parameters to minimize the loss function. The binary cross-entropy loss function is widely used in ML, particularly in binary classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1. The cross-entropy loss increases as the predicted probability diverges from the actual label. <a id="idIndexMarker079"/></p>
<p class="body">We train the neural network we just created as shown in the following listing.</p>
<p class="fm-code-listing-caption">Listing 2.6 Training a binary classification model</p>
<pre class="programlisting">for i in range(50):                                    <span class="fm-combinumeral">①</span>
    tloss=0
    for imgs,labels in binary_train_loader:            <span class="fm-combinumeral">②</span>
        imgs=imgs.reshape(-1,28*28)                    <span class="fm-combinumeral">③</span>
        imgs=imgs.to(device)
        labels=torch.FloatTensor(\
          [x if x==0 else 1 for x in labels])          <span class="fm-combinumeral">④</span>
        labels=labels.reshape(-1,1).to(device)
        preds=binary_model(imgs)    
        loss=loss_fn(preds,labels)                     <span class="fm-combinumeral">⑤</span>
        optimizer.zero_grad()
        loss.backward()                                <span class="fm-combinumeral">⑥</span>
        optimizer.step()
        tloss+=loss.detach()
    tloss=tloss/n
    print(f"at epoch {i}, loss is {tloss}")</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Trains for 50 epochs</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Iterates through all batches</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Flattens the image before moving the tensor to GPU</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Converts labels to 0 and 1</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Calculates the loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Backpropagation</p>
<p class="body">In training deep learning models in PyTorch, <code class="fm-code-in-text">loss.backward()</code> computes the gradient of the loss with respect to each model parameter, enabling backpropagation, while <code class="fm-code-in-text">optimizer.step()</code> updates the model parameters based on these computed gradients to minimize the loss. We train the model for 50 epochs for simplicity (an epoch is when the training data is used to train the model once). In the next section, you’ll use a validation set and an early stopping class to determine how many epochs to train. In binary classifications, we label the targets as 0s and 1s. Since we have kept only t-shirts and ankle boots with labels 0 and 9, respectively, we converted them to 0 and 1 in listing 2.6. As a result, the labels for the two categories of clothing items are 0 and 1, respectively.</p>
<p class="body">This training takes a few minutes if you use GPU training. It takes longer if you use CPU training, but the training time should be less than an hour. <a id="idIndexMarker080"/><a id="idIndexMarker081"/><a id="marker-36"/></p>
<h3 class="fm-head1" id="heading_id_14">2.3.3 Testing the binary classification model</h3>
<p class="body">The prediction from the trained binary classification model is a number between 0 and 1. We’ll use the <code class="fm-code-in-text">torch.where()</code> method to convert the predictions into 0s and 1s: if the predicted probability is less than 0.5, we label the prediction as 0; otherwise, we label the prediction as 1. We then compare these predictions with the actual labels to calculate the accuracy of the predictions. In the following listing, we use the trained model to make predictions on the test dataset.<a id="idIndexMarker082"/><a id="idIndexMarker083"/><a id="idIndexMarker084"/></p>
<p class="fm-code-listing-caption">Listing 2.7 Calculating the accuracy of the predictions</p>
<pre class="programlisting">import numpy as np
results=[]
for imgs,labels in binary_test_loader:                     <span class="fm-combinumeral">①</span>
    imgs=imgs.reshape(-1,28*28).to(device)
    labels=(labels/9).reshape(-1,1).to(device)
    preds=binary_model(imgs)
    pred10=torch.where(preds&gt;0.5,1,0)                      <span class="fm-combinumeral">②</span>
    correct=(pred10==labels)                               <span class="fm-combinumeral">③</span>
    results.append(correct.detach().cpu()\
      .numpy().mean())                                     <span class="fm-combinumeral">④</span>
accuracy=np.array(results).mean()                          <span class="fm-combinumeral">⑤</span>
print(f"the accuracy of the predictions is {accuracy}")</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Iterates through all batches in the test set</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Makes predictions using the trained model</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Compares predictions with labels</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Calculates accuracy in the batch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Calculates accuracy in the test set</p>
<p class="body">We iterate through all batches of data in the test set. The trained model produces a probability that the image is an ankle boot. We then convert the probability into 0 or 1 based on the cutoff value of 0.5, by using the <code class="fm-code-in-text">torch.where()</code> method. The predictions are either 0 (i.e., a t-shirt) or 1 (an ankle boot) after the conversion. We compare the predictions with the actual labels and see how many times the model gets it right. Results show that the accuracy of the predictions is 87.84% in the test set. <a id="idIndexMarker085"/><a id="idIndexMarker086"/></p>
<h2 class="fm-head" id="heading_id_15">2.4 Multicategory classification</h2>
<p class="body"><a id="marker-37"/>In this section, we’ll build a deep neural network in PyTorch to classify the clothing items into one of the 10 categories. We’ll then train the model with the Fashion MNIST dataset. Finally, we’ll use the trained model to make predictions and see how accurate they are. We first create a validation set and define an early stopping class so that we can determine when to stop training.<a id="idIndexMarker087"/><a id="idIndexMarker088"/><a id="idIndexMarker089"/></p>
<h3 class="fm-head1" id="heading_id_16">2.4.1 Validation set and early stopping</h3>
<p class="body">When we build and train a deep neural network, there are many hyperparameters that we can choose (such as the learning rate and the number of epochs to train). These hyperparameters affect the performance of the model. To find the best hyperparameters, we can create a validation set to test the performance of the model with different hyperparameters. <a id="idIndexMarker090"/><a id="idIndexMarker091"/><a id="idIndexMarker092"/><a id="idIndexMarker093"/><a id="idIndexMarker094"/><a id="idIndexMarker095"/><a id="idIndexMarker096"/><a id="idIndexMarker097"/></p>
<p class="body">To give you an example, we’ll create a validation set in the multicategory classification to determine the optimal number of epochs to train. The reason we do this in the validation set instead of the training set is to avoid overfitting, when a model performs well in the training set but poorly in out-of-the-sample tests (i.e., on unseen data).</p>
<p class="body">Here we divide 60,000 observations of the training dataset into a train set and a validation set:</p>
<pre class="programlisting">train_set,val_set=torch.utils.data.random_split(\
    train_set,[50000,10000])</pre>
<p class="body">The original train set now becomes two sets: the new train set with 50,000 observations and a validation set with the remaining 10,000 observations.</p>
<p class="body">We use the <code class="fm-code-in-text">DataLoader</code> class in the PyTorch <i class="fm-italics">utils</i> package to convert the train, validation, and test sets into three data iterators in batches:<a id="idIndexMarker098"/></p>
<pre class="programlisting">train_loader=torch.utils.data.DataLoader(
    train_set,    
    batch_size=batch_size,   
    shuffle=True)   
val_loader=torch.utils.data.DataLoader(
    val_set,    
    batch_size=batch_size,   
    shuffle=True)
test_loader=torch.utils.data.DataLoader(
    test_set,    
    batch_size=batch_size,   
    shuffle=True)</pre>
<p class="body">Next, we define an <code class="fm-code-in-text">EarlyStop()</code> class and create an instance of the class.<a id="idIndexMarker099"/></p>
<p class="fm-code-listing-caption">Listing 2.8 The <code class="fm-code-in-text">EarlyStop()</code> class to determine when to stop training</p>
<pre class="programlisting">class EarlyStop:
    def __init__(self, patience=10):         <span class="fm-combinumeral">①</span>
        self.patience = patience
        self.steps = 0
        self.min_loss = float('inf')
    def stop(self, val_loss):                <span class="fm-combinumeral">②</span>
        if val_loss &lt; self.min_loss:         <span class="fm-combinumeral">③</span>
            self.min_loss = val_loss
            self.steps = 0
        elif val_loss &gt;= self.min_loss:      <span class="fm-combinumeral">④</span>
            self.steps += 1
        if self.steps &gt;= self.patience:
            return True
        else:
            return False
stopper=EarlyStop()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Sets the default value of patience to 10</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines the stop() method</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> If a new minimum loss is reached, updates the value of min_loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Counts how many epochs since the last minimum loss</p>
<p class="body">The <code class="fm-code-in-text">EarlyStop()</code> class determines if the loss in the validation set has stopped improving in the last <code class="fm-code-in-text">patience=10</code> epochs. We set the default value of <code class="fm-code-in-text">patience</code> argument to 10, but you can choose a different value when you instantiate the class. The value of <code class="fm-code-in-text">patience</code> measures how many epochs you want to train since the last time the model reached the minimum loss. The <code class="fm-code-in-text">stop()</code> method keeps a record of the minimum loss and the number of epochs since the minimum loss and compares the number to the value of <code class="fm-code-in-text">patience</code>. The method returns a value of <code class="fm-code-in-text">True</code> if the number of epochs since the minimum loss is greater than the value of <code class="fm-code-in-text">patience</code>.<a id="idIndexMarker100"/><a id="idIndexMarker101"/><a id="idIndexMarker102"/><a id="marker-38"/></p>
<h3 class="fm-head1" id="heading_id_17">2.4.2 Building and training a multicategory classification model</h3>
<p class="body">The Fashion MNIST dataset contains 10 different categories of clothing items. Therefore, we create a multicategory classification model to classify them. Next, you’ll learn how to create such a model and train it. You’ll also learn how to make predictions using the trained model and assess the accuracy of the predictions. We use PyTorch to create the neural network for multicategory classification in the following listing.<a id="idIndexMarker103"/><a id="idIndexMarker104"/><a id="idIndexMarker105"/></p>
<p class="fm-code-listing-caption">Listing 2.9 Creating a multicategory classification model</p>
<pre class="programlisting">model=nn.Sequential(
    nn.Linear(28*28,256),
    nn.ReLU(),
    nn.Linear(256,128),
    nn.ReLU(),
    nn.Linear(128,64),
    nn.ReLU(),
    nn.Linear(64,10)                         <span class="fm-combinumeral">①</span>
    ).to(device)                             <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> There are 10 neurons in the output layer.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Does not apply softmax activation on the output</p>
<p class="body">Compared to the binary classification model we created in the last section, we have made a few changes here. First, the output now has 10 values in it, representing the 10 different types of clothing items in the dataset. Second, we have changed the number of neurons in the last hidden layer from 32 to 64. A rule of thumb in creating deep neural networks is to gradually increase or decrease the number of neurons from one layer to the next. Since the number of output neurons has increased from 1 (in binary classification) to 10 (in multicategory classification), we change the number of neurons from 32 to 64 in the second to last layer to match the increase. However, there is nothing special about the number 64: if you use, say, 100 neurons in the second to last layer, you’ll get similar results.</p>
<p class="body">We’ll use the PyTorch <code class="fm-code-in-text">nn.CrossEntropyLoss()</code> class as our loss function, which combines <code class="fm-code-in-text">nn.LogSoftmax()</code> and <code class="fm-code-in-text">nn.NLLLoss()</code> in one single class. See the documentation here for details: <a class="url" href="https://mng.bz/pxd2">https://mng.bz/pxd2</a>. In particular, the documentation states, “This criterion computes the cross entropy loss between input logits and target.” This explains why we didn’t apply the softmax activation in the proceeding listing. In the book’s GitHub repository, I have demonstrated that if we use <code class="fm-code-in-text">nn.LogSoftmax()</code> in the model and use <code class="fm-code-in-text">nn.NLLLoss()</code> as the loss function, we obtain identical results.<a id="idIndexMarker106"/><a id="idIndexMarker107"/><a id="idIndexMarker108"/><a id="marker-39"/></p>
<p class="body">As a result, the <code class="fm-code-in-text">nn.CrossEntropyLoss()</code> class will apply the softmax activation function on the output to squeeze the 10 numbers into the range [0, 1] before the logarithm operation. The preferred activation function on the output is sigmoid in binary classifications and softmax in multicategory classifications. Further, the 10 numbers after softmax activation add up to 1, which can be interpreted as the probabilities corresponding to the 10 types of clothing items. We’ll use the same learning rate and optimizer as those in the binary classification in the last section.<a id="idIndexMarker109"/></p>
<pre class="programlisting">lr=0.001
optimizer=torch.optim.Adam(model.parameters(),lr=lr)
loss_fn=nn.CrossEntropyLoss()</pre>
<p class="body">We define the <code class="fm-code-in-text">train_epoch()</code> as follows:<a id="idIndexMarker110"/></p>
<pre class="programlisting">def train_epoch():
    tloss=0
    for n,(imgs,labels) in enumerate(train_loader):    
        imgs=imgs.reshape(-1,28*28).to(device)
        labels=labels.reshape(-1,).to(device)
        preds=model(imgs)    
        loss=loss_fn(preds,labels)
        optimizer.zero_grad()
        loss.backward()    
        optimizer.step()
        tloss+=loss.detach()
    return tloss/n</pre>
<p class="body">The function trains the model for one epoch. The code is similar to what we have seen in the binary classification, except that the labels are from 0 to 9, instead of two numbers (0 and 1).</p>
<p class="body">We also define a <code class="fm-code-in-text">val_epoch()</code> function:<a id="idIndexMarker111"/></p>
<pre class="programlisting">def val_epoch():
    vloss=0
    for n,(imgs,labels) in enumerate(val_loader):    
        imgs=imgs.reshape(-1,28*28).to(device)
        labels=labels.reshape(-1,).to(device)
        preds=model(imgs)    
        loss=loss_fn(preds,labels)    
        vloss+=loss.detach()
    return vloss/n</pre>
<p class="body">The function uses the model to make predictions on images in the validation set and calculate the average loss per batch of data.</p>
<p class="body">We now train the multicategory classifier:</p>
<pre class="programlisting">for i in range(1,101):    
    tloss=train_epoch()
    vloss=val_epoch()
    print(f"at epoch {i}, tloss is {tloss}, vloss is {vloss}")
    if stopper.stop(vloss)==True:             
        break  </pre>
<p class="body">We train a maximum of 100 epochs. In each epoch, we first train the model using the training set. We then calculate the average loss per batch in the validation set. We use the <code class="fm-code-in-text">EarlyStop()</code> class to determine if the training should stop by looking at the loss in the validation set. The training stops if the loss hasn’t improved in the last 10 epochs. After 19 epochs, the training stops. <a id="idIndexMarker112"/></p>
<p class="body">The training takes about 5 minutes if you use GPU training, which is longer than the training process in binary classification since we have more observations in the training set now (10 clothing items instead of just 2).</p>
<p class="body"><a id="marker-40"/>The output from the model is a vector of 10 numbers. We use <code class="fm-code-in-text">torch.argmax()</code> to assign each observation a label based on the highest probability. We then compare the predicted label with the actual label. To illustrate how the prediction works, let’s look at the predictions on the first five images in the test set. <a id="idIndexMarker113"/></p>
<p class="fm-code-listing-caption">Listing 2.10 Testing the trained model on five images</p>
<pre class="programlisting">plt.figure(dpi=300,figsize=(5,1))
for i in range(5):                                          <span class="fm-combinumeral">①</span>
    ax=plt.subplot(1,5, i + 1)
    img=test_set[i][0]    
    label=test_set[i][1]
    img=img/2+0.5    
    img=img.reshape(28, 28)    
    plt.imshow(img, cmap="binary")
    plt.axis('off')
    plt.title(text_labels[label]+f"; {label}", fontsize=8)
plt.show()
for i in range(5):
    img,label = test_set[i]                                 <span class="fm-combinumeral">②</span>
    img=img.reshape(-1,28*28).to(device)
    pred=model(img)                                         <span class="fm-combinumeral">③</span>
    index_pred=torch.argmax(pred,dim=1)                     <span class="fm-combinumeral">④</span>
    idx=index_pred.item()
    print(f"the label is {label}; the prediction is {idx}") <span class="fm-combinumeral">⑤</span> </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Plots the first five images in the test set with their labels</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Obtains the i-th image and label in the test set</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Predicts using the trained model</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Uses the torch.argmax() method to obtain the predicted label</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints out the actual label and the predicted label</p>
<p class="body">We plot the first five clothing items in the test set in a 1 <span class="cambria">×</span> 5 grid. We then use the trained model to make a prediction on each clothing item. The prediction is a tensor with 10 values. The <code class="fm-code-in-text">torch.argmax()</code> method returns the position of the highest probability in the tensor, and we use it as the predicted label. Finally, we print out both the actual label and the predicted label to compare and see if the predictions are correct. After running the previous code listing, you should see the image in figure 2.3.<a id="idIndexMarker114"/><a id="marker-41"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="144" src="../../OEBPS/Images/CH02_F03_Liu.png" width="570"/></p>
<p class="figurecaption">Figure 2.3 The first five clothing items in the test dataset and their respective labels. Each clothing item has a text label and a numerical label between 0 and 9.</p>
</div>
<p class="body">Figure 2.3 shows that the first five clothing items in the test set are ankle boot, pullover, trouser, trouser, and shirt, respectively, with numerical labels 9, 2, 1, 1, and 6.</p>
<p class="body">The output after running the code in listing 2.10 is as follows:</p>
<pre class="programlisting">the label is 9; the prediction is 9
the label is 2; the prediction is 2
the label is 1; the prediction is 1
the label is 1; the prediction is 1
the label is 6; the prediction is 6</pre>
<p class="body">The preceding output shows that the model has made correct predictions on all five clothing items.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Fixing the random state in PyTorch</p>
<p class="fm-sidebar-text">The <code class="fm-code-in-text1">torch.manual_seed()</code> method fixes the random state so the results are the same when you rerun your programs. However, you may get different results from those reported in this chapter even if you use the same random seed. This happens because different hardware and different versions of PyTorch handle floating point operations slightly differently. See, for example, the explanations at <a class="url" href="https://mng.bz/RNva">https://mng.bz/RNva</a>. The difference is generally minor, though, so no need to be alarmed. <a id="idIndexMarker115"/></p>
</div>
<p class="body">Next, we calculate the accuracy of the predictions on the whole test dataset.</p>
<p class="fm-code-listing-caption">Listing 2.11 Testing the trained multicategory classification model</p>
<pre class="programlisting">results=[]
  
for imgs,labels in test_loader:                             <span class="fm-combinumeral">①</span>
    imgs=imgs.reshape(-1,28*28).to(device)
    labels=(labels).reshape(-1,).to(device)
    preds=model(imgs)                                       <span class="fm-combinumeral">②</span>
    pred10=torch.argmax(preds,dim=1)                        <span class="fm-combinumeral">③</span>
    correct=(pred10==labels)                                <span class="fm-combinumeral">④</span>
    results.append(correct.detach().cpu().numpy().mean())
  
accuracy=np.array(results).mean()                           <span class="fm-combinumeral">⑤</span>
print(f"the accuracy of the predictions is {accuracy}") </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Iterates through all batches in the test set</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Predicts using the trained model</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Converts probabilities to a predicted label</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Compares the predicted label with the actual label</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Calculates accuracy in the test set</p>
<p class="body">The output is</p>
<pre class="programlisting">the accuracy of the predictions is 0.8819665605095541</pre>
<p class="body">We iterate through all clothing items in the test set and use the trained model to make predictions. We then compare the predictions with the actual labels. The accuracy is about 88% in the out-of-sample test. Given that a random guess has an accuracy of about 10%, 88% accuracy is fairly high. This indicates that we have built and trained two successful deep learning models in PyTorch! You’ll use these skills quite often later in this book. For example, in chapter 3, the discriminator network you’ll construct is essentially a binary classification model, similar to what you have created in this chapter. <a id="idIndexMarker116"/><a id="idIndexMarker117"/><a id="idIndexMarker118"/><a id="idIndexMarker119"/><a id="idIndexMarker120"/><a id="idIndexMarker121"/><a id="marker-42"/></p>
<h2 class="fm-head" id="heading_id_18">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">In PyTorch, we use tensors to hold various forms of input data so we can feed them to deep learning models.</p>
</li>
<li class="fm-list-bullet">
<p class="list">You can index and slice PyTorch tensors, reshape them, and conduct mathematical operations on them.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Deep learning is a type of ML method that uses deep artificial neural networks to learn the relation between input and output data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The ReLU activation function decides whether a neuron should be turned on based on the weighted sum. It introduces nonlinearity to the output of a neuron.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Loss functions measure how well an ML model performs. The training of a model involves adjusting parameters to minimize the loss function.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Binary classification is an ML model to classify observations into one of two categories.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Multicategory classification is an ML model to classify observations into one of multiple categories.</p>
</li>
</ul>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">1</a></sup>  Diederik Kingma and Jimmy Ba, 2014, “Adam: A Method for Stochastic Optimization.” <a class="url" href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>.</p>
</div></body></html>