- en: 'Chapter 2\. AI Engineering: A Proactive Compliance Catalyst'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI engineering can support companies in complying with the EU AI Act by promoting
    appropriate tools and practices. A key part of this is machine learning operations
    (MLOps), which focuses on the operationalization of ML models. Together, these
    disciplines provide the technical components and repeatable processes necessary
    for designing, developing, deploying, and maintaining AI systems in a reliable
    and compliant way.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing engineering practices such as automation, versioning, testing,
    reproducibility, deployment, and monitoring helps ensure AI systems meet the EU
    AI Act’s requirements for high-quality, safe, and trustworthy AI. This chapter
    introduces some practical frameworks that you can use to design and architect
    compliant AI systems (although these are by no means the only options). Tools
    like the Machine Learning Canvas and the MLOps Stack Canvas are designed to address
    key requirements such as risk management, technical documentation, transparency,
    robustness, and post-market monitoring. We’ll also discuss CRISP-ML(Q), a machine
    learning process model with built-in quality assurance that emphasizes risk assessment,
    quality assurance, comprehensive documentation, and continuous monitoring throughout
    the AI lifecycle. The synergy between CRISP-ML(Q) and AI engineering enables organizations
    to proactively engineer compliance into the AI lifecycle, ensuring that AI systems
    are developed and deployed in an EU AI Act–compliant manner.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring the AI System Development Process with CRISP-ML(Q)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll explore developing *AI-powered applications*, which are
    software applications that embed machine learning models as a technological solution
    for one or several features. We’ll focus on CRISP-ML(Q), the Cross-Industry Standard
    Process for the development of Machine Learning applications with Quality assurance
    methodology. CRISP-ML(Q) offers a systematic and quality-focused approach for
    developing AI systems, facilitating compliance with the EU AI Act across various
    risk levels.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Appendix A](app01.html#appendix_a_designing_ai_powered_applications_1748539915159206)
    introduces the principles of designing AI-powered applications with an emphasis
    on aligning AI initiatives with business goals and user needs. As explained in
    the appendix, the “backward thinking” approach and the Machine Learning Canvas
    offer practical frameworks for planning and executing AI projects effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: The Machine Learning Canvas underlines the importance of a holistic perspective,
    which incorporates the value proposition, monitoring, and the integration of both
    prediction and training components within a larger software system. Furthermore,
    it highlights critical considerations around data management, ethical implications
    in feature engineering, and the collaborative nature of building successful AI
    products. See [Appendix A](app01.html#appendix_a_designing_ai_powered_applications_1748539915159206)
    for further details.
  prefs: []
  type: TYPE_NORMAL
- en: 'As described in the paper [“Towards CRISP-ML(Q): A Machine Learning Process
    Model with Quality Assurance Methodology”](https://oreil.ly/srSRa), CRISP-ML(Q)
    builds upon the established CRISP-DM framework (see [Figure 2-1](#chapter_2_figure_1_1748539917622735)).
    The motivation for extending CRISP-DM was twofold: it focused on data mining rather
    than ML models deployed over extended periods, and it lacked guidance on quality
    assurance. CRISP-ML(Q) better meets the needs and challenges of designing and
    operationalizing machine learning products, with a strong emphasis on quality
    assurance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. The phases of the CRISP-ML(Q) framework
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Risk Mitigation, Quality Assurance, and Alignment with the EU AI Act
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Working with CRISP-ML(Q) will help you incorporate the requirements of the EU
    AI Act for high-risk and limited-risk AI systems into your development processes.
    The framework emphasizes continuous risk assessment throughout the ML development
    process, from understanding the business needs to deployment and monitoring. This
    approach aligns with the EU AI Act’s requirements for ongoing quality assessment
    and risk management.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you learned in [Chapter 1](ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819),
    the EU AI Act defines four levels of risk for AI systems: unacceptable, high,
    limited, and minimal or no risk. CRISP-ML(Q) can accommodate these different classifications
    by tailoring its quality assurance methodology accordingly. For example, high-risk
    AI systems will require more extensive testing, documentation, and human oversight
    than limited- or minimal-risk systems.'
  prefs: []
  type: TYPE_NORMAL
- en: For each ML development phase, CRISP-ML(Q) defines specific requirements and
    constraints to address risks (such as bias, overfitting, or lack of reproducibility)
    that could impact the success of the ML system. Appropriate quality assurance
    methods, such as cross-validation and thorough documentation processes, are implemented
    to mitigate these risks. This supports compliance with the EU AI Act’s requirement
    that providers establish robust risk management systems to identify, assess, and
    mitigate risks.
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) also mandates that AI systems be thoroughly tested prior to deployment
    to ensure they perform consistently for their intended purpose and comply with
    requirements. Testing is carried out against defined metrics and thresholds. This
    meets the EU AI Act’s requirement for pre-market conformity assessments for high-risk
    AI systems. Additionally, CRISP-ML(Q) incorporates continuous post-deployment
    monitoring to detect performance degradation or deviations. Serious incidents
    must be reported. The EU AI Act has similar requirements for market surveillance,
    human oversight, and incident reporting by providers and users of high-risk AI.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, CRISP-ML(Q) requires that you document the entire ML development process,
    including the risk management measures. This is important because the EU AI Act
    introduces technical documentation and transparency obligations to ensure humans
    are appropriately informed when interacting with AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the framework’s quality assurance methodology and emphasis on continuous
    risk management throughout the ML lifecycle are well aligned with the risk-based
    approach mandated by the EU AI Act. Adopting CRISP-ML(Q) can help organizations
    proactively identify and mitigate the risks of AI systems to ensure safer and
    more trustworthy AI.
  prefs: []
  type: TYPE_NORMAL
- en: The Six Phases of CRISP-ML(Q)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As described in the previous section, a key principle of CRISP-ML(Q) is integrating
    quality assurance practices into each phase of the AI development lifecycle. This
    includes defining requirements, identifying risks, and applying risk mitigation
    methods based on established best practices. As a result, CRISP-ML(Q) provides
    a comprehensive methodology for developing a high-quality, sustainable AI system
    that meets the defined business objectives and is ready for real-world deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'CRISP-ML(Q) defines six key phases in the ML development process:'
  prefs: []
  type: TYPE_NORMAL
- en: Business and data understanding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monitoring and maintenance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each phase involves specific tasks and results in specific outputs. Importantly,
    while CRISP-ML(Q) is often depicted as a sequential process, it’s actually iterative
    and admits several backward trajectories—insights gained in later phases often
    require revisiting earlier steps.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s briefly examine each phase in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Business and data understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The initial phase focuses on defining the ML application’s business goals and
    success criteria. Key tasks include:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine business objectives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collect initial data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify data quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Close collaboration between business stakeholders and the data science team
    is critical to ensure business alignment. The main outputs include the project
    objectives, success criteria, and an initial assessment of data quality and project
    feasibility.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After business goals are defined and communicated across the organization,
    you are ready to prepare the data for modeling. Data preparation tasks include:'
  prefs: []
  type: TYPE_NORMAL
- en: Select data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Format data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This phase involves feature engineering, handling missing values, and normalizing
    data. Outputs include the final dataset(s) for modeling and documentation of how
    they were constructed.
  prefs: []
  type: TYPE_NORMAL
- en: Model engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The core machine learning work happens in this third phase, which is also called
    the *modeling* phase. Key tasks here include:'
  prefs: []
  type: TYPE_NORMAL
- en: Select modeling techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate test design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should build and compare multiple models using appropriate evaluation metrics
    and cross-validation techniques. Domain knowledge can be incorporated to improve
    results. This phase concludes by selecting the best-performing model(s).
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before deploying a model, you must thoroughly evaluate it from both a data
    science and a business perspective. Model evaluation tasks include:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate prediction results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Review process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decide on next steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is tested on independent data to validate its real-world performance.
    You should also examine the explainability and robustness of the model, and business
    stakeholders should verify that the model meets the defined success criteria.
    Outputs are the final model approval and deployment plan.
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deploying the model into the production environment involves tasks such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Plan deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plan monitoring and maintenance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Produce final report
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Review project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment can range from generating a simple report to persisting the model
    in the model repository or embedding it into a complex production system. Create
    monitoring and maintenance plans to ensure the model performs well over time.
  prefs: []
  type: TYPE_NORMAL
- en: Model monitoring and maintenance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ML models can degrade in performance if not proactively monitored and updated.
    This final phase involves these tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitor the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrain/update the model as needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data inputs, model predictions, and business outcomes are continuously tracked
    to identify issues. Models are retrained on new data to prevent drift. The ML
    application is maintained and enhanced based on changing business needs.
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) provides a robust framework for developing machine learning applications.
    By implementing a systematic process and integrating risk identification with
    quality assurance, organizations can enhance the success rate and business impact
    of their ML initiatives while guaranteeing compliance with the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MLOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we’ll turn our attention to *MLOps*, a set of processes and
    technical components designed to ensure AI systems are developed, deployed, and
    maintained in a continuous and reliable manner. By understanding and implementing
    MLOps, you establish the technical foundation necessary for complying with the
    EU AI Act, as many of its requirements depend on MLOps practices.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps practices are not tied to a specific language, framework, platform, or
    infrastructure. However, deploying ML in production involves various interconnected
    components. [Figure 2-2](#chapter_2_figure_2_1748539917622765) shows a canonical
    architecture for ML systems. In the next section, we’ll explore an application-
    and industry-neutral MLOps Stack Canvas framework that can be used to specify
    appropriate components.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. A canonical architecture for ML systems
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ML/AI technologies are currently widely utilized across various applications.
    Consequently, software systems often incorporate ML/AI components as standard
    practice. However, as organizations seek to enhance their products or processes
    with machine learning, many ML models remain in the experimental phase or fail
    to move beyond the proof of concept (PoC) stage. To increase your chances of success,
    when designing ML systems it’s essential to clearly define the interface, algorithms,
    data, infrastructure, and hardware to ensure they meet the specific requirements
    of the software system.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps has emerged as a discipline aimed at automating the development and deployment
    of machine learning models. Its primary value lies in optimizing business outcomes
    by improving the efficiency, reliability, and scalability of ML workflows through
    a supporting technology stack and infrastructure. MLOps also focuses on automating
    and streamlining the development lifecycle, facilitating deployment into production
    for newly developed ML models. To ensure model quality and adaptability, any changes
    in the code, training data, or model should automatically trigger the build process
    in the ML development pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-1](#chapter_2_table_1_1748539917627746) summarizes the core principles
    of MLOps: automation, version control, rigorous testing, reproducibility, streamlined
    deployment, and continuous monitoring. Later in the book, we will explore relevant
    requirements for the EU AI Act and how the MLOps engineering principles address
    these requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. MLOps core principles
  prefs: []
  type: TYPE_NORMAL
- en: '| MLOps principle | Key aspects | Why is it important? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *Automation* focuses on transitioning experimental ML models into fully operational
    production systems. The goal is to automate repetitive tasks in the areas of data
    preparation, model training, evaluation, deployment, and monitoring. | Consider
    automating:'
  prefs: []
  type: TYPE_NORMAL
- en: Data pipelines for data ingestion, cleaning, and feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training and hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing and validation of models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment of models to production environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and retraining of models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model auditing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Automation permits increased developer efficiency and productivity and reduces
    manual errors. It also enables faster iteration and experimentation, making it
    easier to scale machine learning workflows. Tools like DVC, Airflow, Kubeflow,
    and MLflow can be used to build automated ML pipelines. |'
  prefs: []
  type: TYPE_TB
- en: '| *Version control* is crucial for tracking and managing changes across the
    ML lifecycle. | What should be versioned:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data: Tracking different versions of datasets used for training and evaluation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code: Version control of ML code, scripts, and notebooks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Models: Versioning trained model artifacts and associated metadata'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configurations: Tracking hyperparameters, environment configurations, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Proper versioning is essential as it allows for reproducibility of experiments
    and results, enabling you to roll back to previous versions if any issues occur.
    It also facilitates collaboration between team members and provides the traceability
    needed for auditing and compliance with industry standards. Tools like Git, DVC,
    and MLflow can be used to version different ML assets. |'
  prefs: []
  type: TYPE_TB
- en: '| Thorough *testing* is essential for building robust and reliable ML systems.
    | Key types of tests include:'
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests to test individual components and functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration tests to test interactions between components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data tests to validate data quality and integrity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model tests to evaluate performance and behavior (fairness, non-discrimination,
    bias, robustness)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Automating testing and integrating it into continuous integration/continuous
    delivery (CI/CD) pipelines helps catch issues early in the development process.
    The benefits of comprehensive testing include improved model quality and reliability,
    faster debugging and issue resolution, and increased confidence in model behavior.Tools
    like pytest, Great Expectations, and Deepchecks can be used for ML testing. |'
  prefs: []
  type: TYPE_TB
- en: '| *Reproducibility* ensures that ML experiments and results can be reliably
    re-created. This is crucial for debugging, auditing, and scientific rigor. | Engineering
    processes that guarantee reproducibility include:'
  prefs: []
  type: TYPE_NORMAL
- en: Versioning of code, data, and models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking of random seeds and configurations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documenting the model development, dependencies, and environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerization of ML workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| The benefits of reproducibility include simplifying debugging and troubleshooting,
    enabling validation of results by others, and supporting regulatory compliance.
    When it comes to improving reproducibility in ML experiments, containerization
    tools like Docker and workflow management and metadata management tools play a
    significant role. |'
  prefs: []
  type: TYPE_TB
- en: '| Automated *deployment* focuses on reliably and efficiently moving ML models
    from development to production. | Considerations include:'
  prefs: []
  type: TYPE_NORMAL
- en: Model packaging and containerization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment strategies (e.g., canary, blue–green)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling and load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security and access control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Automated deployment practices allow for ML models to be brought to production
    more quickly, without human intervention, and for model serving to be reliable
    and scalable. If issues arise, the deployment process should allow for easy rollback.
    Tools such as Kubernetes, TensorFlow Serving, and cloud ML platforms can help
    streamline the model deployment process. |'
  prefs: []
  type: TYPE_TB
- en: '| *Monitoring* ensures that ML models perform as expected in production. |
    Key elements to monitor include:'
  prefs: []
  type: TYPE_NORMAL
- en: Model performance metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data drift and concept drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System health and resource utilization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction latency and throughput
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business validation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI product health metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Effective monitoring enables teams to quickly detect and respond to issues,
    gain an understanding of model behavior in production, and make data-driven decisions
    on when to retrain the model. Tools like Prometheus, Grafana, and specialized
    ML monitoring platforms can be used to implement comprehensive monitoring. |'
  prefs: []
  type: TYPE_TB
- en: In general, MLOps addresses the challenge of maintaining software products that
    incorporate ML/AI components in continuous, reliable operation.The specific activities
    and technical components of MLOps depend on the expected operational lifespan
    and performance requirements of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Technical Components with the MLOps Stack Canvas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section outlines a structured approach for defining the infrastructure
    for an AI project. It uses theMLOps Stack Canvas framework to identify and organize
    the workflows, architecture, and infrastructure components needed for the MLOps
    stack.
  prefs: []
  type: TYPE_NORMAL
- en: As you plan for implementation, it’s important to ensure that the ML models
    deliver measurable business value. This involves accounting for the costs associated
    with the infrastructure components within the MLOps stack and the costs of orchestrating
    and maintaining the ML system throughout its lifecycle. Key considerations include
    continuous integration, training, and delivery of ML assets; monitoring to ensure
    the system meets business objectives; and alerting mechanisms to detect and address
    model failures.
  prefs: []
  type: TYPE_NORMAL
- en: The ML system should also be designed to ensure reproducibility (through versioning,
    feature stores, and pipelines), reliability (by minimizing outages and enabling
    safe failovers), and efficiency (by ensuring model predictions are fast and cost-effective).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. The application- and industry-neutral MLOps Stack Canvas framework
    to specify an architecture and infrastructure stack for ML operations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As shown in [Figure 2-3](#chapter_2_figure_3_1748539917622790), the MLOps Stack
    Canvas is structured into three main areas: Data and Code Management, Model Management,
    and Metadata Management, each with its building blocks. In the following sections,
    we’ll go through the items in each area. Note that the general discussion about
    building the ML project’s infrastructure should include consideration of various
    organizational aspects of MLOps, such as tooling, platforms, and required skills.
    Issues that are identified should be noted in the MLOps Dilemmas section of the
    canvas.'
  prefs: []
  type: TYPE_NORMAL
- en: Value Proposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll start with the Value Proposition (item 1 on the MLOps Stack Canvas).
    The framework emphasizes the value proposition as a critical element behind the
    motivation for building the MLOps architecture because it aims to create awareness
    about the pain points being addressed. Here’s an example value proposition for
    a hypothetical “MLOps Platform”:'
  prefs: []
  type: TYPE_NORMAL
- en: '**For** data science and machine learning teams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**w****ho** need to efficiently build, train, and deploy ML models at scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The** **MLOps** **Platform** is an end-to-end machine learning platform'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t****hat** enables teams to rapidly develop and operationalize ML solutions
    to drive business value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unlike** other ML platforms that require complex setup and lack key features
    such as observability and monitoring'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**o****ur** **MLOps** **platform** provides a fully managed, intuitive interface
    with automated workflows, built-in experiment tracking, powerful AutoML capabilities,
    and seamless deployment to accelerate the ML lifecycle from prototype to production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This allows you to identify key elements such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Target customers
  prefs: []
  type: TYPE_NORMAL
- en: Data science and ML teams
  prefs: []
  type: TYPE_NORMAL
- en: Need
  prefs: []
  type: TYPE_NORMAL
- en: Efficiently building and deploying ML models at scale
  prefs: []
  type: TYPE_NORMAL
- en: Product name and category
  prefs: []
  type: TYPE_NORMAL
- en: MLOps Platform, an end-to-end machine learning platform
  prefs: []
  type: TYPE_NORMAL
- en: Key benefit
  prefs: []
  type: TYPE_NORMAL
- en: Rapidly develop and operationalize ML to drive business value
  prefs: []
  type: TYPE_NORMAL
- en: Competition
  prefs: []
  type: TYPE_NORMAL
- en: Other ML platforms that are complex and lack features such as observability
    and monitoring
  prefs: []
  type: TYPE_NORMAL
- en: Differentiation
  prefs: []
  type: TYPE_NORMAL
- en: Fully managed, intuitive, automated, with experiment tracking, AutoML, and seamless
    deployment to accelerate the full ML lifecycle
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the right target customers and their needs is essential for building
    a right-sized MLOps platform without unnecessary technical complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answering the following questions will help you confidently articulate your
    value proposition:'
  prefs: []
  type: TYPE_NORMAL
- en: What are we trying to do for the end user(s)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is this an important problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who is our persona? (ML engineer, data scientist, operation/business user)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who owns the model(s) in production?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can learn more about crafting value propositions in [Appendix A](app01.html#appendix_a_designing_ai_powered_applications_1748539915159206).
  prefs: []
  type: TYPE_NORMAL
- en: Data and Code Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Data and Code Management block of the MLOps Stack Canvas consists of technical
    components required for managing data and code—two essential elements of every
    ML system. Let’s examine the next building blocks of the canvas, namely, Data
    Sources and Data Versioning, Data Analysis and Experiment Management, Feature
    Store and Workflows, and Foundations.
  prefs: []
  type: TYPE_NORMAL
- en: Data Sources and Data Versioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data is the backbone of any machine learning system. After articulating the
    business problem in the Value Proposition, the next step is Data Sources and Data
    Versioning. The goal here is to estimate the cost of data acquisition, storage,
    and processing during the business and data understanding and data preparation
    phases of the CRISP-ML(Q) framework. Dataset development and further processing
    for ML algorithms may be costly. Data versioning is essential for analyzing model
    performance with new data and may be a regulatory requirement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some considerations for the Data Sources and Data Versioning building
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: Is data versioning optional or mandatory? For example, is it a regulatory requirement?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What data sources are available (e.g., owned, public, purchased)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where is that data stored (e.g., in a data lake or data warehouse)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is manual labeling required? Do we have human resources for it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we version data for each trained model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What tooling is available for data pipelines/workflows?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might also use the [Data Landscape Canvas](https://oreil.ly/hsXsZ) to inventory
    your organization’s available, accessible, and required data sources for the ML
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Data Analysis and Experiment Management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to the CRISP-ML(Q) framework, the project’s initial phase includes
    running experiments and implementing a proof of concept. The Data Analysis and
    Experiment Management block focuses on the applicability of ML technology for
    specific business goals and data preparation. Here, you need to answer the following
    questions regarding tooling:'
  prefs: []
  type: TYPE_NORMAL
- en: What programming language will you use for analysis (e.g., R, Python, Scala,
    Julia, SQL)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What ML-specific and business evaluation metrics need to be computed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What metadata about ML experiments is collected, for reproducibility (datasets,
    hyperparameters, etc.)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What ML framework expertise is available on the team?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature Store and Workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Feature engineering* is the process of transforming raw input data into features,
    a numerical representation suitable for machine learning algorithms. The feature
    store is a technical component in the MLOps stack used for managing, reproducing,
    discovering, and reusing features across ML projects and data science teams. It
    separates feature engineering from ML model development and speeds up the process.
    However, as this is an advanced component, using a feature store may add complexity,
    so you need to consider whether it’s appropriate for the project at hand. Here
    are some questions to ask:'
  prefs: []
  type: TYPE_NORMAL
- en: Is a feature store optional or mandatory? Do we have a data governance process
    that requires feature engineering to be reproducible?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How are features computed (workflows) during the training and prediction phases?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the infrastructure requirements for feature engineering?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should we buy or make the feature store?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What databases are involved in feature storage?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we design APIs for feature engineering?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foundations (Reflecting DevOps)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DevOps is a set of practices and tools that integrates software development
    and IT operations to automate, streamline, and accelerate the building, testing,
    and deployment of software applications with a focus on reliability and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: In the next step in the MLOps Stack Canvas, you’ll take stock of the available
    DevOps infrastructure and promote awareness of current DevOps principles within
    the ML project. This helps in extrapolating DevOps best practices to MLOps activities.
    If there are gaps in traditional DevOps practices, it’s essential to address them
    before moving on to more complex activities such as model and data versioning,
    continuous model training, or implementing a feature store.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the guidelines in the DORA [“Accelerate State of DevOps Report”](https://oreil.ly/YzySM)
    and execute a self-assessment of your software delivery performance by answering
    the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we maintain the code? What source version control system is used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we monitor the system performance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need versioning for notebooks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is a trunk-based development approach used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the CI/CD pipeline for the codebase? What tools are used for it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we track metrics such as deployment frequency, lead time for changes, mean
    time to restore, and change failure rate?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adhering to DevOps principles directly influences the performance of software
    delivery. MLOps is built on top of DevOps, so establishing a stable DevOps culture
    for software projects is essential for the success of ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: Model Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Model management encompasses a set of practices, processes, and tools used
    to efficiently track, version, deploy, and monitor machine learning models throughout
    their lifecycle, guaranteeing consistency, reproducibility, and scalability while
    meeting business requirements. The building blocks in this area of the MLOps Stack
    Canvas include CI/CD/CT: ML Pipeline Orchestration, Model Registry and Model Versioning,
    Model Deployment, Prediction Serving, and Model, Data, and Application Monitoring.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CI/CT/CD: ML Pipeline Orchestration'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You reviewed the existing CI/CD pipelines for software delivery in the previous
    block. In this block, you’ll examine the CI/CD routine for ML model release. You’ll
    also introduce continuous training (CT). While continuous integration involves
    the building, testing, and packaging of data and model pipelines, continuous training
    focuses on automatically retraining ML models. You’ll use the pipeline pattern,
    which involves steps such as data verification, feature and data selection, data
    cleaning, feature engineering, and model training.
  prefs: []
  type: TYPE_NORMAL
- en: A pipeline is structured as a directed acyclic graph (DAG) representing the
    overall workflow. This is a common way to represent dependencies and execution
    order. Depending on the maturity level, you can automate the data and model training
    pipeline workflows to operationalize the model. You should trigger data preparation
    and model training pipelines whenever new data is available or when the source
    code for a pipeline has changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this block of the MLOps Stack Canvas, you should clarify the processes and
    the toolchain for CI/CT in the CRISP-ML(Q) model engineering phase by answering
    the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How often are models expected to be retrained? What is the trigger for this
    (scheduled, event-based, or ad hoc)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where does this happen (locally or in the cloud)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the formalized workflow for an ML pipeline? What tech stack is used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is distributed model training required? Do we have the necessary infrastructure
    for distributed training?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the workflow for the CI pipeline? What tools are used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the nonfunctional requirements for the ML model (efficiency, fairness,
    robustness, interpretability, etc.)? How are they tested? Are these tests integrated
    into the CI/CT workflow?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Registry and Model Versioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next section in the MLOps Stack Canvas is concerned with the ML model registry
    and versioning component, which is a crucial element of the model evaluation phase
    in the CRISP-ML(Q) process. The machine learning model, along with the data and
    the software code, is an essential asset. As with code versioning in traditional
    software engineering, establishing a model and data versioning practice is the
    foundation for reproducibility in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your use case, a change in code or data might require retraining
    the model. Models may also need to be updated due to “model decay,” where the
    model’s performance declines over time with new data. In regulated industries
    like healthcare, finance, and the military, all ML models should be versioned
    and thoroughly documented. It’s also important to ensure backward compatibility
    by being able to roll back to previously built models. By tracking multiple versions
    of the ML model, it is possible to implement different deployment strategies,
    such as “canary” or “shadow” deployment (methods used to gradually roll out changes
    in a controlled manner to mitigate risk while evaluating the performance of the
    newly trained model).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section of the MLOps Stack Canvas, you should answer the following
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Is using a model registry and model versioning optional or mandatory? A model
    registry might be required if you have multiple models in production and need
    to track them all. Model versioning might be necessary if you have a requirement
    for reproducibility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where should new ML models be stored and tracked?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What versioning standards are used (e.g., semantic versioning)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that implementing versioning and an ML model registry may be more appropriate
    in the later stages of ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: Model Deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After training and evaluating the model, you’ll move on to the next phase of
    the CRISP-ML(Q) process and deploy the machine learning model. Deploying an ML
    model means making it available in the target environment to receive prediction
    requests. In this section of the MLOps Stack Canvas, you’ll define your model
    exposure strategies and the infrastructure aspects of continuous deployment (the
    automatic deployment of ML models into the target environment based on predetermined
    evaluation metrics) by addressing the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the delivery format for the model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the expected time for changes (time from commit to production)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the target environment to serve predictions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is our model release policy? Is A/B testing or multi-armed bandits testing
    required (e.g., for measuring the effectiveness of the new model on business metrics
    and deciding what model should be promoted in the production environment)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is our deployment strategy? For example, is shadow or canary deployment
    required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction Serving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section of the MLOps Stack Canvas covers the process of applying a machine
    learning model to new input data, commonly referred to as model serving. There
    are two primary serving modes: online and batch. ML models can be deployed using
    five different patterns: Model-as-a-Service, Model-as-a-Dependency, Precompute,
    Model-on-Demand, and Hybrid Serving. Each pattern requires different infrastructure
    settings. For example, Model-as-a-Service involves distributing the model as a
    service for input requests via a REST API and uses an on-demand mode for prediction
    responses. On the other hand, the Precompute pattern uses the batch prediction
    mode, with model predictions precomputed and stored in a relational database.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To identify the environment for model serving, you should answer the following
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the serving mode (batch or online)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is distributed model serving required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is multi-model prediction serving required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is pre-assertion for input data implemented?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What fallback method for an inadequate model output (post-assertion) is implemented?
    (Do we have a heuristic benchmark?)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need ML inference accelerators (like special hardware such as Tensor Processing
    Units)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the expected target volume of predictions per month (or hour/day)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model, Data, and Application Monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the ML model is put into operation, it must be monitored continuously
    to ensure that its quality is maintained and it continues to deliver accurate
    results. This section of the MLOps Stack Canvas addresses the monitoring aspect
    of operating the ML system in production. It corresponds to the sixth stage of
    the CRISP-ML(Q) process, model deployment and monitoring. Here are some questions
    to answer:'
  prefs: []
  type: TYPE_NORMAL
- en: Is continuous monitoring optional or mandatory? For instance, do you need to
    assess the effectiveness of your model during prediction serving? Do you need
    to monitor your model for performance degradation and trigger an alert if it starts
    performing badly? Is model retraining automatically triggered by specific events,
    such as data drift or concept drift?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What ML metrics are collected?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What domain-specific metrics are collected?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is the model performance decay detected?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is the data skew detected?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What operational aspects need to be monitored (e.g., model prediction latency,
    CPU/RAM usage)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the alerting strategy (e.g., thresholds)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What triggers model retraining? Is it ad hoc, event-based, or scheduled?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metadata management for machine learning systems is a holistic process of collecting,
    organizing, storing, and utilizing data about the various components and stages
    of the ML lifecycle. This includes tracking information related to datasets, models,
    experiments, deployments, and performance metrics to ensure reproducibility, comparability,
    and traceability throughout the ML pipeline (see [Figure 2-4](#chapter_2_figure_4_1748539917622886)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Metadata management
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The *m**etadata* *s**tore* is a cross-cutting component that supports all elements
    of the MLOps infrastructure stack. Implementing an ML model governance process—which
    may be necessary, depending on your organization and regulatory requirements—relies
    heavily on metadata, making a metadata store an essential component.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last block of the MLOps Stack Canvas, you’ll answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What kind of metadata related to the code, data, and model management needs
    to be collected (e.g., the pipeline run ID, trigger, performed steps, start/end
    timestamps, train/test dataset split, hyperparameters, model object, various statistics,
    etc.)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are any ML governance processes included in the MLOps lifecycle? What metadata
    will be required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the documentation strategy? Do we treat documentation as a code?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What operational metrics need to be collected (e.g., time to restore, change
    fail percentage)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional Considerations of MLOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An MLOps platform comprises self-service APIs, tools, services, knowledge,
    and support organized as a product (for more on this concept, see the Team Topologies
    video [“WTF Is Platform as Product?”](https://oreil.ly/2IZ7x)). Platforms are
    designed to enable multiple autonomous delivery teams to release product features
    more quickly, with decreased dependencies and coordination between teams. There
    are also organizational aspects of AI engineering to consider as part of the broader
    discussion about constructing the infrastructure for AI projects. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Tooling
  prefs: []
  type: TYPE_NORMAL
- en: Should we purchase, use existing open source, or develop in-house tools for
    each of the MLOps components? What are the risks, trade-offs, and impacts of each
    decision?
  prefs: []
  type: TYPE_NORMAL
- en: Platforms
  prefs: []
  type: TYPE_NORMAL
- en: Should we standardize on a single MLOps platform or create a hybrid solution?
    What are the risks, trade-offs, and impacts of this decision?^([1](ch02.html#id456))
  prefs: []
  type: TYPE_NORMAL
- en: Skills
  prefs: []
  type: TYPE_NORMAL
- en: What is the cost associated with either hiring or training our own machine learning
    engineering talent?
  prefs: []
  type: TYPE_NORMAL
- en: Using the MLOps Stack Canvas helps you identify the workflows, architecture,
    and infrastructure components for your AI project and develop a good cost estimation
    for your AI project in every phase.
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q) and MLOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLOps and CRISP-ML(Q) are complementary approaches that can work together synergistically
    to support and improve the development and deployment of machine learning models.
    While CRISP-ML(Q) provides a structured process framework, MLOps offers technical
    and organizational best practices for implementing and operationalizing ML projects.
    [Figure 2-5](#chapter_2_figure_5_1748539917622906) illustrates the synergy between
    the two.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. MLOps and CRISP-ML(Q) synergy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'These two approaches can be integrated to bind the ML development process with
    technical implementation. In particular, CRISP-ML(Q) suggests a high-level, structured
    process for machine learning projects, dividing the development workflow into
    six main phases: business and data understanding, data preparation, modeling,
    evaluation, deployment, and monitoring and maintenance. MLOps provides the technical
    infrastructure and best practices to implement these phases effectively. For example,
    during the data engineering phase of CRISP-ML(Q), MLOps practices can be applied
    to automate data pipelines, ensure data versioning, and maintain data quality.'
  prefs: []
  type: TYPE_NORMAL
- en: The main strength of CRISP-ML(Q), and what makes it a suitable ML process model
    for EU AI Act implementation, is its integration of quality assurance and risk
    awareness throughout the machine learning lifecycle. MLOps complements this by
    providing tools and practices for continuous integration, continuous delivery,
    and automated testing, which are essential for maintaining quality in production
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: ML development is, by nature, iterative, and both CRISP-ML(Q) and MLOps support
    this. CRISP-ML(Q) provides a framework for iterating through the phases of ML
    development, while MLOps offers technical solutions for rapid experimentation,
    version control, and automated retraining of ML models. Furthermore, MLOps facilitates
    model retraining by incorporating practices such as continuous monitoring, logging,
    and automated model updates. [Research by DORA](https://oreil.ly/YzySM) shows
    that employing continuous delivery (and hence MLOps) practices can significantly
    improve organizations’ technical and business performance. By enabling teams to
    consistently deliver value and adapt to changes in market demands more effectively,
    MLOps practices not only lower the risks of deployment and production failures
    but also reduce time to feedback, decrease cognitive load, and lessen the general
    stress associated with operating models in production.
  prefs: []
  type: TYPE_NORMAL
- en: Being quality-driven, CRISP-ML(Q) emphasizes the importance of documenting business
    decisions, data, the ML model development process, and the models and experiments
    themselves. MLOps complements this approach by providing version control for code,
    data, and models, thereby ensuring the reproducibility of results. It also introduces
    technical components, such as metadata, model, and feature stores, to support
    these functions.
  prefs: []
  type: TYPE_NORMAL
- en: CRISP-ML(Q)’s iterative nature aligns well with the emphasis of MLOps on continuous
    improvement. MLOps practices can be used to implement the feedback loops suggested
    by CRISP-ML(Q), allowing for ongoing refinement of models and processes. MLOps
    also offers technical solutions for scaling ML operations, including automated
    model training, evaluation, and deployment pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, organizations can use CRISP-ML(Q) as a high-level framework for
    structuring their machine learning projects, while applying MLOps best practices
    and tools within each phase. This combination ensures that projects follow a well-defined
    process while benefiting from modern, efficient, and scalable technical implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter introduced CRISP-ML(Q) and MLOps and showed how this pairing provides
    a robust framework for addressing EU AI Act compliance requirements: CRISP-ML(Q)
    provides the process blueprint, while MLOps supplies the tools and practices needed
    to operationalize that process in a compliant manner. Together, they enable organizations
    to proactively embed EU AI Act compliance into the ML lifecycle, from data collection
    to monitoring, rather than treating it as an afterthought.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, to complete the compliance engineering picture, you
    will learn about data governance, a framework of policies, processes, and responsibilities
    that ensures the quality, security, compliance, and ethical use of data throughout
    the AI development lifecycle. You will also see how AI governance ensures that
    AI systems are developed, deployed, and monitored in a responsible, ethical, secure,
    and compliant manner.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.html#id456-marker)) Thoughtworks provides an [excellent guide](https://oreil.ly/nE93Z)
    for understanding the MLOps platform landscape.
  prefs: []
  type: TYPE_NORMAL
