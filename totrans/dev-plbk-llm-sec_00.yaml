- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everywhere in the world, we’re riding the large language model (LLM) wave, and
    it’s exhilarating! When ChatGPT burst onto the scene, it didn’t just walk into
    the record books; it smashed them, becoming the fastest-adopted application in
    history. Now, it’s as if every software vendor on the planet is racing to embed
    generative AI and LLM technologies into their stack, pushing us into uncharted
    territories. The buzz is real, the hype is justified, and the possibilities seem
    limitless.
  prefs: []
  type: TYPE_NORMAL
- en: 'But hold on because there’s a twist. As we marvel at these technological wonders,
    their security scaffolding is, to put it mildly, a work in progress. The hard
    truth? Many developers are stepping into this new era without a map, largely unaware
    of the security and safety quicksand beneath the surface. It’s almost routine
    now: every week, we’re hit with another headline screaming about an LLM hiccup.
    The fallout from these individual incidents has been moderate so far, but make
    no mistake—we’re flirting with disaster.'
  prefs: []
  type: TYPE_NORMAL
- en: The risks aren’t just hypothetical; they’re as real as it gets, and the clock
    is ticking. Without a deep dive into the murky waters of LLM security risks and
    how to navigate them, we’re not just risking minor glitches; we’re courting major
    catastrophes. It’s time for developers to gear up, get informed, and get ahead
    of the curve. Fast!
  prefs: []
  type: TYPE_NORMAL
- en: Who Should Read This Book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary audience for this book is development teams that are building custom
    applications that embed LLM technologies. Through my recent work in this area,
    I’ve come to understand that these teams are often large and their members include
    an incredibly diverse set of backgrounds. These include software developers skilled
    in “web app” technologies who are taking their first steps with AI. These teams
    may also consist of AI experts who are bringing their craft out of the back office
    for the first time and into the limelight, where the security risks are much different.
    They also include application security pros and data science specialists.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond that core audience, I’ve learned that others have found much of this
    information useful. This includes the extended teams involved in these projects,
    who want to understand the underpinnings of the technologies to help mitigate
    the critical risks of adopting these new technologies. These include software
    development executives, chief information security officers (CISOs), quality engineers,
    and security operations teams.
  prefs: []
  type: TYPE_NORMAL
- en: Why I Wrote This Book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’ve always been fascinated by artificial intelligence. As a preteen, I fondly
    remember writing video games on my Atari 400 home computer. Circa 1980, this little
    machine had only 8 kilobytes of RAM. But I still managed to cram a complete clone
    of the Tron Lightcycles game onto that machine, complete with a simple but effective
    AI to drive one of the cycles when you were playing in single-player mode.
  prefs: []
  type: TYPE_NORMAL
- en: In my professional career, I’ve been involved with several AI-related projects.
    After college, my best friend Tom Santos and I started an AI software company
    based on a few thousand lines of handcrafted C++ code that solved seemingly intractable
    problems with genetic algorithms. I’d later help build a large-scale machine learning
    system at Citrix with my friends Kedar Poduri and Ebenezer Schubert. But when
    I saw ChatGPT for the first time, I knew everything had changed.
  prefs: []
  type: TYPE_NORMAL
- en: When I first encountered LLMs, I worked at a company that built cybersecurity
    software. My job was helping large companies find and track vulnerabilities in
    their software. It quickly became apparent that LLMs offered unique and serious
    security vulnerabilities. Over the next few months, I retooled my career to go
    after this disruption. I started a popular open source project around LLM security,
    which you’ll hear more about later. I even switched jobs to join Exabeam, a company
    that works at the intersection of AI and cybersecurity. When an editor from O’Reilly
    approached me about writing a book on this topic, I knew I had to jump at the
    chance.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating This Book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book has 12 chapters that are divided into three logical sections. I’ll
    sketch out each section and chapter here to give you an idea of the approach and
    so you’ll know what’s coming as you read.
  prefs: []
  type: TYPE_NORMAL
- en: 'Section 1: Laying the Foundation (Chapters 1–3)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The initial chapters of this book establish the groundwork for understanding
    the security posture of LLM-based applications. They should give you the grounding
    you can use to confidently unpack the issues facing the development of apps using
    LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 1, “Chatbots Breaking Bad”](ch01.html#chatbots_breaking_bad), walks
    through a real-world case study whereby amateur hackers destroyed an expensive
    and promising chatbot project from one of the world’s largest software companies.
    This will set the stage for your forthcoming battles in this arena.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 2, “The OWASP Top 10 for LLM Applications”](ch02.html#the_owasp_top_10_for_llm_applications),
    introduces a project I founded in 2023 that aims to identify and address the unique
    security challenges posed by LLMs. The knowledge gained working on that directly
    led to my writing this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 3, “Architectures and Trust Boundaries”](ch03.html#architectures_and_trust_boundaries),
    explores the structure of applications using LLMs, emphasizing the importance
    of controlling the various data flows within the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section 2: Risks, Vulnerabilities, and Remediations (Chapters 4–9)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In these chapters, we’ll break down the significant risk areas you face when
    developing LLM applications. These risks include issues with flavors familiar
    to any application security practitioner, such as injection attacks, sensitive
    information leakage, and software supply chain risk. You’ll also be introduced
    to classes of vulnerabilities well known to machine learning aficionados but less
    familiar in web development, such as training data poisoning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Along the way, you’ll also learn about all-new security and safety concerns
    plaguing these new generative AI systems, such as hallucinations, overreliance,
    and excessive agency. I’ll walk you through real-world case studies to help you
    understand the risks and implications and advise you on how to prevent or mitigate
    these risks on a case-by-case basis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 4, “Prompt Injection”](ch04.html#prompt_injection), explores how attackers
    can manipulate LLMs by crafting specific inputs that cause them to perform unintended
    actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 5, “Can Your LLM Know Too Much?”](ch05.html#can_your_llm_know_too_much),
    dives into the risks of sensitive information leakage, showcasing how LLMs can
    inadvertently expose data they’ve been trained on and how to safeguard against
    this vulnerability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 6, “Do Language Models Dream of Electric Sheep?”](ch06.html#do_language_models_dream_of_electric_sheep),
    examines the unique phenomenon of “hallucinations” in LLMs—instances where models
    generate false or misleading information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 7, “Trust No One”](ch07.html#trust_no_one), focuses on the principle
    of zero trust, explaining the importance of not taking any output at face value
    and ensuring rigorous validation processes are in place to handle LLM outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 8, “Don’t Lose Your Wallet”](ch08.html#don_t_lose_your_wallet), tackles
    the economic risks of deploying LLM technologies, focusing on denial-of-service
    (DoS), denial-of-wallet (DoW), and model cloning attacks. These threats exploit
    similar vulnerabilities to impose financial burdens, disrupt services, or steal
    intellectual property.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 9, “Find the Weakest Link”](ch09.html#find_the_weakest_link), highlights
    the vulnerabilities within the software supply chain and the critical steps needed
    to secure it from potential breaches that could compromise the entire application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding and addressing these risks, developers can better secure their
    applications against an evolving landscape of threats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Section 3: Building a Security Process and Preparing for the Future (Chapters
    10–12)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The chapters in Section 2 will give you the tools you need to understand and
    address the various individual threats you’ll see in this space. This last section
    is about bringing it all together:'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 10, “Learning from Future History”](ch10.html#learning_from_future_history),
    I’ll use some famous science fiction anecdotes to illustrate how multiple weaknesses
    and design issues can stitch together to spell disaster. By explaining these futuristic
    case studies, I hope to help you prevent a future like this from ever occurring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 11, “Trust the Process”](ch11.html#trust_the_process), we’ll get
    down to the serious business of building LLM-savvy security practices into your
    software factory—without this, I do not believe you can successfully secure this
    type of software at scale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, in [Chapter 12, “A Practical Framework for Responsible AI Security”](ch12.html#a_practical_framework_for_responsible_ai_security),
    we’ll examine the trajectory of LLM and AI technologies to see where they’re taking
    us and the likely implications to security and safety requirements. I’ll also
    introduce you to the Responsible Artificial Intelligence Software Engineering
    (RAISE) framework that will give you a simple, checklist-based approach to ensuring
    you’re putting into practice the most important tools and lessons to keep your
    software safe and secure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conventions Used in This Book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following typographical conventions are used in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Italic*'
  prefs: []
  type: TYPE_NORMAL
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  prefs: []
  type: TYPE_NORMAL
- en: '`Constant width`'
  prefs: []
  type: TYPE_NORMAL
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  prefs: []
  type: TYPE_NORMAL
- en: '**`Constant width bold`**'
  prefs: []
  type: TYPE_NORMAL
- en: Shows commands or other text that should be typed literally by the user.
  prefs: []
  type: TYPE_NORMAL
- en: '*`Constant width italic`*'
  prefs: []
  type: TYPE_NORMAL
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This element signifies a tip or suggestion.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This element signifies a general note.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This element indicates a warning or caution.
  prefs: []
  type: TYPE_NORMAL
- en: O’Reilly Online Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For more than 40 years, [*O’Reilly Media*](https://oreilly.com) has provided
    technology and business training, knowledge, and insight to help companies succeed.
  prefs: []
  type: TYPE_NORMAL
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*https://oreilly.com*](https://oreilly.com).
  prefs: []
  type: TYPE_NORMAL
- en: How to Contact Us
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please address comments and questions concerning this book to the publisher:'
  prefs: []
  type: TYPE_NORMAL
- en: O’Reilly Media, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1005 Gravenstein Highway North
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sebastopol, CA 95472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 800-889-8969 (in the United States or Canada)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 707-827-7019 (international or local)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 707-829-0104 (fax)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*support@oreilly.com*](mailto:support@oreilly.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://www.oreilly.com/about/contact.html*](https://www.oreilly.com/about/contact.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/the-developers-playbook*](https://oreil.ly/the-developers-playbook).
  prefs: []
  type: TYPE_NORMAL
- en: For news and information about our books and courses, visit [*https://oreilly.com*](https://oreilly.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'Find us on LinkedIn: [*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Watch us on YouTube: [*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia).'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I’d like to thank all the friends, family, and colleagues who encouraged me
    or provided feedback on various aspects of the project: Will Chilcutt, Fabrizio
    Cilli, Ads Dawson, Ron Del Rosario, Sherri Douville, Sandy Dunn, Ken Huang, Gavin
    Klondike, Marko Lihter, Marten Mickos, Eugene Neelou, Chase Peterson, Karla Roland,
    Jason Ross, Tom Santos, Robert Simonoff, Yuvraj Singh, Rachit Sood, Seth Summersett,
    Darcie Tuuri, Ashish Verma, Jeff Williams, Alexa Wilson, Dave Wilson, and Zoe
    Wilson.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I want to thank the team at O’Reilly for supporting and guiding me on this
    project. I also owe a tremendous debt of gratitude to Nicole Butterfield, who
    approached me with the idea for this book and guided me through the proposal phase.
    I also want to express my appreciation for Jeff Bleiel, my editor, whose patience,
    skills, and expertise significantly impacted the book. Special thanks to our technical
    reviewers: Pamela Isom, Chenta Lee, Thomas Nield, and Matteo Dora.'
  prefs: []
  type: TYPE_NORMAL
