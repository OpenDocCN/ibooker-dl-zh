["```py\nimport urllib.request\nimport zipfile\nimport os\nfrom pathlib import Path\n\nurl = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\nzip_path = \"sms_spam_collection.zip\"\nextracted_path = \"sms_spam_collection\"\ndata_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n\ndef download_and_unzip_spam_data(\n        url, zip_path, extracted_path, data_file_path):\n    if data_file_path.exists():\n        print(f\"{data_file_path} already exists. Skipping download \"\n              \"and extraction.\"\n        )\n        return\n\n    with urllib.request.urlopen(url) as response:    #1\n        with open(zip_path, \"wb\") as out_file:\n            out_file.write(response.read())\n\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:    #2\n        zip_ref.extractall(extracted_path)\n\n    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n    os.rename(original_file_path, data_file_path)               #3\n    print(f\"File downloaded and saved as {data_file_path}\")\n\ndownload_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n```", "```py\nimport pandas as pd\ndf = pd.read_csv(\n    data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"]\n)\ndf      #1\n```", "```py\nprint(df[\"Label\"].value_counts())\n```", "```py\nLabel\nham     4825\nspam     747\nName: count, dtype: int64\n```", "```py\ndef create_balanced_dataset(df):\n    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]     #1\n    ham_subset = df[df[\"Label\"] == \"ham\"].sample(\n        num_spam, random_state=123\n    )                                         #2\n    balanced_df = pd.concat([\n        ham_subset, df[df[\"Label\"] == \"spam\"]\n    ])                               #3\n    return balanced_df\n\nbalanced_df = create_balanced_dataset(df)\nprint(balanced_df[\"Label\"].value_counts())\n```", "```py\nLabel\nham     747\nspam    747\nName: count, dtype: int64\n```", "```py\nbalanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n```", "```py\ndef random_split(df, train_frac, validation_frac):\n\n    df = df.sample(\n        frac=1, random_state=123\n    ).reset_index(drop=True)               #1\n    train_end = int(len(df) * train_frac)          #2\n    validation_end = train_end + int(len(df) * validation_frac)\n\n #3\n    train_df = df[:train_end]\n    validation_df = df[train_end:validation_end]\n    test_df = df[validation_end:]\n\n    return train_df, validation_df, test_df\n\ntrain_df, validation_df, test_df = random_split(\n    balanced_df, 0.7, 0.1)                     #4\n```", "```py\ntrain_df.to_csv(\"train.csv\", index=None)\nvalidation_df.to_csv(\"validation.csv\", index=None)\ntest_df.to_csv(\"test.csv\", index=None)\n```", "```py\nimport tiktoken\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nprint(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n```", "```py\nimport torch\nfrom torch.utils.data import Dataset\n\nclass SpamDataset(Dataset):\n    def __init__(self, csv_file, tokenizer, max_length=None,\n                 pad_token_id=50256):\n        self.data = pd.read_csv(csv_file)\n #1\n        self.encoded_texts = [\n            tokenizer.encode(text) for text in self.data[\"Text\"]\n        ]\n\n        if max_length is None:\n            self.max_length = self._longest_encoded_length()\n        else:\n            self.max_length = max_length\n #2\n            self.encoded_texts = [\n                encoded_text[:self.max_length]\n                for encoded_text in self.encoded_texts\n            ]\n\n #3\n        self.encoded_texts = [\n            encoded_text + [pad_token_id] * \n            (self.max_length - len(encoded_text))\n            for encoded_text in self.encoded_texts\n        ]\n\n    def __getitem__(self, index):\n        encoded = self.encoded_texts[index]\n        label = self.data.iloc[index][\"Label\"]\n        return (\n            torch.tensor(encoded, dtype=torch.long),\n            torch.tensor(label, dtype=torch.long)\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def _longest_encoded_length(self):\n        max_length = 0\n        for encoded_text in self.encoded_texts:\n            encoded_length = len(encoded_text)\n            if encoded_length > max_length:\n                max_length = encoded_length\n        return max_length\n```", "```py\ntrain_dataset = SpamDataset(\n    csv_file=\"train.csv\",\n    max_length=None,\n    tokenizer=tokenizer\n)\n```", "```py\nprint(train_dataset.max_length)\n```", "```py\nval_dataset = SpamDataset(\n    csv_file=\"validation.csv\",\n    max_length=train_dataset.max_length,\n    tokenizer=tokenizer\n)\ntest_dataset = SpamDataset(\n    csv_file=\"test.csv\",\n    max_length=train_dataset.max_length,\n    tokenizer=tokenizer\n)\n```", "```py\nfrom torch.utils.data import DataLoader\n\nnum_workers = 0      #1\nbatch_size = 8\ntorch.manual_seed(123)\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,\n    drop_last=True,\n)\nval_loader = DataLoader(\n    dataset=val_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    drop_last=False,\n)\ntest_loader = DataLoader(\n    dataset=test_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    drop_last=False,\n)\n```", "```py\nfor input_batch, target_batch in train_loader:\n    pass\nprint(\"Input batch dimensions:\", input_batch.shape)\nprint(\"Label batch dimensions\", target_batch.shape)\n```", "```py\nInput batch dimensions: torch.Size([8, 120])\nLabel batch dimensions torch.Size([8])\n```", "```py\nprint(f\"{len(train_loader)} training batches\")\nprint(f\"{len(val_loader)} validation batches\")\nprint(f\"{len(test_loader)} test batches\")\n```", "```py\n130 training batches\n19 validation batches\n38 test batches\n```", "```py\nCHOOSE_MODEL = \"gpt2-small (124M)\"\nINPUT_PROMPT = \"Every effort moves\"\nBASE_CONFIG = {\n    \"vocab_size\": 50257,          #1\n    \"context_length\": 1024,       #2\n    \"drop_rate\": 0.0,             #3\n    \"qkv_bias\": True              #4\n}\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n```", "```py\nfrom gpt_download import download_and_load_gpt2\nfrom chapter05 import GPTModel, load_weights_into_gpt\n\nmodel_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\nsettings, params = download_and_load_gpt2(\n    model_size=model_size, models_dir=\"gpt2\"\n)\n\nmodel = GPTModel(BASE_CONFIG)\nload_weights_into_gpt(model, params)\nmodel.eval()\n```", "```py\nfrom chapter04 import generate_text_simple\nfrom chapter05 import text_to_token_ids, token_ids_to_text\n\ntext_1 = \"Every effort moves you\"\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(text_1, tokenizer),\n    max_new_tokens=15,\n    context_size=BASE_CONFIG[\"context_length\"]\n)\nprint(token_ids_to_text(token_ids, tokenizer))\n```", "```py\nEvery effort moves you forward.\nThe first step is to understand the importance of your work\n```", "```py\ntext_2 = (\n    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n    \" 'You are a winner you have been specially\"\n    \" selected to receive $1000 cash or a $2000 award.'\"\n)\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(text_2, tokenizer),\n    max_new_tokens=23,\n    context_size=BASE_CONFIG[\"context_length\"]\n)\nprint(token_ids_to_text(token_ids, tokenizer))\n```", "```py\nIs the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\nyou have been specially selected to receive $1000 cash \nor a $2000 award.'\nThe following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n```", "```py\nGPTModel(\n  (tok_emb): Embedding(50257, 768)\n  (pos_emb): Embedding(1024, 768)\n  (drop_emb): Dropout(p=0.0, inplace=False)\n  (trf_blocks): Sequential(\n...\n    (11): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=768, out_features=768, bias=True)\n        (W_key): Linear(in_features=768, out_features=768, bias=True)\n        (W_value): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n  )\n  (final_norm): LayerNorm()\n  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n```", "```py\nfor param in model.parameters():\n    param.requires_grad = False\n```", "```py\ntorch.manual_seed(123)\nnum_classes = 2\nmodel.out_head = torch.nn.Linear(\n    in_features=BASE_CONFIG[\"emb_dim\"], \n    out_features=num_classes\n)\n```", "```py\nfor param in model.trf_blocks[-1].parameters():\n    param.requires_grad = True\nfor param in model.final_norm.parameters():\n    param.requires_grad = True\n```", "```py\ninputs = tokenizer.encode(\"Do you have time\")\ninputs = torch.tensor(inputs).unsqueeze(0)\nprint(\"Inputs:\", inputs)\nprint(\"Inputs dimensions:\", inputs.shape)    #1\n```", "```py\nInputs: tensor([[5211,  345,  423,  640]])\nInputs dimensions: torch.Size([1, 4])\n```", "```py\nwith torch.no_grad():\n    outputs = model(inputs)\nprint(\"Outputs:\\n\", outputs)\nprint(\"Outputs dimensions:\", outputs.shape)\n```", "```py\nOutputs:\n tensor([[[-1.5854,  0.9904],\n          [-3.7235,  7.4548],\n          [-2.2661,  6.6049],\n          [-3.5983,  3.9902]]])\nOutputs dimensions: torch.Size([1, 4, 2])\n```", "```py\nprint(\"Last output token:\", outputs[:, -1, :])\n```", "```py\nLast output token: tensor([[-3.5983,  3.9902]])\n```", "```py\nprint(\"Last output token:\", outputs[:, -1, :])\n```", "```py\nLast output token: tensor([[-3.5983,  3.9902]])\n```", "```py\nprobas = torch.softmax(outputs[:, -1, :], dim=-1)\nlabel = torch.argmax(probas)\nprint(\"Class label:\", label.item())\n```", "```py\nlogits = outputs[:, -1, :]\nlabel = torch.argmax(logits)\nprint(\"Class label:\", label.item())\n```", "```py\ndef calc_accuracy_loader(data_loader, model, device, num_batches=None):\n    model.eval()\n    correct_predictions, num_examples = 0, 0\n\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches = min(num_batches, len(data_loader))\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            input_batch = input_batch.to(device)\n            target_batch = target_batch.to(device)\n\n            with torch.no_grad():\n                logits = model(input_batch)[:, -1, :]     #1\n            predicted_labels = torch.argmax(logits, dim=-1)\n\n            num_examples += predicted_labels.shape[0]\n            correct_predictions += (\n                (predicted_labels == target_batch).sum().item()\n            )\n\n        else:\n            break\n    return correct_predictions / num_examples\n```", "```py\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ntorch.manual_seed(123)\ntrain_accuracy = calc_accuracy_loader(\n    train_loader, model, device, num_batches=10\n)\nval_accuracy = calc_accuracy_loader(\n    val_loader, model, device, num_batches=10\n)\ntest_accuracy = calc_accuracy_loader(\n    test_loader, model, device, num_batches=10\n)\n\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n```", "```py\nTraining accuracy: 46.25%\nValidation accuracy: 45.00%\nTest accuracy: 48.75%\n```", "```py\ndef calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch = input_batch.to(device)\n    target_batch = target_batch.to(device)\n    logits = model(input_batch)[:, -1, :]     #1\n    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n    return loss\n```", "```py\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:                                        #1\n        num_batches = min(num_batches, len(data_loader))\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(\n                input_batch, target_batch, model, device\n            )\n            total_loss += loss.item()\n        else:\n            break\n    return total_loss / num_batches\n```", "```py\nwith torch.no_grad():                 #1\n    train_loss = calc_loss_loader(\n        train_loader, model, device, num_batches=5\n    )\n    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\nprint(f\"Training loss: {train_loss:.3f}\")\nprint(f\"Validation loss: {val_loss:.3f}\")\nprint(f\"Test loss: {test_loss:.3f}\")\n```", "```py\nTraining loss: 2.453\nValidation loss: 2.583\nTest loss: 2.322\n```", "```py\ndef train_classifier_simple(\n        model, train_loader, val_loader, optimizer, device,\n        num_epochs, eval_freq, eval_iter):\n    train_losses, val_losses, train_accs, val_accs = [], [], [], []   #1\n    examples_seen, global_step = 0, -1\n\n    for epoch in range(num_epochs):    #2\n        model.train()             #3\n\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad()                      #4\n            loss = calc_loss_batch(\n                input_batch, target_batch, model, device\n            )\n            loss.backward()                          #5\n            optimizer.step()                          #6\n            examples_seen += input_batch.shape[0]    #7\n            global_step += 1\n\n #8\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader, device, eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n                      f\"Train loss {train_loss:.3f}, \"\n                      f\"Val loss {val_loss:.3f}\"\n                )\n\n #9\n        train_accuracy = calc_accuracy_loader(\n            train_loader, model, device, num_batches=eval_iter\n        )\n        val_accuracy = calc_accuracy_loader(\n            val_loader, model, device, num_batches=eval_iter\n        )\n\n        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n        train_accs.append(train_accuracy)\n        val_accs.append(val_accuracy)\n\n    return train_losses, val_losses, train_accs, val_accs, examples_seen\n```", "```py\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(\n            train_loader, model, device, num_batches=eval_iter\n        )\n        val_loss = calc_loss_loader(\n            val_loader, model, device, num_batches=eval_iter\n        )\n    model.train()\n    return train_loss, val_loss\n```", "```py\nimport time\n\nstart_time = time.time()\ntorch.manual_seed(123)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\nnum_epochs = 5\n\ntrain_losses, val_losses, train_accs, val_accs, examples_seen = \\\n    train_classifier_simple(\n        model, train_loader, val_loader, optimizer, device,\n        num_epochs=num_epochs, eval_freq=50,\n        eval_iter=5\n    )\n\nend_time = time.time()\nexecution_time_minutes = (end_time - start_time) / 60\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n```", "```py\nEp 1 (Step 000000): Train loss 2.153, Val loss 2.392\nEp 1 (Step 000050): Train loss 0.617, Val loss 0.637\nEp 1 (Step 000100): Train loss 0.523, Val loss 0.557\nTraining accuracy: 70.00% | Validation accuracy: 72.50%\nEp 2 (Step 000150): Train loss 0.561, Val loss 0.489\nEp 2 (Step 000200): Train loss 0.419, Val loss 0.397\nEp 2 (Step 000250): Train loss 0.409, Val loss 0.353\nTraining accuracy: 82.50% | Validation accuracy: 85.00%\nEp 3 (Step 000300): Train loss 0.333, Val loss 0.320\nEp 3 (Step 000350): Train loss 0.340, Val loss 0.306\nTraining accuracy: 90.00% | Validation accuracy: 90.00%\nEp 4 (Step 000400): Train loss 0.136, Val loss 0.200\nEp 4 (Step 000450): Train loss 0.153, Val loss 0.132\nEp 4 (Step 000500): Train loss 0.222, Val loss 0.137\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\nEp 5 (Step 000550): Train loss 0.207, Val loss 0.143\nEp 5 (Step 000600): Train loss 0.083, Val loss 0.074\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\nTraining completed in 5.65 minutes.\n```", "```py\nimport matplotlib.pyplot as plt\n\ndef plot_values(\n        epochs_seen, examples_seen, train_values, val_values,\n        label=\"loss\"):\n    fig, ax1 = plt.subplots(figsize=(5, 3))\n\n #1\n    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n    ax1.plot(\n        epochs_seen, val_values, linestyle=\"-.\",\n        label=f\"Validation {label}\"\n    )\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(label.capitalize())\n    ax1.legend()\n\n #2\n    ax2 = ax1.twiny()\n    ax2.plot(examples_seen, train_values, alpha=0)    #3\n    ax2.set_xlabel(\"Examples seen\")\n\n    fig.tight_layout()             #4\n    plt.savefig(f\"{label}-plot.pdf\")\n    plt.show()\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n\nplot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)\n```", "```py\nepochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n\nplot_values(\n    epochs_tensor, examples_seen_tensor, train_accs, val_accs,\n    label=\"accuracy\"\n)\n```", "```py\ntrain_accuracy = calc_accuracy_loader(train_loader, model, device)\nval_accuracy = calc_accuracy_loader(val_loader, model, device)\ntest_accuracy = calc_accuracy_loader(test_loader, model, device)\n\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n```", "```py\nTraining accuracy: 97.21%\nValidation accuracy: 97.32%\nTest accuracy: 95.67%\n```", "```py\ndef classify_review(\n        text, model, tokenizer, device, max_length=None,\n        pad_token_id=50256):\n    model.eval()\n\n    input_ids = tokenizer.encode(text)          #1\n    supported_context_length = model.pos_emb.weight.shape[0]\n\n    input_ids = input_ids[:min(              #2\n        max_length, supported_context_length\n    )]\n\n    input_ids += [pad_token_id] * (max_length - len(input_ids))    #3\n\n    input_tensor = torch.tensor(\n        input_ids, device=device\n    ).unsqueeze(0)              #4\n\n    with torch.no_grad():                                #5\n        logits = model(input_tensor)[:, -1, :]     #6\n    predicted_label = torch.argmax(logits, dim=-1).item()\n\n    return \"spam\" if predicted_label == 1 else \"not spam\"     #7\n```", "```py\ntext_1 = (\n    \"You are a winner you have been specially\"\n    \" selected to receive $1000 cash or a $2000 award.\"\n)\n\nprint(classify_review(\n    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n))\n```", "```py\ntext_2 = (\n    \"Hey, just wanted to check if we're still on\"\n    \" for dinner tonight? Let me know!\"\n)\n\nprint(classify_review(\n    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n))\n```", "```py\ntorch.save(model.state_dict(), \"review_classifier.pth\")\n```", "```py\nmodel_state_dict = torch.load(\"review_classifier.pth, map_location=device\")\nmodel.load_state_dict(model_state_dict)\n```"]